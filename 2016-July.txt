From drjimlemon at gmail.com  Fri Jul  1 00:05:59 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 1 Jul 2016 08:05:59 +1000
Subject: [R] Writing a formula to Excel
In-Reply-To: <OFD4FC08B0.DD7A26D4-ONC1257FE2.00445D55-C1257FE2.00451CC2@lotus.hawesko.de>
References: <OFD4FC08B0.DD7A26D4-ONC1257FE2.00445D55-C1257FE2.00451CC2@lotus.hawesko.de>
Message-ID: <CA+8X3fXiwFuymHO4kA5oL7+7Lvr8JpoxTsQh5Ou6_23mG8w_jw@mail.gmail.com>

Hi,
I don't have excel.link, but have you tried:

"=G9*100/G6\n"

Jim

On Thu, Jun 30, 2016 at 10:34 PM,  <G.Maubach at weinwolf.de> wrote:
> Hi All,
>
> I am using excel.link to work seemslessly with Excel.
>
> In addition to values, like numbers and strings, I would like to insert a
> full operational formula into a cell.
>
>
> xlc["G14"] <- print(paste("=G9*100/G6"), quote = FALSE)
>
>
> The strings is put into the cell, but the cell is not evaluated. Thus the
> string is show as result of the computation.
>
> If I open that cell b pressing "F2" or by double-clicking the cell and
> pressing RETURN will start the evaluation of the expession.
>
>
> xlc["G14"] <- parse("=G9*100/G6") # does not run
>
>
> How can I put a formula into Excel that is evaluated right away?
>
> Kind regards
>
> Georg
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From paul at stat.auckland.ac.nz  Fri Jul  1 00:51:20 2016
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Fri, 1 Jul 2016 10:51:20 +1200
Subject: [R] [FORGED] Re: Can I increase the size of an asterisk in
 plotmath()?
In-Reply-To: <CAM_vju=vwKv73zxeS0NSUOiwqW1AO--8NnM48Y1_Q+RAMSE0mQ@mail.gmail.com>
References: <6a5aecfe-c095-780f-ee9d-c47ace0e2e8f@auckland.ac.nz>
	<dd6eba2b-f08d-8be1-19da-dfaacb68607a@umu.se>
	<eeb7ccc6-1e44-f665-530d-cd258a7128e8@auckland.ac.nz>
	<CAM_vju=vwKv73zxeS0NSUOiwqW1AO--8NnM48Y1_Q+RAMSE0mQ@mail.gmail.com>
Message-ID: <5775A268.8060309@stat.auckland.ac.nz>

Hi

There are a couple of plotmath expressions that control text size, plus 
in this case you could just drop the superscripting (which I believe was 
Goran's advice) ...

plot(1:10, type="n")
text(2, 8, expression(paste(bolditalic(p)^"*"))) #Rolf's original
text(2, 6, expression(paste(bolditalic(p)^textstyle("*"))))
text(2, 4, expression(paste(bolditalic(p)*"*")))

Paul

On 01/07/16 01:28, Sarah Goslee wrote:
> I don't know of an automatic way, but of course you can make it as
> large as you'd like.
>
>
> plot(1:10, type="n")
> text(2, 8, expression(paste(bolditalic(p)^"*"))) #Rolf's original
> text(2, 6,expression(paste(bolditalic(p),"*", sep = ""))) #Goran's suggestion
>
>
> text(2, 3, expression(paste(bolditalic(p)^"*"))) # repeat Rolf's original
> text(locator(1), "*", cex=3) # click on the asterisk: how big do you want it?
>
> Sarah
>
> On Thu, Jun 30, 2016 at 4:46 AM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>> On 30/06/16 19:38, G?ran Brostr?m wrote:
>>>
>>> Rolf,
>>>
>>> text(7.5,2.5,expression(paste(bolditalic(p),"*", sep = "")))
>>>
>>> looks quite nice. On my Mac at least.
>>
>>
>> <SNIP>
>>
>> Well, it didn't look nice on my laptop.  The asterisk was nearly invisible.
>>
>> cheers,
>>
>> Rolf
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From drjimlemon at gmail.com  Fri Jul  1 01:33:00 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 1 Jul 2016 09:33:00 +1000
Subject: [R] Command to combine means?
In-Reply-To: <CAFh+_DzCWxJ8NAzLTS+dnOjjQGd0SdXzz6RcxT_hK-yd6FVcTg@mail.gmail.com>
References: <CAFh+_DzCWxJ8NAzLTS+dnOjjQGd0SdXzz6RcxT_hK-yd6FVcTg@mail.gmail.com>
Message-ID: <CA+8X3fUYYjKBRubZTbkJf-_D=GiEsK3h40Rn3AwJ0yFb_3M9jg@mail.gmail.com>

Hi Carlos,
The STATA function assumes estimated population SDs. If you have
sample SDs you can specify with this:

combine<-function(n,mu,sd,sd.type=c("pop","sample")) {
 N<-sum(n)
 mean<-sum(n*mu)/N
 if(sd.type[1]=="sample") {
  meanss<-(n[1]*(mean-mu[1])^2+n[2]*(mean-mu[2])^2)/N
  SD<-sqrt(sum(sd*sd*n)/N+meanss)
 } else {
  meanss<-(n[1]*(mean-mu[1])^2+n[2]*(mean-mu[2])^2)/(N-1)
  SD<-sqrt(sum(sd*sd*(n-1))/(N-1)+meanss)
 }
 return(list(N,mean,SD))
}
combine(c(12,34),c(3,45),c(1,4))
combine(c(12,34),c(3,45),c(1,4),"sample")

Jim


On Fri, Jul 1, 2016 at 1:02 AM, Carlos R. Moreira Maia
<crm.maia at gmail.com> wrote:
> Dear all,
> I'm newbie with R (changing from STATA), and I can't find some commands.
> One of those is the "combine", which I use to combine means like this:
>
> --------------------------------------------
> n1 m1 sd1  n2 m2 sd2
>
> combine 12 3 1 34 45 4
>
> Combine has calculated the following values:
>     combined n = 46
>     combined mean = 34.043478
>     combined SD = 18.964829
> --------------------------------------------
>
> Does anybody knows a simmilar command in R to combine means?
>
> Thanks in advance.
>
> Carlos.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Fri Jul  1 04:50:56 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 1 Jul 2016 14:50:56 +1200
Subject: [R] Can I increase the size of an asterisk in plotmath()?
In-Reply-To: <CAM_vju=vwKv73zxeS0NSUOiwqW1AO--8NnM48Y1_Q+RAMSE0mQ@mail.gmail.com>
References: <6a5aecfe-c095-780f-ee9d-c47ace0e2e8f@auckland.ac.nz>
	<dd6eba2b-f08d-8be1-19da-dfaacb68607a@umu.se>
	<eeb7ccc6-1e44-f665-530d-cd258a7128e8@auckland.ac.nz>
	<CAM_vju=vwKv73zxeS0NSUOiwqW1AO--8NnM48Y1_Q+RAMSE0mQ@mail.gmail.com>
Message-ID: <1581cdfa-df35-3215-fa25-e59307ed5273@auckland.ac.nz>

On 01/07/16 01:28, Sarah Goslee wrote:
> I don't know of an automatic way, but of course you can make it as
> large as you'd like.
>
>
> plot(1:10, type="n")
> text(2, 8, expression(paste(bolditalic(p)^"*"))) #Rolf's original
> text(2, 6,expression(paste(bolditalic(p),"*", sep = ""))) #Goran's suggestion
>
>
> text(2, 3, expression(paste(bolditalic(p)^"*"))) # repeat Rolf's original
> text(locator(1), "*", cex=3) # click on the asterisk: how big do you want it?

That's certainly a possibility, but it requires interactive use, which I 
would like to avoid.  Ideally I would like a solution that is completely 
"code-able".

Thanks.

cheers,

Rolf


-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Fri Jul  1 05:02:13 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 1 Jul 2016 15:02:13 +1200
Subject: [R] [FORGED] Re: Can I increase the size of an asterisk in
 plotmath()?
In-Reply-To: <5775A268.8060309@stat.auckland.ac.nz>
References: <6a5aecfe-c095-780f-ee9d-c47ace0e2e8f@auckland.ac.nz>
	<dd6eba2b-f08d-8be1-19da-dfaacb68607a@umu.se>
	<eeb7ccc6-1e44-f665-530d-cd258a7128e8@auckland.ac.nz>
	<CAM_vju=vwKv73zxeS0NSUOiwqW1AO--8NnM48Y1_Q+RAMSE0mQ@mail.gmail.com>
	<5775A268.8060309@stat.auckland.ac.nz>
Message-ID: <400d7498-4f7d-9dd4-4ad3-c9f703be9c5e@auckland.ac.nz>

On 01/07/16 10:51, Paul Murrell wrote:
> Hi
>
> There are a couple of plotmath expressions that control text size, plus
> in this case you could just drop the superscripting (which I believe was
> Goran's advice) ...
>
> plot(1:10, type="n")
> text(2, 8, expression(paste(bolditalic(p)^"*"))) #Rolf's original
> text(2, 6, expression(paste(bolditalic(p)^textstyle("*"))))
> text(2, 4, expression(paste(bolditalic(p)*"*")))

Thanks Paul.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From crm.maia at gmail.com  Fri Jul  1 05:06:00 2016
From: crm.maia at gmail.com (Carlos Renato M. Maia)
Date: Fri, 1 Jul 2016 00:06:00 -0300
Subject: [R] Command to combine means?
In-Reply-To: <CA+8X3fUYYjKBRubZTbkJf-_D=GiEsK3h40Rn3AwJ0yFb_3M9jg@mail.gmail.com>
References: <CAFh+_DzCWxJ8NAzLTS+dnOjjQGd0SdXzz6RcxT_hK-yd6FVcTg@mail.gmail.com>
	<CA+8X3fUYYjKBRubZTbkJf-_D=GiEsK3h40Rn3AwJ0yFb_3M9jg@mail.gmail.com>
Message-ID: <854198F3-DBA7-436E-82AF-8DB2A68359F5@gmail.com>

thank you very much Jim!
I'll try it.
bw
c

Sent from my iPhone

> Em 30/06/2016, ?s 20:33, Jim Lemon <drjimlemon at gmail.com> escreveu:
> 
> Hi Carlos,
> The STATA function assumes estimated population SDs. If you have
> sample SDs you can specify with this:
> 
> combine<-function(n,mu,sd,sd.type=c("pop","sample")) {
> N<-sum(n)
> mean<-sum(n*mu)/N
> if(sd.type[1]=="sample") {
>  meanss<-(n[1]*(mean-mu[1])^2+n[2]*(mean-mu[2])^2)/N
>  SD<-sqrt(sum(sd*sd*n)/N+meanss)
> } else {
>  meanss<-(n[1]*(mean-mu[1])^2+n[2]*(mean-mu[2])^2)/(N-1)
>  SD<-sqrt(sum(sd*sd*(n-1))/(N-1)+meanss)
> }
> return(list(N,mean,SD))
> }
> combine(c(12,34),c(3,45),c(1,4))
> combine(c(12,34),c(3,45),c(1,4),"sample")
> 
> Jim
> 
> 
> On Fri, Jul 1, 2016 at 1:02 AM, Carlos R. Moreira Maia
> <crm.maia at gmail.com> wrote:
>> Dear all,
>> I'm newbie with R (changing from STATA), and I can't find some commands.
>> One of those is the "combine", which I use to combine means like this:
>> 
>> --------------------------------------------
>> n1 m1 sd1  n2 m2 sd2
>> 
>> combine 12 3 1 34 45 4
>> 
>> Combine has calculated the following values:
>>    combined n = 46
>>    combined mean = 34.043478
>>    combined SD = 18.964829
>> --------------------------------------------
>> 
>> Does anybody knows a simmilar command in R to combine means?
>> 
>> Thanks in advance.
>> 
>> Carlos.
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Fri Jul  1 05:29:45 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 1 Jul 2016 13:29:45 +1000
Subject: [R] Can I increase the size of an asterisk in plotmath()?
In-Reply-To: <1581cdfa-df35-3215-fa25-e59307ed5273@auckland.ac.nz>
References: <6a5aecfe-c095-780f-ee9d-c47ace0e2e8f@auckland.ac.nz>
	<dd6eba2b-f08d-8be1-19da-dfaacb68607a@umu.se>
	<eeb7ccc6-1e44-f665-530d-cd258a7128e8@auckland.ac.nz>
	<CAM_vju=vwKv73zxeS0NSUOiwqW1AO--8NnM48Y1_Q+RAMSE0mQ@mail.gmail.com>
	<1581cdfa-df35-3215-fa25-e59307ed5273@auckland.ac.nz>
Message-ID: <CA+8X3fUc8je8v7417tveCFBiOf88jBBc5dDRceHh5qBsvhSDvA@mail.gmail.com>

Hi Rolf,
A bit of poking around reveals that different fonts have different
size asterisks. If you are not already using Times Roman, it might be
worth a look. I only have Windows on this (work) PC, so I can't check
the Postscript fonts.

Jim

On Fri, Jul 1, 2016 at 12:50 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> On 01/07/16 01:28, Sarah Goslee wrote:
>>
>> I don't know of an automatic way, but of course you can make it as
>> large as you'd like.
>>
>>
>> plot(1:10, type="n")
>> text(2, 8, expression(paste(bolditalic(p)^"*"))) #Rolf's original
>> text(2, 6,expression(paste(bolditalic(p),"*", sep = ""))) #Goran's
>> suggestion
>>
>>
>> text(2, 3, expression(paste(bolditalic(p)^"*"))) # repeat Rolf's original
>> text(locator(1), "*", cex=3) # click on the asterisk: how big do you want
>> it?
>
>
> That's certainly a possibility, but it requires interactive use, which I
> would like to avoid.  Ideally I would like a solution that is completely
> "code-able".
>
> Thanks.
>
> cheers,
>
> Rolf
>
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Fri Jul  1 05:58:29 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 1 Jul 2016 15:58:29 +1200
Subject: [R] Can I increase the size of an asterisk in plotmath()?
In-Reply-To: <CA+8X3fUc8je8v7417tveCFBiOf88jBBc5dDRceHh5qBsvhSDvA@mail.gmail.com>
References: <6a5aecfe-c095-780f-ee9d-c47ace0e2e8f@auckland.ac.nz>
	<dd6eba2b-f08d-8be1-19da-dfaacb68607a@umu.se>
	<eeb7ccc6-1e44-f665-530d-cd258a7128e8@auckland.ac.nz>
	<CAM_vju=vwKv73zxeS0NSUOiwqW1AO--8NnM48Y1_Q+RAMSE0mQ@mail.gmail.com>
	<1581cdfa-df35-3215-fa25-e59307ed5273@auckland.ac.nz>
	<CA+8X3fUc8je8v7417tveCFBiOf88jBBc5dDRceHh5qBsvhSDvA@mail.gmail.com>
Message-ID: <6a299c2a-0fe6-560d-14ca-6728fe7a5858@auckland.ac.nz>

On 01/07/16 15:29, Jim Lemon wrote:
> Hi Rolf,
> A bit of poking around reveals that different fonts have different
> size asterisks. If you are not already using Times Roman, it might be
> worth a look. I only have Windows on this (work) PC, so I can't check
> the Postscript fonts.

Thanks Jim.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ruipbarradas at sapo.pt  Fri Jul  1 11:22:02 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Fri, 01 Jul 2016 10:22:02 +0100
Subject: [R] Merge several datasets into one
In-Reply-To: <CAN5afy94Qgmqbnn1jMdzVAoP-H2jfw=njLOH4sUw3mSLCT_xog@mail.gmail.com>
Message-ID: <20160701102202.Horde.xWUyVAUtIwLTfWIXNvWGzXL@mail.sapo.pt>

Hello,

Maybe something like this.

fls <- list.files(pattern = "*.csv")
dat.list <- lapply(fls, read.csv)
dat <- do.call(rbind, dat.list)

Hope this helps,

Rui Barradas
?

Citando lily li <chocold12 at gmail.com>:

> Hi R users,
>
> I'd like to ask that how to merge several datasets into one in R? I put
> these csv files in one folder, and use the lapply function, but it says
> that cannot open file 'xx.csv'. These files have different names, but end
> with .csv extension, and the files have the same header. Thanks for your
> help.
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From faradj.g at gmail.com  Fri Jul  1 14:20:19 2016
From: faradj.g at gmail.com (Faradj Koliev)
Date: Fri, 1 Jul 2016 14:20:19 +0200
Subject: [R] Logistic regression and robust standard errors
Message-ID: <B6E512A5-037A-4E4B-B2DA-A62E29FC470C@gmail.com>

Dear all, 


I use ?polr? command (library: MASS) to estimate an ordered logistic regression.

My model:   summary( model<- polr(y ~ x1+x2+x3+x4+x1*x2 ,data=mydata, Hess = TRUE))

But how do I get robust clustered standard errors? 

I??ve tried   coeftest(resA, vcov=vcovHC(resA, cluster=lipton$ID)) and summary(a <- robcov(model,mydata$ID)). Neither works for me. So I wonder what am I doing wrong here? 


All suggestions are welcome ? thank you! 
	[[alternative HTML version deleted]]


From Achim.Zeileis at uibk.ac.at  Fri Jul  1 14:57:38 2016
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Fri, 1 Jul 2016 14:57:38 +0200 (CEST)
Subject: [R] Logistic regression and robust standard errors
In-Reply-To: <B6E512A5-037A-4E4B-B2DA-A62E29FC470C@gmail.com>
References: <B6E512A5-037A-4E4B-B2DA-A62E29FC470C@gmail.com>
Message-ID: <alpine.DEB.2.20.1607011452130.8909@paninaro>

On Fri, 1 Jul 2016, Faradj Koliev wrote:

> Dear all, 
>
>
> I use ?polr? command (library: MASS) to estimate an ordered logistic regression.
>
> My model:   summary( model<- polr(y ~ x1+x2+x3+x4+x1*x2 ,data=mydata, Hess = TRUE))
>
> But how do I get robust clustered standard errors? 
>
> I??ve tried coeftest(resA, vcov=vcovHC(resA, cluster=lipton$ID))

The vcovHC() function currently does not (yet) have a "cluster" argument. 
We are working on it but it's not finished yet.

As an alternative I include the vcovCL() function below that computes the 
usual simple clustered sandwich estimator. This can be applied to "polr" 
objects and plugged into coeftest(). So

coeftest(resA, vcov=vcovCL(resA, cluster=lipton$ID))

should work.

> and summary(a <- robcov(model,mydata$ID)).

The robcov() function does in principle what you want by I'm not sure 
whether it works with polr(). But for sure it works with lrm() from the 
"rms" package.

Hope that helps,
Z

vcovCL <- function(object, cluster = NULL, adjust = NULL)
{
   stopifnot(require("sandwich"))

   ## cluster specification
   if(is.null(cluster)) cluster <- attr(object, "cluster")
   if(is.null(cluster)) stop("no 'cluster' specification found")
   cluster <- factor(cluster)

   ## estimating functions and dimensions
   ef <- estfun(object)
   n <- NROW(ef)
   k <- NCOL(ef)
   if(n != length(cluster))
     stop("length of 'cluster' does not match number of observations")
   m <- length(levels(cluster))

   ## aggregate estimating functions by cluster and compute meat
   ef <- sapply(levels(cluster), function(i) colSums(ef[cluster == i, ,
     drop = FALSE]))
   ef <- if(NCOL(ef) > 1L) t(ef) else matrix(ef, ncol = 1L)
   mt <- crossprod(ef)/n

   ## bread
   br <- try(bread(object), silent = TRUE)
   if(inherits(br, "try-error")) br <- vcov(object) * n

   ## put together sandwich
   vc <- 1/n * (br %*% mt %*% br)

   ## adjustment
   if(is.null(adjust)) adjust <- class(object)[1L] == "lm"
   adj <- if(adjust) m/(m - 1L) * (n - 1L)/(n - k) else m/(m - 1L)

   ## return
   return(adj * vc)
}


> Neither works for me. So I wonder what am I doing wrong here?
>
>
> All suggestions are welcome ? thank you!
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lid.zigh at gmail.com  Fri Jul  1 16:12:14 2016
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Fri, 1 Jul 2016 09:12:14 -0500
Subject: [R] Merge several datasets into one
In-Reply-To: <CAN5afy94Qgmqbnn1jMdzVAoP-H2jfw=njLOH4sUw3mSLCT_xog@mail.gmail.com>
References: <CAN5afy94Qgmqbnn1jMdzVAoP-H2jfw=njLOH4sUw3mSLCT_xog@mail.gmail.com>
Message-ID: <CAMqbV1BTV1G5RZgMbZZM=2PYTtPkBMMsGVHorU0WpbDCuZkOhg@mail.gmail.com>

Hi Lily,

I think below codes can work:

 f<- list.files("D:/output/test/your folde
rname",full.names=TRUE,recursive=TRUE)

files<- grep(".csv", f)

files_merge<- data.frame()
for (i in 1:length(f[files])){
  data<- read.csv(file=f[files][i],header=TRUE, sep=",")
  files_merge<-  rbind(files_merge,data)
}

Best,
Lida

On Thu, Jun 30, 2016 at 2:26 PM, lily li <chocold12 at gmail.com> wrote:

> Hi R users,
>
> I'd like to ask that how to merge several datasets into one in R? I put
> these csv files in one folder, and use the lapply function, but it says
> that cannot open file 'xx.csv'. These files have different names, but end
> with .csv extension, and the files have the same header. Thanks for your
> help.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From faradj.g at gmail.com  Fri Jul  1 16:12:32 2016
From: faradj.g at gmail.com (Faradj Koliev)
Date: Fri, 1 Jul 2016 16:12:32 +0200
Subject: [R] Logistic regression and robust standard errors
In-Reply-To: <alpine.DEB.2.20.1607011452130.8909@paninaro>
References: <B6E512A5-037A-4E4B-B2DA-A62E29FC470C@gmail.com>
	<alpine.DEB.2.20.1607011452130.8909@paninaro>
Message-ID: <C86276CD-7CC5-4C40-910B-96DD871A6F68@gmail.com>

Dear Achim Zeileis, 

Many thanks for your quick and informative answer. 

I?m sure that the vcovCL should work, however, I experience some problems. 


> coeftest(model, vcov=vcovCL(model, cluster=mydata$ID))

First I got this error: 

Error in vcovCL(model, cluster = mydata$ID) : 
  length of 'cluster' does not match number of observations

After checking the observations I got this error: 

Error in vcovCL(model, cluster = mydata$ID) : object 'tets' not found
Called from: vcovCL(model, cluster = mydata$ID)
Browse[1]> 

What can I do to fix it? What am I doing wrong now? 





> 1 jul 2016 kl. 14:57 skrev Achim Zeileis <Achim.Zeileis at uibk.ac.at>:
> 
> On Fri, 1 Jul 2016, Faradj Koliev wrote:
> 
>> Dear all, 
>> 
>> I use ?polr? command (library: MASS) to estimate an ordered logistic regression.
>> 
>> My model:   summary( model<- polr(y ~ x1+x2+x3+x4+x1*x2 ,data=mydata, Hess = TRUE))
>> 
>> But how do I get robust clustered standard errors? 
>> I??ve tried coeftest(resA, vcov=vcovHC(resA, cluster=lipton$ID))
> 
> The vcovHC() function currently does not (yet) have a "cluster" argument. We are working on it but it's not finished yet.
> 
> As an alternative I include the vcovCL() function below that computes the usual simple clustered sandwich estimator. This can be applied to "polr" objects and plugged into coeftest(). So
> 
> coeftest(resA, vcov=vcovCL(resA, cluster=lipton$ID))
> 
> should work.
> 
>> and summary(a <- robcov(model,mydata$ID)).
> 
> The robcov() function does in principle what you want by I'm not sure whether it works with polr(). But for sure it works with lrm() from the "rms" package.
> 
> Hope that helps,
> Z
> 
> vcovCL <- function(object, cluster = NULL, adjust = NULL)
> {
>  stopifnot(require("sandwich"))
> 
>  ## cluster specification
>  if(is.null(cluster)) cluster <- attr(object, "cluster")
>  if(is.null(cluster)) stop("no 'cluster' specification found")
>  cluster <- factor(cluster)
> 
>  ## estimating functions and dimensions
>  ef <- estfun(object)
>  n <- NROW(ef)
>  k <- NCOL(ef)
>  if(n != length(cluster))
>    stop("length of 'cluster' does not match number of observations")
>  m <- length(levels(cluster))
> 
>  ## aggregate estimating functions by cluster and compute meat
>  ef <- sapply(levels(cluster), function(i) colSums(ef[cluster == i, ,
>    drop = FALSE]))
>  ef <- if(NCOL(ef) > 1L) t(ef) else matrix(ef, ncol = 1L)
>  mt <- crossprod(ef)/n
> 
>  ## bread
>  br <- try(bread(object), silent = TRUE)
>  if(inherits(br, "try-error")) br <- vcov(object) * n
> 
>  ## put together sandwich
>  vc <- 1/n * (br %*% mt %*% br)
> 
>  ## adjustment
>  if(is.null(adjust)) adjust <- class(object)[1L] == "lm"
>  adj <- if(adjust) m/(m - 1L) * (n - 1L)/(n - k) else m/(m - 1L)
> 
>  ## return
>  return(adj * vc)
> }
> 
> 
>> Neither works for me. So I wonder what am I doing wrong here?
>> 
>> 
>> All suggestions are welcome ? thank you!
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From syen04 at gmail.com  Fri Jul  1 16:15:30 2016
From: syen04 at gmail.com (Steven Yen)
Date: Fri, 1 Jul 2016 10:15:30 -0400
Subject: [R] Column product
Message-ID: <f049550b-e998-93ff-a1bd-ec230e4f5d19@gmail.com>

A is a 5 x 3 matrix and a is a 3-vector. I like to exponentiate A[,1] to 
a[1], A[,2] to a[2], and A[,3] to a[3], and obtain the product of the 
resulting columns, as in line 3.

I also accomplish this with lines 4 and 5. I like to have rowProducts(B) 
but there is not so I came up with something ugly in line 
5--exponentiating the row sums of log. Is there a more elegant way than 
than line 5 or, better yet, lines 4 and 5 together? Thanks.

A<-matrix(1:15,nrow=5,byrow=F); A
a<-c(1,2,3)
(A[,1]^a[1])*(A[,2]^a[2])*(A[,3]^a[3])
B<-t(t(A)^a); B
exp(rowSums(log(B)))

Result:

 > A<-matrix(1:15,nrow=5,byrow=F); A
      [,1] [,2] [,3]
[1,]    1    6   11
[2,]    2    7   12
[3,]    3    8   13
[4,]    4    9   14
[5,]    5   10   15
 > a<-c(1,2,3)
 > (A[,1]^a[1])*(A[,2]^a[2])*(A[,3]^a[3])
[1]   47916  169344  421824  889056 1687500
 > B<-t(t(A)^a); B
      [,1] [,2] [,3]
[1,]    1   36 1331
[2,]    2   49 1728
[3,]    3   64 2197
[4,]    4   81 2744
[5,]    5  100 3375
 > exp(rowSums(log(B)))
[1]   47916  169344  421824  889056 1687500
 >


	[[alternative HTML version deleted]]


From Achim.Zeileis at uibk.ac.at  Fri Jul  1 16:23:02 2016
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Fri, 1 Jul 2016 16:23:02 +0200 (CEST)
Subject: [R] Logistic regression and robust standard errors
In-Reply-To: <C86276CD-7CC5-4C40-910B-96DD871A6F68@gmail.com>
References: <B6E512A5-037A-4E4B-B2DA-A62E29FC470C@gmail.com>
	<alpine.DEB.2.20.1607011452130.8909@paninaro>
	<C86276CD-7CC5-4C40-910B-96DD871A6F68@gmail.com>
Message-ID: <alpine.DEB.2.20.1607011619400.8909@paninaro>

On Fri, 1 Jul 2016, Faradj Koliev wrote:

> Dear Achim Zeileis,?
> Many thanks for your quick and informative answer.?
> 
> I?m sure that the vcovCL should work, however, I experience some problems.?
> 
> 
> > coeftest(model, vcov=vcovCL(model, cluster=mydata$ID))
> 
> First I got this error:?
> 
> Error in vcovCL(model, cluster = mydata$ID) :?
> ? length of 'cluster' does not match number of observations

This probably means that not all rows of "mydata" have been used for the 
estimation of the model, e.g., due to missing values or something like 
that. Hence, the mismatch seems to occur.

> After checking the observations I got this error:?
> 
> Error in vcovCL(model, cluster = mydata$ID) : object 'tets' not found
> Called from: vcovCL(model, cluster = mydata$ID)
> Browse[1]>?

The variable "tets" is presumably one of the regressors in your "model" 
and apparently it cannot be found when extracting the model scores. Maybe 
you haven't stored the model frame and deleted the data.

Hard to say without a simple reproducible example.
Z

> What can I do to fix it? What am I doing wrong now??
> 
> 
> 
> 
>
>       1 jul 2016 kl. 14:57 skrev Achim Zeileis
>       <Achim.Zeileis at uibk.ac.at>:
> 
> On Fri, 1 Jul 2016, Faradj Koliev wrote:
>
>       Dear all,
>
>       I use ?polr? command (library: MASS) to estimate an
>       ordered logistic regression.
>
>       My model: ??summary( model<- polr(y ~ x1+x2+x3+x4+x1*x2
>       ,data=mydata, Hess = TRUE))
>
>       But how do I get robust clustered standard errors?
>       I??ve tried coeftest(resA, vcov=vcovHC(resA,
>       cluster=lipton$ID))
> 
> 
> The vcovHC() function currently does not (yet) have a "cluster"
> argument. We are working on it but it's not finished yet.
> 
> As an alternative I include the vcovCL() function below that computes
> the usual simple clustered sandwich estimator. This can be applied to
> "polr" objects and plugged into coeftest(). So
> 
> coeftest(resA, vcov=vcovCL(resA, cluster=lipton$ID))
> 
> should work.
>
>       and summary(a <- robcov(model,mydata$ID)).
> 
> 
> The robcov() function does in principle what you want by I'm not sure
> whether it works with polr(). But for sure it works with lrm() from
> the "rms" package.
> 
> Hope that helps,
> Z
> 
> vcovCL <- function(object, cluster = NULL, adjust = NULL)
> {
> ?stopifnot(require("sandwich"))
> 
> ?## cluster specification
> ?if(is.null(cluster)) cluster <- attr(object, "cluster")
> ?if(is.null(cluster)) stop("no 'cluster' specification found")
> ?cluster <- factor(cluster)
> 
> ?## estimating functions and dimensions
> ?ef <- estfun(object)
> ?n <- NROW(ef)
> ?k <- NCOL(ef)
> ?if(n != length(cluster))
> ???stop("length of 'cluster' does not match number of observations")
> ?m <- length(levels(cluster))
> 
> ?## aggregate estimating functions by cluster and compute meat
> ?ef <- sapply(levels(cluster), function(i) colSums(ef[cluster == i, ,
> ???drop = FALSE]))
> ?ef <- if(NCOL(ef) > 1L) t(ef) else matrix(ef, ncol = 1L)
> ?mt <- crossprod(ef)/n
> 
> ?## bread
> ?br <- try(bread(object), silent = TRUE)
> ?if(inherits(br, "try-error")) br <- vcov(object) * n
> 
> ?## put together sandwich
> ?vc <- 1/n * (br %*% mt %*% br)
> 
> ?## adjustment
> ?if(is.null(adjust)) adjust <- class(object)[1L] == "lm"
> ?adj <- if(adjust) m/(m - 1L) * (n - 1L)/(n - k) else m/(m - 1L)
> 
> ?## return
> ?return(adj * vc)
> }
> 
>
>       Neither works for me. So I wonder what am I doing wrong
>       here?
> 
>
>       All suggestions are welcome ? thank you!
>       [[alternative HTML version deleted]]
>
>       ______________________________________________
>       R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>       more, see
>       https://stat.ethz.ch/mailman/listinfo/r-help
>       PLEASE do read the posting guide
>       http://www.R-project.org/posting-guide.html
>       and provide commented, minimal, self-contained,
>       reproducible code.
> 
> 
> 
>

From sarah.goslee at gmail.com  Fri Jul  1 16:33:53 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 1 Jul 2016 10:33:53 -0400
Subject: [R] Column product
In-Reply-To: <f049550b-e998-93ff-a1bd-ec230e4f5d19@gmail.com>
References: <f049550b-e998-93ff-a1bd-ec230e4f5d19@gmail.com>
Message-ID: <CAM_vjunKSnkp7XoY4tPTGyB4UQJP7=ae1Jj5FCFYdxtFkLEHQQ@mail.gmail.com>

Hi,

I think this is substantially less ugly:


A <- matrix(1:15,nrow=5,byrow=F); A
a <- c(1,2,3)

B <- sweep(A, 2, a, "^")
apply(B, 1, prod)

You could combine it into one line if you wanted, but I find it clearer as two:

> apply(sweep(A, 2, a, "^"), 1, prod)
[1]   47916  169344  421824  889056 1687500


Sarah


On Fri, Jul 1, 2016 at 10:15 AM, Steven Yen <syen04 at gmail.com> wrote:
> A is a 5 x 3 matrix and a is a 3-vector. I like to exponentiate A[,1] to
> a[1], A[,2] to a[2], and A[,3] to a[3], and obtain the product of the
> resulting columns, as in line 3.
>
> I also accomplish this with lines 4 and 5. I like to have rowProducts(B)
> but there is not so I came up with something ugly in line
> 5--exponentiating the row sums of log. Is there a more elegant way than
> than line 5 or, better yet, lines 4 and 5 together? Thanks.
>
> A<-matrix(1:15,nrow=5,byrow=F); A
> a<-c(1,2,3)
> (A[,1]^a[1])*(A[,2]^a[2])*(A[,3]^a[3])
> B<-t(t(A)^a); B
> exp(rowSums(log(B)))
>
> Result:
>
>  > A<-matrix(1:15,nrow=5,byrow=F); A
>       [,1] [,2] [,3]
> [1,]    1    6   11
> [2,]    2    7   12
> [3,]    3    8   13
> [4,]    4    9   14
> [5,]    5   10   15
>  > a<-c(1,2,3)
>  > (A[,1]^a[1])*(A[,2]^a[2])*(A[,3]^a[3])
> [1]   47916  169344  421824  889056 1687500
>  > B<-t(t(A)^a); B
>       [,1] [,2] [,3]
> [1,]    1   36 1331
> [2,]    2   49 1728
> [3,]    3   64 2197
> [4,]    4   81 2744
> [5,]    5  100 3375
>  > exp(rowSums(log(B)))
> [1]   47916  169344  421824  889056 1687500
>  >


From wewolski at gmail.com  Fri Jul  1 16:40:27 2016
From: wewolski at gmail.com (Witold E Wolski)
Date: Fri, 1 Jul 2016 16:40:27 +0200
Subject: [R] [FORGED] Splitting data.frame into a list of small
 data.frames given indices
In-Reply-To: <CAF8bMcZbSRQqOr_WUJyQ_3nNoUoBiUFhrhxa1tFZo+vfksC=3w@mail.gmail.com>
References: <CAAjnpdispjvZL_hBzR-vFLqdh-JPW1p67vUmN5AEar5j7cSY1g@mail.gmail.com>
	<545dedd7-2bdb-2fbd-651d-bc6bd43f8df6@auckland.ac.nz>
	<CAAjnpdiSMpPC+87LBXdu2fxRQx2dzWo9cDezJGa8zpwqdo5xuw@mail.gmail.com>
	<CAF8bMcZbSRQqOr_WUJyQ_3nNoUoBiUFhrhxa1tFZo+vfksC=3w@mail.gmail.com>
Message-ID: <CAAjnpdjs0jZxBt3Z014=NATd+FVs=zU06jB9Z_jCVZgvg_5iqQ@mail.gmail.com>

Hi William,

I tested plyrs dlply function, and it seems to have  have an O(N*
log(R)) complexity (tested for R=N) so I do not know if N is the
number of rows or nr of categories.

For the data.frame example with 2e5 rows and 2e5 categories it is
approx. 10 times faster than split. Still, it is 10 seconds on an
i7-5930K 3.5GHz Intel.

It would be nice if the documentation would contain runtime
complexity information and the documentation of base package function
would point to function which should be used instead.

Thanks




On 29 June 2016 at 16:13, William Dunlap <wdunlap at tibco.com> wrote:
> I won't go into why splitting data.frames (or factors) uses time
> proportional to the number of input rows times the number of
> levels in the splitting factor, but you will get much better mileage
> if you call split individually on each 'atomic' (numeric, character, ...)
> variable and use mapply on the resulting lists.
>
> The plyr and dplyr packages were developed to deal with this
> sort of problem.  Check them out.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Wed, Jun 29, 2016 at 6:21 AM, Witold E Wolski <wewolski at gmail.com> wrote:
>>
>> Hi,
>>
>> Here is an complete example which shows the the complexity of split or
>> by is O(n^2)
>>
>> nrows <- c(1e3,5e3, 1e4 ,5e4, 1e5 ,2e5)
>> res<-list()
>>
>> for(i in nrows){
>>   dum <- data.frame(x = runif(i,1,1000), y=runif(i,1,1000))
>>   res[[length(res)+1]]<-(system.time(x<- split(dum, 1:nrow(dum))))
>> }
>> res <- do.call("rbind",res)
>> plot(nrows^2, res[,"elapsed"])
>>
>> And I can't see a reason why this has to be so slow.
>>
>>
>> cheers
>>
>>
>>
>>
>>
>>
>>
>> On 29 June 2016 at 12:00, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>> > On 29/06/16 21:16, Witold E Wolski wrote:
>> >>
>> >> It's the inverse problem to merging a list of data.frames into a large
>> >> data.frame just discussed in the "performance of do.call("rbind")"
>> >> thread
>> >>
>> >> I would like to split a data.frame into a list of data.frames
>> >> according to first column.
>> >> This SEEMS to be easily possible with the function base::by. However,
>> >> as soon as the data.frame has a few million rows this function CAN NOT
>> >> BE USED (except you have A PLENTY OF TIME).
>> >>
>> >> for 'by' runtime ~ nrow^2, or formally O(n^2)  (see benchmark below).
>> >>
>> >> So basically I am looking for a similar function with better
>> >> complexity.
>> >>
>> >>
>> >>  > nrows <- c(1e5,1e6,2e6,3e6,5e6)
>> >>>
>> >>> timing <- list()
>> >>> for(i in nrows){
>> >>
>> >> + dum <- peaks[1:i,]
>> >> + timing[[length(timing)+1]] <- system.time(x<- by(dum[,2:3],
>> >> INDICES=list(dum[,1]), FUN=function(x){x}, simplify = FALSE))
>> >> + }
>> >>>
>> >>> names(timing)<- nrows
>> >>> timing
>> >>
>> >> $`1e+05`
>> >>    user  system elapsed
>> >>    0.05    0.00    0.05
>> >>
>> >> $`1e+06`
>> >>    user  system elapsed
>> >>    1.48    2.98    4.46
>> >>
>> >> $`2e+06`
>> >>    user  system elapsed
>> >>    7.25   11.39   18.65
>> >>
>> >> $`3e+06`
>> >>    user  system elapsed
>> >>   16.15   25.81   41.99
>> >>
>> >> $`5e+06`
>> >>    user  system elapsed
>> >>   43.22   74.72  118.09
>> >
>> >
>> > I'm not sure that I follow what you're doing, and your example is not
>> > reproducible, since we have no idea what "peaks" is, but on a toy
>> > example
>> > with 5e6 rows in the data frame I got a timing result of
>> >
>> >    user  system elapsed
>> >   0.379 0.025 0.406
>> >
>> > when I applied split().  Is this adequately fast? Seems to me that if
>> > you
>> > want to split something, split() would be a good place to start.
>> >
>> > cheers,
>> >
>> > Rolf Turner
>> >
>> > --
>> > Technical Editor ANZJS
>> > Department of Statistics
>> > University of Auckland
>> > Phone: +64-9-373-7599 ext. 88276
>>
>>
>>
>> --
>> Witold Eryk Wolski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>



-- 
Witold Eryk Wolski


From bgunter.4567 at gmail.com  Fri Jul  1 17:02:03 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 1 Jul 2016 08:02:03 -0700
Subject: [R] [FORGED] Splitting data.frame into a list of small
 data.frames given indices
In-Reply-To: <CAAjnpdjs0jZxBt3Z014=NATd+FVs=zU06jB9Z_jCVZgvg_5iqQ@mail.gmail.com>
References: <CAAjnpdispjvZL_hBzR-vFLqdh-JPW1p67vUmN5AEar5j7cSY1g@mail.gmail.com>
	<545dedd7-2bdb-2fbd-651d-bc6bd43f8df6@auckland.ac.nz>
	<CAAjnpdiSMpPC+87LBXdu2fxRQx2dzWo9cDezJGa8zpwqdo5xuw@mail.gmail.com>
	<CAF8bMcZbSRQqOr_WUJyQ_3nNoUoBiUFhrhxa1tFZo+vfksC=3w@mail.gmail.com>
	<CAAjnpdjs0jZxBt3Z014=NATd+FVs=zU06jB9Z_jCVZgvg_5iqQ@mail.gmail.com>
Message-ID: <CAGxFJbRQr1pHhQ4p6cauUm2cnFzH5he0sR79znpTY50zZWTBRg@mail.gmail.com>

Inline.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jul 1, 2016 at 7:40 AM, Witold E Wolski <wewolski at gmail.com> wrote:
> Hi William,
>
> I tested plyrs dlply function, and it seems to have  have an O(N*
> log(R)) complexity (tested for R=N) so I do not know if N is the
> number of rows or nr of categories.
>
> For the data.frame example with 2e5 rows and 2e5 categories it is
> approx. 10 times faster than split. Still, it is 10 seconds on an
> i7-5930K 3.5GHz Intel.
>
> It would be nice if the documentation would contain runtime
> complexity information and the documentation of base package function
> would point to function which should be used instead.

It would, indeed -- but these things are very dependent on exact data
context, the hardware in use, the OS, etc.  Moreover, how could base
documentation possibly keep track of what all other packages are
doing?! -- that seems unreasonable on the face of it.  I know that
from time to time R docs do mention base algorithmic complexity (e.g.
?sort and the data.table package, I believe), but generally it is
preferable to omit such details, imho: access to a function should be
through its relatively fixed API, while the underlying machinery may
be subject to considerable change.  Obviously, there are circumstances
where something still could (and perhaps should) be said about
efficiency -- and R docs often do say it -- but I think the level of
detail you request is unrealistic and might often even mislead.

Obviously, just my opinion, so contrary views welcome.

Cheers,
Bert


>
> Thanks
>
>
>
>
> On 29 June 2016 at 16:13, William Dunlap <wdunlap at tibco.com> wrote:
>> I won't go into why splitting data.frames (or factors) uses time
>> proportional to the number of input rows times the number of
>> levels in the splitting factor, but you will get much better mileage
>> if you call split individually on each 'atomic' (numeric, character, ...)
>> variable and use mapply on the resulting lists.
>>
>> The plyr and dplyr packages were developed to deal with this
>> sort of problem.  Check them out.
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Wed, Jun 29, 2016 at 6:21 AM, Witold E Wolski <wewolski at gmail.com> wrote:
>>>
>>> Hi,
>>>
>>> Here is an complete example which shows the the complexity of split or
>>> by is O(n^2)
>>>
>>> nrows <- c(1e3,5e3, 1e4 ,5e4, 1e5 ,2e5)
>>> res<-list()
>>>
>>> for(i in nrows){
>>>   dum <- data.frame(x = runif(i,1,1000), y=runif(i,1,1000))
>>>   res[[length(res)+1]]<-(system.time(x<- split(dum, 1:nrow(dum))))
>>> }
>>> res <- do.call("rbind",res)
>>> plot(nrows^2, res[,"elapsed"])
>>>
>>> And I can't see a reason why this has to be so slow.
>>>
>>>
>>> cheers
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> On 29 June 2016 at 12:00, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>>> > On 29/06/16 21:16, Witold E Wolski wrote:
>>> >>
>>> >> It's the inverse problem to merging a list of data.frames into a large
>>> >> data.frame just discussed in the "performance of do.call("rbind")"
>>> >> thread
>>> >>
>>> >> I would like to split a data.frame into a list of data.frames
>>> >> according to first column.
>>> >> This SEEMS to be easily possible with the function base::by. However,
>>> >> as soon as the data.frame has a few million rows this function CAN NOT
>>> >> BE USED (except you have A PLENTY OF TIME).
>>> >>
>>> >> for 'by' runtime ~ nrow^2, or formally O(n^2)  (see benchmark below).
>>> >>
>>> >> So basically I am looking for a similar function with better
>>> >> complexity.
>>> >>
>>> >>
>>> >>  > nrows <- c(1e5,1e6,2e6,3e6,5e6)
>>> >>>
>>> >>> timing <- list()
>>> >>> for(i in nrows){
>>> >>
>>> >> + dum <- peaks[1:i,]
>>> >> + timing[[length(timing)+1]] <- system.time(x<- by(dum[,2:3],
>>> >> INDICES=list(dum[,1]), FUN=function(x){x}, simplify = FALSE))
>>> >> + }
>>> >>>
>>> >>> names(timing)<- nrows
>>> >>> timing
>>> >>
>>> >> $`1e+05`
>>> >>    user  system elapsed
>>> >>    0.05    0.00    0.05
>>> >>
>>> >> $`1e+06`
>>> >>    user  system elapsed
>>> >>    1.48    2.98    4.46
>>> >>
>>> >> $`2e+06`
>>> >>    user  system elapsed
>>> >>    7.25   11.39   18.65
>>> >>
>>> >> $`3e+06`
>>> >>    user  system elapsed
>>> >>   16.15   25.81   41.99
>>> >>
>>> >> $`5e+06`
>>> >>    user  system elapsed
>>> >>   43.22   74.72  118.09
>>> >
>>> >
>>> > I'm not sure that I follow what you're doing, and your example is not
>>> > reproducible, since we have no idea what "peaks" is, but on a toy
>>> > example
>>> > with 5e6 rows in the data frame I got a timing result of
>>> >
>>> >    user  system elapsed
>>> >   0.379 0.025 0.406
>>> >
>>> > when I applied split().  Is this adequately fast? Seems to me that if
>>> > you
>>> > want to split something, split() would be a good place to start.
>>> >
>>> > cheers,
>>> >
>>> > Rolf Turner
>>> >
>>> > --
>>> > Technical Editor ANZJS
>>> > Department of Statistics
>>> > University of Auckland
>>> > Phone: +64-9-373-7599 ext. 88276
>>>
>>>
>>>
>>> --
>>> Witold Eryk Wolski
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>
>
> --
> Witold Eryk Wolski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From c.jallet at laposte.net  Fri Jul  1 09:10:36 2016
From: c.jallet at laposte.net (c.jallet at laposte.net)
Date: Fri, 1 Jul 2016 09:10:36 +0200
Subject: [R] Understanding and predict round-off errors sign on simple
	functions
In-Reply-To: <24156952D190E841BF8E66CB59FAB94A4DE7C2AE@STLWEXMBXPRD14.na.ds.monsanto.com>
References: <002e01d1d1ec$70c99080$525cb180$@laposte.net>	<CAGxFJbRoPRFtBLXu+_uvW6MjPRCd-W1o2n3vNKhJwbWS+TBqVg@mail.gmail.com>	<F53BA2E3-7B45-431F-A1A8-7571E8C7A853@me.com>
	<24156952D190E841BF8E66CB59FAB94A4DE7C2AE@STLWEXMBXPRD14.na.ds.monsanto.com>
Message-ID: <000e01d1d367$b007d2d0$10177870$@laposte.net>

Thank you for all your answers and I will take a look to the 'propagate'
package.

Ps: first time I am participating to a mailing list, I hope I answer to the
right emails. 

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of DIGHE,
NILESH [AG/2362]
Sent: jeudi, 30 juin 2016 14:02
To: Marc Schwartz <marc_schwartz at me.com>; Bert Gunter
<bgunter.4567 at gmail.com>
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Understanding and predict round-off errors sign on simple
functions

Using "runmean" function from caTools package within your SMA function
appears to solve the issue.  Please see details below.

library(caTools)

> dput(m)
structure(c(-0.626453810742332, 0.183643324222082, -0.835628612410047,
1.59528080213779, 0.329507771815361, -0.820468384118015, 0.487429052428485,
0.738324705129217, 0.575781351653492, -0.305388387156356, 3.51178116845085,
2.38984323641143, 1.3787594194582, -0.2146998871775, 3.12493091814311,
1.95506639098477, 1.98380973690105, 2.9438362106853, 2.82122119509809,
2.59390132121751, 5.91897737160822, 5.78213630073107, 5.07456498336519,
3.01064830413663, 5.61982574789471, 4.943871260471, 4.84420449329467,
3.52924761610073, 4.52184994489138, 5.4179415601997), .Dim = c(10L,
3L))


> dput(SMA)
function (x, n = 10, ...)
{
    ma <- runmean(x, n)
    if (!is.null(dim(ma))) {
        colnames(ma) <- "SMA"
    }
    return(ma)
}


mma <- apply(m, 2, SMA, n=1)

results<-mma-m

> dput(results)
structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(10L, 3L))


Nilesh
-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Marc
Schwartz
Sent: Wednesday, June 29, 2016 1:07 PM
To: Bert Gunter
Cc: R-help
Subject: Re: [R] Understanding and predict round-off errors sign on simple
functions

Hi,

Just to augment Bert's comments, I presume that you are aware of the
relevant R FAQ:

 
https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-
numbers-are-equal_003f

That you had an expectation of the difference being 0 suggested to me that
you might not be, but my apologies if that is not the case.

That being said, there are some higher precision CRAN packages that may
offer some additional functionality, with the potential limitations that
Bert references below. More information is available in the Numerical
Mathematics CRAN Task View:

  https://cran.r-project.org/web/views/NumericalMathematics.html

In addition, with the caveat that I have not used it, there is the
'propagate' package on CRAN that may be relevant to what you want to be able
to anticipate, at some level:

  https://cran.r-project.org/web/packages/propagate/index.html

It has not been updated in a while and there are some notes for the CRAN
package checks, that suggest that the maintainer may not be active at this
point.

Regards,

Marc


> On Jun 29, 2016, at 10:13 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> I am certainly no expert, but I would assume that:
> 
> 1. Roundoff errors depend on the exact numerical libraries and 
> versions that are used, and so general language comparisons are 
> impossible without that information;
> 
> 2. Roundoff errors depend on the exact calculations being done and 
> machine precision and are very complicated to determine
> 
> So I would say the answer to your questions is no.
> 
> But you should probably address such a question to a numerical analyst 
> for an authoritative answer. Maybe try stats.stackexchange.com  .
> 
> -- Bert
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along 
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Wed, Jun 29, 2016 at 2:55 AM, Sirhc via R-help <r-help at r-project.org>
wrote:
>> Hi,
>> 
>> 
>> 
>> May be it is a basic thing but I would like to know if we can 
>> anticipate round-off errors sign.
>> 
>> 
>> 
>> Here is an example :
>> 
>> 
>> 
>> # numerical matrix
>> 
>> m <- matrix(data=cbind(rnorm(10, 0), rnorm(10, 2), rnorm(10, 5)), 
>> nrow=10,
>> ncol=3)
>> 
>> 
>> 
>>> m
>> 
>>            [,1]      [,2]     [,3]
>> 
>> [1,]  0.4816247 1.1973502 3.855641
>> 
>> [2,] -1.2174937 0.7356427 4.393279
>> 
>> [3,]  0.8504074 2.5286509 2.689196
>> 
>> [4,]  1.8048642 1.8580804 6.665237
>> 
>> [5,] -0.6749397 1.0944277 4.838608
>> 
>> [6,]  0.8252034 1.5595268 3.681695
>> 
>> [7,]  1.3002208 0.9582693 4.561577
>> 
>> [8,]  1.6950923 3.5677921 6.005078
>> 
>> [9,]  0.6509285 0.9025964 5.082288
>> 
>> [10,] -0.5676040 1.3281102 4.446451
>> 
>> 
>> 
>> #weird moving average of period 1 !
>> 
>> mma <- apply(m, 2, SMA, n=1)
>> 
>> 
>> 
>>> mma
>> 
>>            [,1]      [,2]     [,3]
>> 
>> [1,]         NA        NA       NA
>> 
>> [2,] -1.2174937 0.7356427 4.393279
>> 
>> [3,]  0.8504074 2.5286509 2.689196
>> 
>> [4,]  1.8048642 1.8580804 6.665237
>> 
>> [5,] -0.6749397 1.0944277 4.838608
>> 
>> [6,]  0.8252034 1.5595268 3.681695
>> 
>> [7,]  1.3002208 0.9582693 4.561577
>> 
>> [8,]  1.6950923 3.5677921 6.005078
>> 
>> [9,]  0.6509285 0.9025964 5.082288
>> 
>> [10,] -0.5676040 1.3281102 4.446451
>> 
>> 
>> 
>> 
>> 
>> #difference should be 0 but here is the result
>> 
>>> m - mma
>> 
>>               [,1]         [,2]          [,3]
>> 
>> [1,]            NA           NA            NA
>> 
>> [2,]  0.000000e+00 0.000000e+00 -8.881784e-16
>> 
>> [3,]  0.000000e+00 0.000000e+00 -8.881784e-16
>> 
>> [4,]  0.000000e+00 4.440892e-16 -8.881784e-16
>> 
>> [5,] -1.110223e-16 4.440892e-16 -8.881784e-16
>> 
>> [6,] -1.110223e-16 2.220446e-16 -4.440892e-16
>> 
>> [7,] -2.220446e-16 2.220446e-16  0.000000e+00
>> 
>> [8,] -2.220446e-16 0.000000e+00  0.000000e+00
>> 
>> [9,] -3.330669e-16 2.220446e-16 -8.881784e-16
>> 
>> [10,] -3.330669e-16 4.440892e-16 -8.881784e-16
>> 
>> 
>> 
>> SMA function use runMean
>> 
>> # TTR / R / MovingAverages.R
>> 
>> "SMA" <- function(x, n=10, ...) { # Simple Moving Average
>> 
>>   ma <- runMean( x, n )
>> 
>>   if(!is.null(dim(ma))) {
>> 
>>     colnames(ma) <- "SMA"
>> 
>>   }
>> 
>>  return(ma)
>> 
>> }
>> 
>> 
>> 
>> 
>> 
>> Can anyone explain me that round error type?
>> 
>> Is it possible to reproduce this same error generation in another 
>> language like C++ or C# ?
>> 
>> 
>> 
>> Thanks in advance for your answers
>> 
>> 
>> 
>> Regards
>> 
>> 
>> 
>> Chris

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
This email and any attachments were sent from a Monsanto email account and
may contain confidential and/or privileged information. If you are not the
intended recipient, please contact the sender and delete this email and any
attachments immediately. Any unauthorized use, including disclosing,
printing, storing, copying or distributing this email, is prohibited. All
emails and attachments sent to or from Monsanto email accounts may be
subject to monitoring, reading, and archiving by Monsanto, including its
affiliates and subsidiaries, as permitted by applicable law. Thank you.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gab4 at st-andrews.ac.uk  Fri Jul  1 11:11:48 2016
From: gab4 at st-andrews.ac.uk (Giles Bischoff)
Date: Fri, 1 Jul 2016 02:11:48 -0700
Subject: [R] Difficulty subsetting data frames using logical operators
Message-ID: <CAPvuKKzXBypVoMthRdAXfUc-vjGpExvi=Lv4et7ni1ymgNGbWQ@mail.gmail.com>

So, I uploaded a data set via my directory using the command data <-
data.frame(read.csv("hw1_data.csv")) and then tried to subset that data
using logical operators. Specifically, I was trying to make it so that I
got all the rows in which the values for "Ozone" (a column in the data set)
were greater than 31 (I was trying to get the mean of all said values).
Then, I tried using the command data[ , "Ozone">31]. Additionally, I had
trouble getting it so that I had all the rows where all the values in
"Ozone">31 & "Temp">90 simultaneously. There were some NA values in both of
those columns, so that might be it. If someone could help me to figure out
how to remove those values, that'd be great as well. I'm using a Mac (OS X)
with the latest version of R (3.1.2. I think??).

Here is some of the code I used:

>data <- data.frame(read.csv("hw1_data.csv"))
> data
    Ozone Solar.R Wind Temp Month Day
1      41     190  7.4   67     5   1
2      36     118  8.0   72     5   2
3      12     149 12.6   74     5   3
4      18     313 11.5   62     5   4
5      NA      NA 14.3   56     5   5
6      28      NA 14.9   66     5   6
7      23     299  8.6   65     5   7
8      19      99 13.8   59     5   8
9       8      19 20.1   61     5   9
10     NA     194  8.6   69     5  10
11      7      NA  6.9   74     5  11
12     16     256  9.7   69     5  12
13     11     290  9.2   66     5  13
14     14     274 10.9   68     5  14
15     18      65 13.2   58     5  15
16     14     334 11.5   64     5  16
17     34     307 12.0   66     5  17
18      6      78 18.4   57     5  18
19     30     322 11.5   68     5  19
20     11      44  9.7   62     5  20
21      1       8  9.7   59     5  21
22     11     320 16.6   73     5  22
23      4      25  9.7   61     5  23
24     32      92 12.0   61     5  24
25     NA      66 16.6   57     5  25
26     NA     266 14.9   58     5  26
27     NA      NA  8.0   57     5  27
28     23      13 12.0   67     5  28
29     45     252 14.9   81     5  29
30    115     223  5.7   79     5  30
31     37     279  7.4   76     5  31
32     NA     286  8.6   78     6   1
33     NA     287  9.7   74     6   2
34     NA     242 16.1   67     6   3
35     NA     186  9.2   84     6   4
36     NA     220  8.6   85     6   5
37     NA     264 14.3   79     6   6
38     29     127  9.7   82     6   7
39     NA     273  6.9   87     6   8
40     71     291 13.8   90     6   9
41     39     323 11.5   87     6  10
42     NA     259 10.9   93     6  11
43     NA     250  9.2   92     6  12
44     23     148  8.0   82     6  13
45     NA     332 13.8   80     6  14
46     NA     322 11.5   79     6  15
47     21     191 14.9   77     6  16
48     37     284 20.7   72     6  17
49     20      37  9.2   65     6  18
50     12     120 11.5   73     6  19
51     13     137 10.3   76     6  20
52     NA     150  6.3   77     6  21
53     NA      59  1.7   76     6  22
54     NA      91  4.6   76     6  23
55     NA     250  6.3   76     6  24
56     NA     135  8.0   75     6  25
57     NA     127  8.0   78     6  26
58     NA      47 10.3   73     6  27
59     NA      98 11.5   80     6  28
60     NA      31 14.9   77     6  29
61     NA     138  8.0   83     6  30
62    135     269  4.1   84     7   1
63     49     248  9.2   85     7   2
64     32     236  9.2   81     7   3
65     NA     101 10.9   84     7   4
66     64     175  4.6   83     7   5
67     40     314 10.9   83     7   6
68     77     276  5.1   88     7   7
69     97     267  6.3   92     7   8
70     97     272  5.7   92     7   9
71     85     175  7.4   89     7  10
72     NA     139  8.6   82     7  11
73     10     264 14.3   73     7  12
74     27     175 14.9   81     7  13
75     NA     291 14.9   91     7  14
76      7      48 14.3   80     7  15
77     48     260  6.9   81     7  16
78     35     274 10.3   82     7  17
79     61     285  6.3   84     7  18
80     79     187  5.1   87     7  19
81     63     220 11.5   85     7  20
82     16       7  6.9   74     7  21
83     NA     258  9.7   81     7  22
84     NA     295 11.5   82     7  23
85     80     294  8.6   86     7  24
86    108     223  8.0   85     7  25
87     20      81  8.6   82     7  26
88     52      82 12.0   86     7  27
89     82     213  7.4   88     7  28
90     50     275  7.4   86     7  29
91     64     253  7.4   83     7  30
92     59     254  9.2   81     7  31
93     39      83  6.9   81     8   1
94      9      24 13.8   81     8   2
95     16      77  7.4   82     8   3
96     78      NA  6.9   86     8   4
97     35      NA  7.4   85     8   5
98     66      NA  4.6   87     8   6
99    122     255  4.0   89     8   7
100    89     229 10.3   90     8   8
101   110     207  8.0   90     8   9
102    NA     222  8.6   92     8  10
103    NA     137 11.5   86     8  11
104    44     192 11.5   86     8  12
105    28     273 11.5   82     8  13
106    65     157  9.7   80     8  14
107    NA      64 11.5   79     8  15
108    22      71 10.3   77     8  16
109    59      51  6.3   79     8  17
110    23     115  7.4   76     8  18
111    31     244 10.9   78     8  19
112    44     190 10.3   78     8  20
113    21     259 15.5   77     8  21
114     9      36 14.3   72     8  22
115    NA     255 12.6   75     8  23
116    45     212  9.7   79     8  24
117   168     238  3.4   81     8  25
118    73     215  8.0   86     8  26
119    NA     153  5.7   88     8  27
120    76     203  9.7   97     8  28
121   118     225  2.3   94     8  29
122    84     237  6.3   96     8  30
123    85     188  6.3   94     8  31
124    96     167  6.9   91     9   1
125    78     197  5.1   92     9   2
126    73     183  2.8   93     9   3
127    91     189  4.6   93     9   4
128    47      95  7.4   87     9   5
129    32      92 15.5   84     9   6
130    20     252 10.9   80     9   7
131    23     220 10.3   78     9   8
132    21     230 10.9   75     9   9
133    24     259  9.7   73     9  10
134    44     236 14.9   81     9  11
135    21     259 15.5   76     9  12
136    28     238  6.3   77     9  13
137     9      24 10.9   71     9  14
138    13     112 11.5   71     9  15
139    46     237  6.9   78     9  16
140    18     224 13.8   67     9  17
141    13      27 10.3   76     9  18
142    24     238 10.3   68     9  19
143    16     201  8.0   82     9  20
144    13     238 12.6   64     9  21
145    23      14  9.2   71     9  22
146    36     139 10.3   81     9  23
147     7      49 10.3   69     9  24
148    14      20 16.6   63     9  25
149    30     193  6.9   70     9  26
150    NA     145 13.2   77     9  27
151    14     191 14.3   75     9  28
152    18     131  8.0   76     9  29
153    20     223 11.5   68     9  30

 >colnames(data) <- c("Ozone", "Solar.R", "Wind", "Temp", "Month", "Day")

> mean(data[, Ozone])
Error in `[.data.frame`(data, , Ozone) : object 'Ozone' not found

mean(data[, "Ozone">31])
[1] NA
Warning message:
In mean.default(data[, "Ozone" > 31]) :
  argument is not numeric or logical: returning NA
>  mean(data[, "Ozone">31 & "Ozone"[!is.na("Ozone")]])
Error in "Ozone" > 31 & "Ozone"[!is.na("Ozone")] :
  operations are possible only for numeric, logical or complex types

	[[alternative HTML version deleted]]


From gvirmani at gmail.com  Fri Jul  1 04:38:05 2016
From: gvirmani at gmail.com (Gaurav Virmani)
Date: Fri, 1 Jul 2016 14:38:05 +1200
Subject: [R] RODBC Error - first argument is not an open ODBC Channel
Message-ID: <CALQjaSpFwv03q5oWttfoAZnEe=7XN7BKKV0RmUvodXp-1GE=Bw@mail.gmail.com>

Hi,

Sincere apologies if my mail is inappropriate for this mailbox. Please
ignore the mail.

*Issue*

I am getting error "*first argument is not an open RODBC channel*" when I
publish my application on IIS. It runs perfectly under Visual Studio
development mode and the script runs fine on R Console too. But getting
error once published to IIS

*Code Snippet:*

library(RODBC)
conna <- odbcConnect("XXX",uid='sa',pwd='xxxxx')

coreKPI<-sqlFetch(conna,"vw_GetFields")
odbcClose(conna)

I have tried using odbcDriverConnect instead of odbcConnect and providing
complete DB string but still the issue. Also I have set the PATH variable
to point to 32 bit R. Tried few other options suggested on stack overflow
but no luck. I have been struggling for 3 days now to get this going.
Please advise.

Thanks in anticipation.

Warm Regards
Gaurav

	[[alternative HTML version deleted]]


From markshanks101 at hotmail.com  Fri Jul  1 18:58:28 2016
From: markshanks101 at hotmail.com (Mark Shanks)
Date: Fri, 1 Jul 2016 16:58:28 +0000
Subject: [R] How to identify runs or clusters of events in time
Message-ID: <PS1PR03MB182004C026D955C9149A0AEEE6250@PS1PR03MB1820.apcprd03.prod.outlook.com>

Hi,


Imagine the two problems:


1) You have an event that occurs repeatedly over time. You want to identify periods when the event occurs more frequently than the base rate of occurrence. Ideally, you don't want to have to specify the period (e.g., break into months), so the analysis can be sensitive to scenarios such as many events happening only between, e.g., June 10 and June 15 - even though the overall number of events for the month may not be much greater than usual. Similarly, there may be a cluster of events that occur from March 28 to April 3. Ideally, you want to pull out the base rate of occurrence and highlight only the periods when the frequency is less or greater than the base rate.


2) Events again occur repeatedly over time in an inconsistent way. However, this time, the event has positive or negative outcomes - such as a spot check of conformity to regulations. You again want to know whether there is a group of negative outcomes close together in time. This analysis should take into account the negative outcomes as well though. E.g., if from June 10 to June 15 you get 5 negative outcomes and no positive outcomes it should be flagged. On the other hand, if from June 10 to June 15 you get 5 negative outcomes interspersed between many positive outcomes it should be ignored.


I'm guessing that there is some statistical approach designed to look at these types of issues. What is it called? What package in R implements it? I basically just need to know where to start.


Thanks,


Mark

	[[alternative HTML version deleted]]


From clint at ecy.wa.gov  Fri Jul  1 19:49:33 2016
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 1 Jul 2016 10:49:33 -0700 (PDT)
Subject: [R] How to identify runs or clusters of events in time
In-Reply-To: <PS1PR03MB182004C026D955C9149A0AEEE6250@PS1PR03MB1820.apcprd03.prod.outlook.com>
References: <PS1PR03MB182004C026D955C9149A0AEEE6250@PS1PR03MB1820.apcprd03.prod.outlook.com>
Message-ID: <alpine.LRH.2.20.1607011046330.25867@aeolus.ecy.wa.gov>

Mark,

I did something similar a couple of year ago by coding non-events as 0, 
positive events as +1 and negative events as -1 then summing the value 
through time.  In my case the patterns showed up quite clearly and I used 
other criteria to define the actual periods.

Clint

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600

         USPS:           PO Box 47600, Olympia, WA 98504-7600
         Parcels:        300 Desmond Drive, Lacey, WA 98503-1274

On Fri, 1 Jul 2016, Mark Shanks wrote:

> Hi,
>
>
> Imagine the two problems:
>
>
> 1) You have an event that occurs repeatedly over time. You want to identify periods when the event occurs more frequently than the base rate of occurrence. Ideally, you don't want to have to specify the period (e.g., break into months), so the analysis can be sensitive to scenarios such as many events happening only between, e.g., June 10 and June 15 - even though the overall number of events for the month may not be much greater than usual. Similarly, there may be a cluster of events that occur from March 28 to April 3. Ideally, you want to pull out the base rate of occurrence and highlight only the periods when the frequency is less or greater than the base rate.
>
>
> 2) Events again occur repeatedly over time in an inconsistent way. However, this time, the event has positive or negative outcomes - such as a spot check of conformity to regulations. You again want to know whether there is a group of negative outcomes close together in time. This analysis should take into account the negative outcomes as well though. E.g., if from June 10 to June 15 you get 5 negative outcomes and no positive outcomes it should be flagged. On the other hand, if from June 10 to June 15 you get 5 negative outcomes interspersed between many positive outcomes it should be ignored.
>
>
> I'm guessing that there is some statistical approach designed to look at these types of issues. What is it called? What package in R implements it? I basically just need to know where to start.
>
>
> Thanks,
>
>
> Mark
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From eric.archer at noaa.gov  Fri Jul  1 20:23:31 2016
From: eric.archer at noaa.gov (Eric Archer - NOAA Federal)
Date: Fri, 1 Jul 2016 11:23:31 -0700
Subject: [R] C stack error in as.vector() starting in R 3.3.0
Message-ID: <CAGrYeXjXxsKm7KfP4FNPgc4iWFAbPs1TvNz2B7+TpCuzNsw14Q@mail.gmail.com>

Apologies for the long post. This is an issue I have been struggling with
and I have tried to be as complete, to the point, and reproducible as
possible.

In documenting a package with roxygen2, I have come across an error that
does not occur in R 3.2.4 revised, but does occur in R 3.3.0 and 3.3.1.
Using traceback() and debug(), I've traced the error to a call made to
as.vector(x, "character") that seems to get stuck in a loop which
culminates in this error:

Error: C stack usage  7970892 is too close to the limit

The object that causes this error is of a signature type for a method. With
some playing around, I've been able to work out that the error is actually
associated with the method that roxygen2 creates when doing its magic.
Something happens when this method definition with its associated
environment is in the workspace that causes the error.

At this point, I should stress again that the error does NOT occur in R
3.2.4 revised or earlier, but does occur in R 3.3.0 and 3.3.1. I have also
tested this with several versions of roxygen2 and that does not make a
difference. Thus, my ultimate question is what has changed in R 3.3.0 that
would lead to this so that the roxygen2 maintainers can correct it?

As a test, I created a signature identical to the one that causes the error
and tried the as.vector() command that would generate the loop. I don't get
the error in my command line generated object, nor with the object that is
extracted from the method definition. However, when I load the method
definition into the workspace, the error will happen on either my command
line generated object, or the one extracted from the method definition and
will not stop happening until I restart R.

I have reported this error as an issue on the roxygen2 GitHub repository
and it has been crossposted by another user who had a similar experience
with a different package on the devtools repository. Those posts, which
contain more information are here:

https://github.com/klutometis/roxygen/issues/475
https://github.com/hadley/devtools/issues/1234

Below is the result of sessionInfo() and the output of a session
demonstrating the effect. The files used are in a zip file here:
https://github.com/klutometis/roxygen/files/335417/error.test.rdata.files.zip

 > sessionInfo()
R version 3.3.0 (2016-05-03)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.11.5 (El Capitan)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_3.3.0

> rm(list = ls())
>
> # Create test class and method
> setClass(Class = "gtypes", slots = c(loci = "data.frame", ploidy =
"numeric"), package = "roxygen_devtest")
> setGeneric("nInd", function(x, ...) standardGeneric("nInd"), package =
"adegenet")
[1] "nInd"
> setMethod("nInd", "gtypes", function(x, ...) nrow(x at loci) / x at ploidy)
[1] "nInd"
>
> test.method <- getMethod("nInd", "gtypes")
> str(test.method)
Formal class 'MethodDefinition' [package "methods"] with 4 slots
  ..@ .Data  :function (x, ...)
  ..@ target :Formal class 'signature' [package "methods"] with 3 slots
  .. .. ..@ .Data  : chr "gtypes"
  .. .. ..@ names  : chr "x"
  .. .. ..@ package: chr "roxygen_devtest"
  ..@ defined:Formal class 'signature' [package "methods"] with 3 slots
  .. .. ..@ .Data  : chr "gtypes"
  .. .. ..@ names  : chr "x"
  .. .. ..@ package: chr "roxygen_devtest"
  ..@ generic: atomic [1:1] nInd
  .. ..- attr(*, "package")= chr "adegenet"
>
> # No error:
> as.vector(test.method at defined, "character")
[1] "gtypes"
>
> # This is the method that generates the error
> load("problem.method.rdata")
>
> # Notice the difference in the environments of the two functions:
> test.method at .Data
function (x, ...)
nrow(x at loci)/x at ploidy
> problem.method at .Data
function (x, ...)
nrow(x at loci)/x at ploidy
<environment: 0x101f85040>
>
> # Swap the problem function for the one I created:
> problem.method at .Data <- test.method at .Data
> save(problem.method, file = "fixed.method.rdata")
>
> # Here's the error (both with my method and the original):
> as.vector(test.method at defined, "character")
Error: C stack usage  7970892 is too close to the limit
> as.vector(problem.method at defined, "character")
Error: C stack usage  7970892 is too close to the limit

Restarting R session...

> # *** Restart R for this to work ***
> rm(list = ls())
>
> load("fixed.method.rdata")
> as.vector(problem.method at defined, "character")
[1] "gtypes"

----

*Eric Archer, Ph.D.*
Southwest Fisheries Science Center
NMFS, NOAA
8901 La Jolla Shores Drive
La Jolla, CA 92037 USA
858-546-7121 (work)
858-546-7003 (FAX)

Marine Mammal Genetics Group: swfsc.noaa.gov/mmtd-mmgenetics
ETP Cetacean Assessment Program: swfsc.noaa.gov/mmtd-etp
https://github/ericarcher

"


*The universe doesn't care what you believe. The wonderful thing about
science is that it   doesn't ask for your faith, it just asks   for your
eyes.*"  - Randall Munroe

"*Lighthouses are more helpful than churches.*"
   - Benjamin Franklin

   "*...but I'll take a GPS over either one.*"
       - John C. "Craig" George

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Fri Jul  1 20:47:16 2016
From: jholtman at gmail.com (jim holtman)
Date: Fri, 1 Jul 2016 14:47:16 -0400
Subject: [R] Difficulty subsetting data frames using logical operators
In-Reply-To: <CAPvuKKzXBypVoMthRdAXfUc-vjGpExvi=Lv4et7ni1ymgNGbWQ@mail.gmail.com>
References: <CAPvuKKzXBypVoMthRdAXfUc-vjGpExvi=Lv4et7ni1ymgNGbWQ@mail.gmail.com>
Message-ID: <CAAxdm-4f_DU2-zcmKHYm15P+L4+Yhzt6WZp-wgghzT5W0VT8NQ@mail.gmail.com>

You may need to re-read the Intro to R.

data[data$Ozone > 31,]

or

subset(data, Ozone > 31)


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Fri, Jul 1, 2016 at 5:11 AM, Giles Bischoff <gab4 at st-andrews.ac.uk>
wrote:

> So, I uploaded a data set via my directory using the command data <-
> data.frame(read.csv("hw1_data.csv")) and then tried to subset that data
> using logical operators. Specifically, I was trying to make it so that I
> got all the rows in which the values for "Ozone" (a column in the data set)
> were greater than 31 (I was trying to get the mean of all said values).
> Then, I tried using the command data[ , "Ozone">31]. Additionally, I had
> trouble getting it so that I had all the rows where all the values in
> "Ozone">31 & "Temp">90 simultaneously. There were some NA values in both of
> those columns, so that might be it. If someone could help me to figure out
> how to remove those values, that'd be great as well. I'm using a Mac (OS X)
> with the latest version of R (3.1.2. I think??).
>
> Here is some of the code I used:
>
> >data <- data.frame(read.csv("hw1_data.csv"))
> > data
>     Ozone Solar.R Wind Temp Month Day
> 1      41     190  7.4   67     5   1
> 2      36     118  8.0   72     5   2
> 3      12     149 12.6   74     5   3
> 4      18     313 11.5   62     5   4
> 5      NA      NA 14.3   56     5   5
> 6      28      NA 14.9   66     5   6
> 7      23     299  8.6   65     5   7
> 8      19      99 13.8   59     5   8
> 9       8      19 20.1   61     5   9
> 10     NA     194  8.6   69     5  10
> 11      7      NA  6.9   74     5  11
> 12     16     256  9.7   69     5  12
> 13     11     290  9.2   66     5  13
> 14     14     274 10.9   68     5  14
> 15     18      65 13.2   58     5  15
> 16     14     334 11.5   64     5  16
> 17     34     307 12.0   66     5  17
> 18      6      78 18.4   57     5  18
> 19     30     322 11.5   68     5  19
> 20     11      44  9.7   62     5  20
> 21      1       8  9.7   59     5  21
> 22     11     320 16.6   73     5  22
> 23      4      25  9.7   61     5  23
> 24     32      92 12.0   61     5  24
> 25     NA      66 16.6   57     5  25
> 26     NA     266 14.9   58     5  26
> 27     NA      NA  8.0   57     5  27
> 28     23      13 12.0   67     5  28
> 29     45     252 14.9   81     5  29
> 30    115     223  5.7   79     5  30
> 31     37     279  7.4   76     5  31
> 32     NA     286  8.6   78     6   1
> 33     NA     287  9.7   74     6   2
> 34     NA     242 16.1   67     6   3
> 35     NA     186  9.2   84     6   4
> 36     NA     220  8.6   85     6   5
> 37     NA     264 14.3   79     6   6
> 38     29     127  9.7   82     6   7
> 39     NA     273  6.9   87     6   8
> 40     71     291 13.8   90     6   9
> 41     39     323 11.5   87     6  10
> 42     NA     259 10.9   93     6  11
> 43     NA     250  9.2   92     6  12
> 44     23     148  8.0   82     6  13
> 45     NA     332 13.8   80     6  14
> 46     NA     322 11.5   79     6  15
> 47     21     191 14.9   77     6  16
> 48     37     284 20.7   72     6  17
> 49     20      37  9.2   65     6  18
> 50     12     120 11.5   73     6  19
> 51     13     137 10.3   76     6  20
> 52     NA     150  6.3   77     6  21
> 53     NA      59  1.7   76     6  22
> 54     NA      91  4.6   76     6  23
> 55     NA     250  6.3   76     6  24
> 56     NA     135  8.0   75     6  25
> 57     NA     127  8.0   78     6  26
> 58     NA      47 10.3   73     6  27
> 59     NA      98 11.5   80     6  28
> 60     NA      31 14.9   77     6  29
> 61     NA     138  8.0   83     6  30
> 62    135     269  4.1   84     7   1
> 63     49     248  9.2   85     7   2
> 64     32     236  9.2   81     7   3
> 65     NA     101 10.9   84     7   4
> 66     64     175  4.6   83     7   5
> 67     40     314 10.9   83     7   6
> 68     77     276  5.1   88     7   7
> 69     97     267  6.3   92     7   8
> 70     97     272  5.7   92     7   9
> 71     85     175  7.4   89     7  10
> 72     NA     139  8.6   82     7  11
> 73     10     264 14.3   73     7  12
> 74     27     175 14.9   81     7  13
> 75     NA     291 14.9   91     7  14
> 76      7      48 14.3   80     7  15
> 77     48     260  6.9   81     7  16
> 78     35     274 10.3   82     7  17
> 79     61     285  6.3   84     7  18
> 80     79     187  5.1   87     7  19
> 81     63     220 11.5   85     7  20
> 82     16       7  6.9   74     7  21
> 83     NA     258  9.7   81     7  22
> 84     NA     295 11.5   82     7  23
> 85     80     294  8.6   86     7  24
> 86    108     223  8.0   85     7  25
> 87     20      81  8.6   82     7  26
> 88     52      82 12.0   86     7  27
> 89     82     213  7.4   88     7  28
> 90     50     275  7.4   86     7  29
> 91     64     253  7.4   83     7  30
> 92     59     254  9.2   81     7  31
> 93     39      83  6.9   81     8   1
> 94      9      24 13.8   81     8   2
> 95     16      77  7.4   82     8   3
> 96     78      NA  6.9   86     8   4
> 97     35      NA  7.4   85     8   5
> 98     66      NA  4.6   87     8   6
> 99    122     255  4.0   89     8   7
> 100    89     229 10.3   90     8   8
> 101   110     207  8.0   90     8   9
> 102    NA     222  8.6   92     8  10
> 103    NA     137 11.5   86     8  11
> 104    44     192 11.5   86     8  12
> 105    28     273 11.5   82     8  13
> 106    65     157  9.7   80     8  14
> 107    NA      64 11.5   79     8  15
> 108    22      71 10.3   77     8  16
> 109    59      51  6.3   79     8  17
> 110    23     115  7.4   76     8  18
> 111    31     244 10.9   78     8  19
> 112    44     190 10.3   78     8  20
> 113    21     259 15.5   77     8  21
> 114     9      36 14.3   72     8  22
> 115    NA     255 12.6   75     8  23
> 116    45     212  9.7   79     8  24
> 117   168     238  3.4   81     8  25
> 118    73     215  8.0   86     8  26
> 119    NA     153  5.7   88     8  27
> 120    76     203  9.7   97     8  28
> 121   118     225  2.3   94     8  29
> 122    84     237  6.3   96     8  30
> 123    85     188  6.3   94     8  31
> 124    96     167  6.9   91     9   1
> 125    78     197  5.1   92     9   2
> 126    73     183  2.8   93     9   3
> 127    91     189  4.6   93     9   4
> 128    47      95  7.4   87     9   5
> 129    32      92 15.5   84     9   6
> 130    20     252 10.9   80     9   7
> 131    23     220 10.3   78     9   8
> 132    21     230 10.9   75     9   9
> 133    24     259  9.7   73     9  10
> 134    44     236 14.9   81     9  11
> 135    21     259 15.5   76     9  12
> 136    28     238  6.3   77     9  13
> 137     9      24 10.9   71     9  14
> 138    13     112 11.5   71     9  15
> 139    46     237  6.9   78     9  16
> 140    18     224 13.8   67     9  17
> 141    13      27 10.3   76     9  18
> 142    24     238 10.3   68     9  19
> 143    16     201  8.0   82     9  20
> 144    13     238 12.6   64     9  21
> 145    23      14  9.2   71     9  22
> 146    36     139 10.3   81     9  23
> 147     7      49 10.3   69     9  24
> 148    14      20 16.6   63     9  25
> 149    30     193  6.9   70     9  26
> 150    NA     145 13.2   77     9  27
> 151    14     191 14.3   75     9  28
> 152    18     131  8.0   76     9  29
> 153    20     223 11.5   68     9  30
>
>  >colnames(data) <- c("Ozone", "Solar.R", "Wind", "Temp", "Month", "Day")
>
> > mean(data[, Ozone])
> Error in `[.data.frame`(data, , Ozone) : object 'Ozone' not found
>
> mean(data[, "Ozone">31])
> [1] NA
> Warning message:
> In mean.default(data[, "Ozone" > 31]) :
>   argument is not numeric or logical: returning NA
> >  mean(data[, "Ozone">31 & "Ozone"[!is.na("Ozone")]])
> Error in "Ozone" > 31 & "Ozone"[!is.na("Ozone")] :
>   operations are possible only for numeric, logical or complex types
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From marietta0423 at gmail.com  Fri Jul  1 19:28:26 2016
From: marietta0423 at gmail.com (Marietta Suarez)
Date: Fri, 1 Jul 2016 13:28:26 -0400
Subject: [R] trouble double looping to generate data for a meta-analysis
Message-ID: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>

i'm trying to generate data for a meta analysis. 1- generate data following
a normal distribution, 2- generate data following a skewed distribution, 3-
generate data following a logistic distribution. i need to loop this
because the # of studies in each meta will be either 10 or 15. k or total
number of studies in the meta will be 5. i need to loop twice to repeat
this process 10 times. database should be 3 columns (distributions) by 65
rows x 10 reps


here's my code, not sure what's not working:
library(fGarch)

#n reps =10
rep=10

#begin function here, need to vary n and k, when k=2 n=10, when k3 n=15
fun=function(n, k){

  #prepare to store data
  data=matrix(0,nrow=10*k, ncol=3)
  db=matrix(0,nrow=650, ncol=3)

  for (j in 1:rep)
  {
    for (i in 1:k)
    {
      #generate data under normal, skewed, and logistic distributions here

      data[,1]=rnorm(n, 100, 15)
      data[,2]=rsnorm(n, 100, 15, 1)
      data[,3]=rlogis(n, 100, 15)
    }
      [j]=db
  }
}

save=fun(10,2)

Please help!!!

	[[alternative HTML version deleted]]


From Douglas.Federman at utoledo.edu  Fri Jul  1 21:06:36 2016
From: Douglas.Federman at utoledo.edu (Federman, Douglas)
Date: Fri, 1 Jul 2016 19:06:36 +0000
Subject: [R] trouble double looping to generate data for a meta-analysis
In-Reply-To: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>
References: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>
Message-ID: <F1065E5D886F4D429259CDEAE3F83CB61B05CE40@msgdb20.utad.utoledo.edu>

You might look at the package 
     Wakefield
For data generation

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Marietta Suarez
Sent: Friday, July 01, 2016 1:28 PM
To: r-help at r-project.org
Subject: [R] trouble double looping to generate data for a meta-analysis

i'm trying to generate data for a meta analysis. 1- generate data following
a normal distribution, 2- generate data following a skewed distribution, 3-
generate data following a logistic distribution. i need to loop this
because the # of studies in each meta will be either 10 or 15. k or total
number of studies in the meta will be 5. i need to loop twice to repeat
this process 10 times. database should be 3 columns (distributions) by 65
rows x 10 reps


here's my code, not sure what's not working:
library(fGarch)

#n reps =10
rep=10

#begin function here, need to vary n and k, when k=2 n=10, when k3 n=15
fun=function(n, k){

  #prepare to store data
  data=matrix(0,nrow=10*k, ncol=3)
  db=matrix(0,nrow=650, ncol=3)

  for (j in 1:rep)
  {
    for (i in 1:k)
    {
      #generate data under normal, skewed, and logistic distributions here

      data[,1]=rnorm(n, 100, 15)
      data[,2]=rsnorm(n, 100, 15, 1)
      data[,3]=rlogis(n, 100, 15)
    }
      [j]=db
  }
}

save=fun(10,2)

Please help!!!

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Jul  1 21:17:01 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 1 Jul 2016 12:17:01 -0700
Subject: [R] Difficulty subsetting data frames using logical operators
In-Reply-To: <CAPvuKKzXBypVoMthRdAXfUc-vjGpExvi=Lv4et7ni1ymgNGbWQ@mail.gmail.com>
References: <CAPvuKKzXBypVoMthRdAXfUc-vjGpExvi=Lv4et7ni1ymgNGbWQ@mail.gmail.com>
Message-ID: <4B28F84E-B7E3-412E-BFCA-2A0DC4C8177E@comcast.net>


> On Jul 1, 2016, at 2:11 AM, Giles Bischoff <gab4 at st-andrews.ac.uk> wrote:
> 
> So, I uploaded a data set via my directory using the command data <-
> data.frame(read.csv("hw1_data.csv")) and then tried to subset that data
> using logical operators. Specifically, I was trying to make it so that I
> got all the rows in which the values for "Ozone" (a column in the data set)
> were greater than 31 (I was trying to get the mean of all said values).
> Then, I tried using the command data[ , "Ozone">31]. Additionally, I had
> trouble getting it so that I had all the rows where all the values in
> "Ozone">31 & "Temp">90 simultaneously. There were some NA values in both of
> those columns, so that might be it. If someone could help me to figure out
> how to remove those values, that'd be great as well. I'm using a Mac (OS X)
> with the latest version of R (3.1.2. I think??).
> 
> Here is some of the code I used:
> 

Bad idea to use `data` as an data-object name. There is an R function by that name. 

Look at ?data and see the fortune:

fortunes::fortune("dog")

The threat that it will "clash" is not actually correct, but it is guaranteed to deliver error messages that do not make sense and result in code that will be difficult to read.

>> data <- data.frame(read.csv("hw1_data.csv"))
>> data
>    Ozone Solar.R Wind Temp Month Day
> 1      41     190  7.4   67     5   1
> 2      36     118  8.0   72     5   2
> 3      12     149 12.6   74     5   3
> 4      18     313 11.5   62     5   4
> 5      NA      NA 14.3   56     5   5
> 6      28      NA 14.9   66     5   6
> 7      23     299  8.6   65     5   7
> 8      19      99 13.8   59     5   8
> 9       8      19 20.1   61     5   9
> 10     NA     194  8.6   69     5  10
> 11      7      NA  6.9   74     5  11
> 12     16     256  9.7   69     5  12
> 13     11     290  9.2   66     5  13
> 14     14     274 10.9   68     5  14
> 15     18      65 13.2   58     5  15
> 16     14     334 11.5   64     5  16
> 17     34     307 12.0   66     5  17
> 18      6      78 18.4   57     5  18
> 19     30     322 11.5   68     5  19
> 20     11      44  9.7   62     5  20
> 21      1       8  9.7   59     5  21
> 22     11     320 16.6   73     5  22
> 23      4      25  9.7   61     5  23
> 24     32      92 12.0   61     5  24
> 25     NA      66 16.6   57     5  25
> 26     NA     266 14.9   58     5  26
> 27     NA      NA  8.0   57     5  27
> 28     23      13 12.0   67     5  28
> 29     45     252 14.9   81     5  29
> 30    115     223  5.7   79     5  30
> 31     37     279  7.4   76     5  31
> 32     NA     286  8.6   78     6   1
> 33     NA     287  9.7   74     6   2
> 34     NA     242 16.1   67     6   3
> 35     NA     186  9.2   84     6   4
> 36     NA     220  8.6   85     6   5
> 37     NA     264 14.3   79     6   6
> 38     29     127  9.7   82     6   7
> 39     NA     273  6.9   87     6   8
> 40     71     291 13.8   90     6   9
> 41     39     323 11.5   87     6  10
> 42     NA     259 10.9   93     6  11
> 43     NA     250  9.2   92     6  12
> 44     23     148  8.0   82     6  13
> 45     NA     332 13.8   80     6  14
> 46     NA     322 11.5   79     6  15
> 47     21     191 14.9   77     6  16
> 48     37     284 20.7   72     6  17
> 49     20      37  9.2   65     6  18
> 50     12     120 11.5   73     6  19
> 51     13     137 10.3   76     6  20
> 52     NA     150  6.3   77     6  21
> 53     NA      59  1.7   76     6  22
> 54     NA      91  4.6   76     6  23
> 55     NA     250  6.3   76     6  24
> 56     NA     135  8.0   75     6  25
> 57     NA     127  8.0   78     6  26
> 58     NA      47 10.3   73     6  27
> 59     NA      98 11.5   80     6  28
> 60     NA      31 14.9   77     6  29
> 61     NA     138  8.0   83     6  30
> 62    135     269  4.1   84     7   1
> 63     49     248  9.2   85     7   2
> 64     32     236  9.2   81     7   3
> 65     NA     101 10.9   84     7   4
> 66     64     175  4.6   83     7   5
> 67     40     314 10.9   83     7   6
> 68     77     276  5.1   88     7   7
> 69     97     267  6.3   92     7   8
> 70     97     272  5.7   92     7   9
> 71     85     175  7.4   89     7  10
> 72     NA     139  8.6   82     7  11
> 73     10     264 14.3   73     7  12
> 74     27     175 14.9   81     7  13
> 75     NA     291 14.9   91     7  14
> 76      7      48 14.3   80     7  15
> 77     48     260  6.9   81     7  16
> 78     35     274 10.3   82     7  17
> 79     61     285  6.3   84     7  18
> 80     79     187  5.1   87     7  19
> 81     63     220 11.5   85     7  20
> 82     16       7  6.9   74     7  21
> 83     NA     258  9.7   81     7  22
> 84     NA     295 11.5   82     7  23
> 85     80     294  8.6   86     7  24
> 86    108     223  8.0   85     7  25
> 87     20      81  8.6   82     7  26
> 88     52      82 12.0   86     7  27
> 89     82     213  7.4   88     7  28
> 90     50     275  7.4   86     7  29
> 91     64     253  7.4   83     7  30
> 92     59     254  9.2   81     7  31
> 93     39      83  6.9   81     8   1
> 94      9      24 13.8   81     8   2
> 95     16      77  7.4   82     8   3
> 96     78      NA  6.9   86     8   4
> 97     35      NA  7.4   85     8   5
> 98     66      NA  4.6   87     8   6
> 99    122     255  4.0   89     8   7
> 100    89     229 10.3   90     8   8
> 101   110     207  8.0   90     8   9
> 102    NA     222  8.6   92     8  10
> 103    NA     137 11.5   86     8  11
> 104    44     192 11.5   86     8  12
> 105    28     273 11.5   82     8  13
> 106    65     157  9.7   80     8  14
> 107    NA      64 11.5   79     8  15
> 108    22      71 10.3   77     8  16
> 109    59      51  6.3   79     8  17
> 110    23     115  7.4   76     8  18
> 111    31     244 10.9   78     8  19
> 112    44     190 10.3   78     8  20
> 113    21     259 15.5   77     8  21
> 114     9      36 14.3   72     8  22
> 115    NA     255 12.6   75     8  23
> 116    45     212  9.7   79     8  24
> 117   168     238  3.4   81     8  25
> 118    73     215  8.0   86     8  26
> 119    NA     153  5.7   88     8  27
> 120    76     203  9.7   97     8  28
> 121   118     225  2.3   94     8  29
> 122    84     237  6.3   96     8  30
> 123    85     188  6.3   94     8  31
> 124    96     167  6.9   91     9   1
> 125    78     197  5.1   92     9   2
> 126    73     183  2.8   93     9   3
> 127    91     189  4.6   93     9   4
> 128    47      95  7.4   87     9   5
> 129    32      92 15.5   84     9   6
> 130    20     252 10.9   80     9   7
> 131    23     220 10.3   78     9   8
> 132    21     230 10.9   75     9   9
> 133    24     259  9.7   73     9  10
> 134    44     236 14.9   81     9  11
> 135    21     259 15.5   76     9  12
> 136    28     238  6.3   77     9  13
> 137     9      24 10.9   71     9  14
> 138    13     112 11.5   71     9  15
> 139    46     237  6.9   78     9  16
> 140    18     224 13.8   67     9  17
> 141    13      27 10.3   76     9  18
> 142    24     238 10.3   68     9  19
> 143    16     201  8.0   82     9  20
> 144    13     238 12.6   64     9  21
> 145    23      14  9.2   71     9  22
> 146    36     139 10.3   81     9  23
> 147     7      49 10.3   69     9  24
> 148    14      20 16.6   63     9  25
> 149    30     193  6.9   70     9  26
> 150    NA     145 13.2   77     9  27
> 151    14     191 14.3   75     9  28
> 152    18     131  8.0   76     9  29
> 153    20     223 11.5   68     9  30
> 
>> colnames(data) <- c("Ozone", "Solar.R", "Wind", "Temp", "Month", "Day")
> 
>> mean(data[, Ozone])
> Error in `[.data.frame`(data, , Ozone) : object 'Ozone' not found
> 
> mean(data[, "Ozone">31])
> [1] NA

The mean function has an 'na.rm' argument which you should be setting to TRUE. You should also test the whole column when using logical indexing in the "i" positional argument to "[" rather than trying to do the selection in the second argument to "["

 mean(data[ data$Ozone>31, "Ozone"], na.rm=TRUE)



> Warning message:
> In mean.default(data[, "Ozone" > 31]) :
>  argument is not numeric or logical: returning NA
>> mean(data[, "Ozone">31 & "Ozone"[!is.na("Ozone")]])

If you want to do selection and rejection of NA's inside the "[" function, both operations should be done in the first argument rather than the second:

 mean( data[ data$Ozone > 31 & !is.na(data$Ozone) , "Ozone"])

Alternates (but not recommended for programming use):

with( data, mean( data[ Ozone > 31 & !is.na(Ozone) , "Ozone"]

mean( subset( data, Ozone > 31, select=Ozone) )

> Error in "Ozone" > 31 & "Ozone"[!is.na("Ozone")] :
>  operations are possible only for numeric, logical or complex types
> 
> 	[[alternative HTML version deleted]]
> 
And finally, the r-help mailing list uses plain text. HTML is discouraged due to formatting weirdnesses, although in this instance does not seem to have caused any problems.


> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From marietta0423 at gmail.com  Fri Jul  1 21:53:28 2016
From: marietta0423 at gmail.com (Marietta Suarez)
Date: Fri, 1 Jul 2016 15:53:28 -0400
Subject: [R] turning data in list format into a matrix format
Message-ID: <CAKXurT-Bdbyy3NtHihkuhvg1uvTqpw8AXfB=wqGtmMFPc6-acQ@mail.gmail.com>

As of now, I simulate 6 variables and manipulate 2 of them. The way my
syntax is written my final database is in list mode. I need all of it to be
in just 1 database. any help would be MUCH appreciated. Here's my syntax:

fun=function(n, k,rep){
  #prepare to store data
  data=matrix(0,nrow=10*k, ncol=6)
  db=vector("list",length=rep) #here's the problem

  for (j in 1:rep){
    for (i in 1:k){
      #generate data under normal, skewed, and logistic distributions here
      data[,1]<-seq(1,nrow(data),1)
      data[,2]<-rep(i,nrow(data))
      data[,3]<-rep(n,nrow(data))
      data[,4]<-rnorm(n, 100, 15)
      data[,5]<-rsnorm(n, 100, 15, 1)
      data[,6]<-rlogis(n, 100, 15)
    }
    db[[j]]<-data
  }
  DataReturn<<-db
  return(DataReturn)
  #DataReturn <- data.frame(matrix(unlist(DataReturn), nrow=rep,
byrow=T)) #here's
where I tried to solve it unsuccessfully
}

	[[alternative HTML version deleted]]


From jimplante at me.com  Fri Jul  1 21:56:21 2016
From: jimplante at me.com (James Plante)
Date: Fri, 01 Jul 2016 14:56:21 -0500
Subject: [R] Anybody know the cause of this?
Message-ID: <B54D5C64-6954-4BB7-9D9E-1F4A602FFDBD@me.com>

Just upgraded to R 3.3.1; when I updated the packages on CRAN, I got a BUNCH of warning messages like the ones below:

2016-07-01 14:44:19.840 R[369:3724] IMKClient Stall detected, *please Report* your user scenario attaching a spindump (or sysdiagnose) that captures the problem - (imkxpc_windowLevelWithReply:) block performed very slowly (2.40 secs).
starting httpd help server ... done
> spindump()
Error: could not find function "spindump"
In addition: Warning message:
In library() :
  library ?/Users/jimplante/Library/R/3.3/library? contains no packages
2016-07-01 14:47:32.774 R[369:3724] IMKClient Stall detected, *please Report* your user scenario attaching a spindump (or sysdiagnose) that captures the problem - (imkxpc_attributesForCharacterIndex:reply:) block performed very slowly (1.42 secs).
> 

This is not interfering with any of my work, but I?m curious to know what it?s warning me about. 
Two more questions: 1) What?s a spindump; and 2) What?s a sysdiagnose? 
And finally, how would I get either one of those, and to whom would I send them?

Thanks,

Jim

From miaojpm at gmail.com  Fri Jul  1 23:33:22 2016
From: miaojpm at gmail.com (John)
Date: Fri, 1 Jul 2016 14:33:22 -0700
Subject: [R] Can R read Word fonts and comments?
In-Reply-To: <CAGxFJbSSnMK7_cCfAMXVMT1fM3zWUQsjmNwfgtb_fh5XzRRdCQ@mail.gmail.com>
References: <CABcx46D0gNOHEau_iEd_F-hMM1MA=J0r41sLwoFY8SMgE47nZw@mail.gmail.com>
	<CAGxFJbSSnMK7_cCfAMXVMT1fM3zWUQsjmNwfgtb_fh5XzRRdCQ@mail.gmail.com>
Message-ID: <CABcx46A5O2Oc5XMdBSM=uHHQJdbvMp6U=8AfcOYpx5_Jr3CrpA@mail.gmail.com>

Yes, I have done some search (e.g., tm, markdown, etc), but I can't find
this function.
If you know any package that works for this purpose, that would be quite
helpful.
Thanks,

John

2016-06-28 16:50 GMT-07:00 Bert Gunter <bgunter.4567 at gmail.com>:

> Did you try searching before posting here? -- e.g. a web search or on
> rseek.org ?
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Jun 28, 2016 at 3:53 PM, John <miaojpm at gmail.com> wrote:
> > Hi,
> >
> >    From time to time I highlight the word documents with red/blue color
> or
> > italic/bold fonts, and I also add comments to a file. Is there a
> > package/function to let R extract the italic/bold blue/red words and
> > comments from a docx/doc file?
> >
> >    I am aware that there are a few packages reading Word, but don't know
> > which one is able to do it.
> >
> >    Thanks,
> >
> > John
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Jul  2 00:11:53 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 1 Jul 2016 15:11:53 -0700
Subject: [R] trouble double looping to generate data for a meta-analysis
In-Reply-To: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>
References: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>
Message-ID: <CAGxFJbQJkDN9M3O305SRJsg=ys+SviOtcesMUTv0YpgVwNvP5w@mail.gmail.com>

Hint: It's much more efficient not to loop and generate random data in
a single call only once -- then make your samples. (This can even
often be done with different distribution parameters, as in many cases
these can also be vactorized)

Example:

## 1000 random samples of size 100

> set.seed(1122)
> samps.norm <- matrix(rnorm(1e5),nrow = 100 )
> dim(samps.norm)
[1]  100 1000

## This was instantaneous on my machine.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jul 1, 2016 at 10:28 AM, Marietta Suarez <marietta0423 at gmail.com> wrote:
> i'm trying to generate data for a meta analysis. 1- generate data following
> a normal distribution, 2- generate data following a skewed distribution, 3-
> generate data following a logistic distribution. i need to loop this
> because the # of studies in each meta will be either 10 or 15. k or total
> number of studies in the meta will be 5. i need to loop twice to repeat
> this process 10 times. database should be 3 columns (distributions) by 65
> rows x 10 reps
>
>
> here's my code, not sure what's not working:
> library(fGarch)
>
> #n reps =10
> rep=10
>
> #begin function here, need to vary n and k, when k=2 n=10, when k3 n=15
> fun=function(n, k){
>
>   #prepare to store data
>   data=matrix(0,nrow=10*k, ncol=3)
>   db=matrix(0,nrow=650, ncol=3)
>
>   for (j in 1:rep)
>   {
>     for (i in 1:k)
>     {
>       #generate data under normal, skewed, and logistic distributions here
>
>       data[,1]=rnorm(n, 100, 15)
>       data[,2]=rsnorm(n, 100, 15, 1)
>       data[,3]=rlogis(n, 100, 15)
>     }
>       [j]=db
>   }
> }
>
> save=fun(10,2)
>
> Please help!!!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Sat Jul  2 00:13:52 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 1 Jul 2016 15:13:52 -0700
Subject: [R] Can R read Word fonts and comments?
In-Reply-To: <CABcx46A5O2Oc5XMdBSM=uHHQJdbvMp6U=8AfcOYpx5_Jr3CrpA@mail.gmail.com>
References: <CABcx46D0gNOHEau_iEd_F-hMM1MA=J0r41sLwoFY8SMgE47nZw@mail.gmail.com>
	<CAGxFJbSSnMK7_cCfAMXVMT1fM3zWUQsjmNwfgtb_fh5XzRRdCQ@mail.gmail.com>
	<CABcx46A5O2Oc5XMdBSM=uHHQJdbvMp6U=8AfcOYpx5_Jr3CrpA@mail.gmail.com>
Message-ID: <CAGxFJbS0s0g88v7aN9MJQBjEVkhB+9vUhS1WOJi9+UAAP2H9Uw@mail.gmail.com>

No, sorry -- all I would do is search.

-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jul 1, 2016 at 2:33 PM, John <miaojpm at gmail.com> wrote:
> Yes, I have done some search (e.g., tm, markdown, etc), but I can't find
> this function.
> If you know any package that works for this purpose, that would be quite
> helpful.
> Thanks,
>
> John
>
> 2016-06-28 16:50 GMT-07:00 Bert Gunter <bgunter.4567 at gmail.com>:
>>
>> Did you try searching before posting here? -- e.g. a web search or on
>> rseek.org ?
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Tue, Jun 28, 2016 at 3:53 PM, John <miaojpm at gmail.com> wrote:
>> > Hi,
>> >
>> >    From time to time I highlight the word documents with red/blue color
>> > or
>> > italic/bold fonts, and I also add comments to a file. Is there a
>> > package/function to let R extract the italic/bold blue/red words and
>> > comments from a docx/doc file?
>> >
>> >    I am aware that there are a few packages reading Word, but don't know
>> > which one is able to do it.
>> >
>> >    Thanks,
>> >
>> > John
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>
>


From bgunter.4567 at gmail.com  Sat Jul  2 00:24:08 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 1 Jul 2016 15:24:08 -0700
Subject: [R] Anybody know the cause of this?
In-Reply-To: <B54D5C64-6954-4BB7-9D9E-1F4A602FFDBD@me.com>
References: <B54D5C64-6954-4BB7-9D9E-1F4A602FFDBD@me.com>
Message-ID: <CAGxFJbQvM0SOyThZt5C6CPpCiheHz4N0gRNVEEk6xBuDyqS6Eg@mail.gmail.com>

I don't have a clue, but I suspect that those who might would be
helped by your providing the output of the sessionInfo() command  +
perhaps other relevant info on your computing environment.

Cheers,

Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jul 1, 2016 at 12:56 PM, James Plante <jimplante at me.com> wrote:
> Just upgraded to R 3.3.1; when I updated the packages on CRAN, I got a BUNCH of warning messages like the ones below:
>
> 2016-07-01 14:44:19.840 R[369:3724] IMKClient Stall detected, *please Report* your user scenario attaching a spindump (or sysdiagnose) that captures the problem - (imkxpc_windowLevelWithReply:) block performed very slowly (2.40 secs).
> starting httpd help server ... done
>> spindump()
> Error: could not find function "spindump"
> In addition: Warning message:
> In library() :
>   library ?/Users/jimplante/Library/R/3.3/library? contains no packages
> 2016-07-01 14:47:32.774 R[369:3724] IMKClient Stall detected, *please Report* your user scenario attaching a spindump (or sysdiagnose) that captures the problem - (imkxpc_attributesForCharacterIndex:reply:) block performed very slowly (1.42 secs).
>>
>
> This is not interfering with any of my work, but I?m curious to know what it?s warning me about.
> Two more questions: 1) What?s a spindump; and 2) What?s a sysdiagnose?
> And finally, how would I get either one of those, and to whom would I send them?
>
> Thanks,
>
> Jim
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Sat Jul  2 02:28:01 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 2 Jul 2016 12:28:01 +1200
Subject: [R] [FORGED]  turning data in list format into a matrix format
In-Reply-To: <CAKXurT-Bdbyy3NtHihkuhvg1uvTqpw8AXfB=wqGtmMFPc6-acQ@mail.gmail.com>
References: <CAKXurT-Bdbyy3NtHihkuhvg1uvTqpw8AXfB=wqGtmMFPc6-acQ@mail.gmail.com>
Message-ID: <c25fd0d0-a712-428b-361b-764c251b307c@auckland.ac.nz>


Your question is filled with uh, infelicities.  See inline below.

On 02/07/16 07:53, Marietta Suarez wrote:
> As of now, I simulate 6 variables and manipulate 2 of them. The way my
> syntax is written my final database

    Do you mean "data set"?

is in list mode. I need all of it to be
> in just 1 database

    Do you mean data frame?

any help would be MUCH appreciated. Here's my syntax:
>
> fun=function(n, k,rep){
>   #prepare to store data
>   data=matrix(0,nrow=10*k, ncol=6)
>   db=vector("list",length=rep) #here's the problem
>
>   for (j in 1:rep){
>     for (i in 1:k){
>       #generate data under normal, skewed, and logistic distributions here
>       data[,1]<-seq(1,nrow(data),1)
>       data[,2]<-rep(i,nrow(data))
>       data[,3]<-rep(n,nrow(data))
>       data[,4]<-rnorm(n, 100, 15)
>       data[,5]<-rsnorm(n, 100, 15, 1)

    There is no such function as "rsnorm" in base R or in the standard
    packages.  Where does this function come from?  Do not expect
    your respondents to be psychic.

>       data[,6]<-rlogis(n, 100, 15)
>     }
>     db[[j]]<-data
>   }
>   DataReturn<<-db

    Do *NOT* use "<<-"!  Especially if you actually mean "<-"!!!

    What is the point of "DataReturn" anyway?  It's the same thing
    as "db".  Why confuse things by introducing a new name?

>   return(DataReturn)
>   #DataReturn <- data.frame(matrix(unlist(DataReturn), nrow=rep,
> byrow=T)) #here's
> where I tried to solve it unsuccessfully
> }

In what sense is this unsuccessful?  It returns a data frame.  What is 
wrong with this data frame?  What did you *want* it to look like?

A *reproducible* "toy" example showing what you want to get would be 
helpful.

A wild guess is that you want your reps "stacked" into a single data 
frame.  In which case something like

    do.call(rbind,db)

may be what you want.

Finally, do you really want the result to be a data frame?  Since all 
values dealt with are numeric, you might as well use a matrix, and save 
on the overheads that data frames involve.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ccberry at ucsd.edu  Sat Jul  2 04:31:22 2016
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Fri, 1 Jul 2016 19:31:22 -0700
Subject: [R] How to identify runs or clusters of events in time
In-Reply-To: <PS1PR03MB182004C026D955C9149A0AEEE6250@PS1PR03MB1820.apcprd03.prod.outlook.com>
References: <PS1PR03MB182004C026D955C9149A0AEEE6250@PS1PR03MB1820.apcprd03.prod.outlook.com>
Message-ID: <alpine.OSX.2.20.1607011858050.5560@charles-berrys-macbook.local>


See below

On Fri, 1 Jul 2016, Mark Shanks wrote:

> Hi,
>
>
> Imagine the two problems:
>
>
> 1) You have an event that occurs repeatedly over time. You want to 
> identify periods when the event occurs more frequently than the base 
> rate of occurrence. Ideally, you don't want to have to specify the 
> period (e.g., break into months), so the analysis can be sensitive to 
> scenarios such as many events happening only between, e.g., June 10 and 
> June 15 - even though the overall number of events for the month may not 
> be much greater than usual. Similarly, there may be a cluster of events 
> that occur from March 28 to April 3. Ideally, you want to pull out the 
> base rate of occurrence and highlight only the periods when the 
> frequency is less or greater than the base rate.
>

A good place to start is:

Siegmund, D. O., N. R. Zhang, and B. Yakir. "False discovery rate
for scanning statistics." Biometrika 98.4 (2011): 979-985.

and

Aldous, David. Probability approximations via the Poisson clumping 
heuristic. Vol. 77. Springer Science & Business Media, 2013.

---

A nice illustration of how scan statistcis can be used is:

Aberdein, Jody, and David Spiegelhalter. "Have London's roads
become more dangerous for cyclists?." Significance 10.6 (2013):
46-48.


>
> 2) Events again occur repeatedly over time in an inconsistent way. 
> However, this time, the event has positive or negative outcomes - such 
> as a spot check of conformity to regulations. You again want to know 
> whether there is a group of negative outcomes close together in time. 
> This analysis should take into account the negative outcomes as well 
> though. E.g., if from June 10 to June 15 you get 5 negative outcomes and 
> no positive outcomes it should be flagged. On the other hand, if from 
> June 10 to June 15 you get 5 negative outcomes interspersed between many 
> positive outcomes it should be ignored.
>
>
> I'm guessing that there is some statistical approach designed to look at 
> these types of issues. What is it called?

`Scan statistic' is a good search term. `Poisson clumping', too.

> What package in R implements it? I basically just need to know where to 
> start.
>
>

There are some R packages.

CRAN has packages SNscan and graphscan, which sound like they 
might interest you.

My BioConductor package geneRxCluster:

http://bioconductor.org/packages/release/bioc/html/geneRxCluster.html

seeks clusters in a binary sequence as described in detail at

http://bioinformatics.oxfordjournals.org/content/30/11/1493

HTH,

Chuck


From dwinsemius at comcast.net  Sat Jul  2 05:14:20 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 1 Jul 2016 20:14:20 -0700
Subject: [R] Can R read Word fonts and comments?
In-Reply-To: <CAGxFJbS0s0g88v7aN9MJQBjEVkhB+9vUhS1WOJi9+UAAP2H9Uw@mail.gmail.com>
References: <CABcx46D0gNOHEau_iEd_F-hMM1MA=J0r41sLwoFY8SMgE47nZw@mail.gmail.com>
	<CAGxFJbSSnMK7_cCfAMXVMT1fM3zWUQsjmNwfgtb_fh5XzRRdCQ@mail.gmail.com>
	<CABcx46A5O2Oc5XMdBSM=uHHQJdbvMp6U=8AfcOYpx5_Jr3CrpA@mail.gmail.com>
	<CAGxFJbS0s0g88v7aN9MJQBjEVkhB+9vUhS1WOJi9+UAAP2H9Uw@mail.gmail.com>
Message-ID: <5FFC2C2F-82C2-4773-90CB-0C32DF869086@comcast.net>

It?s my understanding that docx and xlsx files are zipped containers that have their data in XML files. You should try unzipping one and examining it with a viewer. You may then be able to use pkg:XML.

? 
David.

> On Jul 1, 2016, at 3:13 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> No, sorry -- all I would do is search.
> 
> -- Bert
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Fri, Jul 1, 2016 at 2:33 PM, John <miaojpm at gmail.com> wrote:
>> Yes, I have done some search (e.g., tm, markdown, etc), but I can't find
>> this function.
>> If you know any package that works for this purpose, that would be quite
>> helpful.
>> Thanks,
>> 
>> John
>> 
>> 2016-06-28 16:50 GMT-07:00 Bert Gunter <bgunter.4567 at gmail.com>:
>>> 
>>> Did you try searching before posting here? -- e.g. a web search or on
>>> rseek.org ?
>>> 
>>> Cheers,
>>> Bert
>>> 
>>> 
>>> Bert Gunter
>>> 
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>> 
>>> 
>>> On Tue, Jun 28, 2016 at 3:53 PM, John <miaojpm at gmail.com> wrote:
>>>> Hi,
>>>> 
>>>>   From time to time I highlight the word documents with red/blue color
>>>> or
>>>> italic/bold fonts, and I also add comments to a file. Is there a
>>>> package/function to let R extract the italic/bold blue/red words and
>>>> comments from a docx/doc file?
>>>> 
>>>>   I am aware that there are a few packages reading Word, but don't know
>>>> which one is able to do it.
>>>> 
>>>>   Thanks,
>>>> 
>>>> John
>>>> 
>>>>        [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ulrik.stervbo at gmail.com  Sat Jul  2 11:16:34 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Sat, 02 Jul 2016 09:16:34 +0000
Subject: [R] Difficulty subsetting data frames using logical operators
In-Reply-To: <CAPvuKKzXBypVoMthRdAXfUc-vjGpExvi=Lv4et7ni1ymgNGbWQ@mail.gmail.com>
References: <CAPvuKKzXBypVoMthRdAXfUc-vjGpExvi=Lv4et7ni1ymgNGbWQ@mail.gmail.com>
Message-ID: <CAKVAULOGG9bBhrmaVsgOgbiVrrTXveo6DAA9CBH+ahadM7hhkg@mail.gmail.com>

Hi Giles,

Look at ?mean

In addition it seems you need to read a few tutorials on R. Several are
already mentioned on this list otherwise Google can direct.

Hope this helps,
Ulrik

Giles Bischoff <gab4 at st-andrews.ac.uk> schrieb am Fr., 1. Juli 2016 18:55:

> So, I uploaded a data set via my directory using the command data <-
> data.frame(read.csv("hw1_data.csv")) and then tried to subset that data
> using logical operators. Specifically, I was trying to make it so that I
> got all the rows in which the values for "Ozone" (a column in the data set)
> were greater than 31 (I was trying to get the mean of all said values).
> Then, I tried using the command data[ , "Ozone">31]. Additionally, I had
> trouble getting it so that I had all the rows where all the values in
> "Ozone">31 & "Temp">90 simultaneously. There were some NA values in both of
> those columns, so that might be it. If someone could help me to figure out
> how to remove those values, that'd be great as well. I'm using a Mac (OS X)
> with the latest version of R (3.1.2. I think??).
>
> Here is some of the code I used:
>
> >data <- data.frame(read.csv("hw1_data.csv"))
> > data
>     Ozone Solar.R Wind Temp Month Day
> 1      41     190  7.4   67     5   1
> 2      36     118  8.0   72     5   2
> 3      12     149 12.6   74     5   3
> 4      18     313 11.5   62     5   4
> 5      NA      NA 14.3   56     5   5
> 6      28      NA 14.9   66     5   6
> 7      23     299  8.6   65     5   7
> 8      19      99 13.8   59     5   8
> 9       8      19 20.1   61     5   9
> 10     NA     194  8.6   69     5  10
> 11      7      NA  6.9   74     5  11
> 12     16     256  9.7   69     5  12
> 13     11     290  9.2   66     5  13
> 14     14     274 10.9   68     5  14
> 15     18      65 13.2   58     5  15
> 16     14     334 11.5   64     5  16
> 17     34     307 12.0   66     5  17
> 18      6      78 18.4   57     5  18
> 19     30     322 11.5   68     5  19
> 20     11      44  9.7   62     5  20
> 21      1       8  9.7   59     5  21
> 22     11     320 16.6   73     5  22
> 23      4      25  9.7   61     5  23
> 24     32      92 12.0   61     5  24
> 25     NA      66 16.6   57     5  25
> 26     NA     266 14.9   58     5  26
> 27     NA      NA  8.0   57     5  27
> 28     23      13 12.0   67     5  28
> 29     45     252 14.9   81     5  29
> 30    115     223  5.7   79     5  30
> 31     37     279  7.4   76     5  31
> 32     NA     286  8.6   78     6   1
> 33     NA     287  9.7   74     6   2
> 34     NA     242 16.1   67     6   3
> 35     NA     186  9.2   84     6   4
> 36     NA     220  8.6   85     6   5
> 37     NA     264 14.3   79     6   6
> 38     29     127  9.7   82     6   7
> 39     NA     273  6.9   87     6   8
> 40     71     291 13.8   90     6   9
> 41     39     323 11.5   87     6  10
> 42     NA     259 10.9   93     6  11
> 43     NA     250  9.2   92     6  12
> 44     23     148  8.0   82     6  13
> 45     NA     332 13.8   80     6  14
> 46     NA     322 11.5   79     6  15
> 47     21     191 14.9   77     6  16
> 48     37     284 20.7   72     6  17
> 49     20      37  9.2   65     6  18
> 50     12     120 11.5   73     6  19
> 51     13     137 10.3   76     6  20
> 52     NA     150  6.3   77     6  21
> 53     NA      59  1.7   76     6  22
> 54     NA      91  4.6   76     6  23
> 55     NA     250  6.3   76     6  24
> 56     NA     135  8.0   75     6  25
> 57     NA     127  8.0   78     6  26
> 58     NA      47 10.3   73     6  27
> 59     NA      98 11.5   80     6  28
> 60     NA      31 14.9   77     6  29
> 61     NA     138  8.0   83     6  30
> 62    135     269  4.1   84     7   1
> 63     49     248  9.2   85     7   2
> 64     32     236  9.2   81     7   3
> 65     NA     101 10.9   84     7   4
> 66     64     175  4.6   83     7   5
> 67     40     314 10.9   83     7   6
> 68     77     276  5.1   88     7   7
> 69     97     267  6.3   92     7   8
> 70     97     272  5.7   92     7   9
> 71     85     175  7.4   89     7  10
> 72     NA     139  8.6   82     7  11
> 73     10     264 14.3   73     7  12
> 74     27     175 14.9   81     7  13
> 75     NA     291 14.9   91     7  14
> 76      7      48 14.3   80     7  15
> 77     48     260  6.9   81     7  16
> 78     35     274 10.3   82     7  17
> 79     61     285  6.3   84     7  18
> 80     79     187  5.1   87     7  19
> 81     63     220 11.5   85     7  20
> 82     16       7  6.9   74     7  21
> 83     NA     258  9.7   81     7  22
> 84     NA     295 11.5   82     7  23
> 85     80     294  8.6   86     7  24
> 86    108     223  8.0   85     7  25
> 87     20      81  8.6   82     7  26
> 88     52      82 12.0   86     7  27
> 89     82     213  7.4   88     7  28
> 90     50     275  7.4   86     7  29
> 91     64     253  7.4   83     7  30
> 92     59     254  9.2   81     7  31
> 93     39      83  6.9   81     8   1
> 94      9      24 13.8   81     8   2
> 95     16      77  7.4   82     8   3
> 96     78      NA  6.9   86     8   4
> 97     35      NA  7.4   85     8   5
> 98     66      NA  4.6   87     8   6
> 99    122     255  4.0   89     8   7
> 100    89     229 10.3   90     8   8
> 101   110     207  8.0   90     8   9
> 102    NA     222  8.6   92     8  10
> 103    NA     137 11.5   86     8  11
> 104    44     192 11.5   86     8  12
> 105    28     273 11.5   82     8  13
> 106    65     157  9.7   80     8  14
> 107    NA      64 11.5   79     8  15
> 108    22      71 10.3   77     8  16
> 109    59      51  6.3   79     8  17
> 110    23     115  7.4   76     8  18
> 111    31     244 10.9   78     8  19
> 112    44     190 10.3   78     8  20
> 113    21     259 15.5   77     8  21
> 114     9      36 14.3   72     8  22
> 115    NA     255 12.6   75     8  23
> 116    45     212  9.7   79     8  24
> 117   168     238  3.4   81     8  25
> 118    73     215  8.0   86     8  26
> 119    NA     153  5.7   88     8  27
> 120    76     203  9.7   97     8  28
> 121   118     225  2.3   94     8  29
> 122    84     237  6.3   96     8  30
> 123    85     188  6.3   94     8  31
> 124    96     167  6.9   91     9   1
> 125    78     197  5.1   92     9   2
> 126    73     183  2.8   93     9   3
> 127    91     189  4.6   93     9   4
> 128    47      95  7.4   87     9   5
> 129    32      92 15.5   84     9   6
> 130    20     252 10.9   80     9   7
> 131    23     220 10.3   78     9   8
> 132    21     230 10.9   75     9   9
> 133    24     259  9.7   73     9  10
> 134    44     236 14.9   81     9  11
> 135    21     259 15.5   76     9  12
> 136    28     238  6.3   77     9  13
> 137     9      24 10.9   71     9  14
> 138    13     112 11.5   71     9  15
> 139    46     237  6.9   78     9  16
> 140    18     224 13.8   67     9  17
> 141    13      27 10.3   76     9  18
> 142    24     238 10.3   68     9  19
> 143    16     201  8.0   82     9  20
> 144    13     238 12.6   64     9  21
> 145    23      14  9.2   71     9  22
> 146    36     139 10.3   81     9  23
> 147     7      49 10.3   69     9  24
> 148    14      20 16.6   63     9  25
> 149    30     193  6.9   70     9  26
> 150    NA     145 13.2   77     9  27
> 151    14     191 14.3   75     9  28
> 152    18     131  8.0   76     9  29
> 153    20     223 11.5   68     9  30
>
>  >colnames(data) <- c("Ozone", "Solar.R", "Wind", "Temp", "Month", "Day")
>
> > mean(data[, Ozone])
> Error in `[.data.frame`(data, , Ozone) : object 'Ozone' not found
>
> mean(data[, "Ozone">31])
> [1] NA
> Warning message:
> In mean.default(data[, "Ozone" > 31]) :
>   argument is not numeric or logical: returning NA
> >  mean(data[, "Ozone">31 & "Ozone"[!is.na("Ozone")]])
> Error in "Ozone" > 31 & "Ozone"[!is.na("Ozone")] :
>   operations are possible only for numeric, logical or complex types
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Sat Jul  2 13:44:08 2016
From: jrkrideau at inbox.com (John Kane)
Date: Sat, 2 Jul 2016 03:44:08 -0800
Subject: [R] Difficulty subsetting data frames using logical operators
In-Reply-To: <CAPvuKKzXBypVoMthRdAXfUc-vjGpExvi=Lv4et7ni1ymgNGbWQ@mail.gmail.com>
Message-ID: <22B1F691C8A.0000014Ajrkrideau@inbox.com>

Just as a very minor point "read.csv" returns a data.frame. Therefore the data.frame in "data <- data.frame(read.csv("hw1_data.csv"))" is redundant and just adds clutter to the code.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: gab4 at st-andrews.ac.uk
> Sent: Fri, 1 Jul 2016 02:11:48 -0700
> To: r-help at r-project.org
> Subject: [R] Difficulty subsetting data frames using logical operators
> 
> So, I uploaded a data set via my directory using the command data <-
> data.frame(read.csv("hw1_data.csv")) and then tried to subset that data
> using logical operators. Specifically, I was trying to make it so that I
> got all the rows in which the values for "Ozone" (a column in the data
> set)
> were greater than 31 (I was trying to get the mean of all said values).
> Then, I tried using the command data[ , "Ozone">31]. Additionally, I had
> trouble getting it so that I had all the rows where all the values in
> "Ozone">31 & "Temp">90 simultaneously. There were some NA values in both
> of
> those columns, so that might be it. If someone could help me to figure
> out
> how to remove those values, that'd be great as well. I'm using a Mac (OS
> X)
> with the latest version of R (3.1.2. I think??).
> 
> Here is some of the code I used:
> 
> >data <- data.frame(read.csv("hw1_data.csv"))
>> data
>     Ozone Solar.R Wind Temp Month Day
> 1      41     190  7.4   67     5   1
> 2      36     118  8.0   72     5   2
> 3      12     149 12.6   74     5   3
> 4      18     313 11.5   62     5   4
> 5      NA      NA 14.3   56     5   5
> 6      28      NA 14.9   66     5   6
> 7      23     299  8.6   65     5   7
> 8      19      99 13.8   59     5   8
> 9       8      19 20.1   61     5   9
> 10     NA     194  8.6   69     5  10
> 11      7      NA  6.9   74     5  11
> 12     16     256  9.7   69     5  12
> 13     11     290  9.2   66     5  13
> 14     14     274 10.9   68     5  14
> 15     18      65 13.2   58     5  15
> 16     14     334 11.5   64     5  16
> 17     34     307 12.0   66     5  17
> 18      6      78 18.4   57     5  18
> 19     30     322 11.5   68     5  19
> 20     11      44  9.7   62     5  20
> 21      1       8  9.7   59     5  21
> 22     11     320 16.6   73     5  22
> 23      4      25  9.7   61     5  23
> 24     32      92 12.0   61     5  24
> 25     NA      66 16.6   57     5  25
> 26     NA     266 14.9   58     5  26
> 27     NA      NA  8.0   57     5  27
> 28     23      13 12.0   67     5  28
> 29     45     252 14.9   81     5  29
> 30    115     223  5.7   79     5  30
> 31     37     279  7.4   76     5  31
> 32     NA     286  8.6   78     6   1
> 33     NA     287  9.7   74     6   2
> 34     NA     242 16.1   67     6   3
> 35     NA     186  9.2   84     6   4
> 36     NA     220  8.6   85     6   5
> 37     NA     264 14.3   79     6   6
> 38     29     127  9.7   82     6   7
> 39     NA     273  6.9   87     6   8
> 40     71     291 13.8   90     6   9
> 41     39     323 11.5   87     6  10
> 42     NA     259 10.9   93     6  11
> 43     NA     250  9.2   92     6  12
> 44     23     148  8.0   82     6  13
> 45     NA     332 13.8   80     6  14
> 46     NA     322 11.5   79     6  15
> 47     21     191 14.9   77     6  16
> 48     37     284 20.7   72     6  17
> 49     20      37  9.2   65     6  18
> 50     12     120 11.5   73     6  19
> 51     13     137 10.3   76     6  20
> 52     NA     150  6.3   77     6  21
> 53     NA      59  1.7   76     6  22
> 54     NA      91  4.6   76     6  23
> 55     NA     250  6.3   76     6  24
> 56     NA     135  8.0   75     6  25
> 57     NA     127  8.0   78     6  26
> 58     NA      47 10.3   73     6  27
> 59     NA      98 11.5   80     6  28
> 60     NA      31 14.9   77     6  29
> 61     NA     138  8.0   83     6  30
> 62    135     269  4.1   84     7   1
> 63     49     248  9.2   85     7   2
> 64     32     236  9.2   81     7   3
> 65     NA     101 10.9   84     7   4
> 66     64     175  4.6   83     7   5
> 67     40     314 10.9   83     7   6
> 68     77     276  5.1   88     7   7
> 69     97     267  6.3   92     7   8
> 70     97     272  5.7   92     7   9
> 71     85     175  7.4   89     7  10
> 72     NA     139  8.6   82     7  11
> 73     10     264 14.3   73     7  12
> 74     27     175 14.9   81     7  13
> 75     NA     291 14.9   91     7  14
> 76      7      48 14.3   80     7  15
> 77     48     260  6.9   81     7  16
> 78     35     274 10.3   82     7  17
> 79     61     285  6.3   84     7  18
> 80     79     187  5.1   87     7  19
> 81     63     220 11.5   85     7  20
> 82     16       7  6.9   74     7  21
> 83     NA     258  9.7   81     7  22
> 84     NA     295 11.5   82     7  23
> 85     80     294  8.6   86     7  24
> 86    108     223  8.0   85     7  25
> 87     20      81  8.6   82     7  26
> 88     52      82 12.0   86     7  27
> 89     82     213  7.4   88     7  28
> 90     50     275  7.4   86     7  29
> 91     64     253  7.4   83     7  30
> 92     59     254  9.2   81     7  31
> 93     39      83  6.9   81     8   1
> 94      9      24 13.8   81     8   2
> 95     16      77  7.4   82     8   3
> 96     78      NA  6.9   86     8   4
> 97     35      NA  7.4   85     8   5
> 98     66      NA  4.6   87     8   6
> 99    122     255  4.0   89     8   7
> 100    89     229 10.3   90     8   8
> 101   110     207  8.0   90     8   9
> 102    NA     222  8.6   92     8  10
> 103    NA     137 11.5   86     8  11
> 104    44     192 11.5   86     8  12
> 105    28     273 11.5   82     8  13
> 106    65     157  9.7   80     8  14
> 107    NA      64 11.5   79     8  15
> 108    22      71 10.3   77     8  16
> 109    59      51  6.3   79     8  17
> 110    23     115  7.4   76     8  18
> 111    31     244 10.9   78     8  19
> 112    44     190 10.3   78     8  20
> 113    21     259 15.5   77     8  21
> 114     9      36 14.3   72     8  22
> 115    NA     255 12.6   75     8  23
> 116    45     212  9.7   79     8  24
> 117   168     238  3.4   81     8  25
> 118    73     215  8.0   86     8  26
> 119    NA     153  5.7   88     8  27
> 120    76     203  9.7   97     8  28
> 121   118     225  2.3   94     8  29
> 122    84     237  6.3   96     8  30
> 123    85     188  6.3   94     8  31
> 124    96     167  6.9   91     9   1
> 125    78     197  5.1   92     9   2
> 126    73     183  2.8   93     9   3
> 127    91     189  4.6   93     9   4
> 128    47      95  7.4   87     9   5
> 129    32      92 15.5   84     9   6
> 130    20     252 10.9   80     9   7
> 131    23     220 10.3   78     9   8
> 132    21     230 10.9   75     9   9
> 133    24     259  9.7   73     9  10
> 134    44     236 14.9   81     9  11
> 135    21     259 15.5   76     9  12
> 136    28     238  6.3   77     9  13
> 137     9      24 10.9   71     9  14
> 138    13     112 11.5   71     9  15
> 139    46     237  6.9   78     9  16
> 140    18     224 13.8   67     9  17
> 141    13      27 10.3   76     9  18
> 142    24     238 10.3   68     9  19
> 143    16     201  8.0   82     9  20
> 144    13     238 12.6   64     9  21
> 145    23      14  9.2   71     9  22
> 146    36     139 10.3   81     9  23
> 147     7      49 10.3   69     9  24
> 148    14      20 16.6   63     9  25
> 149    30     193  6.9   70     9  26
> 150    NA     145 13.2   77     9  27
> 151    14     191 14.3   75     9  28
> 152    18     131  8.0   76     9  29
> 153    20     223 11.5   68     9  30
> 
>  >colnames(data) <- c("Ozone", "Solar.R", "Wind", "Temp", "Month", "Day")
> 
>> mean(data[, Ozone])
> Error in `[.data.frame`(data, , Ozone) : object 'Ozone' not found
> 
> mean(data[, "Ozone">31])
> [1] NA
> Warning message:
> In mean.default(data[, "Ozone" > 31]) :
>   argument is not numeric or logical: returning NA
>>  mean(data[, "Ozone">31 & "Ozone"[!is.na("Ozone")]])
> Error in "Ozone" > 31 & "Ozone"[!is.na("Ozone")] :
>   operations are possible only for numeric, logical or complex types
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From lists at dewey.myzen.co.uk  Sat Jul  2 16:04:09 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 2 Jul 2016 15:04:09 +0100
Subject: [R] trouble double looping to generate data for a meta-analysis
In-Reply-To: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>
References: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>
Message-ID: <9b97e016-d162-8002-bcf3-f5a6eff54534@dewey.myzen.co.uk>

Dear Marietta

Comments in-line

On 01/07/2016 18:28, Marietta Suarez wrote:
> i'm trying to generate data for a meta analysis. 1- generate data following
> a normal distribution, 2- generate data following a skewed distribution, 3-
> generate data following a logistic distribution. i need to loop this
> because the # of studies in each meta will be either 10 or 15. k or total
> number of studies in the meta will be 5. i need to loop twice to repeat
> this process 10 times. database should be 3 columns (distributions) by 65
> rows x 10 reps
>
>
> here's my code, not sure what's not working:
> library(fGarch)
>
> #n reps =10
> rep=10
>
> #begin function here, need to vary n and k, when k=2 n=10, when k3 n=15

So you define fun

> fun=function(n, k){
>
>   #prepare to store data
>   data=matrix(0,nrow=10*k, ncol=3)
>   db=matrix(0,nrow=650, ncol=3)
>
>   for (j in 1:rep)
>   {
>     for (i in 1:k)
>     {
>       #generate data under normal, skewed, and logistic distributions here
>
>       data[,1]=rnorm(n, 100, 15)
>       data[,2]=rsnorm(n, 100, 15, 1)
>       data[,3]=rlogis(n, 100, 15)
>     }
>       [j]=db

At this point have you actually put anything in db?
>   }
> }
>

Assuming that is where fun ends I think you meant to precede it with data

It is not a good idea to use data as the name of your data as it is used 
by R itself, db would have been fine though.

> save=fun(10,2)
>
> Please help!!!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From KWamae at kemri-wellcome.org  Sat Jul  2 11:57:39 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sat, 2 Jul 2016 09:57:39 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions |
 For a Large Dataset
Message-ID: <1A5B5702-2764-47EE-B6A0-CFE590064198@kemri-wellcome.org>

I have a drug-trial study dataset (attached image).

Since its a large and complex dataset (at least to me) and I hope to be as clear as possible with my question.
The dataset is from a study where individuals are given drugs and followed up over a period spanning two consecutive years. Individuals do not start treatment on the same day and once they start, the variable "drug-admin" is marked "x" as well as the time they stop treatment in the following year.
There exists another variable, "study_id", that I hope to populate as can be seen in the dataset, with the following conditions:

For every individual
?    if the individual has entries that show they received drugs both on the start and end date (marked with the "x")
?    if the start of drug administration falls in month == 2 | 3 and end of administration falls in month == 2 | 4
?    then, using the date that marks the start of drug administration, populate the variable _"study_id"_ in all the rows that fall within the timeframe that the individual was given drugs but excluding the end of drug administration.
I have tried my level best and while I have explored several examples online, I haven't managed to solve this. The dataset contains close to 6000 individuals spanning 10 years and my best bet was to use a loop which keeps crushing R after running for close to 30min. I have also read that dplyr may do the job but my attempts have been in vain.

sample code
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
individual <- unique (df$ID)  #vector of individuals
datalength <- dim(df)[1]      #number of rows in dataframe

for (i in 1:length(individual)) {
  for (j in 1:datalength) {
    start_admin <- df[(df$year == 2007] & df$drug_admin == "x" & c(df$month == 2 | df$month == 3),1]  #capture date of start
    end_admin <- df[(df$year == 2008] & df$drug_admin == "x" & c(df$month == 2 | df$month == 4),1]    #capture date of end

    if(df[datalength,1] == individual(i) & df[datalength,2] >= start_admin & df[datalength,2] < end_admin) {
      df[datalength,6] <- start_admin #populate respective row if condition is met
      }
    }
  }

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

Above is the code that keeps failing..

Any help is highly appreciated....


______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________
-------------- next part --------------
A non-text attachment was scrubbed...
Name: XsOgd.png
Type: image/png
Size: 100935 bytes
Desc: XsOgd.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160702/a84e09ff/attachment.png>

From wewolski at gmail.com  Sat Jul  2 18:25:55 2016
From: wewolski at gmail.com (Witold E Wolski)
Date: Sat, 2 Jul 2016 18:25:55 +0200
Subject: [R] [FORGED] Splitting data.frame into a list of small
 data.frames given indices
In-Reply-To: <CAGxFJbRQr1pHhQ4p6cauUm2cnFzH5he0sR79znpTY50zZWTBRg@mail.gmail.com>
References: <CAAjnpdispjvZL_hBzR-vFLqdh-JPW1p67vUmN5AEar5j7cSY1g@mail.gmail.com>
	<545dedd7-2bdb-2fbd-651d-bc6bd43f8df6@auckland.ac.nz>
	<CAAjnpdiSMpPC+87LBXdu2fxRQx2dzWo9cDezJGa8zpwqdo5xuw@mail.gmail.com>
	<CAF8bMcZbSRQqOr_WUJyQ_3nNoUoBiUFhrhxa1tFZo+vfksC=3w@mail.gmail.com>
	<CAAjnpdjs0jZxBt3Z014=NATd+FVs=zU06jB9Z_jCVZgvg_5iqQ@mail.gmail.com>
	<CAGxFJbRQr1pHhQ4p6cauUm2cnFzH5he0sR79znpTY50zZWTBRg@mail.gmail.com>
Message-ID: <CAAjnpdieuKHVJOn_=GZY0x7+ag7VKi8EwiAg0uxMp=TP-n5E_g@mail.gmail.com>

Hi Bert,

My reply is inline:

On 1 July 2016 at 17:02, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Inline.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Fri, Jul 1, 2016 at 7:40 AM, Witold E Wolski <wewolski at gmail.com> wrote:
>> Hi William,
>>
>> I tested plyrs dlply function, and it seems to have  have an O(N*
>> log(R)) complexity (tested for R=N) so I do not know if N is the
>> number of rows or nr of categories.
>>
>> For the data.frame example with 2e5 rows and 2e5 categories it is
>> approx. 10 times faster than split. Still, it is 10 seconds on an
>> i7-5930K 3.5GHz Intel.
>>
>> It would be nice if the documentation would contain runtime
>> complexity information and the documentation of base package function
>> would point to function which should be used instead.
>
> It would, indeed -- but these things are very dependent on exact data
> context, the hardware in use, the OS, etc.  Moreover, how could base
> documentation possibly keep track of what all other packages are
> doing?! -- that seems unreasonable on the face of it.

Old methods need to stay for backward compatibility but why not
include new packages, which are a clear improvement over the old ones,
into the default packages and reference them?

 I know that
> from time to time R docs do mention base algorithmic complexity (e.g.
> ?sort and the data.table package, I believe), but generally it is
> preferable to omit such details, imho: access to a function should be
> through its relatively fixed API, while the underlying machinery may
> be subject to considerable change.

Just take a look how the C++ stl or the python documents functions for
working with containers. They do provide information about complexity
of operations, or algorithms. Such information is needed to make
reasonable decisions, including not to use a function or a container
type.

If there is a new implementation that changes complexity this
information needs to be updated.


Obviously, there are circumstances
> where something still could (and perhaps should) be said about
> efficiency -- and R docs often do say it -- but I think the level of
> detail you request is unrealistic and might often even mislead.
>
> Obviously, just my opinion, so contrary views welcome.
>
> Cheers,
> Bert
>
>
>>
>> Thanks
>>
>>
>>
>>
>> On 29 June 2016 at 16:13, William Dunlap <wdunlap at tibco.com> wrote:
>>> I won't go into why splitting data.frames (or factors) uses time
>>> proportional to the number of input rows times the number of
>>> levels in the splitting factor, but you will get much better mileage
>>> if you call split individually on each 'atomic' (numeric, character, ...)
>>> variable and use mapply on the resulting lists.
>>>
>>> The plyr and dplyr packages were developed to deal with this
>>> sort of problem.  Check them out.
>>>
>>>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>> On Wed, Jun 29, 2016 at 6:21 AM, Witold E Wolski <wewolski at gmail.com> wrote:
>>>>
>>>> Hi,
>>>>
>>>> Here is an complete example which shows the the complexity of split or
>>>> by is O(n^2)
>>>>
>>>> nrows <- c(1e3,5e3, 1e4 ,5e4, 1e5 ,2e5)
>>>> res<-list()
>>>>
>>>> for(i in nrows){
>>>>   dum <- data.frame(x = runif(i,1,1000), y=runif(i,1,1000))
>>>>   res[[length(res)+1]]<-(system.time(x<- split(dum, 1:nrow(dum))))
>>>> }
>>>> res <- do.call("rbind",res)
>>>> plot(nrows^2, res[,"elapsed"])
>>>>
>>>> And I can't see a reason why this has to be so slow.
>>>>
>>>>
>>>> cheers
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> On 29 June 2016 at 12:00, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>>>> > On 29/06/16 21:16, Witold E Wolski wrote:
>>>> >>
>>>> >> It's the inverse problem to merging a list of data.frames into a large
>>>> >> data.frame just discussed in the "performance of do.call("rbind")"
>>>> >> thread
>>>> >>
>>>> >> I would like to split a data.frame into a list of data.frames
>>>> >> according to first column.
>>>> >> This SEEMS to be easily possible with the function base::by. However,
>>>> >> as soon as the data.frame has a few million rows this function CAN NOT
>>>> >> BE USED (except you have A PLENTY OF TIME).
>>>> >>
>>>> >> for 'by' runtime ~ nrow^2, or formally O(n^2)  (see benchmark below).
>>>> >>
>>>> >> So basically I am looking for a similar function with better
>>>> >> complexity.
>>>> >>
>>>> >>
>>>> >>  > nrows <- c(1e5,1e6,2e6,3e6,5e6)
>>>> >>>
>>>> >>> timing <- list()
>>>> >>> for(i in nrows){
>>>> >>
>>>> >> + dum <- peaks[1:i,]
>>>> >> + timing[[length(timing)+1]] <- system.time(x<- by(dum[,2:3],
>>>> >> INDICES=list(dum[,1]), FUN=function(x){x}, simplify = FALSE))
>>>> >> + }
>>>> >>>
>>>> >>> names(timing)<- nrows
>>>> >>> timing
>>>> >>
>>>> >> $`1e+05`
>>>> >>    user  system elapsed
>>>> >>    0.05    0.00    0.05
>>>> >>
>>>> >> $`1e+06`
>>>> >>    user  system elapsed
>>>> >>    1.48    2.98    4.46
>>>> >>
>>>> >> $`2e+06`
>>>> >>    user  system elapsed
>>>> >>    7.25   11.39   18.65
>>>> >>
>>>> >> $`3e+06`
>>>> >>    user  system elapsed
>>>> >>   16.15   25.81   41.99
>>>> >>
>>>> >> $`5e+06`
>>>> >>    user  system elapsed
>>>> >>   43.22   74.72  118.09
>>>> >
>>>> >
>>>> > I'm not sure that I follow what you're doing, and your example is not
>>>> > reproducible, since we have no idea what "peaks" is, but on a toy
>>>> > example
>>>> > with 5e6 rows in the data frame I got a timing result of
>>>> >
>>>> >    user  system elapsed
>>>> >   0.379 0.025 0.406
>>>> >
>>>> > when I applied split().  Is this adequately fast? Seems to me that if
>>>> > you
>>>> > want to split something, split() would be a good place to start.
>>>> >
>>>> > cheers,
>>>> >
>>>> > Rolf Turner
>>>> >
>>>> > --
>>>> > Technical Editor ANZJS
>>>> > Department of Statistics
>>>> > University of Auckland
>>>> > Phone: +64-9-373-7599 ext. 88276
>>>>
>>>>
>>>>
>>>> --
>>>> Witold Eryk Wolski
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>>
>>
>> --
>> Witold Eryk Wolski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
Witold Eryk Wolski


From jdnewmil at dcn.davis.ca.us  Sat Jul  2 18:37:08 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 02 Jul 2016 09:37:08 -0700
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
	| For a Large Dataset
In-Reply-To: <1A5B5702-2764-47EE-B6A0-CFE590064198@kemri-wellcome.org>
References: <1A5B5702-2764-47EE-B6A0-CFE590064198@kemri-wellcome.org>
Message-ID: <9CC47B74-F7D5-445A-911E-643539593F89@dcn.davis.ca.us>

I can understand you not wanting to supply your actual data online, but only you know what your data looks like so only you can create a simulated data set that we could show you how to work with. 
-- 
Sent from my phone. Please excuse my brevity.

On July 2, 2016 2:57:39 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>I have a drug-trial study dataset (attached image).
>
>Since its a large and complex dataset (at least to me) and I hope to be
>as clear as possible with my question.
>The dataset is from a study where individuals are given drugs and
>followed up over a period spanning two consecutive years. Individuals
>do not start treatment on the same day and once they start, the
>variable "drug-admin" is marked "x" as well as the time they stop
>treatment in the following year.
>There exists another variable, "study_id", that I hope to populate as
>can be seen in the dataset, with the following conditions:
>
>For every individual
>?    if the individual has entries that show they received drugs both
>on the start and end date (marked with the "x")
>?    if the start of drug administration falls in month == 2 | 3 and
>end of administration falls in month == 2 | 4
>?    then, using the date that marks the start of drug administration,
>populate the variable _"study_id"_ in all the rows that fall within the
>timeframe that the individual was given drugs but excluding the end of
>drug administration.
>I have tried my level best and while I have explored several examples
>online, I haven't managed to solve this. The dataset contains close to
>6000 individuals spanning 10 years and my best bet was to use a loop
>which keeps crushing R after running for close to 30min. I have also
>read that dplyr may do the job but my attempts have been in vain.
>
>sample code
>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>individual <- unique (df$ID)  #vector of individuals
>datalength <- dim(df)[1]      #number of rows in dataframe
>
>for (i in 1:length(individual)) {
>  for (j in 1:datalength) {
>start_admin <- df[(df$year == 2007] & df$drug_admin == "x" & c(df$month
>== 2 | df$month == 3),1]  #capture date of start
>end_admin <- df[(df$year == 2008] & df$drug_admin == "x" & c(df$month
>== 2 | df$month == 4),1]    #capture date of end
>
>if(df[datalength,1] == individual(i) & df[datalength,2] >= start_admin
>& df[datalength,2] < end_admin) {
>df[datalength,6] <- start_admin #populate respective row if condition
>is met
>      }
>    }
>  }
>
>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
>Above is the code that keeps failing..
>
>Any help is highly appreciated....
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________
>
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bob at rudis.net  Sat Jul  2 23:12:21 2016
From: bob at rudis.net (boB Rudis)
Date: Sat, 2 Jul 2016 17:12:21 -0400
Subject: [R] Can R read Word fonts and comments?
In-Reply-To: <5FFC2C2F-82C2-4773-90CB-0C32DF869086@comcast.net>
References: <CABcx46D0gNOHEau_iEd_F-hMM1MA=J0r41sLwoFY8SMgE47nZw@mail.gmail.com>
	<CAGxFJbSSnMK7_cCfAMXVMT1fM3zWUQsjmNwfgtb_fh5XzRRdCQ@mail.gmail.com>
	<CABcx46A5O2Oc5XMdBSM=uHHQJdbvMp6U=8AfcOYpx5_Jr3CrpA@mail.gmail.com>
	<CAGxFJbS0s0g88v7aN9MJQBjEVkhB+9vUhS1WOJi9+UAAP2H9Uw@mail.gmail.com>
	<5FFC2C2F-82C2-4773-90CB-0C32DF869086@comcast.net>
Message-ID: <CAJ4QxaPUdgAupbCKzYtR++NQhEmre+CA1jjkESdPx9d8EGo8fg@mail.gmail.com>

I just added `docx_extract_all_cmnts()` (and a cpl other
comments-related things) to the dev version of `docxtractr`
(https://github.com/hrbrmstr/docxtractr). You can use
`devtools::install_github("hrbrmstr/docxtractr")` to install it.
There's an example in the help for that function.

Give it a go and file detailed issues for other functionality you need.

On Fri, Jul 1, 2016 at 11:14 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> It?s my understanding that docx and xlsx files are zipped containers that have their data in XML files. You should try unzipping one and examining it with a viewer. You may then be able to use pkg:XML.
>
> ?
> David.
>
>> On Jul 1, 2016, at 3:13 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> No, sorry -- all I would do is search.
>>
>> -- Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Fri, Jul 1, 2016 at 2:33 PM, John <miaojpm at gmail.com> wrote:
>>> Yes, I have done some search (e.g., tm, markdown, etc), but I can't find
>>> this function.
>>> If you know any package that works for this purpose, that would be quite
>>> helpful.
>>> Thanks,
>>>
>>> John
>>>
>>> 2016-06-28 16:50 GMT-07:00 Bert Gunter <bgunter.4567 at gmail.com>:
>>>>
>>>> Did you try searching before posting here? -- e.g. a web search or on
>>>> rseek.org ?
>>>>
>>>> Cheers,
>>>> Bert
>>>>
>>>>
>>>> Bert Gunter
>>>>
>>>> "The trouble with having an open mind is that people keep coming along
>>>> and sticking things into it."
>>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>>
>>>>
>>>> On Tue, Jun 28, 2016 at 3:53 PM, John <miaojpm at gmail.com> wrote:
>>>>> Hi,
>>>>>
>>>>>   From time to time I highlight the word documents with red/blue color
>>>>> or
>>>>> italic/bold fonts, and I also add comments to a file. Is there a
>>>>> package/function to let R extract the italic/bold blue/red words and
>>>>> comments from a docx/doc file?
>>>>>
>>>>>   I am aware that there are a few packages reading Word, but don't know
>>>>> which one is able to do it.
>>>>>
>>>>>   Thanks,
>>>>>
>>>>> John
>>>>>
>>>>>        [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From KWamae at kemri-wellcome.org  Sat Jul  2 18:40:11 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sat, 2 Jul 2016 16:40:11 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
Message-ID: <3AD6D85BB205904CB78C40CE0077873C94A858CA@KEKLF-XMB01.kwtrp.org>

Hi Jennifer, i attached a file with a snapshot of the dataset..it must have failed to attach, shall I send another?



K. Kariuki

------ Original message ------
From: Jeff Newmiller
Date: 02/07/2016 19:37
To: Kevin Wamae;R | Help Nabble;
Subject:Re: [R] R - Populate Another Variable Based on Multiple Conditions | For a Large Dataset

I can understand you not wanting to supply your actual data online, but only you know what your data looks like so only you can create a simulated data set that we could show you how to work with.
--
Sent from my phone. Please excuse my brevity.

On July 2, 2016 2:57:39 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>I have a drug-trial study dataset (attached image).
>
>Since its a large and complex dataset (at least to me) and I hope to be
>as clear as possible with my question.
>The dataset is from a study where individuals are given drugs and
>followed up over a period spanning two consecutive years. Individuals
>do not start treatment on the same day and once they start, the
>variable "drug-admin" is marked "x" as well as the time they stop
>treatment in the following year.
>There exists another variable, "study_id", that I hope to populate as
>can be seen in the dataset, with the following conditions:
>
>For every individual
>?    if the individual has entries that show they received drugs both
>on the start and end date (marked with the "x")
>?    if the start of drug administration falls in month == 2 | 3 and
>end of administration falls in month == 2 | 4
>?    then, using the date that marks the start of drug administration,
>populate the variable _"study_id"_ in all the rows that fall within the
>timeframe that the individual was given drugs but excluding the end of
>drug administration.
>I have tried my level best and while I have explored several examples
>online, I haven't managed to solve this. The dataset contains close to
>6000 individuals spanning 10 years and my best bet was to use a loop
>which keeps crushing R after running for close to 30min. I have also
>read that dplyr may do the job but my attempts have been in vain.
>
>sample code
>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>individual <- unique (df$ID)  #vector of individuals
>datalength <- dim(df)[1]      #number of rows in dataframe
>
>for (i in 1:length(individual)) {
>  for (j in 1:datalength) {
>start_admin <- df[(df$year == 2007] & df$drug_admin == "x" & c(df$month
>== 2 | df$month == 3),1]  #capture date of start
>end_admin <- df[(df$year == 2008] & df$drug_admin == "x" & c(df$month
>== 2 | df$month == 4),1]    #capture date of end
>
>if(df[datalength,1] == individual(i) & df[datalength,2] >= start_admin
>& df[datalength,2] < end_admin) {
>df[datalength,6] <- start_admin #populate respective row if condition
>is met
>      }
>    }
>  }
>
>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
>Above is the code that keeps failing..
>
>Any help is highly appreciated....
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________
>
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

	[[alternative HTML version deleted]]


From KWamae at kemri-wellcome.org  Sun Jul  3 00:41:07 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sat, 2 Jul 2016 22:41:07 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <9CC47B74-F7D5-445A-911E-643539593F89@dcn.davis.ca.us>
References: <1A5B5702-2764-47EE-B6A0-CFE590064198@kemri-wellcome.org>
	<9CC47B74-F7D5-445A-911E-643539593F89@dcn.davis.ca.us>
Message-ID: <3E0119F6-A962-4210-97C0-515630A04EA9@kemri-wellcome.org>

Hi Jeff, sorry for referring to you as Jennifer earlier, accept my apologies.

I attached a sample dataset in the question, am afraid it must have failed to attach.

I have attached it again..


Regards
-------------------------------------------------------------------------------
Kevin Kariuki
 

On 7/2/16, 7:37 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:

I can understand you not wanting to supply your actual data online, but only you know what your data looks like so only you can create a simulated data set that we could show you how to work with. 
-- 
Sent from my phone. Please excuse my brevity.

On July 2, 2016 2:57:39 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>I have a drug-trial study dataset (attached image).
>
>Since its a large and complex dataset (at least to me) and I hope to be
>as clear as possible with my question.
>The dataset is from a study where individuals are given drugs and
>followed up over a period spanning two consecutive years. Individuals
>do not start treatment on the same day and once they start, the
>variable "drug-admin" is marked "x" as well as the time they stop
>treatment in the following year.
>There exists another variable, "study_id", that I hope to populate as
>can be seen in the dataset, with the following conditions:
>
>For every individual
>?    if the individual has entries that show they received drugs both
>on the start and end date (marked with the "x")
>?    if the start of drug administration falls in month == 2 | 3 and
>end of administration falls in month == 2 | 4
>?    then, using the date that marks the start of drug administration,
>populate the variable _"study_id"_ in all the rows that fall within the
>timeframe that the individual was given drugs but excluding the end of
>drug administration.
>I have tried my level best and while I have explored several examples
>online, I haven't managed to solve this. The dataset contains close to
>6000 individuals spanning 10 years and my best bet was to use a loop
>which keeps crushing R after running for close to 30min. I have also
>read that dplyr may do the job but my attempts have been in vain.
>
>sample code
>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>individual <- unique (df$ID)  #vector of individuals
>datalength <- dim(df)[1]      #number of rows in dataframe
>
>for (i in 1:length(individual)) {
>  for (j in 1:datalength) {
>start_admin <- df[(df$year == 2007] & df$drug_admin == "x" & c(df$month
>== 2 | df$month == 3),1]  #capture date of start
>end_admin <- df[(df$year == 2008] & df$drug_admin == "x" & c(df$month
>== 2 | df$month == 4),1]    #capture date of end
>
>if(df[datalength,1] == individual(i) & df[datalength,2] >= start_admin
>& df[datalength,2] < end_admin) {
>df[datalength,6] <- start_admin #populate respective row if condition
>is met
>      }
>    }
>  }
>
>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>
>Above is the code that keeps failing..
>
>Any help is highly appreciated....
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________
>
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.




______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________
-------------- next part --------------
A non-text attachment was scrubbed...
Name: XsOgd.png
Type: image/png
Size: 100935 bytes
Desc: XsOgd.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160702/c493765e/attachment.png>

From hcatbr at yahoo.co.in  Sun Jul  3 00:43:13 2016
From: hcatbr at yahoo.co.in (Hemant Chowdhary)
Date: Sat, 2 Jul 2016 22:43:13 +0000 (UTC)
Subject: [R] Extracting matrix from netCDF file using ncdf4 package
References: <1495675446.736940.1467499393929.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>

 I am working with a 3-dimensional netCDF file having dimensions of X=100, Y=200, T=365. 
My objective is to extract time vectors of a few specific grids that may not be contiguous on X and/or Y. 

For example, I want to extract a 5x365 matrix where 5 rows are each vectors of length 365 of 5 specific X,Y combinations. 

For this, I am currently using the following 

reqX = c(35,35,40,65,95); 
reqY = c(2,5,10,112,120,120); 
nD = length(reqX) 
for(i in 1:nD){ 
idX = ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i]), count=c(1,1)) 
if(i==1){dX = idX} else {dX = rbind(dX,idX)} 
} 

Is there more elegant/faster way other than to using a For Loop like this? It seems very slow when I may have to get much larger matrix where nD can be more than 1000. 

Thank you HC

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sun Jul  3 01:00:26 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 2 Jul 2016 16:00:26 -0700
Subject: [R] Extracting matrix from netCDF file using ncdf4 package
In-Reply-To: <1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>
References: <1495675446.736940.1467499393929.JavaMail.yahoo.ref@mail.yahoo.com>
	<1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAGxFJbQJa91CvFGTn4FZeskhVfXs+LLiBsfaSQrihSq8uRtrSQ@mail.gmail.com>

I know nothing about netCDF files, but if you can download the file
and make it an array, extraction via indexing takes no time at all:

> ex <-array(rnorm(2*1e4*365, mean = 10), dim = c(100,200,365))

> system.time(test <-ex[35,2,])
   user  system elapsed
      0       0       0

> length(test)
[1] 365


If this can't be done, sorry for the noise.


Cheers,

Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Jul 2, 2016 at 3:43 PM, Hemant Chowdhary via R-help
<r-help at r-project.org> wrote:
>  I am working with a 3-dimensional netCDF file having dimensions of X=100, Y=200, T=365.
> My objective is to extract time vectors of a few specific grids that may not be contiguous on X and/or Y.
>
> For example, I want to extract a 5x365 matrix where 5 rows are each vectors of length 365 of 5 specific X,Y combinations.
>
> For this, I am currently using the following
>
> reqX = c(35,35,40,65,95);
> reqY = c(2,5,10,112,120,120);
> nD = length(reqX)
> for(i in 1:nD){
> idX = ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i]), count=c(1,1))
> if(i==1){dX = idX} else {dX = rbind(dX,idX)}
> }
>
> Is there more elegant/faster way other than to using a For Loop like this? It seems very slow when I may have to get much larger matrix where nD can be more than 1000.
>
> Thank you HC
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From roy.mendelssohn at noaa.gov  Sun Jul  3 01:26:37 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Sat, 2 Jul 2016 16:26:37 -0700
Subject: [R] Extracting matrix from netCDF file using ncdf4 package
In-Reply-To: <1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>
References: <1495675446.736940.1467499393929.JavaMail.yahoo.ref@mail.yahoo.com>
	<1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <259D6135-AA2C-4C0D-A410-54F43F53614F@noaa.gov>

Sending this to Hemant a second time as i forgot to reply to list.

Hi Hemant:

Well technically the code you give below shouldn?t work, because ?start? and ?count? are suppose to be of the same dimensions as the variables.  I guess Pierce?s code must be very forgiving if that is working.  One thing you can do to speed things up is pre-allocate the array you want to create, say

> dX <- array(NA_real_, dim=c(5,365))


and then have the ncvar_get call write directly to the array:

> dX[i,] <- ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i],1), count=c(1,1,-1)) 

The second thing you can do, is to use ?lapply? instead of the ?for? loop, but I don?t know how much faster that will make your code.  The fastest however, if you have the memory, is to just read the array into memory:

> dX <-  ncvar_get(nc=myNC, varid=?myVar?)


and then use R?s subsetting abilities. You can do fancier subsetting of arrays in memory than you can to arrays on disk.

HTH,

-Roy


> On Jul 2, 2016, at 3:43 PM, Hemant Chowdhary via R-help <r-help at r-project.org> wrote:
> 
> I am working with a 3-dimensional netCDF file having dimensions of X=100, Y=200, T=365. 
> My objective is to extract time vectors of a few specific grids that may not be contiguous on X and/or Y. 
> 
> For example, I want to extract a 5x365 matrix where 5 rows are each vectors of length 365 of 5 specific X,Y combinations. 
> 
> For this, I am currently using the following 
> 
> reqX = c(35,35,40,65,95); 
> reqY = c(2,5,10,112,120,120); 
> nD = length(reqX) 
> for(i in 1:nD){ 
> idX = ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i]), count=c(1,1)) 
> if(i==1){dX = idX} else {dX = rbind(dX,idX)} 
> } 
> 
> Is there more elegant/faster way other than to using a For Loop like this? It seems very slow when I may have to get much larger matrix where nD can be more than 1000. 
> 
> Thank you HC
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

> On Jul 2, 2016, at 3:43 PM, Hemant Chowdhary via R-help <r-help at r-project.org> wrote:
> 
> I am working with a 3-dimensional netCDF file having dimensions of X=100, Y=200, T=365. 
> My objective is to extract time vectors of a few specific grids that may not be contiguous on X and/or Y. 
> 
> For example, I want to extract a 5x365 matrix where 5 rows are each vectors of length 365 of 5 specific X,Y combinations. 
> 
> For this, I am currently using the following 
> 
> reqX = c(35,35,40,65,95); 
> reqY = c(2,5,10,112,120,120); 
> nD = length(reqX) 
> for(i in 1:nD){ 
> idX = ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i]), count=c(1,1)) 
> if(i==1){dX = idX} else {dX = rbind(dX,idX)} 
> } 
> 
> Is there more elegant/faster way other than to using a For Loop like this? It seems very slow when I may have to get much larger matrix where nD can be more than 1000. 
> 
> Thank you HC
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From jdnewmil at dcn.davis.ca.us  Sun Jul  3 07:42:52 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 02 Jul 2016 22:42:52 -0700
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
	| For a Large Dataset
In-Reply-To: <3E0119F6-A962-4210-97C0-515630A04EA9@kemri-wellcome.org>
References: <1A5B5702-2764-47EE-B6A0-CFE590064198@kemri-wellcome.org>
	<9CC47B74-F7D5-445A-911E-643539593F89@dcn.davis.ca.us>
	<3E0119F6-A962-4210-97C0-515630A04EA9@kemri-wellcome.org>
Message-ID: <D22A26DE-B66F-455C-81D1-0304414547A9@dcn.davis.ca.us>

You are making this hard on yourself by not paying attention the Posting Guide listed in the footer of every email on this list. You would probably also find [1] helpful also. 

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
-- 
Sent from my phone. Please excuse my brevity.

On July 2, 2016 3:41:07 PM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>Hi Jeff, sorry for referring to you as Jennifer earlier, accept my
>apologies.
>
>I attached a sample dataset in the question, am afraid it must have
>failed to attach.
>
>I have attached it again..
>
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Kariuki
> 
>
>On 7/2/16, 7:37 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>
>I can understand you not wanting to supply your actual data online, but
>only you know what your data looks like so only you can create a
>simulated data set that we could show you how to work with. 
>-- 
>Sent from my phone. Please excuse my brevity.
>
>On July 2, 2016 2:57:39 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org>
>wrote:
>>I have a drug-trial study dataset (attached image).
>>
>>Since its a large and complex dataset (at least to me) and I hope to
>be
>>as clear as possible with my question.
>>The dataset is from a study where individuals are given drugs and
>>followed up over a period spanning two consecutive years. Individuals
>>do not start treatment on the same day and once they start, the
>>variable "drug-admin" is marked "x" as well as the time they stop
>>treatment in the following year.
>>There exists another variable, "study_id", that I hope to populate as
>>can be seen in the dataset, with the following conditions:
>>
>>For every individual
>>?    if the individual has entries that show they received drugs both
>>on the start and end date (marked with the "x")
>>?    if the start of drug administration falls in month == 2 | 3 and
>>end of administration falls in month == 2 | 4
>>?    then, using the date that marks the start of drug administration,
>>populate the variable _"study_id"_ in all the rows that fall within
>the
>>timeframe that the individual was given drugs but excluding the end of
>>drug administration.
>>I have tried my level best and while I have explored several examples
>>online, I haven't managed to solve this. The dataset contains close to
>>6000 individuals spanning 10 years and my best bet was to use a loop
>>which keeps crushing R after running for close to 30min. I have also
>>read that dplyr may do the job but my attempts have been in vain.
>>
>>sample code
>>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>individual <- unique (df$ID)  #vector of individuals
>>datalength <- dim(df)[1]      #number of rows in dataframe
>>
>>for (i in 1:length(individual)) {
>>  for (j in 1:datalength) {
>>start_admin <- df[(df$year == 2007] & df$drug_admin == "x" &
>c(df$month
>>== 2 | df$month == 3),1]  #capture date of start
>>end_admin <- df[(df$year == 2008] & df$drug_admin == "x" & c(df$month
>>== 2 | df$month == 4),1]    #capture date of end
>>
>>if(df[datalength,1] == individual(i) & df[datalength,2] >= start_admin
>>& df[datalength,2] < end_admin) {
>>df[datalength,6] <- start_admin #populate respective row if condition
>>is met
>>      }
>>    }
>>  }
>>
>>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>
>>Above is the code that keeps failing..
>>
>>Any help is highly appreciated....
>>
>>
>>______________________________________________________________________
>>
>>This e-mail contains information which is confidential. It is intended
>>only for the use of the named recipient. If you have received this
>>e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network.
>Although
>>the Programme has taken reasonable precautions to ensure no viruses
>are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>______________________________________________________________________
>>
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________


From drjimlemon at gmail.com  Sun Jul  3 11:03:30 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 3 Jul 2016 19:03:30 +1000
Subject: [R] lineplot.CI xaxis scale change in sciplot?
In-Reply-To: <B3AB066D464A8D48999C6553F24153F96772A636@hall.AD.UWS.EDU.AU>
References: <B3AB066D464A8D48999C6553F24153F96772A636@hall.AD.UWS.EDU.AU>
Message-ID: <CA+8X3fVTx_rNZj2y+-5gvv1yM+fQfCuyR8FE9O9VQQE7nfod2w@mail.gmail.com>

Hi Clemence,
I don't have sciplot installed, but the help page suggests that the
"xaxt" argument is available. This will prevent the x axis from being
displayed and you can then specify the x axis you want. Assume that
you want an x axis from 0 to 300 by 50:

axis(1,at=seq(0,300,by=50))

Jim


On Thu, Jun 30, 2016 at 12:04 PM, Clemence Henry
<C.Henry at westernsydney.edu.au> wrote:
> Hi,
>
> I am trying to change the values of the tick marks on the xaxis of the following multipanel plot (see relevant bits of script below) to increments of 50 or to a custom scale (ie. 50, 100, 150, 200, 300...).
> So far I tried using xaxp or xlim both in par() or lineplot.CI(), as well as axTicks and axisTicks but did not get it to work.
> Suggestions?
>
> #Plots average A/Ci for each day from ACi
> #Parameters of the panels
> par(mfcol=c(3,2), #row,col
>     mar=c(2,2,1,1), #inner margin (bottom, left, top, right)
>     oma=c(4,4,1,1), #outer margin (bottom, left, top, right)
>     omd=c(0.1,0.8,0.1,0.95), #outer dimensions, values {0-1}, (x1, x2, y1, y2)
>     xpd=NA)
>
> ...
>
>
> #PAR = 1000, Day2
> with(subset1000_2,
>      lineplot.CI(x.factor=Ci.average,
>                  response=Photo,
>                  group=Treatment,
>                  ylab=NA,
>                  xlab=NA,
>                  legend=FALSE,
>                  type="p",
>                  x.cont=TRUE, #continuous x axis (spacing proportional to values)
>                  ylim=c(1,45), #range y axis
>                  err.width=0.05,
>                  pch = c(16,16,16), #symbols shape
>                  col=c("gray84","black","gray48"),
>                  fun=
>      ))
> mtext("Day2, PAR=1000", side = 3, line= -1, adj=0, at=1, cex=0.6) #subtitle
>
> ....
>
> #legends
> mtext("Ci", side = 1, line= 1, outer = TRUE, cex=0.7) #x legend
> mtext("Photosynthetic rate", side = 2, line= 1, outer = TRUE, cex=0.7) #y legend
> Thank you kindly for your support.
>
> Clemence
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From KWamae at kemri-wellcome.org  Sun Jul  3 11:39:59 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 3 Jul 2016 09:39:59 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <D22A26DE-B66F-455C-81D1-0304414547A9@dcn.davis.ca.us>
References: <1A5B5702-2764-47EE-B6A0-CFE590064198@kemri-wellcome.org>
	<9CC47B74-F7D5-445A-911E-643539593F89@dcn.davis.ca.us>
	<3E0119F6-A962-4210-97C0-515630A04EA9@kemri-wellcome.org>
	<D22A26DE-B66F-455C-81D1-0304414547A9@dcn.davis.ca.us>
Message-ID: <596C0B0C-F3F8-4C43-A211-617279A9FAEE@kemri-wellcome.org>

Hi Jeff, pardon me, I was surely not making it easy. I hope this time I will ?

Attached is snippet of the dataset in csv format and below is the R.script I have managed so far.

-----------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------

drug_study <- read.csv("drug_study.csv", header = T); head(drug_study)
drug_study$date <- as.Date(drug_study$date, "%m/%d/%Y")
drug_study$study_id <- ""  #create new column

individual <- unique (drug_study$ID)  #vector of individuals
datalength <- dim(drug_study)[1]      #number of rows in dataframe

for (i in 1:length(individual)) {
  for (j in 1:datalength) {
    start_admin <- drug_study[c(drug_study$ID == individual[i] & drug_study$year == 2007 & drug_study$drug_admin == "Y" & drug_study$month == 5),2]  #capture date of start
    end_admin <- drug_study[(drug_study$ID == individual[i] & drug_study$year == 2008 & drug_study$drug_admin == "Y" & drug_study$month == 2),2]    #capture date of end

    if(drug_study[j,1] == individual[i] & drug_study[j,2] >= start_admin & drug_study[j,2] < end_admin) {
      drug_study[j,6] <- paste(start_admin) #populate respective row if condition is met
    } 
  }	
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For this dataset, there exists three individuals, J1/3, R1/3, R10/1.

The script works for the last two individuals but not J1/3 with the error below:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Error in if (drug_study[j, 1] == individual[i] & drug_study[j, 2] >= start_admin &  : 
  argument is of length zero
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

I figured it?s because this individuals start_admin and end_admin dates aren?t captured because the if-loop fails. There?s my first problem, there are thousands of individuals with varying
start_admin and end_admin dates and I need a script to capture these for every individual.

Secondly, the above script is taking almost an hour to run for the entire dataset, just for the individuals whose start_admin and end_admin dates can be captured by the if-loop.

I need help in coming up with a script that will tackle the problem taking into account the different start_admin and end_admin dates and be resourceful with regards to time.

Regards
-------------------------------------------------------------------------------
Kevin Kariuki

###############################################################################################################################################
###############################################################################################################################################

On 7/3/16, 8:42 AM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:

You are making this hard on yourself by not paying attention the Posting Guide listed in the footer of every email on this list. You would probably also find [1] helpful also. 

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
-- 
Sent from my phone. Please excuse my brevity.

On July 2, 2016 3:41:07 PM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>Hi Jeff, sorry for referring to you as Jennifer earlier, accept my
>apologies.
>
>I attached a sample dataset in the question, am afraid it must have
>failed to attach.
>
>I have attached it again..
>
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Kariuki
> 
>
>On 7/2/16, 7:37 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>
>I can understand you not wanting to supply your actual data online, but
>only you know what your data looks like so only you can create a
>simulated data set that we could show you how to work with. 
>-- 
>Sent from my phone. Please excuse my brevity.
>
>On July 2, 2016 2:57:39 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org>
>wrote:
>>I have a drug-trial study dataset (attached image).
>>
>>Since its a large and complex dataset (at least to me) and I hope to
>be
>>as clear as possible with my question.
>>The dataset is from a study where individuals are given drugs and
>>followed up over a period spanning two consecutive years. Individuals
>>do not start treatment on the same day and once they start, the
>>variable "drug-admin" is marked "x" as well as the time they stop
>>treatment in the following year.
>>There exists another variable, "study_id", that I hope to populate as
>>can be seen in the dataset, with the following conditions:
>>
>>For every individual
>>?    if the individual has entries that show they received drugs both
>>on the start and end date (marked with the "x")
>>?    if the start of drug administration falls in month == 2 | 3 and
>>end of administration falls in month == 2 | 4
>>?    then, using the date that marks the start of drug administration,
>>populate the variable _"study_id"_ in all the rows that fall within
>the
>>timeframe that the individual was given drugs but excluding the end of
>>drug administration.
>>I have tried my level best and while I have explored several examples
>>online, I haven't managed to solve this. The dataset contains close to
>>6000 individuals spanning 10 years and my best bet was to use a loop
>>which keeps crushing R after running for close to 30min. I have also
>>read that dplyr may do the job but my attempts have been in vain.
>>
>>sample code
>>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>individual <- unique (df$ID)  #vector of individuals
>>datalength <- dim(df)[1]      #number of rows in dataframe
>>
>>for (i in 1:length(individual)) {
>>  for (j in 1:datalength) {
>>start_admin <- df[(df$year == 2007] & df$drug_admin == "x" &
>c(df$month
>>== 2 | df$month == 3),1]  #capture date of start
>>end_admin <- df[(df$year == 2008] & df$drug_admin == "x" & c(df$month
>>== 2 | df$month == 4),1]    #capture date of end
>>
>>if(df[datalength,1] == individual(i) & df[datalength,2] >= start_admin
>>& df[datalength,2] < end_admin) {
>>df[datalength,6] <- start_admin #populate respective row if condition
>>is met
>>      }
>>    }
>>  }
>>
>>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>
>>Above is the code that keeps failing..
>>
>>Any help is highly appreciated....
>>
>>
>>______________________________________________________________________
>>
>>This e-mail contains information which is confidential. It is intended
>>only for the use of the named recipient. If you have received this
>>e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network.
>Although
>>the Programme has taken reasonable precautions to ensure no viruses
>are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>______________________________________________________________________
>>
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________




______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From drjimlemon at gmail.com  Sun Jul  3 13:01:56 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 3 Jul 2016 21:01:56 +1000
Subject: [R] trouble double looping to generate data for a meta-analysis
In-Reply-To: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>
References: <CAKXurT_1Ba=F3APPwPhViMY3nrNKbRvCKG0Cn8ubfuDrj_6c9Q@mail.gmail.com>
Message-ID: <CA+8X3fX_M1iSiYhLwdpHzRVX1a0uU7-=rCVO1b7fFXaGY6iTdw@mail.gmail.com>

Hi Marietta,
You may not be aware that the variable k is doing nothing in your
example except running the random variable generation 2 or 3 times for
each cycle of the outer loop as each successive run just overwrites
the one before. If you want to include all two or three lots of values
you will have to do something like this:

db<-list()
gen_sample<-function(n,k) {
 for(m in 1:length(n)) {
  for(j in 1:n[m]) {
   for(i in 1:k[m]) {
    dbindx<-i+(j-1)*k[m]+(m-1)*(n[m]+k[1]+k[2])
    db[[dbindx]]<-
     matrix(c(rnorm(n[m], 100, 15),
      rsnorm(n[m], 100, 15),
      rlogis(n[m], 100, 15)),ncol=3)
   }
  }
 }
 return(db)
}
gen_sample(c(10,15),c(2,3))

Jim


On Sat, Jul 2, 2016 at 3:28 AM, Marietta Suarez <marietta0423 at gmail.com> wrote:
> i'm trying to generate data for a meta analysis. 1- generate data following
> a normal distribution, 2- generate data following a skewed distribution, 3-
> generate data following a logistic distribution. i need to loop this
> because the # of studies in each meta will be either 10 or 15. k or total
> number of studies in the meta will be 5. i need to loop twice to repeat
> this process 10 times. database should be 3 columns (distributions) by 65
> rows x 10 reps
>
>
> here's my code, not sure what's not working:
> library(fGarch)
>
> #n reps =10
> rep=10
>
> #begin function here, need to vary n and k, when k=2 n=10, when k3 n=15
> fun=function(n, k){
>
>   #prepare to store data
>   data=matrix(0,nrow=10*k, ncol=3)
>   db=matrix(0,nrow=650, ncol=3)
>
>   for (j in 1:rep)
>   {
>     for (i in 1:k)
>     {
>       #generate data under normal, skewed, and logistic distributions here
>
>       data[,1]=rnorm(n, 100, 15)
>       data[,2]=rsnorm(n, 100, 15, 1)
>       data[,3]=rlogis(n, 100, 15)
>     }
>       [j]=db
>   }
> }
>
> save=fun(10,2)
>
> Please help!!!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From varinsacha at yahoo.fr  Sun Jul  3 13:47:33 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Sun, 3 Jul 2016 11:47:33 +0000 (UTC)
Subject: [R] BCa Bootstrapped regression coefficients from lmrob function
 not working
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>

Dear R-experts,

I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).

My R code here below is showing a warning message ([1] "All values of t are equal to 
22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?

Here is the reproducible example


Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),

QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),

competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),

innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))

library("robustbase")
newdata=na.omit(Dataset)
a=Dataset$PIBparHab
b=Dataset$QUALITESANSREDONDANCE
c=Dataset$competitivite
d=Dataset$innovation

fm.lmrob=lmrob(a~b+c+d,data=newdata)
fm.lmrob

boot.Lmrob=function(formula,data,indices) {
d=data[indices,]
fit=lmrob(formula,data=d)
return(coef(fit))
}

library(boot)
results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
boot.ci(results, type= "bca",index=2)


Any help would be highly appreciated,
S


From jrkrideau at inbox.com  Sun Jul  3 14:16:11 2016
From: jrkrideau at inbox.com (John Kane)
Date: Sun, 3 Jul 2016 04:16:11 -0800
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <596C0B0C-F3F8-4C43-A211-617279A9FAEE@kemri-wellcome.org>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
Message-ID: <2F8C3FB2309.00000089jrkrideau@inbox.com>

The data set did not show up. The R-help list tends to strip out most file types as a safety precaution.  Try renaming the file from xxx.csv to xxx.txt and it should come through alright.



John Kane
Kingston ON Canada


> -----Original Message-----
> From: kwamae at kemri-wellcome.org
> Sent: Sun, 3 Jul 2016 09:39:59 +0000
> To: jdnewmil at dcn.davis.ca.us, r-help at r-project.org
> Subject: Re: [R] R - Populate Another Variable Based on Multiple
> Conditions | For a Large Dataset
> 
> Hi Jeff, pardon me, I was surely not making it easy. I hope this time I
> will ?
> 
> Attached is snippet of the dataset in csv format and below is the
> R.script I have managed so far.
> 
> -----------------------------------------------------------------------------------------------------------------------------------------------
> -----------------------------------------------------------------------------------------------------------------------------------------------
> 
> drug_study <- read.csv("drug_study.csv", header = T); head(drug_study)
> drug_study$date <- as.Date(drug_study$date, "%m/%d/%Y")
> drug_study$study_id <- ""  #create new column
> 
> individual <- unique (drug_study$ID)  #vector of individuals
> datalength <- dim(drug_study)[1]      #number of rows in dataframe
> 
> for (i in 1:length(individual)) {
>   for (j in 1:datalength) {
>     start_admin <- drug_study[c(drug_study$ID == individual[i] &
> drug_study$year == 2007 & drug_study$drug_admin == "Y" & drug_study$month
> == 5),2]  #capture date of start
>     end_admin <- drug_study[(drug_study$ID == individual[i] &
> drug_study$year == 2008 & drug_study$drug_admin == "Y" & drug_study$month
> == 2),2]    #capture date of end
> 
>     if(drug_study[j,1] == individual[i] & drug_study[j,2] >= start_admin
> & drug_study[j,2] < end_admin) {
>       drug_study[j,6] <- paste(start_admin) #populate respective row if
> condition is met
>     }
>   }
> }
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> For this dataset, there exists three individuals, J1/3, R1/3, R10/1.
> 
> The script works for the last two individuals but not J1/3 with the error
> below:
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Error in if (drug_study[j, 1] == individual[i] & drug_study[j, 2] >=
> start_admin &  :
>   argument is of length zero
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> I figured it?s because this individuals start_admin and end_admin dates
> aren?t captured because the if-loop fails. There?s my first problem,
> there are thousands of individuals with varying
> start_admin and end_admin dates and I need a script to capture these for
> every individual.
> 
> Secondly, the above script is taking almost an hour to run for the entire
> dataset, just for the individuals whose start_admin and end_admin dates
> can be captured by the if-loop.
> 
> I need help in coming up with a script that will tackle the problem
> taking into account the different start_admin and end_admin dates and be
> resourceful with regards to time.
> 
> Regards
> -------------------------------------------------------------------------------
> Kevin Kariuki
> 
> ###############################################################################################################################################
> ###############################################################################################################################################
> 
> On 7/3/16, 8:42 AM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
> 
> You are making this hard on yourself by not paying attention the Posting
> Guide listed in the footer of every email on this list. You would
> probably also find [1] helpful also.
> 
> [1]
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> --
> Sent from my phone. Please excuse my brevity.
> 
> On July 2, 2016 3:41:07 PM PDT, Kevin Wamae <KWamae at kemri-wellcome.org>
> wrote:
> >Hi Jeff, sorry for referring to you as Jennifer earlier, accept my
> >apologies.
>> 
> >I attached a sample dataset in the question, am afraid it must have
> >failed to attach.
>> 
> >I have attached it again..
>> 
>> 
> >Regards
> >-------------------------------------------------------------------------------
> >Kevin Kariuki
>> 
>> 
> >On 7/2/16, 7:37 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>> 
> >I can understand you not wanting to supply your actual data online, but
> >only you know what your data looks like so only you can create a
> >simulated data set that we could show you how to work with.
> >--
> >Sent from my phone. Please excuse my brevity.
>> 
> >On July 2, 2016 2:57:39 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org>
> >wrote:
> >>I have a drug-trial study dataset (attached image).
>>> 
> >>Since its a large and complex dataset (at least to me) and I hope to
> >be
> >>as clear as possible with my question.
> >>The dataset is from a study where individuals are given drugs and
> >>followed up over a period spanning two consecutive years. Individuals
> >>do not start treatment on the same day and once they start, the
> >>variable "drug-admin" is marked "x" as well as the time they stop
> >>treatment in the following year.
> >>There exists another variable, "study_id", that I hope to populate as
> >>can be seen in the dataset, with the following conditions:
>>> 
> >>For every individual
> >>?    if the individual has entries that show they received drugs both
> >>on the start and end date (marked with the "x")
> >>?    if the start of drug administration falls in month == 2 | 3 and
> >>end of administration falls in month == 2 | 4
> >>?    then, using the date that marks the start of drug administration,
> >>populate the variable _"study_id"_ in all the rows that fall within
> >the
> >>timeframe that the individual was given drugs but excluding the end of
> >>drug administration.
> >>I have tried my level best and while I have explored several examples
> >>online, I haven't managed to solve this. The dataset contains close to
> >>6000 individuals spanning 10 years and my best bet was to use a loop
> >>which keeps crushing R after running for close to 30min. I have also
> >>read that dplyr may do the job but my attempts have been in vain.
>>> 
> >>sample code
> >>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
> >>individual <- unique (df$ID)  #vector of individuals
> >>datalength <- dim(df)[1]      #number of rows in dataframe
>>> 
> >>for (i in 1:length(individual)) {
>>>  for (j in 1:datalength) {
> >>start_admin <- df[(df$year == 2007] & df$drug_admin == "x" &
> >c(df$month
> >>== 2 | df$month == 3),1]  #capture date of start
> >>end_admin <- df[(df$year == 2008] & df$drug_admin == "x" & c(df$month
> >>== 2 | df$month == 4),1]    #capture date of end
>>> 
> >>if(df[datalength,1] == individual(i) & df[datalength,2] >= start_admin
> >>& df[datalength,2] < end_admin) {
> >>df[datalength,6] <- start_admin #populate respective row if condition
> >>is met
>>>      }
>>>    }
>>>  }
>>> 
> >>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>> 
> >>Above is the code that keeps failing..
>>> 
> >>Any help is highly appreciated....
>>> 
>>> 
> >>______________________________________________________________________
>>> 
> >>This e-mail contains information which is confidential. It is intended
> >>only for the use of the named recipient. If you have received this
> >>e-mail in error, please let us know by replying to the sender, and
> >>immediately delete it from your system.  Please note, that in these
> >>circumstances, the use, disclosure, distribution or copying of this
> >>information is strictly prohibited. KEMRI-Wellcome Trust Programme
> >>cannot accept any responsibility for the  accuracy or completeness of
> >>this message as it has been transmitted over a public network.
> >Although
> >>the Programme has taken reasonable precautions to ensure no viruses
> >are
> >>present in emails, it cannot accept responsibility for any loss or
> >>damage arising from the use of the email or attachments. Any views
> >>expressed in this message are those of the individual sender, except
> >>where the sender specifically states them to be the views of
> >>KEMRI-Wellcome Trust Programme.
> >>______________________________________________________________________
>>> 
>>> 
> >>------------------------------------------------------------------------
>>> 
> >>______________________________________________
> >>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> 
>> 
> >______________________________________________________________________
>> 
> >This e-mail contains information which is confidential. It is intended
> >only for the use of the named recipient. If you have received this
> >e-mail in error, please let us know by replying to the sender, and
> >immediately delete it from your system.  Please note, that in these
> >circumstances, the use, disclosure, distribution or copying of this
> >information is strictly prohibited. KEMRI-Wellcome Trust Programme
> >cannot accept any responsibility for the  accuracy or completeness of
> >this message as it has been transmitted over a public network. Although
> >the Programme has taken reasonable precautions to ensure no viruses are
> >present in emails, it cannot accept responsibility for any loss or
> >damage arising from the use of the email or attachments. Any views
> >expressed in this message are those of the individual sender, except
> >where the sender specifically states them to be the views of
> >KEMRI-Wellcome Trust Programme.
> >______________________________________________________________________
> 
> 
> 
> 
> ______________________________________________________________________
> 
> This e-mail contains information which is confidential. It is intended
> only for the use of the named recipient. If you have received this e-mail
> in error, please let us know by replying to the sender, and immediately
> delete it from your system.  Please note, that in these circumstances,
> the use, disclosure, distribution or copying of this information is
> strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any
> responsibility for the  accuracy or completeness of this message as it
> has been transmitted over a public network. Although the Programme has
> taken reasonable precautions to ensure no viruses are present in emails,
> it cannot accept responsibility for any loss or damage arising from the
> use of the email or attachments. Any views expressed in this message are
> those of the individual sender, except where the sender specifically
> states them to be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From KWamae at kemri-wellcome.org  Sun Jul  3 14:19:01 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 3 Jul 2016 12:19:01 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <2F8C3FB2309.00000089jrkrideau@inbox.com>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
Message-ID: <20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>

Hi John, attached is the file in txt. Kindly let me know if it fails again..

Regards
-------------------------------------------------------------------------------
Kevin Wame | Ph.D. Student (IDeAL)
KEMRI-Wellcome Trust Collaborative Research Programme
Centre for Geographic Medicine Research
P.O. Box 230-80108, Kilifi, Kenya
 

On 7/3/16, 3:16 PM, "John Kane" <jrkrideau at inbox.com> wrote:

The data set did not show up. The R-help list tends to strip out most file types as a safety precaution.  Try renaming the file from xxx.csv to xxx.txt and it should come through alright.



John Kane
Kingston ON Canada


> -----Original Message-----
> From: kwamae at kemri-wellcome.org
> Sent: Sun, 3 Jul 2016 09:39:59 +0000
> To: jdnewmil at dcn.davis.ca.us, r-help at r-project.org
> Subject: Re: [R] R - Populate Another Variable Based on Multiple
> Conditions | For a Large Dataset
> 
> Hi Jeff, pardon me, I was surely not making it easy. I hope this time I
> will ?
> 
> Attached is snippet of the dataset in csv format and below is the
> R.script I have managed so far.
> 
> -----------------------------------------------------------------------------------------------------------------------------------------------
> -----------------------------------------------------------------------------------------------------------------------------------------------
> 
> drug_study <- read.csv("drug_study.csv", header = T); head(drug_study)
> drug_study$date <- as.Date(drug_study$date, "%m/%d/%Y")
> drug_study$study_id <- ""  #create new column
> 
> individual <- unique (drug_study$ID)  #vector of individuals
> datalength <- dim(drug_study)[1]      #number of rows in dataframe
> 
> for (i in 1:length(individual)) {
>   for (j in 1:datalength) {
>     start_admin <- drug_study[c(drug_study$ID == individual[i] &
> drug_study$year == 2007 & drug_study$drug_admin == "Y" & drug_study$month
> == 5),2]  #capture date of start
>     end_admin <- drug_study[(drug_study$ID == individual[i] &
> drug_study$year == 2008 & drug_study$drug_admin == "Y" & drug_study$month
> == 2),2]    #capture date of end
> 
>     if(drug_study[j,1] == individual[i] & drug_study[j,2] >= start_admin
> & drug_study[j,2] < end_admin) {
>       drug_study[j,6] <- paste(start_admin) #populate respective row if
> condition is met
>     }
>   }
> }
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> For this dataset, there exists three individuals, J1/3, R1/3, R10/1.
> 
> The script works for the last two individuals but not J1/3 with the error
> below:
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Error in if (drug_study[j, 1] == individual[i] & drug_study[j, 2] >=
> start_admin &  :
>   argument is of length zero
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> I figured it?s because this individuals start_admin and end_admin dates
> aren?t captured because the if-loop fails. There?s my first problem,
> there are thousands of individuals with varying
> start_admin and end_admin dates and I need a script to capture these for
> every individual.
> 
> Secondly, the above script is taking almost an hour to run for the entire
> dataset, just for the individuals whose start_admin and end_admin dates
> can be captured by the if-loop.
> 
> I need help in coming up with a script that will tackle the problem
> taking into account the different start_admin and end_admin dates and be
> resourceful with regards to time.
> 
> Regards
> -------------------------------------------------------------------------------
> Kevin Kariuki
> 
> ###############################################################################################################################################
> ###############################################################################################################################################
> 
> On 7/3/16, 8:42 AM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
> 
> You are making this hard on yourself by not paying attention the Posting
> Guide listed in the footer of every email on this list. You would
> probably also find [1] helpful also.
> 
> [1]
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> --
> Sent from my phone. Please excuse my brevity.
> 
> On July 2, 2016 3:41:07 PM PDT, Kevin Wamae <KWamae at kemri-wellcome.org>
> wrote:
> >Hi Jeff, sorry for referring to you as Jennifer earlier, accept my
> >apologies.
>> 
> >I attached a sample dataset in the question, am afraid it must have
> >failed to attach.
>> 
> >I have attached it again..
>> 
>> 
> >Regards
> >-------------------------------------------------------------------------------
> >Kevin Kariuki
>> 
>> 
> >On 7/2/16, 7:37 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>> 
> >I can understand you not wanting to supply your actual data online, but
> >only you know what your data looks like so only you can create a
> >simulated data set that we could show you how to work with.
> >--
> >Sent from my phone. Please excuse my brevity.
>> 
> >On July 2, 2016 2:57:39 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org>
> >wrote:
> >>I have a drug-trial study dataset (attached image).
>>> 
> >>Since its a large and complex dataset (at least to me) and I hope to
> >be
> >>as clear as possible with my question.
> >>The dataset is from a study where individuals are given drugs and
> >>followed up over a period spanning two consecutive years. Individuals
> >>do not start treatment on the same day and once they start, the
> >>variable "drug-admin" is marked "x" as well as the time they stop
> >>treatment in the following year.
> >>There exists another variable, "study_id", that I hope to populate as
> >>can be seen in the dataset, with the following conditions:
>>> 
> >>For every individual
> >>?    if the individual has entries that show they received drugs both
> >>on the start and end date (marked with the "x")
> >>?    if the start of drug administration falls in month == 2 | 3 and
> >>end of administration falls in month == 2 | 4
> >>?    then, using the date that marks the start of drug administration,
> >>populate the variable _"study_id"_ in all the rows that fall within
> >the
> >>timeframe that the individual was given drugs but excluding the end of
> >>drug administration.
> >>I have tried my level best and while I have explored several examples
> >>online, I haven't managed to solve this. The dataset contains close to
> >>6000 individuals spanning 10 years and my best bet was to use a loop
> >>which keeps crushing R after running for close to 30min. I have also
> >>read that dplyr may do the job but my attempts have been in vain.
>>> 
> >>sample code
> >>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
> >>individual <- unique (df$ID)  #vector of individuals
> >>datalength <- dim(df)[1]      #number of rows in dataframe
>>> 
> >>for (i in 1:length(individual)) {
>>>  for (j in 1:datalength) {
> >>start_admin <- df[(df$year == 2007] & df$drug_admin == "x" &
> >c(df$month
> >>== 2 | df$month == 3),1]  #capture date of start
> >>end_admin <- df[(df$year == 2008] & df$drug_admin == "x" & c(df$month
> >>== 2 | df$month == 4),1]    #capture date of end
>>> 
> >>if(df[datalength,1] == individual(i) & df[datalength,2] >= start_admin
> >>& df[datalength,2] < end_admin) {
> >>df[datalength,6] <- start_admin #populate respective row if condition
> >>is met
>>>      }
>>>    }
>>>  }
>>> 
> >>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>> 
> >>Above is the code that keeps failing..
>>> 
> >>Any help is highly appreciated....
>>> 
>>> 
> >>______________________________________________________________________
>>> 
> >>This e-mail contains information which is confidential. It is intended
> >>only for the use of the named recipient. If you have received this
> >>e-mail in error, please let us know by replying to the sender, and
> >>immediately delete it from your system.  Please note, that in these
> >>circumstances, the use, disclosure, distribution or copying of this
> >>information is strictly prohibited. KEMRI-Wellcome Trust Programme
> >>cannot accept any responsibility for the  accuracy or completeness of
> >>this message as it has been transmitted over a public network.
> >Although
> >>the Programme has taken reasonable precautions to ensure no viruses
> >are
> >>present in emails, it cannot accept responsibility for any loss or
> >>damage arising from the use of the email or attachments. Any views
> >>expressed in this message are those of the individual sender, except
> >>where the sender specifically states them to be the views of
> >>KEMRI-Wellcome Trust Programme.
> >>______________________________________________________________________
>>> 
>>> 
> >>------------------------------------------------------------------------
>>> 
> >>______________________________________________
> >>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> 
>> 
> >______________________________________________________________________
>> 
> >This e-mail contains information which is confidential. It is intended
> >only for the use of the named recipient. If you have received this
> >e-mail in error, please let us know by replying to the sender, and
> >immediately delete it from your system.  Please note, that in these
> >circumstances, the use, disclosure, distribution or copying of this
> >information is strictly prohibited. KEMRI-Wellcome Trust Programme
> >cannot accept any responsibility for the  accuracy or completeness of
> >this message as it has been transmitted over a public network. Although
> >the Programme has taken reasonable precautions to ensure no viruses are
> >present in emails, it cannot accept responsibility for any loss or
> >damage arising from the use of the email or attachments. Any views
> >expressed in this message are those of the individual sender, except
> >where the sender specifically states them to be the views of
> >KEMRI-Wellcome Trust Programme.
> >______________________________________________________________________
> 
> 
> 
> 
> ______________________________________________________________________
> 
> This e-mail contains information which is confidential. It is intended
> only for the use of the named recipient. If you have received this e-mail
> in error, please let us know by replying to the sender, and immediately
> delete it from your system.  Please note, that in these circumstances,
> the use, disclosure, distribution or copying of this information is
> strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any
> responsibility for the  accuracy or completeness of this message as it
> has been transmitted over a public network. Although the Programme has
> taken reasonable precautions to ensure no viruses are present in emails,
> it cannot accept responsibility for any loss or damage arising from the
> use of the email or attachments. Any views expressed in this message are
> those of the individual sender, except where the sender specifically
> states them to be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.
Check it out at http://mysecurelogon.com/password-manager





______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: drug_study.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20160703/098c5c80/attachment.txt>

From hcatbr at yahoo.co.in  Sun Jul  3 16:27:35 2016
From: hcatbr at yahoo.co.in (Hemant Chowdhary)
Date: Sun, 3 Jul 2016 14:27:35 +0000 (UTC)
Subject: [R] Extracting matrix from netCDF file using ncdf4 package
In-Reply-To: <259D6135-AA2C-4C0D-A410-54F43F53614F@noaa.gov>
References: <1495675446.736940.1467499393929.JavaMail.yahoo.ref@mail.yahoo.com>
	<1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>
	<259D6135-AA2C-4C0D-A410-54F43F53614F@noaa.gov>
Message-ID: <530540922.850388.1467556055507.JavaMail.yahoo@mail.yahoo.com>

Thank you both.
Yes, this is basically the issue of able to subset an array rather than extracting from the netCDF file. The dX = ncvar_get(nc=myNC, varid="myVar")command already results in the array. And one can subset that array using indices.
In turn the problem can be stated as follows:Let us say dX is a 3D array with dimensions 100x200x365. The objective is to extract five specific vectors of 365 each corresponding to reqX = c(35,35,40,65,95); and?reqY = c(2,5,10,112,120); 
dX2 = dX[reqX, reqY,]results again in an array of 5x5x365, i.e., corresponding to all 25 combinations of reqX and reqY. 
Somehow, I was expecting that there is a subsetting function that can result in a matrix of 5x365 directly. If there is none than one can extract one grid at a time and fill the pre-defined matrix as you have suggested.
Thank you againHC



 

    On Saturday, 2 July 2016 7:26 PM, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
 

 Sending this to Hemant a second time as i forgot to reply to list.

Hi Hemant:

Well technically the code you give below shouldn?t work, because ?start? and ?count? are suppose to be of the same dimensions as the variables.? I guess Pierce?s code must be very forgiving if that is working.? One thing you can do to speed things up is pre-allocate the array you want to create, say

> dX <- array(NA_real_, dim=c(5,365))


and then have the ncvar_get call write directly to the array:

> dX[i,] <- ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i],1), count=c(1,1,-1)) 

The second thing you can do, is to use ?lapply? instead of the ?for? loop, but I don?t know how much faster that will make your code.? The fastest however, if you have the memory, is to just read the array into memory:

> dX <-? ncvar_get(nc=myNC, varid=?myVar?)


and then use R?s subsetting abilities. You can do fancier subsetting of arrays in memory than you can to arrays on disk.

HTH,

-Roy


> On Jul 2, 2016, at 3:43 PM, Hemant Chowdhary via R-help <r-help at r-project.org> wrote:
> 
> I am working with a 3-dimensional netCDF file having dimensions of X=100, Y=200, T=365. 
> My objective is to extract time vectors of a few specific grids that may not be contiguous on X and/or Y. 
> 
> For example, I want to extract a 5x365 matrix where 5 rows are each vectors of length 365 of 5 specific X,Y combinations. 
> 
> For this, I am currently using the following 
> 
> reqX = c(35,35,40,65,95); 
> reqY = c(2,5,10,112,120,120); 
> nD = length(reqX) 
> for(i in 1:nD){ 
> idX = ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i]), count=c(1,1)) 
> if(i==1){dX = idX} else {dX = rbind(dX,idX)} 
> } 
> 
> Is there more elegant/faster way other than to using a For Loop like this? It seems very slow when I may have to get much larger matrix where nD can be more than 1000. 
> 
> Thank you HC
> 
> ??? [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

> On Jul 2, 2016, at 3:43 PM, Hemant Chowdhary via R-help <r-help at r-project.org> wrote:
> 
> I am working with a 3-dimensional netCDF file having dimensions of X=100, Y=200, T=365. 
> My objective is to extract time vectors of a few specific grids that may not be contiguous on X and/or Y. 
> 
> For example, I want to extract a 5x365 matrix where 5 rows are each vectors of length 365 of 5 specific X,Y combinations. 
> 
> For this, I am currently using the following 
> 
> reqX = c(35,35,40,65,95); 
> reqY = c(2,5,10,112,120,120); 
> nD = length(reqX) 
> for(i in 1:nD){ 
> idX = ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i]), count=c(1,1)) 
> if(i==1){dX = idX} else {dX = rbind(dX,idX)} 
> } 
> 
> Is there more elegant/faster way other than to using a For Loop like this? It seems very slow when I may have to get much larger matrix where nD can be more than 1000. 
> 
> Thank you HC
> 
> ??? [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


  
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sun Jul  3 17:38:27 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 3 Jul 2016 08:38:27 -0700
Subject: [R] Extracting matrix from netCDF file using ncdf4 package
In-Reply-To: <1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>
References: <1495675446.736940.1467499393929.JavaMail.yahoo.ref@mail.yahoo.com>
	<1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAGxFJbSYG6UMzqCYEstVbjHkOzAs1dAn=w35ZrSa-1c38M2CUA@mail.gmail.com>

Well, yes, ... but no: there is no need to pre-define the matrix.

The following is still a (interpreted) loop, but it is fast and short.

## ex is the downloaded array, here filled with random numbers

reqX = c(35,35,40,65,95)
reqY = c(2,5,10,112,120)

out <-sapply(seq_along(reqX), function(i)ex[reqX[i],reqY[i],] )

> dim(out)
[1] 365   5

You might find it useful to go through a (web) tutorial or two to
learn more about such R functionality.
Useful suggestions can be found here: https://www.rstudio.com/online-learning/#R

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Jul 2, 2016 at 3:43 PM, Hemant Chowdhary via R-help
<r-help at r-project.org> wrote:
>  I am working with a 3-dimensional netCDF file having dimensions of X=100, Y=200, T=365.
> My objective is to extract time vectors of a few specific grids that may not be contiguous on X and/or Y.
>
> For example, I want to extract a 5x365 matrix where 5 rows are each vectors of length 365 of 5 specific X,Y combinations.
>
> For this, I am currently using the following
>
> reqX = c(35,35,40,65,95);
> reqY = c(2,5,10,112,120,120);
> nD = length(reqX)
> for(i in 1:nD){
> idX = ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i]), count=c(1,1))
> if(i==1){dX = idX} else {dX = rbind(dX,idX)}
> }
>
> Is there more elegant/faster way other than to using a For Loop like this? It seems very slow when I may have to get much larger matrix where nD can be more than 1000.
>
> Thank you HC
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sun Jul  3 18:05:02 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 03 Jul 2016 09:05:02 -0700
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
	| For a Large Dataset
In-Reply-To: <20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
Message-ID: <07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>

Your goal of putting character representations of dates in certain rows of a column is hard to imagine a use for.  Your goal of identifying start and end dates seems reasonable enough. It can be accomplished using aggregate from base R (less external dependency) or summarise from dplyr (faster, simpler syntax):

result <- setNames( data.frame( aggregate( date~ID, data=drug_study, FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c( "ID", "start", "end" ) )

or

library( dplyr )
result <- (   drug_study
          %>% group_by( ID )
          %>% summarise( start=min( date ), end=max( date) )
           )

-- 
Sent from my phone. Please excuse my brevity.

On July 3, 2016 5:19:01 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>Hi John, attached is the file in txt. Kindly let me know if it fails
>again..
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Wame | Ph.D. Student (IDeAL)
>KEMRI-Wellcome Trust Collaborative Research Programme
>Centre for Geographic Medicine Research
>P.O. Box 230-80108, Kilifi, Kenya
> 
>
>On 7/3/16, 3:16 PM, "John Kane" <jrkrideau at inbox.com> wrote:
>
>The data set did not show up. The R-help list tends to strip out most
>file types as a safety precaution.  Try renaming the file from xxx.csv
>to xxx.txt and it should come through alright.
>
>
>
>John Kane
>Kingston ON Canada
>
>
>> -----Original Message-----
>> From: kwamae at kemri-wellcome.org
>> Sent: Sun, 3 Jul 2016 09:39:59 +0000
>> To: jdnewmil at dcn.davis.ca.us, r-help at r-project.org
>> Subject: Re: [R] R - Populate Another Variable Based on Multiple
>> Conditions | For a Large Dataset
>> 
>> Hi Jeff, pardon me, I was surely not making it easy. I hope this time
>I
>> will ?
>> 
>> Attached is snippet of the dataset in csv format and below is the
>> R.script I have managed so far.
>> 
>>
>-----------------------------------------------------------------------------------------------------------------------------------------------
>>
>-----------------------------------------------------------------------------------------------------------------------------------------------
>> 
>> drug_study <- read.csv("drug_study.csv", header = T);
>head(drug_study)
>> drug_study$date <- as.Date(drug_study$date, "%m/%d/%Y")
>> drug_study$study_id <- ""  #create new column
>> 
>> individual <- unique (drug_study$ID)  #vector of individuals
>> datalength <- dim(drug_study)[1]      #number of rows in dataframe
>> 
>> for (i in 1:length(individual)) {
>>   for (j in 1:datalength) {
>>     start_admin <- drug_study[c(drug_study$ID == individual[i] &
>> drug_study$year == 2007 & drug_study$drug_admin == "Y" &
>drug_study$month
>> == 5),2]  #capture date of start
>>     end_admin <- drug_study[(drug_study$ID == individual[i] &
>> drug_study$year == 2008 & drug_study$drug_admin == "Y" &
>drug_study$month
>> == 2),2]    #capture date of end
>> 
>>     if(drug_study[j,1] == individual[i] & drug_study[j,2] >=
>start_admin
>> & drug_study[j,2] < end_admin) {
>>       drug_study[j,6] <- paste(start_admin) #populate respective row
>if
>> condition is met
>>     }
>>   }
>> }
>>
>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>
>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> 
>> For this dataset, there exists three individuals, J1/3, R1/3, R10/1.
>> 
>> The script works for the last two individuals but not J1/3 with the
>error
>> below:
>> 
>>
>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>
>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> Error in if (drug_study[j, 1] == individual[i] & drug_study[j, 2] >=
>> start_admin &  :
>>   argument is of length zero
>>
>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>
>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> 
>> I figured it?s because this individuals start_admin and end_admin
>dates
>> aren?t captured because the if-loop fails. There?s my first problem,
>> there are thousands of individuals with varying
>> start_admin and end_admin dates and I need a script to capture these
>for
>> every individual.
>> 
>> Secondly, the above script is taking almost an hour to run for the
>entire
>> dataset, just for the individuals whose start_admin and end_admin
>dates
>> can be captured by the if-loop.
>> 
>> I need help in coming up with a script that will tackle the problem
>> taking into account the different start_admin and end_admin dates and
>be
>> resourceful with regards to time.
>> 
>> Regards
>>
>-------------------------------------------------------------------------------
>> Kevin Kariuki
>> 
>>
>###############################################################################################################################################
>>
>###############################################################################################################################################
>> 
>> On 7/3/16, 8:42 AM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>wrote:
>> 
>> You are making this hard on yourself by not paying attention the
>Posting
>> Guide listed in the footer of every email on this list. You would
>> probably also find [1] helpful also.
>> 
>> [1]
>>
>http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>> --
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 2, 2016 3:41:07 PM PDT, Kevin Wamae
><KWamae at kemri-wellcome.org>
>> wrote:
>> >Hi Jeff, sorry for referring to you as Jennifer earlier, accept my
>> >apologies.
>>> 
>> >I attached a sample dataset in the question, am afraid it must have
>> >failed to attach.
>>> 
>> >I have attached it again..
>>> 
>>> 
>> >Regards
>>
>>-------------------------------------------------------------------------------
>> >Kevin Kariuki
>>> 
>>> 
>> >On 7/2/16, 7:37 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>wrote:
>>> 
>> >I can understand you not wanting to supply your actual data online,
>but
>> >only you know what your data looks like so only you can create a
>> >simulated data set that we could show you how to work with.
>> >--
>> >Sent from my phone. Please excuse my brevity.
>>> 
>> >On July 2, 2016 2:57:39 AM PDT, Kevin Wamae
><KWamae at kemri-wellcome.org>
>> >wrote:
>> >>I have a drug-trial study dataset (attached image).
>>>> 
>> >>Since its a large and complex dataset (at least to me) and I hope
>to
>> >be
>> >>as clear as possible with my question.
>> >>The dataset is from a study where individuals are given drugs and
>> >>followed up over a period spanning two consecutive years.
>Individuals
>> >>do not start treatment on the same day and once they start, the
>> >>variable "drug-admin" is marked "x" as well as the time they stop
>> >>treatment in the following year.
>> >>There exists another variable, "study_id", that I hope to populate
>as
>> >>can be seen in the dataset, with the following conditions:
>>>> 
>> >>For every individual
>> >>?    if the individual has entries that show they received drugs
>both
>> >>on the start and end date (marked with the "x")
>> >>?    if the start of drug administration falls in month == 2 | 3
>and
>> >>end of administration falls in month == 2 | 4
>> >>?    then, using the date that marks the start of drug
>administration,
>> >>populate the variable _"study_id"_ in all the rows that fall within
>> >the
>> >>timeframe that the individual was given drugs but excluding the end
>of
>> >>drug administration.
>> >>I have tried my level best and while I have explored several
>examples
>> >>online, I haven't managed to solve this. The dataset contains close
>to
>> >>6000 individuals spanning 10 years and my best bet was to use a
>loop
>> >>which keeps crushing R after running for close to 30min. I have
>also
>> >>read that dplyr may do the job but my attempts have been in vain.
>>>> 
>> >>sample code
>>
>>>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>> >>individual <- unique (df$ID)  #vector of individuals
>> >>datalength <- dim(df)[1]      #number of rows in dataframe
>>>> 
>> >>for (i in 1:length(individual)) {
>>>>  for (j in 1:datalength) {
>> >>start_admin <- df[(df$year == 2007] & df$drug_admin == "x" &
>> >c(df$month
>> >>== 2 | df$month == 3),1]  #capture date of start
>> >>end_admin <- df[(df$year == 2008] & df$drug_admin == "x" &
>c(df$month
>> >>== 2 | df$month == 4),1]    #capture date of end
>>>> 
>> >>if(df[datalength,1] == individual(i) & df[datalength,2] >=
>start_admin
>> >>& df[datalength,2] < end_admin) {
>> >>df[datalength,6] <- start_admin #populate respective row if
>condition
>> >>is met
>>>>      }
>>>>    }
>>>>  }
>>>> 
>>
>>>-------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>>> 
>> >>Above is the code that keeps failing..
>>>> 
>> >>Any help is highly appreciated....
>>>> 
>>>> 
>>
>>>______________________________________________________________________
>>>> 
>> >>This e-mail contains information which is confidential. It is
>intended
>> >>only for the use of the named recipient. If you have received this
>> >>e-mail in error, please let us know by replying to the sender, and
>> >>immediately delete it from your system.  Please note, that in these
>> >>circumstances, the use, disclosure, distribution or copying of this
>> >>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>> >>cannot accept any responsibility for the  accuracy or completeness
>of
>> >>this message as it has been transmitted over a public network.
>> >Although
>> >>the Programme has taken reasonable precautions to ensure no viruses
>> >are
>> >>present in emails, it cannot accept responsibility for any loss or
>> >>damage arising from the use of the email or attachments. Any views
>> >>expressed in this message are those of the individual sender,
>except
>> >>where the sender specifically states them to be the views of
>> >>KEMRI-Wellcome Trust Programme.
>>
>>>______________________________________________________________________
>>>> 
>>>> 
>>
>>>------------------------------------------------------------------------
>>>> 
>> >>______________________________________________
>> >>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>https://stat.ethz.ch/mailman/listinfo/r-help
>> >>PLEASE do read the posting guide
>> >>http://www.R-project.org/posting-guide.html
>> >>and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> 
>>> 
>>> 
>>
>>______________________________________________________________________
>>> 
>> >This e-mail contains information which is confidential. It is
>intended
>> >only for the use of the named recipient. If you have received this
>> >e-mail in error, please let us know by replying to the sender, and
>> >immediately delete it from your system.  Please note, that in these
>> >circumstances, the use, disclosure, distribution or copying of this
>> >information is strictly prohibited. KEMRI-Wellcome Trust Programme
>> >cannot accept any responsibility for the  accuracy or completeness
>of
>> >this message as it has been transmitted over a public network.
>Although
>> >the Programme has taken reasonable precautions to ensure no viruses
>are
>> >present in emails, it cannot accept responsibility for any loss or
>> >damage arising from the use of the email or attachments. Any views
>> >expressed in this message are those of the individual sender, except
>> >where the sender specifically states them to be the views of
>> >KEMRI-Wellcome Trust Programme.
>>
>>______________________________________________________________________
>> 
>> 
>> 
>> 
>>
>______________________________________________________________________
>> 
>> This e-mail contains information which is confidential. It is
>intended
>> only for the use of the named recipient. If you have received this
>e-mail
>> in error, please let us know by replying to the sender, and
>immediately
>> delete it from your system.  Please note, that in these
>circumstances,
>> the use, disclosure, distribution or copying of this information is
>> strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any
>> responsibility for the  accuracy or completeness of this message as
>it
>> has been transmitted over a public network. Although the Programme
>has
>> taken reasonable precautions to ensure no viruses are present in
>emails,
>> it cannot accept responsibility for any loss or damage arising from
>the
>> use of the email or attachments. Any views expressed in this message
>are
>> those of the individual sender, except where the sender specifically
>> states them to be the views of KEMRI-Wellcome Trust Programme.
>>
>______________________________________________________________________
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>____________________________________________________________
>Can't remember your password? Do you need a strong and secure password?
>Use Password manager! It stores your passwords & protects your account.
>Check it out at http://mysecurelogon.com/password-manager
>
>
>
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________


From pdalgd at gmail.com  Sun Jul  3 18:19:21 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 3 Jul 2016 18:19:21 +0200
Subject: [R] BCa Bootstrapped regression coefficients from lmrob
	function not working
In-Reply-To: <159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
	<159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>


> On 03 Jul 2016, at 13:47 , varin sacha via R-help <r-help at r-project.org> wrote:
> 
> Dear R-experts,
> 
> I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).
> 
> My R code here below is showing a warning message ([1] "All values of t are equal to 
> 22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?

You need to review your code. You calculate a,b,c,d in the global environment and create newdata as a subset of Dataset, then use a,b,c,d in the formula, but no such variables are in newdata. AFAICT, all your bootstrap fits use the _same_ global values for a,b,c,d hence give the same result 1000 times...

-pd


> 
> Here is the reproducible example
> 
> 
> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
> 
> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
> 
> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
> 
> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
> 
> library("robustbase")
> newdata=na.omit(Dataset)
> a=Dataset$PIBparHab
> b=Dataset$QUALITESANSREDONDANCE
> c=Dataset$competitivite
> d=Dataset$innovation
> 
> fm.lmrob=lmrob(a~b+c+d,data=newdata)
> fm.lmrob
> 
> boot.Lmrob=function(formula,data,indices) {
> d=data[indices,]
> fit=lmrob(formula,data=d)
> return(coef(fit))
> }
> 
> library(boot)
> results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
> boot.ci(results, type= "bca",index=2)
> 
> 
> Any help would be highly appreciated,
> S
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From KWamae at kemri-wellcome.org  Sun Jul  3 19:24:51 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 3 Jul 2016 17:24:51 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
Message-ID: <F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>

HI Jeff, it?s been an uphill task working with the dataset and I am not the first to complain. Nonetheless, data-cleaning is ongoing and since I cannot wait for that to get done, I decided to make the most of what the dataset looks like at this time. It appears the process may take a while.

Thanks for the script. From the output, I noticed that ?result? contains the first and last date for each of the individuals and not taking into account the variable ?drug-admin?. 

ID	    start		end
J1/3	    1/5/09	12/25/10
R1/3	    1/4/07	12/15/08
R10/1	    1/4/07	3/5/12

My aim is to pick the date, for example in 2007, where drug-admin == ?Y? as my start and the date in the subsequent year (2008 in this case) where drug-admin == ?Y? as my end. Then, I should populate the variable ?study_id? with ?start? up to the entry just above the one whose date matches ?end?, as the output below shows (I hope its structure is maintained as I have copied it from R-Studio). The goal for now is to then get difference in days between ?date? and ?study_id? and still get to keep that column for ?study_id? as I might use it later.

From the output, it can be seen that for this individual, the dates run from 2007 to 2008. However, for some individuals, the dates run from 2008-2009, 2009-2010 and so on. Therefore, I need to make the script deal with all the years as the dates range from 2001-2016

ID	date	drug_admin	year	month	study_id
R1/3	5/11/07	Y	2007	5	5/11/07
R1/3	5/16/07		2007	5	5/11/07
R1/3	5/22/07		2007	5	5/11/07
R1/3	5/28/07		2007	5	5/11/07
R1/3	6/5/07			2007	6	5/11/07
R1/3	6/11/07		2007	6	5/11/07
R1/3	6/18/07		2007	6	5/11/07
R1/3	6/25/07		2007	6	5/11/07
R1/3	7/2/07			2007	7	5/11/07
R1/3	7/16/07		2007	7	5/11/07
R1/3	7/29/07		2007	7	5/11/07
R1/3	8/2/07			2007	8	5/11/07
R1/3	8/7/07			2007	8	5/11/07
R1/3	8/13/07		2007	8	5/11/07
R1/3	9/18/07		2007	9	5/11/07
R1/3	9/24/07		2007	9	5/11/07
R1/3	10/6/07		2007	10	5/11/07
R1/3	10/8/07		2007	10	5/11/07
R1/3	10/15/07		2007	10	5/11/07
R1/3	10/22/07		2007	10	5/11/07
R1/3	10/29/07		2007	10	5/11/07
R1/3	11/8/07		2007	11	5/11/07
R1/3	11/12/07		2007	11	5/11/07
R1/3	11/19/07		2007	11	5/11/07
R1/3	11/29/07		2007	11	5/11/07
R1/3	12/6/07		2007	12	5/11/07
R1/3	12/10/07		2007	12	5/11/07
R1/3	12/21/07		2007	12	5/11/07
R1/3	1/7/08			2008	1	5/11/07
R1/3	1/14/08		2008	1	5/11/07
R1/3	1/21/08		2008	1	5/11/07
R1/3	1/28/08		2008	1	5/11/07
R1/3	2/4/08		Y	2008	2	


Regards
-------------------------------------------------------------------------------
Kevin Wame 

###############################################################

###############################################################



On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:

result <- setNames( data.frame( aggregate( date~ID, data=drug_study, FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c( "ID", "start", "end" ) )


______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From chocold12 at gmail.com  Sun Jul  3 20:14:42 2016
From: chocold12 at gmail.com (lily li)
Date: Sun, 3 Jul 2016 12:14:42 -0600
Subject: [R] regroup row names
Message-ID: <CAN5afy9uYDhe=3qcv67B9p_HVZiSgBvqKSganjVyqbfTUfbGjQ@mail.gmail.com>

I have a problem in changing row names in a dataframe in R. The first
column is ID, such as aClim_st02, aClim_st03, aClim_st 05, bClim_st01,
bClim_st02, etc. How to rename the names, so that aClim_ all grouped to
aClim, while bClim_ all grouped to bClim? Thanks for your help.
df

ID                    temp   precip   LW   SW
aClim_st02
aClim_st03
aClim_st05
bClim_st01
bClim_st02
...

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Sun Jul  3 20:24:06 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Sun, 03 Jul 2016 18:24:06 +0000
Subject: [R] regroup row names
In-Reply-To: <CAN5afy9uYDhe=3qcv67B9p_HVZiSgBvqKSganjVyqbfTUfbGjQ@mail.gmail.com>
References: <CAN5afy9uYDhe=3qcv67B9p_HVZiSgBvqKSganjVyqbfTUfbGjQ@mail.gmail.com>
Message-ID: <CAKVAULM4SUyvK_daNodMGAb+KjcWsQpj4Rb4aPd6Y2MA3i2csA@mail.gmail.com>

Hi Lily,

you can use gsub:

df$ID <- gsub("_.*", "", df$ID)

HTH
Ulrik

On Sun, 3 Jul 2016 at 20:16 lily li <chocold12 at gmail.com> wrote:

> I have a problem in changing row names in a dataframe in R. The first
> column is ID, such as aClim_st02, aClim_st03, aClim_st 05, bClim_st01,
> bClim_st02, etc. How to rename the names, so that aClim_ all grouped to
> aClim, while bClim_ all grouped to bClim? Thanks for your help.
> df
>
> ID                    temp   precip   LW   SW
> aClim_st02
> aClim_st03
> aClim_st05
> bClim_st01
> bClim_st02
> ...
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Sun Jul  3 20:34:24 2016
From: chocold12 at gmail.com (lily li)
Date: Sun, 3 Jul 2016 12:34:24 -0600
Subject: [R] regroup row names
In-Reply-To: <CAKVAULM4SUyvK_daNodMGAb+KjcWsQpj4Rb4aPd6Y2MA3i2csA@mail.gmail.com>
References: <CAN5afy9uYDhe=3qcv67B9p_HVZiSgBvqKSganjVyqbfTUfbGjQ@mail.gmail.com>
	<CAKVAULM4SUyvK_daNodMGAb+KjcWsQpj4Rb4aPd6Y2MA3i2csA@mail.gmail.com>
Message-ID: <CAN5afy8ANc7SwK1m=uTLTnMy2C1v7QD0hpULdwfuwRBKuqSi8w@mail.gmail.com>

Hi Ulrik,

Thanks. This is for one group, but how to do for several groups? I tried
gsub(c(),c(),df$ID), but it does not work.


On Sun, Jul 3, 2016 at 12:24 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
wrote:

> Hi Lily,
>
> you can use gsub:
>
> df$ID <- gsub("_.*", "", df$ID)
>
> HTH
> Ulrik
>
> On Sun, 3 Jul 2016 at 20:16 lily li <chocold12 at gmail.com> wrote:
>
>> I have a problem in changing row names in a dataframe in R. The first
>> column is ID, such as aClim_st02, aClim_st03, aClim_st 05, bClim_st01,
>> bClim_st02, etc. How to rename the names, so that aClim_ all grouped to
>> aClim, while bClim_ all grouped to bClim? Thanks for your help.
>> df
>>
>> ID                    temp   precip   LW   SW
>> aClim_st02
>> aClim_st03
>> aClim_st05
>> bClim_st01
>> bClim_st02
>> ...
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sun Jul  3 20:34:21 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 03 Jul 2016 11:34:21 -0700
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
	| For a Large Dataset
In-Reply-To: <F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
Message-ID: <9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>

I still get the impression from your mixing of information types that you are thinking like this is Excel.

Perhaps something like

drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin, drug_study$ID, FUN=cumsum )
library(dplyr)
result0 <- (   drug_study
          %>% filter( 0 != admin_period )
          %>% group_by( ID, admin_period )
          %>% summarise( start = min( date ) )
          %>% mutate( admin_period1 = admin_period -1 )
          )
result <- (   result0 
          %>% select( -admin_period )
          %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
                       , by = c( ID="ID", admin_period ="admin_period1" )
                        )
          %>% mutate( ddays = end - start )
          )
-- 
Sent from my phone. Please excuse my brevity.

On July 3, 2016 10:24:51 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>HI Jeff, it?s been an uphill task working with the dataset and I am not
>the first to complain. Nonetheless, data-cleaning is ongoing and since
>I cannot wait for that to get done, I decided to make the most of what
>the dataset looks like at this time. It appears the process may take a
>while.
>
>Thanks for the script. From the output, I noticed that ?result?
>contains the first and last date for each of the individuals and not
>taking into account the variable ?drug-admin?. 
>
>ID	    start		end
>J1/3	    1/5/09	12/25/10
>R1/3	    1/4/07	12/15/08
>R10/1	    1/4/07	3/5/12
>
>My aim is to pick the date, for example in 2007, where drug-admin ==
>?Y? as my start and the date in the subsequent year (2008 in this case)
>where drug-admin == ?Y? as my end. Then, I should populate the variable
>?study_id? with ?start? up to the entry just above the one whose date
>matches ?end?, as the output below shows (I hope its structure is
>maintained as I have copied it from R-Studio). The goal for now is to
>then get difference in days between ?date? and ?study_id? and still get
>to keep that column for ?study_id? as I might use it later.
>
>From the output, it can be seen that for this individual, the dates run
>from 2007 to 2008. However, for some individuals, the dates run from
>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>deal with all the years as the dates range from 2001-2016
>
>ID	date	drug_admin	year	month	study_id
>R1/3	5/11/07	Y	2007	5	5/11/07
>R1/3	5/16/07		2007	5	5/11/07
>R1/3	5/22/07		2007	5	5/11/07
>R1/3	5/28/07		2007	5	5/11/07
>R1/3	6/5/07			2007	6	5/11/07
>R1/3	6/11/07		2007	6	5/11/07
>R1/3	6/18/07		2007	6	5/11/07
>R1/3	6/25/07		2007	6	5/11/07
>R1/3	7/2/07			2007	7	5/11/07
>R1/3	7/16/07		2007	7	5/11/07
>R1/3	7/29/07		2007	7	5/11/07
>R1/3	8/2/07			2007	8	5/11/07
>R1/3	8/7/07			2007	8	5/11/07
>R1/3	8/13/07		2007	8	5/11/07
>R1/3	9/18/07		2007	9	5/11/07
>R1/3	9/24/07		2007	9	5/11/07
>R1/3	10/6/07		2007	10	5/11/07
>R1/3	10/8/07		2007	10	5/11/07
>R1/3	10/15/07		2007	10	5/11/07
>R1/3	10/22/07		2007	10	5/11/07
>R1/3	10/29/07		2007	10	5/11/07
>R1/3	11/8/07		2007	11	5/11/07
>R1/3	11/12/07		2007	11	5/11/07
>R1/3	11/19/07		2007	11	5/11/07
>R1/3	11/29/07		2007	11	5/11/07
>R1/3	12/6/07		2007	12	5/11/07
>R1/3	12/10/07		2007	12	5/11/07
>R1/3	12/21/07		2007	12	5/11/07
>R1/3	1/7/08			2008	1	5/11/07
>R1/3	1/14/08		2008	1	5/11/07
>R1/3	1/21/08		2008	1	5/11/07
>R1/3	1/28/08		2008	1	5/11/07
>R1/3	2/4/08		Y	2008	2	
>
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Wame 
>
>###############################################################
>
>###############################################################
>
>
>
>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>
>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>"ID", "start", "end" ) )
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________


From bgunter.4567 at gmail.com  Sun Jul  3 20:38:54 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 3 Jul 2016 11:38:54 -0700
Subject: [R] regroup row names
In-Reply-To: <CAN5afy9uYDhe=3qcv67B9p_HVZiSgBvqKSganjVyqbfTUfbGjQ@mail.gmail.com>
References: <CAN5afy9uYDhe=3qcv67B9p_HVZiSgBvqKSganjVyqbfTUfbGjQ@mail.gmail.com>
Message-ID: <CAGxFJbSguPwEDM05REpEh0Qeue1brJZb0LG1muQz8UufCR01zw@mail.gmail.com>

I strongly suspect that you do not need to do this. What I think you
do need to do is to create a new column (which will be a factor)
identifying the climate ("a" or "b"), which can then be used to group
climates in plots, used as a covariate in statistical analyses, etc.
Moreover, there is probably no need for things to be in order (R is
not Excel or SPSS or ...).

You can either use regexp's (e.g. grep -- very powerful but with a
hefty learning curve) or because of the simplicity of your ID's,
?substring ; e.g.

yourdat$clim_type <- substring(yourdat$ID,1,1)

Please do some more tutorials on your own, as these (not regexp's) are
fairly basic R features that all users should be aware of.

Incidentally, check out the "stringr" package, which is supposed to
make string manipulation tasks like this easier (I have not used it
though).

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Jul 3, 2016 at 11:14 AM, lily li <chocold12 at gmail.com> wrote:
> I have a problem in changing row names in a dataframe in R. The first
> column is ID, such as aClim_st02, aClim_st03, aClim_st 05, bClim_st01,
> bClim_st02, etc. How to rename the names, so that aClim_ all grouped to
> aClim, while bClim_ all grouped to bClim? Thanks for your help.
> df
>
> ID                    temp   precip   LW   SW
> aClim_st02
> aClim_st03
> aClim_st05
> bClim_st01
> bClim_st02
> ...
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ulrik.stervbo at gmail.com  Sun Jul  3 20:41:46 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Sun, 03 Jul 2016 18:41:46 +0000
Subject: [R] regroup row names
In-Reply-To: <CAN5afy8ANc7SwK1m=uTLTnMy2C1v7QD0hpULdwfuwRBKuqSi8w@mail.gmail.com>
References: <CAN5afy9uYDhe=3qcv67B9p_HVZiSgBvqKSganjVyqbfTUfbGjQ@mail.gmail.com>
	<CAKVAULM4SUyvK_daNodMGAb+KjcWsQpj4Rb4aPd6Y2MA3i2csA@mail.gmail.com>
	<CAN5afy8ANc7SwK1m=uTLTnMy2C1v7QD0hpULdwfuwRBKuqSi8w@mail.gmail.com>
Message-ID: <CAKVAULMRXv7QX5-+mNhQ5g8iK47D8CZm2Gh6S_t-6DK++MNcRA@mail.gmail.com>

Hi Lily,

My suggestion should remove the underscore and everything after it, leaving
just aClim and bClim in the ID column.

Best
Ulrik

On Sun, 3 Jul 2016, 20:34 lily li, <chocold12 at gmail.com> wrote:

> Hi Ulrik,
>
> Thanks. This is for one group, but how to do for several groups? I tried
> gsub(c(),c(),df$ID), but it does not work.
>
>
> On Sun, Jul 3, 2016 at 12:24 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> wrote:
>
>> Hi Lily,
>>
>> you can use gsub:
>>
>> df$ID <- gsub("_.*", "", df$ID)
>>
>> HTH
>> Ulrik
>>
>> On Sun, 3 Jul 2016 at 20:16 lily li <chocold12 at gmail.com> wrote:
>>
>>> I have a problem in changing row names in a dataframe in R. The first
>>> column is ID, such as aClim_st02, aClim_st03, aClim_st 05, bClim_st01,
>>> bClim_st02, etc. How to rename the names, so that aClim_ all grouped to
>>> aClim, while bClim_ all grouped to bClim? Thanks for your help.
>>> df
>>>
>>> ID                    temp   precip   LW   SW
>>> aClim_st02
>>> aClim_st03
>>> aClim_st05
>>> bClim_st01
>>> bClim_st02
>>> ...
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>

	[[alternative HTML version deleted]]


From KWamae at kemri-wellcome.org  Sun Jul  3 20:55:14 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 3 Jul 2016 18:55:14 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
Message-ID: <58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>

Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.

Thanks for the code.  After running it, this is the error I get.

Error: cannot join on columns 'admin_period' x 'admin_period1': index out of bounds

Regards
-------------------------------------------------------------------------------
Kevin Wame | Ph.D. Student (IDeAL)
KEMRI-Wellcome Trust Collaborative Research Programme
Centre for Geographic Medicine Research
P.O. Box 230-80108, Kilifi, Kenya
 

On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:

I still get the impression from your mixing of information types that you are thinking like this is Excel.

Perhaps something like

drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin, drug_study$ID, FUN=cumsum )
library(dplyr)
result0 <- (   drug_study
          %>% filter( 0 != admin_period )
          %>% group_by( ID, admin_period )
          %>% summarise( start = min( date ) )
          %>% mutate( admin_period1 = admin_period -1 )
          )
result <- (   result0 
          %>% select( -admin_period )
          %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
                       , by = c( ID="ID", admin_period ="admin_period1" )
                        )
          %>% mutate( ddays = end - start )
          )
-- 
Sent from my phone. Please excuse my brevity.

On July 3, 2016 10:24:51 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>HI Jeff, it?s been an uphill task working with the dataset and I am not
>the first to complain. Nonetheless, data-cleaning is ongoing and since
>I cannot wait for that to get done, I decided to make the most of what
>the dataset looks like at this time. It appears the process may take a
>while.
>
>Thanks for the script. From the output, I noticed that ?result?
>contains the first and last date for each of the individuals and not
>taking into account the variable ?drug-admin?. 
>
>ID	    start		end
>J1/3	    1/5/09	12/25/10
>R1/3	    1/4/07	12/15/08
>R10/1	    1/4/07	3/5/12
>
>My aim is to pick the date, for example in 2007, where drug-admin ==
>?Y? as my start and the date in the subsequent year (2008 in this case)
>where drug-admin == ?Y? as my end. Then, I should populate the variable
>?study_id? with ?start? up to the entry just above the one whose date
>matches ?end?, as the output below shows (I hope its structure is
>maintained as I have copied it from R-Studio). The goal for now is to
>then get difference in days between ?date? and ?study_id? and still get
>to keep that column for ?study_id? as I might use it later.
>
>From the output, it can be seen that for this individual, the dates run
>from 2007 to 2008. However, for some individuals, the dates run from
>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>deal with all the years as the dates range from 2001-2016
>
>ID	date	drug_admin	year	month	study_id
>R1/3	5/11/07	Y	2007	5	5/11/07
>R1/3	5/16/07		2007	5	5/11/07
>R1/3	5/22/07		2007	5	5/11/07
>R1/3	5/28/07		2007	5	5/11/07
>R1/3	6/5/07			2007	6	5/11/07
>R1/3	6/11/07		2007	6	5/11/07
>R1/3	6/18/07		2007	6	5/11/07
>R1/3	6/25/07		2007	6	5/11/07
>R1/3	7/2/07			2007	7	5/11/07
>R1/3	7/16/07		2007	7	5/11/07
>R1/3	7/29/07		2007	7	5/11/07
>R1/3	8/2/07			2007	8	5/11/07
>R1/3	8/7/07			2007	8	5/11/07
>R1/3	8/13/07		2007	8	5/11/07
>R1/3	9/18/07		2007	9	5/11/07
>R1/3	9/24/07		2007	9	5/11/07
>R1/3	10/6/07		2007	10	5/11/07
>R1/3	10/8/07		2007	10	5/11/07
>R1/3	10/15/07		2007	10	5/11/07
>R1/3	10/22/07		2007	10	5/11/07
>R1/3	10/29/07		2007	10	5/11/07
>R1/3	11/8/07		2007	11	5/11/07
>R1/3	11/12/07		2007	11	5/11/07
>R1/3	11/19/07		2007	11	5/11/07
>R1/3	11/29/07		2007	11	5/11/07
>R1/3	12/6/07		2007	12	5/11/07
>R1/3	12/10/07		2007	12	5/11/07
>R1/3	12/21/07		2007	12	5/11/07
>R1/3	1/7/08			2008	1	5/11/07
>R1/3	1/14/08		2008	1	5/11/07
>R1/3	1/21/08		2008	1	5/11/07
>R1/3	1/28/08		2008	1	5/11/07
>R1/3	2/4/08		Y	2008	2	
>
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Wame 
>
>###############################################################
>
>###############################################################
>
>
>
>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>
>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>"ID", "start", "end" ) )
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________




______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From jdnewmil at dcn.davis.ca.us  Sun Jul  3 21:09:22 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 03 Jul 2016 12:09:22 -0700
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
	| For a Large Dataset
In-Reply-To: <58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
Message-ID: <944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>

Typo on the second line

result <- (   result0 
          %>% select( -admin_period1 )
          %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
                       , by = c( ID="ID", admin_period ="admin_period1" )
                        )
          %>% mutate( ddays = end - start )
          )
-- 
Sent from my phone. Please excuse my brevity.

On July 3, 2016 11:55:14 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.
>
>Thanks for the code.  After running it, this is the error I get.
>
>Error: cannot join on columns 'admin_period' x 'admin_period1': index
>out of bounds
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Wame | Ph.D. Student (IDeAL)
>KEMRI-Wellcome Trust Collaborative Research Programme
>Centre for Geographic Medicine Research
>P.O. Box 230-80108, Kilifi, Kenya
> 
>
>On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>
>I still get the impression from your mixing of information types that
>you are thinking like this is Excel.
>
>Perhaps something like
>
>drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin,
>drug_study$ID, FUN=cumsum )
>library(dplyr)
>result0 <- (   drug_study
>          %>% filter( 0 != admin_period )
>          %>% group_by( ID, admin_period )
>          %>% summarise( start = min( date ) )
>          %>% mutate( admin_period1 = admin_period -1 )
>          )
>result <- (   result0 
>          %>% select( -admin_period )
>     %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
>                     , by = c( ID="ID", admin_period ="admin_period1" )
>                        )
>          %>% mutate( ddays = end - start )
>          )
>-- 
>Sent from my phone. Please excuse my brevity.
>
>On July 3, 2016 10:24:51 AM PDT, Kevin Wamae
><KWamae at kemri-wellcome.org> wrote:
>>HI Jeff, it?s been an uphill task working with the dataset and I am
>not
>>the first to complain. Nonetheless, data-cleaning is ongoing and since
>>I cannot wait for that to get done, I decided to make the most of what
>>the dataset looks like at this time. It appears the process may take a
>>while.
>>
>>Thanks for the script. From the output, I noticed that ?result?
>>contains the first and last date for each of the individuals and not
>>taking into account the variable ?drug-admin?. 
>>
>>ID	    start		end
>>J1/3	    1/5/09	12/25/10
>>R1/3	    1/4/07	12/15/08
>>R10/1	    1/4/07	3/5/12
>>
>>My aim is to pick the date, for example in 2007, where drug-admin ==
>>?Y? as my start and the date in the subsequent year (2008 in this
>case)
>>where drug-admin == ?Y? as my end. Then, I should populate the
>variable
>>?study_id? with ?start? up to the entry just above the one whose date
>>matches ?end?, as the output below shows (I hope its structure is
>>maintained as I have copied it from R-Studio). The goal for now is to
>>then get difference in days between ?date? and ?study_id? and still
>get
>>to keep that column for ?study_id? as I might use it later.
>>
>>From the output, it can be seen that for this individual, the dates
>run
>>from 2007 to 2008. However, for some individuals, the dates run from
>>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>>deal with all the years as the dates range from 2001-2016
>>
>>ID	date	drug_admin	year	month	study_id
>>R1/3	5/11/07	Y	2007	5	5/11/07
>>R1/3	5/16/07		2007	5	5/11/07
>>R1/3	5/22/07		2007	5	5/11/07
>>R1/3	5/28/07		2007	5	5/11/07
>>R1/3	6/5/07			2007	6	5/11/07
>>R1/3	6/11/07		2007	6	5/11/07
>>R1/3	6/18/07		2007	6	5/11/07
>>R1/3	6/25/07		2007	6	5/11/07
>>R1/3	7/2/07			2007	7	5/11/07
>>R1/3	7/16/07		2007	7	5/11/07
>>R1/3	7/29/07		2007	7	5/11/07
>>R1/3	8/2/07			2007	8	5/11/07
>>R1/3	8/7/07			2007	8	5/11/07
>>R1/3	8/13/07		2007	8	5/11/07
>>R1/3	9/18/07		2007	9	5/11/07
>>R1/3	9/24/07		2007	9	5/11/07
>>R1/3	10/6/07		2007	10	5/11/07
>>R1/3	10/8/07		2007	10	5/11/07
>>R1/3	10/15/07		2007	10	5/11/07
>>R1/3	10/22/07		2007	10	5/11/07
>>R1/3	10/29/07		2007	10	5/11/07
>>R1/3	11/8/07		2007	11	5/11/07
>>R1/3	11/12/07		2007	11	5/11/07
>>R1/3	11/19/07		2007	11	5/11/07
>>R1/3	11/29/07		2007	11	5/11/07
>>R1/3	12/6/07		2007	12	5/11/07
>>R1/3	12/10/07		2007	12	5/11/07
>>R1/3	12/21/07		2007	12	5/11/07
>>R1/3	1/7/08			2008	1	5/11/07
>>R1/3	1/14/08		2008	1	5/11/07
>>R1/3	1/21/08		2008	1	5/11/07
>>R1/3	1/28/08		2008	1	5/11/07
>>R1/3	2/4/08		Y	2008	2	
>>
>>
>>Regards
>>-------------------------------------------------------------------------------
>>Kevin Wame 
>>
>>###############################################################
>>
>>###############################################################
>>
>>
>>
>>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>>
>>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>>"ID", "start", "end" ) )
>>
>>
>>______________________________________________________________________
>>
>>This e-mail contains information which is confidential. It is intended
>>only for the use of the named recipient. If you have received this
>>e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network.
>Although
>>the Programme has taken reasonable precautions to ensure no viruses
>are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>______________________________________________________________________
>
>
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________


From KWamae at kemri-wellcome.org  Sun Jul  3 21:13:38 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 3 Jul 2016 19:13:38 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
	<944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
Message-ID: <39C6003E-25BC-4B3A-B199-1CE6A4079676@kemri-wellcome.org>

Thanks Jeff, let me try it on the larger dataset.

Regards
-------------------------------------------------------------------------------
Kevin Wame 
 

On 7/3/16, 10:09 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:

result <- (   result0 
          %>% select( -admin_period1 )
          %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
                       , by = c( ID="ID", admin_period ="admin_period1" )
                        )
          %>% mutate( ddays = end - start )
          )


______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From KWamae at kemri-wellcome.org  Sun Jul  3 22:08:44 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 3 Jul 2016 20:08:44 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
	<944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
Message-ID: <415F1412-7D4B-4CBA-A75B-AE295A6E5DFF@kemri-wellcome.org>

Hi Jeff, It works on well on a dataset with 100000 rows and I figure it will work well with the ?real? dataset. You?ve been of great help and I am starting to make headway. 

It creates a new dataframe (result), as shown below that doesn?t quite have the result as I would want it.

ID	admin_period	start	end	ddays
J1/3	1	5/11/07	8/13/07	94
J1/3	2	8/13/07	11/12/07	91
J1/3	3	11/12/07	2/4/08	           84
J1/3	4	2/4/08	            5/5/08	            91
J1/3	5	5/5/08	             5/4/09            364
J1/3	6	5/4/09	             5/17/10	378
J1/3	7	5/17/10	5/16/11	364
J10/1	1	5/11/07	8/13/07	94
J10/1	2	8/13/07	11/12/07	91
J10/1	3	11/12/07	2/4/08	            84
J10/1	4	2/4/08	              5/5/08	91
J10/1	5	5/5/08	              5/8/09	368
J10/1	6	5/8/09	             5/17/10	374
J10/1	7	5/17/10	5/16/11	364
J102/1	1	5/15/07	8/15/07	92
J102/1	2	8/15/07	11/13/07	90
J102/1	3	11/13/07	2/5/08	           84
J102/1	4	2/5/08	              5/6/08	91
J102/1	5	5/6/08	              5/5/09	364
J102/1	6	5/5/09	              5/19/10	379

My supervisor doesn?t want me to create a new dataset, she?s afraid I might lose some data?I cannot fight that.

Like you mentioned earlier, I might be mixing up things which I think is what you alluded to earlier.

After consultation with my supervisor, this is what we?ve agreed. For every individual, given the start and end date, create a new column (say, diff_days) and for every row that falls within the range of start and end_date, get the difference between the date in that row and start date and add it to the diff_days column. Below is an example of the result. As it can be seen 5/11/2007 is the start while 2/4/2008 is the end. The diff_days has been populated excluding the end date and that is because that is the start of the study in 2008 that will continue into 2009 and thus from 2/4/2008, I should compute diff_days till 2009 and so no (I hope this makes sense).

ID	date	drug_admin	year	month	diff_days
R1/3	5/11/2007	Y	2007	5	0
R1/3	5/16/2007		2007	5	6
R1/3	5/22/2007		2007	5	11
R1/3	5/28/2007		2007	5	17
R1/3	1/14/2008		2008	1	248
R1/3	1/21/2008		2008	1	255
R1/3	1/28/2008		2008	1	263
R1/3	2/4/2008	Y	2008	2	


Regards
-------------------------------------------------------------------------------
Kevin Wame 
 

On 7/3/16, 10:09 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:

Typo on the second line

result <- (   result0 
          %>% select( -admin_period1 )
          %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
                       , by = c( ID="ID", admin_period ="admin_period1" )
                        )
          %>% mutate( ddays = end - start )
          )
-- 
Sent from my phone. Please excuse my brevity.

On July 3, 2016 11:55:14 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.
>
>Thanks for the code.  After running it, this is the error I get.
>
>Error: cannot join on columns 'admin_period' x 'admin_period1': index
>out of bounds
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Wame | Ph.D. Student (IDeAL)
>KEMRI-Wellcome Trust Collaborative Research Programme
>Centre for Geographic Medicine Research
>P.O. Box 230-80108, Kilifi, Kenya
> 
>
>On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>
>I still get the impression from your mixing of information types that
>you are thinking like this is Excel.
>
>Perhaps something like
>
>drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin,
>drug_study$ID, FUN=cumsum )
>library(dplyr)
>result0 <- (   drug_study
>          %>% filter( 0 != admin_period )
>          %>% group_by( ID, admin_period )
>          %>% summarise( start = min( date ) )
>          %>% mutate( admin_period1 = admin_period -1 )
>          )
>result <- (   result0 
>          %>% select( -admin_period )
>     %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
>                     , by = c( ID="ID", admin_period ="admin_period1" )
>                        )
>          %>% mutate( ddays = end - start )
>          )
>-- 
>Sent from my phone. Please excuse my brevity.
>
>On July 3, 2016 10:24:51 AM PDT, Kevin Wamae
><KWamae at kemri-wellcome.org> wrote:
>>HI Jeff, it?s been an uphill task working with the dataset and I am
>not
>>the first to complain. Nonetheless, data-cleaning is ongoing and since
>>I cannot wait for that to get done, I decided to make the most of what
>>the dataset looks like at this time. It appears the process may take a
>>while.
>>
>>Thanks for the script. From the output, I noticed that ?result?
>>contains the first and last date for each of the individuals and not
>>taking into account the variable ?drug-admin?. 
>>
>>ID	    start		end
>>J1/3	    1/5/09	12/25/10
>>R1/3	    1/4/07	12/15/08
>>R10/1	    1/4/07	3/5/12
>>
>>My aim is to pick the date, for example in 2007, where drug-admin ==
>>?Y? as my start and the date in the subsequent year (2008 in this
>case)
>>where drug-admin == ?Y? as my end. Then, I should populate the
>variable
>>?study_id? with ?start? up to the entry just above the one whose date
>>matches ?end?, as the output below shows (I hope its structure is
>>maintained as I have copied it from R-Studio). The goal for now is to
>>then get difference in days between ?date? and ?study_id? and still
>get
>>to keep that column for ?study_id? as I might use it later.
>>
>>From the output, it can be seen that for this individual, the dates
>run
>>from 2007 to 2008. However, for some individuals, the dates run from
>>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>>deal with all the years as the dates range from 2001-2016
>>
>>ID	date	drug_admin	year	month	study_id
>>R1/3	5/11/07	Y	2007	5	5/11/07
>>R1/3	5/16/07		2007	5	5/11/07
>>R1/3	5/22/07		2007	5	5/11/07
>>R1/3	5/28/07		2007	5	5/11/07
>>R1/3	6/5/07			2007	6	5/11/07
>>R1/3	6/11/07		2007	6	5/11/07
>>R1/3	6/18/07		2007	6	5/11/07
>>R1/3	6/25/07		2007	6	5/11/07
>>R1/3	7/2/07			2007	7	5/11/07
>>R1/3	7/16/07		2007	7	5/11/07
>>R1/3	7/29/07		2007	7	5/11/07
>>R1/3	8/2/07			2007	8	5/11/07
>>R1/3	8/7/07			2007	8	5/11/07
>>R1/3	8/13/07		2007	8	5/11/07
>>R1/3	9/18/07		2007	9	5/11/07
>>R1/3	9/24/07		2007	9	5/11/07
>>R1/3	10/6/07		2007	10	5/11/07
>>R1/3	10/8/07		2007	10	5/11/07
>>R1/3	10/15/07		2007	10	5/11/07
>>R1/3	10/22/07		2007	10	5/11/07
>>R1/3	10/29/07		2007	10	5/11/07
>>R1/3	11/8/07		2007	11	5/11/07
>>R1/3	11/12/07		2007	11	5/11/07
>>R1/3	11/19/07		2007	11	5/11/07
>>R1/3	11/29/07		2007	11	5/11/07
>>R1/3	12/6/07		2007	12	5/11/07
>>R1/3	12/10/07		2007	12	5/11/07
>>R1/3	12/21/07		2007	12	5/11/07
>>R1/3	1/7/08			2008	1	5/11/07
>>R1/3	1/14/08		2008	1	5/11/07
>>R1/3	1/21/08		2008	1	5/11/07
>>R1/3	1/28/08		2008	1	5/11/07
>>R1/3	2/4/08		Y	2008	2	
>>
>>
>>Regards
>>-------------------------------------------------------------------------------
>>Kevin Wame 
>>
>>###############################################################
>>
>>###############################################################
>>
>>
>>
>>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>>
>>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>>"ID", "start", "end" ) )
>>
>>
>>______________________________________________________________________
>>
>>This e-mail contains information which is confidential. It is intended
>>only for the use of the named recipient. If you have received this
>>e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network.
>Although
>>the Programme has taken reasonable precautions to ensure no viruses
>are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>______________________________________________________________________
>
>
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________




______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From ulrik.stervbo at gmail.com  Sun Jul  3 22:14:26 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Sun, 03 Jul 2016 20:14:26 +0000
Subject: [R] regroup row names
In-Reply-To: <CAN5afy_L8U6CeUWPvmZ0SuhV6r-JfvbBe3R5_W66MKcvvJQcHQ@mail.gmail.com>
References: <CAN5afy9uYDhe=3qcv67B9p_HVZiSgBvqKSganjVyqbfTUfbGjQ@mail.gmail.com>
	<CAKVAULM4SUyvK_daNodMGAb+KjcWsQpj4Rb4aPd6Y2MA3i2csA@mail.gmail.com>
	<CAN5afy8ANc7SwK1m=uTLTnMy2C1v7QD0hpULdwfuwRBKuqSi8w@mail.gmail.com>
	<CAKVAULMRXv7QX5-+mNhQ5g8iK47D8CZm2Gh6S_t-6DK++MNcRA@mail.gmail.com>
	<CAN5afy_L8U6CeUWPvmZ0SuhV6r-JfvbBe3R5_W66MKcvvJQcHQ@mail.gmail.com>
Message-ID: <CAKVAULPyD7QcCQAc7LvXGSi5uDC=-RoNBxBpJDXsSrtS4JKGEQ@mail.gmail.com>

Do the elements in 'locs' hahabe an _ somewhere? If not the search and
replace find nothing.

Bert's suggestion of taking a substring is better if you are just
interested in characters on fixed positions.

Bert also suggested that you could maybe benefit from reading a few
tutorials and I agree.

Best,
Ulrik

On Sun, 3 Jul 2016, 21:36 lily li, <chocold12 at gmail.com> wrote:

> Hi Ulrik,
>
> I created another column named locs, and used the code df$locs <-
> gsub("_.*", "", df$locs), but I found that the names does not change at
> all. And the new column becomes characters after using the gsub function.
> What is the problem? Thanks again.
>
>
> On Sun, Jul 3, 2016 at 12:41 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> wrote:
>
>> Hi Lily,
>>
>> My suggestion should remove the underscore and everything after it,
>> leaving just aClim and bClim in the ID column.
>>
>> Best
>> Ulrik
>>
>> On Sun, 3 Jul 2016, 20:34 lily li, <chocold12 at gmail.com> wrote:
>>
>>> Hi Ulrik,
>>>
>>> Thanks. This is for one group, but how to do for several groups? I tried
>>> gsub(c(),c(),df$ID), but it does not work.
>>>
>>>
>>> On Sun, Jul 3, 2016 at 12:24 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>>> wrote:
>>>
>>>> Hi Lily,
>>>>
>>>> you can use gsub:
>>>>
>>>> df$ID <- gsub("_.*", "", df$ID)
>>>>
>>>> HTH
>>>> Ulrik
>>>>
>>>> On Sun, 3 Jul 2016 at 20:16 lily li <chocold12 at gmail.com> wrote:
>>>>
>>>>> I have a problem in changing row names in a dataframe in R. The first
>>>>> column is ID, such as aClim_st02, aClim_st03, aClim_st 05, bClim_st01,
>>>>> bClim_st02, etc. How to rename the names, so that aClim_ all grouped to
>>>>> aClim, while bClim_ all grouped to bClim? Thanks for your help.
>>>>> df
>>>>>
>>>>> ID                    temp   precip   LW   SW
>>>>> aClim_st02
>>>>> aClim_st03
>>>>> aClim_st05
>>>>> bClim_st01
>>>>> bClim_st02
>>>>> ...
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sun Jul  3 22:28:32 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 3 Jul 2016 13:28:32 -0700
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <415F1412-7D4B-4CBA-A75B-AE295A6E5DFF@kemri-wellcome.org>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
	<944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
	<415F1412-7D4B-4CBA-A75B-AE295A6E5DFF@kemri-wellcome.org>
Message-ID: <CAGxFJbQLxm-vULXHS2euC1S0TT4PUb1_303y5xj+NxpNXwjWBg@mail.gmail.com>

I haven't followed this thread closely, but if it's not too late, I
might suggest that you stop worrying about how you want your data
frame to look and start worrying about you want to display/analyze
your data. As Jeff suggested, you and your supervisor are probably
being driven by paradigms from Excel, SPSS, or whatever that are
simply unnecessary for R. My guess would be that if you explained the
sort of analyses/plots you wish to do, you will find it can be done
fairly directly from your existing data. At the very least it would
give Jeff and other helpeRs a better idea of what you might need
rather than what you and your supervisor think you need.


Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Jul 3, 2016 at 1:08 PM, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
> Hi Jeff, It works on well on a dataset with 100000 rows and I figure it will work well with the ?real? dataset. You?ve been of great help and I am starting to make headway.
>
> It creates a new dataframe (result), as shown below that doesn?t quite have the result as I would want it.
>
> ID      admin_period    start   end     ddays
> J1/3    1       5/11/07 8/13/07 94
> J1/3    2       8/13/07 11/12/07        91
> J1/3    3       11/12/07        2/4/08             84
> J1/3    4       2/4/08              5/5/08                  91
> J1/3    5       5/5/08               5/4/09            364
> J1/3    6       5/4/09               5/17/10    378
> J1/3    7       5/17/10 5/16/11 364
> J10/1   1       5/11/07 8/13/07 94
> J10/1   2       8/13/07 11/12/07        91
> J10/1   3       11/12/07        2/4/08              84
> J10/1   4       2/4/08                5/5/08    91
> J10/1   5       5/5/08                5/8/09    368
> J10/1   6       5/8/09               5/17/10    374
> J10/1   7       5/17/10 5/16/11 364
> J102/1  1       5/15/07 8/15/07 92
> J102/1  2       8/15/07 11/13/07        90
> J102/1  3       11/13/07        2/5/08             84
> J102/1  4       2/5/08                5/6/08    91
> J102/1  5       5/6/08                5/5/09    364
> J102/1  6       5/5/09                5/19/10   379
>
> My supervisor doesn?t want me to create a new dataset, she?s afraid I might lose some data?I cannot fight that.
>
> Like you mentioned earlier, I might be mixing up things which I think is what you alluded to earlier.
>
> After consultation with my supervisor, this is what we?ve agreed. For every individual, given the start and end date, create a new column (say, diff_days) and for every row that falls within the range of start and end_date, get the difference between the date in that row and start date and add it to the diff_days column. Below is an example of the result. As it can be seen 5/11/2007 is the start while 2/4/2008 is the end. The diff_days has been populated excluding the end date and that is because that is the start of the study in 2008 that will continue into 2009 and thus from 2/4/2008, I should compute diff_days till 2009 and so no (I hope this makes sense).
>
> ID      date    drug_admin      year    month   diff_days
> R1/3    5/11/2007       Y       2007    5       0
> R1/3    5/16/2007               2007    5       6
> R1/3    5/22/2007               2007    5       11
> R1/3    5/28/2007               2007    5       17
> R1/3    1/14/2008               2008    1       248
> R1/3    1/21/2008               2008    1       255
> R1/3    1/28/2008               2008    1       263
> R1/3    2/4/2008        Y       2008    2
>
>
> Regards
> -------------------------------------------------------------------------------
> Kevin Wame
>
>
> On 7/3/16, 10:09 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>
> Typo on the second line
>
> result <- (   result0
>           %>% select( -admin_period1 )
>           %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
>                        , by = c( ID="ID", admin_period ="admin_period1" )
>                         )
>           %>% mutate( ddays = end - start )
>           )
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 3, 2016 11:55:14 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>>Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.
>>
>>Thanks for the code.  After running it, this is the error I get.
>>
>>Error: cannot join on columns 'admin_period' x 'admin_period1': index
>>out of bounds
>>
>>Regards
>>-------------------------------------------------------------------------------
>>Kevin Wame | Ph.D. Student (IDeAL)
>>KEMRI-Wellcome Trust Collaborative Research Programme
>>Centre for Geographic Medicine Research
>>P.O. Box 230-80108, Kilifi, Kenya
>>
>>
>>On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>>
>>I still get the impression from your mixing of information types that
>>you are thinking like this is Excel.
>>
>>Perhaps something like
>>
>>drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin,
>>drug_study$ID, FUN=cumsum )
>>library(dplyr)
>>result0 <- (   drug_study
>>          %>% filter( 0 != admin_period )
>>          %>% group_by( ID, admin_period )
>>          %>% summarise( start = min( date ) )
>>          %>% mutate( admin_period1 = admin_period -1 )
>>          )
>>result <- (   result0
>>          %>% select( -admin_period )
>>     %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
>>                     , by = c( ID="ID", admin_period ="admin_period1" )
>>                        )
>>          %>% mutate( ddays = end - start )
>>          )
>>--
>>Sent from my phone. Please excuse my brevity.
>>
>>On July 3, 2016 10:24:51 AM PDT, Kevin Wamae
>><KWamae at kemri-wellcome.org> wrote:
>>>HI Jeff, it?s been an uphill task working with the dataset and I am
>>not
>>>the first to complain. Nonetheless, data-cleaning is ongoing and since
>>>I cannot wait for that to get done, I decided to make the most of what
>>>the dataset looks like at this time. It appears the process may take a
>>>while.
>>>
>>>Thanks for the script. From the output, I noticed that ?result?
>>>contains the first and last date for each of the individuals and not
>>>taking into account the variable ?drug-admin?.
>>>
>>>ID        start               end
>>>J1/3      1/5/09      12/25/10
>>>R1/3      1/4/07      12/15/08
>>>R10/1     1/4/07      3/5/12
>>>
>>>My aim is to pick the date, for example in 2007, where drug-admin ==
>>>?Y? as my start and the date in the subsequent year (2008 in this
>>case)
>>>where drug-admin == ?Y? as my end. Then, I should populate the
>>variable
>>>?study_id? with ?start? up to the entry just above the one whose date
>>>matches ?end?, as the output below shows (I hope its structure is
>>>maintained as I have copied it from R-Studio). The goal for now is to
>>>then get difference in days between ?date? and ?study_id? and still
>>get
>>>to keep that column for ?study_id? as I might use it later.
>>>
>>>From the output, it can be seen that for this individual, the dates
>>run
>>>from 2007 to 2008. However, for some individuals, the dates run from
>>>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>>>deal with all the years as the dates range from 2001-2016
>>>
>>>ID    date    drug_admin      year    month   study_id
>>>R1/3  5/11/07 Y       2007    5       5/11/07
>>>R1/3  5/16/07         2007    5       5/11/07
>>>R1/3  5/22/07         2007    5       5/11/07
>>>R1/3  5/28/07         2007    5       5/11/07
>>>R1/3  6/5/07                  2007    6       5/11/07
>>>R1/3  6/11/07         2007    6       5/11/07
>>>R1/3  6/18/07         2007    6       5/11/07
>>>R1/3  6/25/07         2007    6       5/11/07
>>>R1/3  7/2/07                  2007    7       5/11/07
>>>R1/3  7/16/07         2007    7       5/11/07
>>>R1/3  7/29/07         2007    7       5/11/07
>>>R1/3  8/2/07                  2007    8       5/11/07
>>>R1/3  8/7/07                  2007    8       5/11/07
>>>R1/3  8/13/07         2007    8       5/11/07
>>>R1/3  9/18/07         2007    9       5/11/07
>>>R1/3  9/24/07         2007    9       5/11/07
>>>R1/3  10/6/07         2007    10      5/11/07
>>>R1/3  10/8/07         2007    10      5/11/07
>>>R1/3  10/15/07                2007    10      5/11/07
>>>R1/3  10/22/07                2007    10      5/11/07
>>>R1/3  10/29/07                2007    10      5/11/07
>>>R1/3  11/8/07         2007    11      5/11/07
>>>R1/3  11/12/07                2007    11      5/11/07
>>>R1/3  11/19/07                2007    11      5/11/07
>>>R1/3  11/29/07                2007    11      5/11/07
>>>R1/3  12/6/07         2007    12      5/11/07
>>>R1/3  12/10/07                2007    12      5/11/07
>>>R1/3  12/21/07                2007    12      5/11/07
>>>R1/3  1/7/08                  2008    1       5/11/07
>>>R1/3  1/14/08         2008    1       5/11/07
>>>R1/3  1/21/08         2008    1       5/11/07
>>>R1/3  1/28/08         2008    1       5/11/07
>>>R1/3  2/4/08          Y       2008    2
>>>
>>>
>>>Regards
>>>-------------------------------------------------------------------------------
>>>Kevin Wame
>>>
>>>###############################################################
>>>
>>>###############################################################
>>>
>>>
>>>
>>>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>>>
>>>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>>>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>>>"ID", "start", "end" ) )
>>>
>>>
>>>______________________________________________________________________
>>>
>>>This e-mail contains information which is confidential. It is intended
>>>only for the use of the named recipient. If you have received this
>>>e-mail in error, please let us know by replying to the sender, and
>>>immediately delete it from your system.  Please note, that in these
>>>circumstances, the use, disclosure, distribution or copying of this
>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>cannot accept any responsibility for the  accuracy or completeness of
>>>this message as it has been transmitted over a public network.
>>Although
>>>the Programme has taken reasonable precautions to ensure no viruses
>>are
>>>present in emails, it cannot accept responsibility for any loss or
>>>damage arising from the use of the email or attachments. Any views
>>>expressed in this message are those of the individual sender, except
>>>where the sender specifically states them to be the views of
>>>KEMRI-Wellcome Trust Programme.
>>>______________________________________________________________________
>>
>>
>>
>>
>>______________________________________________________________________
>>
>>This e-mail contains information which is confidential. It is intended
>>only for the use of the named recipient. If you have received this
>>e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network. Although
>>the Programme has taken reasonable precautions to ensure no viruses are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>______________________________________________________________________
>
>
>
>
> ______________________________________________________________________
>
> This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From KWamae at kemri-wellcome.org  Sun Jul  3 22:47:17 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Sun, 3 Jul 2016 20:47:17 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <CAGxFJbQLxm-vULXHS2euC1S0TT4PUb1_303y5xj+NxpNXwjWBg@mail.gmail.com>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
	<944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
	<415F1412-7D4B-4CBA-A75B-AE295A6E5DFF@kemri-wellcome.org>
	<CAGxFJbQLxm-vULXHS2euC1S0TT4PUb1_303y5xj+NxpNXwjWBg@mail.gmail.com>
Message-ID: <E247E968-E8C1-484D-A966-75C08F04E2CE@kemri-wellcome.org>

Hi Bert, my first task is to make a Kaplan Meier Plot to evaluate the risk of developing disease in the treated vs the non-treated individuals. I therefore figured it might be easier to compute dates first as any further analysis will be based on time, in this case days. I keep getting recommendations on how to tweak my analysis and keeps coming down to dates between the start of drug administration and the end of it.

Can you suggest an ?easier? way to go about this.. 

Regards
-------------------------------------------------------------------------------
Kevin Wame 
 

On 7/3/16, 11:28 PM, "Bert Gunter" <bgunter.4567 at gmail.com> wrote:

I haven't followed this thread closely, but if it's not too late, I
might suggest that you stop worrying about how you want your data
frame to look and start worrying about you want to display/analyze
your data. As Jeff suggested, you and your supervisor are probably
being driven by paradigms from Excel, SPSS, or whatever that are
simply unnecessary for R. My guess would be that if you explained the
sort of analyses/plots you wish to do, you will find it can be done
fairly directly from your existing data. At the very least it would
give Jeff and other helpeRs a better idea of what you might need
rather than what you and your supervisor think you need.


Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Jul 3, 2016 at 1:08 PM, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
> Hi Jeff, It works on well on a dataset with 100000 rows and I figure it will work well with the ?real? dataset. You?ve been of great help and I am starting to make headway.
>
> It creates a new dataframe (result), as shown below that doesn?t quite have the result as I would want it.
>
> ID      admin_period    start   end     ddays
> J1/3    1       5/11/07 8/13/07 94
> J1/3    2       8/13/07 11/12/07        91
> J1/3    3       11/12/07        2/4/08             84
> J1/3    4       2/4/08              5/5/08                  91
> J1/3    5       5/5/08               5/4/09            364
> J1/3    6       5/4/09               5/17/10    378
> J1/3    7       5/17/10 5/16/11 364
> J10/1   1       5/11/07 8/13/07 94
> J10/1   2       8/13/07 11/12/07        91
> J10/1   3       11/12/07        2/4/08              84
> J10/1   4       2/4/08                5/5/08    91
> J10/1   5       5/5/08                5/8/09    368
> J10/1   6       5/8/09               5/17/10    374
> J10/1   7       5/17/10 5/16/11 364
> J102/1  1       5/15/07 8/15/07 92
> J102/1  2       8/15/07 11/13/07        90
> J102/1  3       11/13/07        2/5/08             84
> J102/1  4       2/5/08                5/6/08    91
> J102/1  5       5/6/08                5/5/09    364
> J102/1  6       5/5/09                5/19/10   379
>
> My supervisor doesn?t want me to create a new dataset, she?s afraid I might lose some data?I cannot fight that.
>
> Like you mentioned earlier, I might be mixing up things which I think is what you alluded to earlier.
>
> After consultation with my supervisor, this is what we?ve agreed. For every individual, given the start and end date, create a new column (say, diff_days) and for every row that falls within the range of start and end_date, get the difference between the date in that row and start date and add it to the diff_days column. Below is an example of the result. As it can be seen 5/11/2007 is the start while 2/4/2008 is the end. The diff_days has been populated excluding the end date and that is because that is the start of the study in 2008 that will continue into 2009 and thus from 2/4/2008, I should compute diff_days till 2009 and so no (I hope this makes sense).
>
> ID      date    drug_admin      year    month   diff_days
> R1/3    5/11/2007       Y       2007    5       0
> R1/3    5/16/2007               2007    5       6
> R1/3    5/22/2007               2007    5       11
> R1/3    5/28/2007               2007    5       17
> R1/3    1/14/2008               2008    1       248
> R1/3    1/21/2008               2008    1       255
> R1/3    1/28/2008               2008    1       263
> R1/3    2/4/2008        Y       2008    2
>
>
> Regards
> -------------------------------------------------------------------------------
> Kevin Wame
>
>
> On 7/3/16, 10:09 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>
> Typo on the second line
>
> result <- (   result0
>           %>% select( -admin_period1 )
>           %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
>                        , by = c( ID="ID", admin_period ="admin_period1" )
>                         )
>           %>% mutate( ddays = end - start )
>           )
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 3, 2016 11:55:14 AM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>>Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.
>>
>>Thanks for the code.  After running it, this is the error I get.
>>
>>Error: cannot join on columns 'admin_period' x 'admin_period1': index
>>out of bounds
>>
>>Regards
>>-------------------------------------------------------------------------------
>>Kevin Wame | Ph.D. Student (IDeAL)
>>KEMRI-Wellcome Trust Collaborative Research Programme
>>Centre for Geographic Medicine Research
>>P.O. Box 230-80108, Kilifi, Kenya
>>
>>
>>On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>>
>>I still get the impression from your mixing of information types that
>>you are thinking like this is Excel.
>>
>>Perhaps something like
>>
>>drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin,
>>drug_study$ID, FUN=cumsum )
>>library(dplyr)
>>result0 <- (   drug_study
>>          %>% filter( 0 != admin_period )
>>          %>% group_by( ID, admin_period )
>>          %>% summarise( start = min( date ) )
>>          %>% mutate( admin_period1 = admin_period -1 )
>>          )
>>result <- (   result0
>>          %>% select( -admin_period )
>>     %>% inner_join( result0 %>% select( ID, admin_period1, end=start )
>>                     , by = c( ID="ID", admin_period ="admin_period1" )
>>                        )
>>          %>% mutate( ddays = end - start )
>>          )
>>--
>>Sent from my phone. Please excuse my brevity.
>>
>>On July 3, 2016 10:24:51 AM PDT, Kevin Wamae
>><KWamae at kemri-wellcome.org> wrote:
>>>HI Jeff, it?s been an uphill task working with the dataset and I am
>>not
>>>the first to complain. Nonetheless, data-cleaning is ongoing and since
>>>I cannot wait for that to get done, I decided to make the most of what
>>>the dataset looks like at this time. It appears the process may take a
>>>while.
>>>
>>>Thanks for the script. From the output, I noticed that ?result?
>>>contains the first and last date for each of the individuals and not
>>>taking into account the variable ?drug-admin?.
>>>
>>>ID        start               end
>>>J1/3      1/5/09      12/25/10
>>>R1/3      1/4/07      12/15/08
>>>R10/1     1/4/07      3/5/12
>>>
>>>My aim is to pick the date, for example in 2007, where drug-admin ==
>>>?Y? as my start and the date in the subsequent year (2008 in this
>>case)
>>>where drug-admin == ?Y? as my end. Then, I should populate the
>>variable
>>>?study_id? with ?start? up to the entry just above the one whose date
>>>matches ?end?, as the output below shows (I hope its structure is
>>>maintained as I have copied it from R-Studio). The goal for now is to
>>>then get difference in days between ?date? and ?study_id? and still
>>get
>>>to keep that column for ?study_id? as I might use it later.
>>>
>>>From the output, it can be seen that for this individual, the dates
>>run
>>>from 2007 to 2008. However, for some individuals, the dates run from
>>>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>>>deal with all the years as the dates range from 2001-2016
>>>
>>>ID    date    drug_admin      year    month   study_id
>>>R1/3  5/11/07 Y       2007    5       5/11/07
>>>R1/3  5/16/07         2007    5       5/11/07
>>>R1/3  5/22/07         2007    5       5/11/07
>>>R1/3  5/28/07         2007    5       5/11/07
>>>R1/3  6/5/07                  2007    6       5/11/07
>>>R1/3  6/11/07         2007    6       5/11/07
>>>R1/3  6/18/07         2007    6       5/11/07
>>>R1/3  6/25/07         2007    6       5/11/07
>>>R1/3  7/2/07                  2007    7       5/11/07
>>>R1/3  7/16/07         2007    7       5/11/07
>>>R1/3  7/29/07         2007    7       5/11/07
>>>R1/3  8/2/07                  2007    8       5/11/07
>>>R1/3  8/7/07                  2007    8       5/11/07
>>>R1/3  8/13/07         2007    8       5/11/07
>>>R1/3  9/18/07         2007    9       5/11/07
>>>R1/3  9/24/07         2007    9       5/11/07
>>>R1/3  10/6/07         2007    10      5/11/07
>>>R1/3  10/8/07         2007    10      5/11/07
>>>R1/3  10/15/07                2007    10      5/11/07
>>>R1/3  10/22/07                2007    10      5/11/07
>>>R1/3  10/29/07                2007    10      5/11/07
>>>R1/3  11/8/07         2007    11      5/11/07
>>>R1/3  11/12/07                2007    11      5/11/07
>>>R1/3  11/19/07                2007    11      5/11/07
>>>R1/3  11/29/07                2007    11      5/11/07
>>>R1/3  12/6/07         2007    12      5/11/07
>>>R1/3  12/10/07                2007    12      5/11/07
>>>R1/3  12/21/07                2007    12      5/11/07
>>>R1/3  1/7/08                  2008    1       5/11/07
>>>R1/3  1/14/08         2008    1       5/11/07
>>>R1/3  1/21/08         2008    1       5/11/07
>>>R1/3  1/28/08         2008    1       5/11/07
>>>R1/3  2/4/08          Y       2008    2
>>>
>>>
>>>Regards
>>>-------------------------------------------------------------------------------
>>>Kevin Wame
>>>
>>>###############################################################
>>>
>>>###############################################################
>>>
>>>
>>>
>>>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:
>>>
>>>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>>>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>>>"ID", "start", "end" ) )
>>>
>>>
>>>______________________________________________________________________
>>>
>>>This e-mail contains information which is confidential. It is intended
>>>only for the use of the named recipient. If you have received this
>>>e-mail in error, please let us know by replying to the sender, and
>>>immediately delete it from your system.  Please note, that in these
>>>circumstances, the use, disclosure, distribution or copying of this
>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>cannot accept any responsibility for the  accuracy or completeness of
>>>this message as it has been transmitted over a public network.
>>Although
>>>the Programme has taken reasonable precautions to ensure no viruses
>>are
>>>present in emails, it cannot accept responsibility for any loss or
>>>damage arising from the use of the email or attachments. Any views
>>>expressed in this message are those of the individual sender, except
>>>where the sender specifically states them to be the views of
>>>KEMRI-Wellcome Trust Programme.
>>>______________________________________________________________________
>>
>>
>>
>>
>>______________________________________________________________________
>>
>>This e-mail contains information which is confidential. It is intended
>>only for the use of the named recipient. If you have received this
>>e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network. Although
>>the Programme has taken reasonable precautions to ensure no viruses are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>______________________________________________________________________
>
>
>
>
> ______________________________________________________________________
>
> This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
> ______________________________________________________________________
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From jdnewmil at dcn.davis.ca.us  Sun Jul  3 23:43:27 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 03 Jul 2016 14:43:27 -0700
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
	| For a Large Dataset
In-Reply-To: <E247E968-E8C1-484D-A966-75C08F04E2CE@kemri-wellcome.org>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
	<944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
	<415F1412-7D4B-4CBA-A75B-AE295A6E5DFF@kemri-wellcome.org>
	<CAGxFJbQLxm-vULXHS2euC1S0TT4PUb1_303y5xj+NxpNXwjWBg@mail.gmail.com>
	<E247E968-E8C1-484D-A966-75C08F04E2CE@kemri-wellcome.org>
Message-ID: <0803F18E-7D77-4A17-8579-96CF86B900AC@dcn.davis.ca.us>

There are a great many hits when I search on the keywords "kaplan meier plot R"... so my first reaction is that you should be referring to some of the existing packages for doing this type of analysis. I do not do this type of analysis normally, so am probably not your best helper... perhaps someone else will chime in if you show that you have read some existing KM examples. 

My second reaction is that if you want to avoid losing records you should also avoid adding records. Your example extends from the first matching date to and including the next matching date, which conflicts with analysis of successive treatment periods. You may have a good reason for doing this, but in my experience this is usually a mistake. 

Finally, I think you should more closely study the use of the ave function that I already used if you want to work with the data in its original form. It should not be too difficult to generate your diff_days column using ave if you have the admin_period column that I showed you how to make. 
-- 
Sent from my phone. Please excuse my brevity.

On July 3, 2016 1:47:17 PM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>Hi Bert, my first task is to make a Kaplan Meier Plot to evaluate the
>risk of developing disease in the treated vs the non-treated
>individuals. I therefore figured it might be easier to compute dates
>first as any further analysis will be based on time, in this case days.
>I keep getting recommendations on how to tweak my analysis and keeps
>coming down to dates between the start of drug administration and the
>end of it.
>
>Can you suggest an ?easier? way to go about this.. 
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Wame 
> 
>
>On 7/3/16, 11:28 PM, "Bert Gunter" <bgunter.4567 at gmail.com> wrote:
>
>I haven't followed this thread closely, but if it's not too late, I
>might suggest that you stop worrying about how you want your data
>frame to look and start worrying about you want to display/analyze
>your data. As Jeff suggested, you and your supervisor are probably
>being driven by paradigms from Excel, SPSS, or whatever that are
>simply unnecessary for R. My guess would be that if you explained the
>sort of analyses/plots you wish to do, you will find it can be done
>fairly directly from your existing data. At the very least it would
>give Jeff and other helpeRs a better idea of what you might need
>rather than what you and your supervisor think you need.
>
>
>Cheers,
>Bert
>
>
>Bert Gunter
>
>"The trouble with having an open mind is that people keep coming along
>and sticking things into it."
>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
>On Sun, Jul 3, 2016 at 1:08 PM, Kevin Wamae <KWamae at kemri-wellcome.org>
>wrote:
>> Hi Jeff, It works on well on a dataset with 100000 rows and I figure
>it will work well with the ?real? dataset. You?ve been of great help
>and I am starting to make headway.
>>
>> It creates a new dataframe (result), as shown below that doesn?t
>quite have the result as I would want it.
>>
>> ID      admin_period    start   end     ddays
>> J1/3    1       5/11/07 8/13/07 94
>> J1/3    2       8/13/07 11/12/07        91
>> J1/3    3       11/12/07        2/4/08             84
>> J1/3    4       2/4/08              5/5/08                  91
>> J1/3    5       5/5/08               5/4/09            364
>> J1/3    6       5/4/09               5/17/10    378
>> J1/3    7       5/17/10 5/16/11 364
>> J10/1   1       5/11/07 8/13/07 94
>> J10/1   2       8/13/07 11/12/07        91
>> J10/1   3       11/12/07        2/4/08              84
>> J10/1   4       2/4/08                5/5/08    91
>> J10/1   5       5/5/08                5/8/09    368
>> J10/1   6       5/8/09               5/17/10    374
>> J10/1   7       5/17/10 5/16/11 364
>> J102/1  1       5/15/07 8/15/07 92
>> J102/1  2       8/15/07 11/13/07        90
>> J102/1  3       11/13/07        2/5/08             84
>> J102/1  4       2/5/08                5/6/08    91
>> J102/1  5       5/6/08                5/5/09    364
>> J102/1  6       5/5/09                5/19/10   379
>>
>> My supervisor doesn?t want me to create a new dataset, she?s afraid I
>might lose some data?I cannot fight that.
>>
>> Like you mentioned earlier, I might be mixing up things which I think
>is what you alluded to earlier.
>>
>> After consultation with my supervisor, this is what we?ve agreed. For
>every individual, given the start and end date, create a new column
>(say, diff_days) and for every row that falls within the range of start
>and end_date, get the difference between the date in that row and start
>date and add it to the diff_days column. Below is an example of the
>result. As it can be seen 5/11/2007 is the start while 2/4/2008 is the
>end. The diff_days has been populated excluding the end date and that
>is because that is the start of the study in 2008 that will continue
>into 2009 and thus from 2/4/2008, I should compute diff_days till 2009
>and so no (I hope this makes sense).
>>
>> ID      date    drug_admin      year    month   diff_days
>> R1/3    5/11/2007       Y       2007    5       0
>> R1/3    5/16/2007               2007    5       6
>> R1/3    5/22/2007               2007    5       11
>> R1/3    5/28/2007               2007    5       17
>> R1/3    1/14/2008               2008    1       248
>> R1/3    1/21/2008               2008    1       255
>> R1/3    1/28/2008               2008    1       263
>> R1/3    2/4/2008        Y       2008    2
>>
>>
>> Regards
>>
>-------------------------------------------------------------------------------
>> Kevin Wame
>>
>>
>> On 7/3/16, 10:09 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>wrote:
>>
>> Typo on the second line
>>
>> result <- (   result0
>>           %>% select( -admin_period1 )
>>           %>% inner_join( result0 %>% select( ID, admin_period1,
>end=start )
>>                        , by = c( ID="ID", admin_period
>="admin_period1" )
>>                         )
>>           %>% mutate( ddays = end - start )
>>           )
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 3, 2016 11:55:14 AM PDT, Kevin Wamae
><KWamae at kemri-wellcome.org> wrote:
>>>Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.
>>>
>>>Thanks for the code.  After running it, this is the error I get.
>>>
>>>Error: cannot join on columns 'admin_period' x 'admin_period1': index
>>>out of bounds
>>>
>>>Regards
>>>-------------------------------------------------------------------------------
>>>Kevin Wame | Ph.D. Student (IDeAL)
>>>KEMRI-Wellcome Trust Collaborative Research Programme
>>>Centre for Geographic Medicine Research
>>>P.O. Box 230-80108, Kilifi, Kenya
>>>
>>>
>>>On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>wrote:
>>>
>>>I still get the impression from your mixing of information types that
>>>you are thinking like this is Excel.
>>>
>>>Perhaps something like
>>>
>>>drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin,
>>>drug_study$ID, FUN=cumsum )
>>>library(dplyr)
>>>result0 <- (   drug_study
>>>          %>% filter( 0 != admin_period )
>>>          %>% group_by( ID, admin_period )
>>>          %>% summarise( start = min( date ) )
>>>          %>% mutate( admin_period1 = admin_period -1 )
>>>          )
>>>result <- (   result0
>>>          %>% select( -admin_period )
>>>     %>% inner_join( result0 %>% select( ID, admin_period1, end=start
>)
>>>                     , by = c( ID="ID", admin_period ="admin_period1"
>)
>>>                        )
>>>          %>% mutate( ddays = end - start )
>>>          )
>>>--
>>>Sent from my phone. Please excuse my brevity.
>>>
>>>On July 3, 2016 10:24:51 AM PDT, Kevin Wamae
>>><KWamae at kemri-wellcome.org> wrote:
>>>>HI Jeff, it?s been an uphill task working with the dataset and I am
>>>not
>>>>the first to complain. Nonetheless, data-cleaning is ongoing and
>since
>>>>I cannot wait for that to get done, I decided to make the most of
>what
>>>>the dataset looks like at this time. It appears the process may take
>a
>>>>while.
>>>>
>>>>Thanks for the script. From the output, I noticed that ?result?
>>>>contains the first and last date for each of the individuals and not
>>>>taking into account the variable ?drug-admin?.
>>>>
>>>>ID        start               end
>>>>J1/3      1/5/09      12/25/10
>>>>R1/3      1/4/07      12/15/08
>>>>R10/1     1/4/07      3/5/12
>>>>
>>>>My aim is to pick the date, for example in 2007, where drug-admin ==
>>>>?Y? as my start and the date in the subsequent year (2008 in this
>>>case)
>>>>where drug-admin == ?Y? as my end. Then, I should populate the
>>>variable
>>>>?study_id? with ?start? up to the entry just above the one whose
>date
>>>>matches ?end?, as the output below shows (I hope its structure is
>>>>maintained as I have copied it from R-Studio). The goal for now is
>to
>>>>then get difference in days between ?date? and ?study_id? and still
>>>get
>>>>to keep that column for ?study_id? as I might use it later.
>>>>
>>>>From the output, it can be seen that for this individual, the dates
>>>run
>>>>from 2007 to 2008. However, for some individuals, the dates run from
>>>>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>>>>deal with all the years as the dates range from 2001-2016
>>>>
>>>>ID    date    drug_admin      year    month   study_id
>>>>R1/3  5/11/07 Y       2007    5       5/11/07
>>>>R1/3  5/16/07         2007    5       5/11/07
>>>>R1/3  5/22/07         2007    5       5/11/07
>>>>R1/3  5/28/07         2007    5       5/11/07
>>>>R1/3  6/5/07                  2007    6       5/11/07
>>>>R1/3  6/11/07         2007    6       5/11/07
>>>>R1/3  6/18/07         2007    6       5/11/07
>>>>R1/3  6/25/07         2007    6       5/11/07
>>>>R1/3  7/2/07                  2007    7       5/11/07
>>>>R1/3  7/16/07         2007    7       5/11/07
>>>>R1/3  7/29/07         2007    7       5/11/07
>>>>R1/3  8/2/07                  2007    8       5/11/07
>>>>R1/3  8/7/07                  2007    8       5/11/07
>>>>R1/3  8/13/07         2007    8       5/11/07
>>>>R1/3  9/18/07         2007    9       5/11/07
>>>>R1/3  9/24/07         2007    9       5/11/07
>>>>R1/3  10/6/07         2007    10      5/11/07
>>>>R1/3  10/8/07         2007    10      5/11/07
>>>>R1/3  10/15/07                2007    10      5/11/07
>>>>R1/3  10/22/07                2007    10      5/11/07
>>>>R1/3  10/29/07                2007    10      5/11/07
>>>>R1/3  11/8/07         2007    11      5/11/07
>>>>R1/3  11/12/07                2007    11      5/11/07
>>>>R1/3  11/19/07                2007    11      5/11/07
>>>>R1/3  11/29/07                2007    11      5/11/07
>>>>R1/3  12/6/07         2007    12      5/11/07
>>>>R1/3  12/10/07                2007    12      5/11/07
>>>>R1/3  12/21/07                2007    12      5/11/07
>>>>R1/3  1/7/08                  2008    1       5/11/07
>>>>R1/3  1/14/08         2008    1       5/11/07
>>>>R1/3  1/21/08         2008    1       5/11/07
>>>>R1/3  1/28/08         2008    1       5/11/07
>>>>R1/3  2/4/08          Y       2008    2
>>>>
>>>>
>>>>Regards
>>>>-------------------------------------------------------------------------------
>>>>Kevin Wame
>>>>
>>>>###############################################################
>>>>
>>>>###############################################################
>>>>
>>>>
>>>>
>>>>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>wrote:
>>>>
>>>>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>>>>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>>>>"ID", "start", "end" ) )
>>>>
>>>>
>>>>______________________________________________________________________
>>>>
>>>>This e-mail contains information which is confidential. It is
>intended
>>>>only for the use of the named recipient. If you have received this
>>>>e-mail in error, please let us know by replying to the sender, and
>>>>immediately delete it from your system.  Please note, that in these
>>>>circumstances, the use, disclosure, distribution or copying of this
>>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>>cannot accept any responsibility for the  accuracy or completeness
>of
>>>>this message as it has been transmitted over a public network.
>>>Although
>>>>the Programme has taken reasonable precautions to ensure no viruses
>>>are
>>>>present in emails, it cannot accept responsibility for any loss or
>>>>damage arising from the use of the email or attachments. Any views
>>>>expressed in this message are those of the individual sender, except
>>>>where the sender specifically states them to be the views of
>>>>KEMRI-Wellcome Trust Programme.
>>>>______________________________________________________________________
>>>
>>>
>>>
>>>
>>>______________________________________________________________________
>>>
>>>This e-mail contains information which is confidential. It is
>intended
>>>only for the use of the named recipient. If you have received this
>>>e-mail in error, please let us know by replying to the sender, and
>>>immediately delete it from your system.  Please note, that in these
>>>circumstances, the use, disclosure, distribution or copying of this
>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>cannot accept any responsibility for the  accuracy or completeness of
>>>this message as it has been transmitted over a public network.
>Although
>>>the Programme has taken reasonable precautions to ensure no viruses
>are
>>>present in emails, it cannot accept responsibility for any loss or
>>>damage arising from the use of the email or attachments. Any views
>>>expressed in this message are those of the individual sender, except
>>>where the sender specifically states them to be the views of
>>>KEMRI-Wellcome Trust Programme.
>>>______________________________________________________________________
>>
>>
>>
>>
>>
>______________________________________________________________________
>>
>> This e-mail contains information which is confidential. It is
>intended only for the use of the named recipient. If you have received
>this e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>>
>______________________________________________________________________
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________


From kenobressem at gmail.com  Sun Jul  3 20:27:02 2016
From: kenobressem at gmail.com (Keno Kyrill Bressem)
Date: Sun, 3 Jul 2016 20:27:02 +0200
Subject: [R] Comparing two diagnostic tests with lme4
Message-ID: <CAOWypTfvKmnGqQdxiL0tVjYpGXWCpHUnLQCkbJgjCqubxn7gsw@mail.gmail.com>

Dear R experts,

I compare two diagnostic tests. Therfore, I collected patient data from
several studies. The dataframe is similar to this one:

set.seed(10)
data = data.frame( test1 = rbinom(1000, 1, 0.6),
                   test2 = rbinom(1000, 1, 0.4),
                   reference = rbinom(1000, 1, 0.7),
                   study = sort(paste("study_", round(runif(1000, 1, 20),0)
,sep = "")),
                   id = 1:1000,
                   age = round(rnorm(1000, 60, 10), 0))

I did a lot of research on how to use hierarchical models for calculating
the respective sensitivities and specifities for my tests and tried a lot
of variations in the formula of  glmer. However, I don't have sufficient
statistical knowledge for interpreting these models. So I don't know if my
approach is correct. Therfore I am showing you my latest model.

First, I would like to calculate the logit sensitivity and specifity for
each test. In a paper by Genders et al.
<http://pubs.rsna.org/doi/suppl/10.1148/radiol.12120509> (appendices)
<http://pubs.rsna.org/doi/suppl/10.1148/radiol.12120509/suppl_file/12120509appendices.pdf>
a Stata code to calculate the logit sensitivity and specifity is provided.
I transferred this code to "R", but I am not sure if it's correct this way.


m.sen <- glmer(test1 ~ ( 1 | study) + ( 1 | id ), data = subset(data,
reference == 1), family = binomial(link = "logit"),
control = glmerControl(optimizer = "bobyqa"), nAGQ = 1)

# require("useful")
m.spe <-  glmer(binary.flip(test1) ~ ( 1 | study) + ( 1 | id ), data =
subset(data, reference == 0), family = binomial(link = "logit"),
 control = glmerControl(optimizer = "bobyqa"), nAGQ =

logit.sen = fixef(m.sen)
logit.spe = fixef(m.spe)


My first question is if it is possible to calculate the logit sensitivity
and specifity of a diagnostic test like this. The next step would be to
adjust for different patient characteristics, such as age.

data <- within(data, {age = as.factor(round(age, -1))})
m.sen.age <- glmer(test1 ~ age + ( 1 | study) + ( 1 | id ), data =
subset(data, reference == 1),
family = binomial(link = "logit"), control = glmerControl(optimizer =
"bobyqa"), nAGQ = 1)
fix <- fixef(m.sen.age)

Now I add the estimates. For example, to determine the logit sensitivity of
test1 in patients aged between 55 and 65: sen.50 = fix[1] + fix[5]
As an alternative, I thought about further defining the data subset, which
produces nearly identically results.

m.sen.age <- glmer(test1 ~ ( 1 | study) + ( 1 | id ), data = subset(data,
reference == 1 & age == 60),
family = binomial(link = "logit"), control = glmerControl(optimizer =
"bobyqa"), nAGQ = 1)
sen.age50 = fixef(m.sen.age)

Thank you very much in advance
kb

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Jul  4 08:32:27 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 3 Jul 2016 23:32:27 -0700
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <0803F18E-7D77-4A17-8579-96CF86B900AC@dcn.davis.ca.us>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
	<944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
	<415F1412-7D4B-4CBA-A75B-AE295A6E5DFF@kemri-wellcome.org>
	<CAGxFJbQLxm-vULXHS2euC1S0TT4PUb1_303y5xj+NxpNXwjWBg@mail.gmail.com>
	<E247E968-E8C1-484D-A966-75C08F04E2CE@kemri-wellcome.org>
	<0803F18E-7D77-4A17-8579-96CF86B900AC@dcn.davis.ca.us>
Message-ID: <CAGxFJbT6_xtGaCtiAtggetYtCJVUAuzToMr4hsybk-y5ODsaCw@mail.gmail.com>

A kaplan-meier plot requires for each individual (in each treatment
group, if there are more than one):

1. Survival time,which in your case appears to mean time without disease;
2. Status at end of time on study: whether the individual was censored
(still without disease) or died (in your case, was diseased) on the
last date they are seen in the study.

AFAICT, the 2nd piece of information is not present in your data; if
this is so, then you cannot do the K-M plot or, indeed, any survival
analysis. That is, you can quit the analysis right now.

If you have the status, where is it?

If, for example, the last date for each individual is the date at
which disease is first seen, then you can simply convert the date
column to the Date class with ?as.Date (the year and month columns
appear to be useless as they repeat info already available in the date
columns), and then:

survtimes_byID <- with(datasetname, tapply(date, ID, function(x)diff(range(x))))

will give you a list of survival times (in days) by ID. See ?with,
?tapply for details.

If the status info is in some other form, then this advice should be
ignored of course and you have to incorporate it into your data in
some other way.


Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Jul 3, 2016 at 2:43 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> There are a great many hits when I search on the keywords "kaplan meier plot R"... so my first reaction is that you should be referring to some of the existing packages for doing this type of analysis. I do not do this type of analysis normally, so am probably not your best helper... perhaps someone else will chime in if you show that you have read some existing KM examples.
>
> My second reaction is that if you want to avoid losing records you should also avoid adding records. Your example extends from the first matching date to and including the next matching date, which conflicts with analysis of successive treatment periods. You may have a good reason for doing this, but in my experience this is usually a mistake.
>
> Finally, I think you should more closely study the use of the ave function that I already used if you want to work with the data in its original form. It should not be too difficult to generate your diff_days column using ave if you have the admin_period column that I showed you how to make.
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 3, 2016 1:47:17 PM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>>Hi Bert, my first task is to make a Kaplan Meier Plot to evaluate the
>>risk of developing disease in the treated vs the non-treated
>>individuals. I therefore figured it might be easier to compute dates
>>first as any further analysis will be based on time, in this case days.
>>I keep getting recommendations on how to tweak my analysis and keeps
>>coming down to dates between the start of drug administration and the
>>end of it.
>>
>>Can you suggest an ?easier? way to go about this..
>>
>>Regards
>>-------------------------------------------------------------------------------
>>Kevin Wame
>>
>>
>>On 7/3/16, 11:28 PM, "Bert Gunter" <bgunter.4567 at gmail.com> wrote:
>>
>>I haven't followed this thread closely, but if it's not too late, I
>>might suggest that you stop worrying about how you want your data
>>frame to look and start worrying about you want to display/analyze
>>your data. As Jeff suggested, you and your supervisor are probably
>>being driven by paradigms from Excel, SPSS, or whatever that are
>>simply unnecessary for R. My guess would be that if you explained the
>>sort of analyses/plots you wish to do, you will find it can be done
>>fairly directly from your existing data. At the very least it would
>>give Jeff and other helpeRs a better idea of what you might need
>>rather than what you and your supervisor think you need.
>>
>>
>>Cheers,
>>Bert
>>
>>
>>Bert Gunter
>>
>>"The trouble with having an open mind is that people keep coming along
>>and sticking things into it."
>>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>>On Sun, Jul 3, 2016 at 1:08 PM, Kevin Wamae <KWamae at kemri-wellcome.org>
>>wrote:
>>> Hi Jeff, It works on well on a dataset with 100000 rows and I figure
>>it will work well with the ?real? dataset. You?ve been of great help
>>and I am starting to make headway.
>>>
>>> It creates a new dataframe (result), as shown below that doesn?t
>>quite have the result as I would want it.
>>>
>>> ID      admin_period    start   end     ddays
>>> J1/3    1       5/11/07 8/13/07 94
>>> J1/3    2       8/13/07 11/12/07        91
>>> J1/3    3       11/12/07        2/4/08             84
>>> J1/3    4       2/4/08              5/5/08                  91
>>> J1/3    5       5/5/08               5/4/09            364
>>> J1/3    6       5/4/09               5/17/10    378
>>> J1/3    7       5/17/10 5/16/11 364
>>> J10/1   1       5/11/07 8/13/07 94
>>> J10/1   2       8/13/07 11/12/07        91
>>> J10/1   3       11/12/07        2/4/08              84
>>> J10/1   4       2/4/08                5/5/08    91
>>> J10/1   5       5/5/08                5/8/09    368
>>> J10/1   6       5/8/09               5/17/10    374
>>> J10/1   7       5/17/10 5/16/11 364
>>> J102/1  1       5/15/07 8/15/07 92
>>> J102/1  2       8/15/07 11/13/07        90
>>> J102/1  3       11/13/07        2/5/08             84
>>> J102/1  4       2/5/08                5/6/08    91
>>> J102/1  5       5/6/08                5/5/09    364
>>> J102/1  6       5/5/09                5/19/10   379
>>>
>>> My supervisor doesn?t want me to create a new dataset, she?s afraid I
>>might lose some data?I cannot fight that.
>>>
>>> Like you mentioned earlier, I might be mixing up things which I think
>>is what you alluded to earlier.
>>>
>>> After consultation with my supervisor, this is what we?ve agreed. For
>>every individual, given the start and end date, create a new column
>>(say, diff_days) and for every row that falls within the range of start
>>and end_date, get the difference between the date in that row and start
>>date and add it to the diff_days column. Below is an example of the
>>result. As it can be seen 5/11/2007 is the start while 2/4/2008 is the
>>end. The diff_days has been populated excluding the end date and that
>>is because that is the start of the study in 2008 that will continue
>>into 2009 and thus from 2/4/2008, I should compute diff_days till 2009
>>and so no (I hope this makes sense).
>>>
>>> ID      date    drug_admin      year    month   diff_days
>>> R1/3    5/11/2007       Y       2007    5       0
>>> R1/3    5/16/2007               2007    5       6
>>> R1/3    5/22/2007               2007    5       11
>>> R1/3    5/28/2007               2007    5       17
>>> R1/3    1/14/2008               2008    1       248
>>> R1/3    1/21/2008               2008    1       255
>>> R1/3    1/28/2008               2008    1       263
>>> R1/3    2/4/2008        Y       2008    2
>>>
>>>
>>> Regards
>>>
>>-------------------------------------------------------------------------------
>>> Kevin Wame
>>>
>>>
>>> On 7/3/16, 10:09 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>>wrote:
>>>
>>> Typo on the second line
>>>
>>> result <- (   result0
>>>           %>% select( -admin_period1 )
>>>           %>% inner_join( result0 %>% select( ID, admin_period1,
>>end=start )
>>>                        , by = c( ID="ID", admin_period
>>="admin_period1" )
>>>                         )
>>>           %>% mutate( ddays = end - start )
>>>           )
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On July 3, 2016 11:55:14 AM PDT, Kevin Wamae
>><KWamae at kemri-wellcome.org> wrote:
>>>>Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.
>>>>
>>>>Thanks for the code.  After running it, this is the error I get.
>>>>
>>>>Error: cannot join on columns 'admin_period' x 'admin_period1': index
>>>>out of bounds
>>>>
>>>>Regards
>>>>-------------------------------------------------------------------------------
>>>>Kevin Wame | Ph.D. Student (IDeAL)
>>>>KEMRI-Wellcome Trust Collaborative Research Programme
>>>>Centre for Geographic Medicine Research
>>>>P.O. Box 230-80108, Kilifi, Kenya
>>>>
>>>>
>>>>On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>>wrote:
>>>>
>>>>I still get the impression from your mixing of information types that
>>>>you are thinking like this is Excel.
>>>>
>>>>Perhaps something like
>>>>
>>>>drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin,
>>>>drug_study$ID, FUN=cumsum )
>>>>library(dplyr)
>>>>result0 <- (   drug_study
>>>>          %>% filter( 0 != admin_period )
>>>>          %>% group_by( ID, admin_period )
>>>>          %>% summarise( start = min( date ) )
>>>>          %>% mutate( admin_period1 = admin_period -1 )
>>>>          )
>>>>result <- (   result0
>>>>          %>% select( -admin_period )
>>>>     %>% inner_join( result0 %>% select( ID, admin_period1, end=start
>>)
>>>>                     , by = c( ID="ID", admin_period ="admin_period1"
>>)
>>>>                        )
>>>>          %>% mutate( ddays = end - start )
>>>>          )
>>>>--
>>>>Sent from my phone. Please excuse my brevity.
>>>>
>>>>On July 3, 2016 10:24:51 AM PDT, Kevin Wamae
>>>><KWamae at kemri-wellcome.org> wrote:
>>>>>HI Jeff, it?s been an uphill task working with the dataset and I am
>>>>not
>>>>>the first to complain. Nonetheless, data-cleaning is ongoing and
>>since
>>>>>I cannot wait for that to get done, I decided to make the most of
>>what
>>>>>the dataset looks like at this time. It appears the process may take
>>a
>>>>>while.
>>>>>
>>>>>Thanks for the script. From the output, I noticed that ?result?
>>>>>contains the first and last date for each of the individuals and not
>>>>>taking into account the variable ?drug-admin?.
>>>>>
>>>>>ID        start               end
>>>>>J1/3      1/5/09      12/25/10
>>>>>R1/3      1/4/07      12/15/08
>>>>>R10/1     1/4/07      3/5/12
>>>>>
>>>>>My aim is to pick the date, for example in 2007, where drug-admin ==
>>>>>?Y? as my start and the date in the subsequent year (2008 in this
>>>>case)
>>>>>where drug-admin == ?Y? as my end. Then, I should populate the
>>>>variable
>>>>>?study_id? with ?start? up to the entry just above the one whose
>>date
>>>>>matches ?end?, as the output below shows (I hope its structure is
>>>>>maintained as I have copied it from R-Studio). The goal for now is
>>to
>>>>>then get difference in days between ?date? and ?study_id? and still
>>>>get
>>>>>to keep that column for ?study_id? as I might use it later.
>>>>>
>>>>>From the output, it can be seen that for this individual, the dates
>>>>run
>>>>>from 2007 to 2008. However, for some individuals, the dates run from
>>>>>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>>>>>deal with all the years as the dates range from 2001-2016
>>>>>
>>>>>ID    date    drug_admin      year    month   study_id
>>>>>R1/3  5/11/07 Y       2007    5       5/11/07
>>>>>R1/3  5/16/07         2007    5       5/11/07
>>>>>R1/3  5/22/07         2007    5       5/11/07
>>>>>R1/3  5/28/07         2007    5       5/11/07
>>>>>R1/3  6/5/07                  2007    6       5/11/07
>>>>>R1/3  6/11/07         2007    6       5/11/07
>>>>>R1/3  6/18/07         2007    6       5/11/07
>>>>>R1/3  6/25/07         2007    6       5/11/07
>>>>>R1/3  7/2/07                  2007    7       5/11/07
>>>>>R1/3  7/16/07         2007    7       5/11/07
>>>>>R1/3  7/29/07         2007    7       5/11/07
>>>>>R1/3  8/2/07                  2007    8       5/11/07
>>>>>R1/3  8/7/07                  2007    8       5/11/07
>>>>>R1/3  8/13/07         2007    8       5/11/07
>>>>>R1/3  9/18/07         2007    9       5/11/07
>>>>>R1/3  9/24/07         2007    9       5/11/07
>>>>>R1/3  10/6/07         2007    10      5/11/07
>>>>>R1/3  10/8/07         2007    10      5/11/07
>>>>>R1/3  10/15/07                2007    10      5/11/07
>>>>>R1/3  10/22/07                2007    10      5/11/07
>>>>>R1/3  10/29/07                2007    10      5/11/07
>>>>>R1/3  11/8/07         2007    11      5/11/07
>>>>>R1/3  11/12/07                2007    11      5/11/07
>>>>>R1/3  11/19/07                2007    11      5/11/07
>>>>>R1/3  11/29/07                2007    11      5/11/07
>>>>>R1/3  12/6/07         2007    12      5/11/07
>>>>>R1/3  12/10/07                2007    12      5/11/07
>>>>>R1/3  12/21/07                2007    12      5/11/07
>>>>>R1/3  1/7/08                  2008    1       5/11/07
>>>>>R1/3  1/14/08         2008    1       5/11/07
>>>>>R1/3  1/21/08         2008    1       5/11/07
>>>>>R1/3  1/28/08         2008    1       5/11/07
>>>>>R1/3  2/4/08          Y       2008    2
>>>>>
>>>>>
>>>>>Regards
>>>>>-------------------------------------------------------------------------------
>>>>>Kevin Wame
>>>>>
>>>>>###############################################################
>>>>>
>>>>>###############################################################
>>>>>
>>>>>
>>>>>
>>>>>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>>wrote:
>>>>>
>>>>>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>>>>>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>>>>>"ID", "start", "end" ) )
>>>>>
>>>>>
>>>>>______________________________________________________________________
>>>>>
>>>>>This e-mail contains information which is confidential. It is
>>intended
>>>>>only for the use of the named recipient. If you have received this
>>>>>e-mail in error, please let us know by replying to the sender, and
>>>>>immediately delete it from your system.  Please note, that in these
>>>>>circumstances, the use, disclosure, distribution or copying of this
>>>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>>>cannot accept any responsibility for the  accuracy or completeness
>>of
>>>>>this message as it has been transmitted over a public network.
>>>>Although
>>>>>the Programme has taken reasonable precautions to ensure no viruses
>>>>are
>>>>>present in emails, it cannot accept responsibility for any loss or
>>>>>damage arising from the use of the email or attachments. Any views
>>>>>expressed in this message are those of the individual sender, except
>>>>>where the sender specifically states them to be the views of
>>>>>KEMRI-Wellcome Trust Programme.
>>>>>______________________________________________________________________
>>>>
>>>>
>>>>
>>>>
>>>>______________________________________________________________________
>>>>
>>>>This e-mail contains information which is confidential. It is
>>intended
>>>>only for the use of the named recipient. If you have received this
>>>>e-mail in error, please let us know by replying to the sender, and
>>>>immediately delete it from your system.  Please note, that in these
>>>>circumstances, the use, disclosure, distribution or copying of this
>>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>>cannot accept any responsibility for the  accuracy or completeness of
>>>>this message as it has been transmitted over a public network.
>>Although
>>>>the Programme has taken reasonable precautions to ensure no viruses
>>are
>>>>present in emails, it cannot accept responsibility for any loss or
>>>>damage arising from the use of the email or attachments. Any views
>>>>expressed in this message are those of the individual sender, except
>>>>where the sender specifically states them to be the views of
>>>>KEMRI-Wellcome Trust Programme.
>>>>______________________________________________________________________
>>>
>>>
>>>
>>>
>>>
>>______________________________________________________________________
>>>
>>> This e-mail contains information which is confidential. It is
>>intended only for the use of the named recipient. If you have received
>>this e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network. Although
>>the Programme has taken reasonable precautions to ensure no viruses are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>>
>>______________________________________________________________________
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>______________________________________________________________________
>>
>>This e-mail contains information which is confidential. It is intended
>>only for the use of the named recipient. If you have received this
>>e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network. Although
>>the Programme has taken reasonable precautions to ensure no viruses are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>______________________________________________________________________
>


From KWamae at kemri-wellcome.org  Mon Jul  4 08:48:07 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Mon, 4 Jul 2016 06:48:07 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <0803F18E-7D77-4A17-8579-96CF86B900AC@dcn.davis.ca.us>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
	<944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
	<415F1412-7D4B-4CBA-A75B-AE295A6E5DFF@kemri-wellcome.org>
	<CAGxFJbQLxm-vULXHS2euC1S0TT4PUb1_303y5xj+NxpNXwjWBg@mail.gmail.com>
	<E247E968-E8C1-484D-A966-75C08F04E2CE@kemri-wellcome.org>
	<0803F18E-7D77-4A17-8579-96CF86B900AC@dcn.davis.ca.us>
Message-ID: <C8665527-CDB2-427D-842B-02FFE40397D2@kemri-wellcome.org>

Hi Jeff, thanks and I will explore your suggestions too..

Regards
-------------------------------------------------------------------------------
Kevin Wame 

 

On 7/4/16, 12:43 AM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us> wrote:

There are a great many hits when I search on the keywords "kaplan meier plot R"... so my first reaction is that you should be referring to some of the existing packages for doing this type of analysis. I do not do this type of analysis normally, so am probably not your best helper... perhaps someone else will chime in if you show that you have read some existing KM examples. 

My second reaction is that if you want to avoid losing records you should also avoid adding records. Your example extends from the first matching date to and including the next matching date, which conflicts with analysis of successive treatment periods. You may have a good reason for doing this, but in my experience this is usually a mistake. 

Finally, I think you should more closely study the use of the ave function that I already used if you want to work with the data in its original form. It should not be too difficult to generate your diff_days column using ave if you have the admin_period column that I showed you how to make. 
-- 
Sent from my phone. Please excuse my brevity.

On July 3, 2016 1:47:17 PM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>Hi Bert, my first task is to make a Kaplan Meier Plot to evaluate the
>risk of developing disease in the treated vs the non-treated
>individuals. I therefore figured it might be easier to compute dates
>first as any further analysis will be based on time, in this case days.
>I keep getting recommendations on how to tweak my analysis and keeps
>coming down to dates between the start of drug administration and the
>end of it.
>
>Can you suggest an ?easier? way to go about this.. 
>
>Regards
>-------------------------------------------------------------------------------
>Kevin Wame 
> 
>
>On 7/3/16, 11:28 PM, "Bert Gunter" <bgunter.4567 at gmail.com> wrote:
>
>I haven't followed this thread closely, but if it's not too late, I
>might suggest that you stop worrying about how you want your data
>frame to look and start worrying about you want to display/analyze
>your data. As Jeff suggested, you and your supervisor are probably
>being driven by paradigms from Excel, SPSS, or whatever that are
>simply unnecessary for R. My guess would be that if you explained the
>sort of analyses/plots you wish to do, you will find it can be done
>fairly directly from your existing data. At the very least it would
>give Jeff and other helpeRs a better idea of what you might need
>rather than what you and your supervisor think you need.
>
>
>Cheers,
>Bert
>
>
>Bert Gunter
>
>"The trouble with having an open mind is that people keep coming along
>and sticking things into it."
>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
>On Sun, Jul 3, 2016 at 1:08 PM, Kevin Wamae <KWamae at kemri-wellcome.org>
>wrote:
>> Hi Jeff, It works on well on a dataset with 100000 rows and I figure
>it will work well with the ?real? dataset. You?ve been of great help
>and I am starting to make headway.
>>
>> It creates a new dataframe (result), as shown below that doesn?t
>quite have the result as I would want it.
>>
>> ID      admin_period    start   end     ddays
>> J1/3    1       5/11/07 8/13/07 94
>> J1/3    2       8/13/07 11/12/07        91
>> J1/3    3       11/12/07        2/4/08             84
>> J1/3    4       2/4/08              5/5/08                  91
>> J1/3    5       5/5/08               5/4/09            364
>> J1/3    6       5/4/09               5/17/10    378
>> J1/3    7       5/17/10 5/16/11 364
>> J10/1   1       5/11/07 8/13/07 94
>> J10/1   2       8/13/07 11/12/07        91
>> J10/1   3       11/12/07        2/4/08              84
>> J10/1   4       2/4/08                5/5/08    91
>> J10/1   5       5/5/08                5/8/09    368
>> J10/1   6       5/8/09               5/17/10    374
>> J10/1   7       5/17/10 5/16/11 364
>> J102/1  1       5/15/07 8/15/07 92
>> J102/1  2       8/15/07 11/13/07        90
>> J102/1  3       11/13/07        2/5/08             84
>> J102/1  4       2/5/08                5/6/08    91
>> J102/1  5       5/6/08                5/5/09    364
>> J102/1  6       5/5/09                5/19/10   379
>>
>> My supervisor doesn?t want me to create a new dataset, she?s afraid I
>might lose some data?I cannot fight that.
>>
>> Like you mentioned earlier, I might be mixing up things which I think
>is what you alluded to earlier.
>>
>> After consultation with my supervisor, this is what we?ve agreed. For
>every individual, given the start and end date, create a new column
>(say, diff_days) and for every row that falls within the range of start
>and end_date, get the difference between the date in that row and start
>date and add it to the diff_days column. Below is an example of the
>result. As it can be seen 5/11/2007 is the start while 2/4/2008 is the
>end. The diff_days has been populated excluding the end date and that
>is because that is the start of the study in 2008 that will continue
>into 2009 and thus from 2/4/2008, I should compute diff_days till 2009
>and so no (I hope this makes sense).
>>
>> ID      date    drug_admin      year    month   diff_days
>> R1/3    5/11/2007       Y       2007    5       0
>> R1/3    5/16/2007               2007    5       6
>> R1/3    5/22/2007               2007    5       11
>> R1/3    5/28/2007               2007    5       17
>> R1/3    1/14/2008               2008    1       248
>> R1/3    1/21/2008               2008    1       255
>> R1/3    1/28/2008               2008    1       263
>> R1/3    2/4/2008        Y       2008    2
>>
>>
>> Regards
>>
>-------------------------------------------------------------------------------
>> Kevin Wame
>>
>>
>> On 7/3/16, 10:09 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>wrote:
>>
>> Typo on the second line
>>
>> result <- (   result0
>>           %>% select( -admin_period1 )
>>           %>% inner_join( result0 %>% select( ID, admin_period1,
>end=start )
>>                        , by = c( ID="ID", admin_period
>="admin_period1" )
>>                         )
>>           %>% mutate( ddays = end - start )
>>           )
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 3, 2016 11:55:14 AM PDT, Kevin Wamae
><KWamae at kemri-wellcome.org> wrote:
>>>Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.
>>>
>>>Thanks for the code.  After running it, this is the error I get.
>>>
>>>Error: cannot join on columns 'admin_period' x 'admin_period1': index
>>>out of bounds
>>>
>>>Regards
>>>-------------------------------------------------------------------------------
>>>Kevin Wame | Ph.D. Student (IDeAL)
>>>KEMRI-Wellcome Trust Collaborative Research Programme
>>>Centre for Geographic Medicine Research
>>>P.O. Box 230-80108, Kilifi, Kenya
>>>
>>>
>>>On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>wrote:
>>>
>>>I still get the impression from your mixing of information types that
>>>you are thinking like this is Excel.
>>>
>>>Perhaps something like
>>>
>>>drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin,
>>>drug_study$ID, FUN=cumsum )
>>>library(dplyr)
>>>result0 <- (   drug_study
>>>          %>% filter( 0 != admin_period )
>>>          %>% group_by( ID, admin_period )
>>>          %>% summarise( start = min( date ) )
>>>          %>% mutate( admin_period1 = admin_period -1 )
>>>          )
>>>result <- (   result0
>>>          %>% select( -admin_period )
>>>     %>% inner_join( result0 %>% select( ID, admin_period1, end=start
>)
>>>                     , by = c( ID="ID", admin_period ="admin_period1"
>)
>>>                        )
>>>          %>% mutate( ddays = end - start )
>>>          )
>>>--
>>>Sent from my phone. Please excuse my brevity.
>>>
>>>On July 3, 2016 10:24:51 AM PDT, Kevin Wamae
>>><KWamae at kemri-wellcome.org> wrote:
>>>>HI Jeff, it?s been an uphill task working with the dataset and I am
>>>not
>>>>the first to complain. Nonetheless, data-cleaning is ongoing and
>since
>>>>I cannot wait for that to get done, I decided to make the most of
>what
>>>>the dataset looks like at this time. It appears the process may take
>a
>>>>while.
>>>>
>>>>Thanks for the script. From the output, I noticed that ?result?
>>>>contains the first and last date for each of the individuals and not
>>>>taking into account the variable ?drug-admin?.
>>>>
>>>>ID        start               end
>>>>J1/3      1/5/09      12/25/10
>>>>R1/3      1/4/07      12/15/08
>>>>R10/1     1/4/07      3/5/12
>>>>
>>>>My aim is to pick the date, for example in 2007, where drug-admin ==
>>>>?Y? as my start and the date in the subsequent year (2008 in this
>>>case)
>>>>where drug-admin == ?Y? as my end. Then, I should populate the
>>>variable
>>>>?study_id? with ?start? up to the entry just above the one whose
>date
>>>>matches ?end?, as the output below shows (I hope its structure is
>>>>maintained as I have copied it from R-Studio). The goal for now is
>to
>>>>then get difference in days between ?date? and ?study_id? and still
>>>get
>>>>to keep that column for ?study_id? as I might use it later.
>>>>
>>>>From the output, it can be seen that for this individual, the dates
>>>run
>>>>from 2007 to 2008. However, for some individuals, the dates run from
>>>>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>>>>deal with all the years as the dates range from 2001-2016
>>>>
>>>>ID    date    drug_admin      year    month   study_id
>>>>R1/3  5/11/07 Y       2007    5       5/11/07
>>>>R1/3  5/16/07         2007    5       5/11/07
>>>>R1/3  5/22/07         2007    5       5/11/07
>>>>R1/3  5/28/07         2007    5       5/11/07
>>>>R1/3  6/5/07                  2007    6       5/11/07
>>>>R1/3  6/11/07         2007    6       5/11/07
>>>>R1/3  6/18/07         2007    6       5/11/07
>>>>R1/3  6/25/07         2007    6       5/11/07
>>>>R1/3  7/2/07                  2007    7       5/11/07
>>>>R1/3  7/16/07         2007    7       5/11/07
>>>>R1/3  7/29/07         2007    7       5/11/07
>>>>R1/3  8/2/07                  2007    8       5/11/07
>>>>R1/3  8/7/07                  2007    8       5/11/07
>>>>R1/3  8/13/07         2007    8       5/11/07
>>>>R1/3  9/18/07         2007    9       5/11/07
>>>>R1/3  9/24/07         2007    9       5/11/07
>>>>R1/3  10/6/07         2007    10      5/11/07
>>>>R1/3  10/8/07         2007    10      5/11/07
>>>>R1/3  10/15/07                2007    10      5/11/07
>>>>R1/3  10/22/07                2007    10      5/11/07
>>>>R1/3  10/29/07                2007    10      5/11/07
>>>>R1/3  11/8/07         2007    11      5/11/07
>>>>R1/3  11/12/07                2007    11      5/11/07
>>>>R1/3  11/19/07                2007    11      5/11/07
>>>>R1/3  11/29/07                2007    11      5/11/07
>>>>R1/3  12/6/07         2007    12      5/11/07
>>>>R1/3  12/10/07                2007    12      5/11/07
>>>>R1/3  12/21/07                2007    12      5/11/07
>>>>R1/3  1/7/08                  2008    1       5/11/07
>>>>R1/3  1/14/08         2008    1       5/11/07
>>>>R1/3  1/21/08         2008    1       5/11/07
>>>>R1/3  1/28/08         2008    1       5/11/07
>>>>R1/3  2/4/08          Y       2008    2
>>>>
>>>>
>>>>Regards
>>>>-------------------------------------------------------------------------------
>>>>Kevin Wame
>>>>
>>>>###############################################################
>>>>
>>>>###############################################################
>>>>
>>>>
>>>>
>>>>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>wrote:
>>>>
>>>>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>>>>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>>>>"ID", "start", "end" ) )
>>>>
>>>>
>>>>______________________________________________________________________
>>>>
>>>>This e-mail contains information which is confidential. It is
>intended
>>>>only for the use of the named recipient. If you have received this
>>>>e-mail in error, please let us know by replying to the sender, and
>>>>immediately delete it from your system.  Please note, that in these
>>>>circumstances, the use, disclosure, distribution or copying of this
>>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>>cannot accept any responsibility for the  accuracy or completeness
>of
>>>>this message as it has been transmitted over a public network.
>>>Although
>>>>the Programme has taken reasonable precautions to ensure no viruses
>>>are
>>>>present in emails, it cannot accept responsibility for any loss or
>>>>damage arising from the use of the email or attachments. Any views
>>>>expressed in this message are those of the individual sender, except
>>>>where the sender specifically states them to be the views of
>>>>KEMRI-Wellcome Trust Programme.
>>>>______________________________________________________________________
>>>
>>>
>>>
>>>
>>>______________________________________________________________________
>>>
>>>This e-mail contains information which is confidential. It is
>intended
>>>only for the use of the named recipient. If you have received this
>>>e-mail in error, please let us know by replying to the sender, and
>>>immediately delete it from your system.  Please note, that in these
>>>circumstances, the use, disclosure, distribution or copying of this
>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>cannot accept any responsibility for the  accuracy or completeness of
>>>this message as it has been transmitted over a public network.
>Although
>>>the Programme has taken reasonable precautions to ensure no viruses
>are
>>>present in emails, it cannot accept responsibility for any loss or
>>>damage arising from the use of the email or attachments. Any views
>>>expressed in this message are those of the individual sender, except
>>>where the sender specifically states them to be the views of
>>>KEMRI-Wellcome Trust Programme.
>>>______________________________________________________________________
>>
>>
>>
>>
>>
>______________________________________________________________________
>>
>> This e-mail contains information which is confidential. It is
>intended only for the use of the named recipient. If you have received
>this e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>>
>______________________________________________________________________
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>______________________________________________________________________
>
>This e-mail contains information which is confidential. It is intended
>only for the use of the named recipient. If you have received this
>e-mail in error, please let us know by replying to the sender, and
>immediately delete it from your system.  Please note, that in these
>circumstances, the use, disclosure, distribution or copying of this
>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>cannot accept any responsibility for the  accuracy or completeness of
>this message as it has been transmitted over a public network. Although
>the Programme has taken reasonable precautions to ensure no viruses are
>present in emails, it cannot accept responsibility for any loss or
>damage arising from the use of the email or attachments. Any views
>expressed in this message are those of the individual sender, except
>where the sender specifically states them to be the views of
>KEMRI-Wellcome Trust Programme.
>______________________________________________________________________




______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From KWamae at kemri-wellcome.org  Mon Jul  4 08:57:10 2016
From: KWamae at kemri-wellcome.org (Kevin Wamae)
Date: Mon, 4 Jul 2016 06:57:10 +0000
Subject: [R] R - Populate Another Variable Based on Multiple Conditions
 | For a Large Dataset
In-Reply-To: <CAGxFJbT6_xtGaCtiAtggetYtCJVUAuzToMr4hsybk-y5ODsaCw@mail.gmail.com>
References: <d22a26de-b66f-455c-81d1-0304414547a9@dcn.davis.ca.us>
	<9cc47b74-f7d5-445a-911e-643539593f89@dcn.davis.ca.us>
	<3e0119f6-a962-4210-97c0-515630a04ea9@kemri-wellcome.org>
	<1a5b5702-2764-47ee-b6a0-cfe590064198@kemri-wellcome.org>
	<2F8C3FB2309.00000089jrkrideau@inbox.com>
	<20591F90-6E6F-4953-B916-BABCE6A5517F@kemri-wellcome.org>
	<07B51B7F-8502-431A-B9EA-D9F38131F5DC@dcn.davis.ca.us>
	<F96FB515-B702-4CEB-8D0C-3BC3F4CEC857@kemri-wellcome.org>
	<9A5AA157-E35D-4F00-BE63-8173DE7857F7@dcn.davis.ca.us>
	<58A38FFE-219F-4288-A8CA-C311786338FF@kemri-wellcome.org>
	<944C16EB-BB3D-4862-9A76-DBF95AE5F075@dcn.davis.ca.us>
	<415F1412-7D4B-4CBA-A75B-AE295A6E5DFF@kemri-wellcome.org>
	<CAGxFJbQLxm-vULXHS2euC1S0TT4PUb1_303y5xj+NxpNXwjWBg@mail.gmail.com>
	<E247E968-E8C1-484D-A966-75C08F04E2CE@kemri-wellcome.org>
	<0803F18E-7D77-4A17-8579-96CF86B900AC@dcn.davis.ca.us>
	<CAGxFJbT6_xtGaCtiAtggetYtCJVUAuzToMr4hsybk-y5ODsaCw@mail.gmail.com>
Message-ID: <B0A33C82-EFC5-4566-8498-DCF47A11029D@kemri-wellcome.org>

Hi Bert, The ?status? at the end of the study does exist in the original dataset, what was missing was the time between events. And there exists so many events that fall between the first and last day to be explored in this work.

The suggestion I received then, was to compute time between the initial date for each individual and all sub subsequent events, up to the last day of the study. The rationale being, once I have that column of difference in days, I can then use it to make any other calculations that arise.

Let me try your suggested script and see how that goes..highly appreciated..

Regards
-------------------------------------------------------------------------------
Kevin Wame 
 

On 7/4/16, 9:32 AM, "Bert Gunter" <bgunter.4567 at gmail.com> wrote:

A kaplan-meier plot requires for each individual (in each treatment
group, if there are more than one):

1. Survival time,which in your case appears to mean time without disease;
2. Status at end of time on study: whether the individual was censored
(still without disease) or died (in your case, was diseased) on the
last date they are seen in the study.

AFAICT, the 2nd piece of information is not present in your data; if
this is so, then you cannot do the K-M plot or, indeed, any survival
analysis. That is, you can quit the analysis right now.

If you have the status, where is it?

If, for example, the last date for each individual is the date at
which disease is first seen, then you can simply convert the date
column to the Date class with ?as.Date (the year and month columns
appear to be useless as they repeat info already available in the date
columns), and then:

survtimes_byID <- with(datasetname, tapply(date, ID, function(x)diff(range(x))))

will give you a list of survival times (in days) by ID. See ?with,
?tapply for details.

If the status info is in some other form, then this advice should be
ignored of course and you have to incorporate it into your data in
some other way.


Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Jul 3, 2016 at 2:43 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> There are a great many hits when I search on the keywords "kaplan meier plot R"... so my first reaction is that you should be referring to some of the existing packages for doing this type of analysis. I do not do this type of analysis normally, so am probably not your best helper... perhaps someone else will chime in if you show that you have read some existing KM examples.
>
> My second reaction is that if you want to avoid losing records you should also avoid adding records. Your example extends from the first matching date to and including the next matching date, which conflicts with analysis of successive treatment periods. You may have a good reason for doing this, but in my experience this is usually a mistake.
>
> Finally, I think you should more closely study the use of the ave function that I already used if you want to work with the data in its original form. It should not be too difficult to generate your diff_days column using ave if you have the admin_period column that I showed you how to make.
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 3, 2016 1:47:17 PM PDT, Kevin Wamae <KWamae at kemri-wellcome.org> wrote:
>>Hi Bert, my first task is to make a Kaplan Meier Plot to evaluate the
>>risk of developing disease in the treated vs the non-treated
>>individuals. I therefore figured it might be easier to compute dates
>>first as any further analysis will be based on time, in this case days.
>>I keep getting recommendations on how to tweak my analysis and keeps
>>coming down to dates between the start of drug administration and the
>>end of it.
>>
>>Can you suggest an ?easier? way to go about this..
>>
>>Regards
>>-------------------------------------------------------------------------------
>>Kevin Wame
>>
>>
>>On 7/3/16, 11:28 PM, "Bert Gunter" <bgunter.4567 at gmail.com> wrote:
>>
>>I haven't followed this thread closely, but if it's not too late, I
>>might suggest that you stop worrying about how you want your data
>>frame to look and start worrying about you want to display/analyze
>>your data. As Jeff suggested, you and your supervisor are probably
>>being driven by paradigms from Excel, SPSS, or whatever that are
>>simply unnecessary for R. My guess would be that if you explained the
>>sort of analyses/plots you wish to do, you will find it can be done
>>fairly directly from your existing data. At the very least it would
>>give Jeff and other helpeRs a better idea of what you might need
>>rather than what you and your supervisor think you need.
>>
>>
>>Cheers,
>>Bert
>>
>>
>>Bert Gunter
>>
>>"The trouble with having an open mind is that people keep coming along
>>and sticking things into it."
>>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>>On Sun, Jul 3, 2016 at 1:08 PM, Kevin Wamae <KWamae at kemri-wellcome.org>
>>wrote:
>>> Hi Jeff, It works on well on a dataset with 100000 rows and I figure
>>it will work well with the ?real? dataset. You?ve been of great help
>>and I am starting to make headway.
>>>
>>> It creates a new dataframe (result), as shown below that doesn?t
>>quite have the result as I would want it.
>>>
>>> ID      admin_period    start   end     ddays
>>> J1/3    1       5/11/07 8/13/07 94
>>> J1/3    2       8/13/07 11/12/07        91
>>> J1/3    3       11/12/07        2/4/08             84
>>> J1/3    4       2/4/08              5/5/08                  91
>>> J1/3    5       5/5/08               5/4/09            364
>>> J1/3    6       5/4/09               5/17/10    378
>>> J1/3    7       5/17/10 5/16/11 364
>>> J10/1   1       5/11/07 8/13/07 94
>>> J10/1   2       8/13/07 11/12/07        91
>>> J10/1   3       11/12/07        2/4/08              84
>>> J10/1   4       2/4/08                5/5/08    91
>>> J10/1   5       5/5/08                5/8/09    368
>>> J10/1   6       5/8/09               5/17/10    374
>>> J10/1   7       5/17/10 5/16/11 364
>>> J102/1  1       5/15/07 8/15/07 92
>>> J102/1  2       8/15/07 11/13/07        90
>>> J102/1  3       11/13/07        2/5/08             84
>>> J102/1  4       2/5/08                5/6/08    91
>>> J102/1  5       5/6/08                5/5/09    364
>>> J102/1  6       5/5/09                5/19/10   379
>>>
>>> My supervisor doesn?t want me to create a new dataset, she?s afraid I
>>might lose some data?I cannot fight that.
>>>
>>> Like you mentioned earlier, I might be mixing up things which I think
>>is what you alluded to earlier.
>>>
>>> After consultation with my supervisor, this is what we?ve agreed. For
>>every individual, given the start and end date, create a new column
>>(say, diff_days) and for every row that falls within the range of start
>>and end_date, get the difference between the date in that row and start
>>date and add it to the diff_days column. Below is an example of the
>>result. As it can be seen 5/11/2007 is the start while 2/4/2008 is the
>>end. The diff_days has been populated excluding the end date and that
>>is because that is the start of the study in 2008 that will continue
>>into 2009 and thus from 2/4/2008, I should compute diff_days till 2009
>>and so no (I hope this makes sense).
>>>
>>> ID      date    drug_admin      year    month   diff_days
>>> R1/3    5/11/2007       Y       2007    5       0
>>> R1/3    5/16/2007               2007    5       6
>>> R1/3    5/22/2007               2007    5       11
>>> R1/3    5/28/2007               2007    5       17
>>> R1/3    1/14/2008               2008    1       248
>>> R1/3    1/21/2008               2008    1       255
>>> R1/3    1/28/2008               2008    1       263
>>> R1/3    2/4/2008        Y       2008    2
>>>
>>>
>>> Regards
>>>
>>-------------------------------------------------------------------------------
>>> Kevin Wame
>>>
>>>
>>> On 7/3/16, 10:09 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>>wrote:
>>>
>>> Typo on the second line
>>>
>>> result <- (   result0
>>>           %>% select( -admin_period1 )
>>>           %>% inner_join( result0 %>% select( ID, admin_period1,
>>end=start )
>>>                        , by = c( ID="ID", admin_period
>>="admin_period1" )
>>>                         )
>>>           %>% mutate( ddays = end - start )
>>>           )
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On July 3, 2016 11:55:14 AM PDT, Kevin Wamae
>><KWamae at kemri-wellcome.org> wrote:
>>>>Hi Jeff, ?likes its Excel?, I don?t follow. Pardon me for any mix up.
>>>>
>>>>Thanks for the code.  After running it, this is the error I get.
>>>>
>>>>Error: cannot join on columns 'admin_period' x 'admin_period1': index
>>>>out of bounds
>>>>
>>>>Regards
>>>>-------------------------------------------------------------------------------
>>>>Kevin Wame | Ph.D. Student (IDeAL)
>>>>KEMRI-Wellcome Trust Collaborative Research Programme
>>>>Centre for Geographic Medicine Research
>>>>P.O. Box 230-80108, Kilifi, Kenya
>>>>
>>>>
>>>>On 7/3/16, 9:34 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>>wrote:
>>>>
>>>>I still get the impression from your mixing of information types that
>>>>you are thinking like this is Excel.
>>>>
>>>>Perhaps something like
>>>>
>>>>drug_study$admin_period  <- ave( "Y" == drug_study$drug_admin,
>>>>drug_study$ID, FUN=cumsum )
>>>>library(dplyr)
>>>>result0 <- (   drug_study
>>>>          %>% filter( 0 != admin_period )
>>>>          %>% group_by( ID, admin_period )
>>>>          %>% summarise( start = min( date ) )
>>>>          %>% mutate( admin_period1 = admin_period -1 )
>>>>          )
>>>>result <- (   result0
>>>>          %>% select( -admin_period )
>>>>     %>% inner_join( result0 %>% select( ID, admin_period1, end=start
>>)
>>>>                     , by = c( ID="ID", admin_period ="admin_period1"
>>)
>>>>                        )
>>>>          %>% mutate( ddays = end - start )
>>>>          )
>>>>--
>>>>Sent from my phone. Please excuse my brevity.
>>>>
>>>>On July 3, 2016 10:24:51 AM PDT, Kevin Wamae
>>>><KWamae at kemri-wellcome.org> wrote:
>>>>>HI Jeff, it?s been an uphill task working with the dataset and I am
>>>>not
>>>>>the first to complain. Nonetheless, data-cleaning is ongoing and
>>since
>>>>>I cannot wait for that to get done, I decided to make the most of
>>what
>>>>>the dataset looks like at this time. It appears the process may take
>>a
>>>>>while.
>>>>>
>>>>>Thanks for the script. From the output, I noticed that ?result?
>>>>>contains the first and last date for each of the individuals and not
>>>>>taking into account the variable ?drug-admin?.
>>>>>
>>>>>ID        start               end
>>>>>J1/3      1/5/09      12/25/10
>>>>>R1/3      1/4/07      12/15/08
>>>>>R10/1     1/4/07      3/5/12
>>>>>
>>>>>My aim is to pick the date, for example in 2007, where drug-admin ==
>>>>>?Y? as my start and the date in the subsequent year (2008 in this
>>>>case)
>>>>>where drug-admin == ?Y? as my end. Then, I should populate the
>>>>variable
>>>>>?study_id? with ?start? up to the entry just above the one whose
>>date
>>>>>matches ?end?, as the output below shows (I hope its structure is
>>>>>maintained as I have copied it from R-Studio). The goal for now is
>>to
>>>>>then get difference in days between ?date? and ?study_id? and still
>>>>get
>>>>>to keep that column for ?study_id? as I might use it later.
>>>>>
>>>>>From the output, it can be seen that for this individual, the dates
>>>>run
>>>>>from 2007 to 2008. However, for some individuals, the dates run from
>>>>>2008-2009, 2009-2010 and so on. Therefore, I need to make the script
>>>>>deal with all the years as the dates range from 2001-2016
>>>>>
>>>>>ID    date    drug_admin      year    month   study_id
>>>>>R1/3  5/11/07 Y       2007    5       5/11/07
>>>>>R1/3  5/16/07         2007    5       5/11/07
>>>>>R1/3  5/22/07         2007    5       5/11/07
>>>>>R1/3  5/28/07         2007    5       5/11/07
>>>>>R1/3  6/5/07                  2007    6       5/11/07
>>>>>R1/3  6/11/07         2007    6       5/11/07
>>>>>R1/3  6/18/07         2007    6       5/11/07
>>>>>R1/3  6/25/07         2007    6       5/11/07
>>>>>R1/3  7/2/07                  2007    7       5/11/07
>>>>>R1/3  7/16/07         2007    7       5/11/07
>>>>>R1/3  7/29/07         2007    7       5/11/07
>>>>>R1/3  8/2/07                  2007    8       5/11/07
>>>>>R1/3  8/7/07                  2007    8       5/11/07
>>>>>R1/3  8/13/07         2007    8       5/11/07
>>>>>R1/3  9/18/07         2007    9       5/11/07
>>>>>R1/3  9/24/07         2007    9       5/11/07
>>>>>R1/3  10/6/07         2007    10      5/11/07
>>>>>R1/3  10/8/07         2007    10      5/11/07
>>>>>R1/3  10/15/07                2007    10      5/11/07
>>>>>R1/3  10/22/07                2007    10      5/11/07
>>>>>R1/3  10/29/07                2007    10      5/11/07
>>>>>R1/3  11/8/07         2007    11      5/11/07
>>>>>R1/3  11/12/07                2007    11      5/11/07
>>>>>R1/3  11/19/07                2007    11      5/11/07
>>>>>R1/3  11/29/07                2007    11      5/11/07
>>>>>R1/3  12/6/07         2007    12      5/11/07
>>>>>R1/3  12/10/07                2007    12      5/11/07
>>>>>R1/3  12/21/07                2007    12      5/11/07
>>>>>R1/3  1/7/08                  2008    1       5/11/07
>>>>>R1/3  1/14/08         2008    1       5/11/07
>>>>>R1/3  1/21/08         2008    1       5/11/07
>>>>>R1/3  1/28/08         2008    1       5/11/07
>>>>>R1/3  2/4/08          Y       2008    2
>>>>>
>>>>>
>>>>>Regards
>>>>>-------------------------------------------------------------------------------
>>>>>Kevin Wame
>>>>>
>>>>>###############################################################
>>>>>
>>>>>###############################################################
>>>>>
>>>>>
>>>>>
>>>>>On 7/3/16, 7:05 PM, "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>>wrote:
>>>>>
>>>>>result <- setNames( data.frame( aggregate( date~ID, data=drug_study,
>>>>>FUN=min ),  aggregate( date~ID, data=drug_study, FUN=max )[2] ), c(
>>>>>"ID", "start", "end" ) )
>>>>>
>>>>>
>>>>>______________________________________________________________________
>>>>>
>>>>>This e-mail contains information which is confidential. It is
>>intended
>>>>>only for the use of the named recipient. If you have received this
>>>>>e-mail in error, please let us know by replying to the sender, and
>>>>>immediately delete it from your system.  Please note, that in these
>>>>>circumstances, the use, disclosure, distribution or copying of this
>>>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>>>cannot accept any responsibility for the  accuracy or completeness
>>of
>>>>>this message as it has been transmitted over a public network.
>>>>Although
>>>>>the Programme has taken reasonable precautions to ensure no viruses
>>>>are
>>>>>present in emails, it cannot accept responsibility for any loss or
>>>>>damage arising from the use of the email or attachments. Any views
>>>>>expressed in this message are those of the individual sender, except
>>>>>where the sender specifically states them to be the views of
>>>>>KEMRI-Wellcome Trust Programme.
>>>>>______________________________________________________________________
>>>>
>>>>
>>>>
>>>>
>>>>______________________________________________________________________
>>>>
>>>>This e-mail contains information which is confidential. It is
>>intended
>>>>only for the use of the named recipient. If you have received this
>>>>e-mail in error, please let us know by replying to the sender, and
>>>>immediately delete it from your system.  Please note, that in these
>>>>circumstances, the use, disclosure, distribution or copying of this
>>>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>>>cannot accept any responsibility for the  accuracy or completeness of
>>>>this message as it has been transmitted over a public network.
>>Although
>>>>the Programme has taken reasonable precautions to ensure no viruses
>>are
>>>>present in emails, it cannot accept responsibility for any loss or
>>>>damage arising from the use of the email or attachments. Any views
>>>>expressed in this message are those of the individual sender, except
>>>>where the sender specifically states them to be the views of
>>>>KEMRI-Wellcome Trust Programme.
>>>>______________________________________________________________________
>>>
>>>
>>>
>>>
>>>
>>______________________________________________________________________
>>>
>>> This e-mail contains information which is confidential. It is
>>intended only for the use of the named recipient. If you have received
>>this e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network. Although
>>the Programme has taken reasonable precautions to ensure no viruses are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>>
>>______________________________________________________________________
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>______________________________________________________________________
>>
>>This e-mail contains information which is confidential. It is intended
>>only for the use of the named recipient. If you have received this
>>e-mail in error, please let us know by replying to the sender, and
>>immediately delete it from your system.  Please note, that in these
>>circumstances, the use, disclosure, distribution or copying of this
>>information is strictly prohibited. KEMRI-Wellcome Trust Programme
>>cannot accept any responsibility for the  accuracy or completeness of
>>this message as it has been transmitted over a public network. Although
>>the Programme has taken reasonable precautions to ensure no viruses are
>>present in emails, it cannot accept responsibility for any loss or
>>damage arising from the use of the email or attachments. Any views
>>expressed in this message are those of the individual sender, except
>>where the sender specifically states them to be the views of
>>KEMRI-Wellcome Trust Programme.
>>______________________________________________________________________
>



______________________________________________________________________

This e-mail contains information which is confidential. It is intended only for the use of the named recipient. If you have received this e-mail in error, please let us know by replying to the sender, and immediately delete it from your system.  Please note, that in these circumstances, the use, disclosure, distribution or copying of this information is strictly prohibited. KEMRI-Wellcome Trust Programme cannot accept any responsibility for the  accuracy or completeness of this message as it has been transmitted over a public network. Although the Programme has taken reasonable precautions to ensure no viruses are present in emails, it cannot accept responsibility for any loss or damage arising from the use of the email or attachments. Any views expressed in this message are those of the individual sender, except where the sender specifically states them to be the views of KEMRI-Wellcome Trust Programme.
______________________________________________________________________

From lists at dewey.myzen.co.uk  Mon Jul  4 09:44:05 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Mon, 4 Jul 2016 08:44:05 +0100
Subject: [R] Comparing two diagnostic tests with lme4
In-Reply-To: <CAOWypTfvKmnGqQdxiL0tVjYpGXWCpHUnLQCkbJgjCqubxn7gsw@mail.gmail.com>
References: <CAOWypTfvKmnGqQdxiL0tVjYpGXWCpHUnLQCkbJgjCqubxn7gsw@mail.gmail.com>
Message-ID: <5dbb7629-4680-e9c4-e9dc-dba825086684@dewey.myzen.co.uk>

Dear Keno

I am not sure from your description what your scientific question is but 
if you want to bring together a number of diagnostic test results and 
then see the influence of moderator variables you might want to 
investigate packages directed at meta-analysis. I have used mada but 
there are others mentioned in the CRAN Task View on MetaAnalysis.

Apologies if I have misunderstood you.

On 03/07/2016 19:27, Keno Kyrill Bressem wrote:
> Dear R experts,
>
> I compare two diagnostic tests. Therfore, I collected patient data from
> several studies. The dataframe is similar to this one:
>
> set.seed(10)
> data = data.frame( test1 = rbinom(1000, 1, 0.6),
>                    test2 = rbinom(1000, 1, 0.4),
>                    reference = rbinom(1000, 1, 0.7),
>                    study = sort(paste("study_", round(runif(1000, 1, 20),0)
> ,sep = "")),
>                    id = 1:1000,
>                    age = round(rnorm(1000, 60, 10), 0))
>
> I did a lot of research on how to use hierarchical models for calculating
> the respective sensitivities and specifities for my tests and tried a lot
> of variations in the formula of  glmer. However, I don't have sufficient
> statistical knowledge for interpreting these models. So I don't know if my
> approach is correct. Therfore I am showing you my latest model.
>
> First, I would like to calculate the logit sensitivity and specifity for
> each test. In a paper by Genders et al.
> <http://pubs.rsna.org/doi/suppl/10.1148/radiol.12120509> (appendices)
> <http://pubs.rsna.org/doi/suppl/10.1148/radiol.12120509/suppl_file/12120509appendices.pdf>
> a Stata code to calculate the logit sensitivity and specifity is provided.
> I transferred this code to "R", but I am not sure if it's correct this way.
>
>
> m.sen <- glmer(test1 ~ ( 1 | study) + ( 1 | id ), data = subset(data,
> reference == 1), family = binomial(link = "logit"),
> control = glmerControl(optimizer = "bobyqa"), nAGQ = 1)
>
> # require("useful")
> m.spe <-  glmer(binary.flip(test1) ~ ( 1 | study) + ( 1 | id ), data =
> subset(data, reference == 0), family = binomial(link = "logit"),
>  control = glmerControl(optimizer = "bobyqa"), nAGQ =
>
> logit.sen = fixef(m.sen)
> logit.spe = fixef(m.spe)
>
>
> My first question is if it is possible to calculate the logit sensitivity
> and specifity of a diagnostic test like this. The next step would be to
> adjust for different patient characteristics, such as age.
>
> data <- within(data, {age = as.factor(round(age, -1))})
> m.sen.age <- glmer(test1 ~ age + ( 1 | study) + ( 1 | id ), data =
> subset(data, reference == 1),
> family = binomial(link = "logit"), control = glmerControl(optimizer =
> "bobyqa"), nAGQ = 1)
> fix <- fixef(m.sen.age)
>
> Now I add the estimates. For example, to determine the logit sensitivity of
> test1 in patients aged between 55 and 65: sen.50 = fix[1] + fix[5]
> As an alternative, I thought about further defining the data subset, which
> produces nearly identically results.
>
> m.sen.age <- glmer(test1 ~ ( 1 | study) + ( 1 | id ), data = subset(data,
> reference == 1 & age == 60),
> family = binomial(link = "logit"), control = glmerControl(optimizer =
> "bobyqa"), nAGQ = 1)
> sen.age50 = fixef(m.sen.age)
>
> Thank you very much in advance
> kb
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From G.Maubach at weinwolf.de  Mon Jul  4 10:34:36 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Mon, 4 Jul 2016 10:34:36 +0200
Subject: [R] Dump of new Methods
Message-ID: <OF208D2DB1.2FFC966E-ONC1257FE6.002EB229-C1257FE6.002F2816@lotus.hawesko.de>

Dear Readers,
Hi All,

to drive my R knowlegde a bit further I followed the advice of some of you 
by reading Chambers: Programming with data.

I tried some examples from the book:

-- cut --

setClass("track", representation (x = "numeric",
                                    y = "numeric"))

track <- function(x, y) {
  # an object representing measurements 'y', tracked at positions 'x'
  x <- as(x, "numeric")
  y <- as(y, "numeric")
  if(length(x) != length(y)) {
    stop("x, y should have equal length!")
  }
  new("track", x = x, y = y)
}

dumpMethod("track", "track")

setMethod("show", "track",
          function(object) {
            xy = rbind(object at x, object at y)
            dimanmes(xy) = list(c("x", "y"),
                                1:ncol(y))
            show(xy)
          })

setMethod("plot",
          signature(x = "track", y = "missing"),
          function(x, y, ...)
            plot(unclass(x), xlab = "Position", ylab = "Value", ...)
          )

dumpMethod("plot", "track")

-- cut --

Where do I find the dumped data? Is it in a single file or is every dump 
stored in a separate file? Where is it stored on my drive?

Kind regards

Georg


From hcatbr at yahoo.co.in  Mon Jul  4 14:29:08 2016
From: hcatbr at yahoo.co.in (Hemant Chowdhary)
Date: Mon, 4 Jul 2016 12:29:08 +0000 (UTC)
Subject: [R] Extracting matrix from netCDF file using ncdf4 package
In-Reply-To: <CAGxFJbSYG6UMzqCYEstVbjHkOzAs1dAn=w35ZrSa-1c38M2CUA@mail.gmail.com>
References: <1495675446.736940.1467499393929.JavaMail.yahoo.ref@mail.yahoo.com>
	<1495675446.736940.1467499393929.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbSYG6UMzqCYEstVbjHkOzAs1dAn=w35ZrSa-1c38M2CUA@mail.gmail.com>
Message-ID: <494275392.1231073.1467635348293.JavaMail.yahoo@mail.yahoo.com>

Thank you Bert.
Yes, your suggestion is correct and there is no need to pre-define the matrix and the sapply function works quite fast. This resolves my issue.
Thank you both againHC 

    On Sunday, 3 July 2016 11:38 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
 

 Well, yes, ... but no: there is no need to pre-define the matrix.

The following is still a (interpreted) loop, but it is fast and short.

## ex is the downloaded array, here filled with random numbers

reqX = c(35,35,40,65,95)
reqY = c(2,5,10,112,120)

out <-sapply(seq_along(reqX), function(i)ex[reqX[i],reqY[i],] )

> dim(out)
[1] 365? 5

You might find it useful to go through a (web) tutorial or two to
learn more about such R functionality.
Useful suggestions can be found here: https://www.rstudio.com/online-learning/#R

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Jul 2, 2016 at 3:43 PM, Hemant Chowdhary via R-help
<r-help at r-project.org> wrote:
>? I am working with a 3-dimensional netCDF file having dimensions of X=100, Y=200, T=365.
> My objective is to extract time vectors of a few specific grids that may not be contiguous on X and/or Y.
>
> For example, I want to extract a 5x365 matrix where 5 rows are each vectors of length 365 of 5 specific X,Y combinations.
>
> For this, I am currently using the following
>
> reqX = c(35,35,40,65,95);
> reqY = c(2,5,10,112,120,120);
> nD = length(reqX)
> for(i in 1:nD){
> idX = ncvar_get(nc=myNC, varid="myVar", start=c(reqX[i],reqY[i]), count=c(1,1))
> if(i==1){dX = idX} else {dX = rbind(dX,idX)}
> }
>
> Is there more elegant/faster way other than to using a For Loop like this? It seems very slow when I may have to get much larger matrix where nD can be more than 1000.
>
> Thank you HC
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


  
	[[alternative HTML version deleted]]


From maicel at infomed.sld.cu  Mon Jul  4 15:56:45 2016
From: maicel at infomed.sld.cu (maicel at infomed.sld.cu)
Date: Mon, 04 Jul 2016 09:56:45 -0400
Subject: [R] dplyr : row total for all groups in dplyr summarise
Message-ID: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>

Hello,
How can I aggregate row total for all groups in dplyr summarise ?
library(dplyr)
mtcars %>%
   group_by (am, gear) %>%
   summarise (n=n()) %>%
   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))

best regard
Maicel Monzon



----------------------------------------------------------------




--
Este mensaje le ha llegado mediante el servicio de correo electronico que ofrece Infomed para respaldar el cumplimiento de las misiones del Sistema Nacional de Salud. La persona que envia este correo asume el compromiso de usar el servicio a tales fines y cumplir con las regulaciones establecidas

Infomed: http://www.sld.cu/


From G.Maubach at weinwolf.de  Mon Jul  4 17:36:08 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Mon, 4 Jul 2016 17:36:08 +0200
Subject: [R] Antwort: Re:  Dump of new Methods (SOLVED)
In-Reply-To: <CAGxFJbQbVc-h2x8GGKom4NEwK2EA2Y6y2b9cTnzA9TWG5KS=Dw@mail.gmail.com>
References: <OF208D2DB1.2FFC966E-ONC1257FE6.002EB229-C1257FE6.002F2816@lotus.hawesko.de>
	<CAGxFJbQbVc-h2x8GGKom4NEwK2EA2Y6y2b9cTnzA9TWG5KS=Dw@mail.gmail.com>
Message-ID: <OF5DBBCD1A.4E712996-ONC1257FE6.0055A00C-C1257FE6.0055BFCA@lotus.hawesko.de>

Hi Bert,

many thanks.

Found them.

Kind regards

Georg




Von:    Bert Gunter <bgunter.4567 at gmail.com>
An:     G.Maubach at weinwolf.de, 
Datum:  04.07.2016 16:43
Betreff:        Re: [R] Dump of new Methods



?getwd

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Jul 4, 2016 at 1:34 AM,  <G.Maubach at weinwolf.de> wrote:
> Dear Readers,
> Hi All,
>
> to drive my R knowlegde a bit further I followed the advice of some of 
you
> by reading Chambers: Programming with data.
>
> I tried some examples from the book:
>
> -- cut --
>
> setClass("track", representation (x = "numeric",
>                                     y = "numeric"))
>
> track <- function(x, y) {
>   # an object representing measurements 'y', tracked at positions 'x'
>   x <- as(x, "numeric")
>   y <- as(y, "numeric")
>   if(length(x) != length(y)) {
>     stop("x, y should have equal length!")
>   }
>   new("track", x = x, y = y)
> }
>
> dumpMethod("track", "track")
>
> setMethod("show", "track",
>           function(object) {
>             xy = rbind(object at x, object at y)
>             dimanmes(xy) = list(c("x", "y"),
>                                 1:ncol(y))
>             show(xy)
>           })
>
> setMethod("plot",
>           signature(x = "track", y = "missing"),
>           function(x, y, ...)
>             plot(unclass(x), xlab = "Position", ylab = "Value", ...)
>           )
>
> dumpMethod("plot", "track")
>
> -- cut --
>
> Where do I find the dumped data? Is it in a single file or is every dump
> stored in a separate file? Where is it stored on my drive?
>
> Kind regards
>
> Georg
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Mon Jul  4 18:09:23 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 4 Jul 2016 09:09:23 -0700
Subject: [R] lineplot.CI xaxis scale change in sciplot?
In-Reply-To: <B3AB066D464A8D48999C6553F24153F96772A636@hall.AD.UWS.EDU.AU>
References: <B3AB066D464A8D48999C6553F24153F96772A636@hall.AD.UWS.EDU.AU>
Message-ID: <B75D9AD3-1B82-4097-B35C-F475CA1460D5@comcast.net>


> On Jun 29, 2016, at 7:04 PM, Clemence Henry <C.Henry at westernsydney.edu.au> wrote:
> 
> Hi,
> 
> I am trying to change the values of the tick marks on the xaxis of the following multipanel plot (see relevant bits of script below) to increments of 50 or to a custom scale (ie. 50, 100, 150, 200, 300...).
> So far I tried using xaxp or xlim both in par() or lineplot.CI(), as well as axTicks and axisTicks but did not get it to work.
> Suggestions?

I don?t see a reproducible example so I?m not giving tested advice, but looking at the code for sciplot::lineplot.CI it appears that it might honor an ?xlim' argument and accept xaxt=?n? and then you should be able to add axTicks as you specify.

If you choose to post again on the matter you should use `dput(object name)` to get a version of data suitable for testing.

? 
David.
> 
> #Plots average A/Ci for each day from ACi
> #Parameters of the panels
> par(mfcol=c(3,2), #row,col
>    mar=c(2,2,1,1), #inner margin (bottom, left, top, right)
>    oma=c(4,4,1,1), #outer margin (bottom, left, top, right)
>    omd=c(0.1,0.8,0.1,0.95), #outer dimensions, values {0-1}, (x1, x2, y1, y2)
>    xpd=NA)
> 
> ...
> 
> 
> #PAR = 1000, Day2
> with(subset1000_2,
>     lineplot.CI(x.factor=Ci.average,
>                 response=Photo,
>                 group=Treatment,
>                 ylab=NA,
>                 xlab=NA,
>                 legend=FALSE,
>                 type="p",
>                 x.cont=TRUE, #continuous x axis (spacing proportional to values)
>                 ylim=c(1,45), #range y axis
>                 err.width=0.05,
>                 pch = c(16,16,16), #symbols shape
>                 col=c("gray84","black","gray48"),
>                 fun=
>     ))
> mtext("Day2, PAR=1000", side = 3, line= -1, adj=0, at=1, cex=0.6) #subtitle
> 
> ....
> 
> #legends
> mtext("Ci", side = 1, line= 1, outer = TRUE, cex=0.7) #x legend
> mtext("Photosynthetic rate", side = 2, line= 1, outer = TRUE, cex=0.7) #y legend
> Thank you kindly for your support.
> 
> Clemence
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Mon Jul  4 18:21:00 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 4 Jul 2016 09:21:00 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
Message-ID: <0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>


> On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
> 
> Hello,
> How can I aggregate row total for all groups in dplyr summarise ?

Row total ? of what? Aggregate ? how? What is the desired answer?



> library(dplyr)
> mtcars %>%
>  group_by (am, gear) %>%
>  summarise (n=n()) %>%
>  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
> 
> best regard
> Maicel Monzon
> 
> 
> 
> ----------------------------------------------------------------
> 
> 
> 
> 
> --
> Este mensaje le ha llegado mediante el servicio de correo electronico que ofrece Infomed para respaldar el cumplimiento de las misiones del Sistema Nacional de Salud. La persona que envia este correo asume el compromiso de usar el servicio a tales fines y cumplir con las regulaciones establecidas
> 
> Infomed: http://www.sld.cu/
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ulrik.stervbo at gmail.com  Mon Jul  4 18:59:17 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Mon, 04 Jul 2016 16:59:17 +0000
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
Message-ID: <CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>

If you want the total number of rows in the original data.frame after
counting the rows in each group, you can ungroup and sum the row counts,
like:

library("dplyr")


mtcars %>%
   group_by (am, gear) %>%
   summarise (n=n()) %>%
   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
   ungroup() %>%
   mutate(row.tot = sum(n))

HTH
Ulrik

On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net> wrote:

>
> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
> >
> > Hello,
> > How can I aggregate row total for all groups in dplyr summarise ?
>
> Row total ? of what? Aggregate ? how? What is the desired answer?
>
>
>
> > library(dplyr)
> > mtcars %>%
> >  group_by (am, gear) %>%
> >  summarise (n=n()) %>%
> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
> >
> > best regard
> > Maicel Monzon
> >
> >
> >
> > ----------------------------------------------------------------
> >
> >
> >
> >
> > --
> > Este mensaje le ha llegado mediante el servicio de correo electronico
> que ofrece Infomed para respaldar el cumplimiento de las misiones del
> Sistema Nacional de Salud. La persona que envia este correo asume el
> compromiso de usar el servicio a tales fines y cumplir con las regulaciones
> establecidas
> >
> > Infomed: http://www.sld.cu/
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From maicel at infomed.sld.cu  Mon Jul  4 19:46:50 2016
From: maicel at infomed.sld.cu (maicel at infomed.sld.cu)
Date: Mon, 04 Jul 2016 13:46:50 -0400
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
Message-ID: <20160704134650.45596x5ynds2ugi2@webmail.sld.cu>

Thank you!!! It's what I needed.
best regard
Maicel Monzon, PHD
Centro Nacional de Ensayos Clinicos


Ulrik Stervbo <ulrik.stervbo at gmail.com> escribi?:

> If you want the total number of rows in the original data.frame after
> counting the rows in each group, you can ungroup and sum the row counts,
> like:
>
> library("dplyr")
>
>
> mtcars %>%
>    group_by (am, gear) %>%
>    summarise (n=n()) %>%
>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>    ungroup() %>%
>    mutate(row.tot = sum(n))
>
> HTH
> Ulrik
>
> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net> wrote:
>
>>
>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>> >
>> > Hello,
>> > How can I aggregate row total for all groups in dplyr summarise ?
>>
>> Row total ? of what? Aggregate ? how? What is the desired answer?
>>
>>
>>
>> > library(dplyr)
>> > mtcars %>%
>> >  group_by (am, gear) %>%
>> >  summarise (n=n()) %>%
>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>> >
>> > best regard
>> > Maicel Monzon
>> >
>> >
>> >
>> > ----------------------------------------------------------------
>> >
>> >
>> >
>> >
>> > --
>> > Este mensaje le ha llegado mediante el servicio de correo electronico
>> que ofrece Infomed para respaldar el cumplimiento de las misiones del
>> Sistema Nacional de Salud. La persona que envia este correo asume el
>> compromiso de usar el servicio a tales fines y cumplir con las regulaciones
>> establecidas
>> >
>> > Infomed: http://www.sld.cu/
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>



----------------------------------------------------------------




--
Este mensaje le ha llegado mediante el servicio de correo electronico que ofrece Infomed para respaldar el cumplimiento de las misiones del Sistema Nacional de Salud. La persona que envia este correo asume el compromiso de usar el servicio a tales fines y cumplir con las regulaciones establecidas

Infomed: http://www.sld.cu/


From juansantomero7432 at yahoo.com  Mon Jul  4 11:36:00 2016
From: juansantomero7432 at yahoo.com (juansantomero7432 at yahoo.com)
Date: Mon, 4 Jul 2016 09:36:00 +0000 (UTC)
Subject: [R] Problem with utils package
References: <2053898661.2343979.1467624960576.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <2053898661.2343979.1467624960576.JavaMail.yahoo@mail.yahoo.com>

Dear List,
I am having the following problem.?
I just installed a new version of R on Ubuntu Trust LTS, following instruction on CRAN website, but the associated utils package which got installed appears only as version 3.0.2.
On an older version of R (3.1.2) which I am running on a Windows machine, the associated utils package is version 3.1.2.
So, my question is why was such an outdated utils package installed as part of a brand new R??? Using this older version of utils is causing me all sorts of problems, and because it is part of the base package, I don't seem to be able to install a newer version of utils. Any help would be greatly appreciated!
many thanks,Juan
Session Information below:
R.Version()
$platform
[1] "x86_64-pc-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] ""

$major
[1] "3"

$minor
[1] "3.1"

$year
[1] "2016"

$month
[1] "06"

$day
[1] "21"

$`svn rev`
[1] "70800"

$language
[1] "R"

$version.string
[1] "R version 3.3.1 (2016-06-21)"

$nickname
[1] "Bug in Your Hair"

> sessionInfo()
R version 3.3.1 (2016-06-21)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.3 LTS

locale:
?[1] LC_CTYPE=en_US.UTF-8?????? LC_NUMERIC=C??????????????
?[3] LC_TIME=zh_CN.UTF-8??????? LC_COLLATE=en_US.UTF-8????
?[5] LC_MONETARY=zh_CN.UTF-8??? LC_MESSAGES=en_US.UTF-8???
?[7] LC_PAPER=zh_CN.UTF-8?????? LC_NAME=C?????????????????
?[9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C????????????
[11] LC_MEASUREMENT=zh_CN.UTF-8 LC_IDENTIFICATION=C???????

attached base packages:
[1] stats???? graphics? grDevices utils???? datasets? methods?? base?????

loaded via a namespace (and not attached):
[1] tools_3.3.1

Sent from Yahoo Mail. Get the app
	[[alternative HTML version deleted]]


From christian at echoffmann.ch  Mon Jul  4 13:42:31 2016
From: christian at echoffmann.ch (Christian Hoffmann)
Date: Mon, 4 Jul 2016 13:42:31 +0200
Subject: [R] Sweave vs. R CMD Rd2pdf --no-clean --force
Message-ID: <2e90cd09-8605-1f54-f98e-811bef61730e@echoffmann.ch>

Hi,

I have a *.Rnw which I can Sweave with no problem, but which cannot be 
processed by  R CMD  Rd2pdf --no-clean --force.

Th latter stops with

creating vignettes ... ERROR
Error: processing vignette 'cwhmisc.Rnw' failed with diagnostics:
at cwhmisc.Rnw:31, could not find function "cformat"
Execution halted

Before dissecting my *.Rnw systematically , I wanted to ask, if  R CMD 
Rd2pdf has changed since the last R version.

sessionInfo()
R version 3.2.1 (2015-06-18)
Platform: x86_64-apple-darwin10.8.0 (64-bit)
Running under: OS X 10.7.5 (Lion)

locale:
[1] C

attached base packages:
  [1] tools     stats4    splines   parallel  datasets
  [6] compiler  graphics  grDevices stats     grid
[11] utils     methods   base

other attached packages:
  [1] survival_2.38-3    spatial_7.3-11
......

Any ideas ?
C.

-- 

Christian W. Hoffmann
CH - 8915 Hausen am Albis, Schweiz
Rigiblickstrasse 15 b, Tel.+41-44-7640853
mailto: christian at echoffmann.ch
home: www.echoffmann.ch


From juansantomero7432 at yahoo.com  Mon Jul  4 13:48:10 2016
From: juansantomero7432 at yahoo.com (juansantomero7432 at yahoo.com)
Date: Mon, 4 Jul 2016 11:48:10 +0000 (UTC)
Subject: [R] Problem with utils package
In-Reply-To: <2053898661.2343979.1467624960576.JavaMail.yahoo@mail.yahoo.com>
References: <2053898661.2343979.1467624960576.JavaMail.yahoo.ref@mail.yahoo.com>
	<2053898661.2343979.1467624960576.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <424550603.2490467.1467632890299.JavaMail.yahoo@mail.yahoo.com>

Dear List,
This is an update to the query I sent earlier: I am having the following problem.?
I just installed a new version of R (3.3.1) on Ubuntu Trusty LTS, following instruction on CRAN website, but the associated packages which are installed as part of the base package (e.g. utils) get installed only as an old version 3.0.2.
So, my question is why, if the installation of R 3.3.1 is successful, as indicated below, that such outdated packages like utils have been installed as part of this brand new R??? Using the older version of e.g. utils is causing me all sorts of problems, and because it is part of the base package, I don't seem to be able to install a newer version of utils. Any help would be greatly appreciated!
many thanks,Juan
Session Information below:
R.Version()
$platform
[1] "x86_64-pc-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] ""

$major
[1] "3"

$minor
[1] "3.1"

$year
[1] "2016"

$month
[1] "06"

$day
[1] "21"

$`svn rev`
[1] "70800"

$language
[1] "R"

$version.string
[1] "R version 3.3.1 (2016-06-21)"

$nickname
[1] "Bug in Your Hair"

> sessionInfo()
R version 3.3.1 (2016-06-21)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.3 LTS

locale:
?[1] LC_CTYPE=en_US.UTF-8?????? LC_NUMERIC=C??????????????
?[3] LC_TIME=zh_CN.UTF-8??????? LC_COLLATE=en_US.UTF-8????
?[5] LC_MONETARY=zh_CN.UTF-8??? LC_MESSAGES=en_US.UTF-8???
?[7] LC_PAPER=zh_CN.UTF-8?????? LC_NAME=C?????????????????
?[9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C????????????
[11] LC_MEASUREMENT=zh_CN.UTF-8 LC_IDENTIFICATION=C???????

attached base packages:
[1] stats???? graphics? grDevices utils???? datasets? methods?? base?????

loaded via a namespace (and not attached):
[1] tools_3.3.1

?

Sent from Yahoo Mail. Get the app 

    On Monday, 4 July 2016, 17:36, "juansantomero7432 at yahoo.com" <juansantomero7432 at yahoo.com> wrote:
 

 Dear List,
I am having the following problem.?
I just installed a new version of R on Ubuntu Trust LTS, following instruction on CRAN website, but the associated utils package which got installed appears only as version 3.0.2.
On an older version of R (3.1.2) which I am running on a Windows machine, the associated utils package is version 3.1.2.
So, my question is why was such an outdated utils package installed as part of a brand new R??? Using this older version of utils is causing me all sorts of problems, and because it is part of the base package, I don't seem to be able to install a newer version of utils. Any help would be greatly appreciated!
many thanks,Juan
Session Information below:
R.Version()
$platform
[1] "x86_64-pc-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] ""

$major
[1] "3"

$minor
[1] "3.1"

$year
[1] "2016"

$month
[1] "06"

$day
[1] "21"

$`svn rev`
[1] "70800"

$language
[1] "R"

$version.string
[1] "R version 3.3.1 (2016-06-21)"

$nickname
[1] "Bug in Your Hair"

> sessionInfo()
R version 3.3.1 (2016-06-21)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.3 LTS

locale:
?[1] LC_CTYPE=en_US.UTF-8?????? LC_NUMERIC=C??????????????
?[3] LC_TIME=zh_CN.UTF-8??????? LC_COLLATE=en_US.UTF-8????
?[5] LC_MONETARY=zh_CN.UTF-8??? LC_MESSAGES=en_US.UTF-8???
?[7] LC_PAPER=zh_CN.UTF-8?????? LC_NAME=C?????????????????
?[9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C????????????
[11] LC_MEASUREMENT=zh_CN.UTF-8 LC_IDENTIFICATION=C???????

attached base packages:
[1] stats???? graphics? grDevices utils???? datasets? methods?? base?????

loaded via a namespace (and not attached):
[1] tools_3.3.1

Sent from Yahoo Mail. Get the app

  
	[[alternative HTML version deleted]]


From w1malik at gmail.com  Mon Jul  4 14:24:01 2016
From: w1malik at gmail.com (Waseem Ali)
Date: Mon, 4 Jul 2016 17:24:01 +0500
Subject: [R] FW: How to read multiple raster and serial correlation between
 series of rasters
In-Reply-To: <SNT151-W34862FD0FE44241E21C057EB580@phx.gbl>
References: <SNT151-W95DFBC94C5A64DB9BCC09DEB440@phx.gbl>,
	<SNT151-W34862FD0FE44241E21C057EB580@phx.gbl>
Message-ID: <SNT151-W95D592F086E45017383B3DEB380@phx.gbl>

I have asked the below mentioned question a month before. Please guide me if the question is not properly asked to the list member. 

Waseem Ali 


From: w1malik at gmail.com
To: r-help at r-project.org
Subject: FW: How to read multiple raster and serial correlation between series of rasters
Date: Thu, 2 Jun 2016 16:10:08 +0500







Dear members,
With little effort in producing the code to read all nc files for Carbon dioxide variables I have produced the following code:
library(ncdf)ncfiles <- list.files("C:/site-download/AIRS/", pattern='\\.nc$', full.names = TRUE)ncfilesncfname <- ncfiles[1]ncfnamedname <- "mole_fraction_of_carbon_dioxide_in_free_troposphere"ncin <- open.ncdf(ncfname)print(ncin)lon <- get.var.ncdf(ncin, "Longitude")nlon <- dim(lon)head(lon)lat <- get.var.ncdf(ncin, "Latitude")nlat <- dim(lat)head(lat)print(c(nlon, nlat))co2array <- get.var.ncdf(ncin, "mole_fraction_of_carbon_dioxide_in_free_troposphere")fillvalue <- att.get.ncdf(ncin, "mole_fraction_of_carbon_dioxide_in_free_troposphere", "Missval")fillvalueco2array[co2array == fillvalue] <- NAlibrary(RColorBrewer)image(co2array, col = rev(brewer.pal(10, "RdBu")))grid <- expand.grid(lon = lon, lat = lat)lonlat <- expand.grid(lon, lat)lonlathead(lonlat)m <- 1co2.vec <- as.vector(co2array)length(co2.vec)co2.df01 <- data.frame(cbind(lonlat, co2.vec*1000000))names(co2.df01) <- c("lon", "lat", paste("co2", as.character(m), sep = "_"))head(na.omit(co2.df01), 20)csvfile <- "co20020901.csv"write.table(na.omit(co2.df01), csvfile, row.names = FALSE, sep = ",")With the help of this code I can dump the values of Carbon dioxide variables in latitude and longitude wise. sample of the csv file is as under:      lon  lat    co2_126  -117.5 89.5 381.802050   -57.5 89.5 373.8030147 -175.0 88.0 382.7285148 -172.5 88.0 373.6800152 -162.5 88.0 371.6560153 -160.0 88.0 373.4450154 -157.5 88.0 374.3705I need help to tune code to do this with iterative code for whole 120 raster to write it into csv file. Column names should be like as 200301, 200302....up to 200312. This starts from the January of 2003 till December of 2003 and so on for the next year.   My next task to do the same for LST variable retrievable from MODIS LST product of 0.05 degree resolution which is in hdf file formate [I am trying to write the code to interpret the variable ]. I want to resample it first to have on same spatial resolution as my first raster then write to the another csv file in the same way before. After writing it into the csv files I want to correlate the both variables and serial correlation. Please guide to do it using R or any other way to deal with the situations I have briefly explained. Also suggest for interpolation for carbon dioxide variable first to make the raster layer. It may have missing some values.
Waseem Ali 

From: w1malik at hotmail.com
To: r-help at r-project.org
Subject: How to real multiple raster and serial correlation between series of rasters
Date: Sun, 29 May 2016 23:39:18 +0500







Hi,
I have 120 raster (10 years) files
in tif format of one variable (say X1) and same numbers for second variables (Say
X2). Each raster consists the mean monthly values of corresponding variables. I
want to write a script in R which operates the following operations:

?        
First reads the one by one
raster from folder and save into the objects. 

?        
Resample/aggregate the both
raster over same spatial resolution 2? x 2.5?.

?        
After resampling the all
raster over same resolution, conversions of all raster to points by using the
rasterToPoint() function of raster library.

?        
After retrieving the same
monthly raster values (like month of January for X1 and X2) into data frame, I
want to compute regression and correlation values for all 120 raster for both
variables (X1 and X2) and save into the data frame.

Is there any way out to deal with
such task.


library(raster)

x <- list.files("C:/site-download/",
pattern = "*.tif", full.names = TRUE)

x1 <- raster(x1)

p <- as(x1, 'SpatialPixels')

plot(x1)

points(p)
Resultant figure has been attached for you for only x1 variable. I have also attached the X1 and X2 variable tif raster for January 2002 for computation purpose. I need to operate it through loop for reading all these rasters and computing the correlation of each pairs. My next step to compute the Lag -1 correlations which is Serial Correlation for both variables.
Waseem Ali 
 		 	   		  
 		 	   		   		 	   		  

From CSANGALLI.external2 at unicredit.eu  Mon Jul  4 16:37:17 2016
From: CSANGALLI.external2 at unicredit.eu (Sangalli Cristiano Giovanni (Ext. - UniCredit Business Integrated
	Solutions))
Date: Mon, 4 Jul 2016 14:37:17 +0000
Subject: [R] R-3.3.1 RPM release
Message-ID: <703384B6C7985C4DBDC3C92ABE98F66C015F62F0B9@USEXCPWD01C.mc01.unicreditgroup.eu>

Dear CRAN,

I have installed on a RHEL 5.7 server the recent R-3.3.0 rpm but I would like to upgrade to the latest now available R-3.3.1 . I have not found so far the rpm, I would like to know if and when it will be available?

Thanks in advance,
Cris


This e-mail is confidential and may also contain privileged information. If you are not the intended recipient you are not authorised to read, print, save, process or disclose this message. If you have received this message by mistake, please inform the sender immediately and delete this e-mail, its attachments and any copies.
Any use, distribution, reproduction or disclosure by any person other than the intended recipient is strictly prohibited and the person responsible may incur penalties.
Thank you!

	[[alternative HTML version deleted]]


From djaffe at uw.edu  Mon Jul  4 20:44:17 2016
From: djaffe at uw.edu (Dan Jaffe)
Date: Mon, 4 Jul 2016 11:44:17 -0700
Subject: [R] GAMS, std errors and confidence intervals
Message-ID: <CAK+Bp3nrWeE2cpvq2dYmsM9Ou2Dsjh_L6q0iWD4vXAxhDTvRtw@mail.gmail.com>

Can anyone help me calculating CIs from a GAM analysis?

I have calculated a GAM fit (m3) and the associated std errors using
predict.gam
I assume that the 95% CI around each fit value would be 1.96
times the se.    But when I do this both on the original and a test
dataset, I find the CI's only encompass about half of the true response
values.     I have  tried this using predict.gam using both type="response"
and type="link"  and get nearly the same result.  What am I  doing wrong?

Here is the code I am using to get the 95% CIs.

se3=predict.gam(m3,dat, type="response", se.fit=TRUE)

upr <- se3$fit + (1.96 * se3$se.fit)
upr <- m3$family$linkinv(upr)
lwr <- se3$fit - (1.96 * se3$se.fit)
lwr <- m3$family$linkinv(lwr)
CI95=upr-lwr
CI95=CI95/2

	[[alternative HTML version deleted]]


From C.Henry at westernsydney.edu.au  Tue Jul  5 01:14:24 2016
From: C.Henry at westernsydney.edu.au (Clemence Henry)
Date: Mon, 4 Jul 2016 23:14:24 +0000
Subject: [R] lineplot.CI xaxis scale change in sciplot?
In-Reply-To: <CA+8X3fVTx_rNZj2y+-5gvv1yM+fQfCuyR8FE9O9VQQE7nfod2w@mail.gmail.com>
References: <B3AB066D464A8D48999C6553F24153F96772A636@hall.AD.UWS.EDU.AU>
	<CA+8X3fVTx_rNZj2y+-5gvv1yM+fQfCuyR8FE9O9VQQE7nfod2w@mail.gmail.com>
Message-ID: <B3AB066D464A8D48999C6553F24153F96772AA1E@hall.AD.UWS.EDU.AU>

Hi,

I used the "axis()" suggested by Jim as follows and it worked:

#Plots average A/Ci for each day from ACi
#Parameters of the panels
par(mfcol=c(3,2), #row,col
    mar=c(2,2,1,1), #inner margin (bottom, left, top, right)
    oma=c(4,4,1,1), #outer margin (bottom, left, top, right)
    omd=c(0.1,0.8,0.1,0.95), #outer dimensions, values {0-1}, (x1, x2, y1, y2)
    xpd=NA) 

#PAR = 1000, Day0
with(subset1000_0,
     lineplot.CI(x.factor=Ci.average, 
                 response=Photo, 
                 group=Treatment,
                 ylab=NA, 
                 xlab=NA,
                 legend=TRUE,
                 x.leg=1000,
                 y.leg=18,
                 type="p",
                 x.cont=TRUE, #continuous x axis (spacing proportional to values)
                 xaxt='n', #remove x axis labels
                 xlim=c(0,1150), #range x axis
                 ylim=c(1,45), #range y axis
                 err.width=0.05,
                 pch = c(16,16,16), #symbols shape
                 col=c("gray84","black","gray48"),
                 fun=
     ))
mtext("Day0, PAR=1000", side = 3, line= -1, adj=0, at=1, cex=0.6) #subtitle
axis(1, at=c(0,25,50,100,150,200,300,400,500,600,700,800,900,1000,1100), labels=NULL) #replace xaxes with desired values


Thanks everyone,
Clem

-----Original Message-----
From: Jim Lemon [mailto:drjimlemon at gmail.com] 
Sent: Sunday, 3 July 2016 7:04 PM
To: Clemence Henry
Cc: r-help at r-project.org
Subject: Re: [R] lineplot.CI xaxis scale change in sciplot?

Hi Clemence,
I don't have sciplot installed, but the help page suggests that the "xaxt" argument is available. This will prevent the x axis from being displayed and you can then specify the x axis you want. Assume that you want an x axis from 0 to 300 by 50:

axis(1,at=seq(0,300,by=50))

Jim


On Thu, Jun 30, 2016 at 12:04 PM, Clemence Henry <C.Henry at westernsydney.edu.au> wrote:
> Hi,
>
> I am trying to change the values of the tick marks on the xaxis of the following multipanel plot (see relevant bits of script below) to increments of 50 or to a custom scale (ie. 50, 100, 150, 200, 300...).
> So far I tried using xaxp or xlim both in par() or lineplot.CI(), as well as axTicks and axisTicks but did not get it to work.
> Suggestions?
>
> #Plots average A/Ci for each day from ACi #Parameters of the panels 
> par(mfcol=c(3,2), #row,col
>     mar=c(2,2,1,1), #inner margin (bottom, left, top, right)
>     oma=c(4,4,1,1), #outer margin (bottom, left, top, right)
>     omd=c(0.1,0.8,0.1,0.95), #outer dimensions, values {0-1}, (x1, x2, y1, y2)
>     xpd=NA)
>
> ...
>
>
> #PAR = 1000, Day2
> with(subset1000_2,
>      lineplot.CI(x.factor=Ci.average,
>                  response=Photo,
>                  group=Treatment,
>                  ylab=NA,
>                  xlab=NA,
>                  legend=FALSE,
>                  type="p",
>                  x.cont=TRUE, #continuous x axis (spacing proportional to values)
>                  ylim=c(1,45), #range y axis
>                  err.width=0.05,
>                  pch = c(16,16,16), #symbols shape
>                  col=c("gray84","black","gray48"),
>                  fun=
>      ))
> mtext("Day2, PAR=1000", side = 3, line= -1, adj=0, at=1, cex=0.6) 
> #subtitle
>
> ....
>
> #legends
> mtext("Ci", side = 1, line= 1, outer = TRUE, cex=0.7) #x legend 
> mtext("Photosynthetic rate", side = 2, line= 1, outer = TRUE, cex=0.7) 
> #y legend Thank you kindly for your support.
>
> Clemence
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From dwinsemius at comcast.net  Tue Jul  5 02:10:42 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 4 Jul 2016 17:10:42 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
Message-ID: <8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>

I thought there was an nrow() function?

Sent from my iPhone

> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
> 
> If you want the total number of rows in the original data.frame after counting the rows in each group, you can ungroup and sum the row counts, like:
> 
> library("dplyr")
> 
> 
> mtcars %>%
>    group_by (am, gear) %>%
>    summarise (n=n()) %>%
>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>    ungroup() %>%
>    mutate(row.tot = sum(n))
> 
> HTH
> Ulrik
> 
>> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>> >
>> > Hello,
>> > How can I aggregate row total for all groups in dplyr summarise ?
>> 
>> Row total ? of what? Aggregate ? how? What is the desired answer?
>> 
>> 
>> 
>> > library(dplyr)
>> > mtcars %>%
>> >  group_by (am, gear) %>%
>> >  summarise (n=n()) %>%
>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>> >
>> > best regard
>> > Maicel Monzon
>> >
>> >
>> >
>> > ----------------------------------------------------------------
>> >
>> >
>> >
>> >
>> > --
>> > Este mensaje le ha llegado mediante el servicio de correo electronico que ofrece Infomed para respaldar el cumplimiento de las misiones del Sistema Nacional de Salud. La persona que envia este correo asume el compromiso de usar el servicio a tales fines y cumplir con las regulaciones establecidas
>> >
>> > Infomed: http://www.sld.cu/
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Tue Jul  5 02:37:03 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 5 Jul 2016 12:37:03 +1200
Subject: [R] Sweave vs. R CMD Rd2pdf --no-clean --force
In-Reply-To: <2e90cd09-8605-1f54-f98e-811bef61730e@echoffmann.ch>
References: <2e90cd09-8605-1f54-f98e-811bef61730e@echoffmann.ch>
Message-ID: <c44a3124-813a-c2fe-fe69-065efdc11282@auckland.ac.nz>



Perhaps this is just my ignorance speaking, but it seems 
counterintuitive to me to try to process a *.Rnw file by means of Rd2pdf 
which would appear to be designed to process *.Rd files.

I would have thought that "R CMD Sweave ..." would be the appropriate 
call.  I find that

     R CMD Sweave --pdf <whatever>.Rnw

works for me.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

On 04/07/16 23:42, Christian Hoffmann wrote:
> Hi,
>
> I have a *.Rnw which I can Sweave with no problem, but which cannot be
> processed by  R CMD  Rd2pdf --no-clean --force.
>
> Th latter stops with
>
> creating vignettes ... ERROR
> Error: processing vignette 'cwhmisc.Rnw' failed with diagnostics:
> at cwhmisc.Rnw:31, could not find function "cformat"
> Execution halted
>
> Before dissecting my *.Rnw systematically , I wanted to ask, if  R CMD
> Rd2pdf has changed since the last R version.
>
> sessionInfo()
> R version 3.2.1 (2015-06-18)
> Platform: x86_64-apple-darwin10.8.0 (64-bit)
> Running under: OS X 10.7.5 (Lion)
>
> locale:
> [1] C
>
> attached base packages:
>  [1] tools     stats4    splines   parallel  datasets
>  [6] compiler  graphics  grDevices stats     grid
> [11] utils     methods   base
>
> other attached packages:
>  [1] survival_2.38-3    spatial_7.3-11
> ......
>
> Any ideas ?
> C.


From murdoch.duncan at gmail.com  Tue Jul  5 03:58:42 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 4 Jul 2016 21:58:42 -0400
Subject: [R] Problem with utils package
In-Reply-To: <2053898661.2343979.1467624960576.JavaMail.yahoo@mail.yahoo.com>
References: <2053898661.2343979.1467624960576.JavaMail.yahoo.ref@mail.yahoo.com>
	<2053898661.2343979.1467624960576.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <125cbe86-dff7-18ee-878e-cda31dec4aa1@gmail.com>

On 04/07/2016 5:36 AM, juansantomero7432--- via R-help wrote:
> Dear List,
> I am having the following problem.
> I just installed a new version of R on Ubuntu Trust LTS, following instruction on CRAN website, but the associated utils package which got installed appears only as version 3.0.2.

How are you determining that?  Are you using packageVersion("utils")?

The version of a base package should always match the version of R, so 
if you've really got 3.0.2 there, something is seriously wrong.

Duncan Murdoch

> On an older version of R (3.1.2) which I am running on a Windows machine, the associated utils package is version 3.1.2.
> So, my question is why was such an outdated utils package installed as part of a brand new R??? Using this older version of utils is causing me all sorts of problems, and because it is part of the base package, I don't seem to be able to install a newer version of utils. Any help would be greatly appreciated!
> many thanks,Juan
> Session Information below:
> R.Version()
> $platform
> [1] "x86_64-pc-linux-gnu"
>
> $arch
> [1] "x86_64"
>
> $os
> [1] "linux-gnu"
>
> $system
> [1] "x86_64, linux-gnu"
>
> $status
> [1] ""
>
> $major
> [1] "3"
>
> $minor
> [1] "3.1"
>
> $year
> [1] "2016"
>
> $month
> [1] "06"
>
> $day
> [1] "21"
>
> $`svn rev`
> [1] "70800"
>
> $language
> [1] "R"
>
> $version.string
> [1] "R version 3.3.1 (2016-06-21)"
>
> $nickname
> [1] "Bug in Your Hair"
>
>> sessionInfo()
> R version 3.3.1 (2016-06-21)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 14.04.3 LTS
>
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=zh_CN.UTF-8        LC_COLLATE=en_US.UTF-8
>  [5] LC_MONETARY=zh_CN.UTF-8    LC_MESSAGES=en_US.UTF-8
>  [7] LC_PAPER=zh_CN.UTF-8       LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=zh_CN.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] tools_3.3.1
>
> Sent from Yahoo Mail. Get the app
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch.duncan at gmail.com  Tue Jul  5 04:04:00 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 4 Jul 2016 22:04:00 -0400
Subject: [R] Sweave vs. R CMD Rd2pdf --no-clean --force
In-Reply-To: <2e90cd09-8605-1f54-f98e-811bef61730e@echoffmann.ch>
References: <2e90cd09-8605-1f54-f98e-811bef61730e@echoffmann.ch>
Message-ID: <4547d899-6bee-9ddf-1315-65741857ff84@gmail.com>

On 04/07/2016 7:42 AM, Christian Hoffmann wrote:
> Hi,
>
> I have a *.Rnw which I can Sweave with no problem, but which cannot be
> processed by  R CMD  Rd2pdf --no-clean --force.

That's not the command you're using.  (It would give a "no inputs" 
error, not a vignette error.  If you actually gave the name of a 
vignette on the command line, it would get very confused, because it's 
for processing *.Rd files, not *.Rnw files.)

I'm guessing you're doing a "build" or "INSTALL".
>
> Th latter stops with
>
> creating vignettes ... ERROR
> Error: processing vignette 'cwhmisc.Rnw' failed with diagnostics:
> at cwhmisc.Rnw:31, could not find function "cformat"
> Execution halted
>
> Before dissecting my *.Rnw systematically , I wanted to ask, if  R CMD
> Rd2pdf has changed since the last R version.

I don't think that is relevant.

>
> sessionInfo()
> R version 3.2.1 (2015-06-18)

That's not the latest version.

Duncan Murdoch

> Platform: x86_64-apple-darwin10.8.0 (64-bit)
> Running under: OS X 10.7.5 (Lion)
>
> locale:
> [1] C
>
> attached base packages:
>   [1] tools     stats4    splines   parallel  datasets
>   [6] compiler  graphics  grDevices stats     grid
> [11] utils     methods   base
>
> other attached packages:
>   [1] survival_2.38-3    spatial_7.3-11
> ......
>
> Any ideas ?
> C.
>


From ulrik.stervbo at gmail.com  Tue Jul  5 06:03:47 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Tue, 05 Jul 2016 04:03:47 +0000
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
	<8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>
Message-ID: <CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>

That will give you the wrong result when used on summarised data

David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 2016 02:10:

> I thought there was an nrow() function?
>
> Sent from my iPhone
>
> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>
> If you want the total number of rows in the original data.frame after
> counting the rows in each group, you can ungroup and sum the row counts,
> like:
>
> library("dplyr")
>
>
> mtcars %>%
>    group_by (am, gear) %>%
>    summarise (n=n()) %>%
>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>    ungroup() %>%
>    mutate(row.tot = sum(n))
>
> HTH
> Ulrik
>
> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>>
>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>> >
>> > Hello,
>> > How can I aggregate row total for all groups in dplyr summarise ?
>>
>> Row total ? of what? Aggregate ? how? What is the desired answer?
>>
>>
>>
>> > library(dplyr)
>> > mtcars %>%
>> >  group_by (am, gear) %>%
>> >  summarise (n=n()) %>%
>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>> >
>> > best regard
>> > Maicel Monzon
>> >
>> >
>> >
>> > ----------------------------------------------------------------
>> >
>> >
>> >
>> >
>> > --
>> > Este mensaje le ha llegado mediante el servicio de correo electronico
>> que ofrece Infomed para respaldar el cumplimiento de las misiones del
>> Sistema Nacional de Salud. La persona que envia este correo asume el
>> compromiso de usar el servicio a tales fines y cumplir con las regulaciones
>> establecidas
>> >
>> > Infomed: http://www.sld.cu/
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Jul  5 07:45:06 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 4 Jul 2016 22:45:06 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
	<8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>
	<CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>
Message-ID: <C08DFE89-7941-4220-9270-2DD19B5DB1E9@comcast.net>

nrow(mtcars)


Sent from my iPhone

> On Jul 4, 2016, at 9:03 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
> 
> That will give you the wrong result when used on summarised data
> 
> 
> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 2016 02:10:
>> I thought there was an nrow() function?
>> 
>> Sent from my iPhone
>> 
>>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>>> 
>>> If you want the total number of rows in the original data.frame after counting the rows in each group, you can ungroup and sum the row counts, like:
>>> 
>>> library("dplyr")
>>> 
>>> 
>>> mtcars %>%
>>>    group_by (am, gear) %>%
>>>    summarise (n=n()) %>%
>>>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>>>    ungroup() %>%
>>>    mutate(row.tot = sum(n))
>>> 
>>> HTH
>>> Ulrik
>>> 
>>>> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net> wrote:
>>>> 
>>>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>>>> >
>>>> > Hello,
>>>> > How can I aggregate row total for all groups in dplyr summarise ?
>>>> 
>>>> Row total ? of what? Aggregate ? how? What is the desired answer?
>>>> 
>>>> 
>>>> 
>>>> > library(dplyr)
>>>> > mtcars %>%
>>>> >  group_by (am, gear) %>%
>>>> >  summarise (n=n()) %>%
>>>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>>>> >
>>>> > best regard
>>>> > Maicel Monzon
>>>> >
>>>> >
>>>> >
>>>> > ----------------------------------------------------------------
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > --
>>>> > Este mensaje le ha llegado mediante el servicio de correo electronico que ofrece Infomed para respaldar el cumplimiento de las misiones del Sistema Nacional de Salud. La persona que envia este correo asume el compromiso de usar el servicio a tales fines y cumplir con las regulaciones establecidas
>>>> >
>>>> > Infomed: http://www.sld.cu/
>>>> >
>>>> > ______________________________________________
>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> > and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Tue Jul  5 07:50:39 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Tue, 05 Jul 2016 05:50:39 +0000
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <C08DFE89-7941-4220-9270-2DD19B5DB1E9@comcast.net>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
	<8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>
	<CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>
	<C08DFE89-7941-4220-9270-2DD19B5DB1E9@comcast.net>
Message-ID: <CAKVAULM09da_F22kvmRBdokyhR_1jySWjqys_C13Hq9wr=5MGw@mail.gmail.com>

Yes. But in the sample code the data is summarised. In which case you get 4
rows and not the correct 32.

On Tue, 5 Jul 2016, 07:48 David Winsemius, <dwinsemius at comcast.net> wrote:

> nrow(mtcars)
>
>
> Sent from my iPhone
>
> On Jul 4, 2016, at 9:03 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>
> That will give you the wrong result when used on summarised data
>
> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 2016
> 02:10:
>
>> I thought there was an nrow() function?
>>
>> Sent from my iPhone
>>
>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>> wrote:
>>
>> If you want the total number of rows in the original data.frame after
>> counting the rows in each group, you can ungroup and sum the row counts,
>> like:
>>
>> library("dplyr")
>>
>>
>> mtcars %>%
>>    group_by (am, gear) %>%
>>    summarise (n=n()) %>%
>>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>>    ungroup() %>%
>>    mutate(row.tot = sum(n))
>>
>> HTH
>> Ulrik
>>
>> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net>
>> wrote:
>>
>>>
>>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>>> >
>>> > Hello,
>>> > How can I aggregate row total for all groups in dplyr summarise ?
>>>
>>> Row total ? of what? Aggregate ? how? What is the desired answer?
>>>
>>>
>>>
>>> > library(dplyr)
>>> > mtcars %>%
>>> >  group_by (am, gear) %>%
>>> >  summarise (n=n()) %>%
>>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>>> >
>>> > best regard
>>> > Maicel Monzon
>>> >
>>> >
>>> >
>>> > ----------------------------------------------------------------
>>> >
>>> >
>>> >
>>> >
>>> > --
>>> > Este mensaje le ha llegado mediante el servicio de correo electronico
>>> que ofrece Infomed para respaldar el cumplimiento de las misiones del
>>> Sistema Nacional de Salud. La persona que envia este correo asume el
>>> compromiso de usar el servicio a tales fines y cumplir con las regulaciones
>>> establecidas
>>> >
>>> > Infomed: http://www.sld.cu/
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From G.Maubach at weinwolf.de  Tue Jul  5 11:27:52 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Tue, 5 Jul 2016 11:27:52 +0200
Subject: [R] Antwort: Re: dplyr : row total for all groups in dplyr summarise
In-Reply-To: <CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>	<8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>
	<CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>
Message-ID: <OF5CF4662A.CF56C65F-ONC1257FE7.0031A14D-C1257FE7.003407AF@lotus.hawesko.de>

Hi guys,

I checked out your example but I can't follow the results.:

> mtcars %>%
+   group_by (am, gear) %>%
+   summarise (n=n()) %>%
+   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
+   ungroup() %>%
+   mutate(row.tot = sum(n))
Source: local data frame [4 x 5]

     am  gear     n rel.freq row.tot
  (dbl) (dbl) (int)    (chr)   (int)
1     0     3    15      79%      32
2     0     4     4      21%      32
3     1     4     8      62%      32
4     1     5     5      38%      32

We have a total of 32 cases and 15 * 100 / 32 = 48,9 % instead of 79 %. 
The same with the other columns. How is 79 % calculated?

When searching the web I saw this example:

-- cut --

#-- not run --
url <- "http://www.lock5stat.com/datasets/HollywoodMovies2011.csv"
response <- GET(url)
Hollywoodmovies2011 <- content(x = GET(url), as = data.frame)
#-- end not run

Hollywoodmovies2011 %>% 
  group_by(genre) %>%
  summarize(count = n()) %>%
  mutate(rf = count / sum(count))

-- cut --

which gives

Source: local data frame [9 x 3]

      Genre count           %
     (fctr) (int)       (dbl)
1    Action    32 0.235294118
2 Adventure     1 0.007352941
3 Animation    12 0.088235294
4    Comedy    27 0.198529412
5     Drama    21 0.154411765
6   Fantasy     2 0.014705882
7    Horror    17 0.125000000
8   Romance    11 0.080882353
9  Thriller    13 0.095588235

Here the % correspond to the count and the sum of count, e. g. sum = 136 
and 32 / 136 = 0,2352941.

What is the difference when counting? What do the relative counts in the 
first example mean?

Kind regards

Georg





Von:    Ulrik Stervbo <ulrik.stervbo at gmail.com>
An:     David Winsemius <dwinsemius at comcast.net>, 
Kopie:  r-help at r-project.org, maicel at infomed.sld.cu
Datum:  05.07.2016 06:06
Betreff:        Re: [R] dplyr : row total for all groups in dplyr 
summarise
Gesendet von:   "R-help" <r-help-bounces at r-project.org>



That will give you the wrong result when used on summarised data

David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 2016 
02:10:

> I thought there was an nrow() function?
>
> Sent from my iPhone
>
> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com> 
wrote:
>
> If you want the total number of rows in the original data.frame after
> counting the rows in each group, you can ungroup and sum the row counts,
> like:
>
> library("dplyr")
>
>
> mtcars %>%
>    group_by (am, gear) %>%
>    summarise (n=n()) %>%
>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>    ungroup() %>%
>    mutate(row.tot = sum(n))
>
> HTH
> Ulrik
>
> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>>
>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>> >
>> > Hello,
>> > How can I aggregate row total for all groups in dplyr summarise ?
>>
>> Row total ? of what? Aggregate ? how? What is the desired answer?
>>
>>
>>
>> > library(dplyr)
>> > mtcars %>%
>> >  group_by (am, gear) %>%
>> >  summarise (n=n()) %>%
>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>> >
>> > best regard
>> > Maicel Monzon
>> >
>> >
>> >
>> > ----------------------------------------------------------------
>> >
>> >
>> >
>> >
>> > --
>> > Este mensaje le ha llegado mediante el servicio de correo electronico
>> que ofrece Infomed para respaldar el cumplimiento de las misiones del
>> Sistema Nacional de Salud. La persona que envia este correo asume el
>> compromiso de usar el servicio a tales fines y cumplir con las 
regulaciones
>> establecidas
>> >
>> > Infomed: http://www.sld.cu/
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>

                 [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Tue Jul  5 13:29:58 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 05 Jul 2016 06:29:58 -0500
Subject: [R] R-3.3.1 RPM release
In-Reply-To: <703384B6C7985C4DBDC3C92ABE98F66C015F62F0B9@USEXCPWD01C.mc01.unicreditgroup.eu>
References: <703384B6C7985C4DBDC3C92ABE98F66C015F62F0B9@USEXCPWD01C.mc01.unicreditgroup.eu>
Message-ID: <03284017-768A-4883-BF78-FA4C12B5D13E@me.com>


> On Jul 4, 2016, at 9:37 AM, Sangalli Cristiano Giovanni (Ext. - UniCredit Business Integrated	Solutions) <CSANGALLI.external2 at unicredit.eu> wrote:
> 
> Dear CRAN,
> 
> I have installed on a RHEL 5.7 server the recent R-3.3.0 rpm but I would like to upgrade to the latest now available R-3.3.1 . I have not found so far the rpm, I would like to know if and when it will be available?
> 
> Thanks in advance,
> Cris
> 

Hi,

The CRAN team does not build RPMs for RH and Fedora, that is done via RH maintainers and released via the EPEL:

  https://fedoraproject.org/wiki/EPEL

As an FYI, there is a dedicated SIG list for R on RH/Fedora and the RH maintainers (e.g. Tom Callaway) read that list:

  https://stat.ethz.ch/mailman/listinfo/r-sig-fedora

You should post there and Tom can provide a sense of when 3.3.1 will be made available.

At present, there is no indication that 3.3.1 is in the R build queue, which can be viewed at:

  https://bodhi.fedoraproject.org/updates/?packages=R

Regards,

Marc Schwartz


From maicel at infomed.sld.cu  Tue Jul  5 13:47:05 2016
From: maicel at infomed.sld.cu (maicel at infomed.sld.cu)
Date: Tue, 05 Jul 2016 07:47:05 -0400
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <CAKVAULM09da_F22kvmRBdokyhR_1jySWjqys_C13Hq9wr=5MGw@mail.gmail.com>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
	<8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>
	<CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>
	<C08DFE89-7941-4220-9270-2DD19B5DB1E9@comcast.net>
	<CAKVAULM09da_F22kvmRBdokyhR_1jySWjqys_C13Hq9wr=5MGw@mail.gmail.com>
Message-ID: <20160705074705.14756c5pwj9ree61@webmail.sld.cu>

Sorry, what I wanted to do was to add a total row at the end of the  
summary. The marginal totals by columns correspond to 100% and the sum  
of levels.
best reagard
Maicel Monzon


Ulrik Stervbo <ulrik.stervbo at gmail.com> escribi?:

> Yes. But in the sample code the data is summarised. In which case you get 4
> rows and not the correct 32.
>
> On Tue, 5 Jul 2016, 07:48 David Winsemius, <dwinsemius at comcast.net> wrote:
>
>> nrow(mtcars)
>>
>>
>> Sent from my iPhone
>>
>> On Jul 4, 2016, at 9:03 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>>
>> That will give you the wrong result when used on summarised data
>>
>> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 2016
>> 02:10:
>>
>>> I thought there was an nrow() function?
>>>
>>> Sent from my iPhone
>>>
>>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>>> wrote:
>>>
>>> If you want the total number of rows in the original data.frame after
>>> counting the rows in each group, you can ungroup and sum the row counts,
>>> like:
>>>
>>> library("dplyr")
>>>
>>>
>>> mtcars %>%
>>>    group_by (am, gear) %>%
>>>    summarise (n=n()) %>%
>>>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>>>    ungroup() %>%
>>>    mutate(row.tot = sum(n))
>>>
>>> HTH
>>> Ulrik
>>>
>>> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net>
>>> wrote:
>>>
>>>>
>>>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>>>> >
>>>> > Hello,
>>>> > How can I aggregate row total for all groups in dplyr summarise ?
>>>>
>>>> Row total ? of what? Aggregate ? how? What is the desired answer?
>>>>
>>>>
>>>>
>>>> > library(dplyr)
>>>> > mtcars %>%
>>>> >  group_by (am, gear) %>%
>>>> >  summarise (n=n()) %>%
>>>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>>>> >
>>>> > best regard
>>>> > Maicel Monzon
>>>> >
>>>> >
>>>> >
>>>> > ----------------------------------------------------------------
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > --
>>>> > Este mensaje le ha llegado mediante el servicio de correo electronico
>>>> que ofrece Infomed para respaldar el cumplimiento de las misiones del
>>>> Sistema Nacional de Salud. La persona que envia este correo asume el
>>>> compromiso de usar el servicio a tales fines y cumplir con las  
>>>> regulaciones
>>>> establecidas
>>>> >
>>>> > Infomed: http://www.sld.cu/
>>>> >
>>>> > ______________________________________________
>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>> > PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> > and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>



----------------------------------------------------------------




--
Este mensaje le ha llegado mediante el servicio de correo electronico que ofrece Infomed para respaldar el cumplimiento de las misiones del Sistema Nacional de Salud. La persona que envia este correo asume el compromiso de usar el servicio a tales fines y cumplir con las regulaciones establecidas

Infomed: http://www.sld.cu/


From Gaurang.Mehta at royallondon.com  Tue Jul  5 10:09:52 2016
From: Gaurang.Mehta at royallondon.com (Mehta, Gaurang)
Date: Tue, 5 Jul 2016 09:09:52 +0100
Subject: [R] BesselK dll file use in VBA
Message-ID: <3BB657B92C75C74385A95FAA7C6B17161F0C517735@VRTPRDEXM02.royallondongroup.com>

Hi Team,
I want to use R's Bessel dll file in VBA.
Can anyone help with the commands? Have you used it?
Regards,
Gaurang Mehta


This email is intended for the person or company named and access by anyone else is unauthorised. If you are not the person or company named, please delete this email and notify the sender.

The information in this email, including any attachments, may be confidential or legally privileged (meaning that its disclosure is protected in law). Its unauthorised disclosure, copying, distribution or use is prohibited and may be unlawful.

Email communications sent over the internet are not guaranteed to be secure or virus-free and such messages are potentially at risk.  The Royal London Group accepts no liability for any claims arising from use of the internet to transmit messages by or to any company within the Royal London Group.

The Royal London Group consists of The Royal London Mutual Insurance Society Limited and its subsidiaries.

The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority and provides life assurance and pensions.

Registered in England and Wales number 99064.

Registered office: 55 Gracechurch Street, London, EC3V 0RL.

In the Republic of Ireland: The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority in the UK and is regulated by the Central Bank of Ireland for conduct of business rules.


	[[alternative HTML version deleted]]


From Giacomo_May94 at gmx.de  Tue Jul  5 11:47:09 2016
From: Giacomo_May94 at gmx.de (Giacomo May)
Date: Tue, 5 Jul 2016 11:47:09 +0200
Subject: [R] Problems with parallel processing using the foreach package
Message-ID: <trinity-a0c688d8-8a68-4ab6-9f6e-b11bfd424b2b-1467712029567@3capp-gmx-bs35>


Dear R-users,
I am trying my hand at parallel processing in R using the foreach package but it is not working as I want it to. I am using a function I created myself (Xenopus_Walk) which returns a vector. Now I would like to run this function for every number that is saved in a vector (newly_populated_vec) and obtain a list that stores a every vector that has been created as one element of said list. The command I am currently using is the following (I guess you can ignore most of it since that's mostly only exported packages and parameters my function relies on):

no_cores <- detectCores()-1
cl <- makeCluster(no_cores)
registerDoParallel(cl)
Xenopus_Data <- foreach(b=1:length(newly_populated_vec),.combine=list,.multicombine=TRUE,.packages = c("raster", "gdistance", "rgdal","sp")) %dopar% { Xenopus_Walk(altdata=altdata,water=water,habitat_suitability=habitat_suitability,max_range_without_water=max_range_without_water,max_range=max_range,slope=slope,Start_Pt=newly_populated_vec[b]) }

The problem I have now is that the length of the returned list (Xenopus_Data) si different from the length of the vector I retrieve the iterator from (newly_populated_vec):

> length(Xenopus_Data) [1] 47
> length(newly_populated_vec) [1] 2027

While trying to figure out what is wrong I have read that one has to split up the workload into equal chunks and pass each of them to a core but as you probably can tell my understanding of all this is pretty low. I am have a total of 32 cores at my disposition. Does anyone know why I have this problem and maybe also a way to solve it ? I know that reproducible examples are desired, but the function I use is pretty long and I doubt anyone is going to work through it. Still, if I can help make things clearer by providing additional information I will be glad to do so! Any type of help is greatly appreciated. Thanks in advance!
EDIT: I forgot to add that when I look at the list it is nested, so I don't get one element for every number in the vector but I get one element with multiple sub-elements. Just in case that helps.


From CSANGALLI.external2 at unicredit.eu  Tue Jul  5 13:34:18 2016
From: CSANGALLI.external2 at unicredit.eu (Sangalli Cristiano Giovanni (Ext. - UniCredit Business Integrated
	Solutions))
Date: Tue, 5 Jul 2016 11:34:18 +0000
Subject: [R] R-3.3.1 RPM release
In-Reply-To: <03284017-768A-4883-BF78-FA4C12B5D13E@me.com>
References: <703384B6C7985C4DBDC3C92ABE98F66C015F62F0B9@USEXCPWD01C.mc01.unicreditgroup.eu>
	<03284017-768A-4883-BF78-FA4C12B5D13E@me.com>
Message-ID: <703384B6C7985C4DBDC3C92ABE98F66C015F63B1F9@USEXCPWD01A.mc01.unicreditgroup.eu>

Thanks a lot Mac for your kind answer.
In the meanwhile I could successfully build the 3.3.1 on a CentOS 6.3 (VM)

Anyway I will contact Tom as you suggest.

Thanks and regards,
Cristiano

-----Original Message-----
From: Marc Schwartz [mailto:marc_schwartz at me.com] 
Sent: Tuesday, July 05, 2016 1:30 PM
To: Sangalli Cristiano Giovanni (Ext. - UniCredit Business Integrated Solutions)
Cc: R-help
Subject: Re: [R] R-3.3.1 RPM release


> On Jul 4, 2016, at 9:37 AM, Sangalli Cristiano Giovanni (Ext. - UniCredit Business Integrated	Solutions) <CSANGALLI.external2 at unicredit.eu> wrote:
> 
> Dear CRAN,
> 
> I have installed on a RHEL 5.7 server the recent R-3.3.0 rpm but I would like to upgrade to the latest now available R-3.3.1 . I have not found so far the rpm, I would like to know if and when it will be available?
> 
> Thanks in advance,
> Cris
> 

Hi,

The CRAN team does not build RPMs for RH and Fedora, that is done via RH maintainers and released via the EPEL:

  https://fedoraproject.org/wiki/EPEL

As an FYI, there is a dedicated SIG list for R on RH/Fedora and the RH maintainers (e.g. Tom Callaway) read that list:

  https://stat.ethz.ch/mailman/listinfo/r-sig-fedora

You should post there and Tom can provide a sense of when 3.3.1 will be made available.

At present, there is no indication that 3.3.1 is in the R build queue, which can be viewed at:

  https://bodhi.fedoraproject.org/updates/?packages=R

Regards,

Marc Schwartz


This e-mail is confidential and may also contain privileged information. If you are not the intended recipient you are not authorised to read, print, save, process or disclose this message. If you have received this message by mistake, please inform the sender immediately and delete this e-mail, its attachments and any copies.
Any use, distribution, reproduction or disclosure by any person other than the intended recipient is strictly prohibited and the person responsible may incur penalties.
Thank you!


From dwinsemius at comcast.net  Tue Jul  5 18:30:40 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 5 Jul 2016 09:30:40 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <20160705074705.14756c5pwj9ree61@webmail.sld.cu>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
	<8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>
	<CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>
	<C08DFE89-7941-4220-9270-2DD19B5DB1E9@comcast.net>
	<CAKVAULM09da_F22kvmRBdokyhR_1jySWjqys_C13Hq9wr=5MGw@mail.gmail.com>
	<20160705074705.14756c5pwj9ree61@webmail.sld.cu>
Message-ID: <2C47194E-EA84-4066-BFC4-B97B948CD94F@comcast.net>



mtcars %>%
   group_by (am, gear) %>%
   summarise (n=n()) %>%
   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
   ungroup() %>% plyr::rbind.fill(data.frame( n=nrow(mtcars),rel.freq="100%?))


> On Jul 5, 2016, at 4:47 AM, maicel at infomed.sld.cu wrote:
> 
> Sorry, what I wanted to do was to add a total row at the end of the summary. The marginal totals by columns correspond to 100% and the sum of levels.
> best reagard
> Maicel Monzon
> 
> 
> Ulrik Stervbo <ulrik.stervbo at gmail.com> escribi?:
> 
>> Yes. But in the sample code the data is summarised. In which case you get 4
>> rows and not the correct 32.
>> 
>> On Tue, 5 Jul 2016, 07:48 David Winsemius, <dwinsemius at comcast.net> wrote:
>> 
>>> nrow(mtcars)
>>> 
>>> 
>>> Sent from my iPhone
>>> 
>>> On Jul 4, 2016, at 9:03 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>>> 
>>> That will give you the wrong result when used on summarised data
>>> 
>>> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 2016
>>> 02:10:
>>> 
>>>> I thought there was an nrow() function?
>>>> 
>>>> Sent from my iPhone
>>>> 
>>>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>>>> wrote:
>>>> 
>>>> If you want the total number of rows in the original data.frame after
>>>> counting the rows in each group, you can ungroup and sum the row counts,
>>>> like:
>>>> 
>>>> library("dplyr")
>>>> 
>>>> 
>>>> mtcars %>%
>>>>   group_by (am, gear) %>%
>>>>   summarise (n=n()) %>%
>>>>   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>>>>   ungroup() %>%
>>>>   mutate(row.tot = sum(n))
>>>> 
>>>> HTH
>>>> Ulrik
>>>> 
>>>> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net>
>>>> wrote:
>>>> 
>>>>> 
>>>>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>>>>> >
>>>>> > Hello,
>>>>> > How can I aggregate row total for all groups in dplyr summarise ?
>>>>> 
>>>>> Row total ? of what? Aggregate ? how? What is the desired answer?
>>>>> 
>>>>> 
>>>>> 
>>>>> > library(dplyr)
>>>>> > mtcars %>%
>>>>> >  group_by (am, gear) %>%
>>>>> >  summarise (n=n()) %>%
>>>>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>>>>> >
>>>>> > best regard
>>>>> > Maicel Monzon
>>>>> >
>>>>> >
>>>>> >
>>>>> > ----------------------------------------------------------------
>>>>> >
>>>>> >
>>>>> >
>>>>> >
>>>>> > --
>>>>> > Este mensaje le ha llegado mediante el servicio de correo electronico
>>>>> que ofrece Infomed para respaldar el cumplimiento de las misiones del
>>>>> Sistema Nacional de Salud. La persona que envia este correo asume el
>>>>> compromiso de usar el servicio a tales fines y cumplir con las regulaciones
>>>>> establecidas
>>>>> >
>>>>> > Infomed: http://www.sld.cu/
>>>>> >
>>>>> > ______________________________________________
>>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> > PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> > and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>> 
> 
> 
> 
> ----------------------------------------------------------------
> This message was sent using IMP, the Internet Messaging Program.
> 
> 
> 
> --
> Este mensaje le ha llegado mediante el servicio de correo electronico que ofrece Infomed para respaldar el cumplimiento de las misiones del Sistema Nacional de Salud. La persona que envia este correo asume el compromiso de usar el servicio a tales fines y cumplir con las regulaciones establecidas
> 
> Infomed: http://www.sld.cu/
> 


From dwinsemius at comcast.net  Tue Jul  5 18:47:29 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 5 Jul 2016 09:47:29 -0700
Subject: [R] Antwort: Re: dplyr : row total for all groups in dplyr
	summarise
In-Reply-To: <OF5CF4662A.CF56C65F-ONC1257FE7.0031A14D-C1257FE7.003407AF@lotus.hawesko.de>
References: <20160704095645.87632ya4tn33vbj1@webmail.sld.cu>
	<0400A31C-8F94-42CD-A514-D77080E1DE78@comcast.net>
	<CAKVAULOEBws_cPo48cxHJacUq=_VZFkM70TmJxVjiC5Qub=+uw@mail.gmail.com>
	<8DC629A6-EDAD-4429-B862-B943E0BCD265@comcast.net>
	<CAKVAULPvUT+GLw6St90cokKWmgZ=oMLRBQtLejzxWMpVAqoNCQ@mail.gmail.com>
	<OF5CF4662A.CF56C65F-ONC1257FE7.0031A14D-C1257FE7.003407AF@lotus.hawesko.de>
Message-ID: <855F1A39-42AE-4545-AEBF-BC82935BB1B6@comcast.net>


> On Jul 5, 2016, at 2:27 AM, G.Maubach at weinwolf.de wrote:
> 
> Hi guys,
> 
> I checked out your example but I can't follow the results.:
> 
>> mtcars %>%
> +   group_by (am, gear) %>%
> +   summarise (n=n()) %>%
> +   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
> +   ungroup() %>%
> +   mutate(row.tot = sum(n))
> Source: local data frame [4 x 5]
> 
>     am  gear     n rel.freq row.tot
>  (dbl) (dbl) (int)    (chr)   (int)
> 1     0     3    15      79%      32
> 2     0     4     4      21%      32
> 3     1     4     8      62%      32
> 4     1     5     5      38%      32
> 
> We have a total of 32 cases and 15 * 100 / 32 = 48,9 % instead of 79 %. 
> The same with the other columns. How is 79 % calculated?
> 

It is apparently the number of items in the first ?group determinant? 

> mtcars %>%
+    group_by (am, gear) %>%
+    summarise (n=n()) %>%
+    mutate(sum = sum(n)) %>%
+    ungroup()
Source: local data frame [4 x 4]

     am  gear     n   sum
  (dbl) (dbl) (int) (int)
1     0     3    15    19
2     0     4     4    19
3     1     4     8    13
4     1     5     5    13
> ?n
> with(mtcars,table(am,gear))
   gear
am   3  4  5
  0 15  4  0
  1  0  8  5

The documentation for the `n` functions is particularly unhelpful in letting one know what to expect from it:

"Description

This function is implemented special for each data source and can only be used from within summarise, mutate and filter"
? 

David.


> When searching the web I saw this example:
> 
> -- cut --
> 
> #-- not run --
> url <- "http://www.lock5stat.com/datasets/HollywoodMovies2011.csv"
> response <- GET(url)
> Hollywoodmovies2011 <- content(x = GET(url), as = data.frame)
> #-- end not run
> 
> Hollywoodmovies2011 %>% 
>  group_by(genre) %>%
>  summarize(count = n()) %>%
>  mutate(rf = count / sum(count))
> 
> -- cut --
> 
> which gives
> 
> Source: local data frame [9 x 3]
> 
>      Genre count           %
>     (fctr) (int)       (dbl)
> 1    Action    32 0.235294118
> 2 Adventure     1 0.007352941
> 3 Animation    12 0.088235294
> 4    Comedy    27 0.198529412
> 5     Drama    21 0.154411765
> 6   Fantasy     2 0.014705882
> 7    Horror    17 0.125000000
> 8   Romance    11 0.080882353
> 9  Thriller    13 0.095588235
> 
> Here the % correspond to the count and the sum of count, e. g. sum = 136 
> and 32 / 136 = 0,2352941.
> 
> What is the difference when counting? What do the relative counts in the 
> first example mean?
> 
> Kind regards
> 
> Georg
> 
> 
> 
> 
> 
> Von:    Ulrik Stervbo <ulrik.stervbo at gmail.com>
> An:     David Winsemius <dwinsemius at comcast.net>, 
> Kopie:  r-help at r-project.org, maicel at infomed.sld.cu
> Datum:  05.07.2016 06:06
> Betreff:        Re: [R] dplyr : row total for all groups in dplyr 
> summarise
> Gesendet von:   "R-help" <r-help-bounces at r-project.org>
> 
> 
> 
> That will give you the wrong result when used on summarised data
> 
> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 2016 
> 02:10:
> 
>> I thought there was an nrow() function?
>> 
>> Sent from my iPhone
>> 
>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com> 
> wrote:
>> 
>> If you want the total number of rows in the original data.frame after
>> counting the rows in each group, you can ungroup and sum the row counts,
>> like:
>> 
>> library("dplyr")
>> 
>> 
>> mtcars %>%
>>   group_by (am, gear) %>%
>>   summarise (n=n()) %>%
>>   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>>   ungroup() %>%
>>   mutate(row.tot = sum(n))
>> 
>> HTH
>> Ulrik
>> 
>> On Mon, 4 Jul 2016 at 18:23 David Winsemius <dwinsemius at comcast.net>
>> wrote:
>> 
>>> 
>>>> On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>>>> 
>>>> Hello,
>>>> How can I aggregate row total for all groups in dplyr summarise ?
>>> 
>>> Row total ? of what? Aggregate ? how? What is the desired answer?
>>> 
>>> 
>>> 
>>>> library(dplyr)
>>>> mtcars %>%
>>>> group_by (am, gear) %>%
>>>> summarise (n=n()) %>%
>>>> mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>>>> 
>>>> best regard
>>>> Maicel Monzon
>>>> 
>>>> 
>>>> 
>>>> ----------------------------------------------------------------
>>>> 
>>>> 
>>>> 
>>>> 
>>>> --
>>>> Este mensaje le ha llegado mediante el servicio de correo electronico
>>> que ofrece Infomed para respaldar el cumplimiento de las misiones del
>>> Sistema Nacional de Salud. La persona que envia este correo asume el
>>> compromiso de usar el servicio a tales fines y cumplir con las 
> regulaciones
>>> establecidas
>>>> 
>>>> Infomed: http://www.sld.cu/
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
>                 [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From miaojpm at gmail.com  Tue Jul  5 19:02:18 2016
From: miaojpm at gmail.com (John)
Date: Tue, 5 Jul 2016 10:02:18 -0700
Subject: [R] Can R read Word fonts and comments?
In-Reply-To: <CAJ4QxaPUdgAupbCKzYtR++NQhEmre+CA1jjkESdPx9d8EGo8fg@mail.gmail.com>
References: <CABcx46D0gNOHEau_iEd_F-hMM1MA=J0r41sLwoFY8SMgE47nZw@mail.gmail.com>
	<CAGxFJbSSnMK7_cCfAMXVMT1fM3zWUQsjmNwfgtb_fh5XzRRdCQ@mail.gmail.com>
	<CABcx46A5O2Oc5XMdBSM=uHHQJdbvMp6U=8AfcOYpx5_Jr3CrpA@mail.gmail.com>
	<CAGxFJbS0s0g88v7aN9MJQBjEVkhB+9vUhS1WOJi9+UAAP2H9Uw@mail.gmail.com>
	<5FFC2C2F-82C2-4773-90CB-0C32DF869086@comcast.net>
	<CAJ4QxaPUdgAupbCKzYtR++NQhEmre+CA1jjkESdPx9d8EGo8fg@mail.gmail.com>
Message-ID: <CABcx46BJxR8zC4diZ7yxNDNZdY58iksm4ghSkh6S0+LUsfSgYg@mail.gmail.com>

Thank you, David and Bert, for the info.
Thank you, Bob, for this excellent function. Allow me to request a feature:
You highlighted the following text, and comment "This is the first
comment".

"Lorem ipsum dolor sit amet, cu sit modus voluptua accommodare, meis
disputando voluptatibus eu nec, qui te modo solum delicata. Eam scripta
maluisset urbanitas et, numquam disputationi in pri, vis tibique deserunt
accusamus ut. Vis movet admodum probatus cu, ex pri ludus possit. Molestiae
efficiendi at vix, eu labore elaboraret deterruisset mei, et eos persius
nominati."

Could you let the function output the above text (with the comments, of
course), which you highlighted for comment?

Thanks,

John



2016-07-02 14:12 GMT-07:00 boB Rudis <bob at rudis.net>:

> I just added `docx_extract_all_cmnts()` (and a cpl other
> comments-related things) to the dev version of `docxtractr`
> (https://github.com/hrbrmstr/docxtractr). You can use
> `devtools::install_github("hrbrmstr/docxtractr")` to install it.
> There's an example in the help for that function.
>
> Give it a go and file detailed issues for other functionality you need.
>
> On Fri, Jul 1, 2016 at 11:14 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> > It?s my understanding that docx and xlsx files are zipped containers
> that have their data in XML files. You should try unzipping one and
> examining it with a viewer. You may then be able to use pkg:XML.
> >
> > ?
> > David.
> >
> >> On Jul 1, 2016, at 3:13 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >>
> >> No, sorry -- all I would do is search.
> >>
> >> -- Bert
> >>
> >>
> >> Bert Gunter
> >>
> >> "The trouble with having an open mind is that people keep coming along
> >> and sticking things into it."
> >> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >>
> >>
> >> On Fri, Jul 1, 2016 at 2:33 PM, John <miaojpm at gmail.com> wrote:
> >>> Yes, I have done some search (e.g., tm, markdown, etc), but I can't
> find
> >>> this function.
> >>> If you know any package that works for this purpose, that would be
> quite
> >>> helpful.
> >>> Thanks,
> >>>
> >>> John
> >>>
> >>> 2016-06-28 16:50 GMT-07:00 Bert Gunter <bgunter.4567 at gmail.com>:
> >>>>
> >>>> Did you try searching before posting here? -- e.g. a web search or on
> >>>> rseek.org ?
> >>>>
> >>>> Cheers,
> >>>> Bert
> >>>>
> >>>>
> >>>> Bert Gunter
> >>>>
> >>>> "The trouble with having an open mind is that people keep coming along
> >>>> and sticking things into it."
> >>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >>>>
> >>>>
> >>>> On Tue, Jun 28, 2016 at 3:53 PM, John <miaojpm at gmail.com> wrote:
> >>>>> Hi,
> >>>>>
> >>>>>   From time to time I highlight the word documents with red/blue
> color
> >>>>> or
> >>>>> italic/bold fonts, and I also add comments to a file. Is there a
> >>>>> package/function to let R extract the italic/bold blue/red words and
> >>>>> comments from a docx/doc file?
> >>>>>
> >>>>>   I am aware that there are a few packages reading Word, but don't
> know
> >>>>> which one is able to do it.
> >>>>>
> >>>>>   Thanks,
> >>>>>
> >>>>> John
> >>>>>
> >>>>>        [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bob at rudis.net  Tue Jul  5 19:03:40 2016
From: bob at rudis.net (boB Rudis)
Date: Tue, 5 Jul 2016 13:03:40 -0400
Subject: [R] Can R read Word fonts and comments?
In-Reply-To: <CABcx46BJxR8zC4diZ7yxNDNZdY58iksm4ghSkh6S0+LUsfSgYg@mail.gmail.com>
References: <CABcx46D0gNOHEau_iEd_F-hMM1MA=J0r41sLwoFY8SMgE47nZw@mail.gmail.com>
	<CAGxFJbSSnMK7_cCfAMXVMT1fM3zWUQsjmNwfgtb_fh5XzRRdCQ@mail.gmail.com>
	<CABcx46A5O2Oc5XMdBSM=uHHQJdbvMp6U=8AfcOYpx5_Jr3CrpA@mail.gmail.com>
	<CAGxFJbS0s0g88v7aN9MJQBjEVkhB+9vUhS1WOJi9+UAAP2H9Uw@mail.gmail.com>
	<5FFC2C2F-82C2-4773-90CB-0C32DF869086@comcast.net>
	<CAJ4QxaPUdgAupbCKzYtR++NQhEmre+CA1jjkESdPx9d8EGo8fg@mail.gmail.com>
	<CABcx46BJxR8zC4diZ7yxNDNZdY58iksm4ghSkh6S0+LUsfSgYg@mail.gmail.com>
Message-ID: <CAJ4QxaOAvfhVCnVrf0MkdOD3jPMqQbymamwKCVheAPW0q6HF9A@mail.gmail.com>

I'll dig into that (was hoping the small feature addition wld cause
enhanced feature requests :-)

On Tue, Jul 5, 2016 at 1:02 PM, John <miaojpm at gmail.com> wrote:
> Thank you, David and Bert, for the info.
> Thank you, Bob, for this excellent function. Allow me to request a feature:
> You highlighted the following text, and comment "This is the first comment".
>
> "Lorem ipsum dolor sit amet, cu sit modus voluptua accommodare, meis
> disputando voluptatibus eu nec, qui te modo solum delicata. Eam scripta
> maluisset urbanitas et, numquam disputationi in pri, vis tibique deserunt
> accusamus ut. Vis movet admodum probatus cu, ex pri ludus possit. Molestiae
> efficiendi at vix, eu labore elaboraret deterruisset mei, et eos persius
> nominati."
>
> Could you let the function output the above text (with the comments, of
> course), which you highlighted for comment?
>
> Thanks,
>
> John
>
>
>
> 2016-07-02 14:12 GMT-07:00 boB Rudis <bob at rudis.net>:
>>
>> I just added `docx_extract_all_cmnts()` (and a cpl other
>> comments-related things) to the dev version of `docxtractr`
>> (https://github.com/hrbrmstr/docxtractr). You can use
>> `devtools::install_github("hrbrmstr/docxtractr")` to install it.
>> There's an example in the help for that function.
>>
>> Give it a go and file detailed issues for other functionality you need.
>>
>> On Fri, Jul 1, 2016 at 11:14 PM, David Winsemius <dwinsemius at comcast.net>
>> wrote:
>> > It?s my understanding that docx and xlsx files are zipped containers
>> > that have their data in XML files. You should try unzipping one and
>> > examining it with a viewer. You may then be able to use pkg:XML.
>> >
>> > ?
>> > David.
>> >
>> >> On Jul 1, 2016, at 3:13 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> >>
>> >> No, sorry -- all I would do is search.
>> >>
>> >> -- Bert
>> >>
>> >>
>> >> Bert Gunter
>> >>
>> >> "The trouble with having an open mind is that people keep coming along
>> >> and sticking things into it."
>> >> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> >>
>> >>
>> >> On Fri, Jul 1, 2016 at 2:33 PM, John <miaojpm at gmail.com> wrote:
>> >>> Yes, I have done some search (e.g., tm, markdown, etc), but I can't
>> >>> find
>> >>> this function.
>> >>> If you know any package that works for this purpose, that would be
>> >>> quite
>> >>> helpful.
>> >>> Thanks,
>> >>>
>> >>> John
>> >>>
>> >>> 2016-06-28 16:50 GMT-07:00 Bert Gunter <bgunter.4567 at gmail.com>:
>> >>>>
>> >>>> Did you try searching before posting here? -- e.g. a web search or on
>> >>>> rseek.org ?
>> >>>>
>> >>>> Cheers,
>> >>>> Bert
>> >>>>
>> >>>>
>> >>>> Bert Gunter
>> >>>>
>> >>>> "The trouble with having an open mind is that people keep coming
>> >>>> along
>> >>>> and sticking things into it."
>> >>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>> >>>>
>> >>>>
>> >>>> On Tue, Jun 28, 2016 at 3:53 PM, John <miaojpm at gmail.com> wrote:
>> >>>>> Hi,
>> >>>>>
>> >>>>>   From time to time I highlight the word documents with red/blue
>> >>>>> color
>> >>>>> or
>> >>>>> italic/bold fonts, and I also add comments to a file. Is there a
>> >>>>> package/function to let R extract the italic/bold blue/red words and
>> >>>>> comments from a docx/doc file?
>> >>>>>
>> >>>>>   I am aware that there are a few packages reading Word, but don't
>> >>>>> know
>> >>>>> which one is able to do it.
>> >>>>>
>> >>>>>   Thanks,
>> >>>>>
>> >>>>> John
>> >>>>>
>> >>>>>        [[alternative HTML version deleted]]
>> >>>>>
>> >>>>> ______________________________________________
>> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>> PLEASE do read the posting guide
>> >>>>> http://www.R-project.org/posting-guide.html
>> >>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>>
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From bjpmodi2016 at gmail.com  Tue Jul  5 20:41:53 2016
From: bjpmodi2016 at gmail.com (Narendra Modi)
Date: Tue, 5 Jul 2016 13:41:53 -0500
Subject: [R] How to extract "specific"/"last" intercept value from segmented
	package.
Message-ID: <CAPq=xQDemZdz-J+4bN6-ccRkD7p5NX5MTBCNpcOrHsVZeVgenw@mail.gmail.com>

I am able to perform regression on a dataset as below:

plot(x,y)
lin.mod <- lm(y~x)
m <- mean(x)
m

segmented.mod <- segmented(lin.mod, seg.Z = ~x, psi= m)

plot(segmented.mod, add=T)
sl <- slope(segmented.mod)
inter <- intercept(segmented.mod)

summary(segmented.mod)    # Show Summary
sl                        # show all the slopes
inter                     # show all the intercepts


In my dataset, the above method correctly identifies the breakpoints and
hence I get two intercepts.

> inter
$x
              Est.
intercept1  -3.269
intercept2 -19.980

What I am interested is the "intercept2" value. How can I obtain this?

The method needs to be dynamic as in if the next dataset has 3 intercepts,
I would like to get "intercept3 value.

PD

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Tue Jul  5 21:28:24 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Tue, 05 Jul 2016 20:28:24 +0100
Subject: [R] How to extract "specific"/"last" intercept value from
 segmented package.
In-Reply-To: <CAPq=xQDemZdz-J+4bN6-ccRkD7p5NX5MTBCNpcOrHsVZeVgenw@mail.gmail.com>
Message-ID: <20160705202824.Horde.Tv3Jge5RCcfY2hHb4zx5gL6@mail.sapo.pt>

Hello,

Try

dimnames(inter$x)[[1]]

You could have seen this by inspecting 'inter':

str(inter)

Hope this helps,

Rui Barradas
?

Citando Narendra Modi <bjpmodi2016 at gmail.com>:

> I am able to perform regression on a dataset as below:
>
> plot(x,y)
> lin.mod <- lm(y~x)
> m <- mean(x)
> m
>
> segmented.mod <- segmented(lin.mod, seg.Z = ~x, psi= m)
>
> plot(segmented.mod, add=T)
> sl <- slope(segmented.mod)
> inter <- intercept(segmented.mod)
>
> summary(segmented.mod)? ? # Show Summary
> sl? ? ? ? ? ? ? ? ? ? ? ? # show all the slopes
> inter? ? ? ? ? ? ? ? ? ? ?# show all the intercepts
>
> In my dataset, the above method correctly identifies the breakpoints and
> hence I get two intercepts.
>> inter
>
> $x
> ? ? ? ? ? ? ?Est.
> intercept1? -3.269
> intercept2 -19.980
>
> What I am interested is the "intercept2" value. How can I obtain this?
>
> The method needs to be dynamic as in if the next dataset has 3 intercepts,
> I would like to get "intercept3 value.
>
> PD
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Jul  5 21:33:36 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 5 Jul 2016 19:33:36 +0000
Subject: [R] How to extract "specific"/"last" intercept value from
 segmented	package.
In-Reply-To: <CAPq=xQDemZdz-J+4bN6-ccRkD7p5NX5MTBCNpcOrHsVZeVgenw@mail.gmail.com>
References: <CAPq=xQDemZdz-J+4bN6-ccRkD7p5NX5MTBCNpcOrHsVZeVgenw@mail.gmail.com>
Message-ID: <8610ef4348be4d20af0effe6b70047ea@exch-2p-mbx-t2.ads.tamu.edu>

You should use only plain text emails, provide sample data, and indicate any relevant packages that you had to load. First let's load the necessary package and create some data:

> library(segmented)
> set.seed(42)
> x <- sort(runif(60, 10, 60))
> int <- c(rep(0, 20), rep(60, 20), rep(-80, 20))
> slp <- c(rep(2, 20), rep(0, 20), rep(3, 20))
> y <- int + slp*x + rnorm(60, 0, 2)
> plot(x, y)

You should have a plot showing 2 break points. Using psi=mean(x) puts a single breakpoint in the middle of the distribtution so we have to be more specific:

> lin.mod <- lm(y~x)
> segmented.mod <- segmented(lin.mod, seg.Z = ~x, psi= c(25, 55))
> plot(segmented.mod, add=T)
> sl <- slope(segmented.mod)
> inter <- intercept(segmented.mod)

Now your plot shows the segmented model. You need to know what intercept() is returning so you should look at the manual page: ?intercept. It returns a list of matrices (one matrix for each independent variable). You have only one, x, so you get a list with one element.

> str(inter)
List of 1
 $ x: num [1:3, 1] -0.179 60.25 -86.77
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "intercept1" "intercept2" "intercept3"
  .. ..$ : chr "Est."

To get the first matrix without knowing its name and the last row:

> tail(inter[[1]], 1)
             Est.
intercept3 -86.77

If you want to strip off the labels:

> as.vector(tail(inter[[1]], 1))
[1] -86.77

-----------------------------
David L. Carlson
Department of Anthropology
Texas A&M University



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Narendra Modi
Sent: Tuesday, July 5, 2016 1:42 PM
To: R-help at r-project.org
Subject: [R] How to extract "specific"/"last" intercept value from segmented package.

I am able to perform regression on a dataset as below:

plot(x,y)
lin.mod <- lm(y~x)
m <- mean(x)
m

segmented.mod <- segmented(lin.mod, seg.Z = ~x, psi= m)

plot(segmented.mod, add=T)
sl <- slope(segmented.mod)
inter <- intercept(segmented.mod)

summary(segmented.mod)    # Show Summary
sl                        # show all the slopes
inter                     # show all the intercepts


In my dataset, the above method correctly identifies the breakpoints and
hence I get two intercepts.

> inter
$x
              Est.
intercept1  -3.269
intercept2 -19.980

What I am interested is the "intercept2" value. How can I obtain this?

The method needs to be dynamic as in if the next dataset has 3 intercepts,
I would like to get "intercept3 value.

PD

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From varinsacha at yahoo.fr  Tue Jul  5 23:51:19 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Tue, 5 Jul 2016 21:51:19 +0000 (UTC)
Subject: [R] BCa Bootstrapped regression coefficients from lmrob
 function not working
In-Reply-To: <86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
	<159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
	<86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>
Message-ID: <93801972.4200933.1467755479792.JavaMail.yahoo@mail.yahoo.com>

Dear Professor Dalgaard,

I really thank you lots for your response. I have solved my problem. Now, I have tried to do the same (calculate the BCa bootstrapped CIs) for the MARS regression, and I get an error message. If somebody has a hint to solve my problem, would be highly appreciated.

Reproducible example :


Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),

QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),

competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),

innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))

install.packages("earth")

library(earth)

newdata=na.omit(Dataset)

model=earth(PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation,data=newdata)

summary(model)

plot(model)

plotmo(model)


boot.MARS=function(formula,data,indices) {

d=data[indices,]

fit=earth(formula,data=d)

return(coef(fit))

}

library(boot)

results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)

boot.ci(results, type= "bca",index=2)


Best,
S

________________________________
De : peter dalgaard <pdalgd at gmail.com>

Cc : R-help Mailing List <r-help at r-project.org>
Envoy? le : Dimanche 3 juillet 2016 18h19
Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working



> On 03 Jul 2016, at 13:47 , varin sacha via R-help <r-help at r-project.org> wrote:
> 
> Dear R-experts,
> 
> I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).
> 
> My R code here below is showing a warning message ([1] "All values of t are equal to 
> 22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?

You need to review your code. You calculate a,b,c,d in the global environment and create newdata as a subset of Dataset, then use a,b,c,d in the formula, but no such variables are in newdata. AFAICT, all your bootstrap fits use the _same_ global values for a,b,c,d hence give the same result 1000 times...

-pd



> 
> Here is the reproducible example
> 
> 
> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
> 
> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
> 
> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
> 
> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
> 
> library("robustbase")
> newdata=na.omit(Dataset)
> a=Dataset$PIBparHab
> b=Dataset$QUALITESANSREDONDANCE
> c=Dataset$competitivite
> d=Dataset$innovation
> 
> fm.lmrob=lmrob(a~b+c+d,data=newdata)
> fm.lmrob
> 
> boot.Lmrob=function(formula,data,indices) {
> d=data[indices,]
> fit=lmrob(formula,data=d)
> return(coef(fit))
> }
> 
> library(boot)
> results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
> boot.ci(results, type= "bca",index=2)
> 
> 
> Any help would be highly appreciated,
> S
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From bgunter.4567 at gmail.com  Wed Jul  6 01:19:42 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 5 Jul 2016 16:19:42 -0700
Subject: [R] BCa Bootstrapped regression coefficients from lmrob
 function not working
In-Reply-To: <93801972.4200933.1467755479792.JavaMail.yahoo@mail.yahoo.com>
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
	<159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
	<86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>
	<93801972.4200933.1467755479792.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAGxFJbQJPcsq4WXoDk_c0JFROGkMSerXYvM1xcaRDjFMGYHObg@mail.gmail.com>

It would help to show your  error message, n'est-ce pas?

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Jul 5, 2016 at 2:51 PM, varin sacha via R-help
<r-help at r-project.org> wrote:
> Dear Professor Dalgaard,
>
> I really thank you lots for your response. I have solved my problem. Now, I have tried to do the same (calculate the BCa bootstrapped CIs) for the MARS regression, and I get an error message. If somebody has a hint to solve my problem, would be highly appreciated.
>
> Reproducible example :
>
>
> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>
> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>
> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>
> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>
> install.packages("earth")
>
> library(earth)
>
> newdata=na.omit(Dataset)
>
> model=earth(PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation,data=newdata)
>
> summary(model)
>
> plot(model)
>
> plotmo(model)
>
>
> boot.MARS=function(formula,data,indices) {
>
> d=data[indices,]
>
> fit=earth(formula,data=d)
>
> return(coef(fit))
>
> }
>
> library(boot)
>
> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
>
> boot.ci(results, type= "bca",index=2)
>
>
> Best,
> S
>
> ________________________________
> De : peter dalgaard <pdalgd at gmail.com>
>
> Cc : R-help Mailing List <r-help at r-project.org>
> Envoy? le : Dimanche 3 juillet 2016 18h19
> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>
>
>
>> On 03 Jul 2016, at 13:47 , varin sacha via R-help <r-help at r-project.org> wrote:
>>
>> Dear R-experts,
>>
>> I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).
>>
>> My R code here below is showing a warning message ([1] "All values of t are equal to
>> 22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?
>
> You need to review your code. You calculate a,b,c,d in the global environment and create newdata as a subset of Dataset, then use a,b,c,d in the formula, but no such variables are in newdata. AFAICT, all your bootstrap fits use the _same_ global values for a,b,c,d hence give the same result 1000 times...
>
> -pd
>
>
>
>>
>> Here is the reproducible example
>>
>>
>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>>
>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>>
>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>>
>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>>
>> library("robustbase")
>> newdata=na.omit(Dataset)
>> a=Dataset$PIBparHab
>> b=Dataset$QUALITESANSREDONDANCE
>> c=Dataset$competitivite
>> d=Dataset$innovation
>>
>> fm.lmrob=lmrob(a~b+c+d,data=newdata)
>> fm.lmrob
>>
>> boot.Lmrob=function(formula,data,indices) {
>> d=data[indices,]
>> fit=lmrob(formula,data=d)
>> return(coef(fit))
>> }
>>
>> library(boot)
>> results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
>> boot.ci(results, type= "bca",index=2)
>>
>>
>> Any help would be highly appreciated,
>> S
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From miaojpm at gmail.com  Wed Jul  6 06:21:06 2016
From: miaojpm at gmail.com (John)
Date: Tue, 5 Jul 2016 21:21:06 -0700
Subject: [R] Can R read Word fonts and comments?
In-Reply-To: <CAJ4QxaOAvfhVCnVrf0MkdOD3jPMqQbymamwKCVheAPW0q6HF9A@mail.gmail.com>
References: <CABcx46D0gNOHEau_iEd_F-hMM1MA=J0r41sLwoFY8SMgE47nZw@mail.gmail.com>
	<CAGxFJbSSnMK7_cCfAMXVMT1fM3zWUQsjmNwfgtb_fh5XzRRdCQ@mail.gmail.com>
	<CABcx46A5O2Oc5XMdBSM=uHHQJdbvMp6U=8AfcOYpx5_Jr3CrpA@mail.gmail.com>
	<CAGxFJbS0s0g88v7aN9MJQBjEVkhB+9vUhS1WOJi9+UAAP2H9Uw@mail.gmail.com>
	<5FFC2C2F-82C2-4773-90CB-0C32DF869086@comcast.net>
	<CAJ4QxaPUdgAupbCKzYtR++NQhEmre+CA1jjkESdPx9d8EGo8fg@mail.gmail.com>
	<CABcx46BJxR8zC4diZ7yxNDNZdY58iksm4ghSkh6S0+LUsfSgYg@mail.gmail.com>
	<CAJ4QxaOAvfhVCnVrf0MkdOD3jPMqQbymamwKCVheAPW0q6HF9A@mail.gmail.com>
Message-ID: <CABcx46BbYR8daGurFGX=T+UAmcrYdhHfHoyovVmnnv-AGMnVkA@mail.gmail.com>

Thanks, Bob. Regarding other functionalities I would request:
Extract all text with (1) a specific color or (2) a specific font (3)
underlines. For example, if one highlights in red, then she would like to
extract all texts in red. Occasionally she might use more than one colors,
and each color bears its own implication. Thanks!

John



2016-07-05 10:03 GMT-07:00 boB Rudis <bob at rudis.net>:

> I'll dig into that (was hoping the small feature addition wld cause
> enhanced feature requests :-)
>
> On Tue, Jul 5, 2016 at 1:02 PM, John <miaojpm at gmail.com> wrote:
> > Thank you, David and Bert, for the info.
> > Thank you, Bob, for this excellent function. Allow me to request a
> feature:
> > You highlighted the following text, and comment "This is the first
> comment".
> >
> > "Lorem ipsum dolor sit amet, cu sit modus voluptua accommodare, meis
> > disputando voluptatibus eu nec, qui te modo solum delicata. Eam scripta
> > maluisset urbanitas et, numquam disputationi in pri, vis tibique deserunt
> > accusamus ut. Vis movet admodum probatus cu, ex pri ludus possit.
> Molestiae
> > efficiendi at vix, eu labore elaboraret deterruisset mei, et eos persius
> > nominati."
> >
> > Could you let the function output the above text (with the comments, of
> > course), which you highlighted for comment?
> >
> > Thanks,
> >
> > John
> >
> >
> >
> > 2016-07-02 14:12 GMT-07:00 boB Rudis <bob at rudis.net>:
> >>
> >> I just added `docx_extract_all_cmnts()` (and a cpl other
> >> comments-related things) to the dev version of `docxtractr`
> >> (https://github.com/hrbrmstr/docxtractr). You can use
> >> `devtools::install_github("hrbrmstr/docxtractr")` to install it.
> >> There's an example in the help for that function.
> >>
> >> Give it a go and file detailed issues for other functionality you need.
> >>
> >> On Fri, Jul 1, 2016 at 11:14 PM, David Winsemius <
> dwinsemius at comcast.net>
> >> wrote:
> >> > It?s my understanding that docx and xlsx files are zipped containers
> >> > that have their data in XML files. You should try unzipping one and
> >> > examining it with a viewer. You may then be able to use pkg:XML.
> >> >
> >> > ?
> >> > David.
> >> >
> >> >> On Jul 1, 2016, at 3:13 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >> >>
> >> >> No, sorry -- all I would do is search.
> >> >>
> >> >> -- Bert
> >> >>
> >> >>
> >> >> Bert Gunter
> >> >>
> >> >> "The trouble with having an open mind is that people keep coming
> along
> >> >> and sticking things into it."
> >> >> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >> >>
> >> >>
> >> >> On Fri, Jul 1, 2016 at 2:33 PM, John <miaojpm at gmail.com> wrote:
> >> >>> Yes, I have done some search (e.g., tm, markdown, etc), but I can't
> >> >>> find
> >> >>> this function.
> >> >>> If you know any package that works for this purpose, that would be
> >> >>> quite
> >> >>> helpful.
> >> >>> Thanks,
> >> >>>
> >> >>> John
> >> >>>
> >> >>> 2016-06-28 16:50 GMT-07:00 Bert Gunter <bgunter.4567 at gmail.com>:
> >> >>>>
> >> >>>> Did you try searching before posting here? -- e.g. a web search or
> on
> >> >>>> rseek.org ?
> >> >>>>
> >> >>>> Cheers,
> >> >>>> Bert
> >> >>>>
> >> >>>>
> >> >>>> Bert Gunter
> >> >>>>
> >> >>>> "The trouble with having an open mind is that people keep coming
> >> >>>> along
> >> >>>> and sticking things into it."
> >> >>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >> >>>>
> >> >>>>
> >> >>>> On Tue, Jun 28, 2016 at 3:53 PM, John <miaojpm at gmail.com> wrote:
> >> >>>>> Hi,
> >> >>>>>
> >> >>>>>   From time to time I highlight the word documents with red/blue
> >> >>>>> color
> >> >>>>> or
> >> >>>>> italic/bold fonts, and I also add comments to a file. Is there a
> >> >>>>> package/function to let R extract the italic/bold blue/red words
> and
> >> >>>>> comments from a docx/doc file?
> >> >>>>>
> >> >>>>>   I am aware that there are a few packages reading Word, but don't
> >> >>>>> know
> >> >>>>> which one is able to do it.
> >> >>>>>
> >> >>>>>   Thanks,
> >> >>>>>
> >> >>>>> John
> >> >>>>>
> >> >>>>>        [[alternative HTML version deleted]]
> >> >>>>>
> >> >>>>> ______________________________________________
> >> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>>>> PLEASE do read the posting guide
> >> >>>>> http://www.R-project.org/posting-guide.html
> >> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >> >>>
> >> >>>
> >> >>
> >> >> ______________________________________________
> >> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> PLEASE do read the posting guide
> >> >> http://www.R-project.org/posting-guide.html
> >> >> and provide commented, minimal, self-contained, reproducible code.
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> > http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
>

	[[alternative HTML version deleted]]


From varinsacha at yahoo.fr  Wed Jul  6 10:15:40 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Wed, 6 Jul 2016 08:15:40 +0000 (UTC)
Subject: [R] BCa Bootstrapped regression coefficients from lmrob
 function not working
In-Reply-To: <CAGxFJbQJPcsq4WXoDk_c0JFROGkMSerXYvM1xcaRDjFMGYHObg@mail.gmail.com>
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
	<159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
	<86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>
	<93801972.4200933.1467755479792.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbQJPcsq4WXoDk_c0JFROGkMSerXYvM1xcaRDjFMGYHObg@mail.gmail.com>
Message-ID: <301272418.4387108.1467792940035.JavaMail.yahoo@mail.yahoo.com>

Dear Bert,

You are right.

> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
Erreur dans boot(data = newdata, statistic = boot.MARS, R = 1000, formula = PIBparHab ~  : 
le nombre d'objets ? remplacer n'est pas multiple de la taille du remplacement 

In English it would be something like : number of items to replace is not a multiple of replacement length


Best
S

________________________________
De : Bert Gunter <bgunter.4567 at gmail.com>

Cc : peter dalgaard <pdalgd at gmail.com>; R-help Mailing List <r-help at r-project.org>
Envoy? le : Mercredi 6 juillet 2016 1h19
Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working


It would help to show your  error message, n'est-ce pas?

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Jul 5, 2016 at 2:51 PM, varin sacha via R-help
<r-help at r-project.org> wrote:
> Dear Professor Dalgaard,
>
> I really thank you lots for your response. I have solved my problem. Now, I have tried to do the same (calculate the BCa bootstrapped CIs) for the MARS regression, and I get an error message. If somebody has a hint to solve my problem, would be highly appreciated.
>
> Reproducible example :
>
>
> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>
> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>
> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>
> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>
> install.packages("earth")
>
> library(earth)
>
> newdata=na.omit(Dataset)
>
> model=earth(PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation,data=newdata)
>
> summary(model)
>
> plot(model)
>
> plotmo(model)
>
>
> boot.MARS=function(formula,data,indices) {
>
> d=data[indices,]
>
> fit=earth(formula,data=d)
>
> return(coef(fit))
>
> }
>
> library(boot)
>
> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
>
> boot.ci(results, type= "bca",index=2)
>
>
> Best,
> S
>
> ________________________________
> De : peter dalgaard <pdalgd at gmail.com>
>
> Cc : R-help Mailing List <r-help at r-project.org>
> Envoy? le : Dimanche 3 juillet 2016 18h19
> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>
>
>
>> On 03 Jul 2016, at 13:47 , varin sacha via R-help <r-help at r-project.org> wrote:
>>
>> Dear R-experts,
>>
>> I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).
>>
>> My R code here below is showing a warning message ([1] "All values of t are equal to
>> 22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?
>
> You need to review your code. You calculate a,b,c,d in the global environment and create newdata as a subset of Dataset, then use a,b,c,d in the formula, but no such variables are in newdata. AFAICT, all your bootstrap fits use the _same_ global values for a,b,c,d hence give the same result 1000 times...
>
> -pd
>
>
>
>>
>> Here is the reproducible example
>>
>>
>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>>
>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>>
>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>>
>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>>
>> library("robustbase")
>> newdata=na.omit(Dataset)
>> a=Dataset$PIBparHab
>> b=Dataset$QUALITESANSREDONDANCE
>> c=Dataset$competitivite
>> d=Dataset$innovation
>>
>> fm.lmrob=lmrob(a~b+c+d,data=newdata)
>> fm.lmrob
>>
>> boot.Lmrob=function(formula,data,indices) {
>> d=data[indices,]
>> fit=lmrob(formula,data=d)
>> return(coef(fit))
>> }
>>
>> library(boot)
>> results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
>> boot.ci(results, type= "bca",index=2)
>>
>>
>> Any help would be highly appreciated,
>> S
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From G.Maubach at weinwolf.de  Wed Jul  6 10:34:18 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Wed, 6 Jul 2016 10:34:18 +0200
Subject: [R] WG: Fw: Re: dplyr : row total for all groups in dplyr summarise
Message-ID: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>

Hi All,

if I run the suggested code

mtcars %>%
  group_by (am, gear) %>%
  summarise (n = n()) %>%
  mutate(rel.freq = paste0(round(100 * n / sum(n), 0), "%")) %>%
  ungroup() %>%
  plyr::rbind.fill(data.frame(n = nrow(mtcars), rel.freq =
                                              "100%?))

I get

> mtcars %>%
+   group_by (am, gear) %>%
+   summarise (n = n()) %>%
+   mutate(rel.freq = paste0(round(100 * n / sum(n), 0), "%")) %>%
+   ungroup() %>%
+   plyr::rbind.fill(data.frame(n = nrow(mtcars), rel.freq =
+                                               "100%?))




+ 


R stops execution cause something within the prgram syntax is missing.

What has to be changed to be able to run the code?

Kind regards

Georg Maubach


> Gesendet: Dienstag, 05. Juli 2016 um 18:30 Uhr
> Von: "David Winsemius" <dwinsemius at comcast.net>
> An: maicel at infomed.sld.cu
> Cc: r-help at r-project.org
> Betreff: Re: [R] dplyr : row total for all groups in dplyr summarise
>
> 
> 
> mtcars %>%
>    group_by (am, gear) %>%
>    summarise (n=n()) %>%
>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>    ungroup() %>% plyr::rbind.fill(data.frame( 
n=nrow(mtcars),rel.freq="100%?))
> 
> 
> > On Jul 5, 2016, at 4:47 AM, maicel at infomed.sld.cu wrote:
> > 
> > Sorry, what I wanted to do was to add a total row at the end of the 
summary. The marginal totals by columns correspond to 100% and the sum of 
levels.
> > best reagard
> > Maicel Monzon
> > 
> > 
> > Ulrik Stervbo <ulrik.stervbo at gmail.com> escribi?:
> > 
> >> Yes. But in the sample code the data is summarised. In which case you 
get 4
> >> rows and not the correct 32.
> >> 
> >> On Tue, 5 Jul 2016, 07:48 David Winsemius, <dwinsemius at comcast.net> 
wrote:
> >> 
> >>> nrow(mtcars)
> >>> 
> >>> 
> >>> Sent from my iPhone
> >>> 
> >>> On Jul 4, 2016, at 9:03 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> 
wrote:
> >>> 
> >>> That will give you the wrong result when used on summarised data
> >>> 
> >>> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 
2016
> >>> 02:10:
> >>> 
> >>>> I thought there was an nrow() function?
> >>>> 
> >>>> Sent from my iPhone
> >>>> 
> >>>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> >>>> wrote:
> >>>> 
> >>>> If you want the total number of rows in the original data.frame 
after
> >>>> counting the rows in each group, you can ungroup and sum the row 
counts,
> >>>> like:
> >>>> 
> >>>> library("dplyr")
> >>>> 
> >>>> 
> >>>> mtcars %>%
> >>>>   group_by (am, gear) %>%
> >>>>   summarise (n=n()) %>%
> >>>>   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
> >>>>   ungroup() %>%
> >>>>   mutate(row.tot = sum(n))
> >>>> 
> >>>> HTH
> >>>> Ulrik
> >>>> 
> >>>> On Mon, 4 Jul 2016 at 18:23 David Winsemius 
<dwinsemius at comcast.net>
> >>>> wrote:
> >>>> 
> >>>>> 
> >>>>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
> >>>>> >
> >>>>> > Hello,
> >>>>> > How can I aggregate row total for all groups in dplyr summarise 
?
> >>>>> 
> >>>>> Row total ? of what? Aggregate ? how? What is the desired answer?
> >>>>> 
> >>>>> 
> >>>>> 
> >>>>> > library(dplyr)
> >>>>> > mtcars %>%
> >>>>> >  group_by (am, gear) %>%
> >>>>> >  summarise (n=n()) %>%
> >>>>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
> >>>>> >
> >>>>> > best regard
> >>>>> > Maicel Monzon
> >>>>> >
> >>>>> >
> >>>>> >
> >>>>> > ----------------------------------------------------------------
> >>>>> >
> >>>>> >
> >>>>> >
> >>>>> >
> >>>>> > --
> >>>>> > Este mensaje le ha llegado mediante el servicio de correo 
electronico
> >>>>> que ofrece Infomed para respaldar el cumplimiento de las misiones 
del
> >>>>> Sistema Nacional de Salud. La persona que envia este correo asume 
el
> >>>>> compromiso de usar el servicio a tales fines y cumplir con las 
regulaciones
> >>>>> establecidas
> >>>>> >
> >>>>> > Infomed: http://www.sld.cu/
> >>>>> >
> >>>>> > ______________________________________________
> >>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, 
see
> >>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> > PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> > and provide commented, minimal, self-contained, reproducible 
code.
> >>>>> 
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>> 
> >>>> 
> >> 
> > 
> > 
> > 
> > ----------------------------------------------------------------
> > This message was sent using IMP, the Internet Messaging Program.
> > 
> > 
> > 
> > --
> > Este mensaje le ha llegado mediante el servicio de correo electronico 
que ofrece Infomed para respaldar el cumplimiento de las misiones del 
Sistema Nacional de Salud. La persona que envia este correo asume el 
compromiso de usar el servicio a tales fines y cumplir con las 
regulaciones establecidas
> > 
> > Infomed: http://www.sld.cu/
> > 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pdalgd at gmail.com  Wed Jul  6 11:05:44 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 6 Jul 2016 11:05:44 +0200
Subject: [R] BCa Bootstrapped regression coefficients from lmrob
	function not working
In-Reply-To: <301272418.4387108.1467792940035.JavaMail.yahoo@mail.yahoo.com>
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
	<159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
	<86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>
	<93801972.4200933.1467755479792.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbQJPcsq4WXoDk_c0JFROGkMSerXYvM1xcaRDjFMGYHObg@mail.gmail.com>
	<301272418.4387108.1467792940035.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <4291D32B-BFC7-4324-BA0E-399D9B4C5080@gmail.com>

Offhand, I would suspect that the cause is that earth() does not return a coefficient vector of the same length in every bootstrap iteration (_adaptive_ regression splines). This makes the bootstrap rather tricky to even define.

-pd

> On 06 Jul 2016, at 10:15 , varin sacha <varinsacha at yahoo.fr> wrote:
> 
> Dear Bert,
> 
> You are right.
> 
>> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
> Erreur dans boot(data = newdata, statistic = boot.MARS, R = 1000, formula = PIBparHab ~  : 
> le nombre d'objets ? remplacer n'est pas multiple de la taille du remplacement 
> 
> In English it would be something like : number of items to replace is not a multiple of replacement length
> 
> 
> Best
> S
> 
> ________________________________
> De : Bert Gunter <bgunter.4567 at gmail.com>
> ? : varin sacha <varinsacha at yahoo.fr> 
> Cc : peter dalgaard <pdalgd at gmail.com>; R-help Mailing List <r-help at r-project.org>
> Envoy? le : Mercredi 6 juillet 2016 1h19
> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
> 
> 
> It would help to show your  error message, n'est-ce pas?
> 
> Cheers,
> Bert
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Tue, Jul 5, 2016 at 2:51 PM, varin sacha via R-help
> <r-help at r-project.org> wrote:
>> Dear Professor Dalgaard,
>> 
>> I really thank you lots for your response. I have solved my problem. Now, I have tried to do the same (calculate the BCa bootstrapped CIs) for the MARS regression, and I get an error message. If somebody has a hint to solve my problem, would be highly appreciated.
>> 
>> Reproducible example :
>> 
>> 
>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>> 
>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>> 
>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>> 
>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>> 
>> install.packages("earth")
>> 
>> library(earth)
>> 
>> newdata=na.omit(Dataset)
>> 
>> model=earth(PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation,data=newdata)
>> 
>> summary(model)
>> 
>> plot(model)
>> 
>> plotmo(model)
>> 
>> 
>> boot.MARS=function(formula,data,indices) {
>> 
>> d=data[indices,]
>> 
>> fit=earth(formula,data=d)
>> 
>> return(coef(fit))
>> 
>> }
>> 
>> library(boot)
>> 
>> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
>> 
>> boot.ci(results, type= "bca",index=2)
>> 
>> 
>> Best,
>> S
>> 
>> ________________________________
>> De : peter dalgaard <pdalgd at gmail.com>
>> 
>> Cc : R-help Mailing List <r-help at r-project.org>
>> Envoy? le : Dimanche 3 juillet 2016 18h19
>> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>> 
>> 
>> 
>>> On 03 Jul 2016, at 13:47 , varin sacha via R-help <r-help at r-project.org> wrote:
>>> 
>>> Dear R-experts,
>>> 
>>> I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).
>>> 
>>> My R code here below is showing a warning message ([1] "All values of t are equal to
>>> 22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?
>> 
>> You need to review your code. You calculate a,b,c,d in the global environment and create newdata as a subset of Dataset, then use a,b,c,d in the formula, but no such variables are in newdata. AFAICT, all your bootstrap fits use the _same_ global values for a,b,c,d hence give the same result 1000 times...
>> 
>> -pd
>> 
>> 
>> 
>>> 
>>> Here is the reproducible example
>>> 
>>> 
>>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>>> 
>>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>>> 
>>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>>> 
>>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>>> 
>>> library("robustbase")
>>> newdata=na.omit(Dataset)
>>> a=Dataset$PIBparHab
>>> b=Dataset$QUALITESANSREDONDANCE
>>> c=Dataset$competitivite
>>> d=Dataset$innovation
>>> 
>>> fm.lmrob=lmrob(a~b+c+d,data=newdata)
>>> fm.lmrob
>>> 
>>> boot.Lmrob=function(formula,data,indices) {
>>> d=data[indices,]
>>> fit=lmrob(formula,data=d)
>>> return(coef(fit))
>>> }
>>> 
>>> library(boot)
>>> results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
>>> boot.ci(results, type= "bca",index=2)
>>> 
>>> 
>>> Any help would be highly appreciated,
>>> S
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From varinsacha at yahoo.fr  Wed Jul  6 13:56:39 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Wed, 6 Jul 2016 11:56:39 +0000 (UTC)
Subject: [R] BCa Bootstrapped regression coefficients from lmrob
 function not working
In-Reply-To: <4291D32B-BFC7-4324-BA0E-399D9B4C5080@gmail.com>
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
	<159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
	<86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>
	<93801972.4200933.1467755479792.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbQJPcsq4WXoDk_c0JFROGkMSerXYvM1xcaRDjFMGYHObg@mail.gmail.com>
	<301272418.4387108.1467792940035.JavaMail.yahoo@mail.yahoo.com>
	<4291D32B-BFC7-4324-BA0E-399D9B4C5080@gmail.com>
Message-ID: <386752845.4714872.1467806199114.JavaMail.yahoo@mail.yahoo.com>

Dear Professor Dalgaard,

Okay, this is what I was afraid of.
Many thanks for your response. I know that my next question is off-topic here (on this website) but is it nevertheless possible to calculate confidence intervals for MARS regression or CIs for MARS will just be impossible to calculate ?

Best
S

 

----- Mail original -----
De : peter dalgaard <pdalgd at gmail.com>
? : varin sacha <varinsacha at yahoo.fr>
Cc : Bert Gunter <bgunter.4567 at gmail.com>; R-help Mailing List <r-help at r-project.org>
Envoy? le : Mercredi 6 juillet 2016 11h05
Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working

Offhand, I would suspect that the cause is that earth() does not return a coefficient vector of the same length in every bootstrap iteration (_adaptive_ regression splines). This makes the bootstrap rather tricky to even define.

-pd

> On 06 Jul 2016, at 10:15 , varin sacha <varinsacha at yahoo.fr> wrote:
> 
> Dear Bert,
> 
> You are right.
> 
>> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
> Erreur dans boot(data = newdata, statistic = boot.MARS, R = 1000, formula = PIBparHab ~  : 
> le nombre d'objets ? remplacer n'est pas multiple de la taille du remplacement 
> 
> In English it would be something like : number of items to replace is not a multiple of replacement length
> 
> 
> Best
> S
> 
> ________________________________
> De : Bert Gunter <bgunter.4567 at gmail.com>
> ? : varin sacha <varinsacha at yahoo.fr> 
> Cc : peter dalgaard <pdalgd at gmail.com>; R-help Mailing List <r-help at r-project.org>
> Envoy? le : Mercredi 6 juillet 2016 1h19
> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
> 
> 
> It would help to show your  error message, n'est-ce pas?
> 
> Cheers,
> Bert
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Tue, Jul 5, 2016 at 2:51 PM, varin sacha via R-help
> <r-help at r-project.org> wrote:
>> Dear Professor Dalgaard,
>> 
>> I really thank you lots for your response. I have solved my problem. Now, I have tried to do the same (calculate the BCa bootstrapped CIs) for the MARS regression, and I get an error message. If somebody has a hint to solve my problem, would be highly appreciated.
>> 
>> Reproducible example :
>> 
>> 
>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>> 
>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>> 
>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>> 
>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>> 
>> install.packages("earth")
>> 
>> library(earth)
>> 
>> newdata=na.omit(Dataset)
>> 
>> model=earth(PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation,data=newdata)
>> 
>> summary(model)
>> 
>> plot(model)
>> 
>> plotmo(model)
>> 
>> 
>> boot.MARS=function(formula,data,indices) {
>> 
>> d=data[indices,]
>> 
>> fit=earth(formula,data=d)
>> 
>> return(coef(fit))
>> 
>> }
>> 
>> library(boot)
>> 
>> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
>> 
>> boot.ci(results, type= "bca",index=2)
>> 
>> 
>> Best,
>> S
>> 
>> ________________________________
>> De : peter dalgaard <pdalgd at gmail.com>
>> 
>> Cc : R-help Mailing List <r-help at r-project.org>
>> Envoy? le : Dimanche 3 juillet 2016 18h19
>> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>> 
>> 
>> 
>>> On 03 Jul 2016, at 13:47 , varin sacha via R-help <r-help at r-project.org> wrote:
>>> 
>>> Dear R-experts,
>>> 
>>> I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).
>>> 
>>> My R code here below is showing a warning message ([1] "All values of t are equal to
>>> 22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?
>> 
>> You need to review your code. You calculate a,b,c,d in the global environment and create newdata as a subset of Dataset, then use a,b,c,d in the formula, but no such variables are in newdata. AFAICT, all your bootstrap fits use the _same_ global values for a,b,c,d hence give the same result 1000 times...
>> 
>> -pd
>> 
>> 
>> 
>>> 
>>> Here is the reproducible example
>>> 
>>> 
>>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>>> 
>>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>>> 
>>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>>> 
>>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>>> 
>>> library("robustbase")
>>> newdata=na.omit(Dataset)
>>> a=Dataset$PIBparHab
>>> b=Dataset$QUALITESANSREDONDANCE
>>> c=Dataset$competitivite
>>> d=Dataset$innovation
>>> 
>>> fm.lmrob=lmrob(a~b+c+d,data=newdata)
>>> fm.lmrob
>>> 
>>> boot.Lmrob=function(formula,data,indices) {
>>> d=data[indices,]
>>> fit=lmrob(formula,data=d)
>>> return(coef(fit))
>>> }
>>> 
>>> library(boot)
>>> results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
>>> boot.ci(results, type= "bca",index=2)
>>> 
>>> 
>>> Any help would be highly appreciated,
>>> S
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From murdoch.duncan at gmail.com  Wed Jul  6 14:13:07 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 6 Jul 2016 08:13:07 -0400
Subject: [R] BCa Bootstrapped regression coefficients from lmrob
 function not working
In-Reply-To: <386752845.4714872.1467806199114.JavaMail.yahoo@mail.yahoo.com>
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
	<159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
	<86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>
	<93801972.4200933.1467755479792.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbQJPcsq4WXoDk_c0JFROGkMSerXYvM1xcaRDjFMGYHObg@mail.gmail.com>
	<301272418.4387108.1467792940035.JavaMail.yahoo@mail.yahoo.com>
	<4291D32B-BFC7-4324-BA0E-399D9B4C5080@gmail.com>
	<386752845.4714872.1467806199114.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <7bd657fe-4fb9-5cc0-be58-792e5efe48c2@gmail.com>

On 06/07/2016 7:56 AM, varin sacha via R-help wrote:
> Dear Professor Dalgaard,
>
> Okay, this is what I was afraid of.
> Many thanks for your response. I know that my next question is off-topic here (on this website) but is it nevertheless possible to calculate confidence intervals for MARS regression or CIs for MARS will just be impossible to calculate ?

If you think of the fitted curve or surface as the output rather than 
the coefficients that define it, it makes sense to get pointwise 
confidence intervals, or joint confidence regions for multiple 
locations.  Just do predictions at a set of fixed locations.

Duncan Murdoch

> Best
> S
>
>
>
> ----- Mail original -----
> De : peter dalgaard <pdalgd at gmail.com>
> ? : varin sacha <varinsacha at yahoo.fr>
> Cc : Bert Gunter <bgunter.4567 at gmail.com>; R-help Mailing List <r-help at r-project.org>
> Envoy? le : Mercredi 6 juillet 2016 11h05
> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>
> Offhand, I would suspect that the cause is that earth() does not return a coefficient vector of the same length in every bootstrap iteration (_adaptive_ regression splines). This makes the bootstrap rather tricky to even define.
>
> -pd
>
>> On 06 Jul 2016, at 10:15 , varin sacha <varinsacha at yahoo.fr> wrote:
>>
>> Dear Bert,
>>
>> You are right.
>>
>>> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
>> Erreur dans boot(data = newdata, statistic = boot.MARS, R = 1000, formula = PIBparHab ~  :
>> le nombre d'objets ? remplacer n'est pas multiple de la taille du remplacement
>>
>> In English it would be something like : number of items to replace is not a multiple of replacement length
>>
>>
>> Best
>> S
>>
>> ________________________________
>> De : Bert Gunter <bgunter.4567 at gmail.com>
>> ? : varin sacha <varinsacha at yahoo.fr>
>> Cc : peter dalgaard <pdalgd at gmail.com>; R-help Mailing List <r-help at r-project.org>
>> Envoy? le : Mercredi 6 juillet 2016 1h19
>> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>>
>>
>> It would help to show your  error message, n'est-ce pas?
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Tue, Jul 5, 2016 at 2:51 PM, varin sacha via R-help
>> <r-help at r-project.org> wrote:
>>> Dear Professor Dalgaard,
>>>
>>> I really thank you lots for your response. I have solved my problem. Now, I have tried to do the same (calculate the BCa bootstrapped CIs) for the MARS regression, and I get an error message. If somebody has a hint to solve my problem, would be highly appreciated.
>>>
>>> Reproducible example :
>>>
>>>
>>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>>>
>>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>>>
>>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>>>
>>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>>>
>>> install.packages("earth")
>>>
>>> library(earth)
>>>
>>> newdata=na.omit(Dataset)
>>>
>>> model=earth(PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation,data=newdata)
>>>
>>> summary(model)
>>>
>>> plot(model)
>>>
>>> plotmo(model)
>>>
>>>
>>> boot.MARS=function(formula,data,indices) {
>>>
>>> d=data[indices,]
>>>
>>> fit=earth(formula,data=d)
>>>
>>> return(coef(fit))
>>>
>>> }
>>>
>>> library(boot)
>>>
>>> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
>>>
>>> boot.ci(results, type= "bca",index=2)
>>>
>>>
>>> Best,
>>> S
>>>
>>> ________________________________
>>> De : peter dalgaard <pdalgd at gmail.com>
>>>
>>> Cc : R-help Mailing List <r-help at r-project.org>
>>> Envoy? le : Dimanche 3 juillet 2016 18h19
>>> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>>>
>>>
>>>
>>>> On 03 Jul 2016, at 13:47 , varin sacha via R-help <r-help at r-project.org> wrote:
>>>>
>>>> Dear R-experts,
>>>>
>>>> I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).
>>>>
>>>> My R code here below is showing a warning message ([1] "All values of t are equal to
>>>> 22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?
>>>
>>> You need to review your code. You calculate a,b,c,d in the global environment and create newdata as a subset of Dataset, then use a,b,c,d in the formula, but no such variables are in newdata. AFAICT, all your bootstrap fits use the _same_ global values for a,b,c,d hence give the same result 1000 times...
>>>
>>> -pd
>>>
>>>
>>>
>>>>
>>>> Here is the reproducible example
>>>>
>>>>
>>>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>>>>
>>>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>>>>
>>>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>>>>
>>>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>>>>
>>>> library("robustbase")
>>>> newdata=na.omit(Dataset)
>>>> a=Dataset$PIBparHab
>>>> b=Dataset$QUALITESANSREDONDANCE
>>>> c=Dataset$competitivite
>>>> d=Dataset$innovation
>>>>
>>>> fm.lmrob=lmrob(a~b+c+d,data=newdata)
>>>> fm.lmrob
>>>>
>>>> boot.Lmrob=function(formula,data,indices) {
>>>> d=data[indices,]
>>>> fit=lmrob(formula,data=d)
>>>> return(coef(fit))
>>>> }
>>>>
>>>> library(boot)
>>>> results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
>>>> boot.ci(results, type= "bca",index=2)
>>>>
>>>>
>>>> Any help would be highly appreciated,
>>>> S
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> --
>>> Peter Dalgaard, Professor,
>>> Center for Statistics, Copenhagen Business School
>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>> Phone: (+45)38153501
>>> Office: A 4.23
>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>


From G.Maubach at weinwolf.de  Wed Jul  6 15:50:34 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Wed, 6 Jul 2016 15:50:34 +0200
Subject: [R] Formatting ggplot2 graph
Message-ID: <OFB849146D.473F48B5-ONC1257FE8.004A388F-C1257FE8.004C143B@lotus.hawesko.de>

Hi All,

my current code looks lke this:

freq_ls <- structure(list(Var1 = c("zldkkd", "aakdkdk", 
                                   "aaakdkd", "aaieiwo", "v?alsl", 
"ssddkdk", 
                                   "glowowp", "vvvvlaoiw", "ruklow", 
"rolsl", 
                                   "delk", "inslvnz"), Anzahl = c(1772L, 
761L, 
 536L, 317L, 197L, 160L, 30L, 20L, 10L, 6L, 6L, 1L), Prozent = c(46.4, 
                                                             19.9, 14, 
8.3, 5.2, 4.2, 0.8, 0.5, 0.3, 0.2, 0.2, 0)), .Names = c("Var1", 
                                                      "Anzahl", 
"Prozent"), class = c("tbl_df", "data.frame"), row.names = c(NA, 
                                                     -12L))
ggplot(freq_ls) +
  geom_bar(aes(x = Var1,
               y = Anzahl),
           stat = "identity",
           fill = "gray") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Title of the Plot")

I would like to add the abolute and relative frequencies on top of the 
bars. In addition I want the values printed in descending ording according 
to the data.

I searched the web and found:

geom_text(stat='bin',aes(label=..count..),vjust=-1)

(Source: 
http://stackoverflow.com/questions/26553526/how-to-add-frequency-count-labels-to-the-bars-in-a-bar-graph-using-ggplot2
)

but this does not work in my case. Inserting the code

ggplot(freq_ls) +
  geom_bar(aes(x = Var1,
               y = Anzahl),
           stat = "identity",
           fill = "gray") +
  geom_text(stat='bin',aes(label=..count..),vjust=-1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Title of the Plot")

results in

`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
Warning messages:
1: In is.na(x) : is.na() applied to non-(list or vector) of type 'NULL'
2: Removed 1 rows containing missing values (geom_text). 


I looked in the book Wickhan: ggplot2 but could find an answer to the 
question:

- How to show number if tey are pre-calculated?
- How to sort the bars according to the sequence of values in descending 
order or if - pre-ordered - in the given order?

What do I have to change in my code to do it?

Kind regards

Georg


From ddalthorp at usgs.gov  Wed Jul  6 16:18:12 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Wed, 6 Jul 2016 07:18:12 -0700
Subject: [R] tclArray() --- "active"?!
Message-ID: <CAJeYpE_ZZtoD7EJrm=gBaNNeRgf08d1Jn=zpeL_QXuFjrD_E6w@mail.gmail.com>

Sometimes when working with tclArray's, an empty element called "active" is
appended to the array. Does anyone know when (and why) this happens? And
how to prevent it (or at least predict it so that it can be removed by
hand)?

E.g.,
library(tcltk); library(tcltk2)
tt<-tktoplevel()
dat<-tclArray()
tab<-tk2table(tt, variable = dat)
tkgrid(tab)
dat[[0,0]]<-5
dat[[1,0]]<-4
dat[[2,0]]<-56
tkdelete(tab,"rows",2, 8)
names(dat)

Any help would be greatly appreciated!

-Dan

-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Wed Jul  6 16:41:49 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 6 Jul 2016 14:41:49 +0000
Subject: [R] WG: Fw: Re: dplyr : row total for all groups in dplyr
 summarise
In-Reply-To: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>
References: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>
Message-ID: <2fe9867d8a48431c989f5887b29f08e4@exch-2p-mbx-t2.ads.tamu.edu>

It's the cut and paste monster. Somewhere along the way, the final " got converted to ? which R does not see.

> "100%?

+ > "100%"
[1] "100%"

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of G.Maubach at weinwolf.de
Sent: Wednesday, July 6, 2016 3:34 AM
To: r-help at r-project.org
Subject: [R] WG: Fw: Re: dplyr : row total for all groups in dplyr summarise

Hi All,

if I run the suggested code

mtcars %>%
  group_by (am, gear) %>%
  summarise (n = n()) %>%
  mutate(rel.freq = paste0(round(100 * n / sum(n), 0), "%")) %>%
  ungroup() %>%
  plyr::rbind.fill(data.frame(n = nrow(mtcars), rel.freq =
                                              "100%?))

I get

> mtcars %>%
+   group_by (am, gear) %>%
+   summarise (n = n()) %>%
+   mutate(rel.freq = paste0(round(100 * n / sum(n), 0), "%")) %>%
+   ungroup() %>%
+   plyr::rbind.fill(data.frame(n = nrow(mtcars), rel.freq =
+                                               "100%?))




+ 


R stops execution cause something within the prgram syntax is missing.

What has to be changed to be able to run the code?

Kind regards

Georg Maubach


> Gesendet: Dienstag, 05. Juli 2016 um 18:30 Uhr
> Von: "David Winsemius" <dwinsemius at comcast.net>
> An: maicel at infomed.sld.cu
> Cc: r-help at r-project.org
> Betreff: Re: [R] dplyr : row total for all groups in dplyr summarise
>
> 
> 
> mtcars %>%
>    group_by (am, gear) %>%
>    summarise (n=n()) %>%
>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>    ungroup() %>% plyr::rbind.fill(data.frame( 
n=nrow(mtcars),rel.freq="100%?))
> 
> 
> > On Jul 5, 2016, at 4:47 AM, maicel at infomed.sld.cu wrote:
> > 
> > Sorry, what I wanted to do was to add a total row at the end of the 
summary. The marginal totals by columns correspond to 100% and the sum of 
levels.
> > best reagard
> > Maicel Monzon
> > 
> > 
> > Ulrik Stervbo <ulrik.stervbo at gmail.com> escribi?:
> > 
> >> Yes. But in the sample code the data is summarised. In which case you 
get 4
> >> rows and not the correct 32.
> >> 
> >> On Tue, 5 Jul 2016, 07:48 David Winsemius, <dwinsemius at comcast.net> 
wrote:
> >> 
> >>> nrow(mtcars)
> >>> 
> >>> 
> >>> Sent from my iPhone
> >>> 
> >>> On Jul 4, 2016, at 9:03 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> 
wrote:
> >>> 
> >>> That will give you the wrong result when used on summarised data
> >>> 
> >>> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 
2016
> >>> 02:10:
> >>> 
> >>>> I thought there was an nrow() function?
> >>>> 
> >>>> Sent from my iPhone
> >>>> 
> >>>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> >>>> wrote:
> >>>> 
> >>>> If you want the total number of rows in the original data.frame 
after
> >>>> counting the rows in each group, you can ungroup and sum the row 
counts,
> >>>> like:
> >>>> 
> >>>> library("dplyr")
> >>>> 
> >>>> 
> >>>> mtcars %>%
> >>>>   group_by (am, gear) %>%
> >>>>   summarise (n=n()) %>%
> >>>>   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
> >>>>   ungroup() %>%
> >>>>   mutate(row.tot = sum(n))
> >>>> 
> >>>> HTH
> >>>> Ulrik
> >>>> 
> >>>> On Mon, 4 Jul 2016 at 18:23 David Winsemius 
<dwinsemius at comcast.net>
> >>>> wrote:
> >>>> 
> >>>>> 
> >>>>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
> >>>>> >
> >>>>> > Hello,
> >>>>> > How can I aggregate row total for all groups in dplyr summarise 
?
> >>>>> 
> >>>>> Row total ? of what? Aggregate ? how? What is the desired answer?
> >>>>> 
> >>>>> 
> >>>>> 
> >>>>> > library(dplyr)
> >>>>> > mtcars %>%
> >>>>> >  group_by (am, gear) %>%
> >>>>> >  summarise (n=n()) %>%
> >>>>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
> >>>>> >
> >>>>> > best regard
> >>>>> > Maicel Monzon
> >>>>> >
> >>>>> >
> >>>>> >
> >>>>> > ----------------------------------------------------------------
> >>>>> >
> >>>>> >
> >>>>> >
> >>>>> >
> >>>>> > --
> >>>>> > Este mensaje le ha llegado mediante el servicio de correo 
electronico
> >>>>> que ofrece Infomed para respaldar el cumplimiento de las misiones 
del
> >>>>> Sistema Nacional de Salud. La persona que envia este correo asume 
el
> >>>>> compromiso de usar el servicio a tales fines y cumplir con las 
regulaciones
> >>>>> establecidas
> >>>>> >
> >>>>> > Infomed: http://www.sld.cu/
> >>>>> >
> >>>>> > ______________________________________________
> >>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, 
see
> >>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> > PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> > and provide commented, minimal, self-contained, reproducible 
code.
> >>>>> 
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>> 
> >>>> 
> >> 
> > 
> > 
> > 
> > ----------------------------------------------------------------
> > This message was sent using IMP, the Internet Messaging Program.
> > 
> > 
> > 
> > --
> > Este mensaje le ha llegado mediante el servicio de correo electronico 
que ofrece Infomed para respaldar el cumplimiento de las misiones del 
Sistema Nacional de Salud. La persona que envia este correo asume el 
compromiso de usar el servicio a tales fines y cumplir con las 
regulaciones establecidas
> > 
> > Infomed: http://www.sld.cu/
> > 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From jdnewmil at dcn.davis.ca.us  Wed Jul  6 17:24:03 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 06 Jul 2016 08:24:03 -0700
Subject: [R] WG: Fw: Re: dplyr : row total for all groups in dplyr
	summarise
In-Reply-To: <2fe9867d8a48431c989f5887b29f08e4@exch-2p-mbx-t2.ads.tamu.edu>
References: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>
	<2fe9867d8a48431c989f5887b29f08e4@exch-2p-mbx-t2.ads.tamu.edu>
Message-ID: <67435D66-3CC6-4D6A-B925-4CF4FE7EB30A@dcn.davis.ca.us>

Cut and paste is not to blame... it is the use of word processing software rather than text editors for manipulating code that is the problem. 

Georg: note that plyr does not mix very well with dplyr... try to pick one and stick with it. 
-- 
Sent from my phone. Please excuse my brevity.

On July 6, 2016 7:41:49 AM PDT, David L Carlson <dcarlson at tamu.edu> wrote:
>It's the cut and paste monster. Somewhere along the way, the final "
>got converted to ? which R does not see.
>
>> "100%?
>
>+ > "100%"
>[1] "100%"
>
>-------------------------------------
>David L Carlson
>Department of Anthropology
>Texas A&M University
>College Station, TX 77840-4352
>
>
>-----Original Message-----
>From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>G.Maubach at weinwolf.de
>Sent: Wednesday, July 6, 2016 3:34 AM
>To: r-help at r-project.org
>Subject: [R] WG: Fw: Re: dplyr : row total for all groups in dplyr
>summarise
>
>Hi All,
>
>if I run the suggested code
>
>mtcars %>%
>  group_by (am, gear) %>%
>  summarise (n = n()) %>%
>  mutate(rel.freq = paste0(round(100 * n / sum(n), 0), "%")) %>%
>  ungroup() %>%
>  plyr::rbind.fill(data.frame(n = nrow(mtcars), rel.freq =
>                                              "100%?))
>
>I get
>
>> mtcars %>%
>+   group_by (am, gear) %>%
>+   summarise (n = n()) %>%
>+   mutate(rel.freq = paste0(round(100 * n / sum(n), 0), "%")) %>%
>+   ungroup() %>%
>+   plyr::rbind.fill(data.frame(n = nrow(mtcars), rel.freq =
>+                                               "100%?))
>
>
>
>
>+ 
>
>
>R stops execution cause something within the prgram syntax is missing.
>
>What has to be changed to be able to run the code?
>
>Kind regards
>
>Georg Maubach
>
>
>> Gesendet: Dienstag, 05. Juli 2016 um 18:30 Uhr
>> Von: "David Winsemius" <dwinsemius at comcast.net>
>> An: maicel at infomed.sld.cu
>> Cc: r-help at r-project.org
>> Betreff: Re: [R] dplyr : row total for all groups in dplyr summarise
>>
>> 
>> 
>> mtcars %>%
>>    group_by (am, gear) %>%
>>    summarise (n=n()) %>%
>>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>>    ungroup() %>% plyr::rbind.fill(data.frame( 
>n=nrow(mtcars),rel.freq="100%?))
>> 
>> 
>> > On Jul 5, 2016, at 4:47 AM, maicel at infomed.sld.cu wrote:
>> > 
>> > Sorry, what I wanted to do was to add a total row at the end of the
>
>summary. The marginal totals by columns correspond to 100% and the sum
>of 
>levels.
>> > best reagard
>> > Maicel Monzon
>> > 
>> > 
>> > Ulrik Stervbo <ulrik.stervbo at gmail.com> escribi?:
>> > 
>> >> Yes. But in the sample code the data is summarised. In which case
>you 
>get 4
>> >> rows and not the correct 32.
>> >> 
>> >> On Tue, 5 Jul 2016, 07:48 David Winsemius,
><dwinsemius at comcast.net> 
>wrote:
>> >> 
>> >>> nrow(mtcars)
>> >>> 
>> >>> 
>> >>> Sent from my iPhone
>> >>> 
>> >>> On Jul 4, 2016, at 9:03 PM, Ulrik Stervbo
><ulrik.stervbo at gmail.com> 
>wrote:
>> >>> 
>> >>> That will give you the wrong result when used on summarised data
>> >>> 
>> >>> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 
>2016
>> >>> 02:10:
>> >>> 
>> >>>> I thought there was an nrow() function?
>> >>>> 
>> >>>> Sent from my iPhone
>> >>>> 
>> >>>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo
><ulrik.stervbo at gmail.com>
>> >>>> wrote:
>> >>>> 
>> >>>> If you want the total number of rows in the original data.frame 
>after
>> >>>> counting the rows in each group, you can ungroup and sum the row
>
>counts,
>> >>>> like:
>> >>>> 
>> >>>> library("dplyr")
>> >>>> 
>> >>>> 
>> >>>> mtcars %>%
>> >>>>   group_by (am, gear) %>%
>> >>>>   summarise (n=n()) %>%
>> >>>>   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>> >>>>   ungroup() %>%
>> >>>>   mutate(row.tot = sum(n))
>> >>>> 
>> >>>> HTH
>> >>>> Ulrik
>> >>>> 
>> >>>> On Mon, 4 Jul 2016 at 18:23 David Winsemius 
><dwinsemius at comcast.net>
>> >>>> wrote:
>> >>>> 
>> >>>>> 
>> >>>>> > On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>> >>>>> >
>> >>>>> > Hello,
>> >>>>> > How can I aggregate row total for all groups in dplyr
>summarise 
>?
>> >>>>> 
>> >>>>> Row total ? of what? Aggregate ? how? What is the desired
>answer?
>> >>>>> 
>> >>>>> 
>> >>>>> 
>> >>>>> > library(dplyr)
>> >>>>> > mtcars %>%
>> >>>>> >  group_by (am, gear) %>%
>> >>>>> >  summarise (n=n()) %>%
>> >>>>> >  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>> >>>>> >
>> >>>>> > best regard
>> >>>>> > Maicel Monzon
>> >>>>> >
>> >>>>> >
>> >>>>> >
>> >>>>> >
>----------------------------------------------------------------
>> >>>>> >
>> >>>>> >
>> >>>>> >
>> >>>>> >
>> >>>>> > --
>> >>>>> > Este mensaje le ha llegado mediante el servicio de correo 
>electronico
>> >>>>> que ofrece Infomed para respaldar el cumplimiento de las
>misiones 
>del
>> >>>>> Sistema Nacional de Salud. La persona que envia este correo
>asume 
>el
>> >>>>> compromiso de usar el servicio a tales fines y cumplir con las 
>regulaciones
>> >>>>> establecidas
>> >>>>> >
>> >>>>> > Infomed: http://www.sld.cu/
>> >>>>> >
>> >>>>> > ______________________________________________
>> >>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>
>see
>> >>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>> > PLEASE do read the posting guide
>> >>>>> http://www.R-project.org/posting-guide.html
>> >>>>> > and provide commented, minimal, self-contained, reproducible 
>code.
>> >>>>> 
>> >>>>> ______________________________________________
>> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>> PLEASE do read the posting guide
>> >>>>> http://www.R-project.org/posting-guide.html
>> >>>>> and provide commented, minimal, self-contained, reproducible
>code.
>> >>>> 
>> >>>> 
>> >> 
>> > 
>> > 
>> > 
>> > ----------------------------------------------------------------
>> > This message was sent using IMP, the Internet Messaging Program.
>> > 
>> > 
>> > 
>> > --
>> > Este mensaje le ha llegado mediante el servicio de correo
>electronico 
>que ofrece Infomed para respaldar el cumplimiento de las misiones del 
>Sistema Nacional de Salud. La persona que envia este correo asume el 
>compromiso de usar el servicio a tales fines y cumplir con las 
>regulaciones establecidas
>> > 
>> > Infomed: http://www.sld.cu/
>> > 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ulrik.stervbo at gmail.com  Wed Jul  6 17:26:23 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Wed, 06 Jul 2016 15:26:23 +0000
Subject: [R] Formatting ggplot2 graph
In-Reply-To: <OFB849146D.473F48B5-ONC1257FE8.004A388F-C1257FE8.004C143B@lotus.hawesko.de>
References: <OFB849146D.473F48B5-ONC1257FE8.004A388F-C1257FE8.004C143B@lotus.hawesko.de>
Message-ID: <CAKVAULNctuPsDqaSYhtXze8fVMSdtDGo7s1ndfGY--fY3ji_tA@mail.gmail.com>

Hi Georg,

Your problem with the geom_text was that you tried to count Var1 in some
bins which cannot be defined on characters - the counts you really wanted
to use are in Anzahl. This should do what you want:

library(ggplot2)

freq_ls <- structure(list(Var1 = c("zldkkd", "aakdkdk","aaakdkd",
"aaieiwo", "v?alsl", "ssddkdk","glowowp", "vvvvlaoiw",
"ruklow","rolsl","delk", "inslvnz"),
Anzahl = c(1772L,761L, 536L, 317L, 197L, 160L, 30L, 20L, 10L, 6L, 6L, 1L),
Prozent = c(46.4, 19.9, 14, 8.3, 5.2, 4.2, 0.8, 0.5, 0.3, 0.2, 0.2, 0)),
.Names = c("Var1","Anzahl","Prozent"),
class = c("tbl_df", "data.frame"), row.names = c(NA,-12L))

# Calculate relative frequencies
freq_ls$rel.freq <- freq_ls$Anzahl/sum(freq_ls$Anzahl)

# Create a string for the plot
freq_ls$bar.text <- paste(freq_ls$Anzahl, round(freq_ls$rel.freq * 100,
digits = 2), sep ="/\n")

# Set the order
freq_ls <- freq_ls[order(freq_ls$Anzahl, decreasing = TRUE), ]
freq_ls$Var1 <- factor(freq_ls$Var1, levels = freq_ls$Var1)

ggplot(freq_ls) +
aes(x = Var1, y = Anzahl) +
  geom_bar(stat = "identity", fill = "gray") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_text(aes(label=bar.text),vjust=0) +
  ggtitle("Title of the Plot") +
  coord_cartesian(ylim=c(0, 2000))

I have move the x and y to the global aesteathics so I won't have to repeat
when adding the geom_text.

I have created an extra column with the strings you want to print on the
plot rather than creating it on the fly.

If no order is predefined ggplot created one by calling factor which
results is some alphabetic order. I therefore first sort the table and set
the factors of Var1 to be the order in the table.

The coord_cartesian  adjusts the y-axis to show the label for the first
column.

Best
Ulrik


On Wed, 6 Jul 2016 at 15:53 <G.Maubach at weinwolf.de> wrote:

> Hi All,
>
> my current code looks lke this:
>
> freq_ls <- structure(list(Var1 = c("zldkkd", "aakdkdk",
>                                    "aaakdkd", "aaieiwo", "v?alsl",
> "ssddkdk",
>                                    "glowowp", "vvvvlaoiw", "ruklow",
> "rolsl",
>                                    "delk", "inslvnz"), Anzahl = c(1772L,
> 761L,
>  536L, 317L, 197L, 160L, 30L, 20L, 10L, 6L, 6L, 1L), Prozent = c(46.4,
>                                                              19.9, 14,
> 8.3, 5.2, 4.2, 0.8, 0.5, 0.3, 0.2, 0.2, 0)), .Names = c("Var1",
>                                                       "Anzahl",
> "Prozent"), class = c("tbl_df", "data.frame"), row.names = c(NA,
>                                                      -12L))
> ggplot(freq_ls) +
>   geom_bar(aes(x = Var1,
>                y = Anzahl),
>            stat = "identity",
>            fill = "gray") +
>   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
>   ggtitle("Title of the Plot")
>
> I would like to add the abolute and relative frequencies on top of the
> bars. In addition I want the values printed in descending ording according
> to the data.
>
> I searched the web and found:
>
> geom_text(stat='bin',aes(label=..count..),vjust=-1)
>
> (Source:
>
> http://stackoverflow.com/questions/26553526/how-to-add-frequency-count-labels-to-the-bars-in-a-bar-graph-using-ggplot2
> )
>
> but this does not work in my case. Inserting the code
>
> ggplot(freq_ls) +
>   geom_bar(aes(x = Var1,
>                y = Anzahl),
>            stat = "identity",
>            fill = "gray") +
>   geom_text(stat='bin',aes(label=..count..),vjust=-1) +
>   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
>   ggtitle("Title of the Plot")
>
> results in
>
> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
> Warning messages:
> 1: In is.na(x) : is.na() applied to non-(list or vector) of type 'NULL'
> 2: Removed 1 rows containing missing values (geom_text).
>
>
> I looked in the book Wickhan: ggplot2 but could find an answer to the
> question:
>
> - How to show number if tey are pre-calculated?
> - How to sort the bars according to the sequence of values in descending
> order or if - pre-ordered - in the given order?
>
> What do I have to change in my code to do it?
>
> Kind regards
>
> Georg
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Wed Jul  6 17:35:34 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 6 Jul 2016 17:35:34 +0200
Subject: [R] tclArray() --- "active"?!
In-Reply-To: <CAJeYpE_ZZtoD7EJrm=gBaNNeRgf08d1Jn=zpeL_QXuFjrD_E6w@mail.gmail.com>
References: <CAJeYpE_ZZtoD7EJrm=gBaNNeRgf08d1Jn=zpeL_QXuFjrD_E6w@mail.gmail.com>
Message-ID: <CF6EA76E-0DB4-427E-ADBC-9FBAA33D86EE@gmail.com>

I am not up to speed on Tcl/Tk these days, but I would suspect that the issue lies in tkTable and not in tclArray. The table widget has the notion of an active cell, which can be used in indexing operations. According to the widget documentation

--
Command-Line Name:-variable
Database Name: variable
Database Class: Variable

Global Tcl array variable to attach to the table's C array. It will be created if it doesn't already exist or is a simple variable. Keys used by the table in the array are of the form row,col for cells and the special key active which contains the value of the active cell buffer. [...]
--

I think this suggests that you just need to assume that the "active" element might be there when you deal with the tclArray and work around it if it gets in the way. 

(A conjecture as to why it appears in your example could be that you deleted the row with the active cell, and that the widget needs to record the fact that there is now no active cell.)

-pd
 

> On 06 Jul 2016, at 16:18 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> 
> Sometimes when working with tclArray's, an empty element called "active" is
> appended to the array. Does anyone know when (and why) this happens? And
> how to prevent it (or at least predict it so that it can be removed by
> hand)?
> 
> E.g.,
> library(tcltk); library(tcltk2)
> tt<-tktoplevel()
> dat<-tclArray()
> tab<-tk2table(tt, variable = dat)
> tkgrid(tab)
> dat[[0,0]]<-5
> dat[[1,0]]<-4
> dat[[2,0]]<-56
> tkdelete(tab,"rows",2, 8)
> names(dat)
> 
> Any help would be greatly appreciated!
> 
> -Dan
> 
> -- 
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ddalthorp at usgs.gov  Wed Jul  6 18:00:39 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Wed, 6 Jul 2016 09:00:39 -0700
Subject: [R] tclArray() --- "active"?!
In-Reply-To: <CF6EA76E-0DB4-427E-ADBC-9FBAA33D86EE@gmail.com>
References: <CAJeYpE_ZZtoD7EJrm=gBaNNeRgf08d1Jn=zpeL_QXuFjrD_E6w@mail.gmail.com>
	<CF6EA76E-0DB4-427E-ADBC-9FBAA33D86EE@gmail.com>
Message-ID: <CAJeYpE9zssbfW=ucWcEBwn_wD+Le7S2yca95RQ9AQExvHpOsTw@mail.gmail.com>

Thanks, Peter.

When it comes time to read the array back to R, I can explicitly delete the
"active" cell first...

On Wed, Jul 6, 2016 at 8:35 AM, peter dalgaard <pdalgd at gmail.com> wrote:

> I am not up to speed on Tcl/Tk these days, but I would suspect that the
> issue lies in tkTable and not in tclArray. The table widget has the notion
> of an active cell, which can be used in indexing operations. According to
> the widget documentation
>
> --
> Command-Line Name:-variable
> Database Name: variable
> Database Class: Variable
>
> Global Tcl array variable to attach to the table's C array. It will be
> created if it doesn't already exist or is a simple variable. Keys used by
> the table in the array are of the form row,col for cells and the special
> key active which contains the value of the active cell buffer. [...]
> --
>
> I think this suggests that you just need to assume that the "active"
> element might be there when you deal with the tclArray and work around it
> if it gets in the way.
>
> (A conjecture as to why it appears in your example could be that you
> deleted the row with the active cell, and that the widget needs to record
> the fact that there is now no active cell.)
>
> -pd
>
>
> > On 06 Jul 2016, at 16:18 , Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> >
> > Sometimes when working with tclArray's, an empty element called "active"
> is
> > appended to the array. Does anyone know when (and why) this happens? And
> > how to prevent it (or at least predict it so that it can be removed by
> > hand)?
> >
> > E.g.,
> > library(tcltk); library(tcltk2)
> > tt<-tktoplevel()
> > dat<-tclArray()
> > tab<-tk2table(tt, variable = dat)
> > tkgrid(tab)
> > dat[[0,0]]<-5
> > dat[[1,0]]<-4
> > dat[[2,0]]<-56
> > tkdelete(tab,"rows",2, 8)
> > names(dat)
> >
> > Any help would be greatly appreciated!
> >
> > -Dan
> >
> > --
> > Dan Dalthorp, PhD
> > USGS Forest and Rangeland Ecosystem Science Center
> > Forest Sciences Lab, Rm 189
> > 3200 SW Jefferson Way
> > Corvallis, OR 97331
> > ph: 541-750-0953
> > ddalthorp at usgs.gov
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul  6 18:36:45 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Jul 2016 09:36:45 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <67435D66-3CC6-4D6A-B925-4CF4FE7EB30A@dcn.davis.ca.us>
References: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>
	<2fe9867d8a48431c989f5887b29f08e4@exch-2p-mbx-t2.ads.tamu.edu>
	<67435D66-3CC6-4D6A-B925-4CF4FE7EB30A@dcn.davis.ca.us>
Message-ID: <DDBB9CB7-0789-4105-81D4-FFC714054C44@comcast.net>


> On Jul 6, 2016, at 8:24 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> Cut and paste is not to blame... it is the use of word processing software rather than text editors for manipulating code that is the problem. 

In this case the text was cut from the R session console text and pasted without modification into Mail.app version 8.2. In replicating this action, I see now that hitting "return" then unfortunately converts the final double-quote to a "smart-quote". So the fault is mine because I'm responsible for my software choices.


> 
> Georg: note that plyr does not mix very well with dplyr... try to pick one and stick with it. 

Agreed. Note that I did not load plyr, but rather called a single function from it. Looking at my sessionInfo() from that session, I see that plyr was loaded via NAMESPACE and I'm guessing that was due to having loaded ggplot2 (which many people would also have). However, I'm guessing that `rbind.fill` would fail if the plyr NAMESPACE links were not available since it is dependent on several other plyr functions.

-- 
David.


> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On July 6, 2016 7:41:49 AM PDT, David L Carlson <dcarlson at tamu.edu> wrote:
>> It's the cut and paste monster. Somewhere along the way, the final "
>> got converted to ? which R does not see.
>> 
>>> "100%?
>> 
>> + > "100%"
>> [1] "100%"
>> 
>> -------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77840-4352
>> 
>> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>> G.Maubach at weinwolf.de
>> Sent: Wednesday, July 6, 2016 3:34 AM
>> To: r-help at r-project.org
>> Subject: [R] WG: Fw: Re: dplyr : row total for all groups in dplyr
>> summarise
>> 
>> Hi All,
>> 
>> if I run the suggested code
>> 
>> mtcars %>%
>> group_by (am, gear) %>%
>> summarise (n = n()) %>%
>> mutate(rel.freq = paste0(round(100 * n / sum(n), 0), "%")) %>%
>> ungroup() %>%
>> plyr::rbind.fill(data.frame(n = nrow(mtcars), rel.freq =
>>                                             "100%?))
>> 
>> I get
>> 
>>> mtcars %>%
>> +   group_by (am, gear) %>%
>> +   summarise (n = n()) %>%
>> +   mutate(rel.freq = paste0(round(100 * n / sum(n), 0), "%")) %>%
>> +   ungroup() %>%
>> +   plyr::rbind.fill(data.frame(n = nrow(mtcars), rel.freq =
>> +                                               "100%?))
>> 
>> 
>> 
>> 
>> + 
>> 
>> 
>> R stops execution cause something within the prgram syntax is missing.
>> 
>> What has to be changed to be able to run the code?
>> 
>> Kind regards
>> 
>> Georg Maubach
>> 
>> 
>>> Gesendet: Dienstag, 05. Juli 2016 um 18:30 Uhr
>>> Von: "David Winsemius" <dwinsemius at comcast.net>
>>> An: maicel at infomed.sld.cu
>>> Cc: r-help at r-project.org
>>> Betreff: Re: [R] dplyr : row total for all groups in dplyr summarise
>>> 
>>> 
>>> 
>>> mtcars %>%
>>>   group_by (am, gear) %>%
>>>   summarise (n=n()) %>%
>>>   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>>>   ungroup() %>% plyr::rbind.fill(data.frame( 
>> n=nrow(mtcars),rel.freq="100%?))
>>> 
>>> 
>>>> On Jul 5, 2016, at 4:47 AM, maicel at infomed.sld.cu wrote:
>>>> 
>>>> Sorry, what I wanted to do was to add a total row at the end of the
>> 
>> summary. The marginal totals by columns correspond to 100% and the sum
>> of 
>> levels.
>>>> best reagard
>>>> Maicel Monzon
>>>> 
>>>> 
>>>> Ulrik Stervbo <ulrik.stervbo at gmail.com> escribi?:
>>>> 
>>>>> Yes. But in the sample code the data is summarised. In which case
>> you 
>> get 4
>>>>> rows and not the correct 32.
>>>>> 
>>>>> On Tue, 5 Jul 2016, 07:48 David Winsemius,
>> <dwinsemius at comcast.net> 
>> wrote:
>>>>> 
>>>>>> nrow(mtcars)
>>>>>> 
>>>>>> 
>>>>>> Sent from my iPhone
>>>>>> 
>>>>>> On Jul 4, 2016, at 9:03 PM, Ulrik Stervbo
>> <ulrik.stervbo at gmail.com> 
>> wrote:
>>>>>> 
>>>>>> That will give you the wrong result when used on summarised data
>>>>>> 
>>>>>> David Winsemius <dwinsemius at comcast.net> schrieb am Di., 5. Juli 
>> 2016
>>>>>> 02:10:
>>>>>> 
>>>>>>> I thought there was an nrow() function?
>>>>>>> 
>>>>>>> Sent from my iPhone
>>>>>>> 
>>>>>>> On Jul 4, 2016, at 9:59 AM, Ulrik Stervbo
>> <ulrik.stervbo at gmail.com>
>>>>>>> wrote:
>>>>>>> 
>>>>>>> If you want the total number of rows in the original data.frame 
>> after
>>>>>>> counting the rows in each group, you can ungroup and sum the row
>> 
>> counts,
>>>>>>> like:
>>>>>>> 
>>>>>>> library("dplyr")
>>>>>>> 
>>>>>>> 
>>>>>>> mtcars %>%
>>>>>>>  group_by (am, gear) %>%
>>>>>>>  summarise (n=n()) %>%
>>>>>>>  mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>>>>>>>  ungroup() %>%
>>>>>>>  mutate(row.tot = sum(n))
>>>>>>> 
>>>>>>> HTH
>>>>>>> Ulrik
>>>>>>> 
>>>>>>> On Mon, 4 Jul 2016 at 18:23 David Winsemius 
>> <dwinsemius at comcast.net>
>>>>>>> wrote:
>>>>>>> 
>>>>>>>> 
>>>>>>>>> On Jul 4, 2016, at 6:56 AM, maicel at infomed.sld.cu wrote:
>>>>>>>>> 
>>>>>>>>> Hello,
>>>>>>>>> How can I aggregate row total for all groups in dplyr
>> summarise 
>> ?
>>>>>>>> 
>>>>>>>> Row total ? of what? Aggregate ? how? What is the desired
>> answer?
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> library(dplyr)
>>>>>>>>> mtcars %>%
>>>>>>>>> group_by (am, gear) %>%
>>>>>>>>> summarise (n=n()) %>%
>>>>>>>>> mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%"))
>>>>>>>>> 
>>>>>>>>> best regard
>>>>>>>>> Maicel Monzon
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>> ----------------------------------------------------------------
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> --
>>>>>>>>> Este mensaje le ha llegado mediante el servicio de correo 
>> electronico
>>>>>>>> que ofrece Infomed para respaldar el cumplimiento de las
>> misiones 
>> del
>>>>>>>> Sistema Nacional de Salud. La persona que envia este correo
>> asume 
>> el
>>>>>>>> compromiso de usar el servicio a tales fines y cumplir con las 
>> regulaciones
>>>>>>>> establecidas
>>>>>>>>> 
>>>>>>>>> Infomed: http://www.sld.cu/
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> 
>> see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible 
>> code.
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible
>> code.
>>>>>>> 
>>>>>>> 
>>>>> 
>>>> 
>>>> 
>>>> 
>>>> ----------------------------------------------------------------
>>>> This message was sent using IMP, the Internet Messaging Program.
>>>> 
>>>> 
>>>> 
>>>> --
>>>> Este mensaje le ha llegado mediante el servicio de correo
>> electronico 
>> que ofrece Infomed para respaldar el cumplimiento de las misiones del 
>> Sistema Nacional de Salud. La persona que envia este correo asume el 
>> compromiso de usar el servicio a tales fines y cumplir con las 
>> regulaciones establecidas
>>>> 
>>>> Infomed: http://www.sld.cu/
>>>> 
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bjpmodi2016 at gmail.com  Wed Jul  6 19:49:51 2016
From: bjpmodi2016 at gmail.com (Narendra Modi)
Date: Wed, 6 Jul 2016 12:49:51 -0500
Subject: [R] How to extract "specific"/"last" intercept value from
 segmented package.
In-Reply-To: <20160705202824.Horde.Tv3Jge5RCcfY2hHb4zx5gL6@mail.sapo.pt>
References: <CAPq=xQDemZdz-J+4bN6-ccRkD7p5NX5MTBCNpcOrHsVZeVgenw@mail.gmail.com>
	<20160705202824.Horde.Tv3Jge5RCcfY2hHb4zx5gL6@mail.sapo.pt>
Message-ID: <CAPq=xQCFtWeD9A99ZvzJ9z+UhppTfihG2VYCMmDvboRkakmMOQ@mail.gmail.com>

Thanks! that worked.

I also tested with the below method although your solution is faster
and done in fewer steps.

  inter <- intercept(segmented.mod)
  inter.m <- as.matrix(inter$x)
  inter.row <- nrow(inter.m)
  answer <- inter.m[inter.row,1]

PD

On Tue, Jul 5, 2016 at 2:28 PM,  <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> Try
>
> dimnames(inter$x)[[1]]
>
> You could have seen this by inspecting 'inter':
>
> str(inter)
>
> Hope this helps,
>
> Rui Barradas
>
>
> Citando Narendra Modi <bjpmodi2016 at gmail.com>:
>
> I am able to perform regression on a dataset as below:
>
> plot(x,y)
> lin.mod <- lm(y~x)
> m <- mean(x)
> m
>
> segmented.mod <- segmented(lin.mod, seg.Z = ~x, psi= m)
>
> plot(segmented.mod, add=T)
> sl <- slope(segmented.mod)
> inter <- intercept(segmented.mod)
>
> summary(segmented.mod)    # Show Summary
> sl                        # show all the slopes
> inter                     # show all the intercepts
>
>
> In my dataset, the above method correctly identifies the breakpoints and
> hence I get two intercepts.
>
> inter
>
> $x
>              Est.
> intercept1  -3.269
> intercept2 -19.980
>
> What I am interested is the "intercept2" value. How can I obtain this?
>
> The method needs to be dynamic as in if the next dataset has 3 intercepts,
> I would like to get "intercept3 value.
>
> PD
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.htmland provide commented, minimal,
> self-contained, reproducible code.
>
>
>


From ruipbarradas at sapo.pt  Wed Jul  6 20:15:07 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Wed, 06 Jul 2016 19:15:07 +0100
Subject: [R] How to extract "specific"/"last" intercept value from
 segmented package.
In-Reply-To: <CAPq=xQCFtWeD9A99ZvzJ9z+UhppTfihG2VYCMmDvboRkakmMOQ@mail.gmail.com>
References: <CAPq=xQDemZdz-J+4bN6-ccRkD7p5NX5MTBCNpcOrHsVZeVgenw@mail.gmail.com>
	<20160705202824.Horde.Tv3Jge5RCcfY2hHb4zx5gL6@mail.sapo.pt>
	<CAPq=xQCFtWeD9A99ZvzJ9z+UhppTfihG2VYCMmDvboRkakmMOQ@mail.gmail.com>
Message-ID: <20160706191507.Horde.u4am27SpbYrot77Sd56lwNC@mail.sapo.pt>

Hello,

I'm glad that it helped.
Note, however, that you don't need inter.m <- as.matrix(...) because  
inter$x already is a matrix.
You can simply do

inter.row <- nrow(inter$x)
answer <- inter$x[inter.row,1]

Rui Barradas
?

Citando Narendra Modi <bjpmodi2016 at gmail.com>:

> Thanks! that worked.
>
> I also tested with the below method although your solution is faster
> and done in fewer steps.
>
> inter <- intercept(segmented.mod)
> inter.m <- as.matrix(inter$x)
> inter.row <- nrow(inter.m)
> answer <- inter.m[inter.row,1]
>
> PD
>
> On Tue, Jul 5, 2016 at 2:28 PM,? <ruipbarradas at sapo.pt> wrote:
>> Hello,
>>
>> Try
>>
>> dimnames(inter$x)[[1]]
>>
>> You could have seen this by inspecting 'inter':
>>
>> str(inter)
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> Citando Narendra Modi <bjpmodi2016 at gmail.com>:
>>
>> I am able to perform regression on a dataset as below:
>>
>> plot(x,y)
>> lin.mod <- lm(y~x)
>> m <- mean(x)
>> m
>>
>> segmented.mod <- segmented(lin.mod, seg.Z = ~x, psi= m)
>>
>> plot(segmented.mod, add=T)
>> sl <- slope(segmented.mod)
>> inter <- intercept(segmented.mod)
>>
>> summary(segmented.mod)? ? # Show Summary
>> sl? ? ? ? ? ? ? ? ? ? ? ? # show all the slopes
>> inter? ? ? ? ? ? ? ? ? ? ?# show all the intercepts
>>
>> In my dataset, the above method correctly identifies the breakpoints and
>> hence I get two intercepts.
>>
>> inter
>>
>> $x
>> ? ? ? ? ? ? ?Est.
>> intercept1? -3.269
>> intercept2 -19.980
>>
>> What I am interested is the "intercept2" value. How can I obtain this?
>>
>> The method needs to be dynamic as in if the next dataset has 3 intercepts,
>> I would like to get "intercept3 value.
>>
>> PD
>>
>> ? ? ? ? [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.htmland provide commented, minimal,
>> self-contained, reproducible code.
>> ?
>
> ?

	[[alternative HTML version deleted]]


From rmendelss at gmail.com  Wed Jul  6 18:45:48 2016
From: rmendelss at gmail.com (rmendelss gmail)
Date: Wed, 6 Jul 2016 09:45:48 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <DDBB9CB7-0789-4105-81D4-FFC714054C44@comcast.net>
References: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>
	<2fe9867d8a48431c989f5887b29f08e4@exch-2p-mbx-t2.ads.tamu.edu>
	<67435D66-3CC6-4D6A-B925-4CF4FE7EB30A@dcn.davis.ca.us>
	<DDBB9CB7-0789-4105-81D4-FFC714054C44@comcast.net>
Message-ID: <B1269A76-58D6-4D93-BC41-48DCE39C7F7B@gmail.com>


> On Jul 6, 2016, at 9:36 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> n this case the text was cut from the R session console text and pasted without modification into Mail.app version 8.2. In replicating this action, I see now that hitting "return" then unfortunately converts the final double-quote to a "smart-quote?.

Speaking of this, this happens to me a lot when I post, so any example code can not be copied and paste to be reproducible.  Does anyone know of a workaround for this?

Thanks,

-Roy


From jdnewmil at dcn.davis.ca.us  Wed Jul  6 20:44:22 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 06 Jul 2016 11:44:22 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <B1269A76-58D6-4D93-BC41-48DCE39C7F7B@gmail.com>
References: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>
	<2fe9867d8a48431c989f5887b29f08e4@exch-2p-mbx-t2.ads.tamu.edu>
	<67435D66-3CC6-4D6A-B925-4CF4FE7EB30A@dcn.davis.ca.us>
	<DDBB9CB7-0789-4105-81D4-FFC714054C44@comcast.net>
	<B1269A76-58D6-4D93-BC41-48DCE39C7F7B@gmail.com>
Message-ID: <E1A0BFFC-6EF1-49C8-9449-9E58BBF9D8B1@dcn.davis.ca.us>

My point is that this is highly software-dependent. Certain email programs and editors are worse than others in inclusion of configuration settings that allow you to avoid this problem. In general you need to look for "plain text" options, and some software has "Auto-Correct" options turned on by default. 
-- 
Sent from my phone. Please excuse my brevity.

On July 6, 2016 9:45:48 AM PDT, rmendelss gmail <rmendelss at gmail.com> wrote:
>
>> On Jul 6, 2016, at 9:36 AM, David Winsemius <dwinsemius at comcast.net>
>wrote:
>> 
>> n this case the text was cut from the R session console text and
>pasted without modification into Mail.app version 8.2. In replicating
>this action, I see now that hitting "return" then unfortunately
>converts the final double-quote to a "smart-quote?.
>
>Speaking of this, this happens to me a lot when I post, so any example
>code can not be copied and paste to be reproducible.  Does anyone know
>of a workaround for this?
>
>Thanks,
>
>-Roy


From dwinsemius at comcast.net  Wed Jul  6 20:42:06 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Jul 2016 11:42:06 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <B1269A76-58D6-4D93-BC41-48DCE39C7F7B@gmail.com>
References: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>
	<2fe9867d8a48431c989f5887b29f08e4@exch-2p-mbx-t2.ads.tamu.edu>
	<67435D66-3CC6-4D6A-B925-4CF4FE7EB30A@dcn.davis.ca.us>
	<DDBB9CB7-0789-4105-81D4-FFC714054C44@comcast.net>
	<B1269A76-58D6-4D93-BC41-48DCE39C7F7B@gmail.com>
Message-ID: <D90D4C91-5B0F-4E0E-B161-AC81129C2625@comcast.net>


> On Jul 6, 2016, at 9:45 AM, rmendelss gmail <rmendelss at gmail.com> wrote:
> 
> 
>> On Jul 6, 2016, at 9:36 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>> n this case the text was cut from the R session console text and pasted without modification into Mail.app version 8.2. In replicating this action, I see now that hitting "return" then unfortunately converts the final double-quote to a "smart-quote?.
> 
> Speaking of this, this happens to me a lot when I post, so any example code can not be copied and paste to be reproducible.  Does anyone know of a workaround for this?

Google search: Mail.app changes quotes

https://discussions.apple.com/thread/7065405?tstart=0

If this is on a Mac then the answer it to change setting on the Keyboard/text Control Panel: uncheck the Use smart quotes and dashes:




Test:

mtcars %>%
   group_by (am, gear) %>%
   summarise (n=n()) %>%
   mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
   ungroup() %>% plyr::rbind.fill(data.frame( n=nrow(mtcars),rel.freq="100%"))


Seems to work.
-- 
David
> 
> Thanks,
> 
> -Roy
> 
> 


From roy.mendelssohn at noaa.gov  Wed Jul  6 20:59:50 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 6 Jul 2016 11:59:50 -0700
Subject: [R] dplyr : row total for all groups in dplyr summarise
In-Reply-To: <D90D4C91-5B0F-4E0E-B161-AC81129C2625@comcast.net>
References: <OF3E875E22.D5AFE364-ONC1257FE8.002EE3A9-C1257FE8.002F1FC5@lotus.hawesko.de>
	<2fe9867d8a48431c989f5887b29f08e4@exch-2p-mbx-t2.ads.tamu.edu>
	<67435D66-3CC6-4D6A-B925-4CF4FE7EB30A@dcn.davis.ca.us>
	<DDBB9CB7-0789-4105-81D4-FFC714054C44@comcast.net>
	<B1269A76-58D6-4D93-BC41-48DCE39C7F7B@gmail.com>
	<D90D4C91-5B0F-4E0E-B161-AC81129C2625@comcast.net>
Message-ID: <1F891833-E69B-4E4F-9FC6-2A8B9E51209D@noaa.gov>

Thanks muchly.  I hate the smart quotes!

-Roy


> On Jul 6, 2016, at 11:42 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Jul 6, 2016, at 9:45 AM, rmendelss gmail <rmendelss at gmail.com> wrote:
>> 
>> 
>>> On Jul 6, 2016, at 9:36 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>>> 
>>> n this case the text was cut from the R session console text and pasted without modification into Mail.app version 8.2. In replicating this action, I see now that hitting "return" then unfortunately converts the final double-quote to a "smart-quote?.
>> 
>> Speaking of this, this happens to me a lot when I post, so any example code can not be copied and paste to be reproducible.  Does anyone know of a workaround for this?
> 
> Google search: Mail.app changes quotes
> 
> https://discussions.apple.com/thread/7065405?tstart=0
> 
> If this is on a Mac then the answer it to change setting on the Keyboard/text Control Panel: uncheck the Use smart quotes and dashes:
> 
> 
> <PastedGraphic-1.tiff>
> 
> Test:
> 
> mtcars %>%
>    group_by (am, gear) %>%
>    summarise (n=n()) %>%
>    mutate(rel.freq = paste0(round(100 * n/sum(n), 0), "%")) %>%
>    ungroup() %>% plyr::rbind.fill(data.frame( n=nrow(mtcars),rel.freq="100%"))
> 
> 
> Seems to work.
> -- 
> David
>> 
>> Thanks,
>> 
>> -Roy
>> 
>> 
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From souravray90 at gmail.com  Wed Jul  6 22:03:46 2016
From: souravray90 at gmail.com (Sourav Ray)
Date: Thu, 7 Jul 2016 01:33:46 +0530
Subject: [R] Performing Principal Cluster Analysis,
 k-means clustering etc. with PDB/DCD trajectory
Message-ID: <CAMU2JtAAAJdGPQ5VQ4kORp0EgOFs5-SGy=YcGVZJJ+Q87X33BQ@mail.gmail.com>

Hello

If someone knows how to convert an MD trajectory into a format acceptable
to R or has done any of the above analyses, please let me know.

Thanks and regards
Sourav

	[[alternative HTML version deleted]]


From varinsacha at yahoo.fr  Thu Jul  7 00:37:57 2016
From: varinsacha at yahoo.fr (varin sacha)
Date: Wed, 6 Jul 2016 22:37:57 +0000 (UTC)
Subject: [R] BCa Bootstrapped regression coefficients from lmrob
 function not working
In-Reply-To: <7bd657fe-4fb9-5cc0-be58-792e5efe48c2@gmail.com>
References: <159175670.1695656.1467546453831.JavaMail.yahoo.ref@mail.yahoo.com>
	<159175670.1695656.1467546453831.JavaMail.yahoo@mail.yahoo.com>
	<86C63AE6-9105-4698-91C4-D7A7B3825103@gmail.com>
	<93801972.4200933.1467755479792.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbQJPcsq4WXoDk_c0JFROGkMSerXYvM1xcaRDjFMGYHObg@mail.gmail.com>
	<301272418.4387108.1467792940035.JavaMail.yahoo@mail.yahoo.com>
	<4291D32B-BFC7-4324-BA0E-399D9B4C5080@gmail.com>
	<386752845.4714872.1467806199114.JavaMail.yahoo@mail.yahoo.com>
	<7bd657fe-4fb9-5cc0-be58-792e5efe48c2@gmail.com>
Message-ID: <271595004.5212478.1467844677842.JavaMail.yahoo@mail.yahoo.com>

Dear Duncan,

Many thanks for your reply. What you propose is a very good idea yes. However, I am interested in the coefficients...

Best,
S







________________________________
De : Duncan Murdoch <murdoch.duncan at gmail.com>

Cc : R-help Mailing List <r-help at r-project.org>
Envoy? le : Mercredi 6 juillet 2016 14h13
Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working


On 06/07/2016 7:56 AM, varin sacha via R-help wrote:
> Dear Professor Dalgaard,
>
> Okay, this is what I was afraid of.
> Many thanks for your response. I know that my next question is off-topic here (on this website) but is it nevertheless possible to calculate confidence intervals for MARS regression or CIs for MARS will just be impossible to calculate ?

If you think of the fitted curve or surface as the output rather than 
the coefficients that define it, it makes sense to get pointwise 
confidence intervals, or joint confidence regions for multiple 
locations.  Just do predictions at a set of fixed locations.

Duncan Murdoch


> Best
> S
>
>
>
> ----- Mail original -----
> De : peter dalgaard <pdalgd at gmail.com>

> Cc : Bert Gunter <bgunter.4567 at gmail.com>; R-help Mailing List <r-help at r-project.org>
> Envoy? le : Mercredi 6 juillet 2016 11h05
> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>
> Offhand, I would suspect that the cause is that earth() does not return a coefficient vector of the same length in every bootstrap iteration (_adaptive_ regression splines). This makes the bootstrap rather tricky to even define.
>
> -pd
>

>>
>> Dear Bert,
>>
>> You are right.
>>
>>> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
>> Erreur dans boot(data = newdata, statistic = boot.MARS, R = 1000, formula = PIBparHab ~  :
>> le nombre d'objets ? remplacer n'est pas multiple de la taille du remplacement
>>
>> In English it would be something like : number of items to replace is not a multiple of replacement length
>>
>>
>> Best
>> S
>>
>> ________________________________
>> De : Bert Gunter <bgunter.4567 at gmail.com>

>> Cc : peter dalgaard <pdalgd at gmail.com>; R-help Mailing List <r-help at r-project.org>
>> Envoy? le : Mercredi 6 juillet 2016 1h19
>> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>>
>>
>> It would help to show your  error message, n'est-ce pas?
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Tue, Jul 5, 2016 at 2:51 PM, varin sacha via R-help
>> <r-help at r-project.org> wrote:
>>> Dear Professor Dalgaard,
>>>
>>> I really thank you lots for your response. I have solved my problem. Now, I have tried to do the same (calculate the BCa bootstrapped CIs) for the MARS regression, and I get an error message. If somebody has a hint to solve my problem, would be highly appreciated.
>>>
>>> Reproducible example :
>>>
>>>
>>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>>>
>>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>>>
>>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>>>
>>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>>>
>>> install.packages("earth")
>>>
>>> library(earth)
>>>
>>> newdata=na.omit(Dataset)
>>>
>>> model=earth(PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation,data=newdata)
>>>
>>> summary(model)
>>>
>>> plot(model)
>>>
>>> plotmo(model)
>>>
>>>
>>> boot.MARS=function(formula,data,indices) {
>>>
>>> d=data[indices,]
>>>
>>> fit=earth(formula,data=d)
>>>
>>> return(coef(fit))
>>>
>>> }
>>>
>>> library(boot)
>>>
>>> results=boot(data=newdata, statistic=boot.MARS, R=1000,formula=PIBparHab ~ QUALITESANSREDONDANCE + competitivite + innovation)
>>>
>>> boot.ci(results, type= "bca",index=2)
>>>
>>>
>>> Best,
>>> S
>>>
>>> ________________________________
>>> De : peter dalgaard <pdalgd at gmail.com>
>>>
>>> Cc : R-help Mailing List <r-help at r-project.org>
>>> Envoy? le : Dimanche 3 juillet 2016 18h19
>>> Objet : Re: [R] BCa Bootstrapped regression coefficients from lmrob function not working
>>>
>>>
>>>
>>>> On 03 Jul 2016, at 13:47 , varin sacha via R-help <r-help at r-project.org> wrote:
>>>>
>>>> Dear R-experts,
>>>>
>>>> I am trying to calculate the bootstrapped (BCa) regression coefficients for a robust regression using MM-type estimator (lmrob function from robustbase package).
>>>>
>>>> My R code here below is showing a warning message ([1] "All values of t are equal to
>>>> 22.2073014256803\n Can not calculate confidence intervals" NULL), I was wondering if it was because I am trying to fit a robust regression with lmrob function rather than a simple lm ? I mean maybe the boot.ci function does not work with lmrob function ? If not, I was wondering what was going on ?
>>>
>>> You need to review your code. You calculate a,b,c,d in the global environment and create newdata as a subset of Dataset, then use a,b,c,d in the formula, but no such variables are in newdata. AFAICT, all your bootstrap fits use the _same_ global values for a,b,c,d hence give the same result 1000 times...
>>>
>>> -pd
>>>
>>>
>>>
>>>>
>>>> Here is the reproducible example
>>>>
>>>>
>>>> Dataset = data.frame(PIBparHab=c(43931,67524,48348,44827,52409,15245,24453,57636,28992,17102,51495,47243,40908,22494,12784,48391,44221,32514,35132,46679,106022,9817,99635,38678,49128,12876,20732,17151,19670,41053,22488,57134,83295,10660),
>>>>
>>>> QUALITESANSREDONDANCE=c(1082.5,1066.6,1079.3,1079.9,1074.9,1008.6,1007.5,1111.3,1108.2,1109.7,1059.6,1165.1,1026.7,1035.1,997.8,1044.8,1073.6,1085.7,1083.8,1021.6,1036.2,1075.3,1069.3,1101.4,1086.9,1072.1,1166.7,983.9,1004.5,1082.5,1123.5,1094.9,1105.1,1010.8),
>>>>
>>>> competitivite=c(89,83,78,73,90,71,77,85,61,67,98,82,70,43,57,78,72,79,61,71,86,63,90,75,87,64,60,56,66,80,53,91,97,62),
>>>>
>>>> innovation=c(56,52,53,54,57,43,54,60,47,55,58,62,52,35,47,59,56,56,45,52,58,33,57,57,61,40,45,41,50,61,50,65,68,34))
>>>>
>>>> library("robustbase")
>>>> newdata=na.omit(Dataset)
>>>> a=Dataset$PIBparHab
>>>> b=Dataset$QUALITESANSREDONDANCE
>>>> c=Dataset$competitivite
>>>> d=Dataset$innovation
>>>>
>>>> fm.lmrob=lmrob(a~b+c+d,data=newdata)
>>>> fm.lmrob
>>>>
>>>> boot.Lmrob=function(formula,data,indices) {
>>>> d=data[indices,]
>>>> fit=lmrob(formula,data=d)
>>>> return(coef(fit))
>>>> }
>>>>
>>>> library(boot)
>>>> results=boot(data=newdata, statistic=boot.Lmrob, R=1000,formula=a~b+c+d)
>>>> boot.ci(results, type= "bca",index=2)
>>>>
>>>>
>>>> Any help would be highly appreciated,
>>>> S
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> --
>>> Peter Dalgaard, Professor,
>>> Center for Statistics, Copenhagen Business School
>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>> Phone: (+45)38153501
>>> Office: A 4.23
>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>


From luke-tierney at uiowa.edu  Thu Jul  7 01:06:50 2016
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Wed, 6 Jul 2016 18:06:50 -0500
Subject: [R] C stack error in as.vector() starting in R 3.3.0
In-Reply-To: <CAGrYeXjXxsKm7KfP4FNPgc4iWFAbPs1TvNz2B7+TpCuzNsw14Q@mail.gmail.com>
References: <CAGrYeXjXxsKm7KfP4FNPgc4iWFAbPs1TvNz2B7+TpCuzNsw14Q@mail.gmail.com>
Message-ID: <alpine.DEB.2.10.1607061800310.2693@luke-Latitude>

I cannot reproduce this.  My best guess is that there is a problem,
maybe a version incompatibility, with one of the packages loaded when
load("problem.method.rdata"). This includes

adegenet
apex
copula
Rcpp
DBI
sp
colorspace

and any dependencies these are bringing in.

Best,

luke

On Fri, 1 Jul 2016, Eric Archer - NOAA Federal wrote:

> Apologies for the long post. This is an issue I have been struggling with
> and I have tried to be as complete, to the point, and reproducible as
> possible.
>
> In documenting a package with roxygen2, I have come across an error that
> does not occur in R 3.2.4 revised, but does occur in R 3.3.0 and 3.3.1.
> Using traceback() and debug(), I've traced the error to a call made to
> as.vector(x, "character") that seems to get stuck in a loop which
> culminates in this error:
>
> Error: C stack usage  7970892 is too close to the limit
>
> The object that causes this error is of a signature type for a method. With
> some playing around, I've been able to work out that the error is actually
> associated with the method that roxygen2 creates when doing its magic.
> Something happens when this method definition with its associated
> environment is in the workspace that causes the error.
>
> At this point, I should stress again that the error does NOT occur in R
> 3.2.4 revised or earlier, but does occur in R 3.3.0 and 3.3.1. I have also
> tested this with several versions of roxygen2 and that does not make a
> difference. Thus, my ultimate question is what has changed in R 3.3.0 that
> would lead to this so that the roxygen2 maintainers can correct it?
>
> As a test, I created a signature identical to the one that causes the error
> and tried the as.vector() command that would generate the loop. I don't get
> the error in my command line generated object, nor with the object that is
> extracted from the method definition. However, when I load the method
> definition into the workspace, the error will happen on either my command
> line generated object, or the one extracted from the method definition and
> will not stop happening until I restart R.
>
> I have reported this error as an issue on the roxygen2 GitHub repository
> and it has been crossposted by another user who had a similar experience
> with a different package on the devtools repository. Those posts, which
> contain more information are here:
>
> https://github.com/klutometis/roxygen/issues/475
> https://github.com/hadley/devtools/issues/1234
>
> Below is the result of sessionInfo() and the output of a session
> demonstrating the effect. The files used are in a zip file here:
> https://github.com/klutometis/roxygen/files/335417/error.test.rdata.files.zip
>
> > sessionInfo()
> R version 3.3.0 (2016-05-03)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.11.5 (El Capitan)
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] tools_3.3.0
>
>> rm(list = ls())
>>
>> # Create test class and method
>> setClass(Class = "gtypes", slots = c(loci = "data.frame", ploidy =
> "numeric"), package = "roxygen_devtest")
>> setGeneric("nInd", function(x, ...) standardGeneric("nInd"), package =
> "adegenet")
> [1] "nInd"
>> setMethod("nInd", "gtypes", function(x, ...) nrow(x at loci) / x at ploidy)
> [1] "nInd"
>>
>> test.method <- getMethod("nInd", "gtypes")
>> str(test.method)
> Formal class 'MethodDefinition' [package "methods"] with 4 slots
>  ..@ .Data  :function (x, ...)
>  ..@ target :Formal class 'signature' [package "methods"] with 3 slots
>  .. .. ..@ .Data  : chr "gtypes"
>  .. .. ..@ names  : chr "x"
>  .. .. ..@ package: chr "roxygen_devtest"
>  ..@ defined:Formal class 'signature' [package "methods"] with 3 slots
>  .. .. ..@ .Data  : chr "gtypes"
>  .. .. ..@ names  : chr "x"
>  .. .. ..@ package: chr "roxygen_devtest"
>  ..@ generic: atomic [1:1] nInd
>  .. ..- attr(*, "package")= chr "adegenet"
>>
>> # No error:
>> as.vector(test.method at defined, "character")
> [1] "gtypes"
>>
>> # This is the method that generates the error
>> load("problem.method.rdata")
>>
>> # Notice the difference in the environments of the two functions:
>> test.method at .Data
> function (x, ...)
> nrow(x at loci)/x at ploidy
>> problem.method at .Data
> function (x, ...)
> nrow(x at loci)/x at ploidy
> <environment: 0x101f85040>
>>
>> # Swap the problem function for the one I created:
>> problem.method at .Data <- test.method at .Data
>> save(problem.method, file = "fixed.method.rdata")
>>
>> # Here's the error (both with my method and the original):
>> as.vector(test.method at defined, "character")
> Error: C stack usage  7970892 is too close to the limit
>> as.vector(problem.method at defined, "character")
> Error: C stack usage  7970892 is too close to the limit
>
> Restarting R session...
>
>> # *** Restart R for this to work ***
>> rm(list = ls())
>>
>> load("fixed.method.rdata")
>> as.vector(problem.method at defined, "character")
> [1] "gtypes"
>
> ----
>
> *Eric Archer, Ph.D.*
> Southwest Fisheries Science Center
> NMFS, NOAA
> 8901 La Jolla Shores Drive
> La Jolla, CA 92037 USA
> 858-546-7121 (work)
> 858-546-7003 (FAX)
>
> Marine Mammal Genetics Group: swfsc.noaa.gov/mmtd-mmgenetics
> ETP Cetacean Assessment Program: swfsc.noaa.gov/mmtd-etp
> https://github/ericarcher
>
> "
>
>
> *The universe doesn't care what you believe. The wonderful thing about
> science is that it   doesn't ask for your faith, it just asks   for your
> eyes.*"  - Randall Munroe
>
> "*Lighthouses are more helpful than churches.*"
>   - Benjamin Franklin
>
>   "*...but I'll take a GPS over either one.*"
>       - John C. "Craig" George
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From reichmanj at sbcglobal.net  Thu Jul  7 01:21:42 2016
From: reichmanj at sbcglobal.net (Jeff Reichman)
Date: Wed, 6 Jul 2016 18:21:42 -0500
Subject: [R] Checking for modality (R - circular)
Message-ID: <000901d1d7dd$29427320$7bc75960$@sbcglobal.net>

R Users

 

Is there a way to check for modality using the "circular" package in R or
any other package ?

 

Circular time data.

 

Jeff


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Jul  7 02:36:20 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 6 Jul 2016 17:36:20 -0700
Subject: [R] GAMS, std errors and confidence intervals
In-Reply-To: <CAK+Bp3nrWeE2cpvq2dYmsM9Ou2Dsjh_L6q0iWD4vXAxhDTvRtw@mail.gmail.com>
References: <CAK+Bp3nrWeE2cpvq2dYmsM9Ou2Dsjh_L6q0iWD4vXAxhDTvRtw@mail.gmail.com>
Message-ID: <90D9CFDE-C531-4DEA-8769-70DDDBBF7906@comcast.net>


> On Jul 4, 2016, at 11:44 AM, Dan Jaffe <djaffe at uw.edu> wrote:
> 
> Can anyone help me calculating CIs from a GAM analysis?
> 
> I have calculated a GAM fit (m3) and the associated std errors using
> predict.gam
> I assume that the 95% CI around each fit value would be 1.96
> times the se.    But when I do this both on the original and a test
> dataset, I find the CI's only encompass about half of the true response
> values.

I would have expected a CI to have been constructed around the mean ( and be a CI for the mean) and so would not have thought that this was an odd result. You seem to have expected a "prediction interval" rather than a confidence interval. To get an estimate of the standard deviation within groups, you might consider multiplying the se by the sqrt(n) for each group. At least that would be the approach in an ordinary linear model. Not sure how it translates to GAMs.


>    I have  tried this using predict.gam using both type="response"
> and type="link"  and get nearly the same result.  What am I  doing wrong?
> 
> Here is the code I am using to get the 95% CIs.
> 
> se3=predict.gam(m3,dat, type="response", se.fit=TRUE)
> 
> upr <- se3$fit + (1.96 * se3$se.fit)
> upr <- m3$family$linkinv(upr)
> lwr <- se3$fit - (1.96 * se3$se.fit)
> lwr <- m3$family$linkinv(lwr)
> CI95=upr-lwr
> CI95=CI95/2
> 
> 	[[alternative HTML version deleted]]

Plain text requested.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From eric.archer at noaa.gov  Thu Jul  7 06:02:31 2016
From: eric.archer at noaa.gov (Eric Archer - NOAA Federal)
Date: Wed, 6 Jul 2016 21:02:31 -0700
Subject: [R] C stack error in as.vector() starting in R 3.3.0
In-Reply-To: <alpine.DEB.2.10.1607061800310.2693@luke-Latitude>
References: <CAGrYeXjXxsKm7KfP4FNPgc4iWFAbPs1TvNz2B7+TpCuzNsw14Q@mail.gmail.com>
	<alpine.DEB.2.10.1607061800310.2693@luke-Latitude>
Message-ID: <CAGrYeXgwoYGT6Rd-yZAfRxWBHPr_LN4+pTY1Z-BBPTqXVff7yQ@mail.gmail.com>

Luke,

Thanks for this point out. I started removing packages one-by-one in R
v.3.3.1. Oddly enough, it went away after I removed the Matrix package.
When I restarted R, the same version of Matrix (1.2-6) had been
"reinstalled" (?), but now no C stack error. Lets see if the others that
were having a similar problem have the same result.

Cheers,
Eric


----

*Eric Archer, Ph.D.*
Southwest Fisheries Science Center
NMFS, NOAA
8901 La Jolla Shores Drive
La Jolla, CA 92037 USA
858-546-7121 (work)
858-546-7003 (FAX)

Marine Mammal Genetics Group: swfsc.noaa.gov/mmtd-mmgenetics
ETP Cetacean Assessment Program: swfsc.noaa.gov/mmtd-etp
https://github/ericarcher

"


*The universe doesn't care what you believe. The wonderful thing about
science is that it   doesn't ask for your faith, it just asks   for your
eyes.*"  - Randall Munroe

"*Lighthouses are more helpful than churches.*"
   - Benjamin Franklin

   "*...but I'll take a GPS over either one.*"
       - John C. "Craig" George

On Wed, Jul 6, 2016 at 4:06 PM, <luke-tierney at uiowa.edu> wrote:

> I cannot reproduce this.  My best guess is that there is a problem,
> maybe a version incompatibility, with one of the packages loaded when
> load("problem.method.rdata"). This includes
>
> adegenet
> apex
> copula
> Rcpp
> DBI
> sp
> colorspace
>
> and any dependencies these are bringing in.
>
> Best,
>
> luke
>
> On Fri, 1 Jul 2016, Eric Archer - NOAA Federal wrote:
>
> Apologies for the long post. This is an issue I have been struggling with
>> and I have tried to be as complete, to the point, and reproducible as
>> possible.
>>
>> In documenting a package with roxygen2, I have come across an error that
>> does not occur in R 3.2.4 revised, but does occur in R 3.3.0 and 3.3.1.
>> Using traceback() and debug(), I've traced the error to a call made to
>> as.vector(x, "character") that seems to get stuck in a loop which
>> culminates in this error:
>>
>> Error: C stack usage  7970892 is too close to the limit
>>
>> The object that causes this error is of a signature type for a method.
>> With
>> some playing around, I've been able to work out that the error is actually
>> associated with the method that roxygen2 creates when doing its magic.
>> Something happens when this method definition with its associated
>> environment is in the workspace that causes the error.
>>
>> At this point, I should stress again that the error does NOT occur in R
>> 3.2.4 revised or earlier, but does occur in R 3.3.0 and 3.3.1. I have also
>> tested this with several versions of roxygen2 and that does not make a
>> difference. Thus, my ultimate question is what has changed in R 3.3.0 that
>> would lead to this so that the roxygen2 maintainers can correct it?
>>
>> As a test, I created a signature identical to the one that causes the
>> error
>> and tried the as.vector() command that would generate the loop. I don't
>> get
>> the error in my command line generated object, nor with the object that is
>> extracted from the method definition. However, when I load the method
>> definition into the workspace, the error will happen on either my command
>> line generated object, or the one extracted from the method definition and
>> will not stop happening until I restart R.
>>
>> I have reported this error as an issue on the roxygen2 GitHub repository
>> and it has been crossposted by another user who had a similar experience
>> with a different package on the devtools repository. Those posts, which
>> contain more information are here:
>>
>> https://github.com/klutometis/roxygen/issues/475
>> https://github.com/hadley/devtools/issues/1234
>>
>> Below is the result of sessionInfo() and the output of a session
>> demonstrating the effect. The files used are in a zip file here:
>>
>> https://github.com/klutometis/roxygen/files/335417/error.test.rdata.files.zip
>>
>> > sessionInfo()
>> R version 3.3.0 (2016-05-03)
>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>> Running under: OS X 10.11.5 (El Capitan)
>>
>> locale:
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> loaded via a namespace (and not attached):
>> [1] tools_3.3.0
>>
>> rm(list = ls())
>>>
>>> # Create test class and method
>>> setClass(Class = "gtypes", slots = c(loci = "data.frame", ploidy =
>>>
>> "numeric"), package = "roxygen_devtest")
>>
>>> setGeneric("nInd", function(x, ...) standardGeneric("nInd"), package =
>>>
>> "adegenet")
>> [1] "nInd"
>>
>>> setMethod("nInd", "gtypes", function(x, ...) nrow(x at loci) / x at ploidy)
>>>
>> [1] "nInd"
>>
>>>
>>> test.method <- getMethod("nInd", "gtypes")
>>> str(test.method)
>>>
>> Formal class 'MethodDefinition' [package "methods"] with 4 slots
>>  ..@ .Data  :function (x, ...)
>>  ..@ target :Formal class 'signature' [package "methods"] with 3 slots
>>  .. .. ..@ .Data  : chr "gtypes"
>>  .. .. ..@ names  : chr "x"
>>  .. .. ..@ package: chr "roxygen_devtest"
>>  ..@ defined:Formal class 'signature' [package "methods"] with 3 slots
>>  .. .. ..@ .Data  : chr "gtypes"
>>  .. .. ..@ names  : chr "x"
>>  .. .. ..@ package: chr "roxygen_devtest"
>>  ..@ generic: atomic [1:1] nInd
>>  .. ..- attr(*, "package")= chr "adegenet"
>>
>>>
>>> # No error:
>>> as.vector(test.method at defined, "character")
>>>
>> [1] "gtypes"
>>
>>>
>>> # This is the method that generates the error
>>> load("problem.method.rdata")
>>>
>>> # Notice the difference in the environments of the two functions:
>>> test.method at .Data
>>>
>> function (x, ...)
>> nrow(x at loci)/x at ploidy
>>
>>> problem.method at .Data
>>>
>> function (x, ...)
>> nrow(x at loci)/x at ploidy
>> <environment: 0x101f85040>
>>
>>>
>>> # Swap the problem function for the one I created:
>>> problem.method at .Data <- test.method at .Data
>>> save(problem.method, file = "fixed.method.rdata")
>>>
>>> # Here's the error (both with my method and the original):
>>> as.vector(test.method at defined, "character")
>>>
>> Error: C stack usage  7970892 is too close to the limit
>>
>>> as.vector(problem.method at defined, "character")
>>>
>> Error: C stack usage  7970892 is too close to the limit
>>
>> Restarting R session...
>>
>> # *** Restart R for this to work ***
>>> rm(list = ls())
>>>
>>> load("fixed.method.rdata")
>>> as.vector(problem.method at defined, "character")
>>>
>> [1] "gtypes"
>>
>> ----
>>
>> *Eric Archer, Ph.D.*
>> Southwest Fisheries Science Center
>> NMFS, NOAA
>> 8901 La Jolla Shores Drive
>> La Jolla, CA 92037 USA
>> 858-546-7121 (work)
>> 858-546-7003 (FAX)
>>
>> Marine Mammal Genetics Group: swfsc.noaa.gov/mmtd-mmgenetics
>> ETP Cetacean Assessment Program: swfsc.noaa.gov/mmtd-etp
>> https://github/ericarcher
>>
>> "
>>
>>
>> *The universe doesn't care what you believe. The wonderful thing about
>> science is that it   doesn't ask for your faith, it just asks   for your
>> eyes.*"  - Randall Munroe
>>
>> "*Lighthouses are more helpful than churches.*"
>>   - Benjamin Franklin
>>
>>   "*...but I'll take a GPS over either one.*"
>>       - John C. "Craig" George
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>    Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>

	[[alternative HTML version deleted]]


From chalabi.elahe at yahoo.de  Thu Jul  7 13:34:28 2016
From: chalabi.elahe at yahoo.de (chalabi.elahe at yahoo.de)
Date: Thu, 7 Jul 2016 11:34:28 +0000 (UTC)
Subject: [R] make a right subset!
References: <403711097.5645248.1467891268314.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <403711097.5645248.1467891268314.JavaMail.yahoo@mail.yahoo.com>

Hi all,
I have 2 data frames like the following:
the first one df1:

    'data.frame':  141obs. of 1 variable:
    $SerialNum: int 41006 41013 41044 41046 41067 41166 41202 41262 41274 41290
and the second one df2:

    'data.frame':  194 obs. of 2 variables:
    $Serial: int   41006 41013 41018 41044 41046 41067 41111 41200 41262 41331 
    $Count : int   1 10 103 11 12 13 138 14 15 16 169 18 182 2 20 21  224 226
then I find the intersect of two data frames in serials which is:

    Matched=intersect(df1$SerialNum,df2$Serial)
which gives me matched serials in both data frames. Next I want to get Count of these matched serials from df2 but I don't know how to make the right subset for this! Does anyone know how should I do that?
Thanks for any help,
Elahe


From Gerrit.Eichner at math.uni-giessen.de  Thu Jul  7 13:56:50 2016
From: Gerrit.Eichner at math.uni-giessen.de (Gerrit Eichner)
Date: Thu, 7 Jul 2016 13:56:50 +0200
Subject: [R] make a right subset!
In-Reply-To: <403711097.5645248.1467891268314.JavaMail.yahoo@mail.yahoo.com>
References: <403711097.5645248.1467891268314.JavaMail.yahoo.ref@mail.yahoo.com>
	<403711097.5645248.1467891268314.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <336c5dcc-b609-76d4-d18c-617d2abda475@math.uni-giessen.de>

Hello, Elahe,

look at

?match

and check if

df2$Count[ match( Matched, df2$Serial)]

does what you want/need.

  Hth  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 212
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
Fax: +49-(0)641-99-32109            http://www.uni-giessen.de/eichner
---------------------------------------------------------------------

Am 07.07.2016 um 13:34 schrieb ch.elahe via R-help:
> Hi all,
> I have 2 data frames like the following:
> the first one df1:
>
>     'data.frame':  141obs. of 1 variable:
>     $SerialNum: int 41006 41013 41044 41046 41067 41166 41202 41262 41274 41290
> and the second one df2:
>
>     'data.frame':  194 obs. of 2 variables:
>     $Serial: int   41006 41013 41018 41044 41046 41067 41111 41200 41262 41331
>     $Count : int   1 10 103 11 12 13 138 14 15 16 169 18 182 2 20 21  224 226
> then I find the intersect of two data frames in serials which is:
>
>     Matched=intersect(df1$SerialNum,df2$Serial)
> which gives me matched serials in both data frames. Next I want to get Count of these matched serials from df2 but I don't know how to make the right subset for this! Does anyone know how should I do that?
> Thanks for any help,
> Elahe
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ruipbarradas at sapo.pt  Thu Jul  7 13:59:15 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Thu, 07 Jul 2016 12:59:15 +0100
Subject: [R] make a right subset!
In-Reply-To: <403711097.5645248.1467891268314.JavaMail.yahoo@mail.yahoo.com>
References: <403711097.5645248.1467891268314.JavaMail.yahoo.ref@mail.yahoo.com>
	<403711097.5645248.1467891268314.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <20160707125915.Horde.59soIdVQ8PryE4liCbnA66O@mail.sapo.pt>

Hello,

Maybe something like the following (untested).

idx <- Matched %in% df2$Serial
MatchedCount <- df2$Count[idx]

Hope this helps,

Rui Barradas

?

Citando ch.elahe via R-help <r-help at r-project.org>:

> Hi all,
> I have 2 data frames like the following:
> the first one df1:
>
> ? ?'data.frame':? 141obs. of 1 variable:
> ? ?$SerialNum: int 41006 41013 41044 41046 41067 41166 41202 41262  
> 41274 41290
> and the second one df2:
>
> ? ?'data.frame':? 194 obs. of 2 variables:
> ? ?$Serial: int? ?41006 41013 41018 41044 41046 41067 41111 41200 41262 41331
> ? ?$Count : int? ?1 10 103 11 12 13 138 14 15 16 169 18 182 2 20 21? 224 226
> then I find the intersect of two data frames in serials which is:
>
> ? ?Matched=intersect(df1$SerialNum,df2$Serial)
> which gives me matched serials in both data frames. Next I want to  
> get Count of these matched serials from df2 but I don't know how to  
> make the right subset for this! Does anyone know how should I do that?
> Thanks for any help,
> Elahe
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Thu Jul  7 13:59:34 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Thu, 07 Jul 2016 11:59:34 +0000
Subject: [R] make a right subset!
In-Reply-To: <403711097.5645248.1467891268314.JavaMail.yahoo@mail.yahoo.com>
References: <403711097.5645248.1467891268314.JavaMail.yahoo.ref@mail.yahoo.com>
	<403711097.5645248.1467891268314.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAKVAULNfYq80bBuN=vYwMwqGMi=Kev2BJu-YU0J=8Ys+ZGyrPQ@mail.gmail.com>

Hi Elahe,

Use subset and the %in% operator:

subset(df2, Serial %in% Matched)[["Count "]]

Best wishes,
Ulrik

On Thu, 7 Jul 2016 at 13:37 ch.elahe via R-help <r-help at r-project.org>
wrote:

> Hi all,
> I have 2 data frames like the following:
> the first one df1:
>
>     'data.frame':  141obs. of 1 variable:
>     $SerialNum: int 41006 41013 41044 41046 41067 41166 41202 41262 41274
> 41290
> and the second one df2:
>
>     'data.frame':  194 obs. of 2 variables:
>     $Serial: int   41006 41013 41018 41044 41046 41067 41111 41200 41262
> 41331
>     $Count : int   1 10 103 11 12 13 138 14 15 16 169 18 182 2 20 21  224
> 226
> then I find the intersect of two data frames in serials which is:
>
>     Matched=intersect(df1$SerialNum,df2$Serial)
> which gives me matched serials in both data frames. Next I want to get
> Count of these matched serials from df2 but I don't know how to make the
> right subset for this! Does anyone know how should I do that?
> Thanks for any help,
> Elahe
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From cristina.cametti at gmail.com  Thu Jul  7 12:26:17 2016
From: cristina.cametti at gmail.com (Cristina Cametti)
Date: Thu, 7 Jul 2016 12:26:17 +0200
Subject: [R] r code for multilevel latent class analysis
Message-ID: <A8C817C7-49DA-4C4E-9D3A-CB70504B26EC@yahoo.it>

Dear all,

I am not able to find a reliable r code to run a multilevel latent class model. Indeed, I have to analyze how social trust (three variables form the ESS survey) might vary between countries (21 countries in my database). I tried to use the poLCA package but I am not sure if my code is right. This is my code:
lca <- cbind(ppltrst+1,pplfair+1,pplhlp+1)~cntry
lc <- poLCA(lca,mydata)

However, I get an error message:
Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) : 
undefined columns selected

How can I solve this? Is the code completely wrong or I missed some passages?
Thank you very much for your help!

Cristina
	[[alternative HTML version deleted]]


From cristinacametti at yahoo.it  Thu Jul  7 13:31:35 2016
From: cristinacametti at yahoo.it (Yahoo Mail)
Date: Thu, 7 Jul 2016 13:31:35 +0200
Subject: [R] R code for multilevel latent class analysis
Message-ID: <826524FA-D916-41AA-BF77-AA08E3000834@yahoo.it>

Dear all,

I am not able to find a reliable r code to run a multilevel latent class model. Indeed, I have to analyze how social trust (three variables form the ESS survey) might vary between countries (21 countries in my database). I tried to use the poLCA package but I am not sure if my code is right. This is my code:
lca <-cbind(ppltrst+1,pplfair+1,pplhlp+1)~cntry
lc <- poLCA(lca,mydata)

However, I get an error message:
Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) : 
undefined columns selected

How can I solve this? Is the code completely wrong or I missed some passages?
Thank you very much for your help!

Cristina Cametti 

From Gaurang.Mehta at royallondon.com  Thu Jul  7 14:27:02 2016
From: Gaurang.Mehta at royallondon.com (Mehta, Gaurang)
Date: Thu, 7 Jul 2016 13:27:02 +0100
Subject: [R] Bessel function dll file use in VBA
Message-ID: <3BB657B92C75C74385A95FAA7C6B17161F0C60A79A@VRTPRDEXM02.royallondongroup.com>

Hi Team,
I am trying to use Bessel function dll files in VBA. It would be great if someone can help.
Regards,
Gaurang Mehta

This email is intended for the person or company named and access by anyone else is unauthorised. If you are not the person or company named, please delete this email and notify the sender.

The information in this email, including any attachments, may be confidential or legally privileged (meaning that its disclosure is protected in law). Its unauthorised disclosure, copying, distribution or use is prohibited and may be unlawful.

Email communications sent over the internet are not guaranteed to be secure or virus-free and such messages are potentially at risk.  The Royal London Group accepts no liability for any claims arising from use of the internet to transmit messages by or to any company within the Royal London Group.

The Royal London Group consists of The Royal London Mutual Insurance Society Limited and its subsidiaries.

The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority and provides life assurance and pensions.

Registered in England and Wales number 99064.

Registered office: 55 Gracechurch Street, London, EC3V 0RL.

In the Republic of Ireland: The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority in the UK and is regulated by the Central Bank of Ireland for conduct of business rules.


From sezenismail at gmail.com  Thu Jul  7 09:50:54 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Thu, 7 Jul 2016 10:50:54 +0300
Subject: [R] netcdf data precision or least significant digit
Message-ID: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>

Hello,

I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar reanalysis ncetcdf files. But data is in the form of (9.199998, 8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision and least_significant_digit attributes of ncdf4 object [1]. For uwnd data, precision = 2 and least_significant_digits = 1. My doubt is that should I round data to 2 decimal places or 1 decimal place after decimal point?

Same issue is valid for some header info.

Output of ncdf4 object:


Output of ncdump on terminal:


for instance, ncdump's scale factor is 0.01f but ncdf4 object?s scale_factor is 0.00999999977648258. You can notice same issue for actual_range and add_offset. Also a similar issue exist for the data. How can I truncate those extra unsignificant decimal places or round the numbers to significant decimal places?

1 - http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml <http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>

From ajdamico at gmail.com  Thu Jul  7 16:00:39 2016
From: ajdamico at gmail.com (Anthony Damico)
Date: Thu, 7 Jul 2016 10:00:39 -0400
Subject: [R] r code for multilevel latent class analysis
In-Reply-To: <A8C817C7-49DA-4C4E-9D3A-CB70504B26EC@yahoo.it>
References: <A8C817C7-49DA-4C4E-9D3A-CB70504B26EC@yahoo.it>
Message-ID: <CAOwvMDxuobirt4yOVqUODP6TJoAayUN5WNFYzBfV1HYR9YdNig@mail.gmail.com>

start at
https://github.com/ajdamico/asdfree/blob/master/European%20Social%20Survey/structural%20equation%20modeling%20examples.R
maybe?

On Thu, Jul 7, 2016 at 6:26 AM, Cristina Cametti <cristina.cametti at gmail.com
> wrote:

> Dear all,
>
> I am not able to find a reliable r code to run a multilevel latent class
> model. Indeed, I have to analyze how social trust (three variables form the
> ESS survey) might vary between countries (21 countries in my database). I
> tried to use the poLCA package but I am not sure if my code is right. This
> is my code:
> lca <- cbind(ppltrst+1,pplfair+1,pplhlp+1)~cntry
> lc <- poLCA(lca,mydata)
>
> However, I get an error message:
> Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) :
> undefined columns selected
>
> How can I solve this? Is the code completely wrong or I missed some
> passages?
> Thank you very much for your help!
>
> Cristina
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Thu Jul  7 17:04:36 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Thu, 7 Jul 2016 15:04:36 +0000
Subject: [R] Performing Principal Cluster Analysis,
 k-means clustering etc. with PDB/DCD trajectory
In-Reply-To: <CAMU2JtAAAJdGPQ5VQ4kORp0EgOFs5-SGy=YcGVZJJ+Q87X33BQ@mail.gmail.com>
References: <CAMU2JtAAAJdGPQ5VQ4kORp0EgOFs5-SGy=YcGVZJJ+Q87X33BQ@mail.gmail.com>
Message-ID: <ddfa606bcd6442f29601ffc7ffefaf23@exch-2p-mbx-t2.ads.tamu.edu>

A Google search turns up a possible place to start.

Trajectory Analysis with Bio3D
Bio3D1 is an R package that provides interactive tools for the analysis of bimolecular structure, sequence and simulation data. The aim of this document, termed a vignette2 in R parlance, is to provide a brief task-oriented introduction to basic molecular dynamics trajectory analysis with the Bio3D R package (Grant et al. 2006).

http://thegrantlab.org/bio3d/tutorials/trajectory-analysis

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sourav Ray
Sent: Wednesday, July 6, 2016 3:04 PM
To: R-help at r-project.org
Subject: [R] Performing Principal Cluster Analysis, k-means clustering etc. with PDB/DCD trajectory

Hello

If someone knows how to convert an MD trajectory into a format acceptable
to R or has done any of the above analyses, please let me know.

Thanks and regards
Sourav

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Thu Jul  7 17:42:19 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 07 Jul 2016 08:42:19 -0700
Subject: [R] Bessel function dll file use in VBA
In-Reply-To: <3BB657B92C75C74385A95FAA7C6B17161F0C60A79A@VRTPRDEXM02.royallondongroup.com>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60A79A@VRTPRDEXM02.royallondongroup.com>
Message-ID: <36EDDB83-B04C-4A28-B296-C1E4188A2172@dcn.davis.ca.us>

This is not a VBA support forum. You need to be studying VBA linkage requirements and gcc linkage conventions, and neither of these subject areas are on topic in R-help.
-- 
Sent from my phone. Please excuse my brevity.

On July 7, 2016 5:27:02 AM PDT, "Mehta, Gaurang" <Gaurang.Mehta at royallondon.com> wrote:
>Hi Team,
>I am trying to use Bessel function dll files in VBA. It would be great
>if someone can help.
>Regards,
>Gaurang Mehta
>
>This email is intended for the person or company named and access by
>anyone else is unauthorised. If you are not the person or company
>named, please delete this email and notify the sender.
>
>The information in this email, including any attachments, may be
>confidential or legally privileged (meaning that its disclosure is
>protected in law). Its unauthorised disclosure, copying, distribution
>or use is prohibited and may be unlawful.
>
>Email communications sent over the internet are not guaranteed to be
>secure or virus-free and such messages are potentially at risk.  The
>Royal London Group accepts no liability for any claims arising from use
>of the internet to transmit messages by or to any company within the
>Royal London Group.
>
>The Royal London Group consists of The Royal London Mutual Insurance
>Society Limited and its subsidiaries.
>
>The Royal London Mutual Insurance Society Limited is authorised by the
>Prudential Regulation Authority and regulated by the Financial Conduct
>Authority and the Prudential Regulation Authority and provides life
>assurance and pensions.
>
>Registered in England and Wales number 99064.
>
>Registered office: 55 Gracechurch Street, London, EC3V 0RL.
>
>In the Republic of Ireland: The Royal London Mutual Insurance Society
>Limited is authorised by the Prudential Regulation Authority in the UK
>and is regulated by the Central Bank of Ireland for conduct of business
>rules.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Jul  7 17:54:40 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 7 Jul 2016 08:54:40 -0700
Subject: [R] R code for multilevel latent class analysis
In-Reply-To: <826524FA-D916-41AA-BF77-AA08E3000834@yahoo.it>
References: <826524FA-D916-41AA-BF77-AA08E3000834@yahoo.it>
Message-ID: <376BAFFC-ACFB-486C-9B35-4ACEFD738A9A@comcast.net>


> On Jul 7, 2016, at 4:31 AM, Yahoo Mail via R-help <r-help at r-project.org> wrote:
> 
> Dear all,
> 
> I am not able to find a reliable r code to run a multilevel latent class model. Indeed, I have to analyze how social trust (three variables form the ESS survey) might vary between countries (21 countries in my database). I tried to use the poLCA package but I am not sure if my code is right. This is my code:
> lca <-cbind(ppltrst+1,pplfair+1,pplhlp+1)~cntry
> lc <- poLCA(lca,mydata)
> 

The documentation for the 'ppltrst' variable in the documentation I see has that name in all caps:

http://www.europeansocialsurvey.org/docs/round6/survey/ESS6_appendix_a7_e02_1.pdf


> However, I get an error message:
> Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) : 
> undefined columns selected

Check your spelling of all variables:

 lchar <- character(); for( i in 2:4){ lchar=c(lchar, as.character(lca[[2]][[i]][[2]]))}
 lchar
#[1] "ppltrst" "pplfair" "pplhlp" 
all( c( lchar, 'cntry') %in% colnames(mydata) )

> 
> How can I solve this? Is the code completely wrong or I missed some passages?
> Thank you very much for your help!


-- 

David Winsemius
Alameda, CA, USA


From dwkikuchi at gmail.com  Thu Jul  7 19:53:35 2016
From: dwkikuchi at gmail.com (David Kikuchi)
Date: Thu, 7 Jul 2016 10:53:35 -0700
Subject: [R] lmer causes R session to terminate
Message-ID: <CADydHU0we53fyOquyL+gvnK57qsV1yw4y1CxdoR8kw80ETZ0rA@mail.gmail.com>

Hi,

I am working with a large dataset of neotropical birds, and am trying to
partition the variance in log(body mass) within different taxonomic levels.
To better explain what I mean, the taxonomic levels are species, genus,
family, and order. Species are within genera, genera are within famillies,
and famlies are within orders.

Sample data look like this:

mass        species                   genus         family         order.2
377.0000    Geranospiza caerulescens  Geranospiza   Accipitridae
Accipitriformes
213.1667    Harpagus bidentatus       Harpagus      Accipitridae
Accipitriformes
500.0000    Leptodon cayanensis       Leptodon      Accipitridae
Accipitriformes
1750.0000   Penelope albipennis       Penelope      Cracidae       Galliformes
278.0000    Leucopternis semiplumbeus Leucopternis  Accipitridae
Accipitriformes
66.2500     Notharchus pectoralis     Notharchus    Bucconidae
Galibuliformes
213.1667    Harpagus bidentatus       Harpagus      Accipitridae
Accipitriformes
31.0000     Gymnopithys leucaspis     Gymnopithys   Thamnophilidae Passeriformes
31.0000     Gymnopithys leucaspis     Gymnopithys   Thamnophilidae Passeriformes


I want to know how much variability in log(mass) there is at the species
level, genus level, family level, and order level.

The code that I have been using to do this looks like:

bm.full <- lmer(log(mass) ~ 1 + (1|order.2/family/genus/species))

however, it crashes R every time, whether I run it on my laptop with 4 gb
RAM or a desktop with 8 gb RAM. If I remove any of the taxonomic levels,
though, it does run and produces reasonable output on either machine. There
are 1943 observations (different studies occassionaly give different masses
for the same species), 791 species, 381 genera, 60 families, and 21 orders.
If I am able to modestly increase RAM, e.g. to 16 GB, is it likely that R
will be able to handle the model, or is there such a dramatic increase in
the computation required with four nested groups that it simply won't be
possible?

Thank you for your advice,

David

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Jul  7 23:09:35 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 7 Jul 2016 14:09:35 -0700
Subject: [R] Fwd: Re:  lmer causes R session to terminate
In-Reply-To: <CAGxFJbR2+CASTedPHnbCj=CmkUWoMWHZT9VkdfS-eRmSbYLYNQ@mail.gmail.com>
References: <CADydHU0we53fyOquyL+gvnK57qsV1yw4y1CxdoR8kw80ETZ0rA@mail.gmail.com>
	<CAGxFJbR2+CASTedPHnbCj=CmkUWoMWHZT9VkdfS-eRmSbYLYNQ@mail.gmail.com>
Message-ID: <CAGxFJbTg2x73pAqC+qn-aY0VuJK3+XDbba5f7+SMOLx-U1AxcQ@mail.gmail.com>

I failed to c.c. the list. And it should be r-sig-mixed-models list.

Bert
---------- Forwarded message ----------
From: "Bert Gunter" <bgunter.4567 at gmail.com>
Date: Jul 7, 2016 2:06 PM
Subject: Re: [R] lmer causes R session to terminate
To: "David Kikuchi" <dwkikuchi at gmail.com>
Cc:

Off the top, I would guess that you'd need 10 or more times the data to get
> meaningful estimates of the variance components, and heaven knows what
> sorts of rank deficient matrices imbalance in the data may be producing.
> But post on the r-mixed-models list to  get an authoritative answer, which
> this ain't.
>
> Cheers,
> Bert
> On Jul 7, 2016 10:55 AM, "David Kikuchi" <dwkikuchi at gmail.com> wrote:
>
>> Hi,
>>
>> I am working with a large dataset of neotropical birds, and am trying to
>> partition the variance in log(body mass) within different taxonomic
>> levels.
>> To better explain what I mean, the taxonomic levels are species, genus,
>> family, and order. Species are within genera, genera are within famillies,
>> and famlies are within orders.
>>
>> Sample data look like this:
>>
>> mass        species                   genus         family         order.2
>> 377.0000    Geranospiza caerulescens  Geranospiza   Accipitridae
>> Accipitriformes
>> 213.1667    Harpagus bidentatus       Harpagus      Accipitridae
>> Accipitriformes
>> 500.0000    Leptodon cayanensis       Leptodon      Accipitridae
>> Accipitriformes
>> 1750.0000   Penelope albipennis       Penelope      Cracidae
>>  Galliformes
>> 278.0000    Leucopternis semiplumbeus Leucopternis  Accipitridae
>> Accipitriformes
>> 66.2500     Notharchus pectoralis     Notharchus    Bucconidae
>> Galibuliformes
>> 213.1667    Harpagus bidentatus       Harpagus      Accipitridae
>> Accipitriformes
>> 31.0000     Gymnopithys leucaspis     Gymnopithys   Thamnophilidae
>> Passeriformes
>> 31.0000     Gymnopithys leucaspis     Gymnopithys   Thamnophilidae
>> Passeriformes
>>
>>
>> I want to know how much variability in log(mass) there is at the species
>> level, genus level, family level, and order level.
>>
>> The code that I have been using to do this looks like:
>>
>> bm.full <- lmer(log(mass) ~ 1 + (1|order.2/family/genus/species))
>>
>> however, it crashes R every time, whether I run it on my laptop with 4 gb
>> RAM or a desktop with 8 gb RAM. If I remove any of the taxonomic levels,
>> though, it does run and produces reasonable output on either machine.
>> There
>> are 1943 observations (different studies occassionaly give different
>> masses
>> for the same species), 791 species, 381 genera, 60 families, and 21
>> orders.
>> If I am able to modestly increase RAM, e.g. to 16 GB, is it likely that R
>> will be able to handle the model, or is there such a dramatic increase in
>> the computation required with four nested groups that it simply won't be
>> possible?
>>
>> Thank you for your advice,
>>
>> David
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From davidsmi at microsoft.com  Thu Jul  7 23:38:04 2016
From: davidsmi at microsoft.com (David Smith)
Date: Thu, 7 Jul 2016 21:38:04 +0000
Subject: [R] Revolutions blog: June 2016 roundup
Message-ID: <DM2PR0301MB0848C9E56D33169234F6CC5EC83B0@DM2PR0301MB0848.namprd03.prod.outlook.com>

Since 2008, Microsoft (formerly Revolution Analytics) staff and guests have written about R every weekday at the
Revolutions blog: http://blog.revolutionanalytics.com
and every month I post a summary of articles from the previous month of particular interest to readers of r-help.

And in case you missed them, here are some articles related to R from the month of June:

A preview of the tutorials presented at the useR! 2016 conference:
http://blog.revolutionanalytics.com/2016/06/the-user-2016-tutorials.html

A "advanced beginner's" guide to R published by ComputerWorld includes guides on data wrangling, visualization, and data
APIs: http://blog.revolutionanalytics.com/2016/06/computerworlds-advanced-beginners-guide-to-r.html

Microsoft R Server now runs on Apache Spark:
http://blog.revolutionanalytics.com/2016/06/apache-spark-integrated-with-mrs-for-hadoop.html

Recordings of talks at the useR! 2016 are now available to watch and download, with thanks to Microsoft's livestreaming
of the conference: http://blog.revolutionanalytics.com/2016/06/livestreaming-of-user-2016-conference.html

You can download on-screen actor data for films and TV using R via Amazon's X-ray service:
http://blog.revolutionanalytics.com/2016/06/amazon-x-ray-data-provides-insight-into-movie-characters.html

A review of new Bayesian statistics packages for R, with a focus on Stan:
http://blog.revolutionanalytics.com/2016/06/r-stan-bayesian-stats.html

A short summary of the fixes in R 3.3.1, released on June 21:
http://blog.revolutionanalytics.com/2016/06/r-331-now-available.html

A chart of global internet speeds, created using R:
http://blog.revolutionanalytics.com/2016/06/exploring-global-internet-performance-data-using-r.html

New features in the forecast package, updated to version 7:
http://blog.revolutionanalytics.com/2016/06/updates-to-the-forecast-package-for-r.html

This year's Data Journalism Awards Data Visualization of the Year was created with R:
http://blog.revolutionanalytics.com/2016/06/data-journalism-awards-data-visualization-of-the-year-2016.html

The 150+ R packages presented during useR! 2016:
http://blog.revolutionanalytics.com/2016/06/the-r-packages-of-user-2016.html

Using GitHub avatars and the Microsoft Face API to estimate gender ratios of programmers by language used (R, C++ and
others): http://blog.revolutionanalytics.com/2016/06/programmers-gender.html

Using the Data Science Virtual Machine and Microsoft R Server to analyze data from 600 million taxi rides:
http://blog.revolutionanalytics.com/2016/06/taxi2.html

R is once again the top-ranked software in the KDnuggets annual poll:
http://blog.revolutionanalytics.com/2016/06/r-holds-top-ranking-in-kdnuggets-software-poll.html

A tutorial on making interactive data maps and charts with R:
http://blog.revolutionanalytics.com/2016/06/mapping-in-r.html

News from the R Consortium: http://blog.revolutionanalytics.com/2016/06/r-news.html

Max Kuhn (author of the caret package) on Bayesian optimization of machine learning models in R:
http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html

Ross Ihaka recounts the history of the R project in an interview for the University of Auckland alumni magazine:
http://blog.revolutionanalytics.com/2016/06/ross-ihaka-on-the-history-of-the-r-project.html

An R animation shows the ebb and flow of a recent Delaware River flood:
http://blog.revolutionanalytics.com/2016/06/visualizing-a-flood-with-r.html

A look at comparing statistical models with the caret package:
http://blog.revolutionanalytics.com/2016/05/using-caret-to-compare-models.html

The mscsweblm4r package provides an R interface to several Microsoft Cognitive Services APIs for text analysis:
http://blog.revolutionanalytics.com/2016/06/microsoft-cognitive-services.html

General interest stories (not related to R) in the past month included: a screenplay by a robot
(http://blog.revolutionanalytics.com/2016/06/because-its-friday-a-robot-writes-a-movie.html), parenting tips
(http://blog.revolutionanalytics.com/2016/06/because-its-friday-how-to-get-a-baby-to-sleep.html), contrasts of DNA and
ethnicity (http://blog.revolutionanalytics.com/2016/06/because-its-friday-the-dna-journey.html), and some new favourite
songs (http://blog.revolutionanalytics.com/2016/06/because-its-friday-late-to-the-party-music.html).

Meeting times for local R user groups (http://blog.revolutionanalytics.com/local-r-groups.html) can be found on the
updated R Community Calendar at: http://blog.revolutionanalytics.com/calendar.html
If you're looking for more articles about R, you can find summaries from previous months at
http://blog.revolutionanalytics.com/roundups/. You can receive daily blog posts via email using services like
blogtrottr.com.

As always, thanks for the comments and please keep sending suggestions to me at davidsmi at microsoft.com or via Twitter
(I'm @revodavid).

Cheers,
# David

-- 
David M Smith <davidsmi at microsoft.com>
R Community Lead, Microsoft? 
Tel: +1 (312) 9205766 (Chicago IL, USA)
Twitter: @revodavid | Blog: ?http://blog.revolutionanalytics.com


From hannah.hlx at gmail.com  Thu Jul  7 23:58:40 2016
From: hannah.hlx at gmail.com (li li)
Date: Thu, 7 Jul 2016 17:58:40 -0400
Subject: [R] Fixed Effects in lme function
Message-ID: <CAHLnndY4vVhVGJer4sr4k1sSTasW3+ibJcZY2FvtcCtjAudCpw@mail.gmail.com>

 Dear all,
    For the data below, I would like to fit a model with common random
slope and  common random intercept as shown below. I am interested in
obtaining
separate fixed effect estimates (intercept and slope and corresponding
hypothesis test)
for each method. Instead of performing the analysis method by method, can I
use the syntax as below, specifically, the part "fixed = response ~
method/time"?
    I know this is legitimate specification if there is no random effects
involved.
   Thanks so much in advance!
      Hanna

lme(fixed= response ~ method/time, random=~ 1+time | lot, data=dat,
weights= varIdent(form=~1|method),   control = lmeControl(opt = "optim"),
na.action = na.exclude)



  response individual time method
1    102.9          3    0      3
2    103.0          3    3      3
3    103.0          3    6      3
4    102.8          3    9      3
5    102.2          3   12      3
6    102.5          3   15      3
7    103.0          3   18      3
8    102.0          3   24      3
9    102.8          1    0      3
10   102.7          1    3      3
11   103.0          1    6      3
12   102.2          1    9      3
13   103.0          1   12      3
14   102.8          1   15      3
15   102.8          1   18      3
16   102.9          1   24      3
17   102.2          2    0      3
18   102.6          2    3      3
19   103.4          2    6      3
20   102.3          2    9      3
21   101.3          2   12      3
22   102.1          2   15      3
23   102.1          2   18      3
24   102.2          2   24      3
25   102.7          4    0      3
26   102.3          4    3      3
27   102.6          4    6      3
28   102.7          4    9      3
29   102.8          4   12      3
30   102.5          5    0      3
31   102.4          5    3      3
32   102.1          5    6      3
33   102.3          6    0      3
34   102.3          6    3      3
35   101.9          7    0      3
36   102.0          7    3      3
37   107.4          3    0      1
38   101.3          3   12      1
39    92.8          3   15      1
40    73.7          3   18      1
41   104.7          3   24      1
42    92.6          1    0      1
43   101.9          1   12      1
44   106.3          1   15      1
45   104.1          1   18      1
46    95.6          1   24      1
47    79.8          2    0      1
48    89.7          2   12      1
49    97.0          2   15      1
50   108.4          2   18      1
51   103.5          2   24      1
52    96.4          4    0      1
53    89.3          4   12      1
54   112.6          5    0      1
55    93.3          6    0      1
56    99.6          7    0      1
57   109.5          3    0      2
58    98.5          3   12      2
59   103.5          3   24      2
60   113.5          1    0      2
61    94.5          1   12      2
62    88.5          1   24      2
63    99.5          2    0      2
64    97.5          2   12      2
65    98.5          2   24      2
66   103.5          4    0      2
67    89.5          5    0      2
68    87.5          6    0      2
69    82.5          7    0      2

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Jul  8 00:26:06 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 07 Jul 2016 15:26:06 -0700
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
Message-ID: <0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>

Same as with any floating point numeric computation environment... you don't. There is always uncertainty in any floating point number... it is just larger in this data than you might be used to.

Once you get to the stage where you want to output values, read up on

?round
?par (digits)

and don't worry about the incidental display of extra digits prior to presentation (output). 
-- 
Sent from my phone. Please excuse my brevity.

On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <sezenismail at gmail.com> wrote:
>Hello,
>
>I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar
>reanalysis ncetcdf files. But data is in the form of (9.199998,
>8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision
>and least_significant_digit attributes of ncdf4 object [1]. For uwnd
>data, precision = 2 and least_significant_digits = 1. My doubt is that
>should I round data to 2 decimal places or 1 decimal place after
>decimal point?
>
>Same issue is valid for some header info.
>
>Output of ncdf4 object:
>
>
>Output of ncdump on terminal:
>
>
>for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
>scale_factor is 0.00999999977648258. You can notice same issue for
>actual_range and add_offset. Also a similar issue exist for the data.
>How can I truncate those extra unsignificant decimal places or round
>the numbers to significant decimal places?
>
>1 -
>http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
><http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Fri Jul  8 00:29:23 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 07 Jul 2016 15:29:23 -0700
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
	<0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
Message-ID: <AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>

Correction:

?options (not par)
-- 
Sent from my phone. Please excuse my brevity.

On July 7, 2016 3:26:06 PM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>Same as with any floating point numeric computation environment... you
>don't. There is always uncertainty in any floating point number... it
>is just larger in this data than you might be used to.
>
>Once you get to the stage where you want to output values, read up on
>
>?round
>?par (digits)
>
>and don't worry about the incidental display of extra digits prior to
>presentation (output). 
>-- 
>Sent from my phone. Please excuse my brevity.
>
>On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <sezenismail at gmail.com>
>wrote:
>>Hello,
>>
>>I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar
>>reanalysis ncetcdf files. But data is in the form of (9.199998,
>>8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision
>>and least_significant_digit attributes of ncdf4 object [1]. For uwnd
>>data, precision = 2 and least_significant_digits = 1. My doubt is that
>>should I round data to 2 decimal places or 1 decimal place after
>>decimal point?
>>
>>Same issue is valid for some header info.
>>
>>Output of ncdf4 object:
>>
>>
>>Output of ncdump on terminal:
>>
>>
>>for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
>>scale_factor is 0.00999999977648258. You can notice same issue for
>>actual_range and add_offset. Also a similar issue exist for the data.
>>How can I truncate those extra unsignificant decimal places or round
>>the numbers to significant decimal places?
>>
>>1 -
>>http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>><http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Fri Jul  8 00:36:16 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 8 Jul 2016 08:36:16 +1000
Subject: [R] r code for multilevel latent class analysis
In-Reply-To: <A8C817C7-49DA-4C4E-9D3A-CB70504B26EC@yahoo.it>
References: <A8C817C7-49DA-4C4E-9D3A-CB70504B26EC@yahoo.it>
Message-ID: <CA+8X3fU1unj78Zpg-=hMP3xed5iSaWPgTX1DPEY-KQ5QnfquSQ@mail.gmail.com>

Hi Cristina,
Try this:

names(mydata)

It may be NULL or "ppitrst" may be absent.

Jim


On Thu, Jul 7, 2016 at 8:26 PM, Cristina Cametti
<cristina.cametti at gmail.com> wrote:
> Dear all,
>
> I am not able to find a reliable r code to run a multilevel latent class model. Indeed, I have to analyze how social trust (three variables form the ESS survey) might vary between countries (21 countries in my database). I tried to use the poLCA package but I am not sure if my code is right. This is my code:
> lca <- cbind(ppltrst+1,pplfair+1,pplhlp+1)~cntry
> lc <- poLCA(lca,mydata)
>
> However, I get an error message:
> Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) :
> undefined columns selected
>
> How can I solve this? Is the code completely wrong or I missed some passages?
> Thank you very much for your help!
>
> Cristina
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sezenismail at gmail.com  Fri Jul  8 01:16:39 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 8 Jul 2016 02:16:39 +0300
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
	<0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
	<AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>
Message-ID: <E524DB53-7565-4EEB-A2E8-E7489E60144D@gmail.com>

Thank you very much Jeff.  I think I?m too far to be able to explain myself. Perhaps, this is the wrong list for this question but I sent it in hope there is someone has deep understanding of netcdf data and use R. Let me tell the story simpler. Assume that you read a numeric vector of data from a netcdf file:

data <- c(9.1999979, 8.7999979, 7.9999979, 3.0999980, 6.1000018, 10.1000017, 10.4000017, 9.2000017)

you know that the values above are a model output and also you know that, physically, first and last values must be equal but somehow they are not.

And now, you want to use ?periodic? spline for the values above.

spline(1:8, data, method = ?periodic?)

Voila! spline method throws a warning message: ?spline: first and last y values differ - using y[1] for both?. Then I go on digging and discover 2 attributes in netcdf file: ?precision = 2? and ?least_significant_digit = 1?. And I also found their definitions at [1].

precision -- number of places to right of decimal point that are significant, based on packing used. Type is short.
least_significant_digit -- power of ten of the smallest decimal place in unpacked data that is a reliable value. Type is short.

Please, do not condemn me, english is not my main language :). At this point, as a scientist, what would you do according to explanations above? I think I didn?t exactly understand the difference between precision and least_significant_digit. One says ?significant? and latter says ?reliable?. Should I round the numbers to 2 decimal places or 1 decimal place after decimal point?

Thanks,

1- http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml


> On 08 Jul 2016, at 01:29, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> Correction:
> 
> ?options (not par)
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On July 7, 2016 3:26:06 PM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>> Same as with any floating point numeric computation environment... you
>> don't. There is always uncertainty in any floating point number... it
>> is just larger in this data than you might be used to.
>> 
>> Once you get to the stage where you want to output values, read up on
>> 
>> ?round
>> ?par (digits)
>> 
>> and don't worry about the incidental display of extra digits prior to
>> presentation (output). 
>> -- 
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <sezenismail at gmail.com>
>> wrote:
>>> Hello,
>>> 
>>> I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar
>>> reanalysis ncetcdf files. But data is in the form of (9.199998,
>>> 8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision
>>> and least_significant_digit attributes of ncdf4 object [1]. For uwnd
>>> data, precision = 2 and least_significant_digits = 1. My doubt is that
>>> should I round data to 2 decimal places or 1 decimal place after
>>> decimal point?
>>> 
>>> Same issue is valid for some header info.
>>> 
>>> Output of ncdf4 object:
>>> 
>>> 
>>> Output of ncdump on terminal:
>>> 
>>> 
>>> for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
>>> scale_factor is 0.00999999977648258. You can notice same issue for
>>> actual_range and add_offset. Also a similar issue exist for the data.
>>> How can I truncate those extra unsignificant decimal places or round
>>> the numbers to significant decimal places?
>>> 
>>> 1 -
>>> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>> <http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 


	[[alternative HTML version deleted]]


From roy.mendelssohn at noaa.gov  Fri Jul  8 01:27:20 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Thu, 7 Jul 2016 16:27:20 -0700
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <E524DB53-7565-4EEB-A2E8-E7489E60144D@gmail.com>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
	<0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
	<AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>
	<E524DB53-7565-4EEB-A2E8-E7489E60144D@gmail.com>
Message-ID: <CCD29CF1-E4AA-4670-8B4D-9BBA33A5E54C@noaa.gov>

Hi Ismail:

Can you point me to a particular netcdf file you are working with.  I would like to play with it for awhile.  I am pretty certain the scale factor is 0.01 and what you are seeing in rounding error (or mor precisely I should say problems with representations of floating point numbers),  but i would like to see if there is away around this.

Thank,

-Roy

> On Jul 7, 2016, at 4:16 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
> 
> Thank you very much Jeff.  I think I?m too far to be able to explain myself. Perhaps, this is the wrong list for this question but I sent it in hope there is someone has deep understanding of netcdf data and use R. Let me tell the story simpler. Assume that you read a numeric vector of data from a netcdf file:
> 
> data <- c(9.1999979, 8.7999979, 7.9999979, 3.0999980, 6.1000018, 10.1000017, 10.4000017, 9.2000017)
> 
> you know that the values above are a model output and also you know that, physically, first and last values must be equal but somehow they are not.
> 
> And now, you want to use ?periodic? spline for the values above.
> 
> spline(1:8, data, method = ?periodic?)
> 
> Voila! spline method throws a warning message: ?spline: first and last y values differ - using y[1] for both?. Then I go on digging and discover 2 attributes in netcdf file: ?precision = 2? and ?least_significant_digit = 1?. And I also found their definitions at [1].
> 
> precision -- number of places to right of decimal point that are significant, based on packing used. Type is short.
> least_significant_digit -- power of ten of the smallest decimal place in unpacked data that is a reliable value. Type is short.
> 
> Please, do not condemn me, english is not my main language :). At this point, as a scientist, what would you do according to explanations above? I think I didn?t exactly understand the difference between precision and least_significant_digit. One says ?significant? and latter says ?reliable?. Should I round the numbers to 2 decimal places or 1 decimal place after decimal point?
> 
> Thanks,
> 
> 1- http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
> 
> 
>> On 08 Jul 2016, at 01:29, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>> 
>> Correction:
>> 
>> ?options (not par)
>> -- 
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 7, 2016 3:26:06 PM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>> Same as with any floating point numeric computation environment... you
>>> don't. There is always uncertainty in any floating point number... it
>>> is just larger in this data than you might be used to.
>>> 
>>> Once you get to the stage where you want to output values, read up on
>>> 
>>> ?round
>>> ?par (digits)
>>> 
>>> and don't worry about the incidental display of extra digits prior to
>>> presentation (output). 
>>> -- 
>>> Sent from my phone. Please excuse my brevity.
>>> 
>>> On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <sezenismail at gmail.com>
>>> wrote:
>>>> Hello,
>>>> 
>>>> I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar
>>>> reanalysis ncetcdf files. But data is in the form of (9.199998,
>>>> 8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision
>>>> and least_significant_digit attributes of ncdf4 object [1]. For uwnd
>>>> data, precision = 2 and least_significant_digits = 1. My doubt is that
>>>> should I round data to 2 decimal places or 1 decimal place after
>>>> decimal point?
>>>> 
>>>> Same issue is valid for some header info.
>>>> 
>>>> Output of ncdf4 object:
>>>> 
>>>> 
>>>> Output of ncdump on terminal:
>>>> 
>>>> 
>>>> for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
>>>> scale_factor is 0.00999999977648258. You can notice same issue for
>>>> actual_range and add_offset. Also a similar issue exist for the data.
>>>> How can I truncate those extra unsignificant decimal places or round
>>>> the numbers to significant decimal places?
>>>> 
>>>> 1 -
>>>> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>>> <http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From dwinsemius at comcast.net  Fri Jul  8 01:49:34 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 7 Jul 2016 16:49:34 -0700
Subject: [R] r code for multilevel latent class analysis
In-Reply-To: <CA+8X3fU1unj78Zpg-=hMP3xed5iSaWPgTX1DPEY-KQ5QnfquSQ@mail.gmail.com>
References: <A8C817C7-49DA-4C4E-9D3A-CB70504B26EC@yahoo.it>
	<CA+8X3fU1unj78Zpg-=hMP3xed5iSaWPgTX1DPEY-KQ5QnfquSQ@mail.gmail.com>
Message-ID: <704059C4-C1A8-4342-8783-136280B6C647@comcast.net>


> On Jul 7, 2016, at 3:36 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Hi Cristina,
> Try this:
> 
> names(mydata)
> 
> It may be NULL or "ppitrst" may be absent.

I've already suggested to Christina that she make sure the variables are spelled correctly and she reports they are all present in her dataset. So I tried a formula such as she posed with '1' added to each variable and this does throw the same error with the 'values'-dataframe that is used in the examples for that package.

> data(values,package='poLCA')
> str(values)
'data.frame':	216 obs. of  4 variables:
 $ A: num  2 2 2 2 2 2 2 2 2 2 ...
 $ B: num  2 2 2 2 2 2 2 2 2 2 ...
 $ C: num  2 2 2 2 2 2 2 2 2 2 ...
 $ D: num  2 2 2 2 2 2 2 2 2 2 ...
> library(poLCA)
Loading required package: scatterplot3d
Loading required package: MASS
> poLCA( cbind(A+1,B+1) ~ C, data=values)
Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) : 
  undefined columns selected

So I then tried removeing those "+1`"'s (which didn't seem to have much justification):

> poLCA( cbind(A,B) ~ C, data=values)
Conditional item response (column) probabilities,
 by outcome variable, for each class (row) 
 
$A
           Pr(1)  Pr(2)
class 1:  0.3428 0.6572
class 2:  0.0307 0.9693

$B
           Pr(1)  Pr(2)
class 1:  0.7737 0.2263
class 2:  0.1386 0.8614

snipped the rest of the output.

So "why add 1?" Seems to disturb the functions formula processing logic and is so far not explained.

-- 
David.

> 
> Jim
> 
> 
> On Thu, Jul 7, 2016 at 8:26 PM, Cristina Cametti
> <cristina.cametti at gmail.com> wrote:
>> Dear all,
>> 
>> I am not able to find a reliable r code to run a multilevel latent class model. Indeed, I have to analyze how social trust (three variables form the ESS survey) might vary between countries (21 countries in my database). I tried to use the poLCA package but I am not sure if my code is right. This is my code:
>> lca <- cbind(ppltrst+1,pplfair+1,pplhlp+1)~cntry
>> lc <- poLCA(lca,mydata)
>> 
>> However, I get an error message:
>> Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) :
>> undefined columns selected
>> 
>> How can I solve this? Is the code completely wrong or I missed some passages?
>> Thank you very much for your help!
>> 
>> Cristina
>>        [[alternative HTML version deleted]]
>> 

David Winsemius
Alameda, CA, USA


From sezenismail at gmail.com  Fri Jul  8 01:49:36 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 8 Jul 2016 02:49:36 +0300
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <CCD29CF1-E4AA-4670-8B4D-9BBA33A5E54C@noaa.gov>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
	<0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
	<AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>
	<E524DB53-7565-4EEB-A2E8-E7489E60144D@gmail.com>
	<CCD29CF1-E4AA-4670-8B4D-9BBA33A5E54C@noaa.gov>
Message-ID: <1A495F5B-0E52-4AE9-B3D1-C0B2C988DC50@gmail.com>

Thank you Roy. 

I use NCEP/NCAR Reanalysis 2 data [1]. More precisely, u-wind data of the year 2015 [2]. I am also pretty sure that the variables like scale_factor or add_offset should be precise like 0.01 or 187.65 but somehow (I hope this is not an issue originated by me) they are not, including data. Also let me note that I already contacted to author of ncdf4 package and also sent an email to ESRL, too, but no luck yet.

For a vectoral data, absolute and mutual u components of wind speed at the poles must be equal. For instance, at ?2015-01-01 00 GMT?, u-wind at longitude=0 and latitude=90 is 9.1999979 m/s and u-wind at longitude=180 and latitude=90 is -9.2000017 m/s. Minus sign comes from positive north direction. Physically, their absolute values must be equal.

1- http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis2.html
2- ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis2.dailyavgs/pressure/uwnd.2015.nc



> On 08 Jul 2016, at 02:27, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> 
> Hi Ismail:
> 
> Can you point me to a particular netcdf file you are working with.  I would like to play with it for awhile.  I am pretty certain the scale factor is 0.01 and what you are seeing in rounding error (or mor precisely I should say problems with representations of floating point numbers),  but i would like to see if there is away around this.
> 
> Thank,
> 
> -Roy
> 
>> On Jul 7, 2016, at 4:16 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
>> 
>> Thank you very much Jeff.  I think I?m too far to be able to explain myself. Perhaps, this is the wrong list for this question but I sent it in hope there is someone has deep understanding of netcdf data and use R. Let me tell the story simpler. Assume that you read a numeric vector of data from a netcdf file:
>> 
>> data <- c(9.1999979, 8.7999979, 7.9999979, 3.0999980, 6.1000018, 10.1000017, 10.4000017, 9.2000017)
>> 
>> you know that the values above are a model output and also you know that, physically, first and last values must be equal but somehow they are not.
>> 
>> And now, you want to use ?periodic? spline for the values above.
>> 
>> spline(1:8, data, method = ?periodic?)
>> 
>> Voila! spline method throws a warning message: ?spline: first and last y values differ - using y[1] for both?. Then I go on digging and discover 2 attributes in netcdf file: ?precision = 2? and ?least_significant_digit = 1?. And I also found their definitions at [1].
>> 
>> precision -- number of places to right of decimal point that are significant, based on packing used. Type is short.
>> least_significant_digit -- power of ten of the smallest decimal place in unpacked data that is a reliable value. Type is short.
>> 
>> Please, do not condemn me, english is not my main language :). At this point, as a scientist, what would you do according to explanations above? I think I didn?t exactly understand the difference between precision and least_significant_digit. One says ?significant? and latter says ?reliable?. Should I round the numbers to 2 decimal places or 1 decimal place after decimal point?
>> 
>> Thanks,
>> 
>> 1- http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>> 
>> 
>>> On 08 Jul 2016, at 01:29, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>> 
>>> Correction:
>>> 
>>> ?options (not par)
>>> -- 
>>> Sent from my phone. Please excuse my brevity.
>>> 
>>> On July 7, 2016 3:26:06 PM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>> Same as with any floating point numeric computation environment... you
>>>> don't. There is always uncertainty in any floating point number... it
>>>> is just larger in this data than you might be used to.
>>>> 
>>>> Once you get to the stage where you want to output values, read up on
>>>> 
>>>> ?round
>>>> ?par (digits)
>>>> 
>>>> and don't worry about the incidental display of extra digits prior to
>>>> presentation (output). 
>>>> -- 
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <sezenismail at gmail.com>
>>>> wrote:
>>>>> Hello,
>>>>> 
>>>>> I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar
>>>>> reanalysis ncetcdf files. But data is in the form of (9.199998,
>>>>> 8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision
>>>>> and least_significant_digit attributes of ncdf4 object [1]. For uwnd
>>>>> data, precision = 2 and least_significant_digits = 1. My doubt is that
>>>>> should I round data to 2 decimal places or 1 decimal place after
>>>>> decimal point?
>>>>> 
>>>>> Same issue is valid for some header info.
>>>>> 
>>>>> Output of ncdf4 object:
>>>>> 
>>>>> 
>>>>> Output of ncdump on terminal:
>>>>> 
>>>>> 
>>>>> for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
>>>>> scale_factor is 0.00999999977648258. You can notice same issue for
>>>>> actual_range and add_offset. Also a similar issue exist for the data.
>>>>> How can I truncate those extra unsignificant decimal places or round
>>>>> the numbers to significant decimal places?
>>>>> 
>>>>> 1 -
>>>>> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>>>> <http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new address and phone***
> 110 Shaffer Road
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected" 
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 


From roy.mendelssohn at noaa.gov  Fri Jul  8 02:21:41 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Thu, 7 Jul 2016 17:21:41 -0700
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <1A495F5B-0E52-4AE9-B3D1-C0B2C988DC50@gmail.com>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
	<0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
	<AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>
	<E524DB53-7565-4EEB-A2E8-E7489E60144D@gmail.com>
	<CCD29CF1-E4AA-4670-8B4D-9BBA33A5E54C@noaa.gov>
	<1A495F5B-0E52-4AE9-B3D1-C0B2C988DC50@gmail.com>
Message-ID: <B5D7DA8E-0533-477C-AD9E-CF7EFCCD1C90@noaa.gov>

After looking at the file, doing an extract say into the variable uwind,  if I do:

str(uwind)

I see what I expect, but if I just do:

uwind


I see what you are seeing.  Try:

uwindnew <- round(uwind, digits = 2) 


and see if that gives you the results you would expect.  

HTH,

-Roy

> On Jul 7, 2016, at 4:49 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
> 
> Thank you Roy. 
> 
> I use NCEP/NCAR Reanalysis 2 data [1]. More precisely, u-wind data of the year 2015 [2]. I am also pretty sure that the variables like scale_factor or add_offset should be precise like 0.01 or 187.65 but somehow (I hope this is not an issue originated by me) they are not, including data. Also let me note that I already contacted to author of ncdf4 package and also sent an email to ESRL, too, but no luck yet.
> 
> For a vectoral data, absolute and mutual u components of wind speed at the poles must be equal. For instance, at ?2015-01-01 00 GMT?, u-wind at longitude=0 and latitude=90 is 9.1999979 m/s and u-wind at longitude=180 and latitude=90 is -9.2000017 m/s. Minus sign comes from positive north direction. Physically, their absolute values must be equal.
> 
> 1- http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis2.html
> 2- ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis2.dailyavgs/pressure/uwnd.2015.nc
> 
> 
> 
>> On 08 Jul 2016, at 02:27, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
>> 
>> Hi Ismail:
>> 
>> Can you point me to a particular netcdf file you are working with.  I would like to play with it for awhile.  I am pretty certain the scale factor is 0.01 and what you are seeing in rounding error (or mor precisely I should say problems with representations of floating point numbers),  but i would like to see if there is away around this.
>> 
>> Thank,
>> 
>> -Roy
>> 
>>> On Jul 7, 2016, at 4:16 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
>>> 
>>> Thank you very much Jeff.  I think I?m too far to be able to explain myself. Perhaps, this is the wrong list for this question but I sent it in hope there is someone has deep understanding of netcdf data and use R. Let me tell the story simpler. Assume that you read a numeric vector of data from a netcdf file:
>>> 
>>> data <- c(9.1999979, 8.7999979, 7.9999979, 3.0999980, 6.1000018, 10.1000017, 10.4000017, 9.2000017)
>>> 
>>> you know that the values above are a model output and also you know that, physically, first and last values must be equal but somehow they are not.
>>> 
>>> And now, you want to use ?periodic? spline for the values above.
>>> 
>>> spline(1:8, data, method = ?periodic?)
>>> 
>>> Voila! spline method throws a warning message: ?spline: first and last y values differ - using y[1] for both?. Then I go on digging and discover 2 attributes in netcdf file: ?precision = 2? and ?least_significant_digit = 1?. And I also found their definitions at [1].
>>> 
>>> precision -- number of places to right of decimal point that are significant, based on packing used. Type is short.
>>> least_significant_digit -- power of ten of the smallest decimal place in unpacked data that is a reliable value. Type is short.
>>> 
>>> Please, do not condemn me, english is not my main language :). At this point, as a scientist, what would you do according to explanations above? I think I didn?t exactly understand the difference between precision and least_significant_digit. One says ?significant? and latter says ?reliable?. Should I round the numbers to 2 decimal places or 1 decimal place after decimal point?
>>> 
>>> Thanks,
>>> 
>>> 1- http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>> 
>>> 
>>>> On 08 Jul 2016, at 01:29, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>> 
>>>> Correction:
>>>> 
>>>> ?options (not par)
>>>> -- 
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On July 7, 2016 3:26:06 PM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>> Same as with any floating point numeric computation environment... you
>>>>> don't. There is always uncertainty in any floating point number... it
>>>>> is just larger in this data than you might be used to.
>>>>> 
>>>>> Once you get to the stage where you want to output values, read up on
>>>>> 
>>>>> ?round
>>>>> ?par (digits)
>>>>> 
>>>>> and don't worry about the incidental display of extra digits prior to
>>>>> presentation (output). 
>>>>> -- 
>>>>> Sent from my phone. Please excuse my brevity.
>>>>> 
>>>>> On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <sezenismail at gmail.com>
>>>>> wrote:
>>>>>> Hello,
>>>>>> 
>>>>>> I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar
>>>>>> reanalysis ncetcdf files. But data is in the form of (9.199998,
>>>>>> 8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision
>>>>>> and least_significant_digit attributes of ncdf4 object [1]. For uwnd
>>>>>> data, precision = 2 and least_significant_digits = 1. My doubt is that
>>>>>> should I round data to 2 decimal places or 1 decimal place after
>>>>>> decimal point?
>>>>>> 
>>>>>> Same issue is valid for some header info.
>>>>>> 
>>>>>> Output of ncdf4 object:
>>>>>> 
>>>>>> 
>>>>>> Output of ncdump on terminal:
>>>>>> 
>>>>>> 
>>>>>> for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
>>>>>> scale_factor is 0.00999999977648258. You can notice same issue for
>>>>>> actual_range and add_offset. Also a similar issue exist for the data.
>>>>>> How can I truncate those extra unsignificant decimal places or round
>>>>>> the numbers to significant decimal places?
>>>>>> 
>>>>>> 1 -
>>>>>> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>>>>> <http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> **********************
>> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
>> **********************
>> Roy Mendelssohn
>> Supervisory Operations Research Analyst
>> NOAA/NMFS
>> Environmental Research Division
>> Southwest Fisheries Science Center
>> ***Note new address and phone***
>> 110 Shaffer Road
>> Santa Cruz, CA 95060
>> Phone: (831)-420-3666
>> Fax: (831) 420-3980
>> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>> 
>> "Old age and treachery will overcome youth and skill."
>> "From those who have been given much, much will be expected" 
>> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
>> 
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From sezenismail at gmail.com  Fri Jul  8 03:02:35 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 8 Jul 2016 04:02:35 +0300
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <B5D7DA8E-0533-477C-AD9E-CF7EFCCD1C90@noaa.gov>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
	<0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
	<AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>
	<E524DB53-7565-4EEB-A2E8-E7489E60144D@gmail.com>
	<CCD29CF1-E4AA-4670-8B4D-9BBA33A5E54C@noaa.gov>
	<1A495F5B-0E52-4AE9-B3D1-C0B2C988DC50@gmail.com>
	<B5D7DA8E-0533-477C-AD9E-CF7EFCCD1C90@noaa.gov>
Message-ID: <7F8CC447-2F90-44D6-910C-5E42DFE2C5FE@gmail.com>

Thank you Roy. If I use "round(uwind, digits = 2)?, all data will have 2 decimal places after decimal point. It?s ok. But How do you know you should round the number to 2 decimal digits? According to definitions of precision and least_significant_digit, should I round to 2 decimal digits or 1 decimal digit? 

For instance, If you check the header information of omega.2015.nc file it says;

$ ncdump -h omega.2015.nc

...
omega:precision = 3s;
omega:least_significant_digit = 3s;
?

So, I need to round values to 3 decimal places after point?

and if you check the output of rhum.2015.nc;

$ ncdump -h rhum.2015.nc
...
rhum:precision = 2s ;
rhum:least_significant_digit = 0s ;
?

Then I need to round values to 2 decimal places after point?

Should I accomplish the rounding operation according to precision or least_significant_digit attributes? I think someone put these attributes in netcdf files for some reason. Also I belive, if required, this kind of an operation must be done in related package but author said that it is nothing to do with ncdf4 package.

Please, forgive me for taking your time.


> On 08 Jul 2016, at 03:21, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> 
> After looking at the file, doing an extract say into the variable uwind,  if I do:
> 
> str(uwind)
> 
> I see what I expect, but if I just do:
> 
> uwind
> 
> 
> I see what you are seeing.  Try:
> 
> uwindnew <- round(uwind, digits = 2) 
> 
> 
> and see if that gives you the results you would expect.  
> 
> HTH,
> 
> -Roy
> 
>> On Jul 7, 2016, at 4:49 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
>> 
>> Thank you Roy. 
>> 
>> I use NCEP/NCAR Reanalysis 2 data [1]. More precisely, u-wind data of the year 2015 [2]. I am also pretty sure that the variables like scale_factor or add_offset should be precise like 0.01 or 187.65 but somehow (I hope this is not an issue originated by me) they are not, including data. Also let me note that I already contacted to author of ncdf4 package and also sent an email to ESRL, too, but no luck yet.
>> 
>> For a vectoral data, absolute and mutual u components of wind speed at the poles must be equal. For instance, at ?2015-01-01 00 GMT?, u-wind at longitude=0 and latitude=90 is 9.1999979 m/s and u-wind at longitude=180 and latitude=90 is -9.2000017 m/s. Minus sign comes from positive north direction. Physically, their absolute values must be equal.
>> 
>> 1- http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis2.html
>> 2- ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis2.dailyavgs/pressure/uwnd.2015.nc
>> 
>> 
>> 
>>> On 08 Jul 2016, at 02:27, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
>>> 
>>> Hi Ismail:
>>> 
>>> Can you point me to a particular netcdf file you are working with.  I would like to play with it for awhile.  I am pretty certain the scale factor is 0.01 and what you are seeing in rounding error (or mor precisely I should say problems with representations of floating point numbers),  but i would like to see if there is away around this.
>>> 
>>> Thank,
>>> 
>>> -Roy
>>> 
>>>> On Jul 7, 2016, at 4:16 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
>>>> 
>>>> Thank you very much Jeff.  I think I?m too far to be able to explain myself. Perhaps, this is the wrong list for this question but I sent it in hope there is someone has deep understanding of netcdf data and use R. Let me tell the story simpler. Assume that you read a numeric vector of data from a netcdf file:
>>>> 
>>>> data <- c(9.1999979, 8.7999979, 7.9999979, 3.0999980, 6.1000018, 10.1000017, 10.4000017, 9.2000017)
>>>> 
>>>> you know that the values above are a model output and also you know that, physically, first and last values must be equal but somehow they are not.
>>>> 
>>>> And now, you want to use ?periodic? spline for the values above.
>>>> 
>>>> spline(1:8, data, method = ?periodic?)
>>>> 
>>>> Voila! spline method throws a warning message: ?spline: first and last y values differ - using y[1] for both?. Then I go on digging and discover 2 attributes in netcdf file: ?precision = 2? and ?least_significant_digit = 1?. And I also found their definitions at [1].
>>>> 
>>>> precision -- number of places to right of decimal point that are significant, based on packing used. Type is short.
>>>> least_significant_digit -- power of ten of the smallest decimal place in unpacked data that is a reliable value. Type is short.
>>>> 
>>>> Please, do not condemn me, english is not my main language :). At this point, as a scientist, what would you do according to explanations above? I think I didn?t exactly understand the difference between precision and least_significant_digit. One says ?significant? and latter says ?reliable?. Should I round the numbers to 2 decimal places or 1 decimal place after decimal point?
>>>> 
>>>> Thanks,
>>>> 
>>>> 1- http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>>> 
>>>> 
>>>>> On 08 Jul 2016, at 01:29, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>> 
>>>>> Correction:
>>>>> 
>>>>> ?options (not par)
>>>>> -- 
>>>>> Sent from my phone. Please excuse my brevity.
>>>>> 
>>>>> On July 7, 2016 3:26:06 PM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>> Same as with any floating point numeric computation environment... you
>>>>>> don't. There is always uncertainty in any floating point number... it
>>>>>> is just larger in this data than you might be used to.
>>>>>> 
>>>>>> Once you get to the stage where you want to output values, read up on
>>>>>> 
>>>>>> ?round
>>>>>> ?par (digits)
>>>>>> 
>>>>>> and don't worry about the incidental display of extra digits prior to
>>>>>> presentation (output). 
>>>>>> -- 
>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>> 
>>>>>> On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <sezenismail at gmail.com>
>>>>>> wrote:
>>>>>>> Hello,
>>>>>>> 
>>>>>>> I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar
>>>>>>> reanalysis ncetcdf files. But data is in the form of (9.199998,
>>>>>>> 8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision
>>>>>>> and least_significant_digit attributes of ncdf4 object [1]. For uwnd
>>>>>>> data, precision = 2 and least_significant_digits = 1. My doubt is that
>>>>>>> should I round data to 2 decimal places or 1 decimal place after
>>>>>>> decimal point?
>>>>>>> 
>>>>>>> Same issue is valid for some header info.
>>>>>>> 
>>>>>>> Output of ncdf4 object:
>>>>>>> 
>>>>>>> 
>>>>>>> Output of ncdump on terminal:
>>>>>>> 
>>>>>>> 
>>>>>>> for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
>>>>>>> scale_factor is 0.00999999977648258. You can notice same issue for
>>>>>>> actual_range and add_offset. Also a similar issue exist for the data.
>>>>>>> How can I truncate those extra unsignificant decimal places or round
>>>>>>> the numbers to significant decimal places?
>>>>>>> 
>>>>>>> 1 -
>>>>>>> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>>>>>> <http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>> 
>>>> 
>>>> 	[[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> **********************
>>> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
>>> **********************
>>> Roy Mendelssohn
>>> Supervisory Operations Research Analyst
>>> NOAA/NMFS
>>> Environmental Research Division
>>> Southwest Fisheries Science Center
>>> ***Note new address and phone***
>>> 110 Shaffer Road
>>> Santa Cruz, CA 95060
>>> Phone: (831)-420-3666
>>> Fax: (831) 420-3980
>>> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>>> 
>>> "Old age and treachery will overcome youth and skill."
>>> "From those who have been given much, much will be expected" 
>>> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
>>> 
>> 
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new address and phone***
> 110 Shaffer Road
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected" 
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 


From roy.mendelssohn at noaa.gov  Fri Jul  8 03:28:37 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Thu, 7 Jul 2016 18:28:37 -0700
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <7F8CC447-2F90-44D6-910C-5E42DFE2C5FE@gmail.com>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
	<0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
	<AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>
	<E524DB53-7565-4EEB-A2E8-E7489E60144D@gmail.com>
	<CCD29CF1-E4AA-4670-8B4D-9BBA33A5E54C@noaa.gov>
	<1A495F5B-0E52-4AE9-B3D1-C0B2C988DC50@gmail.com>
	<B5D7DA8E-0533-477C-AD9E-CF7EFCCD1C90@noaa.gov>
	<7F8CC447-2F90-44D6-910C-5E42DFE2C5FE@gmail.com>
Message-ID: <F31BD9D3-7FF7-4B11-BB07-EA052281E8DE@noaa.gov>

I have moved this over to the netcdf-group mail list, which I think is the more appropriate place at this point.  You are copied, and hopefully someone from ESRL will see it and provide the proper response.

HTH,

-Roy

> On Jul 7, 2016, at 6:02 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
> 
> Thank you Roy. If I use "round(uwind, digits = 2)?, all data will have 2 decimal places after decimal point. It?s ok. But How do you know you should round the number to 2 decimal digits? According to definitions of precision and least_significant_digit, should I round to 2 decimal digits or 1 decimal digit? 
> 
> For instance, If you check the header information of omega.2015.nc file it says;
> 
> $ ncdump -h omega.2015.nc
> 
> ...
> omega:precision = 3s;
> omega:least_significant_digit = 3s;
> ?
> 
> So, I need to round values to 3 decimal places after point?
> 
> and if you check the output of rhum.2015.nc;
> 
> $ ncdump -h rhum.2015.nc
> ...
> rhum:precision = 2s ;
> rhum:least_significant_digit = 0s ;
> ?
> 
> Then I need to round values to 2 decimal places after point?
> 
> Should I accomplish the rounding operation according to precision or least_significant_digit attributes? I think someone put these attributes in netcdf files for some reason. Also I belive, if required, this kind of an operation must be done in related package but author said that it is nothing to do with ncdf4 package.
> 
> Please, forgive me for taking your time.
> 
> 
>> On 08 Jul 2016, at 03:21, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
>> 
>> After looking at the file, doing an extract say into the variable uwind,  if I do:
>> 
>> str(uwind)
>> 
>> I see what I expect, but if I just do:
>> 
>> uwind
>> 
>> 
>> I see what you are seeing.  Try:
>> 
>> uwindnew <- round(uwind, digits = 2) 
>> 
>> 
>> and see if that gives you the results you would expect.  
>> 
>> HTH,
>> 
>> -Roy
>> 
>>> On Jul 7, 2016, at 4:49 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
>>> 
>>> Thank you Roy. 
>>> 
>>> I use NCEP/NCAR Reanalysis 2 data [1]. More precisely, u-wind data of the year 2015 [2]. I am also pretty sure that the variables like scale_factor or add_offset should be precise like 0.01 or 187.65 but somehow (I hope this is not an issue originated by me) they are not, including data. Also let me note that I already contacted to author of ncdf4 package and also sent an email to ESRL, too, but no luck yet.
>>> 
>>> For a vectoral data, absolute and mutual u components of wind speed at the poles must be equal. For instance, at ?2015-01-01 00 GMT?, u-wind at longitude=0 and latitude=90 is 9.1999979 m/s and u-wind at longitude=180 and latitude=90 is -9.2000017 m/s. Minus sign comes from positive north direction. Physically, their absolute values must be equal.
>>> 
>>> 1- http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis2.html
>>> 2- ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis2.dailyavgs/pressure/uwnd.2015.nc
>>> 
>>> 
>>> 
>>>> On 08 Jul 2016, at 02:27, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
>>>> 
>>>> Hi Ismail:
>>>> 
>>>> Can you point me to a particular netcdf file you are working with.  I would like to play with it for awhile.  I am pretty certain the scale factor is 0.01 and what you are seeing in rounding error (or mor precisely I should say problems with representations of floating point numbers),  but i would like to see if there is away around this.
>>>> 
>>>> Thank,
>>>> 
>>>> -Roy
>>>> 
>>>>> On Jul 7, 2016, at 4:16 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
>>>>> 
>>>>> Thank you very much Jeff.  I think I?m too far to be able to explain myself. Perhaps, this is the wrong list for this question but I sent it in hope there is someone has deep understanding of netcdf data and use R. Let me tell the story simpler. Assume that you read a numeric vector of data from a netcdf file:
>>>>> 
>>>>> data <- c(9.1999979, 8.7999979, 7.9999979, 3.0999980, 6.1000018, 10.1000017, 10.4000017, 9.2000017)
>>>>> 
>>>>> you know that the values above are a model output and also you know that, physically, first and last values must be equal but somehow they are not.
>>>>> 
>>>>> And now, you want to use ?periodic? spline for the values above.
>>>>> 
>>>>> spline(1:8, data, method = ?periodic?)
>>>>> 
>>>>> Voila! spline method throws a warning message: ?spline: first and last y values differ - using y[1] for both?. Then I go on digging and discover 2 attributes in netcdf file: ?precision = 2? and ?least_significant_digit = 1?. And I also found their definitions at [1].
>>>>> 
>>>>> precision -- number of places to right of decimal point that are significant, based on packing used. Type is short.
>>>>> least_significant_digit -- power of ten of the smallest decimal place in unpacked data that is a reliable value. Type is short.
>>>>> 
>>>>> Please, do not condemn me, english is not my main language :). At this point, as a scientist, what would you do according to explanations above? I think I didn?t exactly understand the difference between precision and least_significant_digit. One says ?significant? and latter says ?reliable?. Should I round the numbers to 2 decimal places or 1 decimal place after decimal point?
>>>>> 
>>>>> Thanks,
>>>>> 
>>>>> 1- http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>>>> 
>>>>> 
>>>>>> On 08 Jul 2016, at 01:29, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>> 
>>>>>> Correction:
>>>>>> 
>>>>>> ?options (not par)
>>>>>> -- 
>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>> 
>>>>>> On July 7, 2016 3:26:06 PM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>>>> Same as with any floating point numeric computation environment... you
>>>>>>> don't. There is always uncertainty in any floating point number... it
>>>>>>> is just larger in this data than you might be used to.
>>>>>>> 
>>>>>>> Once you get to the stage where you want to output values, read up on
>>>>>>> 
>>>>>>> ?round
>>>>>>> ?par (digits)
>>>>>>> 
>>>>>>> and don't worry about the incidental display of extra digits prior to
>>>>>>> presentation (output). 
>>>>>>> -- 
>>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>>> 
>>>>>>> On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <sezenismail at gmail.com>
>>>>>>> wrote:
>>>>>>>> Hello,
>>>>>>>> 
>>>>>>>> I use ncdf4 and ncdf4.helpers packages to get wind data from ncep/ncar
>>>>>>>> reanalysis ncetcdf files. But data is in the form of (9.199998,
>>>>>>>> 8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of precision
>>>>>>>> and least_significant_digit attributes of ncdf4 object [1]. For uwnd
>>>>>>>> data, precision = 2 and least_significant_digits = 1. My doubt is that
>>>>>>>> should I round data to 2 decimal places or 1 decimal place after
>>>>>>>> decimal point?
>>>>>>>> 
>>>>>>>> Same issue is valid for some header info.
>>>>>>>> 
>>>>>>>> Output of ncdf4 object:
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Output of ncdump on terminal:
>>>>>>>> 
>>>>>>>> 
>>>>>>>> for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
>>>>>>>> scale_factor is 0.00999999977648258. You can notice same issue for
>>>>>>>> actual_range and add_offset. Also a similar issue exist for the data.
>>>>>>>> How can I truncate those extra unsignificant decimal places or round
>>>>>>>> the numbers to significant decimal places?
>>>>>>>> 
>>>>>>>> 1 -
>>>>>>>> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
>>>>>>>> <http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml>
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>> 
>>>>> 
>>>>> 	[[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> **********************
>>>> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
>>>> **********************
>>>> Roy Mendelssohn
>>>> Supervisory Operations Research Analyst
>>>> NOAA/NMFS
>>>> Environmental Research Division
>>>> Southwest Fisheries Science Center
>>>> ***Note new address and phone***
>>>> 110 Shaffer Road
>>>> Santa Cruz, CA 95060
>>>> Phone: (831)-420-3666
>>>> Fax: (831) 420-3980
>>>> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>>>> 
>>>> "Old age and treachery will overcome youth and skill."
>>>> "From those who have been given much, much will be expected" 
>>>> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
>>>> 
>>> 
>> 
>> **********************
>> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
>> **********************
>> Roy Mendelssohn
>> Supervisory Operations Research Analyst
>> NOAA/NMFS
>> Environmental Research Division
>> Southwest Fisheries Science Center
>> ***Note new address and phone***
>> 110 Shaffer Road
>> Santa Cruz, CA 95060
>> Phone: (831)-420-3666
>> Fax: (831) 420-3980
>> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>> 
>> "Old age and treachery will overcome youth and skill."
>> "From those who have been given much, much will be expected" 
>> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
>> 
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From sezenismail at gmail.com  Fri Jul  8 03:36:32 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 08 Jul 2016 01:36:32 +0000
Subject: [R] netcdf data precision or least significant digit
In-Reply-To: <F31BD9D3-7FF7-4B11-BB07-EA052281E8DE@noaa.gov>
References: <49AA659C-1E10-41A3-BCDA-3E8226ABFC96@gmail.com>
	<0C2C4678-7DC6-446E-88E5-3611AAA3CD32@dcn.davis.ca.us>
	<AD8AE391-E27A-4E31-9D79-8A18FB7F7DF4@dcn.davis.ca.us>
	<E524DB53-7565-4EEB-A2E8-E7489E60144D@gmail.com>
	<CCD29CF1-E4AA-4670-8B4D-9BBA33A5E54C@noaa.gov>
	<1A495F5B-0E52-4AE9-B3D1-C0B2C988DC50@gmail.com>
	<B5D7DA8E-0533-477C-AD9E-CF7EFCCD1C90@noaa.gov>
	<7F8CC447-2F90-44D6-910C-5E42DFE2C5FE@gmail.com>
	<F31BD9D3-7FF7-4B11-BB07-EA052281E8DE@noaa.gov>
Message-ID: <CABOzBw3G_wTcBHNqWSOpUUZpns5dxXqkeoDkdnLa-+xh1CC=nA@mail.gmail.com>

Thank you very much Roy and Jeff for your help. I contacted dear David, the
author of ncdf4 package, at first and he lead me here. Sorry for any
inconvenience.

I wish you success in your work.

Regards,

On Fri, Jul 8, 2016, 04:28 Roy Mendelssohn - NOAA Federal <
roy.mendelssohn at noaa.gov> wrote:

> I have moved this over to the netcdf-group mail list, which I think is the
> more appropriate place at this point.  You are copied, and hopefully
> someone from ESRL will see it and provide the proper response.
>
> HTH,
>
> -Roy
>
> > On Jul 7, 2016, at 6:02 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:
> >
> > Thank you Roy. If I use "round(uwind, digits = 2)?, all data will have 2
> decimal places after decimal point. It?s ok. But How do you know you should
> round the number to 2 decimal digits? According to definitions of precision
> and least_significant_digit, should I round to 2 decimal digits or 1
> decimal digit?
> >
> > For instance, If you check the header information of omega.2015.nc file
> it says;
> >
> > $ ncdump -h omega.2015.nc
> >
> > ...
> > omega:precision = 3s;
> > omega:least_significant_digit = 3s;
> > ?
> >
> > So, I need to round values to 3 decimal places after point?
> >
> > and if you check the output of rhum.2015.nc;
> >
> > $ ncdump -h rhum.2015.nc
> > ...
> > rhum:precision = 2s ;
> > rhum:least_significant_digit = 0s ;
> > ?
> >
> > Then I need to round values to 2 decimal places after point?
> >
> > Should I accomplish the rounding operation according to precision or
> least_significant_digit attributes? I think someone put these attributes in
> netcdf files for some reason. Also I belive, if required, this kind of an
> operation must be done in related package but author said that it is
> nothing to do with ncdf4 package.
> >
> > Please, forgive me for taking your time.
> >
> >
> >> On 08 Jul 2016, at 03:21, Roy Mendelssohn - NOAA Federal <
> roy.mendelssohn at noaa.gov> wrote:
> >>
> >> After looking at the file, doing an extract say into the variable
> uwind,  if I do:
> >>
> >> str(uwind)
> >>
> >> I see what I expect, but if I just do:
> >>
> >> uwind
> >>
> >>
> >> I see what you are seeing.  Try:
> >>
> >> uwindnew <- round(uwind, digits = 2)
> >>
> >>
> >> and see if that gives you the results you would expect.
> >>
> >> HTH,
> >>
> >> -Roy
> >>
> >>> On Jul 7, 2016, at 4:49 PM, Ismail SEZEN <sezenismail at gmail.com>
> wrote:
> >>>
> >>> Thank you Roy.
> >>>
> >>> I use NCEP/NCAR Reanalysis 2 data [1]. More precisely, u-wind data of
> the year 2015 [2]. I am also pretty sure that the variables like
> scale_factor or add_offset should be precise like 0.01 or 187.65 but
> somehow (I hope this is not an issue originated by me) they are not,
> including data. Also let me note that I already contacted to author of
> ncdf4 package and also sent an email to ESRL, too, but no luck yet.
> >>>
> >>> For a vectoral data, absolute and mutual u components of wind speed at
> the poles must be equal. For instance, at ?2015-01-01 00 GMT?, u-wind at
> longitude=0 and latitude=90 is 9.1999979 m/s and u-wind at longitude=180
> and latitude=90 is -9.2000017 m/s. Minus sign comes from positive north
> direction. Physically, their absolute values must be equal.
> >>>
> >>> 1-
> http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis2.html
> >>> 2-
> ftp://ftp.cdc.noaa.gov/Datasets/ncep.reanalysis2.dailyavgs/pressure/uwnd.2015.nc
> >>>
> >>>
> >>>
> >>>> On 08 Jul 2016, at 02:27, Roy Mendelssohn - NOAA Federal <
> roy.mendelssohn at noaa.gov> wrote:
> >>>>
> >>>> Hi Ismail:
> >>>>
> >>>> Can you point me to a particular netcdf file you are working with.  I
> would like to play with it for awhile.  I am pretty certain the scale
> factor is 0.01 and what you are seeing in rounding error (or mor precisely
> I should say problems with representations of floating point numbers),  but
> i would like to see if there is away around this.
> >>>>
> >>>> Thank,
> >>>>
> >>>> -Roy
> >>>>
> >>>>> On Jul 7, 2016, at 4:16 PM, Ismail SEZEN <sezenismail at gmail.com>
> wrote:
> >>>>>
> >>>>> Thank you very much Jeff.  I think I?m too far to be able to explain
> myself. Perhaps, this is the wrong list for this question but I sent it in
> hope there is someone has deep understanding of netcdf data and use R. Let
> me tell the story simpler. Assume that you read a numeric vector of data
> from a netcdf file:
> >>>>>
> >>>>> data <- c(9.1999979, 8.7999979, 7.9999979, 3.0999980, 6.1000018,
> 10.1000017, 10.4000017, 9.2000017)
> >>>>>
> >>>>> you know that the values above are a model output and also you know
> that, physically, first and last values must be equal but somehow they are
> not.
> >>>>>
> >>>>> And now, you want to use ?periodic? spline for the values above.
> >>>>>
> >>>>> spline(1:8, data, method = ?periodic?)
> >>>>>
> >>>>> Voila! spline method throws a warning message: ?spline: first and
> last y values differ - using y[1] for both?. Then I go on digging and
> discover 2 attributes in netcdf file: ?precision = 2? and
> ?least_significant_digit = 1?. And I also found their definitions at [1].
> >>>>>
> >>>>> precision -- number of places to right of decimal point that are
> significant, based on packing used. Type is short.
> >>>>> least_significant_digit -- power of ten of the smallest decimal
> place in unpacked data that is a reliable value. Type is short.
> >>>>>
> >>>>> Please, do not condemn me, english is not my main language :). At
> this point, as a scientist, what would you do according to explanations
> above? I think I didn?t exactly understand the difference between precision
> and least_significant_digit. One says ?significant? and latter says
> ?reliable?. Should I round the numbers to 2 decimal places or 1 decimal
> place after decimal point?
> >>>>>
> >>>>> Thanks,
> >>>>>
> >>>>> 1-
> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
> >>>>>
> >>>>>
> >>>>>> On 08 Jul 2016, at 01:29, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> >>>>>>
> >>>>>> Correction:
> >>>>>>
> >>>>>> ?options (not par)
> >>>>>> --
> >>>>>> Sent from my phone. Please excuse my brevity.
> >>>>>>
> >>>>>> On July 7, 2016 3:26:06 PM PDT, Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us> wrote:
> >>>>>>> Same as with any floating point numeric computation environment...
> you
> >>>>>>> don't. There is always uncertainty in any floating point number...
> it
> >>>>>>> is just larger in this data than you might be used to.
> >>>>>>>
> >>>>>>> Once you get to the stage where you want to output values, read up
> on
> >>>>>>>
> >>>>>>> ?round
> >>>>>>> ?par (digits)
> >>>>>>>
> >>>>>>> and don't worry about the incidental display of extra digits prior
> to
> >>>>>>> presentation (output).
> >>>>>>> --
> >>>>>>> Sent from my phone. Please excuse my brevity.
> >>>>>>>
> >>>>>>> On July 7, 2016 12:50:54 AM PDT, Ismail SEZEN <
> sezenismail at gmail.com>
> >>>>>>> wrote:
> >>>>>>>> Hello,
> >>>>>>>>
> >>>>>>>> I use ncdf4 and ncdf4.helpers packages to get wind data from
> ncep/ncar
> >>>>>>>> reanalysis ncetcdf files. But data is in the form of (9.199998,
> >>>>>>>> 8.799998, 7.999998, 3.099998, -6.8000018, ?). I?m aware of
> precision
> >>>>>>>> and least_significant_digit attributes of ncdf4 object [1]. For
> uwnd
> >>>>>>>> data, precision = 2 and least_significant_digits = 1. My doubt is
> that
> >>>>>>>> should I round data to 2 decimal places or 1 decimal place after
> >>>>>>>> decimal point?
> >>>>>>>>
> >>>>>>>> Same issue is valid for some header info.
> >>>>>>>>
> >>>>>>>> Output of ncdf4 object:
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> Output of ncdump on terminal:
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> for instance, ncdump's scale factor is 0.01f but ncdf4 object?s
> >>>>>>>> scale_factor is 0.00999999977648258. You can notice same issue for
> >>>>>>>> actual_range and add_offset. Also a similar issue exist for the
> data.
> >>>>>>>> How can I truncate those extra unsignificant decimal places or
> round
> >>>>>>>> the numbers to significant decimal places?
> >>>>>>>>
> >>>>>>>> 1 -
> >>>>>>>>
> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
> >>>>>>>> <
> http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml
> >
> >>>>>>>> ______________________________________________
> >>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide
> >>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>
> >>>>>
> >>>>>   [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>> **********************
> >>>> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> >>>> **********************
> >>>> Roy Mendelssohn
> >>>> Supervisory Operations Research Analyst
> >>>> NOAA/NMFS
> >>>> Environmental Research Division
> >>>> Southwest Fisheries Science Center
> >>>> ***Note new address and phone***
> >>>> 110 Shaffer Road
> >>>> Santa Cruz, CA 95060
> >>>> Phone: (831)-420-3666
> >>>> Fax: (831) 420-3980
> >>>> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> >>>>
> >>>> "Old age and treachery will overcome youth and skill."
> >>>> "From those who have been given much, much will be expected"
> >>>> "the arc of the moral universe is long, but it bends toward justice"
> -MLK Jr.
> >>>>
> >>>
> >>
> >> **********************
> >> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> >> **********************
> >> Roy Mendelssohn
> >> Supervisory Operations Research Analyst
> >> NOAA/NMFS
> >> Environmental Research Division
> >> Southwest Fisheries Science Center
> >> ***Note new address and phone***
> >> 110 Shaffer Road
> >> Santa Cruz, CA 95060
> >> Phone: (831)-420-3666
> >> Fax: (831) 420-3980
> >> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> >>
> >> "Old age and treachery will overcome youth and skill."
> >> "From those who have been given much, much will be expected"
> >> "the arc of the moral universe is long, but it bends toward justice"
> -MLK Jr.
> >>
> >
>
> **********************
> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new address and phone***
> 110 Shaffer Road
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK
> Jr.
>
>

	[[alternative HTML version deleted]]


From tr206 at kent.ac.uk  Fri Jul  8 13:16:26 2016
From: tr206 at kent.ac.uk (T.Riedle)
Date: Fri, 8 Jul 2016 11:16:26 +0000
Subject: [R] source code of a function
Message-ID: <1467976586344.14382@kent.ac.uk>

Dear all,


I am currently working with the "jvnVaR" package and I need to explain the assumptions behind the function jVaR().


Is there a function in R which calls the code behind a function?


Kindest regards


	[[alternative HTML version deleted]]


From S.Ellison at LGCGroup.com  Fri Jul  8 13:23:49 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Fri, 8 Jul 2016 12:23:49 +0100
Subject: [R] source code of a function
In-Reply-To: <1467976586344.14382@kent.ac.uk>
References: <1467976586344.14382@kent.ac.uk>
Message-ID: <1A8C1289955EF649A09086A153E2672403EB035A27@GBTEDVPEXCMB04.corp.lgc-group.com>




> Is there a function in R which calls the code behind a function?
Type the function name without the brackets.




*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From marc_schwartz at me.com  Fri Jul  8 13:34:36 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 08 Jul 2016 06:34:36 -0500
Subject: [R] source code of a function
In-Reply-To: <1467976586344.14382@kent.ac.uk>
References: <1467976586344.14382@kent.ac.uk>
Message-ID: <15737B20-5698-47D9-B45E-FA4457FC4548@me.com>


> On Jul 8, 2016, at 6:16 AM, T.Riedle <tr206 at kent.ac.uk> wrote:
> 
> Dear all,
> 
> 
> I am currently working with the "jvnVaR" package and I need to explain the assumptions behind the function jVaR().
> 
> 
> Is there a function in R which calls the code behind a function?
> 
> 
> Kindest regards


Hi,

If you type the name of the function at the R prompt without the parens:

  > jVaR

That will give you a representation of the source code for the function, save any compiled code (e.g. C, FORTRAN, C++) that may be part of the function call.

However, the true source code is available via the package source tarball (.tar.gz file) on CRAN:

  https://cran.r-project.org/web/packages/jvnVaR/index.html

The tarball will include any comments/annotations in the code and the source for any compiled code that may be a part of the package.

Regards,

Marc Schwartz


From cgw at witthoft.com  Fri Jul  8 14:29:56 2016
From: cgw at witthoft.com (cgw at witthoft.com)
Date: Fri, 08 Jul 2016 08:29:56 -0400
Subject: [R] Problem with loading a package containing operator
In-Reply-To: <mailman.15.1467972003.31872.r-sig-mac@r-project.org>
References: <mailman.15.1467972003.31872.r-sig-mac@r-project.org>
Message-ID: <91546b400b8017a2d6477a3410058cc1@witthoft.com>

I have a published package cgwtools which includes an overload of the !
operator. Here's the code from the source file splatnd.r : 

`!` <- function (e1, e2) 
{
call <- match.call()
original <- function() {
call[[1]] <- quote(base::`!`)
return(eval(call, parent.frame(2)))
} 

switch(paste(as.character(call[[2]]), sep = "", collapse = ""), 
newdev = dev.new(width = 4.5, height = 4.5, restoreConsole = T), 
qapla = cat("batlh tIn chav\n"), return(original()))
}
This is based on the neat tool in the sos package called ??? . If I
simply source the ! code, it runs as expected. In the past, at least
prior to R 3.2.x , loading the package cgwtools let me run ! as
expected. I recently set up both versions 3.2.4 and 3.3.1 on a Windows 7
machine, and got an odd apparent failure to load. Here's some console
text, showing the searchpath before and after loading cgwtools 

> getAnywhere(`!`)
A single object matching '!' was found
It was found in the following places
package:base
namespace:base
with value 

function (x) .Primitive("!")
> library(cgwtools)
> getAnywhere(`!`)
2 differing objects matching '!' were found
in the following places
package:base
namespace:cgwtools
namespace:base
By way of comparison, doing the same with the sos package returns what
I'd expect: 

> getAnywhere(`?`)
2 differing objects matching '?' were found
in the following places
package:sos
package:utils
namespace:utils
namespace:sos
If I access the output, the correct source code is shown: 

> getAnywhere(`!`)[2]
function (e1, e2) 
{
call <- match.call()
original <- function() {
call[[1]] <- quote(base::`!`)
return(eval(call, parent.frame(2)))
}
switch(paste(as.character(call[[2]]), sep = "", collapse = ""), 
newdev = dev.new(width = 4.5, height = 4.5, restoreConsole = T), 
qapla = cat("batlh tIn chav\n"), return(original()))
}
<environment: namespace:cgwtools>
But , as indicated by the lack of the text package:cgwtools , when I try
to run, say !qapla from the gui console, I get the error message that
qapla is not found, i.e. the .Primitive operator ! was invoked. 

So the question is: why is the ! function in the cgwtools package not
being recognized after loading the package? Did I do something dumb when
building the package source, or did I find some bug in the CRAN builder?


Sub-question: I can't figure out any variant of the directed command
cgwtools::'!'qapla that doesn't cause a parse error.
	[[alternative HTML version deleted]]


From lhartman at yorku.ca  Fri Jul  8 02:43:58 2016
From: lhartman at yorku.ca (lhartman at yorku.ca)
Date: Fri, 8 Jul 2016 00:43:58 +0000
Subject: [R] WRS2 package problems with post hoc test lincon
Message-ID: <70b8fe0.8a4f43e19563291d2a50de5ece2de572@mymail.yorku.ca>



	Hello,

	I am analyzing some data that has violated assumptions of ANOVA and am
using the WRS2 package in R. I am comparing three groups, Dx,
(schizophrenia, schizoaffective and control) on various variables, one of
which is premorbid IQ (preIQ).

	preIQdataSDx  #trimmed means > tapply(preIQdataSPreIQ, preIQdataSDx, mean,
tr=.2, na.rm=TRUE)  Schizophrenia Schizoaffective Control 89.29412 95.83333
95.54545
	>
	The post hoc test shows that the schizophrenia and controls group differ
significantly which makes sense when you look at their trimmed means;
however, what does not make sense is that the control and schizoaffective
disorder groups differ significantly from each other (their means are
almost identical). Additionally, if the schizophrenia and control group
differ significantly, shouldn't the schizophrenia group also differ
significantly from the control group too, as the control group and
schizoaffective disorder groups have very similar means. I am wondering if
the lincon post hoc test is flawed in some way or if I am not setting
something up correctly in R. Any help is greatly appreciated. Thank you.

	Best,

	Leah Hartman

	[[alternative HTML version deleted]]


From erinm.hodgess at gmail.com  Fri Jul  8 20:53:46 2016
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Fri, 8 Jul 2016 13:53:46 -0500
Subject: [R] a package-building opinion question, please
Message-ID: <CACxE24m7KRVhruv-59n_KUu+AMxvZhm5aQ6AOGjw0vYgEAMekQ@mail.gmail.com>

Hello everyone:

I'm starting to write a package from scratch; having done that in ages.

My opinion question:  what is the best way, please:  Should I use R studio,
please?  Back in the back, I used package.skeleton, or just copied over
other people's stuff.  But I want to do a nice clean one from the beginning.

For the record, I will have S4 classes.

And I'm sure I will get many differing answering, but that's good too!

Sort of like those "if you put 1000 statisticians end to end" jokes.

Thanks in advance,
Have a great weekend,
Sincerely,
Erin


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From roy.mendelssohn at noaa.gov  Fri Jul  8 21:05:19 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Fri, 8 Jul 2016 12:05:19 -0700
Subject: [R] a package-building opinion question, please
In-Reply-To: <CACxE24m7KRVhruv-59n_KUu+AMxvZhm5aQ6AOGjw0vYgEAMekQ@mail.gmail.com>
References: <CACxE24m7KRVhruv-59n_KUu+AMxvZhm5aQ6AOGjw0vYgEAMekQ@mail.gmail.com>
Message-ID: <969D661C-1AB5-483F-B277-C87F29C22904@noaa.gov>

Hi Erin:

Everyone's tastes differ,  but when I started out knowing nothing about packages,  I found Hadley's guide  (thank you Hadley!!!) indispensable:

     http://r-pkgs.had.co.nz/intro.html


For obvious reasons, his guide really is geared to built-in features of RStudio, though I believe you could do the same things without it.  It is just easier to do the steps he describes from within RStudio.

HTH,

-Roy


> On Jul 8, 2016, at 11:53 AM, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> 
> Hello everyone:
> 
> I'm starting to write a package from scratch; having done that in ages.
> 
> My opinion question:  what is the best way, please:  Should I use R studio,
> please?  Back in the back, I used package.skeleton, or just copied over
> other people's stuff.  But I want to do a nice clean one from the beginning.
> 
> For the record, I will have S4 classes.
> 
> And I'm sure I will get many differing answering, but that's good too!
> 
> Sort of like those "if you put 1000 statisticians end to end" jokes.
> 
> Thanks in advance,
> Have a great weekend,
> Sincerely,
> Erin
> 
> 
> -- 
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From paulino.levara at gmail.com  Fri Jul  8 20:10:26 2016
From: paulino.levara at gmail.com (Paulino Levara)
Date: Fri, 8 Jul 2016 20:10:26 +0200
Subject: [R] Help with constrained portfolio optimization
Message-ID: <CADjzwh8k=Rho72J_=81-5s9OYLzj4=vjLTsJwG2eXy3DPdkLQQ@mail.gmail.com>

Dear R community,

I am a beginner in portfolio optimization and I would appreciate your help
with the next problem:given a set of 10 variables (X), I would like to
obtain the efficient portfolio that minimize the variance taking the
expected return as mean(X), subject to the next constraints:

a) Limit the sum of the weights of the first five variables to 30%
b) Limit the sum of the weights of the last five variables to 70%

What is your suggestion? Can I do this with the portfolio.optim function of
the tseries package?

How Can I do that?

Thanks in advance.

Regards.

	[[alternative HTML version deleted]]


From sezenismail at gmail.com  Fri Jul  8 22:19:00 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 8 Jul 2016 23:19:00 +0300
Subject: [R] a package-building opinion question, please
In-Reply-To: <969D661C-1AB5-483F-B277-C87F29C22904@noaa.gov>
References: <CACxE24m7KRVhruv-59n_KUu+AMxvZhm5aQ6AOGjw0vYgEAMekQ@mail.gmail.com>
	<969D661C-1AB5-483F-B277-C87F29C22904@noaa.gov>
Message-ID: <C589181C-DBD8-4903-8533-03A6FA70F079@gmail.com>

I would suggest Hadley's guide as Roy. Life is easier by shortcuts that RStudio has. Especially in package development stage. You can run your test by <cmd + Shit + T> or build your package by <cmd + shift + B> on the go. When it comes to package development, these are life saving.

> On 08 Jul 2016, at 22:05, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> 
> Hi Erin:
> 
> Everyone's tastes differ,  but when I started out knowing nothing about packages,  I found Hadley's guide  (thank you Hadley!!!) indispensable:
> 
>     http://r-pkgs.had.co.nz/intro.html
> 
> 
> For obvious reasons, his guide really is geared to built-in features of RStudio, though I believe you could do the same things without it.  It is just easier to do the steps he describes from within RStudio.
> 
> HTH,
> 
> -Roy
> 


From jdnewmil at dcn.davis.ca.us  Sat Jul  9 03:46:50 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 08 Jul 2016 18:46:50 -0700
Subject: [R] WRS2 package problems with post hoc test lincon
In-Reply-To: <70b8fe0.8a4f43e19563291d2a50de5ece2de572@mymail.yorku.ca>
References: <70b8fe0.8a4f43e19563291d2a50de5ece2de572@mymail.yorku.ca>
Message-ID: <D0FBB160-81A6-472C-B73C-C10EB26E4422@dcn.davis.ca.us>

1) HTML formatted email does not come through reliably. Please read the Posting Guide. 

2) It is very nearly always necessary to provide a reproducible example when asking for help on this list to avoid complete failure to communicate. 

3) Given the above limitations (meaning I may not be understanding you correctly), this looks like a problem with your understanding of statistics, not a problem with R. If so, you should be asking this question in a forum focused less on the tool and more on the theory, such as stats.stackexchange.com.
-- 
Sent from my phone. Please excuse my brevity.

On July 7, 2016 5:43:58 PM PDT, lhartman at yorku.ca wrote:
>
>
>	Hello,
>
>	I am analyzing some data that has violated assumptions of ANOVA and am
>using the WRS2 package in R. I am comparing three groups, Dx,
>(schizophrenia, schizoaffective and control) on various variables, one
>of
>which is premorbid IQ (preIQ).
>
>	preIQdataSDx  #trimmed means > tapply(preIQdataSPreIQ, preIQdataSDx,
>mean,
>tr=.2, na.rm=TRUE)  Schizophrenia Schizoaffective Control 89.29412
>95.83333
>95.54545
>	>
>	The post hoc test shows that the schizophrenia and controls group
>differ
>significantly which makes sense when you look at their trimmed means;
>however, what does not make sense is that the control and
>schizoaffective
>disorder groups differ significantly from each other (their means are
>almost identical). Additionally, if the schizophrenia and control group
>differ significantly, shouldn't the schizophrenia group also differ
>significantly from the control group too, as the control group and
>schizoaffective disorder groups have very similar means. I am wondering
>if
>the lincon post hoc test is flawed in some way or if I am not setting
>something up correctly in R. Any help is greatly appreciated. Thank
>you.
>
>	Best,
>
>	Leah Hartman
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Jul  9 06:51:42 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 8 Jul 2016 21:51:42 -0700
Subject: [R] WRS2 package problems with post hoc test lincon
In-Reply-To: <70b8fe0.8a4f43e19563291d2a50de5ece2de572@mymail.yorku.ca>
References: <70b8fe0.8a4f43e19563291d2a50de5ece2de572@mymail.yorku.ca>
Message-ID: <FD1DF9FF-E276-48B3-A1E1-58A092577975@comcast.net>


> On Jul 7, 2016, at 5:43 PM, lhartman at yorku.ca wrote:
> 
> 
> 
> 	Hello,
> 
> 	I am analyzing some data that has violated assumptions of ANOVA

Violated? Violated how exactly?


> and am
> using the WRS2 package in R.


Where's the code?


> I am comparing three groups, Dx,
> (schizophrenia, schizoaffective and control) on various variables, one of
> which is premorbid IQ (preIQ).
> 
> 	preIQdataSDx  #trimmed means

> > tapply(preIQdataSPreIQ, preIQdataSDx, mean,
> tr=.2, na.rm=TRUE)  

CR's and minor editing added.

> Schizophrenia Schizoaffective Control

> 89.29412      95.83333        95.54545

> 	>
> 	The post hoc test

Test? Test of what (and how)?


> shows that the schizophrenia and controls group differ
> significantly which makes sense when you look at their trimmed means;
> however, what does not make sense is that the control and schizoaffective
> disorder groups differ significantly from each other (their means are
> almost identical). Additionally, if the schizophrenia and control group
> differ significantly, shouldn't the schizophrenia group also differ
> significantly from the control group too,

Did you really intend that sentence to say that?


> as the control group and
> schizoaffective disorder groups have very similar means. I am wondering if
> the lincon post hoc test is flawed in some way or if I am not setting
> something up correctly in R. Any help is greatly appreciated.

That's the test ( lincon? ) you didn't illustrate with code?


> Thank you.
> 
> 	Best,
> 
> 	Leah Hartman
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From shivipmp82 at gmail.com  Sat Jul  9 12:29:16 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Sat, 9 Jul 2016 15:59:16 +0530
Subject: [R] 'ref' must be an existing level
Message-ID: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>

Dear Team,

I am running a multinomial logistic regression model for one of the
fictitious data before i implement the same on my real data.
Here i am trying to predict based some scores and economic group whether a
person will go for a diploma, general or honors.

The code below:
m11$prog2<- relevel(m11$prog, ref = "honors"

already loaded the nnet library. However i got the below error:
Error in relevel.factor(m11$prog, ref = "honors") :
  'ref' must be an existing level

I have tried searching on SO and nabble but did not find an answer that
could help.

Please suggest what is incorrect. Also checked the class of the var and is
a factor variable.

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Sat Jul  9 12:37:07 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 9 Jul 2016 12:37:07 +0200
Subject: [R] 'ref' must be an existing level
In-Reply-To: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>
References: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>
Message-ID: <C3B9068A-948C-4A90-ABB2-3DAB1D4A29AD@gmail.com>


What does table(m11$prog) tell you?
-pd

> On 09 Jul 2016, at 12:29 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> 
> Dear Team,
> 
> I am running a multinomial logistic regression model for one of the
> fictitious data before i implement the same on my real data.
> Here i am trying to predict based some scores and economic group whether a
> person will go for a diploma, general or honors.
> 
> The code below:
> m11$prog2<- relevel(m11$prog, ref = "honors"
> 
> already loaded the nnet library. However i got the below error:
> Error in relevel.factor(m11$prog, ref = "honors") :
>  'ref' must be an existing level
> 
> I have tried searching on SO and nabble but did not find an answer that
> could help.
> 
> Please suggest what is incorrect. Also checked the class of the var and is
> a factor variable.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From shivipmp82 at gmail.com  Sat Jul  9 12:50:03 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Sat, 9 Jul 2016 16:20:03 +0530
Subject: [R] 'ref' must be an existing level
In-Reply-To: <C3B9068A-948C-4A90-ABB2-3DAB1D4A29AD@gmail.com>
References: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>
	<C3B9068A-948C-4A90-ABB2-3DAB1D4A29AD@gmail.com>
Message-ID: <CAB=p7SprgtTj6Cy9ARWaeDFEQVH71mSqNrXLG3GXh21rRqM06A@mail.gmail.com>

Hi Peter,
It gives me breakdown of all categories with their respective freq.
Diploma     general  honors
         50          45         105

On Sat, Jul 9, 2016 at 4:07 PM, peter dalgaard <pdalgd at gmail.com> wrote:

>
> What does table(m11$prog) tell you?
> -pd
>
> > On 09 Jul 2016, at 12:29 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> >
> > Dear Team,
> >
> > I am running a multinomial logistic regression model for one of the
> > fictitious data before i implement the same on my real data.
> > Here i am trying to predict based some scores and economic group whether
> a
> > person will go for a diploma, general or honors.
> >
> > The code below:
> > m11$prog2<- relevel(m11$prog, ref = "honors"
> >
> > already loaded the nnet library. However i got the below error:
> > Error in relevel.factor(m11$prog, ref = "honors") :
> >  'ref' must be an existing level
> >
> > I have tried searching on SO and nabble but did not find an answer that
> > could help.
> >
> > Please suggest what is incorrect. Also checked the class of the var and
> is
> > a factor variable.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Sat Jul  9 13:32:37 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 9 Jul 2016 13:32:37 +0200
Subject: [R] 'ref' must be an existing level
In-Reply-To: <CAB=p7SprgtTj6Cy9ARWaeDFEQVH71mSqNrXLG3GXh21rRqM06A@mail.gmail.com>
References: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>
	<C3B9068A-948C-4A90-ABB2-3DAB1D4A29AD@gmail.com>
	<CAB=p7SprgtTj6Cy9ARWaeDFEQVH71mSqNrXLG3GXh21rRqM06A@mail.gmail.com>
Message-ID: <F6E6F0BC-F55E-467B-BDBA-EFF99AEF7865@gmail.com>

Hmm, and levels(m11$prog)? 

(There's a chance that a space has sneaked in, but your HTML mail makes it hard to see column alignment)

-pd

> On 09 Jul 2016, at 12:50 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> 
> Hi Peter, 
> It gives me breakdown of all categories with their respective freq. 
> Diploma     general  honors      
>          50          45         105 
> 
> On Sat, Jul 9, 2016 at 4:07 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> What does table(m11$prog) tell you?
> -pd
> 
> > On 09 Jul 2016, at 12:29 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> >
> > Dear Team,
> >
> > I am running a multinomial logistic regression model for one of the
> > fictitious data before i implement the same on my real data.
> > Here i am trying to predict based some scores and economic group whether a
> > person will go for a diploma, general or honors.
> >
> > The code below:
> > m11$prog2<- relevel(m11$prog, ref = "honors"
> >
> > already loaded the nnet library. However i got the below error:
> > Error in relevel.factor(m11$prog, ref = "honors") :
> >  'ref' must be an existing level
> >
> > I have tried searching on SO and nabble but did not find an answer that
> > could help.
> >
> > Please suggest what is incorrect. Also checked the class of the var and is
> > a factor variable.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From lists at dewey.myzen.co.uk  Sat Jul  9 13:36:25 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 9 Jul 2016 12:36:25 +0100
Subject: [R] 'ref' must be an existing level
In-Reply-To: <CAB=p7SprgtTj6Cy9ARWaeDFEQVH71mSqNrXLG3GXh21rRqM06A@mail.gmail.com>
References: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>
	<C3B9068A-948C-4A90-ABB2-3DAB1D4A29AD@gmail.com>
	<CAB=p7SprgtTj6Cy9ARWaeDFEQVH71mSqNrXLG3GXh21rRqM06A@mail.gmail.com>
Message-ID: <a820ae4b-894d-499d-e7de-291cee5eaf03@dewey.myzen.co.uk>

Dear Shivi

Are you sure that the level is "honors" and not " honors" or "honors " 
or something similar?

On 09/07/2016 11:50, Shivi Bhatia wrote:
> Hi Peter,
> It gives me breakdown of all categories with their respective freq.
> Diploma     general  honors
>          50          45         105
>
> On Sat, Jul 9, 2016 at 4:07 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>
>>
>> What does table(m11$prog) tell you?
>> -pd
>>
>>> On 09 Jul 2016, at 12:29 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
>>>
>>> Dear Team,
>>>
>>> I am running a multinomial logistic regression model for one of the
>>> fictitious data before i implement the same on my real data.
>>> Here i am trying to predict based some scores and economic group whether
>> a
>>> person will go for a diploma, general or honors.
>>>
>>> The code below:
>>> m11$prog2<- relevel(m11$prog, ref = "honors"
>>>
>>> already loaded the nnet library. However i got the below error:
>>> Error in relevel.factor(m11$prog, ref = "honors") :
>>>  'ref' must be an existing level
>>>
>>> I have tried searching on SO and nabble but did not find an answer that
>>> could help.
>>>
>>> Please suggest what is incorrect. Also checked the class of the var and
>> is
>>> a factor variable.
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From cristina.cametti at gmail.com  Fri Jul  8 22:57:11 2016
From: cristina.cametti at gmail.com (Cristina Cametti)
Date: Fri, 8 Jul 2016 22:57:11 +0200
Subject: [R] r code for multilevel latent class analysis
In-Reply-To: <704059C4-C1A8-4342-8783-136280B6C647@comcast.net>
References: <A8C817C7-49DA-4C4E-9D3A-CB70504B26EC@yahoo.it>
	<CA+8X3fU1unj78Zpg-=hMP3xed5iSaWPgTX1DPEY-KQ5QnfquSQ@mail.gmail.com>
	<704059C4-C1A8-4342-8783-136280B6C647@comcast.net>
Message-ID: <4D8276CC-B7B6-40D5-9F7B-970ED667EDB1@gmail.com>

Dear all,

thank you very much for your suggestions! About the fact that I put my variables plus 1, it is because if I don?t do it, I get this message:

ALERT: some manifest variables contain values that are not
    positive integers. For poLCA to run, please recode categorical
    outcome variables to increment from 1 to the maximum number of
    outcome categories for each variable. 

I read about the fact that poLCA need integers to run (in the various explanations on the web). So, in the end I used this code:


lca = poLCA(cbind(ppltrst=ppltrst+1,pplfair=pplfair+1,pplhlp=pplhlp+1) ~ cntry, 
             maxiter=50000, nclass=3, 
             nrep=10, data=mydata)

It is working, I mean this is the output that I obtained:
Model 1: llik = -244070.8 ... best llik = -244070.8
Model 2: llik = -241832.9 ... best llik = -241832.9
Model 3: llik = -245111.9 ... best llik = -241832.9
Model 4: llik = -242490.5 ... best llik = -241832.9
Model 5: llik = -240447.7 ... best llik = -240447.7
Model 6: llik = -250882.1 ... best llik = -240447.7
Model 7: llik = -240447.7 ... best llik = -240447.7
Model 8: llik = -242547.3 ... best llik = -240447.7
Model 9: llik = -240447.7 ... best llik = -240447.7
Model 10: llik = -247340.3 ... best llik = -240447.7
Conditional item response (column) probabilities,
 by outcome variable, for each class (row) 
 
$ppltrst
           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)  Pr(7)  Pr(8)  Pr(9) Pr(10) Pr(11)
class 1:  0.0071 0.0047 0.0057 0.0112 0.0163 0.1146 0.0892 0.2794 0.3215 0.0897 0.0605
class 2:  0.2375 0.1703 0.2117 0.1681 0.0564 0.1000 0.0098 0.0097 0.0150 0.0078 0.0137
class 3:  0.0189 0.0104 0.0494 0.1442 0.1687 0.3268 0.1499 0.1094 0.0216 0.0008 0.0000

$pplfair
           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)  Pr(7)  Pr(8)  Pr(9) Pr(10) Pr(11)
class 1:  0.0025 0.0021 0.0054 0.0062 0.0062 0.0512 0.0670 0.2721 0.3697 0.1394 0.0782
class 2:  0.1712 0.1285 0.2048 0.1654 0.0667 0.1586 0.0151 0.0133 0.0261 0.0143 0.0361
class 3:  0.0003 0.0011 0.0186 0.0952 0.1428 0.3413 0.1756 0.1584 0.0577 0.0068 0.0023

$pplhlp
           Pr(1)  Pr(2)  Pr(3)  Pr(4)  Pr(5)  Pr(6)  Pr(7)  Pr(8)  Pr(9) Pr(10) Pr(11)
class 1:  0.0046 0.0051 0.0139 0.0369 0.0495 0.1804 0.1434 0.2374 0.2127 0.0720 0.0442
class 2:  0.2218 0.1893 0.2334 0.1412 0.0471 0.1044 0.0081 0.0098 0.0139 0.0107 0.0205
class 3:  0.0074 0.0159 0.0755 0.1779 0.1731 0.2870 0.1226 0.0984 0.0380 0.0024 0.0018

Estimated class population shares 
 0.3351 0.2014 0.4635 
 
Predicted class memberships (by modal posterior prob.) 
 0.3323 0.187 0.4807 
 
========================================================= 
Fit for 3 latent classes: 
========================================================= 
2 / 1 
            Coefficient  Std. error  t value  Pr(>|t|)
(Intercept)    -0.67015     0.07488   -8.950     0.000
cntryBE         0.39006     0.11038    3.534     0.000
cntryCH        -1.39925     0.14273   -9.804     0.000
cntryCZ         1.26804     0.12295   10.314     0.000
cntryDE         0.14062     0.10330    1.361     0.174
cntryDK        -2.90300     0.22895  -12.680     0.000
cntryES         0.67484     0.11334    5.954     0.000
cntryFI        -2.47880     0.18096  -13.698     0.000
cntryFR         0.73428     0.12339    5.951     0.000
cntryGB        -0.28234     0.11663   -2.421     0.016
cntryGR         2.69529     0.11605   23.225     0.000
cntryHU         1.71068     0.12213   14.006     0.000
cntryIE        -0.65851     0.10951   -6.013     0.000
cntryIT         1.49171     0.13915   10.720     0.000
cntryLU         0.35923     0.11635    3.088     0.002
cntryNL        -1.17781     0.12193   -9.659     0.000
cntryNO        -2.83265     0.19959  -14.192     0.000
cntryPL         2.76831     0.14608   18.950     0.000
cntryPT         1.65176     0.13278   12.440     0.000
cntrySE        -1.93606     0.15250  -12.696     0.000
cntrySI         1.52657     0.11667   13.084     0.000
========================================================= 
3 / 1 
            Coefficient  Std. error  t value  Pr(>|t|)
(Intercept)     0.40117     0.05997    6.689     0.000
cntryBE         0.37591     0.09215    4.079     0.000
cntryCH        -0.27800     0.08100   -3.432     0.001
cntryCZ         0.75694     0.11287    6.707     0.000
cntryDE         0.47418     0.08072    5.874     0.000
cntryDK        -1.92545     0.10423  -18.473     0.000
cntryES         0.55324     0.09713    5.696     0.000
cntryFI        -1.27608     0.08553  -14.920     0.000
cntryFR         0.74221     0.10457    7.098     0.000
cntryGB         0.27381     0.08555    3.201     0.001
cntryGR         1.06585     0.11440    9.317     0.000
cntryHU         1.05480     0.11392    9.259     0.000
cntryIE        -0.61390     0.08243   -7.448     0.000
cntryIT         1.12519     0.12983    8.667     0.000
cntryLU         0.23379     0.09523    2.455     0.014
cntryNL        -0.35230     0.07981   -4.414     0.000
cntryNO        -1.54373     0.08836  -17.471     0.000
cntryPL         1.68826     0.14280   11.823     0.000
cntryPT         1.26210     0.12225   10.324     0.000
cntrySE        -1.09443     0.08383  -13.055     0.000
cntrySI         0.68829     0.11137    6.180     0.000
========================================================= 
number of observations: 39254 
number of estimated parameters: 132 
residual degrees of freedom: 1198 
maximum log-likelihood: -240447.7 
 
AIC(3): 481159.3
BIC(3): 482291.6
X^2(3): 40937.31 (Chi-square goodness of fit) 
 
ALERT: estimation algorithm automatically restarted with new initial values 
 
Warning message:
In sqrt(diag(VCE.beta)) : Si ? prodotto un NaN


Except for the last part, I think that this output makes sense. 
However, if you have additional suggestions, I will be very glad to hear them.
Thank you very much again!

Cristina 


Il giorno 08/lug/2016, alle ore 01:49, David Winsemius <dwinsemius at comcast.net> ha scritto:

>> 
>> On Jul 7, 2016, at 3:36 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
>> 
>> Hi Cristina,
>> Try this:
>> 
>> names(mydata)
>> 
>> It may be NULL or "ppitrst" may be absent.
> 
> I've already suggested to Christina that she make sure the variables are spelled correctly and she reports they are all present in her dataset. So I tried a formula such as she posed with '1' added to each variable and this does throw the same error with the 'values'-dataframe that is used in the examples for that package.
> 
>> data(values,package='poLCA')
>> str(values)
> 'data.frame':	216 obs. of  4 variables:
> $ A: num  2 2 2 2 2 2 2 2 2 2 ...
> $ B: num  2 2 2 2 2 2 2 2 2 2 ...
> $ C: num  2 2 2 2 2 2 2 2 2 2 ...
> $ D: num  2 2 2 2 2 2 2 2 2 2 ...
>> library(poLCA)
> Loading required package: scatterplot3d
> Loading required package: MASS
>> poLCA( cbind(A+1,B+1) ~ C, data=values)
> Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) : 
>  undefined columns selected
> 
> So I then tried removeing those "+1`"'s (which didn't seem to have much justification):
> 
>> poLCA( cbind(A,B) ~ C, data=values)
> Conditional item response (column) probabilities,
> by outcome variable, for each class (row) 
> 
> $A
>           Pr(1)  Pr(2)
> class 1:  0.3428 0.6572
> class 2:  0.0307 0.9693
> 
> $B
>           Pr(1)  Pr(2)
> class 1:  0.7737 0.2263
> class 2:  0.1386 0.8614
> 
> snipped the rest of the output.
> 
> So "why add 1?" Seems to disturb the functions formula processing logic and is so far not explained.
> 
> -- 
> David.
> 
>> 
>> Jim
>> 
>> 
>> On Thu, Jul 7, 2016 at 8:26 PM, Cristina Cametti
>> <cristina.cametti at gmail.com> wrote:
>>> Dear all,
>>> 
>>> I am not able to find a reliable r code to run a multilevel latent class model. Indeed, I have to analyze how social trust (three variables form the ESS survey) might vary between countries (21 countries in my database). I tried to use the poLCA package but I am not sure if my code is right. This is my code:
>>> lca <- cbind(ppltrst+1,pplfair+1,pplhlp+1)~cntry
>>> lc <- poLCA(lca,mydata)
>>> 
>>> However, I get an error message:
>>> Error in `[.data.frame`(data, , match(colnames(y), colnames(data))[j]) :
>>> undefined columns selected
>>> 
>>> How can I solve this? Is the code completely wrong or I missed some passages?
>>> Thank you very much for your help!
>>> 
>>> Cristina
>>>       [[alternative HTML version deleted]]
>>> 
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From stn021 at gmail.com  Sat Jul  9 10:52:51 2016
From: stn021 at gmail.com (stn021)
Date: Sat, 9 Jul 2016 10:52:51 +0200
Subject: [R] Regression with factors ?
Message-ID: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>

Hello,

I would like to analyse a model like this:

y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )

x1 and x2 are not continuous variables but factors, so the observation
contain the level.
Its numerical value is unknown and is to be estimated with the model.


The observations look like this:

y        x1     x2
0.96  Alice  Bob
0.84  Alice  Charlie
0.96  Bob   Charlie
0.64  Dave Alice
etc.

Each person has a numerical value. Here for example Alice = 0.2 and Bob =
0.4

Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.

How can this be done in R ?

THX , stn

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Jul  9 19:08:27 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 9 Jul 2016 10:08:27 -0700
Subject: [R] 'ref' must be an existing level
In-Reply-To: <F6E6F0BC-F55E-467B-BDBA-EFF99AEF7865@gmail.com>
References: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>
	<C3B9068A-948C-4A90-ABB2-3DAB1D4A29AD@gmail.com>
	<CAB=p7SprgtTj6Cy9ARWaeDFEQVH71mSqNrXLG3GXh21rRqM06A@mail.gmail.com>
	<F6E6F0BC-F55E-467B-BDBA-EFF99AEF7865@gmail.com>
Message-ID: <2E320F89-5F94-40AD-A6FF-98AA6EA63ED0@comcast.net>


> On Jul 9, 2016, at 4:32 AM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> Hmm, and levels(m11$prog)? 

Shivi;

See what this returns:

"honors" %in% levels(m11$prog)

The level is probably something like " honors"

It's fairly easy to inadvertently create leading or trailing spaces when reading from comma-separated files.

> test <- read.csv( text = " honors, others\n honors, others\n", header=FALSE)
> test
       V1      V2
1  honors  others
2  honors  others
> nchar(test$V1)
Error in nchar(test$V1) : 'nchar()' requires a character vector
> nchar(as.character(test$V1))
[1] 7 7
> "honors" %in% levels(test$V1)
[1] FALSE

-- 
David.

> 
> (There's a chance that a space has sneaked in, but your HTML mail makes it hard to see column alignment)
> 
> -pd
> 
>> On 09 Jul 2016, at 12:50 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
>> 
>> Hi Peter, 
>> It gives me breakdown of all categories with their respective freq. 
>> Diploma     general  honors      
>>         50          45         105 
>> 
>> On Sat, Jul 9, 2016 at 4:07 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>> 
>> What does table(m11$prog) tell you?
>> -pd
>> 
>>> On 09 Jul 2016, at 12:29 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
>>> 
>>> Dear Team,
>>> 
>>> I am running a multinomial logistic regression model for one of the
>>> fictitious data before i implement the same on my real data.
>>> Here i am trying to predict based some scores and economic group whether a
>>> person will go for a diploma, general or honors.
>>> 
>>> The code below:
>>> m11$prog2<- relevel(m11$prog, ref = "honors"
>>> 
>>> already loaded the nnet library. However i got the below error:
>>> Error in relevel.factor(m11$prog, ref = "honors") :
>>> 'ref' must be an existing level
>>> 
>>> I have tried searching on SO and nabble but did not find an answer that
>>> could help.
>>> 
>>> Please suggest what is incorrect. Also checked the class of the var and is
>>> a factor variable.
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From lists at dewey.myzen.co.uk  Sat Jul  9 16:42:57 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 9 Jul 2016 15:42:57 +0100
Subject: [R] 'ref' must be an existing level
In-Reply-To: <CAB=p7SrysPMeSdze_+Dza1cYBZgyb3OdYP=fMYR3CPykm55mvQ@mail.gmail.com>
References: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>
	<C3B9068A-948C-4A90-ABB2-3DAB1D4A29AD@gmail.com>
	<CAB=p7SprgtTj6Cy9ARWaeDFEQVH71mSqNrXLG3GXh21rRqM06A@mail.gmail.com>
	<a820ae4b-894d-499d-e7de-291cee5eaf03@dewey.myzen.co.uk>
	<CAB=p7SrysPMeSdze_+Dza1cYBZgyb3OdYP=fMYR3CPykm55mvQ@mail.gmail.com>
Message-ID: <0becb60c-05bc-58db-7351-2df81daf6018@dewey.myzen.co.uk>

Dear Shivi

Just printing it will not tell you. I did not mean 'Have you spelled it 
correctly?' I meant 'Is there a stray space somewhere?'. Peter has the 
same suspicion.

On 09/07/2016 12:49, Shivi Bhatia wrote:
> Hi Michael,
>
> I did check again and used the print command but it is spelled as honors.
>
> Hi Peter,
> there are only 3 levels : Diploma  general  honors. I have downloaded
> the data set from url :
>
> "http://quantedu.com/wp-content/uploads/2014/05/d1.txt"
>
> On Sat, Jul 9, 2016 at 5:06 PM, Michael Dewey <lists at dewey.myzen.co.uk
> <mailto:lists at dewey.myzen.co.uk>> wrote:
>
>     Dear Shivi
>
>     Are you sure that the level is "honors" and not " honors" or "honors
>     " or something similar?
>
>
>     On 09/07/2016 11:50, Shivi Bhatia wrote:
>
>         Hi Peter,
>         It gives me breakdown of all categories with their respective freq.
>         Diploma     general  honors
>                  50          45         105
>
>         On Sat, Jul 9, 2016 at 4:07 PM, peter dalgaard <pdalgd at gmail.com
>         <mailto:pdalgd at gmail.com>> wrote:
>
>
>             What does table(m11$prog) tell you?
>             -pd
>
>                 On 09 Jul 2016, at 12:29 , Shivi Bhatia
>                 <shivipmp82 at gmail.com <mailto:shivipmp82 at gmail.com>> wrote:
>
>                 Dear Team,
>
>                 I am running a multinomial logistic regression model for
>                 one of the
>                 fictitious data before i implement the same on my real data.
>                 Here i am trying to predict based some scores and
>                 economic group whether
>
>             a
>
>                 person will go for a diploma, general or honors.
>
>                 The code below:
>                 m11$prog2<- relevel(m11$prog, ref = "honors"
>
>                 already loaded the nnet library. However i got the below
>                 error:
>                 Error in relevel.factor(m11$prog, ref = "honors") :
>                  'ref' must be an existing level
>
>                 I have tried searching on SO and nabble but did not find
>                 an answer that
>                 could help.
>
>                 Please suggest what is incorrect. Also checked the class
>                 of the var and
>
>             is
>
>                 a factor variable.
>
>                       [[alternative HTML version deleted]]
>
>                 ______________________________________________
>                 R-help at r-project.org <mailto:R-help at r-project.org>
>                 mailing list -- To UNSUBSCRIBE and more, see
>                 https://stat.ethz.ch/mailman/listinfo/r-help
>                 PLEASE do read the posting guide
>
>             http://www.R-project.org/posting-guide.html
>
>                 and provide commented, minimal, self-contained,
>                 reproducible code.
>
>
>             --
>             Peter Dalgaard, Professor,
>             Center for Statistics, Copenhagen Business School
>             Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>             Phone: (+45)38153501
>             Office: A 4.23
>             Email: pd.mes at cbs.dk <mailto:pd.mes at cbs.dk>  Priv:
>             PDalgd at gmail.com <mailto:PDalgd at gmail.com>
>
>
>
>
>
>
>
>
>
>
>
>                 [[alternative HTML version deleted]]
>
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>         -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         and provide commented, minimal, self-contained, reproducible code.
>
>
>     --
>     Michael
>     http://www.dewey.myzen.co.uk/home.html
>
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From fernando.marmolejo.ramos at psychology.su.se  Sat Jul  9 17:17:28 2016
From: fernando.marmolejo.ramos at psychology.su.se (Fernando Marmolejo Ramos)
Date: Sat, 9 Jul 2016 15:17:28 +0000
Subject: [R] dependent p.values
Message-ID: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>

hi all


does any one know a method to combine dependent p.values?


best


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Fernando Marmolejo-Ramos
Postdoctoral Fellow
G?sta Ekman Laboratory
Department of Psychology
Stockholm University
Frescati Hagv?g 9A, Stockholm 114 19
Sweden

ph = +46 08-16 46 07
website = http://sites.google.com/site/fernandomarmolejoramos/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	[[alternative HTML version deleted]]


From psychexpert1992 at gmail.com  Sat Jul  9 18:12:22 2016
From: psychexpert1992 at gmail.com (Julia Edeleva)
Date: Sat, 9 Jul 2016 20:12:22 +0400
Subject: [R] R council with regard to the analysis needd
Message-ID: <CAOVRatzxucKKiUcs0Nyo8-8arY3jT4+B6E62UgDrH6djezTGBA@mail.gmail.com>

Dear R community,

I am a PhD student at the University of M?nster writing my thesis in
psycholinguistics.

I am currently running statistical analysis on a dataset with 3 fixed
factors (syntax, position), each with 2 levels and their interaction
(syntax*position). The accuracy rate is the dependent variable.

I first created a general linear mixed effects model (lme4), including both
the factors and their interaction and requested its summary.

 *M1 = glmer (correctness ~ syntax+position + syntax*position +(1|subj_nr)
+ (1|item_sf), data = df.rus2, family = binomial) *

I now want to test the contribution of the factors to the power of the
model, taken separately, by creating further models with individual factors
dropped and comparing the models to the baseline model.

My question is: should I drop the factors from the original model?

e.g. for the effect of syntax

*M2 = glmer (correctness ~ syntax + syntax*position +(1|subj_nr) +
(1|item_sf), data = df.rus2, family = binomial) *

Or should I test the main effects separately from the interaction?

The baseline model:

*M0 = glmer (correctness ~ syntax + position +(1|subj_nr) + (1|item_sf),
data = df.rus2, family = binomial)*

Models for comparison:

*M1 = glmer (correctness ~ syntax  +(1|subj_nr) + (1|item_sf), data =
df.rus2, family = binomial)*


*M2 = glmer (correctness ~ position +(1|subj_nr) + (1|item_sf), data =
df.rus2, family = binomial)*


*M3 = glmer (correctness ~ syntax * position +(1|subj_nr) + (1|item_sf),
data = df.rus2, family = binomial)*


Thank you in advance.
Sincerely
Julia Edeleva

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Sat Jul  9 20:14:04 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Sat, 9 Jul 2016 11:14:04 -0700
Subject: [R] 'ref' must be an existing level
In-Reply-To: <2E320F89-5F94-40AD-A6FF-98AA6EA63ED0@comcast.net>
References: <CAB=p7SpShUU+frd-gAL7p0xXN34N5yVCzfZ8b-nOor7X97jLNw@mail.gmail.com>
	<C3B9068A-948C-4A90-ABB2-3DAB1D4A29AD@gmail.com>
	<CAB=p7SprgtTj6Cy9ARWaeDFEQVH71mSqNrXLG3GXh21rRqM06A@mail.gmail.com>
	<F6E6F0BC-F55E-467B-BDBA-EFF99AEF7865@gmail.com>
	<2E320F89-5F94-40AD-A6FF-98AA6EA63ED0@comcast.net>
Message-ID: <CAF8bMcZjg6pe0b4NSD1dvrZUmM4v12ou7rYQ8572WjWk47MsvQ@mail.gmail.com>

Adding the argument strip.white=TRUE to your call to read.csv() will remove
possible leading or trailing "white space" (spaces and tabs).

> str(read.csv( text = " honors,cum laude\nhonors, cum laude\n",
header=FALSE))
'data.frame':   2 obs. of  2 variables:
 $ V1: Factor w/ 2 levels " honors","honors": 1 2
 $ V2: Factor w/ 2 levels " cum laude","cum laude": 2 1
> str(read.csv( text = " honors,cum laude\nhonors, cum laude\n",
header=FALSE, strip.white=TRUE))
'data.frame':   2 obs. of  2 variables:
 $ V1: Factor w/ 1 level "honors": 1 1
 $ V2: Factor w/ 1 level "cum laude": 1 1



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sat, Jul 9, 2016 at 10:08 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jul 9, 2016, at 4:32 AM, peter dalgaard <pdalgd at gmail.com> wrote:
> >
> > Hmm, and levels(m11$prog)?
>
> Shivi;
>
> See what this returns:
>
> "honors" %in% levels(m11$prog)
>
> The level is probably something like " honors"
>
> It's fairly easy to inadvertently create leading or trailing spaces when
> reading from comma-separated files.
>
> > test <- read.csv( text = " honors, others\n honors, others\n",
> header=FALSE)
> > test
>        V1      V2
> 1  honors  others
> 2  honors  others
> > nchar(test$V1)
> Error in nchar(test$V1) : 'nchar()' requires a character vector
> > nchar(as.character(test$V1))
> [1] 7 7
> > "honors" %in% levels(test$V1)
> [1] FALSE
>
> --
> David.
>
> >
> > (There's a chance that a space has sneaked in, but your HTML mail makes
> it hard to see column alignment)
> >
> > -pd
> >
> >> On 09 Jul 2016, at 12:50 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> >>
> >> Hi Peter,
> >> It gives me breakdown of all categories with their respective freq.
> >> Diploma     general  honors
> >>         50          45         105
> >>
> >> On Sat, Jul 9, 2016 at 4:07 PM, peter dalgaard <pdalgd at gmail.com>
> wrote:
> >>
> >> What does table(m11$prog) tell you?
> >> -pd
> >>
> >>> On 09 Jul 2016, at 12:29 , Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> >>>
> >>> Dear Team,
> >>>
> >>> I am running a multinomial logistic regression model for one of the
> >>> fictitious data before i implement the same on my real data.
> >>> Here i am trying to predict based some scores and economic group
> whether a
> >>> person will go for a diploma, general or honors.
> >>>
> >>> The code below:
> >>> m11$prog2<- relevel(m11$prog, ref = "honors"
> >>>
> >>> already loaded the nnet library. However i got the below error:
> >>> Error in relevel.factor(m11$prog, ref = "honors") :
> >>> 'ref' must be an existing level
> >>>
> >>> I have tried searching on SO and nabble but did not find an answer that
> >>> could help.
> >>>
> >>> Please suggest what is incorrect. Also checked the class of the var
> and is
> >>> a factor variable.
> >>>
> >>>      [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Peter Dalgaard, Professor,
> >> Center for Statistics, Copenhagen Business School
> >> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >> Phone: (+45)38153501
> >> Office: A 4.23
> >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >
> > --
> > Peter Dalgaard, Professor,
> > Center for Statistics, Copenhagen Business School
> > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > Phone: (+45)38153501
> > Office: A 4.23
> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From es at enricoschumann.net  Sat Jul  9 21:16:17 2016
From: es at enricoschumann.net (Enrico Schumann)
Date: Sat, 09 Jul 2016 21:16:17 +0200
Subject: [R] Help with constrained portfolio optimization
In-Reply-To: <CADjzwh8k=Rho72J_=81-5s9OYLzj4=vjLTsJwG2eXy3DPdkLQQ@mail.gmail.com>
	(Paulino Levara's message of "Fri, 8 Jul 2016 20:10:26 +0200")
References: <CADjzwh8k=Rho72J_=81-5s9OYLzj4=vjLTsJwG2eXy3DPdkLQQ@mail.gmail.com>
Message-ID: <87inwehbb2.fsf@enricoschumann.net>

On Fri, 08 Jul 2016, Paulino Levara <paulino.levara at gmail.com> writes:

> Dear R community,
>
> I am a beginner in portfolio optimization and I would appreciate your help
> with the next problem:given a set of 10 variables (X), I would like to
> obtain the efficient portfolio that minimize the variance taking the
> expected return as mean(X), subject to the next constraints:
>
> a) Limit the sum of the weights of the first five variables to 30%
> b) Limit the sum of the weights of the last five variables to 70%
>
> What is your suggestion? Can I do this with the portfolio.optim function of
> the tseries package?
>
> How Can I do that?
>
> Thanks in advance.
>
> Regards.
>

Such a problem can be solved via quadratic programming. I would
start with package 'quadprog' and its function 'solve.QP' (which
is actually what tseries::portfolio.optim uses). You can find
many tutorials on the web on how to use this function for
portfolio optimisation.

The tricky part is setting up the constraint matrices. Here is
some code that may help you get started. (It is adapted from one
of the code examples in package "NMOF".) 

require("quadprog")

## create random returns data
na  <- 10  ## number of assets 
ns  <- 60  ## number of observations
R   <- array(rnorm(ns * na, 
                   mean = 0.005, sd = 0.015), 
             dim = c(ns, na)) ## roughly like monthly equity returns
m    <- colMeans(R)  ## asset means
rd   <- mean(m)      ## desired mean
wmax <- 1            ## maximum holding size
wmin <- 0.0          ## minimum holding size

## set up matrices
A <- rbind(1, c(rep(1,5), rep(0,5)), m,
           -diag(na), diag(na))

bvec <- c(1, 0.3, rd,
          rep(-wmax, na),
          rep(wmin, na))

result <- solve.QP(Dmat = 2*cov(R),
                   dvec = rep(0, na),
                   Amat = t(A),
                   bvec = bvec,
                   meq  = 2)

w <- result$solution 

## check results
sum(w)         ## check budget constraint
sum(w[1:5])    ## check sum-of-weights constraint
w %*% m >= rd  ## check return constraint
summary(w)     ## check holding size constraint


-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From pai1981 at gmail.com  Sat Jul  9 22:19:27 2016
From: pai1981 at gmail.com (Debasish Pai Mazumder)
Date: Sat, 9 Jul 2016 14:19:27 -0600
Subject: [R] How to make the "apply" faster
Message-ID: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>

I have 4-dimension array x(lat,lon,time,var)

I am using "apply" to calculate over time
 new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})

This is very slow. Is there anyway make it faster?

-Debasish

	[[alternative HTML version deleted]]


From peter.langfelder at gmail.com  Sat Jul  9 22:32:57 2016
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Sat, 9 Jul 2016 13:32:57 -0700
Subject: [R] Fwd:  How to make the "apply" faster
In-Reply-To: <CA+hbrhWKoTnr=X04cc3S0GKHfp4u_KMA-M-hkJXQ3V5xKvCRfA@mail.gmail.com>
References: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
	<CA+hbrhWKoTnr=X04cc3S0GKHfp4u_KMA-M-hkJXQ3V5xKvCRfA@mail.gmail.com>
Message-ID: <CA+hbrhXg_rrGhegyQ7SH2NhzKNqTydrf839QxZ0o4qriZdUuVQ@mail.gmail.com>

Forgot to cc the list...

---------- Forwarded message ----------
From: Peter Langfelder <peter.langfelder at gmail.com>
Date: Sat, Jul 9, 2016 at 1:32 PM
Subject: Re: [R] How to make the "apply" faster
To: Debasish Pai Mazumder <pai1981 at gmail.com>


You could try the following (I haven't tested it so check that the
results make sense):

indicator = x>=70;
new = apply(indicator, c(1,2,4), sum);

If you could find a way to make the dimension over which you sum the
last (or first), you could use rowSums (or colSums) on indicator which
would be orders of magnitude faster.

Peter



On Sat, Jul 9, 2016 at 1:19 PM, Debasish Pai Mazumder <pai1981 at gmail.com> wrote:
> I have 4-dimension array x(lat,lon,time,var)
>
> I am using "apply" to calculate over time
>  new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})
>
> This is very slow. Is there anyway make it faster?
>
> -Debasish
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sat Jul  9 22:33:49 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 09 Jul 2016 13:33:49 -0700
Subject: [R] How to make the "apply" faster
In-Reply-To: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
References: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
Message-ID: <CC6E1482-E5FA-495A-BBD7-5DEB21B6464B@dcn.davis.ca.us>

function(y) {sum(y>=70)}
-- 
Sent from my phone. Please excuse my brevity.

On July 9, 2016 1:19:27 PM PDT, Debasish Pai Mazumder <pai1981 at gmail.com> wrote:
>I have 4-dimension array x(lat,lon,time,var)
>
>I am using "apply" to calculate over time
> new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})
>
>This is very slow. Is there anyway make it faster?
>
>-Debasish
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ccberry at ucsd.edu  Sat Jul  9 23:46:23 2016
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Sat, 9 Jul 2016 14:46:23 -0700
Subject: [R] How to make the "apply" faster
In-Reply-To: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
References: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
Message-ID: <alpine.OSX.2.20.1607091428480.543@charles-berrys-macbook.local>

On Sat, 9 Jul 2016, Debasish Pai Mazumder wrote:

> I have 4-dimension array x(lat,lon,time,var)
>
> I am using "apply" to calculate over time
> new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})
>
> This is very slow. Is there anyway make it faster?

If dim(x)[3] << prod(dim(x)[-3]),

new <-  Reduce("+",lapply(1:dim(x)[3],function(z) x[,,z,]>=70))

will be faster.

However, if you can follow Peter Langfelder's suggestion to use rowSums, 
that would be best. Even using rowSums(aperm(x,c(1,2,4,3)>=70,dims=3) and 
paying the price of aperm() might be better.

Chuck


From bbolker at gmail.com  Sun Jul 10 01:40:54 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 9 Jul 2016 23:40:54 +0000
Subject: [R] Fixed Effects in lme function
References: <CAHLnndY4vVhVGJer4sr4k1sSTasW3+ibJcZY2FvtcCtjAudCpw@mail.gmail.com>
Message-ID: <loom.20160710T005305-408@post.gmane.org>

li li <hannah.hlx <at> gmail.com> writes:

> 
>  Dear all,

>     For the data below, I would like to fit a model with common
> random slope and common random intercept as shown below. I am
> interested in obtaining separate fixed effect estimates (intercept
> and slope and corresponding hypothesis test) for each
> method. Instead of performing the analysis method by method, can I
> use the syntax as below, specifically, the part "fixed = response ~
> method/time"?  I know this is legitimate specification if there is
> no random effects involved.  Thanks so much in advance!  Hanna

  This is probably a better question for the r-sig-mixed-models
mailing list ... followups there, please.  (Also, please don't post in HTML)

Yes, that seems as though it should work.

dat <- read.table(header=TRUE,
text="response individual time method
102.9 3 0 3
103.0 3 3 3
103.0 3 6 3
102.8 3 9 3
102.2 3 12 3
102.5 3 15 3
103.0 3 18 3
102.0 3 24 3
102.8 1 0 3
102.7 1 3 3
103.0 1 6 3
102.2 1 9 3
103.0 1 12 3
102.8 1 15 3
102.8 1 18 3
102.9 1 24 3
102.2 2 0 3
102.6 2 3 3
103.4 2 6 3
102.3 2 9 3
101.3 2 12 3
102.1 2 15 3
102.1 2 18 3
102.2 2 24 3
102.7 4 0 3
102.3 4 3 3
102.6 4 6 3
102.7 4 9 3
102.8 4 12 3
102.5 5 0 3
102.4 5 3 3
102.1 5 6 3
102.3 6 0 3
102.3 6 3 3
101.9 7 0 3
102.0 7 3 3
107.4 3 0 1
101.3 3 12 1
92.8 3 15 1
73.7 3 18 1
104.7 3 24 1
92.6 1 0 1
101.9 1 12 1
106.3 1 15 1
104.1 1 18 1
95.6 1 24 1
79.8 2 0 1
89.7 2 12 1
97.0 2 15 1
108.4 2 18 1
103.5 2 24 1
96.4 4 0 1
89.3 4 12 1
112.6 5 0 1
93.3 6 0 1
99.6 7 0 1
109.5 3 0 2
98.5 3 12 2
103.5 3 24 2
113.5 1 0 2
94.5 1 12 2
88.5 1 24 2
99.5 2 0 2
97.5 2 12 2
98.5 2 24 2
103.5 4 0 2
89.5 5 0 2
87.5 6 0 2
82.5 7 0 2
")

library(nlme)

## check design
with(dat,table(method,time))
with(dat,table(method,individual))
with(dat,table(time,individual))
dat <- transform(dat,
    method=factor(method),
    individual=factor(individual))
library(ggplot2); theme_set(theme_bw())
ggplot(dat,aes(time,response,colour=method))+geom_point()+
    stat_summary(fun.y=mean,geom="line",
        aes(group=individual),colour="black",alpha=0.6)

library(nlme)

Is 'lot' supposed to be 'individual' here? Or is there another variable
we don't know about?

m1 <- lme(fixed= response ~ method/time, random=~ 1+time | individual,
    data=dat,
  weights= varIdent(form=~1|method),   control = lmeControl(opt = "optim"),
  na.action = na.exclude)

summary(m1)$tTable


From r.turner at auckland.ac.nz  Sun Jul 10 01:57:24 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 10 Jul 2016 11:57:24 +1200
Subject: [R] dependent p.values
In-Reply-To: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>
References: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>
Message-ID: <13a2a800-80dd-d20e-e533-160889bdaaa7@auckland.ac.nz>

On 10/07/16 03:17, Fernando Marmolejo Ramos wrote:
> hi all
>
>
> does any one know a method to combine dependent p.values?

See fortune(269). :-)

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Sun Jul 10 01:59:30 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 10 Jul 2016 11:59:30 +1200
Subject: [R] [FORGED]  Regression with factors ?
In-Reply-To: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
Message-ID: <3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>

On 09/07/16 20:52, stn021 wrote:
> Hello,
>
> I would like to analyse a model like this:
>
> y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )
>
> x1 and x2 are not continuous variables but factors, so the observation
> contain the level.
> Its numerical value is unknown and is to be estimated with the model.
>
>
> The observations look like this:
>
> y        x1     x2
> 0.96  Alice  Bob
> 0.84  Alice  Charlie
> 0.96  Bob   Charlie
> 0.64  Dave Alice
> etc.
>
> Each person has a numerical value. Here for example Alice = 0.2 and Bob =
> 0.4
>
> Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.
>
> How can this be done in R ?


This question makes about as little sense as it is possible to imagine.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Sun Jul 10 02:06:16 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 9 Jul 2016 17:06:16 -0700
Subject: [R] R council with regard to the analysis needd
In-Reply-To: <CAOVRatzxucKKiUcs0Nyo8-8arY3jT4+B6E62UgDrH6djezTGBA@mail.gmail.com>
References: <CAOVRatzxucKKiUcs0Nyo8-8arY3jT4+B6E62UgDrH6djezTGBA@mail.gmail.com>
Message-ID: <E64EC7FD-3E53-4861-A447-9909CE7F373C@comcast.net>


> On Jul 9, 2016, at 9:12 AM, Julia Edeleva <psychexpert1992 at gmail.com> wrote:
> 
> Dear R community,
> 
> I am a PhD student at the University of M?nster writing my thesis in
> psycholinguistics.
> 
> I am currently running statistical analysis on a dataset with 3 fixed
> factors (syntax, position), each with 2 levels and their interaction
> (syntax*position). The accuracy rate is the dependent variable.
> 
> I first created a general linear mixed effects model (lme4), including both
> the factors and their interaction and requested its summary.
> 
> *M1 = glmer (correctness ~ syntax+position + syntax*position +(1|subj_nr)
> + (1|item_sf), data = df.rus2, family = binomial) *
> 
> I now want to test the contribution of the factors to the power of the
> model, taken separately, by creating further models with individual factors
> dropped and comparing the models to the baseline model.
> 

In English statistical parlance power generally refers to the ability of different statistical test to avoid so-called type 2 errors. The term goodness of fit is usually used to describe the overall capacity of a model to capture or apportion the sources of variation in a process.


> My question is: should I drop the factors from the original model?
> 
> e.g. for the effect of syntax
> 
> *M2 = glmer (correctness ~ syntax + syntax*position +(1|subj_nr) +
> (1|item_sf), data = df.rus2, family = binomial) *
> 
> Or should I test the main effects separately from the interaction?
> 
> The baseline model:
> 
> *M0 = glmer (correctness ~ syntax + position +(1|subj_nr) + (1|item_sf),
> data = df.rus2, family = binomial)*
> 
> Models for comparison:
> 
> *M1 = glmer (correctness ~ syntax  +(1|subj_nr) + (1|item_sf), data =
> df.rus2, family = binomial)*

Let's call that one 'M4' since you already have an M1 defined above.

> 
> 
> *M2 = glmer (correctness ~ position +(1|subj_nr) + (1|item_sf), data =
> df.rus2, family = binomial)*

And call that one 'M5' since a different M2 model was defined above.

> 
> 
> *M3 = glmer (correctness ~ syntax * position +(1|subj_nr) + (1|item_sf),
> data = df.rus2, family = binomial)*

M1, and M3 and M2 are the same models. Read: ?formula

M0 is nested in the M1 (or equivalent M3 or M2) model

M4 and M5 are nested in M0.

You should clarify with your research professor what your research hypothesis (or hypotheses) might be. That is what should determine your model comparison hierarchy.

-- 


David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.ca.us  Sun Jul 10 02:27:18 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 09 Jul 2016 17:27:18 -0700
Subject: [R] [FORGED]  Regression with factors ?
In-Reply-To: <3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
Message-ID: <EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>

I have seen less sensical questions.

It would be nice if the example were a bit more complete (as in it should have excess degrees of freedom and an answer) and less like a homework problem (which are off topic here). It would of course also be helpful if the OP were to conform to the Posting Guide, particularly in respect to using plain text email. 

It looks like the kind of nonlinear optimization problem that evolutionary algorithms are often applied to. It doesn't look (to me) like a typical problem that factors get applied to in formulas though, because multiple instances of the same factor variable are present.
-- 
Sent from my phone. Please excuse my brevity.

On July 9, 2016 4:59:30 PM PDT, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>On 09/07/16 20:52, stn021 wrote:
>> Hello,
>>
>> I would like to analyse a model like this:
>>
>> y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )
>>
>> x1 and x2 are not continuous variables but factors, so the
>observation
>> contain the level.
>> Its numerical value is unknown and is to be estimated with the model.
>>
>>
>> The observations look like this:
>>
>> y        x1     x2
>> 0.96  Alice  Bob
>> 0.84  Alice  Charlie
>> 0.96  Bob   Charlie
>> 0.64  Dave Alice
>> etc.
>>
>> Each person has a numerical value. Here for example Alice = 0.2 and
>Bob =
>> 0.4
>>
>> Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.
>>
>> How can this be done in R ?
>
>
>This question makes about as little sense as it is possible to imagine.
>
>cheers,
>
>Rolf Turner


From kristi.glover at hotmail.com  Sun Jul 10 07:34:53 2016
From: kristi.glover at hotmail.com (Kristi Glover)
Date: Sun, 10 Jul 2016 05:34:53 +0000
Subject: [R] column name changes
Message-ID: <CO2PR13MB01390C09C9AB9074DC21CC80FA3E0@CO2PR13MB0139.namprd13.prod.outlook.com>

Hi R user,?
I wanted to change a column name with new one??but it comes with "." where there was space. Is there any way to keep my formate with space?
Here what I found?


Images<-stack(imageA,imageB,imageC)
names(Images)[3]<-c("dif of AB")
head(Images)
It gives the column name of 3 as a "dif.of.AB", but I wanted to be "dif of AB"

I don't want to put the "." on the spaces.


Any suggestions?

Thanks


From gab4 at st-andrews.ac.uk  Sun Jul 10 07:59:59 2016
From: gab4 at st-andrews.ac.uk (Giles Bischoff)
Date: Sat, 9 Jul 2016 22:59:59 -0700
Subject: [R] Reading a large directory of compressed zips into a data frame
Message-ID: <CAPvuKKwYYs1KwHyJB73YXYrCpWQQDG=jfh-7eC00AvBD8kZaZQ@mail.gmail.com>

Hello R Programmers!
I was wondering if y'all could help me. I'm trying to read data from a
directory containing 332 compressed zips all with about 1000 lines (or
more) of data into a data frame. I have it so that the directory is set to
the file with the zips in it. I figured this way when I tried using the
dir() function, I could do something like d1 <- read.csv(dir()[1:332]) to
read all the data and then find the mean of say "columnA" in that data
table by using something like mean(d1$columnA). Though, so far this has not
worked. Any ideas?
Sincerely,
Giles

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Jul 10 08:20:41 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 9 Jul 2016 23:20:41 -0700
Subject: [R] Reading a large directory of compressed zips into a data
	frame
In-Reply-To: <CAPvuKKwYYs1KwHyJB73YXYrCpWQQDG=jfh-7eC00AvBD8kZaZQ@mail.gmail.com>
References: <CAPvuKKwYYs1KwHyJB73YXYrCpWQQDG=jfh-7eC00AvBD8kZaZQ@mail.gmail.com>
Message-ID: <49BFB476-95AB-4E05-98A5-CA9A1968F235@comcast.net>


> On Jul 9, 2016, at 10:59 PM, Giles Bischoff <gab4 at st-andrews.ac.uk> wrote:
> 
> Hello R Programmers!
> I was wondering if y'all could help me. I'm trying to read data from a
> directory containing 332 compressed zips all with about 1000 lines (or
> more) of data into a data frame. I have it so that the directory is set to
> the file with the zips in it. I figured this way when I tried using the
> dir() function, I could do something like d1 <- read.csv(dir()[1:332]) to
> read all the data and then find the mean of say "columnA" in that data
> table by using something like mean(d1$columnA). Though, so far this has not
> worked. Any ideas?

This is highly likely to be one of the homework problems for one of Peng's Johns Hopkins online data management courses. Questions from htat course have been posted many times on StackOverflow and many of them have been answered there as well. R-help, however, has a no homework policy.


> Sincerely,
> Giles
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From fernando.marmolejo.ramos at psychology.su.se  Sun Jul 10 09:39:36 2016
From: fernando.marmolejo.ramos at psychology.su.se (Fernando Marmolejo Ramos)
Date: Sun, 10 Jul 2016 07:39:36 +0000
Subject: [R] dependent p.values in R
In-Reply-To: <b0ea1652-c9d3-6865-5057-de0064c00ae2@u-psud.fr>
References: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>,
	<b0ea1652-c9d3-6865-5057-de0064c00ae2@u-psud.fr>
Message-ID: <1468136385886.74139@psychology.su.se>

hi marc

say i have a vector with some x number of observations

x = c(23, 56, 123, ..... )

and i want to know how normal it is

as there are many normality tests, i want to combine their p.values

so, suppose i use shapiro.wilk, anderson darling and jarque bera and each will give a pvalue

i could simply average those p,values but to my knowledge that approach is biased

so i thought, in the same way there is a method to combine independent pvalues (e.g. stouffer method); is there a way to combine dependent pvalues?

best

f

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Fernando Marmolejo-Ramos
Postdoctoral Fellow
G?sta Ekman Laboratory
Department of Psychology
Stockholm University
Frescati Hagv?g 9A, Stockholm 114 19
Sweden

ph = +46 08-16 46 07
website = http://sites.google.com/site/fernandomarmolejoramos/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

________________________________________
From: Marc Girondot <marc.girondot at u-psud.fr>
Sent: Sunday, 10 July 2016 8:25 AM
To: r-help at r-project.org; Fernando Marmolejo Ramos
Subject: Re: [R] dependent p.values

Le 09/07/2016 ? 17:17, Fernando Marmolejo Ramos a ?crit :
> hi all
>
>
> does any one know a method to combine dependent p.values?
>
>
First, it is a stats question and not a R question. So you could have
better chance to ask this in stackexchange forum.
Second, your question is difficult to answer without context: why
p.values are dependent ? Do they come from the same dataset ? Or are
they linked by an external source ? For both these situations, combining
dependent p.values seems strange for me.
When you will ask question in stackexchange, be more precise.
Sincerely,
Marc Girondot

--
__________________________________________________________
Marc Girondot, Pr

Laboratoire Ecologie, Syst?matique et Evolution
Equipe de Conservation des Populations et des Communaut?s
CNRS, AgroParisTech et Universit? Paris-Sud 11 , UMR 8079
B?timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1.69.15.73.53
e-mail: marc.girondot at u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot



From r.turner at auckland.ac.nz  Sun Jul 10 11:49:05 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 10 Jul 2016 21:49:05 +1200
Subject: [R] [FORGED]  column name changes
In-Reply-To: <CO2PR13MB01390C09C9AB9074DC21CC80FA3E0@CO2PR13MB0139.namprd13.prod.outlook.com>
References: <CO2PR13MB01390C09C9AB9074DC21CC80FA3E0@CO2PR13MB0139.namprd13.prod.outlook.com>
Message-ID: <78fb5307-425a-64af-5475-ef1eed0e497c@auckland.ac.nz>

On 10/07/16 17:34, Kristi Glover wrote:
> Hi R user,
> I wanted to change a column name with new one  but it comes with "." where there was space. Is there any way to keep my formate with space?
> Here what I found
>
>
> Images<-stack(imageA,imageB,imageC)
> names(Images)[3]<-c("dif of AB")
> head(Images)
> It gives the column name of 3 as a "dif.of.AB", but I wanted to be "dif of AB"
>
> I don't want to put the "." on the spaces.
>
>
> Any suggestions?


(1) Forget about what you "don't want" and leave the dots be. Spaces in 
variable/column names are an abomination, tolerated only by the great 
unwashed (i.e. users of Windoze).

(2) See fortune(37).

(3) It doesn't happen to me:

set.seed(42)
Images <- data.frame(x=rnorm(1),y=rnorm(10),z=rnorm(10))
names(Images)[3] <- "dif of AB"
names(Images)
> [1] "x"         "y"         "dif of AB"

There may be some setting that enforces "syntactically valid" names, but 
I see no such setting associated with names().  (There *is* such a 
setting associated with data.frame() --- are you telling the truth about 
how you formed the new names of "Images"?)

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From drjimlemon at gmail.com  Sun Jul 10 12:18:09 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 10 Jul 2016 20:18:09 +1000
Subject: [R] column name changes
In-Reply-To: <CO2PR13MB01390C09C9AB9074DC21CC80FA3E0@CO2PR13MB0139.namprd13.prod.outlook.com>
References: <CO2PR13MB01390C09C9AB9074DC21CC80FA3E0@CO2PR13MB0139.namprd13.prod.outlook.com>
Message-ID: <CA+8X3fWcyUOjfGhd1gwKjET00H1P9QziDq+-nZKXYLsbfo=Q+Q@mail.gmail.com>

Hi Kristi,
The period is there for a reason. If you want to extract that column like this:

x<-data.frame(a=1:3,b=2:4,c=3:5)
> names(x)[3]<-"dif of AB"
> x
 a b dif of AB
1 1 2         3
2 2 3         4
3 3 4         5
> x$dif of AB
Error: unexpected symbol in "x$dif of"
> x$'dif of AB'
[1] 3 4 5

you will have to quote the column name every time.

Jim


On Sun, Jul 10, 2016 at 3:34 PM, Kristi Glover
<kristi.glover at hotmail.com> wrote:
> Hi R user,
> I wanted to change a column name with new one  but it comes with "." where there was space. Is there any way to keep my formate with space?
> Here what I found
>
>
> Images<-stack(imageA,imageB,imageC)
> names(Images)[3]<-c("dif of AB")
> head(Images)
> It gives the column name of 3 as a "dif.of.AB", but I wanted to be "dif of AB"
>
> I don't want to put the "." on the spaces.
>
>
> Any suggestions?
>
> Thanks
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc.girondot at u-psud.fr  Sun Jul 10 08:25:49 2016
From: marc.girondot at u-psud.fr (Marc Girondot)
Date: Sun, 10 Jul 2016 08:25:49 +0200
Subject: [R] dependent p.values
In-Reply-To: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>
References: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>
Message-ID: <b0ea1652-c9d3-6865-5057-de0064c00ae2@u-psud.fr>

Le 09/07/2016 ? 17:17, Fernando Marmolejo Ramos a ?crit :
> hi all
>
>
> does any one know a method to combine dependent p.values?
>
>
First, it is a stats question and not a R question. So you could have 
better chance to ask this in stackexchange forum.
Second, your question is difficult to answer without context: why 
p.values are dependent ? Do they come from the same dataset ? Or are 
they linked by an external source ? For both these situations, combining 
dependent p.values seems strange for me.
When you will ask question in stackexchange, be more precise.
Sincerely,
Marc Girondot

-- 
__________________________________________________________
Marc Girondot, Pr

Laboratoire Ecologie, Syst?matique et Evolution
Equipe de Conservation des Populations et des Communaut?s
CNRS, AgroParisTech et Universit? Paris-Sud 11 , UMR 8079
B?timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1.69.15.73.53
e-mail: marc.girondot at u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot


From psychexpert1992 at gmail.com  Sun Jul 10 14:34:39 2016
From: psychexpert1992 at gmail.com (Julia Edeleva)
Date: Sun, 10 Jul 2016 16:34:39 +0400
Subject: [R] Statistical Test
Message-ID: <CAOVRatxgwy9EVU+rKGTLKBmKMt+dGz=gb4tgG5qW=R7DFuEd-w@mail.gmail.com>

Dear R-community,

Thanks for replying to my previous post. I would need some more help,
thoug.

I am performing statistical analysis on chidlren's accuracy rates  as a
dependent variable and two predictor variables with two levels each (syntax
- subject vs object; internal NP position - pre vs post).

As an outcome of my study, children committed 3 types of errors. I want to
compare whether children committed significantly more errors of one type as
compared to the other two types, i.e. test the scale *error 1 > error 2 >
error 3 (">" is "more than").*  Which statistical test is most appropriate?

Furthermore, I want to know whether one particular type of error is more
common in one experimental condition than in the other, i.e. test
whether *error
1 in condition 1 is more common than error 1 in condition 2*.

Thnx a lot

Julia Edeleva

*Compare different types of errors in chidren's performance. Statistical
Test? - ResearchGate*. Available from:
https://www.researchgate.net/post/Compare_different_types_of_errors_in_chidrens_performance_Statistical_Test
[accessed Jul 10, 2016].

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Sun Jul 10 18:59:37 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 10 Jul 2016 16:59:37 +0000
Subject: [R] dependent p.values in R
References: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>,
	<b0ea1652-c9d3-6865-5057-de0064c00ae2@u-psud.fr>
	<1468136385886.74139@psychology.su.se>
Message-ID: <loom.20160710T185520-673@post.gmane.org>

Fernando Marmolejo Ramos <fernando.marmolejo.ramos <at> psychology.su.se>
writes:

> 
> hi marc
> 
> say i have a vector with some x number of observations
> 
> x = c(23, 56, 123, ..... )
> 
> and i want to know how normal it is
> 
> as there are many normality tests, i want to combine their p.values
> 
> so, suppose i use shapiro.wilk, anderson darling and
>    jarque bera and each will give a pvalue
> 
> i could simply average those p,values but to my knowledge 
>    that approach is biased
> 
> so i thought, in the same way there is a method to combine
>    independent pvalues (e.g. stouffer method); is
> there a way to combine dependent pvalues?
> 
> best
> 
> f
> 

  Yikes.  There is extensive discussion, e.g. at
http://tinyurl.com/normtests , that suggests that much of the
time (if not always) formal statistical hypothesis tests for
normality are misguided.  Combining p-values from different tests
feels like compounding the issue.  In any case, I would definitely
say that this a question for CrossValidated 
(http://stats.stackexchange.com), rather than r-help ...

  Ben Bolker


From friendly at yorku.ca  Sun Jul 10 19:51:07 2016
From: friendly at yorku.ca (Michael Friendly)
Date: Sun, 10 Jul 2016 13:51:07 -0400
Subject: [R] dependent p.values in R
In-Reply-To: <1468136385886.74139@psychology.su.se>
References: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>
	<b0ea1652-c9d3-6865-5057-de0064c00ae2@u-psud.fr>
	<1468136385886.74139@psychology.su.se>
Message-ID: <fb9f9102-7c32-c6b9-5c2a-af1b38c2dbc2@yorku.ca>

Hello Fernando

First, ask yourself what Gosta Ekman would have said if you asked him 
this question.  He would have asked "does it make any difference to
your conclusion?"  He might also have asked you "Did you do a visual
test?"  Plot your data as a QQ plot or density plot?

If the test doesn't make a difference in conclusions, it is a waste of 
your time (and ours) to worry about how to cite a
'combined p.value' (if such an animal exists), presumably to
more decimal places than is worth worrying about.

If the test *does* make a difference about normality, then ask yourself
does the degree of non-normality impede my substantive conclusions.

HTH,
-ichael

On 7/10/16 3:39 AM, Fernando Marmolejo Ramos wrote:
> hi marc
>
> say i have a vector with some x number of observations
>
> x = c(23, 56, 123, ..... )
>
> and i want to know how normal it is
>
> as there are many normality tests, i want to combine their p.values
>
> so, suppose i use shapiro.wilk, anderson darling and jarque bera and each will give a pvalue
>
> i could simply average those p,values but to my knowledge that approach is biased
>
> so i thought, in the same way there is a method to combine independent pvalues (e.g. stouffer method); is there a way to combine dependent pvalues?
>
> best
>
> f
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Fernando Marmolejo-Ramos
> Postdoctoral Fellow
> G?sta Ekman Laboratory
> Department of Psychology
> Stockholm University
> Frescati Hagv?g 9A, Stockholm 114 19
> Sweden
>
> ph = +46 08-16 46 07
> website = http://sites.google.com/site/fernandomarmolejoramos/
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
> ________________________________________
> From: Marc Girondot <marc.girondot at u-psud.fr>
> Sent: Sunday, 10 July 2016 8:25 AM
> To: r-help at r-project.org; Fernando Marmolejo Ramos
> Subject: Re: [R] dependent p.values
>
> Le 09/07/2016 ? 17:17, Fernando Marmolejo Ramos a ?crit :
>> hi all
>>
>>
>> does any one know a method to combine dependent p.values?
>>
>>
> First, it is a stats question and not a R question. So you could have
> better chance to ask this in stackexchange forum.
> Second, your question is difficult to answer without context: why
> p.values are dependent ? Do they come from the same dataset ? Or are
> they linked by an external source ? For both these situations, combining
> dependent p.values seems strange for me.
> When you will ask question in stackexchange, be more precise.
> Sincerely,
> Marc Girondot
>
> --
> __________________________________________________________
> Marc Girondot, Pr
>
> Laboratoire Ecologie, Syst?matique et Evolution
> Equipe de Conservation des Populations et des Communaut?s
> CNRS, AgroParisTech et Universit? Paris-Sud 11 , UMR 8079
> B?timent 362
> 91405 Orsay Cedex, France
>
> Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1.69.15.73.53
> e-mail: marc.girondot at u-psud.fr
> Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
> Skype: girondot
>
>


From friendly at yorku.ca  Sun Jul 10 19:51:07 2016
From: friendly at yorku.ca (Michael Friendly)
Date: Sun, 10 Jul 2016 13:51:07 -0400
Subject: [R] dependent p.values in R
In-Reply-To: <1468136385886.74139@psychology.su.se>
References: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>
	<b0ea1652-c9d3-6865-5057-de0064c00ae2@u-psud.fr>
	<1468136385886.74139@psychology.su.se>
Message-ID: <fb9f9102-7c32-c6b9-5c2a-af1b38c2dbc2@yorku.ca>

Hello Fernando

First, ask yourself what Gosta Ekman would have said if you asked him 
this question.  He would have asked "does it make any difference to
your conclusion?"  He might also have asked you "Did you do a visual
test?"  Plot your data as a QQ plot or density plot?

If the test doesn't make a difference in conclusions, it is a waste of 
your time (and ours) to worry about how to cite a
'combined p.value' (if such an animal exists), presumably to
more decimal places than is worth worrying about.

If the test *does* make a difference about normality, then ask yourself
does the degree of non-normality impede my substantive conclusions.

HTH,
-ichael

On 7/10/16 3:39 AM, Fernando Marmolejo Ramos wrote:
> hi marc
>
> say i have a vector with some x number of observations
>
> x = c(23, 56, 123, ..... )
>
> and i want to know how normal it is
>
> as there are many normality tests, i want to combine their p.values
>
> so, suppose i use shapiro.wilk, anderson darling and jarque bera and each will give a pvalue
>
> i could simply average those p,values but to my knowledge that approach is biased
>
> so i thought, in the same way there is a method to combine independent pvalues (e.g. stouffer method); is there a way to combine dependent pvalues?
>
> best
>
> f
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Fernando Marmolejo-Ramos
> Postdoctoral Fellow
> G?sta Ekman Laboratory
> Department of Psychology
> Stockholm University
> Frescati Hagv?g 9A, Stockholm 114 19
> Sweden
>
> ph = +46 08-16 46 07
> website = http://sites.google.com/site/fernandomarmolejoramos/
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
> ________________________________________
> From: Marc Girondot <marc.girondot at u-psud.fr>
> Sent: Sunday, 10 July 2016 8:25 AM
> To: r-help at r-project.org; Fernando Marmolejo Ramos
> Subject: Re: [R] dependent p.values
>
> Le 09/07/2016 ? 17:17, Fernando Marmolejo Ramos a ?crit :
>> hi all
>>
>>
>> does any one know a method to combine dependent p.values?
>>
>>
> First, it is a stats question and not a R question. So you could have
> better chance to ask this in stackexchange forum.
> Second, your question is difficult to answer without context: why
> p.values are dependent ? Do they come from the same dataset ? Or are
> they linked by an external source ? For both these situations, combining
> dependent p.values seems strange for me.
> When you will ask question in stackexchange, be more precise.
> Sincerely,
> Marc Girondot
>
> --
> __________________________________________________________
> Marc Girondot, Pr
>
> Laboratoire Ecologie, Syst?matique et Evolution
> Equipe de Conservation des Populations et des Communaut?s
> CNRS, AgroParisTech et Universit? Paris-Sud 11 , UMR 8079
> B?timent 362
> 91405 Orsay Cedex, France
>
> Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1.69.15.73.53
> e-mail: marc.girondot at u-psud.fr
> Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
> Skype: girondot
>
>


From pai1981 at gmail.com  Sun Jul 10 20:58:20 2016
From: pai1981 at gmail.com (Debasish Pai Mazumder)
Date: Sun, 10 Jul 2016 12:58:20 -0600
Subject: [R] How to make the "apply" faster
In-Reply-To: <alpine.OSX.2.20.1607091428480.543@charles-berrys-macbook.local>
References: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
	<alpine.OSX.2.20.1607091428480.543@charles-berrys-macbook.local>
Message-ID: <CAM9mbiDfWprrX7M-DNfux4bTuEuZcRwO3UjR_Mmdf_7mnypBdQ@mail.gmail.com>

Hi Everyone,
Thanks for your help. It works. I have similar problem when I am
calculating number of spell.
I am also calculation spell (definition: period of two or more days where x
exceeds 70) using similar way:

*new = apply(x,c(1,2,4),FUN=function(y) {fun.spell.deb(y, 70)})*

where fun.spell.deb.R:















*## Calculate spell durationfun.spell.deb <- function(data, threshold = 1,
direction = c("above", "below")){  #coln <- grep(weather, names(data))#
var <- data[,8]  if(missing(direction)) {direction <- "above"}
if(direction=="below") {b <- (data <= threshold)} else  {b <- (data >=
threshold)}    b[b==TRUE] = 1  y <-rle(b)  ans
<-length(subset((y$lengths[y$values==1]), (y$lengths[y$values==1])>=2))
return(ans)}*

Do you have any idea how to make the "apply" faster here?

-Deb


On Sat, Jul 9, 2016 at 3:46 PM, Charles C. Berry <ccberry at ucsd.edu> wrote:

> On Sat, 9 Jul 2016, Debasish Pai Mazumder wrote:
>
> I have 4-dimension array x(lat,lon,time,var)
>>
>> I am using "apply" to calculate over time
>> new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})
>>
>> This is very slow. Is there anyway make it faster?
>>
>
> If dim(x)[3] << prod(dim(x)[-3]),
>
> new <-  Reduce("+",lapply(1:dim(x)[3],function(z) x[,,z,]>=70))
>
> will be faster.
>
> However, if you can follow Peter Langfelder's suggestion to use rowSums,
> that would be best. Even using rowSums(aperm(x,c(1,2,4,3)>=70,dims=3) and
> paying the price of aperm() might be better.
>
> Chuck
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Sun Jul 10 21:25:50 2016
From: chocold12 at gmail.com (lily li)
Date: Sun, 10 Jul 2016 13:25:50 -0600
Subject: [R] about smwrgraphs package
Message-ID: <CAN5afy9BS55W=SFujge=C3b70qvoYsr2Usb_jSm_ncR5rhAD6w@mail.gmail.com>

Has anyone used smwrGraphs package? I have some problems and think it may
be better to discuss if you have been using it. Thanks very much.

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Sun Jul 10 22:13:09 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 10 Jul 2016 13:13:09 -0700
Subject: [R] How to make the "apply" faster
In-Reply-To: <CAM9mbiDfWprrX7M-DNfux4bTuEuZcRwO3UjR_Mmdf_7mnypBdQ@mail.gmail.com>
References: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
	<alpine.OSX.2.20.1607091428480.543@charles-berrys-macbook.local>
	<CAM9mbiDfWprrX7M-DNfux4bTuEuZcRwO3UjR_Mmdf_7mnypBdQ@mail.gmail.com>
Message-ID: <CAF8bMcZWSWOwTJ7_PwGfkP7DE03zSb=TqMhifLJ4H6eJk=3UwA@mail.gmail.com>

There is no need to test that a logical equals TRUE: 'logicalVector==TRUE'
is the
same as just 'logicalVector'.

There is no need to convert logical vectors to numeric, since rle() works
on both
types.

There is no need to use length(subset(x, logicalVector)) to count how many
elements
in logicalVector are TRUE, just use sum(logicalVector).

There is no need to make a variable, 'ans', then immediately return it.

Hence your

    b[b == TRUE] = 1
    y <- rle(b)
    ans <- length(subset(y$lengths[y$values == 1], y$lengths[y$values == 1]
>= 2))
    return(ans)

could be replaced by

    y <- rle(b)
    sum(y$lengths[y$values] >= 2)

This gives some speedup, mainly for long vectors, but I find it more
understandable.
E.g., if f1 is your original function and f2 has the above replacement I
get:
  > d <- -sin(1:10000+sqrt(1:4))
  > system.time(for(i in 1:10000)f1(d,.3))
     user  system elapsed
     5.19    0.00    5.19
  > system.time(for(i in 1:10000)f2(d,.3))
     user  system elapsed
     3.65    0.00    3.65
  > c(f1(d,.3), f2(d,.3))
  [1] 1492 1492
  > length(d)
  [1] 10000

If it were my function, I would also get rid of the part that deals with
the threshhold
and direction of the inequality and tell the user to to use f(data <= 0.3)
instead of
f(data, .3, "below").  I would also make the spell length an argument
instead of
fixing it at 2.  E.g.

   > f3 <- function (condition, spellLength = 2)
   {
       stopifnot(is.logical(condition), !anyNA(condition))
       y <- rle(condition)
       sum(y$lengths[y$values] >= spellLength)
   }
   > f3( d >= .3 )
   [1] 1492



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sun, Jul 10, 2016 at 11:58 AM, Debasish Pai Mazumder <pai1981 at gmail.com>
wrote:

> Hi Everyone,
> Thanks for your help. It works. I have similar problem when I am
> calculating number of spell.
> I am also calculation spell (definition: period of two or more days where x
> exceeds 70) using similar way:
>
> *new = apply(x,c(1,2,4),FUN=function(y) {fun.spell.deb(y, 70)})*
>
> where fun.spell.deb.R:
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> *## Calculate spell durationfun.spell.deb <- function(data, threshold = 1,
> direction = c("above", "below")){  #coln <- grep(weather, names(data))#
> var <- data[,8]  if(missing(direction)) {direction <- "above"}
> if(direction=="below") {b <- (data <= threshold)} else  {b <- (data >=
> threshold)}    b[b==TRUE] = 1  y <-rle(b)  ans
> <-length(subset((y$lengths[y$values==1]), (y$lengths[y$values==1])>=2))
> return(ans)}*
>
> Do you have any idea how to make the "apply" faster here?
>
> -Deb
>
>
> On Sat, Jul 9, 2016 at 3:46 PM, Charles C. Berry <ccberry at ucsd.edu> wrote:
>
> > On Sat, 9 Jul 2016, Debasish Pai Mazumder wrote:
> >
> > I have 4-dimension array x(lat,lon,time,var)
> >>
> >> I am using "apply" to calculate over time
> >> new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})
> >>
> >> This is very slow. Is there anyway make it faster?
> >>
> >
> > If dim(x)[3] << prod(dim(x)[-3]),
> >
> > new <-  Reduce("+",lapply(1:dim(x)[3],function(z) x[,,z,]>=70))
> >
> > will be faster.
> >
> > However, if you can follow Peter Langfelder's suggestion to use rowSums,
> > that would be best. Even using rowSums(aperm(x,c(1,2,4,3)>=70,dims=3) and
> > paying the price of aperm() might be better.
> >
> > Chuck
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pai1981 at gmail.com  Sun Jul 10 22:38:14 2016
From: pai1981 at gmail.com (Debasish Pai Mazumder)
Date: Sun, 10 Jul 2016 14:38:14 -0600
Subject: [R] How to make the "apply" faster
In-Reply-To: <CAF8bMcZWSWOwTJ7_PwGfkP7DE03zSb=TqMhifLJ4H6eJk=3UwA@mail.gmail.com>
References: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
	<alpine.OSX.2.20.1607091428480.543@charles-berrys-macbook.local>
	<CAM9mbiDfWprrX7M-DNfux4bTuEuZcRwO3UjR_Mmdf_7mnypBdQ@mail.gmail.com>
	<CAF8bMcZWSWOwTJ7_PwGfkP7DE03zSb=TqMhifLJ4H6eJk=3UwA@mail.gmail.com>
Message-ID: <CAM9mbiCQE5aadAZ0D+s96CJUgHzR4ee9VmAhwPcF1=VHvWgFiw@mail.gmail.com>

Thanks for your response. It is faster than before but still very slow. Any
other suggestion ?
-Deb


On Sun, Jul 10, 2016 at 2:13 PM, William Dunlap <wdunlap at tibco.com> wrote:

> There is no need to test that a logical equals TRUE: 'logicalVector==TRUE'
> is the
> same as just 'logicalVector'.
>
> There is no need to convert logical vectors to numeric, since rle() works
> on both
> types.
>
> There is no need to use length(subset(x, logicalVector)) to count how many
> elements
> in logicalVector are TRUE, just use sum(logicalVector).
>
> There is no need to make a variable, 'ans', then immediately return it.
>
> Hence your
>
>     b[b == TRUE] = 1
>     y <- rle(b)
>     ans <- length(subset(y$lengths[y$values == 1], y$lengths[y$values ==
> 1] >= 2))
>     return(ans)
>
> could be replaced by
>
>     y <- rle(b)
>     sum(y$lengths[y$values] >= 2)
>
> This gives some speedup, mainly for long vectors, but I find it more
> understandable.
> E.g., if f1 is your original function and f2 has the above replacement I
> get:
>   > d <- -sin(1:10000+sqrt(1:4))
>   > system.time(for(i in 1:10000)f1(d,.3))
>      user  system elapsed
>      5.19    0.00    5.19
>   > system.time(for(i in 1:10000)f2(d,.3))
>      user  system elapsed
>      3.65    0.00    3.65
>   > c(f1(d,.3), f2(d,.3))
>   [1] 1492 1492
>   > length(d)
>   [1] 10000
>
> If it were my function, I would also get rid of the part that deals with
> the threshhold
> and direction of the inequality and tell the user to to use f(data <= 0.3)
> instead of
> f(data, .3, "below").  I would also make the spell length an argument
> instead of
> fixing it at 2.  E.g.
>
>    > f3 <- function (condition, spellLength = 2)
>    {
>        stopifnot(is.logical(condition), !anyNA(condition))
>        y <- rle(condition)
>        sum(y$lengths[y$values] >= spellLength)
>    }
>    > f3( d >= .3 )
>    [1] 1492
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Sun, Jul 10, 2016 at 11:58 AM, Debasish Pai Mazumder <pai1981 at gmail.com
> > wrote:
>
>> Hi Everyone,
>> Thanks for your help. It works. I have similar problem when I am
>> calculating number of spell.
>> I am also calculation spell (definition: period of two or more days where
>> x
>> exceeds 70) using similar way:
>>
>> *new = apply(x,c(1,2,4),FUN=function(y) {fun.spell.deb(y, 70)})*
>>
>> where fun.spell.deb.R:
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> *## Calculate spell durationfun.spell.deb <- function(data, threshold = 1,
>> direction = c("above", "below")){  #coln <- grep(weather, names(data))#
>> var <- data[,8]  if(missing(direction)) {direction <- "above"}
>> if(direction=="below") {b <- (data <= threshold)} else  {b <- (data >=
>> threshold)}    b[b==TRUE] = 1  y <-rle(b)  ans
>> <-length(subset((y$lengths[y$values==1]), (y$lengths[y$values==1])>=2))
>> return(ans)}*
>>
>> Do you have any idea how to make the "apply" faster here?
>>
>> -Deb
>>
>>
>> On Sat, Jul 9, 2016 at 3:46 PM, Charles C. Berry <ccberry at ucsd.edu>
>> wrote:
>>
>> > On Sat, 9 Jul 2016, Debasish Pai Mazumder wrote:
>> >
>> > I have 4-dimension array x(lat,lon,time,var)
>> >>
>> >> I am using "apply" to calculate over time
>> >> new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})
>> >>
>> >> This is very slow. Is there anyway make it faster?
>> >>
>> >
>> > If dim(x)[3] << prod(dim(x)[-3]),
>> >
>> > new <-  Reduce("+",lapply(1:dim(x)[3],function(z) x[,,z,]>=70))
>> >
>> > will be faster.
>> >
>> > However, if you can follow Peter Langfelder's suggestion to use rowSums,
>> > that would be best. Even using rowSums(aperm(x,c(1,2,4,3)>=70,dims=3)
>> and
>> > paying the price of aperm() might be better.
>> >
>> > Chuck
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From bogaso.christofer at gmail.com  Mon Jul 11 02:43:06 2016
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Mon, 11 Jul 2016 06:13:06 +0530
Subject: [R] Writing data onto xlsx file without cell formatting
Message-ID: <CA+dpOJ=r=3J5wAc=RvsNtWTdXA3xRNScXMCjQsRSfGvGtZaOOA@mail.gmail.com>

Hi again,

I am trying to write a data frame to an existing Excel file (xlsx)
from row 5 and column 6 of the 1st Sheet. I was going through a
previous instruction which is available here :

http://stackoverflow.com/questions/32632137/using-write-xlsx-in-r-how-to-write-in-a-specific-row-or-column-in-excel-file

However trouble is that it is modifying/removing formatting of all the
affected cells. I have predefined formatting of those cells where data
to be pasted, and I dont want to modify or remove that formatting.

Any idea if I need to pass some additional argument.

Appreciate your valuable feedback.

Thanks,


From mohdsharaiah at gmail.com  Mon Jul 11 02:57:27 2016
From: mohdsharaiah at gmail.com (mohammad alsharaiah)
Date: Mon, 11 Jul 2016 12:57:27 +1200
Subject: [R] Help- Converting the ODE equation to fuzzy logic in R
Message-ID: <CAFUx43yTLhfPnVg8RN5ncb8rE9-y7E4TBoXRrC+otQBY1KJ0_A@mail.gmail.com>

?Hi one and all ,

i have few ordinary differential equations ?(ODE's) and i need to transform
it to fuzzy logic by using any package  work with R language or any R code
that convert these equations. the fuzzy logic must provides the same or
approximate result that from ODE.


Thanks for your help.


*Mohammad*

	[[alternative HTML version deleted]]


From highstat at highstat.com  Mon Jul 11 02:58:06 2016
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Mon, 11 Jul 2016 10:58:06 +1000
Subject: [R] Course in Alice Springs: Data exploration, regression, GLM & GAM
Message-ID: <37d19754-7e97-5b34-12f9-846f18bc2051@highstat.com>

We would like to announce the following statistics course:

Course: Data exploration, regression, GLM & GAM with introduction to R
Where:  Charles Darwin University, Alice Springs, Australia
When:   1-5 August 2016

Course website: http://www.highstat.com/statscourse.htm
Course flyer: 
http://highstat.com/Courses/Flyers/Flyer2016_08AliceSprings_RGG.pdf


Kind regards,

Alain Zuur


-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email:highstat at highstat.com
URL:www.highstat.com


	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Mon Jul 11 03:44:05 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 10 Jul 2016 18:44:05 -0700
Subject: [R] Help- Converting the ODE equation to fuzzy logic in R
In-Reply-To: <CAFUx43yTLhfPnVg8RN5ncb8rE9-y7E4TBoXRrC+otQBY1KJ0_A@mail.gmail.com>
References: <CAFUx43yTLhfPnVg8RN5ncb8rE9-y7E4TBoXRrC+otQBY1KJ0_A@mail.gmail.com>
Message-ID: <CDD96172-1D52-4C9E-B656-CB62293B8680@dcn.davis.ca.us>

The only reason I can imagine for such a "need" is that you have been assigned homework and there is a no-homework policy on this list. That said, Google came up with at least one hit when I looked. 

You really ought to read the Posting Guide before posting again.
-- 
Sent from my phone. Please excuse my brevity.

On July 10, 2016 5:57:27 PM PDT, mohammad alsharaiah <mohdsharaiah at gmail.com> wrote:
>?Hi one and all ,
>
>i have few ordinary differential equations ?(ODE's) and i need to
>transform
>it to fuzzy logic by using any package  work with R language or any R
>code
>that convert these equations. the fuzzy logic must provides the same or
>approximate result that from ODE.
>
>
>Thanks for your help.
>
>
>*Mohammad*
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Mon Jul 11 03:49:51 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 11 Jul 2016 11:49:51 +1000
Subject: [R] Statistical Test
In-Reply-To: <CAOVRatxgwy9EVU+rKGTLKBmKMt+dGz=gb4tgG5qW=R7DFuEd-w@mail.gmail.com>
References: <CAOVRatxgwy9EVU+rKGTLKBmKMt+dGz=gb4tgG5qW=R7DFuEd-w@mail.gmail.com>
Message-ID: <CA+8X3fXfTg36odUgh_gWbY8==d+G-ev9r348NV8nnU5A2q-emw@mail.gmail.com>

Hi Julia,
You seem to be looking for a test for trend in proportions in the
first question. Have a look at this page:

http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R6_CategoricalDataAnalysis/R6_CategoricalDataAnalysis6.html

The second question may require GLMs using experimental condition as a
predictor and proportion of each type of error as the response. Are
the groups balanced?

Jim


On Sun, Jul 10, 2016 at 10:34 PM, Julia Edeleva
<psychexpert1992 at gmail.com> wrote:
> Dear R-community,
>
> Thanks for replying to my previous post. I would need some more help,
> thoug.
>
> I am performing statistical analysis on chidlren's accuracy rates  as a
> dependent variable and two predictor variables with two levels each (syntax
> - subject vs object; internal NP position - pre vs post).
>
> As an outcome of my study, children committed 3 types of errors. I want to
> compare whether children committed significantly more errors of one type as
> compared to the other two types, i.e. test the scale *error 1 > error 2 >
> error 3 (">" is "more than").*  Which statistical test is most appropriate?
>
> Furthermore, I want to know whether one particular type of error is more
> common in one experimental condition than in the other, i.e. test
> whether *error
> 1 in condition 1 is more common than error 1 in condition 2*.
>
> Thnx a lot
>
> Julia Edeleva
>
> *Compare different types of errors in chidren's performance. Statistical
> Test? - ResearchGate*. Available from:
> https://www.researchgate.net/post/Compare_different_types_of_errors_in_chidrens_performance_Statistical_Test
> [accessed Jul 10, 2016].
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sezenismail at gmail.com  Mon Jul 11 07:13:55 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Mon, 11 Jul 2016 08:13:55 +0300
Subject: [R] Writing data onto xlsx file without cell formatting
In-Reply-To: <CA+dpOJ=r=3J5wAc=RvsNtWTdXA3xRNScXMCjQsRSfGvGtZaOOA@mail.gmail.com>
References: <CA+dpOJ=r=3J5wAc=RvsNtWTdXA3xRNScXMCjQsRSfGvGtZaOOA@mail.gmail.com>
Message-ID: <ADBC7880-CB61-446F-BAD1-9A95599C5E6C@gmail.com>

I think, this is what you are looking for:

http://stackoverflow.com/questions/11228942/write-from-r-into-template-in-excel-while-preserving-formatting <http://stackoverflow.com/questions/11228942/write-from-r-into-template-in-excel-while-preserving-formatting>

> On 11 Jul 2016, at 03:43, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Hi again,
> 
> I am trying to write a data frame to an existing Excel file (xlsx)
> from row 5 and column 6 of the 1st Sheet. I was going through a
> previous instruction which is available here :
> 
> http://stackoverflow.com/questions/32632137/using-write-xlsx-in-r-how-to-write-in-a-specific-row-or-column-in-excel-file
> 
> However trouble is that it is modifying/removing formatting of all the
> affected cells. I have predefined formatting of those cells where data
> to be pasted, and I dont want to modify or remove that formatting.
> 
> Any idea if I need to pass some additional argument.
> 
> Appreciate your valuable feedback.
> 
> Thanks,
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From G.Maubach at weinwolf.de  Mon Jul 11 15:08:01 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Mon, 11 Jul 2016 15:08:01 +0200
Subject: [R] Choropleth: Turnover by ZipCode
Message-ID: <OF899A1BE3.93630794-ONC1257FED.0047C4BC-C1257FED.00482F96@lotus.hawesko.de>

Hi All,
Dear Readers,

I need to create a choropleth graph with turnover by zipcode. This is what 
I have so far:

# Not run (Begin)
# Install packages if needed
# install.packages(pkgs = c("maptools", "rgdal", "RColorBrewer", 
"grDevices"))
# Not run (End)

# Load libraries
library(maptools); library(rgdal); library(RColorBrewer); 
library(grDevices)

# Configuration
# Adjust if needed!
file_path <- file.path("C:", "temp")

# Read data 
# Source: http://arnulf.us/PLZ
url <- "http://www.metaspatial.net/download/plz.tar.gz"
file_name_gzip <- basename(url)
file_name_extract <- "post_pl.shp"

download.file(url, file.path(file_path, file_name_gzip))

untar(tarfile = file.path(file_path, file_name_gzip),
      compressed = "gzip",
      exdir = file_path)

# Dataset
# I have the data for all zipcodes available in my region
ds_temp <-
  structure(
    list(
      ZipCode = c(1099, 10178, 13125, 21406, 32429, 41569),
      Sales = c(4, 2, 9, 5, 7, 3),
      Revenue = c(12, 9, 100, 80, 90,
                  25)
    ),
    .Names = c("ZipCode", "Sales", "Revenue"),
    row.names = c(NA,
                  6L),
    class = "data.frame"
  )
print(ds_temp)

# Prepare graphic
file_name_pdf <- file.path(file_path, "sales-and-revenue-by-zipcodes.pdf")
cairo_pdf(bg = "grey98", file_name_pdf, width = 16, height = 9)

y <- readShapeSpatial(file.path(file_path, file_name_extract),
                      proj4string = CRS("+proj=longlat"))
x <- spTransform(y,CRS=CRS("+proj=merc"))

# How do I need to change this line?
# Needs to be replaced by turnover from ds_temp
color <- sample(1:7, length(x), replace=T) 

# Create graphic
plot(x, 
     col = brewer.pal(7, "Oranges")[color],
     border = F)  # How to I tell R to plot turnover from ds_temp?

# Title
mtext(
  "Turnover by Zipcodes",
  side = 3,
  line = -4,
  adj = 0,
  cex = 1.7
)

# Write to disc
dev.off()

# Cleanup
rm("ds_temp", "color", "file_name_extract",
   "file_name_gzip", "file_name_pdf", "file_path",
   "url", "x", "y")
unlink(file.path(file_path, "plz.tar.gz"))
unlink(file.path(file_path, "post_pl.dbf"))
unlink(file.path(file_path, "post_pl.shp"))
unlink(file.path(file_path, "post_pl.shx"))

# unlink(file.path(file_path, "sales-and-revenue-by-zipcodes.pdf"))

What do I need to do to color the amount of turnover or the frequencies of 
sales from the ds_temp dataset in the graph?

Kind regards

Georg Maubach


From bjpmodi2016 at gmail.com  Mon Jul 11 16:38:01 2016
From: bjpmodi2016 at gmail.com (Narendra Modi)
Date: Mon, 11 Jul 2016 09:38:01 -0500
Subject: [R] Nloptr vs Excel GRG optimization result
Message-ID: <CAPq=xQBxDGSBwJ19GnxNTdQLSv9MESmw8NsAjQdEgMnKKyeVkQ@mail.gmail.com>

Hi All,
For a non-linear minimization optimization problem that I have, I am
getting better objective function value in Excel(15) as compared to
nloptr (73).

the nloptr is setup as:

opts = list("algorithm"="NLOPT_LN_COBYLA",
            "xtol_rel"=1.0e-8, "maxeval"= 10000)
lb = vector("numeric",length= length(my.data.var))

result <- nloptr(my.data.var,eval_f = Error.func,lb=lb,
                 ub =
c(Inf,1,1,1,1,1,Inf,1,1,1,1,1,Inf,1,1,1,1,1,Inf,1,1,1,1,1),eval_g_ineq=constraint.func,opts
= opts)


As observed even with 10000 as maximum evaluations, the objective
function is way off as compared to Excel's GRG which solved it in 200
iterations.

Is there a way to improve the objective function value from nloptr? OR
is there any excel's GRG equivalent package in R.

Thanks for your time!

PD


From profjcnash at gmail.com  Mon Jul 11 17:08:59 2016
From: profjcnash at gmail.com (ProfJCNash)
Date: Mon, 11 Jul 2016 11:08:59 -0400
Subject: [R] Nloptr vs Excel GRG optimization result
In-Reply-To: <CAPq=xQBxDGSBwJ19GnxNTdQLSv9MESmw8NsAjQdEgMnKKyeVkQ@mail.gmail.com>
References: <CAPq=xQBxDGSBwJ19GnxNTdQLSv9MESmw8NsAjQdEgMnKKyeVkQ@mail.gmail.com>
Message-ID: <5783B68B.2020005@gmail.com>

Note the "reproducible code" directive. We cannot check your calculations.

It would not surprise me if the objective for Excel was really, really
good BUT the parameters were out of bounds or violated other constraints.

At the EUSPRIG meeting in Klagenfurt in 2004 I sat next to Dan Fijlstra
of Frontline Systems at dinner. He complained that FS had offered
Microsoft a bug fix for GRG (they supply for-money improved versions as
well as the "free" solver) and were told it wasn't wanted. Sigh.

On the other hand, I think the interface Excel provides is nicely
designed for small to medium problems.

You may also want to be very careful with the call to nloptr. It can be
tricky, rendering its results more or less meaningless.

JN


On 16-07-11 10:38 AM, Narendra Modi wrote:
> Hi All,
> For a non-linear minimization optimization problem that I have, I am
> getting better objective function value in Excel(15) as compared to
> nloptr (73).
> 
> the nloptr is setup as:
> 
> opts = list("algorithm"="NLOPT_LN_COBYLA",
>             "xtol_rel"=1.0e-8, "maxeval"= 10000)
> lb = vector("numeric",length= length(my.data.var))
> 
> result <- nloptr(my.data.var,eval_f = Error.func,lb=lb,
>                  ub =
> c(Inf,1,1,1,1,1,Inf,1,1,1,1,1,Inf,1,1,1,1,1,Inf,1,1,1,1,1),eval_g_ineq=constraint.func,opts
> = opts)
> 
> 
> As observed even with 10000 as maximum evaluations, the objective
> function is way off as compared to Excel's GRG which solved it in 200
> iterations.
> 
> Is there a way to improve the objective function value from nloptr? OR
> is there any excel's GRG equivalent package in R.
> 
> Thanks for your time!
> 
> PD
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From hippolionforlily at gmail.com  Mon Jul 11 11:51:33 2016
From: hippolionforlily at gmail.com (pan yang)
Date: Mon, 11 Jul 2016 17:51:33 +0800
Subject: [R] gsl package installation problem: gsl-config not found (even
 though gsl has been installed)
Message-ID: <CACD_QsVO2qx_e5Sicv71WFS5-T2HrbN8OAxZ9xckHFhkyOY5UQ@mail.gmail.com>

Dear R-community,

I faced this problem when I tried to install the R gsl wrapper in my
university's HPC cluster. Before installing the R wrapper, I have already
installed the 'gsl' version 2.1 in my own directory '/home/pyangac/dev'. I
have tested the gsl installation by compiling the simple program in the gsl
manual (
https://www.gnu.org/software/gsl/manual/html_node/An-Example-Program.html#An-Example-Program
).

After this, I followed the steps in page 30 of the R package's document (
https://cran.r-project.org/web/packages/gsl/gsl.pdf) to install the package
(I didn't do step 3 because I really don't know how to do it, and by
looking at the solutions posted online, nobody had done step 3):

$ /home/pyangac/dev/bin/gsl-config --libs
-L/home/pyangac/dev/lib -lgsl -lgslcblas -lm

$ /home/pyangac/dev/bin/gsl-config --cflags
-I/home/pyangac/dev/include

$ LDFLAGS="-L/home/pyangac/dev/lib -lgsl -lgslcblas -lm"; export LDFLAGS
$ CPPFALGS="-I/home/pyangac/dev/include"; export CPPFLAGS

$ R CMD INSTALL '/home/pyangac/gsl_1.9-10.1.tar.gz'
* installing to library '/home/pyangac/R_libs'
* installing *source* package 'gsl' ...
** package 'gsl' successfully unpacked and MD5 sums checked
checking for gsl-config... no
configure: error: gsl-config not found, is GSL installed?
ERROR: configuration failed for package 'gsl'
* removing '/home/pyangac/R_libs/gsl'

Does anyone know why this is happening? Is it because I have not go through
step 3?

Your generous help and assistance will be highly appreciated.

Best regards,
Pan

	[[alternative HTML version deleted]]


From hippolionforlily at gmail.com  Mon Jul 11 13:01:03 2016
From: hippolionforlily at gmail.com (pan yang)
Date: Mon, 11 Jul 2016 19:01:03 +0800
Subject: [R] Problem when installing Rmpi package in HPC cluster
Message-ID: <CACD_QsWasF3A9rforJGL8xG8Nu8_XUvne7WuX9BAUtQpN4b8Qg@mail.gmail.com>

Dear R community,

I faced this problem when I am installing the Rmpi in our university's
linux86-64 cluster:

> install.packages('Rmpi',repos='http://cran.r-project.org
',configure.args=c(
+ '--with-Rmpi-include=/usr/mpi/gcc/openmpi-1.8.2/include/',
+ '--with-Rmpi-libpath=/usr/mpi/gcc/openmpi-1.8.2/lib64/',
+ '--with-Rmpi-type=OPENMPI'))
Installing package into ?d1/pyangac/R_lilbs?
(as ?ib?is unspecified)
trying URL 'http://cran.r-project.org/src/contrib/Rmpi_0.6-6.tar.gz'
Content type 'application/x-gzip' length 105181 bytes (102 Kb)
opened URL
==================================================
downloaded 102 Kb

* installing *source* package ?mpi?...
** package ?mpi?successfully unpacked and MD5 sums checked
checking for openpty in -lutil... no
checking for main in -lpthread... no
configure: creating ./config.status
config.status: creating src/Makevars
** libs
gcc -m64 -std=gnu99 -I/usr/include/R -DNDEBUG -DPACKAGE_NAME=\"\"
-DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\"
-DPACKAGE_BUGREPORT=\"\" -DPACKAGE_URL=\"\"
-I/usr/mpi/gcc/openmpi-1.8.2/include/  -DMPI2 -DOPENMPI
-I/usr/local/include    -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2
-fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64
-mtune=generic  -c Rmpi.c -o Rmpi.o
gcc -m64 -std=gnu99 -I/usr/include/R -DNDEBUG -DPACKAGE_NAME=\"\"
-DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\"
-DPACKAGE_BUGREPORT=\"\" -DPACKAGE_URL=\"\"
-I/usr/mpi/gcc/openmpi-1.8.2/include/  -DMPI2 -DOPENMPI
-I/usr/local/include    -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2
-fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64
-mtune=generic  -c conversion.c -o conversion.o
gcc -m64 -std=gnu99 -I/usr/include/R -DNDEBUG -DPACKAGE_NAME=\"\"
-DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\"
-DPACKAGE_BUGREPORT=\"\" -DPACKAGE_URL=\"\"
-I/usr/mpi/gcc/openmpi-1.8.2/include/  -DMPI2 -DOPENMPI
-I/usr/local/include    -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2
-fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64
-mtune=generic  -c internal.c -o internal.o
gcc -m64 -std=gnu99 -shared -L/usr/local/lib64 -o Rmpi.so Rmpi.o
conversion.o internal.o -L/usr/mpi/gcc/openmpi-1.8.2/lib64/ -lmpi
-L/usr/lib64/R/lib -lR
installing to /d1/pyangac/R_lilbs/Rmpi/libs
** R
** demo
** inst
** preparing package for lazy loading
** help
*** installing help indices
  converting help for package ?mpi?
    finding HTML links ... done
    hosts                                   html
    internal                                html
    mpi.abort                               html
    mpi.apply                               html
    mpi.barrier                             html
    mpi.bcast                               html
    mpi.bcast.Robj                          html
    mpi.bcast.cmd                           html
    mpi.cart.coords                         html
    mpi.cart.create                         html
    mpi.cart.get                            html
    mpi.cart.rank                           html
    mpi.cart.shift                          html
    mpi.cartdim.get                         html
    mpi.comm                                html
    mpi.comm.disconnect                     html
    mpi.comm.free                           html
    mpi.comm.inter                          html
    mpi.comm.set.errhandler                 html
    mpi.comm.spawn                          html
    mpi.const                               html
    mpi.dims.create                         html
    mpi.exit                                html
    mpi.finalize                            html
    mpi.gather                              html
    mpi.gather.Robj                         html
    mpi.get.count                           html
    mpi.get.processor.name                  html
    mpi.get.sourcetag                       html
    mpi.iapply                              html
    mpi.info                                html
    mpi.intercomm.merge                     html
    mpi.parSim                              html
    mpi.parapply                            html
    mpi.probe                               html
    mpi.realloc                             html
    mpi.reduce                              html
    mpi.remote.exec                         html
    mpi.scatter                             html
    mpi.scatter.Robj                        html
    mpi.send                                html
    mpi.send.Robj                           html
    mpi.sendrecv                            html
    mpi.setup.rng                           html
    mpi.spawn.Rslaves                       html
    mpi.universe.size                       html
    mpi.wait                                html
** building package indices
** testing if installed package can be loaded
--------------------------------------------------------------------------
Error obtaining unique transport key from ORTE
(orte_precondition_transports not present in
the environment).

  Local host: login-0

--------------------------------------------------------------------------
[login-0.local:18260] Error: mtl_mxm.c:287 - ompi_mtl_mxm_module_init()
Failed to generate jobid
* DONE (Rmpi)

The downloaded source packages are in
        ?tmp/RtmpKdDI0H/downloaded_packages?
(Please forgive me about the unreadable codes, they just came out with no
reason...)

Before that I have set export
LD_LIBRARY_PATH=/usr/mpi/gcc/openmpi-1.8.2/lib64
and                             export LD_PRELOAD=/usr/lib64/libutil.so

Somehow the Rmpi cannot be loaded even though it has been installed
successfully. Can
anyone kindly give me some hint on what should be done?

Thanks a lot.
Pan

	[[alternative HTML version deleted]]


From Keith.Goldfeld at nyumc.org  Mon Jul 11 14:49:45 2016
From: Keith.Goldfeld at nyumc.org (Goldfeld, Keith)
Date: Mon, 11 Jul 2016 12:49:45 -0000
Subject: [R] [R-pkgs] New package: simstudy
Message-ID: <3C22D6B3-96D1-4C1F-979C-A91EDAB7FFD9@nyumc.org>

Greetings ?

A new package ?simstudy? is now available on CRAN. What started as a small number of functions that enabled me to quickly generate simple data sets for teaching and power/sample size calculations has grown into a more robust set of tools that allows users to simulate more complex data sets in order to explore modeling techniques or better understand data generating processes. The user specifies a set of relationships between covariates in table form (the table can be built interactively or created externally as a csv file), and generates data based on these specifications. The final data sets can represent data from randomized control trials, observed (non-randomized) studies, repeated measure (longitudinal) designs, and cluster randomized trials. Missingness can be generated using various mechanisms (MCAR, MAR, NMAR). Currently, data can be generated from normal/Gaussian, binary, Poisson, truncated Poisson, Gamma, and uniform distributions. Survival data can also be generated.

I will be adding functionality over time, and will be particularly interested in knowing what userRs would be interested in having me add. I look forward to hearing your comments.

- Keith


Keith Goldfeld
Department of Population Health
School of Medicine, New York University
227 East 30th Street, 6th Floor
New York, NY  10016


------------------------------------------------------------
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain information that is proprietary, confidential, and exempt from disclosure under applicable law. Any unauthorized review, use, disclosure, or distribution is prohibited. If you have received this email in error please notify the sender by return email and delete the original message. Please note, the recipient should check this email and any attachments for the presence of viruses. The organization accepts no liability for any damage caused by any virus transmitted by this email.
=================================

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From andreas.hill at usys.ethz.ch  Sat Jul  9 13:38:53 2016
From: andreas.hill at usys.ethz.ch (Hill  Andreas)
Date: Sat, 9 Jul 2016 11:38:53 +0000
Subject: [R] [R-pkgs] New package 'forestinventory: Design-Based Global and
 Small-Area Estimations for Multiphase Forest Inventories'
Message-ID: <0A4219CAB0E2EC4B98F9C7C2F51CC838238384B3@MBX115.d.ethz.ch>

Dear R users,

We are happy to announce that the R package 'forestinventory: Design-Based Global and Small-Area Estimations for Multiphase Forest Inventories'
is now on CRAN (https://cran.r-project.org/web/packages/forestinventory/).

The aim of our package is to provide global- and smallarea estimators for twophase and threephase forest inventories under simple and cluster sampling.
The methods can be used for double sampling for stratification (i.e. classical ANOVA model as prediction model),
double sampling for regression (i.e. multiple regression model as prediction model) and double sampling for regression within strata
(i.e. classical ANCOVA model as prediction model). The implemented estimators have been developed by Daniel Mandallaz at ETH Zurich
and their implementation has been optimized according to current challenges and needs of multiphase inventories (e.g. use of remote sensing / geodata).

Relevant features:

-          Provides point - and variance estimators for 64 inventory scenarios in total,

according to sample design (simple and cluster sampling, two- and threephase inventory designs) and availability of auxiliary information

-          Allows for the computation of Confidence Intervals for the point estimates

-          Completes the range of the already published estimators for threephase small area estimations

-          Also includes estimators for onephase inventories (only using terrestrial inventory data)

Upcoming activities:
We will publish a vignette illustrating the use of the various estimators as soon as possible.


Best,

Andreas Hill
Alexander Massey


-------------------------------------------
ETH Z?rich
Andreas Hill
Forstl. Ingenieurwesen, Heinimann
CHN K 75.1
Universit?tstrasse 16
8092 Z?rich

Telefon: +41 44 632 32 36
Andreas.hill at usys.ethz.ch

	[[alternative HTML version deleted]]

-------------- next part --------------
_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From marielaure.delignettemuller at vetagro-sup.fr  Thu Jul  7 17:12:17 2016
From: marielaure.delignettemuller at vetagro-sup.fr (Marie Laure Delignette-Muller)
Date: Thu, 7 Jul 2016 17:12:17 +0200
Subject: [R] [R-pkgs] new version of the package fitdistrplus
Message-ID: <CANpor5BHktAcQF_74vevaDYx=q8NXhJ=qkED7J9No5yvgNzqng@mail.gmail.com>

We are pleased to announce your a new version of fitdistrplus  (
https://cran.r-project.org/package=fitdistrplus). Among the new features of
the package (https://cran.r-project.org/web/packages/fitdistrplus/NEWS), a
FAQ vignette is now available (
https://cran.r-project.org/web/packages/fitdistrplus/vignettes/FAQ.html).
We will be delighted to update it with new questions sent by users. Do not
hesitate to send us any comment on this vignette or on the package in
general.

Best regards
Marie Laure DELIGNETTE-MULLER and Christophe DUTANG

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From xinxu.199301 at gmail.com  Mon Jul 11 15:00:23 2016
From: xinxu.199301 at gmail.com (Xin Shelia)
Date: Mon, 11 Jul 2016 09:00:23 -0400
Subject: [R]  package flexmix: models for ordinal data?
Message-ID: <77808A95-D7A5-409F-A4D5-54667F8AA9C2@gmail.com>

Hi Klaus,

I saw you post ?package flexmix: models for ordinal data?? on grokbase. I was wondering did you find the answer? Or what package did you use at the end? I really appreciate if you would like to help me. 

Best, 
Xin 

From stn021 at gmail.com  Mon Jul 11 16:28:41 2016
From: stn021 at gmail.com (stn021)
Date: Mon, 11 Jul 2016 16:28:41 +0200
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
Message-ID: <CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>

Hello,

thank you for the replies. Sorry about the html-email, I forgot.
Should be OK with this email.


Don't be fooled be the apparent simplicity of the problem. I have
tried to reduce it to only a single relatively simple question.

The idea here is to model cooperation of two persons. The model is
about one specific aspect of that cooperation, namely that two persons
with similar abilities may be able to produce better results that two
very different persons.

That is only one part of the model with other parts modeling for
example the fact that of course two persons with a higher degree of
ability will produce better results per se.


It is not classic regression with factors. That can be easily done by
something like lm( y ~ (p1-p2)^2 ).

This expands to lm( y ~ p1^2 - 2*p1*p2 + p2^2 ). This contains a
multiplicagtions and for lm() this implies interactions between the
factor-levels and produces one parameter for each combination of
factor-levels that occurs in the data. That is not what the question
is about.

Also p1 and p2 are different levels of the same factor, while for lm()
it would be two different factors with different levels.


As for the sensical part: this has a real world application therefore
it makes sense.

Also it is not so difficult to solve with non-linear optimization. I
was hoping to be able to use R for that purpose because then the
results could easily be checked with statistical tests.

So my question is not "how to solve" but "how to solve with R".


As for the excess degrees of freedom, in real observations there would
of course be added noise due to either random variations or factors
not included in the model. So to generate a more reality-conforming
example I could add some random normal-distributed noise to the
dependent variable y. I previously left that part out because to me it
did not seem relevant.


Would you like me to make a complete example dataset with more records
and noise ?


The answer I look for would be the numerical values of the
factor-levels and numerical values for the multiplier (f) and the
offset (o), with p1 and p2 given as names (here: persons) and y given
as some level of achievement they reach by cooperating.

y = f * ( o - ( p1 - p2 )^2 )

Is that what you meant by "answer" ?


THX
stefan




2016-07-10 2:27 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>
> I have seen less sensical questions.
>
> It would be nice if the example were a bit more complete (as in it should have excess degrees of freedom and an answer) and less like a homework problem (which are off topic here). It would of course also be helpful if the OP were to conform to the Posting Guide, particularly in respect to using plain text email.
>
> It looks like the kind of nonlinear optimization problem that evolutionary algorithms are often applied to. It doesn't look (to me) like a typical problem that factors get applied to in formulas though, because multiple instances of the same factor variable are present.
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 9, 2016 4:59:30 PM PDT, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> >On 09/07/16 20:52, stn021 wrote:
> >> Hello,
> >>
> >> I would like to analyse a model like this:
> >>
> >> y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )
> >>
> >> x1 and x2 are not continuous variables but factors, so the
> >observation
> >> contain the level.
> >> Its numerical value is unknown and is to be estimated with the model.
> >>
> >>
> >> The observations look like this:
> >>
> >> y        x1     x2
> >> 0.96  Alice  Bob
> >> 0.84  Alice  Charlie
> >> 0.96  Bob   Charlie
> >> 0.64  Dave Alice
> >> etc.
> >>
> >> Each person has a numerical value. Here for example Alice = 0.2 and
> >Bob =
> >> 0.4
> >>
> >> Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.
> >>
> >> How can this be done in R ?
> >
> >
> >This question makes about as little sense as it is possible to imagine.
> >
> >cheers,
> >
> >Rolf Turner
>


From cristina.cametti at gmail.com  Mon Jul 11 09:59:15 2016
From: cristina.cametti at gmail.com (Cristina Cametti)
Date: Mon, 11 Jul 2016 09:59:15 +0200
Subject: [R] r and country fixed effect modeling
Message-ID: <2049EA38-7C6F-4E46-8236-F558A81C6667@gmail.com>

Dear all,

I am having problem finding a reliable code for my country fixed effects model.
I have 21 countries in my database and individuals nested within them. I am running a multilevel analysis first, and then I am using the fixed effects approach to check the robustness of my findings. At this point, I need to include in my model some cross-level interaction effects between the country level factors and individual level variables. However, I am not sure if my r code is right:

model1<-lm(safety ~ mixed_neigh + ethnic_neigh + age + gndr + eduyrs + domicil + partner + 
           tvpol + income + victim + trust + trustXprison_pop + trustXforeign_pop + victimXprison_pop + 
           victimXforeign_pop + mixed_neighXprison_pop + mixed_neighXforeign_pop + ethnic_neighXprison_pop + 
           ethnic_neighXforeign_pop + factor(cntry)-1, data=mydata)
As you can see, the interaction are between a individual level variable (such as trst, victim, mixed_neigh and ethnic_neigh) and two country level variables (prison population and foreign population). The code runs fine, so I don?t have warning messages and it is all spelled correctly. I just want to know if you think that this is the right code to do a fixed effect analysis (only cross sections and not cross time). Indeed, I I used the lm and not ppm because my dataset is from only one year. Do you think it is correct?

Thank you very much for your attention,

Cristina 


	[[alternative HTML version deleted]]


From cristina.cametti at gmail.com  Mon Jul 11 10:00:49 2016
From: cristina.cametti at gmail.com (Cristina Cametti)
Date: Mon, 11 Jul 2016 10:00:49 +0200
Subject: [R] r and fixed effect modeling
Message-ID: <43186573-0DC3-45D9-A8E9-10A4B56F08FB@yahoo.it>

Dear all,

I am having problem finding a reliable code for my country fixed effects model.
I have 21 countries in my database and individuals nested within them. I am running a multilevel analysis first, and then I am using the fixed effects approach to check the robustness of my findings. At this point, I need to include in my model some cross-level interaction effects between the country level factors and individual level variables. However, I am not sure if my r code is right:

model1<-lm(safety ~ mixed_neigh + ethnic_neigh + age + gndr + eduyrs + domicil + partner + 
           tvpol + income + victim + trust + trustXprison_pop + trustXforeign_pop + victimXprison_pop + 
           victimXforeign_pop + mixed_neighXprison_pop + mixed_neighXforeign_pop + ethnic_neighXprison_pop + 
           ethnic_neighXforeign_pop + factor(cntry)-1, data=mydata)
As you can see, the interaction are between a individual level variable (such as trst, victim, mixed_neigh and ethnic_neigh) and two country level variables (prison population and foreign population). The code runs fine, so I don?t have warning messages and it is all spelled correctly. I just want to know if you think that this is the right code to do a fixed effect analysis (only cross sections and not cross time). Indeed, I I used the lm and not ppm because my dataset is from only one year. Do you think it is correct?

Thank you very much for your attention,

Cristina 
	[[alternative HTML version deleted]]


From neerajdhanraj at gmail.com  Sat Jul  2 22:09:23 2016
From: neerajdhanraj at gmail.com (Neeraj Dhanraj)
Date: Sun, 3 Jul 2016 01:39:23 +0530
Subject: [R] [R-pkgs] Seasonal PSF - Time Series Forecasting algorithm
Message-ID: <CAC58_Y=nCh6nd19C_-1KsYpW12R+GMYrkDQV+qWeO5QmYU48qg@mail.gmail.com>

Hi friends,
If you are interested in univariate time series data predictions, have a
look in PSF algorithm and it's R Packages available at :
CRAN: https://cran.r-project.org/web/packages/PSF/index.html

GitHub: https://github.com/neerajdhanraj/PSF

How to use:
https://www.researchgate.net/publication/304131481_PSF_Introduction_to_R_Package_for_Pattern_Sequence_Based_Forecasting_Algorithm

and

https://www.researchgate.net/publication/304580701_Introduction_of_seasonality_concept_in_PSF_algorithm_to_improve_univariate_time_series_predictions

For further details contact me at: http://www.neerajbokde.com?

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From wdunlap at tibco.com  Mon Jul 11 18:42:54 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 11 Jul 2016 09:42:54 -0700
Subject: [R] How to make the "apply" faster
In-Reply-To: <CAM9mbiCQE5aadAZ0D+s96CJUgHzR4ee9VmAhwPcF1=VHvWgFiw@mail.gmail.com>
References: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
	<alpine.OSX.2.20.1607091428480.543@charles-berrys-macbook.local>
	<CAM9mbiDfWprrX7M-DNfux4bTuEuZcRwO3UjR_Mmdf_7mnypBdQ@mail.gmail.com>
	<CAF8bMcZWSWOwTJ7_PwGfkP7DE03zSb=TqMhifLJ4H6eJk=3UwA@mail.gmail.com>
	<CAM9mbiCQE5aadAZ0D+s96CJUgHzR4ee9VmAhwPcF1=VHvWgFiw@mail.gmail.com>
Message-ID: <CAF8bMcYqLpQAHc7EONNPMi6C0KJGso7bU0_A4SeS8oW+G7f6uQ@mail.gmail.com>

How fast is fast enough and what size and shape is your dataset
(show the output of str(yourData))?  You will get the fastest execution
time by using C or C++ or Fortran, but you will want to parameterize
the problem well enough that you can amortize the time it takes to
write the code over many problems.

Peter Langflder's suggested you use aperm followed by colSums
for your earlier problem.   For this one you can use aperm followed
by filter (to identify the runs of a given minimum length, column by
column) and then use colSums (to count the number of runs filter
identifies in each column).  E.g.,

  f4 <- function (condition, spellLength = 2)
  {
      # obscure way to count runs of length>spellLength
      # in 3'rd dimension of logical array 'condition'.
      stopifnot(is.logical(condition), !anyNA(condition))
      coef <- c(-1, rep(1, spellLength))
      d <- dim(condition)
      dn <- dimnames(condition)
      tmp <- array(aperm(condition * 2 - 1, c(3, 1, 2, 4)), c(d[3],
          prod(d[-3])))
      fTmp <- filter(rbind(tmp, -1), coef, sides = 1)
     sfTmp <- colSums(fTmp == spellLength + 1, na.rm = TRUE)
     array(sfTmp, dim = d[-3], dimnames = dn[-3])
  }

f4(x>=1) is not a great deal faster (48 s. vs. 67 s.) than
apply(x>=1, c(1,2,4), FUN=f3) where f3 is
  f3 <- function (condition, spellLength = 2)
  {
      stopifnot(is.logical(condition), !anyNA(condition))
      y <- rle(condition)
      sum(y$lengths[y$values] >= spellLength)
  }
and where x has dimensions c(101,107,17,103).

The relative speed will depend on the size and shape of your dataset,
so show the output of str(yourData).





Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sun, Jul 10, 2016 at 1:38 PM, Debasish Pai Mazumder <pai1981 at gmail.com>
wrote:

> Thanks for your response. It is faster than before but still very slow.
> Any other suggestion ?
> -Deb
>
>
> On Sun, Jul 10, 2016 at 2:13 PM, William Dunlap <wdunlap at tibco.com> wrote:
>
>> There is no need to test that a logical equals TRUE:
>> 'logicalVector==TRUE' is the
>> same as just 'logicalVector'.
>>
>> There is no need to convert logical vectors to numeric, since rle() works
>> on both
>> types.
>>
>> There is no need to use length(subset(x, logicalVector)) to count how
>> many elements
>> in logicalVector are TRUE, just use sum(logicalVector).
>>
>> There is no need to make a variable, 'ans', then immediately return it.
>>
>> Hence your
>>
>>     b[b == TRUE] = 1
>>     y <- rle(b)
>>     ans <- length(subset(y$lengths[y$values == 1], y$lengths[y$values ==
>> 1] >= 2))
>>     return(ans)
>>
>> could be replaced by
>>
>>     y <- rle(b)
>>     sum(y$lengths[y$values] >= 2)
>>
>> This gives some speedup, mainly for long vectors, but I find it more
>> understandable.
>> E.g., if f1 is your original function and f2 has the above replacement I
>> get:
>>   > d <- -sin(1:10000+sqrt(1:4))
>>   > system.time(for(i in 1:10000)f1(d,.3))
>>      user  system elapsed
>>      5.19    0.00    5.19
>>   > system.time(for(i in 1:10000)f2(d,.3))
>>      user  system elapsed
>>      3.65    0.00    3.65
>>   > c(f1(d,.3), f2(d,.3))
>>   [1] 1492 1492
>>   > length(d)
>>   [1] 10000
>>
>> If it were my function, I would also get rid of the part that deals with
>> the threshhold
>> and direction of the inequality and tell the user to to use f(data <=
>> 0.3) instead of
>> f(data, .3, "below").  I would also make the spell length an argument
>> instead of
>> fixing it at 2.  E.g.
>>
>>    > f3 <- function (condition, spellLength = 2)
>>    {
>>        stopifnot(is.logical(condition), !anyNA(condition))
>>        y <- rle(condition)
>>        sum(y$lengths[y$values] >= spellLength)
>>    }
>>    > f3( d >= .3 )
>>    [1] 1492
>>
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Sun, Jul 10, 2016 at 11:58 AM, Debasish Pai Mazumder <
>> pai1981 at gmail.com> wrote:
>>
>>> Hi Everyone,
>>> Thanks for your help. It works. I have similar problem when I am
>>> calculating number of spell.
>>> I am also calculation spell (definition: period of two or more days
>>> where x
>>> exceeds 70) using similar way:
>>>
>>> *new = apply(x,c(1,2,4),FUN=function(y) {fun.spell.deb(y, 70)})*
>>>
>>> where fun.spell.deb.R:
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> *## Calculate spell durationfun.spell.deb <- function(data, threshold =
>>> 1,
>>> direction = c("above", "below")){  #coln <- grep(weather, names(data))#
>>> var <- data[,8]  if(missing(direction)) {direction <- "above"}
>>> if(direction=="below") {b <- (data <= threshold)} else  {b <- (data >=
>>> threshold)}    b[b==TRUE] = 1  y <-rle(b)  ans
>>> <-length(subset((y$lengths[y$values==1]), (y$lengths[y$values==1])>=2))
>>> return(ans)}*
>>>
>>> Do you have any idea how to make the "apply" faster here?
>>>
>>> -Deb
>>>
>>>
>>> On Sat, Jul 9, 2016 at 3:46 PM, Charles C. Berry <ccberry at ucsd.edu>
>>> wrote:
>>>
>>> > On Sat, 9 Jul 2016, Debasish Pai Mazumder wrote:
>>> >
>>> > I have 4-dimension array x(lat,lon,time,var)
>>> >>
>>> >> I am using "apply" to calculate over time
>>> >> new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})
>>> >>
>>> >> This is very slow. Is there anyway make it faster?
>>> >>
>>> >
>>> > If dim(x)[3] << prod(dim(x)[-3]),
>>> >
>>> > new <-  Reduce("+",lapply(1:dim(x)[3],function(z) x[,,z,]>=70))
>>> >
>>> > will be faster.
>>> >
>>> > However, if you can follow Peter Langfelder's suggestion to use
>>> rowSums,
>>> > that would be best. Even using rowSums(aperm(x,c(1,2,4,3)>=70,dims=3)
>>> and
>>> > paying the price of aperm() might be better.
>>> >
>>> > Chuck
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Jul 11 18:55:50 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 11 Jul 2016 09:55:50 -0700
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
	<CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
Message-ID: <EA0C0F24-0EB3-48F7-97B1-01ECC2B603A0@comcast.net>


> On Jul 11, 2016, at 7:28 AM, stn021 <stn021 at gmail.com> wrote:
> 
> Hello,
> 
> thank you for the replies. Sorry about the html-email, I forgot.
> Should be OK with this email.
> 
> 
> Don't be fooled be the apparent simplicity of the problem. I have
> tried to reduce it to only a single relatively simple question.

It would be useful to know whether this is a design effort and the data is not yet recorded or this is an analysis effort for data that is "in the can".
> 
> The idea here is to model cooperation of two persons. The model is
> about one specific aspect of that cooperation, namely that two persons
> with similar abilities may be able to produce better results that two
> very different persons.
> 
> That is only one part of the model with other parts modeling for
> example the fact that of course two persons with a higher degree of
> ability will produce better results per se.
> 
> 
> It is not classic regression with factors. That can be easily done by
> something like lm( y ~ (p1-p2)^2 ).

No. The caret "^" is an interaction operator in the formula context (not a power operator) and the minus sign causes variable removal.

Read:

?formula

If you want to create a calculated value that is the squared difference of two variables, then you need to do it either with `I` or in the dataframe before submission to the regression function.


> 
> This expands to lm( y ~ p1^2 - 2*p1*p2 + p2^2 ).

Used in a formula, p1^2 is exactly equal to p1.


> This contains a
> multiplicagtions and for lm() this implies interactions between the
> factor-levels and produces one parameter for each combination of
> factor-levels that occurs in the data. That is not what the question
> is about.
> 
> Also p1 and p2 are different levels of the same factor, while for lm()
> it would be two different factors with different levels.

Given your apparent lack of knowledge about R's formula syntax, we are also now unclear if you are using the word "factor" in the colloquial sense or as a technical term for discrete (factor) variables in R. What kind of values can p1 and p2 take?


> As for the sensical part: this has a real world application therefore
> it makes sense.
> 
> Also it is not so difficult to solve with non-linear optimization. I
> was hoping to be able to use R for that purpose because then the
> results could easily be checked with statistical tests.
> 
> So my question is not "how to solve" but "how to solve with R".
> 
> 
> As for the excess degrees of freedom, in real observations there would
> of course be added noise due to either random variations or factors
> not included in the model. So to generate a more reality-conforming
> example I could add some random normal-distributed noise to the
> dependent variable y. I previously left that part out because to me it
> did not seem relevant.

Knowing the nature of the outcome variable is generally important in statistical design.
> 
> 
> Would you like me to make a complete example dataset with more records
> and noise ?

Yes. And preferably do it with R code.

> 
> The answer I look for would be the numerical values of the
> factor-levels and numerical values for the multiplier (f) and the
> offset (o), with p1 and p2 given as names (here: persons) and y given
> as some level of achievement they reach by cooperating.
> 
> y = f * ( o - ( p1 - p2 )^2 )
> 
> Is that what you meant by "answer" ?

Not really. We would expect to see some data, at least dummy data, in a form that could be used for testing and demonstration.  The nature of "f" is particularly unclear (in large part because the science or "reality" is not described.)  Is it a function?  The "o" is probably going to be returned as an "(Intercept)". You started out with `lm` which would have little to do with non-linear optimization. You then said it "would not be so difficult" to do non-linear optimization of "something" which was not really specified with any substance. Without data and code it still reads as a salad of fragments of terminology lacking reference to a well-described scientific substrate. 

An "answer" would be:

Describe an experiment or a well designed set of observations with a specific outcome. Describe the hypotheses. Present code or data with a desired analysis plan. Ask for problems in R coding.



An off-topic question would be:

Help me design my psychology class project.


-- 
David.


> 
> 
> THX
> stefan
> 
> 
> 
> 
> 2016-07-10 2:27 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>> 
>> I have seen less sensical questions.
>> 
>> It would be nice if the example were a bit more complete (as in it should have excess degrees of freedom and an answer) and less like a homework problem (which are off topic here). It would of course also be helpful if the OP were to conform to the Posting Guide, particularly in respect to using plain text email.
>> 
>> It looks like the kind of nonlinear optimization problem that evolutionary algorithms are often applied to. It doesn't look (to me) like a typical problem that factors get applied to in formulas though, because multiple instances of the same factor variable are present.
>> --
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 9, 2016 4:59:30 PM PDT, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>>> On 09/07/16 20:52, stn021 wrote:
>>>> Hello,
>>>> 
>>>> I would like to analyse a model like this:
>>>> 
>>>> y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )
>>>> 
>>>> x1 and x2 are not continuous variables but factors, so the
>>> observation
>>>> contain the level.
>>>> Its numerical value is unknown and is to be estimated with the model.
>>>> 
>>>> 
>>>> The observations look like this:
>>>> 
>>>> y        x1     x2
>>>> 0.96  Alice  Bob
>>>> 0.84  Alice  Charlie
>>>> 0.96  Bob   Charlie
>>>> 0.64  Dave Alice
>>>> etc.
>>>> 
>>>> Each person has a numerical value. Here for example Alice = 0.2 and
>>> Bob =
>>>> 0.4
>>>> 
>>>> Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.
>>>> 
>>>> How can this be done in R ?
>>> 
>>> 
>>> This question makes about as little sense as it is possible to imagine.
>>> 
>>> cheers,
>>> 
>>> Rolf Turner
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.ca.us  Mon Jul 11 19:16:38 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 11 Jul 2016 10:16:38 -0700
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
	<CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
Message-ID: <AF7E2508-9E20-428C-96D9-1182D27A27A6@dcn.davis.ca.us>

Your clarification is promising.  A reproducible example is always preferred, though never a guarantee. I expect to be somewhat preoccupied this week so responses may be rather delayed, but the less setup we have to the more likely that someone on the list will tackle it.

Re an answer: If you can make the example simple enough that you can tell us what the right numerical result will be, we will have a better chance of understanding what you are after.  E.g. if you start with a solution and use it to create sample input data with then you don't need to actually solve it to illustrate what you are after. [1]

Note that I am not aware of any package dedicated to this type of problem, so unless someone else responds otherwise then you will likely have to use bootstrapping or your own statistical analysis (Bayesian?) of the result. 

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
-- 
Sent from my phone. Please excuse my brevity.

On July 11, 2016 7:28:41 AM PDT, stn021 <stn021 at gmail.com> wrote:
>Hello,
>
>thank you for the replies. Sorry about the html-email, I forgot.
>Should be OK with this email.
>
>
>Don't be fooled be the apparent simplicity of the problem. I have
>tried to reduce it to only a single relatively simple question.
>
>The idea here is to model cooperation of two persons. The model is
>about one specific aspect of that cooperation, namely that two persons
>with similar abilities may be able to produce better results that two
>very different persons.
>
>That is only one part of the model with other parts modeling for
>example the fact that of course two persons with a higher degree of
>ability will produce better results per se.
>
>
>It is not classic regression with factors. That can be easily done by
>something like lm( y ~ (p1-p2)^2 ).
>
>This expands to lm( y ~ p1^2 - 2*p1*p2 + p2^2 ). This contains a
>multiplicagtions and for lm() this implies interactions between the
>factor-levels and produces one parameter for each combination of
>factor-levels that occurs in the data. That is not what the question
>is about.
>
>Also p1 and p2 are different levels of the same factor, while for lm()
>it would be two different factors with different levels.
>
>
>As for the sensical part: this has a real world application therefore
>it makes sense.
>
>Also it is not so difficult to solve with non-linear optimization. I
>was hoping to be able to use R for that purpose because then the
>results could easily be checked with statistical tests.
>
>So my question is not "how to solve" but "how to solve with R".
>
>
>As for the excess degrees of freedom, in real observations there would
>of course be added noise due to either random variations or factors
>not included in the model. So to generate a more reality-conforming
>example I could add some random normal-distributed noise to the
>dependent variable y. I previously left that part out because to me it
>did not seem relevant.
>
>
>Would you like me to make a complete example dataset with more records
>and noise ?
>
>
>The answer I look for would be the numerical values of the
>factor-levels and numerical values for the multiplier (f) and the
>offset (o), with p1 and p2 given as names (here: persons) and y given
>as some level of achievement they reach by cooperating.
>
>y = f * ( o - ( p1 - p2 )^2 )
>
>Is that what you meant by "answer" ?
>
>
>THX
>stefan
>
>
>
>
>2016-07-10 2:27 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>>
>> I have seen less sensical questions.
>>
>> It would be nice if the example were a bit more complete (as in it
>should have excess degrees of freedom and an answer) and less like a
>homework problem (which are off topic here). It would of course also be
>helpful if the OP were to conform to the Posting Guide, particularly in
>respect to using plain text email.
>>
>> It looks like the kind of nonlinear optimization problem that
>evolutionary algorithms are often applied to. It doesn't look (to me)
>like a typical problem that factors get applied to in formulas though,
>because multiple instances of the same factor variable are present.
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 9, 2016 4:59:30 PM PDT, Rolf Turner <r.turner at auckland.ac.nz>
>wrote:
>> >On 09/07/16 20:52, stn021 wrote:
>> >> Hello,
>> >>
>> >> I would like to analyse a model like this:
>> >>
>> >> y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )
>> >>
>> >> x1 and x2 are not continuous variables but factors, so the
>> >observation
>> >> contain the level.
>> >> Its numerical value is unknown and is to be estimated with the
>model.
>> >>
>> >>
>> >> The observations look like this:
>> >>
>> >> y        x1     x2
>> >> 0.96  Alice  Bob
>> >> 0.84  Alice  Charlie
>> >> 0.96  Bob   Charlie
>> >> 0.64  Dave Alice
>> >> etc.
>> >>
>> >> Each person has a numerical value. Here for example Alice = 0.2
>and
>> >Bob =
>> >> 0.4
>> >>
>> >> Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.
>> >>
>> >> How can this be done in R ?
>> >
>> >
>> >This question makes about as little sense as it is possible to
>imagine.
>> >
>> >cheers,
>> >
>> >Rolf Turner
>>


From mccormack at molbio.mgh.harvard.edu  Mon Jul 11 22:01:32 2016
From: mccormack at molbio.mgh.harvard.edu (Matthew)
Date: Mon, 11 Jul 2016 16:01:32 -0400
Subject: [R] use value in variable to be name of another variable
Message-ID: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>

I want to get a value that has been assigned to a variable, and then use 
that value to be the name of a variable.

For example,

tTargTFS[1,1]
# returns:
                 V1
"AT1G01010"

Now, I want to make AT1G01010 the name of a variable:
AT1G01010 <- tTargTFS[-1,1]

Then, go to the next tTargTFS[1,2]. Which produces
                V1
"AT1G01030"
And then,
AT1G01030 <- tTargTFS[-1,2]

I want to do this up to tTargTFS[1, 2666], so I want to do this in a 
script and not manually.
tTargTFS is a list of 2: chr [1:265, 1:2666], but I also have the data 
in a data frame of 265 observations of 2666 variables, if this data 
structure makes things easier.

My initial attempts are not working. Starting with a test data structure 
that is a little simpler I have tried:
for (i in 1:4)
{ ATG <- tTargTFS[1, i]
assign(cat(ATG), tTargTFS[-1, i]) }

Matthew


From lucam1968 at gmail.com  Mon Jul 11 22:24:35 2016
From: lucam1968 at gmail.com (Luca Meyer)
Date: Mon, 11 Jul 2016 22:24:35 +0200
Subject: [R] Script/function/procedure with loop
Message-ID: <CABQyo8542YQAFMVqWKCWXh9hU2K9pt5dP0oBWgR8aOqOx3HHTw@mail.gmail.com>

Can anyone point me to an R script/function/procedure which, starting from
the following sample data

#sample data
#NB: nrow(df) is variable

date =
c("07-jul-16","07-jul-16","07-jul-16","08-jul-16","08-jul-16","08-jul-16","09-jul-16","09-jul-16")
varA = c("text A1","text A2","text A3","text A4","text A5","text A6","text
A7","text A8")
varB = c("link B1","link B2","link B3","link B4","link B5","link B6","link
B7","link B8")
df = data.frame(date, varA, varB)

allows me to obtain a text output such as:

> 07-jul-16

text A1
link B1

text A2
link B2

text A3
link B3

> 08-jul-16

text A4
link B4

text A5
link B5

text A6
link B6

> 09-jul-16

text A7
link B7

text A8
link B8

etc...

Thanks,

Luca

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Jul 11 22:32:09 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 11 Jul 2016 13:32:09 -0700
Subject: [R] How to make the "apply" faster
In-Reply-To: <CAF8bMcYqLpQAHc7EONNPMi6C0KJGso7bU0_A4SeS8oW+G7f6uQ@mail.gmail.com>
References: <CAM9mbiDHTojMzJZD7RzthKd_Smtb-F+SW=z6-29CY2Kazw-99w@mail.gmail.com>
	<alpine.OSX.2.20.1607091428480.543@charles-berrys-macbook.local>
	<CAM9mbiDfWprrX7M-DNfux4bTuEuZcRwO3UjR_Mmdf_7mnypBdQ@mail.gmail.com>
	<CAF8bMcZWSWOwTJ7_PwGfkP7DE03zSb=TqMhifLJ4H6eJk=3UwA@mail.gmail.com>
	<CAM9mbiCQE5aadAZ0D+s96CJUgHzR4ee9VmAhwPcF1=VHvWgFiw@mail.gmail.com>
	<CAF8bMcYqLpQAHc7EONNPMi6C0KJGso7bU0_A4SeS8oW+G7f6uQ@mail.gmail.com>
Message-ID: <CAF8bMcb-rcUnOYGWSSB7HnMx1P9XuV5G=aiWtVgmK_ae_Zam4A@mail.gmail.com>

If you use Rcpp::Rcpp.package.skeleton() to make a package out
of the attached C++ code you can speed up the run counting quite
a bit - to 1.4 s. from 48 s. for the 101 x 107 x 17 x 103 example.

The package you make will define an R function called CountColumnRuns
that computes the number of runs of TRUE of a given minimum length
for each column in a matrix.  You can adapt it to your 4-dimensional array
with

f5 <- function (condition, spellLength = 2)
{
    stopifnot(is.logical(condition), !anyNA(condition))
    d <- dim(condition)
    dn <- dimnames(condition)
    tmp <- array(aperm(condition, c(3, 1, 2, 4)), c(d[3], prod(d[-3])))
    runCounts <- RcppCountRuns::CountColumnRuns(tmp, spellLength)
    array(runCounts, dim = d[-3], dimnames = dn[-3])
}

and call it as

f5( x > 1, spellLength=2)

to get the counts of all runs of length at least 2 in the 3rd dimension of
your 4-d array.



#include <Rcpp.h>

int CountRunsRaw(const int *x, int n, int minRun)
{
    int count(0);
    int runLength(0);
    const int* lastX(x + n);
    for( ; x != lastX ; x++)
    {
        if (*x)
        {
            runLength++;
        }
        else
        {
            if (runLength >= minRun)
            {
                count++;
            }
            runLength = 0;
        }
    }
    if (runLength >= minRun)
    {
        count++;
    }
    return count;
}

// [[Rcpp::export]]
int CountRuns(const Rcpp::LogicalVector& x, int minRun)
{
    return CountRunsRaw(&x[0], x.size(), minRun);
}
// [[Rcpp::export]]
Rcpp::IntegerVector CountColumnRuns(const Rcpp::LogicalMatrix& x, int
minRun)
{
    Rcpp::IntegerVector out(x.ncol());
    for(int i = 0 ; i < x.ncol() ; i++)
    {
        out[i] = CountRuns(x(Rcpp::_, i), minRun);
    }
    return out;
}


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jul 11, 2016 at 9:42 AM, William Dunlap <wdunlap at tibco.com> wrote:

> How fast is fast enough and what size and shape is your dataset
> (show the output of str(yourData))?  You will get the fastest execution
> time by using C or C++ or Fortran, but you will want to parameterize
> the problem well enough that you can amortize the time it takes to
> write the code over many problems.
>
> Peter Langflder's suggested you use aperm followed by colSums
> for your earlier problem.   For this one you can use aperm followed
> by filter (to identify the runs of a given minimum length, column by
> column) and then use colSums (to count the number of runs filter
> identifies in each column).  E.g.,
>
>   f4 <- function (condition, spellLength = 2)
>   {
>       # obscure way to count runs of length>spellLength
>       # in 3'rd dimension of logical array 'condition'.
>       stopifnot(is.logical(condition), !anyNA(condition))
>       coef <- c(-1, rep(1, spellLength))
>       d <- dim(condition)
>       dn <- dimnames(condition)
>       tmp <- array(aperm(condition * 2 - 1, c(3, 1, 2, 4)), c(d[3],
>           prod(d[-3])))
>       fTmp <- filter(rbind(tmp, -1), coef, sides = 1)
>      sfTmp <- colSums(fTmp == spellLength + 1, na.rm = TRUE)
>      array(sfTmp, dim = d[-3], dimnames = dn[-3])
>   }
>
> f4(x>=1) is not a great deal faster (48 s. vs. 67 s.) than
> apply(x>=1, c(1,2,4), FUN=f3) where f3 is
>   f3 <- function (condition, spellLength = 2)
>   {
>       stopifnot(is.logical(condition), !anyNA(condition))
>       y <- rle(condition)
>       sum(y$lengths[y$values] >= spellLength)
>   }
> and where x has dimensions c(101,107,17,103).
>
> The relative speed will depend on the size and shape of your dataset,
> so show the output of str(yourData).
>
>
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Sun, Jul 10, 2016 at 1:38 PM, Debasish Pai Mazumder <pai1981 at gmail.com>
> wrote:
>
>> Thanks for your response. It is faster than before but still very slow.
>> Any other suggestion ?
>> -Deb
>>
>>
>> On Sun, Jul 10, 2016 at 2:13 PM, William Dunlap <wdunlap at tibco.com>
>> wrote:
>>
>>> There is no need to test that a logical equals TRUE:
>>> 'logicalVector==TRUE' is the
>>> same as just 'logicalVector'.
>>>
>>> There is no need to convert logical vectors to numeric, since rle()
>>> works on both
>>> types.
>>>
>>> There is no need to use length(subset(x, logicalVector)) to count how
>>> many elements
>>> in logicalVector are TRUE, just use sum(logicalVector).
>>>
>>> There is no need to make a variable, 'ans', then immediately return it.
>>>
>>> Hence your
>>>
>>>     b[b == TRUE] = 1
>>>     y <- rle(b)
>>>     ans <- length(subset(y$lengths[y$values == 1], y$lengths[y$values ==
>>> 1] >= 2))
>>>     return(ans)
>>>
>>> could be replaced by
>>>
>>>     y <- rle(b)
>>>     sum(y$lengths[y$values] >= 2)
>>>
>>> This gives some speedup, mainly for long vectors, but I find it more
>>> understandable.
>>> E.g., if f1 is your original function and f2 has the above replacement I
>>> get:
>>>   > d <- -sin(1:10000+sqrt(1:4))
>>>   > system.time(for(i in 1:10000)f1(d,.3))
>>>      user  system elapsed
>>>      5.19    0.00    5.19
>>>   > system.time(for(i in 1:10000)f2(d,.3))
>>>      user  system elapsed
>>>      3.65    0.00    3.65
>>>   > c(f1(d,.3), f2(d,.3))
>>>   [1] 1492 1492
>>>   > length(d)
>>>   [1] 10000
>>>
>>> If it were my function, I would also get rid of the part that deals with
>>> the threshhold
>>> and direction of the inequality and tell the user to to use f(data <=
>>> 0.3) instead of
>>> f(data, .3, "below").  I would also make the spell length an argument
>>> instead of
>>> fixing it at 2.  E.g.
>>>
>>>    > f3 <- function (condition, spellLength = 2)
>>>    {
>>>        stopifnot(is.logical(condition), !anyNA(condition))
>>>        y <- rle(condition)
>>>        sum(y$lengths[y$values] >= spellLength)
>>>    }
>>>    > f3( d >= .3 )
>>>    [1] 1492
>>>
>>>
>>>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>> On Sun, Jul 10, 2016 at 11:58 AM, Debasish Pai Mazumder <
>>> pai1981 at gmail.com> wrote:
>>>
>>>> Hi Everyone,
>>>> Thanks for your help. It works. I have similar problem when I am
>>>> calculating number of spell.
>>>> I am also calculation spell (definition: period of two or more days
>>>> where x
>>>> exceeds 70) using similar way:
>>>>
>>>> *new = apply(x,c(1,2,4),FUN=function(y) {fun.spell.deb(y, 70)})*
>>>>
>>>> where fun.spell.deb.R:
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> *## Calculate spell durationfun.spell.deb <- function(data, threshold =
>>>> 1,
>>>> direction = c("above", "below")){  #coln <- grep(weather, names(data))#
>>>> var <- data[,8]  if(missing(direction)) {direction <- "above"}
>>>> if(direction=="below") {b <- (data <= threshold)} else  {b <- (data >=
>>>> threshold)}    b[b==TRUE] = 1  y <-rle(b)  ans
>>>> <-length(subset((y$lengths[y$values==1]), (y$lengths[y$values==1])>=2))
>>>> return(ans)}*
>>>>
>>>> Do you have any idea how to make the "apply" faster here?
>>>>
>>>> -Deb
>>>>
>>>>
>>>> On Sat, Jul 9, 2016 at 3:46 PM, Charles C. Berry <ccberry at ucsd.edu>
>>>> wrote:
>>>>
>>>> > On Sat, 9 Jul 2016, Debasish Pai Mazumder wrote:
>>>> >
>>>> > I have 4-dimension array x(lat,lon,time,var)
>>>> >>
>>>> >> I am using "apply" to calculate over time
>>>> >> new = apply(x,c(1,2,4),FUN=function(y) {length(which(y>=70))})
>>>> >>
>>>> >> This is very slow. Is there anyway make it faster?
>>>> >>
>>>> >
>>>> > If dim(x)[3] << prod(dim(x)[-3]),
>>>> >
>>>> > new <-  Reduce("+",lapply(1:dim(x)[3],function(z) x[,,z,]>=70))
>>>> >
>>>> > will be faster.
>>>> >
>>>> > However, if you can follow Peter Langfelder's suggestion to use
>>>> rowSums,
>>>> > that would be best. Even using rowSums(aperm(x,c(1,2,4,3)>=70,dims=3)
>>>> and
>>>> > paying the price of aperm() might be better.
>>>> >
>>>> > Chuck
>>>> >
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Mon Jul 11 22:43:31 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 11 Jul 2016 16:43:31 -0400
Subject: [R] Script/function/procedure with loop
In-Reply-To: <CABQyo8542YQAFMVqWKCWXh9hU2K9pt5dP0oBWgR8aOqOx3HHTw@mail.gmail.com>
References: <CABQyo8542YQAFMVqWKCWXh9hU2K9pt5dP0oBWgR8aOqOx3HHTw@mail.gmail.com>
Message-ID: <CAM_vjukkHwio89U8je84a2AqKzkdeEeQYLD5P0BuWGL5FDAi-w@mail.gmail.com>

Taking your question at face value, except for the factors in your
original data frame, you can output anything you'd like to text
onscreen using cat(). Output can also be saved to text files with
sink() or using batch files, etc and so forth.



date <- c("07-jul-16","07-jul-16","07-jul-16","08-jul-16","08-jul-16","08-jul-16","09-jul-16","09-jul-16")
varA <- c("text A1","text A2","text A3","text A4","text A5","text
A6","text A7","text A8")
varB <- c("link B1","link B2","link B3","link B4","link B5","link
B6","link B7","link B8")
mydf <- data.frame(date, varA, varB, stringsAsFactors=FALSE)

for(i in sort(unique(mydf$date))) {
  thisdate <- subset(mydf, date==i)
  cat(i, "\n\n")
  for(j in seq_len(nrow(thisdate))) {
    cat(thisdate$varA[j], "\n")
    cat(thisdate$varB[j], "\n\n")
 }
}

This code prints to screen:

07-jul-16

text A1
link B1

text A2
link B2

text A3
link B3

08-jul-16

text A4
link B4

text A5
link B5

text A6
link B6

09-jul-16

text A7
link B7

text A8
link B8


On Mon, Jul 11, 2016 at 4:24 PM, Luca Meyer <lucam1968 at gmail.com> wrote:
> Can anyone point me to an R script/function/procedure which, starting from
> the following sample data
>
> #sample data
> #NB: nrow(df) is variable
>
> date =
> c("07-jul-16","07-jul-16","07-jul-16","08-jul-16","08-jul-16","08-jul-16","09-jul-16","09-jul-16")
> varA = c("text A1","text A2","text A3","text A4","text A5","text A6","text
> A7","text A8")
> varB = c("link B1","link B2","link B3","link B4","link B5","link B6","link
> B7","link B8")
> df = data.frame(date, varA, varB)
>
> allows me to obtain a text output such as:
>
>> 07-jul-16
>
> text A1
> link B1
>
> text A2
> link B2
>
> text A3
> link B3
>
>> 08-jul-16
>
> text A4
> link B4
>
> text A5
> link B5
>
> text A6
> link B6
>
>> 09-jul-16
>
> text A7
> link B7
>
> text A8
> link B8
>
> etc...
>
> Thanks,
>
> Luca
>
>         [[alternative HTML version deleted]]

Please post in plain text.

-- 
Sarah Goslee
http://www.functionaldiversity.org


From chocold12 at gmail.com  Mon Jul 11 22:47:49 2016
From: chocold12 at gmail.com (lily li)
Date: Mon, 11 Jul 2016 14:47:49 -0600
Subject: [R] about smwrgraphs package
In-Reply-To: <CAN5afy9BS55W=SFujge=C3b70qvoYsr2Usb_jSm_ncR5rhAD6w@mail.gmail.com>
References: <CAN5afy9BS55W=SFujge=C3b70qvoYsr2Usb_jSm_ncR5rhAD6w@mail.gmail.com>
Message-ID: <CAN5afy8cEO4PXMpQiERNE09FrOr9pex_Na-tUi6WYA54vyWpTQ@mail.gmail.com>

I still haven't heard back from anyone. Please let me know as I think it is
better to discuss here.

On Sun, Jul 10, 2016 at 1:25 PM, lily li <chocold12 at gmail.com> wrote:

> Has anyone used smwrGraphs package? I have some problems and think it may
> be better to discuss if you have been using it. Thanks very much.
>

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Mon Jul 11 22:56:09 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 11 Jul 2016 16:56:09 -0400
Subject: [R] about smwrgraphs package
In-Reply-To: <CAN5afy8cEO4PXMpQiERNE09FrOr9pex_Na-tUi6WYA54vyWpTQ@mail.gmail.com>
References: <CAN5afy9BS55W=SFujge=C3b70qvoYsr2Usb_jSm_ncR5rhAD6w@mail.gmail.com>
	<CAN5afy8cEO4PXMpQiERNE09FrOr9pex_Na-tUi6WYA54vyWpTQ@mail.gmail.com>
Message-ID: <CAM_vjunF47ArJcx7jo5aPqTk_d3dNJ=jgF_WRVtWdmD2MDtaow@mail.gmail.com>

You really need to follow the posting guide and ask an actual
question, ideally a reproducible one.

Failing that, contacting the package maintainer may be the appropriate action.

You're very unlikely to get much interest on the R-help list in such a
vague question, but many of us are able to help with specific
questions even if we've never used a particular package.

Sarah

On Mon, Jul 11, 2016 at 4:47 PM, lily li <chocold12 at gmail.com> wrote:
> I still haven't heard back from anyone. Please let me know as I think it is
> better to discuss here.
>
> On Sun, Jul 10, 2016 at 1:25 PM, lily li <chocold12 at gmail.com> wrote:
>
>> Has anyone used smwrGraphs package? I have some problems and think it may
>> be better to discuss if you have been using it. Thanks very much.
>>
>
>         [[alternative HTML version deleted]]

Please don't post in HTML.



-- 
Sarah Goslee
http://www.functionaldiversity.org


From drjimlemon at gmail.com  Mon Jul 11 23:59:04 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 12 Jul 2016 07:59:04 +1000
Subject: [R] use value in variable to be name of another variable
In-Reply-To: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
References: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
Message-ID: <CA+8X3fU4rziJRtT+xBy6-yCdVE1LE-F3Z2m2gFCzUuv3P0xNwg@mail.gmail.com>

Hi Matthew,
This question is a bit mysterious as we don't know what the object
"chr" is. However, have a look at this and see if it is close to what
you want to do.

# set up a little matrix of character values
tTargTFS<-matrix(paste("A",rep(1:4,each=4),"B",rep(1:4,4),sep=""),ncol=4)
# try the assignment on the first row and column
assign(tTargTFS[1,1],tTargTFS[-1,1])
# see what it looks like - okay
A1B1
# run the assignment over the matrix
for(i in 1:4) assign(tTargTFS[1,i],tTargTFS[-1,i])
# see what the variables look like
A1B1
A2B1
A3B1
A4B1

It does what I would expect.

Jim


On Tue, Jul 12, 2016 at 6:01 AM, Matthew
<mccormack at molbio.mgh.harvard.edu> wrote:
> I want to get a value that has been assigned to a variable, and then use
> that value to be the name of a variable.
>
> For example,
>
> tTargTFS[1,1]
> # returns:
>                 V1
> "AT1G01010"
>
> Now, I want to make AT1G01010 the name of a variable:
> AT1G01010 <- tTargTFS[-1,1]
>
> Then, go to the next tTargTFS[1,2]. Which produces
>                V1
> "AT1G01030"
> And then,
> AT1G01030 <- tTargTFS[-1,2]
>
> I want to do this up to tTargTFS[1, 2666], so I want to do this in a script
> and not manually.
> tTargTFS is a list of 2: chr [1:265, 1:2666], but I also have the data in a
> data frame of 265 observations of 2666 variables, if this data structure
> makes things easier.
>
> My initial attempts are not working. Starting with a test data structure
> that is a little simpler I have tried:
> for (i in 1:4)
> { ATG <- tTargTFS[1, i]
> assign(cat(ATG), tTargTFS[-1, i]) }
>
> Matthew
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mccormack at molbio.mgh.harvard.edu  Tue Jul 12 00:13:28 2016
From: mccormack at molbio.mgh.harvard.edu (Matthew)
Date: Mon, 11 Jul 2016 18:13:28 -0400
Subject: [R] use value in variable to be name of another variable
In-Reply-To: <CA+8X3fU4rziJRtT+xBy6-yCdVE1LE-F3Z2m2gFCzUuv3P0xNwg@mail.gmail.com>
References: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
	<CA+8X3fU4rziJRtT+xBy6-yCdVE1LE-F3Z2m2gFCzUuv3P0xNwg@mail.gmail.com>
Message-ID: <bba2c7d9-ceaa-72e3-c49a-b5d275edbb14@molbio.mgh.harvard.edu>

Hi Jim,

    Wow ! And it does exactly what I was looking for.  Thank you very much.

That assign function is pretty nice. I should become more familiar with it.

Matthew


On 7/11/2016 5:59 PM, Jim Lemon wrote:
> Hi Matthew,
> This question is a bit mysterious as we don't know what the object
> "chr" is. However, have a look at this and see if it is close to what
> you want to do.
>
> # set up a little matrix of character values
> tTargTFS<-matrix(paste("A",rep(1:4,each=4),"B",rep(1:4,4),sep=""),ncol=4)
> # try the assignment on the first row and column
> assign(tTargTFS[1,1],tTargTFS[-1,1])
> # see what it looks like - okay
> A1B1
> # run the assignment over the matrix
> for(i in 1:4) assign(tTargTFS[1,i],tTargTFS[-1,i])
> # see what the variables look like
> A1B1
> A2B1
> A3B1
> A4B1
>
> It does what I would expect.
>
> Jim
>
>
> On Tue, Jul 12, 2016 at 6:01 AM, Matthew
> <mccormack at molbio.mgh.harvard.edu> wrote:
>> I want to get a value that has been assigned to a variable, and then use
>> that value to be the name of a variable.
>>
>> For example,
>>
>> tTargTFS[1,1]
>> # returns:
>>                  V1
>> "AT1G01010"
>>
>> Now, I want to make AT1G01010 the name of a variable:
>> AT1G01010 <- tTargTFS[-1,1]
>>
>> Then, go to the next tTargTFS[1,2]. Which produces
>>                 V1
>> "AT1G01030"
>> And then,
>> AT1G01030 <- tTargTFS[-1,2]
>>
>> I want to do this up to tTargTFS[1, 2666], so I want to do this in a script
>> and not manually.
>> tTargTFS is a list of 2: chr [1:265, 1:2666], but I also have the data in a
>> data frame of 265 observations of 2666 variables, if this data structure
>> makes things easier.
>>
>> My initial attempts are not working. Starting with a test data structure
>> that is a little simpler I have tried:
>> for (i in 1:4)
>> { ATG <- tTargTFS[1, i]
>> assign(cat(ATG), tTargTFS[-1, i]) }
>>
>> Matthew
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Tue Jul 12 00:24:27 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 11 Jul 2016 15:24:27 -0700
Subject: [R] use value in variable to be name of another variable
In-Reply-To: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
References: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
Message-ID: <CF864B3B-084D-43D9-9785-697D8F7201D8@comcast.net>


> On Jul 11, 2016, at 1:01 PM, Matthew <mccormack at molbio.mgh.harvard.edu> wrote:
> 
> I want to get a value that has been assigned to a variable, and then use that value to be the name of a variable.
> 
> For example,
> 
> tTargTFS[1,1]
> # returns:
>                V1
> "AT1G01010"
> 
> Now, I want to make AT1G01010 the name of a variable:
> AT1G01010 <- tTargTFS[-1,1]
> 
> Then, go to the next tTargTFS[1,2]. Which produces
>               V1
> "AT1G01030"
> And then,
> AT1G01030 <- tTargTFS[-1,2]
> 
> I want to do this up to tTargTFS[1, 2666], so I want to do this in a script and not manually.
> tTargTFS is a list of 2: chr [1:265, 1:2666], but I also have the data in a data frame of 265 observations of 2666 variables, if this data structure makes things easier.
> 
> My initial attempts are not working. Starting with a test data structure that is a little simpler I have tried:
> for (i in 1:4)
> { ATG <- tTargTFS[1, i]
> assign(cat(ATG), tTargTFS[-1, i]) }

Your efforts will come to naught (or more prezactly...  NULL) when you use `cat` as a value. You are essentially doing the R equivalent of answering the question about the sound of one hand clapping.

-- 

David Winsemius
Alameda, CA, USA


From r.turner at auckland.ac.nz  Tue Jul 12 00:31:20 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 12 Jul 2016 10:31:20 +1200
Subject: [R] use value in variable to be name of another variable
In-Reply-To: <bba2c7d9-ceaa-72e3-c49a-b5d275edbb14@molbio.mgh.harvard.edu>
References: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
	<CA+8X3fU4rziJRtT+xBy6-yCdVE1LE-F3Z2m2gFCzUuv3P0xNwg@mail.gmail.com>
	<bba2c7d9-ceaa-72e3-c49a-b5d275edbb14@molbio.mgh.harvard.edu>
Message-ID: <746a1f77-efe0-71fd-3a7b-16b4ebe0ac35@auckland.ac.nz>

On 12/07/16 10:13, Matthew wrote:
> Hi Jim,
>
>    Wow ! And it does exactly what I was looking for.  Thank you very much.
>
> That assign function is pretty nice. I should become more familiar with it.

Indeed you should, and assign() is indeed nice and useful and handy. 
But it should be used with care and circumspection.  It *alters the 
global environment* which is fraught with peril.  Generally speaking 
most things that can be done with assign() (and its companion function 
get()) are better and more safely done using lists and functions and 
other "natural" R-ish constructs.  Resist the temptation to turn R into 
a macro language.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From mccormack at molbio.mgh.harvard.edu  Tue Jul 12 00:34:26 2016
From: mccormack at molbio.mgh.harvard.edu (Matthew)
Date: Mon, 11 Jul 2016 18:34:26 -0400
Subject: [R] use value in variable to be name of another variable
In-Reply-To: <746a1f77-efe0-71fd-3a7b-16b4ebe0ac35@auckland.ac.nz>
References: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
	<CA+8X3fU4rziJRtT+xBy6-yCdVE1LE-F3Z2m2gFCzUuv3P0xNwg@mail.gmail.com>
	<bba2c7d9-ceaa-72e3-c49a-b5d275edbb14@molbio.mgh.harvard.edu>
	<746a1f77-efe0-71fd-3a7b-16b4ebe0ac35@auckland.ac.nz>
Message-ID: <32956aab-c57a-ca51-232a-ae4b31657740@molbio.mgh.harvard.edu>

Hi Rolf,

     Thanks for the warning. I think because my initial efforts used the 
assign function, that Jim provided his solution using it.

     Any suggestions for how it could be done without assign() ?

Matthew

On 7/11/2016 6:31 PM, Rolf Turner wrote:
> On 12/07/16 10:13, Matthew wrote:
>> Hi Jim,
>>
>>    Wow ! And it does exactly what I was looking for.  Thank you very 
>> much.
>>
>> That assign function is pretty nice. I should become more familiar 
>> with it.
>
> Indeed you should, and assign() is indeed nice and useful and handy. 
> But it should be used with care and circumspection.  It *alters the 
> global environment* which is fraught with peril. Generally speaking 
> most things that can be done with assign() (and its companion function 
> get()) are better and more safely done using lists and functions and 
> other "natural" R-ish constructs. Resist the temptation to turn R into 
> a macro language.
>
> cheers,
>
> Rolf Turner
>


From dwinsemius at comcast.net  Tue Jul 12 00:39:09 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 11 Jul 2016 15:39:09 -0700
Subject: [R] [R-pkgs] archivist.github 0.2.1 on CRAN - Task View:
	Reproducible	Research
In-Reply-To: <CAL8y_Qw-EwAtUJ_Y=Kyqgtv6vsOppfMLyPAUq645vOfEBAZE9w@mail.gmail.com>
References: <CAL8y_Qw-EwAtUJ_Y=Kyqgtv6vsOppfMLyPAUq645vOfEBAZE9w@mail.gmail.com>
Message-ID: <B4E6243B-5F30-4C44-A91B-8AC8CD9ECCF1@comcast.net>


> On Jun 26, 2016, at 3:16 PM, Marcin Kosi?ski <m.p.kosinski at gmail.com> wrote:
> 
> Hi all R devs,
> 
> archivist.github has appeared on CRAN in it's updated version.
> You can check the last blog post on how has RHero saved the Backup City
> with the power or archivist and GitHub
> http://www.r-bloggers.com/r-hero-saves-backup-city-with-archivist-and-github/
> 
> There is also going to be a talk on useR2016 about it's core dependency -
> archivist
> 
> How to use the archivist package to boost reproducibility of your research
> <http://schedule.user2016.org/event/7BYx/how-to-use-the-archivist-package-to-boost-reproducibility-of-your-research>
> <http://schedule.user2016.org/event/7BYx/how-to-use-the-archivist-package-to-boost-reproducibility-of-your-research>
> <http://schedule.user2016.org/event/7BYx/how-to-use-the-archivist-package-to-boost-reproducibility-of-your-research>
> 
> If you would like to boost your reproducible engines then this is a talk
> for you :)

This is a link to a video of the talk:

https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/How-to-use-the-archivist-package-to-boost-reproducibility-of-your-research

-- 

David Winsemius
Alameda, CA, USA


From wdunlap at tibco.com  Tue Jul 12 00:43:09 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 11 Jul 2016 15:43:09 -0700
Subject: [R] use value in variable to be name of another variable
In-Reply-To: <bba2c7d9-ceaa-72e3-c49a-b5d275edbb14@molbio.mgh.harvard.edu>
References: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
	<CA+8X3fU4rziJRtT+xBy6-yCdVE1LE-F3Z2m2gFCzUuv3P0xNwg@mail.gmail.com>
	<bba2c7d9-ceaa-72e3-c49a-b5d275edbb14@molbio.mgh.harvard.edu>
Message-ID: <CAF8bMcbCefPMFQxb3g+C2GW4fRj9u6uk_q47gW6dHyBN70PY6A@mail.gmail.com>

I find that instead of using assign() and get(), it is more convenient to
make
an environment in which to store a related set of variables and
then use env[[varName]] instead of get(varName) or assign(varName)
to get and set variables.

The advantages are
* the same syntax works for setting and getting, unlike assign() and get()
* nested replacements work
* you don't accidently overwrite things in the current environment

You can use the same syntax with a list instead of an environment.

E.g.,

geneNames <- c("AT1", "AT2", "PQ1")
envAction <- new.env(parent=emptyenv())
envAction[[ geneNames[2] ]] <- paste("Action for", geneNames[[2]])
names(envAction)
envAction[[ geneNames[2] ]]
envAction[[ geneNames[2] ]] [2] <- "another action" # nested replacement
envAction[[ geneNames[2] ]]



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jul 11, 2016 at 3:13 PM, Matthew <mccormack at molbio.mgh.harvard.edu>
wrote:

> Hi Jim,
>
>    Wow ! And it does exactly what I was looking for.  Thank you very much.
>
> That assign function is pretty nice. I should become more familiar with it.
>
> Matthew
>
>
> On 7/11/2016 5:59 PM, Jim Lemon wrote:
>
>> Hi Matthew,
>> This question is a bit mysterious as we don't know what the object
>> "chr" is. However, have a look at this and see if it is close to what
>> you want to do.
>>
>> # set up a little matrix of character values
>> tTargTFS<-matrix(paste("A",rep(1:4,each=4),"B",rep(1:4,4),sep=""),ncol=4)
>> # try the assignment on the first row and column
>> assign(tTargTFS[1,1],tTargTFS[-1,1])
>> # see what it looks like - okay
>> A1B1
>> # run the assignment over the matrix
>> for(i in 1:4) assign(tTargTFS[1,i],tTargTFS[-1,i])
>> # see what the variables look like
>> A1B1
>> A2B1
>> A3B1
>> A4B1
>>
>> It does what I would expect.
>>
>> Jim
>>
>>
>> On Tue, Jul 12, 2016 at 6:01 AM, Matthew
>> <mccormack at molbio.mgh.harvard.edu> wrote:
>>
>>> I want to get a value that has been assigned to a variable, and then use
>>> that value to be the name of a variable.
>>>
>>> For example,
>>>
>>> tTargTFS[1,1]
>>> # returns:
>>>                  V1
>>> "AT1G01010"
>>>
>>> Now, I want to make AT1G01010 the name of a variable:
>>> AT1G01010 <- tTargTFS[-1,1]
>>>
>>> Then, go to the next tTargTFS[1,2]. Which produces
>>>                 V1
>>> "AT1G01030"
>>> And then,
>>> AT1G01030 <- tTargTFS[-1,2]
>>>
>>> I want to do this up to tTargTFS[1, 2666], so I want to do this in a
>>> script
>>> and not manually.
>>> tTargTFS is a list of 2: chr [1:265, 1:2666], but I also have the data
>>> in a
>>> data frame of 265 observations of 2666 variables, if this data structure
>>> makes things easier.
>>>
>>> My initial attempts are not working. Starting with a test data structure
>>> that is a little simpler I have tried:
>>> for (i in 1:4)
>>> { ATG <- tTargTFS[1, i]
>>> assign(cat(ATG), tTargTFS[-1, i]) }
>>>
>>> Matthew
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kmnanus at gmail.com  Mon Jul 11 18:40:38 2016
From: kmnanus at gmail.com (KMNanus)
Date: Mon, 11 Jul 2016 12:40:38 -0400
Subject: [R] Is there a way to coerce ggplot into using min & max values for
	x & y axes?
Message-ID: <6F4772F5-2BE0-4A8A-83FA-3455693A0283@gmail.com>

I?m plotting few dozen variables, doing so because I need to examine them one at a time.  The range of these variables varies widely.  One can range from -.5 to .03, another from -600 to +750.

I want both axes to automatically plot from both min and max values for both continuous variables.  

I?ve tried  - stat_summary(fun.ymax=max, fun.ymin=min), but it doesn?t plot to the min and max values.

This code neither plots to the min/max nor does it create tick marks on either axis - scale_y_continuous(breaks = c(min(b12.2$myvar), 0, max(b12.2$myvar)))

Has anyone faced this situation before?  I appreciate the help.

Ken
kmnanus at gmail.com
914-450-0816 (tel)
347-730-4813 (fax)




From ulrik.stervbo at gmail.com  Tue Jul 12 06:52:34 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Tue, 12 Jul 2016 04:52:34 +0000
Subject: [R] Is there a way to coerce ggplot into using min & max values
 for x & y axes?
In-Reply-To: <6F4772F5-2BE0-4A8A-83FA-3455693A0283@gmail.com>
References: <6F4772F5-2BE0-4A8A-83FA-3455693A0283@gmail.com>
Message-ID: <CAKVAULP=L=UGUp7iBUcZFSjN8f1QFg=3h53ozt7DhF8eJor9zA@mail.gmail.com>

Hi Ken,

You can use coord_cartesian() to set axis. Or if it makes sense to your
problem you can facet your plot.

Hope this helps
Ulrik

KMNanus <kmnanus at gmail.com> schrieb am Di., 12. Juli 2016 01:35:

> I?m plotting few dozen variables, doing so because I need to examine them
> one at a time.  The range of these variables varies widely.  One can range
> from -.5 to .03, another from -600 to +750.
>
> I want both axes to automatically plot from both min and max values for
> both continuous variables.
>
> I?ve tried  - stat_summary(fun.ymax=max, fun.ymin=min), but it doesn?t
> plot to the min and max values.
>
> This code neither plots to the min/max nor does it create tick marks on
> either axis - scale_y_continuous(breaks = c(min(b12.2$myvar), 0,
> max(b12.2$myvar)))
>
> Has anyone faced this situation before?  I appreciate the help.
>
> Ken
> kmnanus at gmail.com
> 914-450-0816 (tel)
> 347-730-4813 (fax)
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From lucam1968 at gmail.com  Tue Jul 12 07:54:59 2016
From: lucam1968 at gmail.com (Luca Meyer)
Date: Tue, 12 Jul 2016 07:54:59 +0200
Subject: [R] Script/function/procedure with loop
In-Reply-To: <CAM_vjukkHwio89U8je84a2AqKzkdeEeQYLD5P0BuWGL5FDAi-w@mail.gmail.com>
References: <CABQyo8542YQAFMVqWKCWXh9hU2K9pt5dP0oBWgR8aOqOx3HHTw@mail.gmail.com>
	<CAM_vjukkHwio89U8je84a2AqKzkdeEeQYLD5P0BuWGL5FDAi-w@mail.gmail.com>
Message-ID: <CABQyo843fmf4iaMs12Ur-zJgtbcs6YQ1m9fFo9WefwWjm=zrow@mail.gmail.com>

Thanks Sarah,

The code works just fine.

Luca

2016-07-11 22:43 GMT+02:00 Sarah Goslee <sarah.goslee at gmail.com>:

> Taking your question at face value, except for the factors in your
> original data frame, you can output anything you'd like to text
> onscreen using cat(). Output can also be saved to text files with
> sink() or using batch files, etc and so forth.
>
>
>
> date <-
> c("07-jul-16","07-jul-16","07-jul-16","08-jul-16","08-jul-16","08-jul-16","09-jul-16","09-jul-16")
> varA <- c("text A1","text A2","text A3","text A4","text A5","text
> A6","text A7","text A8")
> varB <- c("link B1","link B2","link B3","link B4","link B5","link
> B6","link B7","link B8")
> mydf <- data.frame(date, varA, varB, stringsAsFactors=FALSE)
>
> for(i in sort(unique(mydf$date))) {
>   thisdate <- subset(mydf, date==i)
>   cat(i, "\n\n")
>   for(j in seq_len(nrow(thisdate))) {
>     cat(thisdate$varA[j], "\n")
>     cat(thisdate$varB[j], "\n\n")
>  }
> }
>
> This code prints to screen:
>
> 07-jul-16
>
> text A1
> link B1
>
> text A2
> link B2
>
> text A3
> link B3
>
> 08-jul-16
>
> text A4
> link B4
>
> text A5
> link B5
>
> text A6
> link B6
>
> 09-jul-16
>
> text A7
> link B7
>
> text A8
> link B8
>
>
> On Mon, Jul 11, 2016 at 4:24 PM, Luca Meyer <lucam1968 at gmail.com> wrote:
> > Can anyone point me to an R script/function/procedure which, starting
> from
> > the following sample data
> >
> > #sample data
> > #NB: nrow(df) is variable
> >
> > date =
> >
> c("07-jul-16","07-jul-16","07-jul-16","08-jul-16","08-jul-16","08-jul-16","09-jul-16","09-jul-16")
> > varA = c("text A1","text A2","text A3","text A4","text A5","text
> A6","text
> > A7","text A8")
> > varB = c("link B1","link B2","link B3","link B4","link B5","link
> B6","link
> > B7","link B8")
> > df = data.frame(date, varA, varB)
> >
> > allows me to obtain a text output such as:
> >
> >> 07-jul-16
> >
> > text A1
> > link B1
> >
> > text A2
> > link B2
> >
> > text A3
> > link B3
> >
> >> 08-jul-16
> >
> > text A4
> > link B4
> >
> > text A5
> > link B5
> >
> > text A6
> > link B6
> >
> >> 09-jul-16
> >
> > text A7
> > link B7
> >
> > text A8
> > link B8
> >
> > etc...
> >
> > Thanks,
> >
> > Luca
> >
> >         [[alternative HTML version deleted]]
>
> Please post in plain text.
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Tue Jul 12 08:25:15 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 12 Jul 2016 06:25:15 +0000
Subject: [R] Is there a way to coerce ggplot into using min & max values
 for	x & y axes?
In-Reply-To: <6F4772F5-2BE0-4A8A-83FA-3455693A0283@gmail.com>
References: <6F4772F5-2BE0-4A8A-83FA-3455693A0283@gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034A50@SRVEXCHMBX.precheza.cz>

Hi

If I want to plot several variables in separate plots I usually use  for cycle something like

pdf("graph.pdf")
for ( i in 1:n) {

p<- ggplot(data, aes(x, data[,i], colour=...))
print(p + geom_point() + ...)
}

dev.off

and maybe add xlim=c(min, max)

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of KMNanus
> Sent: Monday, July 11, 2016 6:41 PM
> To: r-help mailing list <r-help at r-project.org>
> Subject: [R] Is there a way to coerce ggplot into using min & max values for x
> & y axes?
>
> I?m plotting few dozen variables, doing so because I need to examine them
> one at a time.  The range of these variables varies widely.  One can range
> from -.5 to .03, another from -600 to +750.
>
> I want both axes to automatically plot from both min and max values for both
> continuous variables.
>
> I?ve tried  - stat_summary(fun.ymax=max, fun.ymin=min), but it doesn?t plot
> to the min and max values.
>
> This code neither plots to the min/max nor does it create tick marks on either
> axis - scale_y_continuous(breaks = c(min(b12.2$myvar), 0,
> max(b12.2$myvar)))
>
> Has anyone faced this situation before?  I appreciate the help.
>
> Ken
> kmnanus at gmail.com
> 914-450-0816 (tel)
> 347-730-4813 (fax)
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From loris.bennett at fu-berlin.de  Tue Jul 12 08:25:51 2016
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Tue, 12 Jul 2016 08:25:51 +0200
Subject: [R] gsl package installation problem: gsl-config not found
	(even though gsl has been installed)
References: <CACD_QsVO2qx_e5Sicv71WFS5-T2HrbN8OAxZ9xckHFhkyOY5UQ@mail.gmail.com>
Message-ID: <87h9bvl6ds.fsf@hornfels.zedat.fu-berlin.de>

Dear Pan Yang,

pan yang <hippolionforlily at gmail.com> writes:

> Dear R-community,
>
> I faced this problem when I tried to install the R gsl wrapper in my
> university's HPC cluster. Before installing the R wrapper, I have already
> installed the 'gsl' version 2.1 in my own directory '/home/pyangac/dev'. I
> have tested the gsl installation by compiling the simple program in the gsl
> manual (
> https://www.gnu.org/software/gsl/manual/html_node/An-Example-Program.html#An-Example-Program
> ).
>
> After this, I followed the steps in page 30 of the R package's document (
> https://cran.r-project.org/web/packages/gsl/gsl.pdf) to install the package
> (I didn't do step 3 because I really don't know how to do it, and by
> looking at the solutions posted online, nobody had done step 3):
>
> $ /home/pyangac/dev/bin/gsl-config --libs
> -L/home/pyangac/dev/lib -lgsl -lgslcblas -lm
>
> $ /home/pyangac/dev/bin/gsl-config --cflags
> -I/home/pyangac/dev/include
>
> $ LDFLAGS="-L/home/pyangac/dev/lib -lgsl -lgslcblas -lm"; export LDFLAGS
> $ CPPFALGS="-I/home/pyangac/dev/include"; export CPPFLAGS
>
> $ R CMD INSTALL '/home/pyangac/gsl_1.9-10.1.tar.gz'
> * installing to library '/home/pyangac/R_libs'
> * installing *source* package 'gsl' ...
> ** package 'gsl' successfully unpacked and MD5 sums checked
> checking for gsl-config... no
> configure: error: gsl-config not found, is GSL installed?
> ERROR: configuration failed for package 'gsl'
> * removing '/home/pyangac/R_libs/gsl'
>
> Does anyone know why this is happening? Is it because I have not go through
> step 3?
>
> Your generous help and assistance will be highly appreciated.
>
> Best regards,
> Pan

As the 'configure' program fails to find your GSL configuration, then,
yes, you probably do need to do step 3.  Why don't you just try it out
and see whether it works?

Cheers,

Loris

-- 
This signature is currently under construction.


From loris.bennett at fu-berlin.de  Tue Jul 12 09:50:44 2016
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Tue, 12 Jul 2016 09:50:44 +0200
Subject: [R] install.packages: Option to keep build directory?
Message-ID: <8737nfl2gb.fsf@hornfels.zedat.fu-berlin.de>

Hi,

I'm trying to install the package 'nloptr' and am getting the error

  configure: error: in `/scratch/tmp/RtmpJ2Y1xA/R.INSTALL3b35583b483/nloptr':
  configure: error: C++ preprocessor "/lib/cpp" fails sanity check
  See `config.log' for more details

I would like to check 'config.log', but the install directory is
automatically deleted.  I seem to remember there being an option to
'install.packages' which prevented the temporary build directory from
being deleted, but with R version 3.2.3 I can't find it.  I've tried
setting 'destdir' and 'clean=FALSE', but neither helps.

Is there such an option?  If so, what is it?  If not, how can I preserve
'config.log'?

Cheers,

Loris

-- 
Dr. Loris Bennett (Mr.)
ZEDAT, Freie Universit?t Berlin         Email loris.bennett at fu-berlin.de


From sezenismail at gmail.com  Tue Jul 12 12:07:55 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Tue, 12 Jul 2016 13:07:55 +0300
Subject: [R] gsl package installation problem: gsl-config not found
	(even though gsl has been installed)
In-Reply-To: <CACD_QsVO2qx_e5Sicv71WFS5-T2HrbN8OAxZ9xckHFhkyOY5UQ@mail.gmail.com>
References: <CACD_QsVO2qx_e5Sicv71WFS5-T2HrbN8OAxZ9xckHFhkyOY5UQ@mail.gmail.com>
Message-ID: <9547B566-5EE8-4E36-B2D5-38EBDADD72B4@gmail.com>


> $ /home/pyangac/dev/bin/gsl-config --libs
> -L/home/pyangac/dev/lib -lgsl -lgslcblas -lm
> 
> $ /home/pyangac/dev/bin/gsl-config --cflags
> -I/home/pyangac/dev/include
> 
> $ LDFLAGS="-L/home/pyangac/dev/lib -lgsl -lgslcblas -lm"; export LDFLAGS
> $ CPPFALGS="-I/home/pyangac/dev/include"; export CPPFLAGS
> 
> $ R CMD INSTALL '/home/pyangac/gsl_1.9-10.1.tar.gz'
> * installing to library '/home/pyangac/R_libs'
> * installing *source* package 'gsl' ...
> ** package 'gsl' successfully unpacked and MD5 sums checked
> checking for gsl-config... no
> configure: error: gsl-config not found, is GSL installed?
> ERROR: configuration failed for package 'gsl'
> * removing '/home/pyangac/R_libs/gsl'
> 
> Does anyone know why this is happening? Is it because I have not go through
> step 3?
> 

Similar solution to your 3th step, if gsl was installed to different location, is to run R with flags and install gsl. Perhaps this helps you build gsl package without touching source.

CFLAGS="-I/usr/local/opt/gsl/include" LDFLAGS="-L/usr/local/opt/gsl/lib -lgsl -lgslcblas" R
...
> install.packages("gsl?)

Please, see [1] for more details:

1- http://stackoverflow.com/questions/24781125/installing-r-gsl-package-on-mac


From utkarsh.iit at gmail.com  Tue Jul 12 15:35:14 2016
From: utkarsh.iit at gmail.com (Utkarsh Singhal)
Date: Tue, 12 Jul 2016 19:05:14 +0530
Subject: [R] Linear model vs Mixed model
Message-ID: <CAJP88kQcCgJ6vg5dVdxPfpMtcZpb72SpcPehA+PoEUWvPUrVRw@mail.gmail.com>

Hi experts,

While the slope is coming out to be identical in the two methods below, the
intercepts are not. As far as I understand, both are formulations are
identical in the sense that these are asking for a slope corresponding to
'Days' and a separate intercept term for each Subject.

# Model-1
library(lmer)
coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))

# Model-2
coef(lm(Reaction ~ Days + Subject, sleepstudy))

Can somebody tell me the reason? Are the above formulations actually
different or is it due to different optimization method used?

Thank you.

Utkarsh Singhal
91.96508.54333

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Tue Jul 12 15:45:39 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 12 Jul 2016 15:45:39 +0200
Subject: [R] Linear model vs Mixed model
In-Reply-To: <CAJP88kQcCgJ6vg5dVdxPfpMtcZpb72SpcPehA+PoEUWvPUrVRw@mail.gmail.com>
References: <CAJP88kQcCgJ6vg5dVdxPfpMtcZpb72SpcPehA+PoEUWvPUrVRw@mail.gmail.com>
Message-ID: <CAJuCY5wRmHOVGxK3kvUHwtAdL85jg0kyLo9GKMEfezdLaBijfQ@mail.gmail.com>

The parametrisation is different.

The intercept in model 1 is the effect of the "average" subject at days ==
0.
The intercept in model 2 is the effect of the first subject at days == 0.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-07-12 15:35 GMT+02:00 Utkarsh Singhal <utkarsh.iit at gmail.com>:

> Hi experts,
>
> While the slope is coming out to be identical in the two methods below, the
> intercepts are not. As far as I understand, both are formulations are
> identical in the sense that these are asking for a slope corresponding to
> 'Days' and a separate intercept term for each Subject.
>
> # Model-1
> library(lmer)
> coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
>
> # Model-2
> coef(lm(Reaction ~ Days + Subject, sleepstudy))
>
> Can somebody tell me the reason? Are the above formulations actually
> different or is it due to different optimization method used?
>
> Thank you.
>
> Utkarsh Singhal
> 91.96508.54333
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Jul 12 17:21:27 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 12 Jul 2016 15:21:27 +0000
Subject: [R] use value in variable to be name of another variable
In-Reply-To: <32956aab-c57a-ca51-232a-ae4b31657740@molbio.mgh.harvard.edu>
References: <f03d444d-5f8e-68d8-d26a-779bb67322bc@molbio.mgh.harvard.edu>
	<CA+8X3fU4rziJRtT+xBy6-yCdVE1LE-F3Z2m2gFCzUuv3P0xNwg@mail.gmail.com>
	<bba2c7d9-ceaa-72e3-c49a-b5d275edbb14@molbio.mgh.harvard.edu>
	<746a1f77-efe0-71fd-3a7b-16b4ebe0ac35@auckland.ac.nz>
	<32956aab-c57a-ca51-232a-ae4b31657740@molbio.mgh.harvard.edu>
Message-ID: <047f66ab1e6b4ee3af50ff39adb89eaf@exch-2p-mbx-t2.ads.tamu.edu>

It appears that you are just trying to use the first row to create a column name for the rest of the column. If that is all you are doing something like this is quicker, but it uses the data frame. 

> set.seed(42)
> tTargTFS <- data.frame(matrix(replicate(100, paste0(sample(LETTERS, 6), collapse="")), 10), stringsAsFactors=FALSE)
> New <- tTargTFS[ -1, ]
> colnames(New)
 [1] "X1"  "X2"  "X3"  "X4"  "X5"  "X6"  "X7"  "X8"  "X9"  "X10"
> colnames(New) <- tTargTFS[1, ]
> colnames(New)
 [1] "XZGTOK" "RYSNXD" "JKNXPI" "XVHFQP" "BOZEMK" "MLBHTV" "OQGWZM" "CMXSLB" "GOUDTJ"
[10] "SYMEXP"
> str(New)
'data.frame':   9 obs. of  10 variables:
 $ XZGTOK: chr  "TDPQKX" "YGLVWC" "MOVDXT" "CMJUXR" ...
 $ RYSNXD: chr  "HUQFAC" "FLEQAH" "NAZDHX" "UOFCBG" ...
 $ JKNXPI: chr  "XYFQTM" "QXUNSC" "TPDBKQ" "TUEVGD" ...
 $ XVHFQP: chr  "XTDGEQ" "DZBXLC" "TSVLYJ" "ELXYFV" ...
 $ BOZEMK: chr  "EDLVHY" "HNASCL" "OPRCGT" "NDUEXS" ...
 $ MLBHTV: chr  "KDHXCU" "MCFVGN" "XYKJDF" "OXITPV" ...
 $ OQGWZM: chr  "QXJGBW" "TCRWEY" "BUNKIF" "PUCWDB" ...
 $ CMXSLB: chr  "MIQLWV" "LCWAJE" "CMPVHR" "HLDSOB" ...
 $ GOUDTJ: chr  "XGCBVK" "VKLEQG" "EADZLR" "CWNDTF" ...
 $ SYMEXP: chr  "RFIHTE" "RDCXUF" "XTYEBA" "HIRFLP" ...


-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matthew
Sent: Monday, July 11, 2016 5:34 PM
To: Rolf Turner
Cc: r-help at r-project.org
Subject: Re: [R] use value in variable to be name of another variable

Hi Rolf,

     Thanks for the warning. I think because my initial efforts used the 
assign function, that Jim provided his solution using it.

     Any suggestions for how it could be done without assign() ?

Matthew

On 7/11/2016 6:31 PM, Rolf Turner wrote:
> On 12/07/16 10:13, Matthew wrote:
>> Hi Jim,
>>
>>    Wow ! And it does exactly what I was looking for.  Thank you very 
>> much.
>>
>> That assign function is pretty nice. I should become more familiar 
>> with it.
>
> Indeed you should, and assign() is indeed nice and useful and handy. 
> But it should be used with care and circumspection.  It *alters the 
> global environment* which is fraught with peril. Generally speaking 
> most things that can be done with assign() (and its companion function 
> get()) are better and more safely done using lists and functions and 
> other "natural" R-ish constructs. Resist the temptation to turn R into 
> a macro language.
>
> cheers,
>
> Rolf Turner
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From samsad.afrin at gmail.com  Tue Jul 12 15:29:35 2016
From: samsad.afrin at gmail.com (Samsad Afrin Himi)
Date: Tue, 12 Jul 2016 15:29:35 +0200
Subject: [R] Error:'subscript out of bounds'
Message-ID: <D63940EA-8B72-43FF-8A9E-0B4FAD342F6C@gmail.com>

Dear R-team,

I have written this code for calculation my data file. But there shows 'subscript out of bounds?.



> computeResponse <- function(data){
+     dataodd <- data[-(1:18),]
+     dataodd <- dataodd[seq(1,nrow(dataodd), 2),]
+     hitsodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[2,2]
+     missesodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[2,1]
+     crejectionsodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[1,2]
+     falsealarmodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[1,1]
+     return(coordinationodd <- data.frame(hitsodd, missesodd,crejectionsodd, falsealarmodd))
+ }
> 
> filenames <- list.files(full.names=TRUE)
> filelist <- lapply(filenames, read.table, fill = TRUE, header = FALSE, sep = "\t")
> coordinationodd <- lapply(filelist, computeResponse)
Error in table(factor(dataodd[, 5]), factor(dataodd[, 15]))[2, 2] : 
  subscript out of bounds
> coordinationodd <- Reduce(rbind, coordinationodd)
Error in Reduce(rbind, coordinationodd) : 
  object 'coordinationodd' not found


Could you please tell me how I can fix this problem?

Best,
Samsad
	[[alternative HTML version deleted]]


From timo at timogrossenbacher.ch  Tue Jul 12 13:28:45 2016
From: timo at timogrossenbacher.ch (timo at timogrossenbacher.ch)
Date: Tue, 12 Jul 2016 13:28:45 +0200 (CEST)
Subject: [R] Forking and adapting an R package
Message-ID: <1880276709.61511.1468322926354.JavaMail.open-xchange@ox11.mail.hostpoint.internal>

Hello. 

I'm trying to adapt the package ?hexbin? to suit my needs. This is the first
time I do this. I've read a bit through Hadley's ?R packages?, but now I'm
pretty lost (from a workflow point of view). I am using RStudio and Hadley's
devtools. 

So I forked the repo I want to adapt: https://github.com/grssnbchr/hexbin  and
cloned it using RStudio (I created a new project). What I basically want to do
is adapt the package slightly and use the adapted source on my use case (an Rmd
file in another location) - ideally, I would call the respective function
(hexbin::grid.hexagons) in the Rmd and the source code of ?hexbin? would be
called and debugged (just for understanding what the package ?hexbin? actually
does in that case, I do not have to build it yet, or even publish it). What is
the workflow for this? 

Also, I tried running devtools::check() and it already fails there: 
R version 3.2.5 (2016-04-14) -- "Very, Very Secure Dishes"

Platform: x86_64-pc-linux-gnu (64-bit)

> devtools::check()
Updating hexbin documentation
Loading hexbin
Creating a generic function for ?plot? from package ?graphics? in package
?hexbin?
Creating a generic function for ?summary? from package ?base? in package
?hexbin?
Setting env vars
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
CFLAGS : -Wall -pedantic
CXXFLAGS: -Wall -pedantic
Building hexbin
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
'/usr/lib/R/bin/R' --no-site-file --no-environ --no-save --no-restore --quiet
CMD build '/home/tgrossen/R/hexbin' --no-resave-data --no-manual 

* checking for file ?/home/tgrossen/R/hexbin/DESCRIPTION? ... OK
* preparing ?hexbin?:
* checking DESCRIPTION meta-information ... OK
* cleaning src
* installing the package to build vignettes
* creating vignettes ... ERROR

Error: processing vignette 'hexagon_binning.Rnw' failed with diagnostics:
 chunk 1 (label = comphexsq) 
Error in eval(expr, envir, enclos) : could not find function ?hexbin?
Execution halted
Error: Command failed (1)

As you can see, I am very much lost. I googled for "adapt R package and debug"
and so forth but couldn't find any tutorial or anything. 

Thanks,

Timo


From sarah.goslee at gmail.com  Tue Jul 12 18:19:13 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 12 Jul 2016 12:19:13 -0400
Subject: [R] Error:'subscript out of bounds'
In-Reply-To: <D63940EA-8B72-43FF-8A9E-0B4FAD342F6C@gmail.com>
References: <D63940EA-8B72-43FF-8A9E-0B4FAD342F6C@gmail.com>
Message-ID: <CAM_vjum+Y7O8f41rPTQGWm4G4EKMb30B6TYveciM1s_ES=2c-w@mail.gmail.com>

You don't provide a reproducible example - what does your data look like?

If this were my problem, I would start by working through the function
step by step with one of my files

data <- filelist[[1]]

dataodd <- data[-(1:18),]
and so on, examining the result at every step with tools like dim() and str()
until I found the problem.

There are more fully-developed debugging tools available, but for such
a short function I wouldn't bother.

Sarah

On Tue, Jul 12, 2016 at 9:29 AM, Samsad Afrin Himi
<samsad.afrin at gmail.com> wrote:
> Dear R-team,
>
> I have written this code for calculation my data file. But there shows 'subscript out of bounds?.
>
>
>
>> computeResponse <- function(data){
> +     dataodd <- data[-(1:18),]
> +     dataodd <- dataodd[seq(1,nrow(dataodd), 2),]
> +     hitsodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[2,2]
> +     missesodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[2,1]
> +     crejectionsodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[1,2]
> +     falsealarmodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[1,1]
> +     return(coordinationodd <- data.frame(hitsodd, missesodd,crejectionsodd, falsealarmodd))
> + }
>>
>> filenames <- list.files(full.names=TRUE)
>> filelist <- lapply(filenames, read.table, fill = TRUE, header = FALSE, sep = "\t")
>> coordinationodd <- lapply(filelist, computeResponse)
> Error in table(factor(dataodd[, 5]), factor(dataodd[, 15]))[2, 2] :
>   subscript out of bounds
>> coordinationodd <- Reduce(rbind, coordinationodd)
> Error in Reduce(rbind, coordinationodd) :
>   object 'coordinationodd' not found
>
>
> Could you please tell me how I can fix this problem?
>
> Best,
> Samsad
>         [[alternative HTML version deleted]]
>


From wdunlap at tibco.com  Tue Jul 12 18:26:15 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 12 Jul 2016 09:26:15 -0700
Subject: [R] Error:'subscript out of bounds'
In-Reply-To: <D63940EA-8B72-43FF-8A9E-0B4FAD342F6C@gmail.com>
References: <D63940EA-8B72-43FF-8A9E-0B4FAD342F6C@gmail.com>
Message-ID: <CAF8bMcaMs6Gf1BZvdFVnKifokYy+HH3gM+e9r2qhwNw5LcnYkg@mail.gmail.com>

Include the levels argument in your calls to factor so the tables
all have the same dimensions.

> table(factor((1:4)>2), factor( (1:4)>0 ))

        TRUE
  FALSE    2
  TRUE     2
> table(factor((1:4)>2, levels=c(FALSE,TRUE)), factor( (1:4)>0,
levels=c(FALSE,TRUE) ))

        FALSE TRUE
  FALSE     0    2
  TRUE      0    2

Trying to extract row 2, column 2 from the former will give a subscript-out-
of-bounds error.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Jul 12, 2016 at 6:29 AM, Samsad Afrin Himi <samsad.afrin at gmail.com>
wrote:

> Dear R-team,
>
> I have written this code for calculation my data file. But there shows
> 'subscript out of bounds?.
>
>
>
> > computeResponse <- function(data){
> +     dataodd <- data[-(1:18),]
> +     dataodd <- dataodd[seq(1,nrow(dataodd), 2),]
> +     hitsodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[2,2]
> +     missesodd <- table(factor(dataodd[,5]), factor(dataodd[,15]))[2,1]
> +     crejectionsodd <- table(factor(dataodd[,5]),
> factor(dataodd[,15]))[1,2]
> +     falsealarmodd <- table(factor(dataodd[,5]),
> factor(dataodd[,15]))[1,1]
> +     return(coordinationodd <- data.frame(hitsodd,
> missesodd,crejectionsodd, falsealarmodd))
> + }
> >
> > filenames <- list.files(full.names=TRUE)
> > filelist <- lapply(filenames, read.table, fill = TRUE, header = FALSE,
> sep = "\t")
> > coordinationodd <- lapply(filelist, computeResponse)
> Error in table(factor(dataodd[, 5]), factor(dataodd[, 15]))[2, 2] :
>   subscript out of bounds
> > coordinationodd <- Reduce(rbind, coordinationodd)
> Error in Reduce(rbind, coordinationodd) :
>   object 'coordinationodd' not found
>
>
> Could you please tell me how I can fix this problem?
>
> Best,
> Samsad
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From aedin at jimmy.harvard.edu  Tue Jul 12 18:48:49 2016
From: aedin at jimmy.harvard.edu (Aedin Culhane)
Date: Tue, 12 Jul 2016 12:48:49 -0400
Subject: [R] use value in variable to be name of another variable
Message-ID: <5d30dcd4-d7f6-e8ab-80b0-b15c01c55495@jimmy.harvard.edu>

Hi  Matt
It looks like you are trying to obtain the upstream sequence data 
information given transcription factor binding sites???

There are a lot of functions in Bioconductor which are optimized for 
this type of analysis. If you wish to either post on the Bioconductor 
mailing list (https://support.bioconductor.org/), or tell me a little 
more, I can perhaps help.

Also see 
http://bioconductor.org/help/search/index.html?q=transcription+factor+binding/. 
There are several workflows and lots of docs on TFBS analysis.

Also Matt as you are local to Boston, I am happy to point you to local 
Boston resources if you are new to R/Bioconductor.

Best
Aedin

-- 
Aedin Culhane
---------------
Department of Biostatistics and Computational Biology,Dana-Farber Cancer Institute
Department of Biostatistics, Harvard TH Chan School of Public Health
email: aedin at jimmy.harvard.edu


From utkarsh.iit at gmail.com  Tue Jul 12 20:10:21 2016
From: utkarsh.iit at gmail.com (Utkarsh Singhal)
Date: Tue, 12 Jul 2016 23:40:21 +0530
Subject: [R] Linear model vs Mixed model
In-Reply-To: <CAJuCY5wRmHOVGxK3kvUHwtAdL85jg0kyLo9GKMEfezdLaBijfQ@mail.gmail.com>
References: <CAJP88kQcCgJ6vg5dVdxPfpMtcZpb72SpcPehA+PoEUWvPUrVRw@mail.gmail.com>
	<CAJuCY5wRmHOVGxK3kvUHwtAdL85jg0kyLo9GKMEfezdLaBijfQ@mail.gmail.com>
Message-ID: <CAJP88kR3C8dgVq+4UyCzedpbsZ_rn4BguOa9+YPwCs5ZoN21aQ@mail.gmail.com>

Hello Thierry,

Thank you for your quick response. Sorry, but I am not sure if I follow
what you said. I get the following outputs from the two models:
> coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
Subject    (Intercept)     Days
308    292.1888 10.46729
309    173.5556 10.46729
310    188.2965 10.46729
330    255.8115 10.46729
331    261.6213 10.46729
332    259.6263 10.46729
333    267.9056 10.46729
334    248.4081 10.46729
335    206.1230 10.46729
337    323.5878 10.46729
349    230.2089 10.46729
350    265.5165 10.46729
351    243.5429 10.46729
352    287.7835 10.46729
369    258.4415 10.46729
370    245.0424 10.46729
371    248.1108 10.46729
372    269.5209 10.46729

> coef(lm(Reaction ~ Days + Subject, sleepstudy))
(Intercept)  295.03104
Days          10.46729
Subject309  -126.90085
Subject310  -111.13256
Subject330   -38.91241
Subject331   -32.69778
Subject332   -34.83176
Subject333   -25.97552
Subject334   -46.83178
Subject335   -92.06379
Subject337    33.58718
Subject349   -66.29936
Subject350   -28.53115
Subject351   -52.03608
Subject352    -4.71229
Subject369   -36.09919
Subject370   -50.43206
Subject371   -47.14979
Subject372   -24.24770

Now, what I expected is the following:

   - 'Intercept' of model-2 to match with Intercept of Subject-308 of
   model-1
   - 'Intercept+Subject309' of model-2 to match with Intercept of
   Subject-309 of model-1
   - and so on...

What am I missing here?

If it is difficult to explain this, can you alternately answer the
following: "Is it possible to define the 'lm' and 'lmer' models above so
they produce the same results (at least in terms of predictions)?"

Thanks again.

Utkarsh Singhal
91.96508.54333


On 12 July 2016 at 19:15, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:

> The parametrisation is different.
>
> The intercept in model 1 is the effect of the "average" subject at days ==
> 0.
> The intercept in model 2 is the effect of the first subject at days == 0.
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-07-12 15:35 GMT+02:00 Utkarsh Singhal <utkarsh.iit at gmail.com>:
>
>> Hi experts,
>>
>> While the slope is coming out to be identical in the two methods below,
>> the
>> intercepts are not. As far as I understand, both are formulations are
>> identical in the sense that these are asking for a slope corresponding to
>> 'Days' and a separate intercept term for each Subject.
>>
>> # Model-1
>> library(lmer)
>> coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
>>
>> # Model-2
>> coef(lm(Reaction ~ Days + Subject, sleepstudy))
>>
>> Can somebody tell me the reason? Are the above formulations actually
>> different or is it due to different optimization method used?
>>
>> Thank you.
>>
>> Utkarsh Singhal
>> 91.96508.54333
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From adeldaoud at gmail.com  Tue Jul 12 20:27:35 2016
From: adeldaoud at gmail.com (adel daoud)
Date: Tue, 12 Jul 2016 20:27:35 +0200
Subject: [R] pairwise deletion in regression models
Message-ID: <CAEJCy7iSrG1q+w1MWHg8Tf8m5VpuB=Qc_WBw+iVJaKYgunLmpg@mail.gmail.com>

Dear R users,



I would like to use a pairwise deletion of missing values in linear
regression (lm or glm preferably). I want to replicate some studies done in
STATA that uses this type of deletion. What options do we have in R to work
with pairwise deletion? Most packages I have found do not have this option,
it seems (lm, glm, plm, psych, sampleSelection).



This question has been raised here
<http://r.789695.n4.nabble.com/set-the-bahavior-that-R-deal-with-missing-values-td803840.html>
and here
<http://r.789695.n4.nabble.com/Pairwise-deletion-in-a-linear-regression-and-in-a-GLM-td4653004.html>,
but without any clear answer.



Any input is welcomed



Thanks in advance


Adel

	[[alternative HTML version deleted]]


From cadeb at usgs.gov  Tue Jul 12 20:48:16 2016
From: cadeb at usgs.gov (Cade, Brian)
Date: Tue, 12 Jul 2016 12:48:16 -0600
Subject: [R] Linear model vs Mixed model
In-Reply-To: <CAJP88kR3C8dgVq+4UyCzedpbsZ_rn4BguOa9+YPwCs5ZoN21aQ@mail.gmail.com>
References: <CAJP88kQcCgJ6vg5dVdxPfpMtcZpb72SpcPehA+PoEUWvPUrVRw@mail.gmail.com>
	<CAJuCY5wRmHOVGxK3kvUHwtAdL85jg0kyLo9GKMEfezdLaBijfQ@mail.gmail.com>
	<CAJP88kR3C8dgVq+4UyCzedpbsZ_rn4BguOa9+YPwCs5ZoN21aQ@mail.gmail.com>
Message-ID: <CAM5M9BSfS-HO7hqU+5=vJ6c+cROG+kWGSfZq95+myA+0peGAAQ@mail.gmail.com>

Your lm() estimates are using the default contrasts of contr.treatment,
providing an intercept corresponding to your subject 308 and the other
subject* estimates are differences from subject 308 intercept.  You could
have specified this with contrasts as contr.sum and the estimates would be
more easily compared to the lmer() model estimates.  They are close but
will never be identical as the lmer() model estimates are based on assuming
a normal distribution with specified variance.  They rarely would be
identical.

Brian

Brian S. Cade, PhD

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  cadeb at usgs.gov <brian_cade at usgs.gov>
tel:  970 226-9326


On Tue, Jul 12, 2016 at 12:10 PM, Utkarsh Singhal <utkarsh.iit at gmail.com>
wrote:

> Hello Thierry,
>
> Thank you for your quick response. Sorry, but I am not sure if I follow
> what you said. I get the following outputs from the two models:
> > coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
> Subject    (Intercept)     Days
> 308    292.1888 10.46729
> 309    173.5556 10.46729
> 310    188.2965 10.46729
> 330    255.8115 10.46729
> 331    261.6213 10.46729
> 332    259.6263 10.46729
> 333    267.9056 10.46729
> 334    248.4081 10.46729
> 335    206.1230 10.46729
> 337    323.5878 10.46729
> 349    230.2089 10.46729
> 350    265.5165 10.46729
> 351    243.5429 10.46729
> 352    287.7835 10.46729
> 369    258.4415 10.46729
> 370    245.0424 10.46729
> 371    248.1108 10.46729
> 372    269.5209 10.46729
>
> > coef(lm(Reaction ~ Days + Subject, sleepstudy))
> (Intercept)  295.03104
> Days          10.46729
> Subject309  -126.90085
> Subject310  -111.13256
> Subject330   -38.91241
> Subject331   -32.69778
> Subject332   -34.83176
> Subject333   -25.97552
> Subject334   -46.83178
> Subject335   -92.06379
> Subject337    33.58718
> Subject349   -66.29936
> Subject350   -28.53115
> Subject351   -52.03608
> Subject352    -4.71229
> Subject369   -36.09919
> Subject370   -50.43206
> Subject371   -47.14979
> Subject372   -24.24770
>
> Now, what I expected is the following:
>
>    - 'Intercept' of model-2 to match with Intercept of Subject-308 of
>    model-1
>    - 'Intercept+Subject309' of model-2 to match with Intercept of
>    Subject-309 of model-1
>    - and so on...
>
> What am I missing here?
>
> If it is difficult to explain this, can you alternately answer the
> following: "Is it possible to define the 'lm' and 'lmer' models above so
> they produce the same results (at least in terms of predictions)?"
>
> Thanks again.
>
> Utkarsh Singhal
> 91.96508.54333
>
>
> On 12 July 2016 at 19:15, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
> > The parametrisation is different.
> >
> > The intercept in model 1 is the effect of the "average" subject at days
> ==
> > 0.
> > The intercept in model 2 is the effect of the first subject at days == 0.
> >
> > ir. Thierry Onkelinx
> > Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and
> > Forest
> > team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> > Kliniekstraat 25
> > 1070 Anderlecht
> > Belgium
> >
> > To call in the statistician after the experiment is done may be no more
> > than asking him to perform a post-mortem examination: he may be able to
> say
> > what the experiment died of. ~ Sir Ronald Aylmer Fisher
> > The plural of anecdote is not data. ~ Roger Brinner
> > The combination of some data and an aching desire for an answer does not
> > ensure that a reasonable answer can be extracted from a given body of
> data.
> > ~ John Tukey
> >
> > 2016-07-12 15:35 GMT+02:00 Utkarsh Singhal <utkarsh.iit at gmail.com>:
> >
> >> Hi experts,
> >>
> >> While the slope is coming out to be identical in the two methods below,
> >> the
> >> intercepts are not. As far as I understand, both are formulations are
> >> identical in the sense that these are asking for a slope corresponding
> to
> >> 'Days' and a separate intercept term for each Subject.
> >>
> >> # Model-1
> >> library(lmer)
> >> coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
> >>
> >> # Model-2
> >> coef(lm(Reaction ~ Days + Subject, sleepstudy))
> >>
> >> Can somebody tell me the reason? Are the above formulations actually
> >> different or is it due to different optimization method used?
> >>
> >> Thank you.
> >>
> >> Utkarsh Singhal
> >> 91.96508.54333
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From roslinaump at gmail.com  Wed Jul 13 00:45:26 2016
From: roslinaump at gmail.com (roslinazairimah zakaria)
Date: Wed, 13 Jul 2016 06:45:26 +0800
Subject: [R] Aggregate rainfall data
Message-ID: <CANTvJZLA-W63e6HPMAgadWMmR6W6wubf5UuB=cVtdW7kqp1J0Q@mail.gmail.com>

Dear R-users,

I have these data:

head(balok, 10); tail(balok, 10)
        Date     Time Rain.mm
1  30/7/2008  9:00:00       0
2  30/7/2008 10:00:00       0
3  30/7/2008 11:00:00       0
4  30/7/2008 12:00:00       0
5  30/7/2008 13:00:00       0
6  30/7/2008 14:00:00       0
7  30/7/2008 15:00:00       0
8  30/7/2008 16:00:00       0
9  30/7/2008 17:00:00       0
10 30/7/2008 18:00:00       0
           Date     Time Rain.mm
63667 4/11/2015  3:00:00       0
63668 4/11/2015  4:00:00       0
63669 4/11/2015  5:00:00       0
63670 4/11/2015  6:00:00       0
63671 4/11/2015  7:00:00       0
63672 4/11/2015  8:00:00       0
63673 4/11/2015  9:00:00     0.1
63674 4/11/2015 10:00:00     0.1
63675 4/11/2015 11:00:00     0.1
63676 4/11/2015 12:00:00    0.1?

> str(balok)
'data.frame':   63676 obs. of  3 variables:
 $ Date   : Factor w/ 2654 levels "1/1/2009","1/1/2010",..: 2056 2056 2056
2056 2056 2056 2056 2056 2056 2056 ...
 $ Time   : Factor w/ 24 levels "1:00:00","10:00:00",..: 24 2 3 4 5 6 7 8 9
10 ...
 $ Rain.mm: Factor w/ 352 levels "0","0.0?","0.1",..: 1 1 1 1 1 1 1 1 1 1
...

and I have change the data as follows:

realdate <- as.Date(balok$Date,format="%d/%m/%Y")
dfdate <- data.frame(date=realdate)
year=as.numeric (format(realdate,"%Y"))
month=as.numeric (format(realdate,"%m"))
day=as.numeric (format(realdate,"%d"))

balok2 <-cbind(dfdate,day,month,year,balok[,2:3])
colnames(balok2)
head(balok2)
        date day month year     Time Rain.mm
1 2008-07-30  30     7 2008  9:00:00       0
2 2008-07-30  30     7 2008 10:00:00       0
3 2008-07-30  30     7 2008 11:00:00       0
4 2008-07-30  30     7 2008 12:00:00       0
5 2008-07-30  30     7 2008 13:00:00       0
6 2008-07-30  30     7 2008 14:00:00       0
...

> balok3 <- balok2[,-1]; head(balok3, n=100)
    day month year     Time Rain.mm
1    30     7 2008  9:00:00       0
2    30     7 2008 10:00:00       0
3    30     7 2008 11:00:00       0
4    30     7 2008 12:00:00       0
5    30     7 2008 13:00:00       0
6    30     7 2008 14:00:00       0
7    30     7 2008 15:00:00       0
8    30     7 2008 16:00:00       0
9    30     7 2008 17:00:00       0
10   30     7 2008 18:00:00       0
11   30     7 2008 19:00:00       0
12   30     7 2008 20:00:00       0
13   30     7 2008 21:00:00       0
14   30     7 2008 22:00:00       0
15   30     7 2008 23:00:00       0
16   30     7 2008 24:00:00       0
17   31     7 2008  1:00:00       0
18   31     7 2008  2:00:00       0
19   31     7 2008  3:00:00       0
20   31     7 2008  4:00:00       0
21   31     7 2008  5:00:00       0
22   31     7 2008  6:00:00       0
23   31     7 2008  7:00:00       0
24   31     7 2008  8:00:00       0
25   31     7 2008  9:00:00       0
26   31     7 2008 10:00:00       0
27   31     7 2008 11:00:00       0
28   31     7 2008 12:00:00       0
29   31     7 2008 13:00:00       0
30   31     7 2008 14:00:00       0
31   31     7 2008 15:00:00       0
32   31     7 2008 16:00:00       0
33   31     7 2008 17:00:00       0
34   31     7 2008 18:00:00       0
35   31     7 2008 19:00:00       0
36   31     7 2008 20:00:00       0
37   31     7 2008 21:00:00       0
38   31     7 2008 22:00:00       0
39   31     7 2008 23:00:00       0
40   31     7 2008 24:00:00       0
41    1     8 2008  1:00:00       0
42    1     8 2008  2:00:00       0
43    1     8 2008  3:00:00       0
44    1     8 2008  4:00:00       0
45    1     8 2008  5:00:00       0
46    1     8 2008  6:00:00       0
47    1     8 2008  7:00:00       0
48    1     8 2008  8:00:00       0
49    1     8 2008  9:00:00       0
50    1     8 2008 10:00:00       0
51    1     8 2008 11:00:00       0
52    1     8 2008 12:00:00       0
53    1     8 2008 13:00:00       0
54    1     8 2008 14:00:00       0
55    1     8 2008 15:00:00       0
56    1     8 2008 16:00:00       0
57    1     8 2008 17:00:00       0
58    1     8 2008 18:00:00       0
59    1     8 2008 19:00:00       0
60    1     8 2008 20:00:00       0
61    1     8 2008 21:00:00       0
62    1     8 2008 22:00:00       0
63    1     8 2008 23:00:00       0
64    1     8 2008 24:00:00       0
65    2     8 2008  1:00:00       0
66    2     8 2008  2:00:00       0
67    2     8 2008  3:00:00       0
68    2     8 2008  4:00:00       0
69    2     8 2008  5:00:00       0
70    2     8 2008  6:00:00       0
71    2     8 2008  7:00:00       0
72    2     8 2008  8:00:00       0
73    2     8 2008  9:00:00       0
74    2     8 2008 10:00:00       0
75    2     8 2008 11:00:00       0
76    2     8 2008 12:00:00       0
77    2     8 2008 13:00:00       0
78    2     8 2008 14:00:00       0
79    2     8 2008 15:00:00       0
80    2     8 2008 16:00:00       0
81    2     8 2008 17:00:00       0
82    2     8 2008 18:00:00       0
83    2     8 2008 19:00:00       0
84    2     8 2008 20:00:00       0
85    2     8 2008 21:00:00       0
86    2     8 2008 22:00:00       0
87    2     8 2008 23:00:00       0
88    2     8 2008 24:00:00    11.1
89    3     8 2008  1:00:00     0.4
90    3     8 2008  2:00:00       0
91    3     8 2008  3:00:00       0
92    3     8 2008  4:00:00       0
93    3     8 2008  5:00:00       0
94    3     8 2008  6:00:00       0
95    3     8 2008  7:00:00       0
96    3     8 2008  8:00:00       0
97    3     8 2008  9:00:00       0
98    3     8 2008 10:00:00       0
99    3     8 2008 11:00:00       0
100   3     8 2008 12:00:00       0

The rainfall data is in hourly unit, and I would like to sum the Rain.mm
according to month.  I tried to use aggregate(), but I got this message:

dt <- balok4
str(dt)
aggbalok <- aggregate(dt[,5], by=dt[,c(1,4)],FUN=sum, na.rm=TRUE)
aggbalok

Error in Summary.factor(1L, na.rm = TRUE) :
  sum not meaningful for factors


Thank you so much for any help given.

Roslina

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul 13 03:42:00 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 12 Jul 2016 18:42:00 -0700
Subject: [R] Aggregate rainfall data
In-Reply-To: <CANTvJZLA-W63e6HPMAgadWMmR6W6wubf5UuB=cVtdW7kqp1J0Q@mail.gmail.com>
References: <CANTvJZLA-W63e6HPMAgadWMmR6W6wubf5UuB=cVtdW7kqp1J0Q@mail.gmail.com>
Message-ID: <6CD80698-D9FE-4D11-BB90-F918EAE2BCD8@comcast.net>


> On Jul 12, 2016, at 3:45 PM, roslinazairimah zakaria <roslinaump at gmail.com> wrote:
> 
> Dear R-users,
> 
> I have these data:
> 
> head(balok, 10); tail(balok, 10)
>        Date     Time Rain.mm
> 1  30/7/2008  9:00:00       0
> 2  30/7/2008 10:00:00       0
> 3  30/7/2008 11:00:00       0
> 4  30/7/2008 12:00:00       0
> 5  30/7/2008 13:00:00       0
> 6  30/7/2008 14:00:00       0
> 7  30/7/2008 15:00:00       0
> 8  30/7/2008 16:00:00       0
> 9  30/7/2008 17:00:00       0
> 10 30/7/2008 18:00:00       0
>           Date     Time Rain.mm
> 63667 4/11/2015  3:00:00       0
> 63668 4/11/2015  4:00:00       0
> 63669 4/11/2015  5:00:00       0
> 63670 4/11/2015  6:00:00       0
> 63671 4/11/2015  7:00:00       0
> 63672 4/11/2015  8:00:00       0
> 63673 4/11/2015  9:00:00     0.1
> 63674 4/11/2015 10:00:00     0.1
> 63675 4/11/2015 11:00:00     0.1
> 63676 4/11/2015 12:00:00    0.1?
> 
>> str(balok)
> 'data.frame':   63676 obs. of  3 variables:
> $ Date   : Factor w/ 2654 levels "1/1/2009","1/1/2010",..: 2056 2056 2056
> 2056 2056 2056 2056 2056 2056 2056 ...
> $ Time   : Factor w/ 24 levels "1:00:00","10:00:00",..: 24 2 3 4 5 6 7 8 9
> 10 ...
> $ Rain.mm: Factor w/ 352 levels "0","0.0?","0.1",..: 1 1 1 1 1 1 1 1 1 1

Thar's your problem:

  Rain.mm: Factor w/ 352 levels "0","0.0?","0.1"

Need to use the standard fix for the screwed-up-factor-on-input-problem

  balok$Rain.mm2 <- as.numeric( as.character(balok$Rain.mm) )

Cannot just do as.numeric because factors are actually already numeric.

-- 
David.


> ...
> 
> and I have change the data as follows:
> 
> realdate <- as.Date(balok$Date,format="%d/%m/%Y")
> dfdate <- data.frame(date=realdate)
> year=as.numeric (format(realdate,"%Y"))
> month=as.numeric (format(realdate,"%m"))
> day=as.numeric (format(realdate,"%d"))
> 
> balok2 <-cbind(dfdate,day,month,year,balok[,2:3])
> colnames(balok2)
> head(balok2)
>        date day month year     Time Rain.mm
> 1 2008-07-30  30     7 2008  9:00:00       0
> 2 2008-07-30  30     7 2008 10:00:00       0
> 3 2008-07-30  30     7 2008 11:00:00       0
> 4 2008-07-30  30     7 2008 12:00:00       0
> 5 2008-07-30  30     7 2008 13:00:00       0
> 6 2008-07-30  30     7 2008 14:00:00       0
> ...
> 
>> balok3 <- balok2[,-1]; head(balok3, n=100)
>    day month year     Time Rain.mm
> 1    30     7 2008  9:00:00       0
> 2    30     7 2008 10:00:00       0
> 3    30     7 2008 11:00:00       0
> 4    30     7 2008 12:00:00       0
> 5    30     7 2008 13:00:00       0
> 6    30     7 2008 14:00:00       0
> 7    30     7 2008 15:00:00       0
> 8    30     7 2008 16:00:00       0
> 9    30     7 2008 17:00:00       0
> 10   30     7 2008 18:00:00       0
> 11   30     7 2008 19:00:00       0
> 12   30     7 2008 20:00:00       0
> 13   30     7 2008 21:00:00       0
> 14   30     7 2008 22:00:00       0
> 15   30     7 2008 23:00:00       0
> 16   30     7 2008 24:00:00       0
> 17   31     7 2008  1:00:00       0
> 18   31     7 2008  2:00:00       0
> 19   31     7 2008  3:00:00       0
> 20   31     7 2008  4:00:00       0
> 21   31     7 2008  5:00:00       0
> 22   31     7 2008  6:00:00       0
> 23   31     7 2008  7:00:00       0
> 24   31     7 2008  8:00:00       0
> 25   31     7 2008  9:00:00       0
> 26   31     7 2008 10:00:00       0
> 27   31     7 2008 11:00:00       0
> 28   31     7 2008 12:00:00       0
> 29   31     7 2008 13:00:00       0
> 30   31     7 2008 14:00:00       0
> 31   31     7 2008 15:00:00       0
> 32   31     7 2008 16:00:00       0
> 33   31     7 2008 17:00:00       0
> 34   31     7 2008 18:00:00       0
> 35   31     7 2008 19:00:00       0
> 36   31     7 2008 20:00:00       0
> 37   31     7 2008 21:00:00       0
> 38   31     7 2008 22:00:00       0
> 39   31     7 2008 23:00:00       0
> 40   31     7 2008 24:00:00       0
> 41    1     8 2008  1:00:00       0
> 42    1     8 2008  2:00:00       0
> 43    1     8 2008  3:00:00       0
> 44    1     8 2008  4:00:00       0
> 45    1     8 2008  5:00:00       0
> 46    1     8 2008  6:00:00       0
> 47    1     8 2008  7:00:00       0
> 48    1     8 2008  8:00:00       0
> 49    1     8 2008  9:00:00       0
> 50    1     8 2008 10:00:00       0
> 51    1     8 2008 11:00:00       0
> 52    1     8 2008 12:00:00       0
> 53    1     8 2008 13:00:00       0
> 54    1     8 2008 14:00:00       0
> 55    1     8 2008 15:00:00       0
> 56    1     8 2008 16:00:00       0
> 57    1     8 2008 17:00:00       0
> 58    1     8 2008 18:00:00       0
> 59    1     8 2008 19:00:00       0
> 60    1     8 2008 20:00:00       0
> 61    1     8 2008 21:00:00       0
> 62    1     8 2008 22:00:00       0
> 63    1     8 2008 23:00:00       0
> 64    1     8 2008 24:00:00       0
> 65    2     8 2008  1:00:00       0
> 66    2     8 2008  2:00:00       0
> 67    2     8 2008  3:00:00       0
> 68    2     8 2008  4:00:00       0
> 69    2     8 2008  5:00:00       0
> 70    2     8 2008  6:00:00       0
> 71    2     8 2008  7:00:00       0
> 72    2     8 2008  8:00:00       0
> 73    2     8 2008  9:00:00       0
> 74    2     8 2008 10:00:00       0
> 75    2     8 2008 11:00:00       0
> 76    2     8 2008 12:00:00       0
> 77    2     8 2008 13:00:00       0
> 78    2     8 2008 14:00:00       0
> 79    2     8 2008 15:00:00       0
> 80    2     8 2008 16:00:00       0
> 81    2     8 2008 17:00:00       0
> 82    2     8 2008 18:00:00       0
> 83    2     8 2008 19:00:00       0
> 84    2     8 2008 20:00:00       0
> 85    2     8 2008 21:00:00       0
> 86    2     8 2008 22:00:00       0
> 87    2     8 2008 23:00:00       0
> 88    2     8 2008 24:00:00    11.1
> 89    3     8 2008  1:00:00     0.4
> 90    3     8 2008  2:00:00       0
> 91    3     8 2008  3:00:00       0
> 92    3     8 2008  4:00:00       0
> 93    3     8 2008  5:00:00       0
> 94    3     8 2008  6:00:00       0
> 95    3     8 2008  7:00:00       0
> 96    3     8 2008  8:00:00       0
> 97    3     8 2008  9:00:00       0
> 98    3     8 2008 10:00:00       0
> 99    3     8 2008 11:00:00       0
> 100   3     8 2008 12:00:00       0
> 
> The rainfall data is in hourly unit, and I would like to sum the Rain.mm
> according to month.  I tried to use aggregate(), but I got this message:
> 
> dt <- balok4
> str(dt)
> aggbalok <- aggregate(dt[,5], by=dt[,c(1,4)],FUN=sum, na.rm=TRUE)
> aggbalok
> 
> Error in Summary.factor(1L, na.rm = TRUE) :
>  sum not meaningful for factors
> 
> 
> Thank you so much for any help given.
> 
> Roslina
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From petr.pikal at precheza.cz  Wed Jul 13 08:21:33 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 13 Jul 2016 06:21:33 +0000
Subject: [R] pairwise deletion in regression models
In-Reply-To: <CAEJCy7iSrG1q+w1MWHg8Tf8m5VpuB=Qc_WBw+iVJaKYgunLmpg@mail.gmail.com>
References: <CAEJCy7iSrG1q+w1MWHg8Tf8m5VpuB=Qc_WBw+iVJaKYgunLmpg@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034C60@SRVEXCHMBX.precheza.cz>

Hi

http://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari

The package is probably not available on CRAN but seems to be still maintained on github.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of adel
> daoud
> Sent: Tuesday, July 12, 2016 8:28 PM
> To: r-help <r-help at r-project.org>
> Subject: [R] pairwise deletion in regression models
>
> Dear R users,
>
>
>
> I would like to use a pairwise deletion of missing values in linear regression
> (lm or glm preferably). I want to replicate some studies done in STATA that
> uses this type of deletion. What options do we have in R to work with
> pairwise deletion? Most packages I have found do not have this option, it
> seems (lm, glm, plm, psych, sampleSelection).
>
>
>
> This question has been raised here
> <http://r.789695.n4.nabble.com/set-the-bahavior-that-R-deal-with-missing-
> values-td803840.html>
> and here
> <http://r.789695.n4.nabble.com/Pairwise-deletion-in-a-linear-regression-
> and-in-a-GLM-td4653004.html>,
> but without any clear answer.
>
>
>
> Any input is welcomed
>
>
>
> Thanks in advance
>
>
> Adel
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From Ramgad82 at gmx.net  Wed Jul 13 09:59:33 2016
From: Ramgad82 at gmx.net (Dagmar)
Date: Wed, 13 Jul 2016 09:59:33 +0200
Subject: [R] graph: horizontal bar reflecting number of data
Message-ID: <c12faa1e-9f2f-77b0-fc07-adbb1bc609c2@gmx.net>

Dear all,

I hope someone can help with my problem:

I have a dataframe like this:

datframe <- data.frame(Name=c("Kati","Kati","Kati","Leon","Leon","Leon" 
), week =c("1","2", "3", "1","2", "3"), numberdata =c("5","12", "1", 
"6","2", "5"))
datframe

I want to create a graph like the attached one (jpg). I can't bring the 
bars to  line up by week... desperating.

(in case you don't see the jpg: I want a horizontal bar which reflects 
the number of data per week by the colour of the shading).

Can anyone help?

Tagmarie





From adel.daoud at sociology.gu.se  Wed Jul 13 10:00:29 2016
From: adel.daoud at sociology.gu.se (Adel Daoud)
Date: Wed, 13 Jul 2016 10:00:29 +0200
Subject: [R] pairwise deletion in regression models
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034C60@SRVEXCHMBX.precheza.cz>
References: <CAEJCy7iSrG1q+w1MWHg8Tf8m5VpuB=Qc_WBw+iVJaKYgunLmpg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034C60@SRVEXCHMBX.precheza.cz>
Message-ID: <CAEJCy7hxCJuMZmYOWWGwTJQ8WqY+8Jh9jUGN-P4bZP4=_VFHYg@mail.gmail.com>

Thanks Petr for the suggestion.



I just took the regtools package for a quick test drive. It looks
promising, but it still needs further development to make it a viable
option. You will not get a standard regression output (as in lm or glm),
only the regression coefficients (without standard errors). I will be happy
to try it out once it is more robust.



I am a bit puzzled that the R universe seems to lack a robust package that
allows for pairwise deletion (which is standard in the otherwise poorer
software packages STATA or SPSS?). Would be very happy if anyone can show
me otherwise.

Best,

Adel

--

Adel Daoud, PhD, Researcher

*Newly published:*

* Daoud, Adel and Kohl, Sebastian, *How Much Do Sociologists Write About
Economic Topics? Using Big-Data to Test Some Conventional Views in Economic
Sociology, 1890 to 2014*. Max Planck Institute for the Study of Societies,
Discussion Paper 16/7  <http://www.mpifg.de/pu/mpifg_dp/dp16-7.pdf>

* Daoud, Adel, Bj?rn Haller?d, and Debarati Guha-Sapir, (2016) ?What Is the
Association between Absolute Child Poverty, Poor Governance, and Natural
Disasters? A Global Comparison of Some of the Realities of Climate
Change?, *PLoS
ONE* 11(4)
<http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153296>

*Shailen Nandy, Adel Daoud, David Gordon, (2016), Examining the changing
profile of undernutrition in the context of food price rises and greater
inequality, *Social Science & Medicine*, Volume 149, Pages 153?163
<http://www.sciencedirect.com/science/article/pii/S0277953615302446>


*Department of Sociology and Work Science,*

*University of Gothenburg*

Box 720

405 30, G?teborg, Sweden

Email: Adel.daoud at sociology.gu.se

Website: http://adeldaoud.se/  <http://adeldaoud.se/>

On Wed, Jul 13, 2016 at 8:21 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
>
> http://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari
>
> The package is probably not available on CRAN but seems to be still
> maintained on github.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of adel
> > daoud
> > Sent: Tuesday, July 12, 2016 8:28 PM
> > To: r-help <r-help at r-project.org>
> > Subject: [R] pairwise deletion in regression models
> >
> > Dear R users,
> >
> >
> >
> > I would like to use a pairwise deletion of missing values in linear
> regression
> > (lm or glm preferably). I want to replicate some studies done in STATA
> that
> > uses this type of deletion. What options do we have in R to work with
> > pairwise deletion? Most packages I have found do not have this option, it
> > seems (lm, glm, plm, psych, sampleSelection).
> >
> >
> >
> > This question has been raised here
> > <
> http://r.789695.n4.nabble.com/set-the-bahavior-that-R-deal-with-missing-
> > values-td803840.html>
> > and here
> > <http://r.789695.n4.nabble.com/Pairwise-deletion-in-a-linear-regression-
> > and-in-a-GLM-td4653004.html>,
> > but without any clear answer.
> >
> >
> >
> > Any input is welcomed
> >
> >
> >
> > Thanks in advance
> >
> >
> > Adel
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Jul 13 10:53:41 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 13 Jul 2016 08:53:41 +0000
Subject: [R] pairwise deletion in regression models
In-Reply-To: <CAEJCy7hxCJuMZmYOWWGwTJQ8WqY+8Jh9jUGN-P4bZP4=_VFHYg@mail.gmail.com>
References: <CAEJCy7iSrG1q+w1MWHg8Tf8m5VpuB=Qc_WBw+iVJaKYgunLmpg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034C60@SRVEXCHMBX.precheza.cz>
	<CAEJCy7hxCJuMZmYOWWGwTJQ8WqY+8Jh9jUGN-P4bZP4=_VFHYg@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034CDA@SRVEXCHMBX.precheza.cz>

Hi

Hm. Are you 100% sure that other software packages can do pairwise deletion in OLS or GLM? I am not at all familiar with them but

http://www.ats.ucla.edu/stat/spss/modules/stats.htm

suggests that option pairwise is available with corr and I believe the same option exists in cor function in R.

My statistical knowledge is inferior but I just cannot imagine how whole model could be computed when one value is missing.

Cheers
Petr

From: adeldaoud at gmail.com [mailto:adeldaoud at gmail.com] On Behalf Of Adel Daoud
Sent: Wednesday, July 13, 2016 10:00 AM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] pairwise deletion in regression models

Thanks Petr for the suggestion.

I just took the regtools package for a quick test drive. It looks promising, but it still needs further development to make it a viable option. You will not get a standard regression output (as in lm or glm), only the regression coefficients (without standard errors). I will be happy to try it out once it is more robust.

I am a bit puzzled that the R universe seems to lack a robust package that allows for pairwise deletion (which is standard in the otherwise poorer software packages STATA or SPSS?). Would be very happy if anyone can show me otherwise.


Best,

Adel

--

Adel Daoud, PhD, Researcher

Newly published:

* Daoud, Adel and Kohl, Sebastian, How Much Do Sociologists Write About Economic Topics? Using Big-Data to Test Some Conventional Views in Economic Sociology, 1890 to 2014. Max Planck Institute for the Study of Societies, Discussion Paper 16/7 <http://www.mpifg.de/pu/mpifg_dp/dp16-7.pdf>

* Daoud, Adel, Bj?rn Haller?d, and Debarati Guha-Sapir, (2016) ?What Is the Association between Absolute Child Poverty, Poor Governance, and Natural Disasters? A Global Comparison of Some of the Realities of Climate Change?, PLoS ONE 11(4)<http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153296>

*Shailen Nandy, Adel Daoud, David Gordon, (2016), Examining the changing profile of undernutrition in the context of food price rises and greater inequality, Social Science & Medicine, Volume 149, Pages 153?163<http://www.sciencedirect.com/science/article/pii/S0277953615302446>



Department of Sociology and Work Science,

University of Gothenburg

Box 720

405 30, G?teborg, Sweden

Email: Adel.daoud at sociology.gu.se<mailto:Adel.daoud at sociology.gu.se>

Website: http://adeldaoud.se/ <http://adeldaoud.se/>

On Wed, Jul 13, 2016 at 8:21 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

http://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari

The package is probably not available on CRAN but seems to be still maintained on github.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of adel
> daoud
> Sent: Tuesday, July 12, 2016 8:28 PM
> To: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: [R] pairwise deletion in regression models
>
> Dear R users,
>
>
>
> I would like to use a pairwise deletion of missing values in linear regression
> (lm or glm preferably). I want to replicate some studies done in STATA that
> uses this type of deletion. What options do we have in R to work with
> pairwise deletion? Most packages I have found do not have this option, it
> seems (lm, glm, plm, psych, sampleSelection).
>
>
>
> This question has been raised here
> <http://r.789695.n4.nabble.com/set-the-bahavior-that-R-deal-with-missing-
> values-td803840.html>
> and here
> <http://r.789695.n4.nabble.com/Pairwise-deletion-in-a-linear-regression-
> and-in-a-GLM-td4653004.html>,
> but without any clear answer.
>
>
>
> Any input is welcomed
>
>
>
> Thanks in advance
>
>
> Adel
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From roslinaump at gmail.com  Wed Jul 13 12:21:56 2016
From: roslinaump at gmail.com (roslinazairimah zakaria)
Date: Wed, 13 Jul 2016 18:21:56 +0800
Subject: [R] Aggregate rainfall data
In-Reply-To: <6CD80698-D9FE-4D11-BB90-F918EAE2BCD8@comcast.net>
References: <CANTvJZLA-W63e6HPMAgadWMmR6W6wubf5UuB=cVtdW7kqp1J0Q@mail.gmail.com>
	<6CD80698-D9FE-4D11-BB90-F918EAE2BCD8@comcast.net>
Message-ID: <CANTvJZJSN=7i-nez4J7APX+HaaNh6B0j-aHTRSzPaaw_LK+vLg@mail.gmail.com>

Dear David,

I got your point.  How do I remove the data that contain "0.0?".

I tried : balok <- cbind(balok3[,-5], balok3$Rain.mm[balok3$Rain.mm==0.0?]
<- NA)

However all the Rain.mm column all become NA.

   day month year     Time balok3$Rain.mm[balok3$Rain.mm == "0.0?"] <- NA
1   30     7 2008  9:00:00                                             NA
2   30     7 2008 10:00:00                                             NA
3   30     7 2008 11:00:00                                             NA
4   30     7 2008 12:00:00                                             NA
5   30     7 2008 13:00:00                                             NA
6   30     7 2008 14:00:00                                             NA
7   30     7 2008 15:00:00                                             NA
8   30     7 2008 16:00:00                                             NA
9   30     7 2008 17:00:00                                             NA
10  30     7 2008 18:00:00                                             NA

Thank you so much.


On Wed, Jul 13, 2016 at 9:42 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jul 12, 2016, at 3:45 PM, roslinazairimah zakaria <
> roslinaump at gmail.com> wrote:
> >
> > Dear R-users,
> >
> > I have these data:
> >
> > head(balok, 10); tail(balok, 10)
> >        Date     Time Rain.mm
> > 1  30/7/2008  9:00:00       0
> > 2  30/7/2008 10:00:00       0
> > 3  30/7/2008 11:00:00       0
> > 4  30/7/2008 12:00:00       0
> > 5  30/7/2008 13:00:00       0
> > 6  30/7/2008 14:00:00       0
> > 7  30/7/2008 15:00:00       0
> > 8  30/7/2008 16:00:00       0
> > 9  30/7/2008 17:00:00       0
> > 10 30/7/2008 18:00:00       0
> >           Date     Time Rain.mm
> > 63667 4/11/2015  3:00:00       0
> > 63668 4/11/2015  4:00:00       0
> > 63669 4/11/2015  5:00:00       0
> > 63670 4/11/2015  6:00:00       0
> > 63671 4/11/2015  7:00:00       0
> > 63672 4/11/2015  8:00:00       0
> > 63673 4/11/2015  9:00:00     0.1
> > 63674 4/11/2015 10:00:00     0.1
> > 63675 4/11/2015 11:00:00     0.1
> > 63676 4/11/2015 12:00:00    0.1?
> >
> >> str(balok)
> > 'data.frame':   63676 obs. of  3 variables:
> > $ Date   : Factor w/ 2654 levels "1/1/2009","1/1/2010",..: 2056 2056 2056
> > 2056 2056 2056 2056 2056 2056 2056 ...
> > $ Time   : Factor w/ 24 levels "1:00:00","10:00:00",..: 24 2 3 4 5 6 7 8
> 9
> > 10 ...
> > $ Rain.mm: Factor w/ 352 levels "0","0.0?","0.1",..: 1 1 1 1 1 1 1 1 1 1
>
> Thar's your problem:
>
>   Rain.mm: Factor w/ 352 levels "0","0.0?","0.1"
>
> Need to use the standard fix for the screwed-up-factor-on-input-problem
>
>   balok$Rain.mm2 <- as.numeric( as.character(balok$Rain.mm) )
>
> Cannot just do as.numeric because factors are actually already numeric.
>
> --
> David.
>
>
> > ...
> >
> > and I have change the data as follows:
> >
> > realdate <- as.Date(balok$Date,format="%d/%m/%Y")
> > dfdate <- data.frame(date=realdate)
> > year=as.numeric (format(realdate,"%Y"))
> > month=as.numeric (format(realdate,"%m"))
> > day=as.numeric (format(realdate,"%d"))
> >
> > balok2 <-cbind(dfdate,day,month,year,balok[,2:3])
> > colnames(balok2)
> > head(balok2)
> >        date day month year     Time Rain.mm
> > 1 2008-07-30  30     7 2008  9:00:00       0
> > 2 2008-07-30  30     7 2008 10:00:00       0
> > 3 2008-07-30  30     7 2008 11:00:00       0
> > 4 2008-07-30  30     7 2008 12:00:00       0
> > 5 2008-07-30  30     7 2008 13:00:00       0
> > 6 2008-07-30  30     7 2008 14:00:00       0
> > ...
> >
> >> balok3 <- balok2[,-1]; head(balok3, n=100)
> >    day month year     Time Rain.mm
> > 1    30     7 2008  9:00:00       0
> > 2    30     7 2008 10:00:00       0
> > 3    30     7 2008 11:00:00       0
> > 4    30     7 2008 12:00:00       0
> > 5    30     7 2008 13:00:00       0
> > 6    30     7 2008 14:00:00       0
> > 7    30     7 2008 15:00:00       0
> > 8    30     7 2008 16:00:00       0
> > 9    30     7 2008 17:00:00       0
> > 10   30     7 2008 18:00:00       0
> > 11   30     7 2008 19:00:00       0
> > 12   30     7 2008 20:00:00       0
> > 13   30     7 2008 21:00:00       0
> > 14   30     7 2008 22:00:00       0
> > 15   30     7 2008 23:00:00       0
> > 16   30     7 2008 24:00:00       0
> > 17   31     7 2008  1:00:00       0
> > 18   31     7 2008  2:00:00       0
> > 19   31     7 2008  3:00:00       0
> > 20   31     7 2008  4:00:00       0
> > 21   31     7 2008  5:00:00       0
> > 22   31     7 2008  6:00:00       0
> > 23   31     7 2008  7:00:00       0
> > 24   31     7 2008  8:00:00       0
> > 25   31     7 2008  9:00:00       0
> > 26   31     7 2008 10:00:00       0
> > 27   31     7 2008 11:00:00       0
> > 28   31     7 2008 12:00:00       0
> > 29   31     7 2008 13:00:00       0
> > 30   31     7 2008 14:00:00       0
> > 31   31     7 2008 15:00:00       0
> > 32   31     7 2008 16:00:00       0
> > 33   31     7 2008 17:00:00       0
> > 34   31     7 2008 18:00:00       0
> > 35   31     7 2008 19:00:00       0
> > 36   31     7 2008 20:00:00       0
> > 37   31     7 2008 21:00:00       0
> > 38   31     7 2008 22:00:00       0
> > 39   31     7 2008 23:00:00       0
> > 40   31     7 2008 24:00:00       0
> > 41    1     8 2008  1:00:00       0
> > 42    1     8 2008  2:00:00       0
> > 43    1     8 2008  3:00:00       0
> > 44    1     8 2008  4:00:00       0
> > 45    1     8 2008  5:00:00       0
> > 46    1     8 2008  6:00:00       0
> > 47    1     8 2008  7:00:00       0
> > 48    1     8 2008  8:00:00       0
> > 49    1     8 2008  9:00:00       0
> > 50    1     8 2008 10:00:00       0
> > 51    1     8 2008 11:00:00       0
> > 52    1     8 2008 12:00:00       0
> > 53    1     8 2008 13:00:00       0
> > 54    1     8 2008 14:00:00       0
> > 55    1     8 2008 15:00:00       0
> > 56    1     8 2008 16:00:00       0
> > 57    1     8 2008 17:00:00       0
> > 58    1     8 2008 18:00:00       0
> > 59    1     8 2008 19:00:00       0
> > 60    1     8 2008 20:00:00       0
> > 61    1     8 2008 21:00:00       0
> > 62    1     8 2008 22:00:00       0
> > 63    1     8 2008 23:00:00       0
> > 64    1     8 2008 24:00:00       0
> > 65    2     8 2008  1:00:00       0
> > 66    2     8 2008  2:00:00       0
> > 67    2     8 2008  3:00:00       0
> > 68    2     8 2008  4:00:00       0
> > 69    2     8 2008  5:00:00       0
> > 70    2     8 2008  6:00:00       0
> > 71    2     8 2008  7:00:00       0
> > 72    2     8 2008  8:00:00       0
> > 73    2     8 2008  9:00:00       0
> > 74    2     8 2008 10:00:00       0
> > 75    2     8 2008 11:00:00       0
> > 76    2     8 2008 12:00:00       0
> > 77    2     8 2008 13:00:00       0
> > 78    2     8 2008 14:00:00       0
> > 79    2     8 2008 15:00:00       0
> > 80    2     8 2008 16:00:00       0
> > 81    2     8 2008 17:00:00       0
> > 82    2     8 2008 18:00:00       0
> > 83    2     8 2008 19:00:00       0
> > 84    2     8 2008 20:00:00       0
> > 85    2     8 2008 21:00:00       0
> > 86    2     8 2008 22:00:00       0
> > 87    2     8 2008 23:00:00       0
> > 88    2     8 2008 24:00:00    11.1
> > 89    3     8 2008  1:00:00     0.4
> > 90    3     8 2008  2:00:00       0
> > 91    3     8 2008  3:00:00       0
> > 92    3     8 2008  4:00:00       0
> > 93    3     8 2008  5:00:00       0
> > 94    3     8 2008  6:00:00       0
> > 95    3     8 2008  7:00:00       0
> > 96    3     8 2008  8:00:00       0
> > 97    3     8 2008  9:00:00       0
> > 98    3     8 2008 10:00:00       0
> > 99    3     8 2008 11:00:00       0
> > 100   3     8 2008 12:00:00       0
> >
> > The rainfall data is in hourly unit, and I would like to sum the Rain.mm
> > according to month.  I tried to use aggregate(), but I got this message:
> >
> > dt <- balok4
> > str(dt)
> > aggbalok <- aggregate(dt[,5], by=dt[,c(1,4)],FUN=sum, na.rm=TRUE)
> > aggbalok
> >
> > Error in Summary.factor(1L, na.rm = TRUE) :
> >  sum not meaningful for factors
> >
> >
> > Thank you so much for any help given.
> >
> > Roslina
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>


-- 
*Dr. Roslinazairimah Binti Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Deputy Dean (Academic & Student Affairs)
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From bob at rudis.net  Wed Jul 13 13:04:44 2016
From: bob at rudis.net (boB Rudis)
Date: Wed, 13 Jul 2016 07:04:44 -0400
Subject: [R] Aggregate rainfall data
In-Reply-To: <CANTvJZJSN=7i-nez4J7APX+HaaNh6B0j-aHTRSzPaaw_LK+vLg@mail.gmail.com>
References: <CANTvJZLA-W63e6HPMAgadWMmR6W6wubf5UuB=cVtdW7kqp1J0Q@mail.gmail.com>
	<6CD80698-D9FE-4D11-BB90-F918EAE2BCD8@comcast.net>
	<CANTvJZJSN=7i-nez4J7APX+HaaNh6B0j-aHTRSzPaaw_LK+vLg@mail.gmail.com>
Message-ID: <CAJ4QxaMRed1P6urrKkwWZ6Kbu=muruA00ezu5JYTWgNcoPPGTQ@mail.gmail.com>

use `gsub()` after the `as.character()` conversion to remove
everything but valid numeric components from the strings.

On Wed, Jul 13, 2016 at 6:21 AM, roslinazairimah zakaria
<roslinaump at gmail.com> wrote:
> Dear David,
>
> I got your point.  How do I remove the data that contain "0.0?".
>
> I tried : balok <- cbind(balok3[,-5], balok3$Rain.mm[balok3$Rain.mm==0.0?]
> <- NA)
>
> However all the Rain.mm column all become NA.
>
>    day month year     Time balok3$Rain.mm[balok3$Rain.mm == "0.0?"] <- NA
> 1   30     7 2008  9:00:00                                             NA
> 2   30     7 2008 10:00:00                                             NA
> 3   30     7 2008 11:00:00                                             NA
> 4   30     7 2008 12:00:00                                             NA
> 5   30     7 2008 13:00:00                                             NA
> 6   30     7 2008 14:00:00                                             NA
> 7   30     7 2008 15:00:00                                             NA
> 8   30     7 2008 16:00:00                                             NA
> 9   30     7 2008 17:00:00                                             NA
> 10  30     7 2008 18:00:00                                             NA
>
> Thank you so much.
>
>
> On Wed, Jul 13, 2016 at 9:42 AM, David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>>
>> > On Jul 12, 2016, at 3:45 PM, roslinazairimah zakaria <
>> roslinaump at gmail.com> wrote:
>> >
>> > Dear R-users,
>> >
>> > I have these data:
>> >
>> > head(balok, 10); tail(balok, 10)
>> >        Date     Time Rain.mm
>> > 1  30/7/2008  9:00:00       0
>> > 2  30/7/2008 10:00:00       0
>> > 3  30/7/2008 11:00:00       0
>> > 4  30/7/2008 12:00:00       0
>> > 5  30/7/2008 13:00:00       0
>> > 6  30/7/2008 14:00:00       0
>> > 7  30/7/2008 15:00:00       0
>> > 8  30/7/2008 16:00:00       0
>> > 9  30/7/2008 17:00:00       0
>> > 10 30/7/2008 18:00:00       0
>> >           Date     Time Rain.mm
>> > 63667 4/11/2015  3:00:00       0
>> > 63668 4/11/2015  4:00:00       0
>> > 63669 4/11/2015  5:00:00       0
>> > 63670 4/11/2015  6:00:00       0
>> > 63671 4/11/2015  7:00:00       0
>> > 63672 4/11/2015  8:00:00       0
>> > 63673 4/11/2015  9:00:00     0.1
>> > 63674 4/11/2015 10:00:00     0.1
>> > 63675 4/11/2015 11:00:00     0.1
>> > 63676 4/11/2015 12:00:00    0.1?
>> >
>> >> str(balok)
>> > 'data.frame':   63676 obs. of  3 variables:
>> > $ Date   : Factor w/ 2654 levels "1/1/2009","1/1/2010",..: 2056 2056 2056
>> > 2056 2056 2056 2056 2056 2056 2056 ...
>> > $ Time   : Factor w/ 24 levels "1:00:00","10:00:00",..: 24 2 3 4 5 6 7 8
>> 9
>> > 10 ...
>> > $ Rain.mm: Factor w/ 352 levels "0","0.0?","0.1",..: 1 1 1 1 1 1 1 1 1 1
>>
>> Thar's your problem:
>>
>>   Rain.mm: Factor w/ 352 levels "0","0.0?","0.1"
>>
>> Need to use the standard fix for the screwed-up-factor-on-input-problem
>>
>>   balok$Rain.mm2 <- as.numeric( as.character(balok$Rain.mm) )
>>
>> Cannot just do as.numeric because factors are actually already numeric.
>>
>> --
>> David.
>>
>>
>> > ...
>> >
>> > and I have change the data as follows:
>> >
>> > realdate <- as.Date(balok$Date,format="%d/%m/%Y")
>> > dfdate <- data.frame(date=realdate)
>> > year=as.numeric (format(realdate,"%Y"))
>> > month=as.numeric (format(realdate,"%m"))
>> > day=as.numeric (format(realdate,"%d"))
>> >
>> > balok2 <-cbind(dfdate,day,month,year,balok[,2:3])
>> > colnames(balok2)
>> > head(balok2)
>> >        date day month year     Time Rain.mm
>> > 1 2008-07-30  30     7 2008  9:00:00       0
>> > 2 2008-07-30  30     7 2008 10:00:00       0
>> > 3 2008-07-30  30     7 2008 11:00:00       0
>> > 4 2008-07-30  30     7 2008 12:00:00       0
>> > 5 2008-07-30  30     7 2008 13:00:00       0
>> > 6 2008-07-30  30     7 2008 14:00:00       0
>> > ...
>> >
>> >> balok3 <- balok2[,-1]; head(balok3, n=100)
>> >    day month year     Time Rain.mm
>> > 1    30     7 2008  9:00:00       0
>> > 2    30     7 2008 10:00:00       0
>> > 3    30     7 2008 11:00:00       0
>> > 4    30     7 2008 12:00:00       0
>> > 5    30     7 2008 13:00:00       0
>> > 6    30     7 2008 14:00:00       0
>> > 7    30     7 2008 15:00:00       0
>> > 8    30     7 2008 16:00:00       0
>> > 9    30     7 2008 17:00:00       0
>> > 10   30     7 2008 18:00:00       0
>> > 11   30     7 2008 19:00:00       0
>> > 12   30     7 2008 20:00:00       0
>> > 13   30     7 2008 21:00:00       0
>> > 14   30     7 2008 22:00:00       0
>> > 15   30     7 2008 23:00:00       0
>> > 16   30     7 2008 24:00:00       0
>> > 17   31     7 2008  1:00:00       0
>> > 18   31     7 2008  2:00:00       0
>> > 19   31     7 2008  3:00:00       0
>> > 20   31     7 2008  4:00:00       0
>> > 21   31     7 2008  5:00:00       0
>> > 22   31     7 2008  6:00:00       0
>> > 23   31     7 2008  7:00:00       0
>> > 24   31     7 2008  8:00:00       0
>> > 25   31     7 2008  9:00:00       0
>> > 26   31     7 2008 10:00:00       0
>> > 27   31     7 2008 11:00:00       0
>> > 28   31     7 2008 12:00:00       0
>> > 29   31     7 2008 13:00:00       0
>> > 30   31     7 2008 14:00:00       0
>> > 31   31     7 2008 15:00:00       0
>> > 32   31     7 2008 16:00:00       0
>> > 33   31     7 2008 17:00:00       0
>> > 34   31     7 2008 18:00:00       0
>> > 35   31     7 2008 19:00:00       0
>> > 36   31     7 2008 20:00:00       0
>> > 37   31     7 2008 21:00:00       0
>> > 38   31     7 2008 22:00:00       0
>> > 39   31     7 2008 23:00:00       0
>> > 40   31     7 2008 24:00:00       0
>> > 41    1     8 2008  1:00:00       0
>> > 42    1     8 2008  2:00:00       0
>> > 43    1     8 2008  3:00:00       0
>> > 44    1     8 2008  4:00:00       0
>> > 45    1     8 2008  5:00:00       0
>> > 46    1     8 2008  6:00:00       0
>> > 47    1     8 2008  7:00:00       0
>> > 48    1     8 2008  8:00:00       0
>> > 49    1     8 2008  9:00:00       0
>> > 50    1     8 2008 10:00:00       0
>> > 51    1     8 2008 11:00:00       0
>> > 52    1     8 2008 12:00:00       0
>> > 53    1     8 2008 13:00:00       0
>> > 54    1     8 2008 14:00:00       0
>> > 55    1     8 2008 15:00:00       0
>> > 56    1     8 2008 16:00:00       0
>> > 57    1     8 2008 17:00:00       0
>> > 58    1     8 2008 18:00:00       0
>> > 59    1     8 2008 19:00:00       0
>> > 60    1     8 2008 20:00:00       0
>> > 61    1     8 2008 21:00:00       0
>> > 62    1     8 2008 22:00:00       0
>> > 63    1     8 2008 23:00:00       0
>> > 64    1     8 2008 24:00:00       0
>> > 65    2     8 2008  1:00:00       0
>> > 66    2     8 2008  2:00:00       0
>> > 67    2     8 2008  3:00:00       0
>> > 68    2     8 2008  4:00:00       0
>> > 69    2     8 2008  5:00:00       0
>> > 70    2     8 2008  6:00:00       0
>> > 71    2     8 2008  7:00:00       0
>> > 72    2     8 2008  8:00:00       0
>> > 73    2     8 2008  9:00:00       0
>> > 74    2     8 2008 10:00:00       0
>> > 75    2     8 2008 11:00:00       0
>> > 76    2     8 2008 12:00:00       0
>> > 77    2     8 2008 13:00:00       0
>> > 78    2     8 2008 14:00:00       0
>> > 79    2     8 2008 15:00:00       0
>> > 80    2     8 2008 16:00:00       0
>> > 81    2     8 2008 17:00:00       0
>> > 82    2     8 2008 18:00:00       0
>> > 83    2     8 2008 19:00:00       0
>> > 84    2     8 2008 20:00:00       0
>> > 85    2     8 2008 21:00:00       0
>> > 86    2     8 2008 22:00:00       0
>> > 87    2     8 2008 23:00:00       0
>> > 88    2     8 2008 24:00:00    11.1
>> > 89    3     8 2008  1:00:00     0.4
>> > 90    3     8 2008  2:00:00       0
>> > 91    3     8 2008  3:00:00       0
>> > 92    3     8 2008  4:00:00       0
>> > 93    3     8 2008  5:00:00       0
>> > 94    3     8 2008  6:00:00       0
>> > 95    3     8 2008  7:00:00       0
>> > 96    3     8 2008  8:00:00       0
>> > 97    3     8 2008  9:00:00       0
>> > 98    3     8 2008 10:00:00       0
>> > 99    3     8 2008 11:00:00       0
>> > 100   3     8 2008 12:00:00       0
>> >
>> > The rainfall data is in hourly unit, and I would like to sum the Rain.mm
>> > according to month.  I tried to use aggregate(), but I got this message:
>> >
>> > dt <- balok4
>> > str(dt)
>> > aggbalok <- aggregate(dt[,5], by=dt[,c(1,4)],FUN=sum, na.rm=TRUE)
>> > aggbalok
>> >
>> > Error in Summary.factor(1L, na.rm = TRUE) :
>> >  sum not meaningful for factors
>> >
>> >
>> > Thank you so much for any help given.
>> >
>> > Roslina
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>>
>
>
> --
> *Dr. Roslinazairimah Binti Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Deputy Dean (Academic & Student Affairs)
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From adel.daoud at sociology.gu.se  Wed Jul 13 13:41:50 2016
From: adel.daoud at sociology.gu.se (Adel Daoud)
Date: Wed, 13 Jul 2016 13:41:50 +0200
Subject: [R] pairwise deletion in regression models
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034CDA@SRVEXCHMBX.precheza.cz>
References: <CAEJCy7iSrG1q+w1MWHg8Tf8m5VpuB=Qc_WBw+iVJaKYgunLmpg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034C60@SRVEXCHMBX.precheza.cz>
	<CAEJCy7hxCJuMZmYOWWGwTJQ8WqY+8Jh9jUGN-P4bZP4=_VFHYg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034CDA@SRVEXCHMBX.precheza.cz>
Message-ID: <CAEJCy7ixD=s9-soP+49kwHs=G+sFZDs8pSoNhxAuUN7P70Ob4w@mail.gmail.com>

Hi



Yes, I am sure. Have a look here for SPSS e.g.:

http://www-01.ibm.com/support/docview.wss?uid=swg21475199 and here

http://www.ats.ucla.edu/stat/spss/modules/missing.htm



and for STATA here,

http://www.ats.ucla.edu/stat/stata/modules/missing.html





I know that R allows for pairwise deletion in the cor() function, but I
need it for regression analysis. The default is listwise (casewise)
deletion. Would be grateful for further input on this.





Best,

Adel

--

Adel Daoud, PhD, Researcher

*Newly published:*

* Daoud, Adel and Kohl, Sebastian, *How Much Do Sociologists Write About
Economic Topics? Using Big-Data to Test Some Conventional Views in Economic
Sociology, 1890 to 2014*. Max Planck Institute for the Study of Societies,
Discussion Paper 16/7  <http://www.mpifg.de/pu/mpifg_dp/dp16-7.pdf>

* Daoud, Adel, Bj?rn Haller?d, and Debarati Guha-Sapir, (2016) ?What Is the
Association between Absolute Child Poverty, Poor Governance, and Natural
Disasters? A Global Comparison of Some of the Realities of Climate
Change?, *PLoS
ONE* 11(4)
<http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153296>

*Shailen Nandy, Adel Daoud, David Gordon, (2016), Examining the changing
profile of undernutrition in the context of food price rises and greater
inequality, *Social Science & Medicine*, Volume 149, Pages 153?163
<http://www.sciencedirect.com/science/article/pii/S0277953615302446>


*Department of Sociology and Work Science,*

*University of Gothenburg*

Box 720

405 30, G?teborg, Sweden

Email: Adel.daoud at sociology.gu.se

Website: http://adeldaoud.se/  <http://adeldaoud.se/>

On Wed, Jul 13, 2016 at 10:53 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
>
>
> Hm. Are you 100% sure that other software packages can do pairwise
> deletion in OLS or GLM? I am not at all familiar with them but
>
>
>
> http://www.ats.ucla.edu/stat/spss/modules/stats.htm
>
>
>
> suggests that option pairwise is available with corr and I believe the
> same option exists in cor function in R.
>
>
>
> My statistical knowledge is inferior but I just cannot imagine how whole
> model could be computed when one value is missing.
>
>
>
> Cheers
>
> Petr
>
>
>
> *From:* adeldaoud at gmail.com [mailto:adeldaoud at gmail.com] *On Behalf Of *Adel
> Daoud
> *Sent:* Wednesday, July 13, 2016 10:00 AM
> *To:* PIKAL Petr <petr.pikal at precheza.cz>
> *Cc:* r-help <r-help at r-project.org>
> *Subject:* Re: [R] pairwise deletion in regression models
>
>
>
> Thanks Petr for the suggestion.
>
>
>
> I just took the regtools package for a quick test drive. It looks
> promising, but it still needs further development to make it a viable
> option. You will not get a standard regression output (as in lm or glm),
> only the regression coefficients (without standard errors). I will be happy
> to try it out once it is more robust.
>
>
>
> I am a bit puzzled that the R universe seems to lack a robust package that
> allows for pairwise deletion (which is standard in the otherwise poorer
> software packages STATA or SPSS?). Would be very happy if anyone can show
> me otherwise.
>
>
> Best,
>
> Adel
>
> --
>
> Adel Daoud, PhD, Researcher
>
> *Newly published:*
>
> * Daoud, Adel and Kohl, Sebastian, *How Much Do Sociologists Write About
> Economic Topics? Using Big-Data to Test Some Conventional Views in Economic
> Sociology, 1890 to 2014*. Max Planck Institute for the Study of
> Societies, Discussion Paper 16/7
> <http://www.mpifg.de/pu/mpifg_dp/dp16-7.pdf>
>
> * Daoud, Adel, Bj?rn Haller?d, and Debarati Guha-Sapir, (2016) ?What Is
> the Association between Absolute Child Poverty, Poor Governance, and
> Natural Disasters? A Global Comparison of Some of the Realities of Climate
> Change?, *PLoS ONE* 11(4)
> <http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153296>
>
> *Shailen Nandy, Adel Daoud, David Gordon, (2016), Examining the changing
> profile of undernutrition in the context of food price rises and greater
> inequality, *Social Science & Medicine*, Volume 149, Pages 153?163
> <http://www.sciencedirect.com/science/article/pii/S0277953615302446>
>
>
>
> *Department of Sociology and Work Science,*
>
> *University of Gothenburg*
>
> Box 720
>
> 405 30, G?teborg, Sweden
>
> Email: Adel.daoud at sociology.gu.se
>
> Website: http://adeldaoud.se/  <http://adeldaoud.se/>
>
>
>
> On Wed, Jul 13, 2016 at 8:21 AM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
>
> Hi
>
>
> http://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari
>
> The package is probably not available on CRAN but seems to be still
> maintained on github.
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of adel
> > daoud
> > Sent: Tuesday, July 12, 2016 8:28 PM
> > To: r-help <r-help at r-project.org>
> > Subject: [R] pairwise deletion in regression models
> >
> > Dear R users,
> >
> >
> >
> > I would like to use a pairwise deletion of missing values in linear
> regression
> > (lm or glm preferably). I want to replicate some studies done in STATA
> that
> > uses this type of deletion. What options do we have in R to work with
> > pairwise deletion? Most packages I have found do not have this option, it
> > seems (lm, glm, plm, psych, sampleSelection).
> >
> >
> >
> > This question has been raised here
> > <
> http://r.789695.n4.nabble.com/set-the-bahavior-that-R-deal-with-missing-
> > values-td803840.html>
> > and here
> > <http://r.789695.n4.nabble.com/Pairwise-deletion-in-a-linear-regression-
> > and-in-a-GLM-td4653004.html>,
> > but without any clear answer.
> >
> >
> >
> > Any input is welcomed
> >
> >
> >
> > Thanks in advance
> >
> >
> > Adel
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>
>
>
> ------------------------------
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Jul 13 13:58:05 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 13 Jul 2016 21:58:05 +1000
Subject: [R] graph: horizontal bar reflecting number of data
In-Reply-To: <c12faa1e-9f2f-77b0-fc07-adbb1bc609c2@gmx.net>
References: <c12faa1e-9f2f-77b0-fc07-adbb1bc609c2@gmx.net>
Message-ID: <CA+8X3fXNqFi4u3Ots6a96R+3RKdgSZaqcE39i3FqeUWHbO6nmw@mail.gmail.com>

Hi Tagmarie,
This might help:

datframe$numberdata<-as.numeric(as.character(datframe$numberdat))
library(plotrix)
barcol<-color.scale(datframe$numberdat,extremes=c("black","white"))
barplot(matrix(datframe$numberdat,nrow=2,byrow=TRUE),
 beside=TRUE, horiz=TRUE,names.arg=paste("Week",1:3),
 col=matrix(barcol,nrow=2,byrow=TRUE))

Jim


On Wed, Jul 13, 2016 at 5:59 PM, Dagmar <Ramgad82 at gmx.net> wrote:
> Dear all,
>
> I hope someone can help with my problem:
>
> I have a dataframe like this:
>
> datframe <- data.frame(Name=c("Kati","Kati","Kati","Leon","Leon","Leon" ),
> week =c("1","2", "3", "1","2", "3"), numberdata =c("5","12", "1", "6","2",
> "5"))
> datframe
>
> I want to create a graph like the attached one (jpg). I can't bring the bars
> to  line up by week... desperating.
>
> (in case you don't see the jpg: I want a horizontal bar which reflects the
> number of data per week by the colour of the shading).
>
> Can anyone help?
>
> Tagmarie
>
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Wed Jul 13 14:14:23 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 13 Jul 2016 12:14:23 +0000
Subject: [R] pairwise deletion in regression models
In-Reply-To: <CAEJCy7ixD=s9-soP+49kwHs=G+sFZDs8pSoNhxAuUN7P70Ob4w@mail.gmail.com>
References: <CAEJCy7iSrG1q+w1MWHg8Tf8m5VpuB=Qc_WBw+iVJaKYgunLmpg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034C60@SRVEXCHMBX.precheza.cz>
	<CAEJCy7hxCJuMZmYOWWGwTJQ8WqY+8Jh9jUGN-P4bZP4=_VFHYg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034CDA@SRVEXCHMBX.precheza.cz>
	<CAEJCy7ixD=s9-soP+49kwHs=G+sFZDs8pSoNhxAuUN7P70Ob4w@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034D62@SRVEXCHMBX.precheza.cz>

Hi

maybe others can give you definitive answer

see my comments in line

From: adeldaoud at gmail.com [mailto:adeldaoud at gmail.com] On Behalf Of Adel Daoud
Sent: Wednesday, July 13, 2016 1:42 PM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] pairwise deletion in regression models

Hi
Yes, I am sure. Have a look here for SPSS e.g.:
http://www-01.ibm.com/support/docview.wss?uid=swg21475199 and here
http://www.ats.ucla.edu/stat/spss/modules/missing.htm
Well there is no explicite statement that pairwise deletion of missing values is performed. However I admit that this
REGRESSION
If values of any of the variables on the var subcommand are missing, the entire case is excluded from the analysis (i.e., listwise deletion of missing data). It is possible to further control the treatment of missing data with the missing subcommand and one of the following keywords: pairwise, meansubstitution, or include.
states that there is pairwise option in regression missing values treatment
and for STATA here,
http://www.ats.ucla.edu/stat/stata/modules/missing.html
Seems to me that Stata performs listwise missing
reg
If any of the variables listed after the reg command are missing, the observations missing that value(s) are excluded from the analysis (i.e., listwise deletion of missing data).
Cheers
Petr

I know that R allows for pairwise deletion in the cor() function, but I need it for regression analysis. The default is listwise (casewise) deletion. Would be grateful for further input on this.




Best,

Adel

--

Adel Daoud, PhD, Researcher

Newly published:

* Daoud, Adel and Kohl, Sebastian, How Much Do Sociologists Write About Economic Topics? Using Big-Data to Test Some Conventional Views in Economic Sociology, 1890 to 2014. Max Planck Institute for the Study of Societies, Discussion Paper 16/7 <http://www.mpifg.de/pu/mpifg_dp/dp16-7.pdf>

* Daoud, Adel, Bj?rn Haller?d, and Debarati Guha-Sapir, (2016) ?What Is the Association between Absolute Child Poverty, Poor Governance, and Natural Disasters? A Global Comparison of Some of the Realities of Climate Change?, PLoS ONE 11(4)<http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153296>

*Shailen Nandy, Adel Daoud, David Gordon, (2016), Examining the changing profile of undernutrition in the context of food price rises and greater inequality, Social Science & Medicine, Volume 149, Pages 153?163<http://www.sciencedirect.com/science/article/pii/S0277953615302446>



Department of Sociology and Work Science,

University of Gothenburg

Box 720

405 30, G?teborg, Sweden

Email: Adel.daoud at sociology.gu.se<mailto:Adel.daoud at sociology.gu.se>

Website: http://adeldaoud.se/ <http://adeldaoud.se/>

On Wed, Jul 13, 2016 at 10:53 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

Hm. Are you 100% sure that other software packages can do pairwise deletion in OLS or GLM? I am not at all familiar with them but

http://www.ats.ucla.edu/stat/spss/modules/stats.htm

suggests that option pairwise is available with corr and I believe the same option exists in cor function in R.

My statistical knowledge is inferior but I just cannot imagine how whole model could be computed when one value is missing.

Cheers
Petr

From: adeldaoud at gmail.com<mailto:adeldaoud at gmail.com> [mailto:adeldaoud at gmail.com<mailto:adeldaoud at gmail.com>] On Behalf Of Adel Daoud
Sent: Wednesday, July 13, 2016 10:00 AM
To: PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>>
Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
Subject: Re: [R] pairwise deletion in regression models

Thanks Petr for the suggestion.

I just took the regtools package for a quick test drive. It looks promising, but it still needs further development to make it a viable option. You will not get a standard regression output (as in lm or glm), only the regression coefficients (without standard errors). I will be happy to try it out once it is more robust.

I am a bit puzzled that the R universe seems to lack a robust package that allows for pairwise deletion (which is standard in the otherwise poorer software packages STATA or SPSS?). Would be very happy if anyone can show me otherwise.


Best,

Adel

--

Adel Daoud, PhD, Researcher

Newly published:

* Daoud, Adel and Kohl, Sebastian, How Much Do Sociologists Write About Economic Topics? Using Big-Data to Test Some Conventional Views in Economic Sociology, 1890 to 2014. Max Planck Institute for the Study of Societies, Discussion Paper 16/7 <http://www.mpifg.de/pu/mpifg_dp/dp16-7.pdf>

* Daoud, Adel, Bj?rn Haller?d, and Debarati Guha-Sapir, (2016) ?What Is the Association between Absolute Child Poverty, Poor Governance, and Natural Disasters? A Global Comparison of Some of the Realities of Climate Change?, PLoS ONE 11(4)<http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153296>

*Shailen Nandy, Adel Daoud, David Gordon, (2016), Examining the changing profile of undernutrition in the context of food price rises and greater inequality, Social Science & Medicine, Volume 149, Pages 153?163<http://www.sciencedirect.com/science/article/pii/S0277953615302446>



Department of Sociology and Work Science,

University of Gothenburg

Box 720

405 30, G?teborg, Sweden

Email: Adel.daoud at sociology.gu.se<mailto:Adel.daoud at sociology.gu.se>

Website: http://adeldaoud.se/ <http://adeldaoud.se/>

On Wed, Jul 13, 2016 at 8:21 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

http://stats.stackexchange.com/questions/158366/fit-multiple-regression-model-with-pairwise-deletion-or-on-a-correlation-covari

The package is probably not available on CRAN but seems to be still maintained on github.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of adel
> daoud
> Sent: Tuesday, July 12, 2016 8:28 PM
> To: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: [R] pairwise deletion in regression models
>
> Dear R users,
>
>
>
> I would like to use a pairwise deletion of missing values in linear regression
> (lm or glm preferably). I want to replicate some studies done in STATA that
> uses this type of deletion. What options do we have in R to work with
> pairwise deletion? Most packages I have found do not have this option, it
> seems (lm, glm, plm, psych, sampleSelection).
>
>
>
> This question has been raised here
> <http://r.789695.n4.nabble.com/set-the-bahavior-that-R-deal-with-missing-
> values-td803840.html>
> and here
> <http://r.789695.n4.nabble.com/Pairwise-deletion-in-a-linear-regression-
> and-in-a-GLM-td4653004.html>,
> but without any clear answer.
>
>
>
> Any input is welcomed
>
>
>
> Thanks in advance
>
>
> Adel
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Jul 13 14:23:43 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 13 Jul 2016 12:23:43 +0000
Subject: [R] Aggregate rainfall data
In-Reply-To: <CANTvJZJSN=7i-nez4J7APX+HaaNh6B0j-aHTRSzPaaw_LK+vLg@mail.gmail.com>
References: <CANTvJZLA-W63e6HPMAgadWMmR6W6wubf5UuB=cVtdW7kqp1J0Q@mail.gmail.com>
	<6CD80698-D9FE-4D11-BB90-F918EAE2BCD8@comcast.net>
	<CANTvJZJSN=7i-nez4J7APX+HaaNh6B0j-aHTRSzPaaw_LK+vLg@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034D78@SRVEXCHMBX.precheza.cz>

Hi

First you could check what levels are not numeric by

levels(balok3$Rain.mm)

If only 0.0? is the offending level you can either change it to 0 by

levels(balok3$Rain.mm)[number of ofending level] <- "0.0"

and then change the factor to numeric by

balok3$Rain.mm <- as.numeric(as.character(balok3$Rain.mm))

Or you can treat those levels as missing.

levels(balok3$Rain.mm)[number of ofending level] <- NA
balok3$Rain.mm <- as.numeric(as.character(balok3$Rain.mm))

Cheers
Petr



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> roslinazairimah zakaria
> Sent: Wednesday, July 13, 2016 12:22 PM
> To: David Winsemius <dwinsemius at comcast.net>
> Cc: r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] Aggregate rainfall data
>
> Dear David,
>
> I got your point.  How do I remove the data that contain "0.0?".
>
> I tried : balok <- cbind(balok3[,-5], balok3$Rain.mm[balok3$Rain.mm==0.0?]
> <- NA)
>
> However all the Rain.mm column all become NA.
>
>    day month year     Time balok3$Rain.mm[balok3$Rain.mm == "0.0?"] <- NA
> 1   30     7 2008  9:00:00                                             NA
> 2   30     7 2008 10:00:00                                             NA
> 3   30     7 2008 11:00:00                                             NA
> 4   30     7 2008 12:00:00                                             NA
> 5   30     7 2008 13:00:00                                             NA
> 6   30     7 2008 14:00:00                                             NA
> 7   30     7 2008 15:00:00                                             NA
> 8   30     7 2008 16:00:00                                             NA
> 9   30     7 2008 17:00:00                                             NA
> 10  30     7 2008 18:00:00                                             NA
>
> Thank you so much.
>
>
> On Wed, Jul 13, 2016 at 9:42 AM, David Winsemius
> <dwinsemius at comcast.net>
> wrote:
>
> >
> > > On Jul 12, 2016, at 3:45 PM, roslinazairimah zakaria <
> > roslinaump at gmail.com> wrote:
> > >
> > > Dear R-users,
> > >
> > > I have these data:
> > >
> > > head(balok, 10); tail(balok, 10)
> > >        Date     Time Rain.mm
> > > 1  30/7/2008  9:00:00       0
> > > 2  30/7/2008 10:00:00       0
> > > 3  30/7/2008 11:00:00       0
> > > 4  30/7/2008 12:00:00       0
> > > 5  30/7/2008 13:00:00       0
> > > 6  30/7/2008 14:00:00       0
> > > 7  30/7/2008 15:00:00       0
> > > 8  30/7/2008 16:00:00       0
> > > 9  30/7/2008 17:00:00       0
> > > 10 30/7/2008 18:00:00       0
> > >           Date     Time Rain.mm
> > > 63667 4/11/2015  3:00:00       0
> > > 63668 4/11/2015  4:00:00       0
> > > 63669 4/11/2015  5:00:00       0
> > > 63670 4/11/2015  6:00:00       0
> > > 63671 4/11/2015  7:00:00       0
> > > 63672 4/11/2015  8:00:00       0
> > > 63673 4/11/2015  9:00:00     0.1
> > > 63674 4/11/2015 10:00:00     0.1
> > > 63675 4/11/2015 11:00:00     0.1
> > > 63676 4/11/2015 12:00:00    0.1?
> > >
> > >> str(balok)
> > > 'data.frame':   63676 obs. of  3 variables:
> > > $ Date   : Factor w/ 2654 levels "1/1/2009","1/1/2010",..: 2056 2056 2056
> > > 2056 2056 2056 2056 2056 2056 2056 ...
> > > $ Time   : Factor w/ 24 levels "1:00:00","10:00:00",..: 24 2 3 4 5 6 7 8
> > 9
> > > 10 ...
> > > $ Rain.mm: Factor w/ 352 levels "0","0.0?","0.1",..: 1 1 1 1 1 1 1 1
> > > 1 1
> >
> > Thar's your problem:
> >
> >   Rain.mm: Factor w/ 352 levels "0","0.0?","0.1"
> >
> > Need to use the standard fix for the
> > screwed-up-factor-on-input-problem
> >
> >   balok$Rain.mm2 <- as.numeric( as.character(balok$Rain.mm) )
> >
> > Cannot just do as.numeric because factors are actually already numeric.
> >
> > --
> > David.
> >
> >
> > > ...
> > >
> > > and I have change the data as follows:
> > >
> > > realdate <- as.Date(balok$Date,format="%d/%m/%Y")
> > > dfdate <- data.frame(date=realdate)
> > > year=as.numeric (format(realdate,"%Y")) month=as.numeric
> > > (format(realdate,"%m")) day=as.numeric (format(realdate,"%d"))
> > >
> > > balok2 <-cbind(dfdate,day,month,year,balok[,2:3])
> > > colnames(balok2)
> > > head(balok2)
> > >        date day month year     Time Rain.mm
> > > 1 2008-07-30  30     7 2008  9:00:00       0
> > > 2 2008-07-30  30     7 2008 10:00:00       0
> > > 3 2008-07-30  30     7 2008 11:00:00       0
> > > 4 2008-07-30  30     7 2008 12:00:00       0
> > > 5 2008-07-30  30     7 2008 13:00:00       0
> > > 6 2008-07-30  30     7 2008 14:00:00       0
> > > ...
> > >
> > >> balok3 <- balok2[,-1]; head(balok3, n=100)
> > >    day month year     Time Rain.mm
> > > 1    30     7 2008  9:00:00       0
> > > 2    30     7 2008 10:00:00       0
> > > 3    30     7 2008 11:00:00       0
> > > 4    30     7 2008 12:00:00       0
> > > 5    30     7 2008 13:00:00       0
> > > 6    30     7 2008 14:00:00       0
> > > 7    30     7 2008 15:00:00       0
> > > 8    30     7 2008 16:00:00       0
> > > 9    30     7 2008 17:00:00       0
> > > 10   30     7 2008 18:00:00       0
> > > 11   30     7 2008 19:00:00       0
> > > 12   30     7 2008 20:00:00       0
> > > 13   30     7 2008 21:00:00       0
> > > 14   30     7 2008 22:00:00       0
> > > 15   30     7 2008 23:00:00       0
> > > 16   30     7 2008 24:00:00       0
> > > 17   31     7 2008  1:00:00       0
> > > 18   31     7 2008  2:00:00       0
> > > 19   31     7 2008  3:00:00       0
> > > 20   31     7 2008  4:00:00       0
> > > 21   31     7 2008  5:00:00       0
> > > 22   31     7 2008  6:00:00       0
> > > 23   31     7 2008  7:00:00       0
> > > 24   31     7 2008  8:00:00       0
> > > 25   31     7 2008  9:00:00       0
> > > 26   31     7 2008 10:00:00       0
> > > 27   31     7 2008 11:00:00       0
> > > 28   31     7 2008 12:00:00       0
> > > 29   31     7 2008 13:00:00       0
> > > 30   31     7 2008 14:00:00       0
> > > 31   31     7 2008 15:00:00       0
> > > 32   31     7 2008 16:00:00       0
> > > 33   31     7 2008 17:00:00       0
> > > 34   31     7 2008 18:00:00       0
> > > 35   31     7 2008 19:00:00       0
> > > 36   31     7 2008 20:00:00       0
> > > 37   31     7 2008 21:00:00       0
> > > 38   31     7 2008 22:00:00       0
> > > 39   31     7 2008 23:00:00       0
> > > 40   31     7 2008 24:00:00       0
> > > 41    1     8 2008  1:00:00       0
> > > 42    1     8 2008  2:00:00       0
> > > 43    1     8 2008  3:00:00       0
> > > 44    1     8 2008  4:00:00       0
> > > 45    1     8 2008  5:00:00       0
> > > 46    1     8 2008  6:00:00       0
> > > 47    1     8 2008  7:00:00       0
> > > 48    1     8 2008  8:00:00       0
> > > 49    1     8 2008  9:00:00       0
> > > 50    1     8 2008 10:00:00       0
> > > 51    1     8 2008 11:00:00       0
> > > 52    1     8 2008 12:00:00       0
> > > 53    1     8 2008 13:00:00       0
> > > 54    1     8 2008 14:00:00       0
> > > 55    1     8 2008 15:00:00       0
> > > 56    1     8 2008 16:00:00       0
> > > 57    1     8 2008 17:00:00       0
> > > 58    1     8 2008 18:00:00       0
> > > 59    1     8 2008 19:00:00       0
> > > 60    1     8 2008 20:00:00       0
> > > 61    1     8 2008 21:00:00       0
> > > 62    1     8 2008 22:00:00       0
> > > 63    1     8 2008 23:00:00       0
> > > 64    1     8 2008 24:00:00       0
> > > 65    2     8 2008  1:00:00       0
> > > 66    2     8 2008  2:00:00       0
> > > 67    2     8 2008  3:00:00       0
> > > 68    2     8 2008  4:00:00       0
> > > 69    2     8 2008  5:00:00       0
> > > 70    2     8 2008  6:00:00       0
> > > 71    2     8 2008  7:00:00       0
> > > 72    2     8 2008  8:00:00       0
> > > 73    2     8 2008  9:00:00       0
> > > 74    2     8 2008 10:00:00       0
> > > 75    2     8 2008 11:00:00       0
> > > 76    2     8 2008 12:00:00       0
> > > 77    2     8 2008 13:00:00       0
> > > 78    2     8 2008 14:00:00       0
> > > 79    2     8 2008 15:00:00       0
> > > 80    2     8 2008 16:00:00       0
> > > 81    2     8 2008 17:00:00       0
> > > 82    2     8 2008 18:00:00       0
> > > 83    2     8 2008 19:00:00       0
> > > 84    2     8 2008 20:00:00       0
> > > 85    2     8 2008 21:00:00       0
> > > 86    2     8 2008 22:00:00       0
> > > 87    2     8 2008 23:00:00       0
> > > 88    2     8 2008 24:00:00    11.1
> > > 89    3     8 2008  1:00:00     0.4
> > > 90    3     8 2008  2:00:00       0
> > > 91    3     8 2008  3:00:00       0
> > > 92    3     8 2008  4:00:00       0
> > > 93    3     8 2008  5:00:00       0
> > > 94    3     8 2008  6:00:00       0
> > > 95    3     8 2008  7:00:00       0
> > > 96    3     8 2008  8:00:00       0
> > > 97    3     8 2008  9:00:00       0
> > > 98    3     8 2008 10:00:00       0
> > > 99    3     8 2008 11:00:00       0
> > > 100   3     8 2008 12:00:00       0
> > >
> > > The rainfall data is in hourly unit, and I would like to sum the
> > > Rain.mm according to month.  I tried to use aggregate(), but I got this
> message:
> > >
> > > dt <- balok4
> > > str(dt)
> > > aggbalok <- aggregate(dt[,5], by=dt[,c(1,4)],FUN=sum, na.rm=TRUE)
> > > aggbalok
> > >
> > > Error in Summary.factor(1L, na.rm = TRUE) :
> > >  sum not meaningful for factors
> > >
> > >
> > > Thank you so much for any help given.
> > >
> > > Roslina
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
>
>
> --
> *Dr. Roslinazairimah Binti Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>* Deputy Dean (Academic
> & Student Affairs) Faculty of Industrial Sciences & Technology University
> Malaysia Pahang Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From jvadams at usgs.gov  Wed Jul 13 15:02:56 2016
From: jvadams at usgs.gov (Adams, Jean)
Date: Wed, 13 Jul 2016 08:02:56 -0500
Subject: [R] Forking and adapting an R package
In-Reply-To: <1880276709.61511.1468322926354.JavaMail.open-xchange@ox11.mail.hostpoint.internal>
References: <1880276709.61511.1468322926354.JavaMail.open-xchange@ox11.mail.hostpoint.internal>
Message-ID: <CAN5YmCFmjTefqB1rs-sZS6Ls7mxrD6ayiy3FkjeptP-sR+OY3w@mail.gmail.com>

Timo,

A couple of thoughts ...

First, could you achieve what you want by simply modifying (your own copies
of) a function or two from the hexbin package, without having to modify the
entire package?  This might give you fewer tripping points.

Second, right after you forked the package (so that you have a copy on your
own space on GitHub) and before you modified anything ... were you able to
install and use the package from there successfully?

library("devtools")
devtools::install_github("grssnbchr/hexbin")
library(hexbin)


Jean

On Tue, Jul 12, 2016 at 6:28 AM, <timo at timogrossenbacher.ch> wrote:

> Hello.
>
> I'm trying to adapt the package ?hexbin? to suit my needs. This is the
> first
> time I do this. I've read a bit through Hadley's ?R packages?, but now I'm
> pretty lost (from a workflow point of view). I am using RStudio and
> Hadley's
> devtools.
>
> So I forked the repo I want to adapt: https://github.com/grssnbchr/hexbin
> and
> cloned it using RStudio (I created a new project). What I basically want
> to do
> is adapt the package slightly and use the adapted source on my use case
> (an Rmd
> file in another location) - ideally, I would call the respective function
> (hexbin::grid.hexagons) in the Rmd and the source code of ?hexbin? would be
> called and debugged (just for understanding what the package ?hexbin?
> actually
> does in that case, I do not have to build it yet, or even publish it).
> What is
> the workflow for this?
>
> Also, I tried running devtools::check() and it already fails there:
> R version 3.2.5 (2016-04-14) -- "Very, Very Secure Dishes"
>
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> > devtools::check()
> Updating hexbin documentation
> Loading hexbin
> Creating a generic function for ?plot? from package ?graphics? in package
> ?hexbin?
> Creating a generic function for ?summary? from package ?base? in package
> ?hexbin?
> Setting env vars
>
> -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> CFLAGS : -Wall -pedantic
> CXXFLAGS: -Wall -pedantic
> Building hexbin
>
> ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
> '/usr/lib/R/bin/R' --no-site-file --no-environ --no-save --no-restore
> --quiet
> CMD build '/home/tgrossen/R/hexbin' --no-resave-data --no-manual
>
> * checking for file ?/home/tgrossen/R/hexbin/DESCRIPTION? ... OK
> * preparing ?hexbin?:
> * checking DESCRIPTION meta-information ... OK
> * cleaning src
> * installing the package to build vignettes
> * creating vignettes ... ERROR
>
> Error: processing vignette 'hexagon_binning.Rnw' failed with diagnostics:
>  chunk 1 (label = comphexsq)
> Error in eval(expr, envir, enclos) : could not find function ?hexbin?
> Execution halted
> Error: Command failed (1)
>
> As you can see, I am very much lost. I googled for "adapt R package and
> debug"
> and so forth but couldn't find any tutorial or anything.
>
> Thanks,
>
> Timo
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Wed Jul 13 15:29:47 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Wed, 13 Jul 2016 13:29:47 +0000
Subject: [R] pairwise deletion in regression models
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034D62@SRVEXCHMBX.precheza.cz>
References: <CAEJCy7iSrG1q+w1MWHg8Tf8m5VpuB=Qc_WBw+iVJaKYgunLmpg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034C60@SRVEXCHMBX.precheza.cz>
	<CAEJCy7hxCJuMZmYOWWGwTJQ8WqY+8Jh9jUGN-P4bZP4=_VFHYg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034CDA@SRVEXCHMBX.precheza.cz>
	<CAEJCy7ixD=s9-soP+49kwHs=G+sFZDs8pSoNhxAuUN7P70Ob4w@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5034D62@SRVEXCHMBX.precheza.cz>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836540D08@FHSDB2D11-2.csu.mcmaster.ca>

Dear Petr and Adel,

I'm unaware of any R software that does "pairwise deletion" of missing data (that is, uses pairwise-complete cases). It's possible to compute LS regression from covariances and means (or, equivalently, correlations, standard deviations, and means), which makes this approach possible, and it would be very simple to program it in R. 

Generally, however, pairwise deletion of missing data is a poor approach, providing consistent estimates of the regression coefficients only when missing data are missing completely at random (MCAR). Complete-case analysis ("listwise deletion" of missing data in SPSS jargon), on the other hand, also provides consistent estimates when data are missing at random (MAR) and missingness doesn't depend on the response variable. It can also, counterintuitively, be more efficient then pairwise deletion of missing data. As I recall, there's a nice example of this in Little and Rubin, Statistical Analysis of Missing Data, 2nd Ed. (Wiley, 2002), which is a good, accessible treatment of the topic. I have some notes on the subject from a course at <http://socserv.socsci.mcmaster.ca/jfox/Courses/soc740/Missing-data-notes.pdf>.

There are some other deficiencies of pairwise deletion of missing data that I won't go into here.

So, it would be easy to do pairwise deletion of missing data in R -- just write a simple program -- but I wouldn't generally advise it. If you have just a little missing data, you can do a complete-case analysis. Otherwise, there are better approaches, such as multiple imputation of missing values, for which there are many implementations in R -- e.g., the mi, mice, and amelia package.

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of PIKAL Petr
> Sent: July 13, 2016 5:14 AM
> To: Adel Daoud <adel.daoud at sociology.gu.se>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] pairwise deletion in regression models
> 
> Hi
> 
> maybe others can give you definitive answer
> 
> see my comments in line
> 
> From: adeldaoud at gmail.com [mailto:adeldaoud at gmail.com] On Behalf Of
> Adel Daoud
> Sent: Wednesday, July 13, 2016 1:42 PM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] pairwise deletion in regression models
> 
> Hi
> Yes, I am sure. Have a look here for SPSS e.g.:
> http://www-01.ibm.com/support/docview.wss?uid=swg21475199 and here
> http://www.ats.ucla.edu/stat/spss/modules/missing.htm
> Well there is no explicite statement that pairwise deletion of missing values is
> performed. However I admit that this REGRESSION If values of any of the
> variables on the var subcommand are missing, the entire case is excluded from
> the analysis (i.e., listwise deletion of missing data). It is possible to further
> control the treatment of missing data with the missing subcommand and one
> of the following keywords: pairwise, meansubstitution, or include.
> states that there is pairwise option in regression missing values treatment and
> for STATA here, http://www.ats.ucla.edu/stat/stata/modules/missing.html
> Seems to me that Stata performs listwise missing reg If any of the variables
> listed after the reg command are missing, the observations missing that
> value(s) are excluded from the analysis (i.e., listwise deletion of missing data).
> Cheers
> Petr
> 
> I know that R allows for pairwise deletion in the cor() function, but I need it for
> regression analysis. The default is listwise (casewise) deletion. Would be
> grateful for further input on this.
> 
> 
> 
> 
> Best,
> 
> Adel
> 
> --
> 
> Adel Daoud, PhD, Researcher
> 
> Newly published:
> 
> * Daoud, Adel and Kohl, Sebastian, How Much Do Sociologists Write About
> Economic Topics? Using Big-Data to Test Some Conventional Views in Economic
> Sociology, 1890 to 2014. Max Planck Institute for the Study of Societies,
> Discussion Paper 16/7 <http://www.mpifg.de/pu/mpifg_dp/dp16-7.pdf>
> 
> * Daoud, Adel, Bj?rn Haller?d, and Debarati Guha-Sapir, (2016) ?What Is the
> Association between Absolute Child Poverty, Poor Governance, and Natural
> Disasters? A Global Comparison of Some of the Realities of Climate Change?,
> PLoS ONE
> 11(4)<http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153
> 296>
> 
> *Shailen Nandy, Adel Daoud, David Gordon, (2016), Examining the changing
> profile of undernutrition in the context of food price rises and greater
> inequality, Social Science & Medicine, Volume 149, Pages 153?
> 163<http://www.sciencedirect.com/science/article/pii/S0277953615302446>
> 
> 
> 
> Department of Sociology and Work Science,
> 
> University of Gothenburg
> 
> Box 720
> 
> 405 30, G?teborg, Sweden
> 
> Email: Adel.daoud at sociology.gu.se<mailto:Adel.daoud at sociology.gu.se>
> 
> Website: http://adeldaoud.se/ <http://adeldaoud.se/>
> 
> On Wed, Jul 13, 2016 at 10:53 AM, PIKAL Petr
> <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
> Hi
> 
> Hm. Are you 100% sure that other software packages can do pairwise deletion
> in OLS or GLM? I am not at all familiar with them but
> 
> http://www.ats.ucla.edu/stat/spss/modules/stats.htm
> 
> suggests that option pairwise is available with corr and I believe the same
> option exists in cor function in R.
> 
> My statistical knowledge is inferior but I just cannot imagine how whole model
> could be computed when one value is missing.
> 
> Cheers
> Petr
> 
> From: adeldaoud at gmail.com<mailto:adeldaoud at gmail.com>
> [mailto:adeldaoud at gmail.com<mailto:adeldaoud at gmail.com>] On Behalf Of
> Adel Daoud
> Sent: Wednesday, July 13, 2016 10:00 AM
> To: PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>>
> Cc: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: Re: [R] pairwise deletion in regression models
> 
> Thanks Petr for the suggestion.
> 
> I just took the regtools package for a quick test drive. It looks promising, but it
> still needs further development to make it a viable option. You will not get a
> standard regression output (as in lm or glm), only the regression coefficients
> (without standard errors). I will be happy to try it out once it is more robust.
> 
> I am a bit puzzled that the R universe seems to lack a robust package that
> allows for pairwise deletion (which is standard in the otherwise poorer
> software packages STATA or SPSS?). Would be very happy if anyone can show
> me otherwise.
> 
> 
> Best,
> 
> Adel
> 
> --
> 
> Adel Daoud, PhD, Researcher
> 
> Newly published:
> 
> * Daoud, Adel and Kohl, Sebastian, How Much Do Sociologists Write About
> Economic Topics? Using Big-Data to Test Some Conventional Views in Economic
> Sociology, 1890 to 2014. Max Planck Institute for the Study of Societies,
> Discussion Paper 16/7 <http://www.mpifg.de/pu/mpifg_dp/dp16-7.pdf>
> 
> * Daoud, Adel, Bj?rn Haller?d, and Debarati Guha-Sapir, (2016) ?What Is the
> Association between Absolute Child Poverty, Poor Governance, and Natural
> Disasters? A Global Comparison of Some of the Realities of Climate Change?,
> PLoS ONE
> 11(4)<http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0153
> 296>
> 
> *Shailen Nandy, Adel Daoud, David Gordon, (2016), Examining the changing
> profile of undernutrition in the context of food price rises and greater
> inequality, Social Science & Medicine, Volume 149, Pages 153?
> 163<http://www.sciencedirect.com/science/article/pii/S0277953615302446>
> 
> 
> 
> Department of Sociology and Work Science,
> 
> University of Gothenburg
> 
> Box 720
> 
> 405 30, G?teborg, Sweden
> 
> Email: Adel.daoud at sociology.gu.se<mailto:Adel.daoud at sociology.gu.se>
> 
> Website: http://adeldaoud.se/ <http://adeldaoud.se/>
> 
> On Wed, Jul 13, 2016 at 8:21 AM, PIKAL Petr
> <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
> Hi
> 
> http://stats.stackexchange.com/questions/158366/fit-multiple-regression-
> model-with-pairwise-deletion-or-on-a-correlation-covari
> 
> The package is probably not available on CRAN but seems to be still maintained
> on github.
> 
> Cheers
> Petr
> 
> > -----Original Message-----
> > From: R-help
> > [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.o
> > rg>] On Behalf Of adel daoud
> > Sent: Tuesday, July 12, 2016 8:28 PM
> > To: r-help <r-help at r-project.org<mailto:r-help at r-project.org>>
> > Subject: [R] pairwise deletion in regression models
> >
> > Dear R users,
> >
> >
> >
> > I would like to use a pairwise deletion of missing values in linear
> > regression (lm or glm preferably). I want to replicate some studies
> > done in STATA that uses this type of deletion. What options do we have
> > in R to work with pairwise deletion? Most packages I have found do not
> > have this option, it seems (lm, glm, plm, psych, sampleSelection).
> >
> >
> >
> > This question has been raised here
> > <http://r.789695.n4.nabble.com/set-the-bahavior-that-R-deal-with-missi
> > ng-
> > values-td803840.html>
> > and here
> > <http://r.789695.n4.nabble.com/Pairwise-deletion-in-a-linear-regressio
> > n-
> > and-in-a-GLM-td4653004.html>,
> > but without any clear answer.
> >
> >
> >
> > Any input is welcomed
> >
> >
> >
> > Thanks in advance
> >
> >
> > Adel
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> > UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html and provide commented, minimal, self-contained,
> > reproducible code.
> 
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen?
> jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze
> sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
> 
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to
> z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce
> s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m
> dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost
> ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo
> p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> 
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender.
> Delete the contents of this e-mail with all attachments and its copies from your
> system.
> If you are not the intended recipient of this e-mail, you are not authorized to
> use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by
> modifications of the e-mail or by delay with transfer of the email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept
> such offer; The sender of this e-mail (offer) excludes any acceptance of the
> offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an
> express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into
> any contracts on behalf of the company except for cases in which he/she is
> expressly authorized to do so in writing, and such authorization or power of
> attorney is submitted to the recipient or the person represented by the
> recipient, or the existence of such authorization is known to the recipient of the
> person represented by the recipient.
> 
> 
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen?
> jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze
> sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
> 
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to
> z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce
> s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m
> dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost
> ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo
> p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> 
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender.
> Delete the contents of this e-mail with all attachments and its copies from your
> system.
> If you are not the intended recipient of this e-mail, you are not authorized to
> use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by
> modifications of the e-mail or by delay with transfer of the email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept
> such offer; The sender of this e-mail (offer) excludes any acceptance of the
> offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an
> express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into
> any contracts on behalf of the company except for cases in which he/she is
> expressly authorized to do so in writing, and such authorization or power of
> attorney is submitted to the recipient or the person represented by the
> recipient, or the existence of such authorization is known to the recipient of the
> person represented by the recipient.
> 
> 
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen?
> jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze
> sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
> 
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to
> z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce
> s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m
> dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost
> ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo
> p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> 
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender.
> Delete the contents of this e-mail with all attachments and its copies from your
> system.
> If you are not the intended recipient of this e-mail, you are not authorized to
> use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by
> modifications of the e-mail or by delay with transfer of the email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept
> such offer; The sender of this e-mail (offer) excludes any acceptance of the
> offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an
> express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into
> any contracts on behalf of the company except for cases in which he/she is
> expressly authorized to do so in writing, and such authorization or power of
> attorney is submitted to the recipient or the person represented by the
> recipient, or the existence of such authorization is known to the recipient of the
> person represented by the recipient.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From pascucci at lpl.arizona.edu  Wed Jul 13 11:47:45 2016
From: pascucci at lpl.arizona.edu (Ilaria Pascucci)
Date: Wed, 13 Jul 2016 11:47:45 +0200
Subject: [R] CensMixReg package - clarifications and working example
Message-ID: <DAB257EC-92B4-432F-9676-61D9AA75325C@lpl.arizona.edu>

Dear All,

	I am relatively new to R but I am very interested in using some of the available packages for astronomical applications.
While I have been able to understand and use other packages like cenken, I have some difficulties with CensMixReg.

Here are the specifics (with info on R and package versions at the end of the email), I would really appreciate your help.

1. First I tried to run the example in the CensMixReg.pdf file that accompanies the download.
While I could reproduce the " left mixture censored Student-t model fit to the data? (first example), the ?left mixture censored Normal model fit to the data? fails with the following error message:
Error in `row.names<-.data.frame`(`*tmp*`, value = value) : 
  invalid 'row.names' length
Is this issue documented somewhere?

2. Also in the CensMixReg.pdf file it is suggested to use the imm.fmcr function to compute the uncertainties. However, the function does not seem to be part of the CensMixReg or CensRegMod libraries (IFmixRegT <- imm.fmcr(y,x1,cc, fitT) -> Error: could not find function "imm.fmcr"). I searched the R site but could not find the imm.fmcr function. I am wondering if this is now osbolete and if there is another function to compute uncertainties from the CensMixReg output. 

3. Finally (and this is maybe the most important part), I am missing a working example with clearly laid out inputs and outputs to use confidently the CensMixReg package. Does anybody have such an example? More specifically, I was not sure of the transformation of x into x1 in the CensMixReg example (where there is an extra column of 1 in x1), is this to have the constant to the fitted line stored in beta0? Are the other betas the slopes of respectively age, educ and hours/1000? Where are the uncertainties on the slope and intercept stored?  

Thank you very much in advance.
With best wishes,

Ilaria


== Info on R and package versions:
> sessionInfo()
R version 3.3.0 (2016-05-03)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.10.5 (Yosemite)

locale:
[1] C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] CensRegMod_1.0 CensMixReg_0.7

loaded via a namespace (and not attached):
[1] tools_3.3.0   mvtnorm_1.0-5 mixsmsn_1.1-1



************************************
Dr. Ilaria Pascucci
Associate Professor
Department of Planetary Sciences
The University of Arizona

ilariapascucci.com
************************************


	[[alternative HTML version deleted]]


From pmakananisa at sars.gov.za  Wed Jul 13 15:25:26 2016
From: pmakananisa at sars.gov.za (Mangalani Peter Makananisa)
Date: Wed, 13 Jul 2016 13:25:26 +0000
Subject: [R] Dates in R (Year Month)
Message-ID: <BBB169C177D06B4E8E650D24548404CEC0B92F62@PTABREXC12M2.sars.gov.za>

Hi All,

I am trying to convert the vector below to dates please assist I have tried to use information on the links you sent, but it is not working.

X  = c(201501, 201502, 201503, 201505, 201506, 201507, 201508, 201509, 201510, 201511, 201512, 201601, 201602, 201603, 201604, 201605, 201606)

library(chron, zoo)
Z = as.yearmon(X)  # it is not working

please assist

Kind regards
Peter

Please Note: This email and its contents are subject to our email legal notice which can be viewed at http://www.sars.gov.za/Pages/Email-disclaimer.aspx

	[[alternative HTML version deleted]]


From stn021 at gmail.com  Wed Jul 13 15:48:33 2016
From: stn021 at gmail.com (stn021)
Date: Wed, 13 Jul 2016 15:48:33 +0200
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <AF7E2508-9E20-428C-96D9-1182D27A27A6@dcn.davis.ca.us>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
	<CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
	<AF7E2508-9E20-428C-96D9-1182D27A27A6@dcn.davis.ca.us>
Message-ID: <CANVHAvZi4SM2i1+UJL-jEHQvQuHe+Txqe=Y-Xv5+q5sCAwuD3w@mail.gmail.com>

Hello,

so here a numerical example in R-code. Code is appended below.

The output should be
1) the numerical values of the abilities of the persons
2) the multiplyer


Please note that

1) I have used non-linear optimization to solve this problem and got
the expected result, though not with R but other software.

2) I have applied lm() to this problem, even before I posted the
question. I am well aware of the syntax of formulas. I my last posting
I wrote the formula "freehand" so I made the previously mentioned
errors. Sorry about that.



Unfortunately the formulas with I() as well as multiplying variables
before running R does not work here. I() does not apply to factors (R
tells me) and multiplying in advance also works only for continuous
variables, not for factors, because there is no known numerical value
to multiply.

The latter is actually what my question is about, along with the
question on how to get R to treat two columns as two instances of the
same factor.


Just to be sure I used R to check if the data really counts as a
factor according to R-terminology. It really is a factor, see code
below.



This is the code for generating the example-data:

# --------------------------------------------------------------- #
pnames    = c( "alice" , "bob" , "charlie" , "don" , "eve" , "freddy"
, "grace" , "henry" )
pcount    = length( pnames )

# abilities = runif( pcount )
abilities = (1:pcount) / 10

persons = data.frame( name = pnames , ability = abilities )
persons

# random subset of possible combinations and extra df
combinations = combn( nrow( persons ) , 2 ) ;
combinations = cbind( combinations,combinations,combinations,combinations )
combinations = combinations[ , runif(ncol(combinations))<0.5 ]
ccount = ncol( combinations )

observed_data = data.frame(
  idx1 = combinations[1,]
, idx2 = combinations[2,]
, p1 = ( persons$name[    combinations[1,] ] )
, p2 = ( persons$name[    combinations[2,] ] )
)

abilities_data = data.frame(
  a1 = persons$ability[ combinations[1,] ]
, a2 = persons$ability[ combinations[2,] ]
)

# y = result of cooperation of each pair
multiplyer = runif(1) + 1
offset     = 1
cat( "multiplyer = " , multiplyer , "\n" )
cat( "offset = " , offset , "\n" )

y0 = multiplyer * ( offset - ( abilities_data$a1 - abilities_data$a2 ) ^ 2 )
noise = .05 * rnorm( ccount )

# check variables are really factors :
str(  observed_data$p1 )
dput( observed_data$p1 )

observed_data = data.frame( y = round( y0+noise,3 ) , observed_data )
observed_data

# --------------------------------------------------------------- #


2016-07-11 19:16 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
> Your clarification is promising.  A reproducible example is always preferred, though never a guarantee. I expect to be somewhat preoccupied this week so responses may be rather delayed, but the less setup we have to the more likely that someone on the list will tackle it.
>
> Re an answer: If you can make the example simple enough that you can tell us what the right numerical result will be, we will have a better chance of understanding what you are after.  E.g. if you start with a solution and use it to create sample input data with then you don't need to actually solve it to illustrate what you are after. [1]
>
> Note that I am not aware of any package dedicated to this type of problem, so unless someone else responds otherwise then you will likely have to use bootstrapping or your own statistical analysis (Bayesian?) of the result.
>
> [1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 11, 2016 7:28:41 AM PDT, stn021 <stn021 at gmail.com> wrote:
>>Hello,
>>
>>thank you for the replies. Sorry about the html-email, I forgot.
>>Should be OK with this email.
>>
>>
>>Don't be fooled be the apparent simplicity of the problem. I have
>>tried to reduce it to only a single relatively simple question.
>>
>>The idea here is to model cooperation of two persons. The model is
>>about one specific aspect of that cooperation, namely that two persons
>>with similar abilities may be able to produce better results that two
>>very different persons.
>>
>>That is only one part of the model with other parts modeling for
>>example the fact that of course two persons with a higher degree of
>>ability will produce better results per se.
>>
>>
>>It is not classic regression with factors. That can be easily done by
>>something like lm( y ~ (p1-p2)^2 ).
>>
>>This expands to lm( y ~ p1^2 - 2*p1*p2 + p2^2 ). This contains a
>>multiplicagtions and for lm() this implies interactions between the
>>factor-levels and produces one parameter for each combination of
>>factor-levels that occurs in the data. That is not what the question
>>is about.
>>
>>Also p1 and p2 are different levels of the same factor, while for lm()
>>it would be two different factors with different levels.
>>
>>
>>As for the sensical part: this has a real world application therefore
>>it makes sense.
>>
>>Also it is not so difficult to solve with non-linear optimization. I
>>was hoping to be able to use R for that purpose because then the
>>results could easily be checked with statistical tests.
>>
>>So my question is not "how to solve" but "how to solve with R".
>>
>>
>>As for the excess degrees of freedom, in real observations there would
>>of course be added noise due to either random variations or factors
>>not included in the model. So to generate a more reality-conforming
>>example I could add some random normal-distributed noise to the
>>dependent variable y. I previously left that part out because to me it
>>did not seem relevant.
>>
>>
>>Would you like me to make a complete example dataset with more records
>>and noise ?
>>
>>
>>The answer I look for would be the numerical values of the
>>factor-levels and numerical values for the multiplier (f) and the
>>offset (o), with p1 and p2 given as names (here: persons) and y given
>>as some level of achievement they reach by cooperating.
>>
>>y = f * ( o - ( p1 - p2 )^2 )
>>
>>Is that what you meant by "answer" ?
>>
>>
>>THX
>>stefan
>>
>>
>>
>>
>>2016-07-10 2:27 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>>>
>>> I have seen less sensical questions.
>>>
>>> It would be nice if the example were a bit more complete (as in it
>>should have excess degrees of freedom and an answer) and less like a
>>homework problem (which are off topic here). It would of course also be
>>helpful if the OP were to conform to the Posting Guide, particularly in
>>respect to using plain text email.
>>>
>>> It looks like the kind of nonlinear optimization problem that
>>evolutionary algorithms are often applied to. It doesn't look (to me)
>>like a typical problem that factors get applied to in formulas though,
>>because multiple instances of the same factor variable are present.
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On July 9, 2016 4:59:30 PM PDT, Rolf Turner <r.turner at auckland.ac.nz>
>>wrote:
>>> >On 09/07/16 20:52, stn021 wrote:
>>> >> Hello,
>>> >>
>>> >> I would like to analyse a model like this:
>>> >>
>>> >> y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )
>>> >>
>>> >> x1 and x2 are not continuous variables but factors, so the
>>> >observation
>>> >> contain the level.
>>> >> Its numerical value is unknown and is to be estimated with the
>>model.
>>> >>
>>> >>
>>> >> The observations look like this:
>>> >>
>>> >> y        x1     x2
>>> >> 0.96  Alice  Bob
>>> >> 0.84  Alice  Charlie
>>> >> 0.96  Bob   Charlie
>>> >> 0.64  Dave Alice
>>> >> etc.
>>> >>
>>> >> Each person has a numerical value. Here for example Alice = 0.2
>>and
>>> >Bob =
>>> >> 0.4
>>> >>
>>> >> Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.
>>> >>
>>> >> How can this be done in R ?
>>> >
>>> >
>>> >This question makes about as little sense as it is possible to
>>imagine.
>>> >
>>> >cheers,
>>> >
>>> >Rolf Turner
>>>
>


From wittmaierk at gmail.com  Wed Jul 13 15:50:34 2016
From: wittmaierk at gmail.com (Kyle Wittmaier)
Date: Wed, 13 Jul 2016 09:50:34 -0400
Subject: [R] Lake Analyzer Help
Message-ID: <CAHFvYA4KCzjX0a-qjKiEWcchgU0_A6bWnpa9HHmXoeX7LBue-g@mail.gmail.com>

I am using LakeAnalyzer in Rstudio to produce heat maps and plots using
data from constant monitoring buoys. I have a prewritten script that is
functioning on a colleagues computer perfectly. I am using Rstudio 0.99.902
and R 3.3.1. I have added four packages to the project (lattice,
manipulate, matrix, and rLakeAnalyzer). I am using the exact same data
files and process as my colleague. When I run the script no error messages
appear and everything appears to work perfectly, except there are
significant gaps in the data shown on the plots. Any ideas?

Thank you

	[[alternative HTML version deleted]]


From e.daadmehr at yahoo.com  Wed Jul 13 12:29:49 2016
From: e.daadmehr at yahoo.com (Elham Daadmehr)
Date: Wed, 13 Jul 2016 10:29:49 +0000 (UTC)
Subject: [R] About "SpatioTemporal" Package
References: <335777775.2317453.1468405789218.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <335777775.2317453.1468405789218.JavaMail.yahoo@mail.yahoo.com>

? Dear all,

?I want to build the data with the structure of STdata (using the
 function "createSTdata()"), I did as the following:

 ####
 datst=cbind(xy)
 yy=cbind(y,xx)
 ID=60101:60128
 date=as.numeric(1:13)
 y1=as.data.frame(t(cbind(yy)))
 datst1=as.data.frame(cbind(ID,datst))
 aa=as.Date(date,format = "%Y-%m-%d",origin = "2016-01-31")
 rownames(y1)<- aa
 colnames(y1)<- ID
 colnames(datst1)<-c("ID","x","y")
 ####

 But I don't know why the function "createSTdata(y1, datst1)" give me
 the following error:

 ####
 Error in stCheckClass(obs$ID, "character", name = "obs$ID") :
 ? obs$ID? must belong to one of class(es)? character
 In addition: Warning messages:
 1: In createSTdata(y1, datst1) :
 ? Unable to find column 'obs$obs', using 'obs[,1]
 2: In createSTdata(y1, datst1) :
 ? Unable to find column 'obs$date', using 'obs[,2]
 3: In createSTdata(y1, datst1) :
 ? Unable to find column 'obs$ID', using 'obs[,3]
 ####
 I checked the example data "mesa.data.raw", I found that my "ID" has
 the same type (integer) as the one in this data.


 Kindly thanks, in advanceElham


	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed Jul 13 16:34:18 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 13 Jul 2016 16:34:18 +0200
Subject: [R] Lake Analyzer Help
In-Reply-To: <CAHFvYA4KCzjX0a-qjKiEWcchgU0_A6bWnpa9HHmXoeX7LBue-g@mail.gmail.com>
References: <CAHFvYA4KCzjX0a-qjKiEWcchgU0_A6bWnpa9HHmXoeX7LBue-g@mail.gmail.com>
Message-ID: <CAJuCY5yDJ5P4p63gRnH2vUBhGPHbdEY62UCvFLGAKC+0DYY62Q@mail.gmail.com>

This will be very hard to answer without a reproducible example.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-07-13 15:50 GMT+02:00 Kyle Wittmaier <wittmaierk at gmail.com>:

> I am using LakeAnalyzer in Rstudio to produce heat maps and plots using
> data from constant monitoring buoys. I have a prewritten script that is
> functioning on a colleagues computer perfectly. I am using Rstudio 0.99.902
> and R 3.3.1. I have added four packages to the project (lattice,
> manipulate, matrix, and rLakeAnalyzer). I am using the exact same data
> files and process as my colleague. When I run the script no error messages
> appear and everything appears to work perfectly, except there are
> significant gaps in the data shown on the plots. Any ideas?
>
> Thank you
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul 13 16:37:19 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 13 Jul 2016 07:37:19 -0700
Subject: [R] Aggregate rainfall data
In-Reply-To: <CANTvJZJSN=7i-nez4J7APX+HaaNh6B0j-aHTRSzPaaw_LK+vLg@mail.gmail.com>
References: <CANTvJZLA-W63e6HPMAgadWMmR6W6wubf5UuB=cVtdW7kqp1J0Q@mail.gmail.com>
	<6CD80698-D9FE-4D11-BB90-F918EAE2BCD8@comcast.net>
	<CANTvJZJSN=7i-nez4J7APX+HaaNh6B0j-aHTRSzPaaw_LK+vLg@mail.gmail.com>
Message-ID: <AB0D589D-5179-439E-947D-9C6B302B71D0@comcast.net>


> On Jul 13, 2016, at 3:21 AM, roslinazairimah zakaria <roslinaump at gmail.com> wrote:
> 
> Dear David,
> 
> I got your point.  How do I remove the data that contain "0.0?".
> 
> I tried : balok <- cbind(balok3[,-5], balok3$Rain.mm[balok3$Rain.mm==0.0?] <- NA)

If you had done as I suggested, the items with factor levels of "0.0?" would have automatically become NA and you would have gotten a warning message:

> testfac <- factor( c(rep("0.0",7), "0.07", "0.0?", '0.01', '0.17'))
> testfac
 [1] 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.07 0.0? 0.01 0.17
Levels: 0.0 0.0? 0.01 0.07 0.17
> as.numeric(as.character( testfac))
 [1] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07   NA 0.01 0.17
Warning message:
NAs introduced by coercion 



> 
> However all the Rain.mm column all become NA.
> 
>    day month year     Time balok3$Rain.mm[balok3$Rain.mm == "0.0?"] <- NA
> 1   30     7 2008  9:00:00                                             NA
> 2   30     7 2008 10:00:00                                             NA
> 3   30     7 2008 11:00:00                                             NA
> 4   30     7 2008 12:00:00                                             NA
> 5   30     7 2008 13:00:00                                             NA
> 6   30     7 2008 14:00:00                                             NA
> 7   30     7 2008 15:00:00                                             NA
> 8   30     7 2008 16:00:00                                             NA
> 9   30     7 2008 17:00:00                                             NA
> 10  30     7 2008 18:00:00                                             NA
> 
> Thank you so much.
> 
> 
> On Wed, Jul 13, 2016 at 9:42 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > On Jul 12, 2016, at 3:45 PM, roslinazairimah zakaria <roslinaump at gmail.com> wrote:
> >
> > Dear R-users,
> >
> > I have these data:
> >
> > head(balok, 10); tail(balok, 10)
> >        Date     Time Rain.mm
> > 1  30/7/2008  9:00:00       0
> > 2  30/7/2008 10:00:00       0
> > 3  30/7/2008 11:00:00       0
> > 4  30/7/2008 12:00:00       0
> > 5  30/7/2008 13:00:00       0
> > 6  30/7/2008 14:00:00       0
> > 7  30/7/2008 15:00:00       0
> > 8  30/7/2008 16:00:00       0
> > 9  30/7/2008 17:00:00       0
> > 10 30/7/2008 18:00:00       0
> >           Date     Time Rain.mm
> > 63667 4/11/2015  3:00:00       0
> > 63668 4/11/2015  4:00:00       0
> > 63669 4/11/2015  5:00:00       0
> > 63670 4/11/2015  6:00:00       0
> > 63671 4/11/2015  7:00:00       0
> > 63672 4/11/2015  8:00:00       0
> > 63673 4/11/2015  9:00:00     0.1
> > 63674 4/11/2015 10:00:00     0.1
> > 63675 4/11/2015 11:00:00     0.1
> > 63676 4/11/2015 12:00:00    0.1?
> >
> >> str(balok)
> > 'data.frame':   63676 obs. of  3 variables:
> > $ Date   : Factor w/ 2654 levels "1/1/2009","1/1/2010",..: 2056 2056 2056
> > 2056 2056 2056 2056 2056 2056 2056 ...
> > $ Time   : Factor w/ 24 levels "1:00:00","10:00:00",..: 24 2 3 4 5 6 7 8 9
> > 10 ...
> > $ Rain.mm: Factor w/ 352 levels "0","0.0?","0.1",..: 1 1 1 1 1 1 1 1 1 1
> 
> Thar's your problem:
> 
>   Rain.mm: Factor w/ 352 levels "0","0.0?","0.1"
> 
> Need to use the standard fix for the screwed-up-factor-on-input-problem
> 
>   balok$Rain.mm2 <- as.numeric( as.character(balok$Rain.mm) )
> 
> Cannot just do as.numeric because factors are actually already numeric.
> 
> --
> David.
> 
> 
> > ...
> >
> > and I have change the data as follows:
> >
> > realdate <- as.Date(balok$Date,format="%d/%m/%Y")
> > dfdate <- data.frame(date=realdate)
> > year=as.numeric (format(realdate,"%Y"))
> > month=as.numeric (format(realdate,"%m"))
> > day=as.numeric (format(realdate,"%d"))
> >
> > balok2 <-cbind(dfdate,day,month,year,balok[,2:3])
> > colnames(balok2)
> > head(balok2)
> >        date day month year     Time Rain.mm
> > 1 2008-07-30  30     7 2008  9:00:00       0
> > 2 2008-07-30  30     7 2008 10:00:00       0
> > 3 2008-07-30  30     7 2008 11:00:00       0
> > 4 2008-07-30  30     7 2008 12:00:00       0
> > 5 2008-07-30  30     7 2008 13:00:00       0
> > 6 2008-07-30  30     7 2008 14:00:00       0
> > ...
> >
> >> balok3 <- balok2[,-1]; head(balok3, n=100)
> >    day month year     Time Rain.mm
> > 1    30     7 2008  9:00:00       0
> > 2    30     7 2008 10:00:00       0
> > 3    30     7 2008 11:00:00       0
> > 4    30     7 2008 12:00:00       0
> > 5    30     7 2008 13:00:00       0
> > 6    30     7 2008 14:00:00       0
> > 7    30     7 2008 15:00:00       0
> > 8    30     7 2008 16:00:00       0
> > 9    30     7 2008 17:00:00       0
> > 10   30     7 2008 18:00:00       0
> > 11   30     7 2008 19:00:00       0
> > 12   30     7 2008 20:00:00       0
> > 13   30     7 2008 21:00:00       0
> > 14   30     7 2008 22:00:00       0
> > 15   30     7 2008 23:00:00       0
> > 16   30     7 2008 24:00:00       0
> > 17   31     7 2008  1:00:00       0
> > 18   31     7 2008  2:00:00       0
> > 19   31     7 2008  3:00:00       0
> > 20   31     7 2008  4:00:00       0
> > 21   31     7 2008  5:00:00       0
> > 22   31     7 2008  6:00:00       0
> > 23   31     7 2008  7:00:00       0
> > 24   31     7 2008  8:00:00       0
> > 25   31     7 2008  9:00:00       0
> > 26   31     7 2008 10:00:00       0
> > 27   31     7 2008 11:00:00       0
> > 28   31     7 2008 12:00:00       0
> > 29   31     7 2008 13:00:00       0
> > 30   31     7 2008 14:00:00       0
> > 31   31     7 2008 15:00:00       0
> > 32   31     7 2008 16:00:00       0
> > 33   31     7 2008 17:00:00       0
> > 34   31     7 2008 18:00:00       0
> > 35   31     7 2008 19:00:00       0
> > 36   31     7 2008 20:00:00       0
> > 37   31     7 2008 21:00:00       0
> > 38   31     7 2008 22:00:00       0
> > 39   31     7 2008 23:00:00       0
> > 40   31     7 2008 24:00:00       0
> > 41    1     8 2008  1:00:00       0
> > 42    1     8 2008  2:00:00       0
> > 43    1     8 2008  3:00:00       0
> > 44    1     8 2008  4:00:00       0
> > 45    1     8 2008  5:00:00       0
> > 46    1     8 2008  6:00:00       0
> > 47    1     8 2008  7:00:00       0
> > 48    1     8 2008  8:00:00       0
> > 49    1     8 2008  9:00:00       0
> > 50    1     8 2008 10:00:00       0
> > 51    1     8 2008 11:00:00       0
> > 52    1     8 2008 12:00:00       0
> > 53    1     8 2008 13:00:00       0
> > 54    1     8 2008 14:00:00       0
> > 55    1     8 2008 15:00:00       0
> > 56    1     8 2008 16:00:00       0
> > 57    1     8 2008 17:00:00       0
> > 58    1     8 2008 18:00:00       0
> > 59    1     8 2008 19:00:00       0
> > 60    1     8 2008 20:00:00       0
> > 61    1     8 2008 21:00:00       0
> > 62    1     8 2008 22:00:00       0
> > 63    1     8 2008 23:00:00       0
> > 64    1     8 2008 24:00:00       0
> > 65    2     8 2008  1:00:00       0
> > 66    2     8 2008  2:00:00       0
> > 67    2     8 2008  3:00:00       0
> > 68    2     8 2008  4:00:00       0
> > 69    2     8 2008  5:00:00       0
> > 70    2     8 2008  6:00:00       0
> > 71    2     8 2008  7:00:00       0
> > 72    2     8 2008  8:00:00       0
> > 73    2     8 2008  9:00:00       0
> > 74    2     8 2008 10:00:00       0
> > 75    2     8 2008 11:00:00       0
> > 76    2     8 2008 12:00:00       0
> > 77    2     8 2008 13:00:00       0
> > 78    2     8 2008 14:00:00       0
> > 79    2     8 2008 15:00:00       0
> > 80    2     8 2008 16:00:00       0
> > 81    2     8 2008 17:00:00       0
> > 82    2     8 2008 18:00:00       0
> > 83    2     8 2008 19:00:00       0
> > 84    2     8 2008 20:00:00       0
> > 85    2     8 2008 21:00:00       0
> > 86    2     8 2008 22:00:00       0
> > 87    2     8 2008 23:00:00       0
> > 88    2     8 2008 24:00:00    11.1
> > 89    3     8 2008  1:00:00     0.4
> > 90    3     8 2008  2:00:00       0
> > 91    3     8 2008  3:00:00       0
> > 92    3     8 2008  4:00:00       0
> > 93    3     8 2008  5:00:00       0
> > 94    3     8 2008  6:00:00       0
> > 95    3     8 2008  7:00:00       0
> > 96    3     8 2008  8:00:00       0
> > 97    3     8 2008  9:00:00       0
> > 98    3     8 2008 10:00:00       0
> > 99    3     8 2008 11:00:00       0
> > 100   3     8 2008 12:00:00       0
> >
> > The rainfall data is in hourly unit, and I would like to sum the Rain.mm
> > according to month.  I tried to use aggregate(), but I got this message:
> >
> > dt <- balok4
> > str(dt)
> > aggbalok <- aggregate(dt[,5], by=dt[,c(1,4)],FUN=sum, na.rm=TRUE)
> > aggbalok
> >
> > Error in Summary.factor(1L, na.rm = TRUE) :
> >  sum not meaningful for factors
> >
> >
> > Thank you so much for any help given.
> >
> > Roslina
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 
> 
> 
> -- 
> Dr. Roslinazairimah Binti Zakaria
> Tel: +609-5492370; Fax. No.+609-5492766
> Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
> Deputy Dean (Academic & Student Affairs)
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

David Winsemius
Alameda, CA, USA


From thierry.onkelinx at inbo.be  Wed Jul 13 16:44:50 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 13 Jul 2016 16:44:50 +0200
Subject: [R] Lake Analyzer Help
In-Reply-To: <CAHFvYA575TxEVrp6+TBLA_b9jjDxE1VXMyPgFNQa1eVA8Voa4A@mail.gmail.com>
References: <CAHFvYA4KCzjX0a-qjKiEWcchgU0_A6bWnpa9HHmXoeX7LBue-g@mail.gmail.com>
	<CAJuCY5yDJ5P4p63gRnH2vUBhGPHbdEY62UCvFLGAKC+0DYY62Q@mail.gmail.com>
	<CAHFvYA575TxEVrp6+TBLA_b9jjDxE1VXMyPgFNQa1eVA8Voa4A@mail.gmail.com>
Message-ID: <CAJuCY5z=qwu8iC74FZ9YcvAs5_Bgx+B=QfW9nLPhYoh+txA16A@mail.gmail.com>

Please keep the mailing list in cc.

See http://adv-r.had.co.nz/Reproducibility.html for some instructions.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-07-13 16:38 GMT+02:00 Kyle Wittmaier <wittmaierk at gmail.com>:

> would providing the script be the most efficient means to show an example?
>
> On Wed, Jul 13, 2016 at 10:34 AM, Thierry Onkelinx <
> thierry.onkelinx at inbo.be> wrote:
>
>> This will be very hard to answer without a reproducible example.
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2016-07-13 15:50 GMT+02:00 Kyle Wittmaier <wittmaierk at gmail.com>:
>>
>>> I am using LakeAnalyzer in Rstudio to produce heat maps and plots using
>>> data from constant monitoring buoys. I have a prewritten script that is
>>> functioning on a colleagues computer perfectly. I am using Rstudio
>>> 0.99.902
>>> and R 3.3.1. I have added four packages to the project (lattice,
>>> manipulate, matrix, and rLakeAnalyzer). I am using the exact same data
>>> files and process as my colleague. When I run the script no error
>>> messages
>>> appear and everything appears to work perfectly, except there are
>>> significant gaps in the data shown on the plots. Any ideas?
>>>
>>> Thank you
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From sezenismail at gmail.com  Wed Jul 13 16:54:39 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Wed, 13 Jul 2016 17:54:39 +0300
Subject: [R] Dates in R (Year Month)
In-Reply-To: <BBB169C177D06B4E8E650D24548404CEC0B92F62@PTABREXC12M2.sars.gov.za>
References: <BBB169C177D06B4E8E650D24548404CEC0B92F62@PTABREXC12M2.sars.gov.za>
Message-ID: <05783494-F8A7-4663-92B7-2BB862DE4E7A@gmail.com>

You can not convert numeric vectors directly to yearmon object. You must convert the X variable to character and add ?-? between year and month. Then as.yearmon function will work properly.

Please, read help pages of ?as.character, ?strptime and ?as.yearmon.

Example:

> library(zoo)
> aaa <- as.yearmon(c("2015-02?, ?2014-06"))
> class(aaa)
[1] "yearmon"

> On 13 Jul 2016, at 16:25, Mangalani Peter Makananisa <pmakananisa at sars.gov.za> wrote:
> 
> Hi All,
> 
> I am trying to convert the vector below to dates please assist I have tried to use information on the links you sent, but it is not working.
> 
> X  = c(201501, 201502, 201503, 201505, 201506, 201507, 201508, 201509, 201510, 201511, 201512, 201601, 201602, 201603, 201604, 201605, 201606)
> 
> library(chron, zoo)
> Z = as.yearmon(X)  # it is not working
> 
> please assist
> 
> Kind regards
> Peter
> 
> Please Note: This email and its contents are subject to our email legal notice which can be viewed at http://www.sars.gov.za/Pages/Email-disclaimer.aspx
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Wed Jul 13 17:01:08 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 13 Jul 2016 08:01:08 -0700
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <CANVHAvZi4SM2i1+UJL-jEHQvQuHe+Txqe=Y-Xv5+q5sCAwuD3w@mail.gmail.com>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
	<CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
	<AF7E2508-9E20-428C-96D9-1182D27A27A6@dcn.davis.ca.us>
	<CANVHAvZi4SM2i1+UJL-jEHQvQuHe+Txqe=Y-Xv5+q5sCAwuD3w@mail.gmail.com>
Message-ID: <A784205C-C35B-442B-B69D-5EACFD3FFFE3@comcast.net>


> On Jul 13, 2016, at 6:48 AM, stn021 <stn021 at gmail.com> wrote:
> 
> Hello,
> 
> so here a numerical example in R-code. Code is appended below.
> 
> The output should be
> 1) the numerical values of the abilities of the persons
> 2) the multiplyer
> 
> 
> Please note that
> 
> 1) I have used non-linear optimization to solve this problem and got
> the expected result, though not with R but other software.
> 
> 2) I have applied lm() to this problem, even before I posted the
> question. I am well aware of the syntax of formulas. I my last posting
> I wrote the formula "freehand" so I made the previously mentioned
> errors. Sorry about that.
> 
> 
> 
> Unfortunately the formulas with I() as well as multiplying variables
> before running R does not work here. I() does not apply to factors (R
> tells me) and multiplying in advance also works only for continuous
> variables, not for factors, because there is no known numerical value
> to multiply.
> 
> The latter is actually what my question is about, along with the
> question on how to get R to treat two columns as two instances of the
> same factor.
> 
> 
> Just to be sure I used R to check if the data really counts as a
> factor according to R-terminology. It really is a factor, see code
> below.
> 
> 
> 
> This is the code for generating the example-data:
> 
> # --------------------------------------------------------------- #
> pnames    = c( "alice" , "bob" , "charlie" , "don" , "eve" , "freddy"
> , "grace" , "henry" )
> pcount    = length( pnames )
> 
> # abilities = runif( pcount )
> abilities = (1:pcount) / 10
> 
> persons = data.frame( name = pnames , ability = abilities )
> persons
> 
> # random subset of possible combinations and extra df
> combinations = combn( nrow( persons ) , 2 ) ;
> combinations = cbind( combinations,combinations,combinations,combinations )
> combinations = combinations[ , runif(ncol(combinations))<0.5 ]
> ccount = ncol( combinations )
> 
> observed_data = data.frame(
>  idx1 = combinations[1,]
> , idx2 = combinations[2,]
> , p1 = ( persons$name[    combinations[1,] ] )
> , p2 = ( persons$name[    combinations[2,] ] )
> )
> 
> abilities_data = data.frame(
>  a1 = persons$ability[ combinations[1,] ]
> , a2 = persons$ability[ combinations[2,] ]
> )
> 
> # y = result of cooperation of each pair
> multiplyer = runif(1) + 1
> offset     = 1
> cat( "multiplyer = " , multiplyer , "\n" )
> cat( "offset = " , offset , "\n" )
> 
> y0 = multiplyer * ( offset - ( abilities_data$a1 - abilities_data$a2 ) ^ 2 )
> noise = .05 * rnorm( ccount )
> 
> # check variables are really factors :
> str(  observed_data$p1 )
> dput( observed_data$p1 )
> 
> observed_data = data.frame( y = round( y0+noise,3 ) , observed_data )
> observed_data
> 
> # --------------------------------------------------------------- #

Is this what is intended?

> observed_data$p1ab <- persons$ability[ match(observed_data$p1, persons$name) ]
> observed_data$p2ab <- persons$ability[ match(observed_data$p2, persons$name) ]
> head(observed_data)
      y idx1 idx2    p1      p2 p1ab p2ab
1 1.149    1    6 alice  freddy  0.1  0.6
2 1.006    1    7 alice   grace  0.1  0.7
3 1.529    2    3   bob charlie  0.2  0.3
4 1.404    2    5   bob     eve  0.2  0.5
5 1.205    2    6   bob  freddy  0.2  0.6
6 1.187    2    7   bob   grace  0.2  0.7


> lm( y ~ I( (p1ab -p2ab)^2 ), data=observed_data)

Call:
lm(formula = y ~ I((p1ab - p2ab)^2), data = observed_data)

Coefficients:
       (Intercept)  I((p1ab - p2ab)^2)  
             1.506              -1.435  

>  separate_term <- lm( y ~ I( (p1ab -p2ab)^2 ), data=observed_data)
> summary(separate_term)

Call:
lm(formula = y ~ I((p1ab - p2ab)^2), data = observed_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.116249 -0.030996  0.002633  0.032765  0.136282 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)         1.50589    0.01067  141.08   <2e-16 ***
I((p1ab - p2ab)^2) -1.43527    0.05863  -24.48   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 0.05304 on 44 degrees of freedom
Multiple R-squared:  0.9316,	Adjusted R-squared:   0.93 
F-statistic: 599.2 on 1 and 44 DF,  p-value: < 2.2e-16

You could also have compared 2 models differing only with rest to the includion of an interaction term that was the squared difference in abilities:

> full <- lm( y ~ p1ab + p2ab + I( (p1ab -p2ab)^2 ), data=observed_data)
> reduced <- lm( y ~ p1ab + p2ab , data=observed_data)
> anova(full,reduced)
Analysis of Variance Table

Model 1: y ~ p1ab + p2ab + I((p1ab - p2ab)^2)
Model 2: y ~ p1ab + p2ab
  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    
1     42 0.11823                                  
2     43 0.17315 -1  -0.05492 19.509 6.892e-05 ***


-- 
David

> 
> 
> 2016-07-11 19:16 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>> Your clarification is promising.  A reproducible example is always preferred, though never a guarantee. I expect to be somewhat preoccupied this week so responses may be rather delayed, but the less setup we have to the more likely that someone on the list will tackle it.
>> 
>> Re an answer: If you can make the example simple enough that you can tell us what the right numerical result will be, we will have a better chance of understanding what you are after.  E.g. if you start with a solution and use it to create sample input data with then you don't need to actually solve it to illustrate what you are after. [1]
>> 
>> Note that I am not aware of any package dedicated to this type of problem, so unless someone else responds otherwise then you will likely have to use bootstrapping or your own statistical analysis (Bayesian?) of the result.
>> 
>> [1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>> --
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 11, 2016 7:28:41 AM PDT, stn021 <stn021 at gmail.com> wrote:
>>> Hello,
>>> 
>>> thank you for the replies. Sorry about the html-email, I forgot.
>>> Should be OK with this email.
>>> 
>>> 
>>> Don't be fooled be the apparent simplicity of the problem. I have
>>> tried to reduce it to only a single relatively simple question.
>>> 
>>> The idea here is to model cooperation of two persons. The model is
>>> about one specific aspect of that cooperation, namely that two persons
>>> with similar abilities may be able to produce better results that two
>>> very different persons.
>>> 
>>> That is only one part of the model with other parts modeling for
>>> example the fact that of course two persons with a higher degree of
>>> ability will produce better results per se.
>>> 
>>> 
>>> It is not classic regression with factors. That can be easily done by
>>> something like lm( y ~ (p1-p2)^2 ).
>>> 
>>> This expands to lm( y ~ p1^2 - 2*p1*p2 + p2^2 ). This contains a
>>> multiplicagtions and for lm() this implies interactions between the
>>> factor-levels and produces one parameter for each combination of
>>> factor-levels that occurs in the data. That is not what the question
>>> is about.
>>> 
>>> Also p1 and p2 are different levels of the same factor, while for lm()
>>> it would be two different factors with different levels.
>>> 
>>> 
>>> As for the sensical part: this has a real world application therefore
>>> it makes sense.
>>> 
>>> Also it is not so difficult to solve with non-linear optimization. I
>>> was hoping to be able to use R for that purpose because then the
>>> results could easily be checked with statistical tests.
>>> 
>>> So my question is not "how to solve" but "how to solve with R".
>>> 
>>> 
>>> As for the excess degrees of freedom, in real observations there would
>>> of course be added noise due to either random variations or factors
>>> not included in the model. So to generate a more reality-conforming
>>> example I could add some random normal-distributed noise to the
>>> dependent variable y. I previously left that part out because to me it
>>> did not seem relevant.
>>> 
>>> 
>>> Would you like me to make a complete example dataset with more records
>>> and noise ?
>>> 
>>> 
>>> The answer I look for would be the numerical values of the
>>> factor-levels and numerical values for the multiplier (f) and the
>>> offset (o), with p1 and p2 given as names (here: persons) and y given
>>> as some level of achievement they reach by cooperating.
>>> 
>>> y = f * ( o - ( p1 - p2 )^2 )
>>> 
>>> Is that what you meant by "answer" ?
>>> 
>>> 
>>> THX
>>> stefan
>>> 
>>> 
>>> 
>>> 
>>> 2016-07-10 2:27 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>>>> 
>>>> I have seen less sensical questions.
>>>> 
>>>> It would be nice if the example were a bit more complete (as in it
>>> should have excess degrees of freedom and an answer) and less like a
>>> homework problem (which are off topic here). It would of course also be
>>> helpful if the OP were to conform to the Posting Guide, particularly in
>>> respect to using plain text email.
>>>> 
>>>> It looks like the kind of nonlinear optimization problem that
>>> evolutionary algorithms are often applied to. It doesn't look (to me)
>>> like a typical problem that factors get applied to in formulas though,
>>> because multiple instances of the same factor variable are present.
>>>> --
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On July 9, 2016 4:59:30 PM PDT, Rolf Turner <r.turner at auckland.ac.nz>
>>> wrote:
>>>>> On 09/07/16 20:52, stn021 wrote:
>>>>>> Hello,
>>>>>> 
>>>>>> I would like to analyse a model like this:
>>>>>> 
>>>>>> y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )
>>>>>> 
>>>>>> x1 and x2 are not continuous variables but factors, so the
>>>>> observation
>>>>>> contain the level.
>>>>>> Its numerical value is unknown and is to be estimated with the
>>> model.
>>>>>> 
>>>>>> 
>>>>>> The observations look like this:
>>>>>> 
>>>>>> y        x1     x2
>>>>>> 0.96  Alice  Bob
>>>>>> 0.84  Alice  Charlie
>>>>>> 0.96  Bob   Charlie
>>>>>> 0.64  Dave Alice
>>>>>> etc.
>>>>>> 
>>>>>> Each person has a numerical value. Here for example Alice = 0.2
>>> and
>>>>> Bob =
>>>>>> 0.4
>>>>>> 
>>>>>> Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.
>>>>>> 
>>>>>> How can this be done in R ?
>>>>> 
>>>>> 
>>>>> This question makes about as little sense as it is possible to
>>> imagine.
>>>>> 
>>>>> cheers,
>>>>> 
>>>>> Rolf Turner
>>>> 
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dcarlson at tamu.edu  Wed Jul 13 17:04:39 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 13 Jul 2016 15:04:39 +0000
Subject: [R] Dates in R (Year Month)
In-Reply-To: <BBB169C177D06B4E8E650D24548404CEC0B92F62@PTABREXC12M2.sars.gov.za>
References: <BBB169C177D06B4E8E650D24548404CEC0B92F62@PTABREXC12M2.sars.gov.za>
Message-ID: <784f72ffe353448f9d216c4c2a1eb8a5@exch-2p-mbx-t2.ads.tamu.edu>

You need to look at the examples on the manual pages for ?yearmon and ?strptime:

> Z <- as.yearmon(as.character(X), "%Y%m")
> Z
 [1] "Jan 2015" "Feb 2015" "Mar 2015" "May 2015" "Jun 2015" "Jul 2015" "Aug 2015"
 [8] "Sep 2015" "Oct 2015" "Nov 2015" "Dec 2015" "Jan 2016" "Feb 2016" "Mar 2016"
[15] "Apr 2016" "May 2016" "Jun 2016"

Or
> Z <- as.yearmon(round(X/100) + (X - round(X/100)*100 - 1)/12)
> Z
 [1] "Jan 2015" "Feb 2015" "Mar 2015" "May 2015" "Jun 2015" "Jul 2015" "Aug 2015"
 [8] "Sep 2015" "Oct 2015" "Nov 2015" "Dec 2015" "Jan 2016" "Feb 2016" "Mar 2016"
[15] "Apr 2016" "May 2016" "Jun 2016"

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Mangalani Peter Makananisa
Sent: Wednesday, July 13, 2016 8:25 AM
To: r-help at r-project.org
Subject: [R] Dates in R (Year Month)

Hi All,

I am trying to convert the vector below to dates please assist I have tried to use information on the links you sent, but it is not working.

X  = c(201501, 201502, 201503, 201505, 201506, 201507, 201508, 201509, 201510, 201511, 201512, 201601, 201602, 201603, 201604, 201605, 201606)

library(chron, zoo)
Z = as.yearmon(X)  # it is not working

please assist

Kind regards
Peter

Please Note: This email and its contents are subject to our email legal notice which can be viewed at http://www.sars.gov.za/Pages/Email-disclaimer.aspx

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From utkarsh.iit at gmail.com  Wed Jul 13 18:06:12 2016
From: utkarsh.iit at gmail.com (Utkarsh Singhal)
Date: Wed, 13 Jul 2016 21:36:12 +0530
Subject: [R] Linear model vs Mixed model
In-Reply-To: <CAM5M9BSfS-HO7hqU+5=vJ6c+cROG+kWGSfZq95+myA+0peGAAQ@mail.gmail.com>
References: <CAJP88kQcCgJ6vg5dVdxPfpMtcZpb72SpcPehA+PoEUWvPUrVRw@mail.gmail.com>
	<CAJuCY5wRmHOVGxK3kvUHwtAdL85jg0kyLo9GKMEfezdLaBijfQ@mail.gmail.com>
	<CAJP88kR3C8dgVq+4UyCzedpbsZ_rn4BguOa9+YPwCs5ZoN21aQ@mail.gmail.com>
	<CAM5M9BSfS-HO7hqU+5=vJ6c+cROG+kWGSfZq95+myA+0peGAAQ@mail.gmail.com>
Message-ID: <CAJP88kSFxqrdQP4YsE4zPWuOzvJmY7BQS_a6+GB3hAfrguLJUw@mail.gmail.com>

Hi Brian,

This makes some sense to me theoretically, but doesn't pan out with my
experiment.

The contrasts default was the following as you said:
> options("contrasts")
$contrasts
        unordered           ordered
"contr.treatment"      "contr.poly"

I changed it as follows:
> options(contracts=c(unordered="contr.sum", ordered="contr.poly"))

But the output of 'lm' model was exactly the same as before.

Then I tried the following:
> coef(lm(Reaction ~ Days + Subject, sleepstudy,
contrasts=list(Subject="contr.sum")))

The model output appears slightly different, but the 'Subject* + Intercept'
is still exactly the same as the default 'lm' model.

Also, as you can verify below, the predictions from two 'lm' models are
exactly the  same

> fit_lm = lm(Reaction ~ Days + Subject, sleepstudy);
> fit_lm2 = lm(Reaction ~ Days + Subject, sleepstudy,
contrasts=list(Subject="contr.sum"))
>
> head(predict(fit_lm))
       1        2        3        4        5        6
295.0310 305.4983 315.9656 326.4329 336.9002 347.3675

> head(predict(fit_lm2))
       1        2        3        4        5        6
295.0310 305.4983 315.9656 326.4329 336.9002 347.3675

And these are quite different from the mixedmodel:

> fit_lmer = lmer(Reaction ~ Days + (1| Subject), sleepstudy)
> head(predict(fit_lmer))
       1        2        3        4        5        6
292.1888 302.6561 313.1234 323.5907 334.0580 344.5252

Any idea?



Utkarsh Singhal
91.96508.54333


On 13 July 2016 at 00:18, Cade, Brian <cadeb at usgs.gov> wrote:

> Your lm() estimates are using the default contrasts of contr.treatment,
> providing an intercept corresponding to your subject 308 and the other
> subject* estimates are differences from subject 308 intercept.  You could
> have specified this with contrasts as contr.sum and the estimates would be
> more easily compared to the lmer() model estimates.  They are close but
> will never be identical as the lmer() model estimates are based on assuming
> a normal distribution with specified variance.  They rarely would be
> identical.
>
> Brian
>
> Brian S. Cade, PhD
>
> U. S. Geological Survey
> Fort Collins Science Center
> 2150 Centre Ave., Bldg. C
> Fort Collins, CO  80526-8818
>
> email:  cadeb at usgs.gov <brian_cade at usgs.gov>
> tel:  970 226-9326
>
>
> On Tue, Jul 12, 2016 at 12:10 PM, Utkarsh Singhal <utkarsh.iit at gmail.com>
> wrote:
>
>> Hello Thierry,
>>
>> Thank you for your quick response. Sorry, but I am not sure if I follow
>> what you said. I get the following outputs from the two models:
>> > coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
>> Subject    (Intercept)     Days
>> 308    292.1888 10.46729
>> 309    173.5556 10.46729
>> 310    188.2965 10.46729
>> 330    255.8115 10.46729
>> 331    261.6213 10.46729
>> 332    259.6263 10.46729
>> 333    267.9056 10.46729
>> 334    248.4081 10.46729
>> 335    206.1230 10.46729
>> 337    323.5878 10.46729
>> 349    230.2089 10.46729
>> 350    265.5165 10.46729
>> 351    243.5429 10.46729
>> 352    287.7835 10.46729
>> 369    258.4415 10.46729
>> 370    245.0424 10.46729
>> 371    248.1108 10.46729
>> 372    269.5209 10.46729
>>
>> > coef(lm(Reaction ~ Days + Subject, sleepstudy))
>> (Intercept)  295.03104
>> Days          10.46729
>> Subject309  -126.90085
>> Subject310  -111.13256
>> Subject330   -38.91241
>> Subject331   -32.69778
>> Subject332   -34.83176
>> Subject333   -25.97552
>> Subject334   -46.83178
>> Subject335   -92.06379
>> Subject337    33.58718
>> Subject349   -66.29936
>> Subject350   -28.53115
>> Subject351   -52.03608
>> Subject352    -4.71229
>> Subject369   -36.09919
>> Subject370   -50.43206
>> Subject371   -47.14979
>> Subject372   -24.24770
>>
>> Now, what I expected is the following:
>>
>>    - 'Intercept' of model-2 to match with Intercept of Subject-308 of
>>    model-1
>>    - 'Intercept+Subject309' of model-2 to match with Intercept of
>>    Subject-309 of model-1
>>    - and so on...
>>
>>
>> What am I missing here?
>>
>> If it is difficult to explain this, can you alternately answer the
>> following: "Is it possible to define the 'lm' and 'lmer' models above so
>> they produce the same results (at least in terms of predictions)?"
>>
>> Thanks again.
>>
>> Utkarsh Singhal
>> 91.96508.54333
>>
>>
>> On 12 July 2016 at 19:15, Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>> > The parametrisation is different.
>> >
>> > The intercept in model 1 is the effect of the "average" subject at days
>> ==
>> > 0.
>> > The intercept in model 2 is the effect of the first subject at days ==
>> 0.
>> >
>> > ir. Thierry Onkelinx
>> > Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and
>> > Forest
>> > team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> > Kliniekstraat 25
>> > 1070 Anderlecht
>> > Belgium
>> >
>> > To call in the statistician after the experiment is done may be no more
>> > than asking him to perform a post-mortem examination: he may be able to
>> say
>> > what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> > The plural of anecdote is not data. ~ Roger Brinner
>> > The combination of some data and an aching desire for an answer does not
>> > ensure that a reasonable answer can be extracted from a given body of
>> data.
>> > ~ John Tukey
>> >
>> > 2016-07-12 15:35 GMT+02:00 Utkarsh Singhal <utkarsh.iit at gmail.com>:
>> >
>> >> Hi experts,
>> >>
>> >> While the slope is coming out to be identical in the two methods below,
>> >> the
>> >> intercepts are not. As far as I understand, both are formulations are
>> >> identical in the sense that these are asking for a slope corresponding
>> to
>> >> 'Days' and a separate intercept term for each Subject.
>> >>
>> >> # Model-1
>> >> library(lmer)
>> >> coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
>> >>
>> >> # Model-2
>> >> coef(lm(Reaction ~ Days + Subject, sleepstudy))
>> >>
>> >> Can somebody tell me the reason? Are the above formulations actually
>> >> different or is it due to different optimization method used?
>> >>
>> >> Thank you.
>> >>
>> >> Utkarsh Singhal
>> >> 91.96508.54333
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From stn021 at gmail.com  Wed Jul 13 18:48:27 2016
From: stn021 at gmail.com (stn021)
Date: Wed, 13 Jul 2016 18:48:27 +0200
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <A784205C-C35B-442B-B69D-5EACFD3FFFE3@comcast.net>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
	<CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
	<AF7E2508-9E20-428C-96D9-1182D27A27A6@dcn.davis.ca.us>
	<CANVHAvZi4SM2i1+UJL-jEHQvQuHe+Txqe=Y-Xv5+q5sCAwuD3w@mail.gmail.com>
	<A784205C-C35B-442B-B69D-5EACFD3FFFE3@comcast.net>
Message-ID: <CANVHAvYi6JF41qGDG7xXWFZbOV5j2UOp=Z9j7PSfK+kTYbXRnQ@mail.gmail.com>

> Is this what is intended?
>
>> observed_data$p1ab <- persons$ability[ match(observed_data$p1, persons$name) ]
>> observed_data$p2ab <- persons$ability[ match(observed_data$p2, persons$name) ]


Hello David,

thank you for your answer.


The code in my previous post was intended as an answer to the question
in an earlier post about example-data, quote:

> > Would you like me to make a complete example dataset with more records and noise ?
> Yes. And preferably do it with R code.

I should have re-stated this connection in the post.


The code generates a matrix 'observed_data' which is the data the
experimenter would get during the experiment.

This matrix is output in the last line. All other output is only meant
to document the generation-process.

So the only thing visible to the experimenter before analysis is
exactly that matrix 'observed_data'  (usually in the form of some
written documentation which is later entered into statistical
software). Everything before that last line simulates those unknown
parameters that the experiment is supposed to reveal.

The unknown parameters are specifically
- the matrix 'persons'
- and the variable 'multiplyer'

Both are supposed to be revealed by the analyis. p1ab and p2ab would
therefore depend on the unknown parameters and could not be added to
'observed_data' before the analysis.

Sorry again for omitting the back-reference.


I would like to know:

- how to get R to use p1 and p2 as levels of the same factor
(=persons) instead of levels of two different factors.

- how to get R to multiply the numerical levels of factors during the
search for the solution. Factors cannot be multiplied before running
lm() or some other package because before the analysis their numerical
values are not known.


THX, Stefan


From dwinsemius at comcast.net  Wed Jul 13 18:56:44 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 13 Jul 2016 09:56:44 -0700
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <A784205C-C35B-442B-B69D-5EACFD3FFFE3@comcast.net>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
	<CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
	<AF7E2508-9E20-428C-96D9-1182D27A27A6@dcn.davis.ca.us>
	<CANVHAvZi4SM2i1+UJL-jEHQvQuHe+Txqe=Y-Xv5+q5sCAwuD3w@mail.gmail.com>
	<A784205C-C35B-442B-B69D-5EACFD3FFFE3@comcast.net>
Message-ID: <0F646AB6-EEF8-498B-911C-FEAB028DA347@comcast.net>


> On Jul 13, 2016, at 8:01 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Jul 13, 2016, at 6:48 AM, stn021 <stn021 at gmail.com> wrote:
>> 
>> Hello,
>> 
>> so here a numerical example in R-code. Code is appended below.
>> 
>> The output should be
>> 1) the numerical values of the abilities of the persons
>> 2) the multiplyer
>> 
>> 
>> Please note that
>> 
>> 1) I have used non-linear optimization to solve this problem and got
>> the expected result, though not with R but other software.
>> 
>> 2) I have applied lm() to this problem, even before I posted the
>> question. I am well aware of the syntax of formulas. I my last posting
>> I wrote the formula "freehand" so I made the previously mentioned
>> errors. Sorry about that.
>> 
>> 
>> 
>> Unfortunately the formulas with I() as well as multiplying variables
>> before running R does not work here. I() does not apply to factors (R
>> tells me) and multiplying in advance also works only for continuous
>> variables, not for factors, because there is no known numerical value
>> to multiply.
>> 
>> The latter is actually what my question is about, along with the
>> question on how to get R to treat two columns as two instances of the
>> same factor.
>> 
>> 
>> Just to be sure I used R to check if the data really counts as a
>> factor according to R-terminology. It really is a factor, see code
>> below.
>> 
>> 
>> 
>> This is the code for generating the example-data:
>> 
>> # --------------------------------------------------------------- #
>> pnames    = c( "alice" , "bob" , "charlie" , "don" , "eve" , "freddy"
>> , "grace" , "henry" )
>> pcount    = length( pnames )
>> 
>> # abilities = runif( pcount )
>> abilities = (1:pcount) / 10
>> 
>> persons = data.frame( name = pnames , ability = abilities )
>> persons
>> 
>> # random subset of possible combinations and extra df
>> combinations = combn( nrow( persons ) , 2 ) ;
>> combinations = cbind( combinations,combinations,combinations,combinations )
>> combinations = combinations[ , runif(ncol(combinations))<0.5 ]
>> ccount = ncol( combinations )
>> 
>> observed_data = data.frame(
>> idx1 = combinations[1,]
>> , idx2 = combinations[2,]
>> , p1 = ( persons$name[    combinations[1,] ] )
>> , p2 = ( persons$name[    combinations[2,] ] )
>> )
>> 
>> abilities_data = data.frame(
>> a1 = persons$ability[ combinations[1,] ]
>> , a2 = persons$ability[ combinations[2,] ]
>> )
>> 
>> # y = result of cooperation of each pair
>> multiplyer = runif(1) + 1
>> offset     = 1
>> cat( "multiplyer = " , multiplyer , "\n" )
>> cat( "offset = " , offset , "\n" )
>> 
>> y0 = multiplyer * ( offset - ( abilities_data$a1 - abilities_data$a2 ) ^ 2 )
>> noise = .05 * rnorm( ccount )
>> 
>> # check variables are really factors :
>> str(  observed_data$p1 )
>> dput( observed_data$p1 )
>> 
>> observed_data = data.frame( y = round( y0+noise,3 ) , observed_data )
>> observed_data
>> 
>> # --------------------------------------------------------------- #
> 
> Is this what is intended?
> 
>> observed_data$p1ab <- persons$ability[ match(observed_data$p1, persons$name) ]
>> observed_data$p2ab <- persons$ability[ match(observed_data$p2, persons$name) ]
>> head(observed_data)
>      y idx1 idx2    p1      p2 p1ab p2ab
> 1 1.149    1    6 alice  freddy  0.1  0.6
> 2 1.006    1    7 alice   grace  0.1  0.7
> 3 1.529    2    3   bob charlie  0.2  0.3
> 4 1.404    2    5   bob     eve  0.2  0.5
> 5 1.205    2    6   bob  freddy  0.2  0.6
> 6 1.187    2    7   bob   grace  0.2  0.7
> 
> 
>> lm( y ~ I( (p1ab -p2ab)^2 ), data=observed_data)
> 
> Call:
> lm(formula = y ~ I((p1ab - p2ab)^2), data = observed_data)
> 
> Coefficients:
>       (Intercept)  I((p1ab - p2ab)^2)  
>             1.506              -1.435  
> 
>> separate_term <- lm( y ~ I( (p1ab -p2ab)^2 ), data=observed_data)
>> summary(separate_term)
> 
> Call:
> lm(formula = y ~ I((p1ab - p2ab)^2), data = observed_data)
> 
> Residuals:
>      Min        1Q    Median        3Q       Max 
> -0.116249 -0.030996  0.002633  0.032765  0.136282 
> 
> Coefficients:
>                   Estimate Std. Error t value Pr(>|t|)    
> (Intercept)         1.50589    0.01067  141.08   <2e-16 ***
> I((p1ab - p2ab)^2) -1.43527    0.05863  -24.48   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 0.05304 on 44 degrees of freedom
> Multiple R-squared:  0.9316,	Adjusted R-squared:   0.93 
> F-statistic: 599.2 on 1 and 44 DF,  p-value: < 2.2e-16
> 
> You could also have compared 2 models differing only with rest to the includion of an interaction term that was the squared difference in abilities:
> 
>> full <- lm( y ~ p1ab + p2ab + I( (p1ab -p2ab)^2 ), data=observed_data)
>> reduced <- lm( y ~ p1ab + p2ab , data=observed_data)
>> anova(full,reduced)
> Analysis of Variance Table
> 
> Model 1: y ~ p1ab + p2ab + I((p1ab - p2ab)^2)
> Model 2: y ~ p1ab + p2ab
>  Res.Df     RSS Df Sum of Sq      F    Pr(>F)    
> 1     42 0.11823                                  
> 2     43 0.17315 -1  -0.05492 19.509 6.892e-05 ***

I also tested whether "nesting" the I( ) calls would succeed in giving you any results from what I thought was a rather odd  construction

>>>> y = f * ( o - ( p1 - p2 )^2 )

> separate_term <- lm( y ~ I( (p1ab -p2ab)^2 ), data=observed_data)
> separate_term

Call:
lm(formula = y ~ I((p1ab - p2ab)^2), data = observed_data)

Coefficients:
       (Intercept)  I((p1ab - p2ab)^2)  
             1.506              -1.435  

> separate_term2 <- lm( y ~ I( 1- I( (p1ab -p2ab)^2 )), data=observed_data)
> separate_term2

Call:
lm(formula = y ~ I(1 - I((p1ab - p2ab)^2)), data = observed_data)

Coefficients:
              (Intercept)  I(1 - I((p1ab - p2ab)^2))  
                  0.07062                    1.43527  


The sign of the "interaction term" just gets reversed. I don't see that any offset term got calculated for the "1" term. Offsets can be included but I think this particular form where you are asking for two different coefficients, they might not be separable in the linear model framework. 

-- 
david.


> 
> 
> -- 
> David
> 
>> 
>> 
>> 2016-07-11 19:16 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>>> Your clarification is promising.  A reproducible example is always preferred, though never a guarantee. I expect to be somewhat preoccupied this week so responses may be rather delayed, but the less setup we have to the more likely that someone on the list will tackle it.
>>> 
>>> Re an answer: If you can make the example simple enough that you can tell us what the right numerical result will be, we will have a better chance of understanding what you are after.  E.g. if you start with a solution and use it to create sample input data with then you don't need to actually solve it to illustrate what you are after. [1]
>>> 
>>> Note that I am not aware of any package dedicated to this type of problem, so unless someone else responds otherwise then you will likely have to use bootstrapping or your own statistical analysis (Bayesian?) of the result.
>>> 
>>> [1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>> 
>>> On July 11, 2016 7:28:41 AM PDT, stn021 <stn021 at gmail.com> wrote:
>>>> Hello,
>>>> 
>>>> thank you for the replies. Sorry about the html-email, I forgot.
>>>> Should be OK with this email.
>>>> 
>>>> 
>>>> Don't be fooled be the apparent simplicity of the problem. I have
>>>> tried to reduce it to only a single relatively simple question.
>>>> 
>>>> The idea here is to model cooperation of two persons. The model is
>>>> about one specific aspect of that cooperation, namely that two persons
>>>> with similar abilities may be able to produce better results that two
>>>> very different persons.
>>>> 
>>>> That is only one part of the model with other parts modeling for
>>>> example the fact that of course two persons with a higher degree of
>>>> ability will produce better results per se.
>>>> 
>>>> 
>>>> It is not classic regression with factors. That can be easily done by
>>>> something like lm( y ~ (p1-p2)^2 ).
>>>> 
>>>> This expands to lm( y ~ p1^2 - 2*p1*p2 + p2^2 ). This contains a
>>>> multiplicagtions and for lm() this implies interactions between the
>>>> factor-levels and produces one parameter for each combination of
>>>> factor-levels that occurs in the data. That is not what the question
>>>> is about.
>>>> 
>>>> Also p1 and p2 are different levels of the same factor, while for lm()
>>>> it would be two different factors with different levels.
>>>> 
>>>> 
>>>> As for the sensical part: this has a real world application therefore
>>>> it makes sense.
>>>> 
>>>> Also it is not so difficult to solve with non-linear optimization. I
>>>> was hoping to be able to use R for that purpose because then the
>>>> results could easily be checked with statistical tests.
>>>> 
>>>> So my question is not "how to solve" but "how to solve with R".
>>>> 
>>>> 
>>>> As for the excess degrees of freedom, in real observations there would
>>>> of course be added noise due to either random variations or factors
>>>> not included in the model. So to generate a more reality-conforming
>>>> example I could add some random normal-distributed noise to the
>>>> dependent variable y. I previously left that part out because to me it
>>>> did not seem relevant.
>>>> 
>>>> 
>>>> Would you like me to make a complete example dataset with more records
>>>> and noise ?
>>>> 
>>>> 
>>>> The answer I look for would be the numerical values of the
>>>> factor-levels and numerical values for the multiplier (f) and the
>>>> offset (o), with p1 and p2 given as names (here: persons) and y given
>>>> as some level of achievement they reach by cooperating.
>>>> 
>>>> y = f * ( o - ( p1 - p2 )^2 )
>>>> 
>>>> Is that what you meant by "answer" ?
>>>> 
>>>> 
>>>> THX
>>>> stefan
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 2016-07-10 2:27 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>>>>> 
>>>>> I have seen less sensical questions.
>>>>> 
>>>>> It would be nice if the example were a bit more complete (as in it
>>>> should have excess degrees of freedom and an answer) and less like a
>>>> homework problem (which are off topic here). It would of course also be
>>>> helpful if the OP were to conform to the Posting Guide, particularly in
>>>> respect to using plain text email.
>>>>> 
>>>>> It looks like the kind of nonlinear optimization problem that
>>>> evolutionary algorithms are often applied to. It doesn't look (to me)
>>>> like a typical problem that factors get applied to in formulas though,
>>>> because multiple instances of the same factor variable are present.
>>>>> --
>>>>> Sent from my phone. Please excuse my brevity.
>>>>> 
>>>>> On July 9, 2016 4:59:30 PM PDT, Rolf Turner <r.turner at auckland.ac.nz>
>>>> wrote:
>>>>>> On 09/07/16 20:52, stn021 wrote:
>>>>>>> Hello,
>>>>>>> 
>>>>>>> I would like to analyse a model like this:
>>>>>>> 
>>>>>>> y = 1 *  ( 1 - ( x1 - x2 )  ^ 2   )
>>>>>>> 
>>>>>>> x1 and x2 are not continuous variables but factors, so the
>>>>>> observation
>>>>>>> contain the level.
>>>>>>> Its numerical value is unknown and is to be estimated with the
>>>> model.
>>>>>>> 
>>>>>>> 
>>>>>>> The observations look like this:
>>>>>>> 
>>>>>>> y        x1     x2
>>>>>>> 0.96  Alice  Bob
>>>>>>> 0.84  Alice  Charlie
>>>>>>> 0.96  Bob   Charlie
>>>>>>> 0.64  Dave Alice
>>>>>>> etc.
>>>>>>> 
>>>>>>> Each person has a numerical value. Here for example Alice = 0.2
>>>> and
>>>>>> Bob =
>>>>>>> 0.4
>>>>>>> 
>>>>>>> Then y = 0.96 = 1* ( 1- ( 0.2-0.4 ) ^ 2 ) , see first observation.
>>>>>>> 
>>>>>>> How can this be done in R ?
>>>>>> 
>>>>>> 
>>>>>> This question makes about as little sense as it is possible to
>>>> imagine.
>>>>>> 
>>>>>> cheers,
>>>>>> 
>>>>>> Rolf Turner
>>>>> 
>>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From cadeb at usgs.gov  Wed Jul 13 19:26:52 2016
From: cadeb at usgs.gov (Cade, Brian)
Date: Wed, 13 Jul 2016 11:26:52 -0600
Subject: [R] Linear model vs Mixed model
In-Reply-To: <CAJP88kSFxqrdQP4YsE4zPWuOzvJmY7BQS_a6+GB3hAfrguLJUw@mail.gmail.com>
References: <CAJP88kQcCgJ6vg5dVdxPfpMtcZpb72SpcPehA+PoEUWvPUrVRw@mail.gmail.com>
	<CAJuCY5wRmHOVGxK3kvUHwtAdL85jg0kyLo9GKMEfezdLaBijfQ@mail.gmail.com>
	<CAJP88kR3C8dgVq+4UyCzedpbsZ_rn4BguOa9+YPwCs5ZoN21aQ@mail.gmail.com>
	<CAM5M9BSfS-HO7hqU+5=vJ6c+cROG+kWGSfZq95+myA+0peGAAQ@mail.gmail.com>
	<CAJP88kSFxqrdQP4YsE4zPWuOzvJmY7BQS_a6+GB3hAfrguLJUw@mail.gmail.com>
Message-ID: <CAM5M9BRY1X6JutTYrEMSsxq4q8Ca5BJK43xST366Ow=KK+W7dg@mail.gmail.com>

Utkarsh:  I think the differences between the lm and lmer estimates of the
intercept are consistent with the regularization effect expected with
mixed-effects models where the estimates shrink towards the mean slightly.
I don't think there is any reason to expect exact agreement between the lm
and lmer estimates.  I didn't mean to imply that the different
parameterization of the contrasts would make the lm estimates agree more
with the lmer estimates, only that it might be easier to compare the
regression summary output to see how similar/dissimilar they were.  But
your way of comparing them are adequate.  Why would you think they should
agree exactly?  Larger sample sizes might make them closer (but I don't
know your n) but my expectation is that they could only be close not
exactly in agreement.  The lmer intercept estimates are based on using a
normal distribution with an estimated variance to locate the subject
specific intercepts.  Why do you think those modeled intercepts should
exactly coincide with your fixed effects intercepts that makes no such
distributional assumption?

Brian

Brian S. Cade, PhD

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  cadeb at usgs.gov <brian_cade at usgs.gov>
tel:  970 226-9326


On Wed, Jul 13, 2016 at 10:06 AM, Utkarsh Singhal <utkarsh.iit at gmail.com>
wrote:

> Hi Brian,
>
> This makes some sense to me theoretically, but doesn't pan out with my
> experiment.
>
> The contrasts default was the following as you said:
> > options("contrasts")
> $contrasts
>         unordered           ordered
> "contr.treatment"      "contr.poly"
>
> I changed it as follows:
> > options(contracts=c(unordered="contr.sum", ordered="contr.poly"))
>
> But the output of 'lm' model was exactly the same as before.
>
> Then I tried the following:
> > coef(lm(Reaction ~ Days + Subject, sleepstudy,
> contrasts=list(Subject="contr.sum")))
>
> The model output appears slightly different, but the 'Subject* +
> Intercept' is still exactly the same as the default 'lm' model.
>
> Also, as you can verify below, the predictions from two 'lm' models are
> exactly the  same
>
> > fit_lm = lm(Reaction ~ Days + Subject, sleepstudy);
> > fit_lm2 = lm(Reaction ~ Days + Subject, sleepstudy,
> contrasts=list(Subject="contr.sum"))
> >
> > head(predict(fit_lm))
>        1        2        3        4        5        6
> 295.0310 305.4983 315.9656 326.4329 336.9002 347.3675
>
> > head(predict(fit_lm2))
>        1        2        3        4        5        6
> 295.0310 305.4983 315.9656 326.4329 336.9002 347.3675
>
> And these are quite different from the mixedmodel:
>
> > fit_lmer = lmer(Reaction ~ Days + (1| Subject), sleepstudy)
> > head(predict(fit_lmer))
>        1        2        3        4        5        6
> 292.1888 302.6561 313.1234 323.5907 334.0580 344.5252
>
> Any idea?
>
>
>
> Utkarsh Singhal
> 91.96508.54333
>
>
> On 13 July 2016 at 00:18, Cade, Brian <cadeb at usgs.gov> wrote:
>
>> Your lm() estimates are using the default contrasts of contr.treatment,
>> providing an intercept corresponding to your subject 308 and the other
>> subject* estimates are differences from subject 308 intercept.  You could
>> have specified this with contrasts as contr.sum and the estimates would be
>> more easily compared to the lmer() model estimates.  They are close but
>> will never be identical as the lmer() model estimates are based on assuming
>> a normal distribution with specified variance.  They rarely would be
>> identical.
>>
>> Brian
>>
>> Brian S. Cade, PhD
>>
>> U. S. Geological Survey
>> Fort Collins Science Center
>> 2150 Centre Ave., Bldg. C
>> Fort Collins, CO  80526-8818
>>
>> email:  cadeb at usgs.gov <brian_cade at usgs.gov>
>> tel:  970 226-9326
>>
>>
>> On Tue, Jul 12, 2016 at 12:10 PM, Utkarsh Singhal <utkarsh.iit at gmail.com>
>> wrote:
>>
>>> Hello Thierry,
>>>
>>> Thank you for your quick response. Sorry, but I am not sure if I follow
>>> what you said. I get the following outputs from the two models:
>>> > coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
>>> Subject    (Intercept)     Days
>>> 308    292.1888 10.46729
>>> 309    173.5556 10.46729
>>> 310    188.2965 10.46729
>>> 330    255.8115 10.46729
>>> 331    261.6213 10.46729
>>> 332    259.6263 10.46729
>>> 333    267.9056 10.46729
>>> 334    248.4081 10.46729
>>> 335    206.1230 10.46729
>>> 337    323.5878 10.46729
>>> 349    230.2089 10.46729
>>> 350    265.5165 10.46729
>>> 351    243.5429 10.46729
>>> 352    287.7835 10.46729
>>> 369    258.4415 10.46729
>>> 370    245.0424 10.46729
>>> 371    248.1108 10.46729
>>> 372    269.5209 10.46729
>>>
>>> > coef(lm(Reaction ~ Days + Subject, sleepstudy))
>>> (Intercept)  295.03104
>>> Days          10.46729
>>> Subject309  -126.90085
>>> Subject310  -111.13256
>>> Subject330   -38.91241
>>> Subject331   -32.69778
>>> Subject332   -34.83176
>>> Subject333   -25.97552
>>> Subject334   -46.83178
>>> Subject335   -92.06379
>>> Subject337    33.58718
>>> Subject349   -66.29936
>>> Subject350   -28.53115
>>> Subject351   -52.03608
>>> Subject352    -4.71229
>>> Subject369   -36.09919
>>> Subject370   -50.43206
>>> Subject371   -47.14979
>>> Subject372   -24.24770
>>>
>>> Now, what I expected is the following:
>>>
>>>    - 'Intercept' of model-2 to match with Intercept of Subject-308 of
>>>    model-1
>>>    - 'Intercept+Subject309' of model-2 to match with Intercept of
>>>    Subject-309 of model-1
>>>    - and so on...
>>>
>>>
>>> What am I missing here?
>>>
>>> If it is difficult to explain this, can you alternately answer the
>>> following: "Is it possible to define the 'lm' and 'lmer' models above so
>>> they produce the same results (at least in terms of predictions)?"
>>>
>>> Thanks again.
>>>
>>> Utkarsh Singhal
>>> 91.96508.54333
>>>
>>>
>>> On 12 July 2016 at 19:15, Thierry Onkelinx <thierry.onkelinx at inbo.be>
>>> wrote:
>>>
>>> > The parametrisation is different.
>>> >
>>> > The intercept in model 1 is the effect of the "average" subject at
>>> days ==
>>> > 0.
>>> > The intercept in model 2 is the effect of the first subject at days ==
>>> 0.
>>> >
>>> > ir. Thierry Onkelinx
>>> > Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>>> and
>>> > Forest
>>> > team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>>> > Kliniekstraat 25
>>> > 1070 Anderlecht
>>> > Belgium
>>> >
>>> > To call in the statistician after the experiment is done may be no more
>>> > than asking him to perform a post-mortem examination: he may be able
>>> to say
>>> > what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> > The plural of anecdote is not data. ~ Roger Brinner
>>> > The combination of some data and an aching desire for an answer does
>>> not
>>> > ensure that a reasonable answer can be extracted from a given body of
>>> data.
>>> > ~ John Tukey
>>> >
>>> > 2016-07-12 15:35 GMT+02:00 Utkarsh Singhal <utkarsh.iit at gmail.com>:
>>> >
>>> >> Hi experts,
>>> >>
>>> >> While the slope is coming out to be identical in the two methods
>>> below,
>>> >> the
>>> >> intercepts are not. As far as I understand, both are formulations are
>>> >> identical in the sense that these are asking for a slope
>>> corresponding to
>>> >> 'Days' and a separate intercept term for each Subject.
>>> >>
>>> >> # Model-1
>>> >> library(lmer)
>>> >> coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
>>> >>
>>> >> # Model-2
>>> >> coef(lm(Reaction ~ Days + Subject, sleepstudy))
>>> >>
>>> >> Can somebody tell me the reason? Are the above formulations actually
>>> >> different or is it due to different optimization method used?
>>> >>
>>> >> Thank you.
>>> >>
>>> >> Utkarsh Singhal
>>> >> 91.96508.54333
>>> >>
>>> >>         [[alternative HTML version deleted]]
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide
>>> >> http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.
>>> >>
>>> >
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Wed Jul 13 20:09:35 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 13 Jul 2016 11:09:35 -0700 (PDT)
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <CANVHAvYi6JF41qGDG7xXWFZbOV5j2UOp=Z9j7PSfK+kTYbXRnQ@mail.gmail.com>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
	<CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
	<AF7E2508-9E20-428C-96D9-1182D27A27A6@dcn.davis.ca.us>
	<CANVHAvZi4SM2i1+UJL-jEHQvQuHe+Txqe=Y-Xv5+q5sCAwuD3w@mail.gmail.com>
	<A784205C-C35B-442B-B69D-5EACFD3FFFE3@comcast.net>
	<CANVHAvYi6JF41qGDG7xXWFZbOV5j2UOp=Z9j7PSfK+kTYbXRnQ@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1607131102400.62638@pedal.dcn.davis.ca.us>

The formula interface as used in lm and nls searches for separate 
coefficients for each variable.. it will take someone more clever than I 
to figure out how to get the formula interface to think of two variables 
as instances of one factor.

However, R can do nonlinear optimization just fine:

##------------
# as if read in using read.csv( fname, as.is=TRUE )
dta <- data.frame( y = observed_data$y
                  , p1 = as.character( observed_data$p1 )
                  , p2 = as.character( observed_data$p2 )
                  , stringsAsFactors = FALSE
                  )

lvls <- with( dta, unique( c( p1, p2 ) ) )
dta$p1f <- factor( dta$p1, levels = lvls )
dta$p2f <- factor( dta$p2, levels = lvls )

idxvmult <- length( lvls ) + 1L
idxvoffs <- length( lvls ) + 2L

# all values in a numeric vector
# x = c( valice, vbob, ..., vmult, voffs )
calcY <- function( x ) {
   vmult <- x[ idxvmult ]
   voffs <- x[ idxvoffs ]
   vp1 <- x[ dta$p1f ]
   vp2 <- x[ dta$p2f ]
   vmult * ( voffs - ( vp1 - vp2 )^2 )
}

optfcn <- function( x ) {
   sum( ( dta$y - calcY( x ) ) ^ 2 )
}

oresult <- optim( par = rep( 1, idxvoffs ), optfcn)

result <- list( multiplier = oresult$par[ idxvmult ]
               , offset = oresult$par[ idxvoffs ]
               , values = data.frame( lvls = lvls
                                    , values = oresult$par[ seq.int( 
length( lvls ) ) ] )
               )
result

#---------

I highly recommend reading the help page for optim and the CRAN Task View 
on optimization [1]

[1] https://cran.r-project.org/web/views/Optimization.html

On Wed, 13 Jul 2016, stn021 wrote:

>> Is this what is intended?
>>
>>> observed_data$p1ab <- persons$ability[ match(observed_data$p1, persons$name) ]
>>> observed_data$p2ab <- persons$ability[ match(observed_data$p2, persons$name) ]
>
>
> Hello David,
>
> thank you for your answer.
>
>
> The code in my previous post was intended as an answer to the question
> in an earlier post about example-data, quote:
>
>>> Would you like me to make a complete example dataset with more records and noise ?
>> Yes. And preferably do it with R code.
>
> I should have re-stated this connection in the post.
>
>
> The code generates a matrix 'observed_data' which is the data the
> experimenter would get during the experiment.
>
> This matrix is output in the last line. All other output is only meant
> to document the generation-process.
>
> So the only thing visible to the experimenter before analysis is
> exactly that matrix 'observed_data'  (usually in the form of some
> written documentation which is later entered into statistical
> software). Everything before that last line simulates those unknown
> parameters that the experiment is supposed to reveal.
>
> The unknown parameters are specifically
> - the matrix 'persons'
> - and the variable 'multiplyer'
>
> Both are supposed to be revealed by the analyis. p1ab and p2ab would
> therefore depend on the unknown parameters and could not be added to
> 'observed_data' before the analysis.
>
> Sorry again for omitting the back-reference.
>
>
> I would like to know:
>
> - how to get R to use p1 and p2 as levels of the same factor
> (=persons) instead of levels of two different factors.
>
> - how to get R to multiply the numerical levels of factors during the
> search for the solution. Factors cannot be multiplied before running
> lm() or some other package because before the analysis their numerical
> values are not known.
>
>
> THX, Stefan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From utkarsh.iit at gmail.com  Wed Jul 13 20:19:16 2016
From: utkarsh.iit at gmail.com (Utkarsh Singhal)
Date: Wed, 13 Jul 2016 23:49:16 +0530
Subject: [R] Linear model vs Mixed model
In-Reply-To: <CAM5M9BRY1X6JutTYrEMSsxq4q8Ca5BJK43xST366Ow=KK+W7dg@mail.gmail.com>
References: <CAJP88kQcCgJ6vg5dVdxPfpMtcZpb72SpcPehA+PoEUWvPUrVRw@mail.gmail.com>
	<CAJuCY5wRmHOVGxK3kvUHwtAdL85jg0kyLo9GKMEfezdLaBijfQ@mail.gmail.com>
	<CAJP88kR3C8dgVq+4UyCzedpbsZ_rn4BguOa9+YPwCs5ZoN21aQ@mail.gmail.com>
	<CAM5M9BSfS-HO7hqU+5=vJ6c+cROG+kWGSfZq95+myA+0peGAAQ@mail.gmail.com>
	<CAJP88kSFxqrdQP4YsE4zPWuOzvJmY7BQS_a6+GB3hAfrguLJUw@mail.gmail.com>
	<CAM5M9BRY1X6JutTYrEMSsxq4q8Ca5BJK43xST366Ow=KK+W7dg@mail.gmail.com>
Message-ID: <CAJP88kSHT70bZksFMG930JkwK6hKsWhMPSx7B2CQDhC4ZccF3w@mail.gmail.com>

Thanks Brian for all your kind help.

"didn't mean to imply that the different parameterization of the contrasts
would make the lm estimates agree more with the lmer estimates, only that
it might be easier to compare the regression summary output to see how
similar/dissimilar they were ".
Got it now, and yes it makes sense for easy comparison.

And yes, my 'n' is indeed small. I don't want to match them exactly, but
just wanted to confirm that the underlying principle in the two methods is
not entirely different and will go away with large data. In absence of
that, I don't know how to choose between the two models. This brings to my
real problem. Take the below two models for example:

lm(y ~ x1 + x2*Subject)
lmer(y ~ x1 + x2 + Subject + (x2|Subject))

With my limited knowledge, both of these should do the following:

   - fit linear model
   - to predict 'y'
   - by capturing the effect due to predictor variables x1, x2 and Subject
   - and an interaction effect of the combination of x2 and Subject

These can of course use different optimization methods or make different
distributional assumptions, but that effect should ideally go away with
large data. So, I am not too concerned about that.

I can sleep peacefully if you just say that the difference is due to
optimization method used or distributional assumptions :).

But if it is anything more too it, then how do I decide which of the above
two models to choose in what scenario.

Regards,


Utkarsh Singhal
91.96508.54333


On 13 July 2016 at 22:56, Cade, Brian <cadeb at usgs.gov> wrote:

> Utkarsh:  I think the differences between the lm and lmer estimates of the
> intercept are consistent with the regularization effect expected with
> mixed-effects models where the estimates shrink towards the mean slightly.
> I don't think there is any reason to expect exact agreement between the lm
> and lmer estimates.  I didn't mean to imply that the different
> parameterization of the contrasts would make the lm estimates agree more
> with the lmer estimates, only that it might be easier to compare the
> regression summary output to see how similar/dissimilar they were.  But
> your way of comparing them are adequate.  Why would you think they should
> agree exactly?  Larger sample sizes might make them closer (but I don't
> know your n) but my expectation is that they could only be close not
> exactly in agreement.  The lmer intercept estimates are based on using a
> normal distribution with an estimated variance to locate the subject
> specific intercepts.  Why do you think those modeled intercepts should
> exactly coincide with your fixed effects intercepts that makes no such
> distributional assumption?
>
> Brian
>
> Brian S. Cade, PhD
>
> U. S. Geological Survey
> Fort Collins Science Center
> 2150 Centre Ave., Bldg. C
> Fort Collins, CO  80526-8818
>
> email:  cadeb at usgs.gov <brian_cade at usgs.gov>
> tel:  970 226-9326
>
>
> On Wed, Jul 13, 2016 at 10:06 AM, Utkarsh Singhal <utkarsh.iit at gmail.com>
> wrote:
>
>> Hi Brian,
>>
>> This makes some sense to me theoretically, but doesn't pan out with my
>> experiment.
>>
>> The contrasts default was the following as you said:
>> > options("contrasts")
>> $contrasts
>>         unordered           ordered
>> "contr.treatment"      "contr.poly"
>>
>> I changed it as follows:
>> > options(contracts=c(unordered="contr.sum", ordered="contr.poly"))
>>
>> But the output of 'lm' model was exactly the same as before.
>>
>> Then I tried the following:
>> > coef(lm(Reaction ~ Days + Subject, sleepstudy,
>> contrasts=list(Subject="contr.sum")))
>>
>> The model output appears slightly different, but the 'Subject* +
>> Intercept' is still exactly the same as the default 'lm' model.
>>
>> Also, as you can verify below, the predictions from two 'lm' models are
>> exactly the  same
>>
>> > fit_lm = lm(Reaction ~ Days + Subject, sleepstudy);
>> > fit_lm2 = lm(Reaction ~ Days + Subject, sleepstudy,
>> contrasts=list(Subject="contr.sum"))
>> >
>> > head(predict(fit_lm))
>>        1        2        3        4        5        6
>> 295.0310 305.4983 315.9656 326.4329 336.9002 347.3675
>>
>> > head(predict(fit_lm2))
>>        1        2        3        4        5        6
>> 295.0310 305.4983 315.9656 326.4329 336.9002 347.3675
>>
>> And these are quite different from the mixedmodel:
>>
>> > fit_lmer = lmer(Reaction ~ Days + (1| Subject), sleepstudy)
>> > head(predict(fit_lmer))
>>        1        2        3        4        5        6
>> 292.1888 302.6561 313.1234 323.5907 334.0580 344.5252
>>
>> Any idea?
>>
>>
>>
>> Utkarsh Singhal
>> 91.96508.54333
>>
>>
>> On 13 July 2016 at 00:18, Cade, Brian <cadeb at usgs.gov> wrote:
>>
>>> Your lm() estimates are using the default contrasts of contr.treatment,
>>> providing an intercept corresponding to your subject 308 and the other
>>> subject* estimates are differences from subject 308 intercept.  You could
>>> have specified this with contrasts as contr.sum and the estimates would be
>>> more easily compared to the lmer() model estimates.  They are close but
>>> will never be identical as the lmer() model estimates are based on assuming
>>> a normal distribution with specified variance.  They rarely would be
>>> identical.
>>>
>>> Brian
>>>
>>> Brian S. Cade, PhD
>>>
>>> U. S. Geological Survey
>>> Fort Collins Science Center
>>> 2150 Centre Ave., Bldg. C
>>> Fort Collins, CO  80526-8818
>>>
>>> email:  cadeb at usgs.gov <brian_cade at usgs.gov>
>>> tel:  970 226-9326
>>>
>>>
>>> On Tue, Jul 12, 2016 at 12:10 PM, Utkarsh Singhal <utkarsh.iit at gmail.com
>>> > wrote:
>>>
>>>> Hello Thierry,
>>>>
>>>> Thank you for your quick response. Sorry, but I am not sure if I follow
>>>> what you said. I get the following outputs from the two models:
>>>> > coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
>>>> Subject    (Intercept)     Days
>>>> 308    292.1888 10.46729
>>>> 309    173.5556 10.46729
>>>> 310    188.2965 10.46729
>>>> 330    255.8115 10.46729
>>>> 331    261.6213 10.46729
>>>> 332    259.6263 10.46729
>>>> 333    267.9056 10.46729
>>>> 334    248.4081 10.46729
>>>> 335    206.1230 10.46729
>>>> 337    323.5878 10.46729
>>>> 349    230.2089 10.46729
>>>> 350    265.5165 10.46729
>>>> 351    243.5429 10.46729
>>>> 352    287.7835 10.46729
>>>> 369    258.4415 10.46729
>>>> 370    245.0424 10.46729
>>>> 371    248.1108 10.46729
>>>> 372    269.5209 10.46729
>>>>
>>>> > coef(lm(Reaction ~ Days + Subject, sleepstudy))
>>>> (Intercept)  295.03104
>>>> Days          10.46729
>>>> Subject309  -126.90085
>>>> Subject310  -111.13256
>>>> Subject330   -38.91241
>>>> Subject331   -32.69778
>>>> Subject332   -34.83176
>>>> Subject333   -25.97552
>>>> Subject334   -46.83178
>>>> Subject335   -92.06379
>>>> Subject337    33.58718
>>>> Subject349   -66.29936
>>>> Subject350   -28.53115
>>>> Subject351   -52.03608
>>>> Subject352    -4.71229
>>>> Subject369   -36.09919
>>>> Subject370   -50.43206
>>>> Subject371   -47.14979
>>>> Subject372   -24.24770
>>>>
>>>> Now, what I expected is the following:
>>>>
>>>>    - 'Intercept' of model-2 to match with Intercept of Subject-308 of
>>>>    model-1
>>>>    - 'Intercept+Subject309' of model-2 to match with Intercept of
>>>>    Subject-309 of model-1
>>>>    - and so on...
>>>>
>>>>
>>>> What am I missing here?
>>>>
>>>> If it is difficult to explain this, can you alternately answer the
>>>> following: "Is it possible to define the 'lm' and 'lmer' models above so
>>>> they produce the same results (at least in terms of predictions)?"
>>>>
>>>> Thanks again.
>>>>
>>>> Utkarsh Singhal
>>>> 91.96508.54333
>>>>
>>>>
>>>> On 12 July 2016 at 19:15, Thierry Onkelinx <thierry.onkelinx at inbo.be>
>>>> wrote:
>>>>
>>>> > The parametrisation is different.
>>>> >
>>>> > The intercept in model 1 is the effect of the "average" subject at
>>>> days ==
>>>> > 0.
>>>> > The intercept in model 2 is the effect of the first subject at days
>>>> == 0.
>>>> >
>>>> > ir. Thierry Onkelinx
>>>> > Instituut voor natuur- en bosonderzoek / Research Institute for
>>>> Nature and
>>>> > Forest
>>>> > team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>>>> > Kliniekstraat 25
>>>> > 1070 Anderlecht
>>>> > Belgium
>>>> >
>>>> > To call in the statistician after the experiment is done may be no
>>>> more
>>>> > than asking him to perform a post-mortem examination: he may be able
>>>> to say
>>>> > what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>>> > The plural of anecdote is not data. ~ Roger Brinner
>>>> > The combination of some data and an aching desire for an answer does
>>>> not
>>>> > ensure that a reasonable answer can be extracted from a given body of
>>>> data.
>>>> > ~ John Tukey
>>>> >
>>>> > 2016-07-12 15:35 GMT+02:00 Utkarsh Singhal <utkarsh.iit at gmail.com>:
>>>> >
>>>> >> Hi experts,
>>>> >>
>>>> >> While the slope is coming out to be identical in the two methods
>>>> below,
>>>> >> the
>>>> >> intercepts are not. As far as I understand, both are formulations are
>>>> >> identical in the sense that these are asking for a slope
>>>> corresponding to
>>>> >> 'Days' and a separate intercept term for each Subject.
>>>> >>
>>>> >> # Model-1
>>>> >> library(lmer)
>>>> >> coef(lmer(Reaction ~ Days + (1| Subject), sleepstudy))
>>>> >>
>>>> >> # Model-2
>>>> >> coef(lm(Reaction ~ Days + Subject, sleepstudy))
>>>> >>
>>>> >> Can somebody tell me the reason? Are the above formulations actually
>>>> >> different or is it due to different optimization method used?
>>>> >>
>>>> >> Thank you.
>>>> >>
>>>> >> Utkarsh Singhal
>>>> >> 91.96508.54333
>>>> >>
>>>> >>         [[alternative HTML version deleted]]
>>>> >>
>>>> >> ______________________________________________
>>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> >> PLEASE do read the posting guide
>>>> >> http://www.R-project.org/posting-guide.html
>>>> >> and provide commented, minimal, self-contained, reproducible code.
>>>> >>
>>>> >
>>>> >
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Thu Jul 14 00:40:55 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 14 Jul 2016 08:40:55 +1000
Subject: [R] About "SpatioTemporal" Package
In-Reply-To: <335777775.2317453.1468405789218.JavaMail.yahoo@mail.yahoo.com>
References: <335777775.2317453.1468405789218.JavaMail.yahoo.ref@mail.yahoo.com>
	<335777775.2317453.1468405789218.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+8X3fVwNoojGxAxy43QO2U+F3PDP-g=tmQBm2sdKKT4dnSmGw@mail.gmail.com>

Hi Elham,
It looks to me as though you have created the numeric variable "ID"
and then passed it to a function that expects it to be a character
variable. Try changing the line:

ID<-60101:60128

to:

ID<-paste("ID",60101:60128,sep="")

and see what happens.

Jim


On Wed, Jul 13, 2016 at 8:29 PM, Elham Daadmehr via R-help
<r-help at r-project.org> wrote:
>   Dear all,
>
>  I want to build the data with the structure of STdata (using the
>  function "createSTdata()"), I did as the following:
>
>  ####
>  datst=cbind(xy)
>  yy=cbind(y,xx)
>  ID=60101:60128
>  date=as.numeric(1:13)
>  y1=as.data.frame(t(cbind(yy)))
>  datst1=as.data.frame(cbind(ID,datst))
>  aa=as.Date(date,format = "%Y-%m-%d",origin = "2016-01-31")
>  rownames(y1)<- aa
>  colnames(y1)<- ID
>  colnames(datst1)<-c("ID","x","y")
>  ####
>
>  But I don't know why the function "createSTdata(y1, datst1)" give me
>  the following error:
>
>  ####
>  Error in stCheckClass(obs$ID, "character", name = "obs$ID") :
>    obs$ID  must belong to one of class(es)  character
>  In addition: Warning messages:
>  1: In createSTdata(y1, datst1) :
>    Unable to find column 'obs$obs', using 'obs[,1]
>  2: In createSTdata(y1, datst1) :
>    Unable to find column 'obs$date', using 'obs[,2]
>  3: In createSTdata(y1, datst1) :
>    Unable to find column 'obs$ID', using 'obs[,3]
>  ####
>  I checked the example data "mesa.data.raw", I found that my "ID" has
>  the same type (integer) as the one in this data.
>
>
>  Kindly thanks, in advanceElham
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Thu Jul 14 01:03:46 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 14 Jul 2016 09:03:46 +1000
Subject: [R] Lake Analyzer Help
In-Reply-To: <CAHFvYA4KCzjX0a-qjKiEWcchgU0_A6bWnpa9HHmXoeX7LBue-g@mail.gmail.com>
References: <CAHFvYA4KCzjX0a-qjKiEWcchgU0_A6bWnpa9HHmXoeX7LBue-g@mail.gmail.com>
Message-ID: <CA+8X3fWB2yP_fu5b7PhAm9vy+11OSR81xXxvuHxdB6y0S=thig@mail.gmail.com>

Hi Kyle,
First, see if you can identify which data are getting lost. This will
often reveal what is losing them if there is some common
characteristic.

If not, try to create some toy data (a puddle, not a lake) that will
produce the same problem. Then send an email with the toy data as
formatted by "dput" and the minimum R script that causes the error so
that we can run exactly what you ran by copying your example and
pasting it into our R sessions.

Jim


On Wed, Jul 13, 2016 at 11:50 PM, Kyle Wittmaier <wittmaierk at gmail.com> wrote:
> I am using LakeAnalyzer in Rstudio to produce heat maps and plots using
> data from constant monitoring buoys. I have a prewritten script that is
> functioning on a colleagues computer perfectly. I am using Rstudio 0.99.902
> and R 3.3.1. I have added four packages to the project (lattice,
> manipulate, matrix, and rLakeAnalyzer). I am using the exact same data
> files and process as my colleague. When I run the script no error messages
> appear and everything appears to work perfectly, except there are
> significant gaps in the data shown on the plots. Any ideas?
>
> Thank you
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wittmaierk at gmail.com  Wed Jul 13 18:43:10 2016
From: wittmaierk at gmail.com (Kyle Wittmaier)
Date: Wed, 13 Jul 2016 12:43:10 -0400
Subject: [R] Lake Analyzer Help
In-Reply-To: <CAJuCY5z=qwu8iC74FZ9YcvAs5_Bgx+B=QfW9nLPhYoh+txA16A@mail.gmail.com>
References: <CAHFvYA4KCzjX0a-qjKiEWcchgU0_A6bWnpa9HHmXoeX7LBue-g@mail.gmail.com>
	<CAJuCY5yDJ5P4p63gRnH2vUBhGPHbdEY62UCvFLGAKC+0DYY62Q@mail.gmail.com>
	<CAHFvYA575TxEVrp6+TBLA_b9jjDxE1VXMyPgFNQa1eVA8Voa4A@mail.gmail.com>
	<CAJuCY5z=qwu8iC74FZ9YcvAs5_Bgx+B=QfW9nLPhYoh+txA16A@mail.gmail.com>
Message-ID: <CAHFvYA7=Q8Pg6qGaabXpgXTdAYAQUkeRx2djWPecBqBRfROW2g@mail.gmail.com>

Below is the code that was used. The packages that were included are
rLakeAnalyzer, lattice, matrix, manipulate. The same script and data files
works perfectly on a colleagues computer. On my computer it runs fine with
no error messages however there gaps in the plots that it produces. Any
help that can be provided would be extremely appreciated. Thank you.





area = 12755360 #Area of lake in m^2

wnd.height = 2 #Height of lake anemometer

bathy = load.bathy("cbr_bth.txt")

head(bathy)

temp = load.ts("cbr_wtr.txt", tz = "EST")

wind = load.ts("cbr_wnd.txt", tz = "EST")

head(temp[,1:5])

head(wind)

temp.dly = aggregate(temp[,-1],by=list(as.Date(temp[,1])),FUN=mean)

names(temp.dly) = names(temp)



wind.dly = aggregate(wind[,-1],by=list(as.Date(wind[,1])),FUN=mean)

names(wind.dly) = names(wind)

temp.1 = as.numeric(temp.dly[temp.dly$datetime == "2014-07-07",-1])

wind.1 = as.numeric(wind.dly[wind.dly$datetime == "2014-07-07",-1])

depths = get.offsets(temp.dly)

depths

t.d = thermo.depth(temp.1, depths)

cat('Thermocline depth is:', t.d)

m.d = meta.depths(temp.1, depths)

cat('The top depth of the metalimnion is:', m.d[1])

cat('The bottom depth of the metalimnion is:', m.d[2])

meta.temp = layer.temperature(m.d[1],m.d[2],temp.1,

                  depths,bathy$areas,bathy$depths)

cat('The average temperature of the metalimnion is:', meta.temp)

dens = water.density(temp.1)

plot(temp.1, dens, xlab="Temp(deg C)", ylab="Density(kg/m^3)")

epi.dens = layer.density(0,m.d[1],temp.1,

                  depths,bathy$areas,bathy$depths)

hypo.dens = layer.density(m.d[2],max(depths),temp.1,

                  depths,bathy$areas,bathy$depths)

SS = schmidt.stability(temp.1, depths, bathy$areas,bathy$depths)

SS

uS = uStar(wind.1,wnd.height,epi.dens)

uS

LN = lake.number(bathy$areas, bathy$depths, uS, SS, m.d[1], m.d[2], hypo.dens)

LN

WN = wedderburn.number(hypo.dens - epi.dens, m.d[1],

                           uS, area, hypo.dens)

WN

ts.TD = ts.thermo.depth(temp.dly)

ts.MD = ts.meta.depths(temp.dly)



plot(ts.TD$datetime, -ts.TD$thermo.depth, type='l', ylab='Depth (m)',
xlab='Date')

lines(ts.MD$datetime, -ts.MD$top, col='red')

lines(ts.MD$datetime, -ts.MD$bottom, col='red')



ts.SS = ts.schmidt.stability(temp.dly, bathy)

ts.uS = ts.uStar(temp.dly, wind.dly, wnd.height, bathy)

ts.LN = ts.lake.number(temp.dly, wind.dly, wnd.height, bathy)

plot(ts.LN$datetime, ts.LN$lake.number, type='l', ylab='Lake Number',
xlab='Date')

meta.temp = ts.layer.temperature(temp.dly, ts.MD$top, ts.MD$bottom, bathy)

plot(meta.temp$datetime, meta.temp$layer.temp, type='l',

ylab='Volumetrically averaged lake temperature', xlab='Date')

# buoyancy.freq

buoyancy.freq(as.numeric(temp.dly[1,-1]), depths)



# ts.buoyancy.freq

N2 = ts.buoyancy.freq(temp.dly, seasonal=FALSE)

SN2 = ts.buoyancy.freq(temp.dly, seasonal=TRUE)



plot(N2, type='l', ylab='Buoyancy Frequency', xlab='Date')

lines(SN2, col='red')

ts.WN = ts.wedderburn.number(temp.dly, wind.dly, wnd.height, bathy, area)

plot(ts.WN$datetime, ts.WN$wedderburn.number, type='l',

     ylab='Wedderburn Number', xlab='Date')

schmidt.plot(temp.dly,bathy)

lake.number.plot(temp.dly,wind.dly,wnd.height,bathy)

wtr.plot.temp(temp.dly)

wtr.heat.map(temp.dly)

wtr.heatmap.layers(temp.dly)

wtr.lineseries(temp.dly, ylab = "Temperature C")



On Wed, Jul 13, 2016 at 10:44 AM, Thierry Onkelinx <thierry.onkelinx at inbo.be
> wrote:

> Please keep the mailing list in cc.
>
> See http://adv-r.had.co.nz/Reproducibility.html for some instructions.
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-07-13 16:38 GMT+02:00 Kyle Wittmaier <wittmaierk at gmail.com>:
>
>> would providing the script be the most efficient means to show an example?
>>
>> On Wed, Jul 13, 2016 at 10:34 AM, Thierry Onkelinx <
>> thierry.onkelinx at inbo.be> wrote:
>>
>>> This will be very hard to answer without a reproducible example.
>>>
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>>> and Forest
>>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>>> Kliniekstraat 25
>>> 1070 Anderlecht
>>> Belgium
>>>
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> 2016-07-13 15:50 GMT+02:00 Kyle Wittmaier <wittmaierk at gmail.com>:
>>>
>>>> I am using LakeAnalyzer in Rstudio to produce heat maps and plots using
>>>> data from constant monitoring buoys. I have a prewritten script that is
>>>> functioning on a colleagues computer perfectly. I am using Rstudio
>>>> 0.99.902
>>>> and R 3.3.1. I have added four packages to the project (lattice,
>>>> manipulate, matrix, and rLakeAnalyzer). I am using the exact same data
>>>> files and process as my colleague. When I run the script no error
>>>> messages
>>>> appear and everything appears to work perfectly, except there are
>>>> significant gaps in the data shown on the plots. Any ideas?
>>>>
>>>> Thank you
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From justinthong93 at gmail.com  Wed Jul 13 18:18:12 2016
From: justinthong93 at gmail.com (Justin Thong)
Date: Wed, 13 Jul 2016 17:18:12 +0100
Subject: [R] Reference for aov()
Message-ID: <CAEtAGepf2Ake07BzsGrak-J6=vTxTpkPL=V=825thBRkoUjnQA@mail.gmail.com>

Hi

*I have been looking for a reference to explain how R uses the aov
command(at a deeper level)*. More specifically, how R reads the formulae
and R computes the sums of squares. I am not interested in understanding
what the difference of Type 1,2,3 sum of squares are. I am more interested
in finding out about how R computes ~x1:x2:x3  or how R computes ~A:x1
emphasizing sequential nature of the way it computes, and models even more
complicated than this.

Yours sincerely,
Justin

*I check my email at 9AM and 4PM everyday*
*If you have an EMERGENCY, contact me at +447938674419(UK) or
+60125056192(Malaysia)*

	[[alternative HTML version deleted]]


From wsp at uwm.edu  Wed Jul 13 22:20:06 2016
From: wsp at uwm.edu (Walker Pedersen)
Date: Wed, 13 Jul 2016 15:20:06 -0500
Subject: [R] glmmLasso with interactions errors
Message-ID: <CAAb_rUmWQZVQqnABGaEdz3M9Dmh8xQvjEzhQ4c1K5v7_c++isA@mail.gmail.com>

Hi Everyone,

I am having trouble running glmmLasso.

An abbreviated version of my dataset is here:

https://drive.google.com/open?id=0B_LliPDGUoZbVVFQS2VOV3hGN3c

Activity is a measure of brain activity, Novelty and Valence are
categorical variables coding the type of stimulus used to elicit the
response, ROI is a categorical variable coding three regions of the
brain that we have sampled this activity from, and STAIt is a
continuous measure representing degree of a specific personality trait
of the subjects. Subject is an ID number for the individuals the data
was sampled from.

Before glmmLasso I am running:

KNov$Subject <- factor(KNov$Subject)

to ensure the subject ID is not treated as a continuous variable.

If I run:

glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
STAIt + as.factor(ROI)
+ as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov, lambda=10)
summary(glm1)

I don't get any warning messages, but the output contains b estimates
only, no SE or p-values.

If I try to include a 3-way interaction, such as:

glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
STAIt + as.factor(ROI)
+ as.factor(Novelty):as.factor(Valence):as.factor(ROI),
list(Subject=~1), data = Nov7T, lambda=10)
summary(glm2)

I get the warnings:

Warning messages:
1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
  data length is not a multiple of split variable
2: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length

And again, I do get parameter estimates, and no SE or p-values.

If I include my continuous variable in any interaction, such as:

glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
STAIt + as.factor(ROI)
+ as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
list(Subject=~1), data = Nov7T, lambda=10)
summary(glm3)

I get the error message:

Error in rep(control$index[i], length.fac) : invalid 'times' argument

and no output.

If anyone has an input as to (1) why I am not getting SE or p-values
in my outputs (2) the meaning of there warnings I get when I include a
3-way variable, and if they are something to worry about, how to fix
them and (3) how to fix the error message I get when I include my
continuous factor in an interatction, I would be very appreciative.

Thanks!

Walker


From e.daadmehr at yahoo.com  Thu Jul 14 05:01:38 2016
From: e.daadmehr at yahoo.com (Elham Daadmehr)
Date: Thu, 14 Jul 2016 03:01:38 +0000 (UTC)
Subject: [R] About "SpatioTemporal" Package
In-Reply-To: <CA+8X3fVwNoojGxAxy43QO2U+F3PDP-g=tmQBm2sdKKT4dnSmGw@mail.gmail.com>
References: <335777775.2317453.1468405789218.JavaMail.yahoo.ref@mail.yahoo.com>
	<335777775.2317453.1468405789218.JavaMail.yahoo@mail.yahoo.com>
	<CA+8X3fVwNoojGxAxy43QO2U+F3PDP-g=tmQBm2sdKKT4dnSmGw@mail.gmail.com>
Message-ID: <1287530644.2883119.1468465298562.JavaMail.yahoo@mail.yahoo.com>

Thank you for the reply, Jim.
It gave me the same error.
I checked the example data "mesa.data.raw" in this package, I found that the "ID" has
the same type as the one in this data but I don't know why the function "createSTdata(y1, datst1)" give me
the error.
Regards,
 

    On Thursday, July 14, 2016 3:10 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
 

 Hi Elham,
It looks to me as though you have created the numeric variable "ID"
and then passed it to a function that expects it to be a character
variable. Try changing the line:

ID<-60101:60128

to:

ID<-paste("ID",60101:60128,sep="")

and see what happens.

Jim


On Wed, Jul 13, 2016 at 8:29 PM, Elham Daadmehr via R-help
<r-help at r-project.org> wrote:
>? Dear all,
>
>? I want to build the data with the structure of STdata (using the
>? function "createSTdata()"), I did as the following:
>
>? ####
>? datst=cbind(xy)
>? yy=cbind(y,xx)
>? ID=60101:60128
>? date=as.numeric(1:13)
>? y1=as.data.frame(t(cbind(yy)))
>? datst1=as.data.frame(cbind(ID,datst))
>? aa=as.Date(date,format = "%Y-%m-%d",origin = "2016-01-31")
>? rownames(y1)<- aa
>? colnames(y1)<- ID
>? colnames(datst1)<-c("ID","x","y")
>? ####
>
>? But I don't know why the function "createSTdata(y1, datst1)" give me
>? the following error:
>
>? ####
>? Error in stCheckClass(obs$ID, "character", name = "obs$ID") :
>? ? obs$ID? must belong to one of class(es)? character
>? In addition: Warning messages:
>? 1: In createSTdata(y1, datst1) :
>? ? Unable to find column 'obs$obs', using 'obs[,1]
>? 2: In createSTdata(y1, datst1) :
>? ? Unable to find column 'obs$date', using 'obs[,2]
>? 3: In createSTdata(y1, datst1) :
>? ? Unable to find column 'obs$ID', using 'obs[,3]
>? ####
>? I checked the example data "mesa.data.raw", I found that my "ID" has
>? the same type (integer) as the one in this data.
>
>
>? Kindly thanks, in advanceElham
>
>
>? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


  
	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Thu Jul 14 06:01:47 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 14 Jul 2016 14:01:47 +1000
Subject: [R] About "SpatioTemporal" Package
In-Reply-To: <1287530644.2883119.1468465298562.JavaMail.yahoo@mail.yahoo.com>
References: <335777775.2317453.1468405789218.JavaMail.yahoo.ref@mail.yahoo.com>
	<335777775.2317453.1468405789218.JavaMail.yahoo@mail.yahoo.com>
	<CA+8X3fVwNoojGxAxy43QO2U+F3PDP-g=tmQBm2sdKKT4dnSmGw@mail.gmail.com>
	<1287530644.2883119.1468465298562.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+8X3fULtto4MG5AK=9R-n2oDe8D3dnCYNu0Zn4s=r9Thy7QSQ@mail.gmail.com>

Hi Elham,
The data might have been read in as a factor, which is neither
character nor numeric. In any case, it seems more likely to be a
problem with the "createSTdata" function, which I don't have.

Jim


On Thu, Jul 14, 2016 at 1:01 PM, Elham Daadmehr <e.daadmehr at yahoo.com> wrote:
> Thank you for the reply, Jim.
>
> It gave me the same error.
> I checked the example data "mesa.data.raw" in this package, I found that the
> "ID" has
> the same type as the one in this data but I don't know why the function
> "createSTdata(y1, datst1)" give me
> the error.
>
> Regards,
>
>
>
> On Thursday, July 14, 2016 3:10 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
>
>
> Hi Elham,
> It looks to me as though you have created the numeric variable "ID"
> and then passed it to a function that expects it to be a character
> variable. Try changing the line:
>
> ID<-60101:60128
>
> to:
>
> ID<-paste("ID",60101:60128,sep="")
>
> and see what happens.
>
> Jim
>
>
> On Wed, Jul 13, 2016 at 8:29 PM, Elham Daadmehr via R-help
> <r-help at r-project.org> wrote:
>>  Dear all,
>>
>>  I want to build the data with the structure of STdata (using the
>>  function "createSTdata()"), I did as the following:
>>
>>  ####
>>  datst=cbind(xy)
>>  yy=cbind(y,xx)
>>  ID=60101:60128
>>  date=as.numeric(1:13)
>>  y1=as.data.frame(t(cbind(yy)))
>>  datst1=as.data.frame(cbind(ID,datst))
>>  aa=as.Date(date,format = "%Y-%m-%d",origin = "2016-01-31")
>>  rownames(y1)<- aa
>>  colnames(y1)<- ID
>>  colnames(datst1)<-c("ID","x","y")
>>  ####
>>
>>  But I don't know why the function "createSTdata(y1, datst1)" give me
>>  the following error:
>>
>>  ####
>>  Error in stCheckClass(obs$ID, "character", name = "obs$ID") :
>>    obs$ID  must belong to one of class(es)  character
>>  In addition: Warning messages:
>>  1: In createSTdata(y1, datst1) :
>>    Unable to find column 'obs$obs', using 'obs[,1]
>>  2: In createSTdata(y1, datst1) :
>>    Unable to find column 'obs$date', using 'obs[,2]
>>  3: In createSTdata(y1, datst1) :
>>    Unable to find column 'obs$ID', using 'obs[,3]
>>  ####
>>  I checked the example data "mesa.data.raw", I found that my "ID" has
>>  the same type (integer) as the one in this data.
>>
>>
>>  Kindly thanks, in advanceElham
>
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From farzana.akbari2013 at gmail.com  Thu Jul 14 07:07:55 2016
From: farzana.akbari2013 at gmail.com (farzana akbari)
Date: Wed, 13 Jul 2016 19:07:55 -1000
Subject: [R] unit root test
Message-ID: <CAL3rq9icYzcOp+hyLyHjCbxRw2_ekHme0M5LWJ-o20zVnKQ6pg@mail.gmail.com>

in thr name of God

Hi Dear

i try to test unit root of eff1 in my panel data = data1 but this is the
eroor what should i do ?


purtest(eff1~1,data=data1,index="indu",pmax=4,test="levinlin")


Error in purtest(eff1 ~ 1, data = data1, index = "indu", pmax = 4, test =
"levinlin") :
  the individual dimension is undefined

	[[alternative HTML version deleted]]


From farzana.akbari2013 at gmail.com  Thu Jul 14 07:10:02 2016
From: farzana.akbari2013 at gmail.com (farzana akbari)
Date: Wed, 13 Jul 2016 19:10:02 -1000
Subject: [R] eroor in pglm in regression analyze
Message-ID: <CAL3rq9gnSO2LOEJnew_RdL9oLM2UHp0GNH5CYJbeGA67+diFOw@mail.gmail.com>

*?In the name of God?*

*Hi dear*



I try to analyze a regression model but can?t. I have 2 problem in this
presses as below:

And also I send you details in attached file.

Best Regards



One of the is:

> phtest(fixed,random)

Error in solve.default(dvcov) :

  system is computationally singular: reciprocal condition number =
1.34013e-25



and another one is :



>
fixedglm=pglm(RiskTaking~HHI+GDPGROWTH+EBIT+MB+DEBT+bonus+FATA+MKTSIZE+factor

+ (indu),data=data1,index=c("code","year"),model="within",family="gaussian")

> summary(fixedglm)

--------------------------------------------

Maximum Likelihood estimation

Newton-Raphson maximisation, 150 iterations

Return code 4: Iteration limit exceeded.

Log-Likelihood: -2040.749

13  free parameters

Estimates:

                Estimate Std. error t value Pr(> t)

(Intercept)    7.234e+04        Inf       0       1

HHI           -5.956e+05        Inf       0       1

GDPGROWTH     -4.833e-04        Inf       0       1

EBIT           4.414e-01        Inf       0       1

MB            -3.507e-04        Inf       0       1

DEBT          -5.451e+08        Inf       0       1

bonus          5.178e+00        Inf       0       1

FATA          -2.642e+04        Inf       0       1

MKTSIZE        6.339e-06        Inf       0       1

factor(indu)2 -3.722e+04        Inf       0       1

factor(indu)3  1.214e+05        Inf       0       1

sd.mu          1.997e+05        Inf       0       1

sd.eps         8.907e+04        Inf       0       1

	[[alternative HTML version deleted]]


From timo at timogrossenbacher.ch  Thu Jul 14 08:40:37 2016
From: timo at timogrossenbacher.ch (timo at timogrossenbacher.ch)
Date: Thu, 14 Jul 2016 08:40:37 +0200 (CEST)
Subject: [R] Forking and adapting an R package
In-Reply-To: <CAN5YmCFmjTefqB1rs-sZS6Ls7mxrD6ayiy3FkjeptP-sR+OY3w@mail.gmail.com>
References: <1880276709.61511.1468322926354.JavaMail.open-xchange@ox11.mail.hostpoint.internal>
	<CAN5YmCFmjTefqB1rs-sZS6Ls7mxrD6ayiy3FkjeptP-sR+OY3w@mail.gmail.com>
Message-ID: <1193303305.78628.1468478438223.JavaMail.open-xchange@ox11.mail.hostpoint.internal>

On Jul 13, 2016 15:02, Adams, Jean wrote:
>
> Timo,
>
> A couple of thoughts ...
>
> First, could you achieve what you want by simply modifying (your own copies
> of) a function or two from the hexbin package, without having to modify the
> entire package?  This might give you fewer tripping points.
>

Good point, how would I do this? Just copy-paste the original source and
redeclare/overwrite the functions within my use case script?

> Second, right after you forked the package (so that you have a copy on your
> own space on GitHub) and before you modified anything ... were you able to
> install and use the package from there successfully?
>>
>> library("devtools")
>> devtools::install_github("grssnbchr/hexbin")
>> library(hexbin)
>

I basically forked and cloned it, and then ran devtools::check() without
modifying anything, and it threw an error when compiling the vignette. Also
building fails this way. It seems to work fine when installed with
devtools::install_github(). Weird, I know.

Anyhow, the question with (modifying and) debugging a (forked) package in order
to understand how it works still stands, maybe there's something in Hadley's
book that I oversaw.

Timo

>
> Jean
>
> On Tue, Jul 12, 2016 at 6:28 AM, <timo at timogrossenbacher.ch> wrote:
>>
>> Hello.
>>
>> I'm trying to adapt the package ?hexbin? to suit my needs. This is the first
>> time I do this. I've read a bit through Hadley's ?R packages?, but now I'm
>> pretty lost (from a workflow point of view). I am using RStudio and Hadley's
>> devtools.
>>
>> So I forked the repo I want to adapt: https://github.com/grssnbchr/hexbin
>>  and
>> cloned it using RStudio (I created a new project). What I basically want to
>> do
>> is adapt the package slightly and use the adapted source on my use case (an
>> Rmd
>> file in another location) - ideally, I would call the respective function
>> (hexbin::grid.hexagons) in the Rmd and the source code of ?hexbin? would be
>> called and debugged (just for understanding what the package ?hexbin?
>> actually
>> does in that case, I do not have to build it yet, or even publish it). What
>> is
>> the workflow for this?
>>
>> Also, I tried running devtools::check() and it already fails there:
>> R version 3.2.5 (2016-04-14) -- "Very, Very Secure Dishes"
>>
>> Platform: x86_64-pc-linux-gnu (64-bit)
>>
>> > devtools::check()
>> Updating hexbin documentation
>> Loading hexbin
>> Creating a generic function for ?plot? from package ?graphics? in package
>> ?hexbin?
>> Creating a generic function for ?summary? from package ?base? in package
>> ?hexbin?
>> Setting env vars
>> -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>> CFLAGS : -Wall -pedantic
>> CXXFLAGS: -Wall -pedantic
>> Building hexbin
>> ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>> '/usr/lib/R/bin/R' --no-site-file --no-environ --no-save --no-restore --quiet
>> CMD build '/home/tgrossen/R/hexbin' --no-resave-data --no-manual
>>
>> * checking for file ?/home/tgrossen/R/hexbin/DESCRIPTION? ... OK
>> * preparing ?hexbin?:
>> * checking DESCRIPTION meta-information ... OK
>> * cleaning src
>> * installing the package to build vignettes
>> * creating vignettes ... ERROR
>>
>> Error: processing vignette 'hexagon_binning.Rnw' failed with diagnostics:
>>  chunk 1 (label = comphexsq)
>> Error in eval(expr, envir, enclos) : could not find function ?hexbin?
>> Execution halted
>> Error: Command failed (1)
>>
>> As you can see, I am very much lost. I googled for "adapt R package and
>> debug"
>> and so forth but couldn't find any tutorial or anything.
>>
>> Thanks,
>>
>> Timo
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From adriens_cachan at yahoo.fr  Thu Jul 14 10:33:45 2016
From: adriens_cachan at yahoo.fr (BONACHE Adrien)
Date: Thu, 14 Jul 2016 08:33:45 +0000 (UTC)
Subject: [R] Forking and adapting an R package
In-Reply-To: <1193303305.78628.1468478438223.JavaMail.open-xchange@ox11.mail.hostpoint.internal>
References: <1880276709.61511.1468322926354.JavaMail.open-xchange@ox11.mail.hostpoint.internal>
	<CAN5YmCFmjTefqB1rs-sZS6Ls7mxrD6ayiy3FkjeptP-sR+OY3w@mail.gmail.com>
	<1193303305.78628.1468478438223.JavaMail.open-xchange@ox11.mail.hostpoint.internal>
Message-ID: <657736091.4439865.1468485225356.JavaMail.yahoo@mail.yahoo.com>

Hi Timo,
To perform the first point, you just have to write the function name in R without using parentheses and arguments after it. You will see the code of the function. Copy and Paste it on notepad, change the name of the function in the notepad, then change what you want to change in your function. After it copy and paste the function in R. Use it!
Best wishes with your code!
Adrien.

      De?: "timo at timogrossenbacher.ch" <timo at timogrossenbacher.ch>
 ??: "Adams, Jean" <jvadams at usgs.gov>; r-help <r-help at r-project.org> 
 Envoy? le : Jeudi 14 juillet 2016 8h40
 Objet?: Re: [R] Forking and adapting an R package
   
On Jul 13, 2016 15:02, Adams, Jean wrote:
>
> Timo,
>
> A couple of thoughts ...
>
> First, could you achieve what you want by simply modifying (your own copies
> of) a function or two from the hexbin package, without having to modify the
> entire package?? This might give you fewer tripping points.
>

Good point, how would I do this? Just copy-paste the original source and
redeclare/overwrite the functions within my use case script?

> Second, right after you forked the package (so that you have a copy on your
> own space on GitHub) and before you modified anything ... were you able to
> install and use the package from there successfully?
>>
>> library("devtools")
>> devtools::install_github("grssnbchr/hexbin")
>> library(hexbin)
>

I basically forked and cloned it, and then ran devtools::check() without
modifying anything, and it threw an error when compiling the vignette. Also
building fails this way. It seems to work fine when installed with
devtools::install_github(). Weird, I know.

Anyhow, the question with (modifying and) debugging a (forked) package in order
to understand how it works still stands, maybe there's something in Hadley's
book that I oversaw.

Timo

>
> Jean
>
> On Tue, Jul 12, 2016 at 6:28 AM, <timo at timogrossenbacher.ch> wrote:
>>
>> Hello.
>>
>> I'm trying to adapt the package ?hexbin? to suit my needs. This is the first
>> time I do this. I've read a bit through Hadley's ?R packages?, but now I'm
>> pretty lost (from a workflow point of view). I am using RStudio and Hadley's
>> devtools.
>>
>> So I forked the repo I want to adapt: https://github.com/grssnbchr/hexbin
>>? and
>> cloned it using RStudio (I created a new project). What I basically want to
>> do
>> is adapt the package slightly and use the adapted source on my use case (an
>> Rmd
>> file in another location) - ideally, I would call the respective function
>> (hexbin::grid.hexagons) in the Rmd and the source code of ?hexbin? would be
>> called and debugged (just for understanding what the package ?hexbin?
>> actually
>> does in that case, I do not have to build it yet, or even publish it). What
>> is
>> the workflow for this?
>>
>> Also, I tried running devtools::check() and it already fails there:
>> R version 3.2.5 (2016-04-14) -- "Very, Very Secure Dishes"
>>
>> Platform: x86_64-pc-linux-gnu (64-bit)
>>
>> > devtools::check()
>> Updating hexbin documentation
>> Loading hexbin
>> Creating a generic function for ?plot? from package ?graphics? in package
>> ?hexbin?
>> Creating a generic function for ?summary? from package ?base? in package
>> ?hexbin?
>> Setting env vars
>> -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>> CFLAGS : -Wall -pedantic
>> CXXFLAGS: -Wall -pedantic
>> Building hexbin
>> ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>> '/usr/lib/R/bin/R' --no-site-file --no-environ --no-save --no-restore --quiet
>> CMD build '/home/tgrossen/R/hexbin' --no-resave-data --no-manual
>>
>> * checking for file ?/home/tgrossen/R/hexbin/DESCRIPTION? ... OK
>> * preparing ?hexbin?:
>> * checking DESCRIPTION meta-information ... OK
>> * cleaning src
>> * installing the package to build vignettes
>> * creating vignettes ... ERROR
>>
>> Error: processing vignette 'hexagon_binning.Rnw' failed with diagnostics:
>>? chunk 1 (label = comphexsq)
>> Error in eval(expr, envir, enclos) : could not find function ?hexbin?
>> Execution halted
>> Error: Command failed (1)
>>
>> As you can see, I am very much lost. I googled for "adapt R package and
>> debug"
>> and so forth but couldn't find any tutorial or anything.
>>
>> Thanks,
>>
>> Timo
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

  
	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Thu Jul 14 10:50:42 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 14 Jul 2016 10:50:42 +0200
Subject: [R] Reference for aov()
In-Reply-To: <CAEtAGepf2Ake07BzsGrak-J6=vTxTpkPL=V=825thBRkoUjnQA@mail.gmail.com>
References: <CAEtAGepf2Ake07BzsGrak-J6=vTxTpkPL=V=825thBRkoUjnQA@mail.gmail.com>
Message-ID: <03376F5C-47DD-4259-8791-9110D857A53E@gmail.com>

I am not aware of a detailed documentation of this beyond the actual source code. 
However, the principles are fairly straightforward, except that the rules for constructing the design matrix from the model formula can be a bit arcane at times.

The two main tools are the design matrix constructor (model.matrix) and a Gram-Schmidt type ortogonalization of its columns (the latter is called a QR decomposition in R, which it is, but there are several algorithms for QR, and the linear models codes depend on the QR algorithm being based on orthogonalization - so LINPACK works and LAPACK doesn't).

An ortogonalized model matrix generates a decomposition of the model space into orthogonal subspaces corresponding to the terms of the model. Projections onto each of the subspaces are easily worked out.  E.g., for a two-way analysis (Y~row+col+row:col) you can decompose the model effect as a row effect, a column effect, and an interaction effect. This allows straightforward calculation of the sums of squares of the ANOVA table. As you are probably well aware, if the design is unbalanced, the results will depend on the order of terms -- col + row + col:row gives a different result.

What aov() does is that it first decomposes the observations according to the Error() term, forming the error strata, then fits the systematic part of the model to each stratum in turn. In the nice cases, each term of the model will be estimable in exactly one stratum, and part of the aov() logic is to detect and remove unestimable terms. E.g., if you have a balanced two way layout, say individual x treatment, the variable gender is a subfactor of individual, so Y ~ gender * treatment + Error(individual/treatment), the gender effect is estimated in the individual stratum, whereas treatment and gender:treatment are estimated in the individual:treatment stratum.

It should be noted that it is very hard to interpret the results of aov() unless the Error() part of the model corresponds to a balanced experimental design. Or put more sharply: The model implied by the decomposition into error strata becomes nonsensical otherwise. If you do have a balanced design, the error strata reduce to simple combinations of means and observation, so the aov() algorithm is quite inefficient, but to my knowledge nobody has bothered to try and do better.

-pd

> On 13 Jul 2016, at 18:18 , Justin Thong <justinthong93 at gmail.com> wrote:
> 
> Hi
> 
> *I have been looking for a reference to explain how R uses the aov
> command(at a deeper level)*. More specifically, how R reads the formulae
> and R computes the sums of squares. I am not interested in understanding
> what the difference of Type 1,2,3 sum of squares are. I am more interested
> in finding out about how R computes ~x1:x2:x3  or how R computes ~A:x1
> emphasizing sequential nature of the way it computes, and models even more
> complicated than this.
> 
> Yours sincerely,
> Justin
> 
> *I check my email at 9AM and 4PM everyday*
> *If you have an EMERGENCY, contact me at +447938674419(UK) or
> +60125056192(Malaysia)*
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Gaurang.Mehta at royallondon.com  Thu Jul 14 14:44:52 2016
From: Gaurang.Mehta at royallondon.com (Mehta, Gaurang)
Date: Thu, 14 Jul 2016 13:44:52 +0100
Subject: [R] Selecting 1st and last dates from a set of dates
Message-ID: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>

Hi Team,
I am struggling to select the first date and last date of the month where there is data in a dataframe/matrix.
Is there any r function that can help to easily select data on the first and last day of the month from the given set of dates and values.
Thanks for the help in advance.
Regards,
Gaurang


This email is intended for the person or company named and access by anyone else is unauthorised. If you are not the person or company named, please delete this email and notify the sender.

The information in this email, including any attachments, may be confidential or legally privileged (meaning that its disclosure is protected in law). Its unauthorised disclosure, copying, distribution or use is prohibited and may be unlawful.

Email communications sent over the internet are not guaranteed to be secure or virus-free and such messages are potentially at risk.  The Royal London Group accepts no liability for any claims arising from use of the internet to transmit messages by or to any company within the Royal London Group.

The Royal London Group consists of The Royal London Mutual Insurance Society Limited and its subsidiaries.

The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority and provides life assurance and pensions.

Registered in England and Wales number 99064.

Registered office: 55 Gracechurch Street, London, EC3V 0RL.

In the Republic of Ireland: The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority in the UK and is regulated by the Central Bank of Ireland for conduct of business rules.


	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Thu Jul 14 16:31:49 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 14 Jul 2016 07:31:49 -0700
Subject: [R] Forking and adapting an R package
In-Reply-To: <657736091.4439865.1468485225356.JavaMail.yahoo@mail.yahoo.com>
References: <1880276709.61511.1468322926354.JavaMail.open-xchange@ox11.mail.hostpoint.internal>
	<CAN5YmCFmjTefqB1rs-sZS6Ls7mxrD6ayiy3FkjeptP-sR+OY3w@mail.gmail.com>
	<1193303305.78628.1468478438223.JavaMail.open-xchange@ox11.mail.hostpoint.internal>
	<657736091.4439865.1468485225356.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <85AE1C7F-E3BC-4BB3-AE3F-2C168BD0C182@dcn.davis.ca.us>

1) Although it can be as easy as this, when you are dealing with packages the complications of namespaces may prevent your modified version of the function now in the global namespace from being used by other functions in the package. That is, you may have to redefine all of the interrelated functions in the global namespace to get the desired behaviour. I do agree that for self study or a one-off task working in the global environment can serve just fine, but reading "Advanced R" by Hadley Wickham may help you understand some of these mysteries if you have multiple functions working together. 

2) I seem to recall that compiling without vignettes is typical for development. Not that failing to compile with vignettes is okay (there could be a bug in the package or even in R), but you might get further down the road if you disable vignettes for now. Read the Writing R Extensions document and/or your the documentation for your development environment (RStudio).

3) Reading the Posting Guide would have informed the OP that there is an R-package-dev mailing list where this topic might be better raised now (though it used to belong in R-help).
-- 
Sent from my phone. Please excuse my brevity.

On July 14, 2016 1:33:45 AM PDT, BONACHE Adrien via R-help <r-help at r-project.org> wrote:
>Hi Timo,
>To perform the first point, you just have to write the function name in
>R without using parentheses and arguments after it. You will see the
>code of the function. Copy and Paste it on notepad, change the name of
>the function in the notepad, then change what you want to change in
>your function. After it copy and paste the function in R. Use it!
>Best wishes with your code!
>Adrien.
>
>      De?: "timo at timogrossenbacher.ch" <timo at timogrossenbacher.ch>
> ??: "Adams, Jean" <jvadams at usgs.gov>; r-help <r-help at r-project.org> 
> Envoy? le : Jeudi 14 juillet 2016 8h40
> Objet?: Re: [R] Forking and adapting an R package
>   
>On Jul 13, 2016 15:02, Adams, Jean wrote:
>>
>> Timo,
>>
>> A couple of thoughts ...
>>
>> First, could you achieve what you want by simply modifying (your own
>copies
>> of) a function or two from the hexbin package, without having to
>modify the
>> entire package?? This might give you fewer tripping points.
>>
>
>Good point, how would I do this? Just copy-paste the original source
>and
>redeclare/overwrite the functions within my use case script?
>
>> Second, right after you forked the package (so that you have a copy
>on your
>> own space on GitHub) and before you modified anything ... were you
>able to
>> install and use the package from there successfully?
>>>
>>> library("devtools")
>>> devtools::install_github("grssnbchr/hexbin")
>>> library(hexbin)
>>
>
>I basically forked and cloned it, and then ran devtools::check()
>without
>modifying anything, and it threw an error when compiling the vignette.
>Also
>building fails this way. It seems to work fine when installed with
>devtools::install_github(). Weird, I know.
>
>Anyhow, the question with (modifying and) debugging a (forked) package
>in order
>to understand how it works still stands, maybe there's something in
>Hadley's
>book that I oversaw.
>
>Timo
>
>>
>> Jean
>>
>> On Tue, Jul 12, 2016 at 6:28 AM, <timo at timogrossenbacher.ch> wrote:
>>>
>>> Hello.
>>>
>>> I'm trying to adapt the package ?hexbin? to suit my needs. This is
>the first
>>> time I do this. I've read a bit through Hadley's ?R packages?, but
>now I'm
>>> pretty lost (from a workflow point of view). I am using RStudio and
>Hadley's
>>> devtools.
>>>
>>> So I forked the repo I want to adapt:
>https://github.com/grssnbchr/hexbin
>>>? and
>>> cloned it using RStudio (I created a new project). What I basically
>want to
>>> do
>>> is adapt the package slightly and use the adapted source on my use
>case (an
>>> Rmd
>>> file in another location) - ideally, I would call the respective
>function
>>> (hexbin::grid.hexagons) in the Rmd and the source code of ?hexbin?
>would be
>>> called and debugged (just for understanding what the package
>?hexbin?
>>> actually
>>> does in that case, I do not have to build it yet, or even publish
>it). What
>>> is
>>> the workflow for this?
>>>
>>> Also, I tried running devtools::check() and it already fails there:
>>> R version 3.2.5 (2016-04-14) -- "Very, Very Secure Dishes"
>>>
>>> Platform: x86_64-pc-linux-gnu (64-bit)
>>>
>>> > devtools::check()
>>> Updating hexbin documentation
>>> Loading hexbin
>>> Creating a generic function for ?plot? from package ?graphics? in
>package
>>> ?hexbin?
>>> Creating a generic function for ?summary? from package ?base? in
>package
>>> ?hexbin?
>>> Setting env vars
>>>
>-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>> CFLAGS : -Wall -pedantic
>>> CXXFLAGS: -Wall -pedantic
>>> Building hexbin
>>>
>------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
>>> '/usr/lib/R/bin/R' --no-site-file --no-environ --no-save
>--no-restore --quiet
>>> CMD build '/home/tgrossen/R/hexbin' --no-resave-data --no-manual
>>>
>>> * checking for file ?/home/tgrossen/R/hexbin/DESCRIPTION? ... OK
>>> * preparing ?hexbin?:
>>> * checking DESCRIPTION meta-information ... OK
>>> * cleaning src
>>> * installing the package to build vignettes
>>> * creating vignettes ... ERROR
>>>
>>> Error: processing vignette 'hexagon_binning.Rnw' failed with
>diagnostics:
>>>? chunk 1 (label = comphexsq)
>>> Error in eval(expr, envir, enclos) : could not find function
>?hexbin?
>>> Execution halted
>>> Error: Command failed (1)
>>>
>>> As you can see, I am very much lost. I googled for "adapt R package
>and
>>> debug"
>>> and so forth but couldn't find any tutorial or anything.
>>>
>>> Thanks,
>>>
>>> Timo
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>  
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From mxkuhn at gmail.com  Thu Jul 14 16:51:46 2016
From: mxkuhn at gmail.com (Max Kuhn)
Date: Thu, 14 Jul 2016 10:51:46 -0400
Subject: [R] failure with merge
Message-ID: <CAJ9CoW=K98Mj=MfLsD6BPmd3zTKZxa-9Nn2TzLL8G0ShtOLgQg@mail.gmail.com>

I am merging two data frames:

tuneAcc <- structure(list(select = c(FALSE, TRUE), method =
structure(c(1L, 1L), .Label = "GCV.Cp", class = "factor"), RMSE =
c(29.2102056093962, 28.9743318817886), Rsquared =
c(0.0322612161559773, 0.0281713457306074), RMSESD = c(0.981573768028697,
0.791307778398384), RsquaredSD = c(0.0388188469162352,
0.0322578925071113)),
.Names = c("select", "method", "RMSE", "Rsquared", "RMSESD",
"RsquaredSD"),
class = "data.frame", row.names = 1:2)

finalTune <- structure(list(select = TRUE, method = structure(1L,
.Label = "GCV.Cp", class = "factor"), Selected = "*"), .Names =
c("select", "method", "Selected"), row.names = 2L, class = "data.frame")

using

   merge(x = tuneAcc, y = finalTune, all.x = TRUE)

The error is

  "Error in match.arg(method) : 'arg' must be NULL or a character vector"

This is R version 3.3.1 (2016-06-21), Platform: x86_64-apple-darwin13.4.0
(64-bit), Running under: OS X 10.11.5 (El Capitan).

<some digging>

These do not stop execution:

  merge(x = tuneAcc, y = finalTune)
  merge(x = tuneAcc, y = finalTune, all.x = TRUE, sort = FALSE)

The latter produces (what I consider to be) incorrect results.

Walking through the code, the original call with just `all.x = TRUE` fails
when sorting at the line:

  res <- res[if (all.x || all.y)
    do.call("order", x[, seq_len(l.b), drop = FALSE]) else
     sort.list(bx[m$xi]), , drop = FALSE]

Specifically, on the `do.call` bit. For these data:

  Browse[3]> x
  select method RMSE Rsquared RMSESD RsquaredSD
  2 TRUE GCV.Cp 28.97433 0.02817135 0.7913078 0.03225789
  1 FALSE GCV.Cp 29.21021 0.03226122 0.9815738 0.03881885


  Browse[3]> x[, seq_len(l.b), drop = FALSE]
  select method
  2 TRUE GCV.Cp
  1 FALSE GCV.Cp

and this line executes:

  Browse[3]> order(x[, seq_len(l.b), drop = FALSE])
  [1] 1 2 3 4

although nrow(x) = 2 so this is an issue.

Calling it this way stops execution:

Browse[3]> do.call("order", x[, seq_len(l.b), drop = FALSE])
Error in match.arg(method) : 'arg' must be NULL or a character vector

Thanks,

Max


From jdnewmil at dcn.davis.ca.us  Thu Jul 14 17:03:09 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 14 Jul 2016 08:03:09 -0700
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
Message-ID: <D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>

I suspect the answer to your question (is there a function...) is almost certainly yes, but your question is too vague to be sure.

1) Data frames and matrices are different in important ways... it is highly unlikely that matrices would be appropriate for date data. 

2) Do you mean "select records with earliest date in each month" or "select records whose day of month is 1"? If you need to work with time of day along with date then the solution will be different than if you are working with date only.

3) Have you converted your dates to Date or POSIXct or chron already? Which? 

4) There are a lot of useful functions in base R [1][2], as well as contributed packages such as chron and lubridate.

A reproducible example [3] is the standard way to communicate what problem you actually have.  In particular, including the output of dput for a representative sample of data is a key element of that example. 

[1] ?DateTimeClasses
[2] R News 4/1 p29
[3] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
-- 
Sent from my phone. Please excuse my brevity.

On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang" <Gaurang.Mehta at royallondon.com> wrote:
>Hi Team,
>I am struggling to select the first date and last date of the month
>where there is data in a dataframe/matrix.
>Is there any r function that can help to easily select data on the
>first and last day of the month from the given set of dates and values.
>Thanks for the help in advance.
>Regards,
>Gaurang
>
>
>This email is intended for the person or company named and access by
>anyone else is unauthorised. If you are not the person or company
>named, please delete this email and notify the sender.
>
>The information in this email, including any attachments, may be
>confidential or legally privileged (meaning that its disclosure is
>protected in law). Its unauthorised disclosure, copying, distribution
>or use is prohibited and may be unlawful.
>
>Email communications sent over the internet are not guaranteed to be
>secure or virus-free and such messages are potentially at risk.  The
>Royal London Group accepts no liability for any claims arising from use
>of the internet to transmit messages by or to any company within the
>Royal London Group.
>
>The Royal London Group consists of The Royal London Mutual Insurance
>Society Limited and its subsidiaries.
>
>The Royal London Mutual Insurance Society Limited is authorised by the
>Prudential Regulation Authority and regulated by the Financial Conduct
>Authority and the Prudential Regulation Authority and provides life
>assurance and pensions.
>
>Registered in England and Wales number 99064.
>
>Registered office: 55 Gracechurch Street, London, EC3V 0RL.
>
>In the Republic of Ireland: The Royal London Mutual Insurance Society
>Limited is authorised by the Prudential Regulation Authority in the UK
>and is regulated by the Central Bank of Ireland for conduct of business
>rules.
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From cadeb at usgs.gov  Thu Jul 14 17:08:34 2016
From: cadeb at usgs.gov (Cade, Brian)
Date: Thu, 14 Jul 2016 09:08:34 -0600
Subject: [R] glmmLasso with interactions errors
In-Reply-To: <CAAb_rUmWQZVQqnABGaEdz3M9Dmh8xQvjEzhQ4c1K5v7_c++isA@mail.gmail.com>
References: <CAAb_rUmWQZVQqnABGaEdz3M9Dmh8xQvjEzhQ4c1K5v7_c++isA@mail.gmail.com>
Message-ID: <CAM5M9BTwzukAvLeRXdiuBxa+xmkCnf4CyzOXE5w23h2bf8=yjg@mail.gmail.com>

It has never been obvious to me that the lasso approach can handle
interactions among predictor variables well at all.  I'ld be curious to see
what others think and what you learn.

Brian

Brian S. Cade, PhD

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  cadeb at usgs.gov <brian_cade at usgs.gov>
tel:  970 226-9326


On Wed, Jul 13, 2016 at 2:20 PM, Walker Pedersen <wsp at uwm.edu> wrote:

> Hi Everyone,
>
> I am having trouble running glmmLasso.
>
> An abbreviated version of my dataset is here:
>
> https://drive.google.com/open?id=0B_LliPDGUoZbVVFQS2VOV3hGN3c
>
> Activity is a measure of brain activity, Novelty and Valence are
> categorical variables coding the type of stimulus used to elicit the
> response, ROI is a categorical variable coding three regions of the
> brain that we have sampled this activity from, and STAIt is a
> continuous measure representing degree of a specific personality trait
> of the subjects. Subject is an ID number for the individuals the data
> was sampled from.
>
> Before glmmLasso I am running:
>
> KNov$Subject <- factor(KNov$Subject)
>
> to ensure the subject ID is not treated as a continuous variable.
>
> If I run:
>
> glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
> STAIt + as.factor(ROI)
> + as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov,
> lambda=10)
> summary(glm1)
>
> I don't get any warning messages, but the output contains b estimates
> only, no SE or p-values.
>
> If I try to include a 3-way interaction, such as:
>
> glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
> STAIt + as.factor(ROI)
> + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
> list(Subject=~1), data = Nov7T, lambda=10)
> summary(glm2)
>
> I get the warnings:
>
> Warning messages:
> 1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
>   data length is not a multiple of split variable
> 2: In lambda_vec * sqrt(block2) :
>   longer object length is not a multiple of shorter object length
>
> And again, I do get parameter estimates, and no SE or p-values.
>
> If I include my continuous variable in any interaction, such as:
>
> glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
> STAIt + as.factor(ROI)
> + as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
> list(Subject=~1), data = Nov7T, lambda=10)
> summary(glm3)
>
> I get the error message:
>
> Error in rep(control$index[i], length.fac) : invalid 'times' argument
>
> and no output.
>
> If anyone has an input as to (1) why I am not getting SE or p-values
> in my outputs (2) the meaning of there warnings I get when I include a
> 3-way variable, and if they are something to worry about, how to fix
> them and (3) how to fix the error message I get when I include my
> continuous factor in an interatction, I would be very appreciative.
>
> Thanks!
>
> Walker
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Jul 14 17:12:17 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 14 Jul 2016 08:12:17 -0700
Subject: [R] failure with merge
In-Reply-To: <CAJ9CoW=K98Mj=MfLsD6BPmd3zTKZxa-9Nn2TzLL8G0ShtOLgQg@mail.gmail.com>
References: <CAJ9CoW=K98Mj=MfLsD6BPmd3zTKZxa-9Nn2TzLL8G0ShtOLgQg@mail.gmail.com>
Message-ID: <CAF8bMcavvUhDLj-ewWoa9UgrS1=OUTdW0aLzrZT3ZcNzb87Y5A@mail.gmail.com>

It looks like a common problem when using do.call("order", dataFrame).
If dataFrame has a column whose name matches an argument to order
you will get this problem.  The solution is to use do.call("order",
unname(dataFrame)).
E.g.,
  > do.call("order", tuneAcc)
  Error in match.arg(method) : 'arg' must be NULL or a character vector
  > do.call("order", unname(tuneAcc))
  [1] 1 2

This is probably buried in the the code for merge().  You can work around
it by changing the name of the "method" column.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 14, 2016 at 7:51 AM, Max Kuhn <mxkuhn at gmail.com> wrote:

> I am merging two data frames:
>
> tuneAcc <- structure(list(select = c(FALSE, TRUE), method =
> structure(c(1L, 1L), .Label = "GCV.Cp", class = "factor"), RMSE =
> c(29.2102056093962, 28.9743318817886), Rsquared =
> c(0.0322612161559773, 0.0281713457306074), RMSESD = c(0.981573768028697,
> 0.791307778398384), RsquaredSD = c(0.0388188469162352,
> 0.0322578925071113)),
> .Names = c("select", "method", "RMSE", "Rsquared", "RMSESD",
> "RsquaredSD"),
> class = "data.frame", row.names = 1:2)
>
> finalTune <- structure(list(select = TRUE, method = structure(1L,
> .Label = "GCV.Cp", class = "factor"), Selected = "*"), .Names =
> c("select", "method", "Selected"), row.names = 2L, class = "data.frame")
>
> using
>
>    merge(x = tuneAcc, y = finalTune, all.x = TRUE)
>
> The error is
>
>   "Error in match.arg(method) : 'arg' must be NULL or a character vector"
>
> This is R version 3.3.1 (2016-06-21), Platform: x86_64-apple-darwin13.4.0
> (64-bit), Running under: OS X 10.11.5 (El Capitan).
>
> <some digging>
>
> These do not stop execution:
>
>   merge(x = tuneAcc, y = finalTune)
>   merge(x = tuneAcc, y = finalTune, all.x = TRUE, sort = FALSE)
>
> The latter produces (what I consider to be) incorrect results.
>
> Walking through the code, the original call with just `all.x = TRUE` fails
> when sorting at the line:
>
>   res <- res[if (all.x || all.y)
>     do.call("order", x[, seq_len(l.b), drop = FALSE]) else
>      sort.list(bx[m$xi]), , drop = FALSE]
>
> Specifically, on the `do.call` bit. For these data:
>
>   Browse[3]> x
>   select method RMSE Rsquared RMSESD RsquaredSD
>   2 TRUE GCV.Cp 28.97433 0.02817135 0.7913078 0.03225789
>   1 FALSE GCV.Cp 29.21021 0.03226122 0.9815738 0.03881885
>
>
>   Browse[3]> x[, seq_len(l.b), drop = FALSE]
>   select method
>   2 TRUE GCV.Cp
>   1 FALSE GCV.Cp
>
> and this line executes:
>
>   Browse[3]> order(x[, seq_len(l.b), drop = FALSE])
>   [1] 1 2 3 4
>
> although nrow(x) = 2 so this is an issue.
>
> Calling it this way stops execution:
>
> Browse[3]> do.call("order", x[, seq_len(l.b), drop = FALSE])
> Error in match.arg(method) : 'arg' must be NULL or a character vector
>
> Thanks,
>
> Max
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Gaurang.Mehta at royallondon.com  Thu Jul 14 17:26:16 2016
From: Gaurang.Mehta at royallondon.com (Mehta, Gaurang)
Date: Thu, 14 Jul 2016 16:26:16 +0100
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
	<D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
Message-ID: <3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>

Hi Jeff,
I would say my problem is what you described in 2 below. My data is as follows:
Date	UC11	UC12	UC13
02/01/1997	1	2	0
03/01/1997	5	6	3
06/01/1997	5	4	6
07/01/1997	6	4	3
08/01/1997	6	5	5
09/01/1997	7	6	8
10/01/1997	8	5	5
13/01/1997	8	6	5
14/01/1997	7	4	4
15/01/1997	6	3	3
16/01/1997	8	5	5
17/01/1997	6	4	3
20/01/1997	5	4	2
21/01/1997	7	5	5
22/01/1997	16	12	12
23/01/1997	5	3	4
24/01/1997	5	2	2
27/01/1997	8	4	5
28/01/1997	7	5	9
29/01/1997	4	4	4
30/01/1997	4	4	6
31/01/1997	9	7	8
03/02/1997	9	6	8


I want to select the data on the first date it can be 1st , 2nd or 3rd or any and last date it can be 31st, 30th and /or29th. I don?t need time.
It would be great if you could help.
Regards,
Gaurang Mehta



-----Original Message-----
From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us] 
Sent: 14 July 2016 16:03
To: Mehta, Gaurang; R-help Mailing List
Subject: Re: [R] Selecting 1st and last dates from a set of dates

I suspect the answer to your question (is there a function...) is almost certainly yes, but your question is too vague to be sure.

1) Data frames and matrices are different in important ways... it is highly unlikely that matrices would be appropriate for date data. 

2) Do you mean "select records with earliest date in each month" or "select records whose day of month is 1"? If you need to work with time of day along with date then the solution will be different than if you are working with date only.

3) Have you converted your dates to Date or POSIXct or chron already? Which? 

4) There are a lot of useful functions in base R [1][2], as well as contributed packages such as chron and lubridate.

A reproducible example [3] is the standard way to communicate what problem you actually have.  In particular, including the output of dput for a representative sample of data is a key element of that example. 

[1] ?DateTimeClasses
[2] R News 4/1 p29
[3] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
--
Sent from my phone. Please excuse my brevity.

On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang" <Gaurang.Mehta at royallondon.com> wrote:
>Hi Team,
>I am struggling to select the first date and last date of the month 
>where there is data in a dataframe/matrix.
>Is there any r function that can help to easily select data on the 
>first and last day of the month from the given set of dates and values.
>Thanks for the help in advance.
>Regards,
>Gaurang
>
>
>This email is intended for the person or company named and access by 
>anyone else is unauthorised. If you are not the person or company 
>named, please delete this email and notify the sender.
>
>The information in this email, including any attachments, may be 
>confidential or legally privileged (meaning that its disclosure is 
>protected in law). Its unauthorised disclosure, copying, distribution 
>or use is prohibited and may be unlawful.
>
>Email communications sent over the internet are not guaranteed to be 
>secure or virus-free and such messages are potentially at risk.  The 
>Royal London Group accepts no liability for any claims arising from use 
>of the internet to transmit messages by or to any company within the 
>Royal London Group.
>
>The Royal London Group consists of The Royal London Mutual Insurance 
>Society Limited and its subsidiaries.
>
>The Royal London Mutual Insurance Society Limited is authorised by the 
>Prudential Regulation Authority and regulated by the Financial Conduct 
>Authority and the Prudential Regulation Authority and provides life 
>assurance and pensions.
>
>Registered in England and Wales number 99064.
>
>Registered office: 55 Gracechurch Street, London, EC3V 0RL.
>
>In the Republic of Ireland: The Royal London Mutual Insurance Society 
>Limited is authorised by the Prudential Regulation Authority in the UK 
>and is regulated by the Central Bank of Ireland for conduct of business 
>rules.
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Thu Jul 14 17:41:55 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 14 Jul 2016 08:41:55 -0700
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
	<D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>
Message-ID: <CAF8bMcYcXQtprEjNJPzFS3HCKr-zckZ5nEKmnpUpB1HkpdS08w@mail.gmail.com>

Does the following example help?  isFirstInRun() and isLastInRun() are handy
utility functions.

> d <- transform(data.frame(Date=as.Date(c("2016-01-05", "2016-03-04",
"2016-03-30", "2015-12-02", "2016-03-04", "2015-12-21"))),
DaysSince1970=as.integer(Date), I=seq_along(Date))
> d
        Date DaysSince1970 I
1 2016-01-05         16805 1
2 2016-03-04         16864 2
3 2016-03-30         16890 3
4 2015-12-02         16771 4
5 2016-03-04         16864 5
6 2015-12-21         16790 6
> isFirstInRun <- function(x) c(TRUE, x[-1] != x[-length(x)])
> isLastInRun <- function(x) c(x[-1] != x[-length(x)], TRUE)
> ds <- d[order(d$Date),]
> ds
        Date DaysSince1970 I
4 2015-12-02         16771 4
6 2015-12-21         16790 6
1 2016-01-05         16805 1
2 2016-03-04         16864 2
5 2016-03-04         16864 5
3 2016-03-30         16890 3
> ds[isFirstInRun(format(ds$Date, "%Y-%m")),]
        Date DaysSince1970 I
4 2015-12-02         16771 4
1 2016-01-05         16805 1
2 2016-03-04         16864 2
> ds[isLastInRun(format(ds$Date, "%Y-%m")),]
        Date DaysSince1970 I
6 2015-12-21         16790 6
1 2016-01-05         16805 1
3 2016-03-30         16890 3


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 14, 2016 at 8:26 AM, Mehta, Gaurang <
Gaurang.Mehta at royallondon.com> wrote:

> Hi Jeff,
> I would say my problem is what you described in 2 below. My data is as
> follows:
> Date    UC11    UC12    UC13
> 02/01/1997      1       2       0
> 03/01/1997      5       6       3
> 06/01/1997      5       4       6
> 07/01/1997      6       4       3
> 08/01/1997      6       5       5
> 09/01/1997      7       6       8
> 10/01/1997      8       5       5
> 13/01/1997      8       6       5
> 14/01/1997      7       4       4
> 15/01/1997      6       3       3
> 16/01/1997      8       5       5
> 17/01/1997      6       4       3
> 20/01/1997      5       4       2
> 21/01/1997      7       5       5
> 22/01/1997      16      12      12
> 23/01/1997      5       3       4
> 24/01/1997      5       2       2
> 27/01/1997      8       4       5
> 28/01/1997      7       5       9
> 29/01/1997      4       4       4
> 30/01/1997      4       4       6
> 31/01/1997      9       7       8
> 03/02/1997      9       6       8
>
>
> I want to select the data on the first date it can be 1st , 2nd or 3rd or
> any and last date it can be 31st, 30th and /or29th. I don?t need time.
> It would be great if you could help.
> Regards,
> Gaurang Mehta
>
>
>
> -----Original Message-----
> From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us]
> Sent: 14 July 2016 16:03
> To: Mehta, Gaurang; R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
>
> I suspect the answer to your question (is there a function...) is almost
> certainly yes, but your question is too vague to be sure.
>
> 1) Data frames and matrices are different in important ways... it is
> highly unlikely that matrices would be appropriate for date data.
>
> 2) Do you mean "select records with earliest date in each month" or
> "select records whose day of month is 1"? If you need to work with time of
> day along with date then the solution will be different than if you are
> working with date only.
>
> 3) Have you converted your dates to Date or POSIXct or chron already?
> Which?
>
> 4) There are a lot of useful functions in base R [1][2], as well as
> contributed packages such as chron and lubridate.
>
> A reproducible example [3] is the standard way to communicate what problem
> you actually have.  In particular, including the output of dput for a
> representative sample of data is a key element of that example.
>
> [1] ?DateTimeClasses
> [2] R News 4/1 p29
> [3]
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang" <
> Gaurang.Mehta at royallondon.com> wrote:
> >Hi Team,
> >I am struggling to select the first date and last date of the month
> >where there is data in a dataframe/matrix.
> >Is there any r function that can help to easily select data on the
> >first and last day of the month from the given set of dates and values.
> >Thanks for the help in advance.
> >Regards,
> >Gaurang
> >
> >
> >This email is intended for the person or company named and access by
> >anyone else is unauthorised. If you are not the person or company
> >named, please delete this email and notify the sender.
> >
> >The information in this email, including any attachments, may be
> >confidential or legally privileged (meaning that its disclosure is
> >protected in law). Its unauthorised disclosure, copying, distribution
> >or use is prohibited and may be unlawful.
> >
> >Email communications sent over the internet are not guaranteed to be
> >secure or virus-free and such messages are potentially at risk.  The
> >Royal London Group accepts no liability for any claims arising from use
> >of the internet to transmit messages by or to any company within the
> >Royal London Group.
> >
> >The Royal London Group consists of The Royal London Mutual Insurance
> >Society Limited and its subsidiaries.
> >
> >The Royal London Mutual Insurance Society Limited is authorised by the
> >Prudential Regulation Authority and regulated by the Financial Conduct
> >Authority and the Prudential Regulation Authority and provides life
> >assurance and pensions.
> >
> >Registered in England and Wales number 99064.
> >
> >Registered office: 55 Gracechurch Street, London, EC3V 0RL.
> >
> >In the Republic of Ireland: The Royal London Mutual Insurance Society
> >Limited is authorised by the Prudential Regulation Authority in the UK
> >and is regulated by the Central Bank of Ireland for conduct of business
> >rules.
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Gaurang.Mehta at royallondon.com  Thu Jul 14 18:01:27 2016
From: Gaurang.Mehta at royallondon.com (Mehta, Gaurang)
Date: Thu, 14 Jul 2016 17:01:27 +0100
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <CAF8bMcYcXQtprEjNJPzFS3HCKr-zckZ5nEKmnpUpB1HkpdS08w@mail.gmail.com>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
	<D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>
	<CAF8bMcYcXQtprEjNJPzFS3HCKr-zckZ5nEKmnpUpB1HkpdS08w@mail.gmail.com>
Message-ID: <3BB657B92C75C74385A95FAA7C6B17161F0C60B668@VRTPRDEXM02.royallondongroup.com>

Thanks William.
This works. Thanks again.

From: William Dunlap [mailto:wdunlap at tibco.com]
Sent: 14 July 2016 16:42
To: Mehta, Gaurang
Cc: Jeff Newmiller; R-help Mailing List
Subject: Re: [R] Selecting 1st and last dates from a set of dates

Does the following example help?  isFirstInRun() and isLastInRun() are handy
utility functions.

> d <- transform(data.frame(Date=as.Date(c("2016-01-05", "2016-03-04", "2016-03-30", "2015-12-02", "2016-03-04", "2015-12-21"))), DaysSince1970=as.integer(Date), I=seq_along(Date))
> d
        Date DaysSince1970 I
1 2016-01-05         16805 1
2 2016-03-04         16864 2
3 2016-03-30         16890 3
4 2015-12-02         16771 4
5 2016-03-04         16864 5
6 2015-12-21         16790 6
> isFirstInRun <- function(x) c(TRUE, x[-1] != x[-length(x)])
> isLastInRun <- function(x) c(x[-1] != x[-length(x)], TRUE)
> ds <- d[order(d$Date),]
> ds
        Date DaysSince1970 I
4 2015-12-02         16771 4
6 2015-12-21         16790 6
1 2016-01-05         16805 1
2 2016-03-04         16864 2
5 2016-03-04         16864 5
3 2016-03-30         16890 3
> ds[isFirstInRun(format(ds$Date, "%Y-%m")),]
        Date DaysSince1970 I
4 2015-12-02         16771 4
1 2016-01-05         16805 1
2 2016-03-04         16864 2
> ds[isLastInRun(format(ds$Date, "%Y-%m")),]
        Date DaysSince1970 I
6 2015-12-21         16790 6
1 2016-01-05         16805 1
3 2016-03-30         16890 3


Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>

On Thu, Jul 14, 2016 at 8:26 AM, Mehta, Gaurang <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.com>> wrote:
Hi Jeff,
I would say my problem is what you described in 2 below. My data is as follows:
Date    UC11    UC12    UC13
02/01/1997      1       2       0
03/01/1997      5       6       3
06/01/1997      5       4       6
07/01/1997      6       4       3
08/01/1997      6       5       5
09/01/1997      7       6       8
10/01/1997      8       5       5
13/01/1997      8       6       5
14/01/1997      7       4       4
15/01/1997      6       3       3
16/01/1997      8       5       5
17/01/1997      6       4       3
20/01/1997      5       4       2
21/01/1997      7       5       5
22/01/1997      16      12      12
23/01/1997      5       3       4
24/01/1997      5       2       2
27/01/1997      8       4       5
28/01/1997      7       5       9
29/01/1997      4       4       4
30/01/1997      4       4       6
31/01/1997      9       7       8
03/02/1997      9       6       8


I want to select the data on the first date it can be 1st , 2nd or 3rd or any and last date it can be 31st, 30th and /or29th. I don?t need time.
It would be great if you could help.
Regards,
Gaurang Mehta



-----Original Message-----
From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>]
Sent: 14 July 2016 16:03
To: Mehta, Gaurang; R-help Mailing List
Subject: Re: [R] Selecting 1st and last dates from a set of dates

I suspect the answer to your question (is there a function...) is almost certainly yes, but your question is too vague to be sure.

1) Data frames and matrices are different in important ways... it is highly unlikely that matrices would be appropriate for date data.

2) Do you mean "select records with earliest date in each month" or "select records whose day of month is 1"? If you need to work with time of day along with date then the solution will be different than if you are working with date only.

3) Have you converted your dates to Date or POSIXct or chron already? Which?

4) There are a lot of useful functions in base R [1][2], as well as contributed packages such as chron and lubridate.

A reproducible example [3] is the standard way to communicate what problem you actually have.  In particular, including the output of dput for a representative sample of data is a key element of that example.

[1] ?DateTimeClasses
[2] R News 4/1 p29
[3] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
--
Sent from my phone. Please excuse my brevity.

On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang" <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.com>> wrote:
>Hi Team,
>I am struggling to select the first date and last date of the month
>where there is data in a dataframe/matrix.
>Is there any r function that can help to easily select data on the
>first and last day of the month from the given set of dates and values.
>Thanks for the help in advance.
>Regards,
>Gaurang
>
>
>This email is intended for the person or company named and access by
>anyone else is unauthorised. If you are not the person or company
>named, please delete this email and notify the sender.
>
>The information in this email, including any attachments, may be
>confidential or legally privileged (meaning that its disclosure is
>protected in law). Its unauthorised disclosure, copying, distribution
>or use is prohibited and may be unlawful.
>
>Email communications sent over the internet are not guaranteed to be
>secure or virus-free and such messages are potentially at risk.  The
>Royal London Group accepts no liability for any claims arising from use
>of the internet to transmit messages by or to any company within the
>Royal London Group.
>
>The Royal London Group consists of The Royal London Mutual Insurance
>Society Limited and its subsidiaries.
>
>The Royal London Mutual Insurance Society Limited is authorised by the
>Prudential Regulation Authority and regulated by the Financial Conduct
>Authority and the Prudential Regulation Authority and provides life
>assurance and pensions.
>
>Registered in England and Wales number 99064.
>
>Registered office: 55 Gracechurch Street, London, EC3V 0RL.
>
>In the Republic of Ireland: The Royal London Mutual Insurance Society
>Limited is authorised by the Prudential Regulation Authority in the UK
>and is regulated by the Central Bank of Ireland for conduct of business
>rules.
>
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From NordlDJ at dshs.wa.gov  Thu Jul 14 18:22:11 2016
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Thu, 14 Jul 2016 16:22:11 +0000
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <3BB657B92C75C74385A95FAA7C6B17161F0C60B668@VRTPRDEXM02.royallondongroup.com>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
	<D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>
	<CAF8bMcYcXQtprEjNJPzFS3HCKr-zckZ5nEKmnpUpB1HkpdS08w@mail.gmail.com>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B668@VRTPRDEXM02.royallondongroup.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27663097DA84@WAXMXOLYMB025.WAX.wa.lcl>

Using William Dunlap's data, here is another alternative:

library(zoo)
aggregate(d$Date,list(as.yearmon(d$Date)),min)
aggregate(d$Date,list(as.yearmon(d$Date)),max)


Hope this is helpful,

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Mehta,
> Gaurang
> Sent: Thursday, July 14, 2016 9:01 AM
> To: William Dunlap
> Cc: R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
> 
> Thanks William.
> This works. Thanks again.
> 
> From: William Dunlap [mailto:wdunlap at tibco.com]
> Sent: 14 July 2016 16:42
> To: Mehta, Gaurang
> Cc: Jeff Newmiller; R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
> 
> Does the following example help?  isFirstInRun() and isLastInRun() are handy
> utility functions.
> 
> > d <- transform(data.frame(Date=as.Date(c("2016-01-05", "2016-03-04",
> > "2016-03-30", "2015-12-02", "2016-03-04", "2015-12-21"))),
> > DaysSince1970=as.integer(Date), I=seq_along(Date)) d
>         Date DaysSince1970 I
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> 3 2016-03-30         16890 3
> 4 2015-12-02         16771 4
> 5 2016-03-04         16864 5
> 6 2015-12-21         16790 6
> > isFirstInRun <- function(x) c(TRUE, x[-1] != x[-length(x)])
> > isLastInRun <- function(x) c(x[-1] != x[-length(x)], TRUE) ds <-
> > d[order(d$Date),] ds
>         Date DaysSince1970 I
> 4 2015-12-02         16771 4
> 6 2015-12-21         16790 6
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> 5 2016-03-04         16864 5
> 3 2016-03-30         16890 3
> > ds[isFirstInRun(format(ds$Date, "%Y-%m")),]
>         Date DaysSince1970 I
> 4 2015-12-02         16771 4
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> > ds[isLastInRun(format(ds$Date, "%Y-%m")),]
>         Date DaysSince1970 I
> 6 2015-12-21         16790 6
> 1 2016-01-05         16805 1
> 3 2016-03-30         16890 3
> 
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com<http://tibco.com>
> 
> On Thu, Jul 14, 2016 at 8:26 AM, Mehta, Gaurang
> <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.c
> om>> wrote:
> Hi Jeff,
> I would say my problem is what you described in 2 below. My data is as
> follows:
> Date    UC11    UC12    UC13
> 02/01/1997      1       2       0
> 03/01/1997      5       6       3
> 06/01/1997      5       4       6
> 07/01/1997      6       4       3
> 08/01/1997      6       5       5
> 09/01/1997      7       6       8
> 10/01/1997      8       5       5
> 13/01/1997      8       6       5
> 14/01/1997      7       4       4
> 15/01/1997      6       3       3
> 16/01/1997      8       5       5
> 17/01/1997      6       4       3
> 20/01/1997      5       4       2
> 21/01/1997      7       5       5
> 22/01/1997      16      12      12
> 23/01/1997      5       3       4
> 24/01/1997      5       2       2
> 27/01/1997      8       4       5
> 28/01/1997      7       5       9
> 29/01/1997      4       4       4
> 30/01/1997      4       4       6
> 31/01/1997      9       7       8
> 03/02/1997      9       6       8
> 
> 
> I want to select the data on the first date it can be 1st , 2nd or 3rd or any and
> last date it can be 31st, 30th and /or29th. I don?t need time.
> It would be great if you could help.
> Regards,
> Gaurang Mehta
> 
> 
> 
> -----Original Message-----
> From: Jeff Newmiller
> [mailto:jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>]
> Sent: 14 July 2016 16:03
> To: Mehta, Gaurang; R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
> 
> I suspect the answer to your question (is there a function...) is almost
> certainly yes, but your question is too vague to be sure.
> 
> 1) Data frames and matrices are different in important ways... it is highly
> unlikely that matrices would be appropriate for date data.
> 
> 2) Do you mean "select records with earliest date in each month" or "select
> records whose day of month is 1"? If you need to work with time of day
> along with date then the solution will be different than if you are working
> with date only.
> 
> 3) Have you converted your dates to Date or POSIXct or chron already?
> Which?
> 
> 4) There are a lot of useful functions in base R [1][2], as well as contributed
> packages such as chron and lubridate.
> 
> A reproducible example [3] is the standard way to communicate what
> problem you actually have.  In particular, including the output of dput for a
> representative sample of data is a key element of that example.
> 
> [1] ?DateTimeClasses
> [2] R News 4/1 p29
> [3] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-
> reproducible-example
> --
> Sent from my phone. Please excuse my brevity.
> 
> On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang"
> <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.c
> om>> wrote:
> >Hi Team,
> >I am struggling to select the first date and last date of the month
> >where there is data in a dataframe/matrix.
> >Is there any r function that can help to easily select data on the
> >first and last day of the month from the given set of dates and values.
> >Thanks for the help in advance.
> >Regards,
> >Gaurang
> >
> >
> >This email is intended for the person or company named and access by
> >anyone else is unauthorised. If you are not the person or company
> >named, please delete this email and notify the sender.
> >
> >The information in this email, including any attachments, may be
> >confidential or legally privileged (meaning that its disclosure is
> >protected in law). Its unauthorised disclosure, copying, distribution
> >or use is prohibited and may be unlawful.
> >
> >Email communications sent over the internet are not guaranteed to be
> >secure or virus-free and such messages are potentially at risk.  The
> >Royal London Group accepts no liability for any claims arising from use
> >of the internet to transmit messages by or to any company within the
> >Royal London Group.
> >
> >The Royal London Group consists of The Royal London Mutual Insurance
> >Society Limited and its subsidiaries.
> >
> >The Royal London Mutual Insurance Society Limited is authorised by the
> >Prudential Regulation Authority and regulated by the Financial Conduct
> >Authority and the Prudential Regulation Authority and provides life
> >assurance and pensions.
> >
> >Registered in England and Wales number 99064.
> >
> >Registered office: 55 Gracechurch Street, London, EC3V 0RL.
> >
> >In the Republic of Ireland: The Royal London Mutual Insurance Society
> >Limited is authorised by the Prudential Regulation Authority in the UK
> >and is regulated by the Central Bank of Ireland for conduct of business
> >rules.
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> >UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From Gaurang.Mehta at royallondon.com  Thu Jul 14 18:25:46 2016
From: Gaurang.Mehta at royallondon.com (Mehta, Gaurang)
Date: Thu, 14 Jul 2016 17:25:46 +0100
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <F7E6D18CC2877149AB5296CE54EA27663097DA84@WAXMXOLYMB025.WAX.wa.lcl>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
	<D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>
	<CAF8bMcYcXQtprEjNJPzFS3HCKr-zckZ5nEKmnpUpB1HkpdS08w@mail.gmail.com>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B668@VRTPRDEXM02.royallondongroup.com>
	<F7E6D18CC2877149AB5296CE54EA27663097DA84@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <3BB657B92C75C74385A95FAA7C6B17161F0C60B675@VRTPRDEXM02.royallondongroup.com>

Thanks. That is a slick one.


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Nordlund, Dan (DSHS/RDA)
Sent: 14 July 2016 17:22
To: R-help Mailing List
Subject: Re: [R] Selecting 1st and last dates from a set of dates

Using William Dunlap's data, here is another alternative:

library(zoo)
aggregate(d$Date,list(as.yearmon(d$Date)),min)
aggregate(d$Date,list(as.yearmon(d$Date)),max)


Hope this is helpful,

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration Washington State Department of Social and Health Services


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Mehta, 
> Gaurang
> Sent: Thursday, July 14, 2016 9:01 AM
> To: William Dunlap
> Cc: R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
> 
> Thanks William.
> This works. Thanks again.
> 
> From: William Dunlap [mailto:wdunlap at tibco.com]
> Sent: 14 July 2016 16:42
> To: Mehta, Gaurang
> Cc: Jeff Newmiller; R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
> 
> Does the following example help?  isFirstInRun() and isLastInRun() are 
> handy utility functions.
> 
> > d <- transform(data.frame(Date=as.Date(c("2016-01-05", "2016-03-04", 
> > "2016-03-30", "2015-12-02", "2016-03-04", "2015-12-21"))), 
> > DaysSince1970=as.integer(Date), I=seq_along(Date)) d
>         Date DaysSince1970 I
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> 3 2016-03-30         16890 3
> 4 2015-12-02         16771 4
> 5 2016-03-04         16864 5
> 6 2015-12-21         16790 6
> > isFirstInRun <- function(x) c(TRUE, x[-1] != x[-length(x)]) 
> > isLastInRun <- function(x) c(x[-1] != x[-length(x)], TRUE) ds <- 
> > d[order(d$Date),] ds
>         Date DaysSince1970 I
> 4 2015-12-02         16771 4
> 6 2015-12-21         16790 6
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> 5 2016-03-04         16864 5
> 3 2016-03-30         16890 3
> > ds[isFirstInRun(format(ds$Date, "%Y-%m")),]
>         Date DaysSince1970 I
> 4 2015-12-02         16771 4
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> > ds[isLastInRun(format(ds$Date, "%Y-%m")),]
>         Date DaysSince1970 I
> 6 2015-12-21         16790 6
> 1 2016-01-05         16805 1
> 3 2016-03-30         16890 3
> 
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com<http://tibco.com>
> 
> On Thu, Jul 14, 2016 at 8:26 AM, Mehta, Gaurang 
> <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.c
> om>> wrote:
> Hi Jeff,
> I would say my problem is what you described in 2 below. My data is as
> follows:
> Date    UC11    UC12    UC13
> 02/01/1997      1       2       0
> 03/01/1997      5       6       3
> 06/01/1997      5       4       6
> 07/01/1997      6       4       3
> 08/01/1997      6       5       5
> 09/01/1997      7       6       8
> 10/01/1997      8       5       5
> 13/01/1997      8       6       5
> 14/01/1997      7       4       4
> 15/01/1997      6       3       3
> 16/01/1997      8       5       5
> 17/01/1997      6       4       3
> 20/01/1997      5       4       2
> 21/01/1997      7       5       5
> 22/01/1997      16      12      12
> 23/01/1997      5       3       4
> 24/01/1997      5       2       2
> 27/01/1997      8       4       5
> 28/01/1997      7       5       9
> 29/01/1997      4       4       4
> 30/01/1997      4       4       6
> 31/01/1997      9       7       8
> 03/02/1997      9       6       8
> 
> 
> I want to select the data on the first date it can be 1st , 2nd or 3rd 
> or any and last date it can be 31st, 30th and /or29th. I don?t need time.
> It would be great if you could help.
> Regards,
> Gaurang Mehta
> 
> 
> 
> -----Original Message-----
> From: Jeff Newmiller
> [mailto:jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>]
> Sent: 14 July 2016 16:03
> To: Mehta, Gaurang; R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
> 
> I suspect the answer to your question (is there a function...) is 
> almost certainly yes, but your question is too vague to be sure.
> 
> 1) Data frames and matrices are different in important ways... it is 
> highly unlikely that matrices would be appropriate for date data.
> 
> 2) Do you mean "select records with earliest date in each month" or 
> "select records whose day of month is 1"? If you need to work with 
> time of day along with date then the solution will be different than 
> if you are working with date only.
> 
> 3) Have you converted your dates to Date or POSIXct or chron already?
> Which?
> 
> 4) There are a lot of useful functions in base R [1][2], as well as 
> contributed packages such as chron and lubridate.
> 
> A reproducible example [3] is the standard way to communicate what 
> problem you actually have.  In particular, including the output of 
> dput for a representative sample of data is a key element of that example.
> 
> [1] ?DateTimeClasses
> [2] R News 4/1 p29
> [3] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-
> reproducible-example
> --
> Sent from my phone. Please excuse my brevity.
> 
> On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang"
> <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.c
> om>> wrote:
> >Hi Team,
> >I am struggling to select the first date and last date of the month 
> >where there is data in a dataframe/matrix.
> >Is there any r function that can help to easily select data on the 
> >first and last day of the month from the given set of dates and values.
> >Thanks for the help in advance.
> >Regards,
> >Gaurang
> >
> >
> >This email is intended for the person or company named and access by 
> >anyone else is unauthorised. If you are not the person or company 
> >named, please delete this email and notify the sender.
> >
> >The information in this email, including any attachments, may be 
> >confidential or legally privileged (meaning that its disclosure is 
> >protected in law). Its unauthorised disclosure, copying, distribution 
> >or use is prohibited and may be unlawful.
> >
> >Email communications sent over the internet are not guaranteed to be 
> >secure or virus-free and such messages are potentially at risk.  The 
> >Royal London Group accepts no liability for any claims arising from 
> >use of the internet to transmit messages by or to any company within 
> >the Royal London Group.
> >
> >The Royal London Group consists of The Royal London Mutual Insurance 
> >Society Limited and its subsidiaries.
> >
> >The Royal London Mutual Insurance Society Limited is authorised by 
> >the Prudential Regulation Authority and regulated by the Financial 
> >Conduct Authority and the Prudential Regulation Authority and 
> >provides life assurance and pensions.
> >
> >Registered in England and Wales number 99064.
> >
> >Registered office: 55 Gracechurch Street, London, EC3V 0RL.
> >
> >In the Republic of Ireland: The Royal London Mutual Insurance Society 
> >Limited is authorised by the Prudential Regulation Authority in the 
> >UK and is regulated by the Central Bank of Ireland for conduct of 
> >business rules.
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To 
> >UNSUBSCRIBE and more, see 
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To 
> UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html and provide commented, minimal, self-contained, 
> reproducible code.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html and provide commented, minimal, self-contained, 
> reproducible code.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From wdunlap at tibco.com  Thu Jul 14 18:31:33 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 14 Jul 2016 09:31:33 -0700
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <F7E6D18CC2877149AB5296CE54EA27663097DA84@WAXMXOLYMB025.WAX.wa.lcl>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
	<D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>
	<CAF8bMcYcXQtprEjNJPzFS3HCKr-zckZ5nEKmnpUpB1HkpdS08w@mail.gmail.com>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B668@VRTPRDEXM02.royallondongroup.com>
	<F7E6D18CC2877149AB5296CE54EA27663097DA84@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <CAF8bMcbt_OiyVTDOPARKh8gGR8g7GroGzx9o3JhUuUiSv-+Jeg@mail.gmail.com>

I did not use aggregate because it did not make it convenient to
return the rest of the row with the min or max data in it.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 14, 2016 at 9:22 AM, Nordlund, Dan (DSHS/RDA) <
NordlDJ at dshs.wa.gov> wrote:

> Using William Dunlap's data, here is another alternative:
>
> library(zoo)
> aggregate(d$Date,list(as.yearmon(d$Date)),min)
> aggregate(d$Date,list(as.yearmon(d$Date)),max)
>
>
> Hope this is helpful,
>
> Dan
>
> Daniel Nordlund, PhD
> Research and Data Analysis Division
> Services & Enterprise Support Administration
> Washington State Department of Social and Health Services
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Mehta,
> > Gaurang
> > Sent: Thursday, July 14, 2016 9:01 AM
> > To: William Dunlap
> > Cc: R-help Mailing List
> > Subject: Re: [R] Selecting 1st and last dates from a set of dates
> >
> > Thanks William.
> > This works. Thanks again.
> >
> > From: William Dunlap [mailto:wdunlap at tibco.com]
> > Sent: 14 July 2016 16:42
> > To: Mehta, Gaurang
> > Cc: Jeff Newmiller; R-help Mailing List
> > Subject: Re: [R] Selecting 1st and last dates from a set of dates
> >
> > Does the following example help?  isFirstInRun() and isLastInRun() are
> handy
> > utility functions.
> >
> > > d <- transform(data.frame(Date=as.Date(c("2016-01-05", "2016-03-04",
> > > "2016-03-30", "2015-12-02", "2016-03-04", "2015-12-21"))),
> > > DaysSince1970=as.integer(Date), I=seq_along(Date)) d
> >         Date DaysSince1970 I
> > 1 2016-01-05         16805 1
> > 2 2016-03-04         16864 2
> > 3 2016-03-30         16890 3
> > 4 2015-12-02         16771 4
> > 5 2016-03-04         16864 5
> > 6 2015-12-21         16790 6
> > > isFirstInRun <- function(x) c(TRUE, x[-1] != x[-length(x)])
> > > isLastInRun <- function(x) c(x[-1] != x[-length(x)], TRUE) ds <-
> > > d[order(d$Date),] ds
> >         Date DaysSince1970 I
> > 4 2015-12-02         16771 4
> > 6 2015-12-21         16790 6
> > 1 2016-01-05         16805 1
> > 2 2016-03-04         16864 2
> > 5 2016-03-04         16864 5
> > 3 2016-03-30         16890 3
> > > ds[isFirstInRun(format(ds$Date, "%Y-%m")),]
> >         Date DaysSince1970 I
> > 4 2015-12-02         16771 4
> > 1 2016-01-05         16805 1
> > 2 2016-03-04         16864 2
> > > ds[isLastInRun(format(ds$Date, "%Y-%m")),]
> >         Date DaysSince1970 I
> > 6 2015-12-21         16790 6
> > 1 2016-01-05         16805 1
> > 3 2016-03-30         16890 3
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com<http://tibco.com>
> >
> > On Thu, Jul 14, 2016 at 8:26 AM, Mehta, Gaurang
> > <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.c
> > om>> wrote:
> > Hi Jeff,
> > I would say my problem is what you described in 2 below. My data is as
> > follows:
> > Date    UC11    UC12    UC13
> > 02/01/1997      1       2       0
> > 03/01/1997      5       6       3
> > 06/01/1997      5       4       6
> > 07/01/1997      6       4       3
> > 08/01/1997      6       5       5
> > 09/01/1997      7       6       8
> > 10/01/1997      8       5       5
> > 13/01/1997      8       6       5
> > 14/01/1997      7       4       4
> > 15/01/1997      6       3       3
> > 16/01/1997      8       5       5
> > 17/01/1997      6       4       3
> > 20/01/1997      5       4       2
> > 21/01/1997      7       5       5
> > 22/01/1997      16      12      12
> > 23/01/1997      5       3       4
> > 24/01/1997      5       2       2
> > 27/01/1997      8       4       5
> > 28/01/1997      7       5       9
> > 29/01/1997      4       4       4
> > 30/01/1997      4       4       6
> > 31/01/1997      9       7       8
> > 03/02/1997      9       6       8
> >
> >
> > I want to select the data on the first date it can be 1st , 2nd or 3rd
> or any and
> > last date it can be 31st, 30th and /or29th. I don?t need time.
> > It would be great if you could help.
> > Regards,
> > Gaurang Mehta
> >
> >
> >
> > -----Original Message-----
> > From: Jeff Newmiller
> > [mailto:jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>]
> > Sent: 14 July 2016 16:03
> > To: Mehta, Gaurang; R-help Mailing List
> > Subject: Re: [R] Selecting 1st and last dates from a set of dates
> >
> > I suspect the answer to your question (is there a function...) is almost
> > certainly yes, but your question is too vague to be sure.
> >
> > 1) Data frames and matrices are different in important ways... it is
> highly
> > unlikely that matrices would be appropriate for date data.
> >
> > 2) Do you mean "select records with earliest date in each month" or
> "select
> > records whose day of month is 1"? If you need to work with time of day
> > along with date then the solution will be different than if you are
> working
> > with date only.
> >
> > 3) Have you converted your dates to Date or POSIXct or chron already?
> > Which?
> >
> > 4) There are a lot of useful functions in base R [1][2], as well as
> contributed
> > packages such as chron and lubridate.
> >
> > A reproducible example [3] is the standard way to communicate what
> > problem you actually have.  In particular, including the output of dput
> for a
> > representative sample of data is a key element of that example.
> >
> > [1] ?DateTimeClasses
> > [2] R News 4/1 p29
> > [3] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-
> > reproducible-example
> > --
> > Sent from my phone. Please excuse my brevity.
> >
> > On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang"
> > <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.c
> > om>> wrote:
> > >Hi Team,
> > >I am struggling to select the first date and last date of the month
> > >where there is data in a dataframe/matrix.
> > >Is there any r function that can help to easily select data on the
> > >first and last day of the month from the given set of dates and values.
> > >Thanks for the help in advance.
> > >Regards,
> > >Gaurang
> > >
> > >
> > >This email is intended for the person or company named and access by
> > >anyone else is unauthorised. If you are not the person or company
> > >named, please delete this email and notify the sender.
> > >
> > >The information in this email, including any attachments, may be
> > >confidential or legally privileged (meaning that its disclosure is
> > >protected in law). Its unauthorised disclosure, copying, distribution
> > >or use is prohibited and may be unlawful.
> > >
> > >Email communications sent over the internet are not guaranteed to be
> > >secure or virus-free and such messages are potentially at risk.  The
> > >Royal London Group accepts no liability for any claims arising from use
> > >of the internet to transmit messages by or to any company within the
> > >Royal London Group.
> > >
> > >The Royal London Group consists of The Royal London Mutual Insurance
> > >Society Limited and its subsidiaries.
> > >
> > >The Royal London Mutual Insurance Society Limited is authorised by the
> > >Prudential Regulation Authority and regulated by the Financial Conduct
> > >Authority and the Prudential Regulation Authority and provides life
> > >assurance and pensions.
> > >
> > >Registered in England and Wales number 99064.
> > >
> > >Registered office: 55 Gracechurch Street, London, EC3V 0RL.
> > >
> > >In the Republic of Ireland: The Royal London Mutual Insurance Society
> > >Limited is authorised by the Prudential Regulation Authority in the UK
> > >and is regulated by the Central Bank of Ireland for conduct of business
> > >rules.
> > >
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > >______________________________________________
> > >R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> > >UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> > UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From NordlDJ at dshs.wa.gov  Thu Jul 14 19:30:37 2016
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Thu, 14 Jul 2016 17:30:37 +0000
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <CAF8bMcbt_OiyVTDOPARKh8gGR8g7GroGzx9o3JhUuUiSv-+Jeg@mail.gmail.com>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
	<D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>
	<CAF8bMcYcXQtprEjNJPzFS3HCKr-zckZ5nEKmnpUpB1HkpdS08w@mail.gmail.com>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B668@VRTPRDEXM02.royallondongroup.com>
	<F7E6D18CC2877149AB5296CE54EA27663097DA84@WAXMXOLYMB025.WAX.wa.lcl>
	<CAF8bMcbt_OiyVTDOPARKh8gGR8g7GroGzx9o3JhUuUiSv-+Jeg@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27663097DAE9@WAXMXOLYMB025.WAX.wa.lcl>

It would be possible to ease the task of returning full rows from the matching dates (min or max) by wrapping the aggregate function up in a utility function and just returning the dates.  I also borrowed the use of format() from Bill Dunlap?s example so that loading the zoo package is not necessary.  I don?t have any notion about the efficiency of this approach as compared to Bill?s.

isFirstInMonth <- function(y) aggregate(y,list(format(y, "%Y-%m")),min)$x
d[d$Date %in% isFirstInMonth(d$Date), ]

There are other differences between the two approaches.   Bill?s code will return one row per minimum and maximum date in the month.  My approach would return all rows where d$Date was equal to the minimum or maximum dates in a month.  Which to use depends on what the OP needs.


Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services

From: William Dunlap [mailto:wdunlap at tibco.com]
Sent: Thursday, July 14, 2016 9:32 AM
To: Nordlund, Dan (DSHS/RDA)
Cc: R-help Mailing List
Subject: Re: [R] Selecting 1st and last dates from a set of dates

I did not use aggregate because it did not make it convenient to
return the rest of the row with the min or max data in it.

Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>

On Thu, Jul 14, 2016 at 9:22 AM, Nordlund, Dan (DSHS/RDA) <NordlDJ at dshs.wa.gov<mailto:NordlDJ at dshs.wa.gov>> wrote:
Using William Dunlap's data, here is another alternative:

library(zoo)
aggregate(d$Date,list(as.yearmon(d$Date)),min)
aggregate(d$Date,list(as.yearmon(d$Date)),max)


Hope this is helpful,

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Mehta,
> Gaurang
> Sent: Thursday, July 14, 2016 9:01 AM
> To: William Dunlap
> Cc: R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
>
> Thanks William.
> This works. Thanks again.
>
> From: William Dunlap [mailto:wdunlap at tibco.com<mailto:wdunlap at tibco.com>]
> Sent: 14 July 2016 16:42
> To: Mehta, Gaurang
> Cc: Jeff Newmiller; R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
>
> Does the following example help?  isFirstInRun() and isLastInRun() are handy
> utility functions.
>
> > d <- transform(data.frame(Date=as.Date(c("2016-01-05", "2016-03-04",
> > "2016-03-30", "2015-12-02", "2016-03-04", "2015-12-21"))),
> > DaysSince1970=as.integer(Date), I=seq_along(Date)) d
>         Date DaysSince1970 I
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> 3 2016-03-30         16890 3
> 4 2015-12-02         16771 4
> 5 2016-03-04         16864 5
> 6 2015-12-21         16790 6
> > isFirstInRun <- function(x) c(TRUE, x[-1] != x[-length(x)])
> > isLastInRun <- function(x) c(x[-1] != x[-length(x)], TRUE) ds <-
> > d[order(d$Date),] ds
>         Date DaysSince1970 I
> 4 2015-12-02         16771 4
> 6 2015-12-21         16790 6
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> 5 2016-03-04         16864 5
> 3 2016-03-30         16890 3
> > ds[isFirstInRun(format(ds$Date, "%Y-%m")),]
>         Date DaysSince1970 I
> 4 2015-12-02         16771 4
> 1 2016-01-05         16805 1
> 2 2016-03-04         16864 2
> > ds[isLastInRun(format(ds$Date, "%Y-%m")),]
>         Date DaysSince1970 I
> 6 2015-12-21         16790 6
> 1 2016-01-05         16805 1
> 3 2016-03-30         16890 3
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com<http://tibco.com><http://tibco.com>
>
> On Thu, Jul 14, 2016 at 8:26 AM, Mehta, Gaurang
> <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.com><mailto:Gaurang.Mehta at royallondon.c<mailto:Gaurang.Mehta at royallondon.c>
> om>> wrote:
> Hi Jeff,
> I would say my problem is what you described in 2 below. My data is as
> follows:
> Date    UC11    UC12    UC13
> 02/01/1997      1       2       0
> 03/01/1997      5       6       3
> 06/01/1997      5       4       6
> 07/01/1997      6       4       3
> 08/01/1997      6       5       5
> 09/01/1997      7       6       8
> 10/01/1997      8       5       5
> 13/01/1997      8       6       5
> 14/01/1997      7       4       4
> 15/01/1997      6       3       3
> 16/01/1997      8       5       5
> 17/01/1997      6       4       3
> 20/01/1997      5       4       2
> 21/01/1997      7       5       5
> 22/01/1997      16      12      12
> 23/01/1997      5       3       4
> 24/01/1997      5       2       2
> 27/01/1997      8       4       5
> 28/01/1997      7       5       9
> 29/01/1997      4       4       4
> 30/01/1997      4       4       6
> 31/01/1997      9       7       8
> 03/02/1997      9       6       8
>
>
> I want to select the data on the first date it can be 1st , 2nd or 3rd or any and
> last date it can be 31st, 30th and /or29th. I don?t need time.
> It would be great if you could help.
> Regards,
> Gaurang Mehta
>
>
>
> -----Original Message-----
> From: Jeff Newmiller
> [mailto:jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us><mailto:jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>>]
> Sent: 14 July 2016 16:03
> To: Mehta, Gaurang; R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
>
> I suspect the answer to your question (is there a function...) is almost
> certainly yes, but your question is too vague to be sure.
>
> 1) Data frames and matrices are different in important ways... it is highly
> unlikely that matrices would be appropriate for date data.
>
> 2) Do you mean "select records with earliest date in each month" or "select
> records whose day of month is 1"? If you need to work with time of day
> along with date then the solution will be different than if you are working
> with date only.
>
> 3) Have you converted your dates to Date or POSIXct or chron already?
> Which?
>
> 4) There are a lot of useful functions in base R [1][2], as well as contributed
> packages such as chron and lubridate.
>
> A reproducible example [3] is the standard way to communicate what
> problem you actually have.  In particular, including the output of dput for a
> representative sample of data is a key element of that example.
>
> [1] ?DateTimeClasses
> [2] R News 4/1 p29
> [3] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-
> reproducible-example
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang"
> <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.com><mailto:Gaurang.Mehta at royallondon.c<mailto:Gaurang.Mehta at royallondon.c>
> om>> wrote:
> >Hi Team,
> >I am struggling to select the first date and last date of the month
> >where there is data in a dataframe/matrix.
> >Is there any r function that can help to easily select data on the
> >first and last day of the month from the given set of dates and values.
> >Thanks for the help in advance.
> >Regards,
> >Gaurang
> >
> >
> >This email is intended for the person or company named and access by
> >anyone else is unauthorised. If you are not the person or company
> >named, please delete this email and notify the sender.
> >
> >The information in this email, including any attachments, may be
> >confidential or legally privileged (meaning that its disclosure is
> >protected in law). Its unauthorised disclosure, copying, distribution
> >or use is prohibited and may be unlawful.
> >
> >Email communications sent over the internet are not guaranteed to be
> >secure or virus-free and such messages are potentially at risk.  The
> >Royal London Group accepts no liability for any claims arising from use
> >of the internet to transmit messages by or to any company within the
> >Royal London Group.
> >
> >The Royal London Group consists of The Royal London Mutual Insurance
> >Society Limited and its subsidiaries.
> >
> >The Royal London Mutual Insurance Society Limited is authorised by the
> >Prudential Regulation Authority and regulated by the Financial Conduct
> >Authority and the Prudential Regulation Authority and provides life
> >assurance and pensions.
> >
> >Registered in England and Wales number 99064.
> >
> >Registered office: 55 Gracechurch Street, London, EC3V 0RL.
> >
> >In the Republic of Ireland: The Royal London Mutual Insurance Society
> >Limited is authorised by the Prudential Regulation Authority in the UK
> >and is regulated by the Central Bank of Ireland for conduct of business
> >rules.
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org<mailto:R-help at r-project.org><mailto:R-help at r-project.org<mailto:R-help at r-project.org>> mailing list -- To
> >UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org><mailto:R-help at r-project.org<mailto:R-help at r-project.org>> mailing list -- To
> UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From pmakananisa at sars.gov.za  Thu Jul 14 08:48:48 2016
From: pmakananisa at sars.gov.za (Mangalani Peter Makananisa)
Date: Thu, 14 Jul 2016 06:48:48 +0000
Subject: [R] Dates in R (Year Month)
In-Reply-To: <784f72ffe353448f9d216c4c2a1eb8a5@exch-2p-mbx-t2.ads.tamu.edu>
References: <BBB169C177D06B4E8E650D24548404CEC0B92F62@PTABREXC12M2.sars.gov.za>
	<784f72ffe353448f9d216c4c2a1eb8a5@exch-2p-mbx-t2.ads.tamu.edu>
Message-ID: <BBB169C177D06B4E8E650D24548404CEC0B93198@PTABREXC12M2.sars.gov.za>

Thanks.

-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu] 
Sent: 13 July 2016 05:05 PM
To: Mangalani Peter Makananisa; r-help at r-project.org
Subject: RE: Dates in R (Year Month)

You need to look at the examples on the manual pages for ?yearmon and ?strptime:

> Z <- as.yearmon(as.character(X), "%Y%m") Z
 [1] "Jan 2015" "Feb 2015" "Mar 2015" "May 2015" "Jun 2015" "Jul 2015" "Aug 2015"
 [8] "Sep 2015" "Oct 2015" "Nov 2015" "Dec 2015" "Jan 2016" "Feb 2016" "Mar 2016"
[15] "Apr 2016" "May 2016" "Jun 2016"

Or
> Z <- as.yearmon(round(X/100) + (X - round(X/100)*100 - 1)/12) Z
 [1] "Jan 2015" "Feb 2015" "Mar 2015" "May 2015" "Jun 2015" "Jul 2015" "Aug 2015"
 [8] "Sep 2015" "Oct 2015" "Nov 2015" "Dec 2015" "Jan 2016" "Feb 2016" "Mar 2016"
[15] "Apr 2016" "May 2016" "Jun 2016"

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Mangalani Peter Makananisa
Sent: Wednesday, July 13, 2016 8:25 AM
To: r-help at r-project.org
Subject: [R] Dates in R (Year Month)

Hi All,

I am trying to convert the vector below to dates please assist I have tried to use information on the links you sent, but it is not working.

X  = c(201501, 201502, 201503, 201505, 201506, 201507, 201508, 201509, 201510, 201511, 201512, 201601, 201602, 201603, 201604, 201605, 201606)

library(chron, zoo)
Z = as.yearmon(X)  # it is not working

please assist

Kind regards
Peter

Please Note: This email and its contents are subject to our email legal notice which can be viewed at http://www.sars.gov.za/Pages/Email-disclaimer.aspx

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

Please Note: This email and its contents are subject to our email legal notice which can be viewed at http://www.sars.gov.za/Pages/Email-disclaimer.aspx

	[[alternative HTML version deleted]]


From jorgeivanvelez at gmail.com  Thu Jul 14 19:55:41 2016
From: jorgeivanvelez at gmail.com (Jorge I Velez)
Date: Thu, 14 Jul 2016 12:55:41 -0500
Subject: [R] scatter3D and colours
Message-ID: <CAKL8G3Ei0tOkgVy0tFS1ZF5b-GcVUi5fYp3=sNqa-c+QCEB6wg@mail.gmail.com>

Dear R-help,

I am using the plot3D package to produce 3D spheres along with 95% CIs
distinguishing each sphere with a predefined colour (see the reproducible
example at the end).

I have been successful in producing a similar plot using a different data
set (kindly see
https://www.dropbox.com/s/snpgiqgqaiqgpiv/exp1combined.pdf?dl=0), but in
this case I cannot arrange the colours as desired. Specifically, the
colours obtaind do not correspond to those in the "cols" object below (kindly
see https://www.dropbox.com/s/nhljsuare84g811/test_exp2.pdf?dl=0).

Any help/tips would be greatly appreciated.

Thank you very much in advance.

Best regards,
Jorge Velez.-


## package needed
if(!require(plot3D)) install.packages("plot3D")
require(plot3D)

## data to be plotted
d0 <- structure(list(X = c(2.5, -2, 1), Z = c(-3.5, 4, -1), Y = c(8,
-8.5, -1)), .Names = c("X", "Z", "Y"), row.names = c("high",
"low", "medium"), class = "data.frame")
d0

## confidence intervals to be added
CI2 <- structure(list(z = structure(c(2, 3, 4, 2, 3, 4), .Dim = c(3L,
2L), .Dimnames = list(c("high", "low", "medium"), c("97.5%",
"97.5%"))), y = structure(c(4, 5, 5, 4, 5, 5), .Dim = c(3L, 2L
), .Dimnames = list(c("high", "low", "medium"), c("97.5%", "97.5%"
))), x = structure(c(3, 7, 5, 3, 7, 5), .Dim = c(3L, 2L), .Dimnames = list(
    c("high", "low", "medium"), c("97.5%", "97.5%"))), alen = 0,
    lwd = 2), .Names = c("z", "y", "x", "alen", "lwd"))

## colours I would like to have
cols <- c("#0080ff", "#ff00ff", "darkgreen")

# this produces the 3D plot, but the colours are not properly assigned
with(d0, scatter3D(X, Z, Y, bty = "b2", col = cols,
           pch = 20, cex = 4, ticktype = "detailed", colkey = FALSE, phi =
20, theta = -140, zlim = c(-14, 14), xlim = c(-14, 14), ylim = c(-14, 14),
xlab = "X", ylab = "Z", zlab = "Y", CI = CI2))
mtext('Experiment 2', line = -0.5, side = 3, cex = 1.3)


?## R session details
R> sessionInfo()
R version 3.2.4 Patched (2016-03-28 r70416)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.11.5 (El Capitan)

locale:
[1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  parallel  compiler
[8] methods   base

other attached packages:
[1] plot3D_1.1   readxl_0.1.1

loaded via a namespace (and not attached):
[1] tools_3.2.4  misc3d_0.8-4 Rcpp_0.12.5

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Thu Jul 14 20:29:08 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 14 Jul 2016 14:29:08 -0400
Subject: [R] scatter3D and colours
In-Reply-To: <CAKL8G3Ei0tOkgVy0tFS1ZF5b-GcVUi5fYp3=sNqa-c+QCEB6wg@mail.gmail.com>
References: <CAKL8G3Ei0tOkgVy0tFS1ZF5b-GcVUi5fYp3=sNqa-c+QCEB6wg@mail.gmail.com>
Message-ID: <CAM_vjukDVM+MrazSpNo-_1tou+vHhfBpntPF0njd6mBma068Ng@mail.gmail.com>

I assume you want some variant of:
with(d0, scatter3D(X, Z, Y, bty = "b2", col = cols, colvar = c(1, 2, 3),
           pch = 20, cex = 4, ticktype = "detailed", colkey = FALSE, phi =
20, theta = -140, zlim = c(-14, 14), xlim = c(-14, 14), ylim = c(-14, 14),
xlab = "X", ylab = "Z", zlab = "Y", CI = CI2))
mtext('Experiment 2', line = -0.5, side = 3, cex = 1.3)

You should reread the help for scatter3D, paying close attention to
col and colvar arguments. The default value of colvar is z, and that's
what scatter3D() was using. Setting colvar to your chosen levels gives
you the result you expect.

Thanks for the complete reproducible example. I wouldn't have even
looked at your problem without something to test.

Sarah

On Thu, Jul 14, 2016 at 1:55 PM, Jorge I Velez <jorgeivanvelez at gmail.com> wrote:
> Dear R-help,
>
> I am using the plot3D package to produce 3D spheres along with 95% CIs
> distinguishing each sphere with a predefined colour (see the reproducible
> example at the end).
>
> I have been successful in producing a similar plot using a different data
> set (kindly see
> https://www.dropbox.com/s/snpgiqgqaiqgpiv/exp1combined.pdf?dl=0), but in
> this case I cannot arrange the colours as desired. Specifically, the
> colours obtaind do not correspond to those in the "cols" object below (kindly
> see https://www.dropbox.com/s/nhljsuare84g811/test_exp2.pdf?dl=0).
>
> Any help/tips would be greatly appreciated.
>
> Thank you very much in advance.
>
> Best regards,
> Jorge Velez.-
>
>
> ## package needed
> if(!require(plot3D)) install.packages("plot3D")
> require(plot3D)
>
> ## data to be plotted
> d0 <- structure(list(X = c(2.5, -2, 1), Z = c(-3.5, 4, -1), Y = c(8,
> -8.5, -1)), .Names = c("X", "Z", "Y"), row.names = c("high",
> "low", "medium"), class = "data.frame")
> d0
>
> ## confidence intervals to be added
> CI2 <- structure(list(z = structure(c(2, 3, 4, 2, 3, 4), .Dim = c(3L,
> 2L), .Dimnames = list(c("high", "low", "medium"), c("97.5%",
> "97.5%"))), y = structure(c(4, 5, 5, 4, 5, 5), .Dim = c(3L, 2L
> ), .Dimnames = list(c("high", "low", "medium"), c("97.5%", "97.5%"
> ))), x = structure(c(3, 7, 5, 3, 7, 5), .Dim = c(3L, 2L), .Dimnames = list(
>     c("high", "low", "medium"), c("97.5%", "97.5%"))), alen = 0,
>     lwd = 2), .Names = c("z", "y", "x", "alen", "lwd"))
>
> ## colours I would like to have
> cols <- c("#0080ff", "#ff00ff", "darkgreen")
>
> # this produces the 3D plot, but the colours are not properly assigned
> with(d0, scatter3D(X, Z, Y, bty = "b2", col = cols,
>            pch = 20, cex = 4, ticktype = "detailed", colkey = FALSE, phi =
> 20, theta = -140, zlim = c(-14, 14), xlim = c(-14, 14), ylim = c(-14, 14),
> xlab = "X", ylab = "Z", zlab = "Y", CI = CI2))
> mtext('Experiment 2', line = -0.5, side = 3, cex = 1.3)
>


From jorgeivanvelez at gmail.com  Thu Jul 14 20:48:50 2016
From: jorgeivanvelez at gmail.com (Jorge I Velez)
Date: Thu, 14 Jul 2016 13:48:50 -0500
Subject: [R] scatter3D and colours
In-Reply-To: <CAM_vjukDVM+MrazSpNo-_1tou+vHhfBpntPF0njd6mBma068Ng@mail.gmail.com>
References: <CAKL8G3Ei0tOkgVy0tFS1ZF5b-GcVUi5fYp3=sNqa-c+QCEB6wg@mail.gmail.com>
	<CAM_vjukDVM+MrazSpNo-_1tou+vHhfBpntPF0njd6mBma068Ng@mail.gmail.com>
Message-ID: <CAKL8G3F4OdzueLvJM92xm0hr+GUcpfeQdHk-EToOQ+nN2qfdFA@mail.gmail.com>

Thank you very much, Sarah, for your help.  The colvar argument was
certainly what I needed.  I will follow your suggestion.
Cheers,
Jorge.-



On Thu, Jul 14, 2016 at 1:29 PM, Sarah Goslee <sarah.goslee at gmail.com>
wrote:

> I assume you want some variant of:
> with(d0, scatter3D(X, Z, Y, bty = "b2", col = cols, colvar = c(1, 2, 3),
>            pch = 20, cex = 4, ticktype = "detailed", colkey = FALSE, phi =
> 20, theta = -140, zlim = c(-14, 14), xlim = c(-14, 14), ylim = c(-14, 14),
> xlab = "X", ylab = "Z", zlab = "Y", CI = CI2))
> mtext('Experiment 2', line = -0.5, side = 3, cex = 1.3)
>
> You should reread the help for scatter3D, paying close attention to
> col and colvar arguments. The default value of colvar is z, and that's
> what scatter3D() was using. Setting colvar to your chosen levels gives
> you the result you expect.
>
> Thanks for the complete reproducible example. I wouldn't have even
> looked at your problem without something to test.
>
> Sarah
>
> On Thu, Jul 14, 2016 at 1:55 PM, Jorge I Velez <jorgeivanvelez at gmail.com>
> wrote:
> > Dear R-help,
> >
> > I am using the plot3D package to produce 3D spheres along with 95% CIs
> > distinguishing each sphere with a predefined colour (see the reproducible
> > example at the end).
> >
> > I have been successful in producing a similar plot using a different data
> > set (kindly see
> > https://www.dropbox.com/s/snpgiqgqaiqgpiv/exp1combined.pdf?dl=0), but in
> > this case I cannot arrange the colours as desired. Specifically, the
> > colours obtaind do not correspond to those in the "cols" object below
> (kindly
> > see https://www.dropbox.com/s/nhljsuare84g811/test_exp2.pdf?dl=0).
> >
> > Any help/tips would be greatly appreciated.
> >
> > Thank you very much in advance.
> >
> > Best regards,
> > Jorge Velez.-
> >
> >
> > ## package needed
> > if(!require(plot3D)) install.packages("plot3D")
> > require(plot3D)
> >
> > ## data to be plotted
> > d0 <- structure(list(X = c(2.5, -2, 1), Z = c(-3.5, 4, -1), Y = c(8,
> > -8.5, -1)), .Names = c("X", "Z", "Y"), row.names = c("high",
> > "low", "medium"), class = "data.frame")
> > d0
> >
> > ## confidence intervals to be added
> > CI2 <- structure(list(z = structure(c(2, 3, 4, 2, 3, 4), .Dim = c(3L,
> > 2L), .Dimnames = list(c("high", "low", "medium"), c("97.5%",
> > "97.5%"))), y = structure(c(4, 5, 5, 4, 5, 5), .Dim = c(3L, 2L
> > ), .Dimnames = list(c("high", "low", "medium"), c("97.5%", "97.5%"
> > ))), x = structure(c(3, 7, 5, 3, 7, 5), .Dim = c(3L, 2L), .Dimnames =
> list(
> >     c("high", "low", "medium"), c("97.5%", "97.5%"))), alen = 0,
> >     lwd = 2), .Names = c("z", "y", "x", "alen", "lwd"))
> >
> > ## colours I would like to have
> > cols <- c("#0080ff", "#ff00ff", "darkgreen")
> >
> > # this produces the 3D plot, but the colours are not properly assigned
> > with(d0, scatter3D(X, Z, Y, bty = "b2", col = cols,
> >            pch = 20, cex = 4, ticktype = "detailed", colkey = FALSE, phi
> =
> > 20, theta = -140, zlim = c(-14, 14), xlim = c(-14, 14), ylim = c(-14,
> 14),
> > xlab = "X", ylab = "Z", zlab = "Y", CI = CI2))
> > mtext('Experiment 2', line = -0.5, side = 3, cex = 1.3)
> >
>

	[[alternative HTML version deleted]]


From satish.vadlamani at gmail.com  Thu Jul 14 21:43:49 2016
From: satish.vadlamani at gmail.com (Satish Vadlamani)
Date: Thu, 14 Jul 2016 12:43:49 -0700
Subject: [R] How to group by and get distinct rows of of grouped rows based
 on certain criteria
Message-ID: <CAK3+11Ri7C-x+GJh2GCHu=xWeRrjTTYH3FU6UNdEGbtOxDrVEg@mail.gmail.com>

Hello All:
I would like to get your help on the following problem.

I have the following data and the first row is the header. Spaces are not
important.
I want to find out distinct combinations of ATP Group and Business Event
(these are the field names that you can see in the data below) that have
the Category EQ (Category is the third field) and those that do not have
the category EQ. In the example below, the combinations 02/A and 02/B have
EQ and the combination ZM/A does not.

If I have a larger file, how to get to this answer?

What did I try (with dplyr)?

# I know that the below is not correct and not giving desired results
file1_1 <- file1  %>% group_by(ATP.Group,Business.Event) %>%
filter(Category != "EQ") %>% distinct(ATP.Group,Business.Event)
# for some reason, I have to convert to data.frame to print the data
correctly
file1_1 <- as.data.frame(file1_1)
file1_1


*Data shown below*
|ATP Group|Business Event|Category|
|02       |A             |AC      |
|02       |A             |AD      |
|02       |A             |EQ      |
|ZM       |A             |AU      |
|ZM       |A             |AV      |
|ZM       |A             |AW      |
|02       |B             |AC      |
|02       |B             |AY      |
|02       |B             |EQ      |

-- 

Satish Vadlamani

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Thu Jul 14 22:50:20 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 14 Jul 2016 16:50:20 -0400
Subject: [R] How to group by and get distinct rows of of grouped rows
 based on certain criteria
In-Reply-To: <CAK3+11Ri7C-x+GJh2GCHu=xWeRrjTTYH3FU6UNdEGbtOxDrVEg@mail.gmail.com>
References: <CAK3+11Ri7C-x+GJh2GCHu=xWeRrjTTYH3FU6UNdEGbtOxDrVEg@mail.gmail.com>
Message-ID: <CAM_vju=QzMfP8tsmOehrV9cLVu_gQhJsxAp79R1Jkb+wyis4+Q@mail.gmail.com>

I took a wild guess as to what your data looked like (please use
dput() to provide data, and please do not post in HTML), and took your
request literally.

Here's one way to approach the problem:


mydat <- structure(list(ATP.Group = c("02", "02", "02", "ZM", "ZM", "ZM",
"02", "02", "02"), Business.Event = c("A", "A", "A", "A", "A",
"A", "B", "B", "B"), Category = c("AC", "AD", "EQ", "AU", "AV",
"AW", "AC", "AY", "EQ")), .Names = c("ATP.Group", "Business.Event",
"Category"), class = "data.frame", row.names = c(NA, -9L))


hasEQ <- subset(mydat, Category == "EQ")
hasEQ <- unique(do.call("paste", c(hasEQ[, c("ATP.Group",
"Business.Event")], sep="/")))

notEQ <- subset(mydat, Category != "EQ")
notEQ <- unique(do.call("paste", c(notEQ[, c("ATP.Group",
"Business.Event")], sep="/")))
notEQ <- notEQ[!(notEQ %in% hasEQ)]

> hasEQ
[1] "02/A" "02/B"
> notEQ
[1] "ZM/A"

Sarah

On Thu, Jul 14, 2016 at 3:43 PM, Satish Vadlamani
<satish.vadlamani at gmail.com> wrote:
> Hello All:
> I would like to get your help on the following problem.
>
> I have the following data and the first row is the header. Spaces are not
> important.
> I want to find out distinct combinations of ATP Group and Business Event
> (these are the field names that you can see in the data below) that have
> the Category EQ (Category is the third field) and those that do not have
> the category EQ. In the example below, the combinations 02/A and 02/B have
> EQ and the combination ZM/A does not.
>
> If I have a larger file, how to get to this answer?
>
> What did I try (with dplyr)?
>
> # I know that the below is not correct and not giving desired results
> file1_1 <- file1  %>% group_by(ATP.Group,Business.Event) %>%
> filter(Category != "EQ") %>% distinct(ATP.Group,Business.Event)
> # for some reason, I have to convert to data.frame to print the data
> correctly
> file1_1 <- as.data.frame(file1_1)
> file1_1
>
>
> *Data shown below*
> |ATP Group|Business Event|Category|
> |02       |A             |AC      |
> |02       |A             |AD      |
> |02       |A             |EQ      |
> |ZM       |A             |AU      |
> |ZM       |A             |AV      |
> |ZM       |A             |AW      |
> |02       |B             |AC      |
> |02       |B             |AY      |
> |02       |B             |EQ      |
>
> --


From wdunlap at tibco.com  Thu Jul 14 22:53:51 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 14 Jul 2016 13:53:51 -0700
Subject: [R] How to group by and get distinct rows of of grouped rows
 based on certain criteria
In-Reply-To: <CAK3+11Ri7C-x+GJh2GCHu=xWeRrjTTYH3FU6UNdEGbtOxDrVEg@mail.gmail.com>
References: <CAK3+11Ri7C-x+GJh2GCHu=xWeRrjTTYH3FU6UNdEGbtOxDrVEg@mail.gmail.com>
Message-ID: <CAF8bMcbsm231nMXhOarn_v46JHm5RYCbs-jm3qxdodS=LiXUSQ@mail.gmail.com>

> txt <- "|ATP Group|Business Event|Category|
|02       |A             |AC      |
|02       |A             |AD      |
|02       |A             |EQ      |
|ZM       |A             |AU      |
|ZM       |A             |AV      |
|ZM       |A             |AW      |
|02       |B             |AC      |
|02       |B             |AY      |
|02       |B             |EQ      |
"
> d <- read.table(sep="|", text=txt, header=TRUE, strip.white=TRUE,
check.names=FALSE)[,2:4]
> str(d)
'data.frame':   9 obs. of  3 variables:
 $ ATP Group     : Factor w/ 2 levels "02","ZM": 1 1 1 2 2 2 1 1 1
 $ Business Event: Factor w/ 2 levels "A","B": 1 1 1 1 1 1 2 2 2
 $ Category      : Factor w/ 7 levels "AC","AD","AU",..: 1 2 7 3 4 5 1 6 7
> unique(d[d[,"Category"]!="EQ", c("ATP Group", "Business Event")])
  ATP Group Business Event
1        02              A
4        ZM              A
7        02              B
> unique(d[d[,"Category"]=="EQ", c("ATP Group", "Business Event")])
  ATP Group Business Event
3        02              A
9        02              B

Some folks prefer to use subset() instead of "[".  The previous expression
is equivalent to:

> unique( subset(d, Category=="EQ", c("ATP Group", "Business Event")))
  ATP Group Business Event
3        02              A
9        02              B


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 14, 2016 at 12:43 PM, Satish Vadlamani <
satish.vadlamani at gmail.com> wrote:

> Hello All:
> I would like to get your help on the following problem.
>
> I have the following data and the first row is the header. Spaces are not
> important.
> I want to find out distinct combinations of ATP Group and Business Event
> (these are the field names that you can see in the data below) that have
> the Category EQ (Category is the third field) and those that do not have
> the category EQ. In the example below, the combinations 02/A and 02/B have
> EQ and the combination ZM/A does not.
>
> If I have a larger file, how to get to this answer?
>
> What did I try (with dplyr)?
>
> # I know that the below is not correct and not giving desired results
> file1_1 <- file1  %>% group_by(ATP.Group,Business.Event) %>%
> filter(Category != "EQ") %>% distinct(ATP.Group,Business.Event)
> # for some reason, I have to convert to data.frame to print the data
> correctly
> file1_1 <- as.data.frame(file1_1)
> file1_1
>
>
> *Data shown below*
> |ATP Group|Business Event|Category|
> |02       |A             |AC      |
> |02       |A             |AD      |
> |02       |A             |EQ      |
> |ZM       |A             |AU      |
> |ZM       |A             |AV      |
> |ZM       |A             |AW      |
> |02       |B             |AC      |
> |02       |B             |AY      |
> |02       |B             |EQ      |
>
> --
>
> Satish Vadlamani
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From faradj.g at gmail.com  Fri Jul 15 01:19:29 2016
From: faradj.g at gmail.com (Faradj Koliev)
Date: Fri, 15 Jul 2016 01:19:29 +0200
Subject: [R] =?utf-8?q?How_to_generate_=E2=80=9Caggregated=E2=80=9D_time_l?=
 =?utf-8?q?ags=3F?=
Message-ID: <E7A3A0EB-7571-4796-BBE4-3BB5762B82C3@gmail.com>

Dear all, 

I hope you?re enjoying your summer! 


I've been asked to "aggregate" my time lags from simple 1 year time lag to 1-3 year time lag. This could be done --I've been told --by simply taking the sum or mean of time lag 1,2, and 3. 

I need your help here. How can I generate this "aggregated" time lag variable? 

This is how far I've come:

First, my (example) logistic model: 

print( model<- lrm(Y~X+A2, data=mydata))

Second, I created time lags 1,2,3, for the covariate X in the model. 

mydata$lag1X <- Lag(mydata$X, +1) 
mydata$lag2X <- Lag(mydata$X, +2) 
mydata$lag3X <- Lag(mydata$X, +3) 
Not sure how to go further....How do I take the sum or mean of these lag variables? How to create ?aggregated? time lag 1-3? All suggestions are very welcome!

A reproducible example (with lagged variables included )

dput(mydata)
structure(list(Subject = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c("A", 
"B", "C", "D"), class = "factor"), Year = c(1990L, 1991L, 1992L, 
1993L, 1994L, 1995L, 1990L, 1991L, 1992L, 1993L, 1991L, 1992L, 
1993L, 1994L, 1991L, 1992L, 1993L, 1994L, 1995L, 1996L, 1997L
), X = c(1L, 1L, 2L, 3L, 4L, 4L, 0L, 1L, 1L, 2L, 1L, 2L, 3L, 
3L, 1L, 2L, 3L, 4L, 5L, 5L, 6L), A1 = c(1L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L), 
    Y = c(0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
    0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L), A2 = c(0L, 0L, 0L, 0L, 0L, 
    0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 
    0L), lag1X = c(NA, 1L, 1L, 2L, 3L, 4L, 4L, 0L, 1L, 1L, 2L, 
    1L, 2L, 3L, 3L, 1L, 2L, 3L, 4L, 5L, 5L), lag2X = c(NA, NA, 
    1L, 1L, 2L, 3L, 4L, 4L, 0L, 1L, 1L, 2L, 1L, 2L, 3L, 3L, 1L, 
    2L, 3L, 4L, 5L), lag3X = c(NA, NA, NA, 1L, 1L, 2L, 3L, 4L, 
    4L, 0L, 1L, 1L, 2L, 1L, 2L, 3L, 3L, 1L, 2L, 3L, 4L)), .Names = c("Subject", 
"Year", "X", "A1", "Y", "A2", "lag1X", "lag2X", "lag3X"), row.names = c(NA, 
-21L), class = "data.frame")


	[[alternative HTML version deleted]]


From xarose at gmail.com  Fri Jul 15 00:49:26 2016
From: xarose at gmail.com (Christa Rose)
Date: Thu, 14 Jul 2016 18:49:26 -0400
Subject: [R] Please help: cannot import files from Windows into R
Message-ID: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>

Can someone please help? The read.table command is not working for me I
think because Windows has my account name stored as "Christa Rose" with a
space in it. Is this a problem? I have tried many syntax iterations, and
cannot get around this.

Here is an example of the syntax I've used:
> RanMinePlay<-read.table("c:\\Users\\Christa
Rose\\Desktop\\Ran_mine_play_data.txt",header=T)
And returned error messages:
 in read.table("c:\\Users\\Christa Rose\\Desktop\\Ran_mine_play_data.txt",
:
  empty beginning of file
In addition: Warning messages:
1: In read.table("c:\\Users\\Christa
Rose\\Desktop\\Ran_mine_play_data.txt",  :
  line 1 appears to contain embedded nulls
2: In read.table("c:\\Users\\Christa
Rose\\Desktop\\Ran_mine_play_data.txt",  :
  incomplete final line found by readTableHeader on 'c:\Users\Christa
Rose\Desktop\Ran_mine_play_data.txt

Can someone help me resolve this??

Christa Rose



*He is certain too that the cats, of whom there are many in the woods, have
a language of their own - some kind of old Irish.     From "Enchanted
Woods", **William Butler Yeats*



*It is in stone that the most abundant testimony is found of craftsmanship
in medieval Ireland.     From Heritage of Ireland, **Brian de Breffny*

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Jul 15 02:30:02 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 15 Jul 2016 10:30:02 +1000
Subject: [R] Please help: cannot import files from Windows into R
In-Reply-To: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
Message-ID: <CA+8X3fXt7iXFbKuenaTXVcgNkCmpXgCa8OBhaCPe+mLScUY8HQ@mail.gmail.com>

Hi Christa,
The error messages tell you that the file contains NULL characters,
which can cause problems with reading files. You can remove these
characters with a "hex editor". I am not familiar with those used on
Windows, but look at this Web page:

http://alternativeto.net/software/okteta/?platform=windows

The second error message simply tells you that there is no end-of-line
on the final line of your file. Look at the file with the directory
view "Details" and make sure that it is not an empty (zero byte) file.

Jim


On Fri, Jul 15, 2016 at 8:49 AM, Christa Rose <xarose at gmail.com> wrote:
> Can someone please help? The read.table command is not working for me I
> think because Windows has my account name stored as "Christa Rose" with a
> space in it. Is this a problem? I have tried many syntax iterations, and
> cannot get around this.
>
> Here is an example of the syntax I've used:
>> RanMinePlay<-read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",header=T)
> And returned error messages:
>  in read.table("c:\\Users\\Christa Rose\\Desktop\\Ran_mine_play_data.txt",
> :
>   empty beginning of file
> In addition: Warning messages:
> 1: In read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",  :
>   line 1 appears to contain embedded nulls
> 2: In read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",  :
>   incomplete final line found by readTableHeader on 'c:\Users\Christa
> Rose\Desktop\Ran_mine_play_data.txt
>
> Can someone help me resolve this??
>
> Christa Rose
>
>
>
> *He is certain too that the cats, of whom there are many in the woods, have
> a language of their own - some kind of old Irish.     From "Enchanted
> Woods", **William Butler Yeats*
>
>
>
> *It is in stone that the most abundant testimony is found of craftsmanship
> in medieval Ireland.     From Heritage of Ireland, **Brian de Breffny*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Fri Jul 15 02:32:52 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 14 Jul 2016 20:32:52 -0400
Subject: [R] Please help: cannot import files from Windows into R
In-Reply-To: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
Message-ID: <fa7f67ce-01b5-af70-ddb7-bd92bb7c9d32@gmail.com>

On 14/07/2016 6:49 PM, Christa Rose wrote:
> Can someone please help? The read.table command is not working for me I
> think because Windows has my account name stored as "Christa Rose" with a
> space in it. Is this a problem? I have tried many syntax iterations, and
> cannot get around this.

I don't think that is the problem.  The error message
>
> Here is an example of the syntax I've used:
>> RanMinePlay<-read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",header=T)
> And returned error messages:
>  in read.table("c:\\Users\\Christa Rose\\Desktop\\Ran_mine_play_data.txt",
> :
>   empty beginning of file

indicates that it found the file, but is confused by the content of it.

> In addition: Warning messages:
> 1: In read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",  :
>   line 1 appears to contain embedded nulls

This warning suggests that the file is not really a plain text file, but 
is some binary format, e.g. a file produced by Word or similar.

> 2: In read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",  :
>   incomplete final line found by readTableHeader on 'c:\Users\Christa
> Rose\Desktop\Ran_mine_play_data.txt
>
> Can someone help me resolve this??

How did you produce the file Ran_mine_play_data.txt?  If you saved it 
from Word, you need to tell it to save in plain Ascii format.  I don't 
have a copy of Word to look at, so I can't tell you how that is 
described, but it is probably something like "Text (.txt)".

Duncan Murdoch


From jwd at surewest.net  Fri Jul 15 03:13:09 2016
From: jwd at surewest.net (John Dougherty)
Date: Thu, 14 Jul 2016 18:13:09 -0700
Subject: [R] Please help: cannot import files from Windows into R
In-Reply-To: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
Message-ID: <20160714181309.0221138a@draco>

Christa,

As others are noting, the file is not a text file.

Open the file in Notebook or Notepad.  If the editor complains, then
open it in Word.  Save it as a "plain text" file.  You will have to
choose the file type in the "Save As" dialog.  You may run into other
issues such as the field separator.  It is best to limit the file to a
header line containing any field names separated by commas or tabs,
followed by columnar data, again with values separated by the same
sepator used to separate the field names.  Any metadata can be
maintained in a separate text file.  

All of this presumes that your file came from Word or a compatible word
processor.  If it came from some other program and the "txt" type was
simply by you added when you saved the file, we'll need to know what
program the data was created in.

-- 

John


From wdunlap at tibco.com  Fri Jul 15 05:50:44 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 14 Jul 2016 20:50:44 -0700
Subject: [R] Please help: cannot import files from Windows into R
In-Reply-To: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
Message-ID: <CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>

The file may have been stored in the UTF-16 encoding (what Windows Notepad
calls "Unicode").  If so, adding fileEncoding="UTF-16" to the read.table
command might read it correctly.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 14, 2016 at 3:49 PM, Christa Rose <xarose at gmail.com> wrote:

> Can someone please help? The read.table command is not working for me I
> think because Windows has my account name stored as "Christa Rose" with a
> space in it. Is this a problem? I have tried many syntax iterations, and
> cannot get around this.
>
> Here is an example of the syntax I've used:
> > RanMinePlay<-read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",header=T)
> And returned error messages:
>  in read.table("c:\\Users\\Christa Rose\\Desktop\\Ran_mine_play_data.txt",
> :
>   empty beginning of file
> In addition: Warning messages:
> 1: In read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",  :
>   line 1 appears to contain embedded nulls
> 2: In read.table("c:\\Users\\Christa
> Rose\\Desktop\\Ran_mine_play_data.txt",  :
>   incomplete final line found by readTableHeader on 'c:\Users\Christa
> Rose\Desktop\Ran_mine_play_data.txt
>
> Can someone help me resolve this??
>
> Christa Rose
>
>
>
> *He is certain too that the cats, of whom there are many in the woods, have
> a language of their own - some kind of old Irish.     From "Enchanted
> Woods", **William Butler Yeats*
>
>
>
> *It is in stone that the most abundant testimony is found of craftsmanship
> in medieval Ireland.     From Heritage of Ireland, **Brian de Breffny*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Ramgad82 at gmx.net  Fri Jul 15 09:14:59 2016
From: Ramgad82 at gmx.net (Dagmar)
Date: Fri, 15 Jul 2016 09:14:59 +0200
Subject: [R] graph: horizontal bar reflecting number of data
In-Reply-To: <CA+8X3fXNqFi4u3Ots6a96R+3RKdgSZaqcE39i3FqeUWHbO6nmw@mail.gmail.com>
References: <c12faa1e-9f2f-77b0-fc07-adbb1bc609c2@gmx.net>
	<CA+8X3fXNqFi4u3Ots6a96R+3RKdgSZaqcE39i3FqeUWHbO6nmw@mail.gmail.com>
Message-ID: <7e9b54f1-806f-6ec4-a7c2-a9d29e6f6bb5@gmx.net>

Dear all, dear Jim,

Thank you for trying to help Jim. Unfortunately it didn't solve my problem.

I want the names of the weeks on the x axis and the animals on the y-axis.

Then, the shading of the barplot is supposed to represent the number of 
data per week.

Any help?

Dagmar


Am 13.07.2016 um 13:58 schrieb Jim Lemon:
> datframe$numberdata<-as.numeric(as.character(datframe$numberdat))
> library(plotrix)
> barcol<-color.scale(datframe$numberdat,extremes=c("black","white"))
> barplot(matrix(datframe$numberdat,nrow=2,byrow=TRUE),
>   beside=TRUE, horiz=TRUE,names.arg=paste("Week",1:3),
>   col=matrix(barcol,nrow=2,byrow=TRUE))


From ulrik.stervbo at gmail.com  Fri Jul 15 09:31:17 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Fri, 15 Jul 2016 07:31:17 +0000
Subject: [R] graph: horizontal bar reflecting number of data
In-Reply-To: <7e9b54f1-806f-6ec4-a7c2-a9d29e6f6bb5@gmx.net>
References: <c12faa1e-9f2f-77b0-fc07-adbb1bc609c2@gmx.net>
	<CA+8X3fXNqFi4u3Ots6a96R+3RKdgSZaqcE39i3FqeUWHbO6nmw@mail.gmail.com>
	<7e9b54f1-806f-6ec4-a7c2-a9d29e6f6bb5@gmx.net>
Message-ID: <CAKVAULPemDN8UdDT-pezsQGjtPa8LRnf-6OVp_pAHqZovKPqGA@mail.gmail.com>

Dear Dagmar,

must the numberdata be character?

Here are tew solutions. The first solution summarise before plotting and
the second does everything in the plot

library("dplyr")
library("ggplot2")
datframe <- data.frame(Name=c("Kati","Kati","Kati","Leon","Leon","Leon"
), week =c("1","2", "3", "1","2", "3"), numberdata =c(5,12, 1,
6,2, 5))

datframe %>% group_by(week) %>% summarise(numberdata.sum = sum(numberdata))
%>%
ggplot() +
aes(x = week, y = numberdata.sum) +
geom_bar(stat = "identity")

ggplot(datframe) +
aes(x = week, y = ..count.., weights = numberdata) +
geom_bar()

Hope this helps
Ulrik

On Fri, 15 Jul 2016 at 09:18 Dagmar <Ramgad82 at gmx.net> wrote:

> Dear all, dear Jim,
>
> Thank you for trying to help Jim. Unfortunately it didn't solve my problem.
>
> I want the names of the weeks on the x axis and the animals on the y-axis.
>
> Then, the shading of the barplot is supposed to represent the number of
> data per week.
>
> Any help?
>
> Dagmar
>
>
> Am 13.07.2016 um 13:58 schrieb Jim Lemon:
> > datframe$numberdata<-as.numeric(as.character(datframe$numberdat))
> > library(plotrix)
> > barcol<-color.scale(datframe$numberdat,extremes=c("black","white"))
> > barplot(matrix(datframe$numberdat,nrow=2,byrow=TRUE),
> >   beside=TRUE, horiz=TRUE,names.arg=paste("Week",1:3),
> >   col=matrix(barcol,nrow=2,byrow=TRUE))
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Jul 15 09:40:09 2016
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 15 Jul 2016 07:40:09 +0000
Subject: [R]
 =?utf-8?q?How_to_generate_=E2=80=9Caggregated=E2=80=9D_time_l?=
 =?utf-8?q?ags=3F?=
In-Reply-To: <E7A3A0EB-7571-4796-BBE4-3BB5762B82C3@gmail.com>
References: <E7A3A0EB-7571-4796-BBE4-3BB5762B82C3@gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C50363ED@SRVEXCHMBX.precheza.cz>

Hi

see in line

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Faradj
> Koliev
> Sent: Friday, July 15, 2016 1:19 AM
> To: r-help at r-project.org
> Subject: [R] How to generate ?aggregated? time lags?
>
> Dear all,
>
> I hope you?re enjoying your summer!
>
>
> I've been asked to "aggregate" my time lags from simple 1 year time lag to 1-
> 3 year time lag. This could be done --I've been told --by simply taking the sum
> or mean of time lag 1,2, and 3.
>
> I need your help here. How can I generate this "aggregated" time lag
> variable?
>
> This is how far I've come:
>
> First, my (example) logistic model:
>
> print( model<- lrm(Y~X+A2, data=mydata))
>
> Second, I created time lags 1,2,3, for the covariate X in the model.
>
> mydata$lag1X <- Lag(mydata$X, +1)
> mydata$lag2X <- Lag(mydata$X, +2)
> mydata$lag3X <- Lag(mydata$X, +3)
> Not sure how to go further....How do I take the sum or mean of these lag
> variables? How to create ?aggregated? time lag 1-3? All suggestions are very
> welcome!

Thanks for example, it helps to figure out your intention.

rowMeans(mydata[, 7:9], na.rm=T)
rowSums(mydata[, 7:9], na.rm=T)

You can also get the result also directly from X variable

rowSums(embed(c(rep(NA, 3),mydata$X),3), na.rm=T)[1:nrow(mydata)]

Cheers
Petr

>
> A reproducible example (with lagged variables included )
>
> dput(mydata)
> structure(list(Subject = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 3L, 3L,
> 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c("A", "B", "C", "D"), class = "factor"),
> Year = c(1990L, 1991L, 1992L, 1993L, 1994L, 1995L, 1990L, 1991L, 1992L, 1993L,
> 1991L, 1992L, 1993L, 1994L, 1991L, 1992L, 1993L, 1994L, 1995L, 1996L, 1997L ), X
> = c(1L, 1L, 2L, 3L, 4L, 4L, 0L, 1L, 1L, 2L, 1L, 2L, 3L, 3L, 1L, 2L, 3L, 4L, 5L, 5L, 6L), A1
> = c(1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L),
>     Y = c(0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L,
>     0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L), A2 = c(0L, 0L, 0L, 0L, 0L,
>     0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L,
>     0L), lag1X = c(NA, 1L, 1L, 2L, 3L, 4L, 4L, 0L, 1L, 1L, 2L,
>     1L, 2L, 3L, 3L, 1L, 2L, 3L, 4L, 5L, 5L), lag2X = c(NA, NA,
>     1L, 1L, 2L, 3L, 4L, 4L, 0L, 1L, 1L, 2L, 1L, 2L, 3L, 3L, 1L,
>     2L, 3L, 4L, 5L), lag3X = c(NA, NA, NA, 1L, 1L, 2L, 3L, 4L,
>     4L, 0L, 1L, 1L, 2L, 1L, 2L, 3L, 3L, 1L, 2L, 3L, 4L)), .Names = c("Subject", "Year",
> "X", "A1", "Y", "A2", "lag1X", "lag2X", "lag3X"), row.names = c(NA, -21L), class
> = "data.frame")
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From drjimlemon at gmail.com  Fri Jul 15 10:27:34 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 15 Jul 2016 18:27:34 +1000
Subject: [R] graph: horizontal bar reflecting number of data
In-Reply-To: <7e9b54f1-806f-6ec4-a7c2-a9d29e6f6bb5@gmx.net>
References: <c12faa1e-9f2f-77b0-fc07-adbb1bc609c2@gmx.net>
	<CA+8X3fXNqFi4u3Ots6a96R+3RKdgSZaqcE39i3FqeUWHbO6nmw@mail.gmail.com>
	<7e9b54f1-806f-6ec4-a7c2-a9d29e6f6bb5@gmx.net>
Message-ID: <CA+8X3fVCSRbJRn4MXiET4iS+HGR3DLm_C+OWGt_NmyM3YDivHg@mail.gmail.com>

Hi Dagmar,
Maybe your want something like this?

datframe<-data.frame(Name=c("Kati","Kati","Kati","Leon","Leon","Leon" ),
 week =c("1","2", "3", "1","2", "3"),
 numberdata =c("5","12", "1", "6","2", "5"))
datframe
datframe$numberdata<-as.numeric(as.character(datframe$numberdat))
library(plotrix)
barcol<-color.scale(datframe$numberdat,extremes=c("blue","red"))
barpos<-barplot(matrix(datframe$numberdat,nrow=2,byrow=TRUE),
  beside=TRUE,names.arg=paste("Week",1:3),ylim=c(0,13),
  col=matrix(barcol,nrow=2,byrow=TRUE))
barlabels(barpos,matrix(datframe$numberdat,nrow=2,byrow=TRUE))
barlabels(barpos,matrix(datframe$numberdat+0.4,nrow=2,byrow=TRUE),
 labels=rep(c("Kati","Leon"),3),prop=1)

I don't see any other way to put the names on the y-axis that makes any sense.

Jim


On Fri, Jul 15, 2016 at 5:14 PM, Dagmar <Ramgad82 at gmx.net> wrote:
> Dear all, dear Jim,
>
> Thank you for trying to help Jim. Unfortunately it didn't solve my problem.
>
> I want the names of the weeks on the x axis and the animals on the y-axis.
>
> Then, the shading of the barplot is supposed to represent the number of data
> per week.
>
> Any help?
>
> Dagmar
>
>
> Am 13.07.2016 um 13:58 schrieb Jim Lemon:
>>
>> datframe$numberdata<-as.numeric(as.character(datframe$numberdat))
>> library(plotrix)
>> barcol<-color.scale(datframe$numberdat,extremes=c("black","white"))
>> barplot(matrix(datframe$numberdat,nrow=2,byrow=TRUE),
>>   beside=TRUE, horiz=TRUE,names.arg=paste("Week",1:3),
>>   col=matrix(barcol,nrow=2,byrow=TRUE))
>
>


From R.E.Crump at warwick.ac.uk  Fri Jul 15 12:48:27 2016
From: R.E.Crump at warwick.ac.uk (Crump, Ron)
Date: Fri, 15 Jul 2016 10:48:27 +0000
Subject: [R] graph: horizontal bar reflecting number of data
Message-ID: <D3AE7E0A.6A64%lfslbx@live.warwick.ac.uk>

Hi Dagmar,

<quote>
I want the names of the weeks on the x axis and the animals on the y-axis.

Then, the shading of the barplot is supposed to represent the number of
data
per week.
</quote>


If I understand the above correctly, and using the example dataset
constructed by Ulrik:

datframe <- data.frame(Name=c("Kati","Kati","Kati","Leon","Leon","Leon"
+ ), week =c("1","2", "3", "1","2", "3"), numberdata =c(5,12, 1,
+ 6,2, 5))

I think this might do the job:

library(ggplot)
ggplot(datframe)+geom_tile(aes(x=week,y=Name,fill=numberdata))


Regards,
Ron.


From mridulgarg11 at tamu.edu  Fri Jul 15 08:13:44 2016
From: mridulgarg11 at tamu.edu (mridulgarg11 at tamu.edu)
Date: Fri, 15 Jul 2016 01:13:44 -0500
Subject: [R]  Coefficients: (20 not defined because of singularities)
Message-ID: <C9593BC9-BAA9-4710-88D1-71AC2E7B54CD@tamu.edu>

Hello Sir

I came across an old post of your discussing the singularity of the model matrix, where the data matrix suffers from multicollinearity. You mentioned that using a control variable with binary indicator is a possible option for that.

I wanted to ask you what is the correct remedy in case where we do not want to drop the collinear variables? Actually, I?m trying to estimate the coefficient of the binary variable, and it?s the same in both cases: including and not including interaction terms. Should I interpret the coefficient of the binary variable as the treatment effect or do I find another way to produce different results in both cases?


From bbolker at gmail.com  Fri Jul 15 16:23:51 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Fri, 15 Jul 2016 14:23:51 +0000
Subject: [R] glmmLasso with interactions errors
References: <CAAb_rUmWQZVQqnABGaEdz3M9Dmh8xQvjEzhQ4c1K5v7_c++isA@mail.gmail.com>
	<CAM5M9BTwzukAvLeRXdiuBxa+xmkCnf4CyzOXE5w23h2bf8=yjg@mail.gmail.com>
Message-ID: <loom.20160715T160901-734@post.gmane.org>

Cade, Brian <cadeb <at> usgs.gov> writes:

> 
> It has never been obvious to me that the lasso approach can handle
> interactions among predictor variables well at all. 
> I'ld be curious to see
> what others think and what you learn.
> 
> Brian
> 

  For what it's worth I think lasso *does* handle interactions
reasonably (although I forget where I read that) -- there is a
newer "hierarchical lasso" that tries to deal with marginality
concerns more carefully.

  Related questions asked on StackOverflow:

http://stackoverflow.com/questions/37910042/glmmlasso-warning-messages/
  37922918#37922918
(warning, broken URL)

My answer (in comments) there was

my guess is that you're going to have to build your own model
matrix/dummy variables; I think that as.factor() in formulas is
treated specially, so including the interaction term will probably
just confuse it. (It would be worth trying as.factor(Novelty:ROI) - I
doubt it'll work but if it does it would be the easiest way forward.)


> 
> On Wed, Jul 13, 2016 at 2:20 PM, Walker Pedersen <wsp <at> uwm.edu> wrote:

[snip]

> >
> > An abbreviated version of my dataset is here:
> >
> > https://drive.google.com/open?id=0B_LliPDGUoZbVVFQS2VOV3hGN3c
> >

[snip snip]

> > Before glmmLasso I am running:
> >
> > KNov$Subject <- factor(KNov$Subject)
> >
> > to ensure the subject ID is not treated as a continuous variable.
> >
> > If I run:
> >
> > glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
> > STAIt + as.factor(ROI)
> > + as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov,
> > lambda=10)
> > summary(glm1)
> >
> > I don't get any warning messages, but the output contains b estimates
> > only, no SE or p-values.
> >
> > If I try to include a 3-way interaction, such as:
> >
> > glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
> > STAIt + as.factor(ROI)
> > + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
> > list(Subject=~1), data = Nov7T, lambda=10)
> > summary(glm2)
> >
> > I get the warnings:
> >
> > Warning messages:
> > 1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
> >   data length is not a multiple of split variable
> > 2: In lambda_vec * sqrt(block2) :
> >   longer object length is not a multiple of shorter object length
> >
> > And again, I do get parameter estimates, and no SE or p-values.
> >
> > If I include my continuous variable in any interaction, such as:
> >
> > glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
> > STAIt + as.factor(ROI)
> > + as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
> > list(Subject=~1), data = Nov7T, lambda=10)
> > summary(glm3)
> >
> > I get the error message:
> >
> > Error in rep(control$index[i], length.fac) : invalid 'times' argument
> >
> > and no output.
> >
> > If anyone has an input as to (1) why I am not getting SE or p-values
> > in my outputs (2) the meaning of there warnings I get when I include a
> > 3-way variable, and if they are something to worry about, how to fix
> > them and (3) how to fix the error message I get when I include my
> > continuous factor in an interatction, I would be very appreciative.


 [snip snip snip]


From drsimonjackson at gmail.com  Fri Jul 15 01:04:13 2016
From: drsimonjackson at gmail.com (Simon Jackson)
Date: Fri, 15 Jul 2016 09:04:13 +1000
Subject: [R] [R-pkgs] New package: corrr 0.1.0
Message-ID: <002501d1de24$0bea7a00$23bf6e00$@gmail.com>

Dear R users,

 

I'm glad to announce the release of a new package on CRAN for exploring
correlations in R: corrr
<https://cran.rstudio.com/web/packages/corrr/index.html> .

 

corrr provides a handful of methods to create and explore a correlation tbl
(rather than a matrix). E.g., rearrange() the correlations based on their
strength, or focus() on selected variables against others. All methods are
designed to work in data pipelines, and the tbl (data frame) structure
allows for custom exploration using packages like dplyr.

 

corrr is my first package on CRAN, and I'd greatly appreciate any feedback,
suggestions, or contributions on Github: https://github.com/drsimonj/corrr

 

Enjoy,

 

Simon

SIMON A JACKSON | Postdoctoral Research Fellow
Cognitive and Decision Sciences Research Lab (CODES), School of Psychology

THE UNIVERSITY OF SYDNEY
Rm 440, Brennan MacCallum Building A18 | The University of Sydney | NSW |
2006
drsimonjackson at gmail.com <mailto:drsimonjackson at gmail.com>   | @drsimonj
<https://twitter.com/drsimonj> 

 

 


	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From wdunlap at tibco.com  Fri Jul 15 18:46:37 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 15 Jul 2016 09:46:37 -0700
Subject: [R] Selecting 1st and last dates from a set of dates
In-Reply-To: <3BB657B92C75C74385A95FAA7C6B17161F0C72261E@VRTPRDEXM02.royallondongroup.com>
References: <3BB657B92C75C74385A95FAA7C6B17161F0C60B5A9@VRTPRDEXM02.royallondongroup.com>
	<D1BA5F04-BDC1-4405-9C0C-9F4AE8FBE6C5@dcn.davis.ca.us>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B649@VRTPRDEXM02.royallondongroup.com>
	<CAF8bMcYcXQtprEjNJPzFS3HCKr-zckZ5nEKmnpUpB1HkpdS08w@mail.gmail.com>
	<3BB657B92C75C74385A95FAA7C6B17161F0C60B668@VRTPRDEXM02.royallondongroup.com>
	<F7E6D18CC2877149AB5296CE54EA27663097DA84@WAXMXOLYMB025.WAX.wa.lcl>
	<CAF8bMcbt_OiyVTDOPARKh8gGR8g7GroGzx9o3JhUuUiSv-+Jeg@mail.gmail.com>
	<3BB657B92C75C74385A95FAA7C6B17161F0C72261E@VRTPRDEXM02.royallondongroup.com>
Message-ID: <CAF8bMcY_=8O1VZb5FsmYVt=+TiT9eALWWDk3hFr1aKCzohm8yA@mail.gmail.com>

A problem with finding the last but 1, or the 3rd, etc., element in
a run is what to do with runs too short to have such an element.
The following 2 functions just ignore such runs, so you would probably
want to also drop the short runs from the output of isFirstInRun.

isLastButOneInRun <- function (x)
{
    L <- isLastInRun(x)
    retval <- logical(length(x))
    retval[L] <- FALSE
    retval[!L] <- isLastInRun(x[!L])
    retval
}
isLastButNInRun <- function (x, n)
{
    rev(sequence(rle(rev(x))$lengths)) == n + 1
}

There are lots of other ways to do this.



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Jul 15, 2016 at 2:43 AM, Mehta, Gaurang <
Gaurang.Mehta at royallondon.com> wrote:

> Hi William, on below lets say I want to arrange the data using last but 1
> day of the month, what modification would be required. I have tried the
> following but no help.
> isFirstInRun <- function(x) c(TRUE, x[-1] != x[-length(x)])
> # isLastInRun <- function(x) c(x[-1] != x[-length(x)], TRUE)
> isLastInRun <- function(x) c(x[-1] != x[-length(x)-1], TRUE)
>
> It would be great if you could help here. I think I am nearly there but
> not quite.
> Regards,
> Gaurang Mehta
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of William
> Dunlap via R-help
> Sent: 14 July 2016 17:32
> To: Nordlund, Dan (DSHS/RDA)
> Cc: R-help Mailing List
> Subject: Re: [R] Selecting 1st and last dates from a set of dates
>
> I did not use aggregate because it did not make it convenient to return
> the rest of the row with the min or max data in it.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, Jul 14, 2016 at 9:22 AM, Nordlund, Dan (DSHS/RDA) <
> NordlDJ at dshs.wa.gov> wrote:
>
> > Using William Dunlap's data, here is another alternative:
> >
> > library(zoo)
> > aggregate(d$Date,list(as.yearmon(d$Date)),min)
> > aggregate(d$Date,list(as.yearmon(d$Date)),max)
> >
> >
> > Hope this is helpful,
> >
> > Dan
> >
> > Daniel Nordlund, PhD
> > Research and Data Analysis Division
> > Services & Enterprise Support Administration Washington State
> > Department of Social and Health Services
> >
> >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> > > Mehta, Gaurang
> > > Sent: Thursday, July 14, 2016 9:01 AM
> > > To: William Dunlap
> > > Cc: R-help Mailing List
> > > Subject: Re: [R] Selecting 1st and last dates from a set of dates
> > >
> > > Thanks William.
> > > This works. Thanks again.
> > >
> > > From: William Dunlap [mailto:wdunlap at tibco.com]
> > > Sent: 14 July 2016 16:42
> > > To: Mehta, Gaurang
> > > Cc: Jeff Newmiller; R-help Mailing List
> > > Subject: Re: [R] Selecting 1st and last dates from a set of dates
> > >
> > > Does the following example help?  isFirstInRun() and isLastInRun()
> > > are
> > handy
> > > utility functions.
> > >
> > > > d <- transform(data.frame(Date=as.Date(c("2016-01-05",
> > > > "2016-03-04", "2016-03-30", "2015-12-02", "2016-03-04",
> > > > "2015-12-21"))), DaysSince1970=as.integer(Date),
> > > > I=seq_along(Date)) d
> > >         Date DaysSince1970 I
> > > 1 2016-01-05         16805 1
> > > 2 2016-03-04         16864 2
> > > 3 2016-03-30         16890 3
> > > 4 2015-12-02         16771 4
> > > 5 2016-03-04         16864 5
> > > 6 2015-12-21         16790 6
> > > > isFirstInRun <- function(x) c(TRUE, x[-1] != x[-length(x)])
> > > > isLastInRun <- function(x) c(x[-1] != x[-length(x)], TRUE) ds <-
> > > > d[order(d$Date),] ds
> > >         Date DaysSince1970 I
> > > 4 2015-12-02         16771 4
> > > 6 2015-12-21         16790 6
> > > 1 2016-01-05         16805 1
> > > 2 2016-03-04         16864 2
> > > 5 2016-03-04         16864 5
> > > 3 2016-03-30         16890 3
> > > > ds[isFirstInRun(format(ds$Date, "%Y-%m")),]
> > >         Date DaysSince1970 I
> > > 4 2015-12-02         16771 4
> > > 1 2016-01-05         16805 1
> > > 2 2016-03-04         16864 2
> > > > ds[isLastInRun(format(ds$Date, "%Y-%m")),]
> > >         Date DaysSince1970 I
> > > 6 2015-12-21         16790 6
> > > 1 2016-01-05         16805 1
> > > 3 2016-03-30         16890 3
> > >
> > >
> > > Bill Dunlap
> > > TIBCO Software
> > > wdunlap tibco.com<http://tibco.com>
> > >
> > > On Thu, Jul 14, 2016 at 8:26 AM, Mehta, Gaurang
> > > <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.c
> > > om>> wrote:
> > > Hi Jeff,
> > > I would say my problem is what you described in 2 below. My data is
> > > as
> > > follows:
> > > Date    UC11    UC12    UC13
> > > 02/01/1997      1       2       0
> > > 03/01/1997      5       6       3
> > > 06/01/1997      5       4       6
> > > 07/01/1997      6       4       3
> > > 08/01/1997      6       5       5
> > > 09/01/1997      7       6       8
> > > 10/01/1997      8       5       5
> > > 13/01/1997      8       6       5
> > > 14/01/1997      7       4       4
> > > 15/01/1997      6       3       3
> > > 16/01/1997      8       5       5
> > > 17/01/1997      6       4       3
> > > 20/01/1997      5       4       2
> > > 21/01/1997      7       5       5
> > > 22/01/1997      16      12      12
> > > 23/01/1997      5       3       4
> > > 24/01/1997      5       2       2
> > > 27/01/1997      8       4       5
> > > 28/01/1997      7       5       9
> > > 29/01/1997      4       4       4
> > > 30/01/1997      4       4       6
> > > 31/01/1997      9       7       8
> > > 03/02/1997      9       6       8
> > >
> > >
> > > I want to select the data on the first date it can be 1st , 2nd or
> > > 3rd
> > or any and
> > > last date it can be 31st, 30th and /or29th. I don?t need time.
> > > It would be great if you could help.
> > > Regards,
> > > Gaurang Mehta
> > >
> > >
> > >
> > > -----Original Message-----
> > > From: Jeff Newmiller
> > > [mailto:jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>]
> > > Sent: 14 July 2016 16:03
> > > To: Mehta, Gaurang; R-help Mailing List
> > > Subject: Re: [R] Selecting 1st and last dates from a set of dates
> > >
> > > I suspect the answer to your question (is there a function...) is
> > > almost certainly yes, but your question is too vague to be sure.
> > >
> > > 1) Data frames and matrices are different in important ways... it is
> > highly
> > > unlikely that matrices would be appropriate for date data.
> > >
> > > 2) Do you mean "select records with earliest date in each month" or
> > "select
> > > records whose day of month is 1"? If you need to work with time of
> > > day along with date then the solution will be different than if you
> > > are
> > working
> > > with date only.
> > >
> > > 3) Have you converted your dates to Date or POSIXct or chron already?
> > > Which?
> > >
> > > 4) There are a lot of useful functions in base R [1][2], as well as
> > contributed
> > > packages such as chron and lubridate.
> > >
> > > A reproducible example [3] is the standard way to communicate what
> > > problem you actually have.  In particular, including the output of
> > > dput
> > for a
> > > representative sample of data is a key element of that example.
> > >
> > > [1] ?DateTimeClasses
> > > [2] R News 4/1 p29
> > > [3]
> > > http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-
> > > reproducible-example
> > > --
> > > Sent from my phone. Please excuse my brevity.
> > >
> > > On July 14, 2016 5:44:52 AM PDT, "Mehta, Gaurang"
> > > <Gaurang.Mehta at royallondon.com<mailto:Gaurang.Mehta at royallondon.c
> > > om>> wrote:
> > > >Hi Team,
> > > >I am struggling to select the first date and last date of the month
> > > >where there is data in a dataframe/matrix.
> > > >Is there any r function that can help to easily select data on the
> > > >first and last day of the month from the given set of dates and
> values.
> > > >Thanks for the help in advance.
> > > >Regards,
> > > >Gaurang
> > > >
> > > >
> > > >This email is intended for the person or company named and access
> > > >by anyone else is unauthorised. If you are not the person or
> > > >company named, please delete this email and notify the sender.
> > > >
> > > >The information in this email, including any attachments, may be
> > > >confidential or legally privileged (meaning that its disclosure is
> > > >protected in law). Its unauthorised disclosure, copying,
> > > >distribution or use is prohibited and may be unlawful.
> > > >
> > > >Email communications sent over the internet are not guaranteed to
> > > >be secure or virus-free and such messages are potentially at risk.
> > > >The Royal London Group accepts no liability for any claims arising
> > > >from use of the internet to transmit messages by or to any company
> > > >within the Royal London Group.
> > > >
> > > >The Royal London Group consists of The Royal London Mutual
> > > >Insurance Society Limited and its subsidiaries.
> > > >
> > > >The Royal London Mutual Insurance Society Limited is authorised by
> > > >the Prudential Regulation Authority and regulated by the Financial
> > > >Conduct Authority and the Prudential Regulation Authority and
> > > >provides life assurance and pensions.
> > > >
> > > >Registered in England and Wales number 99064.
> > > >
> > > >Registered office: 55 Gracechurch Street, London, EC3V 0RL.
> > > >
> > > >In the Republic of Ireland: The Royal London Mutual Insurance
> > > >Society Limited is authorised by the Prudential Regulation
> > > >Authority in the UK and is regulated by the Central Bank of Ireland
> > > >for conduct of business rules.
> > > >
> > > >
> > > >       [[alternative HTML version deleted]]
> > > >
> > > >______________________________________________
> > > >R-help at r-project.org<mailto:R-help at r-project.org> mailing list --
> > > >To UNSUBSCRIBE and more, see
> > > >https://stat.ethz.ch/mailman/listinfo/r-help
> > > >PLEASE do read the posting guide
> > > >http://www.R-project.org/posting-guide.html
> > > >and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> > > UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > guide.html and provide commented, minimal, self-contained,
> > > reproducible code.
> > >
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > guide.html and provide commented, minimal, self-contained,
> > > reproducible code.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Ramgad82 at gmx.net  Fri Jul 15 22:29:38 2016
From: Ramgad82 at gmx.net (Dagmar)
Date: Fri, 15 Jul 2016 22:29:38 +0200
Subject: [R] graph: horizontal bar reflecting number of data
In-Reply-To: <D3AE7E0A.6A64%lfslbx@live.warwick.ac.uk>
References: <D3AE7E0A.6A64%lfslbx@live.warwick.ac.uk>
Message-ID: <3440bbff-eb3b-5d18-d408-587f29c91c76@gmx.net>

Ron: That was exactly what I was looking for!

Thank you Ron!

Also thanks to Ulrik and Jim who tried to help. I learned a lot!

Dagmar


Am 15.07.2016 um 12:48 schrieb Crump, Ron:
> Hi Dagmar,
>
> <quote>
> I want the names of the weeks on the x axis and the animals on the y-axis.
>
> Then, the shading of the barplot is supposed to represent the number of
> data
> per week.
> </quote>
>
>
> If I understand the above correctly, and using the example dataset
> constructed by Ulrik:
>
> datframe <- data.frame(Name=c("Kati","Kati","Kati","Leon","Leon","Leon"
> + ), week =c("1","2", "3", "1","2", "3"), numberdata =c(5,12, 1,
> + 6,2, 5))
>
> I think this might do the job:
>
> library(ggplot)
> ggplot(datframe)+geom_tile(aes(x=week,y=Name,fill=numberdata))
>
>
> Regards,
> Ron.
>
>


From satish.vadlamani at gmail.com  Fri Jul 15 22:43:16 2016
From: satish.vadlamani at gmail.com (Satish Vadlamani)
Date: Fri, 15 Jul 2016 13:43:16 -0700
Subject: [R] How to group by and get distinct rows of of grouped rows
 based on certain criteria
In-Reply-To: <CAF8bMcbsm231nMXhOarn_v46JHm5RYCbs-jm3qxdodS=LiXUSQ@mail.gmail.com>
References: <CAK3+11Ri7C-x+GJh2GCHu=xWeRrjTTYH3FU6UNdEGbtOxDrVEg@mail.gmail.com>
	<CAF8bMcbsm231nMXhOarn_v46JHm5RYCbs-jm3qxdodS=LiXUSQ@mail.gmail.com>
Message-ID: <CAK3+11S-q_tW1gmoy5eBYvn3POZeVuo+8R77-JPedOhVTGiP2Q@mail.gmail.com>

Thank you Bill and Sarah for your help. I was able to do the same with
dplyr with the following code. But I could not post this since at that time
my message was not posted yet.

>>
file1 <- select(file1, ATP.Group,Business.Event,Category)

file1_1 <- file1  %>% group_by(ATP.Group,Business.Event) %>%
filter(Category == "EQ") %>% distinct(ATP.Group,Business.Event)
file1_1 <- as.data.frame(file1_1)
file1_1

file1_2 <- file1  %>% group_by(ATP.Group,Business.Event) %>%
distinct(ATP.Group,Business.Event)
file1_2 <- as.data.frame(file1_2)
file1_2

setdiff(select(file1_2,ATP.Group,Business.Event),
select(file1_1,ATP.Group,Business.Event))
>>

On Thu, Jul 14, 2016 at 1:53 PM, William Dunlap <wdunlap at tibco.com> wrote:

> > txt <- "|ATP Group|Business Event|Category|
> |02       |A             |AC      |
> |02       |A             |AD      |
> |02       |A             |EQ      |
> |ZM       |A             |AU      |
> |ZM       |A             |AV      |
> |ZM       |A             |AW      |
> |02       |B             |AC      |
> |02       |B             |AY      |
> |02       |B             |EQ      |
> "
> > d <- read.table(sep="|", text=txt, header=TRUE, strip.white=TRUE,
> check.names=FALSE)[,2:4]
> > str(d)
> 'data.frame':   9 obs. of  3 variables:
>  $ ATP Group     : Factor w/ 2 levels "02","ZM": 1 1 1 2 2 2 1 1 1
>  $ Business Event: Factor w/ 2 levels "A","B": 1 1 1 1 1 1 2 2 2
>  $ Category      : Factor w/ 7 levels "AC","AD","AU",..: 1 2 7 3 4 5 1 6 7
> > unique(d[d[,"Category"]!="EQ", c("ATP Group", "Business Event")])
>   ATP Group Business Event
> 1        02              A
> 4        ZM              A
> 7        02              B
> > unique(d[d[,"Category"]=="EQ", c("ATP Group", "Business Event")])
>   ATP Group Business Event
> 3        02              A
> 9        02              B
>
> Some folks prefer to use subset() instead of "[".  The previous expression
> is equivalent to:
>
> > unique( subset(d, Category=="EQ", c("ATP Group", "Business Event")))
>   ATP Group Business Event
> 3        02              A
> 9        02              B
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, Jul 14, 2016 at 12:43 PM, Satish Vadlamani <
> satish.vadlamani at gmail.com> wrote:
>
>> Hello All:
>> I would like to get your help on the following problem.
>>
>> I have the following data and the first row is the header. Spaces are not
>> important.
>> I want to find out distinct combinations of ATP Group and Business Event
>> (these are the field names that you can see in the data below) that have
>> the Category EQ (Category is the third field) and those that do not have
>> the category EQ. In the example below, the combinations 02/A and 02/B have
>> EQ and the combination ZM/A does not.
>>
>> If I have a larger file, how to get to this answer?
>>
>> What did I try (with dplyr)?
>>
>> # I know that the below is not correct and not giving desired results
>> file1_1 <- file1  %>% group_by(ATP.Group,Business.Event) %>%
>> filter(Category != "EQ") %>% distinct(ATP.Group,Business.Event)
>> # for some reason, I have to convert to data.frame to print the data
>> correctly
>> file1_1 <- as.data.frame(file1_1)
>> file1_1
>>
>>
>> *Data shown below*
>> |ATP Group|Business Event|Category|
>> |02       |A             |AC      |
>> |02       |A             |AD      |
>> |02       |A             |EQ      |
>> |ZM       |A             |AU      |
>> |ZM       |A             |AV      |
>> |ZM       |A             |AW      |
>> |02       |B             |AC      |
>> |02       |B             |AY      |
>> |02       |B             |EQ      |
>>
>> --
>>
>> Satish Vadlamani
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 

Satish Vadlamani

	[[alternative HTML version deleted]]


From fernando.marmolejo.ramos at psychology.su.se  Sat Jul 16 14:47:08 2016
From: fernando.marmolejo.ramos at psychology.su.se (Fernando Marmolejo Ramos)
Date: Sat, 16 Jul 2016 12:47:08 +0000
Subject: [R] dependent p.values in R
In-Reply-To: <fb9f9102-7c32-c6b9-5c2a-af1b38c2dbc2@yorku.ca>
References: <42b2dedbb7a9458d97dcaf3677776693@ebox-prod-srv07.win.su.se>
	<b0ea1652-c9d3-6865-5057-de0064c00ae2@u-psud.fr>
	<1468136385886.74139@psychology.su.se>,
	<fb9f9102-7c32-c6b9-5c2a-af1b38c2dbc2@yorku.ca>
Message-ID: <1468673227817.31529@psychology.su.se>

dear michael

thanks for your input

i do agree in visual tests (indeed a recent paper in TAS=http://amstat.tandfonline.com/doi/abs/10.1080/00031305.2015.1077728 )

as a matter of fact, the test i'm after is simply for comparative purposes with some visualisation techniques

best

f

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Fernando Marmolejo-Ramos
Postdoctoral Fellow
G?sta Ekman Laboratory
Department of Psychology
Stockholm University
Frescati Hagv?g 9A, Stockholm 114 19
Sweden

ph = +46 08-16 46 07
website = http://sites.google.com/site/fernandomarmolejoramos/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

________________________________________
From: Michael Friendly <friendly at yorku.ca>
Sent: Sunday, 10 July 2016 7:51 PM
To: Fernando Marmolejo Ramos; Marc Girondot; r-help at r-project.org
Subject: Re: dependent p.values in R

Hello Fernando

First, ask yourself what Gosta Ekman would have said if you asked him
this question.  He would have asked "does it make any difference to
your conclusion?"  He might also have asked you "Did you do a visual
test?"  Plot your data as a QQ plot or density plot?

If the test doesn't make a difference in conclusions, it is a waste of
your time (and ours) to worry about how to cite a
'combined p.value' (if such an animal exists), presumably to
more decimal places than is worth worrying about.

If the test *does* make a difference about normality, then ask yourself
does the degree of non-normality impede my substantive conclusions.

HTH,
-ichael

On 7/10/16 3:39 AM, Fernando Marmolejo Ramos wrote:
> hi marc
>
> say i have a vector with some x number of observations
>
> x = c(23, 56, 123, ..... )
>
> and i want to know how normal it is
>
> as there are many normality tests, i want to combine their p.values
>
> so, suppose i use shapiro.wilk, anderson darling and jarque bera and each will give a pvalue
>
> i could simply average those p,values but to my knowledge that approach is biased
>
> so i thought, in the same way there is a method to combine independent pvalues (e.g. stouffer method); is there a way to combine dependent pvalues?
>
> best
>
> f
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Fernando Marmolejo-Ramos
> Postdoctoral Fellow
> G?sta Ekman Laboratory
> Department of Psychology
> Stockholm University
> Frescati Hagv?g 9A, Stockholm 114 19
> Sweden
>
> ph = +46 08-16 46 07
> website = http://sites.google.com/site/fernandomarmolejoramos/
>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
> ________________________________________
> From: Marc Girondot <marc.girondot at u-psud.fr>
> Sent: Sunday, 10 July 2016 8:25 AM
> To: r-help at r-project.org; Fernando Marmolejo Ramos
> Subject: Re: [R] dependent p.values
>
> Le 09/07/2016 ? 17:17, Fernando Marmolejo Ramos a ?crit :
>> hi all
>>
>>
>> does any one know a method to combine dependent p.values?
>>
>>
> First, it is a stats question and not a R question. So you could have
> better chance to ask this in stackexchange forum.
> Second, your question is difficult to answer without context: why
> p.values are dependent ? Do they come from the same dataset ? Or are
> they linked by an external source ? For both these situations, combining
> dependent p.values seems strange for me.
> When you will ask question in stackexchange, be more precise.
> Sincerely,
> Marc Girondot
>
> --
> __________________________________________________________
> Marc Girondot, Pr
>
> Laboratoire Ecologie, Syst?matique et Evolution
> Equipe de Conservation des Populations et des Communaut?s
> CNRS, AgroParisTech et Universit? Paris-Sud 11 , UMR 8079
> B?timent 362
> 91405 Orsay Cedex, France
>
> Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1.69.15.73.53
> e-mail: marc.girondot at u-psud.fr
> Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
> Skype: girondot
>
>



From wsp at uwm.edu  Sat Jul 16 18:29:01 2016
From: wsp at uwm.edu (Walker Pedersen)
Date: Sat, 16 Jul 2016 11:29:01 -0500
Subject: [R] glmmLasso with interactions errors
In-Reply-To: <CAM5M9BTwzukAvLeRXdiuBxa+xmkCnf4CyzOXE5w23h2bf8=yjg@mail.gmail.com>
References: <CAAb_rUmWQZVQqnABGaEdz3M9Dmh8xQvjEzhQ4c1K5v7_c++isA@mail.gmail.com>
	<CAM5M9BTwzukAvLeRXdiuBxa+xmkCnf4CyzOXE5w23h2bf8=yjg@mail.gmail.com>
Message-ID: <CAAb_rUnCAoRNhP5W17L-66Yq20DhWt-MvSF6RKaA3vPEa-wzaQ@mail.gmail.com>

Thank you for the input Brian and Ben.

It is odd how it seems to handle a two way interaction fine (as long
as the continuous variable is not in the mix), but not a 3-way.

In any case would anyone be able to give me a rundown of how I would
create a matrix/dummy variable for these interactions to input into
glmmLASSO?

Alternatively, is there a method for paring down a model that is a bit
less sketchy than simple backfitting, that you would expect to be more
straight forward software-wise?

Thanks!

Walker

UW-MKE

On Thu, Jul 14, 2016 at 10:08 AM, Cade, Brian <cadeb at usgs.gov> wrote:
> It has never been obvious to me that the lasso approach can handle
> interactions among predictor variables well at all.  I'ld be curious to see
> what others think and what you learn.
>
> Brian
>
> Brian S. Cade, PhD
>
> U. S. Geological Survey
> Fort Collins Science Center
> 2150 Centre Ave., Bldg. C
> Fort Collins, CO  80526-8818
>
> email:  cadeb at usgs.gov
> tel:  970 226-9326
>
>
> On Wed, Jul 13, 2016 at 2:20 PM, Walker Pedersen <wsp at uwm.edu> wrote:
>>
>> Hi Everyone,
>>
>> I am having trouble running glmmLasso.
>>
>> An abbreviated version of my dataset is here:
>>
>> https://drive.google.com/open?id=0B_LliPDGUoZbVVFQS2VOV3hGN3c
>>
>> Activity is a measure of brain activity, Novelty and Valence are
>> categorical variables coding the type of stimulus used to elicit the
>> response, ROI is a categorical variable coding three regions of the
>> brain that we have sampled this activity from, and STAIt is a
>> continuous measure representing degree of a specific personality trait
>> of the subjects. Subject is an ID number for the individuals the data
>> was sampled from.
>>
>> Before glmmLasso I am running:
>>
>> KNov$Subject <- factor(KNov$Subject)
>>
>> to ensure the subject ID is not treated as a continuous variable.
>>
>> If I run:
>>
>> glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>> STAIt + as.factor(ROI)
>> + as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov,
>> lambda=10)
>> summary(glm1)
>>
>> I don't get any warning messages, but the output contains b estimates
>> only, no SE or p-values.
>>
>> If I try to include a 3-way interaction, such as:
>>
>> glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>> STAIt + as.factor(ROI)
>> + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
>> list(Subject=~1), data = Nov7T, lambda=10)
>> summary(glm2)
>>
>> I get the warnings:
>>
>> Warning messages:
>> 1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
>>   data length is not a multiple of split variable
>> 2: In lambda_vec * sqrt(block2) :
>>   longer object length is not a multiple of shorter object length
>>
>> And again, I do get parameter estimates, and no SE or p-values.
>>
>> If I include my continuous variable in any interaction, such as:
>>
>> glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>> STAIt + as.factor(ROI)
>> + as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
>> list(Subject=~1), data = Nov7T, lambda=10)
>> summary(glm3)
>>
>> I get the error message:
>>
>> Error in rep(control$index[i], length.fac) : invalid 'times' argument
>>
>> and no output.
>>
>> If anyone has an input as to (1) why I am not getting SE or p-values
>> in my outputs (2) the meaning of there warnings I get when I include a
>> 3-way variable, and if they are something to worry about, how to fix
>> them and (3) how to fix the error message I get when I include my
>> continuous factor in an interatction, I would be very appreciative.
>>
>> Thanks!
>>
>> Walker
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From dwinsemius at comcast.net  Sat Jul 16 19:51:05 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 16 Jul 2016 10:51:05 -0700
Subject: [R] glmmLasso with interactions errors
In-Reply-To: <CAAb_rUnCAoRNhP5W17L-66Yq20DhWt-MvSF6RKaA3vPEa-wzaQ@mail.gmail.com>
References: <CAAb_rUmWQZVQqnABGaEdz3M9Dmh8xQvjEzhQ4c1K5v7_c++isA@mail.gmail.com>
	<CAM5M9BTwzukAvLeRXdiuBxa+xmkCnf4CyzOXE5w23h2bf8=yjg@mail.gmail.com>
	<CAAb_rUnCAoRNhP5W17L-66Yq20DhWt-MvSF6RKaA3vPEa-wzaQ@mail.gmail.com>
Message-ID: <E73FC148-00F6-4B32-BA41-72B649C79F95@comcast.net>


> On Jul 16, 2016, at 9:29 AM, Walker Pedersen <wsp at uwm.edu> wrote:
> 
> Thank you for the input Brian and Ben.
> 
> It is odd how it seems to handle a two way interaction fine (as long
> as the continuous variable is not in the mix), but not a 3-way.

You should post code and data to demonstrate what is "not being handled".
> 
> In any case would anyone be able to give me a rundown of how I would
> create a matrix/dummy variable for these interactions to input into
> glmmLASSO?

Your first question on this dataset June 17 to CrossValidated.com was closed because no reproducible example was offered. You then posted two further questions on StackOverflow and got guesses as to the solutions  because you again posted no reproducible examples. One of those questions was given in this thread as a possible solution. IN the otehr one you did post some output that gave clues as to the arrangement of your data and suggested that the categorical data was relatively sparse:

http://stackoverflow.com/questions/38132830/getting-p-values-for-all-included-parameters-using-glmmlasso

Now you are getting advice that is similarly just speculation due to lack of code,  data and output. You are unlikely to get further advice that addresses what ever problems you have vaguely described unless you post examples of code that is failing along with either a) the real data or b) R code that creates a simulation with covariate features resembling your data.
> 
> Alternatively, is there a method for paring down a model that is a bit
> less sketchy than simple backfitting, that you would expect to be more
> straight forward software-wise?

That appears incredibly vague. Exactly what is "sketchy"? And what would be "more straightforward"?

-- 
David.


> Thanks!
> 
> Walker
> 
> UW-MKE
> 
> On Thu, Jul 14, 2016 at 10:08 AM, Cade, Brian <cadeb at usgs.gov> wrote:
>> It has never been obvious to me that the lasso approach can handle
>> interactions among predictor variables well at all.  I'ld be curious to see
>> what others think and what you learn.
>> 
>> Brian
>> 
>> Brian S. Cade, PhD
>> 
>> U. S. Geological Survey
>> Fort Collins Science Center
>> 2150 Centre Ave., Bldg. C
>> Fort Collins, CO  80526-8818
>> 
>> email:  cadeb at usgs.gov
>> tel:  970 226-9326
>> 
>> 
>> On Wed, Jul 13, 2016 at 2:20 PM, Walker Pedersen <wsp at uwm.edu> wrote:
>>> 
>>> Hi Everyone,
>>> 
>>> I am having trouble running glmmLasso.
>>> 
>>> An abbreviated version of my dataset is here:
>>> 
>>> https://drive.google.com/open?id=0B_LliPDGUoZbVVFQS2VOV3hGN3c
>>> 
>>> Activity is a measure of brain activity, Novelty and Valence are
>>> categorical variables coding the type of stimulus used to elicit the
>>> response, ROI is a categorical variable coding three regions of the
>>> brain that we have sampled this activity from, and STAIt is a
>>> continuous measure representing degree of a specific personality trait
>>> of the subjects. Subject is an ID number for the individuals the data
>>> was sampled from.
>>> 
>>> Before glmmLasso I am running:
>>> 
>>> KNov$Subject <- factor(KNov$Subject)
>>> 
>>> to ensure the subject ID is not treated as a continuous variable.
>>> 
>>> If I run:
>>> 
>>> glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>> STAIt + as.factor(ROI)
>>> + as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov,
>>> lambda=10)
>>> summary(glm1)
>>> 
>>> I don't get any warning messages, but the output contains b estimates
>>> only, no SE or p-values.
>>> 
>>> If I try to include a 3-way interaction, such as:
>>> 
>>> glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>> STAIt + as.factor(ROI)
>>> + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
>>> list(Subject=~1), data = Nov7T, lambda=10)
>>> summary(glm2)
>>> 
>>> I get the warnings:
>>> 
>>> Warning messages:
>>> 1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
>>>  data length is not a multiple of split variable
>>> 2: In lambda_vec * sqrt(block2) :
>>>  longer object length is not a multiple of shorter object length
>>> 
>>> And again, I do get parameter estimates, and no SE or p-values.
>>> 
>>> If I include my continuous variable in any interaction, such as:
>>> 
>>> glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>> STAIt + as.factor(ROI)
>>> + as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
>>> list(Subject=~1), data = Nov7T, lambda=10)
>>> summary(glm3)
>>> 
>>> I get the error message:
>>> 
>>> Error in rep(control$index[i], length.fac) : invalid 'times' argument
>>> 
>>> and no output.
>>> 
>>> If anyone has an input as to (1) why I am not getting SE or p-values
>>> in my outputs (2) the meaning of there warnings I get when I include a
>>> 3-way variable, and if they are something to worry about, how to fix
>>> them and (3) how to fix the error message I get when I include my
>>> continuous factor in an interatction, I would be very appreciative.
>>> 
>>> Thanks!
>>> 
>>> Walker
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From wsp at uwm.edu  Sat Jul 16 20:26:34 2016
From: wsp at uwm.edu (Walker Pedersen)
Date: Sat, 16 Jul 2016 13:26:34 -0500
Subject: [R] glmmLasso with interactions errors
In-Reply-To: <E73FC148-00F6-4B32-BA41-72B649C79F95@comcast.net>
References: <CAAb_rUmWQZVQqnABGaEdz3M9Dmh8xQvjEzhQ4c1K5v7_c++isA@mail.gmail.com>
	<CAM5M9BTwzukAvLeRXdiuBxa+xmkCnf4CyzOXE5w23h2bf8=yjg@mail.gmail.com>
	<CAAb_rUnCAoRNhP5W17L-66Yq20DhWt-MvSF6RKaA3vPEa-wzaQ@mail.gmail.com>
	<E73FC148-00F6-4B32-BA41-72B649C79F95@comcast.net>
Message-ID: <CAAb_rUkZpkXwYMN3FzC3c_JveKouBTwOU=Y-o+d3MyP6UJuNGw@mail.gmail.com>

Hi,

Thanks for the response.

The warnings and errors can be reproduced with the data and code I
included in my first mailing list post. I will provide the full output
at the end of this post.

By sketchy, I mean having a higher likelihood of resulting in
overfitting.  By more straightforward, I mean having a less steep
learning curve for implementation.

Thanks for your help!


> KNov <- read.table("Novelty_abr.txt", header = TRUE)
> KNov$Subject <- factor(KNov$Subject)
> glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) + STAIt + as.factor(ROI)
+ + as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov, lambda=10)
> summary(glm1)
Call:
glmmLasso(fix = Activity ~ as.factor(Novelty) + as.factor(Valence) +
    STAIt + as.factor(ROI) + as.factor(Valence):as.factor(ROI),
    rnd = list(Subject = ~1), data = KNov, lambda = 10)


Fixed Effects:

Coefficients:
                                       Estimate StdErr z.value p.value
(Intercept)                          0.14047593     NA      NA      NA
as.factor(Novelty)R                 -0.06333466     NA      NA      NA
as.factor(Valence)N                 -0.03537854     NA      NA      NA
STAIt                               -0.00173351     NA      NA      NA
as.factor(ROI)B                     -0.00438142     NA      NA      NA
as.factor(ROI)H                      0.00016285     NA      NA      NA
as.factor(Valence)N:as.factor(ROI)B -0.00739870     NA      NA      NA
as.factor(Valence)N:as.factor(ROI)H  0.00000000     NA      NA      NA

Random Effects:

StdDev:
           Subject
Subject 0.05186835
> glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) + STAIt + as.factor(ROI)
+ + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
list(Subject=~1), data = Nov7T, lambda=10)
Warning messages:
1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
  data length is not a multiple of split variable
2: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length
3: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length
4: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length
5: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length
6: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length
7: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length
8: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length
9: In lambda_vec * sqrt(block2) :
  longer object length is not a multiple of shorter object length
> summary(glm2)
Call:
glmmLasso(fix = Activity ~ as.factor(Novelty) + as.factor(Valence) +
    STAIt + as.factor(ROI) +
as.factor(Novelty):as.factor(Valence):as.factor(ROI),
    rnd = list(Subject = ~1), data = Nov7T, lambda = 10)


Fixed Effects:

Coefficients:
                                                             Estimate
StdErr z.value p.value
(Intercept)                                                -0.0562165
   NA      NA      NA
as.factor(Novelty)R                                        -0.0218362
   NA      NA      NA
as.factor(Valence)N                                        -0.0067723
   NA      NA      NA
STAIt                                                       0.0028832
   NA      NA      NA
as.factor(ROI)BNST                                         -0.0457882
   NA      NA      NA
as.factor(ROI)Hip                                          -0.0430477
   NA      NA      NA
as.factor(Novelty)N:as.factor(Valence)E:as.factor(ROI)Amy   0.0000000
   NA      NA      NA
as.factor(Novelty)R:as.factor(Valence)E:as.factor(ROI)Amy   0.0000000
   NA      NA      NA
as.factor(Novelty)N:as.factor(Valence)N:as.factor(ROI)Amy   0.0164788
   NA      NA      NA
as.factor(Novelty)R:as.factor(Valence)N:as.factor(ROI)Amy   0.0067723
   NA      NA      NA
as.factor(Novelty)N:as.factor(Valence)E:as.factor(ROI)BNST  0.0000000
   NA      NA      NA
as.factor(Novelty)R:as.factor(Valence)E:as.factor(ROI)BNST  0.0000000
   NA      NA      NA
as.factor(Novelty)N:as.factor(Valence)N:as.factor(ROI)BNST  0.0000000
   NA      NA      NA
as.factor(Novelty)R:as.factor(Valence)N:as.factor(ROI)BNST  0.0000000
   NA      NA      NA
as.factor(Novelty)N:as.factor(Valence)E:as.factor(ROI)Hip   0.0000000
   NA      NA      NA
as.factor(Novelty)R:as.factor(Valence)E:as.factor(ROI)Hip   0.0000000
   NA      NA      NA
as.factor(Novelty)N:as.factor(Valence)N:as.factor(ROI)Hip   0.0338616
   NA      NA      NA
as.factor(Novelty)R:as.factor(Valence)N:as.factor(ROI)Hip   0.0000000
   NA      NA      NA

Random Effects:

StdDev:
           Subject
Subject 0.09132963
> glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) + STAIt + as.factor(ROI)
+ + as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
list(Subject=~1), data = Nov7T, lambda=10)
Error in rep(control$index[i], length.fac) : invalid 'times' argument
> summary(glm3)
Error in summary(glm3) : object 'glm3' not found

On Sat, Jul 16, 2016 at 12:51 PM, David Winsemius
<dwinsemius at comcast.net> wrote:
>
>> On Jul 16, 2016, at 9:29 AM, Walker Pedersen <wsp at uwm.edu> wrote:
>>
>> Thank you for the input Brian and Ben.
>>
>> It is odd how it seems to handle a two way interaction fine (as long
>> as the continuous variable is not in the mix), but not a 3-way.
>
> You should post code and data to demonstrate what is "not being handled".
>>
>> In any case would anyone be able to give me a rundown of how I would
>> create a matrix/dummy variable for these interactions to input into
>> glmmLASSO?
>
> Your first question on this dataset June 17 to CrossValidated.com was closed because no reproducible example was offered. You then posted two further questions on StackOverflow and got guesses as to the solutions  because you again posted no reproducible examples. One of those questions was given in this thread as a possible solution. IN the otehr one you did post some output that gave clues as to the arrangement of your data and suggested that the categorical data was relatively sparse:
>
> http://stackoverflow.com/questions/38132830/getting-p-values-for-all-included-parameters-using-glmmlasso
>
> Now you are getting advice that is similarly just speculation due to lack of code,  data and output. You are unlikely to get further advice that addresses what ever problems you have vaguely described unless you post examples of code that is failing along with either a) the real data or b) R code that creates a simulation with covariate features resembling your data.
>>
>> Alternatively, is there a method for paring down a model that is a bit
>> less sketchy than simple backfitting, that you would expect to be more
>> straight forward software-wise?
>
> That appears incredibly vague. Exactly what is "sketchy"? And what would be "more straightforward"?
>
> --
> David.
>
>
>> Thanks!
>>
>> Walker
>>
>> UW-MKE
>>
>> On Thu, Jul 14, 2016 at 10:08 AM, Cade, Brian <cadeb at usgs.gov> wrote:
>>> It has never been obvious to me that the lasso approach can handle
>>> interactions among predictor variables well at all.  I'ld be curious to see
>>> what others think and what you learn.
>>>
>>> Brian
>>>
>>> Brian S. Cade, PhD
>>>
>>> U. S. Geological Survey
>>> Fort Collins Science Center
>>> 2150 Centre Ave., Bldg. C
>>> Fort Collins, CO  80526-8818
>>>
>>> email:  cadeb at usgs.gov
>>> tel:  970 226-9326
>>>
>>>
>>> On Wed, Jul 13, 2016 at 2:20 PM, Walker Pedersen <wsp at uwm.edu> wrote:
>>>>
>>>> Hi Everyone,
>>>>
>>>> I am having trouble running glmmLasso.
>>>>
>>>> An abbreviated version of my dataset is here:
>>>>
>>>> https://drive.google.com/open?id=0B_LliPDGUoZbVVFQS2VOV3hGN3c
>>>>
>>>> Activity is a measure of brain activity, Novelty and Valence are
>>>> categorical variables coding the type of stimulus used to elicit the
>>>> response, ROI is a categorical variable coding three regions of the
>>>> brain that we have sampled this activity from, and STAIt is a
>>>> continuous measure representing degree of a specific personality trait
>>>> of the subjects. Subject is an ID number for the individuals the data
>>>> was sampled from.
>>>>
>>>> Before glmmLasso I am running:
>>>>
>>>> KNov$Subject <- factor(KNov$Subject)
>>>>
>>>> to ensure the subject ID is not treated as a continuous variable.
>>>>
>>>> If I run:
>>>>
>>>> glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>>> STAIt + as.factor(ROI)
>>>> + as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov,
>>>> lambda=10)
>>>> summary(glm1)
>>>>
>>>> I don't get any warning messages, but the output contains b estimates
>>>> only, no SE or p-values.
>>>>
>>>> If I try to include a 3-way interaction, such as:
>>>>
>>>> glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>>> STAIt + as.factor(ROI)
>>>> + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
>>>> list(Subject=~1), data = Nov7T, lambda=10)
>>>> summary(glm2)
>>>>
>>>> I get the warnings:
>>>>
>>>> Warning messages:
>>>> 1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
>>>>  data length is not a multiple of split variable
>>>> 2: In lambda_vec * sqrt(block2) :
>>>>  longer object length is not a multiple of shorter object length
>>>>
>>>> And again, I do get parameter estimates, and no SE or p-values.
>>>>
>>>> If I include my continuous variable in any interaction, such as:
>>>>
>>>> glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>>> STAIt + as.factor(ROI)
>>>> + as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
>>>> list(Subject=~1), data = Nov7T, lambda=10)
>>>> summary(glm3)
>>>>
>>>> I get the error message:
>>>>
>>>> Error in rep(control$index[i], length.fac) : invalid 'times' argument
>>>>
>>>> and no output.
>>>>
>>>> If anyone has an input as to (1) why I am not getting SE or p-values
>>>> in my outputs (2) the meaning of there warnings I get when I include a
>>>> 3-way variable, and if they are something to worry about, how to fix
>>>> them and (3) how to fix the error message I get when I include my
>>>> continuous factor in an interatction, I would be very appreciative.
>>>>
>>>> Thanks!
>>>>
>>>> Walker
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>


From dwinsemius at comcast.net  Sat Jul 16 21:22:41 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 16 Jul 2016 12:22:41 -0700
Subject: [R] glmmLasso with interactions errors
In-Reply-To: <CAAb_rUkZpkXwYMN3FzC3c_JveKouBTwOU=Y-o+d3MyP6UJuNGw@mail.gmail.com>
References: <CAAb_rUmWQZVQqnABGaEdz3M9Dmh8xQvjEzhQ4c1K5v7_c++isA@mail.gmail.com>
	<CAM5M9BTwzukAvLeRXdiuBxa+xmkCnf4CyzOXE5w23h2bf8=yjg@mail.gmail.com>
	<CAAb_rUnCAoRNhP5W17L-66Yq20DhWt-MvSF6RKaA3vPEa-wzaQ@mail.gmail.com>
	<E73FC148-00F6-4B32-BA41-72B649C79F95@comcast.net>
	<CAAb_rUkZpkXwYMN3FzC3c_JveKouBTwOU=Y-o+d3MyP6UJuNGw@mail.gmail.com>
Message-ID: <B5CB3E31-112C-4E94-B1DD-37A22C07D58B@comcast.net>


> On Jul 16, 2016, at 11:26 AM, Walker Pedersen <wsp at uwm.edu> wrote:
> 
> Hi,
> 
> Thanks for the response.
> 
> The warnings and errors can be reproduced with the data and code I
> included in my first mailing list post. I will provide the full output
> at the end of this post.
> 

I get:

> glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
+ STAIt + as.factor(ROI)
+ + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
+ list(Subject=~1), data = Nov7T, lambda=10)
Error in is.data.frame(data) : object 'Nov7T' not found

If I instead run with KNov rather than the missing Nov7T object and use the `interaction` function to build a three-way interaction, I get:

glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
STAIt + as.factor(ROI) + interaction(Novelty,Valence,ROI),
list(Subject=~1), data = KNov, lambda=10)
summary(glm2)
Call:
glmmLasso(fix = Activity ~ as.factor(Novelty) + as.factor(Valence) + 
    STAIt + as.factor(ROI) + interaction(Novelty, Valence, ROI), 
    rnd = list(Subject = ~1), data = KNov, lambda = 10)


Fixed Effects:

Coefficients:
                                           Estimate StdErr z.value p.value
(Intercept)                              1.4584e-01     NA      NA      NA
as.factor(Novelty)R                     -6.3017e-02     NA      NA      NA
as.factor(Valence)N                     -3.8093e-02     NA      NA      NA
STAIt                                   -1.7146e-03     NA      NA      NA
as.factor(ROI)B                         -1.3502e-02     NA      NA      NA
as.factor(ROI)H                          1.1962e-03     NA      NA      NA
interaction(Novelty, Valence, ROI)R.E.A  0.0000e+00     NA      NA      NA
interaction(Novelty, Valence, ROI)N.N.A -1.9828e-02     NA      NA      NA
interaction(Novelty, Valence, ROI)R.N.A  2.5937e-19     NA      NA      NA
interaction(Novelty, Valence, ROI)N.E.B  0.0000e+00     NA      NA      NA
interaction(Novelty, Valence, ROI)R.E.B  0.0000e+00     NA      NA      NA
interaction(Novelty, Valence, ROI)N.N.B  0.0000e+00     NA      NA      NA
interaction(Novelty, Valence, ROI)R.N.B  0.0000e+00     NA      NA      NA
interaction(Novelty, Valence, ROI)N.E.H  0.0000e+00     NA      NA      NA
interaction(Novelty, Valence, ROI)R.E.H -2.1495e-02     NA      NA      NA
interaction(Novelty, Valence, ROI)N.N.H  0.0000e+00     NA      NA      NA
interaction(Novelty, Valence, ROI)R.N.H  0.0000e+00     NA      NA      NA

Random Effects:

StdDev:
          Subject
Subject 0.0644229

It does appear that the author has tried to discourage using formula-mediated interactions, since with a continuous-by-factor interaction I get this error message:

Error in est.glmmLasso.RE(fix = fix, rnd = rnd, data = data, lambda = lambda,  : 
  Usage of '*' not allowed in formula! Please specify the corresponding variables separately.


Neither of the first two examples on the glmmLasso page returns standard errors either. Perhaps you should correspond with the package author. Are you familiar with the maintainer function?

David



> By sketchy, I mean having a higher likelihood of resulting in
> overfitting.  By more straightforward, I mean having a less steep
> learning curve for implementation.
> 
>       Thanks for your help!
> 
> 
>> KNov <- read.table("Novelty_abr.txt", header = TRUE)
>> KNov$Subject <- factor(KNov$Subject)
>> glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) + STAIt + as.factor(ROI)
> + + as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov, lambda=10)
>> summary(glm1)
> Call:
> glmmLasso(fix = Activity ~ as.factor(Novelty) + as.factor(Valence) +
>    STAIt + as.factor(ROI) + as.factor(Valence):as.factor(ROI),
>    rnd = list(Subject = ~1), data = KNov, lambda = 10)
> 
> 
> Fixed Effects:
> 
> Coefficients:
>                                       Estimate StdErr z.value p.value
> (Intercept)                          0.14047593     NA      NA      NA
> as.factor(Novelty)R                 -0.06333466     NA      NA      NA
> as.factor(Valence)N                 -0.03537854     NA      NA      NA
> STAIt                               -0.00173351     NA      NA      NA
> as.factor(ROI)B                     -0.00438142     NA      NA      NA
> as.factor(ROI)H                      0.00016285     NA      NA      NA
> as.factor(Valence)N:as.factor(ROI)B -0.00739870     NA      NA      NA
> as.factor(Valence)N:as.factor(ROI)H  0.00000000     NA      NA      NA
> 
> Random Effects:
> 
> StdDev:
>           Subject
> Subject 0.05186835
>> glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) + STAIt + as.factor(ROI)
> + + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
> list(Subject=~1), data = Nov7T, lambda=10)
> Warning messages:
> 1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
>  data length is not a multiple of split variable
> 2: In lambda_vec * sqrt(block2) :
>  longer object length is not a multiple of shorter object length
> 3: In lambda_vec * sqrt(block2) :
>  longer object length is not a multiple of shorter object length
> 4: In lambda_vec * sqrt(block2) :
>  longer object length is not a multiple of shorter object length
> 5: In lambda_vec * sqrt(block2) :
>  longer object length is not a multiple of shorter object length
> 6: In lambda_vec * sqrt(block2) :
>  longer object length is not a multiple of shorter object length
> 7: In lambda_vec * sqrt(block2) :
>  longer object length is not a multiple of shorter object length
> 8: In lambda_vec * sqrt(block2) :
>  longer object length is not a multiple of shorter object length
> 9: In lambda_vec * sqrt(block2) :
>  longer object length is not a multiple of shorter object length
>> summary(glm2)
> Call:
> glmmLasso(fix = Activity ~ as.factor(Novelty) + as.factor(Valence) +
>    STAIt + as.factor(ROI) +
> as.factor(Novelty):as.factor(Valence):as.factor(ROI),
>    rnd = list(Subject = ~1), data = Nov7T, lambda = 10)
> 
> 
> Fixed Effects:
> 
> Coefficients:
>                                                             Estimate
> StdErr z.value p.value
> (Intercept)                                                -0.0562165
>   NA      NA      NA
> as.factor(Novelty)R                                        -0.0218362
>   NA      NA      NA
> as.factor(Valence)N                                        -0.0067723
>   NA      NA      NA
> STAIt                                                       0.0028832
>   NA      NA      NA
> as.factor(ROI)BNST                                         -0.0457882
>   NA      NA      NA
> as.factor(ROI)Hip                                          -0.0430477
>   NA      NA      NA
> as.factor(Novelty)N:as.factor(Valence)E:as.factor(ROI)Amy   0.0000000
>   NA      NA      NA
> as.factor(Novelty)R:as.factor(Valence)E:as.factor(ROI)Amy   0.0000000
>   NA      NA      NA
> as.factor(Novelty)N:as.factor(Valence)N:as.factor(ROI)Amy   0.0164788
>   NA      NA      NA
> as.factor(Novelty)R:as.factor(Valence)N:as.factor(ROI)Amy   0.0067723
>   NA      NA      NA
> as.factor(Novelty)N:as.factor(Valence)E:as.factor(ROI)BNST  0.0000000
>   NA      NA      NA
> as.factor(Novelty)R:as.factor(Valence)E:as.factor(ROI)BNST  0.0000000
>   NA      NA      NA
> as.factor(Novelty)N:as.factor(Valence)N:as.factor(ROI)BNST  0.0000000
>   NA      NA      NA
> as.factor(Novelty)R:as.factor(Valence)N:as.factor(ROI)BNST  0.0000000
>   NA      NA      NA
> as.factor(Novelty)N:as.factor(Valence)E:as.factor(ROI)Hip   0.0000000
>   NA      NA      NA
> as.factor(Novelty)R:as.factor(Valence)E:as.factor(ROI)Hip   0.0000000
>   NA      NA      NA
> as.factor(Novelty)N:as.factor(Valence)N:as.factor(ROI)Hip   0.0338616
>   NA      NA      NA
> as.factor(Novelty)R:as.factor(Valence)N:as.factor(ROI)Hip   0.0000000
>   NA      NA      NA
> 
> Random Effects:
> 
> StdDev:
>           Subject
> Subject 0.09132963
>> glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) + STAIt + as.factor(ROI)
> + + as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
> list(Subject=~1), data = Nov7T, lambda=10)
> Error in rep(control$index[i], length.fac) : invalid 'times' argument
>> summary(glm3)
> Error in summary(glm3) : object 'glm3' not found
> 
> On Sat, Jul 16, 2016 at 12:51 PM, David Winsemius
> <dwinsemius at comcast.net> wrote:
>> 
>>> On Jul 16, 2016, at 9:29 AM, Walker Pedersen <wsp at uwm.edu> wrote:
>>> 
>>> Thank you for the input Brian and Ben.
>>> 
>>> It is odd how it seems to handle a two way interaction fine (as long
>>> as the continuous variable is not in the mix), but not a 3-way.
>> 
>> You should post code and data to demonstrate what is "not being handled".
>>> 
>>> In any case would anyone be able to give me a rundown of how I would
>>> create a matrix/dummy variable for these interactions to input into
>>> glmmLASSO?
>> 
>> Your first question on this dataset June 17 to CrossValidated.com was closed because no reproducible example was offered. You then posted two further questions on StackOverflow and got guesses as to the solutions  because you again posted no reproducible examples. One of those questions was given in this thread as a possible solution. IN the otehr one you did post some output that gave clues as to the arrangement of your data and suggested that the categorical data was relatively sparse:
>> 
>> http://stackoverflow.com/questions/38132830/getting-p-values-for-all-included-parameters-using-glmmlasso
>> 
>> Now you are getting advice that is similarly just speculation due to lack of code,  data and output. You are unlikely to get further advice that addresses what ever problems you have vaguely described unless you post examples of code that is failing along with either a) the real data or b) R code that creates a simulation with covariate features resembling your data.
>>> 
>>> Alternatively, is there a method for paring down a model that is a bit
>>> less sketchy than simple backfitting, that you would expect to be more
>>> straight forward software-wise?
>> 
>> That appears incredibly vague. Exactly what is "sketchy"? And what would be "more straightforward"?
>> 
>> --
>> David.
>> 
>> 
>>> Thanks!
>>> 
>>> Walker
>>> 
>>> UW-MKE
>>> 
>>> On Thu, Jul 14, 2016 at 10:08 AM, Cade, Brian <cadeb at usgs.gov> wrote:
>>>> It has never been obvious to me that the lasso approach can handle
>>>> interactions among predictor variables well at all.  I'ld be curious to see
>>>> what others think and what you learn.
>>>> 
>>>> Brian
>>>> 
>>>> Brian S. Cade, PhD
>>>> 
>>>> U. S. Geological Survey
>>>> Fort Collins Science Center
>>>> 2150 Centre Ave., Bldg. C
>>>> Fort Collins, CO  80526-8818
>>>> 
>>>> email:  cadeb at usgs.gov
>>>> tel:  970 226-9326
>>>> 
>>>> 
>>>> On Wed, Jul 13, 2016 at 2:20 PM, Walker Pedersen <wsp at uwm.edu> wrote:
>>>>> 
>>>>> Hi Everyone,
>>>>> 
>>>>> I am having trouble running glmmLasso.
>>>>> 
>>>>> An abbreviated version of my dataset is here:
>>>>> 
>>>>> https://drive.google.com/open?id=0B_LliPDGUoZbVVFQS2VOV3hGN3c
>>>>> 
>>>>> Activity is a measure of brain activity, Novelty and Valence are
>>>>> categorical variables coding the type of stimulus used to elicit the
>>>>> response, ROI is a categorical variable coding three regions of the
>>>>> brain that we have sampled this activity from, and STAIt is a
>>>>> continuous measure representing degree of a specific personality trait
>>>>> of the subjects. Subject is an ID number for the individuals the data
>>>>> was sampled from.
>>>>> 
>>>>> Before glmmLasso I am running:
>>>>> 
>>>>> KNov$Subject <- factor(KNov$Subject)
>>>>> 
>>>>> to ensure the subject ID is not treated as a continuous variable.
>>>>> 
>>>>> If I run:
>>>>> 
>>>>> glm1 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>>>> STAIt + as.factor(ROI)
>>>>> + as.factor(Valence):as.factor(ROI), list(Subject=~1), data = KNov,
>>>>> lambda=10)
>>>>> summary(glm1)
>>>>> 
>>>>> I don't get any warning messages, but the output contains b estimates
>>>>> only, no SE or p-values.
>>>>> 
>>>>> If I try to include a 3-way interaction, such as:
>>>>> 
>>>>> glm2 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>>>> STAIt + as.factor(ROI)
>>>>> + as.factor(Novelty):as.factor(Valence):as.factor(ROI),
>>>>> list(Subject=~1), data = Nov7T, lambda=10)
>>>>> summary(glm2)
>>>>> 
>>>>> I get the warnings:
>>>>> 
>>>>> Warning messages:
>>>>> 1: In split.default((1:ncol(X))[-inotpen.which], ipen) :
>>>>> data length is not a multiple of split variable
>>>>> 2: In lambda_vec * sqrt(block2) :
>>>>> longer object length is not a multiple of shorter object length
>>>>> 
>>>>> And again, I do get parameter estimates, and no SE or p-values.
>>>>> 
>>>>> If I include my continuous variable in any interaction, such as:
>>>>> 
>>>>> glm3 <- glmmLasso(Activity~as.factor(Novelty) + as.factor(Valence) +
>>>>> STAIt + as.factor(ROI)
>>>>> + as.factor(Valence):as.factor(ROI) + as.factor(Novelty):STAIt,
>>>>> list(Subject=~1), data = Nov7T, lambda=10)
>>>>> summary(glm3)
>>>>> 
>>>>> I get the error message:
>>>>> 
>>>>> Error in rep(control$index[i], length.fac) : invalid 'times' argument
>>>>> 
>>>>> and no output.
>>>>> 
>>>>> If anyone has an input as to (1) why I am not getting SE or p-values
>>>>> in my outputs (2) the meaning of there warnings I get when I include a
>>>>> 3-way variable, and if they are something to worry about, how to fix
>>>>> them and (3) how to fix the error message I get when I include my
>>>>> continuous factor in an interatction, I would be very appreciative.
>>>>> 
>>>>> Thanks!
>>>>> 
>>>>> Walker
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From shivipmp82 at gmail.com  Sat Jul 16 23:25:24 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Sun, 17 Jul 2016 02:55:24 +0530
Subject: [R] findAssocs in TM package in R help?
Message-ID: <CAB=p7Sps0FhtCyuH_h+iBsvA6mOMn7XBqaZ17SD8KXwYn4Hejw@mail.gmail.com>

Good Day,

I am working on a text mining assignment and have built the document matrix
using the tm package.

Now I need to run findAssocs from my dtm with some word say 'like' with a
correlation of 0.70 but  as far as i have been researching it tells it this
function is only viable when we have more than 1 doc however in my case i
only have 1. So please suggest if there is an alternate to this issue.

This is because there are words that correlate to each other in the data I
am dealing with.

Kindly advice.

	[[alternative HTML version deleted]]


From sewashm at gmail.com  Sun Jul 17 00:25:34 2016
From: sewashm at gmail.com (Ashta)
Date: Sat, 16 Jul 2016 17:25:34 -0500
Subject: [R] Matrix
Message-ID: <CADDFq31yKqgdjoEnpOkhqLii38RrBshe_qx624O-2xYVUPixZw@mail.gmail.com>

Hi all,

I have a large square matrix (60 x 60)  and found it hard to
visualize. Is it possible to change it  as shown below?

Sample example (3 x 3)

    A   B   C
A  3   4   5
B  4   7   8
C  5   8   9

Desired output
A A  3
A B  4
A C  5
B B  7
B C  8
C C  9

Thank you in advance


From murdoch.duncan at gmail.com  Sun Jul 17 01:39:32 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 16 Jul 2016 19:39:32 -0400
Subject: [R] Matrix
In-Reply-To: <CADDFq31yKqgdjoEnpOkhqLii38RrBshe_qx624O-2xYVUPixZw@mail.gmail.com>
References: <CADDFq31yKqgdjoEnpOkhqLii38RrBshe_qx624O-2xYVUPixZw@mail.gmail.com>
Message-ID: <a58bf3d1-3059-8c5e-cd80-4d8b7758c1a6@gmail.com>

On 16/07/2016 6:25 PM, Ashta wrote:
 > Hi all,
 >
 > I have a large square matrix (60 x 60)  and found it hard to
 > visualize. Is it possible to change it  as shown below?
 >
 > Sample example (3 x 3)
 >
 >     A   B   C
 > A  3   4   5
 > B  4   7   8
 > C  5   8   9
 >
 > Desired output
 > A A  3
 > A B  4
 > A C  5
 > B B  7
 > B C  8
 > C C  9

Yes, use matrix indexing.  I don't think the 3600 values are going to be 
very easy to read, but here's how to produce them:

m <- matrix(1:3600, 60, 60)
indices <- expand.grid(row = 1:60, col = 1:60)
cbind(indices$row, indices$col, m[as.matrix(indices)])

Duncan Murdoch


From jmhannon.ucdavis at gmail.com  Sun Jul 17 02:26:28 2016
From: jmhannon.ucdavis at gmail.com (Michael Hannon)
Date: Sat, 16 Jul 2016 17:26:28 -0700
Subject: [R] Matrix
In-Reply-To: <a58bf3d1-3059-8c5e-cd80-4d8b7758c1a6@gmail.com>
References: <CADDFq31yKqgdjoEnpOkhqLii38RrBshe_qx624O-2xYVUPixZw@mail.gmail.com>
	<a58bf3d1-3059-8c5e-cd80-4d8b7758c1a6@gmail.com>
Message-ID: <CACdH2ZaSiz0z+A5P+=2rxhMX375bb9h93fGV9TaeE8hCr62rQA@mail.gmail.com>

I'm not sure what the OP is looking for in the first two columns, but
he does seem to be looking for only the diagonal and super-diagonal
elements.  Here's some code that makes output that looks similar to
the "Desired output":

mat1 <- matrix(rbind(c(3, 4, 5),
                     c(4, 7, 8),
                     c(5, 8, 9)), nrow=3)
colnames(mat1) <- c('A', 'B', 'C')
rownames(mat1) <- c('A', 'B', 'C')
mat1

col3 <- mat1[row(mat1) >= col(mat1)]
idx12 <- which(row(mat1) >= col(mat1), arr.ind=TRUE)
desiredOutput <- cbind(idx12[ , 2], idx12[ , 1], col3)
desiredOutput

#### If you really want 'A', 'B', 'C'
col1 <- colnames(mat1)[idx12[ , 2]]
col1
col2 <- rownames(mat1)[idx12[ , 1]]
col2

desiredOutput <- cbind(col1, col2, col3)
desiredOutput

-- Mike


On Sat, Jul 16, 2016 at 4:39 PM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 16/07/2016 6:25 PM, Ashta wrote:
>> Hi all,
>>
>> I have a large square matrix (60 x 60)  and found it hard to
>> visualize. Is it possible to change it  as shown below?
>>
>> Sample example (3 x 3)
>>
>>     A   B   C
>> A  3   4   5
>> B  4   7   8
>> C  5   8   9
>>
>> Desired output
>> A A  3
>> A B  4
>> A C  5
>> B B  7
>> B C  8
>> C C  9
>
> Yes, use matrix indexing.  I don't think the 3600 values are going to be
> very easy to read, but here's how to produce them:
>
> m <- matrix(1:3600, 60, 60)
> indices <- expand.grid(row = 1:60, col = 1:60)
> cbind(indices$row, indices$col, m[as.matrix(indices)])
>
> Duncan Murdoch
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From stn021 at gmail.com  Sun Jul 17 02:27:15 2016
From: stn021 at gmail.com (stn021)
Date: Sun, 17 Jul 2016 02:27:15 +0200
Subject: [R] [FORGED] Regression with factors ?
In-Reply-To: <alpine.BSF.2.00.1607131102400.62638@pedal.dcn.davis.ca.us>
References: <CANVHAvaPOro2SysAiRYM8gnQoMA4M54Tt_Y7R2SHY3o=-idh=g@mail.gmail.com>
	<3f9e0284-38f2-10f2-9041-0185884503d5@auckland.ac.nz>
	<EAF1867B-31DD-41D4-8149-B155D7A6305A@dcn.davis.ca.us>
	<CANVHAvYYqB3fv85CFYtSVkvd3mUU=o-etd-RF6sfU2iRQHmO1w@mail.gmail.com>
	<AF7E2508-9E20-428C-96D9-1182D27A27A6@dcn.davis.ca.us>
	<CANVHAvZi4SM2i1+UJL-jEHQvQuHe+Txqe=Y-Xv5+q5sCAwuD3w@mail.gmail.com>
	<A784205C-C35B-442B-B69D-5EACFD3FFFE3@comcast.net>
	<CANVHAvYi6JF41qGDG7xXWFZbOV5j2UOp=Z9j7PSfK+kTYbXRnQ@mail.gmail.com>
	<alpine.BSF.2.00.1607131102400.62638@pedal.dcn.davis.ca.us>
Message-ID: <CANVHAvYpv82Zc2GDYvtmAG=NxemVtxQRGUoeunBwpwPbF-ifCg@mail.gmail.com>

2016-07-13 20:09 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
> The formula interface as used in lm and nls searches for separate
> coefficients for each variable.. it will take someone more clever than I to
> figure out how to get the formula interface to think of two variables as
> instances of one factor.
>
> However, R can do nonlinear optimization just fine:
...

Hello Jeff, hello all,


thank you. This appears to be what I was looking for.


Is there a straightforward way to test for significance ?


THX,
stn


From toth.denes at ttk.mta.hu  Sun Jul 17 02:27:42 2016
From: toth.denes at ttk.mta.hu (=?UTF-8?B?RMOpbmVzIFTDs3Ro?=)
Date: Sun, 17 Jul 2016 02:27:42 +0200
Subject: [R] Matrix
In-Reply-To: <a58bf3d1-3059-8c5e-cd80-4d8b7758c1a6@gmail.com>
References: <CADDFq31yKqgdjoEnpOkhqLii38RrBshe_qx624O-2xYVUPixZw@mail.gmail.com>
	<a58bf3d1-3059-8c5e-cd80-4d8b7758c1a6@gmail.com>
Message-ID: <578AD0FE.7020603@ttk.mta.hu>



On 07/17/2016 01:39 AM, Duncan Murdoch wrote:
> On 16/07/2016 6:25 PM, Ashta wrote:
>  > Hi all,
>  >
>  > I have a large square matrix (60 x 60)  and found it hard to
>  > visualize. Is it possible to change it  as shown below?
>  >
>  > Sample example (3 x 3)
>  >
>  >     A   B   C
>  > A  3   4   5
>  > B  4   7   8
>  > C  5   8   9
>  >
>  > Desired output
>  > A A  3
>  > A B  4
>  > A C  5
>  > B B  7
>  > B C  8
>  > C C  9
>
> Yes, use matrix indexing.  I don't think the 3600 values are going to be
> very easy to read, but here's how to produce them:
>
> m <- matrix(1:3600, 60, 60)
> indices <- expand.grid(row = 1:60, col = 1:60)
> cbind(indices$row, indices$col, m[as.matrix(indices)])
>

Or use as.data.frame.table():

m <- matrix(1:9, 3, 3,
             dimnames = list(dimA = letters[1:3],
                             dimB = letters[1:3]))
m
as.data.frame.table(m, responseName = "value")

---

I do not know what you mean by "visualize", but image() or heatmap() are 
good starting points if you need a plot of the values. If you really 
need to inspect the raw values, you can try interactive (scrollable) 
tables, e.g.:

library(DT)
m <- provideDimnames(matrix(1:3600, 60, 60))
datatable(m, options = list(pageLength = 60))


Cheers,
   Denes



> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From eva.germovsek at farmbio.uu.se  Fri Jul 15 20:21:23 2016
From: eva.germovsek at farmbio.uu.se (Eva Germovsek)
Date: Fri, 15 Jul 2016 18:21:23 +0000
Subject: [R] Including correlation between weights in multivariate regression
Message-ID: <9EED2074D7489340AD2883454A37587D0E325258@COLUMBA02.user.uu.se>

Dear Rusers,

We want to fit a multivariable normal distribution to multiple observation variables, where these are observed with uncertainty represented by a covariance matrix which is different for each individual.

An example dataset (simplified) might look like this:
set.seed(101010)
nobs=20
test.data <- data.frame(ID=1:nobs,
                        y1=rnorm(n=nobs, mean=0, sd=0.15),
                        y2=rnorm(n=nobs, mean=0.15, sd=0.20),
                        var11=abs(rnorm(n=nobs, mean=0.1, sd=0.0011)),
                        cov12=rnorm(n=nobs, mean=0.001, sd=0.0001),
                        var22=abs(rnorm(n=nobs, mean=0.1, sd=0.0012)))

Where varX are the uncertainties of the observations (in variance units), and cov is covariance between the variance for the first and the second column (i.e. dependent variable).

One can do something like this, using lm, if we include only a single weight:
lm(cbind(y1, y2) ~ 1, data = test.data, weights = 1/test.data$var11)

But we would like to include different weights for each of the dependent variables, and also correlation between the weights.  Additionally, we may have up to ~30 observation variables (y1 through y30). Does anyone have any experience with this, or know a package that can do that?

Many thanks.

Best wishes,

Eva




Eva Germovsek, PhD
Pharmacometrics Research Group
Department of Pharmaceutical Biosciences
Uppsala University
P.O. Box 591
751 24 Uppsala
Sweden

	[[alternative HTML version deleted]]


From pamela.wong at mail.utoronto.ca  Sat Jul 16 07:11:56 2016
From: pamela.wong at mail.utoronto.ca (Pamela Wong)
Date: Sat, 16 Jul 2016 05:11:56 +0000
Subject: [R] Troubleshooting Type III SS and drop1()
Message-ID: <51319F34-E799-45FA-A6A0-60B799E70A61@mail.utoronto.ca>

Hi there

I am trying to compute Type III SS plus model selection and find that an error pops up when I'm trying to use the drop1() function with a model output using Anova(). I am looking at effects of four factors and their interactions (with unbalanced sample sizes across some factor groups), on a continuous variable. 

I get an error that reads: "Error in terms.default(object) : no terms component not attribute". I also can't compute an AIC for the model outputs. 

I am wondering if there is something I need to do to my data or if there is some limitation to the analysis I am trying to do.

From Ljanowitch at alum.mit.edu  Sat Jul 16 18:06:49 2016
From: Ljanowitch at alum.mit.edu (Lawrence A. Janowitch)
Date: Sat, 16 Jul 2016 12:06:49 -0400
Subject: [R] newbee Caret load  question
Message-ID: <004801d1df7c$0fab0520$2f010f60$@gmail.com>

I'm trying to load the caret package in R-Studio (Version 0.99.902) on a
Windows 10 OS using bootcamp windows connection on a MacBook Pro.

I have used caret in the past, with no problems.  

 

My issue is I don't understand what this means and what I should do about
it.

 

I get:

 

> library(caret)

Loading required package: lattice

Loading required package: ggplot2

Need help? Try the ggplot2 mailing list:
http://groups.google.com/group/ggplot2.

Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
vI[[j]]) : 

  namespace 'pbkrtest' 0.4-2 is being loaded, but >= 0.4.4 is required

In addition: Warning messages:

1: package 'caret' was built under R version 3.2.5 

2: package 'ggplot2' was built under R version 3.2.3 

Error: package or namespace load failed for 'caret'

 

Any help would be much appreciated.

Thank you.


	[[alternative HTML version deleted]]


From sewashm at gmail.com  Sun Jul 17 04:43:17 2016
From: sewashm at gmail.com (Ashta)
Date: Sat, 16 Jul 2016 21:43:17 -0500
Subject: [R] Matrix
In-Reply-To: <578AD0FE.7020603@ttk.mta.hu>
References: <CADDFq31yKqgdjoEnpOkhqLii38RrBshe_qx624O-2xYVUPixZw@mail.gmail.com>
	<a58bf3d1-3059-8c5e-cd80-4d8b7758c1a6@gmail.com>
	<578AD0FE.7020603@ttk.mta.hu>
Message-ID: <CADDFq33Tx9MC2jKN5Q4xpZsNZG+u463VPXg90vXtaqk_fc=cuw@mail.gmail.com>

HI Denes, Duncan,Michael and all,

Thank you very much  for the helpful suggestion.  Some of my data sets
were not square matrix, however, Denes's suggestion,"
as.data.frame.table() ", handled that one.

Thank you again.


On Sat, Jul 16, 2016 at 7:27 PM, D?nes T?th <toth.denes at ttk.mta.hu> wrote:
>
>
> On 07/17/2016 01:39 AM, Duncan Murdoch wrote:
>>
>> On 16/07/2016 6:25 PM, Ashta wrote:
>>  > Hi all,
>>  >
>>  > I have a large square matrix (60 x 60)  and found it hard to
>>  > visualize. Is it possible to change it  as shown below?
>>  >
>>  > Sample example (3 x 3)
>>  >
>>  >     A   B   C
>>  > A  3   4   5
>>  > B  4   7   8
>>  > C  5   8   9
>>  >
>>  > Desired output
>>  > A A  3
>>  > A B  4
>>  > A C  5
>>  > B B  7
>>  > B C  8
>>  > C C  9
>>
>> Yes, use matrix indexing.  I don't think the 3600 values are going to be
>> very easy to read, but here's how to produce them:
>>
>> m <- matrix(1:3600, 60, 60)
>> indices <- expand.grid(row = 1:60, col = 1:60)
>> cbind(indices$row, indices$col, m[as.matrix(indices)])
>>
>
> Or use as.data.frame.table():
>
> m <- matrix(1:9, 3, 3,
>             dimnames = list(dimA = letters[1:3],
>                             dimB = letters[1:3]))
> m
> as.data.frame.table(m, responseName = "value")
>
> ---
>
> I do not know what you mean by "visualize", but image() or heatmap() are
> good starting points if you need a plot of the values. If you really need to
> inspect the raw values, you can try interactive (scrollable) tables, e.g.:
>
> library(DT)
> m <- provideDimnames(matrix(1:3600, 60, 60))
> datatable(m, options = list(pageLength = 60))
>
>
> Cheers,
>   Denes
>
>
>
>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From chris_kelvin2001 at yahoo.com  Sun Jul 17 05:33:48 2016
From: chris_kelvin2001 at yahoo.com (Christopher Kelvin)
Date: Sun, 17 Jul 2016 03:33:48 +0000 (UTC)
Subject: [R] R2WinBUGS with Multivariate Logistic Regression
References: <722063481.363251.1468726428102.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <722063481.363251.1468726428102.JavaMail.yahoo@mail.yahoo.com>

Dear R-User,
I have written a simple code to analyze some data using Bayesian logistic regression via the R2WinBUGS package. The code when run in WinBUGS stops WinBUGS from running it and using the package returns no results also. 


I attach herewith, the code and a sample of the dataset.

Any suggestion will be greatly appreciated.

Chris Guure
Biostatistics Department

University of Ghana




library(R2WinBUGS)
library(coda)model1<-function(){
for (i in 1:N) {
# likelihood function
ms[i] ~ dbin( p [ i ], N )
logit(p [ i ] ) <- alpha + bage*age[ i ] + bpam*pam[i ] + bpah*pah[i] 

}
### prior for intercept
alpha ~ dnorm(0,0.0001)

# prior for slopes
bage ~ dnorm(0,0.0001)
bpam ~ dnorm(0,0.0001) 
bpah ~ dnorm(0,0.0001)


# OR for alpha
or.age<-exp(bage)
# OR for hbp
or.pam <- exp(bpam)
# OR for fdm
or.pah <- exp(bpah)

}

data=cbind(ms=c(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0),
age=c(77, 83, 75, 78, 75, 83, 85, 80, 80, 85, 76, 77, 80, 76, 88, 77, 81, 78, 85, 81),
pam=c(0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1),
pah=c(1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0))
N=20

#### initial values                                                                                             ###
lineinits <- function(){
list(alpha=1,bage= 0.05, bpam=0.05, bpah=0.05)
}

## CODA output 
lineout1 <- bugs(data, lineinits, c("bage", "alpha","bpam", "bpah", "or.age", "or.pam", "or.pah"), model1,n.iter = 11000, n.burnin = 1000, n.chains = 2, codaPkg = T, DIC = TRUE) 


### Posterior summaries
line.coda <- read.bugs(lineout1) 
summary(line.coda)


From bgunter.4567 at gmail.com  Sun Jul 17 05:43:12 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 16 Jul 2016 20:43:12 -0700
Subject: [R] Troubleshooting Type III SS and drop1()
In-Reply-To: <51319F34-E799-45FA-A6A0-60B799E70A61@mail.utoronto.ca>
References: <51319F34-E799-45FA-A6A0-60B799E70A61@mail.utoronto.ca>
Message-ID: <CAGxFJbSYFSV+Gs4x6zzZOZ7qWa-C1L-Rb+7TJj8BV2ER7FuFHg@mail.gmail.com>

Omg omg! Search  on "type III as R good or bad" or similar for why you
should of should not be doing this. As for your specific question, I doubt
that you'll get a useful reply until you post the code that elicited the
error. Maybe not even then if it is due to estimability/overfitting of your
data.
And that you cannot obtain an aic seems unsurprising, as you said you
cannot fit the model and so cannot get a likelihood and so cannot possibly
obtain a penalized likelihood.

Or do I misunderstand? If so, please ignore. If not, my best advice would
be to ignore all this SS business altogether and talk with a local
statistician about how or if your data might answer scientific questions of
interest.

Cheers,
Bert
On Jul 16, 2016 6:16 PM, "Pamela Wong" <pamela.wong at mail.utoronto.ca> wrote:

> Hi there
>
> I am trying to compute Type III SS plus model selection and find that an
> error pops up when I'm trying to use the drop1() function with a model
> output using Anova(). I am looking at effects of four factors and their
> interactions (with unbalanced sample sizes across some factor groups), on a
> continuous variable.
>
> I get an error that reads: "Error in terms.default(object) : no terms
> component not attribute". I also can't compute an AIC for the model outputs.
>
> I am wondering if there is something I need to do to my data or if there
> is some limitation to the analysis I am trying to do.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sun Jul 17 07:50:50 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 17 Jul 2016 15:50:50 +1000
Subject: [R] newbee Caret load question
In-Reply-To: <004801d1df7c$0fab0520$2f010f60$@gmail.com>
References: <004801d1df7c$0fab0520$2f010f60$@gmail.com>
Message-ID: <CA+8X3fU2i3MPO21RHB5T7+Z2-mJpqN1kSCZ04T0i8nneBUT+tA@mail.gmail.com>

Hi Lawrence,
Try installing pbkrtest on its own:

update.packages("pbkrtest")

version 0.4.6 is up  on CRAN, so this may allow you to make an end run.

Jim


On Sun, Jul 17, 2016 at 2:06 AM, Lawrence A. Janowitch
<Ljanowitch at alum.mit.edu> wrote:
> I'm trying to load the caret package in R-Studio (Version 0.99.902) on a
> Windows 10 OS using bootcamp windows connection on a MacBook Pro.
>
> I have used caret in the past, with no problems.
>
>
>
> My issue is I don't understand what this means and what I should do about
> it.
>
>
>
> I get:
>
>
>
>> library(caret)
>
> Loading required package: lattice
>
> Loading required package: ggplot2
>
> Need help? Try the ggplot2 mailing list:
> http://groups.google.com/group/ggplot2.
>
> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
> vI[[j]]) :
>
>   namespace 'pbkrtest' 0.4-2 is being loaded, but >= 0.4.4 is required
>
> In addition: Warning messages:
>
> 1: package 'caret' was built under R version 3.2.5
>
> 2: package 'ggplot2' was built under R version 3.2.3
>
> Error: package or namespace load failed for 'caret'
>
>
>
> Any help would be much appreciated.
>
> Thank you.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Sun Jul 17 10:08:05 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 17 Jul 2016 10:08:05 +0200
Subject: [R] newbee Caret load question
In-Reply-To: <CA+8X3fU2i3MPO21RHB5T7+Z2-mJpqN1kSCZ04T0i8nneBUT+tA@mail.gmail.com>
References: <004801d1df7c$0fab0520$2f010f60$@gmail.com>
	<CA+8X3fU2i3MPO21RHB5T7+Z2-mJpqN1kSCZ04T0i8nneBUT+tA@mail.gmail.com>
Message-ID: <863BA6C0-AE00-4CA7-BEA0-25E697220415@gmail.com>

The R version was not stated (RStudio is a shell, so you cannot infer the version of R from its version number), but the warning messages suggest that it is less than 3.2.3. Recent versions of pbkrtest depend on R >= 3.2.3. I happen to know that because it led to infelicities which should probably have been avoided by not introducing new features in the middle of the 3.2.x series. 

I.e., an upgrade of R is what is called for.

-pd

> On 17 Jul 2016, at 07:50 , Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Hi Lawrence,
> Try installing pbkrtest on its own:
> 
> update.packages("pbkrtest")
> 
> version 0.4.6 is up  on CRAN, so this may allow you to make an end run.
> 
> Jim
> 
> 
> On Sun, Jul 17, 2016 at 2:06 AM, Lawrence A. Janowitch
> <Ljanowitch at alum.mit.edu> wrote:
>> I'm trying to load the caret package in R-Studio (Version 0.99.902) on a
>> Windows 10 OS using bootcamp windows connection on a MacBook Pro.
>> 
>> I have used caret in the past, with no problems.
>> 
>> 
>> 
>> My issue is I don't understand what this means and what I should do about
>> it.
>> 
>> 
>> 
>> I get:
>> 
>> 
>> 
>>> library(caret)
>> 
>> Loading required package: lattice
>> 
>> Loading required package: ggplot2
>> 
>> Need help? Try the ggplot2 mailing list:
>> http://groups.google.com/group/ggplot2.
>> 
>> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck =
>> vI[[j]]) :
>> 
>>  namespace 'pbkrtest' 0.4-2 is being loaded, but >= 0.4.4 is required
>> 
>> In addition: Warning messages:
>> 
>> 1: package 'caret' was built under R version 3.2.5
>> 
>> 2: package 'ggplot2' was built under R version 3.2.3
>> 
>> Error: package or namespace load failed for 'caret'
>> 
>> 
>> 
>> Any help would be much appreciated.
>> 
>> Thank you.
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From erinm.hodgess at gmail.com  Sun Jul 17 17:33:58 2016
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Sun, 17 Jul 2016 11:33:58 -0400
Subject: [R] writing a matrix to a file with trailing zeroes
Message-ID: <CACxE24kncgEVHyR=EBGD8tqKA8gOSnGSAs8kDFTSw02ZjOGTZA@mail.gmail.com>

Hello everyone!

I'm looking at this problem, and I'm sure there is a straightforward
solution.  I have a matrix and some of the values have zeroes to the right
of the decimal point.  When I write these to a file, the zeroes disappear.
The material below is what I have tried so far.



> x2
              x          y
[1,] -1.3611521  5.0000000
[2,] -0.8521120 -1.0512976
[3,] -0.9652820 -0.1306795
[4,] -0.4319187  1.2483199
[5,] -0.7548402  1.3404867
[6,]  6.0000000  2.5000000

>write.table(sprintf("%.1f",t(x2)),row.name=FALSE,col.name=FALSE,file="")
"-1.4"
"5.0"
"-0.9"
"-1.1"
"-1.0"
"-0.1"
"-0.4"
"1.2"
"-0.8"
"1.3"
"6.0"
"2.5"
> write(sprintf("%.1f",t(x2)),file="")
-1.4
5.0
-0.9
-1.1
-1.0
-0.1
-0.4
1.2
-0.8
1.3
6.0
2.5
> write(round(t(x2),1),file="")
-1.4 5 -0.9 -1.1 -1
-0.1 -0.4 1.2 -0.8 1.3
6 2.5
> write(round(t(x2),1),file="",ncol=2)
-1.4 5
-0.9 -1.1
-1 -0.1
-0.4 1.2
-0.8 1.3
6 2.5
>


Still no luck.  I need the 5 in the first line to be 5.0 and the 6 in the
last line to be 6.0.

Any suggestions, please?

Thanks,
Erin

-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From erinm.hodgess at gmail.com  Sun Jul 17 18:28:02 2016
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Sun, 17 Jul 2016 12:28:02 -0400
Subject: [R] Solution to my own question
Message-ID: <CACxE24m=YqV-QCRGA+hgAvd6LqGM5p-QYF-QF08qvRhesgLDhg@mail.gmail.com>

write.table(format(x2,drop0trailing=FALSE),file="",col.name=FALSE,row.name
=FALSE,quote=FALSE)

-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sun Jul 17 18:28:05 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 17 Jul 2016 12:28:05 -0400
Subject: [R] writing a matrix to a file with trailing zeroes
In-Reply-To: <CACxE24kncgEVHyR=EBGD8tqKA8gOSnGSAs8kDFTSw02ZjOGTZA@mail.gmail.com>
References: <CACxE24kncgEVHyR=EBGD8tqKA8gOSnGSAs8kDFTSw02ZjOGTZA@mail.gmail.com>
Message-ID: <2f1533fa-3d5c-08f2-eb7d-e5512e98fd05@gmail.com>

On 17/07/2016 11:33 AM, Erin Hodgess wrote:
> Hello everyone!
>
> I'm looking at this problem, and I'm sure there is a straightforward
> solution.  I have a matrix and some of the values have zeroes to the right
> of the decimal point.  When I write these to a file, the zeroes disappear.
> The material below is what I have tried so far.
>
>
>
>> x2
>               x          y
> [1,] -1.3611521  5.0000000
> [2,] -0.8521120 -1.0512976
> [3,] -0.9652820 -0.1306795
> [4,] -0.4319187  1.2483199
> [5,] -0.7548402  1.3404867
> [6,]  6.0000000  2.5000000
>
>> write.table(sprintf("%.1f",t(x2)),row.name=FALSE,col.name=FALSE,file="")
> "-1.4"
> "5.0"
> "-0.9"
> "-1.1"
> "-1.0"
> "-0.1"
> "-0.4"
> "1.2"
> "-0.8"
> "1.3"
> "6.0"
> "2.5"
>> write(sprintf("%.1f",t(x2)),file="")
> -1.4
> 5.0
> -0.9
> -1.1
> -1.0
> -0.1
> -0.4
> 1.2
> -0.8
> 1.3
> 6.0
> 2.5
>> write(round(t(x2),1),file="")
> -1.4 5 -0.9 -1.1 -1
> -0.1 -0.4 1.2 -0.8 1.3
> 6 2.5
>> write(round(t(x2),1),file="",ncol=2)
> -1.4 5
> -0.9 -1.1
> -1 -0.1
> -0.4 1.2
> -0.8 1.3
> 6 2.5
>>
>
>
> Still no luck.  I need the 5 in the first line to be 5.0 and the 6 in the
> last line to be 6.0.
>
> Any suggestions, please?

Your second try comes close.  Why not just add ncol=2 to it, i.e.

write(sprintf("%.1f",t(x2)),file="", ncol=2)

Duncan Murdoch


From dwinsemius at comcast.net  Sun Jul 17 20:47:18 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 17 Jul 2016 11:47:18 -0700
Subject: [R] Matrix
In-Reply-To: <CADDFq33Tx9MC2jKN5Q4xpZsNZG+u463VPXg90vXtaqk_fc=cuw@mail.gmail.com>
References: <CADDFq31yKqgdjoEnpOkhqLii38RrBshe_qx624O-2xYVUPixZw@mail.gmail.com>
	<a58bf3d1-3059-8c5e-cd80-4d8b7758c1a6@gmail.com>
	<578AD0FE.7020603@ttk.mta.hu>
	<CADDFq33Tx9MC2jKN5Q4xpZsNZG+u463VPXg90vXtaqk_fc=cuw@mail.gmail.com>
Message-ID: <95425587-88BB-4831-BCAA-B0585AF23137@comcast.net>


> On Jul 16, 2016, at 7:43 PM, Ashta <sewashm at gmail.com> wrote:
> 
> HI Denes, Duncan,Michael and all,
> 
> Thank you very much  for the helpful suggestion.  Some of my data sets
> were not square matrix, however, Denes's suggestion,"
> as.data.frame.table() ", handled that one.
> 

`as.data.frame.table` should work with any matrix, not just sqaure ones:

m <- matrix(1:12, 3, 4,
           dimnames = list(dimA = letters[1:3],
                           dimB = letters[1:4]))
m
#-----
    dimB
dimA a b c  d
   a 1 4 7 10
   b 2 5 8 11
   c 3 6 9 12
#--------
 as.data.frame.table(m, responseName = "value")
   dimA dimB value

1     a    a     1
2     b    a     2
3     c    a     3
4     a    b     4
5     b    b     5
6     c    b     6
7     a    c     7
8     b    c     8
9     c    c     9
10    a    d    10
11    b    d    11
12    c    d    12




> Thank you again.
> 
> 
> On Sat, Jul 16, 2016 at 7:27 PM, D?nes T?th <toth.denes at ttk.mta.hu> wrote:
>> 
>> 
>> On 07/17/2016 01:39 AM, Duncan Murdoch wrote:
>>> 
>>> On 16/07/2016 6:25 PM, Ashta wrote:
>>>> Hi all,
>>>> 
>>>> I have a large square matrix (60 x 60)  and found it hard to
>>>> visualize. Is it possible to change it  as shown below?
>>>> 
>>>> Sample example (3 x 3)
>>>> 
>>>>    A   B   C
>>>> A  3   4   5
>>>> B  4   7   8
>>>> C  5   8   9
>>>> 
>>>> Desired output
>>>> A A  3
>>>> A B  4
>>>> A C  5
>>>> B B  7
>>>> B C  8
>>>> C C  9
>>> 
>>> Yes, use matrix indexing.  I don't think the 3600 values are going to be
>>> very easy to read, but here's how to produce them:
>>> 
>>> m <- matrix(1:3600, 60, 60)
>>> indices <- expand.grid(row = 1:60, col = 1:60)
>>> cbind(indices$row, indices$col, m[as.matrix(indices)])
>>> 
>> 
>> Or use as.data.frame.table():
>> 
>> m <- matrix(1:9, 3, 3,
>>            dimnames = list(dimA = letters[1:3],
>>                            dimB = letters[1:3]))
>> m
>> as.data.frame.table(m, responseName = "value")
>> 
>> ---
>> 
>> I do not know what you mean by "visualize", but image() or heatmap() are
>> good starting points if you need a plot of the values. If you really need to
>> inspect the raw values, you can try interactive (scrollable) tables, e.g.:
>> 
>> library(DT)
>> m <- provideDimnames(matrix(1:3600, 60, 60))
>> datatable(m, options = list(pageLength = 60))
>> 
>> 
>> Cheers,
>>  Denes
>> 
>> 
>> 
>> 
>>> Duncan Murdoch
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jfox at mcmaster.ca  Sun Jul 17 22:15:30 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Sun, 17 Jul 2016 20:15:30 +0000
Subject: [R] Troubleshooting Type III SS and drop1()
In-Reply-To: <51319F34-E799-45FA-A6A0-60B799E70A61@mail.utoronto.ca>
References: <51319F34-E799-45FA-A6A0-60B799E70A61@mail.utoronto.ca>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836541D66@FHSDB2D11-2.csu.mcmaster.ca>

Dear Pamela,

It's very hard to tell from your message what you did, but I'm guessing that you passed the object produced by Anova() to drop1(). If this is in fact what you did, it makes no sense, because Anova() doesn't produce a linear-model object but rather takes such an object as its argument.

I hope this helps,
 John

--------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
Web: socserv.mcmaster.ca/jfox



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Pamela
> Wong
> Sent: Saturday, July 16, 2016 1:12 AM
> To: r-help at r-project.org
> Subject: [R] Troubleshooting Type III SS and drop1()
> 
> Hi there
> 
> I am trying to compute Type III SS plus model selection and find that an
> error pops up when I'm trying to use the drop1() function with a model
> output using Anova(). I am looking at effects of four factors and their
> interactions (with unbalanced sample sizes across some factor groups),
> on a continuous variable.
> 
> I get an error that reads: "Error in terms.default(object) : no terms
> component not attribute". I also can't compute an AIC for the model
> outputs.
> 
> I am wondering if there is something I need to do to my data or if there
> is some limitation to the analysis I am trying to do.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pamelawong86 at icloud.com  Sun Jul 17 07:57:30 2016
From: pamelawong86 at icloud.com (Pamela Wong)
Date: Sun, 17 Jul 2016 01:57:30 -0400
Subject: [R] Type III SS and collinearity in R
Message-ID: <5430039E-FB8C-4F19-A760-BF52C2ABF987@icloud.com>

Hi there
It appears the Anova() function in the car package cannot compute type III SS when there is collinearity in the model. Has anyone else run into this problem? 

I would use drop1() to obtain the SS but it does not provide the intercept...

From tom at vims.edu  Sun Jul 17 14:22:23 2016
From: tom at vims.edu (Tom Mosca)
Date: Sun, 17 Jul 2016 12:22:23 +0000
Subject: [R] means by year, month and day
Message-ID: <CB5791F5EA3D82408B277900997482D27F7DA2D5@mboxes2.campus.vims.edu>

Hello Good Folk,

My dataframe looks like this:
> mydata
     X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
1    2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
2    2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
3    2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
4    2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
5    2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
6    2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
7    2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
8    2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
9    2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
10   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3 ?

The first four columns are year, month, day, hour (0 ? 23).  I wish to take the means of the next six columns (WDIR, WSPD, GST, PRES, ATMP and DEWP) by year, month and day.  That is, I want daily averages.

Please help.  Thank you.

Tom

	[[alternative HTML version deleted]]


From tom at vims.edu  Sun Jul 17 20:01:27 2016
From: tom at vims.edu (Tom Mosca)
Date: Sun, 17 Jul 2016 18:01:27 +0000
Subject: [R] daily means, follow-up
Message-ID: <CB5791F5EA3D82408B277900997482D27F7DA2EC@mboxes2.campus.vims.edu>

Solved it myself:

aggregate(mydata, by=mydata[c("MM","DD")], FUN=mean)

Thank you.



# -----------------------------------------------------------------------------

My dataframe looks like this:
> mydata
     X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
1    2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
2    2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
3    2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
4    2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
5    2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
6    2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
7    2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
8    2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
9    2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
10   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3 ?

The first four columns are year, month, day, hour (0 ? 23).  I wish to take the means of the next six columns (WDIR, WSPD, GST, PRES, ATMP and DEWP) by day.

	[[alternative HTML version deleted]]


From fanjianling at gmail.com  Sun Jul 17 23:42:10 2016
From: fanjianling at gmail.com (Jianling Fan)
Date: Sun, 17 Jul 2016 15:42:10 -0600
Subject: [R] means by year, month and day
In-Reply-To: <CB5791F5EA3D82408B277900997482D27F7DA2D5@mboxes2.campus.vims.edu>
References: <CB5791F5EA3D82408B277900997482D27F7DA2D5@mboxes2.campus.vims.edu>
Message-ID: <CAJ7mryJGyC=KethC7xLz7eMGYhchuctAW6BCUNC4N7RX3yc-RA@mail.gmail.com>

Hello Tom,

try aggregate() or cast(). Both works.I prefer the latter.


library(reshape)
desc<-melt(mydata, measure.vars=c("WDI","R.WSP", "D.GST", "PRES",
"ATMP", "DEWP"),
           id.vars=c("X.YY","MM","DD"))
summary<-cast(desc, X.YY+MM+DD~variable, mean)









On 17 July 2016 at 06:22, Tom Mosca <tom at vims.edu> wrote:
> Hello Good Folk,
>
> My dataframe looks like this:
>> mydata
>      X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
> 1    2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
> 2    2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
> 3    2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
> 4    2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
> 5    2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
> 6    2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
> 7    2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
> 8    2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
> 9    2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
> 10   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3 ?
>
> The first four columns are year, month, day, hour (0 ? 23).  I wish to take the means of the next six columns (WDIR, WSPD, GST, PRES, ATMP and DEWP) by year, month and day.  That is, I want daily averages.
>
> Please help.  Thank you.
>
> Tom
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Jianling Fan
???


From shivipmp82 at gmail.com  Mon Jul 18 00:31:40 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Mon, 18 Jul 2016 04:01:40 +0530
Subject: [R] findAssocs in TM package in R help?
In-Reply-To: <CAB=p7Sps0FhtCyuH_h+iBsvA6mOMn7XBqaZ17SD8KXwYn4Hejw@mail.gmail.com>
References: <CAB=p7Sps0FhtCyuH_h+iBsvA6mOMn7XBqaZ17SD8KXwYn4Hejw@mail.gmail.com>
Message-ID: <CAB=p7SrtHVwvmTKbM=FzOsxXyTjZxexLcggP9WWjsEHbWcKaGg@mail.gmail.com>

Hi Team,

Please suggest.

On Sun, Jul 17, 2016 at 2:55 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:

> Good Day,
>
> I am working on a text mining assignment and have built the document
> matrix using the tm package.
>
> Now I need to run findAssocs from my dtm with some word say 'like' with a
> correlation of 0.70 but  as far as i have been researching it tells it this
> function is only viable when we have more than 1 doc however in my case i
> only have 1. So please suggest if there is an alternate to this issue.
>
> This is because there are words that correlate to each other in the data I
> am dealing with.
>
> Kindly advice.
>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Mon Jul 18 00:55:04 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Sun, 17 Jul 2016 22:55:04 +0000
Subject: [R] Type III SS and collinearity in R
In-Reply-To: <5430039E-FB8C-4F19-A760-BF52C2ABF987@icloud.com>
References: <5430039E-FB8C-4F19-A760-BF52C2ABF987@icloud.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836541DFD@FHSDB2D11-2.csu.mcmaster.ca>

Dear Pamela,

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Pamela
> Wong
> Sent: Sunday, July 17, 2016 1:58 AM
> To: r-help at r-project.org
> Subject: [R] Type III SS and collinearity in R
> 
> Hi there
> It appears the Anova() function in the car package cannot compute type
> III SS when there is collinearity in the model. Has anyone else run into
> this problem?

That's right, by default anyway -- when there's *perfect* collinearity -- but in that circumstance what would be the point? That is, the SSs for the collinear terms are 0. You can force Anova() to (arbitrarily) deal with the collinearities

> library(car)
> mod <- lm(prestige ~ income + education + I(income + education), data=Duncan)
> Anova(mod, type="III", singular.ok=TRUE)

Anova Table (Type III tests)

Response: prestige
                      Sum Sq Df F value    Pr(>F)    
(Intercept)            360.2  1  2.0154    0.1631    
income                4474.2  1 25.0331 1.053e-05 ***
education             5516.1  1 30.8626 1.727e-06 ***
I(income + education)         0                      
Residuals             7506.7 42                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' '

But, again, what's the point?

> 
> I would use drop1() to obtain the SS but it does not provide the
> intercept...

Yes, drop1() reports the 0 SSs for all the collinear terms (on 0 df), which is better in a sense -- e.g.,

> drop1(mod)
Single term deletions

Model:
prestige ~ income + education + I(income + education)
                      Df Sum of Sq    RSS    AIC
<none>                             7506.7 236.26
income                 0         0 7506.7 236.26
education              0         0 7506.7 236.26
I(income + education)  0         0 7506.7 236.26


Why would it be interesting in this circumstance to know whether the intercept is 0? As you can see, Anova() reports the test; you can also get it from linearHypothesis():

> linearHypothesis(mod, "(Intercept)", singular.ok=TRUE)
Linear hypothesis test

Hypothesis:
(Intercept) = 0

Model 1: restricted model
Model 2: prestige ~ income + education + I(income + education)

  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     43 7866.9                           
2     42 7506.7  1    360.22 2.0154 0.1631

I suspect that there's a statistical issue lurking here that's not addressed by showing you how to get the tests you're asking for.

Best,
 John

--------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
Web: socserv.mcmaster.ca/jfox



> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Mon Jul 18 01:14:02 2016
From: jholtman at gmail.com (jim holtman)
Date: Sun, 17 Jul 2016 19:14:02 -0400
Subject: [R] means by year, month and day
In-Reply-To: <CAJ7mryJGyC=KethC7xLz7eMGYhchuctAW6BCUNC4N7RX3yc-RA@mail.gmail.com>
References: <CB5791F5EA3D82408B277900997482D27F7DA2D5@mboxes2.campus.vims.edu>
	<CAJ7mryJGyC=KethC7xLz7eMGYhchuctAW6BCUNC4N7RX3yc-RA@mail.gmail.com>
Message-ID: <CAAxdm-4PiOpwoX0qoc5KcCFy_7uwCXTL074csdUT4MySO65sgw@mail.gmail.com>

Here is an example of using dplyr.  Please provide a reasonable subset of
data.  Your was all for the same date.  Use 'dput' to put in your email.

> x <- read.table(text = "     X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP
DEWP
+   2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+   2015  2  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  2  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  2  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  2  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  2  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  2  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  2  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  2  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  2  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  2  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+   2015  3  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  3  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  3  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  3  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  3  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  3  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  3  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  3  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  3  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  3  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+  ",
+  header = TRUE,
+  as.is = TRUE)
>
>  library(dplyr)
>  by_year <- x %>%
+             group_by(X.YY) %>%
+             summarise_each(funs(mean))
>
>  by_ym <- x %>%
+             group_by(X.YY, MM ) %>%
+             summarise_each(funs(mean))
>
>  by_ymd <- x %>%
+             group_by(X.YY, MM, DD) %>%
+             summarise_each(funs(mean))
>
> by_year
Source: local data frame [1 x 10]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> by_ym
Source: local data frame [3 x 10]
Groups: X.YY [?]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <int> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     1     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
2  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
3  2015     3     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> by_ymd
Source: local data frame [3 x 10]
Groups: X.YY, MM [?]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <int> <int> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     1     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
2  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
3  2015     3     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
>


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Sun, Jul 17, 2016 at 5:42 PM, Jianling Fan <fanjianling at gmail.com> wrote:

> Hello Tom,
>
> try aggregate() or cast(). Both works.I prefer the latter.
>
>
> library(reshape)
> desc<-melt(mydata, measure.vars=c("WDI","R.WSP", "D.GST", "PRES",
> "ATMP", "DEWP"),
>            id.vars=c("X.YY","MM","DD"))
> summary<-cast(desc, X.YY+MM+DD~variable, mean)
>
>
>
>
>
>
>
>
>
> On 17 July 2016 at 06:22, Tom Mosca <tom at vims.edu> wrote:
> > Hello Good Folk,
> >
> > My dataframe looks like this:
> >> mydata
> >      X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
> > 1    2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
> > 2    2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
> > 3    2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
> > 4    2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
> > 5    2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
> > 6    2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
> > 7    2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
> > 8    2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
> > 9    2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
> > 10   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3 ?
> >
> > The first four columns are year, month, day, hour (0 ? 23).  I wish to
> take the means of the next six columns (WDIR, WSPD, GST, PRES, ATMP and
> DEWP) by year, month and day.  That is, I want daily averages.
> >
> > Please help.  Thank you.
> >
> > Tom
> >
> >         [[alternative HTML version deleted]]
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Jianling Fan
> ???
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Jul 18 01:39:48 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 17 Jul 2016 19:39:48 -0400
Subject: [R] findAssocs in TM package in R help?
In-Reply-To: <CAB=p7SrtHVwvmTKbM=FzOsxXyTjZxexLcggP9WWjsEHbWcKaGg@mail.gmail.com>
References: <CAB=p7Sps0FhtCyuH_h+iBsvA6mOMn7XBqaZ17SD8KXwYn4Hejw@mail.gmail.com>
	<CAB=p7SrtHVwvmTKbM=FzOsxXyTjZxexLcggP9WWjsEHbWcKaGg@mail.gmail.com>
Message-ID: <2e1ab681-8e32-87d8-5957-2122caefb258@gmail.com>

On 17/07/2016 6:31 PM, Shivi Bhatia wrote:
> Hi Team,
>
> Please suggest.
>
> On Sun, Jul 17, 2016 at 2:55 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:
>
>> Good Day,
>>
>> I am working on a text mining assignment and have built the document
>> matrix using the tm package.

We don't do homework here.  You should ask your instructor for help.

Duncan Murdoch

>>
>> Now I need to run findAssocs from my dtm with some word say 'like' with a
>> correlation of 0.70 but  as far as i have been researching it tells it this
>> function is only viable when we have more than 1 doc however in my case i
>> only have 1. So please suggest if there is an alternate to this issue.
>>
>> This is because there are words that correlate to each other in the data I
>> am dealing with.
>>
>> Kindly advice.
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mchajewski at hotmail.com  Mon Jul 18 02:18:06 2016
From: mchajewski at hotmail.com (Michael)
Date: Mon, 18 Jul 2016 00:18:06 +0000
Subject: [R] RStan 8 school example will not run
Message-ID: <DM3PR13MB06864B8086D942495ED98BD1C1360@DM3PR13MB0686.namprd13.prod.outlook.com>

Dear R-Help Forum,


I have successfully installed and run RStan 8schools.stan example on two laptops of my colleagues. Yet it will not run on mine. The only difference between my machine and those of my colleagues is that I have a 8GB RAM machine 9as possessed to their 4GB). I am running R 3.3.1, RStudio 0.99.902, Rtools33, and RStan 2.10.1. I have run the sample syntax (just as I did on my colleagues machines where it worked) in a 64-bit version, 32-bit, in RStudio, just in plain R. all with the same failure. Any insight would be greatly appreciated.


Michael


> setwd("C:/Workshop/") # Where 8schools.stan is located
> getwd()

[1] "C:/Workshop"

> fx <- inline::cxxfunction( signature(x = "integer", y = "numeric" ) , '
+       return ScalarReal( INTEGER(x)[0] * REAL(y)[0] ) ;
+ ' )

> fx( 2L, 5 ) # should be 10

[1] 10

> library(rstan) # observe startup messages

Loading required package: ggplot2
Loading required package: StanHeaders
rstan (Version 2.10.1, packaged: 2016-06-24 13:22:16 UTC, GitRev: 85f7a56811da)
For execution on a local, multicore CPU with excess RAM we recommend calling
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

> rstan_options(auto_write = TRUE)
> options(mc.cores = parallel::detectCores())

> # Running example
>
> schools_dat <- list(J = 8,
+                     y = c(28,  8, -3,  7, -1,  1, 18, 12),
+                     sigma = c(15, 10, 16, 11,  9, 11, 10, 18))

> fit <- stan(file = '8schools.stan', data = schools_dat,
+             iter = 1000, chains = 4)

In file included from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/mat.hpp:36:0,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/rev/mat.hpp:8,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math.hpp:4,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/src/stan/model/model_header.hpp:4,
                 from file1c2878472ee4.cpp:8:
C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/mat/err/check_positive_ordered.hpp: In function 'bool stan::math::check_positive_ordered(const char*, const char*, const Eigen::Matrix<Scalar, -1, 1>&)':
C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/mat/err/check_positive_ordered.hpp:39:67: warning: typedef 'size_type' locally defined but not used [-Wunused-local-typedefs]
       typedef typename index_type<Matrix<T_y, Dynamic, 1> >::type size_type;
                                                                   ^
In file included from C:/Program Files/R/R-3.3.1/library/BH/include/boost/multi_array/base.hpp:28:0,
                 from C:/Program Files/R/R-3.3.1/library/BH/include/boost/multi_array.hpp:21,
                 from C:/Program Files/R/R-3.3.1/library/BH/include/boost/numeric/odeint/util/multi_array_adaption.hpp:29,
                 from C:/Program Files/R/R-3.3.1/library/BH/include/boost/numeric/odeint.hpp:61,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/arr/functor/integrate_ode_rk45.hpp:13,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/arr.hpp:33,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/mat.hpp:232,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/rev/mat.hpp:8,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math.hpp:4,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/src/stan/model/model_header.hpp:4,
                 from file1c2878472ee4.cpp:8:
C:/Program Files/R/R-3.3.1/library/BH/include/boost/multi_array/concept_checks.hpp: In static member function 'static void boost::multi_array_concepts::detail::idgen_helper<N>::call(Array&, const IdxGen&, Call_Type)':
C:/Program Files/R/R-3.3.1/library/BH/include/boost/multi_array/concept_checks.hpp:42:43: warning: typedef 'index_range' locally defined but not used [-Wunused-local-typedefs]
       typedef typename Array::index_range index_range;
                                           ^
C:/Program Files/R/R-3.3.1/library/BH/include/boost/multi_array/concept_checks.hpp:43:37: warning: typedef 'index' locally defined but not used [-Wunused-local-typedefs]
       typedef typename Array::index index;
                                     ^
C:/Program Files/R/R-3.3.1/library/BH/include/boost/multi_array/concept_checks.hpp: In static member function 'static void boost::multi_array_concepts::detail::idgen_helper<0ull>::call(Array&, const IdxGen&, Call_Type)':
C:/Program Files/R/R-3.3.1/library/BH/include/boost/multi_array/concept_checks.hpp:53:43: warning: typedef 'index_range' locally defined but not used [-Wunused-local-typedefs]
       typedef typename Array::index_range index_range;
                                           ^
C:/Program Files/R/R-3.3.1/library/BH/include/boost/multi_array/concept_checks.hpp:54:37: warning: typedef 'index' locally defined but not used [-Wunused-local-typedefs]
       typedef typename Array::index index;
                                     ^
In file included from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/rev/core.hpp:42:0,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/rev/mat.hpp:4,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math.hpp:4,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/src/stan/model/model_header.hpp:4,
                 from file1c2878472ee4.cpp:8:
C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp: At global scope:
C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/rev/core/set_zero_all_adjoints.hpp:14:17: warning: 'void stan::math::set_zero_all_adjoints()' defined but not used [-Wunused-function]
     static void set_zero_all_adjoints() {
                 ^
g++.exe: error: Files/R/R-3.3.1/library/StanHeaders/libs/x64: No such file or directory

ERROR(s) during compilation: source code errors or compiler configuration errors!

Program source:
  1:
  2: // includes from the plugin
  3:
  4:
  5: // user includes
  6: #define STAN__SERVICES__COMMAND_HPP// Code generated by Stan version 2.10
  7:
  8: #include <stan/model/model_header.hpp>
  9:
 10: namespace model1c2860cf53a3_8schools_namespace {
 11:
 12: using std::istream;
 13: using std::string;
 14: using std::stringstream;
 15: using std::vector;
 16: using stan::io::dump;
 17: using stan::math::lgamma;
 18: using stan::model::prob_grad;
 19: using namespace stan::math;
 20:
 21: typedef Eigen::Matrix<double,Eigen::Dynamic,1> vector_d;
 22: typedef Eigen::Matrix<double,1,Eigen::Dynamic> row_vector_d;
 23: typedef Eigen::Matrix<double,Eigen::Dynamic,Eigen::Dynamic> matrix_d;
 24:
 25: static int current_statement_begin__;
 26:
 27: class model1c2860cf53a3_8schools : public prob_grad {
 28: private:
 29:     int J;
 30:     vector<double> y;
 31:     vector<double> sigma;
 32: public:
 33:     model1c2860cf53a3_8schools(stan::io::var_context& context__,
 34:         std::ostream* pstream__ = 0)
 35:         : prob_grad(0) {
 36:         current_statement_begin__ = -1;
 37:
 38:         static const char* function__ = "model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools";
 39:         (void) function__; // dummy call to supress warning
 40:         size_t pos__;
 41:         (void) pos__; // dummy call to supress warning
 42:         std::vector<int> vals_i__;
 43:         std::vector<double> vals_r__;
 44:         context__.validate_dims("data initialization", "J", "int", context__.to_vec());
 45:         J = int(0);
 46:         vals_i__ = context__.vals_i("J");
 47:         pos__ = 0;
 48:         J = vals_i__[pos__++];
 49:         context__.validate_dims("data initialization", "y", "double", context__.to_vec(J));
 50:         validate_non_negative_index("y", "J", J);
 51:         y = std::vector<double>(J,double(0));
 52:         vals_r__ = context__.vals_r("y");
 53:         pos__ = 0;
 54:         size_t y_limit_0__ = J;
 55:         for (size_t i_0__ = 0; i_0__ < y_limit_0__; ++i_0__) {
 56:             y[i_0__] = vals_r__[pos__++];
 57:         }
 58:         context__.validate_dims("data initialization", "sigma", "double", context__.to_vec(J));
 59:         validate_non_negative_index("sigma", "J", J);
 60:         sigma = std::vector<double>(J,double(0));
 61:         vals_r__ = context__.vals_r("sigma");
 62:         pos__ = 0;
 63:         size_t sigma_limit_0__ = J;
 64:         for (size_t i_0__ = 0; i_0__ < sigma_limit_0__; ++i_0__) {
 65:             sigma[i_0__] = vals_r__[pos__++];
 66:         }
 67:
 68:         // validate data
 69:         check_greater_or_equal(function__,"J",J,0);
 70:         for (int k0__ = 0; k0__ < J; ++k0__) {
 71:             check_greater_or_equal(function__,"sigma[k0__]",sigma[k0__],0);
 72:         }
 73:
 74:         double DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
 75:         (void) DUMMY_VAR__;  // suppress unused var warning
 76:
 77:
 78:         // initialize transformed variables to avoid seg fault on val access
 79:
 80:         try {
 81:         } catch (const std::exception& e) {
 82:             stan::lang::rethrow_located(e,current_statement_begin__);
 83:             // Next line prevents compiler griping about no return
 84:             throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***");
 85:         }
 86:
 87:         // validate transformed data
 88:
 89:         // set parameter ranges
 90:         num_params_r__ = 0U;
 91:         param_ranges_i__.clear();
 92:         ++num_params_r__;
 93:         ++num_params_r__;
 94:         num_params_r__ += J;
 95:     }
 96:
 97:     ~model1c2860cf53a3_8schools() { }
 98:
 99:
100:     void transform_inits(const stan::io::var_context& context__,
101:                          std::vector<int>& params_i__,
102:                          std::vector<double>& params_r__,
103:                          std::ostream* pstream__) const {
104:         stan::io::writer<double> writer__(params_r__,params_i__);
105:         size_t pos__;
106:         (void) pos__; // dummy call to supress warning
107:         std::vector<double> vals_r__;
108:         std::vector<int> vals_i__;
109:
110:         if (!(context__.contains_r("mu")))
111:             throw std::runtime_error("variable mu missing");
112:         vals_r__ = context__.vals_r("mu");
113:         pos__ = 0U;
114:         context__.validate_dims("initialization", "mu", "double", context__.to_vec());
115:         double mu(0);
116:         mu = vals_r__[pos__++];
117:         try {
118:             writer__.scalar_unconstrain(mu);
119:         } catch (const std::exception& e) {
120:             throw std::runtime_error(std::string("Error transforming variable mu: ") + e.what());
121:         }
122:
123:         if (!(context__.contains_r("tau")))
124:             throw std::runtime_error("variable tau missing");
125:         vals_r__ = context__.vals_r("tau");
126:         pos__ = 0U;
127:         context__.validate_dims("initialization", "tau", "double", context__.to_vec());
128:         double tau(0);
129:         tau = vals_r__[pos__++];
130:         try {
131:             writer__.scalar_lb_unconstrain(0,tau);
132:         } catch (const std::exception& e) {
133:             throw std::runtime_error(std::string("Error transforming variable tau: ") + e.what());
134:         }
135:
136:         if (!(context__.contains_r("eta")))
137:             throw std::runtime_error("variable eta missing");
138:         vals_r__ = context__.vals_r("eta");
139:         pos__ = 0U;
140:         context__.validate_dims("initialization", "eta", "double", context__.to_vec(J));
141:         std::vector<double> eta(J,double(0));
142:         for (int i0__ = 0U; i0__ < J; ++i0__)
143:             eta[i0__] = vals_r__[pos__++];
144:         for (int i0__ = 0U; i0__ < J; ++i0__)
145:             try {
146:             writer__.scalar_unconstrain(eta[i0__]);
147:         } catch (const std::exception& e) {
148:             throw std::runtime_error(std::string("Error transforming variable eta: ") + e.what());
149:         }
150:
151:         params_r__ = writer__.data_r();
152:         params_i__ = writer__.data_i();
153:     }
154:
155:     void transform_inits(const stan::io::var_context& context,
156:                          Eigen::Matrix<double,Eigen::Dynamic,1>& params_r,
157:                          std::ostream* pstream__) const {
158:       std::vector<double> params_r_vec;
159:       std::vector<int> params_i_vec;
160:       transform_inits(context, params_i_vec, params_r_vec, pstream__);
161:       params_r.resize(params_r_vec.size());
162:       for (int i = 0; i < params_r.size(); ++i)
163:         params_r(i) = params_r_vec[i];
164:     }
165:
166:
167:     template <bool propto__, bool jacobian__, typename T__>
168:     T__ log_prob(vector<T__>& params_r__,
169:                  vector<int>& params_i__,
170:                  std::ostream* pstream__ = 0) const {
171:
172:         T__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
173:         (void) DUMMY_VAR__;  // suppress unused var warning
174:
175:         T__ lp__(0.0);
176:         stan::math::accumulator<T__> lp_accum__;
177:
178:         // model parameters
179:         stan::io::reader<T__> in__(params_r__,params_i__);
180:
181:         T__ mu;
182:         (void) mu;  // dummy to suppress unused var warning
183:         if (jacobian__)
184:             mu = in__.scalar_constrain(lp__);
185:         else
186:             mu = in__.scalar_constrain();
187:
188:         T__ tau;
189:         (void) tau;  // dummy to suppress unused var warning
190:         if (jacobian__)
191:             tau = in__.scalar_lb_constrain(0,lp__);
192:         else
193:             tau = in__.scalar_lb_constrain(0);
194:
195:         vector<T__> eta;
196:         size_t dim_eta_0__ = J;
197:         eta.reserve(dim_eta_0__);
198:         for (size_t k_0__ = 0; k_0__ < dim_eta_0__; ++k_0__) {
199:             if (jacobian__)
200:                 eta.push_back(in__.scalar_constrain(lp__));
201:             else
202:                 eta.push_back(in__.scalar_constrain());
203:         }
204:
205:
206:         // transformed parameters
207:         vector<T__> theta(J);
208:
209:         // initialize transformed variables to avoid seg fault on val access
210:         stan::math::fill(theta,DUMMY_VAR__);
211:
212:         try {
213:             current_statement_begin__ = 13;
214:             for (int j = 1; j <= J; ++j) {
215:                 current_statement_begin__ = 14;
216:                 stan::math::assign(get_base1_lhs(theta,j,"theta",1), (mu + (tau * get_base1(eta,j,"eta",1))));
217:             }
218:         } catch (const std::exception& e) {
219:             stan::lang::rethrow_located(e,current_statement_begin__);
220:             // Next line prevents compiler griping about no return
221:             throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***");
222:         }
223:
224:         // validate transformed parameters
225:         for (int i0__ = 0; i0__ < J; ++i0__) {
226:             if (stan::math::is_uninitialized(theta[i0__])) {
227:                 std::stringstream msg__;
228:                 msg__ << "Undefined transformed parameter: theta" << '[' << i0__ << ']';
229:                 throw std::runtime_error(msg__.str());
230:             }
231:         }
232:
233:         const char* function__ = "validate transformed params";
234:         (void) function__;  // dummy to suppress unused var warning
235:
236:         // model body
237:         try {
238:             current_statement_begin__ = 17;
239:             lp_accum__.add(normal_log<propto__>(eta, 0, 1));
240:             current_statement_begin__ = 18;
241:             lp_accum__.add(normal_log<propto__>(y, theta, sigma));
242:         } catch (const std::exception& e) {
243:             stan::lang::rethrow_located(e,current_statement_begin__);
244:             // Next line prevents compiler griping about no return
245:             throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***");
246:         }
247:
248:         lp_accum__.add(lp__);
249:         return lp_accum__.sum();
250:
251:     } // log_prob()
252:
253:     template <bool propto, bool jacobian, typename T_>
254:     T_ log_prob(Eigen::Matrix<T_,Eigen::Dynamic,1>& params_r,
255:                std::ostream* pstream = 0) const {
256:       std::vector<T_> vec_params_r;
257:       vec_params_r.reserve(params_r.size());
258:       for (int i = 0; i < params_r.size(); ++i)
259:         vec_params_r.push_back(params_r(i));
260:       std::vector<int> vec_params_i;
261:       return log_prob<propto,jacobian,T_>(vec_params_r, vec_params_i, pstream);
262:     }
263:
264:
265:     void get_param_names(std::vector<std::string>& names__) const {
266:         names__.resize(0);
267:         names__.push_back("mu");
268:         names__.push_back("tau");
269:         names__.push_back("eta");
270:         names__.push_back("theta");
271:     }
272:
273:
274:     void get_dims(std::vector<std::vector<size_t> >& dimss__) const {
275:         dimss__.resize(0);
276:         std::vector<size_t> dims__;
277:         dims__.resize(0);
278:         dimss__.push_back(dims__);
279:         dims__.resize(0);
280:         dimss__.push_back(dims__);
281:         dims__.resize(0);
282:         dims__.push_back(J);
283:         dimss__.push_back(dims__);
284:         dims__.resize(0);
285:         dims__.push_back(J);
286:         dimss__.push_back(dims__);
287:     }
288:
289:     template <typename RNG>
290:     void write_array(RNG& base_rng__,
291:                      std::vector<double>& params_r__,
292:                      std::vector<int>& params_i__,
293:                      std::vector<double>& vars__,
294:                      bool include_tparams__ = true,
295:                      bool include_gqs__ = true,
296:                      std::ostream* pstream__ = 0) const {
297:         vars__.resize(0);
298:         stan::io::reader<double> in__(params_r__,params_i__);
299:         static const char* function__ = "model1c2860cf53a3_8schools_namespace::write_array";
300:         (void) function__; // dummy call to supress warning
301:         // read-transform, write parameters
302:         double mu = in__.scalar_constrain();
303:         double tau = in__.scalar_lb_constrain(0);
304:         vector<double> eta;
305:         size_t dim_eta_0__ = J;
306:         for (size_t k_0__ = 0; k_0__ < dim_eta_0__; ++k_0__) {
307:             eta.push_back(in__.scalar_constrain());
308:         }
309:         vars__.push_back(mu);
310:         vars__.push_back(tau);
311:         for (int k_0__ = 0; k_0__ < J; ++k_0__) {
312:             vars__.push_back(eta[k_0__]);
313:         }
314:
315:         if (!include_tparams__) return;
316:         // declare and define transformed parameters
317:         double lp__ = 0.0;
318:         (void) lp__; // dummy call to supress warning
319:         stan::math::accumulator<double> lp_accum__;
320:
321:         vector<double> theta(J, 0.0);
322:
323:         try {
324:             current_statement_begin__ = 13;
325:             for (int j = 1; j <= J; ++j) {
326:                 current_statement_begin__ = 14;
327:                 stan::math::assign(get_base1_lhs(theta,j,"theta",1), (mu + (tau * get_base1(eta,j,"eta",1))));
328:             }
329:         } catch (const std::exception& e) {
330:             stan::lang::rethrow_located(e,current_statement_begin__);
331:             // Next line prevents compiler griping about no return
332:             throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***");
333:         }
334:
335:         // validate transformed parameters
336:
337:         // write transformed parameters
338:         for (int k_0__ = 0; k_0__ < J; ++k_0__) {
339:             vars__.push_back(theta[k_0__]);
340:         }
341:
342:         if (!include_gqs__) return;
343:         // declare and define generated quantities
344:
345:         double DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
346:         (void) DUMMY_VAR__;  // suppress unused var warning
347:
348:
349:         // initialize transformed variables to avoid seg fault on val access
350:
351:         try {
352:         } catch (const std::exception& e) {
353:             stan::lang::rethrow_located(e,current_statement_begin__);
354:             // Next line prevents compiler griping about no return
355:             throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***");
356:         }
357:
358:         // validate generated quantities
359:
360:         // write generated quantities
361:     }
362:
363:     template <typename RNG>
364:     void write_array(RNG& base_rng,
365:                      Eigen::Matrix<double,Eigen::Dynamic,1>& params_r,
366:                      Eigen::Matrix<double,Eigen::Dynamic,1>& vars,
367:                      bool include_tparams = true,
368:                      bool include_gqs = true,
369:                      std::ostream* pstream = 0) const {
370:       std::vector<double> params_r_vec(params_r.size());
371:       for (int i = 0; i < params_r.size(); ++i)
372:         params_r_vec[i] = params_r(i);
373:       std::vector<double> vars_vec;
374:       std::vector<int> params_i_vec;
375:       write_array(base_rng,params_r_vec,params_i_vec,vars_vec,include_tparams,include_gqs,pstream);
376:       vars.resize(vars_vec.size());
377:       for (int i = 0; i < vars.size(); ++i)
378:         vars(i) = vars_vec[i];
379:     }
380:
381:     static std::string model_name() {
382:         return "model1c2860cf53a3_8schools";
383:     }
384:
385:
386:     void constrained_param_names(std::vector<std::string>& param_names__,
387:                                  bool include_tparams__ = true,
388:                                  bool include_gqs__ = true) const {
389:         std::stringstream param_name_stream__;
390:         param_name_stream__.str(std::string());
391:         param_name_stream__ << "mu";
392:         param_names__.push_back(param_name_stream__.str());
393:         param_name_stream__.str(std::string());
394:         param_name_stream__ << "tau";
395:         param_names__.push_back(param_name_stream__.str());
396:         for (int k_0__ = 1; k_0__ <= J; ++k_0__) {
397:             param_name_stream__.str(std::string());
398:             param_name_stream__ << "eta" << '.' << k_0__;
399:             param_names__.push_back(param_name_stream__.str());
400:         }
401:
402:         if (!include_gqs__ && !include_tparams__) return;
403:         for (int k_0__ = 1; k_0__ <= J; ++k_0__) {
404:             param_name_stream__.str(std::string());
405:             param_name_stream__ << "theta" << '.' << k_0__;
406:             param_names__.push_back(param_name_stream__.str());
407:         }
408:
409:         if (!include_gqs__) return;
410:     }
411:
412:
413:     void unconstrained_param_names(std::vector<std::string>& param_names__,
414:                                    bool include_tparams__ = true,
415:                                    bool include_gqs__ = true) const {
416:         std::stringstream param_name_stream__;
417:         param_name_stream__.str(std::string());
418:         param_name_stream__ << "mu";
419:         param_names__.push_back(param_name_stream__.str());
420:         param_name_stream__.str(std::string());
421:         param_name_stream__ << "tau";
422:         param_names__.push_back(param_name_stream__.str());
423:         for (int k_0__ = 1; k_0__ <= J; ++k_0__) {
424:             param_name_stream__.str(std::string());
425:             param_name_stream__ << "eta" << '.' << k_0__;
426:             param_names__.push_back(param_name_stream__.str());
427:         }
428:
429:         if (!include_gqs__ && !include_tparams__) return;
430:         for (int k_0__ = 1; k_0__ <= J; ++k_0__) {
431:             param_name_stream__.str(std::string());
432:             param_name_stream__ << "theta" << '.' << k_0__;
433:             param_names__.push_back(param_name_stream__.str());
434:         }
435:
436:         if (!include_gqs__) return;
437:     }
438:
439: }; // model
440:
441: } // namespace
442:
443: typedef model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools stan_model;
444:
445: #include <rstan/rstaninc.hpp>
446: /**
447:  * Define Rcpp Module to expose stan_fit's functions to R.
448:  */
449: RCPP_MODULE(stan_fit4model1c2860cf53a3_8schools_mod){
450:   Rcpp::class_<rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools,
451:                boost::random::ecuyer1988> >("stan_fit4model1c2860cf53a3_8schools")
452:     // .constructor<Rcpp::List>()
453:     .constructor<SEXP, SEXP>()
454:     // .constructor<SEXP, SEXP>()
455:     .method("call_sampler",
456:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::call_sampler)
457:     .method("param_names",
458:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::param_names)
459:     .method("param_names_oi",
460:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::param_names_oi)
461:     .method("param_fnames_oi",
462:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::param_fnames_oi)
463:     .method("param_dims",
464:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::param_dims)
465:     .method("param_dims_oi",
466:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::param_dims_oi)
467:     .method("update_param_oi",
468:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::update_param_oi)
469:     .method("param_oi_tidx",
470:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::param_oi_tidx)
471:     .method("grad_log_prob",
472:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::grad_log_prob)
473:     .method("log_prob",
474:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::log_prob)
475:     .method("unconstrain_pars",
476:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::unconstrain_pars)
477:     .method("constrain_pars",
478:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::constrain_pars)
479:     .method("num_pars_unconstrained",
480:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::num_pars_unconstrained)
481:     .method("unconstrained_param_names",
482:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::unconstrained_param_names)
483:     .method("constrained_param_names",
484:             &rstan::stan_fit<model1c2860cf53a3_8schools_namespace::model1c2860cf53a3_8schools, boost::random::ecuyer1988>::constrained_param_names)
485:     ;
486: }
487:
488: // declarations
489: extern "C" {
490: SEXP file1c2878472ee4( ) ;
491: }
492:
493: // definition
494:
495: SEXP file1c2878472ee4(  ){
496:  return Rcpp::wrap("8schools");
497: }
498:
499:

Error in compileCode(f, code, language = language, verbose = verbose) :
  Compilation ERROR, function(s)/method(s) not created! In file included from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/mat.hpp:36:0,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/rev/mat.hpp:8,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math.hpp:4,
                 from C:/Program Files/R/R-3.3.1/library/StanHeaders/include/src/stan/model/model_header.hpp:4,
                 from file1c2878472ee4.cpp:8:
C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/mat/err/check_positive_ordered.hpp: In function 'bool stan::math::check_positive_ordered(const char*, const char*, const Eigen::Matrix<Scalar, -1, 1>&)':
C:/Program Files/R/R-3.3.1/library/StanHeaders/include/stan/math/prim/mat/err/check_positive_ordered.hpp:39:67: warning: typedef 'size_type' locally defined but not used [-Wunused-local-typedefs]
       typedef typename index_type<Matrix<T_y, Dynamic,



	[[alternative HTML version deleted]]


From jean-externe.maurice at edf.fr  Mon Jul 18 10:24:50 2016
From: jean-externe.maurice at edf.fr (MAURICE Jean - externe)
Date: Mon, 18 Jul 2016 08:24:50 +0000
Subject: [R] I can't see my questions and their answers in this mailing list
	!
Message-ID: <2dba0863126c4e28852c6b44353f90a3@NOEINTPEXMU007.NEOPROD.EDF.FR>

HI,

I tried, for some months, to ask questions on this list. I think that my questions are 'sent somewhere' because I received answers for them in private but not via the list and I don't see neither my questions neither your answers on the list.. What am I doing wrong ?

Jean in France
-------------- next part --------------



Ce message et toutes les pi?ces jointes (ci-apr?s le 'Message') sont ?tablis ? l'intention exclusive des destinataires et les informations qui y figurent sont strictement confidentielles. Toute utilisation de ce Message non conforme ? sa destination, toute diffusion ou toute publication totale ou partielle, est interdite sauf autorisation expresse.

Si vous n'?tes pas le destinataire de ce Message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d'en utiliser tout ou partie. Si vous avez re?u ce Message par erreur, merci de le supprimer de votre syst?me, ainsi que toutes ses copies, et de n'en garder aucune trace sur quelque support que ce soit. Nous vous remercions ?galement d'en avertir imm?diatement l'exp?diteur par retour du message.

Il est impossible de garantir que les communications par messagerie ?lectronique arrivent en temps utile, sont s?curis?es ou d?nu?es de toute erreur ou virus.
____________________________________________________

This message and any attachments (the 'Message') are intended solely for the addressees. The information contained in this Message is confidential. Any use of information contained in this Message not in accord with its purpose, any dissemination or disclosure, either whole or partial, is prohibited except formal approval.

If you are not the addressee, you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return message.

E-mail communication cannot be guaranteed to be timely secure, error or virus-free.

From Katharina.Manderscheid at unilu.ch  Mon Jul 18 10:58:52 2016
From: Katharina.Manderscheid at unilu.ch (Manderscheid Katharina)
Date: Mon, 18 Jul 2016 08:58:52 +0000
Subject: [R] GREA - installation problems
Message-ID: <068DFFD3BCE5DE42813CE7C004FA883D30223A40@unetmxs02>

i came across the GREA add-in for RStudio on R-bloggers: http://feedproxy.google.com/~r/RBloggers/~3/dU-3B7fQJ2c/?utm_source=feedburner&utm_medium=email
https://github.com/Stan125/GREA
and wanted to try it straight away - yet I failed already with the installation.
after typing in
> devtools::install_github("Stan125/GREA")
I get the following:

Downloading GitHub repo Stan125/GREA at master

from URL https://api.github.com/repos/Stan125/GREA/zipball/master

Installing GREA

"C:/PROGRA~1/R/R-33~1.1/bin/x64/R" --no-site-file --no-environ --no-save  \

  --no-restore --quiet CMD INSTALL  \

  "C:/Users/mandersk/AppData/Local/Temp/RtmpSEKk1L/devtools18e0379148ae/Stan125-GREA-e80db34"  \

  --library="\\unetna01/mandersk$/Daten/R/win-library/3.3" --install-tests



* installing *source* package 'GREA' ...

** R

** inst

** preparing package for lazy loading

** help

*** installing help indices

** building package indices

** testing if installed package can be loaded

*** arch - i386

Warnung in library(pkg_name, lib.loc = lib, character.only = TRUE, logical.return = TRUE)

  there is no package called 'GREA'

Fehler: Laden fehlgeschlagen

Ausf?hrung angehalten

*** arch - x64

Warnung in library(pkg_name, lib.loc = lib, character.only = TRUE, logical.return = TRUE)

  there is no package called 'GREA'

Fehler: Laden fehlgeschlagen

Ausf?hrung angehalten

ERROR: loading failed for 'i386', 'x64'

* removing '\\unetna01/mandersk$/Daten/R/win-library/3.3/GREA'

Error: Command failed (1)

any suggestions??
thanks, k@

	[[alternative HTML version deleted]]


From ivan.calandra at univ-reims.fr  Mon Jul 18 11:00:22 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Mon, 18 Jul 2016 11:00:22 +0200
Subject: [R] I can't see my questions and their answers in this mailing
 list !
In-Reply-To: <2dba0863126c4e28852c6b44353f90a3@NOEINTPEXMU007.NEOPROD.EDF.FR>
References: <2dba0863126c4e28852c6b44353f90a3@NOEINTPEXMU007.NEOPROD.EDF.FR>
Message-ID: <e2449d39-291b-d90f-41df-9457da55e4d1@univ-reims.fr>

Hi Jean,

This question made it to the list at last. My answer probably 
(hopefully) will.
But I cannot tell you why your previous attempts did not work. Have your 
questions been moderated...?

Ivan

--
Ivan Calandra, PhD
Scientific Mediator
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/

Le 18/07/2016 ? 10:24, MAURICE Jean - externe a ?crit :
> HI,
>
> I tried, for some months, to ask questions on this list. I think that my questions are 'sent somewhere' because I received answers for them in private but not via the list and I don't see neither my questions neither your answers on the list.. What am I doing wrong ?
>
> Jean in France
>
>
>
>
> Ce message et toutes les pi?ces jointes (ci-apr?s le 'Message') sont ?tablis ? l'intention exclusive des destinataires et les informations qui y figurent sont strictement confidentielles. Toute utilisation de ce Message non conforme ? sa destination, toute diffusion ou toute publication totale ou partielle, est interdite sauf autorisation expresse.
>
> Si vous n'?tes pas le destinataire de ce Message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d'en utiliser tout ou partie. Si vous avez re?u ce Message par erreur, merci de le supprimer de votre syst?me, ainsi que toutes ses copies, et de n'en garder aucune trace sur quelque support que ce soit. Nous vous remercions ?galement d'en avertir imm?diatement l'exp?diteur par retour du message.
>
> Il est impossible de garantir que les communications par messagerie ?lectronique arrivent en temps utile, sont s?curis?es ou d?nu?es de toute erreur ou virus.
> ____________________________________________________
>
> This message and any attachments (the 'Message') are intended solely for the addressees. The information contained in this Message is confidential. Any use of information contained in this Message not in accord with its purpose, any dissemination or disclosure, either whole or partial, is prohibited except formal approval.
>
> If you are not the addressee, you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return message.
>
> E-mail communication cannot be guaranteed to be timely secure, error or virus-free.
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jean-externe.maurice at edf.fr  Mon Jul 18 11:15:11 2016
From: jean-externe.maurice at edf.fr (MAURICE Jean - externe)
Date: Mon, 18 Jul 2016 09:15:11 +0000
Subject: [R] I can't see my questions and their answers in this mailing
 list !
In-Reply-To: <e2449d39-291b-d90f-41df-9457da55e4d1@univ-reims.fr>
References: <2dba0863126c4e28852c6b44353f90a3@NOEINTPEXMU007.NEOPROD.EDF.FR>
	<e2449d39-291b-d90f-41df-9457da55e4d1@univ-reims.fr>
Message-ID: <c8fbab10c35c4bb68231d6c27f7bc3c0@NOEINTPEXMU007.NEOPROD.EDF.FR>

Hi Ivan : I can see your answer on the list (but not my question!).
The thing I have done this time is to send my question to both @ r-help at r-project.org and r-help-bounces at r-project.org

I don't think my previous questions were moderated : they were strictly about R problems.

Thank Ivan
Jean in France

PS Mais peut-?tre que le chemin de Reims ? Grenoble est plus court !!!!



Ce message et toutes les pi?ces jointes (ci-apr?s le 'Message') sont ?tablis ? l'intention exclusive des destinataires et les informations qui y figurent sont strictement confidentielles. Toute utilisation de ce Message non conforme ? sa destination, toute diffusion ou toute publication totale ou partielle, est interdite sauf autorisation expresse.

Si vous n'?tes pas le destinataire de ce Message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d'en utiliser tout ou partie. Si vous avez re?u ce Message par erreur, merci de le supprimer de votre syst?me, ainsi que toutes ses copies, et de n'en garder aucune trace sur quelque support que ce soit. Nous vous remercions ?galement d'en avertir imm?diatement l'exp?diteur par retour du message.

Il est impossible de garantir que les communications par messagerie ?lectronique arrivent en temps utile, sont s?curis?es ou d?nu?es de toute erreur ou virus.
____________________________________________________

This message and any attachments (the 'Message') are intended solely for the addressees. The information contained in this Message is confidential. Any use of information contained in this Message not in accord with its purpose, any dissemination or disclosure, either whole or partial, is prohibited except formal approval.

If you are not the addressee, you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return message.

E-mail communication cannot be guaranteed to be timely secure, error or virus-free.


From ivan.calandra at univ-reims.fr  Mon Jul 18 11:19:44 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Mon, 18 Jul 2016 11:19:44 +0200
Subject: [R] I can't see my questions and their answers in this mailing
 list !
In-Reply-To: <c8fbab10c35c4bb68231d6c27f7bc3c0@NOEINTPEXMU007.NEOPROD.EDF.FR>
References: <2dba0863126c4e28852c6b44353f90a3@NOEINTPEXMU007.NEOPROD.EDF.FR>
	<e2449d39-291b-d90f-41df-9457da55e4d1@univ-reims.fr>
	<c8fbab10c35c4bb68231d6c27f7bc3c0@NOEINTPEXMU007.NEOPROD.EDF.FR>
Message-ID: <e588b73e-b3bf-739c-3267-7685362560c9@univ-reims.fr>

Jean,

I got an idea. In the mailing list membership configuration, there is an 
option called " Receive your own posts to the list?". Yours might be 
turned off.

Follow this link to edit your subscription (at the end of each e-mail): 
https://stat.ethz.ch/mailman/listinfo/r-help

HTH,
Ivan

--
Ivan Calandra, PhD
Scientific Mediator
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/

Le 18/07/2016 ? 11:15, MAURICE Jean - externe a ?crit :
> Hi Ivan : I can see your answer on the list (but not my question!).
> The thing I have done this time is to send my question to both @ r-help at r-project.org and r-help-bounces at r-project.org
>
> I don't think my previous questions were moderated : they were strictly about R problems.
>
> Thank Ivan
> Jean in France
>
> PS Mais peut-?tre que le chemin de Reims ? Grenoble est plus court !!!!
>
>
>
> Ce message et toutes les pi?ces jointes (ci-apr?s le 'Message') sont ?tablis ? l'intention exclusive des destinataires et les informations qui y figurent sont strictement confidentielles. Toute utilisation de ce Message non conforme ? sa destination, toute diffusion ou toute publication totale ou partielle, est interdite sauf autorisation expresse.
>
> Si vous n'?tes pas le destinataire de ce Message, il vous est interdit de le copier, de le faire suivre, de le divulguer ou d'en utiliser tout ou partie. Si vous avez re?u ce Message par erreur, merci de le supprimer de votre syst?me, ainsi que toutes ses copies, et de n'en garder aucune trace sur quelque support que ce soit. Nous vous remercions ?galement d'en avertir imm?diatement l'exp?diteur par retour du message.
>
> Il est impossible de garantir que les communications par messagerie ?lectronique arrivent en temps utile, sont s?curis?es ou d?nu?es de toute erreur ou virus.
> ____________________________________________________
>
> This message and any attachments (the 'Message') are intended solely for the addressees. The information contained in this Message is confidential. Any use of information contained in this Message not in accord with its purpose, any dissemination or disclosure, either whole or partial, is prohibited except formal approval.
>
> If you are not the addressee, you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return message.
>
> E-mail communication cannot be guaranteed to be timely secure, error or virus-free.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From amwootte at ncsu.edu  Mon Jul 18 15:16:48 2016
From: amwootte at ncsu.edu (Adrienne Wootten)
Date: Mon, 18 Jul 2016 09:16:48 -0400
Subject: [R] intersection of two polygons which are not shapefiles
Message-ID: <CAOV3wDBSa3oBs1zsZzO7Vb_x1=dyGx6zXrr9=Ghsy_a1sqsBhg@mail.gmail.com>

All,

Greetings! I hope things are going well for all!  I apologize if someone's
already answered this and I'm just not finding it.  Here's what I'd like to
do, but I'm hitting a brick wall with it.

I have two sets of points for which I've already determined which ones
points for the boundaries with the chull function.  What I need for what
I'm doing is the coordinates where the two resulting polygons overlap.
These are not raster grids, nor shapefiles.  What I have to work with are
the two data frames with the points at the vertices of each polygon
(included below).

> chx1
              x          y
1  0.5822569 -0.5555878
2  0.5338428 -0.5883604
3 -0.3442943 -0.6701115
4  -0.7409293  0.2286962
5  0.2147221  0.8061485
6  0.4914146  0.4941865
7  0.5822569 -0.5555878

> chx2
              x          y
1  0.7163506 -0.4357497
2  0.6513128 -0.5395180
3   0.1818315 -0.6317423
4  -0.6394281 -0.5765610
5 -0.6044681  0.1831627
6 -0.5799485  0.3231473
7  0.2248476  0.9601908
8  0.7163506 -0.4357497


If anyone has any ideas on how to get what I'm after I'd appreciate it!
I've tried a lot of things from the raster, rgeos, and more.  Knowing me
it's something obvious I'm just not seeing right now.

Thanks all!
Adrienne


-- 
Adrienne Wootten
Ph.D Candidate / Graduate Research Assistant
State Climate Office of North Carolina
Department of Marine, Earth and Atmospheric Sciences
North Carolina State University

	[[alternative HTML version deleted]]


From S.Ellison at LGCGroup.com  Mon Jul 18 16:29:11 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Mon, 18 Jul 2016 15:29:11 +0100
Subject: [R] I can't see my questions and their answers in this mailing
 list !
In-Reply-To: <e588b73e-b3bf-739c-3267-7685362560c9@univ-reims.fr>
References: <2dba0863126c4e28852c6b44353f90a3@NOEINTPEXMU007.NEOPROD.EDF.FR>
	<e2449d39-291b-d90f-41df-9457da55e4d1@univ-reims.fr>
	<c8fbab10c35c4bb68231d6c27f7bc3c0@NOEINTPEXMU007.NEOPROD.EDF.FR>
	<e588b73e-b3bf-739c-3267-7685362560c9@univ-reims.fr>
Message-ID: <1A8C1289955EF649A09086A153E2672403EB13367B@GBTEDVPEXCMB04.corp.lgc-group.com>

> I got an idea. In the mailing list membership configuration, there is an option
> called " Receive your own posts to the list?". Yours might be turned off.

(At least) one other issue might be intruding: I found recently that our chosen email filter had been instructed to use indiscriminate spoofing detection, which at least by default it does without notification or warning*. I found after a mystifying absence of own posts that this is recognising _all_ my own posts to R-help as spoofed emails because of the (perfectly reasonable) policy of showing the OP's email in the 'from' field. It was unfortunately set to do so without notification or warning. For this particular filtering product, whitelisting R-help does not fix the problem - the product ignores the originating server information when whitelisting. They seem to have forgotten that legitimate mailing lists exist.

Presumably other email filtering products could be doing the same. 

S Ellison

*To be fair to the product**, an email report is available - if you go to the relevant web portal and manually request it. Of course, the only reason you'd do that is if you know that it's needed and where the website is. You may have to ask your IT staff about that.

**Which I am very happy to identify on an individual basis if you think this might be your problem.




*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From murdoch.duncan at gmail.com  Mon Jul 18 16:47:10 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 18 Jul 2016 10:47:10 -0400
Subject: [R] findAssocs in TM package in R help?
In-Reply-To: <CAB=p7SoPD-c3DOXEyAmAcEWeSGQDsE7sryTvnLOdxMyB82EsHQ@mail.gmail.com>
References: <CAB=p7Sps0FhtCyuH_h+iBsvA6mOMn7XBqaZ17SD8KXwYn4Hejw@mail.gmail.com>
	<CAB=p7SrtHVwvmTKbM=FzOsxXyTjZxexLcggP9WWjsEHbWcKaGg@mail.gmail.com>
	<2e1ab681-8e32-87d8-5957-2122caefb258@gmail.com>
	<CAB=p7SoPD-c3DOXEyAmAcEWeSGQDsE7sryTvnLOdxMyB82EsHQ@mail.gmail.com>
Message-ID: <15cacdf0-f741-509e-14a8-8e9e7c825faa@gmail.com>

On 18/07/2016 10:17 AM, Shivi Bhatia wrote:
> Hi Duncan,
>
> This is no homework, i am asking for a package a solution or other
> measures to achieve this. I am not asking for a homework or to have some
> one else do the work for me.

Sorry, "text mining assignment" sounded like homework to me.  But some 
assignments aren't homework.

Duncan Murdoch

>
> On Mon, Jul 18, 2016 at 5:09 AM, Duncan Murdoch
> <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>> wrote:
>
>     On 17/07/2016 6:31 PM, Shivi Bhatia wrote:
>
>         Hi Team,
>
>         Please suggest.
>
>         On Sun, Jul 17, 2016 at 2:55 AM, Shivi Bhatia
>         <shivipmp82 at gmail.com <mailto:shivipmp82 at gmail.com>> wrote:
>
>             Good Day,
>
>             I am working on a text mining assignment and have built the
>             document
>             matrix using the tm package.
>
>
>     We don't do homework here.  You should ask your instructor for help.
>
>     Duncan Murdoch
>
>
>             Now I need to run findAssocs from my dtm with some word say
>             'like' with a
>             correlation of 0.70 but  as far as i have been researching
>             it tells it this
>             function is only viable when we have more than 1 doc however
>             in my case i
>             only have 1. So please suggest if there is an alternate to
>             this issue.
>
>             This is because there are words that correlate to each other
>             in the data I
>             am dealing with.
>
>             Kindly advice.
>
>
>                 [[alternative HTML version deleted]]
>
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>         -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         and provide commented, minimal, self-contained, reproducible code.
>
>
>


From S.Ellison at LGCGroup.com  Mon Jul 18 16:47:20 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Mon, 18 Jul 2016 15:47:20 +0100
Subject: [R] intersection of two polygons which are not shapefiles
In-Reply-To: <CAOV3wDBSa3oBs1zsZzO7Vb_x1=dyGx6zXrr9=Ghsy_a1sqsBhg@mail.gmail.com>
References: <CAOV3wDBSa3oBs1zsZzO7Vb_x1=dyGx6zXrr9=Ghsy_a1sqsBhg@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403EB133687@GBTEDVPEXCMB04.corp.lgc-group.com>

> I have two sets of points for which I've already determined which ones points
> for the boundaries with the chull function.  What I need for what I'm doing is
> the coordinates where the two resulting polygons overlap.

There's a relevant answer on mathoverflow that might help if you're thinking of writing code: see
http://mathoverflow.net/questions/130577/how-to-find-overlap-between-two-convex-hulls-along-with-the-overlap-area. 

The other 'obvious' approach is to find one of the various shapefile packages in R, convert your chull to such an object and use something like gIntersection() from rgeos or intersect from raster (see http://gis.stackexchange.com/questions/140504/extracting-intersection-areas-in-r)

S Ellison



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From omlerembr at gmail.com  Mon Jul 18 11:45:00 2016
From: omlerembr at gmail.com (Mbr Mbr)
Date: Mon, 18 Jul 2016 11:45:00 +0200
Subject: [R] Install R version 3.3 on Debian server
Message-ID: <CAEhiiiuUB5cL=qTR6FO5GcJLSgf6M25k_-JChipp6booGL8Mww@mail.gmail.com>

Hello,

I'm currently in a internship and I'm working on a Debian server.
However, I have some errors in my scripts and I think it's because of the
version of R installed on the server (works well on my PC with the latest
version of R on Windows aka 3.3.1).

Here are the details from sessionInfo() of the server :

   - R version 2.15.1 (2012-06-22)
   - Platform : i486-pc-linux-gnu (32 bit)

Since I'm totally a beginner about Linux server and my internship's tutor
doesn't want to read the tutorial to install R for Debian (even if the
server is his) or help for this tank for some unknown reason, I would like
to know how to upgrade the current R version on the server (from 2.15.1 to
3.3).

Thanks in advance

Sincelerly,

Mbr

	[[alternative HTML version deleted]]


From damjanfaks2015 at gmail.com  Mon Jul 18 12:16:51 2016
From: damjanfaks2015 at gmail.com (Damjan /)
Date: Mon, 18 Jul 2016 12:16:51 +0200
Subject: [R] R code, heat wave statistics
Message-ID: <CADsz97Ox6jF3DsmJ=kqkYWv6K4t2HUhWpwhsXEfi7FAwzuY4hQ@mail.gmail.com>

Dar all,

I have a txt file with 4 column data about daily maximum temperature for
some years.

I need to find the number of heat waves and :

Average (+ standard deviation) and maximum duration of the heatwave, ? The
number of heat waves, ? The number of days in the summer heat wave .

txt file is like this:

  year  month   day   temmax 1879 1 1 7,8 1879 1 2 7,5 1879 1 3 5,6 1879 1
4 6,2 1879 1 5 4 1879 1 6 -0,9 1879 1 7 -3 1879 1 8 -2,6 1879 1 9 -4,6 1879
1 10 -3,2 1879 1 11 -0,4 1879 1 12 3 1879 1 13 1,2 1879 1 14 -1,4 1879 1 15
0 1879 1 16 -2,4 1879 1 17 -3 1879 1 18 0,5 1879 1 19 0,6 1879 1 20 -2,6

	[[alternative HTML version deleted]]


From abdoulayesar at gmail.com  Mon Jul 18 13:31:17 2016
From: abdoulayesar at gmail.com (Abdoulaye SARR)
Date: Mon, 18 Jul 2016 11:31:17 +0000
Subject: [R] color problem barplot
Message-ID: <4B7CA1EE-117C-4C6B-9B4E-C934D9D237FB@gmail.com>

I am doing a basic bar plot which works but the color of bars positive (green) and negative (brown) don?t show up from the below command:

barplot(z, ylim=c(-2,2), col=ifelse(x>0,"brown","green ?))

any help? or other methods?

asarr

From tom at vims.edu  Mon Jul 18 15:18:19 2016
From: tom at vims.edu (Tom Mosca)
Date: Mon, 18 Jul 2016 13:18:19 +0000
Subject: [R] means by year, month and day
In-Reply-To: <CAAxdm-4PiOpwoX0qoc5KcCFy_7uwCXTL074csdUT4MySO65sgw@mail.gmail.com>
References: <CB5791F5EA3D82408B277900997482D27F7DA2D5@mboxes2.campus.vims.edu>
	<CAJ7mryJGyC=KethC7xLz7eMGYhchuctAW6BCUNC4N7RX3yc-RA@mail.gmail.com>,
	<CAAxdm-4PiOpwoX0qoc5KcCFy_7uwCXTL074csdUT4MySO65sgw@mail.gmail.com>
Message-ID: <CB5791F5EA3D82408B277900997482D27F7DA365@mboxes2.campus.vims.edu>

Dear Jim,



I'm very inexperienced with R.



I'm sorry I failed to recognize the flaw in my example.  I just clipped the first few lines of data, and should have realized that they were for the first few hours of a single day.



My first problem was to take the means by day.  As all the data come from a single year, I accomplished the goal by using:

> datATMP<-aggregate(mydata, by=mydata[c("MM","DD")], FUN=mean)



My second problem was to sort the results by month, for which I used:

> attach(datATMP)
> datATMP1 <- datATMP[order(MM),]
> datATMP1
> detach(datATMP)



I have read that I should avoid using attach(), and should use with() instead.  However, I have not yet figured out how to do this with with().  I'm only running short segments of code, and think I'll be alright with attach() for the time being, but do want to develop better form.  So, I'll keep working on it.



Thank you for your kind response and examples.  I will study them.



Sincerely, Tom





________________________________
From: jim holtman [jholtman at gmail.com]
Sent: Sunday, July 17, 2016 7:14 PM
To: Jianling Fan
Cc: Tom Mosca; r-help at r-project.org
Subject: Re: [R] means by year, month and day

Here is an example of using dplyr.  Please provide a reasonable subset of data.  Your was all for the same date.  Use 'dput' to put in your email.

> x <- read.table(text = "     X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
+   2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+   2015  2  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  2  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  2  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  2  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  2  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  2  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  2  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  2  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  2  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  2  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+   2015  3  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  3  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  3  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  3  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  3  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  3  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  3  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  3  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  3  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  3  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+  ",
+  header = TRUE,
+  as.is<http://as.is> = TRUE)
>
>  library(dplyr)
>  by_year <- x %>%
+             group_by(X.YY) %>%
+             summarise_each(funs(mean))
>
>  by_ym <- x %>%
+             group_by(X.YY, MM ) %>%
+             summarise_each(funs(mean))
>
>  by_ymd <- x %>%
+             group_by(X.YY, MM, DD) %>%
+             summarise_each(funs(mean))
>
> by_year
Source: local data frame [1 x 10]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> by_ym
Source: local data frame [3 x 10]
Groups: X.YY [?]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <int> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     1     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
2  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
3  2015     3     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> by_ymd
Source: local data frame [3 x 10]
Groups: X.YY, MM [?]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <int> <int> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     1     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
2  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
3  2015     3     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
>


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Sun, Jul 17, 2016 at 5:42 PM, Jianling Fan <fanjianling at gmail.com<mailto:fanjianling at gmail.com>> wrote:
Hello Tom,

try aggregate() or cast(). Both works.I prefer the latter.


library(reshape)
desc<-melt(mydata, measure.vars=c("WDI","R.WSP", "D.GST", "PRES",
"ATMP", "DEWP"),
           id.vars=c("X.YY","MM","DD"))
summary<-cast(desc, X.YY+MM+DD~variable, mean)









On 17 July 2016 at 06:22, Tom Mosca <tom at vims.edu<mailto:tom at vims.edu>> wrote:
> Hello Good Folk,
>
> My dataframe looks like this:
>> mydata
>      X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
> 1    2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
> 2    2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
> 3    2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
> 4    2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
> 5    2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
> 6    2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
> 7    2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
> 8    2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
> 9    2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
> 10   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3 ?K
>
> The first four columns are year, month, day, hour (0 ?V 23).  I wish to take the means of the next six columns (WDIR, WSPD, GST, PRES, ATMP and DEWP) by year, month and day.  That is, I want daily averages.
>
> Please help.  Thank you.
>
> Tom
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Jianling Fan
??????

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Jul 18 18:14:38 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 18 Jul 2016 09:14:38 -0700
Subject: [R] means by year, month and day
In-Reply-To: <CB5791F5EA3D82408B277900997482D27F7DA365@mboxes2.campus.vims.edu>
References: <CB5791F5EA3D82408B277900997482D27F7DA2D5@mboxes2.campus.vims.edu>
	<CAJ7mryJGyC=KethC7xLz7eMGYhchuctAW6BCUNC4N7RX3yc-RA@mail.gmail.com>
	<CAAxdm-4PiOpwoX0qoc5KcCFy_7uwCXTL074csdUT4MySO65sgw@mail.gmail.com>
	<CB5791F5EA3D82408B277900997482D27F7DA365@mboxes2.campus.vims.edu>
Message-ID: <CAF8bMcbHhHAtT2xBPD+w49JOjqn5QPp-7k_QUFjQVe3CVrsuKg@mail.gmail.com>

If you are very inexperienced with R you still have time to forget you ever
heard
of the attach function.  Your code
    > attach(datATMP)
    > datATMP1 <- datATMP[order(MM),]
    > detach(datATMP)
can be replaced by
    > datATMP1 <- datATMP[order(datATMP[["MM"]]),]


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jul 18, 2016 at 6:18 AM, Tom Mosca <tom at vims.edu> wrote:

> Dear Jim,
>
>
>
> I'm very inexperienced with R.
>
>
>
> I'm sorry I failed to recognize the flaw in my example.  I just clipped
> the first few lines of data, and should have realized that they were for
> the first few hours of a single day.
>
>
>
> My first problem was to take the means by day.  As all the data come from
> a single year, I accomplished the goal by using:
>
> > datATMP<-aggregate(mydata, by=mydata[c("MM","DD")], FUN=mean)
>
>
>
> My second problem was to sort the results by month, for which I used:
>
> > attach(datATMP)
> > datATMP1 <- datATMP[order(MM),]
> > datATMP1
> > detach(datATMP)
>
>
>
> I have read that I should avoid using attach(), and should use with()
> instead.  However, I have not yet figured out how to do this with with().
> I'm only running short segments of code, and think I'll be alright with
> attach() for the time being, but do want to develop better form.  So, I'll
> keep working on it.
>
>
>
> Thank you for your kind response and examples.  I will study them.
>
>
>
> Sincerely, Tom
>
>
>
>
>
> ________________________________
> From: jim holtman [jholtman at gmail.com]
> Sent: Sunday, July 17, 2016 7:14 PM
> To: Jianling Fan
> Cc: Tom Mosca; r-help at r-project.org
> Subject: Re: [R] means by year, month and day
>
> Here is an example of using dplyr.  Please provide a reasonable subset of
> data.  Your was all for the same date.  Use 'dput' to put in your email.
>
> > x <- read.table(text = "     X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP
> DEWP
> +   2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
> +   2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
> +   2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
> +   2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
> +   2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
> +   2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
> +   2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
> +   2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
> +   2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
> +   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3
> +   2015  2  1  0 328   3.6   4.5 1028.0   3.8  -3.5
> +   2015  2  1  1 300   2.1   2.7 1027.9   3.7  -4.4
> +   2015  2  1  2 264   2.4   2.9 1027.7   3.6  -4.5
> +   2015  2  1  3 230   4.1   4.5 1027.4   4.2  -3.8
> +   2015  2  1  4 242   8.1   9.2 1026.6   4.4  -3.1
> +   2015  2  1  5 262   9.3  10.1 1026.6   4.1  -3.8
> +   2015  2  1  6 267   8.6   9.6 1026.3   4.2  -3.8
> +   2015  2  1  7 264   9.3   9.9 1026.1   3.9  -2.8
> +   2015  2  1  8 268   8.2   9.1 1026.1   3.5  -3.0
> +   2015  2  1  9 272   8.8   9.6 1025.4   3.2  -3.3
> +   2015  3  1  0 328   3.6   4.5 1028.0   3.8  -3.5
> +   2015  3  1  1 300   2.1   2.7 1027.9   3.7  -4.4
> +   2015  3  1  2 264   2.4   2.9 1027.7   3.6  -4.5
> +   2015  3  1  3 230   4.1   4.5 1027.4   4.2  -3.8
> +   2015  3  1  4 242   8.1   9.2 1026.6   4.4  -3.1
> +   2015  3  1  5 262   9.3  10.1 1026.6   4.1  -3.8
> +   2015  3  1  6 267   8.6   9.6 1026.3   4.2  -3.8
> +   2015  3  1  7 264   9.3   9.9 1026.1   3.9  -2.8
> +   2015  3  1  8 268   8.2   9.1 1026.1   3.5  -3.0
> +   2015  3  1  9 272   8.8   9.6 1025.4   3.2  -3.3
> +  ",
> +  header = TRUE,
> +  as.is<http://as.is> = TRUE)
> >
> >  library(dplyr)
> >  by_year <- x %>%
> +             group_by(X.YY) %>%
> +             summarise_each(funs(mean))
> >
> >  by_ym <- x %>%
> +             group_by(X.YY, MM ) %>%
> +             summarise_each(funs(mean))
> >
> >  by_ymd <- x %>%
> +             group_by(X.YY, MM, DD) %>%
> +             summarise_each(funs(mean))
> >
> > by_year
> Source: local data frame [1 x 10]
>    X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
>   <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
> 1  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> > by_ym
> Source: local data frame [3 x 10]
> Groups: X.YY [?]
>    X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
>   <int> <int> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
> 1  2015     1     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> 2  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> 3  2015     3     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> > by_ymd
> Source: local data frame [3 x 10]
> Groups: X.YY, MM [?]
>    X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
>   <int> <int> <int> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
> 1  2015     1     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> 2  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> 3  2015     3     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> >
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Sun, Jul 17, 2016 at 5:42 PM, Jianling Fan <fanjianling at gmail.com
> <mailto:fanjianling at gmail.com>> wrote:
> Hello Tom,
>
> try aggregate() or cast(). Both works.I prefer the latter.
>
>
> library(reshape)
> desc<-melt(mydata, measure.vars=c("WDI","R.WSP", "D.GST", "PRES",
> "ATMP", "DEWP"),
>            id.vars=c("X.YY","MM","DD"))
> summary<-cast(desc, X.YY+MM+DD~variable, mean)
>
>
>
>
>
>
>
>
>
> On 17 July 2016 at 06:22, Tom Mosca <tom at vims.edu<mailto:tom at vims.edu>>
> wrote:
> > Hello Good Folk,
> >
> > My dataframe looks like this:
> >> mydata
> >      X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
> > 1    2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
> > 2    2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
> > 3    2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
> > 4    2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
> > 5    2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
> > 6    2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
> > 7    2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
> > 8    2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
> > 9    2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
> > 10   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3 ?
> >
> > The first four columns are year, month, day, hour (0 ? 23).  I wish to
> take the means of the next six columns (WDIR, WSPD, GST, PRES, ATMP and
> DEWP) by year, month and day.  That is, I want daily averages.
> >
> > Please help.  Thank you.
> >
> > Tom
> >
> >         [[alternative HTML version deleted]]
> >
> >
> > ______________________________________________
> > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Jianling Fan
> ???
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From cryan at binghamton.edu  Mon Jul 18 18:58:29 2016
From: cryan at binghamton.edu (Christopher W Ryan)
Date: Mon, 18 Jul 2016 12:58:29 -0400
Subject: [R] R code, heat wave statistics
In-Reply-To: <CADsz97Ox6jF3DsmJ=kqkYWv6K4t2HUhWpwhsXEfi7FAwzuY4hQ@mail.gmail.com>
References: <CADsz97Ox6jF3DsmJ=kqkYWv6K4t2HUhWpwhsXEfi7FAwzuY4hQ@mail.gmail.com>
Message-ID: <CAM+rpY=xx-aP+OBSKhURg-CqUxMZP_zJwsfaGbw5W1W7hWPwhQ@mail.gmail.com>

I think much depends on how you define heat wave.

--Chris Ryan
Broome County Health Department
Binghamton, NY USA

On Mon, Jul 18, 2016 at 6:16 AM, Damjan / <damjanfaks2015 at gmail.com> wrote:
> Dar all,
>
> I have a txt file with 4 column data about daily maximum temperature for
> some years.
>
> I need to find the number of heat waves and :
>
> Average (+ standard deviation) and maximum duration of the heatwave, ? The
> number of heat waves, ? The number of days in the summer heat wave .
>
> txt file is like this:
>
>   year  month   day   temmax 1879 1 1 7,8 1879 1 2 7,5 1879 1 3 5,6 1879 1
> 4 6,2 1879 1 5 4 1879 1 6 -0,9 1879 1 7 -3 1879 1 8 -2,6 1879 1 9 -4,6 1879
> 1 10 -3,2 1879 1 11 -0,4 1879 1 12 3 1879 1 13 1,2 1879 1 14 -1,4 1879 1 15
> 0 1879 1 16 -2,4 1879 1 17 -3 1879 1 18 0,5 1879 1 19 0,6 1879 1 20 -2,6
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Mon Jul 18 18:59:36 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 18 Jul 2016 18:59:36 +0200
Subject: [R] Install R version 3.3 on Debian server
In-Reply-To: <CAEhiiiuUB5cL=qTR6FO5GcJLSgf6M25k_-JChipp6booGL8Mww@mail.gmail.com>
References: <CAEhiiiuUB5cL=qTR6FO5GcJLSgf6M25k_-JChipp6booGL8Mww@mail.gmail.com>
Message-ID: <22413.2808.50285.186484@stat.math.ethz.ch>

>>>>> Mbr Mbr <omlerembr at gmail.com>
>>>>>     on Mon, 18 Jul 2016 11:45:00 +0200 writes:

    > Hello,
    > I'm currently in a internship and I'm working on a Debian server.
    > However, I have some errors in my scripts and I think it's because of the
    > version of R installed on the server (works well on my PC with the latest
    > version of R on Windows aka 3.3.1).

    > Here are the details from sessionInfo() of the server :

    > - R version 2.15.1 (2012-06-22)
    > - Platform : i486-pc-linux-gnu (32 bit)

Yes,  2.15.1  is considered antique,  and nobody should be using
it unless for "Rchaelogy" (the pre-history and history of R
implementations), and so I do occasionally start such a version.

    > Since I'm totally a beginner about Linux server and my internship's tutor
    > doesn't want to read the tutorial to install R for Debian (even if the
    > server is his) or help for this tank for some unknown reason, I would like
    > to know how to upgrade the current R version on the server (from 2.15.1 to
    > 3.3).

Very good proposal.
Two things:

- We have a *dedicated* mailing list for Debian (and
  Debian-derived such as "ubuntu") Linux distributions:
  --> https://www.R-project.org/mail.html does list the "SIG"
  (Special Interest Groups), including R-SIG-Debian
  which points to http://stat.ethz.ch/mailman/listinfo/r-sig-debian

- CRAN itself has sections about this, notably
      https://cloud.r-project.org/bin/linux/debian/
  will tell you how to tell your debian system to get R from
  CRAN as opposed from the debian repositories.
  

    > Thanks in advance

You are welcome,
Martin Maechler

    > Sincelerly,
    > Mbr

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From tom at vims.edu  Mon Jul 18 18:33:48 2016
From: tom at vims.edu (Tom Mosca)
Date: Mon, 18 Jul 2016 16:33:48 +0000
Subject: [R] means by year, month and day
In-Reply-To: <CAF8bMcbHhHAtT2xBPD+w49JOjqn5QPp-7k_QUFjQVe3CVrsuKg@mail.gmail.com>
References: <CB5791F5EA3D82408B277900997482D27F7DA2D5@mboxes2.campus.vims.edu>
	<CAJ7mryJGyC=KethC7xLz7eMGYhchuctAW6BCUNC4N7RX3yc-RA@mail.gmail.com>
	<CAAxdm-4PiOpwoX0qoc5KcCFy_7uwCXTL074csdUT4MySO65sgw@mail.gmail.com>
	<CB5791F5EA3D82408B277900997482D27F7DA365@mboxes2.campus.vims.edu>,
	<CAF8bMcbHhHAtT2xBPD+w49JOjqn5QPp-7k_QUFjQVe3CVrsuKg@mail.gmail.com>
Message-ID: <CB5791F5EA3D82408B277900997482D27F7DA3CC@mboxes2.campus.vims.edu>

Dear William,



The line of code you composed works perfectly, as you knew it would.  Thank you for your kind response.  I will now endeavor to forget that other function exists.



Sincerely, Tom

________________________________
From: William Dunlap [wdunlap at tibco.com]
Sent: Monday, July 18, 2016 12:14 PM
To: Tom Mosca
Cc: jim holtman; Jianling Fan; r-help at r-project.org
Subject: Re: [R] means by year, month and day

If you are very inexperienced with R you still have time to forget you ever heard
of the attach function.  Your code
    > attach(datATMP)
    > datATMP1 <- datATMP[order(MM),]
    > detach(datATMP)
can be replaced by
    > datATMP1 <- datATMP[order(datATMP[["MM"]]),]


Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>

On Mon, Jul 18, 2016 at 6:18 AM, Tom Mosca <tom at vims.edu<mailto:tom at vims.edu>> wrote:
Dear Jim,



I'm very inexperienced with R.



I'm sorry I failed to recognize the flaw in my example.  I just clipped the first few lines of data, and should have realized that they were for the first few hours of a single day.



My first problem was to take the means by day.  As all the data come from a single year, I accomplished the goal by using:

> datATMP<-aggregate(mydata, by=mydata[c("MM","DD")], FUN=mean)



My second problem was to sort the results by month, for which I used:

> attach(datATMP)
> datATMP1 <- datATMP[order(MM),]
> datATMP1
> detach(datATMP)



I have read that I should avoid using attach(), and should use with() instead.  However, I have not yet figured out how to do this with with().  I'm only running short segments of code, and think I'll be alright with attach() for the time being, but do want to develop better form.  So, I'll keep working on it.



Thank you for your kind response and examples.  I will study them.



Sincerely, Tom





________________________________
From: jim holtman [jholtman at gmail.com<mailto:jholtman at gmail.com>]
Sent: Sunday, July 17, 2016 7:14 PM
To: Jianling Fan
Cc: Tom Mosca; r-help at r-project.org<mailto:r-help at r-project.org>
Subject: Re: [R] means by year, month and day

Here is an example of using dplyr.  Please provide a reasonable subset of data.  Your was all for the same date.  Use 'dput' to put in your email.

> x <- read.table(text = "     X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
+   2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+   2015  2  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  2  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  2  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  2  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  2  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  2  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  2  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  2  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  2  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  2  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+   2015  3  1  0 328   3.6   4.5 1028.0   3.8  -3.5
+   2015  3  1  1 300   2.1   2.7 1027.9   3.7  -4.4
+   2015  3  1  2 264   2.4   2.9 1027.7   3.6  -4.5
+   2015  3  1  3 230   4.1   4.5 1027.4   4.2  -3.8
+   2015  3  1  4 242   8.1   9.2 1026.6   4.4  -3.1
+   2015  3  1  5 262   9.3  10.1 1026.6   4.1  -3.8
+   2015  3  1  6 267   8.6   9.6 1026.3   4.2  -3.8
+   2015  3  1  7 264   9.3   9.9 1026.1   3.9  -2.8
+   2015  3  1  8 268   8.2   9.1 1026.1   3.5  -3.0
+   2015  3  1  9 272   8.8   9.6 1025.4   3.2  -3.3
+  ",
+  header = TRUE,
+  as.is<http://as.is><http://as.is> = TRUE)
>
>  library(dplyr)
>  by_year <- x %>%
+             group_by(X.YY) %>%
+             summarise_each(funs(mean))
>
>  by_ym <- x %>%
+             group_by(X.YY, MM ) %>%
+             summarise_each(funs(mean))
>
>  by_ymd <- x %>%
+             group_by(X.YY, MM, DD) %>%
+             summarise_each(funs(mean))
>
> by_year
Source: local data frame [1 x 10]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> by_ym
Source: local data frame [3 x 10]
Groups: X.YY [?]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <int> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     1     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
2  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
3  2015     3     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
> by_ymd
Source: local data frame [3 x 10]
Groups: X.YY, MM [?]
   X.YY    MM    DD    hh   WDI R.WSP D.GST    PRES  ATMP  DEWP
  <int> <int> <int> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>
1  2015     1     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
2  2015     2     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
3  2015     3     1   4.5 269.7  6.45  7.21 1026.81  3.86  -3.6
>


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Sun, Jul 17, 2016 at 5:42 PM, Jianling Fan <fanjianling at gmail.com<mailto:fanjianling at gmail.com><mailto:fanjianling at gmail.com<mailto:fanjianling at gmail.com>>> wrote:
Hello Tom,

try aggregate() or cast(). Both works.I prefer the latter.


library(reshape)
desc<-melt(mydata, measure.vars=c("WDI","R.WSP", "D.GST", "PRES",
"ATMP", "DEWP"),
           id.vars=c("X.YY","MM","DD"))
summary<-cast(desc, X.YY+MM+DD~variable, mean)









On 17 July 2016 at 06:22, Tom Mosca <tom at vims.edu<mailto:tom at vims.edu><mailto:tom at vims.edu<mailto:tom at vims.edu>>> wrote:
> Hello Good Folk,
>
> My dataframe looks like this:
>> mydata
>      X.YY MM DD hh WDI R.WSP D.GST   PRES  ATMP  DEWP
> 1    2015  1  1  0 328   3.6   4.5 1028.0   3.8  -3.5
> 2    2015  1  1  1 300   2.1   2.7 1027.9   3.7  -4.4
> 3    2015  1  1  2 264   2.4   2.9 1027.7   3.6  -4.5
> 4    2015  1  1  3 230   4.1   4.5 1027.4   4.2  -3.8
> 5    2015  1  1  4 242   8.1   9.2 1026.6   4.4  -3.1
> 6    2015  1  1  5 262   9.3  10.1 1026.6   4.1  -3.8
> 7    2015  1  1  6 267   8.6   9.6 1026.3   4.2  -3.8
> 8    2015  1  1  7 264   9.3   9.9 1026.1   3.9  -2.8
> 9    2015  1  1  8 268   8.2   9.1 1026.1   3.5  -3.0
> 10   2015  1  1  9 272   8.8   9.6 1025.4   3.2  -3.3 ?K
>
> The first four columns are year, month, day, hour (0 ?V 23).  I wish to take the means of the next six columns (WDIR, WSPD, GST, PRES, ATMP and DEWP) by year, month and day.  That is, I want daily averages.
>
> Please help.  Thank you.
>
> Tom
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org><mailto:R-help at r-project.org<mailto:R-help at r-project.org>> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Jianling Fan
??????

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org><mailto:R-help at r-project.org<mailto:R-help at r-project.org>> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


        [[alternative HTML version deleted]]


______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From abdoulayesar at gmail.com  Mon Jul 18 20:06:40 2016
From: abdoulayesar at gmail.com (Abdoulaye Sarr)
Date: Mon, 18 Jul 2016 18:06:40 +0000
Subject: [R] barplot colour problem
Message-ID: <CAN=6O0L499-xKODR0v+A0ZwzTP0NijVcDEqunFAEczEEDtrLmw@mail.gmail.com>

I am doing a basic bar plot which works but the color of bars positive
(green) and negative (brown) don?t show up from the below command:

barplot(z, ylim=c(-2,2), col=ifelse(x>0,"brown","green ?))

any help? or other methods?

fipou

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Mon Jul 18 22:38:36 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Mon, 18 Jul 2016 15:38:36 -0500
Subject: [R] barplot colour problem
In-Reply-To: <CAN=6O0L499-xKODR0v+A0ZwzTP0NijVcDEqunFAEczEEDtrLmw@mail.gmail.com>
References: <CAN=6O0L499-xKODR0v+A0ZwzTP0NijVcDEqunFAEczEEDtrLmw@mail.gmail.com>
Message-ID: <E2930655-E629-4D64-A5C0-C2AB8E772753@me.com>


> On Jul 18, 2016, at 1:06 PM, Abdoulaye Sarr <abdoulayesar at gmail.com> wrote:
> 
> I am doing a basic bar plot which works but the color of bars positive
> (green) and negative (brown) don?t show up from the below command:
> 
> barplot(z, ylim=c(-2,2), col=ifelse(x>0,"brown","green ?))
> 
> any help? or other methods?
> 
> fipou


Presuming that the above is a direct copy and paste, your ifelse() statement is using 'x' to determine the color, rather than 'z'. Presumably a typo?

This works, for example, with 'z' as a vector:

  z <- seq(from = -5, to = 5)

  barplot(z, col = ifelse(z > 0, "brown", "green"))


Regards,

Marc Schwartz


From r.turner at auckland.ac.nz  Mon Jul 18 23:56:35 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 19 Jul 2016 09:56:35 +1200
Subject: [R] [FORGED] intersection of two polygons which are not
 shapefiles
In-Reply-To: <CAOV3wDBSa3oBs1zsZzO7Vb_x1=dyGx6zXrr9=Ghsy_a1sqsBhg@mail.gmail.com>
References: <CAOV3wDBSa3oBs1zsZzO7Vb_x1=dyGx6zXrr9=Ghsy_a1sqsBhg@mail.gmail.com>
Message-ID: <1227e3d3-5056-b254-8378-4bb438fa32e8@auckland.ac.nz>

On 19/07/16 01:16, Adrienne Wootten wrote:
> All,
>
> Greetings! I hope things are going well for all!  I apologize if someone's
> already answered this and I'm just not finding it.  Here's what I'd like to
> do, but I'm hitting a brick wall with it.
>
> I have two sets of points for which I've already determined which ones
> points for the boundaries with the chull function.  What I need for what
> I'm doing is the coordinates where the two resulting polygons overlap.
> These are not raster grids, nor shapefiles.  What I have to work with are
> the two data frames with the points at the vertices of each polygon
> (included below).
>
>> chx1
>               x          y
> 1  0.5822569 -0.5555878
> 2  0.5338428 -0.5883604
> 3 -0.3442943 -0.6701115
> 4  -0.7409293  0.2286962
> 5  0.2147221  0.8061485
> 6  0.4914146  0.4941865
> 7  0.5822569 -0.5555878
>
>> chx2
>               x          y
> 1  0.7163506 -0.4357497
> 2  0.6513128 -0.5395180
> 3   0.1818315 -0.6317423
> 4  -0.6394281 -0.5765610
> 5 -0.6044681  0.1831627
> 6 -0.5799485  0.3231473
> 7  0.2248476  0.9601908
> 8  0.7163506 -0.4357497
>
>
> If anyone has any ideas on how to get what I'm after I'd appreciate it!
> I've tried a lot of things from the raster, rgeos, and more.  Knowing me
> it's something obvious I'm just not seeing right now.

You can do this very easily using the spatstat package:

library(spatstat)
X1 <- read.table(textConnection("
              x          y
1  0.5822569 -0.5555878
2  0.5338428 -0.5883604
3 -0.3442943 -0.6701115
4  -0.7409293  0.2286962
5  0.2147221  0.8061485
6  0.4914146  0.4941865
7  0.5822569 -0.5555878"))

X2 <- read.table(textConnection("
                x          y
1  0.7163506 -0.4357497
2  0.6513128 -0.5395180
3   0.1818315 -0.6317423
4  -0.6394281 -0.5765610
5 -0.6044681  0.1831627
6 -0.5799485  0.3231473
7  0.2248476  0.9601908
8  0.7163506 -0.4357497"))

X1 <- reverse.xypolygon(X1) # Vertices are in the wrong
X2 <- reverse.xypolygon(X2) # (clockwise) order.
W1 <- owin(poly=X1)
W2 <- owin(poly=X2)
WI <- intersect.owin(W1,W2)

plot(union.owin(W1,W2),col="blue",main="")
plot(WI,add=TRUE,col="red")

HTH

cheers,

Rolf Turner

P.S.  To extract the coordinates of the intersection polygon you can do:

WI$bdry[[1]]

R. T.

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Tue Jul 19 00:09:55 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 19 Jul 2016 10:09:55 +1200
Subject: [R] [FORGED] Re:  R code, heat wave statistics
In-Reply-To: <CAM+rpY=xx-aP+OBSKhURg-CqUxMZP_zJwsfaGbw5W1W7hWPwhQ@mail.gmail.com>
References: <CADsz97Ox6jF3DsmJ=kqkYWv6K4t2HUhWpwhsXEfi7FAwzuY4hQ@mail.gmail.com>
	<CAM+rpY=xx-aP+OBSKhURg-CqUxMZP_zJwsfaGbw5W1W7hWPwhQ@mail.gmail.com>
Message-ID: <ea803223-31f3-8bfa-e875-29a659fdfa67@auckland.ac.nz>


On 19/07/16 04:58, Christopher W Ryan wrote:

> I think much depends on how you define heat wave.
>
> --Chris Ryan
> Broome County Health Department
> Binghamton, NY USA

Check out:

@Article{Wong:2015:ASA,
   author =       {T. S. T. Wong},
   title =        {Statistical Analysis of Heat Waves in
                  the State of {Victoria} in {Australia}},
   journal =      {Australian \& New Zealand Journal of Statistics},
   volume =       {57},
   pages =        {463--480},
   year =         {2015}
}

for some relevant discussion.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

>
> On Mon, Jul 18, 2016 at 6:16 AM, Damjan / <damjanfaks2015 at gmail.com> wrote:
>> Dar all,
>>
>> I have a txt file with 4 column data about daily maximum temperature for
>> some years.
>>
>> I need to find the number of heat waves and :
>>
>> Average (+ standard deviation) and maximum duration of the heatwave, ? The
>> number of heat waves, ? The number of days in the summer heat wave .
>>
>> txt file is like this:
>>
>>   year  month   day   temmax 1879 1 1 7,8 1879 1 2 7,5 1879 1 3 5,6 1879 1
>> 4 6,2 1879 1 5 4 1879 1 6 -0,9 1879 1 7 -3 1879 1 8 -2,6 1879 1 9 -4,6 1879
>> 1 10 -3,2 1879 1 11 -0,4 1879 1 12 3 1879 1 13 1,2 1879 1 14 -1,4 1879 1 15
>> 0 1879 1 16 -2,4 1879 1 17 -3 1879 1 18 0,5 1879 1 19 0,6 1879 1 20 -2,6


From gcchenleidenuniv at gmail.com  Mon Jul 18 23:11:02 2016
From: gcchenleidenuniv at gmail.com (gcchenleidenuniv)
Date: Mon, 18 Jul 2016 23:11:02 +0200
Subject: [R] How to plot density distribution based on mean values and
	quantiles in R?
Message-ID: <201607182311017019593@gmail.com>

Hi all,

I need to draw density curves based on some published data. But in the article only mean value (0.015 ) and quantiles (Q0.15=0.012 , Q0.85=0.057) were given. So I was thinking if it is possible to plot density curves solely based on the mean value and quantiles. The dnorm(x, mean, sd, log) function needs the standard deviation which was not mentioned, so it can not be used in this situation.

Many thanks!!
Daniel
	[[alternative HTML version deleted]]


From abdoulayesar at gmail.com  Mon Jul 18 23:12:08 2016
From: abdoulayesar at gmail.com (Abdoulaye Sarr)
Date: Mon, 18 Jul 2016 21:12:08 +0000
Subject: [R] barplot colour problem
In-Reply-To: <E2930655-E629-4D64-A5C0-C2AB8E772753@me.com>
References: <CAN=6O0L499-xKODR0v+A0ZwzTP0NijVcDEqunFAEczEEDtrLmw@mail.gmail.com>
	<E2930655-E629-4D64-A5C0-C2AB8E772753@me.com>
Message-ID: <CAN=6O0J=JWe7tuSvG4OmkTmGcoc5iRoo3=3u3jc5GoSWi+BMyg@mail.gmail.com>

Thank you Marc,

The typo was causing the problem, solved now.

Regards,
Fipou

On Mon, Jul 18, 2016 at 8:38 PM, Marc Schwartz <marc_schwartz at me.com> wrote:

>
> > On Jul 18, 2016, at 1:06 PM, Abdoulaye Sarr <abdoulayesar at gmail.com>
> wrote:
> >
> > I am doing a basic bar plot which works but the color of bars positive
> > (green) and negative (brown) don?t show up from the below command:
> >
> > barplot(z, ylim=c(-2,2), col=ifelse(x>0,"brown","green ?))
> >
> > any help? or other methods?
> >
> > fipou
>
>
> Presuming that the above is a direct copy and paste, your ifelse()
> statement is using 'x' to determine the color, rather than 'z'. Presumably
> a typo?
>
> This works, for example, with 'z' as a vector:
>
>   z <- seq(from = -5, to = 5)
>
>   barplot(z, col = ifelse(z > 0, "brown", "green"))
>
>
> Regards,
>
> Marc Schwartz
>
>
>

	[[alternative HTML version deleted]]


From pamelawong86 at icloud.com  Tue Jul 19 04:39:05 2016
From: pamelawong86 at icloud.com (Pamela Wong)
Date: Mon, 18 Jul 2016 22:39:05 -0400
Subject: [R] Anova() type iii SS plots and diagnostics
Message-ID: <84D05724-8221-4389-AE08-B9F7C0337CEE@icloud.com>

I am wondering if there is a way to plot results and model diagnostics (to check for outliers, homoscedasticity, normality, collinearity) using type III sums of squares in R

From drjimlemon at gmail.com  Tue Jul 19 08:20:49 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 19 Jul 2016 16:20:49 +1000
Subject: [R] How to plot density distribution based on mean values and
 quantiles in R?
In-Reply-To: <201607182311017019593@gmail.com>
References: <201607182311017019593@gmail.com>
Message-ID: <CA+8X3fV++eAjuMd-r4HJWGx2LN-aikJGfNQcimJPd=qPB1Nr+A@mail.gmail.com>

Hi Daniel,
Judging by the numbers you mention, the distribution is either very
skewed or not at all normal. If you look at this:

plot(c(0,0.012,0.015,0.057,0.07),c(0,0.05,0.4,0.05,0),type="b")

you will see the general shape of whatever distribution produced these
summary statistics. Did the paper give any hints as to what the model
distribution might be?

Jim


On Tue, Jul 19, 2016 at 7:11 AM, gcchenleidenuniv
<gcchenleidenuniv at gmail.com> wrote:
> Hi all,
>
> I need to draw density curves based on some published data. But in the article only mean value (0.015 ) and quantiles (Q0.15=0.012 , Q0.85=0.057) were given. So I was thinking if it is possible to plot density curves solely based on the mean value and quantiles. The dnorm(x, mean, sd, log) function needs the standard deviation which was not mentioned, so it can not be used in this situation.
>
> Many thanks!!
> Daniel
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.tu-dortmund.de  Tue Jul 19 10:33:30 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Tue, 19 Jul 2016 10:33:30 +0200
Subject: [R] R2WinBUGS with Multivariate Logistic Regression
In-Reply-To: <722063481.363251.1468726428102.JavaMail.yahoo@mail.yahoo.com>
References: <722063481.363251.1468726428102.JavaMail.yahoo.ref@mail.yahoo.com>
	<722063481.363251.1468726428102.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <dd9c2b1a-7bc7-6b19-d8ee-63fd5977ec21@statistik.tu-dortmund.de>



On 17.07.2016 05:33, Christopher Kelvin via R-help wrote:
> Dear R-User,
> I have written a simple code to analyze some data using Bayesian logistic regression via the R2WinBUGS package. The code when run in WinBUGS stops WinBUGS from running it and using the package returns no results also.


I'd suggest to reduce it to a WinBUGS only problem, try OpenBUGS (which 
iw more recent) and if it still fails, ask on the WinBUGS/OpenBUGS 
mailing list. A trap in WinBUGS is not an R related problem.

Best,
Uwe Ligges



>
> I attach herewith, the code and a sample of the dataset.
>
> Any suggestion will be greatly appreciated.
>
> Chris Guure
> Biostatistics Department
>
> University of Ghana
>
>
>
>
> library(R2WinBUGS)
> library(coda)model1<-function(){
> for (i in 1:N) {
> # likelihood function
> ms[i] ~ dbin( p [ i ], N )
> logit(p [ i ] ) <- alpha + bage*age[ i ] + bpam*pam[i ] + bpah*pah[i]
>
> }
> ### prior for intercept
> alpha ~ dnorm(0,0.0001)
>
> # prior for slopes
> bage ~ dnorm(0,0.0001)
> bpam ~ dnorm(0,0.0001)
> bpah ~ dnorm(0,0.0001)
>
>
> # OR for alpha
> or.age<-exp(bage)
> # OR for hbp
> or.pam <- exp(bpam)
> # OR for fdm
> or.pah <- exp(bpah)
>
> }
>
> data=cbind(ms=c(0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0),
> age=c(77, 83, 75, 78, 75, 83, 85, 80, 80, 85, 76, 77, 80, 76, 88, 77, 81, 78, 85, 81),
> pam=c(0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1),
> pah=c(1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0))
> N=20
>
> #### initial values                                                                                             ###
> lineinits <- function(){
> list(alpha=1,bage= 0.05, bpam=0.05, bpah=0.05)
> }
>
> ## CODA output
> lineout1 <- bugs(data, lineinits, c("bage", "alpha","bpam", "bpah", "or.age", "or.pam", "or.pah"), model1,n.iter = 11000, n.burnin = 1000, n.chains = 2, codaPkg = T, DIC = TRUE)
>
>
> ### Posterior summaries
> line.coda <- read.bugs(lineout1)
> summary(line.coda)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Gerrit.Eichner at math.uni-giessen.de  Tue Jul 19 11:53:06 2016
From: Gerrit.Eichner at math.uni-giessen.de (Gerrit Eichner)
Date: Tue, 19 Jul 2016 11:53:06 +0200
Subject: [R] update of formula with conditional rhs introduces
	unwantedparentheses
Message-ID: <e4dcc84f-4cf1-7232-1e0d-3d8b486badc4@math.uni-giessen.de>

Dear list members,

is anyone aware of a (simple) strategy to prevent update() from
introducing parentheses around a conditional rhs of a formula
when updating, e.g., only its lhs? Simple example:

update( y ~ a|b, log(.) ~ .)

gives log(y) ~ (a | b), but I want/need log(y) ~ a | b


I do know how to extract various components of a formula and how to
"re-build" formulae from such single components, but I would like to
avoid that.

  Thx for any hint  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 212
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
Fax: +49-(0)641-99-32109            http://www.uni-giessen.de/eichner


From maechler at stat.math.ethz.ch  Tue Jul 19 12:15:38 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Jul 2016 12:15:38 +0200
Subject: [R] ... distribution based on mean values and quantiles in R?
In-Reply-To: <CA+8X3fV++eAjuMd-r4HJWGx2LN-aikJGfNQcimJPd=qPB1Nr+A@mail.gmail.com>
References: <201607182311017019593@gmail.com>
	<CA+8X3fV++eAjuMd-r4HJWGx2LN-aikJGfNQcimJPd=qPB1Nr+A@mail.gmail.com>
Message-ID: <22413.64970.484218.516189@stat.math.ethz.ch>

>>>>> Jim Lemon <drjimlemon at gmail.com>
>>>>>     on Tue, 19 Jul 2016 16:20:49 +1000 writes:

    > Hi Daniel,
    > Judging by the numbers you mention, the distribution is either very
    > skewed or not at all normal. If you look at this:

    > plot(c(0,0.012,0.015,0.057,0.07),c(0,0.05,0.4,0.05,0),type="b")

    > you will see the general shape of whatever distribution produced these
    > summary statistics. Did the paper give any hints as to what the model
    > distribution might be?

Yes, that's the correct question:  At first, it's not about
plotting, about *fitting* a distribution with the desired
properties, and then you can easily visualize it.

So, what were the data?  If they are 'amounts' of any kind, they
are necessarily non-negative often always positive, and hence
--- according to John W Tukey --- should be analyzed after
taking logs (Tukey's "first aid transformation" for *any*
amounts data).

Taking logs, and analyzing means to consider a normal
("Gaussian") distribution for the log(<data>)  which is
equivalent to fitting a  lognormal distribution -- R functions [dpqrr]lnorm() 
to the original data.  I'd strongly recommend doing that.

And I did so, finding out, however that if indeed it is the
*mean* and the 15% and 95% quantiles,  a log normal is not
fitting.  Here is the reproducible R code .. with lots of
comments :


##
## MM  strongly recommends to fit a  log-normal distribution .. however it does *not* fit

## The "data statistics"
qlN <- c(q15 = 0.012, q85 = 0.057) # Quantiles original scale
mlN <- 0.015

(qn <- log(qlN))                  # Quantiles log scale [assumed normal (Gaussian) here]
## as the Gaussian is symmetri, the mid value of the two quantiles is the mean and median :
(mu <- mean(qn))   # -3.644
(medlN <- exp(mu)) # 0.02615
## an estimate for the median(.)  -- but  it is *larger* than the mean = 0.015 above !
## ===> Log-normal does *NOT* fit well :

## From the help page, we learn that
##        E[lN] = exp(mu + 1/2 sigma^2)    {and it is trivial that}
##   median[lN] = exp(mu)
## where here, our medLn is a (moment) estimate of median[lN]

## If the number were different, this would solve the problem :
## Consequently, a (moment / plugin) estimate for sigma is
(e12sig <- mlN / medlN) ## ~= exp( 1/2 sigma^2)
(sig2 <- 2*log(e12sig)) ## ~=  sigma^2  [--- is *NEGATIVE* (<==> 'est.median' > mean !)]
(sig <- sqrt(sig2))     ## ~=  sigma    [here of course 'NaN' with a warning !]


My conclusion would be that other distributions (than the
log-normal; the normal  is out of question !!) have to be
considered, if you want to be serious about the topic.

Maybe the poweRlaw package (https://cloud.r-project.org/package=poweRlaw)
may help you (it has 4 vignettes, the last being a nice JSS publication).

The above is a "cute" non-standard problem in any case: to fit very skewed
distributions, given two quantiles and the mean only, and the
approach taken by the "poweRlawyers", namely to minimize the KS
(Kolmogorov-Smirnoff) decrepancy seems a good start to me.

Martin Maechler,
ETH Zurich



    > Jim


    > On Tue, Jul 19, 2016 at 7:11 AM, gcchenleidenuniv
    > <gcchenleidenuniv at gmail.com> wrote:
    >> Hi all,
    >> 
    >> I need to draw density curves based on some published data. But in the article only mean value (0.015 ) and quantiles (Q0.15=0.012 , Q0.85=0.057) were given. So I was thinking if it is possible to plot density curves solely based on the mean value and quantiles. The dnorm(x, mean, sd, log) function needs the standard deviation which was not mentioned, so it can not be used in this situation.
    >> 
    >> Many thanks!!
    >> Daniel
    >> [[alternative HTML version deleted]]
    >> 
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From ashenkin at ufl.edu  Tue Jul 19 12:30:45 2016
From: ashenkin at ufl.edu (Alexander Shenkin)
Date: Tue, 19 Jul 2016 11:30:45 +0100
Subject: [R] Factors in nlme: singularity in backsolve error
Message-ID: <9fccb34e-52de-6702-02f3-3fa3e74e69f5@ufl.edu>

Hello all,

I tried posting on r-sig-ME, but it's not going through.  So I thought I 
would try here.  Apologies if this ends up being a cross-post if it 
eventually goes through there....

I am parameterizing exponential fits for some metabolic scaling models. 
I have done this in lmer already, without problem, using logged 
dependent and independent variables.  However, I would now like to 
incorporate other parameters that aren't necessarily exponentially 
related to the dependent variable.  Hence, I've turned to nlme, but I 
don't have much experience with it.  Apologies in advance for newbie 
mistakes.

With the code below, I am getting the following error.  I'm guessing 
that it has something to do with the 'site' factor being misspecified:

Error in nlme.formula(dep ~ scaling_fun(alpha, beta, ind, site), data = 
scale_df,  :
   Singularity in backsolve at level 0, block 1

When I fit a simpler function that does not involve 'site', the model 
seems to work correctly.

Any thoughts would be greatly appreciated!  I've added the dput object 
at the bottom of the email to avoid distraction...

Thanks,
Allie


 > head(scale_df)
           dep        ind spp site
2  0.28069471 -0.0322841 157    A
3 -0.69719050 -1.2568901 183    A
4  0.29252012  0.1592420 246    A
5  0.72030740 -0.3282789 154    A
6 -0.08601891  0.3623756 110    A
7  0.30793594  0.2230840 154    A


scaling_fun <- function(alpha, beta, ind, site) {
     return(beta + ind^alpha + site*(ind^alpha))
}

# results in singularity in backsolve error

nlme(dep ~ trait_scaling_fun(alpha, beta, ind, site),
      data = scale_df,
      fixed = list(alpha + beta + site ~ 1), random = alpha ~ 1|spp,
      start = list(fixed = c(0.7, 0, 1)))


##############################
# simpler function converges #
##############################

scaling_fun <- function(alpha, beta, ind) {
     return(beta + ind^alpha)
}

nlme(dep ~ scaling_fun(alpha, beta, ind),
      data = scale_df,
      fixed = list(alpha + beta ~ 1), random = alpha ~ 1|spp,
      start = list(fixed = c(0.7, 0)))


# dput for data

scale_df = structure(list(dep = structure(c(0.280694709349609, 
-0.697190504779316,
0.29252012378561, 0.720307403285465, -0.0860189064012942, 0.307935942585907,
0.862281389721553, -0.265859454130292, -0.662753116661401, 
-0.281590648203109,
1.17791119955669, 0.631729125553481, -0.556750637034717, -0.666643297610538,
0.78495323332528, 0.355622302250686, -1.26351337875593, 0.343598303486811,
-1.17350365646135, 0.660405635460007, -0.0444581036277384, 
0.722773887452235,
-0.0522058979055465, 0.65594868736652, -0.471798559774723, 
0.580581816047275,
0.514408011465886, -0.736138172672309, 0.70774857676606, 
-0.0810467855114514,
2.87816317587089, -0.224750322639999, -0.497119013269447, 0.031618943500335,
1.93466683745019, 1.0320275291612, 0.874248439252774, -0.327339718896358,
-0.854016320390115, -1.14806351731618, -0.380316794335449, 
0.298626272822636,
-0.600084009068698, 0.836832381353764, -1.02141473458357, 
-0.0626961141428343,
-1.28013982373819, -0.126310567347687, -0.998914167194147, 
-0.966356030272785,
-0.0311625866367497, -0.456212263484603, 2.22345644904786, 1.06808914728274,
2.04676618838053, 2.57057502230498, -1.1354565673953, -0.617861267279932,
-1.29433182429494, 0.0191424880072437, -0.31539529665173, 
-0.161941249027648,
-0.582100497448459, 1.62041284412996, 4.12468548155128, 0.181382553309611,
0.395970527486673, 0.949165316445473, 0.536508784640814, 1.88628468601066,
0.460108411105217, 1.50237201805536, -0.248693954731227, 0.604338152411222,
-0.511271387322619, -0.273625967472597, 2.54906943158448, 1.25797640582933,
-0.615499666866161, -0.83022966646613, -0.569798846726899, 1.41607035018281,
0.806876783115155, 3.03500743372039, 0.371311069097266, 0.605523223251687,
0.960528362908707, 0.536545286280021, -1.07783545300095, 0.145069857506269,
-0.390394315762374, -0.524314770428875, -0.205267560118326, 2.8197556390129,
0.548519073742583, 2.25991992849428, -0.274997856331803, -0.748248845217783,
1.27303941400619, 0.23884797601163, -0.582228991340883, -0.380185805876727,
-0.0858712197617979, -0.181441660791883, -0.122600427784157,
1.44111920784939, 0.163470634515893, -0.322754741867578, -1.08355296391174,
-0.27943230825155, -0.367498411981987, -1.25029544932326, 1.96800896885018,
0.741382953698137, 0.525448033325593, 0.58486584900894, -1.39293857038504,
0.0401527810688047, -0.428292900882259, -0.384773565802791, 
-0.347537789997483,
0.259461537066455, 1.04953062690743, 0.905652281045827, 0.921052829745329,
0.52450347921673, 1.71595979136317, -0.474652211889487, 0.986966207710898,
-0.256654313404628, 2.99291703386829, -0.874752003654369, 1.01178420482112,
5.77381325416311, -0.167933435302975, -0.746365585369451, 2.80467857407177,
1.95866740218257, -0.503053147492697, -0.0160574809046825, 
0.0121332489691898,
0.155574669334708, 0.916506565829058, -0.448074058381429, 0.316834873904141,
1.04981848557227, -0.824609748750358, 1.06744876285501, 0.063863664699137,
-0.765846025184139, -0.557645050339089, 0.647420847566618, 
-0.481436855350583,
0.145702813289175, 0.0908652712388498, -1.18095990275663, -1.06285986971149,
0.152516462446235, -0.301967070920705, -0.517918792168663, 
0.184004599405834,
-0.566811364523435, 1.6331783242747, 0.690245821132717, -1.42142389781947,
0.491457304176974, 0.0627154091195769, -1.18747542803504, -1.82414795780551,
-0.54473248220664, -0.38718643742941, 0.202890301793843, -0.937796467882627,
-0.977104998380747, -0.226054462319793, -0.2570514108737, 
-0.605519390336332,
0.893219084521843, 1.2406495934532, -0.486391839711066, 0.0164259858282593,
-0.412489997094812, 0.956475434372077, 0.170823095308925, 1.12234696851596,
0.864881638376067, 2.94395902271581, -0.653625296526159, 0.0344781331483653,
-1.11047554289386, 0.0240754964789988, 1.76294743083037, -0.628386617245746,
0.192639895843502, -0.0927451932802299, -0.540571719710502, 
-1.15181016752959,
0.700555244490985, -0.755243667920522, 0.00602960712621028, 
-0.586868245391815,
-1.17155166667398, 1.56392615917735, 1.97004404631982, -0.280915309650685,
-1.09667960854681, -1.26761042957248, 0.390167171160699, -0.868437544908412,
0.197261528411407, -1.05462502319379, 0.458230531075614, 0.670664261262915,
-1.26355771555643, 0.0544626276730709, -0.49989719331403, -0.73427888393413,
-1.18799702275245, -1.3936130622378, -0.934938915880832, -0.531815063104187,
-1.33502744780389, -1.085565964999, -0.449461886445409, -1.43582054600884,
-0.794423040257847, -1.56211060016727, -1.43010121209506, -1.10083462220113,
-1.01314643463123, -1.19811510591181, -1.37565527646516, -1.02664156130493,
-1.62967267575906, -1.50150574382491, -1.62882785195591, -1.12359789668463,
-1.02398490326148, -0.683499336412734, -1.1155545604915, -1.53360659069892,
-1.52610133563353, -1.81097302479998, -0.948446274692255, 
-0.419546048614108,
-1.16932468032093, -0.595422041225307, -1.22737900938278, -1.28673992555364,
-1.05512677626171, -1.57557323497678, -1.4766946827855, -1.57668824537509,
-1.67142161688021, -1.72152348613828, -0.711645696230758, 
-0.766535980524732,
-1.08905113531769, -1.35316559445918, -1.30885914425461, -1.61047595500881,
-1.59760049766984, -1.6142112883589, -1.39999819034864, -0.440030335981056,
-0.19564891994752, -1.23448123395016, -0.692666934199905, -0.96869628836432,
-1.42647633086653, -0.819947428887185, -0.870491162400872, 
-1.13186316445368,
-1.31536639789785, -0.106780345783472, -1.34173728791556, 1.22625909771354,
1.37982852819105, 0.273104569490682, 1.90808233104915, 0.638422159751792,
0.660885352597854, -0.424886845771443, 0.00042860845924302, 
0.545140506066279,
0.136397731561079, 1.00695758951666, -1.24917788259541, -0.0605209308736711,
-1.05532600435696, -0.602964427120747, 1.04776665949433, -0.852428413815971,
0.83212090285218, 0.195305492380899, 1.02775755445631, 0.140274631965342,
0.176679362074364, 0.0219452557235226, -0.493395579450221, 
0.00664958388467811,
-0.406358793092381, 1.61195643785565, -0.746014740839289, 0.55469286672927,
0.311803194810194, 0.267929653276485, -0.108631151585054, -0.27545070897134,
1.66808467737734, 0.413698916068735, 0.476741830667972, 0.449627013023383,
-1.06761893237941, -0.654520314456918, 0.244263943352045, 
-0.355457763818747,
1.65477719162908, -0.682875622056975, -0.592661905670813, 1.26758657033146,
1.5767017934966, 0.529018946178275, 0.845073921558493, -0.348403367842978,
0.868892260109614, 0.119903351368414, 1.1244419562376, 1.15855607750289,
1.85097015161797, 1.16296132699817, 1.04113374483116, -0.831136183842197,
1.16570524825761, 0.214085647908677, -0.193445673416235, 0.205325290939172,
0.807867170100101, 0.211944635191694, 1.01903833452256, 0.581515608742661,
-0.974294411550401, -0.0653578433223884, -0.407272960790088,
1.64357613943039, -0.411002962687008, 1.00606886817971, 0.31992987519873,
0.574580679321335, -0.620942679148953, 0.409192244773901, 
0.00488543951394021,
-0.186426274273604, 1.17953640790635, -0.385619455756059, 
-0.0895609553579481,
0.777527991507575, 1.89696362786571, 2.62789700445807, 1.48857113802541,
-0.191049329070274, 0.472885020910262, 0.953200081084007, 
-0.573874399465063,
0.39582312427189, 2.73340372487198, 0.451643036437629, 0.322328118551198,
2.41612502144468, 2.44313584825065, 0.791039923156798, 1.77391333560054,
0.678584228379532, 1.34117549430457, 1.02560460915508, 0.642953566577357,
0.6327160646872, 0.50920127124515, 1.26542872863897, 0.0513291757520665,
1.55913957405783, -0.346674887091317, 0.977805034363923, 0.18481458582038,
0.89380652637702, -1.35984199639655, -0.597199729998613, 0.476124154483983,
0.20382213848194, 0.86625758376961, 1.81576370038761, -1.14986784324603,
-0.890658192055045, -0.690103903496631, -1.34792259851183, 
-1.17277627037773,
-1.0232707668005, -1.07601506735639, -1.03419875253434, -1.25465531843128,
-0.908238115191498, -1.14888293768223, -1.30516701279404, 
-0.959154271504791,
-0.3606188318332, -1.04295641838087, -1.12344997609438, -0.657745265539639,
-1.41046953794939, -0.497392498376635, -0.696910242295569, 
-0.79994809305917,
-0.804048698121188, -0.97867617698862, -0.605950164307889, -1.2529239165814,
-1.05824118171373, -0.739898241678675, -1.3700045895137, -0.907918586540374,
-0.0398672197549609, -0.848902846699456, -1.19548457643008, 
-0.771068776601233,
-1.23790437612789, -0.843259415629804, -1.34109470352155, 
-0.585935748553824,
-1.18194756416151, -0.615789961562636, -1.05572577647201, -1.20921702465334,
-0.858056601314504, -1.00905885844125, -1.4075681417088, -1.51741859735151,
-0.614902309207153, -0.584063095404852, -1.07771848178745, 
-0.487689111687606,
-0.898600085149827, -0.417964362181435, -0.417964362181435, 
-0.863814796557971,
-0.613668504598238, -0.927173429103902, -0.669736198478399, 
-1.04537229065607,
-0.0171362011083743, -0.902905529489174, -0.751999926891823,
-0.661935376847641, -0.292306155060246, -0.720123706185327, 
-1.44984333068773,
-1.20609086253812, -0.38786397451965, -0.606124145901586, 
-0.570194590926828,
-0.555598194042265, -1.29979056344798, -1.05384597250481, 
-0.919468542032538,
-0.805603652782657, -1.41844766043539, -1.07175900822383, 
-0.946797457139922,
-0.828724319228962, -1.35293541590241, -1.13689539904128, 
-0.0798785843494353,
-0.618331634124303, -1.4559931897786, -1.28355330735043, -1.18554433269035,
0.0275586777219822, -1.20481537984552, -1.19024513862434, 
-0.395235767461283,
0.173381346382336, -0.572762255164991, 0.295292095379503, -1.31089510954771,
-0.344143720486926, -0.831061609120973, -0.977390370945667, 
-1.10149327988756,
-1.22275975457248, -1.33834521638168, -1.19157963477974, -0.600156277847245,
-0.614748187765859, -0.334699889183784, -1.15753800754865, 
0.517156155164548,
-0.851381825394844, -1.34285436618629, -0.465564394208394, 
-0.857524846029498,
-1.23380675705904, -1.12582955730158, -0.456185133090216, 0.23190642474228,
-0.984542065676207, -0.77473508935259, -1.01321825674572, -1.2622364069534,
-1.23803897205162, -0.559758221202299, -0.617857643563247, 
-0.729003550279869,
1.67033204887905, 1.12343207191695, -1.28987472337684, 2.0565586571076,
1.41831394144223, -0.79327474118706, -0.844458165979936, 1.36162646313719,
-0.735512619154859, 0.457415170349701, 1.10256412021404, -0.448778199917757,
0.635659979331729, 0.109707699307732, 0.571517444621334, 0.130565845156641,
1.14015671592146, 0.438918139189911, 0.48245879856675, -0.36344252273592,
0.359469359816509, 0.961985442469504, -0.414918795726815, 0.50782962629854,
0.140676974228199, -0.165812716346512, -0.424918313974613, 0.42504278510862,
1.05196638494678, -0.393350736608016, 1.11771810459841, 1.43391168791847,
0.39340397596376, -0.709672569088468, -0.144338248059084, 
0.0775307115107999,
1.44356408445035, -0.552115474298283, 1.53938158728436, -0.701883587307108,
-0.511433954141368, -0.321205045336655, 1.3077446149256, -0.500856144844018,
-1.35028194740113, 0.0689021854703949, -1.05663439286345, 
-0.479361494389175,
-1.14712646963448, -0.33976856614697, -0.507349510671996, 
-0.0259999489797301,
-0.686791114098016, -0.0660192693974952, 0.958536171714724, 
-1.19425028261728,
-1.30201800698104, 0.0181862986910217, -0.323169846214161, 
-0.202267550155569,
-0.270093592196032, -0.477254499566796, 0.343629416747284, 
0.258301427656493,
-0.108507399582493, -0.518547267380918, 0.784352752216299, 1.28156350529702,
-0.0256570974228843, -0.991083038992896, -0.487725059199562,
-0.422621679257172, -0.684418686964736, -1.19892127178684, 
0.240114999970401,
2.01218082075944, -0.236079750988335, 1.37283957051306, -0.75599578784428,
0.623580279836249, 0.708113106342748, -0.688153841083081, 3.60722906673547,
0.759049637554329, 1.4411579949362, 1.08909900092446, 1.6246620814854,
-0.647246307456457, 0.370499849449406, 0.652972877878095, -1.04587057320382,
0.0240362239588807, -0.564302132865917, -0.00719208739026172,
-0.802264523635872, -0.502056532221687, -0.531100031965904, 
0.134731185000183,
0.464741362416112, 0.0858545743494743, -0.203380824028566, 
-0.139617643642184,
0.0290979826069877, 0.307708207664093, 2.22672128459229, -1.22464809677757,
-1.10218061681262, 0.904737955147255, 0.972129130855392, -0.226595107037789,
0.410779833414035, -0.0831945498868851, 4.80336110525691, -1.20383802572621,
1.11015423685492, 0.272998753752381, 1.05834570185903, -1.04379953686758,
1.14094437312265, -0.166836699017191, -0.829479403350024, 
-0.544895495862237,
-0.256760348818661, 1.13183594065731, -0.306740700327563, 
-0.965005061283116,
-1.22682429772968, -0.90066572612443, 0.914781029105528, 1.50164156261844,
1.43392799973895, 0.76675185329739, -0.614541255047314, 1.59124682310782,
-0.292903109178336, 0.0437947277942228, 0.909638870833166, 
0.157439402274253,
-0.649023057842128, -0.89904283658918, -0.0494456615615109, 
-0.862661876601541,
1.79534325310167, 1.48776342093183, -0.170268098255415, 0.244234371640275,
0.0656365425837528, 3.04859243927187, -0.377879879379748, -0.57984475996934,
0.434312783276935, -0.741370663180875, 1.49604414859944, -0.343376197892771,
0.50226906215812, -0.902293716502219, 0.00523088901025858, 
0.654483332297671,
-0.170339481430111, 0.569385500260194, 1.29708132747283, 0.949891709243599,
1.20192717443625, -0.766434795856235, 3.54615434428768, -0.627194078598971,
-0.801089674979064, -0.0156250162179901, -0.0913233365645065,
-1.576270790215, 1.06019312010792, -0.241573857379441, -1.04878400540424,
1.78135316587079, 1.75136314621452, 0.121059484084754, -0.729585197945405,
-0.253763588678462, -0.765335513857749, -0.0682249843684433,
-0.757514694726899, 0.525240178263924, -0.728620523204325, 2.33556017567608,
0.831482474057568, 0.267723083429966, 3.06414117686316, 0.516536741692109,
0.397979235532163, 1.62540741520308, -0.824322291436025, 0.87883692183814,
-0.595905611378605, 0.819628688272022, 0.0130998672563871, 0.27866740023993,
0.327826922666158, -0.91217335427202, -0.0128997998821542, 1.15361199011977,
-0.652123923400212, 0.994679625975757, 0.425444003449748, 1.0209709004156,
0.748675438295394, -0.418172442758958, -0.247421605889191, 
0.115407071389731,
-0.939588835555946, 0.415957718219395, -0.210853857347349, 
-1.01689729422553,
-0.118672444729454, -0.670536021957206, 0.512127117097271, 
-0.933737443761046,
-0.325407928293206, 1.99066909971259, 2.11822588799375, 1.89578238367564,
-0.170422993077358, -0.0937119632635209, 0.963642858067328, 
-1.18957320152876,
0.575037959739622, -0.598232718509322, 0.0865829904633331, 
0.140316763193935,
0.246977670608818, 0.320664180323731, -0.626286121471306, 
-0.997899468341695,
1.95830331119489, -0.131272700828542, 0.232379419567843, -0.985173539576702,
-0.421302206906856, 0.719866793214859, -0.808528893389443, 1.49610940062615,
-0.322184668631678, -0.397844427788061, 2.25066389072725, 
-0.718298922628435,
1.7336715373154, -0.0490072846030716, -1.38509092720132, -1.2673299761835,
-0.13298402797288, 0.716003617037563, 0.552069356380146, 1.00257592453502,
-0.295043364377787, 0.950213996305332, 0.495790542757127, 0.57661021943655,
-0.420378246127038, -0.717375856671705, 2.86757862340638, 0.539020158310173,
-1.07377878731358, 1.1768421077863, 0.249601351958952, -1.02511717045039,
1.45126647442164, -1.29170021969669, 0.421802881497714, -0.294788388358285,
3.13607505464184, 0.531654963997734, 0.541089679177317, -0.154443762799115,
-0.678742469446553, -1.2968841742999, 0.603209991887586, -0.889599767420185,
0.574983037297298, 0.888294795357737, -0.981200601088735, 
-0.248800774377499,
2.23872202487327, 1.45807901070214, 1.70947150738919, 0.842698757529027,
0.629457655482024, 1.48346552344249, -0.910220708656389, -0.91104462395209,
-0.967812554610126, 0.333303577393829, -1.23289939141487, 
-0.623948441249796,
-1.04557765707376, -0.511149584993588, 0.657917102535818, 1.72349601168195,
-0.505551130665463, -0.722207697897722, -0.826154616230647, 
0.438821379956384,
0.692250428907892, 0.369702384394565, 0.526982261432735, -0.199848590151177,
-0.500972808833397, 0.690034724978254, -0.300285672765374, 2.18931317864781,
0.359962106949574, -0.274359391989523, -0.578477526168633, 
-0.106353704075415,
0.126543379888488, -0.530977640306322, 1.8021888590031, 0.91371639163851,
-0.419473507839852, -0.636642704272862, 0.070574820953649, 
-0.363102753245237,
0.0784513887767794, -0.606742912164582, -1.38485656500315, 
-0.358942491040509,
0.14414325698563, -0.0586691624262829, 0.208305530911087, 
-0.00640100211613056,
-0.162313210537066, 0.722114360564238, 0.841186462360913, 
-0.175214926423166,
-0.733000440953664, 0.756173176638179, 0.759264225082295, 
-0.326797072459469,
-0.832117865342888, -0.806384912082174, 1.07151282993077, 0.236865489028706,
-0.357787477083334, 0.180039069927537, -1.01862572197358, 
-0.943922931154998,
0.571036310590255, 0.0758243131539529, -0.0155964444689164, 
0.508435963327637,
-0.900451810217602, -0.0894419454719398, -0.354777105248062,
0.754081289897291, 0.624467605018215, -0.534161069048875, -1.14831307445649,
-1.28543905099042, 0.435061812416575, -0.766819447435734, 0.500219186135188,
-1.03642620903013, 2.12313575629967, 0.876201936584842, -0.037367788496134,
-0.511406317864558, -1.29263144300578, -1.281373053822, -0.231606941618388,
1.0270793305418, 1.10904020302581, -0.6242548359575, 0.163451962008105,
-0.20586520466426, 0.131665031446973, -0.263156156848662, 0.86818662592479,
-0.759536348783918, -1.03546204964, -0.702032586059559, -0.769461675438033,
1.02762245412912, 0.89948746366051, 0.783190529671046, -1.34065131726446,
-1.43247863929325, 0.589340070423619, 0.766923092825367, 1.19227523001165,
0.491068864745838, -0.92639769882172, 0.535940059205437, -1.30884332264566,
0.116859814976594, 0.325910252332917, 1.07023758286568, 0.55536987714887,
-0.0312433573485808, -1.07925151983716, -0.602062409484926, 
0.362069664768365,
0.611270190860704, -0.487133706728766, -1.04016426308067, 0.549828966583864,
0.464684478827522, 0.0233754636460338, -0.423388563082668, 1.05691867046465,
-0.144737921460911, 1.07731088886499, 0.8010888195835, -0.628247003372029,
-1.06003677995511, 0.938048793594014, 1.3772110568739, 1.37165471665825,
0.67047120889471, -0.722632622717748, 2.03747110312245, -0.540915113375918,
2.1839349928197, -1.03583488295698, 0.694983867158175, -0.845376245686622,
0.0812659357920631, -0.290238014918826, -0.788703711337302, 
1.16284681818975,
-0.391078306224855, -1.01536636228426, 0.756046471133255, 
-0.497306106220214,
-0.389680294002286, -0.948221054776585, -0.574223937984445, 
-0.740551178777692,
-0.086257682593323, -1.02077727468241, 2.10825047525659, 1.48053924017256,
-1.06064183851885, 0.264599318172967, -0.274780900769782, 
-0.562783015381503,
0.0747242652473198, -1.06692305214069, 1.34114707801284, -0.214786021744423,
-1.00660578744318, -0.619734561974843, 0.13883383360632, -1.04446106115123,
-1.16646901520906, 0.053439688163081, -0.33244489064943, -0.146391879488098,
-0.417834371990972, -0.410436675023389, -0.230692724853316, 
-0.303296189568602,
0.154776202108604, -0.759784832019004, 1.76953915496648, -0.581705843221114,
-0.290550611444112, 1.22453794657615, -0.313242436099215, 0.280361303638651,
1.16941715267016, 0.671368282201419, 1.17660382154998, 1.32063041970405,
1.83708244744656, 0.767212806762993, 0.319221830621221, 0.711990041471,
0.54261827797175, 0.320845426207599, 0.0755812981633723, 0.216445367962462,
-0.248821258024012, -0.0896546285357983, -0.0546349754076262,
-0.258921320080528, 0.587985680867172, -0.753321654054947, 1.06836890962014,
0.129668082323122, 0.437038272562228, 0.512841479900359, 
-0.0339534723408688,
-0.224575904615199, 0.0166529230882547, 0.545015632823714, 
0.0875276207265216,
0.370872999818545, -0.621989052426517, -0.144215006896801, 
-0.260695447170179,
-0.597150007022914, -0.196861390450694, -0.179084733400039, 
0.0474693158209794,
-0.445157234240807, 0.228044591488753, 0.550149913314531, 
-0.0298135611035657,
-0.112556863534103, 1.37623259452526, -0.441313218213333, 0.699681221358181,
2.81486831067775, 1.28723894766397, 0.332527373874222, 0.0405597227285201,
1.00386457223451, -0.805639258354661, -0.232028562703358, 
-0.108806568471449,
0.460506445228382, 1.45536791171866), .Dim = c(1031L, 1L), 
"`scaled:center`" = 4.44880503362187, "`scaled:scale`" = 2.29257418665355),
     ind = structure(c(-0.0322840994850495, -1.25689007787688,
     0.159241954007512, -0.328278909428099, 0.362375647105683,
     0.223083971838366, 0.815073591724465, 0.51327496197861, 
-0.989914366947856,
     0.269514530260805, 0.652566637245927, 1.12267604127312, 
-0.287652170808465,
     0.385590926316902, 1.44188613042739, -0.165771954949562,
     -1.23367479866566, 1.55215870668069, -0.926072349117003,
     0.408806205528122, 0.0315579183458044, 0.983384366005806,
     -0.177379594555172, 0.205672512429951, -0.316671269822489,
     0.292729809472024, 1.36063265318812, -0.363101828244928,
     -0.229613972780416, 0.194064872824341, 1.31420209476568,
     -0.206398693569196, -1.11179458280676, 1.62180454431434,
     1.65662746313117, 1.11687222147032, 1.12847986107593, 
0.333356548091658,
     -0.409532386667367, 0.890523249160928, 0.739623934288001,
     -0.00906882027383001, -0.136752855935538, 0.902130888766538,
     -1.21626333925725, 1.29679063535727, -0.194791053963586,
     -0.751957755032856, -0.949287628328222, -0.409532386667367,
     1.50572814825825, -0.0264802796822445, 0.629351358034708,
     1.05883402344227, 2.0454833899191, 2.13254068696118, 
-0.705527196610417,
     -0.177379594555172, -0.972502907539442, 0.159241954007512,
     0.884719429358123, -0.659096638187978, -0.572039341145904,
     1.19812569870959, 2.18477506518642, 0.217280152035561, 
-1.16983278083481,
     1.71466566115922, 0.815073591724465, 3.98395920405593, 
1.49412050865264,
     0.461040583753366, -0.769369214441271, 2.0396795701163, 
-0.235417792583221,
     -0.0671070183018789, 1.07624548285068, 4.20450435656252,
     -0.705527196610417, -0.154164315343952, 0.130222854993487,
     0.768643033302025, -0.136752855935538, 2.79997996428374,
     0.28692598966922, 0.0721846569654384, 1.66243128293398, 
0.77444685310483,
     -1.23367479866566, 0.333356548091658, -0.0438917390906594,
     -0.154164315343952, -0.148360495541147, 1.41286703141337,
     1.90619171465178, -0.29345599061127, -1.22787097886286, 
0.844092690738489,
     1.20392951851239, -0.0496955588934642, -0.136752855935538,
     0.25210307085239, 1.72046948096203, 0.623547538231903, 
-0.368905648047733,
     0.954365266991782, -0.879641790694564, -1.18144042044042,
     -1.14661750162359, -0.746153935230051, -0.0380879192878545,
     -0.798388313455295, 1.64501982352556, 2.96248691876227, 
0.530686421387025,
     1.15169514028715, -0.171575774752367, 0.820877411527269,
     -0.444355305484197, 2.08030630873593, 0.600332259020684,
     1.34322119377971, 1.07624548285068, 1.47090522944142, 3.06115185540996,
     1.41286703141337, 0.52488260158422, 1.40706321161056, 
-0.0961261173159034,
     0.594528439217879, 3.52545743963435, 1.97003373248264, 
0.432021484739342,
     3.01472129698752, 0.118615215387878, 0.0721846569654384,
     0.00253881933177994, -0.711331016413222, 0.217280152035561,
     0.815073591724465, -0.218006333174806, -0.212202513372001,
     2.23700944341166, 0.194064872824341, 0.0083426391345845,
     2.79417614448093, -0.432747665878587, 1.29098681555447, 
-0.107733756921513,
     0.118615215387878, -0.438551485681392, -0.194791053963586,
     -0.206398693569196, 0.217280152035561, 1.16330277989276,
     -0.54302024213188, -0.51980496292066, 0.658370457048732,
     -0.29345599061127, -0.345690368836514, 1.32580973437129,
     -0.252829251991635, 2.90444872073423, 1.43027849082178, 
-0.960895267933832,
     -0.154164315343952, 0.745427754090806, -0.711331016413222,
     -0.897053250102978, 0.0083426391345845, 2.58523863157996,
     1.81913441760971, -0.775173034244076, -0.798388313455295,
     -0.27024071140005, 0.281122169866415, 0.339160367894463,
     1.05883402344227, 0.907934708569343, -0.960895267933832,
     0.902130888766538, 0.51327496197861, 1.20392951851239, 
0.124419035190682,
     1.54055106707508, 0.0489693777542189, 1.58698162549751, 
0.25210307085239,
     -0.299259810414075, -1.11179458280676, -0.218006333174806,
     1.82493823741252, -0.339886549033709, -0.624273719371149,
     0.25210307085239, -0.27024071140005, -0.618469899568344,
     0.507471142175805, -0.386317107456148, 0.263710710458, 
-1.17563660063761,
     -0.165771954949562, -0.595254620357124, 2.167363605778, 
-0.560431701540295,
     -0.81579977286371, -0.908660889708588, 0.907934708569343,
     0.971776726400196, -0.165771954949562, -0.119341396527123,
     1.47670904924422, 0.832485051132879, -1.01312964615908, 
0.27531835006361,
     -0.897053250102978, -0.601058440159929, -1.07116784418713,
     -0.972502907539442, -1.05956020458152, -0.339886549033709,
     -0.868034151088954, -1.0421487451731, -0.392120927258953,
     -1.14661750162359, 0.176653413415927, -1.19885187984883,
     -1.25689007787688, -1.07116784418713, -1.07697166398993,
     -0.688115737202002, -0.589450800554319, -1.02473728576469,
     -1.23947861846847, -1.25689007787688, -1.08857930359554,
     -0.421140026272977, -0.368905648047733, -1.01893346596188,
     -0.339886549033709, -1.0363449253703, -1.09438312339834,
     -1.22787097886286, -0.630077539173953, -0.618469899568344,
     -0.583646980751514, -0.693919557004807, -0.316671269822489,
     -1.07697166398993, -0.125145216329928, -1.26849771748249,
     -1.06536402438432, -1.08277548379273, -0.833211232272124,
     -1.12920604221517, -0.775173034244076, -0.647488998582368,
     -0.873837970891759, -1.08277548379273, -0.943483808525417,
     -1.21045951945444, -1.26269389767969, -1.11179458280676,
     -0.966699087736637, -0.444355305484197, 0.582920799612269,
     -1.11759840260956, -0.711331016413222, -0.943483808525417,
     -0.81579977286371, -1.05956020458152, -0.641685178779563,
     -0.931876168919808, -1.26269389767969, 0.0895961163738533,
     -0.467570584695416, 0.153438134204707, 0.664174276851537,
     -0.734546295624441, 0.722212474879586, -0.902857069905783,
     -1.01893346596188, -0.978306727342247, -0.937679988722612,
     -0.780976854046881, -0.972502907539442, -0.577843160948709,
     -0.978306727342247, -0.368905648047733, -0.635881358976758,
     -0.873837970891759, 0.693193375865562, -1.11759840260956,
     -0.775173034244076, -0.82740741246932, -0.717134836216027,
     -0.81579977286371, -0.299259810414075, -0.25863307179444,
     -0.693919557004807, -0.548824061934685, -0.618469899568344,
     -0.0380879192878545, -0.676508097596392, -0.171575774752367,
     -0.310867450019684, -0.670704277793587, 0.118615215387878,
     -0.415336206470172, 0.304337449077634, -0.873837970891759,
     -0.786780673849685, 0.873111789752514, -0.572039341145904,
     -0.740350115427246, -0.769369214441271, -0.0671070183018789,
     0.698997195668367, -0.693919557004807, -0.821603592666515,
     1.04142256403386, 0.25210307085239, -0.705527196610417, 
0.54809788079544,
     -0.432747665878587, -0.537216422329075, -0.699723376807612,
     0.321748908486049, 0.466844403556171, 0.710604835273976,
     0.495863502570196, 0.675781916457147, -0.618469899568344,
     1.23875243732922, -0.235417792583221, 0.675781916457147,
     0.553901700598245, 1.49412050865264, -0.200594873766391,
     1.28518299575166, 1.41286703141337, -0.995718186750661, 
1.38965175220215,
     0.269514530260805, 1.20392951851239, -0.0845184777102936,
     1.51733578786386, -0.27024071140005, 0.188261053021536, 
-0.142556675738343,
     0.0721846569654384, 1.23875243732922, 0.739623934288001,
     2.88123344152301, 0.745427754090806, -0.55462788173749, 
0.194064872824341,
     1.53474724727227, 0.635155177837513, 3.7053758535213, 
0.965972906597391,
     1.87136879583495, 1.97583755228544, -0.351494188639318, 
0.223083971838366,
     1.76109621958166, 1.30259445516008, 0.739623934288001, 
2.07450248893313,
     3.64733765549325, -0.00906882027383001, 1.40125939180776,
     0.461040583753366, 2.81739142369215, 1.929406993863, 1.97003373248264,
     1.55796252648349, -0.299259810414075, 1.65662746313117, 
1.81333059780691,
     1.47090522944142, -0.601058440159929, 1.2097333383152, 
-0.200594873766391,
     0.954365266991782, -0.618469899568344, -0.403728566864563,
     1.06463784324507, 0.600332259020684, 1.5695701660891, 
0.919542348174953,
     -1.23367479866566, -0.943483808525417, -0.403728566864563,
     -1.14081368182078, -1.11759840260956, -1.03054110556749,
     -1.00732582635627, -1.07697166398993, -1.03054110556749,
     -0.931876168919808, -1.25689007787688, -1.17563660063761,
     -0.809995953060905, -0.409532386667367, -0.635881358976758,
     -1.22206715906005, -0.392120927258953, -1.11179458280676,
     -0.287652170808465, -1.21626333925725, -1.03054110556749,
     -1.07116784418713, -1.05375638477871, -1.0421487451731, 
-1.08277548379273,
     -1.21626333925725, -1.04795256497591, -1.31492827590493,
     -0.955091448131027, -0.467570584695416, -0.601058440159929,
     -1.10018694320115, -1.19304806004603, -0.757761574835661,
     -0.438551485681392, -1.164028961032, -1.01312964615908, 
-1.24528243827127,
     -0.490785863906636, -0.879641790694564, -1.09438312339834,
     -1.0421487451731, -0.409532386667367, -1.19885187984883,
     -1.05956020458152, -0.55462788173749, -0.763565394638466,
     -0.995718186750661, -0.763565394638466, -0.763565394638466,
     -0.374709467850538, -0.937679988722612, -1.07116784418713,
     -1.03054110556749, -0.891249430300173, -0.879641790694564,
     -0.966699087736637, 0.0779884767682434, -1.29751681649652,
     -0.937679988722612, -0.728742475821636, -0.81579977286371,
     -1.11179458280676, -1.00152200655347, -1.21626333925725,
     -0.444355305484197, -1.26269389767969, -0.79258449365249,
     -0.467570584695416, -0.960895267933832, -1.01312964615908,
     -1.04795256497591, -0.751957755032856, -1.2743015372853,
     -1.2743015372853, -1.19885187984883, -0.479178224301026,
     -1.30912445610213, -0.885445610497369, -0.212202513372001,
     -0.699723376807612, -1.19885187984883, -1.03054110556749,
     -1.00732582635627, -1.164028961032, -1.30332063629932, 
-1.00152200655347,
     -0.572039341145904, -0.659096638187978, -1.32653591551054,
     -0.357298008442123, -0.775173034244076, -0.363101828244928,
     -0.653292818385173, -1.2743015372853, -0.8041921332581, 
0.0663808371626334,
     -0.908660889708588, -0.844818871877734, -0.560431701540295,
     -0.682311917399197, -0.0787146579074886, -0.908660889708588,
     0.28692598966922, -0.931876168919808, -1.03054110556749,
     -0.159968135146757, -0.618469899568344, -0.496589683709441,
     -1.18144042044042, -0.351494188639318, -0.00326500047102503,
     -0.548824061934685, -0.647488998582368, -0.589450800554319,
     -1.01893346596188, -0.873837970891759, -0.455962945089807,
     -0.455962945089807, -0.484982044103831, 2.97989837817069,
     0.437825304542147, -1.28590917689091, 1.22714479772361, 
-0.345690368836514,
     2.08030630873593, 1.05303020363946, 1.30259445516008, 
-0.0787146579074886,
     0.606136078823489, 0.925346167977757, -0.508197323315051,
     1.15169514028715, -0.525608782723465, -0.194791053963586,
     -0.252829251991635, -0.0554993786962692, 0.0141464589373895,
     0.565509340203854, -0.426943846075782, -0.374709467850538,
     1.26196771654044, 0.194064872824341, 0.571313160006659, 
-0.223810152977611,
     0.443629124344951, -0.82740741246932, -0.305063630216879,
     -0.368905648047733, 0.356571827302878, 0.948561447188977,
     0.408806205528122, -0.200594873766391, -0.119341396527123,
     -0.27024071140005, 0.159241954007512, 0.327552728288854,
     -0.444355305484197, 1.97003373248264, -0.833211232272124,
     0.24049543124678, 0.170849593613122, 0.0779884767682434,
     -0.612666079765539, -0.374709467850538, -0.212202513372001,
     -0.769369214441271, 1.453493770033, -0.931876168919808, 
-0.119341396527123,
     -0.891249430300173, 0.170849593613122, -0.53141260252627,
     0.339160367894463, 0.971776726400196, -0.902857069905783,
     -1.164028961032, -0.0671070183018789, -0.421140026272977,
     -0.467570584695416, -0.664900457990783, -0.734546295624441,
     -0.0729108381046839, 0.716408655076781, 0.484255862964586,
     -0.241221612386026, -0.415336206470172, 0.432021484739342,
     -0.310867450019684, -0.937679988722612, -0.537216422329075,
     -0.014872640076635, -0.844818871877734, -0.484982044103831,
     -0.212202513372001, 0.385590926316902, -0.363101828244928,
     0.182457233218731, -0.479178224301026, -0.316671269822489,
     -0.734546295624441, -0.8041921332581, 2.7535494058613, 
0.542294060992635,
     3.03793657619874, 1.04722638383666, 1.73207712056764, 
-0.682311917399197,
     0.867307969949709, -0.682311917399197, -0.839015052074929,
     -0.978306727342247, -0.479178224301026, -0.577843160948709,
     -0.670704277793587, 0.0199502787401944, -0.717134836216027,
     0.00253881933177994, -0.252829251991635, 0.298533629274829,
     -0.583646980751514, -0.357298008442123, -0.212202513372001,
     0.391394746119707, 1.05883402344227, -0.763565394638466,
     -0.775173034244076, 1.18071423930117, 0.948561447188977,
     -0.241221612386026, 0.403002385725317, -0.455962945089807,
     5.06346968737765, -0.879641790694564, 0.913738528372148,
     0.588724619415074, 0.368179466908488, -0.386317107456148,
     0.51327496197861, 1.05883402344227, -0.873837970891759, 
0.826681231330074,
     -0.514001143117855, -0.508197323315051, 0.565509340203854,
     -0.937679988722612, 0.199868692627146, -0.734546295624441,
     -0.241221612386026, -0.223810152977611, -0.421140026272977,
     -0.455962945089807, 1.37804411259654, 1.41867085121617, 
-0.688115737202002,
     -0.29345599061127, 1.81913441760971, -0.27024071140005, 
-0.601058440159929,
     -0.154164315343952, -0.653292818385173, -0.305063630216879,
     0.826681231330074, 1.59858926510312, -0.560431701540295,
     -0.0322840994850495, -0.357298008442123, 0.965972906597391,
     -0.403728566864563, -0.438551485681392, -0.386317107456148,
     -0.821603592666515, 0.751231573893611, -0.467570584695416,
     -0.339886549033709, -0.949287628328222, -0.630077539173953,
     0.339160367894463, 0.0489693777542189, -0.206398693569196,
     -0.00326500047102503, -0.833211232272124, 0.269514530260805,
     -0.780976854046881, 1.5753739858919, -1.02473728576469, 
-0.780976854046881,
     0.321748908486049, -0.0729108381046839, -0.984110547145052,
     0.455236763950561, 0.27531835006361, -0.821603592666515,
     0.582920799612269, 0.339160367894463, -0.0903222975130984,
     -0.165771954949562, -0.0787146579074886, -0.53141260252627,
     0.495863502570196, -1.03054110556749, 0.344964187697268,
     -1.11759840260956, 1.5695701660891, 0.675781916457147, 
0.333356548091658,
     4.20450435656252, 1.91779935425739, -0.949287628328222, 
0.397198565922512,
     -1.19304806004603, 0.228887791641171, -0.931876168919808,
     0.832485051132879, -0.601058440159929, -0.125145216329928,
     0.182457233218731, -0.839015052074929, -0.200594873766391,
     1.99324901169386, -0.978306727342247, -0.0380879192878545,
     -0.25863307179444, -0.757761574835661, 0.443629124344951,
     -0.577843160948709, 0.368179466908488, -0.339886549033709,
     -1.12920604221517, -0.287652170808465, -0.0438917390906594,
     -1.25108625807408, 1.0936569422591, 0.0431655579514139, 
-0.502393503512246,
     -0.856426511483344, -0.136752855935538, 0.223083971838366,
     -0.101929937118708, 0.263710710458, -0.873837970891759, 
-0.51980496292066,
     -0.374709467850538, -0.757761574835661, -0.734546295624441,
     -0.28184835100566, -0.82740741246932, 0.687389556062757,
     0.420413845133732, -0.0671070183018789, -0.722938656018832,
     -0.618469899568344, 2.76515704546691, 0.0083426391345845,
     -0.421140026272977, -0.351494188639318, -0.955091448131027,
     1.91199553445459, -0.595254620357124, 1.46510140963861, 
-0.618469899568344,
     0.0953999361766579, 3.18883589107166, -0.235417792583221,
     3.15401297225484, 0.0489693777542189, -1.10018694320115,
     -0.125145216329928, -0.194791053963586, 0.199868692627146,
     1.0936569422591, 0.53649024118983, -0.368905648047733, 
1.77270385918727,
     0.617743718429098, 1.12267604127312, -0.142556675738343,
     -0.635881358976758, 4.08842796050642, -0.25863307179444,
     -0.0729108381046839, 2.71292266724166, -0.548824061934685,
     0.571313160006659, 0.884719429358123, -0.914464709511393,
     0.623547538231903, -0.0903222975130984, 5.27240720027862,
     1.88297643544056, -0.473374404498221, -0.339886549033709,
     -0.82740741246932, -0.798388313455295, -0.212202513372001,
     -0.699723376807612, 0.675781916457147, 0.0315579183458044,
     -0.653292818385173, -0.81579977286371, 1.00079582541422,
     0.768643033302025, 4.72684813881496, 0.199868692627146, 
0.571313160006659,
     0.426217664936537, -0.415336206470172, -0.502393503512246,
     -0.572039341145904, 1.22134097792081, -1.06536402438432,
     -0.287652170808465, -0.142556675738343, -0.328278909428099,
     -0.357298008442123, 3.12499387324081, 0.170849593613122,
     -0.159968135146757, -0.351494188639318, 0.00253881933177994,
     -0.595254620357124, 0.553901700598245, 1.11106840166751,
     -0.833211232272124, -0.0613031984990739, 0.687389556062757,
     -0.868034151088954, 1.94101463346861, 0.194064872824341,
     -0.380513287653343, -0.148360495541147, -0.676508097596392,
     -0.27024071140005, -0.833211232272124, 1.29098681555447,
     0.507471142175805, -0.450159125287002, -0.717134836216027,
     -0.873837970891759, 0.269514530260805, -0.659096638187978,
     -0.891249430300173, -0.908660889708588, 0.472648223358976,
     -0.229613972780416, -1.07697166398993, -0.624273719371149,
     -0.780976854046881, 1.27357535614605, -0.0613031984990739,
     1.34322119377971, 0.577116979809464, -0.415336206470172,
     1.05883402344227, 0.0779884767682434, -0.751957755032856,
     -0.0845184777102936, -0.821603592666515, 0.855700330344099,
     -0.978306727342247, -0.218006333174806, -0.833211232272124,
     -0.734546295624441, -0.955091448131027, 1.25036007693483,
     -0.920268529314198, 0.199868692627146, -0.897053250102978,
     -1.03054110556749, -1.22206715906005, -0.931876168919808,
     -0.455962945089807, -0.177379594555172, 0.27531835006361,
     -0.995718186750661, -0.926072349117003, -0.757761574835661,
     -0.839015052074929, 0.385590926316902, -1.14081368182078,
     1.00659964521703, 0.716408655076781, 0.664174276851537, 
-0.728742475821636,
     -0.943483808525417, -0.891249430300173, -0.740350115427246,
     0.51327496197861, 0.362375647105683, -0.502393503512246,
     -0.374709467850538, -0.374709467850538, 1.15169514028715,
     0.890523249160928, 0.716408655076781, -0.0380879192878545,
     -0.717134836216027, -0.264436891597245, -0.27024071140005,
     0.844092690738489, -0.276044531202855, 0.77444685310483,
     -0.537216422329075, -0.525608782723465, 0.194064872824341,
     0.77444685310483, 0.832485051132879, 0.147634314401902, 
-1.13500986201798,
     -0.455962945089807, -0.960895267933832, -1.0421487451731,
     -0.496589683709441, 0.902130888766538, 0.646762817443122,
     -0.798388313455295, -0.0554993786962692, -0.879641790694564,
     0.397198565922512, 0.983384366005806, 0.54809788079544, 
-0.502393503512246,
     1.12267604127312, -0.177379594555172, 0.0779884767682434,
     -0.421140026272977, 1.35482883338532, -0.386317107456148,
     0.188261053021536, -0.676508097596392, 0.124419035190682,
     0.472648223358976, -0.496589683709441, 0.553901700598245,
     1.25036007693483, 0.130222854993487, -0.339886549033709,
     1.58698162549751, 0.815073591724465, 0.693193375865562, 
-0.55462788173749,
     1.23875243732922, -0.705527196610417, -0.821603592666515,
     -0.537216422329075, -0.780976854046881, -0.136752855935538,
     2.83480288310057, -0.444355305484197, 1.02981492442825, 
0.611939898626293,
     -0.444355305484197, -0.426943846075782, -1.03054110556749,
     -0.287652170808465, -0.728742475821636, -0.415336206470172,
     2.96248691876227, 2.83480288310057, -0.345690368836514, 
0.037361738148609,
     -0.339886549033709, 0.333356548091658, 0.118615215387878,
     -0.450159125287002, 1.38965175220215, -0.0903222975130984,
     -0.809995953060905, -0.241221612386026, -0.276044531202855,
     -0.914464709511393, -0.241221612386026, -0.194791053963586,
     -0.0729108381046839, -0.0729108381046839, -0.00906882027383001,
     -0.154164315343952, 0.861504150146904, 0.443629124344951,
     0.263710710458, -0.821603592666515, 2.06289484932752, 
-0.856426511483344,
     -0.148360495541147, 0.165045773810317, -0.351494188639318,
     0.466844403556171, -0.688115737202002, 0.107007575782268,
     0.890523249160928, 1.01820728482264, 1.07624548285068, 
0.234691611443976,
     0.223083971838366, 0.0083426391345845, 0.107007575782268,
     0.0663808371626334, -1.34394737491895, -0.908660889708588,
     -1.2743015372853, 0.530686421387025, -1.03054110556749, 
-0.188987234160782,
     -1.21045951945444, -1.06536402438432, 1.38965175220215, 
-1.30912445610213,
     -1.0421487451731, -0.392120927258953, 0.25210307085239, 
0.472648223358976,
     0.344964187697268, -0.247025432188831, 0.449432944147756,
     1.32000591456849, -0.0438917390906594, -0.5662355213431,
     -0.148360495541147, -0.624273719371149, -0.589450800554319,
     -0.165771954949562, -0.0845184777102936, 0.165045773810317,
     0.188261053021536, -0.374709467850538, -0.432747665878587,
     -0.0380879192878545, 0.478452043161781, -0.223810152977611,
     1.47090522944142, 0.582920799612269, -0.119341396527123,
     -0.426943846075782, -0.107733756921513, 0.0721846569654384,
     -0.775173034244076, -0.107733756921513, 0.141830494599097,
     0.27531835006361, 0.00253881933177994), .Dim = c(1031L, 1L
     ), "`scaled:center`" = 16.3281280310378, "`scaled:scale`" = 
8.61501592034884),
     spp = structure(c(157L, 183L, 246L, 154L, 110L, 154L, 186L,
     154L, 165L, 245L, 110L, 275L, 246L, 154L, 55L, 157L, 183L,
     55L, 183L, 275L, 157L, 55L, 246L, 154L, 245L, 110L, 110L,
     246L, 245L, 245L, 186L, 245L, 183L, 275L, 186L, 186L, 186L,
     110L, 246L, 275L, 55L, 157L, 157L, 55L, 183L, 275L, 270L,
     80L, 99L, 80L, 141L, 75L, 160L, 10L, 70L, 265L, 64L, 78L,
     147L, 75L, 239L, 64L, 101L, 239L, 70L, 99L, 222L, 226L, 114L,
     265L, 23L, 160L, 241L, 114L, 270L, 20L, 226L, 70L, 241L,
     75L, 101L, 114L, 147L, 226L, 50L, 180L, 71L, 270L, 141L,
     51L, 180L, 101L, 160L, 10L, 10L, 20L, 51L, 50L, 51L, 71L,
     64L, 180L, 265L, 71L, 20L, 239L, 99L, 222L, 222L, 80L, 241L,
     147L, 70L, 239L, 114L, 114L, 227L, 173L, 37L, 81L, 81L, 239L,
     197L, 114L, 265L, 239L, 63L, 197L, 63L, 227L, 265L, 197L,
     179L, 179L, 70L, 173L, 70L, 179L, 227L, 81L, 20L, 37L, 265L,
     20L, 37L, 265L, 46L, 44L, 63L, 43L, 210L, 124L, 240L, 63L,
     105L, 262L, 262L, 210L, 98L, 62L, 237L, 236L, 209L, 168L,
     2L, 124L, 263L, 240L, 2L, 63L, 168L, 44L, 263L, 263L, 162L,
     105L, 236L, 2L, 236L, 98L, 105L, 209L, 136L, 260L, 264L,
     46L, 2L, 184L, 2L, 69L, 86L, 260L, 264L, 136L, 237L, 209L,
     86L, 136L, 236L, 168L, 86L, 98L, 44L, 69L, 237L, 185L, 185L,
     184L, 237L, 264L, 86L, 260L, 44L, 69L, 236L, 116L, 208L,
     92L, 13L, 13L, 266L, 249L, 201L, 266L, 249L, 79L, 201L, 90L,
     123L, 25L, 145L, 214L, 13L, 72L, 134L, 134L, 214L, 208L,
     212L, 8L, 92L, 92L, 134L, 212L, 208L, 153L, 214L, 8L, 211L,
     8L, 90L, 133L, 133L, 211L, 201L, 25L, 145L, 92L, 92L, 153L,
     72L, 90L, 72L, 123L, 266L, 79L, 211L, 145L, 212L, 153L, 25L,
     116L, 249L, 123L, 79L, 116L, 61L, 273L, 150L, 57L, 156L,
     156L, 156L, 174L, 57L, 156L, 247L, 156L, 274L, 200L, 166L,
     61L, 111L, 247L, 166L, 166L, 247L, 200L, 200L, 200L, 174L,
     166L, 56L, 56L, 56L, 274L, 274L, 274L, 274L, 61L, 166L, 247L,
     57L, 150L, 150L, 174L, 150L, 273L, 150L, 174L, 273L, 111L,
     174L, 200L, 56L, 56L, 247L, 111L, 61L, 61L, 273L, 273L, 156L,
     71L, 24L, 77L, 1L, 135L, 234L, 131L, 63L, 135L, 1L, 234L,
     9L, 192L, 71L, 234L, 192L, 43L, 131L, 63L, 43L, 132L, 43L,
     132L, 238L, 89L, 89L, 159L, 238L, 159L, 24L, 24L, 135L, 1L,
     63L, 238L, 89L, 131L, 192L, 71L, 135L, 9L, 9L, 9L, 205L,
     26L, 205L, 259L, 258L, 26L, 259L, 258L, 269L, 26L, 258L,
     205L, 259L, 269L, 135L, 212L, 145L, 33L, 201L, 253L, 25L,
     253L, 253L, 272L, 153L, 34L, 133L, 153L, 242L, 242L, 34L,
     219L, 72L, 88L, 76L, 76L, 76L, 15L, 17L, 220L, 145L, 15L,
     201L, 83L, 88L, 153L, 133L, 102L, 83L, 33L, 34L, 193L, 212L,
     220L, 214L, 214L, 17L, 88L, 73L, 220L, 193L, 17L, 272L, 15L,
     242L, 93L, 93L, 257L, 39L, 25L, 25L, 39L, 93L, 35L, 72L,
     214L, 83L, 211L, 102L, 102L, 93L, 145L, 211L, 219L, 72L,
     257L, 272L, 39L, 133L, 257L, 212L, 219L, 72L, 193L, 33L,
     83L, 271L, 47L, 146L, 149L, 233L, 195L, 83L, 278L, 201L,
     120L, 163L, 253L, 254L, 112L, 195L, 88L, 163L, 278L, 253L,
     266L, 271L, 14L, 120L, 14L, 149L, 88L, 47L, 112L, 112L, 83L,
     254L, 14L, 254L, 163L, 233L, 47L, 253L, 253L, 88L, 148L,
     130L, 169L, 127L, 117L, 169L, 65L, 127L, 130L, 169L, 254L,
     228L, 172L, 148L, 187L, 204L, 19L, 204L, 117L, 228L, 187L,
     148L, 204L, 117L, 187L, 65L, 172L, 172L, 19L, 19L, 254L,
     65L, 191L, 191L, 190L, 130L, 127L, 228L, 254L, 94L, 229L,
     118L, 118L, 191L, 229L, 252L, 94L, 213L, 224L, 252L, 172L,
     158L, 231L, 38L, 213L, 224L, 230L, 213L, 191L, 38L, 94L,
     228L, 230L, 158L, 158L, 172L, 191L, 38L, 172L, 224L, 228L,
     38L, 230L, 229L, 252L, 41L, 107L, 4L, 152L, 41L, 107L, 107L,
     6L, 126L, 6L, 6L, 255L, 276L, 4L, 104L, 140L, 152L, 125L,
     104L, 151L, 196L, 104L, 57L, 199L, 276L, 4L, 57L, 60L, 40L,
     95L, 50L, 140L, 171L, 40L, 125L, 196L, 196L, 216L, 140L,
     40L, 95L, 125L, 50L, 171L, 178L, 82L, 178L, 74L, 204L, 194L,
     82L, 276L, 82L, 204L, 126L, 74L, 60L, 194L, 178L, 74L, 57L,
     95L, 126L, 50L, 68L, 60L, 40L, 255L, 216L, 194L, 41L, 68L,
     255L, 68L, 204L, 235L, 68L, 250L, 41L, 41L, 161L, 31L, 256L,
     85L, 267L, 66L, 38L, 255L, 139L, 85L, 161L, 215L, 268L, 204L,
     161L, 250L, 221L, 68L, 250L, 255L, 221L, 221L, 139L, 215L,
     4L, 104L, 256L, 104L, 3L, 3L, 267L, 66L, 96L, 235L, 256L,
     38L, 4L, 103L, 4L, 104L, 41L, 31L, 68L, 268L, 119L, 103L,
     187L, 119L, 187L, 53L, 215L, 267L, 235L, 119L, 204L, 113L,
     53L, 268L, 181L, 113L, 68L, 181L, 204L, 113L, 255L, 53L,
     139L, 38L, 68L, 103L, 68L, 187L, 31L, 36L, 206L, 177L, 128L,
     225L, 190L, 189L, 28L, 218L, 11L, 177L, 54L, 137L, 202L,
     121L, 202L, 129L, 206L, 121L, 177L, 91L, 190L, 189L, 169L,
     189L, 129L, 202L, 169L, 203L, 137L, 42L, 22L, 190L, 21L,
     106L, 138L, 194L, 251L, 138L, 36L, 21L, 194L, 137L, 54L,
     218L, 203L, 251L, 128L, 251L, 36L, 128L, 28L, 11L, 28L, 42L,
     138L, 206L, 169L, 22L, 218L, 91L, 203L, 106L, 106L, 91L,
     22L, 243L, 27L, 128L, 128L, 164L, 223L, 243L, 164L, 279L,
     16L, 194L, 164L, 207L, 27L, 223L, 244L, 188L, 45L, 143L,
     232L, 84L, 45L, 194L, 217L, 232L, 54L, 207L, 217L, 232L,
     176L, 45L, 217L, 143L, 243L, 144L, 143L, 49L, 176L, 176L,
     175L, 223L, 194L, 128L, 16L, 169L, 188L, 244L, 188L, 169L,
     84L, 54L, 244L, 49L, 166L, 108L, 52L, 166L, 166L, 56L, 273L,
     155L, 155L, 155L, 155L, 273L, 108L, 5L, 5L, 273L, 100L, 155L,
     100L, 97L, 59L, 59L, 100L, 198L, 165L, 171L, 171L, 171L,
     165L, 100L, 198L, 67L, 273L, 97L, 277L, 67L, 67L, 5L, 5L,
     97L, 165L, 108L, 198L, 182L, 182L, 182L, 59L, 277L, 108L,
     165L, 52L, 97L, 5L, 277L, 100L, 59L, 182L, 198L, 52L, 273L,
     56L, 56L, 198L, 165L, 67L, 166L, 277L, 59L, 56L, 52L, 171L,
     277L, 182L, 67L, 171L, 155L, 166L, 97L, 52L, 56L, 122L, 248L,
     12L, 47L, 142L, 142L, 32L, 7L, 248L, 87L, 122L, 122L, 7L,
     170L, 170L, 29L, 12L, 7L, 32L, 261L, 48L, 170L, 87L, 48L,
     29L, 142L, 12L, 261L, 47L, 87L, 261L, 29L, 248L, 48L, 122L,
     32L, 47L, 274L, 97L, 109L, 30L, 115L, 58L, 58L, 58L, 115L,
     56L, 274L, 56L, 55L, 273L, 273L, 274L, 56L, 274L, 274L, 274L,
     166L, 55L, 58L, 274L, 273L, 273L, 56L, 277L, 115L, 115L,
     58L, 55L, 18L, 18L, 166L, 166L, 277L, 109L, 56L, 274L, 115L,
     200L, 55L, 18L, 18L, 58L, 277L, 277L, 200L, 277L, 200L, 167L,
     273L, 200L, 274L, 274L), .Label = c("1", "2", "3", "4", "5",
     "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16",
     "17", "18", "19", "20", "21", "22", "23", "24", "25", "26",
     "27", "28", "29", "30", "31", "32", "33", "34", "35", "36",
     "37", "38", "39", "40", "41", "42", "43", "44", "45", "46",
     "47", "48", "49", "50", "51", "52", "53", "54", "55", "56",
     "57", "58", "59", "60", "61", "62", "63", "64", "65", "66",
     "67", "68", "69", "70", "71", "72", "73", "74", "75", "76",
     "77", "78", "79", "80", "81", "82", "83", "84", "85", "86",
     "87", "88", "89", "90", "91", "92", "93", "94", "95", "96",
     "97", "98", "99", "100", "101", "102", "103", "104", "105",
     "106", "107", "108", "109", "110", "111", "112", "113", "114",
     "115", "116", "117", "118", "119", "120", "121", "122", "123",
     "124", "125", "126", "127", "128", "129", "130", "131", "132",
     "133", "134", "135", "136", "137", "138", "139", "140", "141",
     "142", "143", "144", "145", "146", "147", "148", "149", "150",
     "151", "152", "153", "154", "155", "156", "157", "158", "159",
     "160", "161", "162", "163", "164", "165", "166", "167", "168",
     "169", "170", "171", "172", "173", "174", "175", "176", "177",
     "178", "179", "180", "181", "182", "183", "184", "185", "186",
     "187", "188", "189", "190", "191", "192", "193", "194", "195",
     "196", "197", "198", "199", "200", "201", "202", "203", "204",
     "205", "206", "207", "208", "209", "210", "211", "212", "213",
     "214", "215", "216", "217", "218", "219", "220", "221", "222",
     "223", "224", "225", "226", "227", "228", "229", "230", "231",
     "232", "233", "234", "235", "236", "237", "238", "239", "240",
     "241", "242", "243", "244", "245", "246", "247", "248", "249",
     "250", "251", "252", "253", "254", "255", "256", "257", "258",
     "259", "260", "261", "262", "263", "264", "265", "266", "267",
     "268", "269", "270", "271", "272", "273", "274", "275", "276",
     "277", "278", "279"), class = "factor"), site = structure(c(1L,
     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
     1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
     19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
     19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
     19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
     19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
     19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
     19L, 19L, 19L, 19L, 19L, 19L, 20L, 20L, 20L, 20L, 20L, 20L,
     20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L,
     20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L,
     20L, 20L, 20L, 20L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L,
     18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L,
     18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L,
     18L, 18L, 18L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L,
     17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L,
     17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L,
     17L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
     11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
     11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
     11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
     11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
     11L, 11L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
     16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
     16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
     16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L,
     16L, 16L, 16L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L,
     15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 12L, 12L,
     12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
     12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
     12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
     12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
     12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
     12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
     12L, 12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L,
     13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
     13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L,
     13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 8L, 8L,
     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
     8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
     8L, 8L, 8L, 8L, 8L, 8L, 8L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 5L, 5L,
     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
     5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L,
     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
     6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
     6L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L,
     10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L,
     10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L,
     10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L,
     10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L,
     10L, 10L, 10L, 10L, 10L, 10L, 10L, 9L, 9L, 9L, 9L, 9L, 9L,
     9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
     9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
     9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
     9L, 9L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
     4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
     4L, 4L, 4L, 4L, 4L, 4L, 4L, 14L, 14L, 14L, 14L, 14L, 14L,
     14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
     14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L,
     14L, 14L, 14L, 14L, 14L, 14L, 14L, 2L, 2L, 2L, 2L, 2L, 2L,
     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
     2L, 2L, 2L, 2L, 2L), .Label = c("A", "B", "C", "D", "E",
     "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "Q", "R",
     "S", "T", "U"), class = "factor")), .Names = c("dep", "ind",
"spp", "site"), row.names = c(2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L, 33L, 34L, 35L,
36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 44L, 45L, 46L, 47L, 48L,
49L, 50L, 51L, 52L, 53L, 54L, 55L, 56L, 57L, 58L, 59L, 60L, 61L,
62L, 63L, 64L, 65L, 66L, 67L, 68L, 69L, 70L, 71L, 72L, 73L, 74L,
75L, 76L, 77L, 78L, 79L, 80L, 81L, 82L, 83L, 84L, 85L, 86L, 87L,
88L, 89L, 90L, 91L, 92L, 93L, 94L, 95L, 96L, 97L, 98L, 99L, 100L,
101L, 102L, 103L, 104L, 105L, 106L, 107L, 108L, 109L, 110L, 111L,
112L, 113L, 114L, 115L, 116L, 117L, 118L, 119L, 120L, 121L, 122L,
123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L,
134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L,
145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L,
156L, 157L, 158L, 159L, 160L, 162L, 163L, 164L, 165L, 166L, 167L,
168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L,
179L, 180L, 181L, 182L, 184L, 185L, 186L, 187L, 188L, 189L, 190L,
191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L,
202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L,
213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L,
224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L,
235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L,
246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L,
257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L,
268L, 269L, 270L, 271L, 272L, 273L, 274L, 275L, 276L, 277L, 279L,
280L, 290L, 291L, 292L, 293L, 294L, 295L, 296L, 297L, 298L, 299L,
300L, 301L, 302L, 303L, 304L, 305L, 306L, 307L, 308L, 309L, 310L,
311L, 312L, 313L, 314L, 315L, 316L, 317L, 318L, 319L, 320L, 321L,
322L, 323L, 324L, 325L, 326L, 327L, 328L, 329L, 330L, 331L, 332L,
333L, 334L, 335L, 336L, 337L, 338L, 339L, 340L, 341L, 342L, 343L,
344L, 345L, 346L, 347L, 348L, 349L, 350L, 352L, 353L, 354L, 355L,
356L, 357L, 358L, 359L, 360L, 361L, 362L, 363L, 364L, 365L, 366L,
367L, 368L, 369L, 370L, 371L, 372L, 373L, 374L, 375L, 376L, 377L,
378L, 379L, 380L, 382L, 383L, 384L, 385L, 386L, 387L, 389L, 390L,
391L, 392L, 393L, 394L, 395L, 396L, 397L, 398L, 399L, 400L, 401L,
402L, 403L, 404L, 405L, 406L, 407L, 408L, 409L, 410L, 411L, 412L,
413L, 414L, 415L, 416L, 418L, 419L, 420L, 422L, 423L, 424L, 425L,
427L, 428L, 429L, 430L, 431L, 432L, 433L, 434L, 435L, 436L, 437L,
438L, 439L, 440L, 442L, 443L, 444L, 445L, 446L, 447L, 448L, 449L,
450L, 451L, 452L, 453L, 454L, 455L, 456L, 458L, 459L, 460L, 462L,
463L, 464L, 465L, 466L, 467L, 468L, 469L, 470L, 472L, 473L, 474L,
475L, 476L, 477L, 479L, 480L, 481L, 482L, 483L, 484L, 485L, 486L,
487L, 489L, 490L, 491L, 492L, 493L, 494L, 495L, 496L, 497L, 498L,
499L, 501L, 502L, 503L, 504L, 505L, 506L, 507L, 508L, 509L, 510L,
511L, 512L, 513L, 514L, 515L, 516L, 517L, 518L, 519L, 520L, 521L,
522L, 523L, 524L, 525L, 526L, 527L, 528L, 529L, 530L, 531L, 532L,
533L, 534L, 535L, 536L, 537L, 538L, 539L, 540L, 541L, 542L, 543L,
544L, 545L, 546L, 547L, 548L, 549L, 550L, 551L, 552L, 553L, 554L,
555L, 556L, 557L, 558L, 559L, 560L, 561L, 562L, 563L, 564L, 565L,
566L, 567L, 568L, 569L, 570L, 571L, 572L, 573L, 574L, 575L, 576L,
578L, 579L, 580L, 581L, 582L, 583L, 584L, 585L, 586L, 587L, 588L,
589L, 590L, 591L, 592L, 593L, 594L, 595L, 596L, 597L, 598L, 599L,
600L, 601L, 602L, 603L, 604L, 605L, 606L, 607L, 608L, 609L, 610L,
611L, 612L, 613L, 614L, 615L, 616L, 617L, 618L, 619L, 620L, 621L,
622L, 623L, 624L, 625L, 626L, 627L, 628L, 629L, 630L, 631L, 632L,
633L, 634L, 635L, 636L, 637L, 638L, 639L, 640L, 641L, 642L, 643L,
644L, 645L, 646L, 647L, 648L, 649L, 650L, 651L, 652L, 653L, 654L,
655L, 656L, 657L, 658L, 659L, 660L, 661L, 662L, 663L, 664L, 665L,
666L, 667L, 668L, 669L, 670L, 671L, 672L, 673L, 674L, 675L, 676L,
677L, 678L, 679L, 680L, 681L, 682L, 683L, 684L, 685L, 686L, 687L,
688L, 690L, 691L, 692L, 693L, 694L, 695L, 696L, 697L, 698L, 699L,
700L, 701L, 702L, 703L, 704L, 705L, 706L, 707L, 708L, 709L, 710L,
711L, 712L, 713L, 714L, 715L, 716L, 717L, 718L, 719L, 720L, 721L,
722L, 723L, 724L, 725L, 726L, 727L, 728L, 729L, 730L, 731L, 732L,
733L, 734L, 735L, 736L, 737L, 738L, 739L, 740L, 741L, 742L, 743L,
744L, 745L, 746L, 747L, 748L, 749L, 750L, 751L, 752L, 753L, 754L,
755L, 756L, 757L, 758L, 760L, 761L, 762L, 763L, 764L, 765L, 766L,
767L, 768L, 777L, 778L, 779L, 780L, 781L, 782L, 783L, 784L, 785L,
786L, 787L, 788L, 789L, 790L, 791L, 792L, 793L, 794L, 795L, 796L,
797L, 798L, 799L, 800L, 801L, 802L, 803L, 804L, 805L, 806L, 807L,
808L, 809L, 810L, 811L, 812L, 813L, 814L, 815L, 816L, 817L, 818L,
819L, 820L, 821L, 822L, 823L, 824L, 825L, 826L, 827L, 828L, 829L,
830L, 831L, 832L, 833L, 834L, 835L, 836L, 837L, 838L, 839L, 840L,
841L, 842L, 847L, 848L, 849L, 850L, 851L, 852L, 853L, 854L, 855L,
856L, 857L, 858L, 859L, 860L, 861L, 862L, 863L, 864L, 865L, 866L,
867L, 868L, 869L, 870L, 871L, 872L, 873L, 874L, 875L, 876L, 877L,
878L, 879L, 880L, 882L, 883L, 884L, 885L, 886L, 887L, 888L, 889L,
890L, 891L, 892L, 893L, 894L, 896L, 897L, 898L, 899L, 900L, 902L,
903L, 904L, 905L, 906L, 907L, 908L, 909L, 910L, 911L, 912L, 913L,
914L, 915L, 916L, 917L, 918L, 919L, 920L, 921L, 922L, 923L, 924L,
925L, 926L, 927L, 928L, 929L, 930L, 931L, 932L, 933L, 934L, 935L,
936L, 937L, 938L, 939L, 940L, 941L, 942L, 943L, 944L, 945L, 946L,
947L, 948L, 949L, 950L, 951L, 952L, 953L, 954L, 955L, 956L, 957L,
958L, 959L, 960L, 961L, 962L, 963L, 964L, 965L, 966L, 967L, 968L,
969L, 970L, 971L, 972L, 973L, 974L, 975L, 976L, 977L, 978L, 979L,
980L, 981L, 982L, 983L, 984L, 985L, 986L, 987L, 988L, 989L, 990L,
991L, 992L, 993L, 994L, 995L, 996L, 997L, 998L, 999L, 1000L,
1001L, 1002L, 1003L, 1004L, 1005L, 1006L, 1007L, 1008L, 1009L,
1010L, 1011L, 1012L, 1013L, 1014L, 1015L, 1016L, 1017L, 1018L,
1019L, 1024L, 1025L, 1026L, 1027L, 1028L, 1029L, 1030L, 1031L,
1032L, 1033L, 1034L, 1035L, 1036L, 1037L, 1038L, 1039L, 1040L,
1041L, 1042L, 1043L, 1044L, 1045L, 1046L, 1047L, 1048L, 1049L,
1050L, 1051L, 1052L, 1053L, 1054L, 1055L, 1056L, 1057L, 1058L,
1059L, 1060L, 1061L, 1062L, 1063L, 1064L, 1065L, 1066L, 1067L,
1069L, 1070L, 1071L, 1073L, 1074L, 1075L, 1076L, 1077L, 1078L,
1080L, 1081L, 1082L), class = "data.frame")


From justinthong93 at gmail.com  Tue Jul 19 13:53:39 2016
From: justinthong93 at gmail.com (Justin Thong)
Date: Tue, 19 Jul 2016 12:53:39 +0100
Subject: [R] Missing rows anova
Message-ID: <CAEtAGep+YPEbnxEOV8rf1XZd-G+uENS=eQ+U2U1ykPtoMmm_bQ@mail.gmail.com>

Why does the S:x1 column disappear (presumably S:x1 goes into ID but I dont
know why)? S is a factor, x1 is a covariate and ID is a factor.

    rich.side<-aov(y~S*x1+ID)
    summary(rich.side)

Below is the model frame

    model.frame(~S*x1+ID)

        S x1  ID
    1   1 12   A
    2   1 12   A
    3   1 12   A
    4   1 12   A
    5   1  0   B
    6   1  0   B
    7   1  0   B
    8   1  0   B
    9   1  0   C
    10  1  0   C
    11  1  0   C
    12  1  0   C
    13  1  0   D
    14  1  0   D
    15  1  0   D
    16  1  0   D
    17  1  0   E
    18  1  0   E
    19  1  0   E
    20  1  0   E
    21  1  0   F
    22  1  0   F
    23  1  0   F
    24  1  0   F
    25  2  6  AB
    26  2  6  AB
    27  2  6  AB
    28  2  6  AB
    29  2  6  AC
    30  2  6  AC
    31  2  6  AC
    32  2  6  AC
    33  2  6  AD
    34  2  6  AD
    35  2  6  AD
    36  2  6  AD
    37  2  6  AE
    38  2  6  AE
    39  2  6  AE
    40  2  6  AE
    41  2  6  AF
    42  2  6  AF
    43  2  6  AF
    44  2  6  AF
    45  2  0  BC
    46  2  0  BC
    47  2  0  BC
    48  2  0  BC
    49  2  0  BD
    50  2  0  BD
    51  2  0  BD
    52  2  0  BD
    53  2  0  BE
    54  2  0  BE
    55  2  0  BE
    56  2  0  BE
    57  2  0  BF
    58  2  0  BF
    59  2  0  BF
    60  2  0  BF
    61  2  0  CD
    62  2  0  CD
    63  2  0  CD
    64  2  0  CD
    65  2  0  CE
    66  2  0  CE
    67  2  0  CE
    68  2  0  CE
    69  2  0  CF
    70  2  0  CF
    71  2  0  CF
    72  2  0  CF
    73  2  0  DE
    74  2  0  DE
    75  2  0  DE
    76  2  0  DE
    77  2  0  DF
    78  2  0  DF
    79  2  0  DF
    80  2  0  DF
    81  2  0  EF
    82  2  0  EF
    83  2  0  EF
    84  2  0  EF
    85  3  4 ABC
    86  3  4 ABC
    87  3  4 ABC
    88  3  4 ABC
    89  3  4 ABD
    90  3  4 ABD
    91  3  4 ABD
    92  3  4 ABD
    93  3  4 ABE
    94  3  4 ABE
    95  3  4 ABE
    96  3  4 ABE
    97  3  4 ABF
    98  3  4 ABF
    99  3  4 ABF
    100 3  4 ABF
    101 3  4 ACD
    102 3  4 ACD
    103 3  4 ACD
    104 3  4 ACD
    105 3  4 ACE
    106 3  4 ACE
    107 3  4 ACE
    108 3  4 ACE
    109 3  4 ACF
    110 3  4 ACF
    111 3  4 ACF
    112 3  4 ACF
    113 3  4 ADE
    114 3  4 ADE
    115 3  4 ADE
    116 3  4 ADE
    117 3  4 ADF
    118 3  4 ADF
    119 3  4 ADF
    120 3  4 ADF
    121 3  4 AEF
    122 3  4 AEF
    123 3  4 AEF
    124 3  4 AEF
    125 3  0 BCD
    126 3  0 BCD
    127 3  0 BCD
    128 3  0 BCD
    129 3  0 BCE
    130 3  0 BCE
    131 3  0 BCE
    132 3  0 BCE
    133 3  0 BCF
    134 3  0 BCF
    135 3  0 BCF
    136 3  0 BCF
    137 3  0 BDE
    138 3  0 BDE
    139 3  0 BDE
    140 3  0 BDE
    141 3  0 BDF
    142 3  0 BDF
    143 3  0 BDF
    144 3  0 BDF
    145 3  0 BEF
    146 3  0 BEF
    147 3  0 BEF
    148 3  0 BEF
    149 3  0 CDE
    150 3  0 CDE
    151 3  0 CDE
    152 3  0 CDE
    153 3  0 CDF
    154 3  0 CDF
    155 3  0 CDF
    156 3  0 CDF
    157 3  0 CEF
    158 3  0 CEF
    159 3  0 CEF
    160 3  0 CEF
    161 3  0 DEF
    162 3  0 DEF
    163 3  0 DEF
    164 3  0 DEF

-- 
Yours sincerely,
Justin

*I check my email at 9AM and 4PM everyday*
*If you have an EMERGENCY, contact me at +447938674419(UK) or
+60125056192(Malaysia)*

	[[alternative HTML version deleted]]


From maettuw at students.unibe.ch  Tue Jul 19 14:27:29 2016
From: maettuw at students.unibe.ch (maettuw at students.unibe.ch)
Date: Tue, 19 Jul 2016 12:27:29 +0000
Subject: [R] Summing up layers
Message-ID: <E9DA6CABC0F0734A9980C6E78E9024BA070AA8E0@aai-exch-mbx8.campus.unibe.ch>

Hello everyone

I have gridded output data from a climate model. I would like to take the average of these grids and plot them. I have tried to plot it with gglot or image.plot (field/raster package). I have to do this extensively meaning that a manual way is not what I am looking for. My data has three dimensions. Longitude,Latitude and the variable itself. Below is a code that works perfectly fine. This is a plot for one single year.

H3 <- ((detrended_TOA_rad[,,28:28]))
image.plot(lon,lat,H3,col=tim.colors())

setwd("/scratch/maettu/maettu/data/fup013/echam5/net_Radiation")
spatial_TOA_net_Rad <- open.nc("spatial_yearmean_TS_TOA_net_radiation.nc")
TOA_net_rad <- read.nc(spatial_TOA_net_Rad)
detrended_TOA_rad <- TOA_net_rad$ttr
lon_TOA <- TOA_net_rad$lon
lat_TOA <- TOA_net_rad$lat


Down here I have tried to make a plot over several years, which did not work!!

H3 <- ((detrended_TOA_rad[,,28:38]))
image.plot(lon,lat,H3,col=tim.colors())

-->

Error in seq.default(zlim[1], zlim[2], , nlevel) :
  'from' cannot be NA, NaN or infinite

I googled that error message but didnt get smart out of it.
The ggplot way worked better but the problem is that I cannot create a dataset that contains the coordinates as well as the values. Therefore the projection is not correct.

melted1 <- melt(H3)

ggplot(aes(x = Var1, y = Var2, fill = value), data = melted1) +geom_tile() +geom_tile() + scale_fill_distiller(palette = "Set1") +
  geom_polygon(data=map,aes(x=long, y=lat, group=group), colour="black", fill="transparent")

I have also tried to aggregate with the "overlay" function from the raster packege. However I cannot coerce my array files into a raster...
So the main problem is that I cannot aggrregate the layers.

Appreciate the help! Matthias

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Tue Jul 19 14:41:51 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Tue, 19 Jul 2016 12:41:51 +0000
Subject: [R] Anova() type iii SS plots and diagnostics
In-Reply-To: <84D05724-8221-4389-AE08-B9F7C0337CEE@icloud.com>
References: <84D05724-8221-4389-AE08-B9F7C0337CEE@icloud.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836544514@FHSDB2D11-2.csu.mcmaster.ca>

Dear Pamela,

I'm afraid that this question seems confused. Type III sums of squares are computed for a linear model -- it's the *linear model* that you'd check, in the usual manner, for outliers, etc., and this has nothing to do with how the ANOVA table for the model is computed.

Best,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Pamela
> Wong
> Sent: July 18, 2016 10:39 PM
> To: r-help at r-project.org
> Subject: [R] Anova() type iii SS plots and diagnostics
> 
> I am wondering if there is a way to plot results and model diagnostics (to check
> for outliers, homoscedasticity, normality, collinearity) using type III sums of
> squares in R ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From amwootte at ncsu.edu  Tue Jul 19 15:25:41 2016
From: amwootte at ncsu.edu (Adrienne Wootten)
Date: Tue, 19 Jul 2016 09:25:41 -0400
Subject: [R] [FORGED] intersection of two polygons which are not
	shapefiles
In-Reply-To: <1227e3d3-5056-b254-8378-4bb438fa32e8@auckland.ac.nz>
References: <CAOV3wDBSa3oBs1zsZzO7Vb_x1=dyGx6zXrr9=Ghsy_a1sqsBhg@mail.gmail.com>
	<1227e3d3-5056-b254-8378-4bb438fa32e8@auckland.ac.nz>
Message-ID: <CAOV3wDDpgA1GWmMUpR=qtzWXWrWjNkaiZd9HEv3ygfy15metow@mail.gmail.com>

Thanks! That's just what I was looking for!
A

On Mon, Jul 18, 2016 at 5:56 PM, Rolf Turner <r.turner at auckland.ac.nz>
wrote:

> On 19/07/16 01:16, Adrienne Wootten wrote:
>
>> All,
>>
>> Greetings! I hope things are going well for all!  I apologize if someone's
>> already answered this and I'm just not finding it.  Here's what I'd like
>> to
>> do, but I'm hitting a brick wall with it.
>>
>> I have two sets of points for which I've already determined which ones
>> points for the boundaries with the chull function.  What I need for what
>> I'm doing is the coordinates where the two resulting polygons overlap.
>> These are not raster grids, nor shapefiles.  What I have to work with are
>> the two data frames with the points at the vertices of each polygon
>> (included below).
>>
>> chx1
>>>
>>               x          y
>> 1  0.5822569 -0.5555878
>> 2  0.5338428 -0.5883604
>> 3 -0.3442943 -0.6701115
>> 4  -0.7409293  0.2286962
>> 5  0.2147221  0.8061485
>> 6  0.4914146  0.4941865
>> 7  0.5822569 -0.5555878
>>
>> chx2
>>>
>>               x          y
>> 1  0.7163506 -0.4357497
>> 2  0.6513128 -0.5395180
>> 3   0.1818315 -0.6317423
>> 4  -0.6394281 -0.5765610
>> 5 -0.6044681  0.1831627
>> 6 -0.5799485  0.3231473
>> 7  0.2248476  0.9601908
>> 8  0.7163506 -0.4357497
>>
>>
>> If anyone has any ideas on how to get what I'm after I'd appreciate it!
>> I've tried a lot of things from the raster, rgeos, and more.  Knowing me
>> it's something obvious I'm just not seeing right now.
>>
>
> You can do this very easily using the spatstat package:
>
> library(spatstat)
> X1 <- read.table(textConnection("
>              x          y
> 1  0.5822569 -0.5555878
> 2  0.5338428 -0.5883604
> 3 -0.3442943 -0.6701115
> 4  -0.7409293  0.2286962
> 5  0.2147221  0.8061485
> 6  0.4914146  0.4941865
> 7  0.5822569 -0.5555878"))
>
> X2 <- read.table(textConnection("
>                x          y
> 1  0.7163506 -0.4357497
> 2  0.6513128 -0.5395180
> 3   0.1818315 -0.6317423
> 4  -0.6394281 -0.5765610
> 5 -0.6044681  0.1831627
> 6 -0.5799485  0.3231473
> 7  0.2248476  0.9601908
> 8  0.7163506 -0.4357497"))
>
> X1 <- reverse.xypolygon(X1) # Vertices are in the wrong
> X2 <- reverse.xypolygon(X2) # (clockwise) order.
> W1 <- owin(poly=X1)
> W2 <- owin(poly=X2)
> WI <- intersect.owin(W1,W2)
>
> plot(union.owin(W1,W2),col="blue",main="")
> plot(WI,add=TRUE,col="red")
>
> HTH
>
> cheers,
>
> Rolf Turner
>
> P.S.  To extract the coordinates of the intersection polygon you can do:
>
> WI$bdry[[1]]
>
> R. T.
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>



-- 
Adrienne Wootten
Ph.D Candidate / Graduate Research Assistant
State Climate Office of North Carolina
Department of Marine, Earth and Atmospheric Sciences
North Carolina State University

	[[alternative HTML version deleted]]


From yhbrent at yahoo.com  Tue Jul 19 15:40:50 2016
From: yhbrent at yahoo.com (Brent)
Date: Tue, 19 Jul 2016 13:40:50 +0000 (UTC)
Subject: [R] Has R recently made performance improvements in accumulation?
In-Reply-To: <mailman.6.1468922401.11334.r-help@r-project.org>
References: <mailman.6.1468922401.11334.r-help@r-project.org>
Message-ID: <843397525.1408616.1468935651020.JavaMail.yahoo@mail.yahoo.com>

Subtitle: or, more likely, am I benchmarking wrong?

I am new to R, but I have read that R is a hive of performance pitfalls.  A very common case is if you try to accumulate results into a typical immutable R data structure.

Exhibit A for this confusion is this StackOverflow question on an algorithm for O(1) performance in appending to a list:
    https://stackoverflow.com/questions/2436688/append-an-object-to-a-list-in-r-in-amortized-constant-time-o1
The original question was asked in 2010-03-13, and responses have trickled in for at least the next 5 years.

Before mentioning my problem, I instead offer an example from someone vastly better than me in R, which I am sure should be free of mistakes.  Consider this interesting article on efficient accumulation in R:
    http://www.win-vector.com/blog/2015/07/efficient-accumulation-in-r/

In that article's first example, it claims that the "obvious ?for-loop? solution" (accumulation into a data.frame using rbind) is:
    is incredibly slow.
    ...
    we pay the cost of processing n*(n+1)/2 rows of data

In other words, what looks like an O(n) algorithm is actually O(n^2).

Sounds awful.  But when I tried executing their example on my machine, I see O(n) NOT O(n^2) behavior!

Here is the complete code that I executed:

    mkFrameForLoop <- function(nRow,nCol) {
        d <- c()
        for(i in seq_len(nRow)) {
            ri <- mkRow(nCol)
            di <- data.frame(ri, stringsAsFactors=FALSE)
            d <- rbind(d,di)
        }
        d
    }
    
    mkRow <- function(nCol) {
        x <- as.list(rnorm(nCol))
        # make row mixed types by changing first column to string
        x[[1]] <- ifelse(x[[1]]>0,'pos','neg')
        names(x) <- paste('x',seq_len(nCol),sep='.')
        x
    }
    
    t1 = Sys.time()
    x = mkFrameForLoop(100, 10)
    tail(x)
    t2 = Sys.time()
    t2 - t1
    
    t1 = Sys.time()
    x = mkFrameForLoop(200, 10)
    tail(x)
    t2 = Sys.time()
    t2 - t1
    
    t1 = Sys.time()
    x = mkFrameForLoop(400, 10)
    tail(x)
    t2 = Sys.time()
    t2 - t1

And here is what I got for the execution times:

    Time difference of 0.113005876541138 secs
    Time difference of 0.205012083053589 secs
    Time difference of 0.390022993087769 secs

That's a linear, not quadratic, increase in execution time as a function of nRow!  It is NOT what I was expecting, altho it was nice to see.

Notes:
    --the functions above are copy and pastes from the article
    --the execution time measurement code is all that I added
        --yes, that time measurement code is crude
        --I am aware of R benchmarking frameworks, in fact, I first tried using some of them
        --but when I got unexpected results, I went back to the simplest code possible, which is the above
        --it turns out that I get the same results regardless of how I measure
    --each measurement doubles the number of rows (from 100 to 200 to 400), which is what should be increasing to bring out rbind's allegedly bad behavior
    --my machine has a Xeon E3 1245, 8 GB of RAM, running Win 7 Pro 64 bit, using R 3.3.1 (latest release as of today)

Given those unexpected results, you now understand the title of my post.  So, has R's runtime somehow gotten greater intelligence recently so that it knows when it does not need to copy a data structure?  Maybe in the case above, it does a quick shallow copy instead of a deep copy?  Or, am I benchmarking wrong?

What motivated my investigation is that I want to store a bunch of Monte Carlo simulation results in a numeric matrix.  When I am finished with my code, I know that it will be a massive matrix (e.g. 1,000,000 or more rows).  So I was concerned about performance, and went about benchmarking.  Let me know if you want to see that benchmark code.  I found that assignment to a matrix does not seem to generate a copy, even tho assignment is a mutating operation, so I was worried that it might.  But when I investigated deeper, such as with the above code, I got worried that I cannot trust my results.


I look forward to any feedback that you can give.

I would especially appreciate any links that explain how you can determine what the R runtime actually does.


From thierry.onkelinx at inbo.be  Tue Jul 19 16:27:39 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 19 Jul 2016 16:27:39 +0200
Subject: [R] Has R recently made performance improvements in
	accumulation?
In-Reply-To: <843397525.1408616.1468935651020.JavaMail.yahoo@mail.yahoo.com>
References: <mailman.6.1468922401.11334.r-help@r-project.org>
	<843397525.1408616.1468935651020.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAJuCY5zy7TuTLZAajctNv1qny7QUnREaG-LpEL-6cNHnWkUzNw@mail.gmail.com>

Dear Brent,

I can confirm your timings with

library(microbenchmark)
microbenchmark(
  mkFrameForLoop(100, 10),
  mkFrameForLoop(200, 10),
  mkFrameForLoop(400, 10)
)

but profiling your code shown that rbind only uses a small fraction on the
cpu time used by the function.

profvis::profvis({mkFrameForLoop(100, 10)})

So I cleaned your example further into the function below. Now rbind is
using most of cpu time. And the timings indicate an O(n^2) relation.

minimal <- function(rows, cols){
  x <- matrix(NA_integer_, ncol = cols, nrow = 0)
  for (i in seq_len(rows)){
    x <- rbind(x, rep(i, 10))
  }
}

profvis::profvis({minimal(1000, 100)})

timing <- microbenchmark(
  X50 = minimal(50, 50),
  X100 = minimal(100, 50),
  X200 = minimal(200, 50),
  X400 = minimal(400, 50),
  X800 = minimal(800, 50),
  X1600 = minimal(1600, 50)
)
timing
Unit: microseconds
  expr        min         lq        mean     median          uq        max
neval cld
   X50    199.006    212.278    233.8444    235.728    247.3770    296.987
  100 a
  X100    565.693    593.957    827.8733    618.835    640.1925   2950.139
  100 a
  X200   1804.059   1876.390   2166.1106   1903.370   1938.7115   4263.967
  100 a
  X400   6453.913   8755.848   8546.4339   8890.884   8961.7535  13259.024
  100 a
  X800  30575.048  32913.186  36555.0118  33093.243  34620.5895 178740.765
  100  b
 X1600 130976.429 133674.679 151494.6492 135197.087 137327.1235 292291.385
  100   c
timing$N <- as.integer(gsub("X", "", levels(timing$expr)))[timing$expr]
model <- lm(time ~ poly(N, 4), data = timing)
summary(model)

Call:
lm(formula = time ~ poly(N, 4), data = timing)

Residuals:
      Min        1Q    Median        3Q       Max
-20518162  -3378940   -130815    -45881 142183951

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept)   33303987     843350  39.490   <2e-16 ***
poly(N, 4)1 1286962014   20657783  62.299   <2e-16 ***
poly(N, 4)2  338770077   20657783  16.399   <2e-16 ***
poly(N, 4)3     222734   20657783   0.011    0.991
poly(N, 4)4   -2260902   20657783  -0.109    0.913
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 20660000 on 595 degrees of freedom
Multiple R-squared:  0.8746, Adjusted R-squared:  0.8738
F-statistic:  1038 on 4 and 595 DF,  p-value: < 2.2e-16
newdata <- data.frame(N = pretty(timing$N, 40))
newdata$time <- predict(model, newdata = newdata)
plot(newdata$N, newdata$time, type = "l")
plot(newdata$N, sqrt(newdata$time), type = "l")

model2 <- lm(sqrt(time) ~ poly(N, 4), data = timing)
summary(model2)
Call:
lm(formula = sqrt(time) ~ poly(N, 4), data = timing)

Residuals:
   Min     1Q Median     3Q    Max
-756.3 -202.8  -54.7   -5.5 7416.5

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)   3980.36      33.13 120.160  < 2e-16 ***
poly(N, 4)1 100395.40     811.41 123.730  < 2e-16 ***
poly(N, 4)2   2191.34     811.41   2.701  0.00712 **
poly(N, 4)3   -803.54     811.41  -0.990  0.32243
poly(N, 4)4     82.09     811.41   0.101  0.91945
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 811.4 on 595 degrees of freedom
Multiple R-squared:  0.9626, Adjusted R-squared:  0.9624
F-statistic:  3829 on 4 and 595 DF,  p-value: < 2.2e-16


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-07-19 15:40 GMT+02:00 Brent via R-help <r-help at r-project.org>:

> Subtitle: or, more likely, am I benchmarking wrong?
>
> I am new to R, but I have read that R is a hive of performance pitfalls.
> A very common case is if you try to accumulate results into a typical
> immutable R data structure.
>
> Exhibit A for this confusion is this StackOverflow question on an
> algorithm for O(1) performance in appending to a list:
>
> https://stackoverflow.com/questions/2436688/append-an-object-to-a-list-in-r-in-amortized-constant-time-o1
> The original question was asked in 2010-03-13, and responses have trickled
> in for at least the next 5 years.
>
> Before mentioning my problem, I instead offer an example from someone
> vastly better than me in R, which I am sure should be free of mistakes.
> Consider this interesting article on efficient accumulation in R:
>     http://www.win-vector.com/blog/2015/07/efficient-accumulation-in-r/
>
> In that article's first example, it claims that the "obvious ?for-loop?
> solution" (accumulation into a data.frame using rbind) is:
>     is incredibly slow.
>     ...
>     we pay the cost of processing n*(n+1)/2 rows of data
>
> In other words, what looks like an O(n) algorithm is actually O(n^2).
>
> Sounds awful.  But when I tried executing their example on my machine, I
> see O(n) NOT O(n^2) behavior!
>
> Here is the complete code that I executed:
>
>     mkFrameForLoop <- function(nRow,nCol) {
>         d <- c()
>         for(i in seq_len(nRow)) {
>             ri <- mkRow(nCol)
>             di <- data.frame(ri, stringsAsFactors=FALSE)
>             d <- rbind(d,di)
>         }
>         d
>     }
>
>     mkRow <- function(nCol) {
>         x <- as.list(rnorm(nCol))
>         # make row mixed types by changing first column to string
>         x[[1]] <- ifelse(x[[1]]>0,'pos','neg')
>         names(x) <- paste('x',seq_len(nCol),sep='.')
>         x
>     }
>
>     t1 = Sys.time()
>     x = mkFrameForLoop(100, 10)
>     tail(x)
>     t2 = Sys.time()
>     t2 - t1
>
>     t1 = Sys.time()
>     x = mkFrameForLoop(200, 10)
>     tail(x)
>     t2 = Sys.time()
>     t2 - t1
>
>     t1 = Sys.time()
>     x = mkFrameForLoop(400, 10)
>     tail(x)
>     t2 = Sys.time()
>     t2 - t1
>
> And here is what I got for the execution times:
>
>     Time difference of 0.113005876541138 secs
>     Time difference of 0.205012083053589 secs
>     Time difference of 0.390022993087769 secs
>
> That's a linear, not quadratic, increase in execution time as a function
> of nRow!  It is NOT what I was expecting, altho it was nice to see.
>
> Notes:
>     --the functions above are copy and pastes from the article
>     --the execution time measurement code is all that I added
>         --yes, that time measurement code is crude
>         --I am aware of R benchmarking frameworks, in fact, I first tried
> using some of them
>         --but when I got unexpected results, I went back to the simplest
> code possible, which is the above
>         --it turns out that I get the same results regardless of how I
> measure
>     --each measurement doubles the number of rows (from 100 to 200 to
> 400), which is what should be increasing to bring out rbind's allegedly bad
> behavior
>     --my machine has a Xeon E3 1245, 8 GB of RAM, running Win 7 Pro 64
> bit, using R 3.3.1 (latest release as of today)
>
> Given those unexpected results, you now understand the title of my post.
> So, has R's runtime somehow gotten greater intelligence recently so that it
> knows when it does not need to copy a data structure?  Maybe in the case
> above, it does a quick shallow copy instead of a deep copy?  Or, am I
> benchmarking wrong?
>
> What motivated my investigation is that I want to store a bunch of Monte
> Carlo simulation results in a numeric matrix.  When I am finished with my
> code, I know that it will be a massive matrix (e.g. 1,000,000 or more
> rows).  So I was concerned about performance, and went about benchmarking.
> Let me know if you want to see that benchmark code.  I found that
> assignment to a matrix does not seem to generate a copy, even tho
> assignment is a mutating operation, so I was worried that it might.  But
> when I investigated deeper, such as with the above code, I got worried that
> I cannot trust my results.
>
>
> I look forward to any feedback that you can give.
>
> I would especially appreciate any links that explain how you can determine
> what the R runtime actually does.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bob at rudis.net  Tue Jul 19 16:46:17 2016
From: bob at rudis.net (boB Rudis)
Date: Tue, 19 Jul 2016 10:46:17 -0400
Subject: [R] Has R recently made performance improvements in
	accumulation?
In-Reply-To: <CAJuCY5zy7TuTLZAajctNv1qny7QUnREaG-LpEL-6cNHnWkUzNw@mail.gmail.com>
References: <mailman.6.1468922401.11334.r-help@r-project.org>
	<843397525.1408616.1468935651020.JavaMail.yahoo@mail.yahoo.com>
	<CAJuCY5zy7TuTLZAajctNv1qny7QUnREaG-LpEL-6cNHnWkUzNw@mail.gmail.com>
Message-ID: <CAJ4QxaN92Rtd5UjQQGV5suPvN_TczCbTLaKqoy-LPRYQhT=reA@mail.gmail.com>

Ideally, you would use a more functional programming approach:

minimal <- function(rows, cols){
  x <- matrix(NA_integer_, ncol = cols, nrow = 0)
  for (i in seq_len(rows)){
    x <- rbind(x, rep(i, 10))
  }
  x
}

minimaly <- function(rows, cols){
  x <- matrix(NA_integer_, ncol = cols, nrow = 0)
  do.call(rbind, lapply(seq_len(rows), rep, cols))
}

identical(minimal(100, 100), minimaly(100, 100))
# [1] TRUE

microbenchmark(
  .for=minimal(100, 100),
  .lap=minimaly(100, 100)
)

## Unit: microseconds
##  expr     min        lq      mean   median       uq      max neval cld
## .for 943.936 1062.3710 1416.1399 1120.259 1366.860 4655.322   100   b
## .lap 111.566  118.1945  160.9058  124.520  146.991 2862.391   100  a

On Tue, Jul 19, 2016 at 10:27 AM, Thierry Onkelinx
<thierry.onkelinx at inbo.be> wrote:
> Dear Brent,
>
> I can confirm your timings with
>
> library(microbenchmark)
> microbenchmark(
>   mkFrameForLoop(100, 10),
>   mkFrameForLoop(200, 10),
>   mkFrameForLoop(400, 10)
> )
>
> but profiling your code shown that rbind only uses a small fraction on the
> cpu time used by the function.
>
> profvis::profvis({mkFrameForLoop(100, 10)})
>
> So I cleaned your example further into the function below. Now rbind is
> using most of cpu time. And the timings indicate an O(n^2) relation.
>
> minimal <- function(rows, cols){
>   x <- matrix(NA_integer_, ncol = cols, nrow = 0)
>   for (i in seq_len(rows)){
>     x <- rbind(x, rep(i, 10))
>   }
> }
>
> profvis::profvis({minimal(1000, 100)})
>
> timing <- microbenchmark(
>   X50 = minimal(50, 50),
>   X100 = minimal(100, 50),
>   X200 = minimal(200, 50),
>   X400 = minimal(400, 50),
>   X800 = minimal(800, 50),
>   X1600 = minimal(1600, 50)
> )
> timing
> Unit: microseconds
>   expr        min         lq        mean     median          uq        max
> neval cld
>    X50    199.006    212.278    233.8444    235.728    247.3770    296.987
>   100 a
>   X100    565.693    593.957    827.8733    618.835    640.1925   2950.139
>   100 a
>   X200   1804.059   1876.390   2166.1106   1903.370   1938.7115   4263.967
>   100 a
>   X400   6453.913   8755.848   8546.4339   8890.884   8961.7535  13259.024
>   100 a
>   X800  30575.048  32913.186  36555.0118  33093.243  34620.5895 178740.765
>   100  b
>  X1600 130976.429 133674.679 151494.6492 135197.087 137327.1235 292291.385
>   100   c
> timing$N <- as.integer(gsub("X", "", levels(timing$expr)))[timing$expr]
> model <- lm(time ~ poly(N, 4), data = timing)
> summary(model)
>
> Call:
> lm(formula = time ~ poly(N, 4), data = timing)
>
> Residuals:
>       Min        1Q    Median        3Q       Max
> -20518162  -3378940   -130815    -45881 142183951
>
> Coefficients:
>               Estimate Std. Error t value Pr(>|t|)
> (Intercept)   33303987     843350  39.490   <2e-16 ***
> poly(N, 4)1 1286962014   20657783  62.299   <2e-16 ***
> poly(N, 4)2  338770077   20657783  16.399   <2e-16 ***
> poly(N, 4)3     222734   20657783   0.011    0.991
> poly(N, 4)4   -2260902   20657783  -0.109    0.913
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Residual standard error: 20660000 on 595 degrees of freedom
> Multiple R-squared:  0.8746, Adjusted R-squared:  0.8738
> F-statistic:  1038 on 4 and 595 DF,  p-value: < 2.2e-16
> newdata <- data.frame(N = pretty(timing$N, 40))
> newdata$time <- predict(model, newdata = newdata)
> plot(newdata$N, newdata$time, type = "l")
> plot(newdata$N, sqrt(newdata$time), type = "l")
>
> model2 <- lm(sqrt(time) ~ poly(N, 4), data = timing)
> summary(model2)
> Call:
> lm(formula = sqrt(time) ~ poly(N, 4), data = timing)
>
> Residuals:
>    Min     1Q Median     3Q    Max
> -756.3 -202.8  -54.7   -5.5 7416.5
>
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept)   3980.36      33.13 120.160  < 2e-16 ***
> poly(N, 4)1 100395.40     811.41 123.730  < 2e-16 ***
> poly(N, 4)2   2191.34     811.41   2.701  0.00712 **
> poly(N, 4)3   -803.54     811.41  -0.990  0.32243
> poly(N, 4)4     82.09     811.41   0.101  0.91945
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Residual standard error: 811.4 on 595 degrees of freedom
> Multiple R-squared:  0.9626, Adjusted R-squared:  0.9624
> F-statistic:  3829 on 4 and 595 DF,  p-value: < 2.2e-16
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2016-07-19 15:40 GMT+02:00 Brent via R-help <r-help at r-project.org>:
>
>> Subtitle: or, more likely, am I benchmarking wrong?
>>
>> I am new to R, but I have read that R is a hive of performance pitfalls.
>> A very common case is if you try to accumulate results into a typical
>> immutable R data structure.
>>
>> Exhibit A for this confusion is this StackOverflow question on an
>> algorithm for O(1) performance in appending to a list:
>>
>> https://stackoverflow.com/questions/2436688/append-an-object-to-a-list-in-r-in-amortized-constant-time-o1
>> The original question was asked in 2010-03-13, and responses have trickled
>> in for at least the next 5 years.
>>
>> Before mentioning my problem, I instead offer an example from someone
>> vastly better than me in R, which I am sure should be free of mistakes.
>> Consider this interesting article on efficient accumulation in R:
>>     http://www.win-vector.com/blog/2015/07/efficient-accumulation-in-r/
>>
>> In that article's first example, it claims that the "obvious ?for-loop?
>> solution" (accumulation into a data.frame using rbind) is:
>>     is incredibly slow.
>>     ...
>>     we pay the cost of processing n*(n+1)/2 rows of data
>>
>> In other words, what looks like an O(n) algorithm is actually O(n^2).
>>
>> Sounds awful.  But when I tried executing their example on my machine, I
>> see O(n) NOT O(n^2) behavior!
>>
>> Here is the complete code that I executed:
>>
>>     mkFrameForLoop <- function(nRow,nCol) {
>>         d <- c()
>>         for(i in seq_len(nRow)) {
>>             ri <- mkRow(nCol)
>>             di <- data.frame(ri, stringsAsFactors=FALSE)
>>             d <- rbind(d,di)
>>         }
>>         d
>>     }
>>
>>     mkRow <- function(nCol) {
>>         x <- as.list(rnorm(nCol))
>>         # make row mixed types by changing first column to string
>>         x[[1]] <- ifelse(x[[1]]>0,'pos','neg')
>>         names(x) <- paste('x',seq_len(nCol),sep='.')
>>         x
>>     }
>>
>>     t1 = Sys.time()
>>     x = mkFrameForLoop(100, 10)
>>     tail(x)
>>     t2 = Sys.time()
>>     t2 - t1
>>
>>     t1 = Sys.time()
>>     x = mkFrameForLoop(200, 10)
>>     tail(x)
>>     t2 = Sys.time()
>>     t2 - t1
>>
>>     t1 = Sys.time()
>>     x = mkFrameForLoop(400, 10)
>>     tail(x)
>>     t2 = Sys.time()
>>     t2 - t1
>>
>> And here is what I got for the execution times:
>>
>>     Time difference of 0.113005876541138 secs
>>     Time difference of 0.205012083053589 secs
>>     Time difference of 0.390022993087769 secs
>>
>> That's a linear, not quadratic, increase in execution time as a function
>> of nRow!  It is NOT what I was expecting, altho it was nice to see.
>>
>> Notes:
>>     --the functions above are copy and pastes from the article
>>     --the execution time measurement code is all that I added
>>         --yes, that time measurement code is crude
>>         --I am aware of R benchmarking frameworks, in fact, I first tried
>> using some of them
>>         --but when I got unexpected results, I went back to the simplest
>> code possible, which is the above
>>         --it turns out that I get the same results regardless of how I
>> measure
>>     --each measurement doubles the number of rows (from 100 to 200 to
>> 400), which is what should be increasing to bring out rbind's allegedly bad
>> behavior
>>     --my machine has a Xeon E3 1245, 8 GB of RAM, running Win 7 Pro 64
>> bit, using R 3.3.1 (latest release as of today)
>>
>> Given those unexpected results, you now understand the title of my post.
>> So, has R's runtime somehow gotten greater intelligence recently so that it
>> knows when it does not need to copy a data structure?  Maybe in the case
>> above, it does a quick shallow copy instead of a deep copy?  Or, am I
>> benchmarking wrong?
>>
>> What motivated my investigation is that I want to store a bunch of Monte
>> Carlo simulation results in a numeric matrix.  When I am finished with my
>> code, I know that it will be a massive matrix (e.g. 1,000,000 or more
>> rows).  So I was concerned about performance, and went about benchmarking.
>> Let me know if you want to see that benchmark code.  I found that
>> assignment to a matrix does not seem to generate a copy, even tho
>> assignment is a mutating operation, so I was worried that it might.  But
>> when I investigated deeper, such as with the above code, I got worried that
>> I cannot trust my results.
>>
>>
>> I look forward to any feedback that you can give.
>>
>> I would especially appreciate any links that explain how you can determine
>> what the R runtime actually does.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From srivibish at gmail.com  Tue Jul 19 12:59:21 2016
From: srivibish at gmail.com (sri vathsan)
Date: Tue, 19 Jul 2016 16:29:21 +0530
Subject: [R] frequency of items
Message-ID: <CAGO7QoN_gBYLyTjtfjWzKUSOP=Z98TcNNCORoWvo+et8HunoNQ@mail.gmail.com>

Hi,

I have a data frame like below.
11,15,12,25
11,12
15,25
134,45,56
46
45,56
15,12
66,45,56,24,14,11,25,12,134

I want to identify the frequency of pairs/triplets or higher that occurs in
the data. Say for example, in above data the occurrence of pairs looks like
below

item     No of occurrence
11,12         3
11,25         2
15,12         2
15,25         2
.
.
45,56          3
134,45,56    2

 ....and so on

I am trying to write R code for the above and I am finding difficulty to
approach this. Looking forward some help.

Thanks!

-- 

Regards,
Srivathsan.K

	[[alternative HTML version deleted]]


From pmakananisa at sars.gov.za  Tue Jul 19 13:16:35 2016
From: pmakananisa at sars.gov.za (Mangalani Peter Makananisa)
Date: Tue, 19 Jul 2016 11:16:35 +0000
Subject: [R] Regression with ARMA residual
Message-ID: <BBB169C177D06B4E8E650D24548404CEC0B93F46@PTABREXC12M2.sars.gov.za>


Hi All,

From the library forecast I have fitted a regression model with ARMA residuals on a transformed variable diff(log(Y),1). 

What "code(s)" must I use to get the fitted and forecasted values on level values ( or original scale of Y) without doing my own manual manipulation?

Please advice

Kind regards

Peter
082 456 4669

-----Original Message-----
From: Ismail SEZEN [mailto:sezenismail at gmail.com]
Sent: 13 July 2016 04:55 PM
To: Mangalani Peter Makananisa
Cc: r-help at r-project.org
Subject: Re: [R] Dates in R (Year Month)

You can not convert numeric vectors directly to yearmon object. You must convert the X variable to character and add ?-? between year and month. Then as.yearmon function will work properly.

Please, read help pages of ?as.character, ?strptime and ?as.yearmon.

Example:

> library(zoo)
> aaa <- as.yearmon(c("2015-02?, ?2014-06"))
> class(aaa)
[1] "yearmon"

> On 13 Jul 2016, at 16:25, Mangalani Peter Makananisa <pmakananisa at sars.gov.za> wrote:
> 
> Hi All,
> 
> I am trying to convert the vector below to dates please assist I have tried to use information on the links you sent, but it is not working.
> 
> X  = c(201501, 201502, 201503, 201505, 201506, 201507, 201508, 201509, 
> 201510, 201511, 201512, 201601, 201602, 201603, 201604, 201605,
> 201606)
> 
> library(chron, zoo)
> Z = as.yearmon(X)  # it is not working
> 
> please assist
> 
> Kind regards
> Peter
> 
> Please Note: This email and its contents are subject to our email 
> legal notice which can be viewed at 
> http://www.sars.gov.za/Pages/Email-disclaimer.aspx
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Please Note: This email and its contents are subject to our email legal notice which can be viewed at http://www.sars.gov.za/Pages/Email-disclaimer.aspx

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Tue Jul 19 17:19:36 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 19 Jul 2016 16:19:36 +0100
Subject: [R] Missing rows anova
In-Reply-To: <CAEtAGep+YPEbnxEOV8rf1XZd-G+uENS=eQ+U2U1ykPtoMmm_bQ@mail.gmail.com>
References: <CAEtAGep+YPEbnxEOV8rf1XZd-G+uENS=eQ+U2U1ykPtoMmm_bQ@mail.gmail.com>
Message-ID: <cf2cf482-345a-2434-53e2-76c2db377b9d@dewey.myzen.co.uk>

Presumably it disappears because there is a unique value of ID for eac 
combination of S*x1 so they are indistinguishable.

On 19/07/2016 12:53, Justin Thong wrote:
> Why does the S:x1 column disappear (presumably S:x1 goes into ID but I dont
> know why)? S is a factor, x1 is a covariate and ID is a factor.
>
>     rich.side<-aov(y~S*x1+ID)
>     summary(rich.side)
>
> Below is the model frame
>
>     model.frame(~S*x1+ID)
>
>         S x1  ID
>     1   1 12   A
>     2   1 12   A
>     3   1 12   A
>     4   1 12   A
>     5   1  0   B
>     6   1  0   B
>     7   1  0   B
>     8   1  0   B
>     9   1  0   C
>     10  1  0   C
>     11  1  0   C
>     12  1  0   C
>     13  1  0   D
>     14  1  0   D
>     15  1  0   D
>     16  1  0   D
>     17  1  0   E
>     18  1  0   E
>     19  1  0   E
>     20  1  0   E
>     21  1  0   F
>     22  1  0   F
>     23  1  0   F
>     24  1  0   F
>     25  2  6  AB
>     26  2  6  AB
>     27  2  6  AB
>     28  2  6  AB
>     29  2  6  AC
>     30  2  6  AC
>     31  2  6  AC
>     32  2  6  AC
>     33  2  6  AD
>     34  2  6  AD
>     35  2  6  AD
>     36  2  6  AD
>     37  2  6  AE
>     38  2  6  AE
>     39  2  6  AE
>     40  2  6  AE
>     41  2  6  AF
>     42  2  6  AF
>     43  2  6  AF
>     44  2  6  AF
>     45  2  0  BC
>     46  2  0  BC
>     47  2  0  BC
>     48  2  0  BC
>     49  2  0  BD
>     50  2  0  BD
>     51  2  0  BD
>     52  2  0  BD
>     53  2  0  BE
>     54  2  0  BE
>     55  2  0  BE
>     56  2  0  BE
>     57  2  0  BF
>     58  2  0  BF
>     59  2  0  BF
>     60  2  0  BF
>     61  2  0  CD
>     62  2  0  CD
>     63  2  0  CD
>     64  2  0  CD
>     65  2  0  CE
>     66  2  0  CE
>     67  2  0  CE
>     68  2  0  CE
>     69  2  0  CF
>     70  2  0  CF
>     71  2  0  CF
>     72  2  0  CF
>     73  2  0  DE
>     74  2  0  DE
>     75  2  0  DE
>     76  2  0  DE
>     77  2  0  DF
>     78  2  0  DF
>     79  2  0  DF
>     80  2  0  DF
>     81  2  0  EF
>     82  2  0  EF
>     83  2  0  EF
>     84  2  0  EF
>     85  3  4 ABC
>     86  3  4 ABC
>     87  3  4 ABC
>     88  3  4 ABC
>     89  3  4 ABD
>     90  3  4 ABD
>     91  3  4 ABD
>     92  3  4 ABD
>     93  3  4 ABE
>     94  3  4 ABE
>     95  3  4 ABE
>     96  3  4 ABE
>     97  3  4 ABF
>     98  3  4 ABF
>     99  3  4 ABF
>     100 3  4 ABF
>     101 3  4 ACD
>     102 3  4 ACD
>     103 3  4 ACD
>     104 3  4 ACD
>     105 3  4 ACE
>     106 3  4 ACE
>     107 3  4 ACE
>     108 3  4 ACE
>     109 3  4 ACF
>     110 3  4 ACF
>     111 3  4 ACF
>     112 3  4 ACF
>     113 3  4 ADE
>     114 3  4 ADE
>     115 3  4 ADE
>     116 3  4 ADE
>     117 3  4 ADF
>     118 3  4 ADF
>     119 3  4 ADF
>     120 3  4 ADF
>     121 3  4 AEF
>     122 3  4 AEF
>     123 3  4 AEF
>     124 3  4 AEF
>     125 3  0 BCD
>     126 3  0 BCD
>     127 3  0 BCD
>     128 3  0 BCD
>     129 3  0 BCE
>     130 3  0 BCE
>     131 3  0 BCE
>     132 3  0 BCE
>     133 3  0 BCF
>     134 3  0 BCF
>     135 3  0 BCF
>     136 3  0 BCF
>     137 3  0 BDE
>     138 3  0 BDE
>     139 3  0 BDE
>     140 3  0 BDE
>     141 3  0 BDF
>     142 3  0 BDF
>     143 3  0 BDF
>     144 3  0 BDF
>     145 3  0 BEF
>     146 3  0 BEF
>     147 3  0 BEF
>     148 3  0 BEF
>     149 3  0 CDE
>     150 3  0 CDE
>     151 3  0 CDE
>     152 3  0 CDE
>     153 3  0 CDF
>     154 3  0 CDF
>     155 3  0 CDF
>     156 3  0 CDF
>     157 3  0 CEF
>     158 3  0 CEF
>     159 3  0 CEF
>     160 3  0 CEF
>     161 3  0 DEF
>     162 3  0 DEF
>     163 3  0 DEF
>     164 3  0 DEF
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From luca.cerone at gmail.com  Tue Jul 19 18:34:38 2016
From: luca.cerone at gmail.com (Luca Cerone)
Date: Tue, 19 Jul 2016 18:34:38 +0200
Subject: [R] Concatenate two lists replacing elements with the same name.
Message-ID: <CAFnz2--RTpYcjCLtx1ySC4nG1QRvkLs=rvkY4U6LbhsFt8nEBA@mail.gmail.com>

Dear all,
I would like to know if there is a function to concatenate two lists
while replacing elements with the same name.

For example:

x <- list(a=1,b=2,c=3)
y <- list( b=4, d=5)
z <- list(a = 6, b = 8, e= 7)

I am looking for a function "concatfun" so that

u <- concatfun(x,y,z)

returns:

u$a=6
u$b=8
u$c=3
u$d=5
u$e=7

I.e. it combines the 3 lists, but when names have the same value it
keeps the most recent one.

Does such a function exists?

Thanks for the help,

Cheers,
Luca


From wdunlap at tibco.com  Tue Jul 19 18:44:28 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 19 Jul 2016 09:44:28 -0700
Subject: [R] Concatenate two lists replacing elements with the same name.
In-Reply-To: <CAFnz2--RTpYcjCLtx1ySC4nG1QRvkLs=rvkY4U6LbhsFt8nEBA@mail.gmail.com>
References: <CAFnz2--RTpYcjCLtx1ySC4nG1QRvkLs=rvkY4U6LbhsFt8nEBA@mail.gmail.com>
Message-ID: <CAF8bMcbn_qHMqhM6tRwJFmCwdhzPv_0hLhk+N+iXHB6QKJ=r2w@mail.gmail.com>

concatfun <- function(...) {
   Reduce(f=function(a,b){ a[names(b)] <- b ; a },x=list(...), init=list())
}


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Jul 19, 2016 at 9:34 AM, Luca Cerone <luca.cerone at gmail.com> wrote:

> Dear all,
> I would like to know if there is a function to concatenate two lists
> while replacing elements with the same name.
>
> For example:
>
> x <- list(a=1,b=2,c=3)
> y <- list( b=4, d=5)
> z <- list(a = 6, b = 8, e= 7)
>
> I am looking for a function "concatfun" so that
>
> u <- concatfun(x,y,z)
>
> returns:
>
> u$a=6
> u$b=8
> u$c=3
> u$d=5
> u$e=7
>
> I.e. it combines the 3 lists, but when names have the same value it
> keeps the most recent one.
>
> Does such a function exists?
>
> Thanks for the help,
>
> Cheers,
> Luca
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ggrothendieck at gmail.com  Tue Jul 19 19:20:48 2016
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 19 Jul 2016 13:20:48 -0400
Subject: [R] Concatenate two lists replacing elements with the same name.
In-Reply-To: <CAFnz2--RTpYcjCLtx1ySC4nG1QRvkLs=rvkY4U6LbhsFt8nEBA@mail.gmail.com>
References: <CAFnz2--RTpYcjCLtx1ySC4nG1QRvkLs=rvkY4U6LbhsFt8nEBA@mail.gmail.com>
Message-ID: <CAP01uR=AnHTXhY4psSbi_ju38EH4nzoBzz5oE5RkiqAJniRGqw@mail.gmail.com>

Try this:

    Reduce(modifyList, list(x, y, z))

On Tue, Jul 19, 2016 at 12:34 PM, Luca Cerone <luca.cerone at gmail.com> wrote:
> Dear all,
> I would like to know if there is a function to concatenate two lists
> while replacing elements with the same name.
>
> For example:
>
> x <- list(a=1,b=2,c=3)
> y <- list( b=4, d=5)
> z <- list(a = 6, b = 8, e= 7)
>
> I am looking for a function "concatfun" so that
>
> u <- concatfun(x,y,z)
>
> returns:
>
> u$a=6
> u$b=8
> u$c=3
> u$d=5
> u$e=7
>
> I.e. it combines the 3 lists, but when names have the same value it
> keeps the most recent one.
>
> Does such a function exists?
>
> Thanks for the help,
>
> Cheers,
> Luca
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From ntfredo at gmail.com  Tue Jul 19 19:41:06 2016
From: ntfredo at gmail.com (Frederic Ntirenganya)
Date: Tue, 19 Jul 2016 20:41:06 +0300
Subject: [R] Sampe numbers
Message-ID: <CAGh51gQx3PnOJq4ELVeMGVvg_Yq537=QO2LgY4HZFDdfpKpyJQ@mail.gmail.com>

Hi Guys,
I am trying to sample 100 numbers from 1:100
i did it  like this:

sample(1:100,100, replace= TRUE)
but i want again include a constraint that their sum must be equal to 50

How could I do it?

Cheers

Frederic Ntirenganya
Maseno University,
African Maths Initiative,
Kenya.
Mobile:(+254)718492836
Email: fredo at aims.ac.za
https://sites.google.com/a/aims.ac.za/fredo/

	[[alternative HTML version deleted]]


From nutnutnutterson at gmail.com  Tue Jul 19 19:29:37 2016
From: nutnutnutterson at gmail.com (michael young)
Date: Tue, 19 Jul 2016 10:29:37 -0700
Subject: [R] pairs: adjusting margins and labeling axes
Message-ID: <CAGk=u4XugCx1JOXSaAck=DDh=cVouQaea6b59Jex45UJp8oUUw@mail.gmail.com>

The default shape for this correlation scatterplot is rectangle.  I changed
it to square, but then the x-axis spacing between squares are off.  Is
there an easy way to change x-axis spacing between squares to that of the
y-axis spacing size?

I decided to hide the name values of the diagonal squares.  I want them
along the x and y axis instead, outside of the fixed number scale I have.
I haven't seen any online example of 'pairs' with this and all my searches
have yielded nothing.  Any ideas?  Thanks

par(pty="s")
panel.cor <- function(x, y, digits = 2, prefix="", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1),xlog=FALSE,ylog=FALSE)
    # correlation coefficient
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste("r= ", txt, sep = "")
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.6, txt, cex=cex.cor * r)

    # p-value calculation
    p <- cor.test(x, y)$p.value
    txt2 <- format(c(p, 0.123456789), digits = digits)[1]
    txt2 <- paste("p= ", txt2, sep = "")
    if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
    text(0.5, 0.4, txt2)
}

pairs(iris, upper.panel = panel.cor,xlim=c(0.1,100000),
ylim=c(0.1,100000),log="xy",text.panel = NULL,pch=".")

	[[alternative HTML version deleted]]


From 538280 at gmail.com  Tue Jul 19 20:05:57 2016
From: 538280 at gmail.com (Greg Snow)
Date: Tue, 19 Jul 2016 12:05:57 -0600
Subject: [R] Sampe numbers
In-Reply-To: <CAGh51gQx3PnOJq4ELVeMGVvg_Yq537=QO2LgY4HZFDdfpKpyJQ@mail.gmail.com>
References: <CAGh51gQx3PnOJq4ELVeMGVvg_Yq537=QO2LgY4HZFDdfpKpyJQ@mail.gmail.com>
Message-ID: <CAFEqCdyDASh2zCOb4j7DkKUXx_PLnt_0hPrawqS8Gfvyi4Lwww@mail.gmail.com>

I think that you need to reconsider your conditions.

The smallest number in your candidate set is 1, so if you sample 100
1's they will add to 100 which is greater than 50.  So to have a set
of numbers that sums to 50 you will need to either include negative
numbers, 0's, or sample fewer than 50 values.

If you really meant sum to 500 or sample 10 numbers (still selecting
any single number over 50 will make the sum to large) then this
becomes possible.  There are a few ways to do it, which is best
depends on what you are really trying to accomplish (and note that
with the constraint, the values will not be iid).

One option is rejection sampling:  take a sample, if it sums correctly
then you are done, if not then throw out that sample and try a new
one.  This will be very inefficient, but is easy enough to code and
with a fast enough computer may be acceptable.

Another option is adjustment:  take a sample and compute the sum, if
it is to low then add the difference to one of your values (lowest,
chosen at random, other) or split it up and add to multiple values.
If the sum is too high, then subtract from your numbers until the sum
is correct (checking that you don't go below any lower bounds).

Another option is to round values generated from a Dirichlet
distribution multiplied by your total (you may need to round one value
differently than the default).

Another option is to consider it a balls and urns problem:  You have
500 balls (assuming the true sum is to be 500) that you want
distributed into 100 urns.  If each urn needs at least 1 ball (minimum
value of 1) then put 1 ball in each urn, then for the remaining 400
balls, randomly choose an urn to put each into.  Then count the number
of balls in each urn and those are your 100 numbers that sum to 500
(as long as no balls bounced off the lip of the urn and rolled under
the couch).

There are probably other ways, but these are some things to get you started.

On Tue, Jul 19, 2016 at 11:41 AM, Frederic Ntirenganya
<ntfredo at gmail.com> wrote:
> Hi Guys,
> I am trying to sample 100 numbers from 1:100
> i did it  like this:
>
> sample(1:100,100, replace= TRUE)
> but i want again include a constraint that their sum must be equal to 50
>
> How could I do it?
>
> Cheers
>
> Frederic Ntirenganya
> Maseno University,
> African Maths Initiative,
> Kenya.
> Mobile:(+254)718492836
> Email: fredo at aims.ac.za
> https://sites.google.com/a/aims.ac.za/fredo/
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From JLucke at ria.buffalo.edu  Tue Jul 19 20:11:31 2016
From: JLucke at ria.buffalo.edu (JLucke at ria.buffalo.edu)
Date: Tue, 19 Jul 2016 14:11:31 -0400
Subject: [R] Sampe numbers
In-Reply-To: <CAGh51gQx3PnOJq4ELVeMGVvg_Yq537=QO2LgY4HZFDdfpKpyJQ@mail.gmail.com>
References: <CAGh51gQx3PnOJq4ELVeMGVvg_Yq537=QO2LgY4HZFDdfpKpyJQ@mail.gmail.com>
Message-ID: <OFB16360F6.084B3918-ON85257FF5.00636B09-85257FF5.0063EEB1@ria.buffalo.edu>

N <- 100
C <- 50
x <- numeric(N)
for (i in 1:N){
  x[i] <- sample(C-sum(x),1)
}
x
sum(x)





Frederic Ntirenganya <ntfredo at gmail.com> 
Sent by: "R-help" <r-help-bounces at r-project.org>
07/19/2016 01:41 PM

To
"r-help at r-project.org" <r-help at r-project.org>, 
cc

Subject
[R] Sampe numbers






Hi Guys,
I am trying to sample 100 numbers from 1:100
i did it  like this:

sample(1:100,100, replace= TRUE)
but i want again include a constraint that their sum must be equal to 50

How could I do it?

Cheers

Frederic Ntirenganya
Maseno University,
African Maths Initiative,
Kenya.
Mobile:(+254)718492836
Email: fredo at aims.ac.za
https://sites.google.com/a/aims.ac.za/fredo/

                 [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From 538280 at gmail.com  Tue Jul 19 20:16:14 2016
From: 538280 at gmail.com (Greg Snow)
Date: Tue, 19 Jul 2016 12:16:14 -0600
Subject: [R] pairs: adjusting margins and labeling axes
In-Reply-To: <CAGk=u4XugCx1JOXSaAck=DDh=cVouQaea6b59Jex45UJp8oUUw@mail.gmail.com>
References: <CAGk=u4XugCx1JOXSaAck=DDh=cVouQaea6b59Jex45UJp8oUUw@mail.gmail.com>
Message-ID: <CAFEqCdxCBqg-xjOjj0bshspnSPCok9fCjpGrmPdq1hOPQP4KiA@mail.gmail.com>

If you want square plots on a rectangular plotting region, then where
do you want the extra space to go?

One option would be to add outer margins to use up the extra space.
The calculations to figure out exactly how much space to put in the
outer margins will probably not be trivial.

Another option would be to not use `pairs`, but use the `layout`
function directly and loops to do your plots (and use the `respect`
argument to `layout`).

On Tue, Jul 19, 2016 at 11:29 AM, michael young
<nutnutnutterson at gmail.com> wrote:
> The default shape for this correlation scatterplot is rectangle.  I changed
> it to square, but then the x-axis spacing between squares are off.  Is
> there an easy way to change x-axis spacing between squares to that of the
> y-axis spacing size?
>
> I decided to hide the name values of the diagonal squares.  I want them
> along the x and y axis instead, outside of the fixed number scale I have.
> I haven't seen any online example of 'pairs' with this and all my searches
> have yielded nothing.  Any ideas?  Thanks
>
> par(pty="s")
> panel.cor <- function(x, y, digits = 2, prefix="", cex.cor, ...)
> {
>     usr <- par("usr"); on.exit(par(usr))
>     par(usr = c(0, 1, 0, 1),xlog=FALSE,ylog=FALSE)
>     # correlation coefficient
>     r <- cor(x, y)
>     txt <- format(c(r, 0.123456789), digits = digits)[1]
>     txt <- paste("r= ", txt, sep = "")
>     if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
>     text(0.5, 0.6, txt, cex=cex.cor * r)
>
>     # p-value calculation
>     p <- cor.test(x, y)$p.value
>     txt2 <- format(c(p, 0.123456789), digits = digits)[1]
>     txt2 <- paste("p= ", txt2, sep = "")
>     if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
>     text(0.5, 0.4, txt2)
> }
>
> pairs(iris, upper.panel = panel.cor,xlim=c(0.1,100000),
> ylim=c(0.1,100000),log="xy",text.panel = NULL,pch=".")
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From johnwasige at gmail.com  Tue Jul 19 20:24:41 2016
From: johnwasige at gmail.com (John Wasige)
Date: Tue, 19 Jul 2016 20:24:41 +0200
Subject: [R] multiple-line plot
Message-ID: <CAJgdCD7Ad6-K3+iud9aqMSd9L=sW7N4t-e9zCZ-HKisjrB3iyQ@mail.gmail.com>

?Dear all,

This is to kindly request for your help. I would like to plot my data.

The R script below gives some plot that is not clear. How can I get a clear
multiple-line plot. The data is attached herewith.

##R Script
for_jhon = read.csv("C:/LVM_share/for_ jhon.csv", header=TRUE, sep=";")
matplot(for_jhon$ID, cbind(for_jhon[,2:73]))?

Thanks for your help

John

From 538280 at gmail.com  Tue Jul 19 20:46:45 2016
From: 538280 at gmail.com (Greg Snow)
Date: Tue, 19 Jul 2016 12:46:45 -0600
Subject: [R] multiple-line plot
In-Reply-To: <CAJgdCD7Ad6-K3+iud9aqMSd9L=sW7N4t-e9zCZ-HKisjrB3iyQ@mail.gmail.com>
References: <CAJgdCD7Ad6-K3+iud9aqMSd9L=sW7N4t-e9zCZ-HKisjrB3iyQ@mail.gmail.com>
Message-ID: <CAFEqCdxswaUQEbVBqpoXTFficqhjh00HcBuEPTbRW2NMok8SrA@mail.gmail.com>

Most attachments get stripped off, so your data did not make it through.

But try:

matplot(for_jhon$ID, for_jhon[,2:73], type='l')


On Tue, Jul 19, 2016 at 12:24 PM, John Wasige <johnwasige at gmail.com> wrote:
> Dear all,
>
> This is to kindly request for your help. I would like to plot my data.
>
> The R script below gives some plot that is not clear. How can I get a clear
> multiple-line plot. The data is attached herewith.
>
> ##R Script
> for_jhon = read.csv("C:/LVM_share/for_ jhon.csv", header=TRUE, sep=";")
> matplot(for_jhon$ID, cbind(for_jhon[,2:73]))
>
> Thanks for your help
>
> John
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From ntfredo at gmail.com  Tue Jul 19 20:54:57 2016
From: ntfredo at gmail.com (Frederic Ntirenganya)
Date: Tue, 19 Jul 2016 21:54:57 +0300
Subject: [R] Sampe numbers
In-Reply-To: <OFB16360F6.084B3918-ON85257FF5.00636B09-85257FF5.0063EEB1@ria.buffalo.edu>
References: <CAGh51gQx3PnOJq4ELVeMGVvg_Yq537=QO2LgY4HZFDdfpKpyJQ@mail.gmail.com>
	<OFB16360F6.084B3918-ON85257FF5.00636B09-85257FF5.0063EEB1@ria.buffalo.edu>
Message-ID: <CAGh51gS6qBJhnR1=0Xk5b3h3C4_OfXp04v2JP7hdtJYyuqksOA@mail.gmail.com>

The problem is that there
are some zeros  while the numbers should range from 1 to  100

As the replace= TRUE if the smallest candidate is 1 may be replaced
because if I can have 1, fifty times this is equal to 50.
Remember that it is a sampling with a replacement

I am still wondering how I can remove the zero value.


Frederic Ntirenganya
Maseno University,
African Maths Initiative,
Kenya.
Mobile:(+254)718492836
Email: fredo at aims.ac.za
https://sites.google.com/a/aims.ac.za/fredo/

On Tue, Jul 19, 2016 at 9:11 PM, <JLucke at ria.buffalo.edu> wrote:

> N <- 100
> C <- 50
> x <- numeric(N)
> for (i in 1:N){
>   x[i] <- sample(C-sum(x),1)
> }
> x
> sum(x)
>
>
>
>
> *Frederic Ntirenganya <ntfredo at gmail.com <ntfredo at gmail.com>>*
> Sent by: "R-help" <r-help-bounces at r-project.org>
>
> 07/19/2016 01:41 PM
> To
> "r-help at r-project.org" <r-help at r-project.org>,
>
> cc
> Subject
> [R] Sampe numbers
>
>
>
>
> Hi Guys,
> I am trying to sample 100 numbers from 1:100
> i did it  like this:
>
> sample(1:100,100, replace= TRUE)
> but i want again include a constraint that their sum must be equal to 50
>
> How could I do it?
>
> Cheers
>
> Frederic Ntirenganya
> Maseno University,
> African Maths Initiative,
> Kenya.
> Mobile:(+254)718492836
> Email: fredo at aims.ac.za
> https://sites.google.com/a/aims.ac.za/fredo/
>
>
>                 [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From johnwasige at gmail.com  Tue Jul 19 21:36:46 2016
From: johnwasige at gmail.com (John Wasige)
Date: Tue, 19 Jul 2016 21:36:46 +0200
Subject: [R] Label multiple-line plot
Message-ID: <CAJgdCD64C3j0CX_WC6M-yZD4A6We6RhaGgZ+mOD2qSXeYnQzfA@mail.gmail.com>

Thanks! It worked  for me.
?matplot(for_jhon$ID, for_jhon[,2:73], type='l')

Any I dea on how I can label multiple-line plot based on column names?

Thanks for your help

John

On Tue, Jul 19, 2016 at 8:46 PM, Greg Snow <538280 at gmail.com> wrote:

> Most attachments get stripped off, so your data did not make it through.
>
> But try:
>
> ??
> matplot(for_jhon$ID, for_jhon[,2:73], type='l')
>
>
> On Tue, Jul 19, 2016 at 12:24 PM, John Wasige <johnwasige at gmail.com>
> wrote:
> > Dear all,
> >
> > This is to kindly request for your help. I would like to plot my data.
> >
> > The R script below gives some plot that is not clear. How can I get a
> clear
> > multiple-line plot. The data is attached herewith.
> >
> > ##R Script
> > for_jhon = read.csv("C:/LVM_share/for_ jhon.csv", header=TRUE, sep=";")
> > matplot(for_jhon$ID, cbind(for_jhon[,2:73]))
> >
> > Thanks for your help
> >
> > John
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>



-- 
John Wasige
"There are no REGRATES in LIFE, just lessons (Jennifer Aniston)?

	[[alternative HTML version deleted]]


From 538280 at gmail.com  Tue Jul 19 21:56:23 2016
From: 538280 at gmail.com (Greg Snow)
Date: Tue, 19 Jul 2016 13:56:23 -0600
Subject: [R] Label multiple-line plot
In-Reply-To: <CAJgdCD64C3j0CX_WC6M-yZD4A6We6RhaGgZ+mOD2qSXeYnQzfA@mail.gmail.com>
References: <CAJgdCD64C3j0CX_WC6M-yZD4A6We6RhaGgZ+mOD2qSXeYnQzfA@mail.gmail.com>
Message-ID: <CAFEqCdz5=yAnfr12bXgxQf-PF4-uUP4FcLuEX=zRedms-zWmag@mail.gmail.com>

with 72 lines to label it will be crowded whatever you do.

Here is one option (though I am showing with fewer lines):

x <- sapply(1:15, function(i) cumsum(rnorm(100)))
par(mar=c(5,4,4,3)+0.1)
matplot(1:100, x, type='l', lty=1)
mtext(LETTERS[1:15], side=4, at=x[100,], las=1, line=1)

One way to spread out the labels is:

library(TeachingDemos)
new.y <- spread.labs(x[100,], mindiff = 1.2*strheight('A'),
                     min=min(x), max=max(x))
mtext(LETTERS[1:15], side=4, at=new.y, las=1, line=2)


There is also another label spreading function in the plotrix package
and another option for finding space in a plot for labeling lines in
either the rms or Hmisc package.  There are probably other packages
with tools available now as well, but those are the ones that I am
familiar with.



On Tue, Jul 19, 2016 at 1:36 PM, John Wasige <johnwasige at gmail.com> wrote:
> Thanks! It worked  for me.
> matplot(for_jhon$ID, for_jhon[,2:73], type='l')
>
> Any I dea on how I can label multiple-line plot based on column names?
>
> Thanks for your help
>
> John
>
> On Tue, Jul 19, 2016 at 8:46 PM, Greg Snow <538280 at gmail.com> wrote:
>>
>> Most attachments get stripped off, so your data did not make it through.
>>
>> But try:
>>
>> matplot(for_jhon$ID, for_jhon[,2:73], type='l')
>>
>>
>> On Tue, Jul 19, 2016 at 12:24 PM, John Wasige <johnwasige at gmail.com>
>> wrote:
>> > Dear all,
>> >
>> > This is to kindly request for your help. I would like to plot my data.
>> >
>> > The R script below gives some plot that is not clear. How can I get a
>> > clear
>> > multiple-line plot. The data is attached herewith.
>> >
>> > ##R Script
>> > for_jhon = read.csv("C:/LVM_share/for_ jhon.csv", header=TRUE, sep=";")
>> > matplot(for_jhon$ID, cbind(for_jhon[,2:73]))
>> >
>> > Thanks for your help
>> >
>> > John
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Gregory (Greg) L. Snow Ph.D.
>> 538280 at gmail.com
>
>
>
>
> --
> John Wasige
> "There are no REGRATES in LIFE, just lessons (Jennifer Aniston)?



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From syen04 at gmail.com  Tue Jul 19 22:15:58 2016
From: syen04 at gmail.com (Steven Yen)
Date: Tue, 19 Jul 2016 16:15:58 -0400
Subject: [R] Build command in library(devtools)
Message-ID: <291e222a-a541-9b79-459c-bcdcd7f8e66b@gmail.com>

I recently updated my R and RStudio to the latest version and now the 
binary option in the "build" command in devtools stops working.

I went around and used the binary=F option which worked by I get the 
.tar.gz file instead of the .zip file which I prefer.

Does anyone understand the following error message:

status 127
running 'zip' failed

===
setwd("A:/R/yenlib/"); library(devtools)
#build("yenlib",binary=T) # Thisfailed with an error message
build("yenlib",binary=F) # This works

 > build("yenlib",binary=T)
"C:/PROGRA~1/R/R-33~1.1/bin/x64/R" --no-site-file  \
   --no-environ --no-save --no-restore --quiet CMD INSTALL \
   "A:\R\yenlib\yenlib" --build

* installing to library 
'C:/Users/syen01/AppData/Local/Temp/Rtmp8A7KEw/temp_libpath4074149a528e'
* installing *source* package 'yenlib' ...
** R
** data
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
*** arch - i386
*** arch - x64
* MD5 sums
Warning: running command '"zip" -r9Xq "A:/R/yenlib/yenlib_16.3.zip" 
yenlib' had status 127
running 'zip' failed
* DONE (yenlib)
[1] "A:/R/yenlib/yenlib_16.3.zip"
 >


	[[alternative HTML version deleted]]


From johnwasige at gmail.com  Tue Jul 19 22:36:58 2016
From: johnwasige at gmail.com (John Wasige)
Date: Tue, 19 Jul 2016 22:36:58 +0200
Subject: [R] Label multiple-line plot
In-Reply-To: <CAFEqCdz5=yAnfr12bXgxQf-PF4-uUP4FcLuEX=zRedms-zWmag@mail.gmail.com>
References: <CAJgdCD64C3j0CX_WC6M-yZD4A6We6RhaGgZ+mOD2qSXeYnQzfA@mail.gmail.com>
	<CAFEqCdz5=yAnfr12bXgxQf-PF4-uUP4FcLuEX=zRedms-zWmag@mail.gmail.com>
Message-ID: <CAJgdCD70H-c6ZrBRPZJS_Bkz=vad5cnL0KgaDMieBPweDD52qg@mail.gmail.com>

Thanks so much
Best Rgds
John

On Tue, Jul 19, 2016 at 9:56 PM, Greg Snow <538280 at gmail.com> wrote:

> with 72 lines to label it will be crowded whatever you do.
>
> Here is one option (though I am showing with fewer lines):
>
> x <- sapply(1:15, function(i) cumsum(rnorm(100)))
> par(mar=c(5,4,4,3)+0.1)
> matplot(1:100, x, type='l', lty=1)
> mtext(LETTERS[1:15], side=4, at=x[100,], las=1, line=1)
>
> One way to spread out the labels is:
>
> library(TeachingDemos)
> new.y <- spread.labs(x[100,], mindiff = 1.2*strheight('A'),
>                      min=min(x), max=max(x))
> mtext(LETTERS[1:15], side=4, at=new.y, las=1, line=2)
>
>
> There is also another label spreading function in the plotrix package
> and another option for finding space in a plot for labeling lines in
> either the rms or Hmisc package.  There are probably other packages
> with tools available now as well, but those are the ones that I am
> familiar with.
>
>
>
> On Tue, Jul 19, 2016 at 1:36 PM, John Wasige <johnwasige at gmail.com> wrote:
> > Thanks! It worked  for me.
> > matplot(for_jhon$ID, for_jhon[,2:73], type='l')
> >
> > Any I dea on how I can label multiple-line plot based on column names?
> >
> > Thanks for your help
> >
> > John
> >
> > On Tue, Jul 19, 2016 at 8:46 PM, Greg Snow <538280 at gmail.com> wrote:
> >>
> >> Most attachments get stripped off, so your data did not make it through.
> >>
> >> But try:
> >>
> >> matplot(for_jhon$ID, for_jhon[,2:73], type='l')
> >>
> >>
> >> On Tue, Jul 19, 2016 at 12:24 PM, John Wasige <johnwasige at gmail.com>
> >> wrote:
> >> > Dear all,
> >> >
> >> > This is to kindly request for your help. I would like to plot my data.
> >> >
> >> > The R script below gives some plot that is not clear. How can I get a
> >> > clear
> >> > multiple-line plot. The data is attached herewith.
> >> >
> >> > ##R Script
> >> > for_jhon = read.csv("C:/LVM_share/for_ jhon.csv", header=TRUE,
> sep=";")
> >> > matplot(for_jhon$ID, cbind(for_jhon[,2:73]))
> >> >
> >> > Thanks for your help
> >> >
> >> > John
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> > http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >> --
> >> Gregory (Greg) L. Snow Ph.D.
> >> 538280 at gmail.com
> >
> >
> >
> >
> > --
> > John Wasige
> > "There are no REGRATES in LIFE, just lessons (Jennifer Aniston)?
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>



-- 
John Wasige
"There are no REGRATES in LIFE, just lessons (Jennifer Aniston)?

	[[alternative HTML version deleted]]


From john.archie.mckown at gmail.com  Tue Jul 19 22:38:31 2016
From: john.archie.mckown at gmail.com (John McKown)
Date: Tue, 19 Jul 2016 15:38:31 -0500
Subject: [R] Build command in library(devtools)
In-Reply-To: <291e222a-a541-9b79-459c-bcdcd7f8e66b@gmail.com>
References: <291e222a-a541-9b79-459c-bcdcd7f8e66b@gmail.com>
Message-ID: <CAAJSdjhC6V-RE6pSg41+1o6LKiPibWreszXQLAQiC1Qfy2DWsQ@mail.gmail.com>

On Tue, Jul 19, 2016 at 3:15 PM, Steven Yen <syen04 at gmail.com> wrote:

> I recently updated my R and RStudio to the latest version and now the
> binary option in the "build" command in devtools stops working.
>
> I went around and used the binary=F option which worked by I get the
> .tar.gz file instead of the .zip file which I prefer.
>
> Does anyone understand the following error message:
>
> status 127
> running 'zip' failed
>

?I'm not totally sure, but I think that means that R cannot find the "zip"
program in order to run it. ?



>
> ===
> setwd("A:/R/yenlib/"); library(devtools)
> #build("yenlib",binary=T) # Thisfailed with an error message
> build("yenlib",binary=F) # This works
>
>  > build("yenlib",binary=T)
> "C:/PROGRA~1/R/R-33~1.1/bin/x64/R" --no-site-file  \
>    --no-environ --no-save --no-restore --quiet CMD INSTALL \
>    "A:\R\yenlib\yenlib" --build
>
> * installing to library
> 'C:/Users/syen01/AppData/Local/Temp/Rtmp8A7KEw/temp_libpath4074149a528e'
> * installing *source* package 'yenlib' ...
> ** R
> ** data
> ** preparing package for lazy loading
> ** help
> *** installing help indices
> ** building package indices
> ** testing if installed package can be loaded
> *** arch - i386
> *** arch - x64
> * MD5 sums
> Warning: running command '"zip" -r9Xq "A:/R/yenlib/yenlib_16.3.zip"
> yenlib' had status 127
> running 'zip' failed
> * DONE (yenlib)
> [1] "A:/R/yenlib/yenlib_16.3.zip"
>  >
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
"Worry was nothing more than paying interest on a loan that a man may never
borrow"

From: "Quest for the White Wind" by Alan Black

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From djnordlund at gmail.com  Tue Jul 19 22:47:39 2016
From: djnordlund at gmail.com (Daniel Nordlund)
Date: Tue, 19 Jul 2016 13:47:39 -0700
Subject: [R] Sampe numbers
In-Reply-To: <CAGh51gS6qBJhnR1=0Xk5b3h3C4_OfXp04v2JP7hdtJYyuqksOA@mail.gmail.com>
References: <CAGh51gQx3PnOJq4ELVeMGVvg_Yq537=QO2LgY4HZFDdfpKpyJQ@mail.gmail.com>
	<OFB16360F6.084B3918-ON85257FF5.00636B09-85257FF5.0063EEB1@ria.buffalo.edu>
	<CAGh51gS6qBJhnR1=0Xk5b3h3C4_OfXp04v2JP7hdtJYyuqksOA@mail.gmail.com>
Message-ID: <6ca36d25-f4cb-cb53-1b83-0d005eb2272a@gmail.com>

Frederic,

you need to be more clear about what you want to do.  If you are 
sampling 100 numbers from 1:100 with replacement, the sum will always be 
greater than 50 (as has already been pointed out).

In addition, you mention trying to eliminate zeros.  If you are sampling 
from 1:100 there will be no zeros.  So what are you really trying to do?


Dan

Dan Nordlund
Port Townsend, WA  USA


On 7/19/2016 11:54 AM, Frederic Ntirenganya wrote:
> The problem is that there
> are some zeros  while the numbers should range from 1 to  100
>
> As the replace= TRUE if the smallest candidate is 1 may be replaced
> because if I can have 1, fifty times this is equal to 50.
> Remember that it is a sampling with a replacement
>
> I am still wondering how I can remove the zero value.
>
>
> Frederic Ntirenganya
> Maseno University,
> African Maths Initiative,
> Kenya.
> Mobile:(+254)718492836
> Email: fredo at aims.ac.za
> https://sites.google.com/a/aims.ac.za/fredo/
>
> On Tue, Jul 19, 2016 at 9:11 PM, <JLucke at ria.buffalo.edu> wrote:
>
>> N <- 100
>> C <- 50
>> x <- numeric(N)
>> for (i in 1:N){
>>   x[i] <- sample(C-sum(x),1)
>> }
>> x
>> sum(x)
>>
>>
>>
>>
>> *Frederic Ntirenganya <ntfredo at gmail.com <ntfredo at gmail.com>>*
>> Sent by: "R-help" <r-help-bounces at r-project.org>
>>
>> 07/19/2016 01:41 PM
>> To
>> "r-help at r-project.org" <r-help at r-project.org>,
>>
>> cc
>> Subject
>> [R] Sampe numbers
>>
>>
>>
>>
>> Hi Guys,
>> I am trying to sample 100 numbers from 1:100
>> i did it  like this:
>>
>> sample(1:100,100, replace= TRUE)
>> but i want again include a constraint that their sum must be equal to 50
>>
>> How could I do it?
>>
>> Cheers
>>
>> Frederic Ntirenganya
>> Maseno University,
>> African Maths Initiative,
>> Kenya.
>> Mobile:(+254)718492836
>> Email: fredo at aims.ac.za
>> https://sites.google.com/a/aims.ac.za/fredo/
>>
>>
>>                 [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> <http://www.r-project.org/posting-guide.html>
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Daniel Noredlund
Bothell, WA USA


From dulcalma at bigpond.com  Wed Jul 20 03:43:02 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Wed, 20 Jul 2016 11:43:02 +1000
Subject: [R] pairs: adjusting margins and labeling axes
In-Reply-To: <CAGk=u4XugCx1JOXSaAck=DDh=cVouQaea6b59Jex45UJp8oUUw@mail.gmail.com>
References: <CAGk=u4XugCx1JOXSaAck=DDh=cVouQaea6b59Jex45UJp8oUUw@mail.gmail.com>
Message-ID: <000301d1e228$0deefb70$29ccf250$@bigpond.com>

Hi
Will doing in lattice suite

>From  https://stat.ethz.ch/pipermail/r-help/2007-October/142116.html and
https://stat.ethz.ch/pipermail/r-help/2007-October/142124.html]
This is a direct cut and paste from the last url to give you an idea of
Deepayan Sarkar's script

library(lattice)

panel.corval2 <- function(x, y, digits=2, prefix="", cex.cor, ...)
{
    require(grid)
    r <- abs(cor(x, y, use = "complete.obs"))
    txt <- format(c(r, 0.123456789), digits=digits)[1]
    txt <- paste(prefix, txt, sep="")
    if (missing(cex.cor)) cex.cor <- 10 / nchar(txt)
    grid.text(txt, 0.5, 0.5, gp = gpar(cex = cex.cor))
}

splom(iris[1:4], groups = iris$Species, pch = 16,
      lower.panel = function(...) {
          panel.xyplot(...)
          panel.loess(..., col = 1, lwd = 3)
      },
      upper.panel = panel.corval2)


Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of michael
young
Sent: Wednesday, 20 July 2016 03:30
To: r-help at r-project.org
Subject: [R] pairs: adjusting margins and labeling axes

The default shape for this correlation scatterplot is rectangle.  I changed
it to square, but then the x-axis spacing between squares are off.  Is
there an easy way to change x-axis spacing between squares to that of the
y-axis spacing size?

I decided to hide the name values of the diagonal squares.  I want them
along the x and y axis instead, outside of the fixed number scale I have.
I haven't seen any online example of 'pairs' with this and all my searches
have yielded nothing.  Any ideas?  Thanks

par(pty="s")
panel.cor <- function(x, y, digits = 2, prefix="", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1),xlog=FALSE,ylog=FALSE)
    # correlation coefficient
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste("r= ", txt, sep = "")
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.6, txt, cex=cex.cor * r)

    # p-value calculation
    p <- cor.test(x, y)$p.value
    txt2 <- format(c(p, 0.123456789), digits = digits)[1]
    txt2 <- paste("p= ", txt2, sep = "")
    if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
    text(0.5, 0.4, txt2)
}

pairs(iris, upper.panel = panel.cor,xlim=c(0.1,100000),
ylim=c(0.1,100000),log="xy",text.panel = NULL,pch=".")

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From syen04 at gmail.com  Wed Jul 20 04:06:38 2016
From: syen04 at gmail.com (Steven Yen)
Date: Tue, 19 Jul 2016 22:06:38 -0400
Subject: [R] Build command in library(devtools)
In-Reply-To: <CAAJSdjhC6V-RE6pSg41+1o6LKiPibWreszXQLAQiC1Qfy2DWsQ@mail.gmail.com>
References: <291e222a-a541-9b79-459c-bcdcd7f8e66b@gmail.com>
	<CAAJSdjhC6V-RE6pSg41+1o6LKiPibWreszXQLAQiC1Qfy2DWsQ@mail.gmail.com>
Message-ID: <196aed4a-f7a8-f898-bff4-84c548119d67@gmail.com>

Thanks. I found the reason was Rtools does not run under the new version 
of R. I had to go back to as early as R 3.0.2 (September 2013) to make 
Rtools work.
Any idea for a go-around? Thanks.

On 7/19/2016 4:38 PM, John McKown wrote:
> On Tue, Jul 19, 2016 at 3:15 PM, Steven Yen <syen04 at gmail.com 
> <mailto:syen04 at gmail.com>>wrote:
>
>     I recently updated my R and RStudio to the latest version and now the
>     binary option in the "build" command in devtools stops working.
>
>     I went around and used the binary=F option which worked by I get the
>     .tar.gz file instead of the .zip file which I prefer.
>
>     Does anyone understand the following error message:
>
>     status 127
>     running 'zip' failed
>
>
> ?I'm not totally sure, but I think that means that R cannot find the 
> "zip" program in order to run it. ?
>
>
>     ===
>     setwd("A:/R/yenlib/"); library(devtools)
>     #build("yenlib",binary=T) # Thisfailed with an error message
>     build("yenlib",binary=F) # This works
>
>      > build("yenlib",binary=T)
>     "C:/PROGRA~1/R/R-33~1.1/bin/x64/R" --no-site-file  \
>        --no-environ --no-save --no-restore --quiet CMD INSTALL \
>        "A:\R\yenlib\yenlib" --build
>
>     * installing to library
>     'C:/Users/syen01/AppData/Local/Temp/Rtmp8A7KEw/temp_libpath4074149a528e'
>     * installing *source* package 'yenlib' ...
>     ** R
>     ** data
>     ** preparing package for lazy loading
>     ** help
>     *** installing help indices
>     ** building package indices
>     ** testing if installed package can be loaded
>     *** arch - i386
>     *** arch - x64
>     * MD5 sums
>     Warning: running command '"zip" -r9Xq "A:/R/yenlib/yenlib_16.3.zip"
>     yenlib' had status 127
>     running 'zip' failed
>     * DONE (yenlib)
>     [1] "A:/R/yenlib/yenlib_16.3.zip"
>      >
>
>
>             [[alternative HTML version deleted]]
>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> -- 
> "Worry was nothing more than paying interest on a loan that a man may 
> never borrow"
>
> From: "Quest for the White Wind" by Alan Black
>
> Maranatha! <><
> John McKown


	[[alternative HTML version deleted]]


From roslinaump at gmail.com  Wed Jul 20 04:42:16 2016
From: roslinaump at gmail.com (roslinazairimah zakaria)
Date: Wed, 20 Jul 2016 10:42:16 +0800
Subject: [R] Aggregate rainfall data
In-Reply-To: <AB0D589D-5179-439E-947D-9C6B302B71D0@comcast.net>
References: <CANTvJZLA-W63e6HPMAgadWMmR6W6wubf5UuB=cVtdW7kqp1J0Q@mail.gmail.com>
	<6CD80698-D9FE-4D11-BB90-F918EAE2BCD8@comcast.net>
	<CANTvJZJSN=7i-nez4J7APX+HaaNh6B0j-aHTRSzPaaw_LK+vLg@mail.gmail.com>
	<AB0D589D-5179-439E-947D-9C6B302B71D0@comcast.net>
Message-ID: <CANTvJZJU7c3dqog5xtOV962RwLr1C_Skbu7nw1TJ46sBHoZEBw@mail.gmail.com>

Hi David,

Thank you so much for your help and others.  Here is the code.

balok <- read.csv("G:/A_backup 11 mei 2015/DATA (D)/1 Universiti Malaysia
Pahang/ISM-3 2016 UM/Data/Hourly Rainfall/balok2.csv",header=TRUE)
head(balok, 10); tail(balok, 10)
str(balok)

## Introduce NAs for
balok$Rain.mm2 <- as.numeric( as.character(balok$Rain.mm) )
head(balok$Rain.mm2); tail(balok$Rain.mm2)
head(balok, 10); tail(balok, 10)

## Change date format from DD/MM/YYYY to Day, Month, Year separately
realdate <- as.Date(balok$Date,format="%d/%m/%Y")
dfdate <- data.frame(date=realdate)
year=as.numeric (format(realdate,"%Y"))
month=as.numeric (format(realdate,"%m"))
day=as.numeric (format(realdate,"%d"))

balok2 <-cbind(dfdate,day,month,year,balok)
colnames(balok2)
head(balok2)

## New data format
balok2_new <- balok2[,c(-1,-5,-7)]
colnames(balok2_new)
head(balok2_new); tail(balok2_new)

## Aggregate data
## Sum rainfall amount from HOURLY to DAILY
dt <- balok2_new
str(dt)
aggbalok_day <- aggregate(dt[,5], by=dt[,c(1,2,3)],FUN=sum, na.rm=TRUE)
head(aggbalok_day)

## Sum rainfall amount from HOURLY to MONTHLY
dt <- balok2_new
str(dt)
aggbalok_mth <- aggregate(dt[,5], by=dt[,c(2,3)],FUN=sum, na.rm=TRUE)
head(aggbalok_mth)

Now I would like to find the basic statistics summary for the data
according to monthly.


Best regards




On Wed, Jul 13, 2016 at 10:37 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jul 13, 2016, at 3:21 AM, roslinazairimah zakaria <
> roslinaump at gmail.com> wrote:
> >
> > Dear David,
> >
> > I got your point.  How do I remove the data that contain "0.0?".
> >
> > I tried : balok <- cbind(balok3[,-5],
> balok3$Rain.mm[balok3$Rain.mm==0.0?] <- NA)
>
> If you had done as I suggested, the items with factor levels of "0.0?"
> would have automatically become NA and you would have gotten a warning
> message:
>
> > testfac <- factor( c(rep("0.0",7), "0.07", "0.0?", '0.01', '0.17'))
> > testfac
>  [1] 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.07 0.0? 0.01 0.17
> Levels: 0.0 0.0? 0.01 0.07 0.17
> > as.numeric(as.character( testfac))
>  [1] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07   NA 0.01 0.17
> Warning message:
> NAs introduced by coercion
>
>
>
> >
> > However all the Rain.mm column all become NA.
> >
> >    day month year     Time balok3$Rain.mm[balok3$Rain.mm == "0.0?"] <- NA
> > 1   30     7 2008  9:00:00                                             NA
> > 2   30     7 2008 10:00:00                                             NA
> > 3   30     7 2008 11:00:00                                             NA
> > 4   30     7 2008 12:00:00                                             NA
> > 5   30     7 2008 13:00:00                                             NA
> > 6   30     7 2008 14:00:00                                             NA
> > 7   30     7 2008 15:00:00                                             NA
> > 8   30     7 2008 16:00:00                                             NA
> > 9   30     7 2008 17:00:00                                             NA
> > 10  30     7 2008 18:00:00                                             NA
> >
> > Thank you so much.
> >
> >
> > On Wed, Jul 13, 2016 at 9:42 AM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> > > On Jul 12, 2016, at 3:45 PM, roslinazairimah zakaria <
> roslinaump at gmail.com> wrote:
> > >
> > > Dear R-users,
> > >
> > > I have these data:
> > >
> > > head(balok, 10); tail(balok, 10)
> > >        Date     Time Rain.mm
> > > 1  30/7/2008  9:00:00       0
> > > 2  30/7/2008 10:00:00       0
> > > 3  30/7/2008 11:00:00       0
> > > 4  30/7/2008 12:00:00       0
> > > 5  30/7/2008 13:00:00       0
> > > 6  30/7/2008 14:00:00       0
> > > 7  30/7/2008 15:00:00       0
> > > 8  30/7/2008 16:00:00       0
> > > 9  30/7/2008 17:00:00       0
> > > 10 30/7/2008 18:00:00       0
> > >           Date     Time Rain.mm
> > > 63667 4/11/2015  3:00:00       0
> > > 63668 4/11/2015  4:00:00       0
> > > 63669 4/11/2015  5:00:00       0
> > > 63670 4/11/2015  6:00:00       0
> > > 63671 4/11/2015  7:00:00       0
> > > 63672 4/11/2015  8:00:00       0
> > > 63673 4/11/2015  9:00:00     0.1
> > > 63674 4/11/2015 10:00:00     0.1
> > > 63675 4/11/2015 11:00:00     0.1
> > > 63676 4/11/2015 12:00:00    0.1?
> > >
> > >> str(balok)
> > > 'data.frame':   63676 obs. of  3 variables:
> > > $ Date   : Factor w/ 2654 levels "1/1/2009","1/1/2010",..: 2056 2056
> 2056
> > > 2056 2056 2056 2056 2056 2056 2056 ...
> > > $ Time   : Factor w/ 24 levels "1:00:00","10:00:00",..: 24 2 3 4 5 6 7
> 8 9
> > > 10 ...
> > > $ Rain.mm: Factor w/ 352 levels "0","0.0?","0.1",..: 1 1 1 1 1 1 1 1 1
> 1
> >
> > Thar's your problem:
> >
> >   Rain.mm: Factor w/ 352 levels "0","0.0?","0.1"
> >
> > Need to use the standard fix for the screwed-up-factor-on-input-problem
> >
> >   balok$Rain.mm2 <- as.numeric( as.character(balok$Rain.mm) )
> >
> > Cannot just do as.numeric because factors are actually already numeric.
> >
> > --
> > David.
> >
> >
> > > ...
> > >
> > > and I have change the data as follows:
> > >
> > > realdate <- as.Date(balok$Date,format="%d/%m/%Y")
> > > dfdate <- data.frame(date=realdate)
> > > year=as.numeric (format(realdate,"%Y"))
> > > month=as.numeric (format(realdate,"%m"))
> > > day=as.numeric (format(realdate,"%d"))
> > >
> > > balok2 <-cbind(dfdate,day,month,year,balok[,2:3])
> > > colnames(balok2)
> > > head(balok2)
> > >        date day month year     Time Rain.mm
> > > 1 2008-07-30  30     7 2008  9:00:00       0
> > > 2 2008-07-30  30     7 2008 10:00:00       0
> > > 3 2008-07-30  30     7 2008 11:00:00       0
> > > 4 2008-07-30  30     7 2008 12:00:00       0
> > > 5 2008-07-30  30     7 2008 13:00:00       0
> > > 6 2008-07-30  30     7 2008 14:00:00       0
> > > ...
> > >
> > >> balok3 <- balok2[,-1]; head(balok3, n=100)
> > >    day month year     Time Rain.mm
> > > 1    30     7 2008  9:00:00       0
> > > 2    30     7 2008 10:00:00       0
> > > 3    30     7 2008 11:00:00       0
> > > 4    30     7 2008 12:00:00       0
> > > 5    30     7 2008 13:00:00       0
> > > 6    30     7 2008 14:00:00       0
> > > 7    30     7 2008 15:00:00       0
> > > 8    30     7 2008 16:00:00       0
> > > 9    30     7 2008 17:00:00       0
> > > 10   30     7 2008 18:00:00       0
> > > 11   30     7 2008 19:00:00       0
> > > 12   30     7 2008 20:00:00       0
> > > 13   30     7 2008 21:00:00       0
> > > 14   30     7 2008 22:00:00       0
> > > 15   30     7 2008 23:00:00       0
> > > 16   30     7 2008 24:00:00       0
> > > 17   31     7 2008  1:00:00       0
> > > 18   31     7 2008  2:00:00       0
> > > 19   31     7 2008  3:00:00       0
> > > 20   31     7 2008  4:00:00       0
> > > 21   31     7 2008  5:00:00       0
> > > 22   31     7 2008  6:00:00       0
> > > 23   31     7 2008  7:00:00       0
> > > 24   31     7 2008  8:00:00       0
> > > 25   31     7 2008  9:00:00       0
> > > 26   31     7 2008 10:00:00       0
> > > 27   31     7 2008 11:00:00       0
> > > 28   31     7 2008 12:00:00       0
> > > 29   31     7 2008 13:00:00       0
> > > 30   31     7 2008 14:00:00       0
> > > 31   31     7 2008 15:00:00       0
> > > 32   31     7 2008 16:00:00       0
> > > 33   31     7 2008 17:00:00       0
> > > 34   31     7 2008 18:00:00       0
> > > 35   31     7 2008 19:00:00       0
> > > 36   31     7 2008 20:00:00       0
> > > 37   31     7 2008 21:00:00       0
> > > 38   31     7 2008 22:00:00       0
> > > 39   31     7 2008 23:00:00       0
> > > 40   31     7 2008 24:00:00       0
> > > 41    1     8 2008  1:00:00       0
> > > 42    1     8 2008  2:00:00       0
> > > 43    1     8 2008  3:00:00       0
> > > 44    1     8 2008  4:00:00       0
> > > 45    1     8 2008  5:00:00       0
> > > 46    1     8 2008  6:00:00       0
> > > 47    1     8 2008  7:00:00       0
> > > 48    1     8 2008  8:00:00       0
> > > 49    1     8 2008  9:00:00       0
> > > 50    1     8 2008 10:00:00       0
> > > 51    1     8 2008 11:00:00       0
> > > 52    1     8 2008 12:00:00       0
> > > 53    1     8 2008 13:00:00       0
> > > 54    1     8 2008 14:00:00       0
> > > 55    1     8 2008 15:00:00       0
> > > 56    1     8 2008 16:00:00       0
> > > 57    1     8 2008 17:00:00       0
> > > 58    1     8 2008 18:00:00       0
> > > 59    1     8 2008 19:00:00       0
> > > 60    1     8 2008 20:00:00       0
> > > 61    1     8 2008 21:00:00       0
> > > 62    1     8 2008 22:00:00       0
> > > 63    1     8 2008 23:00:00       0
> > > 64    1     8 2008 24:00:00       0
> > > 65    2     8 2008  1:00:00       0
> > > 66    2     8 2008  2:00:00       0
> > > 67    2     8 2008  3:00:00       0
> > > 68    2     8 2008  4:00:00       0
> > > 69    2     8 2008  5:00:00       0
> > > 70    2     8 2008  6:00:00       0
> > > 71    2     8 2008  7:00:00       0
> > > 72    2     8 2008  8:00:00       0
> > > 73    2     8 2008  9:00:00       0
> > > 74    2     8 2008 10:00:00       0
> > > 75    2     8 2008 11:00:00       0
> > > 76    2     8 2008 12:00:00       0
> > > 77    2     8 2008 13:00:00       0
> > > 78    2     8 2008 14:00:00       0
> > > 79    2     8 2008 15:00:00       0
> > > 80    2     8 2008 16:00:00       0
> > > 81    2     8 2008 17:00:00       0
> > > 82    2     8 2008 18:00:00       0
> > > 83    2     8 2008 19:00:00       0
> > > 84    2     8 2008 20:00:00       0
> > > 85    2     8 2008 21:00:00       0
> > > 86    2     8 2008 22:00:00       0
> > > 87    2     8 2008 23:00:00       0
> > > 88    2     8 2008 24:00:00    11.1
> > > 89    3     8 2008  1:00:00     0.4
> > > 90    3     8 2008  2:00:00       0
> > > 91    3     8 2008  3:00:00       0
> > > 92    3     8 2008  4:00:00       0
> > > 93    3     8 2008  5:00:00       0
> > > 94    3     8 2008  6:00:00       0
> > > 95    3     8 2008  7:00:00       0
> > > 96    3     8 2008  8:00:00       0
> > > 97    3     8 2008  9:00:00       0
> > > 98    3     8 2008 10:00:00       0
> > > 99    3     8 2008 11:00:00       0
> > > 100   3     8 2008 12:00:00       0
> > >
> > > The rainfall data is in hourly unit, and I would like to sum the
> Rain.mm
> > > according to month.  I tried to use aggregate(), but I got this
> message:
> > >
> > > dt <- balok4
> > > str(dt)
> > > aggbalok <- aggregate(dt[,5], by=dt[,c(1,4)],FUN=sum, na.rm=TRUE)
> > > aggbalok
> > >
> > > Error in Summary.factor(1L, na.rm = TRUE) :
> > >  sum not meaningful for factors
> > >
> > >
> > > Thank you so much for any help given.
> > >
> > > Roslina
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
> >
> >
> > --
> > Dr. Roslinazairimah Binti Zakaria
> > Tel: +609-5492370; Fax. No.+609-5492766
> > Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
> > Deputy Dean (Academic & Student Affairs)
> > Faculty of Industrial Sciences & Technology
> > University Malaysia Pahang
> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
> David Winsemius
> Alameda, CA, USA
>
>


-- 
*Dr. Roslinazairimah Binti Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Deputy Dean (Academic & Student Affairs)
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From ivan.calandra at univ-reims.fr  Wed Jul 20 10:00:39 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Wed, 20 Jul 2016 10:00:39 +0200
Subject: [R] txtProgressBar()
Message-ID: <43c8e65f-d535-5b96-c7c1-080517d648fd@univ-reims.fr>

Dear useRs,

In a script that will be source()d, I want to install the uninstalled 
packages and follow the progression with a bar. So I looked at 
txtProgressBar() but I cannot figure out how to use it to show the 
progression of the installation.

All the examples I have found just display the progress of... the 
progress bar itself ?

Any idea?

Thanks in advance,
Ivan

--
Ivan Calandra, PhD
Scientific Mediator
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/


From wewolski at gmail.com  Wed Jul 20 10:26:26 2016
From: wewolski at gmail.com (Witold E Wolski)
Date: Wed, 20 Jul 2016 10:26:26 +0200
Subject: [R] documenting R reference class methods with roxygen2
Message-ID: <CAAjnpdiX9Ckz-6N4RBTmPA51+AYTffooN05J-=r+azFrSW=HCw@mail.gmail.com>

I would like to document function arguments in R reference classes and
I do not know how.

I know that methods description can be provided by adding
a string below the function declaration. But how to document the
function arguments in similar fashion like the arguments of R
functions (which can be documented with @param).

Thank you.

PS. Sorry if for some this question is off topic but I did not found a
mailing list dedicated to roxygen2.



-- 
Witold Eryk Wolski


From lists at dewey.myzen.co.uk  Wed Jul 20 10:55:43 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 20 Jul 2016 09:55:43 +0100
Subject: [R] frequency of items
In-Reply-To: <CAGO7QoN_gBYLyTjtfjWzKUSOP=Z98TcNNCORoWvo+et8HunoNQ@mail.gmail.com>
References: <CAGO7QoN_gBYLyTjtfjWzKUSOP=Z98TcNNCORoWvo+et8HunoNQ@mail.gmail.com>
Message-ID: <d63d6872-1934-81da-db87-8dee975db63c@dewey.myzen.co.uk>

It seems very unlikely that what you quote can be a data frame. It could 
be a list I suppose. Can you clarify?

On 19/07/2016 11:59, sri vathsan wrote:
> Hi,
>
> I have a data frame like below.
> 11,15,12,25
> 11,12
> 15,25
> 134,45,56
> 46
> 45,56
> 15,12
> 66,45,56,24,14,11,25,12,134
>
> I want to identify the frequency of pairs/triplets or higher that occurs in
> the data. Say for example, in above data the occurrence of pairs looks like
> below
>
> item     No of occurrence
> 11,12         3
> 11,25         2
> 15,12         2
> 15,25         2
> .
> .
> 45,56          3
> 134,45,56    2
>
>  ....and so on
>
> I am trying to write R code for the above and I am finding difficulty to
> approach this. Looking forward some help.
>
> Thanks!
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From drjimlemon at gmail.com  Wed Jul 20 11:18:37 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 20 Jul 2016 19:18:37 +1000
Subject: [R] frequency of items
In-Reply-To: <CAGO7QoN_gBYLyTjtfjWzKUSOP=Z98TcNNCORoWvo+et8HunoNQ@mail.gmail.com>
References: <CAGO7QoN_gBYLyTjtfjWzKUSOP=Z98TcNNCORoWvo+et8HunoNQ@mail.gmail.com>
Message-ID: <CA+8X3fW1_Ec5VpC9pZoA=tBsWWWqQdQvLYizqcJN2YCyRhAc0w@mail.gmail.com>

Hi sri,
Maybe something like this?

has_values<-function(x,values) {
 if(is.list(x)) {
  return(sum(unlist(lapply(svlist,
   function(x,values) return(all(values %in% x)),c(11,12)))))
 }
}

svlist<-list(a=c(11,15,12,25),
 b=c(11,12),
 c=c(15,25),
 d=c(134,45,56),
 e=46,
 f=c(45,56),
 g=c(15,12),
 h=c(66,45,56,24,14,11,25,12,134))

has_values(svlist,c(11,12))

Jim

On Tue, Jul 19, 2016 at 8:59 PM, sri vathsan <srivibish at gmail.com> wrote:
> Hi,
>
> I have a data frame like below.
> 11,15,12,25
> 11,12
> 15,25
> 134,45,56
> 46
> 45,56
> 15,12
> 66,45,56,24,14,11,25,12,134
>
> I want to identify the frequency of pairs/triplets or higher that occurs in
> the data. Say for example, in above data the occurrence of pairs looks like
> below
>
> item     No of occurrence
> 11,12         3
> 11,25         2
> 15,12         2
> 15,25         2
> .
> .
> 45,56          3
> 134,45,56    2
>
>  ....and so on
>
> I am trying to write R code for the above and I am finding difficulty to
> approach this. Looking forward some help.
>
> Thanks!
>
> --
>
> Regards,
> Srivathsan.K
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r_goertz at web.de  Wed Jul 20 11:35:31 2016
From: r_goertz at web.de (Ralf Goertz)
Date: Wed, 20 Jul 2016 11:35:31 +0200
Subject: [R] readline issue with 3.3.1
Message-ID: <20160720113531.2bacaa0a@delli.home.local>

Hi,

after a recent update to version 3.3.1 on Opensuse Leap I have problems
with command lines longer than the terminal width. E.g. when I do this

> print("This is a very long line which is in fact 
so long that it gets wrapped while writing it")

and then hit enter I end up with:

> print("This is a very long line which is in fact 
[1] "This is a very long line which is in fact so l
ong that it gets wrapped while writing it"


So the output overwrites the second line of input. This does not happen
when I start R without readline support using "R --no-readline". That's
why I thought it could be a readline problem. But my current readline6
version (6.2) was installed way before the update of R and I had no
problems with the previous R version. Furthermore no other program using
readline seems to have that problem. E.g. in bash:

me at host:~/some/dir> echo This is a very long line which is in fact 
so long that it gets wrapped while writing it
This is a very long line which is in fact so long that it gets wrapped whi
le writing it


From drjimlemon at gmail.com  Wed Jul 20 11:41:20 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 20 Jul 2016 19:41:20 +1000
Subject: [R] frequency of items
In-Reply-To: <CA+8X3fW1_Ec5VpC9pZoA=tBsWWWqQdQvLYizqcJN2YCyRhAc0w@mail.gmail.com>
References: <CAGO7QoN_gBYLyTjtfjWzKUSOP=Z98TcNNCORoWvo+et8HunoNQ@mail.gmail.com>
	<CA+8X3fW1_Ec5VpC9pZoA=tBsWWWqQdQvLYizqcJN2YCyRhAc0w@mail.gmail.com>
Message-ID: <CA+8X3fXRXo0VYQ37RZm-25vaC=zAdRfjbA17pis4Ovd=nxBjtg@mail.gmail.com>

Oops, didn't translate that function correctly:

has_values<-function(x,values) {
 if(is.list(x)) {
  return(sum(unlist(lapply(svlist,
   function(x,values) return(all(values %in% x)),values))))
 }
}

Jim

On Wed, Jul 20, 2016 at 7:18 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi sri,
> Maybe something like this?
>
> has_values<-function(x,values) {
>  if(is.list(x)) {
>   return(sum(unlist(lapply(svlist,
>    function(x,values) return(all(values %in% x)),c(11,12)))))
>  }
> }
>
> svlist<-list(a=c(11,15,12,25),
>  b=c(11,12),
>  c=c(15,25),
>  d=c(134,45,56),
>  e=46,
>  f=c(45,56),
>  g=c(15,12),
>  h=c(66,45,56,24,14,11,25,12,134))
>
> has_values(svlist,c(11,12))
>
> Jim
>
> On Tue, Jul 19, 2016 at 8:59 PM, sri vathsan <srivibish at gmail.com> wrote:
>> Hi,
>>
>> I have a data frame like below.
>> 11,15,12,25
>> 11,12
>> 15,25
>> 134,45,56
>> 46
>> 45,56
>> 15,12
>> 66,45,56,24,14,11,25,12,134
>>
>> I want to identify the frequency of pairs/triplets or higher that occurs in
>> the data. Say for example, in above data the occurrence of pairs looks like
>> below
>>
>> item     No of occurrence
>> 11,12         3
>> 11,25         2
>> 15,12         2
>> 15,25         2
>> .
>> .
>> 45,56          3
>> 134,45,56    2
>>
>>  ....and so on
>>
>> I am trying to write R code for the above and I am finding difficulty to
>> approach this. Looking forward some help.
>>
>> Thanks!
>>
>> --
>>
>> Regards,
>> Srivathsan.K
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Wed Jul 20 12:49:12 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jul 2016 06:49:12 -0400
Subject: [R] Build command in library(devtools)
In-Reply-To: <196aed4a-f7a8-f898-bff4-84c548119d67@gmail.com>
References: <291e222a-a541-9b79-459c-bcdcd7f8e66b@gmail.com>
	<CAAJSdjhC6V-RE6pSg41+1o6LKiPibWreszXQLAQiC1Qfy2DWsQ@mail.gmail.com>
	<196aed4a-f7a8-f898-bff4-84c548119d67@gmail.com>
Message-ID: <2d7ab661-863a-0bef-e521-f02dfe166561@gmail.com>

On 19/07/2016 10:06 PM, Steven Yen wrote:
> Thanks. I found the reason was Rtools does not run under the new version
> of R. I had to go back to as early as R 3.0.2 (September 2013) to make
> Rtools work.
> Any idea for a go-around? Thanks.

Which version of Rtools are you using?  Current Rtools is used to build 
current R (and all of its packages), so of course it works there (though 
we may not have tested on the latest version of Windows, and devtools is 
its own project, unrelated to Rtools).

Duncan Murdoch

>
> On 7/19/2016 4:38 PM, John McKown wrote:
>> On Tue, Jul 19, 2016 at 3:15 PM, Steven Yen <syen04 at gmail.com
>> <mailto:syen04 at gmail.com>>wrote:
>>
>>     I recently updated my R and RStudio to the latest version and now the
>>     binary option in the "build" command in devtools stops working.
>>
>>     I went around and used the binary=F option which worked by I get the
>>     .tar.gz file instead of the .zip file which I prefer.
>>
>>     Does anyone understand the following error message:
>>
>>     status 127
>>     running 'zip' failed
>>
>>
>> ?I'm not totally sure, but I think that means that R cannot find the
>> "zip" program in order to run it. ?
>>
>>
>>     ===
>>     setwd("A:/R/yenlib/"); library(devtools)
>>     #build("yenlib",binary=T) # Thisfailed with an error message
>>     build("yenlib",binary=F) # This works
>>
>>      > build("yenlib",binary=T)
>>     "C:/PROGRA~1/R/R-33~1.1/bin/x64/R" --no-site-file  \
>>        --no-environ --no-save --no-restore --quiet CMD INSTALL \
>>        "A:\R\yenlib\yenlib" --build
>>
>>     * installing to library
>>     'C:/Users/syen01/AppData/Local/Temp/Rtmp8A7KEw/temp_libpath4074149a528e'
>>     * installing *source* package 'yenlib' ...
>>     ** R
>>     ** data
>>     ** preparing package for lazy loading
>>     ** help
>>     *** installing help indices
>>     ** building package indices
>>     ** testing if installed package can be loaded
>>     *** arch - i386
>>     *** arch - x64
>>     * MD5 sums
>>     Warning: running command '"zip" -r9Xq "A:/R/yenlib/yenlib_16.3.zip"
>>     yenlib' had status 127
>>     running 'zip' failed
>>     * DONE (yenlib)
>>     [1] "A:/R/yenlib/yenlib_16.3.zip"
>>      >
>>
>>
>>             [[alternative HTML version deleted]]
>>
>>     ______________________________________________
>>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>>     To UNSUBSCRIBE and more, see
>>     https://stat.ethz.ch/mailman/listinfo/r-help
>>     PLEASE do read the posting guide
>>     http://www.R-project.org/posting-guide.html
>>     and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>>
>> --
>> "Worry was nothing more than paying interest on a loan that a man may
>> never borrow"
>>
>> From: "Quest for the White Wind" by Alan Black
>>
>> Maranatha! <><
>> John McKown
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ulrik.stervbo at gmail.com  Wed Jul 20 12:59:02 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Wed, 20 Jul 2016 10:59:02 +0000
Subject: [R] frequency of items
In-Reply-To: <CA+8X3fXRXo0VYQ37RZm-25vaC=zAdRfjbA17pis4Ovd=nxBjtg@mail.gmail.com>
References: <CAGO7QoN_gBYLyTjtfjWzKUSOP=Z98TcNNCORoWvo+et8HunoNQ@mail.gmail.com>
	<CA+8X3fW1_Ec5VpC9pZoA=tBsWWWqQdQvLYizqcJN2YCyRhAc0w@mail.gmail.com>
	<CA+8X3fXRXo0VYQ37RZm-25vaC=zAdRfjbA17pis4Ovd=nxBjtg@mail.gmail.com>
Message-ID: <CAKVAULM=iOqa9c9Y7H5zgYEWSQWj5itr8iRhQ=7EN5fdyb183w@mail.gmail.com>

What you show cannot be a data.frame. Using what you gave, this should help
you along:

x <- c(11,15,12,25, 11,12, 15,25, 134,45,56, 46, 45,56, 15,12,
66,45,56,24,14,11,25,12,134)
table(x)

On Wed, 20 Jul 2016 at 11:44 Jim Lemon <drjimlemon at gmail.com> wrote:

> Oops, didn't translate that function correctly:
>
> has_values<-function(x,values) {
>  if(is.list(x)) {
>   return(sum(unlist(lapply(svlist,
>    function(x,values) return(all(values %in% x)),values))))
>  }
> }
>
> Jim
>
> On Wed, Jul 20, 2016 at 7:18 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> > Hi sri,
> > Maybe something like this?
> >
> > has_values<-function(x,values) {
> >  if(is.list(x)) {
> >   return(sum(unlist(lapply(svlist,
> >    function(x,values) return(all(values %in% x)),c(11,12)))))
> >  }
> > }
> >
> > svlist<-list(a=c(11,15,12,25),
> >  b=c(11,12),
> >  c=c(15,25),
> >  d=c(134,45,56),
> >  e=46,
> >  f=c(45,56),
> >  g=c(15,12),
> >  h=c(66,45,56,24,14,11,25,12,134))
> >
> > has_values(svlist,c(11,12))
> >
> > Jim
> >
> > On Tue, Jul 19, 2016 at 8:59 PM, sri vathsan <srivibish at gmail.com>
> wrote:
> >> Hi,
> >>
> >> I have a data frame like below.
> >> 11,15,12,25
> >> 11,12
> >> 15,25
> >> 134,45,56
> >> 46
> >> 45,56
> >> 15,12
> >> 66,45,56,24,14,11,25,12,134
> >>
> >> I want to identify the frequency of pairs/triplets or higher that
> occurs in
> >> the data. Say for example, in above data the occurrence of pairs looks
> like
> >> below
> >>
> >> item     No of occurrence
> >> 11,12         3
> >> 11,25         2
> >> 15,12         2
> >> 15,25         2
> >> .
> >> .
> >> 45,56          3
> >> 134,45,56    2
> >>
> >>  ....and so on
> >>
> >> I am trying to write R code for the above and I am finding difficulty to
> >> approach this. Looking forward some help.
> >>
> >> Thanks!
> >>
> >> --
> >>
> >> Regards,
> >> Srivathsan.K
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From justinthong93 at gmail.com  Wed Jul 20 13:34:34 2016
From: justinthong93 at gmail.com (Justin Thong)
Date: Wed, 20 Jul 2016 12:34:34 +0100
Subject: [R] Missing rows anova
In-Reply-To: <cf2cf482-345a-2434-53e2-76c2db377b9d@dewey.myzen.co.uk>
References: <CAEtAGep+YPEbnxEOV8rf1XZd-G+uENS=eQ+U2U1ykPtoMmm_bQ@mail.gmail.com>
	<cf2cf482-345a-2434-53e2-76c2db377b9d@dewey.myzen.co.uk>
Message-ID: <CAEtAGeoBYyLARue=GYO3aGZsYDbu-LWmnkYYJi5NweZLgCOfyA@mail.gmail.com>

Hi Michael,

Thank you for the reply.

I am sorry I forgot to print out the anova table to make my question clear.

              Df            Sum Sq      Mean Sq     F value   Pr(>F)
S             2            0.000019    9.630e-06    0.818    0.444
x1            1            0.000256    2.560e-04   21.751   9.44e-06 ***
ID           47           0.003524    7.498e-05    6.370     3.35e-15 ***
Resid    102           0.001201    1.177e-05
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

There is *no* unique value for ID for each combination of S and x1. For
example, S=1 and x1=0 can equal to either  B or C or  D or  E or  F .
Perhaps you mean that for each combination of S and x1 have different
values. If that is the case, I think maybe it makes sense.

*What I think? *
Anova has this thing where it fits the terms of 1st order first ( a formula
term including no interactions) before it fits a 2nd order term ( a formula
term including 1 interaction) and so on.

First Order--> Second Order--> Third Order--> etc

Therefore, it is known that the true fitting formula is not S+x1+S:x1+ID
but it is S+x1+ID+S:x1. Hence, it appears that ID is fitted before S:x1 but
since ID is a more refined factor than S:x1, it can be said that S:x1 is
already included in the fit of ID so R recognizes the linear dependance and
excludes the term S:x1.
In other words, S:x1 is linearly dependant to ID. And so, the row S:x1
disappears because it is considered within ID.

Does this makes sense?








On 19 July 2016 at 16:19, Michael Dewey <lists at dewey.myzen.co.uk> wrote:

> Presumably it disappears because there is a unique value of ID for eac
> combination of S*x1 so they are indistinguishable.
>
>
> On 19/07/2016 12:53, Justin Thong wrote:
>
>> Why does the S:x1 column disappear (presumably S:x1 goes into ID but I
>> dont
>> know why)? S is a factor, x1 is a covariate and ID is a factor.
>>
>>     rich.side<-aov(y~S*x1+ID)
>>     summary(rich.side)
>>
>> Below is the model frame
>>
>>     model.frame(~S*x1+ID)
>>
>>         S x1  ID
>>     1   1 12   A
>>     2   1 12   A
>>     3   1 12   A
>>     4   1 12   A
>>     5   1  0   B
>>     6   1  0   B
>>     7   1  0   B
>>     8   1  0   B
>>     9   1  0   C
>>     10  1  0   C
>>     11  1  0   C
>>     12  1  0   C
>>     13  1  0   D
>>     14  1  0   D
>>     15  1  0   D
>>     16  1  0   D
>>     17  1  0   E
>>     18  1  0   E
>>     19  1  0   E
>>     20  1  0   E
>>     21  1  0   F
>>     22  1  0   F
>>     23  1  0   F
>>     24  1  0   F
>>     25  2  6  AB
>>     26  2  6  AB
>>     27  2  6  AB
>>     28  2  6  AB
>>     29  2  6  AC
>>     30  2  6  AC
>>     31  2  6  AC
>>     32  2  6  AC
>>     33  2  6  AD
>>     34  2  6  AD
>>     35  2  6  AD
>>     36  2  6  AD
>>     37  2  6  AE
>>     38  2  6  AE
>>     39  2  6  AE
>>     40  2  6  AE
>>     41  2  6  AF
>>     42  2  6  AF
>>     43  2  6  AF
>>     44  2  6  AF
>>     45  2  0  BC
>>     46  2  0  BC
>>     47  2  0  BC
>>     48  2  0  BC
>>     49  2  0  BD
>>     50  2  0  BD
>>     51  2  0  BD
>>     52  2  0  BD
>>     53  2  0  BE
>>     54  2  0  BE
>>     55  2  0  BE
>>     56  2  0  BE
>>     57  2  0  BF
>>     58  2  0  BF
>>     59  2  0  BF
>>     60  2  0  BF
>>     61  2  0  CD
>>     62  2  0  CD
>>     63  2  0  CD
>>     64  2  0  CD
>>     65  2  0  CE
>>     66  2  0  CE
>>     67  2  0  CE
>>     68  2  0  CE
>>     69  2  0  CF
>>     70  2  0  CF
>>     71  2  0  CF
>>     72  2  0  CF
>>     73  2  0  DE
>>     74  2  0  DE
>>     75  2  0  DE
>>     76  2  0  DE
>>     77  2  0  DF
>>     78  2  0  DF
>>     79  2  0  DF
>>     80  2  0  DF
>>     81  2  0  EF
>>     82  2  0  EF
>>     83  2  0  EF
>>     84  2  0  EF
>>     85  3  4 ABC
>>     86  3  4 ABC
>>     87  3  4 ABC
>>     88  3  4 ABC
>>     89  3  4 ABD
>>     90  3  4 ABD
>>     91  3  4 ABD
>>     92  3  4 ABD
>>     93  3  4 ABE
>>     94  3  4 ABE
>>     95  3  4 ABE
>>     96  3  4 ABE
>>     97  3  4 ABF
>>     98  3  4 ABF
>>     99  3  4 ABF
>>     100 3  4 ABF
>>     101 3  4 ACD
>>     102 3  4 ACD
>>     103 3  4 ACD
>>     104 3  4 ACD
>>     105 3  4 ACE
>>     106 3  4 ACE
>>     107 3  4 ACE
>>     108 3  4 ACE
>>     109 3  4 ACF
>>     110 3  4 ACF
>>     111 3  4 ACF
>>     112 3  4 ACF
>>     113 3  4 ADE
>>     114 3  4 ADE
>>     115 3  4 ADE
>>     116 3  4 ADE
>>     117 3  4 ADF
>>     118 3  4 ADF
>>     119 3  4 ADF
>>     120 3  4 ADF
>>     121 3  4 AEF
>>     122 3  4 AEF
>>     123 3  4 AEF
>>     124 3  4 AEF
>>     125 3  0 BCD
>>     126 3  0 BCD
>>     127 3  0 BCD
>>     128 3  0 BCD
>>     129 3  0 BCE
>>     130 3  0 BCE
>>     131 3  0 BCE
>>     132 3  0 BCE
>>     133 3  0 BCF
>>     134 3  0 BCF
>>     135 3  0 BCF
>>     136 3  0 BCF
>>     137 3  0 BDE
>>     138 3  0 BDE
>>     139 3  0 BDE
>>     140 3  0 BDE
>>     141 3  0 BDF
>>     142 3  0 BDF
>>     143 3  0 BDF
>>     144 3  0 BDF
>>     145 3  0 BEF
>>     146 3  0 BEF
>>     147 3  0 BEF
>>     148 3  0 BEF
>>     149 3  0 CDE
>>     150 3  0 CDE
>>     151 3  0 CDE
>>     152 3  0 CDE
>>     153 3  0 CDF
>>     154 3  0 CDF
>>     155 3  0 CDF
>>     156 3  0 CDF
>>     157 3  0 CEF
>>     158 3  0 CEF
>>     159 3  0 CEF
>>     160 3  0 CEF
>>     161 3  0 DEF
>>     162 3  0 DEF
>>     163 3  0 DEF
>>     164 3  0 DEF
>>
>>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>



-- 
Yours sincerely,
Justin

*I check my email at 9AM and 4PM everyday*
*If you have an EMERGENCY, contact me at +447938674419(UK) or
+60125056192(Malaysia)*

	[[alternative HTML version deleted]]


From btupper at bigelow.org  Wed Jul 20 15:07:57 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Wed, 20 Jul 2016 09:07:57 -0400
Subject: [R] documenting R reference class methods with roxygen2
In-Reply-To: <CAAjnpdiX9Ckz-6N4RBTmPA51+AYTffooN05J-=r+azFrSW=HCw@mail.gmail.com>
References: <CAAjnpdiX9Ckz-6N4RBTmPA51+AYTffooN05J-=r+azFrSW=HCw@mail.gmail.com>
Message-ID: <C80095AC-2F03-41F6-8225-4A21A4646843@bigelow.org>

Hi,

Yes, it takes a lot of searching for find out how to use roxygen with Ref Classes.  Thank goodness for Hadley Wickham's book ( http://r-pkgs.had.co.nz/ ).

For a Ref Class methods are actually documenting NULL, and you need to add the "@name Classname_Methodname" tag.  Otherwise, documentation is similar to that for functions.

#' Title
#'
#' Other stuff
#'
#' @name MyClass_fooey
#' @param foo_value numeric blah blah blah
#' @return numeric
#' @examples{
#'	\dontrun{
#'		blah blah blah
#'      }
#' }
NULL
MyClass$methods(
	fooey = function(foo_value = 3){
	    cat("in the fooey method\n")
	    return(foo_value)
	})

You could scout out some of the examples here ... https://github.com/BigelowLab/flowcamr 


Cheers,
Ben


> On Jul 20, 2016, at 4:26 AM, Witold E Wolski <wewolski at gmail.com> wrote:
> 
> I would like to document function arguments in R reference classes and
> I do not know how.
> 
> I know that methods description can be provided by adding
> a string below the function declaration. But how to document the
> function arguments in similar fashion like the arguments of R
> functions (which can be documented with @param).
> 
> Thank you.
> 
> PS. Sorry if for some this question is off topic but I did not found a
> mailing list dedicated to roxygen2.
> 
> 
> 
> -- 
> Witold Eryk Wolski
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Report Gulf of Maine jellyfish sightings to jellyfish at bigelow.org or tweet them to #MaineJellies -- include date, time, and location, as well as any descriptive information such as size or type.  Learn more at https://www.bigelow.org/research/srs/nick-record/nick-record-laboratory/mainejellies/


From jvadams at usgs.gov  Wed Jul 20 15:22:07 2016
From: jvadams at usgs.gov (Adams, Jean)
Date: Wed, 20 Jul 2016 08:22:07 -0500
Subject: [R] multiple-line plot
In-Reply-To: <CAJgdCD7Ad6-K3+iud9aqMSd9L=sW7N4t-e9zCZ-HKisjrB3iyQ@mail.gmail.com>
References: <CAJgdCD7Ad6-K3+iud9aqMSd9L=sW7N4t-e9zCZ-HKisjrB3iyQ@mail.gmail.com>
Message-ID: <CAN5YmCFSsbRY9TTLjZrxG3qWRWg8Cx28740AyMY2OhAwGipT4A@mail.gmail.com>

Use the dput() function to share a subset of your data with the list
(attachments are not supported).

For example, submit this command
     dput(for_jhon[1:20, ])
and post the result to the list along with your question.

Jean

On Tue, Jul 19, 2016 at 1:24 PM, John Wasige <johnwasige at gmail.com> wrote:

> ?Dear all,
>
> This is to kindly request for your help. I would like to plot my data.
>
> The R script below gives some plot that is not clear. How can I get a clear
> multiple-line plot. The data is attached herewith.
>
> ##R Script
> for_jhon = read.csv("C:/LVM_share/for_ jhon.csv", header=TRUE, sep=";")
> matplot(for_jhon$ID, cbind(for_jhon[,2:73]))?
>
> Thanks for your help
>
> John
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From r_goertz at web.de  Wed Jul 20 16:37:53 2016
From: r_goertz at web.de (Ralf Goertz)
Date: Wed, 20 Jul 2016 16:37:53 +0200
Subject: [R] readline issue with 3.3.1
In-Reply-To: <20160720113531.2bacaa0a@delli.home.local>
References: <20160720113531.2bacaa0a@delli.home.local>
Message-ID: <20160720163753.039cbccb@delli.home.local>

Am Wed, 20 Jul 2016 11:35:31 +0200
schrieb Ralf Goertz <r_goertz at web.de>:

> Hi,
> 
> after a recent update to version 3.3.1 on Opensuse Leap I have
> problems with command lines longer than the terminal width. E.g. when
> I do this

I installed readline version 6.3 and the issue is gone. So probably some
of the recent changes in R's readline code are incompatible with version
readline version 6.2.


From srivibish at gmail.com  Wed Jul 20 16:58:11 2016
From: srivibish at gmail.com (sri vathsan)
Date: Wed, 20 Jul 2016 20:28:11 +0530
Subject: [R] frequency of items
In-Reply-To: <CAKVAULM=iOqa9c9Y7H5zgYEWSQWj5itr8iRhQ=7EN5fdyb183w@mail.gmail.com>
References: <CAGO7QoN_gBYLyTjtfjWzKUSOP=Z98TcNNCORoWvo+et8HunoNQ@mail.gmail.com>
	<CA+8X3fW1_Ec5VpC9pZoA=tBsWWWqQdQvLYizqcJN2YCyRhAc0w@mail.gmail.com>
	<CA+8X3fXRXo0VYQ37RZm-25vaC=zAdRfjbA17pis4Ovd=nxBjtg@mail.gmail.com>
	<CAKVAULM=iOqa9c9Y7H5zgYEWSQWj5itr8iRhQ=7EN5fdyb183w@mail.gmail.com>
Message-ID: <CAGO7QoOkPUVvH2JaaJ_40SBwvakjq_mKeS1zsNZmz9QE5ENUOg@mail.gmail.com>

Hi all,

I understand this is not a data frame but an individual variable from the
data frame which is a list. Since, I need to work on this particular
variable I just added here as a sample data.

Jim, your code solved the purpose.Thanks!

Regards,
Srivathsan


On Wed, Jul 20, 2016 at 4:29 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
wrote:

> What you show cannot be a data.frame. Using what you gave, this should
> help you along:
>
> x <- c(11,15,12,25, 11,12, 15,25, 134,45,56, 46, 45,56, 15,12,
> 66,45,56,24,14,11,25,12,134)
> table(x)
>
> On Wed, 20 Jul 2016 at 11:44 Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Oops, didn't translate that function correctly:
>>
>> has_values<-function(x,values) {
>>  if(is.list(x)) {
>>   return(sum(unlist(lapply(svlist,
>>    function(x,values) return(all(values %in% x)),values))))
>>  }
>> }
>>
>> Jim
>>
>> On Wed, Jul 20, 2016 at 7:18 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
>> > Hi sri,
>> > Maybe something like this?
>> >
>> > has_values<-function(x,values) {
>> >  if(is.list(x)) {
>> >   return(sum(unlist(lapply(svlist,
>> >    function(x,values) return(all(values %in% x)),c(11,12)))))
>> >  }
>> > }
>> >
>> > svlist<-list(a=c(11,15,12,25),
>> >  b=c(11,12),
>> >  c=c(15,25),
>> >  d=c(134,45,56),
>> >  e=46,
>> >  f=c(45,56),
>> >  g=c(15,12),
>> >  h=c(66,45,56,24,14,11,25,12,134))
>> >
>> > has_values(svlist,c(11,12))
>> >
>> > Jim
>> >
>> > On Tue, Jul 19, 2016 at 8:59 PM, sri vathsan <srivibish at gmail.com>
>> wrote:
>> >> Hi,
>> >>
>> >> I have a data frame like below.
>> >> 11,15,12,25
>> >> 11,12
>> >> 15,25
>> >> 134,45,56
>> >> 46
>> >> 45,56
>> >> 15,12
>> >> 66,45,56,24,14,11,25,12,134
>> >>
>> >> I want to identify the frequency of pairs/triplets or higher that
>> occurs in
>> >> the data. Say for example, in above data the occurrence of pairs looks
>> like
>> >> below
>> >>
>> >> item     No of occurrence
>> >> 11,12         3
>> >> 11,25         2
>> >> 15,12         2
>> >> 15,25         2
>> >> .
>> >> .
>> >> 45,56          3
>> >> 134,45,56    2
>> >>
>> >>  ....and so on
>> >>
>> >> I am trying to write R code for the above and I am finding difficulty
>> to
>> >> approach this. Looking forward some help.
>> >>
>> >> Thanks!
>> >>
>> >> --
>> >>
>> >> Regards,
>> >> Srivathsan.K
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


-- 

Regards,
Srivathsan.K
Phone : 9600165206

	[[alternative HTML version deleted]]


From luca.cerone at gmail.com  Wed Jul 20 17:01:29 2016
From: luca.cerone at gmail.com (Luca Cerone)
Date: Wed, 20 Jul 2016 17:01:29 +0200
Subject: [R] Concatenate two lists replacing elements with the same name.
In-Reply-To: <CAP01uR=AnHTXhY4psSbi_ju38EH4nzoBzz5oE5RkiqAJniRGqw@mail.gmail.com>
References: <CAFnz2--RTpYcjCLtx1ySC4nG1QRvkLs=rvkY4U6LbhsFt8nEBA@mail.gmail.com>
	<CAP01uR=AnHTXhY4psSbi_ju38EH4nzoBzz5oE5RkiqAJniRGqw@mail.gmail.com>
Message-ID: <CAFnz2-9nQ4_TR5kUKnxsiz37s8QhQbJAt7LTHmVTsx4kRb-6Dg@mail.gmail.com>

thanks a lot for the help!

On Tue, Jul 19, 2016 at 7:20 PM, Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
> Try this:
>
>     Reduce(modifyList, list(x, y, z))
>
> On Tue, Jul 19, 2016 at 12:34 PM, Luca Cerone <luca.cerone at gmail.com> wrote:
>> Dear all,
>> I would like to know if there is a function to concatenate two lists
>> while replacing elements with the same name.
>>
>> For example:
>>
>> x <- list(a=1,b=2,c=3)
>> y <- list( b=4, d=5)
>> z <- list(a = 6, b = 8, e= 7)
>>
>> I am looking for a function "concatfun" so that
>>
>> u <- concatfun(x,y,z)
>>
>> returns:
>>
>> u$a=6
>> u$b=8
>> u$c=3
>> u$d=5
>> u$e=7
>>
>> I.e. it combines the 3 lists, but when names have the same value it
>> keeps the most recent one.
>>
>> Does such a function exists?
>>
>> Thanks for the help,
>>
>> Cheers,
>> Luca
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com


From 538280 at gmail.com  Wed Jul 20 17:39:41 2016
From: 538280 at gmail.com (Greg Snow)
Date: Wed, 20 Jul 2016 09:39:41 -0600
Subject: [R] txtProgressBar()
In-Reply-To: <43c8e65f-d535-5b96-c7c1-080517d648fd@univ-reims.fr>
References: <43c8e65f-d535-5b96-c7c1-080517d648fd@univ-reims.fr>
Message-ID: <CAFEqCdyLx0H-G8Uoh1c_KYCiN2tB3SeA7o0xrW_PGJmkceMcAw@mail.gmail.com>

You need to figure out how to tell txtProgressBar what the progress is.

One simple option would be that if you are installing 10 packages,
then create the bar with a range of values from 0 to 10 and initialize
it at 0, then after the first package installs update it to show 1,
after the 2nd installs update it to show 2, etc. until all 10 are
installed.

This is the simplest from the programming side, but the packages may
take different amounts of time to install.  If you have a feel for how
long they take to install (relative to each other) then you can
incorporate this with a percentage, e.g. after the 1st package
installs you may set the bar to 28%, after the second installs you may
then update it to 31%, etc. with the jumps proportional to expected
time to install.


On Wed, Jul 20, 2016 at 2:00 AM, Ivan Calandra
<ivan.calandra at univ-reims.fr> wrote:
> Dear useRs,
>
> In a script that will be source()d, I want to install the uninstalled
> packages and follow the progression with a bar. So I looked at
> txtProgressBar() but I cannot figure out how to use it to show the
> progression of the installation.
>
> All the examples I have found just display the progress of... the progress
> bar itself ?
>
> Any idea?
>
> Thanks in advance,
> Ivan
>
> --
> Ivan Calandra, PhD
> Scientific Mediator
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> --
> https://www.researchgate.net/profile/Ivan_Calandra
> https://publons.com/author/705639/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From aimfrendio at gmail.com  Tue Jul 19 23:37:18 2016
From: aimfrendio at gmail.com (Michael Young)
Date: Tue, 19 Jul 2016 14:37:18 -0700
Subject: [R] pairs: adjusting margins and labeling axes
In-Reply-To: <CAFEqCdxCBqg-xjOjj0bshspnSPCok9fCjpGrmPdq1hOPQP4KiA@mail.gmail.com>
References: <CAGk=u4XugCx1JOXSaAck=DDh=cVouQaea6b59Jex45UJp8oUUw@mail.gmail.com>
	<CAFEqCdxCBqg-xjOjj0bshspnSPCok9fCjpGrmPdq1hOPQP4KiA@mail.gmail.com>
Message-ID: <CACGazgsaVXdyAj-QjxoTAPyr0gO6gZ8YFq7HvWQiZcf8hgJ8=Q@mail.gmail.com>

I want to make this as easy as possible.  The extra space could just go
around the plot in the margin area.  I could then use a cropping tool to
paste the plot into Excel or Word.

I'm not opposed to using another package, but I'd need some kind of
pre-existing code to tinker with.

On Tue, Jul 19, 2016 at 11:16 AM, Greg Snow <538280 at gmail.com> wrote:

> If you want square plots on a rectangular plotting region, then where
> do you want the extra space to go?
>
> One option would be to add outer margins to use up the extra space.
> The calculations to figure out exactly how much space to put in the
> outer margins will probably not be trivial.
>
> Another option would be to not use `pairs`, but use the `layout`
> function directly and loops to do your plots (and use the `respect`
> argument to `layout`).
>
> On Tue, Jul 19, 2016 at 11:29 AM, michael young
> <nutnutnutterson at gmail.com> wrote:
> > The default shape for this correlation scatterplot is rectangle.  I
> changed
> > it to square, but then the x-axis spacing between squares are off.  Is
> > there an easy way to change x-axis spacing between squares to that of the
> > y-axis spacing size?
> >
> > I decided to hide the name values of the diagonal squares.  I want them
> > along the x and y axis instead, outside of the fixed number scale I have.
> > I haven't seen any online example of 'pairs' with this and all my
> searches
> > have yielded nothing.  Any ideas?  Thanks
> >
> > par(pty="s")
> > panel.cor <- function(x, y, digits = 2, prefix="", cex.cor, ...)
> > {
> >     usr <- par("usr"); on.exit(par(usr))
> >     par(usr = c(0, 1, 0, 1),xlog=FALSE,ylog=FALSE)
> >     # correlation coefficient
> >     r <- cor(x, y)
> >     txt <- format(c(r, 0.123456789), digits = digits)[1]
> >     txt <- paste("r= ", txt, sep = "")
> >     if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
> >     text(0.5, 0.6, txt, cex=cex.cor * r)
> >
> >     # p-value calculation
> >     p <- cor.test(x, y)$p.value
> >     txt2 <- format(c(p, 0.123456789), digits = digits)[1]
> >     txt2 <- paste("p= ", txt2, sep = "")
> >     if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
> >     text(0.5, 0.4, txt2)
> > }
> >
> > pairs(iris, upper.panel = panel.cor,xlim=c(0.1,100000),
> > ylim=c(0.1,100000),log="xy",text.panel = NULL,pch=".")
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>

	[[alternative HTML version deleted]]


From Michael.Power at newcastle.ac.uk  Wed Jul 20 07:42:39 2016
From: Michael.Power at newcastle.ac.uk (Michael Power)
Date: Wed, 20 Jul 2016 05:42:39 +0000
Subject: [R] shinyApp.io error message "
Message-ID: <DB6PR0701MB2199CFB594E5CD612D5C95E5A0080@DB6PR0701MB2199.eurprd07.prod.outlook.com>

Hi

I am new to this list and quite new to R and Shiny, so am likely to expose my ignorance about things I should know.

My first ShinyApp runs as expected under RStudio, but when I publish it using ShinyApp.io it gives on startup the error message:

   "plot.new has not been called yet", even when I do call plot.new.


Earlier versions (which I have not kept) did work as expected, and I do not know what I am doing differently now.

The R code can be viewed here:

	https://drive.google.com/folderview?id=0B2pp1x41KL5XZUVmMFAydUxMemM&usp=sharing 

Any advice would be much appreciated.

Thanks

Michael


From abdoulayesar at gmail.com  Wed Jul 20 11:00:29 2016
From: abdoulayesar at gmail.com (Abdoulaye SARR)
Date: Wed, 20 Jul 2016 09:00:29 +0000
Subject: [R] probelm with xlab ylab and xaxp barplot
Message-ID: <C7042838-33C6-466B-8978-217A2E2320DC@gmail.com>

I have the color of my bar plot displayed correctly but don?t have xlab,  ylab  and xaxp don?t show up.

here is example of yearly data (25 years 1981-2005)
> head(z1)
[1] -0.1001726  0.2014272 -0.8556950  0.1920669 -0.8013520  1.3324949

code to display values

par(mar=rep(2,4))
op <- par(oma=c(5,7,1,1))
par(mfrow=c(4,2))

line = 3

barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei", xaxp=c(181,2005,1), col=ifelse(z1>0,"green","brown ?))

hoe help on this issue

Fipou

From gcchenleidenuniv at gmail.com  Wed Jul 20 15:22:52 2016
From: gcchenleidenuniv at gmail.com (Guangchao Chen)
Date: Wed, 20 Jul 2016 15:22:52 +0200
Subject: [R] ... distribution based on mean values and quantiles in R?
In-Reply-To: <22413.64970.484218.516189@stat.math.ethz.ch>
References: <201607182311017019593@gmail.com>
	<CA+8X3fV++eAjuMd-r4HJWGx2LN-aikJGfNQcimJPd=qPB1Nr+A@mail.gmail.com>
	<22413.64970.484218.516189@stat.math.ethz.ch>
Message-ID: <CADXiw4y5Wm+Cvi+ppOUN4axWAuGr-Ugc7EJAFUwFy+M9ja8avQ@mail.gmail.com>

Dear Martin,

Thank you very much for your detailed explanation! I will have a look at the
poweRlaw package and see if things can be sorted out!

Best,
Daniel

On 19 July 2016 at 12:15, Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Jim Lemon <drjimlemon at gmail.com>
> >>>>>     on Tue, 19 Jul 2016 16:20:49 +1000 writes:
>
>     > Hi Daniel,
>     > Judging by the numbers you mention, the distribution is either very
>     > skewed or not at all normal. If you look at this:
>
>     > plot(c(0,0.012,0.015,0.057,0.07),c(0,0.05,0.4,0.05,0),type="b")
>
>     > you will see the general shape of whatever distribution produced
> these
>     > summary statistics. Did the paper give any hints as to what the model
>     > distribution might be?
>
> Yes, that's the correct question:  At first, it's not about
> plotting, about *fitting* a distribution with the desired
> properties, and then you can easily visualize it.
>
> So, what were the data?  If they are 'amounts' of any kind, they
> are necessarily non-negative often always positive, and hence
> --- according to John W Tukey --- should be analyzed after
> taking logs (Tukey's "first aid transformation" for *any*
> amounts data).
>
> Taking logs, and analyzing means to consider a normal
> ("Gaussian") distribution for the log(<data>)  which is
> equivalent to fitting a  lognormal distribution -- R functions
> [dpqrr]lnorm()
> to the original data.  I'd strongly recommend doing that.
>
> And I did so, finding out, however that if indeed it is the
> *mean* and the 15% and 95% quantiles,  a log normal is not
> fitting.  Here is the reproducible R code .. with lots of
> comments :
>
>
> ##
> ## MM  strongly recommends to fit a  log-normal distribution .. however it
> does *not* fit
>
> ## The "data statistics"
> qlN <- c(q15 = 0.012, q85 = 0.057) # Quantiles original scale
> mlN <- 0.015
>
> (qn <- log(qlN))                  # Quantiles log scale [assumed normal
> (Gaussian) here]
> ## as the Gaussian is symmetri, the mid value of the two quantiles is the
> mean and median :
> (mu <- mean(qn))   # -3.644
> (medlN <- exp(mu)) # 0.02615
> ## an estimate for the median(.)  -- but  it is *larger* than the mean =
> 0.015 above !
> ## ===> Log-normal does *NOT* fit well :
>
> ## From the help page, we learn that
> ##        E[lN] = exp(mu + 1/2 sigma^2)    {and it is trivial that}
> ##   median[lN] = exp(mu)
> ## where here, our medLn is a (moment) estimate of median[lN]
>
> ## If the number were different, this would solve the problem :
> ## Consequently, a (moment / plugin) estimate for sigma is
> (e12sig <- mlN / medlN) ## ~= exp( 1/2 sigma^2)
> (sig2 <- 2*log(e12sig)) ## ~=  sigma^2  [--- is *NEGATIVE* (<==>
> 'est.median' > mean !)]
> (sig <- sqrt(sig2))     ## ~=  sigma    [here of course 'NaN' with a
> warning !]
>
>
> My conclusion would be that other distributions (than the
> log-normal; the normal  is out of question !!) have to be
> considered, if you want to be serious about the topic.
>
> Maybe the poweRlaw package (https://cloud.r-project.org/package=poweRlaw)
> may help you (it has 4 vignettes, the last being a nice JSS publication).
>
> The above is a "cute" non-standard problem in any case: to fit very skewed
> distributions, given two quantiles and the mean only, and the
> approach taken by the "poweRlawyers", namely to minimize the KS
> (Kolmogorov-Smirnoff) decrepancy seems a good start to me.
>
> Martin Maechler,
> ETH Zurich
>
>
>
>     > Jim
>
>
>     > On Tue, Jul 19, 2016 at 7:11 AM, gcchenleidenuniv
>     > <gcchenleidenuniv at gmail.com> wrote:
>     >> Hi all,
>     >>
>     >> I need to draw density curves based on some published data. But in
> the article only mean value (0.015 ) and quantiles (Q0.15=0.012 ,
> Q0.85=0.057) were given. So I was thinking if it is possible to plot
> density curves solely based on the mean value and quantiles. The dnorm(x,
> mean, sd, log) function needs the standard deviation which was not
> mentioned, so it can not be used in this situation.
>     >>
>     >> Many thanks!!
>     >> Daniel
>     >> [[alternative HTML version deleted]]
>     >>
>     >> ______________________________________________
>     >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     >> https://stat.ethz.ch/mailman/listinfo/r-help
>     >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>     >> and provide commented, minimal, self-contained, reproducible code.
>
>     > ______________________________________________
>     > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tgs77m at gmail.com  Wed Jul 20 19:01:44 2016
From: tgs77m at gmail.com (Tom Subia)
Date: Wed, 20 Jul 2016 10:01:44 -0700
Subject: [R] Geom_smooth
Message-ID: <CAAkwYerAUSef5sifFSL=SPhcd+PM_45RvooAa80Hcvs3D0xhMw@mail.gmail.com>

Default level = 0.95.
Does this mean +/- 0.025 from estimate?

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Wed Jul 20 19:32:14 2016
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 20 Jul 2016 09:32:14 -0800
Subject: [R] Geom_smooth
In-Reply-To: <CAAkwYerAUSef5sifFSL=SPhcd+PM_45RvooAa80Hcvs3D0xhMw@mail.gmail.com>
Message-ID: <080BC186075.00000E38jrkrideau@inbox.com>

The question could use a bit more information but have a look at http://docs.ggplot2.org/0.9.3.1/stat_smooth.html#.  This may explain it. 

You might also want to have a look at http://adv-r.had.co.nz/Reproducibility.html for some guidelines on asking questions in Rhelp or StackOverflow, etc.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: tgs77m at gmail.com
> Sent: Wed, 20 Jul 2016 10:01:44 -0700
> To: r-help at r-project.org
> Subject: [R] Geom_smooth
> 
> Default level = 0.95.
> Does this mean +/- 0.025 from estimate?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Receive Notifications of Incoming Messages
Easily monitor multiple email accounts & access them with a click.
Visit http://www.inbox.com/notifier and check it out!


From ruipbarradas at sapo.pt  Wed Jul 20 19:33:45 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Wed, 20 Jul 2016 18:33:45 +0100
Subject: [R] Geom_smooth
In-Reply-To: <CAAkwYerAUSef5sifFSL=SPhcd+PM_45RvooAa80Hcvs3D0xhMw@mail.gmail.com>
Message-ID: <20160720183345.Horde.CAc6eYQOlUS6vCTn9xNjBm-@mail.sapo.pt>

No, it means precisely the opposite.
Google "confidence interval", please.

Rui Barradas
?

Citando Tom Subia <tgs77m at gmail.com>:

> Default level = 0.95.
> Does this mean +/- 0.025 from estimate?
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Wed Jul 20 19:37:01 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 20 Jul 2016 10:37:01 -0700
Subject: [R] Geom_smooth
In-Reply-To: <CAAkwYerAUSef5sifFSL=SPhcd+PM_45RvooAa80Hcvs3D0xhMw@mail.gmail.com>
References: <CAAkwYerAUSef5sifFSL=SPhcd+PM_45RvooAa80Hcvs3D0xhMw@mail.gmail.com>
Message-ID: <0AE8B04C-D348-484A-9897-584A71C9C555@dcn.davis.ca.us>

No, it refers to confidence level. Refer to your training in statistics for that definition.
-- 
Sent from my phone. Please excuse my brevity.

On July 20, 2016 10:01:44 AM PDT, Tom Subia <tgs77m at gmail.com> wrote:
>Default level = 0.95.
>Does this mean +/- 0.025 from estimate?
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From sacios at hotmail.it  Wed Jul 20 19:20:16 2016
From: sacios at hotmail.it (Sachin Kuruvithadam)
Date: Wed, 20 Jul 2016 17:20:16 +0000
Subject: [R] System of equations with unknowns in R
Message-ID: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>



I have a 3x3 matrix Omega whose elements are unknown, and a defined 3x1 parameter vector alpha. I want to define a 3x1 vector delta whose elements are still unknown but depend on alpha and Omega as shown in this image (sorry, but when I write in Latex format it doesn't appear formatted in my preview):

https://www.dropbox.com/home/Public?preview=model.png

The term in the brackets simplifies into a number K, so I wrote this function:

Alpha=c(-0.248,1.092,-0.518)
K=function(gamma1,gamma2,gamma3,gamma12,gamma23,gamma13){
    (1+Alpha[1]*(Alpha[1]+Alpha[2]*gamma12/(gamma1*gamma2)+Alpha[3]*gamma13/(gamma1*gamma3)))^(-1/2)
    }

gamma1, gamma2, gamma3 are the elements in the diagonal of the 3x3 matrix Omega, whereas gamma12, gamma13, gamma23 are the off-diagonal elements (each elements repeats itself twice, e.g. gamma12=gamma21). So, by putting 6 arbitrary values in K() I get the scalar. So far so clear.

The rest I'm not sure about. I want R to return me a vector delta defined as shown above. How can I write a function that would perform this algebraic calculation, and return a 3x1 vector delta whose elements are the same unknowns as in Omega, but shifted/multiplied by the numbers in alpha?


	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Wed Jul 20 20:03:56 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 20 Jul 2016 13:03:56 -0500
Subject: [R] probelm with xlab ylab and xaxp barplot
In-Reply-To: <C7042838-33C6-466B-8978-217A2E2320DC@gmail.com>
References: <C7042838-33C6-466B-8978-217A2E2320DC@gmail.com>
Message-ID: <779771D7-EBEF-47EA-BE2C-4FF822E5D2DA@me.com>


> On Jul 20, 2016, at 4:00 AM, Abdoulaye SARR <abdoulayesar at gmail.com> wrote:
> 
> I have the color of my bar plot displayed correctly but don?t have xlab,  ylab  and xaxp don?t show up.
> 
> here is example of yearly data (25 years 1981-2005)
>> head(z1)
> [1] -0.1001726  0.2014272 -0.8556950  0.1920669 -0.8013520  1.3324949
> 
> code to display values
> 
> par(mar=rep(2,4))
> op <- par(oma=c(5,7,1,1))
> par(mfrow=c(4,2))
> 
> line = 3
> 
> barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei", xaxp=c(181,2005,1), col=ifelse(z1>0,"green","brown ?))
> 
> hoe help on this issue
> 
> Fipou


Hi,

First, a general comment, which is that barplots are typically good for displaying counts and percentages, not continuous data points or perhaps estimates of means, etc. Your values for z1 above, suggest that you might be better off just plotting the points on the y axis against the years on the x axis. That is, for example:

  plot(1981:2005, z1, col = ifelse(z1 > 0, "green", "brown"), 
       ylab = "spei", xlab = "Years", pch = 19)

presuming that z1 has 25 values.

That being said, some additional notes to hopefully guide you here with barplot():

1. You appear to be wanting to plot a matrix of 8 plots in a 4 row by 2 column matrix. That is fine, but note that changing the graphic parameters associated with the spacing of margins, etc. in a matrix don't always provide a result similar to what you might find in a single plot. I would start by not adjusting par(mar) and par(oma) from their default values to get an idea of what the plot looks like with default settings and then modify from there so that you can see how any adjustments affect the result. You may be adjusting the margins for each plot and the outer margins of the overall matrix in a manner that conflicts.


2. In the case of a vertical barplot, the bars are not centered around integer values on the x axis, as they would be in say a boxplot. In the help for barplot() you will note that the Value section indicates that barplot returns a vector (by default) of the bar midpoints, which can then be used for annotation on the relevant axis. There are examples of the use of this on the barplot help page. Your values for 'xaxp' (which presumably has a typo for 1981, as 181) will not be correct here. Thus:

  MP <- barplot(z1, ...)

where 'MP' will contain the individual bar midpoints and then you can use code like:

  axis(1, at = MP, labels = 1981:2005, ...)

to place annotations below each bar. See ?axis as well as ?mtext for additional information on plot annotations.

Another option is to use the names.arg argument in barplot, to provide the names for each bar:

  barplot(z1, names.arg = 1981:2005, ...)

You will also likely have to adjust the font sizes for text spacing, as the defaults may be too large for all labels to display given the large number of bars. The cex* family of graphic parameters can be helpful. See the arguments in ?barplot and in ?par for more information.

Regards,

Marc Schwartz


From bhh at xs4all.nl  Wed Jul 20 20:07:30 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 20 Jul 2016 20:07:30 +0200
Subject: [R] System of equations with unknowns in R
In-Reply-To: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
References: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
Message-ID: <B0FC4953-B7A7-4C66-9474-EEF8E922BE68@xs4all.nl>


> On 20 Jul 2016, at 19:20, Sachin Kuruvithadam <sacios at hotmail.it> wrote:
> 
> 
> 
> I have a 3x3 matrix Omega whose elements are unknown, and a defined 3x1 parameter vector alpha. I want to define a 3x1 vector delta whose elements are still unknown but depend on alpha and Omega as shown in this image (sorry, but when I write in Latex format it doesn't appear formatted in my preview):
> 
> https://www.dropbox.com/home/Public?preview=model.png
> 

I cannot access your image.

> The term in the brackets simplifies into a number K, so I wrote this function:
> 
> Alpha=c(-0.248,1.092,-0.518)
> K=function(gamma1,gamma2,gamma3,gamma12,gamma23,gamma13){
>    (1+Alpha[1]*(Alpha[1]+Alpha[2]*gamma12/(gamma1*gamma2)+Alpha[3]*gamma13/(gamma1*gamma3)))^(-1/2)
>    }
> 

This functions a scalar not a function

> gamma1, gamma2, gamma3 are the elements in the diagonal of the 3x3 matrix Omega, whereas gamma12, gamma13, gamma23 are the off-diagonal elements (each elements repeats itself twice, e.g. gamma12=gamma21). So, by putting 6 arbitrary values in K() I get the scalar. So far so clear.
> 
> The rest I'm not sure about. I want R to return me a vector delta defined as shown above. How can I write a function that would perform this algebraic calculation, and return a 3x1 vector delta whose elements are the same unknowns as in Omega, but shifted/multiplied by the numbers in alpha?
> 

You can simplify the gamma arguments of your function K by passing the matrix Omega.
Like this

<code>
K1 <- function(Omega,Alpha){
    gamma1 <- Omega[1,1]   
    gamma2 <- Omega[2,2]   
    gamma3 <- Omega[3,3]   
    gamma12 <- Omega[1,2]
    gamma13 <- Omega[1,2]
    gamma23 <- Omega[2,3]
    z <- 1+Alpha[1]*(Alpha[1]+Alpha[2]*gamma12/(gamma1*gamma2)+Alpha[3]*gamma13/(gamma1*gamma3))^(-1/2)
    z
}
</code>

Define as given in your mail

Alpha <- c(-0.248,1.092,-0.518)


Fake a symmetric matrix Omega 

<code>
set.seed(413:
library(Matrix)
x <- Matrix(round(runif(9),2), 3)
x

Omega <- forceSymmetric(x)
Omega

# and run the function

K1(Omega,Alpha)
</code>

The result is a scalar: 0.8149383

Depending on the contents of Omega the result of K1 may be NaN.

You will have to redefine your function if you want it to return a vector.
You have not given enough information to give an answer to your question.

> 	[[alternative HTML version deleted]]
> 


Please do not post in HTML.

Berend Hasselman
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bhh at xs4all.nl  Wed Jul 20 20:13:32 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 20 Jul 2016 20:13:32 +0200
Subject: [R] System of equations with unknowns in R
In-Reply-To: <B0FC4953-B7A7-4C66-9474-EEF8E922BE68@xs4all.nl>
References: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
	<B0FC4953-B7A7-4C66-9474-EEF8E922BE68@xs4all.nl>
Message-ID: <87EA7BAC-BB26-41FB-B59E-68E5770F4592@xs4all.nl>


> On 20 Jul 2016, at 20:07, Berend Hasselman <bhh at xs4all.nl> wrote:
> 
> 
> 
> This functions a scalar not a function

Correction. This should have been

This function returns a scalar not a vector.

Berend Hasselman


From sacios at hotmail.it  Wed Jul 20 20:57:00 2016
From: sacios at hotmail.it (Sachin Kuruvithadam)
Date: Wed, 20 Jul 2016 18:57:00 +0000
Subject: [R] System of equations with unknowns in R
In-Reply-To: <87EA7BAC-BB26-41FB-B59E-68E5770F4592@xs4all.nl>
References: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
	<B0FC4953-B7A7-4C66-9474-EEF8E922BE68@xs4all.nl>,
	<87EA7BAC-BB26-41FB-B59E-68E5770F4592@xs4all.nl>
Message-ID: <DB4PR06MB0685DCC816766194715EE09DB7080@DB4PR06MB0685.eurprd06.prod.outlook.com>



Sorry for the empty email, here's a link to the formula: https://unsee.cc/zasobuge/

K is actually intended to be a number. What I need is a way to find the vector delta as a function of the elements of omega.


________________________________
Da: Berend Hasselman <bhh at xs4all.nl>
Inviato: mercoled? 20 luglio 2016 20.13
A: Sachin Kuruvithadam
Cc: r-help at r-project.org
Oggetto: Re: [R] System of equations with unknowns in R


> On 20 Jul 2016, at 20:07, Berend Hasselman <bhh at xs4all.nl> wrote:
>
>
>
> This functions a scalar not a function

Correction. This should have been

This function returns a scalar not a vector.

Berend Hasselman


	[[alternative HTML version deleted]]


From sacios at hotmail.it  Wed Jul 20 21:17:00 2016
From: sacios at hotmail.it (Sachin Kuruvithadam)
Date: Wed, 20 Jul 2016 19:17:00 +0000
Subject: [R] System of equations with unknowns in R
In-Reply-To: <DB4PR06MB0685DCC816766194715EE09DB7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
References: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
	<B0FC4953-B7A7-4C66-9474-EEF8E922BE68@xs4all.nl>,
	<87EA7BAC-BB26-41FB-B59E-68E5770F4592@xs4all.nl>,
	<DB4PR06MB0685DCC816766194715EE09DB7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
Message-ID: <DB4PR06MB068529C5B35E5E6402910AC2B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>

Sorry, didn't know that the other site only hosted temporarily. This link should be permanent: https://postimg.org/image/gpp3sohip/





________________________________
Da: R-help <r-help-bounces at r-project.org> per conto di Sachin Kuruvithadam <sacios at hotmail.it>
Inviato: mercoled? 20 luglio 2016 20.57
A: Berend Hasselman
Cc: r-help at r-project.org
Oggetto: Re: [R] System of equations with unknowns in R



Sorry for the empty email, here's a link to the formula: https://unsee.cc/zasobuge/

unsee.cc





K is actually intended to be a number. What I need is a way to find the vector delta as a function of the elements of omega.


________________________________
Da: Berend Hasselman <bhh at xs4all.nl>
Inviato: mercoled? 20 luglio 2016 20.13
A: Sachin Kuruvithadam
Cc: r-help at r-project.org
Oggetto: Re: [R] System of equations with unknowns in R


> On 20 Jul 2016, at 20:07, Berend Hasselman <bhh at xs4all.nl> wrote:
>
>
>
> This functions a scalar not a function

Correction. This should have been

This function returns a scalar not a vector.

Berend Hasselman


        [[alternative HTML version deleted]]


	[[alternative HTML version deleted]]


From bhh at xs4all.nl  Wed Jul 20 21:17:44 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 20 Jul 2016 21:17:44 +0200
Subject: [R] System of equations with unknowns in R
In-Reply-To: <DB4PR06MB0685DCC816766194715EE09DB7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
References: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
	<B0FC4953-B7A7-4C66-9474-EEF8E922BE68@xs4all.nl>
	<87EA7BAC-BB26-41FB-B59E-68E5770F4592@xs4all.nl>
	<DB4PR06MB0685DCC816766194715EE09DB7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
Message-ID: <5155551B-1F60-425F-A416-2D3483C78C3F@xs4all.nl>


> On 20 Jul 2016, at 20:57, Sachin Kuruvithadam <sacios at hotmail.it> wrote:
> 
> 
> 
> Sorry for the empty email, here's a link to the formula: https://unsee.cc/zasobuge/
> 

Link gives eror message:

Unfortunately the images were deleted as requested.

> K is actually intended to be a number. What I need is a way to find the vector delta as a function of the elements of omega. 
> 

This make your problem incomprehensible (at least to me).
I really haven't got a clue of what you want.

Berend Hasselman
> 
> Da: Berend Hasselman <bhh at xs4all.nl>
> Inviato: mercoled? 20 luglio 2016 20.13
> A: Sachin Kuruvithadam
> Cc: r-help at r-project.org
> Oggetto: Re: [R] System of equations with unknowns in R
>  
> 
> > On 20 Jul 2016, at 20:07, Berend Hasselman <bhh at xs4all.nl> wrote:
> > 
> > 
> > 
> > This functions a scalar not a function
> 
> Correction. This should have been
> 
> This function returns a scalar not a vector.
> 
> Berend Hasselman


From bhh at xs4all.nl  Wed Jul 20 21:37:22 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 20 Jul 2016 21:37:22 +0200
Subject: [R] System of equations with unknowns in R
In-Reply-To: <DB4PR06MB068529C5B35E5E6402910AC2B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
References: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
	<B0FC4953-B7A7-4C66-9474-EEF8E922BE68@xs4all.nl>
	<87EA7BAC-BB26-41FB-B59E-68E5770F4592@xs4all.nl>
	<DB4PR06MB0685DCC816766194715EE09DB7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
	<DB4PR06MB068529C5B35E5E6402910AC2B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
Message-ID: <432C8DCF-53E9-4BFD-A936-027981715A72@xs4all.nl>


You don't need the function K.
From the image you need this:


Well at last.
You don't need the function K.


From the image you only need  this:

G <- as.numeric(1+t(Alpha) %*% Omega %*% Alpha)
G
delta <- (Omega %*% Alpha) / sqrt(G)
delta


Berend Hasselman

> On 20 Jul 2016, at 21:17, Sachin Kuruvithadam <sacios at hotmail.it> wrote:
> 
> Sorry, didn't know that the other site only hosted temporarily. This link should be permanent: https://postimg.org/image/gpp3sohip/
> 
> 
> 
> 
> 
> Da: R-help <r-help-bounces at r-project.org> per conto di Sachin Kuruvithadam <sacios at hotmail.it>
> Inviato: mercoled? 20 luglio 2016 20.57
> A: Berend Hasselman
> Cc: r-help at r-project.org
> Oggetto: Re: [R] System of equations with unknowns in R
>  
> 
> 
> Sorry for the empty email, here's a link to the formula: https://unsee.cc/zasobuge/
> Unsee &mdash; Free online private photo sharing
> unsee.cc
> Free online private photo sharing
> 
> 
> 
> K is actually intended to be a number. What I need is a way to find the vector delta as a function of the elements of omega.
> 
> 
> ________________________________
> Da: Berend Hasselman <bhh at xs4all.nl>
> Inviato: mercoled? 20 luglio 2016 20.13
> A: Sachin Kuruvithadam
> Cc: r-help at r-project.org
> Oggetto: Re: [R] System of equations with unknowns in R
> 
> 
> > On 20 Jul 2016, at 20:07, Berend Hasselman <bhh at xs4all.nl> wrote:
> >
> >
> >
> > This functions a scalar not a function
> 
> Correction. This should have been
> 
> This function returns a scalar not a vector.
> 
> Berend Hasselman
> 
> 
>         [[alternative HTML version deleted]]
> 


From bjpmodi2016 at gmail.com  Wed Jul 20 21:56:21 2016
From: bjpmodi2016 at gmail.com (Narendra Modi)
Date: Wed, 20 Jul 2016 14:56:21 -0500
Subject: [R] Upper bound vector in Nloptr
Message-ID: <CAPq=xQA3NMPw6scMSD77xvhJqejesiOqXKAiHSd4xRd3iEJ=FA@mail.gmail.com>

Hello Folks,

I am using "nloptr" optimizer in my program as below.

my.data.var <- c(10,0.25,0.25,0.25,0.25,0.25,
                 10,0.25,0.25,0.25,0.25,0.25,
                 10,0.25,0.25,0.25,0.25,0.25,
                 10,0.25,0.25,0.25,0.25,0.25)

#Option for non-linear optimization algorithm NLOPTR
opts = list("algorithm"="NLOPT_LN_COBYLA",
            "xtol_rel"=1.0e-6, "maxeval"= 10000)
lb = vector("numeric",length= length(my.data.var))

#NLOPT_LN_COBYLA

result <- nloptr(my.data.var,eval_f = Error.func,lb=lb,
                 ub =
c(Inf,1,1,1,1,1,Inf,1,1,1,1,1,Inf,1,1,1,1,1,Inf,1,1,1,1,1),eval_g_ineq=constraint.func,
                 opts = opts)

As you can see, I have to explicitly define the "upper bound: ub" as a
long vector in this scenario.

In another case, the number of variables "my.data.var" (24 in the
above example) can go up to 100 and hence. the "upper bound" needs to
be modified. I would like to avoid writing that explicilty in the
"ub".

How can I dynamically do it ?  I am out of ideas here.

Thanks for the help.


From dcarlson at tamu.edu  Wed Jul 20 22:14:37 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 20 Jul 2016 20:14:37 +0000
Subject: [R] How to plot density distribution based on mean values
	and	quantiles in R?
In-Reply-To: <201607182311017019593@gmail.com>
References: <201607182311017019593@gmail.com>
Message-ID: <d1353a927f9e4e8583c1b022d082e7fa@exch-2p-mbx-t2.ads.tamu.edu>

You can estimate the standard deviation from the quantiles, but based on your example, the data will not be accurately modeled with a normal distribution.

The quantiles .15 and .85 should be 1.036 standard deviations from the mean:

> qnorm(c(.15, .85))
[1] -1.036433  1.036433

However the .15-quantile is .003 smaller than the mean and the .85-quantile is .042 larger than the mean so the data are very strongly right-skewed:

> c(.015-.012, .057-.015)
[1] 0.003 0.042

> round(c(.015-.012, .057-.015)/qnorm(.85), 4)
[1] 0.0029 0.0405

So the standard deviation estimates are .0029 on the left and .0405 on the right. 
Even using a log-normal distribution does not help much.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of gcchenleidenuniv
Sent: Monday, July 18, 2016 4:11 PM
To: r-help
Subject: [R] How to plot density distribution based on mean values and quantiles in R?

Hi all,

I need to draw density curves based on some published data. But in the article only mean value (0.015 ) and quantiles (Q0.15=0.012 , Q0.85=0.057) were given. So I was thinking if it is possible to plot density curves solely based on the mean value and quantiles. The dnorm(x, mean, sd, log) function needs the standard deviation which was not mentioned, so it can not be used in this situation.

Many thanks!!
Daniel
	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From chocold12 at gmail.com  Wed Jul 20 22:31:26 2016
From: chocold12 at gmail.com (lily li)
Date: Wed, 20 Jul 2016 14:31:26 -0600
Subject: [R] how to expand the dataframe
Message-ID: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>

Hi R users,

I have a dataframe, where there is a column 'time' represents time series
but is not complete. How to expand the dataframe so this column will become
complete, where other columns with the newly added rows have NA values?
Thanks.

df
A     B     C     time
10    5     3.3   1990-01-01
11    5      4     1990-02-07
12    4     3      1990-02-14
...

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul 20 20:15:41 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 20 Jul 2016 11:15:41 -0700
Subject: [R] System of equations with unknowns in R
In-Reply-To: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
References: <DB4PR06MB06858B2935CAE8A8A84F5DA8B7080@DB4PR06MB0685.eurprd06.prod.outlook.com>
Message-ID: <9DD542B9-454B-4B03-A889-BA0DAEEE29C2@comcast.net>


> On Jul 20, 2016, at 10:20 AM, Sachin Kuruvithadam <sacios at hotmail.it> wrote:
> 
> 
> 
> I have a 3x3 matrix Omega whose elements are unknown, and a defined 3x1 parameter vector alpha. I want to define a 3x1 vector delta whose elements are still unknown but depend on alpha and Omega as shown in this image (sorry, but when I write in Latex format it doesn't appear formatted in my preview):
> 
> https://www.dropbox.com/home/Public?preview=model.png
> 
> The term in the brackets simplifies into a number K, so I wrote this function:
> 
> Alpha=c(-0.248,1.092,-0.518)
> K=function(gamma1,gamma2,gamma3,gamma12,gamma23,gamma13){
>    (1+Alpha[1]*(Alpha[1]+Alpha[2]*gamma12/(gamma1*gamma2)+Alpha[3]*gamma13/(gamma1*gamma3)))^(-1/2)
>    }
> 
> gamma1, gamma2, gamma3 are the elements in the diagonal of the 3x3 matrix Omega, whereas gamma12, gamma13, gamma23 are the off-diagonal elements (each elements repeats itself twice, e.g. gamma12=gamma21). So, by putting 6 arbitrary values in K() I get the scalar. So far so clear.
> 
> The rest I'm not sure about. I want R to return me a vector delta defined as shown above.

No. Not clear at all. What vector delta?

> How can I write a function that would perform this algebraic calculation, and return a 3x1 vector delta whose elements are the same unknowns as in Omega, but shifted/multiplied by the numbers in alpha?

That expression inside the K()-function would return a single value. I worry that you are expecting the Alpha's to retain their distinct bases as would happen if they were true 3-vectors.
> 
> 
> 	[[alternative HTML version deleted]]

Crossposting to Rhelp and StackOverflow at the same time is deprecated. This is the case on  Rhelp, anyway. As far as I can tell SO has no such advice. Please read the Posting Guide where you should see that HTML formatted postings are discouraged and that most attachments are scrubbed by the email server.  There has already been a comment-response on SO where another person reported difficulty accessing that image. You would be more likely to learn R coding if you used R code to define an Omega matrix with "dummy" values for testing and then tried passing it just as a square matrix to a function.

______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.ca.us  Thu Jul 21 01:19:58 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 20 Jul 2016 16:19:58 -0700
Subject: [R] how to expand the dataframe
In-Reply-To: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
References: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
Message-ID: <26C968E6-B28B-4F12-A39F-15802B67D4D8@dcn.davis.ca.us>

Look at the zoo or data.table packages. 
-- 
Sent from my phone. Please excuse my brevity.

On July 20, 2016 1:31:26 PM PDT, lily li <chocold12 at gmail.com> wrote:
>Hi R users,
>
>I have a dataframe, where there is a column 'time' represents time
>series
>but is not complete. How to expand the dataframe so this column will
>become
>complete, where other columns with the newly added rows have NA values?
>Thanks.
>
>df
>A     B     C     time
>10    5     3.3   1990-01-01
>11    5      4     1990-02-07
>12    4     3      1990-02-14
>...
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From wjm1 at caa.columbia.edu  Thu Jul 21 01:57:42 2016
From: wjm1 at caa.columbia.edu (William Michels)
Date: Wed, 20 Jul 2016 16:57:42 -0700
Subject: [R] pairs: adjusting margins and labeling axes
In-Reply-To: <CACGazgsaVXdyAj-QjxoTAPyr0gO6gZ8YFq7HvWQiZcf8hgJ8=Q@mail.gmail.com>
References: <CAGk=u4XugCx1JOXSaAck=DDh=cVouQaea6b59Jex45UJp8oUUw@mail.gmail.com>
	<CAFEqCdxCBqg-xjOjj0bshspnSPCok9fCjpGrmPdq1hOPQP4KiA@mail.gmail.com>
	<CACGazgsaVXdyAj-QjxoTAPyr0gO6gZ8YFq7HvWQiZcf8hgJ8=Q@mail.gmail.com>
Message-ID: <CAA99HCwPckZoKivOWMLnkpxvMqaoOXYQ4NOs_+2AJQpN7vSUow@mail.gmail.com>

Hi Michael, is this the direction you'd like to go (simplified)?

?pairs
pairs(iris, log="xy", asp=1, gap=0.1)

--Bill.


On Tue, Jul 19, 2016 at 2:37 PM, Michael Young <aimfrendio at gmail.com> wrote:

> I want to make this as easy as possible.  The extra space could just go
> around the plot in the margin area.  I could then use a cropping tool to
> paste the plot into Excel or Word.
>
> I'm not opposed to using another package, but I'd need some kind of
> pre-existing code to tinker with.
>
> On Tue, Jul 19, 2016 at 11:16 AM, Greg Snow <538280 at gmail.com> wrote:
>
> > If you want square plots on a rectangular plotting region, then where
> > do you want the extra space to go?
> >
> > One option would be to add outer margins to use up the extra space.
> > The calculations to figure out exactly how much space to put in the
> > outer margins will probably not be trivial.
> >
> > Another option would be to not use `pairs`, but use the `layout`
> > function directly and loops to do your plots (and use the `respect`
> > argument to `layout`).
> >
> > On Tue, Jul 19, 2016 at 11:29 AM, michael young
> > <nutnutnutterson at gmail.com> wrote:
> > > The default shape for this correlation scatterplot is rectangle.  I
> > changed
> > > it to square, but then the x-axis spacing between squares are off.  Is
> > > there an easy way to change x-axis spacing between squares to that of
> the
> > > y-axis spacing size?
> > >
> > > I decided to hide the name values of the diagonal squares.  I want them
> > > along the x and y axis instead, outside of the fixed number scale I
> have.
> > > I haven't seen any online example of 'pairs' with this and all my
> > searches
> > > have yielded nothing.  Any ideas?  Thanks
> > >
> > > par(pty="s")
> > > panel.cor <- function(x, y, digits = 2, prefix="", cex.cor, ...)
> > > {
> > >     usr <- par("usr"); on.exit(par(usr))
> > >     par(usr = c(0, 1, 0, 1),xlog=FALSE,ylog=FALSE)
> > >     # correlation coefficient
> > >     r <- cor(x, y)
> > >     txt <- format(c(r, 0.123456789), digits = digits)[1]
> > >     txt <- paste("r= ", txt, sep = "")
> > >     if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
> > >     text(0.5, 0.6, txt, cex=cex.cor * r)
> > >
> > >     # p-value calculation
> > >     p <- cor.test(x, y)$p.value
> > >     txt2 <- format(c(p, 0.123456789), digits = digits)[1]
> > >     txt2 <- paste("p= ", txt2, sep = "")
> > >     if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
> > >     text(0.5, 0.4, txt2)
> > > }
> > >
> > > pairs(iris, upper.panel = panel.cor,xlim=c(0.1,100000),
> > > ylim=c(0.1,100000),log="xy",text.panel = NULL,pch=".")
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > 538280 at gmail.com
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From syen04 at gmail.com  Thu Jul 21 02:24:22 2016
From: syen04 at gmail.com (Steven Yen)
Date: Wed, 20 Jul 2016 20:24:22 -0400
Subject: [R] Build command in library(devtools)
In-Reply-To: <415f7fbc-8039-635c-a1db-941e27b0fd1b@gmail.com>
References: <291e222a-a541-9b79-459c-bcdcd7f8e66b@gmail.com>
	<CAAJSdjhC6V-RE6pSg41+1o6LKiPibWreszXQLAQiC1Qfy2DWsQ@mail.gmail.com>
	<196aed4a-f7a8-f898-bff4-84c548119d67@gmail.com>
	<2d7ab661-863a-0bef-e521-f02dfe166561@gmail.com>
	<d8a3da84-d636-b581-e594-cec257a32e8d@gmail.com>
	<b0c3f6a6-26b4-65f5-8794-cd7f0bcb3ca7@gmail.com>
	<415f7fbc-8039-635c-a1db-941e27b0fd1b@gmail.com>
Message-ID: <ac656da0-e208-83bb-6e68-e8e602158c2a@gmail.com>

Here is what I found. I had to go back to as early as R 3.0.3 (March, 
2014) along with Rtools30.exe that works with that version of R, in 
order for devtools to work right. With other/later version of R, I end 
up building a package with
library(devtools); build("yenlib",binary=F)
with no error message but the package does not run correctly; or with
library(devtools); build("yenlib",binary=T)
which deliver an error that says zip command failed (bevtools calls 
Rtools when binary=T).

Updated versions are good, but what's the use if they do not work for a 
situation like this.

Any help/insight would be appreciated.

On 7/20/2016 10:08 AM, Steven Yen wrote:
> On 7/19/2016 4:38 PM, John McKown wrote:
>> On Tue, Jul 19, 2016 at 3:15 PM, Steven Yen <syen04 at gmail.com
>> <mailto:syen04 at gmail.com>>wrote:
>>
>>     I recently updated my R and RStudio to the latest version and
>> now the
>>     binary option in the "build" command in devtools stops working.
>>
>>     I went around and used the binary=F option which worked by I get
>> the
>>     .tar.gz file instead of the .zip file which I prefer.
>>
>>     Does anyone understand the following error message:
>>
>>     status 127
>>     running 'zip' failed
>>
>>
>> ?I'm not totally sure, but I think that means that R cannot find the
>> "zip" program in order to run it. ?


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Jul 21 02:38:13 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 20 Jul 2016 17:38:13 -0700
Subject: [R] how to expand the dataframe
In-Reply-To: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
References: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
Message-ID: <37BE3680-939B-4AB2-88E3-5C9B4EFE2DD1@comcast.net>


> On Jul 20, 2016, at 1:31 PM, lily li <chocold12 at gmail.com> wrote:
> 
> Hi R users,
> 
> I have a dataframe, where there is a column 'time' represents time series
> but is not complete. How to expand the dataframe so this column will become
> complete, where other columns with the newly added rows have NA values?
> Thanks.
> 
> df
> A     B     C     time
> 10    5     3.3   1990-01-01
> 11    5      4     1990-02-07
> 12    4     3      1990-02-14
> ...

Make a dataframe with a 'time' column using seq.Date and merge that dataframe with your df dataframe.

> 
> 	[[alternative HTML version deleted]]

Really .... isn't it time you learned how to send plain text. You've posted many questions on Rhelp.  It's really not that difficult on gmail. I also have a gmail account and have had no difficulty finding instructions on how to do it.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From h.wickham at gmail.com  Thu Jul 21 03:33:00 2016
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 20 Jul 2016 20:33:00 -0500
Subject: [R] Build command in library(devtools)
In-Reply-To: <ac656da0-e208-83bb-6e68-e8e602158c2a@gmail.com>
References: <291e222a-a541-9b79-459c-bcdcd7f8e66b@gmail.com>
	<CAAJSdjhC6V-RE6pSg41+1o6LKiPibWreszXQLAQiC1Qfy2DWsQ@mail.gmail.com>
	<196aed4a-f7a8-f898-bff4-84c548119d67@gmail.com>
	<2d7ab661-863a-0bef-e521-f02dfe166561@gmail.com>
	<d8a3da84-d636-b581-e594-cec257a32e8d@gmail.com>
	<b0c3f6a6-26b4-65f5-8794-cd7f0bcb3ca7@gmail.com>
	<415f7fbc-8039-635c-a1db-941e27b0fd1b@gmail.com>
	<ac656da0-e208-83bb-6e68-e8e602158c2a@gmail.com>
Message-ID: <CABdHhvGWR4qvNn7zFuhRh4eHSugFp2dUAKmGNB+f=_-v9MX0XQ@mail.gmail.com>

The first place to start is to make sure you have the latest version of
devtools. If that doesn't work, please file an issue on devtools' GitHub.

Hadley

On Wednesday, July 20, 2016, Steven Yen <syen04 at gmail.com> wrote:

> Here is what I found. I had to go back to as early as R 3.0.3 (March,
> 2014) along with Rtools30.exe that works with that version of R, in
> order for devtools to work right. With other/later version of R, I end
> up building a package with
> library(devtools); build("yenlib",binary=F)
> with no error message but the package does not run correctly; or with
> library(devtools); build("yenlib",binary=T)
> which deliver an error that says zip command failed (bevtools calls
> Rtools when binary=T).
>
> Updated versions are good, but what's the use if they do not work for a
> situation like this.
>
> Any help/insight would be appreciated.
>
> On 7/20/2016 10:08 AM, Steven Yen wrote:
> > On 7/19/2016 4:38 PM, John McKown wrote:
> >> On Tue, Jul 19, 2016 at 3:15 PM, Steven Yen <syen04 at gmail.com
> <javascript:;>
> >> <mailto:syen04 at gmail.com <javascript:;>>>wrote:
> >>
> >>     I recently updated my R and RStudio to the latest version and
> >> now the
> >>     binary option in the "build" command in devtools stops working.
> >>
> >>     I went around and used the binary=F option which worked by I get
> >> the
> >>     .tar.gz file instead of the .zip file which I prefer.
> >>
> >>     Does anyone understand the following error message:
> >>
> >>     status 127
> >>     running 'zip' failed
> >>
> >>
> >> ?I'm not totally sure, but I think that means that R cannot find the
> >> "zip" program in order to run it. ?
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://hadley.nz

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Thu Jul 21 05:26:08 2016
From: chocold12 at gmail.com (lily li)
Date: Wed, 20 Jul 2016 21:26:08 -0600
Subject: [R] how to expand the dataframe
In-Reply-To: <37BE3680-939B-4AB2-88E3-5C9B4EFE2DD1@comcast.net>
References: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
	<37BE3680-939B-4AB2-88E3-5C9B4EFE2DD1@comcast.net>
Message-ID: <CAN5afy8-Y4_ZSN55y1m2hqXN4hWhNbqfxKusVD7Wxeh+9Yq8tA@mail.gmail.com>

Yes, I tried to create a dataframe and merge it with the shortened
dataframe. The resulting dataframe goes with the short one and truncates
the complete date column, so it does not work.

On Wed, Jul 20, 2016 at 6:38 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jul 20, 2016, at 1:31 PM, lily li <chocold12 at gmail.com> wrote:
> >
> > Hi R users,
> >
> > I have a dataframe, where there is a column 'time' represents time series
> > but is not complete. How to expand the dataframe so this column will
> become
> > complete, where other columns with the newly added rows have NA values?
> > Thanks.
> >
> > df
> > A     B     C     time
> > 10    5     3.3 1990-01-01
> > 11    5      4     1990-02-07
> > 12    4     3      1990-02-14
> > ...
>
> Make a dataframe with a 'time' column using seq.Date and merge that
> dataframe with your df dataframe.
>
> >
> >       [[alternative HTML version deleted]]
>
> Really .... isn't it time you learned how to send plain text. You've
> posted many questions on Rhelp.  It's really not that difficult on gmail. I
> also have a gmail account and have had no difficulty finding instructions
> on how to do it.
>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Jul 21 06:39:58 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 20 Jul 2016 21:39:58 -0700
Subject: [R] how to expand the dataframe
In-Reply-To: <CAN5afy8-Y4_ZSN55y1m2hqXN4hWhNbqfxKusVD7Wxeh+9Yq8tA@mail.gmail.com>
References: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
	<37BE3680-939B-4AB2-88E3-5C9B4EFE2DD1@comcast.net>
	<CAN5afy8-Y4_ZSN55y1m2hqXN4hWhNbqfxKusVD7Wxeh+9Yq8tA@mail.gmail.com>
Message-ID: <CAF8bMcaHSeNBybuvOi=bAmArKmXBbbxzCU9a-6WGq946ShuEjA@mail.gmail.com>

You need to show, not tell, what you did to get better answers.  Did you
use the all=TRUE argument to merge()?

> df <-
data.frame(A=c(10,11,12),B=c(5,5,4),C=c(3.3,4,3),time=as.Date(c("1990-01-01","1990-02-07","1990-02-14")))
0
> merge(df, data.frame(time=seq(as.Date("1990-01-01"),
to=as.Date("1990-02-15"), by="days")), all.y=TRUE)
         time  A  B   C
1  1990-01-01 10  5 3.3
2  1990-01-02 NA NA  NA
3  1990-01-03 NA NA  NA
...
36 1990-02-05 NA NA  NA
37 1990-02-06 NA NA  NA
38 1990-02-07 11  5 4.0
39 1990-02-08 NA NA  NA
...


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Jul 20, 2016 at 8:26 PM, lily li <chocold12 at gmail.com> wrote:

> Yes, I tried to create a dataframe and merge it with the shortened
> dataframe. The resulting dataframe goes with the short one and truncates
> the complete date column, so it does not work.
>
> On Wed, Jul 20, 2016 at 6:38 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
>
> >
> > > On Jul 20, 2016, at 1:31 PM, lily li <chocold12 at gmail.com> wrote:
> > >
> > > Hi R users,
> > >
> > > I have a dataframe, where there is a column 'time' represents time
> series
> > > but is not complete. How to expand the dataframe so this column will
> > become
> > > complete, where other columns with the newly added rows have NA values?
> > > Thanks.
> > >
> > > df
> > > A     B     C     time
> > > 10    5     3.3 1990-01-01
> > > 11    5      4     1990-02-07
> > > 12    4     3      1990-02-14
> > > ...
> >
> > Make a dataframe with a 'time' column using seq.Date and merge that
> > dataframe with your df dataframe.
> >
> > >
> > >       [[alternative HTML version deleted]]
> >
> > Really .... isn't it time you learned how to send plain text. You've
> > posted many questions on Rhelp.  It's really not that difficult on
> gmail. I
> > also have a gmail account and have had no difficulty finding instructions
> > on how to do it.
> >
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ivan.calandra at univ-reims.fr  Thu Jul 21 09:23:40 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Thu, 21 Jul 2016 09:23:40 +0200
Subject: [R] txtProgressBar()
In-Reply-To: <CAFEqCdyLx0H-G8Uoh1c_KYCiN2tB3SeA7o0xrW_PGJmkceMcAw@mail.gmail.com>
References: <43c8e65f-d535-5b96-c7c1-080517d648fd@univ-reims.fr>
	<CAFEqCdyLx0H-G8Uoh1c_KYCiN2tB3SeA7o0xrW_PGJmkceMcAw@mail.gmail.com>
Message-ID: <a3d9a025-4a09-f44f-97ea-22d92e85399c@univ-reims.fr>

Thank you Greg,

This is what I figured out... The problem with txtProgressBar() is that 
many packages display some information during installation (even with 
quiet=TRUE), especially the installation of dependencies, so that the 
progress bar is not very useful. So I have tried with tkProgressBar() 
and it seems to work, although it takes some time to initialize.

Ivan

--
Ivan Calandra, PhD
Scientific Mediator
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/

Le 20/07/2016 ? 17:39, Greg Snow a ?crit :
> You need to figure out how to tell txtProgressBar what the progress is.
>
> One simple option would be that if you are installing 10 packages,
> then create the bar with a range of values from 0 to 10 and initialize
> it at 0, then after the first package installs update it to show 1,
> after the 2nd installs update it to show 2, etc. until all 10 are
> installed.
>
> This is the simplest from the programming side, but the packages may
> take different amounts of time to install.  If you have a feel for how
> long they take to install (relative to each other) then you can
> incorporate this with a percentage, e.g. after the 1st package
> installs you may set the bar to 28%, after the second installs you may
> then update it to 31%, etc. with the jumps proportional to expected
> time to install.
>
>
> On Wed, Jul 20, 2016 at 2:00 AM, Ivan Calandra
> <ivan.calandra at univ-reims.fr> wrote:
>> Dear useRs,
>>
>> In a script that will be source()d, I want to install the uninstalled
>> packages and follow the progression with a bar. So I looked at
>> txtProgressBar() but I cannot figure out how to use it to show the
>> progression of the installation.
>>
>> All the examples I have found just display the progress of... the progress
>> bar itself ?
>>
>> Any idea?
>>
>> Thanks in advance,
>> Ivan
>>
>> --
>> Ivan Calandra, PhD
>> Scientific Mediator
>> University of Reims Champagne-Ardenne
>> GEGENAA - EA 3795
>> CREA - 2 esplanade Roland Garros
>> 51100 Reims, France
>> +33(0)3 26 77 36 89
>> ivan.calandra at univ-reims.fr
>> --
>> https://www.researchgate.net/profile/Ivan_Calandra
>> https://publons.com/author/705639/
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From G.Maubach at weinwolf.de  Thu Jul 21 13:23:17 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Thu, 21 Jul 2016 13:23:17 +0200
Subject: [R] R Toolbox (Release 2 of 2016-07-21)
Message-ID: <OFF71EE3F4.250DC639-ONC1257FF7.003ABA33-C1257FF7.003E98AE@lotus.hawesko.de>

Hi All,

I have uploaded a new release of the R Toolbox.

R Toolbox is a collection of simple but useful functions which I developed 
for myself to shorten the develoment process. Currently all functions use 
base R. No other packages are needed. One exception is "t_openxlsx" cause 
this module deals explicitly with the openxlsx package.

It is simple to install the functions. Just copy them to an appropriety 
place on your hard disk and adjust the variable "t_toolbox_location" to 
the place you stored the toolbox in. Running "r_toolbox.R" from that 
location will load all modules.

In addition to new functions (see Release Comparison below) some functions 
were improved. The are called with their package names, e. g. 
openxlsx::read.xlsx() instead of "read.xlsx()". This way confusion with 
functions having the same name but comming from other packages is avoided.

Pleae be aware that I have include some not tested function in this 
release. All modules have a variable "t_status" now, stating the 
development status, e. g. "development", "testing", "release". 

Here is a Releae Comparison:

-- cut --

release_comparison <-
   structure(list(Module = c("r_toolbox.R", "t_adjust_packages.R", 
                                  "t_conventions.r", 
"t_create_variable.R", "t_definitions.R", 
                                  "t_find_originals_and_duplicates.R", 
"t_get_factor_levels.R", 
                                  "t_merge_variables.R", "t_n_miss.R", 
"t_n_valid.R", "t_openxlsx_shortcuts.r", 
                                  "t_rename_variables.R", 
"t_replace_na.R", "t_report_memory.R", 
                                  "t_select_vars_by_type.R"), Release1 = 
c(TRUE, FALSE, FALSE, 
 FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, 
 FALSE, FALSE), Release2 = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, 
                             TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, 
TRUE, TRUE)), .Names = c("Module", 
                       "Release1", "Release2"), row.names = c(NA, 15L), 
class = "data.frame")
edit(release_comparison)

-- cut ---

Release 1 is of 2016-05-31, Releae 2 of 2016-06-21.

You can download the toolbox from

https://sourceforge.net/projects/r-project-utilities/

Kind regards

Georg Maubach


From bhaskar.kolkata at gmail.com  Thu Jul 21 03:00:58 2016
From: bhaskar.kolkata at gmail.com (Bhaskar Mitra)
Date: Wed, 20 Jul 2016 21:00:58 -0400
Subject: [R] Issue with Transform function in R
Message-ID: <CAEGXkYW2qFwL-H0Gjbh2BDHZhgio6r_pFmfTN7FJJUOjWpR9rg@mail.gmail.com>

Hello Everyone,


I am trying to replace the values in the 2nd column (Variable 1)
corresponding to certain dates  (Date)


with NAs as shown below. Both Date and Variable1 are numeric vectors . I am
trying to use the transform function


as shown below but it doesn?t seem to work even though if I am not getting
any error


Any suggestions/help in this regard?

regards,

---------------------------------------------------------------------------------------------------------------------------------------------------------



Df1 <- data.frame(Date, Variable1)




a1 <- transform(Df1, ifelse(Date  > "010301000300 " && Date <
"010501000300", Variable1 ==NA, Variable1))


Original Data frame



     Date                          Variable1

010101000300                     1

010201000300                     2

010301000300                     3

010401000300                     4

010501000300                     5

010601000300                     6

010701000300                     7

 .

.

.

???.




Transformed data frame (i hope to transform)



      Date                       Variable1

010101000300                    1

010201000300                    2

010301000300                   NA

010401000300                   NA

010501000300                    NA

010601000300                    6

010701000300                    7



???.

	[[alternative HTML version deleted]]


From j.munday at uq.edu.au  Thu Jul 21 05:47:23 2016
From: j.munday at uq.edu.au (Judy Munday)
Date: Thu, 21 Jul 2016 03:47:23 +0000
Subject: [R] Help installing Rcmdr on Mac
Message-ID: <1469072843698.33223@uq.edu.au>

I am having trouble installing R Commander on a Mac (OS X 10.7.5).  I would really appreciate some help!


I receive the following message after following the instructions to install Rcmdr via Package Installer:


Warning: dependency 'rglwidget' is not available
trying URL 'http://cran.ms.unimelb.edu.au/bin/macosx/contrib/3.2/Rcmdr_2.2-5.tgz'
Content type 'application/x-gzip' length 5456133 bytes (5.2 MB)
==================================================
downloaded 5.2 MB

? If someone could please help me, I would be very grateful.   It is quite urgent!

Many thanks in advance

Judy Munday


	[[alternative HTML version deleted]]


From Eric.Elguero at ird.fr  Thu Jul 21 13:54:54 2016
From: Eric.Elguero at ird.fr (Eric Elguero)
Date: Thu, 21 Jul 2016 13:54:54 +0200
Subject: [R] splitting a vector of strings
Message-ID: <5790B80E.90701@ird.fr>

Hi everybody,

I have a vector of character strings.
Each string has the same pattern and I want
to split them in pieces and get a vector made
of the first pieces of each string.

The problem is that strsplit returns a list.

All I found is

uu<- matrix(unlist(strsplit(x,";")),ncol=3,byrow=T)[,1]

where x is the vector ";" is the delimiting character
and I know that each string will be cut in 3 pieces.

That works for my problem but I would prefer a
more elegant solution. Besides, it would not
work if all the string didn't have the same
number of pieces.

does someone have a better solution?

sorry if that topic was discussed recently.
There is too much traffic on the r-help list,
I cannot catch up.

-- 
Eric Elguero
------------
MIVEGEC. - UMR (CNRS/IRD/UM) 5290
Maladies Infectieuses et Vecteurs, G?n?tique, Evolution et Contr?le
Institut de Recherche pour le D?veloppement (IRD)
911, Avenue Agropolis
BP 64501
34394 Montpellier Cedex 5, France


From btupper at bigelow.org  Thu Jul 21 14:13:11 2016
From: btupper at bigelow.org (Ben Tupper)
Date: Thu, 21 Jul 2016 08:13:11 -0400
Subject: [R] splitting a vector of strings
In-Reply-To: <5790B80E.90701@ird.fr>
References: <5790B80E.90701@ird.fr>
Message-ID: <B01AADAD-72BB-499A-B6D9-C67B72AB72BF@bigelow.org>

Hi,

I'm not sure about the more generalized solution, but how about this for a start.


x <- c("a;b;c", "d;e", "foo;g;h;i")
x
#[1] "a;b;c"     "d;e"       "foo;g;h;i"

sapply(strsplit(x, ";",fixed = TRUE), '[',1)
#[1] "a"   "d"   "foo"

If you want elegance then I suggest you take a look at the stringr package. 

https://cran.r-project.org/web/packages/stringr/index.html

Cheers,
Ben


> On Jul 21, 2016, at 7:54 AM, Eric Elguero <Eric.Elguero at ird.fr> wrote:
> 
> Hi everybody,
> 
> I have a vector of character strings.
> Each string has the same pattern and I want
> to split them in pieces and get a vector made
> of the first pieces of each string.
> 
> The problem is that strsplit returns a list.
> 
> All I found is
> 
> uu<- matrix(unlist(strsplit(x,";")),ncol=3,byrow=T)[,1]
> 
> where x is the vector ";" is the delimiting character
> and I know that each string will be cut in 3 pieces.
> 
> That works for my problem but I would prefer a
> more elegant solution. Besides, it would not
> work if all the string didn't have the same
> number of pieces.
> 
> does someone have a better solution?
> 
> sorry if that topic was discussed recently.
> There is too much traffic on the r-help list,
> I cannot catch up.
> 
> -- 
> Eric Elguero
> ------------
> MIVEGEC. - UMR (CNRS/IRD/UM) 5290
> Maladies Infectieuses et Vecteurs, G?n?tique, Evolution et Contr?le
> Institut de Recherche pour le D?veloppement (IRD)
> 911, Avenue Agropolis
> BP 64501
> 34394 Montpellier Cedex 5, France
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Report Gulf of Maine jellyfish sightings to jellyfish at bigelow.org or tweet them to #MaineJellies -- include date, time, and location, as well as any descriptive information such as size or type.  Learn more at https://www.bigelow.org/research/srs/nick-record/nick-record-laboratory/mainejellies/


From ivan.calandra at univ-reims.fr  Thu Jul 21 14:19:46 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Thu, 21 Jul 2016 14:19:46 +0200
Subject: [R] Issue with Transform function in R
In-Reply-To: <CAEGXkYW2qFwL-H0Gjbh2BDHZhgio6r_pFmfTN7FJJUOjWpR9rg@mail.gmail.com>
References: <CAEGXkYW2qFwL-H0Gjbh2BDHZhgio6r_pFmfTN7FJJUOjWpR9rg@mail.gmail.com>
Message-ID: <2d82e636-dfcb-21d6-04be-266560bac143@univ-reims.fr>

This might not be the whole story, but part of the problem is that you 
want to select a _*character string*_ greater/smaller than another. That 
doesn't make much sense!

I am not sure how to best compare two dates, but if you convert the Date 
values into numeric, then that would work. The problem is that it seems 
your Date values are character, and the comparison in your ifelse is 
also a character.

So something like this might work (untested because no reproducible 
example):
transform(Df1, ifelse(as.numeric(Date) > numeric.value1 && 
as.numeric(Date) < numeric.value2, Variable1 ==NA, Variable1))

HTH,
Ivan

--
Ivan Calandra, PhD
Scientific Mediator
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/

Le 21/07/2016 ? 03:00, Bhaskar Mitra a ?crit :
> Hello Everyone,
>
>
> I am trying to replace the values in the 2nd column (Variable 1)
> corresponding to certain dates  (Date)
>
>
> with NAs as shown below. Both Date and Variable1 are numeric vectors . I am
> trying to use the transform function
>
>
> as shown below but it doesn?t seem to work even though if I am not getting
> any error
>
>
> Any suggestions/help in this regard?
>
> regards,
>
> ---------------------------------------------------------------------------------------------------------------------------------------------------------
>
>
>
> Df1 <- data.frame(Date, Variable1)
>
>
>
>
> a1 <- transform(Df1, ifelse(Date  > "010301000300 " && Date <
> "010501000300", Variable1 ==NA, Variable1))
>
>
> Original Data frame
>
>
>
>       Date                          Variable1
>
> 010101000300                     1
>
> 010201000300                     2
>
> 010301000300                     3
>
> 010401000300                     4
>
> 010501000300                     5
>
> 010601000300                     6
>
> 010701000300                     7
>
>   .
>
> .
>
> .
>
> ???.
>
>
>
>
> Transformed data frame (i hope to transform)
>
>
>
>        Date                       Variable1
>
> 010101000300                    1
>
> 010201000300                    2
>
> 010301000300                   NA
>
> 010401000300                   NA
>
> 010501000300                    NA
>
> 010601000300                    6
>
> 010701000300                    7
>
>
>
> ???.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lists at dewey.myzen.co.uk  Thu Jul 21 14:21:38 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Thu, 21 Jul 2016 13:21:38 +0100
Subject: [R] splitting a vector of strings
In-Reply-To: <5790B80E.90701@ird.fr>
References: <5790B80E.90701@ird.fr>
Message-ID: <3d01c1f5-968b-2351-9995-540dfcb772ec@dewey.myzen.co.uk>

Dear Eric

I think you are looking for sub or gsub

Without an example set of input and output I am not quite sure but you 
would need to define an expression which matches your separator (;) 
followed by any characters up to the end of line. If you have trouble 
with that then someone here will no doubt write the pattern for you but 
learning about regular expressions is well worthwhile

On 21/07/2016 12:54, Eric Elguero wrote:
> Hi everybody,
>
> I have a vector of character strings.
> Each string has the same pattern and I want
> to split them in pieces and get a vector made
> of the first pieces of each string.
>
> The problem is that strsplit returns a list.
>
> All I found is
>
> uu<- matrix(unlist(strsplit(x,";")),ncol=3,byrow=T)[,1]
>
> where x is the vector ";" is the delimiting character
> and I know that each string will be cut in 3 pieces.
>
> That works for my problem but I would prefer a
> more elegant solution. Besides, it would not
> work if all the string didn't have the same
> number of pieces.
>
> does someone have a better solution?
>
> sorry if that topic was discussed recently.
> There is too much traffic on the r-help list,
> I cannot catch up.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From ruipbarradas at sapo.pt  Thu Jul 21 14:33:09 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Thu, 21 Jul 2016 13:33:09 +0100
Subject: [R] Issue with Transform function in R
In-Reply-To: <2d82e636-dfcb-21d6-04be-266560bac143@univ-reims.fr>
References: <CAEGXkYW2qFwL-H0Gjbh2BDHZhgio6r_pFmfTN7FJJUOjWpR9rg@mail.gmail.com>
	<2d82e636-dfcb-21d6-04be-266560bac143@univ-reims.fr>
Message-ID: <20160721133309.Horde.5xbjLvN9zlQZ4yTGCtIrRo9@mail.sapo.pt>

Hello,

Another thing to consider is to use Variable1 = NA, not '=='.
With '==' it will probably return TRUE/FALSE/NA.

Hope this helps,

Rui Barradas
?

Citando Ivan Calandra <ivan.calandra at univ-reims.fr>:

> This might not be the whole story, but part of the problem is that  
> you want to select a _*character string*_ greater/smaller than  
> another. That doesn't make much sense!
>
> I am not sure how to best compare two dates, but if you convert the  
> Date values into numeric, then that would work. The problem is that  
> it seems your Date values are character, and the comparison in your  
> ifelse is also a character.
>
> So something like this might work (untested because no reproducible example):
> transform(Df1, ifelse(as.numeric(Date) > numeric.value1 &&  
> as.numeric(Date) < numeric.value2, Variable1 ==NA, Variable1))
>
> HTH,
> Ivan
>
> --
> Ivan Calandra, PhD
> Scientific Mediator
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> --
> https://www.researchgate.net/profile/Ivan_Calandra
> https://publons.com/author/705639/
>
> Le 21/07/2016 ? 03:00, Bhaskar Mitra a ?crit :
>> Hello Everyone,
>>
>> I am trying to replace the values in the 2nd column (Variable 1)
>> corresponding to certain dates? (Date)
>>
>> with NAs as shown below. Both Date and Variable1 are numeric vectors . I am
>> trying to use the transform function
>>
>> as shown below but it doesn?t seem to work even though if I am not getting
>> any error
>>
>> Any suggestions/help in this regard?
>>
>> regards,
>>
>> ---------------------------------------------------------------------------------------------------------------------------------------------------------
>>
>> Df1 <- data.frame(Date, Variable1)
>>
>> a1 <- transform(Df1, ifelse(Date? > "010301000300 " && Date <
>> "010501000300", Variable1 ==NA, Variable1))
>>
>> Original Data frame
>>
>> ? ? ?Date? ? ? ? ? ? ? ? ? ? ? ? ? Variable1
>>
>> 010101000300? ? ? ? ? ? ? ? ? ? ?1
>>
>> 010201000300? ? ? ? ? ? ? ? ? ? ?2
>>
>> 010301000300? ? ? ? ? ? ? ? ? ? ?3
>>
>> 010401000300? ? ? ? ? ? ? ? ? ? ?4
>>
>> 010501000300? ? ? ? ? ? ? ? ? ? ?5
>>
>> 010601000300? ? ? ? ? ? ? ? ? ? ?6
>>
>> 010701000300? ? ? ? ? ? ? ? ? ? ?7
>>
>> .
>>
>> .
>>
>> .
>>
>> ???.
>>
>> Transformed data frame (i hope to transform)
>>
>> ? ? ? Date? ? ? ? ? ? ? ? ? ? ? ?Variable1
>>
>> 010101000300? ? ? ? ? ? ? ? ? ? 1
>>
>> 010201000300? ? ? ? ? ? ? ? ? ? 2
>>
>> 010301000300? ? ? ? ? ? ? ? ? ?NA
>>
>> 010401000300? ? ? ? ? ? ? ? ? ?NA
>>
>> 010501000300? ? ? ? ? ? ? ? ? ? NA
>>
>> 010601000300? ? ? ? ? ? ? ? ? ? 6
>>
>> 010701000300? ? ? ? ? ? ? ? ? ? 7
>>
>> ???.
>>
>> ? ? ? ? [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Thu Jul 21 14:33:30 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 21 Jul 2016 08:33:30 -0400
Subject: [R] Help installing Rcmdr on Mac
In-Reply-To: <1469072843698.33223@uq.edu.au>
References: <1469072843698.33223@uq.edu.au>
Message-ID: <b3214d83-2af6-6e36-f232-eefdcd5d752f@gmail.com>

On 20/07/2016 11:47 PM, Judy Munday wrote:
> I am having trouble installing R Commander on a Mac (OS X 10.7.5).  I would really appreciate some help!
>
>
> I receive the following message after following the instructions to install Rcmdr via Package Installer:
>
>
> Warning: dependency 'rglwidget' is not available

This is an indication that the CRAN mirror you're using is incomplete. 
I recommend cloud.r-project.org as a reliable mirror; the folks at 
RStudio run it, and they do a good job.

> trying URL 'http://cran.ms.unimelb.edu.au/bin/macosx/contrib/3.2/Rcmdr_2.2-5.tgz'
> Content type 'application/x-gzip' length 5456133 bytes (5.2 MB)
> ==================================================
> downloaded 5.2 MB

That part was fine.

Duncan Murdoch
>
> ? If someone could please help me, I would be very grateful.   It is quite urgent!
>
> Many thanks in advance
>
> Judy Munday
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Thu Jul 21 14:35:02 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Thu, 21 Jul 2016 12:35:02 +0000
Subject: [R] Help installing Rcmdr on Mac
In-Reply-To: <1469072843698.33223@uq.edu.au>
References: <1469072843698.33223@uq.edu.au>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836544F91@FHSDB2D11-2.csu.mcmaster.ca>

Dear Judy,

I see that the rglwidget package *is* available on CRAN but also that you're using a very old version of Mac OS X. You may have to upgrade OS X and install the current version of R (which is not available for OS X 10.7) in order to use the Rcmdr package.

You may also be able to get a more definitive answer from others more familiar with R package availability on Mac OS X.

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Judy Munday
> Sent: July 20, 2016 11:47 PM
> To: r-help at R-project.org
> Subject: [R] Help installing Rcmdr on Mac
> 
> I am having trouble installing R Commander on a Mac (OS X 10.7.5).  I would
> really appreciate some help!
> 
> 
> I receive the following message after following the instructions to install Rcmdr
> via Package Installer:
> 
> 
> Warning: dependency 'rglwidget' is not available trying URL
> 'http://cran.ms.unimelb.edu.au/bin/macosx/contrib/3.2/Rcmdr_2.2-5.tgz'
> Content type 'application/x-gzip' length 5456133 bytes (5.2 MB)
> ==================================================
> downloaded 5.2 MB
> 
> ? If someone could please help me, I would be very grateful.   It is quite urgent!
> 
> Many thanks in advance
> 
> Judy Munday
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ivan.calandra at univ-reims.fr  Thu Jul 21 14:45:31 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Thu, 21 Jul 2016 14:45:31 +0200
Subject: [R] Issue with Transform function in R
In-Reply-To: <20160721133309.Horde.5xbjLvN9zlQZ4yTGCtIrRo9@mail.sapo.pt>
References: <CAEGXkYW2qFwL-H0Gjbh2BDHZhgio6r_pFmfTN7FJJUOjWpR9rg@mail.gmail.com>
	<2d82e636-dfcb-21d6-04be-266560bac143@univ-reims.fr>
	<20160721133309.Horde.5xbjLvN9zlQZ4yTGCtIrRo9@mail.sapo.pt>
Message-ID: <1d17e921-2c0a-9ebc-615b-ae72293872e8@univ-reims.fr>

Oops, missed that one!

--
Ivan Calandra, PhD
Scientific Mediator
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/

Le 21/07/2016 ? 14:33, ruipbarradas at sapo.pt a ?crit :
>
> Hello,
>
> Another thing to consider is to use Variable1 = NA, not '=='.
> With '==' it will probably return TRUE/FALSE/NA.
>
> Hope this helps,
>
> Rui Barradas
>
> Citando Ivan Calandra <ivan.calandra at univ-reims.fr 
> <mailto:ivan.calandra at univ-reims.fr>>:
>
>> This might not be the whole story, but part of the problem is that 
>> you want to select a _*character string*_ greater/smaller than 
>> another. That doesn't make much sense!
>>
>> I am not sure how to best compare two dates, but if you convert the 
>> Date values into numeric, then that would work. The problem is that 
>> it seems your Date values are character, and the comparison in your 
>> ifelse is also a character.
>>
>> So something like this might work (untested because no reproducible 
>> example):
>> transform(Df1, ifelse(as.numeric(Date) > numeric.value1 && 
>> as.numeric(Date) < numeric.value2, Variable1 ==NA, Variable1))
>>
>> HTH,
>> Ivan
>>
>> --
>> Ivan Calandra, PhD
>> Scientific Mediator
>> University of Reims Champagne-Ardenne
>> GEGENAA - EA 3795
>> CREA - 2 esplanade Roland Garros
>> 51100 Reims, France
>> +33(0)3 26 77 36 89
>> ivan.calandra at univ-reims.fr <mailto:ivan.calandra at univ-reims.fr>
>> --
>> https://www.researchgate.net/profile/Ivan_Calandra
>> https://publons.com/author/705639/
>>
>> Le 21/07/2016 ? 03:00, Bhaskar Mitra a ?crit :
>>
>>> Hello Everyone,
>>>
>>>
>>> I am trying to replace the values in the 2nd column (Variable 1)
>>> corresponding to certain dates  (Date)
>>>
>>>
>>> with NAs as shown below. Both Date and Variable1 are numeric vectors 
>>> . I am
>>> trying to use the transform function
>>>
>>>
>>> as shown below but it doesn?t seem to work even though if I am not 
>>> getting
>>> any error
>>>
>>>
>>> Any suggestions/help in this regard?
>>>
>>> regards,
>>>
>>> ---------------------------------------------------------------------------------------------------------------------------------------------------------
>>>
>>>
>>>
>>> Df1 <- data.frame(Date, Variable1)
>>>
>>>
>>>
>>>
>>> a1 <- transform(Df1, ifelse(Date  > "010301000300 " && Date <
>>> "010501000300", Variable1 ==NA, Variable1))
>>>
>>>
>>> Original Data frame
>>>
>>>
>>>
>>>      Date                          Variable1
>>>
>>> 010101000300                     1
>>>
>>> 010201000300                     2
>>>
>>> 010301000300                     3
>>>
>>> 010401000300                     4
>>>
>>> 010501000300                     5
>>>
>>> 010601000300                     6
>>>
>>> 010701000300                     7
>>>
>>> .
>>>
>>> .
>>>
>>> .
>>>
>>> ???.
>>>
>>>
>>>
>>>
>>> Transformed data frame (i hope to transform)
>>>
>>>
>>>
>>>       Date                       Variable1
>>>
>>> 010101000300                    1
>>>
>>> 010201000300                    2
>>>
>>> 010301000300                   NA
>>>
>>> 010401000300                   NA
>>>
>>> 010501000300                    NA
>>>
>>> 010601000300                    6
>>>
>>> 010701000300                    7
>>>
>>>
>>>
>>> ???.
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- 
>>> To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>> ______________________________________________
>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To 
>> UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.htmland provide commented, 
>> minimal, self-contained, reproducible code.
>
>


From tom at maladmin.com  Thu Jul 21 15:56:01 2016
From: tom at maladmin.com (tom at maladmin.com)
Date: Thu, 21 Jul 2016 09:56:01 -0400
Subject: [R] Issue with Transform function in R
In-Reply-To: <CAEGXkYW2qFwL-H0Gjbh2BDHZhgio6r_pFmfTN7FJJUOjWpR9rg@mail.gmail.com>
References: <CAEGXkYW2qFwL-H0Gjbh2BDHZhgio6r_pFmfTN7FJJUOjWpR9rg@mail.gmail.com>
Message-ID: <5790d471.1419240a.b4f42.35ea@mx.google.com>

Not sure I can translate the format of your Date column correctly, however the command
DF1$Date <- as.Date(DF1$Date, format=?formatstr?)

Will convert the dates into a format correctly handled by R.
?strptime 

Should give you an idea of what formatstr should look like. 
I.e. if 
date = 160721
as.Date(date, format=?%y%M%d?)



From: Bhaskar Mitra
	[[alternative HTML version deleted]]


From tom at maladmin.com  Thu Jul 21 15:58:52 2016
From: tom at maladmin.com (tom at maladmin.com)
Date: Thu, 21 Jul 2016 09:58:52 -0400
Subject: [R] txtProgressBar()
In-Reply-To: <a3d9a025-4a09-f44f-97ea-22d92e85399c@univ-reims.fr>
References: <43c8e65f-d535-5b96-c7c1-080517d648fd@univ-reims.fr>
	<CAFEqCdyLx0H-G8Uoh1c_KYCiN2tB3SeA7o0xrW_PGJmkceMcAw@mail.gmail.com>
	<a3d9a025-4a09-f44f-97ea-22d92e85399c@univ-reims.fr>
Message-ID: <5790d51c.c2816b0a.80549.92fa@mx.google.com>

You may like to look at 
?suppressMessages

P.S. sorry for posting in HTML, new laptop and it?s next on my list of things to fix.


From: Ivan Calandra
	[[alternative HTML version deleted]]


From paul.johnston at manchester.ac.uk  Thu Jul 21 16:48:44 2016
From: paul.johnston at manchester.ac.uk (Paul Johnston)
Date: Thu, 21 Jul 2016 14:48:44 +0000
Subject: [R] Reading PCorpus created using tm package
Message-ID: <EE85AFC56BE9E84EB86F7739169F1BD6A2483BF6@MBXP15.ds.man.ac.uk>

Dear All

I have created a permanent corpus and I can see the file exist:

setwd("E:/textmining/texts")
(data_mined_permanent <- PCorpus(DirSource(("E:/textmining/texts")),
                                 readerControl = list(languages = "eng"), 
                                 dbControl = list(dbName="E:/textmining/db_one",dbType = "DB1")))
print(list.files(path = "E:/textmining/"))


## [1] "corpora" "db_one"
## [3] "part_one.pdf" "part_one.rmd"
## [5] "processing_corpra.pdf" "processing_corpra.rmd"
## [7] "textmining.pdf" "textmining.rmd"
## [9] "textmining2.pdf" "textmining2.Rmd"
## [11] "textmining3.pdf" "textmining3.Rmd"

However I cannot see how to reload the said corpus after restarting an R session.

I've seen this http://stackoverflow.com/questions/28377646/how-to-reconnect-to-the-pcorpus-in-the-r-tm-package

So can I create a corpus and just reload the file persistent copy or not?

Cheers Paul

Paul Johnston
Research Infrastructure
Room B39
Sackville Street Building

?http://bit.ly/ResearchITFeedback
We would run with all of our might 
Push the king off to take the hill 
And to learn who was king 


From djnordlund at gmail.com  Thu Jul 21 17:22:02 2016
From: djnordlund at gmail.com (Daniel Nordlund)
Date: Thu, 21 Jul 2016 08:22:02 -0700
Subject: [R] how to expand the dataframe
In-Reply-To: <CAN5afy8-Y4_ZSN55y1m2hqXN4hWhNbqfxKusVD7Wxeh+9Yq8tA@mail.gmail.com>
References: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
	<37BE3680-939B-4AB2-88E3-5C9B4EFE2DD1@comcast.net>
	<CAN5afy8-Y4_ZSN55y1m2hqXN4hWhNbqfxKusVD7Wxeh+9Yq8tA@mail.gmail.com>
Message-ID: <b17bc1c8-65b1-96fa-26cb-e3f91f46e700@gmail.com>

On 7/20/2016 8:26 PM, lily li wrote:
> Yes, I tried to create a dataframe and merge it with the shortened
> dataframe. The resulting dataframe goes with the short one and truncates
> the complete date column, so it does not work.
>
> On Wed, Jul 20, 2016 at 6:38 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>>
>>> On Jul 20, 2016, at 1:31 PM, lily li <chocold12 at gmail.com> wrote:
>>>
>>> Hi R users,
>>>
>>> I have a dataframe, where there is a column 'time' represents time series
>>> but is not complete. How to expand the dataframe so this column will
>> become
>>> complete, where other columns with the newly added rows have NA values?
>>> Thanks.
>>>
>>> df
>>> A     B     C     time
>>> 10    5     3.3 1990-01-01
>>> 11    5      4     1990-02-07
>>> 12    4     3      1990-02-14
>>> ...
>>
>> Make a dataframe with a 'time' column using seq.Date and merge that
>> dataframe with your df dataframe.
>>
>>>
>>>       [[alternative HTML version deleted]]
>>
>> Really .... isn't it time you learned how to send plain text. You've
>> posted many questions on Rhelp.  It's really not that difficult on gmail. I
>> also have a gmail account and have had no difficulty finding instructions
>> on how to do it.
>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>>
>

Don't just say you tried to do the merge and it doesn't work.  At a 
minimum show us the ACTUAL code you used and give us any error messages 
you got or show us a portion of the results and explain why it is not 
what you expected.  If possible, give us a small amount of data using 
dput() so that we can "play along at home" (i.e. give us a reproducible 
example).

Dan

Daniel Nordlund
Port Townsend, WA



-- 
Daniel Noredlund
Bothell, WA USA


From maechler at stat.math.ethz.ch  Thu Jul 21 18:07:43 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Jul 2016 18:07:43 +0200
Subject: [R] readline issue with 3.3.1
In-Reply-To: <20160720163753.039cbccb@delli.home.local>
References: <20160720113531.2bacaa0a@delli.home.local>
	<20160720163753.039cbccb@delli.home.local>
Message-ID: <22416.62287.429899.107013@stat.math.ethz.ch>

>>>>> Ralf Goertz <r_goertz at web.de>
>>>>>     on Wed, 20 Jul 2016 16:37:53 +0200 writes:

    > Am Wed, 20 Jul 2016 11:35:31 +0200
    > schrieb Ralf Goertz <r_goertz at web.de>:

    >> Hi,
    >> 
    >> after a recent update to version 3.3.1 on Opensuse Leap I have
    >> problems with command lines longer than the terminal width. E.g. when
    >> I do this

    > I installed readline version 6.3 and the issue is gone. So probably some
    > of the recent changes in R's readline code are incompatible with version
    > readline version 6.2.

Yes, it seems so, unfortunately.

Thank you for reporting !

Our plan had been different: the NEWS entry for 3.3.1 (among 'BUG FIXES') says

    ? Use of Ctrl-C to terminate a reverse incremental search started
      by Ctrl-R in the readline-based Unix terminal interface is now
      supported when R was compiled against readline >= 6.0 (Ctrl-G
      always worked).  (PR#16603)

So we had hoped that change (fixing a bug you *should* have been
able to see with readline 6.2, as well) would work correctly in all
versions of readline >= 6.0,
but evidently it did not in yours.

Martin Maechler
ETH Zurich / R Core Team


From roundsjeremiah at gmail.com  Thu Jul 21 20:02:57 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Thu, 21 Jul 2016 11:02:57 -0700
Subject: [R] C/C++/Fortran Rolling Window Regressions
Message-ID: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>

Hi,

A not unusual task is performing a multiple regression in a rolling window
on a time-series.    A standard piece of advice for doing in R is something
like the code that follows at the end of the email.  I am currently using
an "embed" variant of that code and that piece of advice is out there too.

But, it occurs to me that for such an easily specified matrix operation
standard R code is really slow.   rollapply constantly returns to R
interpreter at each window step for a new lm.   All lm is at its heart is
(X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in rolling
window you are just incrementing a counter and peeling off rows (or columns
of X and y) of a particular window size, and following that up with some
matrix multiplication in a loop.   The psuedo-code for that Rcpp
practically writes itself and you might want a wrapper of something like:
rolling_lm (y=y, x=x, width=4).

My question is this: has any of the thousands of R packages out there
published anything like that.  Rolling window multiple regressions that
stay in C/C++ until the rolling window completes?  No sense and writing it
if it exist.


Thanks,
Jeremiah

Standard (slow) advice for "rolling window regression" follows:


set.seed(1)
z <- zoo(matrix(rnorm(10), ncol = 2))
colnames(z) <- c("y", "x")

## rolling regression of width 4
rollapply(z, width = 4,
   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
   by.column = FALSE, align = "right")

## result is identical to
coef(lm(y ~ x, data = z[1:4,]))
coef(lm(y ~ x, data = z[2:5,]))

	[[alternative HTML version deleted]]


From ggrothendieck at gmail.com  Thu Jul 21 21:31:36 2016
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Jul 2016 15:31:36 -0400
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
Message-ID: <CAP01uR=8zsGBS2W4FOJLUZM2BcH-gqTOtPhQ-GfEE8Kr01M0BA@mail.gmail.com>

Just replacing lm with a faster version would speed it up.  Try lm.fit
or even faster is fastLm in the RcppArmadillo package.

On Thu, Jul 21, 2016 at 2:02 PM, jeremiah rounds
<roundsjeremiah at gmail.com> wrote:
> Hi,
>
> A not unusual task is performing a multiple regression in a rolling window
> on a time-series.    A standard piece of advice for doing in R is something
> like the code that follows at the end of the email.  I am currently using
> an "embed" variant of that code and that piece of advice is out there too.
>
> But, it occurs to me that for such an easily specified matrix operation
> standard R code is really slow.   rollapply constantly returns to R
> interpreter at each window step for a new lm.   All lm is at its heart is
> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in rolling
> window you are just incrementing a counter and peeling off rows (or columns
> of X and y) of a particular window size, and following that up with some
> matrix multiplication in a loop.   The psuedo-code for that Rcpp
> practically writes itself and you might want a wrapper of something like:
> rolling_lm (y=y, x=x, width=4).
>
> My question is this: has any of the thousands of R packages out there
> published anything like that.  Rolling window multiple regressions that
> stay in C/C++ until the rolling window completes?  No sense and writing it
> if it exist.
>
>
> Thanks,
> Jeremiah
>
> Standard (slow) advice for "rolling window regression" follows:
>
>
> set.seed(1)
> z <- zoo(matrix(rnorm(10), ncol = 2))
> colnames(z) <- c("y", "x")
>
> ## rolling regression of width 4
> rollapply(z, width = 4,
>    function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>    by.column = FALSE, align = "right")
>
> ## result is identical to
> coef(lm(y ~ x, data = z[1:4,]))
> coef(lm(y ~ x, data = z[2:5,]))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From Achim.Zeileis at uibk.ac.at  Thu Jul 21 21:36:49 2016
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Thu, 21 Jul 2016 21:36:49 +0200 (CEST)
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
Message-ID: <alpine.DEB.2.20.1607212128490.3823@paninaro>

Jeremiah,

for this purpose there are the "roll" and "RcppRoll" packages. Both use 
Rcpp and the former also provides rolling lm models. The latter has a 
generic interface that let's you define your own function.

One thing to pay attention to, though, is the numerical reliability. 
Especially on large time series with relatively short windows there is a 
good chance of encountering numerically challenging situations. The QR 
decomposition used by lm is fairly robust while other more straightforward 
matrix multiplications may not be. This should be kept in mind when 
writing your own Rcpp code for plugging it into RcppRoll.

But I haven't check what the roll package does and how reliable that is...

hth,
Z

On Thu, 21 Jul 2016, jeremiah rounds wrote:

> Hi,
>
> A not unusual task is performing a multiple regression in a rolling window
> on a time-series.    A standard piece of advice for doing in R is something
> like the code that follows at the end of the email.  I am currently using
> an "embed" variant of that code and that piece of advice is out there too.
>
> But, it occurs to me that for such an easily specified matrix operation
> standard R code is really slow.   rollapply constantly returns to R
> interpreter at each window step for a new lm.   All lm is at its heart is
> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in rolling
> window you are just incrementing a counter and peeling off rows (or columns
> of X and y) of a particular window size, and following that up with some
> matrix multiplication in a loop.   The psuedo-code for that Rcpp
> practically writes itself and you might want a wrapper of something like:
> rolling_lm (y=y, x=x, width=4).
>
> My question is this: has any of the thousands of R packages out there
> published anything like that.  Rolling window multiple regressions that
> stay in C/C++ until the rolling window completes?  No sense and writing it
> if it exist.
>
>
> Thanks,
> Jeremiah
>
> Standard (slow) advice for "rolling window regression" follows:
>
>
> set.seed(1)
> z <- zoo(matrix(rnorm(10), ncol = 2))
> colnames(z) <- c("y", "x")
>
> ## rolling regression of width 4
> rollapply(z, width = 4,
>   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>   by.column = FALSE, align = "right")
>
> ## result is identical to
> coef(lm(y ~ x, data = z[1:4,]))
> coef(lm(y ~ x, data = z[2:5,]))
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From roundsjeremiah at gmail.com  Thu Jul 21 21:45:13 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Thu, 21 Jul 2016 12:45:13 -0700
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <alpine.DEB.2.20.1607212128490.3823@paninaro>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
	<alpine.DEB.2.20.1607212128490.3823@paninaro>
Message-ID: <CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>

 Thanks all.  roll::roll_lm was essentially what I wanted.   I think maybe
I would prefer it to have options to return a few more things, but it is
the coefficients, and the remaining statistics you might want can be
calculated fast enough from there.


On Thu, Jul 21, 2016 at 12:36 PM, Achim Zeileis <Achim.Zeileis at uibk.ac.at>
wrote:

> Jeremiah,
>
> for this purpose there are the "roll" and "RcppRoll" packages. Both use
> Rcpp and the former also provides rolling lm models. The latter has a
> generic interface that let's you define your own function.
>
> One thing to pay attention to, though, is the numerical reliability.
> Especially on large time series with relatively short windows there is a
> good chance of encountering numerically challenging situations. The QR
> decomposition used by lm is fairly robust while other more straightforward
> matrix multiplications may not be. This should be kept in mind when writing
> your own Rcpp code for plugging it into RcppRoll.
>
> But I haven't check what the roll package does and how reliable that is...
>
> hth,
> Z
>
>
> On Thu, 21 Jul 2016, jeremiah rounds wrote:
>
> Hi,
>>
>> A not unusual task is performing a multiple regression in a rolling window
>> on a time-series.    A standard piece of advice for doing in R is
>> something
>> like the code that follows at the end of the email.  I am currently using
>> an "embed" variant of that code and that piece of advice is out there too.
>>
>> But, it occurs to me that for such an easily specified matrix operation
>> standard R code is really slow.   rollapply constantly returns to R
>> interpreter at each window step for a new lm.   All lm is at its heart is
>> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in rolling
>> window you are just incrementing a counter and peeling off rows (or
>> columns
>> of X and y) of a particular window size, and following that up with some
>> matrix multiplication in a loop.   The psuedo-code for that Rcpp
>> practically writes itself and you might want a wrapper of something like:
>> rolling_lm (y=y, x=x, width=4).
>>
>> My question is this: has any of the thousands of R packages out there
>> published anything like that.  Rolling window multiple regressions that
>> stay in C/C++ until the rolling window completes?  No sense and writing it
>> if it exist.
>>
>>
>> Thanks,
>> Jeremiah
>>
>> Standard (slow) advice for "rolling window regression" follows:
>>
>>
>> set.seed(1)
>> z <- zoo(matrix(rnorm(10), ncol = 2))
>> colnames(z) <- c("y", "x")
>>
>> ## rolling regression of width 4
>> rollapply(z, width = 4,
>>   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>>   by.column = FALSE, align = "right")
>>
>> ## result is identical to
>> coef(lm(y ~ x, data = z[1:4,]))
>> coef(lm(y ~ x, data = z[2:5,]))
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From michael.gang.peng at gmail.com  Thu Jul 21 22:00:22 2016
From: michael.gang.peng at gmail.com (Michael Peng)
Date: Thu, 21 Jul 2016 16:00:22 -0400
Subject: [R] how to stop interpretation in system()
Message-ID: <CAMjJGR37+ad8dh8+L3KhVQRWPLxnRhNxUDMxfwq0exbJZzTdMw@mail.gmail.com>

Hi,

I am trying to use system() to run some command in OS. such as

system("cmd 'a\tb')

however,  it alway runs
cmd 'a    b'
instead of
cmd 'a\tb'

How can I prevent system to interpret 'a\tb' to 'a    b'?


Thanks

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Thu Jul 21 22:07:19 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 21 Jul 2016 16:07:19 -0400
Subject: [R] how to stop interpretation in system()
In-Reply-To: <CAMjJGR37+ad8dh8+L3KhVQRWPLxnRhNxUDMxfwq0exbJZzTdMw@mail.gmail.com>
References: <CAMjJGR37+ad8dh8+L3KhVQRWPLxnRhNxUDMxfwq0exbJZzTdMw@mail.gmail.com>
Message-ID: <CAM_vjunFawa9S0SnaX1twuycLEJB0_n=6ZksE5TzcQM0khWExQ@mail.gmail.com>

You could escape the backslash.

system("cmd 'a\\tb'")


On Thu, Jul 21, 2016 at 4:00 PM, Michael Peng
<michael.gang.peng at gmail.com> wrote:
> Hi,
>
> I am trying to use system() to run some command in OS. such as
>
> system("cmd 'a\tb')
>
> however,  it alway runs
> cmd 'a    b'
> instead of
> cmd 'a\tb'
>
> How can I prevent system to interpret 'a\tb' to 'a    b'?
>
>
> Thanks

-- 
Sarah Goslee
http://www.functionaldiversity.org


From michael.gang.peng at gmail.com  Thu Jul 21 22:15:43 2016
From: michael.gang.peng at gmail.com (Michael Peng)
Date: Thu, 21 Jul 2016 16:15:43 -0400
Subject: [R] how to stop interpretation in system()
In-Reply-To: <CAM_vjunFawa9S0SnaX1twuycLEJB0_n=6ZksE5TzcQM0khWExQ@mail.gmail.com>
References: <CAMjJGR37+ad8dh8+L3KhVQRWPLxnRhNxUDMxfwq0exbJZzTdMw@mail.gmail.com>
	<CAM_vjunFawa9S0SnaX1twuycLEJB0_n=6ZksE5TzcQM0khWExQ@mail.gmail.com>
Message-ID: <CAMjJGR0GXM_F+eAV0w9zMGH-x8+3utTF9JqdU4oFUk978eH_SA@mail.gmail.com>

Yes. I forgot to  escape the escape. Thank you so much.

2016-07-21 16:07 GMT-04:00 Sarah Goslee <sarah.goslee at gmail.com>:

> You could escape the backslash.
>
> system("cmd 'a\\tb'")
>
>
> On Thu, Jul 21, 2016 at 4:00 PM, Michael Peng
> <michael.gang.peng at gmail.com> wrote:
> > Hi,
> >
> > I am trying to use system() to run some command in OS. such as
> >
> > system("cmd 'a\tb')
> >
> > however,  it alway runs
> > cmd 'a    b'
> > instead of
> > cmd 'a\tb'
> >
> > How can I prevent system to interpret 'a\tb' to 'a    b'?
> >
> >
> > Thanks
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>

	[[alternative HTML version deleted]]


From ggrothendieck at gmail.com  Thu Jul 21 22:28:33 2016
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 21 Jul 2016 16:28:33 -0400
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
	<alpine.DEB.2.20.1607212128490.3823@paninaro>
	<CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>
Message-ID: <CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>

I would be careful about making assumptions regarding what is faster.
Performance tends to be nonintuitive.

When I ran rollapply/lm, rollapply/fastLm and roll_lm on the example
you provided rollapply/fastLm was three times faster than roll_lm.  Of
course this could change with data of different dimensions but it
would be worthwhile to do actual benchmarks before making assumptions.

I also noticed that roll_lm did not give the same coefficients as the other two.

set.seed(1)
library(zoo)
library(RcppArmadillo)
library(roll)
z <- zoo(matrix(rnorm(10), ncol = 2))
colnames(z) <- c("y", "x")

## rolling regression of width 4
library(rbenchmark)
benchmark(fastLm = rollapplyr(z, width = 4,
     function(x) coef(fastLm(cbind(1, x[, 2]), x[, 1])),
     by.column = FALSE),
   lm = rollapplyr(z, width = 4,
     function(x) coef(lm(y ~ x, data = as.data.frame(x))),
     by.column = FALSE),
   roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, 2, drop = F]), 4,
     center = FALSE))[1:4]


     test replications elapsed relative
1  fastLm          100    0.22    1.000
2      lm          100    0.72    3.273
3 roll_lm          100    0.64    2.909

On Thu, Jul 21, 2016 at 3:45 PM, jeremiah rounds
<roundsjeremiah at gmail.com> wrote:
>  Thanks all.  roll::roll_lm was essentially what I wanted.   I think maybe
> I would prefer it to have options to return a few more things, but it is
> the coefficients, and the remaining statistics you might want can be
> calculated fast enough from there.
>
>
> On Thu, Jul 21, 2016 at 12:36 PM, Achim Zeileis <Achim.Zeileis at uibk.ac.at>
> wrote:
>
>> Jeremiah,
>>
>> for this purpose there are the "roll" and "RcppRoll" packages. Both use
>> Rcpp and the former also provides rolling lm models. The latter has a
>> generic interface that let's you define your own function.
>>
>> One thing to pay attention to, though, is the numerical reliability.
>> Especially on large time series with relatively short windows there is a
>> good chance of encountering numerically challenging situations. The QR
>> decomposition used by lm is fairly robust while other more straightforward
>> matrix multiplications may not be. This should be kept in mind when writing
>> your own Rcpp code for plugging it into RcppRoll.
>>
>> But I haven't check what the roll package does and how reliable that is...
>>
>> hth,
>> Z
>>
>>
>> On Thu, 21 Jul 2016, jeremiah rounds wrote:
>>
>> Hi,
>>>
>>> A not unusual task is performing a multiple regression in a rolling window
>>> on a time-series.    A standard piece of advice for doing in R is
>>> something
>>> like the code that follows at the end of the email.  I am currently using
>>> an "embed" variant of that code and that piece of advice is out there too.
>>>
>>> But, it occurs to me that for such an easily specified matrix operation
>>> standard R code is really slow.   rollapply constantly returns to R
>>> interpreter at each window step for a new lm.   All lm is at its heart is
>>> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in rolling
>>> window you are just incrementing a counter and peeling off rows (or
>>> columns
>>> of X and y) of a particular window size, and following that up with some
>>> matrix multiplication in a loop.   The psuedo-code for that Rcpp
>>> practically writes itself and you might want a wrapper of something like:
>>> rolling_lm (y=y, x=x, width=4).
>>>
>>> My question is this: has any of the thousands of R packages out there
>>> published anything like that.  Rolling window multiple regressions that
>>> stay in C/C++ until the rolling window completes?  No sense and writing it
>>> if it exist.
>>>
>>>
>>> Thanks,
>>> Jeremiah
>>>
>>> Standard (slow) advice for "rolling window regression" follows:
>>>
>>>
>>> set.seed(1)
>>> z <- zoo(matrix(rnorm(10), ncol = 2))
>>> colnames(z) <- c("y", "x")
>>>
>>> ## rolling regression of width 4
>>> rollapply(z, width = 4,
>>>   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>>>   by.column = FALSE, align = "right")
>>>
>>> ## result is identical to
>>> coef(lm(y ~ x, data = z[1:4,]))
>>> coef(lm(y ~ x, data = z[2:5,]))
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From roundsjeremiah at gmail.com  Thu Jul 21 22:56:17 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Thu, 21 Jul 2016 13:56:17 -0700
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
	<alpine.DEB.2.20.1607212128490.3823@paninaro>
	<CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>
	<CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>
Message-ID: <CAOjnRsYGVBqFEJJn4M8ui=hB1wKDEw25dRO9p5K_Hn2riU0jpQ@mail.gmail.com>

I appreciate the timing, so much so I changed the code to show the issue.
 It is a problem of scale.

 roll_lm probably has a heavy start-up cost but otherwise completely
out-performs those other versions at scale.  I suspect you are timing the
nearly  constant time start-up cost in small data.  I did give code to
paint a picture, but it was just cartoon code lifted from stackexchange.
If you want to characterize the real problem it is closer to:
30 day rolling windows on 24 daily (by hour) measurements for 5 years with
24+7 -1 dummy predictor variables and finally you need to do this for 300
sets of data.

Pseudo-code is closer to what follows and roll_lm can handle that input in
a timely manner.  You can do it with lm.fit, but you need to spend a lot of
time waiting.  The issue of accuracy needs a follow-up check.  Not sure why
it would be different.  Worth a check on that.

Thanks,
Jeremiah


library(rbenchmark)
N = 30*24*12*5
window = 30*24
npred = 15  #15 chosen arbitrarily...
set.seed(1)
library(zoo)
library(RcppArmadillo)
library(roll)
x = matrix(rnorm(N*(npred+1)), ncol = npred+1)
colnames(x) <- c("y",  paste0("x", 1:npred))
z <- zoo(x)


benchmark(
   roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, -1, drop =
F]), window,
     center = FALSE), replications=3)

Which comes out as:
     test replications elapsed relative user.self sys.self user.child
sys.child
1 roll_lm            3   6.273        1    38.312    0.654          0
  0





## You arn't going to get that below...

benchmark(fastLm = rollapplyr(z, width = window,
     function(x) coef(fastLm(cbind(1, x[, -1]), x[, 1])),
     by.column = FALSE),
   lm = rollapplyr(z, width = window,
     function(x) coef(lm(y ~ ., data = as.data.frame(x))),
     by.column = FALSE), replications=3)



On Thu, Jul 21, 2016 at 1:28 PM, Gabor Grothendieck <ggrothendieck at gmail.com
> wrote:

> I would be careful about making assumptions regarding what is faster.
> Performance tends to be nonintuitive.
>
> When I ran rollapply/lm, rollapply/fastLm and roll_lm on the example
> you provided rollapply/fastLm was three times faster than roll_lm.  Of
> course this could change with data of different dimensions but it
> would be worthwhile to do actual benchmarks before making assumptions.
>
> I also noticed that roll_lm did not give the same coefficients as the
> other two.
>
> set.seed(1)
> library(zoo)
> library(RcppArmadillo)
> library(roll)
> z <- zoo(matrix(rnorm(10), ncol = 2))
> colnames(z) <- c("y", "x")
>
> ## rolling regression of width 4
> library(rbenchmark)
> benchmark(fastLm = rollapplyr(z, width = 4,
>      function(x) coef(fastLm(cbind(1, x[, 2]), x[, 1])),
>      by.column = FALSE),
>    lm = rollapplyr(z, width = 4,
>      function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>      by.column = FALSE),
>    roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, 2, drop =
> F]), 4,
>      center = FALSE))[1:4]
>
>
>      test replications elapsed relative
> 1  fastLm          100    0.22    1.000
> 2      lm          100    0.72    3.273
> 3 roll_lm          100    0.64    2.909
>
> On Thu, Jul 21, 2016 at 3:45 PM, jeremiah rounds
> <roundsjeremiah at gmail.com> wrote:
> >  Thanks all.  roll::roll_lm was essentially what I wanted.   I think
> maybe
> > I would prefer it to have options to return a few more things, but it is
> > the coefficients, and the remaining statistics you might want can be
> > calculated fast enough from there.
> >
> >
> > On Thu, Jul 21, 2016 at 12:36 PM, Achim Zeileis <
> Achim.Zeileis at uibk.ac.at>
> > wrote:
> >
> >> Jeremiah,
> >>
> >> for this purpose there are the "roll" and "RcppRoll" packages. Both use
> >> Rcpp and the former also provides rolling lm models. The latter has a
> >> generic interface that let's you define your own function.
> >>
> >> One thing to pay attention to, though, is the numerical reliability.
> >> Especially on large time series with relatively short windows there is a
> >> good chance of encountering numerically challenging situations. The QR
> >> decomposition used by lm is fairly robust while other more
> straightforward
> >> matrix multiplications may not be. This should be kept in mind when
> writing
> >> your own Rcpp code for plugging it into RcppRoll.
> >>
> >> But I haven't check what the roll package does and how reliable that
> is...
> >>
> >> hth,
> >> Z
> >>
> >>
> >> On Thu, 21 Jul 2016, jeremiah rounds wrote:
> >>
> >> Hi,
> >>>
> >>> A not unusual task is performing a multiple regression in a rolling
> window
> >>> on a time-series.    A standard piece of advice for doing in R is
> >>> something
> >>> like the code that follows at the end of the email.  I am currently
> using
> >>> an "embed" variant of that code and that piece of advice is out there
> too.
> >>>
> >>> But, it occurs to me that for such an easily specified matrix operation
> >>> standard R code is really slow.   rollapply constantly returns to R
> >>> interpreter at each window step for a new lm.   All lm is at its heart
> is
> >>> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in
> rolling
> >>> window you are just incrementing a counter and peeling off rows (or
> >>> columns
> >>> of X and y) of a particular window size, and following that up with
> some
> >>> matrix multiplication in a loop.   The psuedo-code for that Rcpp
> >>> practically writes itself and you might want a wrapper of something
> like:
> >>> rolling_lm (y=y, x=x, width=4).
> >>>
> >>> My question is this: has any of the thousands of R packages out there
> >>> published anything like that.  Rolling window multiple regressions that
> >>> stay in C/C++ until the rolling window completes?  No sense and
> writing it
> >>> if it exist.
> >>>
> >>>
> >>> Thanks,
> >>> Jeremiah
> >>>
> >>> Standard (slow) advice for "rolling window regression" follows:
> >>>
> >>>
> >>> set.seed(1)
> >>> z <- zoo(matrix(rnorm(10), ncol = 2))
> >>> colnames(z) <- c("y", "x")
> >>>
> >>> ## rolling regression of width 4
> >>> rollapply(z, width = 4,
> >>>   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
> >>>   by.column = FALSE, align = "right")
> >>>
> >>> ## result is identical to
> >>> coef(lm(y ~ x, data = z[1:4,]))
> >>> coef(lm(y ~ x, data = z[2:5,]))
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>

	[[alternative HTML version deleted]]


From milujisb at gmail.com  Thu Jul 21 22:57:54 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Thu, 21 Jul 2016 22:57:54 +0200
Subject: [R] Aggregate data to lower resolution
Message-ID: <CAMLwc7O1nWa7Akou2RA8S1Rt=FWzTzUrBjfZFaFz8wTXDckEFg@mail.gmail.com>

Dear all,

I have the following GDP data by latitude and longitude at 0.5 degree by
0.5 degree.

temp <- dput(head(ptsDF,10))
structure(list(longitude = c(-68.25, -67.75, -67.25, -68.25,
-67.75, -67.25, -71.25, -70.75, -69.25, -68.75), latitude = c(-54.75,
-54.75, -54.75, -54.25, -54.25, -54.25, -53.75, -53.75, -53.75,
-53.75), GDP = c(1.683046, 0.3212307, 0.0486207, 0.1223268, 0.0171909,
0.0062104, 0.22379, 0.1406729, 0.0030038, 0.0057422)), .Names =
c("longitude",
"latitude", "GDP"), row.names = c(4L, 17L, 30L, 43L, 56L, 69L,
82L, 95L, 108L, 121L), class = "data.frame")

I would like to aggregate the data 1 degree by 1 degree. I understand that
the first step is to convert to raster. I have tried:

rasterDF <- rasterFromXYZ(temp)
r <- aggregate(rasterDF,fact=2, fun=sum)

But this does not seem to work. Could anyone help me out please? Thank you
in advance.

Sincerely,

Milu

	[[alternative HTML version deleted]]


From markleeds2 at gmail.com  Thu Jul 21 23:08:01 2016
From: markleeds2 at gmail.com (Mark Leeds)
Date: Thu, 21 Jul 2016 17:08:01 -0400
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
	<alpine.DEB.2.20.1607212128490.3823@paninaro>
	<CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>
	<CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>
Message-ID: <CAHz+bWbDLEU4snYrskAWsA_ceGPyf=6vUr2wxJ0cgR91trShsQ@mail.gmail.com>

Hi Jermiah: another possibly faster way would be to use a kalman filtering
framework. I forget the details but duncan and horne have a paper which
shows how a regression can be re-computed each time a new data point is
added .I
forget if they handle taking one off of the back also which is what you
need.

The paper at the link below isn't the paper I'm talking about but it's
reference[1] in that paper. Note that this suggestion might not be a better
approach  than the various approaches already suggested so I wouldn't go
this route unless you're very interested.


Mark

https://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/recurse.pdf






On Thu, Jul 21, 2016 at 4:28 PM, Gabor Grothendieck <ggrothendieck at gmail.com
> wrote:

> I would be careful about making assumptions regarding what is faster.
> Performance tends to be nonintuitive.
>
> When I ran rollapply/lm, rollapply/fastLm and roll_lm on the example
> you provided rollapply/fastLm was three times faster than roll_lm.  Of
> course this could change with data of different dimensions but it
> would be worthwhile to do actual benchmarks before making assumptions.
>
> I also noticed that roll_lm did not give the same coefficients as the
> other two.
>
> set.seed(1)
> library(zoo)
> library(RcppArmadillo)
> library(roll)
> z <- zoo(matrix(rnorm(10), ncol = 2))
> colnames(z) <- c("y", "x")
>
> ## rolling regression of width 4
> library(rbenchmark)
> benchmark(fastLm = rollapplyr(z, width = 4,
>      function(x) coef(fastLm(cbind(1, x[, 2]), x[, 1])),
>      by.column = FALSE),
>    lm = rollapplyr(z, width = 4,
>      function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>      by.column = FALSE),
>    roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, 2, drop =
> F]), 4,
>      center = FALSE))[1:4]
>
>
>      test replications elapsed relative
> 1  fastLm          100    0.22    1.000
> 2      lm          100    0.72    3.273
> 3 roll_lm          100    0.64    2.909
>
> On Thu, Jul 21, 2016 at 3:45 PM, jeremiah rounds
> <roundsjeremiah at gmail.com> wrote:
> >  Thanks all.  roll::roll_lm was essentially what I wanted.   I think
> maybe
> > I would prefer it to have options to return a few more things, but it is
> > the coefficients, and the remaining statistics you might want can be
> > calculated fast enough from there.
> >
> >
> > On Thu, Jul 21, 2016 at 12:36 PM, Achim Zeileis <
> Achim.Zeileis at uibk.ac.at>
> > wrote:
> >
> >> Jeremiah,
> >>
> >> for this purpose there are the "roll" and "RcppRoll" packages. Both use
> >> Rcpp and the former also provides rolling lm models. The latter has a
> >> generic interface that let's you define your own function.
> >>
> >> One thing to pay attention to, though, is the numerical reliability.
> >> Especially on large time series with relatively short windows there is a
> >> good chance of encountering numerically challenging situations. The QR
> >> decomposition used by lm is fairly robust while other more
> straightforward
> >> matrix multiplications may not be. This should be kept in mind when
> writing
> >> your own Rcpp code for plugging it into RcppRoll.
> >>
> >> But I haven't check what the roll package does and how reliable that
> is...
> >>
> >> hth,
> >> Z
> >>
> >>
> >> On Thu, 21 Jul 2016, jeremiah rounds wrote:
> >>
> >> Hi,
> >>>
> >>> A not unusual task is performing a multiple regression in a rolling
> window
> >>> on a time-series.    A standard piece of advice for doing in R is
> >>> something
> >>> like the code that follows at the end of the email.  I am currently
> using
> >>> an "embed" variant of that code and that piece of advice is out there
> too.
> >>>
> >>> But, it occurs to me that for such an easily specified matrix operation
> >>> standard R code is really slow.   rollapply constantly returns to R
> >>> interpreter at each window step for a new lm.   All lm is at its heart
> is
> >>> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in
> rolling
> >>> window you are just incrementing a counter and peeling off rows (or
> >>> columns
> >>> of X and y) of a particular window size, and following that up with
> some
> >>> matrix multiplication in a loop.   The psuedo-code for that Rcpp
> >>> practically writes itself and you might want a wrapper of something
> like:
> >>> rolling_lm (y=y, x=x, width=4).
> >>>
> >>> My question is this: has any of the thousands of R packages out there
> >>> published anything like that.  Rolling window multiple regressions that
> >>> stay in C/C++ until the rolling window completes?  No sense and
> writing it
> >>> if it exist.
> >>>
> >>>
> >>> Thanks,
> >>> Jeremiah
> >>>
> >>> Standard (slow) advice for "rolling window regression" follows:
> >>>
> >>>
> >>> set.seed(1)
> >>> z <- zoo(matrix(rnorm(10), ncol = 2))
> >>> colnames(z) <- c("y", "x")
> >>>
> >>> ## rolling regression of width 4
> >>> rollapply(z, width = 4,
> >>>   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
> >>>   by.column = FALSE, align = "right")
> >>>
> >>> ## result is identical to
> >>> coef(lm(y ~ x, data = z[1:4,]))
> >>> coef(lm(y ~ x, data = z[2:5,]))
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From faradj.g at gmail.com  Thu Jul 21 23:22:44 2016
From: faradj.g at gmail.com (Faradj Koliev)
Date: Thu, 21 Jul 2016 23:22:44 +0200
Subject: [R] How to plot marginal effects (MEM) in R?
Message-ID: <9991F499-B90A-468E-9DDC-2AC264AA5806@gmail.com>

Dear all, 

I have two logistic regression models:


   ? model <- glm(Y ~ X1+X2+X3+X4, data = data, family = "binomial")



   ? modelInteraction <- glm(Y ~ X1+X2+X3+X4+X1*X4, data = data, family = "binomial")

To calculate the marginal effects (MEM approach) for these models, I used the `mfx` package:


   ? a<- logitmfx(model, data=data, atmean=TRUE)



    ?b<- logitmfx(modelInteraction, data=data, atmean=TRUE)


What I want to do now is 1) plot all the results for "model" and 2) show the result just for two variables: X1 and X2. 
3) I also want to plot the interaction term in ?modelInteraction?.


I have been looking around for the solutions but haven't been able to find any. I would appreciate any suggestions. 

A reproducible sample: 

> dput(data)
structure(list(Y = c(0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 
0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 
0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), X1 = c(1L, 0L, 1L, 
0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 
1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 
0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 0L), X2 = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), X3 = c(0L, 0L, 0L, 0L, 0L, 
0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 2L, 2L, 3L, 4L, 5L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
), X4 = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
7L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L)), .Names = c("Y", "X1", "X2", 
"X3", "X4"), row.names = c(NA, -69L), class = "data.frame")




	[[alternative HTML version deleted]]


From roy.mendelssohn at noaa.gov  Thu Jul 21 23:23:38 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Thu, 21 Jul 2016 14:23:38 -0700
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAHz+bWbDLEU4snYrskAWsA_ceGPyf=6vUr2wxJ0cgR91trShsQ@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
	<alpine.DEB.2.20.1607212128490.3823@paninaro>
	<CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>
	<CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>
	<CAHz+bWbDLEU4snYrskAWsA_ceGPyf=6vUr2wxJ0cgR91trShsQ@mail.gmail.com>
Message-ID: <1928D2A4-1B1A-4A27-B676-AFCF52172B0C@noaa.gov>

I have no idea which method produces the fastest results,  but the package KFAS has a function to do recursive regressions using the Kalman filter.  One difference is that it is not, as far as a I can telll, a moving window (so past data are being dropped),  just a recursively computed regression.

HTH,

-Roy

> On Jul 21, 2016, at 2:08 PM, Mark Leeds <markleeds2 at gmail.com> wrote:
> 
> Hi Jermiah: another possibly faster way would be to use a kalman filtering
> framework. I forget the details but duncan and horne have a paper which
> shows how a regression can be re-computed each time a new data point is
> added .I
> forget if they handle taking one off of the back also which is what you
> need.
> 
> The paper at the link below isn't the paper I'm talking about but it's
> reference[1] in that paper. Note that this suggestion might not be a better
> approach  than the various approaches already suggested so I wouldn't go
> this route unless you're very interested.
> 
> 
> Mark
> 
> https://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/recurse.pdf
> 
> 
> 
> 
> 
> 
> On Thu, Jul 21, 2016 at 4:28 PM, Gabor Grothendieck <ggrothendieck at gmail.com
>> wrote:
> 
>> I would be careful about making assumptions regarding what is faster.
>> Performance tends to be nonintuitive.
>> 
>> When I ran rollapply/lm, rollapply/fastLm and roll_lm on the example
>> you provided rollapply/fastLm was three times faster than roll_lm.  Of
>> course this could change with data of different dimensions but it
>> would be worthwhile to do actual benchmarks before making assumptions.
>> 
>> I also noticed that roll_lm did not give the same coefficients as the
>> other two.
>> 
>> set.seed(1)
>> library(zoo)
>> library(RcppArmadillo)
>> library(roll)
>> z <- zoo(matrix(rnorm(10), ncol = 2))
>> colnames(z) <- c("y", "x")
>> 
>> ## rolling regression of width 4
>> library(rbenchmark)
>> benchmark(fastLm = rollapplyr(z, width = 4,
>>     function(x) coef(fastLm(cbind(1, x[, 2]), x[, 1])),
>>     by.column = FALSE),
>>   lm = rollapplyr(z, width = 4,
>>     function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>>     by.column = FALSE),
>>   roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, 2, drop =
>> F]), 4,
>>     center = FALSE))[1:4]
>> 
>> 
>>     test replications elapsed relative
>> 1  fastLm          100    0.22    1.000
>> 2      lm          100    0.72    3.273
>> 3 roll_lm          100    0.64    2.909
>> 
>> On Thu, Jul 21, 2016 at 3:45 PM, jeremiah rounds
>> <roundsjeremiah at gmail.com> wrote:
>>> Thanks all.  roll::roll_lm was essentially what I wanted.   I think
>> maybe
>>> I would prefer it to have options to return a few more things, but it is
>>> the coefficients, and the remaining statistics you might want can be
>>> calculated fast enough from there.
>>> 
>>> 
>>> On Thu, Jul 21, 2016 at 12:36 PM, Achim Zeileis <
>> Achim.Zeileis at uibk.ac.at>
>>> wrote:
>>> 
>>>> Jeremiah,
>>>> 
>>>> for this purpose there are the "roll" and "RcppRoll" packages. Both use
>>>> Rcpp and the former also provides rolling lm models. The latter has a
>>>> generic interface that let's you define your own function.
>>>> 
>>>> One thing to pay attention to, though, is the numerical reliability.
>>>> Especially on large time series with relatively short windows there is a
>>>> good chance of encountering numerically challenging situations. The QR
>>>> decomposition used by lm is fairly robust while other more
>> straightforward
>>>> matrix multiplications may not be. This should be kept in mind when
>> writing
>>>> your own Rcpp code for plugging it into RcppRoll.
>>>> 
>>>> But I haven't check what the roll package does and how reliable that
>> is...
>>>> 
>>>> hth,
>>>> Z
>>>> 
>>>> 
>>>> On Thu, 21 Jul 2016, jeremiah rounds wrote:
>>>> 
>>>> Hi,
>>>>> 
>>>>> A not unusual task is performing a multiple regression in a rolling
>> window
>>>>> on a time-series.    A standard piece of advice for doing in R is
>>>>> something
>>>>> like the code that follows at the end of the email.  I am currently
>> using
>>>>> an "embed" variant of that code and that piece of advice is out there
>> too.
>>>>> 
>>>>> But, it occurs to me that for such an easily specified matrix operation
>>>>> standard R code is really slow.   rollapply constantly returns to R
>>>>> interpreter at each window step for a new lm.   All lm is at its heart
>> is
>>>>> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in
>> rolling
>>>>> window you are just incrementing a counter and peeling off rows (or
>>>>> columns
>>>>> of X and y) of a particular window size, and following that up with
>> some
>>>>> matrix multiplication in a loop.   The psuedo-code for that Rcpp
>>>>> practically writes itself and you might want a wrapper of something
>> like:
>>>>> rolling_lm (y=y, x=x, width=4).
>>>>> 
>>>>> My question is this: has any of the thousands of R packages out there
>>>>> published anything like that.  Rolling window multiple regressions that
>>>>> stay in C/C++ until the rolling window completes?  No sense and
>> writing it
>>>>> if it exist.
>>>>> 
>>>>> 
>>>>> Thanks,
>>>>> Jeremiah
>>>>> 
>>>>> Standard (slow) advice for "rolling window regression" follows:
>>>>> 
>>>>> 
>>>>> set.seed(1)
>>>>> z <- zoo(matrix(rnorm(10), ncol = 2))
>>>>> colnames(z) <- c("y", "x")
>>>>> 
>>>>> ## rolling regression of width 4
>>>>> rollapply(z, width = 4,
>>>>>  function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>>>>>  by.column = FALSE, align = "right")
>>>>> 
>>>>> ## result is identical to
>>>>> coef(lm(y ~ x, data = z[1:4,]))
>>>>> coef(lm(y ~ x, data = z[2:5,]))
>>>>> 
>>>>>        [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> 
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> 
>> --
>> Statistics & Software Consulting
>> GKX Group, GKX Associates Inc.
>> tel: 1-877-GKX-GROUP
>> email: ggrothendieck at gmail.com
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From chocold12 at gmail.com  Thu Jul 21 23:35:01 2016
From: chocold12 at gmail.com (lily li)
Date: Thu, 21 Jul 2016 15:35:01 -0600
Subject: [R] how to expand the dataframe
In-Reply-To: <b17bc1c8-65b1-96fa-26cb-e3f91f46e700@gmail.com>
References: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
	<37BE3680-939B-4AB2-88E3-5C9B4EFE2DD1@comcast.net>
	<CAN5afy8-Y4_ZSN55y1m2hqXN4hWhNbqfxKusVD7Wxeh+9Yq8tA@mail.gmail.com>
	<b17bc1c8-65b1-96fa-26cb-e3f91f46e700@gmail.com>
Message-ID: <CAN5afy9s=c9M3BF-miyk3nqjbSCz-zmL-VOqYwxpd1DpP6dvng@mail.gmail.com>

I use this code, and it works. So has to set 'all=TRUE'.

merge(df, data.frame(time=seq(as.Date("1990-01-01"),
to=as.Date("1990-12-31"), by="days")), all=TRUE)


On Thu, Jul 21, 2016 at 9:22 AM, Daniel Nordlund <djnordlund at gmail.com>
wrote:

> On 7/20/2016 8:26 PM, lily li wrote:
>
>> Yes, I tried to create a dataframe and merge it with the shortened
>> dataframe. The resulting dataframe goes with the short one and truncates
>> the complete date column, so it does not work.
>>
>> On Wed, Jul 20, 2016 at 6:38 PM, David Winsemius <dwinsemius at comcast.net>
>> wrote:
>>
>>
>>> On Jul 20, 2016, at 1:31 PM, lily li <chocold12 at gmail.com> wrote:
>>>>
>>>> Hi R users,
>>>>
>>>> I have a dataframe, where there is a column 'time' represents time
>>>> series
>>>> but is not complete. How to expand the dataframe so this column will
>>>>
>>> become
>>>
>>>> complete, where other columns with the newly added rows have NA values?
>>>> Thanks.
>>>>
>>>> df
>>>> A     B     C     time
>>>> 10    5     3.3 1990-01-01
>>>> 11    5      4     1990-02-07
>>>> 12    4     3      1990-02-14
>>>> ...
>>>>
>>>
>>> Make a dataframe with a 'time' column using seq.Date and merge that
>>> dataframe with your df dataframe.
>>>
>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>
>>> Really .... isn't it time you learned how to send plain text. You've
>>> posted many questions on Rhelp.  It's really not that difficult on
>>> gmail. I
>>> also have a gmail account and have had no difficulty finding instructions
>>> on how to do it.
>>>
>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>>
>>> http://www.R-project.org/posting-guide.html
>>>
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>>
>>>
>>
> Don't just say you tried to do the merge and it doesn't work.  At a
> minimum show us the ACTUAL code you used and give us any error messages you
> got or show us a portion of the results and explain why it is not what you
> expected.  If possible, give us a small amount of data using dput() so that
> we can "play along at home" (i.e. give us a reproducible example).
>
> Dan
>
> Daniel Nordlund
> Port Townsend, WA
>
>
>
> --
> Daniel Noredlund
> Bothell, WA USA
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From roundsjeremiah at gmail.com  Thu Jul 21 23:43:54 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Thu, 21 Jul 2016 14:43:54 -0700
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAHz+bWbDLEU4snYrskAWsA_ceGPyf=6vUr2wxJ0cgR91trShsQ@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
	<alpine.DEB.2.20.1607212128490.3823@paninaro>
	<CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>
	<CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>
	<CAHz+bWbDLEU4snYrskAWsA_ceGPyf=6vUr2wxJ0cgR91trShsQ@mail.gmail.com>
Message-ID: <CAOjnRsZvS7NJ9ujVd9wUv_fsO0bpFamj44bckt7U-92_YuurqQ@mail.gmail.com>

I agree that when appropriate Kalman Filter/Smoothing the higher-quality
way to go about estimating a time-varying coefficient (given that is what
they do),  and I have noted that both the R package "dlm" and the function
"StructTS" handle these problems quickly.  I am working on that in
parallel.

One of the things I am unsure about with Kalman Filters is how to estimate
variance parameters when the process is unusual in some way that isn't in
the model and it is not feasible to adjust the model by-hand.  dlm's dlmMLE
seems to produce non-sense (not because of the author's work but because of
assumptions).  At least with moving window regressions after the unusual
event is past your window the influence of that event is gone.    That
isn't really a question for this group it is more about me reading more.
When I get that "how to handle all the strange things big data throws at
you" worked out for Kalman Filters, I will go back to those because I
certainly like what I see when everything is right.  There is a plethora of
related topics right?  Bayesian Model Averaging, G-ARCH models for
heteroscedasticity, etc.

Anyway... roll::roll_lm, cheers!

Thanks,
Jeremiah



On Thu, Jul 21, 2016 at 2:08 PM, Mark Leeds <markleeds2 at gmail.com> wrote:

> Hi Jermiah: another possibly faster way would be to use a kalman filtering
> framework. I forget the details but duncan and horne have a paper which
> shows how a regression can be re-computed each time a new data point is
> added .I
> forget if they handle taking one off of the back also which is what you
> need.
>
> The paper at the link below isn't the paper I'm talking about but it's
> reference[1] in that paper. Note that this suggestion might not be a better
> approach  than the various approaches already suggested so I wouldn't go
> this route unless you're very interested.
>
>
> Mark
>
> https://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/recurse.pdf
>
>
>
>
>
>
> On Thu, Jul 21, 2016 at 4:28 PM, Gabor Grothendieck <
> ggrothendieck at gmail.com> wrote:
>
>> I would be careful about making assumptions regarding what is faster.
>> Performance tends to be nonintuitive.
>>
>> When I ran rollapply/lm, rollapply/fastLm and roll_lm on the example
>> you provided rollapply/fastLm was three times faster than roll_lm.  Of
>> course this could change with data of different dimensions but it
>> would be worthwhile to do actual benchmarks before making assumptions.
>>
>> I also noticed that roll_lm did not give the same coefficients as the
>> other two.
>>
>> set.seed(1)
>> library(zoo)
>> library(RcppArmadillo)
>> library(roll)
>> z <- zoo(matrix(rnorm(10), ncol = 2))
>> colnames(z) <- c("y", "x")
>>
>> ## rolling regression of width 4
>> library(rbenchmark)
>> benchmark(fastLm = rollapplyr(z, width = 4,
>>      function(x) coef(fastLm(cbind(1, x[, 2]), x[, 1])),
>>      by.column = FALSE),
>>    lm = rollapplyr(z, width = 4,
>>      function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>>      by.column = FALSE),
>>    roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, 2, drop =
>> F]), 4,
>>      center = FALSE))[1:4]
>>
>>
>>      test replications elapsed relative
>> 1  fastLm          100    0.22    1.000
>> 2      lm          100    0.72    3.273
>> 3 roll_lm          100    0.64    2.909
>>
>> On Thu, Jul 21, 2016 at 3:45 PM, jeremiah rounds
>> <roundsjeremiah at gmail.com> wrote:
>> >  Thanks all.  roll::roll_lm was essentially what I wanted.   I think
>> maybe
>> > I would prefer it to have options to return a few more things, but it is
>> > the coefficients, and the remaining statistics you might want can be
>> > calculated fast enough from there.
>> >
>> >
>> > On Thu, Jul 21, 2016 at 12:36 PM, Achim Zeileis <
>> Achim.Zeileis at uibk.ac.at>
>> > wrote:
>> >
>> >> Jeremiah,
>> >>
>> >> for this purpose there are the "roll" and "RcppRoll" packages. Both use
>> >> Rcpp and the former also provides rolling lm models. The latter has a
>> >> generic interface that let's you define your own function.
>> >>
>> >> One thing to pay attention to, though, is the numerical reliability.
>> >> Especially on large time series with relatively short windows there is
>> a
>> >> good chance of encountering numerically challenging situations. The QR
>> >> decomposition used by lm is fairly robust while other more
>> straightforward
>> >> matrix multiplications may not be. This should be kept in mind when
>> writing
>> >> your own Rcpp code for plugging it into RcppRoll.
>> >>
>> >> But I haven't check what the roll package does and how reliable that
>> is...
>> >>
>> >> hth,
>> >> Z
>> >>
>> >>
>> >> On Thu, 21 Jul 2016, jeremiah rounds wrote:
>> >>
>> >> Hi,
>> >>>
>> >>> A not unusual task is performing a multiple regression in a rolling
>> window
>> >>> on a time-series.    A standard piece of advice for doing in R is
>> >>> something
>> >>> like the code that follows at the end of the email.  I am currently
>> using
>> >>> an "embed" variant of that code and that piece of advice is out there
>> too.
>> >>>
>> >>> But, it occurs to me that for such an easily specified matrix
>> operation
>> >>> standard R code is really slow.   rollapply constantly returns to R
>> >>> interpreter at each window step for a new lm.   All lm is at its
>> heart is
>> >>> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in
>> rolling
>> >>> window you are just incrementing a counter and peeling off rows (or
>> >>> columns
>> >>> of X and y) of a particular window size, and following that up with
>> some
>> >>> matrix multiplication in a loop.   The psuedo-code for that Rcpp
>> >>> practically writes itself and you might want a wrapper of something
>> like:
>> >>> rolling_lm (y=y, x=x, width=4).
>> >>>
>> >>> My question is this: has any of the thousands of R packages out there
>> >>> published anything like that.  Rolling window multiple regressions
>> that
>> >>> stay in C/C++ until the rolling window completes?  No sense and
>> writing it
>> >>> if it exist.
>> >>>
>> >>>
>> >>> Thanks,
>> >>> Jeremiah
>> >>>
>> >>> Standard (slow) advice for "rolling window regression" follows:
>> >>>
>> >>>
>> >>> set.seed(1)
>> >>> z <- zoo(matrix(rnorm(10), ncol = 2))
>> >>> colnames(z) <- c("y", "x")
>> >>>
>> >>> ## rolling regression of width 4
>> >>> rollapply(z, width = 4,
>> >>>   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>> >>>   by.column = FALSE, align = "right")
>> >>>
>> >>> ## result is identical to
>> >>> coef(lm(y ~ x, data = z[1:4,]))
>> >>> coef(lm(y ~ x, data = z[2:5,]))
>> >>>
>> >>>         [[alternative HTML version deleted]]
>> >>>
>> >>> ______________________________________________
>> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> PLEASE do read the posting guide
>> >>> http://www.R-project.org/posting-guide.html
>> >>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>>
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Statistics & Software Consulting
>> GKX Group, GKX Associates Inc.
>> tel: 1-877-GKX-GROUP
>> email: ggrothendieck at gmail.com
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Fri Jul 22 00:34:36 2016
From: chocold12 at gmail.com (lily li)
Date: Thu, 21 Jul 2016 16:34:36 -0600
Subject: [R] about interpolating data in r
Message-ID: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>

I have a question about interpolating missing values in a dataframe. The
dataframe is in the following, Column C has no data before 2009-01-05 and
after 2009-12-31, how to interpolate data for the blanks? That is to say,
interpolate linearly between these two gaps using 5.4 and 6.1? Thanks.


df
time                A      B     C
2009-01-01    3      4.5
2009-01-02    4      5
2009-01-03    3.3   6
2009-01-04    4.1   7
2009-01-05    4.4   6.2   5.4
...

2009-11-20    5.1   5.5   6.1
2009-11-21    5.4   4
...
2009-12-31    4.5   6

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Jul 22 00:36:26 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 22 Jul 2016 08:36:26 +1000
Subject: [R] plotCI linetypes
In-Reply-To: <CA+8X3fVb=e_4Du6BW_yFem6-fVb9K5tU6Zz5A_WzkfcVqpkEVQ@mail.gmail.com>
References: <3ba75db0-bc06-d2e4-7993-efcdc4c9c137@staff.uni-marburg.de>
	<CA+8X3fVb=e_4Du6BW_yFem6-fVb9K5tU6Zz5A_WzkfcVqpkEVQ@mail.gmail.com>
Message-ID: <CA+8X3fUWffFXQDQWa6p3wL2izTrVO4e+q_ELtVa2P2qCjj4HPA@mail.gmail.com>

Hi Florian,
As I suspected, you will have to use something other than the "arrows"
function, which most people use to draw "error bars". This is where
the "lty" argument gets used for all of the lines. You could
substitute a function like this:

foobars<-function(x0,y0,x1,y1,length=0.02,col=par("fg"),
 lty=par("lty"),lwd=par("lwd")) {

 segments(x0,y0,x1,y1,col=col,lty=lty,lwd=lwd)
 capx<-diff(par("usr"))[1]*length
 segments(x1-capx,y1,x1+capx,y1,col=col,lty=1,lwd=lwd)
}

for arrows and then replace calls to "arrows" with calls to "foobars".
Obviously "foobars" will only work for vertical bars, but could easily
be modified to handle horizontal bars. I think that should be all you
need.

Jim


On Thu, Jul 21, 2016 at 10:05 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi Florian,
> I'll have to think about this one.Neither plotCI (which Ben Bolker
> wrote) nor dispersion (which I wrote) do this at the moment, but
> perhaps I can work out something that won't be too hard to program.
> I'll  get back to you.
>
> JIm
>
>
> On Thu, Jul 21, 2016 at 8:07 PM, Florian Detsch
> <florian.detsch at staff.uni-marburg.de> wrote:
>> Dear Jim,
>>
>> first of all, thanks for your wonderful work on the plotrix package.
>> In plotCI, I wonder if it was possible to control the line types of lines
>> (i.e., the part connecting point symbols with caps) and caps separately?
>> Please consider the following code snippet to clarify my point.
>>
>> ## sample data from ?plotCI
>> set.seed(10)
>> y <- runif(10)
>> err <- runif(10)
>>
>> ## visualize data
>> par(mfrow = c(2, 1))
>> for (lwd in c(2, 1.25))
>>   plotCI(1:10, y, err, slty = 2, lwd = lwd)
>>
>> Using slty = 2 alongside with lwd = 2 results in only half sides of the line
>> caps being drawn. That means, if I wanted to stick with dashed lines, I'd be
>> forced to downregulate lwd e.g. to a value of 1.25 in order to achieve fully
>> drawn caps. Would it be possible to implement something like llty (for line
>> type of lines; which I would then set to 2) and clty (for line type of caps;
>> which I would then set to 1) or is there any other, probably more convenient
>> solution?
>>
>> Best,
>> Florian
>>
>> --
>> Florian Detsch (M.Sc. Physical Geography)
>> Environmental Informatics
>> Department of Geography
>> Philipps-Universit?t Marburg
>> Deutschhausstra?e 12
>> 35032 (parcel post: 35037) Marburg, Germany
>>
>> Phone: +49 (0) 6421 28-25323
>> Web: http://umweltinformatik-marburg.de/en/staff/florian-detsch/
>>


From wdunlap at tibco.com  Fri Jul 22 00:40:43 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 21 Jul 2016 15:40:43 -0700
Subject: [R] how to expand the dataframe
In-Reply-To: <CAN5afy9s=c9M3BF-miyk3nqjbSCz-zmL-VOqYwxpd1DpP6dvng@mail.gmail.com>
References: <CAN5afy-VpK_avASmz6rTyQ84qSsj_U1OJoNi55fqU-p9Gy8q5Q@mail.gmail.com>
	<37BE3680-939B-4AB2-88E3-5C9B4EFE2DD1@comcast.net>
	<CAN5afy8-Y4_ZSN55y1m2hqXN4hWhNbqfxKusVD7Wxeh+9Yq8tA@mail.gmail.com>
	<b17bc1c8-65b1-96fa-26cb-e3f91f46e700@gmail.com>
	<CAN5afy9s=c9M3BF-miyk3nqjbSCz-zmL-VOqYwxpd1DpP6dvng@mail.gmail.com>
Message-ID: <CAF8bMcavONnvUKmd16nm+e8TZPqPoQiqUyDiA1VLP1DNfi3L+g@mail.gmail.com>

Depending on your situation you may want all=TRUE, all.x=TRUE, or
all.y=TRUE.
I think the SQL people call these outer joins, left outer joins, and right
outer joins.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 21, 2016 at 2:35 PM, lily li <chocold12 at gmail.com> wrote:

> I use this code, and it works. So has to set 'all=TRUE'.
>
> merge(df, data.frame(time=seq(as.Date("1990-01-01"),
> to=as.Date("1990-12-31"), by="days")), all=TRUE)
>
>
> On Thu, Jul 21, 2016 at 9:22 AM, Daniel Nordlund <djnordlund at gmail.com>
> wrote:
>
> > On 7/20/2016 8:26 PM, lily li wrote:
> >
> >> Yes, I tried to create a dataframe and merge it with the shortened
> >> dataframe. The resulting dataframe goes with the short one and truncates
> >> the complete date column, so it does not work.
> >>
> >> On Wed, Jul 20, 2016 at 6:38 PM, David Winsemius <
> dwinsemius at comcast.net>
> >> wrote:
> >>
> >>
> >>> On Jul 20, 2016, at 1:31 PM, lily li <chocold12 at gmail.com> wrote:
> >>>>
> >>>> Hi R users,
> >>>>
> >>>> I have a dataframe, where there is a column 'time' represents time
> >>>> series
> >>>> but is not complete. How to expand the dataframe so this column will
> >>>>
> >>> become
> >>>
> >>>> complete, where other columns with the newly added rows have NA
> values?
> >>>> Thanks.
> >>>>
> >>>> df
> >>>> A     B     C     time
> >>>> 10    5     3.3 1990-01-01
> >>>> 11    5      4     1990-02-07
> >>>> 12    4     3      1990-02-14
> >>>> ...
> >>>>
> >>>
> >>> Make a dataframe with a 'time' column using seq.Date and merge that
> >>> dataframe with your df dataframe.
> >>>
> >>>
> >>>>       [[alternative HTML version deleted]]
> >>>>
> >>>
> >>> Really .... isn't it time you learned how to send plain text. You've
> >>> posted many questions on Rhelp.  It's really not that difficult on
> >>> gmail. I
> >>> also have a gmail account and have had no difficulty finding
> instructions
> >>> on how to do it.
> >>>
> >>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>>
> >>> http://www.R-project.org/posting-guide.html
> >>>
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>> David Winsemius
> >>> Alameda, CA, USA
> >>>
> >>>
> >>>
> >>
> > Don't just say you tried to do the merge and it doesn't work.  At a
> > minimum show us the ACTUAL code you used and give us any error messages
> you
> > got or show us a portion of the results and explain why it is not what
> you
> > expected.  If possible, give us a small amount of data using dput() so
> that
> > we can "play along at home" (i.e. give us a reproducible example).
> >
> > Dan
> >
> > Daniel Nordlund
> > Port Townsend, WA
> >
> >
> >
> > --
> > Daniel Noredlund
> > Bothell, WA USA
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Jul 22 00:48:23 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 21 Jul 2016 15:48:23 -0700
Subject: [R] about interpolating data in r
In-Reply-To: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
References: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
Message-ID: <CAF8bMcYPSgThf7ZLiqnJFXfqRTLgs_ZLS6K5_TPqhQbcgPh-QA@mail.gmail.com>

Try approx(), as in:

df <-
data.frame(A=c(10,11,12),B=c(5,5,4),C=c(3.3,4,3),time=as.Date(c("1990-01-01","1990-02-07","1990-02-14")))
with(df, approx(x=time, y=C, xout=seq(min(time), max(time), by="days")))

Do you notice how one can copy and paste that example out of the
mail an into R to see how it works?  It would help if your questions
had that same property - show how the example data could be created.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 21, 2016 at 3:34 PM, lily li <chocold12 at gmail.com> wrote:

> I have a question about interpolating missing values in a dataframe. The
> dataframe is in the following, Column C has no data before 2009-01-05 and
> after 2009-12-31, how to interpolate data for the blanks? That is to say,
> interpolate linearly between these two gaps using 5.4 and 6.1? Thanks.
>
>
> df
> time                A      B     C
> 2009-01-01    3      4.5
> 2009-01-02    4      5
> 2009-01-03    3.3   6
> 2009-01-04    4.1   7
> 2009-01-05    4.4   6.2   5.4
> ...
>
> 2009-11-20    5.1   5.5   6.1
> 2009-11-21    5.4   4
> ...
> 2009-12-31    4.5   6
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Fri Jul 22 00:54:39 2016
From: chocold12 at gmail.com (lily li)
Date: Thu, 21 Jul 2016 16:54:39 -0600
Subject: [R] about interpolating data in r
In-Reply-To: <CAF8bMcYPSgThf7ZLiqnJFXfqRTLgs_ZLS6K5_TPqhQbcgPh-QA@mail.gmail.com>
References: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
	<CAF8bMcYPSgThf7ZLiqnJFXfqRTLgs_ZLS6K5_TPqhQbcgPh-QA@mail.gmail.com>
Message-ID: <CAN5afy-CY8k==SixH4VYKO50n_=Ye2wefDzf4iW9eyMWcTCEMQ@mail.gmail.com>

Thanks, I meant if there are missing data at the beginning and end of a
dataframe, how to interpolate according to available data?

For example, the A column has missing values at the beginning and end, how
to interpolate linearly between 10 and 12 for the missing values?

df <- data.frame(A=c(NA, NA,10,11,12, NA),B=c(5,5,4,3,4,5),C=c(3.3,4,3,1.5,
2.2,4),time=as.Date(c("1990-01-01","1990-02-
07","1990-02-14","1990-02-28","1990-03-01","1990-03-20")))


On Thu, Jul 21, 2016 at 4:48 PM, William Dunlap <wdunlap at tibco.com> wrote:

> Try approx(), as in:
>
> df <-
> data.frame(A=c(10,11,12),B=c(5,5,4),C=c(3.3,4,3),time=as.Date(c("1990-01-01","1990-02-07","1990-02-14")))
> with(df, approx(x=time, y=C, xout=seq(min(time), max(time), by="days")))
>
> Do you notice how one can copy and paste that example out of the
> mail an into R to see how it works?  It would help if your questions
> had that same property - show how the example data could be created.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, Jul 21, 2016 at 3:34 PM, lily li <chocold12 at gmail.com> wrote:
>
>> I have a question about interpolating missing values in a dataframe. The
>> dataframe is in the following, Column C has no data before 2009-01-05 and
>> after 2009-12-31, how to interpolate data for the blanks? That is to say,
>> interpolate linearly between these two gaps using 5.4 and 6.1? Thanks.
>>
>>
>> df
>> time                A      B     C
>> 2009-01-01    3      4.5
>> 2009-01-02    4      5
>> 2009-01-03    3.3   6
>> 2009-01-04    4.1   7
>> 2009-01-05    4.4   6.2   5.4
>> ...
>>
>> 2009-11-20    5.1   5.5   6.1
>> 2009-11-21    5.4   4
>> ...
>> 2009-12-31    4.5   6
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From sezenismail at gmail.com  Fri Jul 22 01:14:54 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 22 Jul 2016 02:14:54 +0300
Subject: [R] about interpolating data in r
In-Reply-To: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
References: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
Message-ID: <4F96E751-7A44-40E7-8CE5-4DE6E249A57B@gmail.com>


> On 22 Jul 2016, at 01:34, lily li <chocold12 at gmail.com> wrote:
> 
> I have a question about interpolating missing values in a dataframe.

First of all, filling missing values action must be taken into account very carefully. It must be known the nature of the data that wanted to be filled and most of the time, to let them be NA is the most appropriate action.

> The
> dataframe is in the following, Column C has no data before 2009-01-05 and
> after 2009-12-31, how to interpolate data for the blanks?

Why a dataframe? Is there any relationship between columns A,B and C? If there is, then you might want to consider filling missing values by a linear model approach instead of interpolation. You said that there is not data before 2009-01-05 and after 2009-12-31 but according to dataframe, there is not data after 2009-11-20?

> That is to say,
> interpolate linearly between these two gaps using 5.4 and 6.1? Thanks.

Also you metion interpolating blanks but you want interpolation between two gaps? Do you want to fill missing values before 2009-01-05 and after 2009-11-20 or do you want to find intermediate values between 2009-01-05 and 2009-11-20? This is a bit unclear.

> 
> 
> df
> time                A      B     C
> 2009-01-01    3      4.5
> 2009-01-02    4      5
> 2009-01-03    3.3   6
> 2009-01-04    4.1   7
> 2009-01-05    4.4   6.2   5.4
> ...
> 
> 2009-11-20    5.1   5.5   6.1
> 2009-11-21    5.4   4
> ...
> 2009-12-31    4.5   6


If you want to fill missing values at the end-points for column C (before 2009-01-05 and after 2009-11-20), and all data you have is between 2009-01-05 and 2009-11-20, this means that you want extrapolation (guessing unkonwn values that is out of known values). So, you can use only values at column C to guess missing end-point values. You can use splinefun (or spline) functions for this purpose. But let me note that this kind of approach might help you only for a few missing values close to end-points. Otherwise, you might find yourself in a huge mistake. 

As I mentioned in my first sentence, If you have a relationship between all columns or you have data for column C for other years (for instance, assume that you have data for column C for 2007, 2008, and 2010 but not 2009) you may want to try a statistical approach to fill the missing values.


From sezenismail at gmail.com  Fri Jul 22 01:38:07 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 22 Jul 2016 02:38:07 +0300
Subject: [R] about interpolating data in r
In-Reply-To: <CAN5afy-CY8k==SixH4VYKO50n_=Ye2wefDzf4iW9eyMWcTCEMQ@mail.gmail.com>
References: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
	<CAF8bMcYPSgThf7ZLiqnJFXfqRTLgs_ZLS6K5_TPqhQbcgPh-QA@mail.gmail.com>
	<CAN5afy-CY8k==SixH4VYKO50n_=Ye2wefDzf4iW9eyMWcTCEMQ@mail.gmail.com>
Message-ID: <140985CB-8A57-4406-9A80-E75C68375AB7@gmail.com>


> On 22 Jul 2016, at 01:54, lily li <chocold12 at gmail.com> wrote:
> 
> Thanks, I meant if there are missing data at the beginning and end of a
> dataframe, how to interpolate according to available data?
> 
> For example, the A column has missing values at the beginning and end, how
> to interpolate linearly between 10 and 12 for the missing values?
> 
> df <- data.frame(A=c(NA, NA,10,11,12, NA),B=c(5,5,4,3,4,5),C=c(3.3,4,3,1.5,
> 2.2,4),time=as.Date(c("1990-01-01","1990-02-
> 07","1990-02-14","1990-02-28","1990-03-01","1990-03-20")))
> 

As William was answered;

with(df, approx(x=time, y=A, xout=seq(min(time, na.rm =T), max(time, na.rm = T), by="days")))

will help you interpolate linearly between knwon values even column has NA?s.


> 
> On Thu, Jul 21, 2016 at 4:48 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
>> Try approx(), as in:
>> 
>> df <-
>> data.frame(A=c(10,11,12),B=c(5,5,4),C=c(3.3,4,3),time=as.Date(c("1990-01-01","1990-02-07","1990-02-14")))
>> with(df, approx(x=time, y=C, xout=seq(min(time), max(time), by="days")))
>> 
>> Do you notice how one can copy and paste that example out of the
>> mail an into R to see how it works?  It would help if your questions
>> had that same property - show how the example data could be created.
>> 
>> 
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>> 
>> On Thu, Jul 21, 2016 at 3:34 PM, lily li <chocold12 at gmail.com> wrote:
>> 
>>> I have a question about interpolating missing values in a dataframe. The
>>> dataframe is in the following, Column C has no data before 2009-01-05 and
>>> after 2009-12-31, how to interpolate data for the blanks? That is to say,
>>> interpolate linearly between these two gaps using 5.4 and 6.1? Thanks.
>>> 
>>> 
>>> df
>>> time                A      B     C
>>> 2009-01-01    3      4.5
>>> 2009-01-02    4      5
>>> 2009-01-03    3.3   6
>>> 2009-01-04    4.1   7
>>> 2009-01-05    4.4   6.2   5.4
>>> ...
>>> 
>>> 2009-11-20    5.1   5.5   6.1
>>> 2009-11-21    5.4   4
>>> ...
>>> 2009-12-31    4.5   6
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From stochastickang at gmail.com  Fri Jul 22 02:26:26 2016
From: stochastickang at gmail.com (Steven Kang)
Date: Fri, 22 Jul 2016 10:26:26 +1000
Subject: [R] PDF extraction with tm package
Message-ID: <CAKu3pN7m7r5gWh3LLaFPa60RPnZnf86Wy2XdQEd6mSp2ROAxvg@mail.gmail.com>

Hi R users,

I?m having some issues trying to extract texts from PDF file using tm
package.

Here are the steps that were carried out:

1. Downloaded and installed the following programs:

- Xpdf (Copied the ?bin32?, ?bin64?, ?doc? folders into ?C:\Program
Files\Xpdf? directory; also added C:\Program Files\Xpdf\bin64\pdfinfo.exe &
C:\Program Files\Xpdf\bin64\pdftotext.exe in existing PATH

- Tesseract

- Imagemagick

2. Used the following scripts and the corresponding error messages:

# Directory where PDF files are stored

>cname <- getwd()

>Corpus(DirSource(cname), readerControl=list(reader = readPDF))

Error in system2("pdftotext", c(control$text, shQuote(x), "-"), stdout =
TRUE) :
'"pdftotext"' not found

 In addition: Warning message:

running command '"pdfinfo" "C:\Users\R_Files\XXX.pdf"' had status 127

>file.exists(Sys.which(c("pdfinfo","pdftpotext")))
[1] FALSE FALSE

It seems like R can?t find pdfinfo & pdftotext exe files, but not sure as
to why this would be the case despite xpdf files being copied into
?C:\Program Files? (Im using Windows 7 64bits)

I?m aware that ?pdf_text? function from pdftools package can extract texts
from PDF file and outputs into a string. But I was after something which is
able to convert PDF (ie transaction data) into a dataframe without regular
expression. Is tm package capable of doing this conversion? Are there any
other alternatives to these methods?

Your expertise in resolving this problem would be highly appreciated.


Steve

	[[alternative HTML version deleted]]


From arbautjc at gmail.com  Thu Jul 21 23:13:35 2016
From: arbautjc at gmail.com (Jean-Claude Arbaut)
Date: Thu, 21 Jul 2016 23:13:35 +0200
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
Message-ID: <CANufCk5AjWrnN5_TYqAAX3nH7XpGQUwMxsbpSj84vQuOUR2Jgg@mail.gmail.com>

This may be useful:

Sven Hammarling and Craig Lucas
"Updating the QR factorization and the least squares problem"
http://eprints.ma.man.ac.uk/1192/01/covered/MIMS_ep2008_111.pdf
http://www.maths.manchester.ac.uk/~clucas/updating/

2016-07-21 20:02 GMT+02:00 jeremiah rounds <roundsjeremiah at gmail.com>:
> Hi,
>
> A not unusual task is performing a multiple regression in a rolling window
> on a time-series.    A standard piece of advice for doing in R is something
> like the code that follows at the end of the email.  I am currently using
> an "embed" variant of that code and that piece of advice is out there too.
>
> But, it occurs to me that for such an easily specified matrix operation
> standard R code is really slow.   rollapply constantly returns to R
> interpreter at each window step for a new lm.   All lm is at its heart is
> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in rolling
> window you are just incrementing a counter and peeling off rows (or columns
> of X and y) of a particular window size, and following that up with some
> matrix multiplication in a loop.   The psuedo-code for that Rcpp
> practically writes itself and you might want a wrapper of something like:
> rolling_lm (y=y, x=x, width=4).
>
> My question is this: has any of the thousands of R packages out there
> published anything like that.  Rolling window multiple regressions that
> stay in C/C++ until the rolling window completes?  No sense and writing it
> if it exist.
>
>
> Thanks,
> Jeremiah
>
> Standard (slow) advice for "rolling window regression" follows:
>
>
> set.seed(1)
> z <- zoo(matrix(rnorm(10), ncol = 2))
> colnames(z) <- c("y", "x")
>
> ## rolling regression of width 4
> rollapply(z, width = 4,
>    function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>    by.column = FALSE, align = "right")
>
> ## result is identical to
> coef(lm(y ~ x, data = z[1:4,]))
> coef(lm(y ~ x, data = z[2:5,]))
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From chocold12 at gmail.com  Thu Jul 21 23:36:42 2016
From: chocold12 at gmail.com (lily li)
Date: Thu, 21 Jul 2016 15:36:42 -0600
Subject: [R] installing the lubricate package
Message-ID: <CAN5afy_S9ABWsc6aB0tucofdh_7QSnHWM5X94C8OJ9638hgEEw@mail.gmail.com>

Hi R users,

I'm trying to download lubricate from this website, and then install it on
my mac.
https://github.com/hadley/lubridate

but it says windows version does not apply to mac. How to install the
package for mac? Thanks.

	[[alternative HTML version deleted]]


From qinghua.he at yahoo.com  Fri Jul 22 00:04:22 2016
From: qinghua.he at yahoo.com (Qinghua He)
Date: Thu, 21 Jul 2016 22:04:22 +0000 (UTC)
Subject: [R] Why the order of parameters in a logistic regression affects
 results significantly?
References: <1104610284.3375689.1469138662609.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1104610284.3375689.1469138662609.JavaMail.yahoo@mail.yahoo.com>

Using the same data, if I ran
fit2 <-glm(formula=AR~Age+LumA+LumB+HER2+Basal+Normal,family=binomial,data=RacComp1)summary(fit2)exp(coef(fit2))?
I obtained:
> exp(coef(fit2))(Intercept) ? ? ? ? Age ? ? ? ?LumA ? ? ? ?LumB ? ? ? ?HER2 ? ? ? Basal ? ? ?Normal??0.24866935 ?1.00433781 ?0.10639937 ?0.31614001 ?0.08220685 20.25180956 ? ? ? ? ?NA?
while if I ran

fit2 <-glm(formula=AR~Age+LumA+LumB+Basal+Normal+HER2,family=binomial,data=RacComp1)summary(fit2)exp(coef(fit2))
I obtained:
> exp(coef(fit2))?(Intercept) ? ? ? ? ?Age ? ? ? ? LumA ? ? ? ? LumB ? ? ? ?Basal ? ? ? Normal ? ? ? ? HER2?? 0.02044232 ? 1.00433781 ? 1.29428846 ? 3.84566516 246.35185956 ?12.16443690 ? ? ? ? ? NA?

Essentially they're the same model - I just moved HER2 to the last. But the OR changed significantly. Can someone explain?
For the latter result, I don't even know how to interpret as all factors have OR>1 (except Intercept), how could that possible? Can I eliminate the effect of intercept?
Also, I cannot obtain OR for the last factor due to collinearity. However, I know others obtained OR for all factors for the same dataset. Can someone tell me how to obtain OR for all factors? All factors are categorical variables (i.e., 0 or 1).
Thanks!
Peter
	[[alternative HTML version deleted]]


From Alexander.Herr at csiro.au  Fri Jul 22 04:45:32 2016
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Fri, 22 Jul 2016 02:45:32 +0000
Subject: [R] adding new values to dataframe based on existing values in two
 columns
In-Reply-To: <mailman.7.1468922401.11334.r-help@r-project.org>
References: <mailman.7.1468922401.11334.r-help@r-project.org>
Message-ID: <1469155527868.90189@csiro.au>

Hiya,

I am trying to assign minimum values to a dataframe based on existing columns.  I can do this via loops, but surely R has much more elegant solutions...

Here is my code:
set.seed(666)
xyz<-as.data.frame(cbind(x=rep(rpois(50,10),2)+1, y=rep(rpois(50,10),2)+1,z=runif(100, min=-3, max=40)))
xyz[order(xyz[,1], xyz[,2]),]->xyz
 unique(xyz[,1:2])
dim(xyz)
   aggregate(xyz[,3],by=list(x=xyz[,1],y=xyz[,2]), min)->mins
   
 xyz$mins<-rep(NA, nrow(xyz))

#now assign min values to each xy combination
for(i in unique(xyz[,1])) {
       mins[mins[,1]==i,]->mm
        for( j in unique(mm[,2])) {
            mins[mins[,1]==i & mins[,2] == j,3]->xyz[xyz[,1]==i & xyz[,2]==j,4]
        }
}

Thanks and cheers
Herry



From markleeds2 at gmail.com  Fri Jul 22 06:48:51 2016
From: markleeds2 at gmail.com (Mark Leeds)
Date: Fri, 22 Jul 2016 00:48:51 -0400
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAOjnRsZvS7NJ9ujVd9wUv_fsO0bpFamj44bckt7U-92_YuurqQ@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
	<alpine.DEB.2.20.1607212128490.3823@paninaro>
	<CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>
	<CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>
	<CAHz+bWbDLEU4snYrskAWsA_ceGPyf=6vUr2wxJ0cgR91trShsQ@mail.gmail.com>
	<CAOjnRsZvS7NJ9ujVd9wUv_fsO0bpFamj44bckt7U-92_YuurqQ@mail.gmail.com>
Message-ID: <CAHz+bWZtmN03SUgcr9fkEzYnSgx0r7=zVap3bUhDGTiXouNh6g@mail.gmail.com>

Hi Jeremiah: I think I wasn't that clear. I'm not suggesting  the kalman
filter to deal with time varying coefficients. As Roy pointed out, one can
use the kalman filter to do regular regression where one "sees" a new data
point as each time unit passes. It can be assumed that the coefficients do
not vary ( basically by having no variance in the system equation ).

The problem as I see it, is that Duncan and Horn's approach ( and Roy
alluded
to this problem also ), only deals with adding one point at a time to the
front of the data set.  It doesn't handle the fact that you want to drop
the nth observation and everything older than that observation  also.

don't know how easy it would be  to modify their approach to deal with the
fact that you are using a moving window rather than just adding one point
at a time.

The main point I wanted to get across here is that I was not suggesting the
KF as a way to handle varying coefficients. You can assume that they're
fixed and still use it. See the reference I pointed out for more on this
approach and my apologies for the confusion.
















On Thu, Jul 21, 2016 at 5:43 PM, jeremiah rounds <roundsjeremiah at gmail.com>
wrote:

> I agree that when appropriate Kalman Filter/Smoothing the higher-quality
> way to go about estimating a time-varying coefficient (given that is what
> they do),  and I have noted that both the R package "dlm" and the function
> "StructTS" handle these problems quickly.  I am working on that in
> parallel.
>
> One of the things I am unsure about with Kalman Filters is how to estimate
> variance parameters when the process is unusual in some way that isn't in
> the model and it is not feasible to adjust the model by-hand.  dlm's dlmMLE
> seems to produce non-sense (not because of the author's work but because of
> assumptions).  At least with moving window regressions after the unusual
> event is past your window the influence of that event is gone.    That
> isn't really a question for this group it is more about me reading more.
> When I get that "how to handle all the strange things big data throws at
> you" worked out for Kalman Filters, I will go back to those because I
> certainly like what I see when everything is right.  There is a plethora of
> related topics right?  Bayesian Model Averaging, G-ARCH models for
> heteroscedasticity, etc.
>
> Anyway... roll::roll_lm, cheers!
>
> Thanks,
> Jeremiah
>
>
>
> On Thu, Jul 21, 2016 at 2:08 PM, Mark Leeds <markleeds2 at gmail.com> wrote:
>
>> Hi Jermiah: another possibly faster way would be to use a kalman
>> filtering framework. I forget the details but duncan and horne have a paper
>> which shows how a regression can be re-computed each time a new data point
>> is added .I
>> forget if they handle taking one off of the back also which is what you
>> need.
>>
>> The paper at the link below isn't the paper I'm talking about but it's
>> reference[1] in that paper. Note that this suggestion might not be a better
>> approach  than the various approaches already suggested so I wouldn't go
>> this route unless you're very interested.
>>
>>
>> Mark
>>
>> https://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/recurse.pdf
>>
>>
>>
>>
>>
>>
>> On Thu, Jul 21, 2016 at 4:28 PM, Gabor Grothendieck <
>> ggrothendieck at gmail.com> wrote:
>>
>>> I would be careful about making assumptions regarding what is faster.
>>> Performance tends to be nonintuitive.
>>>
>>> When I ran rollapply/lm, rollapply/fastLm and roll_lm on the example
>>> you provided rollapply/fastLm was three times faster than roll_lm.  Of
>>> course this could change with data of different dimensions but it
>>> would be worthwhile to do actual benchmarks before making assumptions.
>>>
>>> I also noticed that roll_lm did not give the same coefficients as the
>>> other two.
>>>
>>> set.seed(1)
>>> library(zoo)
>>> library(RcppArmadillo)
>>> library(roll)
>>> z <- zoo(matrix(rnorm(10), ncol = 2))
>>> colnames(z) <- c("y", "x")
>>>
>>> ## rolling regression of width 4
>>> library(rbenchmark)
>>> benchmark(fastLm = rollapplyr(z, width = 4,
>>>      function(x) coef(fastLm(cbind(1, x[, 2]), x[, 1])),
>>>      by.column = FALSE),
>>>    lm = rollapplyr(z, width = 4,
>>>      function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>>>      by.column = FALSE),
>>>    roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, 2, drop =
>>> F]), 4,
>>>      center = FALSE))[1:4]
>>>
>>>
>>>      test replications elapsed relative
>>> 1  fastLm          100    0.22    1.000
>>> 2      lm          100    0.72    3.273
>>> 3 roll_lm          100    0.64    2.909
>>>
>>> On Thu, Jul 21, 2016 at 3:45 PM, jeremiah rounds
>>> <roundsjeremiah at gmail.com> wrote:
>>> >  Thanks all.  roll::roll_lm was essentially what I wanted.   I think
>>> maybe
>>> > I would prefer it to have options to return a few more things, but it
>>> is
>>> > the coefficients, and the remaining statistics you might want can be
>>> > calculated fast enough from there.
>>> >
>>> >
>>> > On Thu, Jul 21, 2016 at 12:36 PM, Achim Zeileis <
>>> Achim.Zeileis at uibk.ac.at>
>>> > wrote:
>>> >
>>> >> Jeremiah,
>>> >>
>>> >> for this purpose there are the "roll" and "RcppRoll" packages. Both
>>> use
>>> >> Rcpp and the former also provides rolling lm models. The latter has a
>>> >> generic interface that let's you define your own function.
>>> >>
>>> >> One thing to pay attention to, though, is the numerical reliability.
>>> >> Especially on large time series with relatively short windows there
>>> is a
>>> >> good chance of encountering numerically challenging situations. The QR
>>> >> decomposition used by lm is fairly robust while other more
>>> straightforward
>>> >> matrix multiplications may not be. This should be kept in mind when
>>> writing
>>> >> your own Rcpp code for plugging it into RcppRoll.
>>> >>
>>> >> But I haven't check what the roll package does and how reliable that
>>> is...
>>> >>
>>> >> hth,
>>> >> Z
>>> >>
>>> >>
>>> >> On Thu, 21 Jul 2016, jeremiah rounds wrote:
>>> >>
>>> >> Hi,
>>> >>>
>>> >>> A not unusual task is performing a multiple regression in a rolling
>>> window
>>> >>> on a time-series.    A standard piece of advice for doing in R is
>>> >>> something
>>> >>> like the code that follows at the end of the email.  I am currently
>>> using
>>> >>> an "embed" variant of that code and that piece of advice is out
>>> there too.
>>> >>>
>>> >>> But, it occurs to me that for such an easily specified matrix
>>> operation
>>> >>> standard R code is really slow.   rollapply constantly returns to R
>>> >>> interpreter at each window step for a new lm.   All lm is at its
>>> heart is
>>> >>> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in
>>> rolling
>>> >>> window you are just incrementing a counter and peeling off rows (or
>>> >>> columns
>>> >>> of X and y) of a particular window size, and following that up with
>>> some
>>> >>> matrix multiplication in a loop.   The psuedo-code for that Rcpp
>>> >>> practically writes itself and you might want a wrapper of something
>>> like:
>>> >>> rolling_lm (y=y, x=x, width=4).
>>> >>>
>>> >>> My question is this: has any of the thousands of R packages out there
>>> >>> published anything like that.  Rolling window multiple regressions
>>> that
>>> >>> stay in C/C++ until the rolling window completes?  No sense and
>>> writing it
>>> >>> if it exist.
>>> >>>
>>> >>>
>>> >>> Thanks,
>>> >>> Jeremiah
>>> >>>
>>> >>> Standard (slow) advice for "rolling window regression" follows:
>>> >>>
>>> >>>
>>> >>> set.seed(1)
>>> >>> z <- zoo(matrix(rnorm(10), ncol = 2))
>>> >>> colnames(z) <- c("y", "x")
>>> >>>
>>> >>> ## rolling regression of width 4
>>> >>> rollapply(z, width = 4,
>>> >>>   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
>>> >>>   by.column = FALSE, align = "right")
>>> >>>
>>> >>> ## result is identical to
>>> >>> coef(lm(y ~ x, data = z[1:4,]))
>>> >>> coef(lm(y ~ x, data = z[2:5,]))
>>> >>>
>>> >>>         [[alternative HTML version deleted]]
>>> >>>
>>> >>> ______________________________________________
>>> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>> PLEASE do read the posting guide
>>> >>> http://www.R-project.org/posting-guide.html
>>> >>> and provide commented, minimal, self-contained, reproducible code.
>>> >>>
>>> >>>
>>> >
>>> >         [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>> --
>>> Statistics & Software Consulting
>>> GKX Group, GKX Associates Inc.
>>> tel: 1-877-GKX-GROUP
>>> email: ggrothendieck at gmail.com
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From sezenismail at gmail.com  Fri Jul 22 02:44:01 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Fri, 22 Jul 2016 00:44:01 +0000
Subject: [R] installing the lubricate package
In-Reply-To: <CAN5afy_S9ABWsc6aB0tucofdh_7QSnHWM5X94C8OJ9638hgEEw@mail.gmail.com>
References: <CAN5afy_S9ABWsc6aB0tucofdh_7QSnHWM5X94C8OJ9638hgEEw@mail.gmail.com>
Message-ID: <CABOzBw2UPcoww9uq2SupK3=G8C6iV+gbv2oc0ShnAVxuoGB+tQ@mail.gmail.com>

You don't have to download and install from github. You can install
lubridate package easly from cran repository. If you really intend to
install from github, i advise you install devtools package first and use
install_github function.

http://www.inside-r.org/packages/cran/devtools/docs/install_github

On Fri, Jul 22, 2016, 03:36 lily li <chocold12 at gmail.com> wrote:

> Hi R users,
>
> I'm trying to download lubricate from this website, and then install it on
> my mac.
> https://github.com/hadley/lubridate
>
> but it says windows version does not apply to mac. How to install the
> package for mac? Thanks.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jul 22 08:35:18 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 21 Jul 2016 23:35:18 -0700
Subject: [R] How to plot marginal effects (MEM) in R?
In-Reply-To: <9991F499-B90A-468E-9DDC-2AC264AA5806@gmail.com>
References: <9991F499-B90A-468E-9DDC-2AC264AA5806@gmail.com>
Message-ID: <994EE187-1B2F-49F5-9A6A-D01FF2C4EF66@comcast.net>


> On Jul 21, 2016, at 2:22 PM, Faradj Koliev <faradj.g at gmail.com> wrote:
> 
> Dear all, 
> 
> I have two logistic regression models:
> 
> 
>   ? model <- glm(Y ~ X1+X2+X3+X4, data = data, family = "binomial")
> 
> 
> 
>   ? modelInteraction <- glm(Y ~ X1+X2+X3+X4+X1*X4, data = data, family = "binomial")
> 
> To calculate the marginal effects (MEM approach) for these models, I used the `mfx` package:
> 
> 
>   ? a<- logitmfx(model, data=data, atmean=TRUE)
> 
> 
> 
>    ?b<- logitmfx(modelInteraction, data=data, atmean=TRUE)
> 
> 
> What I want to do now is 1) plot all the results for "model" and 2) show the result just for two variables: X1 and X2. 
> 3) I also want to plot the interaction term in ?modelInteraction?.

There is no longer a single "effect" for X1 in modelInteraction in contrast to the manner as there might be an "effect" for X2. There can only be predictions for combined situations with particular combinations of values for X1 and X4.

> model

Call:  glm(formula = Y ~ X1 + X2 + X3 + X4, family = "binomial", data = data)

Coefficients:
(Intercept)           X1           X2           X3           X4  
    -0.3601       1.3353       0.1056       0.2898      -0.3705  

Degrees of Freedom: 68 Total (i.e. Null);  64 Residual
Null Deviance:	    66.78 
Residual Deviance: 62.27 	AIC: 72.27


> modelInteraction

Call:  glm(formula = Y ~ X1 + X2 + X3 + X4 + X1 * X4, family = "binomial", 
    data = data)

Coefficients:
(Intercept)           X1           X2           X3           X4        X1:X4  
    90.0158     -90.0747       0.1183       0.3064     -15.3688      15.1593  

Degrees of Freedom: 68 Total (i.e. Null);  63 Residual
Null Deviance:	    66.78 
Residual Deviance: 61.49 	AIC: 73.49

Notice that a naive attempt to plot an X1  "effect" in modelInteraction might pick the -90.07 value which would then ignore both the much larger Intercept value and also ignore the fact that the interaction term has now split the X4 (and X1) "effects" into multiple pieces.

You need to interpret the effects of X1 in the context of a specification of a particular X4 value and not forget that the Intercept should not be ignored. It appears to me that the estimates of the mfx package are essentially meaningless with the problem you have thrown at it.

> a
Call:
logitmfx(formula = model, data = data, atmean = TRUE)

Marginal Effects:
       dF/dx Std. Err.       z   P>|z|  
X1  0.147532  0.087865  1.6791 0.09314 .
X2  0.015085  0.193888  0.0778 0.93798  
X3  0.040309  0.063324  0.6366 0.52441  
X4 -0.050393  0.092947 -0.5422 0.58770  
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

dF/dx is for discrete change for the following variables:

[1] "X1" "X2" "X4"
> b
Call:
logitmfx(formula = modelInteraction, data = data, atmean = TRUE)

Marginal Effects:
            dF/dx   Std. Err.         z  P>|z|    
X1    -1.0000e+00  1.2121e-07 -8.25e+06 <2e-16 ***
X2     6.5595e-03  8.1616e-01  8.00e-03 0.9936    
X3     1.6312e-02  2.0326e+00  8.00e-03 0.9936    
X4    -9.6831e-01  1.5806e+01 -6.13e-02 0.9511    
X1:X4  8.0703e-01  1.4572e+01  5.54e-02 0.9558    
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

dF/dx is for discrete change for the following variables:

[1] "X1" "X2" "X4"

I see no sensible interpretation of the phrase "X1 effect" in the comparison tables above. The "p-value" in the second table appears to be nonsense induced by throwing a model formulation that was not anticipated. There is a negligible improvement in the glm fits:

> anova(model,modelInteraction)
Analysis of Deviance Table

Model 1: Y ~ X1 + X2 + X3 + X4
Model 2: Y ~ X1 + X2 + X3 + X4 + X1 * X4
  Resid. Df Resid. Dev Df Deviance
1        64     62.274            
2        63     61.495  1  0.77908


So the notion that the "X1 effect" is now "highly significant" where it was before not even suggestive of significance seem to point to either an error in the underlying theory or a failure to anticipate and trap (and warn the user) that an erroneous model (or at least an unanticipated model) is being passed to a procedure.

At least the 'effects- package gives you a tiny warning about this issue, although I think it really should throw an informative error when a user attempts to estimate only a "main effect" in a model that has an interaction involving such a covariate:

> library(effects)

> effect('X1', model)

 X1 effect
X1
         0        0.2        0.4        0.6        0.8          1 
0.06706123 0.08582757 0.10923061 0.13805139 0.17299973 0.21459275 
> effect('X1', modelInteraction)
NOTE: X1 is not a high-order term in the model

 X1 effect
X1
           0          0.2          0.4          0.6          0.8            1 
0.0002418661 0.0009864740 0.0040142251 0.0161843996 0.0629206979 0.2151098752 
> effect('X1:X4', modelInteraction)

 X1*X4 effect
     X4
X1            6         6.2          6.4          6.6          6.8            7
  0   0.1100479 0.005686142 0.0002643982 1.223058e-05 5.656287e-07 2.615838e-08
  0.2 0.1285241 0.012352473 0.0010595321 8.994106e-05 7.628099e-06 6.469071e-07
  0.4 0.1495811 0.026625017 0.0042357682 6.610806e-04 1.028639e-04 1.599803e-05
  0.6 0.1734015 0.056446132 0.0167737545 4.841483e-03 1.385458e-03 3.954877e-04
  0.8 0.2001225 0.115698003 0.0640377838 3.454327e-02 1.836677e-02 9.689636e-03
  1   0.2298165 0.222481442 0.2153150766 2.083177e-01 2.014893e-01 1.948297e-01

-- 
David.



> 
> 
> I have been looking around for the solutions but haven't been able to find any. I would appreciate any suggestions. 
> 
> A reproducible sample: 
> 
>> dput(data)
> structure(list(Y = c(0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 
> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
> 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 
> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), X1 = c(1L, 0L, 1L, 
> 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 
> 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 
> 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
> 1L, 0L), X2 = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
> 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
> 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), X3 = c(0L, 0L, 0L, 0L, 0L, 
> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 2L, 2L, 3L, 4L, 5L, 0L, 0L, 
> 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
> ), X4 = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
> 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 6L, 6L, 6L, 6L, 6L, 6L, 
> 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
> 7L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 
> 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L)), .Names = c("Y", "X1", "X2", 
> "X3", "X4"), row.names = c(NA, -69L), class = "data.frame")
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From faradj.g at gmail.com  Fri Jul 22 08:44:35 2016
From: faradj.g at gmail.com (Faradj Koliev)
Date: Fri, 22 Jul 2016 08:44:35 +0200
Subject: [R] How to plot marginal effects (MEM) in R?
In-Reply-To: <994EE187-1B2F-49F5-9A6A-D01FF2C4EF66@comcast.net>
References: <9991F499-B90A-468E-9DDC-2AC264AA5806@gmail.com>
	<994EE187-1B2F-49F5-9A6A-D01FF2C4EF66@comcast.net>
Message-ID: <154826AF-AA98-4A0F-88ED-0928D12AFCCD@gmail.com>

Dear David Winsemius, 

Thank you!  

The sample make no sense, I know. The real data is too big. So, I only want to understand how to plot marginal effects, to visualize them in a proper way. 

Best,


> 22 juli 2016 kl. 08:35 skrev David Winsemius <dwinsemius at comcast.net>:
> 
>> 
>> On Jul 21, 2016, at 2:22 PM, Faradj Koliev <faradj.g at gmail.com> wrote:
>> 
>> Dear all, 
>> 
>> I have two logistic regression models:
>> 
>> 
>>  ? model <- glm(Y ~ X1+X2+X3+X4, data = data, family = "binomial")
>> 
>> 
>> 
>>  ? modelInteraction <- glm(Y ~ X1+X2+X3+X4+X1*X4, data = data, family = "binomial")
>> 
>> To calculate the marginal effects (MEM approach) for these models, I used the `mfx` package:
>> 
>> 
>>  ? a<- logitmfx(model, data=data, atmean=TRUE)
>> 
>> 
>> 
>>   ?b<- logitmfx(modelInteraction, data=data, atmean=TRUE)
>> 
>> 
>> What I want to do now is 1) plot all the results for "model" and 2) show the result just for two variables: X1 and X2. 
>> 3) I also want to plot the interaction term in ?modelInteraction?.
> 
> There is no longer a single "effect" for X1 in modelInteraction in contrast to the manner as there might be an "effect" for X2. There can only be predictions for combined situations with particular combinations of values for X1 and X4.
> 
>> model
> 
> Call:  glm(formula = Y ~ X1 + X2 + X3 + X4, family = "binomial", data = data)
> 
> Coefficients:
> (Intercept)           X1           X2           X3           X4  
>    -0.3601       1.3353       0.1056       0.2898      -0.3705  
> 
> Degrees of Freedom: 68 Total (i.e. Null);  64 Residual
> Null Deviance:	    66.78 
> Residual Deviance: 62.27 	AIC: 72.27
> 
> 
>> modelInteraction
> 
> Call:  glm(formula = Y ~ X1 + X2 + X3 + X4 + X1 * X4, family = "binomial", 
>    data = data)
> 
> Coefficients:
> (Intercept)           X1           X2           X3           X4        X1:X4  
>    90.0158     -90.0747       0.1183       0.3064     -15.3688      15.1593  
> 
> Degrees of Freedom: 68 Total (i.e. Null);  63 Residual
> Null Deviance:	    66.78 
> Residual Deviance: 61.49 	AIC: 73.49
> 
> Notice that a naive attempt to plot an X1  "effect" in modelInteraction might pick the -90.07 value which would then ignore both the much larger Intercept value and also ignore the fact that the interaction term has now split the X4 (and X1) "effects" into multiple pieces.
> 
> You need to interpret the effects of X1 in the context of a specification of a particular X4 value and not forget that the Intercept should not be ignored. It appears to me that the estimates of the mfx package are essentially meaningless with the problem you have thrown at it.
> 
>> a
> Call:
> logitmfx(formula = model, data = data, atmean = TRUE)
> 
> Marginal Effects:
>       dF/dx Std. Err.       z   P>|z|  
> X1  0.147532  0.087865  1.6791 0.09314 .
> X2  0.015085  0.193888  0.0778 0.93798  
> X3  0.040309  0.063324  0.6366 0.52441  
> X4 -0.050393  0.092947 -0.5422 0.58770  
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> dF/dx is for discrete change for the following variables:
> 
> [1] "X1" "X2" "X4"
>> b
> Call:
> logitmfx(formula = modelInteraction, data = data, atmean = TRUE)
> 
> Marginal Effects:
>            dF/dx   Std. Err.         z  P>|z|    
> X1    -1.0000e+00  1.2121e-07 -8.25e+06 <2e-16 ***
> X2     6.5595e-03  8.1616e-01  8.00e-03 0.9936    
> X3     1.6312e-02  2.0326e+00  8.00e-03 0.9936    
> X4    -9.6831e-01  1.5806e+01 -6.13e-02 0.9511    
> X1:X4  8.0703e-01  1.4572e+01  5.54e-02 0.9558    
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> dF/dx is for discrete change for the following variables:
> 
> [1] "X1" "X2" "X4"
> 
> I see no sensible interpretation of the phrase "X1 effect" in the comparison tables above. The "p-value" in the second table appears to be nonsense induced by throwing a model formulation that was not anticipated. There is a negligible improvement in the glm fits:
> 
>> anova(model,modelInteraction)
> Analysis of Deviance Table
> 
> Model 1: Y ~ X1 + X2 + X3 + X4
> Model 2: Y ~ X1 + X2 + X3 + X4 + X1 * X4
>  Resid. Df Resid. Dev Df Deviance
> 1        64     62.274            
> 2        63     61.495  1  0.77908
> 
> 
> So the notion that the "X1 effect" is now "highly significant" where it was before not even suggestive of significance seem to point to either an error in the underlying theory or a failure to anticipate and trap (and warn the user) that an erroneous model (or at least an unanticipated model) is being passed to a procedure.
> 
> At least the 'effects- package gives you a tiny warning about this issue, although I think it really should throw an informative error when a user attempts to estimate only a "main effect" in a model that has an interaction involving such a covariate:
> 
>> library(effects)
> 
>> effect('X1', model)
> 
> X1 effect
> X1
>         0        0.2        0.4        0.6        0.8          1 
> 0.06706123 0.08582757 0.10923061 0.13805139 0.17299973 0.21459275 
>> effect('X1', modelInteraction)
> NOTE: X1 is not a high-order term in the model
> 
> X1 effect
> X1
>           0          0.2          0.4          0.6          0.8            1 
> 0.0002418661 0.0009864740 0.0040142251 0.0161843996 0.0629206979 0.2151098752 
>> effect('X1:X4', modelInteraction)
> 
> X1*X4 effect
>     X4
> X1            6         6.2          6.4          6.6          6.8            7
>  0   0.1100479 0.005686142 0.0002643982 1.223058e-05 5.656287e-07 2.615838e-08
>  0.2 0.1285241 0.012352473 0.0010595321 8.994106e-05 7.628099e-06 6.469071e-07
>  0.4 0.1495811 0.026625017 0.0042357682 6.610806e-04 1.028639e-04 1.599803e-05
>  0.6 0.1734015 0.056446132 0.0167737545 4.841483e-03 1.385458e-03 3.954877e-04
>  0.8 0.2001225 0.115698003 0.0640377838 3.454327e-02 1.836677e-02 9.689636e-03
>  1   0.2298165 0.222481442 0.2153150766 2.083177e-01 2.014893e-01 1.948297e-01
> 
> -- 
> David.
> 
> 
> 
>> 
>> 
>> I have been looking around for the solutions but haven't been able to find any. I would appreciate any suggestions. 
>> 
>> A reproducible sample: 
>> 
>>> dput(data)
>> structure(list(Y = c(0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 
>> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
>> 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 
>> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), X1 = c(1L, 0L, 1L, 
>> 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 
>> 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 
>> 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
>> 1L, 0L), X2 = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
>> 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
>> 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), X3 = c(0L, 0L, 0L, 0L, 0L, 
>> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 2L, 2L, 3L, 4L, 5L, 0L, 0L, 
>> 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
>> ), X4 = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
>> 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 6L, 6L, 6L, 6L, 6L, 6L, 
>> 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
>> 7L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 
>> 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L)), .Names = c("Y", "X1", "X2", 
>> "X3", "X4"), row.names = c(NA, -69L), class = "data.frame")
>> 
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA


	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Fri Jul 22 08:52:36 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 22 Jul 2016 08:52:36 +0200
Subject: [R] C/C++/Fortran Rolling Window Regressions
In-Reply-To: <CAOjnRsYGVBqFEJJn4M8ui=hB1wKDEw25dRO9p5K_Hn2riU0jpQ@mail.gmail.com>
References: <CAOjnRsapezs9Oa5RehKC0V617CR5u-Q7X-_2iodQSPcQgXnJ2A@mail.gmail.com>
	<alpine.DEB.2.20.1607212128490.3823@paninaro>
	<CAOjnRsZzc2XJTQUL46NzFZv0VQ4XQfuzFyxB=KYZucyK3gaAPA@mail.gmail.com>
	<CAP01uRnqi_cCan=4MyshMwpOiuFK-Og0_t3f0A3khUHipQvB+Q@mail.gmail.com>
	<CAOjnRsYGVBqFEJJn4M8ui=hB1wKDEw25dRO9p5K_Hn2riU0jpQ@mail.gmail.com>
Message-ID: <22417.49844.947460.848626@stat.math.ethz.ch>

>>>>> jeremiah rounds <roundsjeremiah at gmail.com>
>>>>>     on Thu, 21 Jul 2016 13:56:17 -0700 writes:

    > I appreciate the timing, so much so I changed the code to show the issue.
    > It is a problem of scale.

    > roll_lm probably has a heavy start-up cost but otherwise completely
    > out-performs those other versions at scale.  I suspect you are timing the
    > nearly  constant time start-up cost in small data.  I did give code to
    > paint a picture, but it was just cartoon code lifted from stackexchange.
    > If you want to characterize the real problem it is closer to:
    > 30 day rolling windows on 24 daily (by hour) measurements for 5 years with
    > 24+7 -1 dummy predictor variables and finally you need to do this for 300
    > sets of data.

    > Pseudo-code is closer to what follows and roll_lm can handle that input in
    > a timely manner.  You can do it with lm.fit, but you need to spend a lot of
    > time waiting.  The issue of accuracy needs a follow-up check.  Not sure why
    > it would be different.  Worth a check on that.

Just a note about lm(), lm.fit():
As Achim Zeileis noted, they are very reliable implementations
(based on QR) indeed.   For a year or so now, there's the bare
bone  .lm.fit()  which is even one order of magnitued factor
faster than lm.fit()  at least for simple cases, see the final
example on the  ?.lm.fit help page.

Martin Maechler
(who had added .lm.fit() to R )


    > Thanks,
    > Jeremiah


    > library(rbenchmark)
    > N = 30*24*12*5
    > window = 30*24
    > npred = 15  #15 chosen arbitrarily...
    > set.seed(1)
    > library(zoo)
    > library(RcppArmadillo)
    > library(roll)
    > x = matrix(rnorm(N*(npred+1)), ncol = npred+1)
    > colnames(x) <- c("y",  paste0("x", 1:npred))
    > z <- zoo(x)


    > benchmark(
    > roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, -1, drop =
    > F]), window,
    > center = FALSE), replications=3)

    > Which comes out as:
    > test replications elapsed relative user.self sys.self user.child
    > sys.child
    > 1 roll_lm            3   6.273        1    38.312    0.654          0
    > 0





    > ## You arn't going to get that below...

    > benchmark(fastLm = rollapplyr(z, width = window,
    > function(x) coef(fastLm(cbind(1, x[, -1]), x[, 1])),
    > by.column = FALSE),
    > lm = rollapplyr(z, width = window,
    > function(x) coef(lm(y ~ ., data = as.data.frame(x))),
    > by.column = FALSE), replications=3)



    > On Thu, Jul 21, 2016 at 1:28 PM, Gabor Grothendieck <ggrothendieck at gmail.com
    >> wrote:

    >> I would be careful about making assumptions regarding what is faster.
    >> Performance tends to be nonintuitive.
    >> 
    >> When I ran rollapply/lm, rollapply/fastLm and roll_lm on the example
    >> you provided rollapply/fastLm was three times faster than roll_lm.  Of
    >> course this could change with data of different dimensions but it
    >> would be worthwhile to do actual benchmarks before making assumptions.
    >> 
    >> I also noticed that roll_lm did not give the same coefficients as the
    >> other two.
    >> 
    >> set.seed(1)
    >> library(zoo)
    >> library(RcppArmadillo)
    >> library(roll)
    >> z <- zoo(matrix(rnorm(10), ncol = 2))
    >> colnames(z) <- c("y", "x")
    >> 
    >> ## rolling regression of width 4
    >> library(rbenchmark)
    >> benchmark(fastLm = rollapplyr(z, width = 4,
    >> function(x) coef(fastLm(cbind(1, x[, 2]), x[, 1])),
    >> by.column = FALSE),
    >> lm = rollapplyr(z, width = 4,
    >> function(x) coef(lm(y ~ x, data = as.data.frame(x))),
    >> by.column = FALSE),
    >> roll_lm =  roll_lm(coredata(z[, 1, drop = F]), coredata(z[, 2, drop =
    >> F]), 4,
    >> center = FALSE))[1:4]
    >> 
    >> 
    >> test replications elapsed relative
    >> 1  fastLm          100    0.22    1.000
    >> 2      lm          100    0.72    3.273
    >> 3 roll_lm          100    0.64    2.909
    >> 
    >> On Thu, Jul 21, 2016 at 3:45 PM, jeremiah rounds
    >> <roundsjeremiah at gmail.com> wrote:
    >> >  Thanks all.  roll::roll_lm was essentially what I wanted.   I think
    >> maybe
    >> > I would prefer it to have options to return a few more things, but it is
    >> > the coefficients, and the remaining statistics you might want can be
    >> > calculated fast enough from there.
    >> >
    >> >
    >> > On Thu, Jul 21, 2016 at 12:36 PM, Achim Zeileis <
    >> Achim.Zeileis at uibk.ac.at>
    >> > wrote:
    >> >
    >> >> Jeremiah,
    >> >>
    >> >> for this purpose there are the "roll" and "RcppRoll" packages. Both use
    >> >> Rcpp and the former also provides rolling lm models. The latter has a
    >> >> generic interface that let's you define your own function.
    >> >>
    >> >> One thing to pay attention to, though, is the numerical reliability.
    >> >> Especially on large time series with relatively short windows there is a
    >> >> good chance of encountering numerically challenging situations. The QR
    >> >> decomposition used by lm is fairly robust while other more
    >> straightforward
    >> >> matrix multiplications may not be. This should be kept in mind when
    >> writing
    >> >> your own Rcpp code for plugging it into RcppRoll.
    >> >>
    >> >> But I haven't check what the roll package does and how reliable that
    >> is...
    >> >>
    >> >> hth,
    >> >> Z
    >> >>
    >> >>
    >> >> On Thu, 21 Jul 2016, jeremiah rounds wrote:
    >> >>
    >> >> Hi,
    >> >>>
    >> >>> A not unusual task is performing a multiple regression in a rolling
    >> window
    >> >>> on a time-series.    A standard piece of advice for doing in R is
    >> >>> something
    >> >>> like the code that follows at the end of the email.  I am currently
    >> using
    >> >>> an "embed" variant of that code and that piece of advice is out there
    >> too.
    >> >>>
    >> >>> But, it occurs to me that for such an easily specified matrix operation
    >> >>> standard R code is really slow.   rollapply constantly returns to R
    >> >>> interpreter at each window step for a new lm.   All lm is at its heart
    >> is
    >> >>> (X^t X)^(-1) * Xy,  and if you think about doing that with Rcpp in
    >> rolling
    >> >>> window you are just incrementing a counter and peeling off rows (or
    >> >>> columns
    >> >>> of X and y) of a particular window size, and following that up with
    >> some
    >> >>> matrix multiplication in a loop.   The psuedo-code for that Rcpp
    >> >>> practically writes itself and you might want a wrapper of something
    >> like:
    >> >>> rolling_lm (y=y, x=x, width=4).
    >> >>>
    >> >>> My question is this: has any of the thousands of R packages out there
    >> >>> published anything like that.  Rolling window multiple regressions that
    >> >>> stay in C/C++ until the rolling window completes?  No sense and
    >> writing it
    >> >>> if it exist.
    >> >>>
    >> >>>
    >> >>> Thanks,
    >> >>> Jeremiah
    >> >>>
    >> >>> Standard (slow) advice for "rolling window regression" follows:
    >> >>>
    >> >>>
    >> >>> set.seed(1)
    >> >>> z <- zoo(matrix(rnorm(10), ncol = 2))
    >> >>> colnames(z) <- c("y", "x")
    >> >>>
    >> >>> ## rolling regression of width 4
    >> >>> rollapply(z, width = 4,
    >> >>>   function(x) coef(lm(y ~ x, data = as.data.frame(x))),
    >> >>>   by.column = FALSE, align = "right")
    >> >>>
    >> >>> ## result is identical to
    >> >>> coef(lm(y ~ x, data = z[1:4,]))
    >> >>> coef(lm(y ~ x, data = z[2:5,]))
    >> >>>
    >> >>>         [[alternative HTML version deleted]]
    >> >>>
    >> >>> ______________________________________________
    >> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >> >>> https://stat.ethz.ch/mailman/listinfo/r-help
    >> >>> PLEASE do read the posting guide
    >> >>> http://www.R-project.org/posting-guide.html
    >> >>> and provide commented, minimal, self-contained, reproducible code.
    >> >>>
    >> >>>
    >> >
    >> >         [[alternative HTML version deleted]]
    >> >
    >> > ______________________________________________
    >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >> > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > PLEASE do read the posting guide
    >> http://www.R-project.org/posting-guide.html
    >> > and provide commented, minimal, self-contained, reproducible code.
    >> 
    >> 
    >> 
    >> --
    >> Statistics & Software Consulting
    >> GKX Group, GKX Associates Inc.
    >> tel: 1-877-GKX-GROUP
    >> email: ggrothendieck at gmail.com
    >> 

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From r_goertz at web.de  Fri Jul 22 10:15:36 2016
From: r_goertz at web.de (Ralf Goertz)
Date: Fri, 22 Jul 2016 10:15:36 +0200
Subject: [R] readline issue with 3.3.1
In-Reply-To: <22416.62287.429899.107013@stat.math.ethz.ch>
References: <20160720113531.2bacaa0a@delli.home.local>
	<20160720163753.039cbccb@delli.home.local>
	<22416.62287.429899.107013@stat.math.ethz.ch>
Message-ID: <20160722101536.26f5b5fe@delli.home.local>

Am Thu, 21 Jul 2016 18:07:43 +0200
schrieb Martin Maechler <maechler at stat.math.ethz.ch>:

> Ralf Goertz <r_goertz at web.de> on Wed, 20 Jul 2016 16:37:53 +0200
> writes:
 
>> I installed readline version 6.3 and the issue is gone. So probably
>> some of the recent changes in R's readline code are incompatible with
>> version readline version 6.2.
> 
> Yes, it seems so, unfortunately.
> 
> Thank you for reporting !

It would be great if ? while fixing this ? you also took care of the
SIGWINCH problem described in bug report
https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=16604 

Thanks, Ralf


From dwinsemius at comcast.net  Fri Jul 22 10:38:43 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 22 Jul 2016 01:38:43 -0700
Subject: [R] How to plot marginal effects (MEM) in R?
In-Reply-To: <154826AF-AA98-4A0F-88ED-0928D12AFCCD@gmail.com>
References: <9991F499-B90A-468E-9DDC-2AC264AA5806@gmail.com>
	<994EE187-1B2F-49F5-9A6A-D01FF2C4EF66@comcast.net>
	<154826AF-AA98-4A0F-88ED-0928D12AFCCD@gmail.com>
Message-ID: <D65C5DC6-9AE2-4BC3-8E6D-9C323832668C@comcast.net>


> On Jul 21, 2016, at 11:44 PM, Faradj Koliev <faradj.g at gmail.com> wrote:
> 
> Dear David Winsemius, 
> 
> Thank you!  
> 
> The sample make no sense, I know. The real data is too big. So, I only want to understand how to plot marginal effects, to visualize them in a proper way. 
> 

Before you start plotting, you first need to first understand that with interactions in the model there are N X M effects where N and M are the number of levels in the two covariates. The whole point of models is to boil down large amounts of "real data", but not so much boiling that you get a congealed, burned molasses.

The package you chose to examine the interactions appears ill-equipted to provide you with the necessary analysis.

-- 
David.
> Best,
> 
> 
>> 22 juli 2016 kl. 08:35 skrev David Winsemius <dwinsemius at comcast.net>:
>> 
>>> 
>>> On Jul 21, 2016, at 2:22 PM, Faradj Koliev <faradj.g at gmail.com> wrote:
>>> 
>>> Dear all, 
>>> 
>>> I have two logistic regression models:
>>> 
>>> 
>>>  ? model <- glm(Y ~ X1+X2+X3+X4, data = data, family = "binomial")
>>> 
>>> 
>>> 
>>>  ? modelInteraction <- glm(Y ~ X1+X2+X3+X4+X1*X4, data = data, family = "binomial")
>>> 
>>> To calculate the marginal effects (MEM approach) for these models, I used the `mfx` package:
>>> 
>>> 
>>>  ? a<- logitmfx(model, data=data, atmean=TRUE)
>>> 
>>> 
>>> 
>>>   ?b<- logitmfx(modelInteraction, data=data, atmean=TRUE)
>>> 
>>> 
>>> What I want to do now is 1) plot all the results for "model" and 2) show the result just for two variables: X1 and X2. 
>>> 3) I also want to plot the interaction term in ?modelInteraction?.
>> 
>> There is no longer a single "effect" for X1 in modelInteraction in contrast to the manner as there might be an "effect" for X2. There can only be predictions for combined situations with particular combinations of values for X1 and X4.
>> 
>>> model
>> 
>> Call:  glm(formula = Y ~ X1 + X2 + X3 + X4, family = "binomial", data = data)
>> 
>> Coefficients:
>> (Intercept)           X1           X2           X3           X4  
>>    -0.3601       1.3353       0.1056       0.2898      -0.3705  
>> 
>> Degrees of Freedom: 68 Total (i.e. Null);  64 Residual
>> Null Deviance:	    66.78 
>> Residual Deviance: 62.27 	AIC: 72.27
>> 
>> 
>>> modelInteraction
>> 
>> Call:  glm(formula = Y ~ X1 + X2 + X3 + X4 + X1 * X4, family = "binomial", 
>>    data = data)
>> 
>> Coefficients:
>> (Intercept)           X1           X2           X3           X4        X1:X4  
>>    90.0158     -90.0747       0.1183       0.3064     -15.3688      15.1593  
>> 
>> Degrees of Freedom: 68 Total (i.e. Null);  63 Residual
>> Null Deviance:	    66.78 
>> Residual Deviance: 61.49 	AIC: 73.49
>> 
>> Notice that a naive attempt to plot an X1  "effect" in modelInteraction might pick the -90.07 value which would then ignore both the much larger Intercept value and also ignore the fact that the interaction term has now split the X4 (and X1) "effects" into multiple pieces.
>> 
>> You need to interpret the effects of X1 in the context of a specification of a particular X4 value and not forget that the Intercept should not be ignored. It appears to me that the estimates of the mfx package are essentially meaningless with the problem you have thrown at it.
>> 
>>> a
>> Call:
>> logitmfx(formula = model, data = data, atmean = TRUE)
>> 
>> Marginal Effects:
>>       dF/dx Std. Err.       z   P>|z|  
>> X1  0.147532  0.087865  1.6791 0.09314 .
>> X2  0.015085  0.193888  0.0778 0.93798  
>> X3  0.040309  0.063324  0.6366 0.52441  
>> X4 -0.050393  0.092947 -0.5422 0.58770  
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> 
>> dF/dx is for discrete change for the following variables:
>> 
>> [1] "X1" "X2" "X4"
>>> b
>> Call:
>> logitmfx(formula = modelInteraction, data = data, atmean = TRUE)
>> 
>> Marginal Effects:
>>            dF/dx   Std. Err.         z  P>|z|    
>> X1    -1.0000e+00  1.2121e-07 -8.25e+06 <2e-16 ***
>> X2     6.5595e-03  8.1616e-01  8.00e-03 0.9936    
>> X3     1.6312e-02  2.0326e+00  8.00e-03 0.9936    
>> X4    -9.6831e-01  1.5806e+01 -6.13e-02 0.9511    
>> X1:X4  8.0703e-01  1.4572e+01  5.54e-02 0.9558    
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> 
>> dF/dx is for discrete change for the following variables:
>> 
>> [1] "X1" "X2" "X4"
>> 
>> I see no sensible interpretation of the phrase "X1 effect" in the comparison tables above. The "p-value" in the second table appears to be nonsense induced by throwing a model formulation that was not anticipated. There is a negligible improvement in the glm fits:
>> 
>>> anova(model,modelInteraction)
>> Analysis of Deviance Table
>> 
>> Model 1: Y ~ X1 + X2 + X3 + X4
>> Model 2: Y ~ X1 + X2 + X3 + X4 + X1 * X4
>>  Resid. Df Resid. Dev Df Deviance
>> 1        64     62.274            
>> 2        63     61.495  1  0.77908
>> 
>> 
>> So the notion that the "X1 effect" is now "highly significant" where it was before not even suggestive of significance seem to point to either an error in the underlying theory or a failure to anticipate and trap (and warn the user) that an erroneous model (or at least an unanticipated model) is being passed to a procedure.
>> 
>> At least the 'effects- package gives you a tiny warning about this issue, although I think it really should throw an informative error when a user attempts to estimate only a "main effect" in a model that has an interaction involving such a covariate:
>> 
>>> library(effects)
>> 
>>> effect('X1', model)
>> 
>> X1 effect
>> X1
>>         0        0.2        0.4        0.6        0.8          1 
>> 0.06706123 0.08582757 0.10923061 0.13805139 0.17299973 0.21459275 
>>> effect('X1', modelInteraction)
>> NOTE: X1 is not a high-order term in the model
>> 
>> X1 effect
>> X1
>>           0          0.2          0.4          0.6          0.8            1 
>> 0.0002418661 0.0009864740 0.0040142251 0.0161843996 0.0629206979 0.2151098752 
>>> effect('X1:X4', modelInteraction)
>> 
>> X1*X4 effect
>>     X4
>> X1            6         6.2          6.4          6.6          6.8            7
>>  0   0.1100479 0.005686142 0.0002643982 1.223058e-05 5.656287e-07 2.615838e-08
>>  0.2 0.1285241 0.012352473 0.0010595321 8.994106e-05 7.628099e-06 6.469071e-07
>>  0.4 0.1495811 0.026625017 0.0042357682 6.610806e-04 1.028639e-04 1.599803e-05
>>  0.6 0.1734015 0.056446132 0.0167737545 4.841483e-03 1.385458e-03 3.954877e-04
>>  0.8 0.2001225 0.115698003 0.0640377838 3.454327e-02 1.836677e-02 9.689636e-03
>>  1   0.2298165 0.222481442 0.2153150766 2.083177e-01 2.014893e-01 1.948297e-01
>> 
>> -- 
>> David.
>> 
>> 
>> 
>>> 
>>> 
>>> I have been looking around for the solutions but haven't been able to find any. I would appreciate any suggestions. 
>>> 
>>> A reproducible sample: 
>>> 
>>>> dput(data)
>>> structure(list(Y = c(0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 
>>> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
>>> 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
>>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 
>>> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), X1 = c(1L, 0L, 1L, 
>>> 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 
>>> 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
>>> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 
>>> 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
>>> 1L, 0L), X2 = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
>>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
>>> 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
>>> 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
>>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), X3 = c(0L, 0L, 0L, 0L, 0L, 
>>> 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
>>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 2L, 2L, 3L, 4L, 5L, 0L, 0L, 
>>> 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
>>> 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
>>> ), X4 = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
>>> 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 6L, 6L, 6L, 6L, 6L, 6L, 
>>> 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
>>> 7L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 
>>> 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L)), .Names = c("Y", "X1", "X2", 
>>> "X3", "X4"), row.names = c(NA, -69L), class = "data.frame")
>>> 
>>> 
>>> 
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
> 

David Winsemius
Alameda, CA, USA


From nicholas.wray at ntlworld.com  Fri Jul 22 10:47:14 2016
From: nicholas.wray at ntlworld.com (WRAY NICHOLAS)
Date: Fri, 22 Jul 2016 09:47:14 +0100 (BST)
Subject: [R] Changing Index Labelling on x axis
Message-ID: <562969210.1788767.1469177234720.JavaMail.open-xchange@oxbe18.tb.ukmail.iss.as9143.net>

Hi  I have a vector of data (for example c(2,3,4,5,4,3,2)

data<-c(2,3,4,5,4,3,2)
plot(data)

I simply want to up the index values along the x axis by 2, so that instead of
1, I have 3, instead of 2 I have 4 etc etc.  Despite ages playing around with
the axis function I can't get it to work, and keep getting weird error messages.
 This is likely to be laughably simple but if someone can help me out I'd be
grateful

Thanks Nick W
	[[alternative HTML version deleted]]


From ivan.calandra at univ-reims.fr  Fri Jul 22 10:58:31 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Fri, 22 Jul 2016 10:58:31 +0200
Subject: [R] Changing Index Labelling on x axis
In-Reply-To: <562969210.1788767.1469177234720.JavaMail.open-xchange@oxbe18.tb.ukmail.iss.as9143.net>
References: <562969210.1788767.1469177234720.JavaMail.open-xchange@oxbe18.tb.ukmail.iss.as9143.net>
Message-ID: <8d7b6aa5-ce0e-59d1-2694-6525274817c7@univ-reims.fr>

Hi Nick,

If I understand you correctly, that should do it:
plot(x=seq_along(data)+2, y=data)

HTH,
Ivan

--
Ivan Calandra, PhD
Scientific Mediator
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/

Le 22/07/2016 ? 10:47, WRAY NICHOLAS a ?crit :
> Hi  I have a vector of data (for example c(2,3,4,5,4,3,2)
>
> data<-c(2,3,4,5,4,3,2)
> plot(data)
>
> I simply want to up the index values along the x axis by 2, so that instead of
> 1, I have 3, instead of 2 I have 4 etc etc.  Despite ages playing around with
> the axis function I can't get it to work, and keep getting weird error messages.
>   This is likely to be laughably simple but if someone can help me out I'd be
> grateful
>
> Thanks Nick W
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From hpages at fredhutch.org  Fri Jul 22 11:15:53 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Fri, 22 Jul 2016 02:15:53 -0700
Subject: [R] installing the lubricate package
In-Reply-To: <CABOzBw2UPcoww9uq2SupK3=G8C6iV+gbv2oc0ShnAVxuoGB+tQ@mail.gmail.com>
References: <CAN5afy_S9ABWsc6aB0tucofdh_7QSnHWM5X94C8OJ9638hgEEw@mail.gmail.com>
	<CABOzBw2UPcoww9uq2SupK3=G8C6iV+gbv2oc0ShnAVxuoGB+tQ@mail.gmail.com>
Message-ID: <fc419924-63df-1c03-e42a-789017abc287@fredhutch.org>

also it's lubridate, not lubricate :-/

On 07/21/2016 05:44 PM, Ismail SEZEN wrote:
> You don't have to download and install from github. You can install
> lubridate package easly from cran repository. If you really intend to
> install from github, i advise you install devtools package first and use
> install_github function.
>
> http://www.inside-r.org/packages/cran/devtools/docs/install_github
>
> On Fri, Jul 22, 2016, 03:36 lily li <chocold12 at gmail.com> wrote:
>
>> Hi R users,
>>
>> I'm trying to download lubricate from this website, and then install it on
>> my mac.
>> https://github.com/hadley/lubridate
>>
>> but it says windows version does not apply to mac. How to install the
>> package for mac? Thanks.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From drjimlemon at gmail.com  Fri Jul 22 12:07:37 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 22 Jul 2016 20:07:37 +1000
Subject: [R] about interpolating data in r
In-Reply-To: <140985CB-8A57-4406-9A80-E75C68375AB7@gmail.com>
References: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
	<CAF8bMcYPSgThf7ZLiqnJFXfqRTLgs_ZLS6K5_TPqhQbcgPh-QA@mail.gmail.com>
	<CAN5afy-CY8k==SixH4VYKO50n_=Ye2wefDzf4iW9eyMWcTCEMQ@mail.gmail.com>
	<140985CB-8A57-4406-9A80-E75C68375AB7@gmail.com>
Message-ID: <CA+8X3fV6D4WH1BWwWoGWSxw17c-01Z4Eum9h=nDP+1YYGo4ZJQ@mail.gmail.com>

Hi lili,
The problem may lie in the fact that I think you are using
"interpolate" when you mean "extrapolate". In that case, the best you
can do is spread values beyond the points that you have. Find the
slope of the line, put a point at each end of your time data
(2009-01-01 and 2009-12-31) and use "approx" on all three gaps. Note
that this slope is a slippery one indeed and few will accept that the
values so generated mean anything.

Jim

On Fri, Jul 22, 2016 at 9:38 AM, Ismail SEZEN <sezenismail at gmail.com> wrote:
>
>> On 22 Jul 2016, at 01:54, lily li <chocold12 at gmail.com> wrote:
>>
>> Thanks, I meant if there are missing data at the beginning and end of a
>> dataframe, how to interpolate according to available data?
>>
>> For example, the A column has missing values at the beginning and end, how
>> to interpolate linearly between 10 and 12 for the missing values?
>>
>> df <- data.frame(A=c(NA, NA,10,11,12, NA),B=c(5,5,4,3,4,5),C=c(3.3,4,3,1.5,
>> 2.2,4),time=as.Date(c("1990-01-01","1990-02-
>> 07","1990-02-14","1990-02-28","1990-03-01","1990-03-20")))
>>
>
> As William was answered;
>
> with(df, approx(x=time, y=A, xout=seq(min(time, na.rm =T), max(time, na.rm = T), by="days")))
>
> will help you interpolate linearly between knwon values even column has NA?s.
>
>
>>
>> On Thu, Jul 21, 2016 at 4:48 PM, William Dunlap <wdunlap at tibco.com> wrote:
>>
>>> Try approx(), as in:
>>>
>>> df <-
>>> data.frame(A=c(10,11,12),B=c(5,5,4),C=c(3.3,4,3),time=as.Date(c("1990-01-01","1990-02-07","1990-02-14")))
>>> with(df, approx(x=time, y=C, xout=seq(min(time), max(time), by="days")))
>>>
>>> Do you notice how one can copy and paste that example out of the
>>> mail an into R to see how it works?  It would help if your questions
>>> had that same property - show how the example data could be created.
>>>
>>>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>> On Thu, Jul 21, 2016 at 3:34 PM, lily li <chocold12 at gmail.com> wrote:
>>>
>>>> I have a question about interpolating missing values in a dataframe. The
>>>> dataframe is in the following, Column C has no data before 2009-01-05 and
>>>> after 2009-12-31, how to interpolate data for the blanks? That is to say,
>>>> interpolate linearly between these two gaps using 5.4 and 6.1? Thanks.
>>>>
>>>>
>>>> df
>>>> time                A      B     C
>>>> 2009-01-01    3      4.5
>>>> 2009-01-02    4      5
>>>> 2009-01-03    3.3   6
>>>> 2009-01-04    4.1   7
>>>> 2009-01-05    4.4   6.2   5.4
>>>> ...
>>>>
>>>> 2009-11-20    5.1   5.5   6.1
>>>> 2009-11-21    5.4   4
>>>> ...
>>>> 2009-12-31    4.5   6
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jvadams at usgs.gov  Fri Jul 22 15:06:54 2016
From: jvadams at usgs.gov (Adams, Jean)
Date: Fri, 22 Jul 2016 08:06:54 -0500
Subject: [R] Aggregate data to lower resolution
In-Reply-To: <CAMLwc7O1nWa7Akou2RA8S1Rt=FWzTzUrBjfZFaFz8wTXDckEFg@mail.gmail.com>
References: <CAMLwc7O1nWa7Akou2RA8S1Rt=FWzTzUrBjfZFaFz8wTXDckEFg@mail.gmail.com>
Message-ID: <CAN5YmCGiejFeenv8bJV525Tm+jhCoqHbEyS6TPrY5ePhPHaeLw@mail.gmail.com>

Milu,

Perhaps an approach like this would work.  In the example below, I
calculate the mean GDP for each 1 degree by 1 degree.

temp$long1 <- floor(temp$longitude)
temp$lat1 <- floor(temp$latitude)
temp1 <- aggregate(GDP ~ long1 + lat1, temp, mean)

  long1 lat1        GDP
1   -69  -55 0.90268640
2   -68  -55 0.09831317
3   -72  -54 0.22379000
4   -71  -54 0.14067290
5   -70  -54 0.00300380
6   -69  -54 0.00574220

Jean

On Thu, Jul 21, 2016 at 3:57 PM, Miluji Sb <milujisb at gmail.com> wrote:

> Dear all,
>
> I have the following GDP data by latitude and longitude at 0.5 degree by
> 0.5 degree.
>
> temp <- dput(head(ptsDF,10))
> structure(list(longitude = c(-68.25, -67.75, -67.25, -68.25,
> -67.75, -67.25, -71.25, -70.75, -69.25, -68.75), latitude = c(-54.75,
> -54.75, -54.75, -54.25, -54.25, -54.25, -53.75, -53.75, -53.75,
> -53.75), GDP = c(1.683046, 0.3212307, 0.0486207, 0.1223268, 0.0171909,
> 0.0062104, 0.22379, 0.1406729, 0.0030038, 0.0057422)), .Names =
> c("longitude",
> "latitude", "GDP"), row.names = c(4L, 17L, 30L, 43L, 56L, 69L,
> 82L, 95L, 108L, 121L), class = "data.frame")
>
> I would like to aggregate the data 1 degree by 1 degree. I understand that
> the first step is to convert to raster. I have tried:
>
> rasterDF <- rasterFromXYZ(temp)
> r <- aggregate(rasterDF,fact=2, fun=sum)
>
> But this does not seem to work. Could anyone help me out please? Thank you
> in advance.
>
> Sincerely,
>
> Milu
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From 538280 at gmail.com  Fri Jul 22 16:50:11 2016
From: 538280 at gmail.com (Greg Snow)
Date: Fri, 22 Jul 2016 08:50:11 -0600
Subject: [R] Why the order of parameters in a logistic regression
 affects results significantly?
In-Reply-To: <1104610284.3375689.1469138662609.JavaMail.yahoo@mail.yahoo.com>
References: <1104610284.3375689.1469138662609.JavaMail.yahoo.ref@mail.yahoo.com>
	<1104610284.3375689.1469138662609.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAFEqCdyZ=MANDuyJcCkF_F4tK09wR1BOzJFp8yQ=X9mA0Ov7Qw@mail.gmail.com>

Please post in plain text, the message is very hard to read with the
reformatting that was done.

Did you receive any warnings when you fit your models?

The fact that the last coefficient is NA in both outputs suggests that
there was some co-linearity in your predictor variables and R chose to
drop one of the offending variables from the model (the last one in
each case).  Depending on the nature of the co-linearity, the
interpretation (and therefore the estimates) can change.

For example lets say that you have 3 predictors, red, green, and blue
that are indicator variables (0/1) and that every subject has a 1 in
exactly one of those variables (so they are co-linear with the
intercept).  If you put the 3 variables into a model with the
intercept in the above order, then R will drop the blue variable and
the interpretation of the coefficients is that the intercept is the
average for blue subjects and the other coefficients are the
differences between red/green and blue on average.  If you refit the
model with the order blue, green, red, then R will drop red from the
model and now the interpretation is that the intercept is the mean for
red subjects and the others are the differences from red on average, a
very different interpretation and therefore different estimates.

I expect something along those lines is going on here.

On Thu, Jul 21, 2016 at 4:04 PM, Qinghua He via R-help
<r-help at r-project.org> wrote:
> Using the same data, if I ran
> fit2 <-glm(formula=AR~Age+LumA+LumB+HER2+Basal+Normal,family=binomial,data=RacComp1)summary(fit2)exp(coef(fit2))
> I obtained:
>> exp(coef(fit2))(Intercept)         Age        LumA        LumB        HER2       Basal      Normal  0.24866935  1.00433781  0.10639937  0.31614001  0.08220685 20.25180956          NA
> while if I ran
>
> fit2 <-glm(formula=AR~Age+LumA+LumB+Basal+Normal+HER2,family=binomial,data=RacComp1)summary(fit2)exp(coef(fit2))
> I obtained:
>> exp(coef(fit2)) (Intercept)          Age         LumA         LumB        Basal       Normal         HER2   0.02044232   1.00433781   1.29428846   3.84566516 246.35185956  12.16443690           NA
>
> Essentially they're the same model - I just moved HER2 to the last. But the OR changed significantly. Can someone explain?
> For the latter result, I don't even know how to interpret as all factors have OR>1 (except Intercept), how could that possible? Can I eliminate the effect of intercept?
> Also, I cannot obtain OR for the last factor due to collinearity. However, I know others obtained OR for all factors for the same dataset. Can someone tell me how to obtain OR for all factors? All factors are categorical variables (i.e., 0 or 1).
> Thanks!
> Peter
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From jdnewmil at dcn.davis.ca.us  Fri Jul 22 17:20:06 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 22 Jul 2016 08:20:06 -0700
Subject: [R] PDF extraction with tm package
In-Reply-To: <CAKu3pN7m7r5gWh3LLaFPa60RPnZnf86Wy2XdQEd6mSp2ROAxvg@mail.gmail.com>
References: <CAKu3pN7m7r5gWh3LLaFPa60RPnZnf86Wy2XdQEd6mSp2ROAxvg@mail.gmail.com>
Message-ID: <5E608D00-96B4-40B0-82AF-09DF59133311@dcn.davis.ca.us>

This is neither the Xpdf support forum nor the Windows Setup Program Reinvention support group... and you really need to read and follow the Posting Guide for the R mailing lists.

FWIW I would guess that you need to learn about environment variables and in particular about the PATH variable. There are subtleties about when and how they get defined that are OS-specific and certainly off topic here that may trip you up along the way. Alternatively, you may read the Xpdf documentation or a how-to blog about Xpdf that gives you a recipe, but again that is not about R. Once you can start a CMD shell and run the command directly then you are most of the way to getting R to invoke it.
-- 
Sent from my phone. Please excuse my brevity.

On July 21, 2016 5:26:26 PM PDT, Steven Kang <stochastickang at gmail.com> wrote:
>Hi R users,
>
>I?m having some issues trying to extract texts from PDF file using tm
>package.
>
>Here are the steps that were carried out:
>
>1. Downloaded and installed the following programs:
>
>- Xpdf (Copied the ?bin32?, ?bin64?, ?doc? folders into ?C:\Program
>Files\Xpdf? directory; also added C:\Program
>Files\Xpdf\bin64\pdfinfo.exe &
>C:\Program Files\Xpdf\bin64\pdftotext.exe in existing PATH
>
>- Tesseract
>
>- Imagemagick
>
>2. Used the following scripts and the corresponding error messages:
>
># Directory where PDF files are stored
>
>>cname <- getwd()
>
>>Corpus(DirSource(cname), readerControl=list(reader = readPDF))
>
>Error in system2("pdftotext", c(control$text, shQuote(x), "-"), stdout
>=
>TRUE) :
>'"pdftotext"' not found
>
> In addition: Warning message:
>
>running command '"pdfinfo" "C:\Users\R_Files\XXX.pdf"' had status 127
>
>>file.exists(Sys.which(c("pdfinfo","pdftpotext")))
>[1] FALSE FALSE
>
>It seems like R can?t find pdfinfo & pdftotext exe files, but not sure
>as
>to why this would be the case despite xpdf files being copied into
>?C:\Program Files? (Im using Windows 7 64bits)
>
>I?m aware that ?pdf_text? function from pdftools package can extract
>texts
>from PDF file and outputs into a string. But I was after something
>which is
>able to convert PDF (ie transaction data) into a dataframe without
>regular
>expression. Is tm package capable of doing this conversion? Are there
>any
>other alternatives to these methods?
>
>Your expertise in resolving this problem would be highly appreciated.
>
>
>Steve
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From chocold12 at gmail.com  Fri Jul 22 17:29:14 2016
From: chocold12 at gmail.com (lily li)
Date: Fri, 22 Jul 2016 09:29:14 -0600
Subject: [R] about interpolating data in r
In-Reply-To: <4F96E751-7A44-40E7-8CE5-4DE6E249A57B@gmail.com>
References: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
	<4F96E751-7A44-40E7-8CE5-4DE6E249A57B@gmail.com>
Message-ID: <CAN5afy8ZOHcxKTGGEk17ivUi6y=svCVe3yNPPxCbqBijSomwQw@mail.gmail.com>

Thanks, Ismail.
For the gaps before 2009-01-05 and after 2009-11-20, I use the year 2010 to
fill in the missing values for column C. There is no relationship between
column A, B, and C.
For the missing values between 2009-01-05 and 2009-11-20, if there are any,
I found this approach is very helpful.
with(df, approx(x=time, y=C, xout=seq(min(time), max(time), by="days")))



On Thu, Jul 21, 2016 at 5:14 PM, Ismail SEZEN <sezenismail at gmail.com> wrote:

>
> > On 22 Jul 2016, at 01:34, lily li <chocold12 at gmail.com> wrote:
> >
> > I have a question about interpolating missing values in a dataframe.
>
> First of all, filling missing values action must be taken into account
> very carefully. It must be known the nature of the data that wanted to be
> filled and most of the time, to let them be NA is the most appropriate
> action.
>
> > The
> > dataframe is in the following, Column C has no data before 2009-01-05 and
> > after 2009-12-31, how to interpolate data for the blanks?
>
> Why a dataframe? Is there any relationship between columns A,B and C? If
> there is, then you might want to consider filling missing values by a
> linear model approach instead of interpolation. You said that there is not
> data before 2009-01-05 and after 2009-12-31 but according to dataframe,
> there is not data after 2009-11-20?
>
> > That is to say,
> > interpolate linearly between these two gaps using 5.4 and 6.1? Thanks.
>
> Also you metion interpolating blanks but you want interpolation between
> two gaps? Do you want to fill missing values before 2009-01-05 and after
> 2009-11-20 or do you want to find intermediate values between 2009-01-05
> and 2009-11-20? This is a bit unclear.
>
> >
> >
> > df
> > time                A      B     C
> > 2009-01-01    3      4.5
> > 2009-01-02    4      5
> > 2009-01-03    3.3   6
> > 2009-01-04    4.1   7
> > 2009-01-05    4.4   6.2   5.4
> > ...
> >
> > 2009-11-20    5.1   5.5   6.1
> > 2009-11-21    5.4   4
> > ...
> > 2009-12-31    4.5   6
>
>
> If you want to fill missing values at the end-points for column C (before
> 2009-01-05 and after 2009-11-20), and all data you have is between
> 2009-01-05 and 2009-11-20, this means that you want extrapolation (guessing
> unkonwn values that is out of known values). So, you can use only values at
> column C to guess missing end-point values. You can use splinefun (or
> spline) functions for this purpose. But let me note that this kind of
> approach might help you only for a few missing values close to end-points.
> Otherwise, you might find yourself in a huge mistake.
>
> As I mentioned in my first sentence, If you have a relationship between
> all columns or you have data for column C for other years (for instance,
> assume that you have data for column C for 2007, 2008, and 2010 but not
> 2009) you may want to try a statistical approach to fill the missing values.
>
>
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Jul 22 17:39:46 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 22 Jul 2016 08:39:46 -0700
Subject: [R] about interpolating data in r
In-Reply-To: <CAN5afy8ZOHcxKTGGEk17ivUi6y=svCVe3yNPPxCbqBijSomwQw@mail.gmail.com>
References: <CAN5afy-LDiWUN3UfupVJoxfOmwwQ68sE4yvkRUDF6JnZ3gefZA@mail.gmail.com>
	<4F96E751-7A44-40E7-8CE5-4DE6E249A57B@gmail.com>
	<CAN5afy8ZOHcxKTGGEk17ivUi6y=svCVe3yNPPxCbqBijSomwQw@mail.gmail.com>
Message-ID: <CAF8bMca-dAFjq3Rz1-WVJSR-Wgdde-jw0XNXmLiANa_v8tR9sA@mail.gmail.com>

approx() has a 'rule' argument that controls how it deals with
extrapolation.  Run help(approx) and read about the details.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Jul 22, 2016 at 8:29 AM, lily li <chocold12 at gmail.com> wrote:

> Thanks, Ismail.
> For the gaps before 2009-01-05 and after 2009-11-20, I use the year 2010 to
> fill in the missing values for column C. There is no relationship between
> column A, B, and C.
> For the missing values between 2009-01-05 and 2009-11-20, if there are any,
> I found this approach is very helpful.
> with(df, approx(x=time, y=C, xout=seq(min(time), max(time), by="days")))
>
>
>
> On Thu, Jul 21, 2016 at 5:14 PM, Ismail SEZEN <sezenismail at gmail.com>
> wrote:
>
> >
> > > On 22 Jul 2016, at 01:34, lily li <chocold12 at gmail.com> wrote:
> > >
> > > I have a question about interpolating missing values in a dataframe.
> >
> > First of all, filling missing values action must be taken into account
> > very carefully. It must be known the nature of the data that wanted to be
> > filled and most of the time, to let them be NA is the most appropriate
> > action.
> >
> > > The
> > > dataframe is in the following, Column C has no data before 2009-01-05
> and
> > > after 2009-12-31, how to interpolate data for the blanks?
> >
> > Why a dataframe? Is there any relationship between columns A,B and C? If
> > there is, then you might want to consider filling missing values by a
> > linear model approach instead of interpolation. You said that there is
> not
> > data before 2009-01-05 and after 2009-12-31 but according to dataframe,
> > there is not data after 2009-11-20?
> >
> > > That is to say,
> > > interpolate linearly between these two gaps using 5.4 and 6.1? Thanks.
> >
> > Also you metion interpolating blanks but you want interpolation between
> > two gaps? Do you want to fill missing values before 2009-01-05 and after
> > 2009-11-20 or do you want to find intermediate values between 2009-01-05
> > and 2009-11-20? This is a bit unclear.
> >
> > >
> > >
> > > df
> > > time                A      B     C
> > > 2009-01-01    3      4.5
> > > 2009-01-02    4      5
> > > 2009-01-03    3.3   6
> > > 2009-01-04    4.1   7
> > > 2009-01-05    4.4   6.2   5.4
> > > ...
> > >
> > > 2009-11-20    5.1   5.5   6.1
> > > 2009-11-21    5.4   4
> > > ...
> > > 2009-12-31    4.5   6
> >
> >
> > If you want to fill missing values at the end-points for column C (before
> > 2009-01-05 and after 2009-11-20), and all data you have is between
> > 2009-01-05 and 2009-11-20, this means that you want extrapolation
> (guessing
> > unkonwn values that is out of known values). So, you can use only values
> at
> > column C to guess missing end-point values. You can use splinefun (or
> > spline) functions for this purpose. But let me note that this kind of
> > approach might help you only for a few missing values close to
> end-points.
> > Otherwise, you might find yourself in a huge mistake.
> >
> > As I mentioned in my first sentence, If you have a relationship between
> > all columns or you have data for column C for other years (for instance,
> > assume that you have data for column C for 2007, 2008, and 2010 but not
> > 2009) you may want to try a statistical approach to fill the missing
> values.
> >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Fri Jul 22 18:21:04 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 22 Jul 2016 17:21:04 +0100
Subject: [R] Why the order of parameters in a logistic regression
 affects results significantly?
In-Reply-To: <1104610284.3375689.1469138662609.JavaMail.yahoo@mail.yahoo.com>
References: <1104610284.3375689.1469138662609.JavaMail.yahoo.ref@mail.yahoo.com>
	<1104610284.3375689.1469138662609.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <a8187909-3349-19dd-54c0-29854b36208c@dewey.myzen.co.uk>

Dear Peter

Have you tried removing the intercept? Just put -1 at the end of your 
formula.

On 21/07/2016 23:04, Qinghua He via R-help wrote:
> Using the same data, if I ran
> fit2 <-glm(formula=AR~Age+LumA+LumB+HER2+Basal+Normal,family=binomial,data=RacComp1)summary(fit2)exp(coef(fit2))
> I obtained:
>> exp(coef(fit2))(Intercept)         Age        LumA        LumB        HER2       Basal      Normal  0.24866935  1.00433781  0.10639937  0.31614001  0.08220685 20.25180956          NA
> while if I ran
>
> fit2 <-glm(formula=AR~Age+LumA+LumB+Basal+Normal+HER2,family=binomial,data=RacComp1)summary(fit2)exp(coef(fit2))
> I obtained:
>> exp(coef(fit2)) (Intercept)          Age         LumA         LumB        Basal       Normal         HER2   0.02044232   1.00433781   1.29428846   3.84566516 246.35185956  12.16443690           NA
>
> Essentially they're the same model - I just moved HER2 to the last. But the OR changed significantly. Can someone explain?
> For the latter result, I don't even know how to interpret as all factors have OR>1 (except Intercept), how could that possible? Can I eliminate the effect of intercept?
> Also, I cannot obtain OR for the last factor due to collinearity. However, I know others obtained OR for all factors for the same dataset. Can someone tell me how to obtain OR for all factors? All factors are categorical variables (i.e., 0 or 1).
> Thanks!
> Peter
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dwinsemius at comcast.net  Fri Jul 22 19:24:48 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 22 Jul 2016 10:24:48 -0700
Subject: [R] Why the order of parameters in a logistic regression
	affects results significantly?
In-Reply-To: <1104610284.3375689.1469138662609.JavaMail.yahoo@mail.yahoo.com>
References: <1104610284.3375689.1469138662609.JavaMail.yahoo.ref@mail.yahoo.com>
	<1104610284.3375689.1469138662609.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <3F912370-F4B7-45DE-84EC-5DD6D0C9ED51@comcast.net>


> On Jul 21, 2016, at 3:04 PM, Qinghua He via R-help <r-help at r-project.org> wrote:
> 
> Using the same data, if I ran
> fit2 <-glm(formula=AR~Age+LumA+LumB+HER2+Basal+Normal,family=binomial,data=RacComp1)summary(fit2)exp(coef(fit2))
> I obtained:

exp(coef(fit2))(Intercept)         Age        LumA        LumB        HER2       Basal      Normal

                0.24866935  1.00433781  0.10639937  0.31614001  0.08220685 20.25180956          NA 

> while if I ran
> 
> fit2 <-glm(formula=AR~Age+LumA+LumB+Basal+Normal+HER2,family=binomial,data=RacComp1)summary(fit2)exp(coef(fit2))
> I obtained:

exp(coef(fit2)) (Intercept)          Age         LumA         LumB        Basal       Normal        HER2

                 0.02044232   1.00433781   1.29428846   3.84566516 246.35185956  12.16443690           NA 

> 
> Essentially they're the same model - I just moved HER2 to the last. But the OR changed significantly. Can someone explain?

You have collinearity and one of your variables will be dropped as redundant. Which one is dropped is determined by the order of the variable names in the model formula.


> For the latter result, I don't even know how to interpret as all factors have OR>1 (except Intercept), how could that possible? Can I eliminate the effect of intercept?

In the first model (with the defaults of  treatment contrasts) the Intercept is actually an estimate for cases with LumA, LumB,Basal,Her2 all at their lowest level and this not coincidentally also precisely defines your Normal variable. They all (excepting Normal) have adverse impact in your study of AR whatever it might be. If these various categories (which I suspect are breast cancer risk predictors) are all distinct with no overlaps, then use this:

fit2 <-glm(formula=AR~Age+ Normal+ LumA+LumB+HER2+Basal+ 0,family=binomial,data=RacComp1)

The results will probably be the same as your first model except that Intercept's parameter will now be the parameter for Normal.


> Also, I cannot obtain OR for the last factor due to collinearity. However, I know others obtained OR for all factors for the same dataset. Can someone tell me how to obtain OR for all factors? All factors are categorical variables (i.e., 0 or 1).
> Thanks!
> Peter
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From chenmenis at hotmail.com  Fri Jul 22 16:08:45 2016
From: chenmenis at hotmail.com (Chen Meng)
Date: Fri, 22 Jul 2016 14:08:45 +0000
Subject: [R] Turn character /string as variable/column name in summarize in
	dplyr
Message-ID: <BN3PR05MB2641529D518BCC4A29ECB57FB30A0@BN3PR05MB2641.namprd05.prod.outlook.com>

Hi all,


Trying to turn string in to variable in dplyr , it R interprets it as strings rather than column name in the data.

Any ideas?


shock5 =paste0(shocksName[5],"fit")
  print(shock5)

  x<-group_by(plotdata,grp) %>% summarize(
    Actuals=sum(weight*response/sum(weight)),
...
...
    # assign(shocksName[4],sum(weight*as.name(paste(shocksName[4],"fit"))/sum(weight))),
     assign(shocksName[5],sum(weight*(as.environment(shock5)) /sum(weight)))
  )


Sent from Outlook<http://aka.ms/weboutlook>

	[[alternative HTML version deleted]]


From carlganz at ucla.edu  Fri Jul 22 20:12:57 2016
From: carlganz at ucla.edu (Ganz, Carl)
Date: Fri, 22 Jul 2016 18:12:57 +0000
Subject: [R] Using C library in R
Message-ID: <A5A30F1D451F924B902DDE8063410E670EE735@EM1A.ad.ucla.edu>

Hello everyone,
I am attempting to link to a C library named libxlsxwriter (http://libxlsxwriter.github.io/) that creates and styles XLSX files, but after several days of repeatedly reading "Writing R Extensions", and I am stuck and hoping someone can help me.
The C library is easy to use and works with C++ as you would expect.
For example, here is test.cpp:
#include <xlsxwriter.h>
int main() {
  lxw_workbook  *workbook  = workbook_new("myexcel.xlsx");
  lxw_worksheet *worksheet = workbook_add_worksheet(workbook, NULL);
  int row = 0;
  int col = 0;
  worksheet_write_string(worksheet, row, col, "Hello me!", NULL);
  return workbook_close(workbook);
}
To compile this I need to run the makefile in the libxlsxwriter folder, which compiles the libxlsxwriter library, and then call:
cc test.cpp -o test -Ipath.to.xlsxwriter.h path.to.libxlsxwriter.a -lz
This generates an executable that creates an excel document. I understand this command says to compile test.cpp and specifies where to search for headers, and what libraries to link to so to get this to work with R all I should need to do is make the library, point to the header, and link to the .a file.
To get this to work with R, I created a libxlsxwriter folder in my /src folder with the libxlsxwriter library in it, and added this makevars to my /src:
PKG_CFLAGS=
# specify header location
PKG_CPPFLAGS=-Ilibxlsxwriter/include
# specify libs to link to
PKG_LIBS=liblsxwriter/lib/libxlsxwriter.a -lz
# make libxlsxwriter
libxlsxwriter/lib/libxlsxwriter.a:
            cd libxlsxwriter;$(MAKE)

When I build the package I can see that this runs the makevars, and generates the .a and .dll files in libxlsxwriter. I get no errors associated with headers or the libraries so I thought I would be good to go, but I can't seem to get anything to run.
Here is my test.cpp code I am trying to run in R with Rcpp:
#include <Rcpp.h>
#include <xlsxwriter.h>
using namespace Rcpp;
//' @useDynLib libxlsxwriter
//' @export
//' @import Rcpp
// [[Rcpp::export]]
void test() {
  lxw_workbook  *workbook  = workbook_new("myexcel.xlsx");
  lxw_worksheet *worksheet = workbook_add_worksheet(workbook, NULL);
  int row = 0;
  int col = 0;
  worksheet_write_string(worksheet, row, col, "Hello me!", NULL);
  workbook_close(workbook);
}
I am fairly certain I am doing something wrong with my makevars that is preventing test.cpp from being compiled. I tried running the makevars for libxlsxwriter from the command line and then building using just this makevars:
PKG_CFLAGS=
# specify header location
PKG_CPPFLAGS=-Ilibxlsxwriter/include
# specify libs to link to
PKG_LIBS=liblsxwriter/lib/libxlsxwriter.a -lz
I see the .o files for test.cpp being built, which is a good sign, and indicates there was a problem with my previous makevars, but I get errors saying undefined reference for all the functions from libxlsxwriter, so clearly thinks aren't linking like I need them to. I am out of ideas at this point, so any guidance would be greatly appreciated.
A github repo with my code is here: https://github.com/carlganz/lwritexlsx
I was using this webpage as a guide for how to include a C library in an R package: http://mazamascience.com/WorkingWithData/?p=1151
Kind Regards,
Carl Ganz

	[[alternative HTML version deleted]]


From milujisb at gmail.com  Fri Jul 22 23:52:13 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Fri, 22 Jul 2016 23:52:13 +0200
Subject: [R] Aggregate data to lower resolution
In-Reply-To: <CAN5YmCGiejFeenv8bJV525Tm+jhCoqHbEyS6TPrY5ePhPHaeLw@mail.gmail.com>
References: <CAMLwc7O1nWa7Akou2RA8S1Rt=FWzTzUrBjfZFaFz8wTXDckEFg@mail.gmail.com>
	<CAN5YmCGiejFeenv8bJV525Tm+jhCoqHbEyS6TPrY5ePhPHaeLw@mail.gmail.com>
Message-ID: <CAMLwc7Pe=TzF-i6de-mA8ziMFmdeYxDz9QEOYjiAjJXdbrd_7Q@mail.gmail.com>

Dear Jean,

Thank you so much for your reply and the solution, This does work. I was
wondering is this similar to 'rasterFromXYZ'? Thanks again!

Sincerely,

Milu

On Fri, Jul 22, 2016 at 3:06 PM, Adams, Jean <jvadams at usgs.gov> wrote:

> Milu,
>
> Perhaps an approach like this would work.  In the example below, I
> calculate the mean GDP for each 1 degree by 1 degree.
>
> temp$long1 <- floor(temp$longitude)
> temp$lat1 <- floor(temp$latitude)
> temp1 <- aggregate(GDP ~ long1 + lat1, temp, mean)
>
>   long1 lat1        GDP
> 1   -69  -55 0.90268640
> 2   -68  -55 0.09831317
> 3   -72  -54 0.22379000
> 4   -71  -54 0.14067290
> 5   -70  -54 0.00300380
> 6   -69  -54 0.00574220
>
> Jean
>
> On Thu, Jul 21, 2016 at 3:57 PM, Miluji Sb <milujisb at gmail.com> wrote:
>
>> Dear all,
>>
>> I have the following GDP data by latitude and longitude at 0.5 degree by
>> 0.5 degree.
>>
>> temp <- dput(head(ptsDF,10))
>> structure(list(longitude = c(-68.25, -67.75, -67.25, -68.25,
>> -67.75, -67.25, -71.25, -70.75, -69.25, -68.75), latitude = c(-54.75,
>> -54.75, -54.75, -54.25, -54.25, -54.25, -53.75, -53.75, -53.75,
>> -53.75), GDP = c(1.683046, 0.3212307, 0.0486207, 0.1223268, 0.0171909,
>> 0.0062104, 0.22379, 0.1406729, 0.0030038, 0.0057422)), .Names =
>> c("longitude",
>> "latitude", "GDP"), row.names = c(4L, 17L, 30L, 43L, 56L, 69L,
>> 82L, 95L, 108L, 121L), class = "data.frame")
>>
>> I would like to aggregate the data 1 degree by 1 degree. I understand that
>> the first step is to convert to raster. I have tried:
>>
>> rasterDF <- rasterFromXYZ(temp)
>> r <- aggregate(rasterDF,fact=2, fun=sum)
>>
>> But this does not seem to work. Could anyone help me out please? Thank you
>> in advance.
>>
>> Sincerely,
>>
>> Milu
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Sat Jul 23 00:16:22 2016
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 22 Jul 2016 14:16:22 -0800
Subject: [R] Turn character /string as variable/column name in summarize
 in dplyr
In-Reply-To: <BN3PR05MB2641529D518BCC4A29ECB57FB30A0@BN3PR05MB2641.namprd05.prod.outlook.com>
Message-ID: <23AC215DA9D.00000BD8jrkrideau@inbox.com>

It really might help to have a minimum working example
Have a look at 
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
and/or 
http://adv-r.had.co.nz/Reproducibility.html

John Kane
Kingston ON Canada


> -----Original Message-----
> From: chenmenis at hotmail.com
> Sent: Fri, 22 Jul 2016 14:08:45 +0000
> To: r-help at r-project.org
> Subject: [R] Turn character /string as variable/column name in summarize
> in dplyr
> 
> Hi all,
> 
> 
> Trying to turn string in to variable in dplyr , it R interprets it as
> strings rather than column name in the data.
> 
> Any ideas?
> 
> 
> shock5 =paste0(shocksName[5],"fit")
>   print(shock5)
> 
>   x<-group_by(plotdata,grp) %>% summarize(
>     Actuals=sum(weight*response/sum(weight)),
> ...
> ...
>     #
> assign(shocksName[4],sum(weight*as.name(paste(shocksName[4],"fit"))/sum(weight))),
>      assign(shocksName[5],sum(weight*(as.environment(shock5))
> /sum(weight)))
>   )
> 
> 
> Sent from Outlook<http://aka.ms/weboutlook>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From jdnewmil at dcn.davis.ca.us  Sat Jul 23 02:43:59 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 22 Jul 2016 17:43:59 -0700
Subject: [R] Using C library in R
In-Reply-To: <A5A30F1D451F924B902DDE8063410E670EE735@EM1A.ad.ucla.edu>
References: <A5A30F1D451F924B902DDE8063410E670EE735@EM1A.ad.ucla.edu>
Message-ID: <CD37BE56-DA8C-4BCD-93AF-0A9CD1C3A981@dcn.davis.ca.us>

Read the Posting Guide. This will tell you at least two important things:

1) Post using plain text. HTML mangles code.

2) Interfacing R with other languages is off-topic on this list. There are other lists where such issues are on-topic. Your post is a bit like walking into a bowling alley and asking if anyone there can solve your chess problem... someone might be able to, but it isn't very efficient and disturbs the bowlers unnecessarily.
-- 
Sent from my phone. Please excuse my brevity.

On July 22, 2016 11:12:57 AM PDT, "Ganz, Carl" <carlganz at ucla.edu> wrote:
>Hello everyone,
>I am attempting to link to a C library named libxlsxwriter
>(http://libxlsxwriter.github.io/) that creates and styles XLSX files,
>but after several days of repeatedly reading "Writing R Extensions",
>and I am stuck and hoping someone can help me.
>The C library is easy to use and works with C++ as you would expect.
>For example, here is test.cpp:
>#include <xlsxwriter.h>
>int main() {
>  lxw_workbook  *workbook  = workbook_new("myexcel.xlsx");
>  lxw_worksheet *worksheet = workbook_add_worksheet(workbook, NULL);
>  int row = 0;
>  int col = 0;
>  worksheet_write_string(worksheet, row, col, "Hello me!", NULL);
>  return workbook_close(workbook);
>}
>To compile this I need to run the makefile in the libxlsxwriter folder,
>which compiles the libxlsxwriter library, and then call:
>cc test.cpp -o test -Ipath.to.xlsxwriter.h path.to.libxlsxwriter.a -lz
>This generates an executable that creates an excel document. I
>understand this command says to compile test.cpp and specifies where to
>search for headers, and what libraries to link to so to get this to
>work with R all I should need to do is make the library, point to the
>header, and link to the .a file.
>To get this to work with R, I created a libxlsxwriter folder in my /src
>folder with the libxlsxwriter library in it, and added this makevars to
>my /src:
>PKG_CFLAGS=
># specify header location
>PKG_CPPFLAGS=-Ilibxlsxwriter/include
># specify libs to link to
>PKG_LIBS=liblsxwriter/lib/libxlsxwriter.a -lz
># make libxlsxwriter
>libxlsxwriter/lib/libxlsxwriter.a:
>            cd libxlsxwriter;$(MAKE)
>
>When I build the package I can see that this runs the makevars, and
>generates the .a and .dll files in libxlsxwriter. I get no errors
>associated with headers or the libraries so I thought I would be good
>to go, but I can't seem to get anything to run.
>Here is my test.cpp code I am trying to run in R with Rcpp:
>#include <Rcpp.h>
>#include <xlsxwriter.h>
>using namespace Rcpp;
>//' @useDynLib libxlsxwriter
>//' @export
>//' @import Rcpp
>// [[Rcpp::export]]
>void test() {
>  lxw_workbook  *workbook  = workbook_new("myexcel.xlsx");
>  lxw_worksheet *worksheet = workbook_add_worksheet(workbook, NULL);
>  int row = 0;
>  int col = 0;
>  worksheet_write_string(worksheet, row, col, "Hello me!", NULL);
>  workbook_close(workbook);
>}
>I am fairly certain I am doing something wrong with my makevars that is
>preventing test.cpp from being compiled. I tried running the makevars
>for libxlsxwriter from the command line and then building using just
>this makevars:
>PKG_CFLAGS=
># specify header location
>PKG_CPPFLAGS=-Ilibxlsxwriter/include
># specify libs to link to
>PKG_LIBS=liblsxwriter/lib/libxlsxwriter.a -lz
>I see the .o files for test.cpp being built, which is a good sign, and
>indicates there was a problem with my previous makevars, but I get
>errors saying undefined reference for all the functions from
>libxlsxwriter, so clearly thinks aren't linking like I need them to. I
>am out of ideas at this point, so any guidance would be greatly
>appreciated.
>A github repo with my code is here:
>https://github.com/carlganz/lwritexlsx
>I was using this webpage as a guide for how to include a C library in
>an R package: http://mazamascience.com/WorkingWithData/?p=1151
>Kind Regards,
>Carl Ganz
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From edd at debian.org  Sat Jul 23 03:00:19 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 23 Jul 2016 01:00:19 +0000
Subject: [R] Using C library in R
References: <A5A30F1D451F924B902DDE8063410E670EE735@EM1A.ad.ucla.edu>
	<CD37BE56-DA8C-4BCD-93AF-0A9CD1C3A981@dcn.davis.ca.us>
Message-ID: <loom.20160723T025924-383@post.gmane.org>

Jeff Newmiller <jdnewmil <at> dcn.davis.ca.us> writes:
> 2) Interfacing R with other languages is off-topic on this list. There are 
other lists where such issues are
> on-topic. Your post is a bit like walking into a bowling alley and asking if 
anyone there can solve your
> chess problem... someone might be able to, but it isn't very efficient and 
disturbs the bowlers unnecessarily.

Not sure I agree. It is an advanced topic, but it is not off-list.

Dirk


From jdnewmil at dcn.davis.ca.us  Sat Jul 23 04:02:38 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 22 Jul 2016 19:02:38 -0700
Subject: [R] Using C library in R
In-Reply-To: <loom.20160723T025924-383@post.gmane.org>
References: <A5A30F1D451F924B902DDE8063410E670EE735@EM1A.ad.ucla.edu>
	<CD37BE56-DA8C-4BCD-93AF-0A9CD1C3A981@dcn.davis.ca.us>
	<loom.20160723T025924-383@post.gmane.org>
Message-ID: <5480A265-9DC2-4176-845F-6825FD48339E@dcn.davis.ca.us>

You are entitled to your opinion,  but apparently you have not read the Posting Guide either.
-- 
Sent from my phone. Please excuse my brevity.

On July 22, 2016 6:00:19 PM PDT, Dirk Eddelbuettel <edd at debian.org> wrote:
>Jeff Newmiller <jdnewmil <at> dcn.davis.ca.us> writes:
>> 2) Interfacing R with other languages is off-topic on this list.
>There are 
>other lists where such issues are
>> on-topic. Your post is a bit like walking into a bowling alley and
>asking if 
>anyone there can solve your
>> chess problem... someone might be able to, but it isn't very
>efficient and 
>disturbs the bowlers unnecessarily.
>
>Not sure I agree. It is an advanced topic, but it is not off-list.
>
>Dirk
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From grm.ales at gmail.com  Sat Jul 23 01:16:18 2016
From: grm.ales at gmail.com (=?UTF-8?B?QWxlxaEgR3Jt?=)
Date: Fri, 22 Jul 2016 18:16:18 -0500
Subject: [R] Improving performance by rewriting for loops into apply
	functions
Message-ID: <CAOM6uJVf=37W=2+s5x=AqDgDxOsm3eZJ-yDUoCL6U2_VgWq1Ag@mail.gmail.com>

Hello,

I have a slight performance issue that I'd like to solve by rewriting a
short bit of code that uses for loops so that it would use apply in order
to get some performance gains. My problem is that I can't modify the
variables that are passed to apply function during apply functions
execution and use it's latest results. My first thought was to use apply
functions but if this isn't possible I'm open to other suggestions.

For example:
path1 = matrix(c(1,2,3,4,5), ncol=1);
path2 = matrix(c(1,2,3,4,5,6), ncol=1);
apply(path1, 2, function(x, path2){
  tmp = x*path2[x+1];
  path2[x+1] = tmp;
  return(tmp);
}, path2)

In the code above, path2 should have its elements updated in the course of
the apply function and its value should be use in the next iteration while
executing apply function but that doesn't happen. It seems that when a
variable is passed into apply function (path2) it is immutable.

BR Ale?

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Jul 23 08:22:02 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 22 Jul 2016 23:22:02 -0700
Subject: [R] Improving performance by rewriting for loops into
	apply	functions
In-Reply-To: <CAOM6uJVf=37W=2+s5x=AqDgDxOsm3eZJ-yDUoCL6U2_VgWq1Ag@mail.gmail.com>
References: <CAOM6uJVf=37W=2+s5x=AqDgDxOsm3eZJ-yDUoCL6U2_VgWq1Ag@mail.gmail.com>
Message-ID: <1B35942E-76CA-4380-A634-B40D4AFF0740@dcn.davis.ca.us>

If you complain to the doctor that it hurts when you ram your head into the wall, (s)he is going to tell you to not so that. What do you expect us to say? 

You seem full of misinformation. The apply family functions do not necessarily speed anything up... they are just more compact than for loops.

And yes, their arguments are immutable... as are pretty much all arguments to functions other than environments. So your screwdriver is not going to work on that nail.

Also, working with N x 1 matrices is silly... this is not Matlab.

In many cases you can restructure the problem so that you don't need incremental calculation... but I am not really sure what you wanted in this case. If this were a for loop the code you have looks to me like it would produce

path1 <- c(1,2,3,4,5)
path2 <- c(1,2,3,4,5,6)
path1 * path2[ -1 ]

Maybe it is time for you to read "The R Inferno"? 

Also please read the Posting Guide, which warns you to post plain text (so your email does not get corrupted when it is stripped by the mailing list).
-- 
Sent from my phone. Please excuse my brevity.

On July 22, 2016 4:16:18 PM PDT, "Ale? Grm" <grm.ales at gmail.com> wrote:
>Hello,
>
>I have a slight performance issue that I'd like to solve by rewriting a
>short bit of code that uses for loops so that it would use apply in
>order
>to get some performance gains. My problem is that I can't modify the
>variables that are passed to apply function during apply functions
>execution and use it's latest results. My first thought was to use
>apply
>functions but if this isn't possible I'm open to other suggestions.
>
>For example:
>path1 = matrix(c(1,2,3,4,5), ncol=1);
>path2 = matrix(c(1,2,3,4,5,6), ncol=1);
>apply(path1, 2, function(x, path2){
>  tmp = x*path2[x+1];
>  path2[x+1] = tmp;
>  return(tmp);
>}, path2)
>
>In the code above, path2 should have its elements updated in the course
>of
>the apply function and its value should be use in the next iteration
>while
>executing apply function but that doesn't happen. It seems that when a
>variable is passed into apply function (path2) it is immutable.
>
>BR Ale?
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From maitra.mbox.ignored at inbox.com  Sat Jul 23 13:55:11 2016
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Sat, 23 Jul 2016 06:55:11 -0500
Subject: [R] Improving performance by rewriting for loops into apply
 functions
In-Reply-To: <CAOM6uJVf=37W=2+s5x=AqDgDxOsm3eZJ-yDUoCL6U2_VgWq1Ag@mail.gmail.com>
References: <CAOM6uJVf=37W=2+s5x=AqDgDxOsm3eZJ-yDUoCL6U2_VgWq1Ag@mail.gmail.com>
Message-ID: <20160723065511.6ffafd4a0dd73f86fef54bef@inbox.com>

Hi,

The apply here does exactly what you would expect. (Your problem seems to be with the function that you have written. For some reason, you want to change the values of path2 but are passing it as a variable o a function. The global value of path2 will not change.) For that, you have to also return path2. 

Try:

path1 <- matrix(c(1,2,3,4,5), ncol=1);
path2 <- matrix(c(1,2,3,4,5,6), ncol=1);
apply(X = path1, MAR = 2, function(x, path2 = path2){
    tmp <- x*path2[x+1]
    path2[x+1] <- tmp
    list(tmp, path2)
}, path2 = path2)


Btw, it is good for your sanity to make the distinction between assignments and arguments, and also name your arguments for your function when you call them. I have done that. It makes understanding your code at a glance easier. 

As a personal preference, I have no idea what this is supposed to do, but you may be better off avoiding apply here. I am always nervous about passing arguments such as you do with the x in the apply function. But it is correct, and hopefully does what you want. so I will say nothing more.

HTH!

Best wishes,
Ranjan


On Fri, 22 Jul 2016 18:16:18 -0500 Ale? Grm <grm.ales at gmail.com> wrote:

> Hello,
> 
> I have a slight performance issue that I'd like to solve by rewriting a
> short bit of code that uses for loops so that it would use apply in order
> to get some performance gains. My problem is that I can't modify the
> variables that are passed to apply function during apply functions
> execution and use it's latest results. My first thought was to use apply
> functions but if this isn't possible I'm open to other suggestions.
> 
> For example:
> path1 = matrix(c(1,2,3,4,5), ncol=1);
> path2 = matrix(c(1,2,3,4,5,6), ncol=1);
> apply(path1, 2, function(x, path2){
>   tmp = x*path2[x+1];
>   path2[x+1] = tmp;
>   return(tmp);
> }, path2)
> 
> In the code above, path2 should have its elements updated in the course of
> the apply function and its value should be use in the next iteration while
> executing apply function but that doesn't happen. It seems that when a
> variable is passed into apply function (path2) it is immutable.
> 
> BR Ale?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From edd at debian.org  Sat Jul 23 18:58:15 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 23 Jul 2016 16:58:15 +0000
Subject: [R] Using C library in R
References: <A5A30F1D451F924B902DDE8063410E670EE735@EM1A.ad.ucla.edu>
	<CD37BE56-DA8C-4BCD-93AF-0A9CD1C3A981@dcn.davis.ca.us>
	<loom.20160723T025924-383@post.gmane.org>
	<5480A265-9DC2-4176-845F-6825FD48339E@dcn.davis.ca.us>
Message-ID: <loom.20160723T185635-646@post.gmane.org>

Jeff Newmiller <jdnewmil <at> dcn.davis.ca.us> writes:
> You are entitled to your opinion,  but apparently you have not read the 
Posting Guide either.

Where as you chose to ignore it (cf paragraph about Good Manners). That 
is clearly superior.

The Posting Guide is not specific about this and does NOT say that all 
questions related to compiled code should go to r-devel.  

Dirk


From cristina.cametti at gmail.com  Sat Jul 23 16:49:53 2016
From: cristina.cametti at gmail.com (Cristina Cametti)
Date: Sat, 23 Jul 2016 16:49:53 +0200
Subject: [R] country fixed effects model with binary dependent variable
Message-ID: <450D2061-6663-4D8E-8490-ECF75F5189A1@yahoo.it>

Dear all,

I am having problems finding a reliable code for a country fixed effects model with binary dependent variable. I was able to run it for another part of my research, because in that case the dependent variable is continuous. 
This is my code for the continuous dependent variable ?imwbcrm_rec?:

modelfe2 <- lm(imwbcrm_rec ~ tvpol + victim + agea + gndr + eduyrs + lrscale_GM + imgfrnd_dum +  qfimwht +factor(cntry)-1, data=mydata)

Please don?t mind to how I wrote the variables, they are from the first wave of the ESS survey. At this point, I have three questions: 
- do you think this code is correct? Since the data are all from the same year (2002), I did not used the ppm package since it is only for panel data. The results of the previous code make sense, so I am satisfied. However, I want to be sure that I am running the right code.
- second questions: someone knows the code for the same analysis, but having a BINARY dependent variable (aesfdrk_dummy)? I found very different information on the internet, and unfortunately, I do not know how to use STATA, so I need to find a reliable code in r. This is the code that I have now:

modelfe1 <-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea+ gndr + eduyrs + domicil + partner + tvpol + hincfel_dum + factor(cntry) -1, data=mydata)

-last question: I have to add some interaction between country level variables and individual level variables. So, do you think that this code is right?

mydata$ppltrst_GMXprison_pop <- mydata$ppltrst_GM*mydata$prison_pop 
mydata$ppltrst_GMXforeign_pop <- mydata$ppltrst_GM*mydata$foreign_pop
mydata$victimXprison_pop<- mydata$victim*mydata$prison_pop 
mydata$victimXforeign_pop<- mydata$victim*mydata$foreign_pop
mydata$mixed_neighXprison_pop<- mydata$mixed_neigh*mydata$prison_pop 
mydata$mixed_neighXforeign_pop <- mydata$mixed_neigh*mydata$foreign_pop
mydata$ethnic_neighXprison_pop <- mydata$ethnic_neigh*mydata$prison_pop 
mydata$ethnic_neighXforeign_pop <- mydata$ethnic_neigh*mydata$foreign_pop

modelfe1.2<-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea + gndr + eduyrs + domicil + partner + tvpol + hincfel_dum + victim + ppltrst + ppltrstXprison_pop + ppltrst_Xforeign_pop_ + victimXprison_pop_ + victimXforeign_pop + mixed_neighXprison_pop + mixed_neighXforeign_pop + ethnic_neighXprison_pop + ethnic_neighXforeign_pop + factor(cntry)-1, data=mydata)


Thank you very much for your attention.
Kind Regards,

Cristina 

From nika.susac at gmail.com  Sat Jul 23 19:39:48 2016
From: nika.susac at gmail.com (=?UTF-8?Q?Nika_Su=C5=A1ac?=)
Date: Sat, 23 Jul 2016 19:39:48 +0200
Subject: [R] Estimators for non-normal data
Message-ID: <CAMmS1Wh8_b1yPeUwoLp=BiKyxNyiVxtJJb=+X+L7joepy87t7Q@mail.gmail.com>

Hi!
I have non-normal data (items are continuous on a 9-point scale, but not
normally distributed) and I want to conduct cfa. Which of the estimators
available in lavaan do you recommend me to use?
Thanks in advance!

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Sun Jul 24 02:52:01 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 24 Jul 2016 12:52:01 +1200
Subject: [R] [FORGED] country fixed effects model with binary dependent
 variable
In-Reply-To: <450D2061-6663-4D8E-8490-ECF75F5189A1@yahoo.it>
References: <450D2061-6663-4D8E-8490-ECF75F5189A1@yahoo.it>
Message-ID: <e6250168-8cd9-1531-e145-66dc4253a5e3@auckland.ac.nz>


On 24/07/16 02:49, Cristina Cametti wrote:

> Dear all,
>
> I am having problems finding a reliable code for a country fixed
effects model with binary dependent variable. I was able to run it for
another part of my research, because in that case the dependent variable
is continuous.
> This is my code for the continuous dependent variable ?imwbcrm_rec?:
>
> modelfe2 <- lm(imwbcrm_rec ~ tvpol + victim + agea + gndr + eduyrs +
lrscale_GM + imgfrnd_dum + qfimwht +factor(cntry)-1, data=mydata)
>
> Please don?t mind to how I wrote the variables, they are from the
first wave of the ESS survey. At this point, I have three questions:
> - do you think this code is correct?

It is impossible to say; members of this list are not in general telepathic.

You need to describe your problem and your data more clearly, or 
possibly supply a reproducible example, in order for anyone to have a 
ghost of a chance of being able to advise you.

> Since the data are all from the
same year (2002), I did not used the ppm package since it is only for
panel data. The results of the previous code make sense, so I am
satisfied. However, I want to be sure that I am running the right code.
> - second questions: someone knows the code for the same analysis,
> but
having a BINARY dependent variable (aesfdrk_dummy)? I found very
different information on the internet, and unfortunately, I do not know
how to use STATA, so I need to find a reliable code in r. This is the
code that I have now:
>
> modelfe1 <-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea+ gndr
> +
eduyrs + domicil + partner + tvpol + hincfel_dum + factor(cntry) -1,
data=mydata)

It is very common to model a binary response using a logit model
(i.e. use glm() with family=binomial).  Whether this is appropriate in 
the given case is impossible to say.

> -last question: I have to add some interaction between country level
variables and individual level variables. So, do you think that this
code is right?
>
> mydata$ppltrst_GMXprison_pop <- mydata$ppltrst_GM*mydata$prison_pop
> mydata$ppltrst_GMXforeign_pop <-
> mydata$ppltrst_GM*mydata$foreign_pop mydata$victimXprison_pop<-
> mydata$victim*mydata$prison_pop mydata$victimXforeign_pop<-
> mydata$victim*mydata$foreign_pop mydata$mixed_neighXprison_pop<-
> mydata$mixed_neigh*mydata$prison_pop mydata$mixed_neighXforeign_pop
> <- mydata$mixed_neigh*mydata$foreign_pop
> mydata$ethnic_neighXprison_pop <-
> mydata$ethnic_neigh*mydata$prison_pop mydata$ethnic_neighXforeign_pop
> <- mydata$ethnic_neigh*mydata$foreign_pop
>
> modelfe1.2<-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea +
> gndr
+ eduyrs + domicil + partner + tvpol + hincfel_dum + victim + ppltrst +
ppltrstXprison_pop + ppltrst_Xforeign_pop_ + victimXprison_pop_ +
victimXforeign_pop + mixed_neighXprison_pop + mixed_neighXforeign_pop +
ethnic_neighXprison_pop + ethnic_neighXforeign_pop + factor(cntry)-1,
data=mydata)

The foregoing makes no sense at all to me.  Perhaps I am just being 
obtuse, but I doubt it.

You seem to be very much out of your depth.  I think you would be 
well-advised to seek local statistical consultation.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Sun Jul 24 06:04:02 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 23 Jul 2016 21:04:02 -0700
Subject: [R] country fixed effects model with binary dependent variable
In-Reply-To: <450D2061-6663-4D8E-8490-ECF75F5189A1@yahoo.it>
References: <450D2061-6663-4D8E-8490-ECF75F5189A1@yahoo.it>
Message-ID: <CD8E21EF-A15D-4758-BDF8-5EB3056D1174@comcast.net>


> On Jul 23, 2016, at 7:49 AM, Cristina Cametti <cristina.cametti at gmail.com> wrote:
> 
> Dear all,
> 
> I am having problems finding a reliable code for a country fixed effects model with binary dependent variable. I was able to run it for another part of my research, because in that case the dependent variable is continuous. 
> This is my code for the continuous dependent variable ?imwbcrm_rec?:
> 
> modelfe2 <- lm(imwbcrm_rec ~ tvpol + victim + agea + gndr + eduyrs + lrscale_GM + imgfrnd_dum +  qfimwht +factor(cntry)-1, data=mydata)
> 
> Please don?t mind to how I wrote the variables,

That is the most <bite-my-tongue> <deleted> preamble I have ever seen. How can it not be important how you constructed your covariates?   .... ?????

> they are from the first wave of the ESS survey. At this point, I have three questions: 
> - do you think this code is correct?

Really? First you ask us not to question your construction of covariates, and ... THEN ask us if it is "correct"?


> Since the data are all from the same year (2002), I did not used the ppm package since it is only for panel data. The results of the previous code make sense, so I am satisfied. However, I want to be sure that I am running the right code.
> - second questions: someone knows the code for the same analysis, but having a BINARY dependent variable (aesfdrk_dummy)? I found very different information on the internet, and unfortunately, I do not know how to use STATA, so I need to find a reliable code in r. This is the code that I have now:
> 
> modelfe1 <-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea+ gndr + eduyrs + domicil + partner + tvpol + hincfel_dum + factor(cntry) -1, data=mydata)
> 
> -last question: I have to add some interaction between country level variables and individual level variables. So, do you think that this code is right?
> 
> mydata$ppltrst_GMXprison_pop <- mydata$ppltrst_GM*mydata$prison_pop 
> mydata$ppltrst_GMXforeign_pop <- mydata$ppltrst_GM*mydata$foreign_pop
> mydata$victimXprison_pop<- mydata$victim*mydata$prison_pop 
> mydata$victimXforeign_pop<- mydata$victim*mydata$foreign_pop
> mydata$mixed_neighXprison_pop<- mydata$mixed_neigh*mydata$prison_pop 
> mydata$mixed_neighXforeign_pop <- mydata$mixed_neigh*mydata$foreign_pop
> mydata$ethnic_neighXprison_pop <- mydata$ethnic_neigh*mydata$prison_pop 
> mydata$ethnic_neighXforeign_pop <- mydata$ethnic_neigh*mydata$foreign_pop
> 
> modelfe1.2<-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea + gndr + eduyrs + domicil + partner + tvpol + hincfel_dum + victim + ppltrst + ppltrstXprison_pop + ppltrst_Xforeign_pop_ + victimXprison_pop_ + victimXforeign_pop + mixed_neighXprison_pop + mixed_neighXforeign_pop + ethnic_neighXprison_pop + ethnic_neighXforeign_pop + factor(cntry)-1, data=mydata)
> 
> 
> Thank you very much for your attention.
> Kind Regards,
> 
> Cristina 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Sun Jul 24 06:50:30 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 24 Jul 2016 14:50:30 +1000
Subject: [R] country fixed effects model with binary dependent variable
In-Reply-To: <450D2061-6663-4D8E-8490-ECF75F5189A1@yahoo.it>
References: <450D2061-6663-4D8E-8490-ECF75F5189A1@yahoo.it>
Message-ID: <CA+8X3fVMPzzg+pNXV4Q3U1prDv8SRE9DnVa6UJksmSPoMvPe+Q@mail.gmail.com>

Hi Cristina,
As Rolf has noted, you probably don't want to persist with "lm" since
I think you have dichotomized your initial dependent variable. I also
think that you meant "don't worry about the change of variable names"
with "how I wrote the variables". I also think that you want to test
interactions between the variables you are adding. _Maybe_ something
like this:

modelfe1.2<-glm(aesfdrk_dummy ~ agea + gndr + eduyrs + domicil +
partner + tvpol + hincfel_dum + ppltrst_GM:prison_pop +
ppltrst_GM:foreign_pop_ + victim*prison_pop_ + victim*foreign_pop +
mixed_neigh*prison_pop + mixed_neigh*foreign_pop +
ethnic_neigh*prison_pop + ethnic_neigh*foreign_pop + factor(cntry)-1,
data=mydata, family="binomial")

Jim


On Sun, Jul 24, 2016 at 12:49 AM, Cristina Cametti
<cristina.cametti at gmail.com> wrote:
> Dear all,
>
> I am having problems finding a reliable code for a country fixed effects model with binary dependent variable. I was able to run it for another part of my research, because in that case the dependent variable is continuous.
> This is my code for the continuous dependent variable ?imwbcrm_rec?:
>
> modelfe2 <- lm(imwbcrm_rec ~ tvpol + victim + agea + gndr + eduyrs + lrscale_GM + imgfrnd_dum +  qfimwht +factor(cntry)-1, data=mydata)
>
> Please don?t mind to how I wrote the variables, they are from the first wave of the ESS survey. At this point, I have three questions:
> - do you think this code is correct? Since the data are all from the same year (2002), I did not used the ppm package since it is only for panel data. The results of the previous code make sense, so I am satisfied. However, I want to be sure that I am running the right code.
> - second questions: someone knows the code for the same analysis, but having a BINARY dependent variable (aesfdrk_dummy)? I found very different information on the internet, and unfortunately, I do not know how to use STATA, so I need to find a reliable code in r. This is the code that I have now:
>
> modelfe1 <-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea+ gndr + eduyrs + domicil + partner + tvpol + hincfel_dum + factor(cntry) -1, data=mydata)
>
> -last question: I have to add some interaction between country level variables and individual level variables. So, do you think that this code is right?
>
> mydata$ppltrst_GMXprison_pop <- mydata$ppltrst_GM*mydata$prison_pop
> mydata$ppltrst_GMXforeign_pop <- mydata$ppltrst_GM*mydata$foreign_pop
> mydata$victimXprison_pop<- mydata$victim*mydata$prison_pop
> mydata$victimXforeign_pop<- mydata$victim*mydata$foreign_pop
> mydata$mixed_neighXprison_pop<- mydata$mixed_neigh*mydata$prison_pop
> mydata$mixed_neighXforeign_pop <- mydata$mixed_neigh*mydata$foreign_pop
> mydata$ethnic_neighXprison_pop <- mydata$ethnic_neigh*mydata$prison_pop
> mydata$ethnic_neighXforeign_pop <- mydata$ethnic_neigh*mydata$foreign_pop
>
> modelfe1.2<-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea + gndr + eduyrs + domicil + partner + tvpol + hincfel_dum + victim + ppltrst + ppltrstXprison_pop + ppltrst_Xforeign_pop_ + victimXprison_pop_ + victimXforeign_pop + mixed_neighXprison_pop + mixed_neighXforeign_pop + ethnic_neighXprison_pop + ethnic_neighXforeign_pop + factor(cntry)-1, data=mydata)
>
>
> Thank you very much for your attention.
> Kind Regards,
>
> Cristina
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rhurlin at gwdg.de  Sun Jul 24 16:57:21 2016
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Sun, 24 Jul 2016 16:57:21 +0200
Subject: [R] Build command in library(devtools)
In-Reply-To: <ac656da0-e208-83bb-6e68-e8e602158c2a@gmail.com>
References: <291e222a-a541-9b79-459c-bcdcd7f8e66b@gmail.com>
	<CAAJSdjhC6V-RE6pSg41+1o6LKiPibWreszXQLAQiC1Qfy2DWsQ@mail.gmail.com>
	<196aed4a-f7a8-f898-bff4-84c548119d67@gmail.com>
	<2d7ab661-863a-0bef-e521-f02dfe166561@gmail.com>
	<d8a3da84-d636-b581-e594-cec257a32e8d@gmail.com>
	<b0c3f6a6-26b4-65f5-8794-cd7f0bcb3ca7@gmail.com>
	<415f7fbc-8039-635c-a1db-941e27b0fd1b@gmail.com>
	<ac656da0-e208-83bb-6e68-e8e602158c2a@gmail.com>
Message-ID: <73f8d4d0-4186-a318-d4c6-b6bb5bcd546e@gwdg.de>

I think, in your case the problem might be the version of Rtools. If you
want to use R > 3.0.x, you also have to use newer versions of Rtools [1].

HTH,
Rainer Hurling

[1]  https://cran.r-project.org/bin/windows/Rtools/


Am 21.07.16 um 02:24 schrieb Steven Yen:
> Here is what I found. I had to go back to as early as R 3.0.3 (March, 
> 2014) along with Rtools30.exe that works with that version of R, in 
> order for devtools to work right. With other/later version of R, I end 
> up building a package with
> library(devtools); build("yenlib",binary=F)
> with no error message but the package does not run correctly; or with
> library(devtools); build("yenlib",binary=T)
> which deliver an error that says zip command failed (bevtools calls 
> Rtools when binary=T).
> 
> Updated versions are good, but what's the use if they do not work for a 
> situation like this.
> 
> Any help/insight would be appreciated.
> 
> On 7/20/2016 10:08 AM, Steven Yen wrote:
>> On 7/19/2016 4:38 PM, John McKown wrote:
>>> On Tue, Jul 19, 2016 at 3:15 PM, Steven Yen <syen04 at gmail.com
>>> <mailto:syen04 at gmail.com>>wrote:
>>>
>>>     I recently updated my R and RStudio to the latest version and
>>> now the
>>>     binary option in the "build" command in devtools stops working.
>>>
>>>     I went around and used the binary=F option which worked by I get
>>> the
>>>     .tar.gz file instead of the .zip file which I prefer.
>>>
>>>     Does anyone understand the following error message:
>>>
>>>     status 127
>>>     running 'zip' failed
>>>
>>>
>>> ?I'm not totally sure, but I think that means that R cannot find the
>>> "zip" program in order to run it. ?
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Marios.BARLAS at cea.fr  Sun Jul 24 17:14:49 2016
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Sun, 24 Jul 2016 15:14:49 +0000
Subject: [R] Create a common wafer map colour scale for different data sets
Message-ID: <01BFC0B2B4ABFC4CB432008F852D7661793C59@EXDAG0-A1.intra.cea.fr>

Hello Every1, 

I'm working on analysing some data and I want to make some cartography maps. 

Since I treat data in batch, I need to have a common reference colour scale for all the datasets I need to plot. This is important otherwise it becomes hard to quickly assert visually changes from one experiment to another if the colour scale resets all the time. 

So I though of finding the min and max value of my data values and create a colour scale versus that. 

Nonetheless the code does not work as intended, 

Here is a part of the code: 


  r.colour.map <- c("lightgreen", "darkgreen", "yellow","red" )

  r.range.breaks <- c(0, 1.5e4, 1e5, r.max )/r.max  

  r.breaks.guide <- c(0, 1.5e4, 1e5, r.max )  # r.max is function paramer
  g.map <- ggplot(dies.in.wafer, aes(X, Y)) + # data set to be plotted and X,Y coordinates
    geom_raster(aes(fill = R))+                       # values to be mapped
    scale_fill_gradientn(name="R", na.value = "grey50", colours = r.colour.map, values = r.range.breaks, breaks=r.breaks.guide )
  g.map

This does no work as intended. My intention is to have a code that interpolates colors according to set value ranges:

for values from 0 to 1.5e4 interpolate gradient between lightgreen and green
for values from 1.5e4 to 5e5 interpolate gradient between green and yellow
for values from 5e5 to max  interpolate gradient between yellow and red


Any1 has some ideas of what I'm doing wrong, or workarounds ?

Thanks in advance!

Regards,
Marios Barlas

From qinghua.he at yahoo.com  Sun Jul 24 17:20:11 2016
From: qinghua.he at yahoo.com (Qinghua He)
Date: Sun, 24 Jul 2016 15:20:11 +0000 (UTC)
Subject: [R] Firth logistic regression and odds ratio
References: <616531008.4808688.1469373611694.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <616531008.4808688.1469373611694.JavaMail.yahoo@mail.yahoo.com>

Dear all,
I have the following table for a breast cancer study and I was trying to calculate the odds ratio of different subtypes based on AR (AR+ as the base case).
? ? ? ? ? ? ? ? ? AR- ? ?AR+Luminal A ? ?1 ? ? 19Luminal B ? ?1 ? ? 15Her2 ? ? ? ? ? ?0 ? ? 4Basal-like ? ?20 ? ?1Normal Like ?2 ? ? 2
I did Firth logistic regression using the model:?
fit <- logistf(AR ~ Age+LumA+LumB+HER2+Basal+Normal-1,data=df).summary(fit)exp(coef(fit))exp(confint(fit))
? ? ? ? ? ? ? coef ? se(coef) ?lower 0.95 ?upper 0.95 ? ? Chisq ? ? ? ? ?pAge ? ? 0.02675758 0.03138425 -0.03462915 ?0.09404934 0.7210455 0.39580118LumA ? -4.04312870 2.08208148 -8.91195072 -0.32580912 4.6216454 0.03157094LumB ? -3.74769116 1.96613514 -8.19454796 -0.16290619 4.2262682 0.03980286HER2 ? -3.71796180 2.45080140 -9.50654675 ?0.54467296 2.8798910 0.08969209Basal ? 1.12126338 1.83884710 -2.45335167 ?5.00553060 0.3763601 0.53955773Normal -1.51899257 2.05597939 -5.97976670 ?2.40100862 0.5806537 0.44605621
My question is: how come the p-value of Basal is so large? I was expecting a smaller p-value than LumA or LumB.
Some extra information:
> exp(coef(fit))? ? ? ?Age ? ? ? LumA ? ? ? LumB ? ? ? HER2 ? ? ?Basal ? ? Normal?1.02711878 0.01754250 0.02357211 0.02428341 3.06872873 0.21893233?
> exp(confint(fit))? ? ? ? ? Lower 95% ? Upper 95%Age ? ?9.659636e-01 ? 1.0986139LumA ? 1.347687e-04 ? 0.7219430LumB ? 2.761551e-04 ? 0.8496709HER2 ? 7.436339e-05 ? 1.7240445Basal ?8.600484e-02 149.2362476Normal 2.529416e-03 ?11.0343002
If I calculate odds ratio using an online calculator based on:? ? ? ? ? ? AR- ? ?AR+Basal ? ? ? 20 ? ?1Non-basal ? 4 ? ? 40
I got:
OR:200 95% CI: 20.9510 to 1909.2138 p < 0.0001
Can someone please help me explain?
Thank you very much!
Peter

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sun Jul 24 18:03:31 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 24 Jul 2016 09:03:31 -0700
Subject: [R] Firth logistic regression and odds ratio
In-Reply-To: <616531008.4808688.1469373611694.JavaMail.yahoo@mail.yahoo.com>
References: <616531008.4808688.1469373611694.JavaMail.yahoo.ref@mail.yahoo.com>
	<616531008.4808688.1469373611694.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <E4887074-D2C4-4AFE-82A3-43EE1B9357D1@comcast.net>


> On Jul 24, 2016, at 8:20 AM, Qinghua He via R-help <r-help at r-project.org> wrote:
> 
> Dear all,
> I have the following table for a breast cancer study and I was trying to calculate the odds ratio of different subtypes based on AR (AR+ as the base case).
>                   AR-    AR+Luminal A    1     19Luminal B    1     15Her2            0     4Basal-like    20    1Normal Like  2     2
> I did Firth logistic regression using the model: 
> fit <- logistf(AR ~ Age+LumA+LumB+HER2+Basal+Normal-1,data=df).summary(fit)exp(coef(fit))exp(confint(fit))


Edited from the HTML-garbled posting that appeared. Rhelp is a plain text mailing list and HTML ends up as a mess.

              coef   se(coef)  lower 0.95  upper 0.95     Chisq         p
Age     0.02675758 0.03138425 -0.03462915  0.09404934 0.7210455 0.39580118
LumA   -4.04312870 2.08208148 -8.91195072 -0.32580912 4.6216454 0.03157094
LumB   -3.74769116 1.96613514 -8.19454796 -0.16290619 4.2262682 0.03980286
HER2   -3.71796180 2.45080140 -9.50654675  0.54467296 2.8798910 0.08969209
Basal   1.12126338 1.83884710 -2.45335167  5.00553060 0.3763601 0.53955773
Normal -1.51899257 2.05597939 -5.97976670  2.40100862 0.5806537 0.44605621

> My question is: how come the p-value of Basal is so large? I was expecting a smaller p-value than LumA or LumB.

Your question is not on-topic for Rhelp., since it really has nothing to do with problems in R programming. You should be posing the question on a mailing list or a web forum where purely statistical questions are welcomed. I would expect that you would get a followup question asking you for more information, since there are many aspects of datasets that may affect p-values that you have not yet described. You for instance have not even described what "AR" might be. I'd suggest that you first read the Posting Guide and then post a question on http://stats.stackexchange.com with some more information about the study and better descriptive statistics about your population size and the covariates.


> Some extra information:
>> exp(coef(fit))

>>       Age       LumA       LumB       HER2      Basal     Normal

>> 1.02711878 0.01754250 0.02357211 0.02428341 3.06872873 0.21893233 

>> exp(confint(fit))

>>          Lower 95%   Upper 95%

>> Age    9.659636e-01   1.0986139
>> LumA   1.347687e-04   0.7219430
>> LumB   2.761551e-04   0.8496709
>> HER2   7.436339e-05   1.7240445
>> Basal  8.600484e-02 149.2362476
>> Normal 2.529416e-03  11.0343002

> If I calculate odds ratio using an online calculator based on:            AR-    AR+Basal       20    1Non-basal   4     40
> I got:
> OR:200 95% CI: 20.9510 to 1909.2138 p < 0.0001
> Can someone please help me explain?
> Thank you very much!
> Peter
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From cristina.cametti at gmail.com  Sun Jul 24 11:18:46 2016
From: cristina.cametti at gmail.com (Cristina Cametti)
Date: Sun, 24 Jul 2016 11:18:46 +0200
Subject: [R] country fixed effects model with binary dependent variable
In-Reply-To: <CA+8X3fVMPzzg+pNXV4Q3U1prDv8SRE9DnVa6UJksmSPoMvPe+Q@mail.gmail.com>
References: <450D2061-6663-4D8E-8490-ECF75F5189A1@yahoo.it>
	<CA+8X3fVMPzzg+pNXV4Q3U1prDv8SRE9DnVa6UJksmSPoMvPe+Q@mail.gmail.com>
Message-ID: <C898FABA-0B18-4A96-8DB7-BD594DED7D14@gmail.com>

Dear all,

thank you very much for all your answers. Probably, I don?t know how to write in this mailing list, so I apologize if I was not clear. 
My questions were all code related: I am not an R user, this means that I started using R just few months ago. So, I am seeking advise about the coding and that is why I was writing here. 
The part about not to mind how I wrote the variables meant only that the names may appear weird because I used the same in the ESS survey and in other cases, I created new names that do not make sense. So it was just a simple suggestions about my way to write the name of the variables, because the focus of my questions were others.
So, I am sorry if I upset you and basically, all wrote by Jim was correct about my questions. 
So thank you Jim, in the meantime I was working on the code and in the end I came up with the same code as you. I used ?glm? and added ?family=binomial?.  So now that I have the confirmation from an expert that this is a reliable code for my analysis, I am more sure about my results. 

Thank you all again for your attention. 
Kind Regards,

Cristina 


Il giorno 24/lug/2016, alle ore 06:50, Jim Lemon <drjimlemon at gmail.com> ha scritto:

> Hi Cristina,
> As Rolf has noted, you probably don't want to persist with "lm" since
> I think you have dichotomized your initial dependent variable. I also
> think that you meant "don't worry about the change of variable names"
> with "how I wrote the variables". I also think that you want to test
> interactions between the variables you are adding. _Maybe_ something
> like this:
> 
> modelfe1.2<-glm(aesfdrk_dummy ~ agea + gndr + eduyrs + domicil +
> partner + tvpol + hincfel_dum + ppltrst_GM:prison_pop +
> ppltrst_GM:foreign_pop_ + victim*prison_pop_ + victim*foreign_pop +
> mixed_neigh*prison_pop + mixed_neigh*foreign_pop +
> ethnic_neigh*prison_pop + ethnic_neigh*foreign_pop + factor(cntry)-1,
> data=mydata, family="binomial")
> 
> Jim
> 
> 
> On Sun, Jul 24, 2016 at 12:49 AM, Cristina Cametti
> <cristina.cametti at gmail.com> wrote:
>> Dear all,
>> 
>> I am having problems finding a reliable code for a country fixed effects model with binary dependent variable. I was able to run it for another part of my research, because in that case the dependent variable is continuous.
>> This is my code for the continuous dependent variable ?imwbcrm_rec?:
>> 
>> modelfe2 <- lm(imwbcrm_rec ~ tvpol + victim + agea + gndr + eduyrs + lrscale_GM + imgfrnd_dum +  qfimwht +factor(cntry)-1, data=mydata)
>> 
>> Please don?t mind to how I wrote the variables, they are from the first wave of the ESS survey. At this point, I have three questions:
>> - do you think this code is correct? Since the data are all from the same year (2002), I did not used the ppm package since it is only for panel data. The results of the previous code make sense, so I am satisfied. However, I want to be sure that I am running the right code.
>> - second questions: someone knows the code for the same analysis, but having a BINARY dependent variable (aesfdrk_dummy)? I found very different information on the internet, and unfortunately, I do not know how to use STATA, so I need to find a reliable code in r. This is the code that I have now:
>> 
>> modelfe1 <-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea+ gndr + eduyrs + domicil + partner + tvpol + hincfel_dum + factor(cntry) -1, data=mydata)
>> 
>> -last question: I have to add some interaction between country level variables and individual level variables. So, do you think that this code is right?
>> 
>> mydata$ppltrst_GMXprison_pop <- mydata$ppltrst_GM*mydata$prison_pop
>> mydata$ppltrst_GMXforeign_pop <- mydata$ppltrst_GM*mydata$foreign_pop
>> mydata$victimXprison_pop<- mydata$victim*mydata$prison_pop
>> mydata$victimXforeign_pop<- mydata$victim*mydata$foreign_pop
>> mydata$mixed_neighXprison_pop<- mydata$mixed_neigh*mydata$prison_pop
>> mydata$mixed_neighXforeign_pop <- mydata$mixed_neigh*mydata$foreign_pop
>> mydata$ethnic_neighXprison_pop <- mydata$ethnic_neigh*mydata$prison_pop
>> mydata$ethnic_neighXforeign_pop <- mydata$ethnic_neigh*mydata$foreign_pop
>> 
>> modelfe1.2<-lm(aesfdrk_dummy ~mixed_neigh + ethnic_neigh + agea + gndr + eduyrs + domicil + partner + tvpol + hincfel_dum + victim + ppltrst + ppltrstXprison_pop + ppltrst_Xforeign_pop_ + victimXprison_pop_ + victimXforeign_pop + mixed_neighXprison_pop + mixed_neighXforeign_pop + ethnic_neighXprison_pop + ethnic_neighXforeign_pop + factor(cntry)-1, data=mydata)
>> 
>> 
>> Thank you very much for your attention.
>> Kind Regards,
>> 
>> Cristina
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From paola_fortini at yahoo.it  Sun Jul 24 13:31:18 2016
From: paola_fortini at yahoo.it (paola_fortini at yahoo.it)
Date: Sun, 24 Jul 2016 11:31:18 +0000 (UTC)
Subject: [R] Changing legend to fill colour in ggplot
References: <149988375.6107893.1469359878999.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <149988375.6107893.1469359878999.JavaMail.yahoo@mail.yahoo.com>



Inviato da Yahoo Mail su Android
	[[alternative HTML version deleted]]


From paola_fortini at yahoo.it  Sun Jul 24 13:32:16 2016
From: paola_fortini at yahoo.it (paola_fortini at yahoo.it)
Date: Sun, 24 Jul 2016 11:32:16 +0000 (UTC)
Subject: [R] Changing legend to fill colour in ggplot
References: <1128350898.6645049.1469359936793.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1128350898.6645049.1469359936793.JavaMail.yahoo@mail.yahoo.com>



Inviato da Yahoo Mail su Android
	[[alternative HTML version deleted]]


From patrick.schratz at gmail.com  Sun Jul 24 15:10:37 2016
From: patrick.schratz at gmail.com (Patrick Johann Schratz)
Date: Sun, 24 Jul 2016 15:10:37 +0200
Subject: [R] glmmPQL crashes on inclusion of corSpatial object
Message-ID: <32c79e9b-4efb-74eb-e97a-c09aec1dc32d@gmail.com>

Link to data: <https://www.dropbox.com/s/yi3vf0bmqvydr8h/data.Rd?dl=0>  
(1170 obs, 9 variables, .Rd file) [plain link in case sth goes wrong 
with the hyperlink: https://www.dropbox.com/s/yi3vf0bmqvydr8h/data.Rd?dl=0]

Simply read it in using `readRDS(file)`.

I?m trying to setup a GLMM using the `glmmPQL` function from the `MASS` 
package including a random effects part and accounting for spatial 
autocorrelation. However, R (Version: 3.3.1) crashes upon execution.

     library(nlme)

     # setup model formula
     fo <- hail ~ prec_nov_apr + t_min_nov_apr + srad_nov_apr + age

     # setup corSpatial object
     correl = corSpatial(value = c(10000, 0.1), form = ~ry + rx, nugget 
= TRUE,
                         fixed = FALSE, type = "exponential")
     correl = Initialize(correl, data = d)

     # fit model
     fit5 <- MASS::glmmPQL(fo, random = ~1 | date, data = d,
                     correl = correl, family = binomial)

What I tried so far:

- reduce number of observation
- play with `corSpatial` parameters (range and nugget)
- reduce number of fixed predictors
- execute code on Windows, Linux (Debian) and Mac R installations


While I get no error message on my local pc (RStudio just crashes), 
running the script on a server returns the following error message:

`R: malloc.c:3540: _int_malloc: Assertion (fwd->size & 0x4) == 0' 
failed. Aborted`

Debugging leads me to a "glibc" c++ library problem. I also run valgrind 
on it. If you need the output, just ask!

Help ist highly appreciated!
Cheers, Patrick

	[[alternative HTML version deleted]]


From fsanchez at euclides.com  Sun Jul 24 19:30:11 2016
From: fsanchez at euclides.com (=?iso-8859-1?Q?Fernando_S=E1nchez_Lasheras?=)
Date: Sun, 24 Jul 2016 19:30:11 +0200
Subject: [R] Quade test
Message-ID: <00aa01d1e5d1$081451d0$183cf570$@euclides.com>

Hello to all the list members. I would like to perform a Quade test with
more than one covariate. I know the command quade.test and I have seen the
example below:

## Conover (1999, p. 375f):
## Numbers of five brands of a new hand lotion sold in seven stores
## during one week.
y <- matrix(c( 5,  4,  7, 10, 12,
           1,  3,  1,  0,  2,
          16, 12, 22, 22, 35,
           5,  4,  3,  5,  4,
          10,  9,  7, 13, 10,
          19, 18, 28, 37, 58,
          10,  7,  6,  8,  7),
        nrow = 7, byrow = TRUE,
        dimnames =
        list(Store = as.character(1:7),
             Brand = LETTERS[1:5]))
y
quade.test(y)

My question is as follows: how could I introduce more than one covariate? In
this example the covariate is the Store variable. But what if I also have
another covariate i.e.: the the number of inhabitants of the town in which
each store is located.

Thanks in advanced for your help!

Fernando


From Marios.BARLAS at cea.fr  Sun Jul 24 23:11:45 2016
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Sun, 24 Jul 2016 21:11:45 +0000
Subject: [R] Create a colour scale for different data sets
Message-ID: <01BFC0B2B4ABFC4CB432008F852D7661793C6C@EXDAG0-A1.intra.cea.fr>

Hello Every1, 

I'm working on analysing some data and I want to make some cartography maps. 

Since I treat data in batch, I need to have a common reference colour scale for all the datasets I need to plot. This is important otherwise it becomes hard to quickly assert visually changes from one experiment to another if the colour scale resets all the time. 

So I though of finding the min and max value of my data values and create a colour scale versus that. 

Nonetheless the code does not work as intended, 
I realised my code was not easily reproducible so I send some with initialized values to help :)

Here is a part of the code (reproducible):

meas.data <- c( 2.38762e+02, 2.54709e+02, 2.45204e+02, 2.32134e+02, 2.28587e+02, 2.31493e+02, 2.34867e+02, 2.41127e+02, 2.76113e+02, 2.57450e+02, 2.23804e+02, 2.39064e+02, 2.28636e+02, 2.37417e+02, 2.53686e+02, 2.45593e+02, 2.29043e+02, 9.25698e+10, 2.31001e+02, 2.36022e+02, 2.67654e+02, 2.50275e+02, 2.72641e+02, 2.24342e+02, 9.36361e+10, 2.28610e+02, 2.20854e+02, 2.37955e+02, 2.58944e+02, 2.68088e+02, 2.36356e+02, 2.52802e+02, 2.32976e+02, 2.39819e+02, 2.48820e+02, 2.80206e+02, 2.63318e+02, 2.43431e+02, 2.83426e+02, 2.48557e+02, 2.45493e+02, 2.53264e+02, 2.51834e+02, 2.34187e+02, 2.56326e+02, 2.96419e+02, 2.52473e+02, 2.68144e+02, 2.40842e+02)

die.codes <- c("X5_Y4"  , "X5_Y6" ,  "X5_Y8" ,  "X5_Y9", "X5_Y10", "X5_Y11", "X5_Y12", "X7_Y4", "X7_Y6", "X7_Y8", "X7_Y9", "X7_Y10", "X7_Y11", "X7_Y12", "X9_Y4", "X9_Y6", "X9_Y8", "X9_Y9", "X9_Y10", "X9_Y11", "X9_Y12", "X11_Y4", "X11_Y6", "X11_Y8", "X11_Y9", "X11_Y10", "X11_Y11", "X11_Y12", "X13_Y4", "X13_Y6", "X13_Y8", "X13_Y9",  "X13_Y10", "X13_Y11", "X13_Y12", "X15_Y4",  "X15_Y6",  "X15_Y8",  "X15_Y9",  "X15_Y10", "X15_Y11", "X15_Y12", "X17_Y4",  "X17_Y6", "X17_Y8" ,"X17_Y9",  "X17_Y10", "X17_Y11", "X17_Y12")
dies.x.total <- 17
dies.y.total <- 15
r.max <- 1e13



    dies.in.wafer <- expand.grid(X=as.numeric(1:dies.x.total), Y=as.numeric(1:dies.y.total))
      # This procedure can be used to generate the patterns for the test labels as well. To be Tested and verified.
      # Catch all numerical values in the string
      # 4 Is the column for which the names of the die coordinates are stored
      dies.tested <-strsplit(die.codes, "[^[:digit:]]")
      # Convert to Numeric
      dies.tested<- lapply(dies.tested,as.numeric )

      dies.tested<- data.frame((matrix(unlist(dies.tested), ncol = dim.data.frame(dies.tested[[1]])[2], byrow = TRUE)))
      # Drop all Columns containing NAs
      dies.tested <- dies.tested[colSums(!is.na(dies.tested)) > 0]
      dies.tested$test.order <- as.factor(1:dim.data.frame(dies.tested)[1])
      colnames(dies.tested)  <- c("X","Y","Order")

      dies.in.wafer.x <- dies.in.wafer$X
      dies.in.wafer.y <- dies.in.wafer$Y
      dies.in.wafer.tested <- rep(F, length = dim.data.frame(dies.in.wafer)[1])
      dies.in.wafer.Order = rep(NA, length = dim.data.frame(dies.in.wafer)[1])
      dies.in.wafer.r     = rep(NA, length = dim.data.frame(dies.in.wafer)[1])
      dies.in.wafer.op    = rep(NA, length = dim.data.frame(dies.in.wafer)[1])

      dies.tested.x <- dies.tested$X
      dies.tested.y <- dies.tested$Y
      dies.tested.order<- dies.tested$Order

      dim.dies.in.wafer <- dim(dies.in.wafer)[1]
      for(ixx in seq(1, dim(dies.tested)[1], by=1) ){
        for (inn in seq(1, dim.dies.in.wafer, by=1) ){
          if (dies.in.wafer.tested[inn]==F) {
            dies.in.wafer.tested[inn] <- dies.tested.x[ixx] == dies.in.wafer.x[inn] & dies.tested.y[ixx] == dies.in.wafer.y[inn]
            if (dies.in.wafer.tested[inn]==T) {
              dies.in.wafer.Order[inn]  <- dies.tested$Order[ixx]
              dies.in.wafer.r[inn]      <- meas.data
              dies.in.wafer.op[inn]     <- op.type 
            }
          }
        }
      }
      dies.in.wafer$tested  <- dies.in.wafer.tested
      dies.in.wafer$Order   <- dies.in.wafer.Order
      dies.in.wafer$op.type <- as.factor(dies.in.wafer.op)
      dies.in.wafer$R       <- dies.in.wafer.r


      r.colour.map <- c("lightgreen", "darkgreen", "yellow","red" )
      r.range.breaks <- c(0, 1.5e4, 1e5, r.max )/r.max  
      r.breaks.guide <- c(0, 1.5e4, 1e5, r.max ) 
      g.map <- ggplot(dies.in.wafer, aes(X, Y)) +
        geom_raster(aes(fill = R))+
        scale_fill_gradientn(name="R", na.value = "grey50", colours = r.colour.map, values = r.range.breaks, breaks=r.breaks.guide, limits=c(0,r.max) )
      g.map

From Marios.BARLAS at cea.fr  Mon Jul 25 01:29:17 2016
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Sun, 24 Jul 2016 23:29:17 +0000
Subject: [R] Create a colour scale for different data sets
In-Reply-To: <CA+8X3fVcwFPzUQbqYn5FDtQWyNAy0MYFW7MBjf6A=N=OoL=buA@mail.gmail.com>
References: <01BFC0B2B4ABFC4CB432008F852D7661793C6C@EXDAG0-A1.intra.cea.fr>,
	<CA+8X3fVcwFPzUQbqYn5FDtQWyNAy0MYFW7MBjf6A=N=OoL=buA@mail.gmail.com>
Message-ID: <01BFC0B2B4ABFC4CB432008F852D7661793C9E@EXDAG0-A1.intra.cea.fr>

Dear Jim,

Thanks for the advice! 

Turns out there was nothing wrong with my graphing code. I was just stupid when modifying the wafer map reconstruction code. If in the previous code you change this line : 

               dies.in.wafer.r[inn]      <- meas.data

to
              dies.in.wafer.r[inn]      <- meas.data[ixx]

It works perfectly. 

In case any1 else needs cartography mapping I will generalize this function, so feel free to ask for the code. 

Best,
Mario
________________________________________
From: Jim Lemon [drjimlemon at gmail.com]
Sent: Monday, July 25, 2016 12:17 AM
To: BARLAS Marios 247554
Subject: Re: [R] Create a colour scale for different data sets

Hi Marios,
One way to get a common color scale for a number of different sets of
values is the color.scale (plotrix) function. By setting the "xrange"
argument to the range of all values, the color for a particular value
will be the same in all subsets calculated. See the second example in
the "barp" help page. This might be what you want, where "x" is a
vector of your values:

color.scale(x,c(0.565,0.933,0.565),c(0,1,0), c(1,1,0),c(1,0,0),
 xrange=c(0,1e13))

Jim


On Mon, Jul 25, 2016 at 7:11 AM, BARLAS Marios 247554
<Marios.BARLAS at cea.fr> wrote:
> Hello Every1,
>
> I'm working on analysing some data and I want to make some cartography maps.
>
> Since I treat data in batch, I need to have a common reference colour scale for all the datasets I need to plot. This is important otherwise it becomes hard to quickly assert visually changes from one experiment to another if the colour scale resets all the time.
>
> So I though of finding the min and max value of my data values and create a colour scale versus that.
>
> Nonetheless the code does not work as intended,
> I realised my code was not easily reproducible so I send some with initialized values to help :)
>
> Here is a part of the code (reproducible):
>
> meas.data <- c( 2.38762e+02, 2.54709e+02, 2.45204e+02, 2.32134e+02, 2.28587e+02, 2.31493e+02, 2.34867e+02, 2.41127e+02, 2.76113e+02, 2.57450e+02, 2.23804e+02, 2.39064e+02, 2.28636e+02, 2.37417e+02, 2.53686e+02, 2.45593e+02, 2.29043e+02, 9.25698e+10, 2.31001e+02, 2.36022e+02, 2.67654e+02, 2.50275e+02, 2.72641e+02, 2.24342e+02, 9.36361e+10, 2.28610e+02, 2.20854e+02, 2.37955e+02, 2.58944e+02, 2.68088e+02, 2.36356e+02, 2.52802e+02, 2.32976e+02, 2.39819e+02, 2.48820e+02, 2.80206e+02, 2.63318e+02, 2.43431e+02, 2.83426e+02, 2.48557e+02, 2.45493e+02, 2.53264e+02, 2.51834e+02, 2.34187e+02, 2.56326e+02, 2.96419e+02, 2.52473e+02, 2.68144e+02, 2.40842e+02)
>
> die.codes <- c("X5_Y4"  , "X5_Y6" ,  "X5_Y8" ,  "X5_Y9", "X5_Y10", "X5_Y11", "X5_Y12", "X7_Y4", "X7_Y6", "X7_Y8", "X7_Y9", "X7_Y10", "X7_Y11", "X7_Y12", "X9_Y4", "X9_Y6", "X9_Y8", "X9_Y9", "X9_Y10", "X9_Y11", "X9_Y12", "X11_Y4", "X11_Y6", "X11_Y8", "X11_Y9", "X11_Y10", "X11_Y11", "X11_Y12", "X13_Y4", "X13_Y6", "X13_Y8", "X13_Y9",  "X13_Y10", "X13_Y11", "X13_Y12", "X15_Y4",  "X15_Y6",  "X15_Y8",  "X15_Y9",  "X15_Y10", "X15_Y11", "X15_Y12", "X17_Y4",  "X17_Y6", "X17_Y8" ,"X17_Y9",  "X17_Y10", "X17_Y11", "X17_Y12")
> dies.x.total <- 17
> dies.y.total <- 15
> r.max <- 1e13
>
>
>
>     dies.in.wafer <- expand.grid(X=as.numeric(1:dies.x.total), Y=as.numeric(1:dies.y.total))
>       # This procedure can be used to generate the patterns for the test labels as well. To be Tested and verified.
>       # Catch all numerical values in the string
>       # 4 Is the column for which the names of the die coordinates are stored
>       dies.tested <-strsplit(die.codes, "[^[:digit:]]")
>       # Convert to Numeric
>       dies.tested<- lapply(dies.tested,as.numeric )
>
>       dies.tested<- data.frame((matrix(unlist(dies.tested), ncol = dim.data.frame(dies.tested[[1]])[2], byrow = TRUE)))
>       # Drop all Columns containing NAs
>       dies.tested <- dies.tested[colSums(!is.na(dies.tested)) > 0]
>       dies.tested$test.order <- as.factor(1:dim.data.frame(dies.tested)[1])
>       colnames(dies.tested)  <- c("X","Y","Order")
>
>       dies.in.wafer.x <- dies.in.wafer$X
>       dies.in.wafer.y <- dies.in.wafer$Y
>       dies.in.wafer.tested <- rep(F, length = dim.data.frame(dies.in.wafer)[1])
>       dies.in.wafer.Order = rep(NA, length = dim.data.frame(dies.in.wafer)[1])
>       dies.in.wafer.r     = rep(NA, length = dim.data.frame(dies.in.wafer)[1])
>       dies.in.wafer.op    = rep(NA, length = dim.data.frame(dies.in.wafer)[1])
>
>       dies.tested.x <- dies.tested$X
>       dies.tested.y <- dies.tested$Y
>       dies.tested.order<- dies.tested$Order
>
>       dim.dies.in.wafer <- dim(dies.in.wafer)[1]
>       for(ixx in seq(1, dim(dies.tested)[1], by=1) ){
>         for (inn in seq(1, dim.dies.in.wafer, by=1) ){
>           if (dies.in.wafer.tested[inn]==F) {
>             dies.in.wafer.tested[inn] <- dies.tested.x[ixx] == dies.in.wafer.x[inn] & dies.tested.y[ixx] == dies.in.wafer.y[inn]
>             if (dies.in.wafer.tested[inn]==T) {
>               dies.in.wafer.Order[inn]  <- dies.tested$Order[ixx]
>               dies.in.wafer.r[inn]      <- meas.data
>               dies.in.wafer.op[inn]     <- op.type
>             }
>           }
>         }
>       }
>       dies.in.wafer$tested  <- dies.in.wafer.tested
>       dies.in.wafer$Order   <- dies.in.wafer.Order
>       dies.in.wafer$op.type <- as.factor(dies.in.wafer.op)
>       dies.in.wafer$R       <- dies.in.wafer.r
>
>
>       r.colour.map <- c("lightgreen", "darkgreen", "yellow","red" )
>       r.range.breaks <- c(0, 1.5e4, 1e5, r.max )/r.max
>       r.breaks.guide <- c(0, 1.5e4, 1e5, r.max )
>       g.map <- ggplot(dies.in.wafer, aes(X, Y)) +
>         geom_raster(aes(fill = R))+
>         scale_fill_gradientn(name="R", na.value = "grey50", colours = r.colour.map, values = r.range.breaks, breaks=r.breaks.guide, limits=c(0,r.max) )
>       g.map
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Alexander.Herr at csiro.au  Mon Jul 25 03:30:10 2016
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Mon, 25 Jul 2016 01:30:10 +0000
Subject: [R] adding new values to dataframe based on existing values in
 two columns
Message-ID: <533967e327604fcf9c57f24c50cf1c1a@exch1-mel.nexus.csiro.au>


Here is another solution using interaction, aggregate and merge:
interaction(xyz[,1],xyz[,2])->nf          #create a unique x.y grouping
  cbind(xyz[,1:2],nf)
    aggregate(xyz[,3],list(nf),min)->nmins
   names(nmins)<-c('nf','mins.1')
  xyz$nf<-nf
 merge(xyz,nmins, by='nf', all=TRUE)->nd
head(nd)

any other improvements?



XXXXXXXXXXXXXXX Herry wrote XXXXXXXXXXXXXXXXXX
Hiya,

I am trying to assign minimum values to a dataframe based on existing columns.  I can do this via loops, but surely R has much more elegant solutions...

Here is my code:
set.seed(666)
xyz<-as.data.frame(cbind(x=rep(rpois(50,10),2)+1, y=rep(rpois(50,10),2)+1,z=runif(100, min=-3, max=40)))
xyz[order(xyz[,1], xyz[,2]),]->xyz
 unique(xyz[,1:2])
dim(xyz)
   aggregate(xyz[,3],by=list(x=xyz[,1],y=xyz[,2]), min)->mins
   
 xyz$mins<-rep(NA, nrow(xyz))

#now assign min values to each xy combination
for(i in unique(xyz[,1])) {
       mins[mins[,1]==i,]->mm
        for( j in unique(mm[,2])) {
            mins[mins[,1]==i & mins[,2] == j,3]->xyz[xyz[,1]==i & xyz[,2]==j,4]
        }
}

Thanks and cheers
Herry


From maechler at stat.math.ethz.ch  Mon Jul 25 08:57:17 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 25 Jul 2016 08:57:17 +0200
Subject: [R] readline issue with 3.3.1
In-Reply-To: <20160722101536.26f5b5fe@delli.home.local>
References: <20160720113531.2bacaa0a@delli.home.local>
	<20160720163753.039cbccb@delli.home.local>
	<22416.62287.429899.107013@stat.math.ethz.ch>
	<20160722101536.26f5b5fe@delli.home.local>
Message-ID: <22421.47181.153477.351807@stat.math.ethz.ch>

>>>>> Ralf Goertz <r_goertz at web.de>
>>>>>     on Fri, 22 Jul 2016 10:15:36 +0200 writes:

    > Am Thu, 21 Jul 2016 18:07:43 +0200 schrieb Martin Maechler
    > <maechler at stat.math.ethz.ch>:

    >> Ralf Goertz <r_goertz at web.de> on Wed, 20 Jul 2016
    >> 16:37:53 +0200 writes:
 
    >>> I installed readline version 6.3 and the issue is
    >>> gone. So probably some of the recent changes in R's
    >>> readline code are incompatible with version readline
    >>> version 6.2.
    >> 
    >> Yes, it seems so, unfortunately.
    >> 
    >> Thank you for reporting !

    > It would be great if ? while fixing this ? you also took
    > care of the SIGWINCH problem described in bug report
    > https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=16604

Well, that *has* been fixed in 'R-devel' and 'R 3.3.1 patched'
..  but again only for readline >= 6.3

(The release process of 3.3.1 was already too much advanced, and
 such source changes are delicate)

It is quite a bit of work to get correct code for the different
versions of readline, also dealing with the fact that e.g. on
Mac OSX the code needs to work with the libedit/editlib
"substitute".

One reason we have not closed the bug is that the fix is only
for readline >= 6.3, ... and also that I don't think we got much
of user confirmation of the form
   " yes, the bug goes away once I compile R-devel or R-patched
   "

    > Thanks, Ralf

You are welcome, Martin

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From r_goertz at web.de  Mon Jul 25 10:35:11 2016
From: r_goertz at web.de (Ralf Goertz)
Date: Mon, 25 Jul 2016 10:35:11 +0200
Subject: [R] readline issue with 3.3.1
In-Reply-To: <22421.47181.153477.351807@stat.math.ethz.ch>
References: <20160720113531.2bacaa0a@delli.home.local>
	<20160720163753.039cbccb@delli.home.local>
	<22416.62287.429899.107013@stat.math.ethz.ch>
	<20160722101536.26f5b5fe@delli.home.local>
	<22421.47181.153477.351807@stat.math.ethz.ch>
Message-ID: <20160725103511.68948ecd@assi.home.local>

Am Mon, 25 Jul 2016 08:57:17 +0200
schrieb Martin Maechler <maechler at stat.math.ethz.ch>:

>> Ralf Goertz <r_goertz at web.de> on Fri, 22 Jul 2016 10:15:36 +0200
>> writes:  
> 
>> It would be great if ? while fixing this ? you also took care of the
>> SIGWINCH problem described in bug report
>> https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=16604  

> Well, that *has* been fixed in 'R-devel' and 'R 3.3.1 patched' ... but
> again only for readline >= 6.3

[snip]

> One reason we have not closed the bug is that the fix is only for
> readline >= 6.3, ... and also that I don't think we got much of user
> confirmation of the form " yes, the bug goes away once I compile
> R-devel or R-patched "

Thanks for the explanation. I will provide confirmation as soon as
R-patched hits my repositories. ;-)

Ralf


From loris.bennett at fu-berlin.de  Mon Jul 25 11:37:51 2016
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Mon, 25 Jul 2016 11:37:51 +0200
Subject: [R] Interpreting error message probably caused by directlabels
Message-ID: <87mvl65a8g.fsf@hornfels.zedat.fu-berlin.de>

Dear all,

Possibly as a result of an update to R 3.3.0, a script which I had been
running regularly via cron, the following line

  p_named <- p_anon + geom_dl(aes(label=user,colour=user),
                              list("top.points",
                              cex = 0.5, rot = -45, hjust = 1))
now produces the error:

  Error in ggplot2::layer(data = data, mapping = mapping, geom = GeomDl,  : 
    must specify method= argument

Given that I don't call ggplot2::layer directly and given that
?ggplot2::layer doesn't contain any reference to an argument 'method',
what is the interpretation of this error message?

Aside from understanding the error message I would also be interested in
a solution to the problem, which I assume is due to an incompatibility
between the new version of R and the  package 'directlabels'.

Cheers,

Loris

-- 
This signature is currently under construction.


From maechler at stat.math.ethz.ch  Mon Jul 25 11:46:25 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 25 Jul 2016 11:46:25 +0200
Subject: [R] Estimators for non-normal data
In-Reply-To: <CAMmS1Wh8_b1yPeUwoLp=BiKyxNyiVxtJJb=+X+L7joepy87t7Q@mail.gmail.com>
References: <CAMmS1Wh8_b1yPeUwoLp=BiKyxNyiVxtJJb=+X+L7joepy87t7Q@mail.gmail.com>
Message-ID: <22421.57329.286791.309542@stat.math.ethz.ch>

>>>>> Nika Su?ac <nika.susac at gmail.com>
>>>>>     on Sat, 23 Jul 2016 19:39:48 +0200 writes:

    > Hi!  I have non-normal data (items are continuous on a
    > 9-point scale, but not normally distributed) and I want to
    > conduct cfa. Which of the estimators available in lavaan
    > do you recommend me to use?  Thanks in advance!

I think you want *robust* statistical methods then.

Robust factor analysis (if 'cfa' means something like that) is
somewhat prominent topic.
Simple approaches will already be available by using
MASS::cov.rob() for a robust covariance matrix which you then
can pass to other methods.

For more (and more modern) methods and approaches,

- for R packages, I'd recommend you consult the CRAN task view
  about robust statistical methods,
  	https://cran.r-project.org/web/views/Robust.html
  notably the section on 'Multivariate Analysis'

- for more specific help and expertise, I strongly recommend
  the dedicated  R-SIG-robust mailing list (instead of R-help),
   --> https://stat.ethz.ch/mailman/listinfo/r-sig-robust

Best regards,
Martin Maechler, ETH Zurich

    > 	[[alternative HTML version deleted]]


From justinthong93 at gmail.com  Mon Jul 25 12:48:25 2016
From: justinthong93 at gmail.com (Justin Thong)
Date: Mon, 25 Jul 2016 11:48:25 +0100
Subject: [R] Soft Question: Where to find this reference.
Message-ID: <CAEtAGeq-aVS=jjMjBV-uFBEvwkTcxXoo4tS+20YM6h6Pn_6fYw@mail.gmail.com>

I notice a lot of r documentation refer to this reference below. I can't
seem to find it anywhere.
Does anyone have a link to point to where I can either view it or buy it?


*Chambers, J. M., Freeny, A and Heiberger, R. M. (1992) Analysis of
variance; designed experiments*

-- 
Yours sincerely,
Justin

*I check my email at 9AM and 4PM everyday*
*If you have an EMERGENCY, contact me at +447938674419(UK) or
+60125056192(Malaysia)*

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Jul 25 13:27:05 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 25 Jul 2016 07:27:05 -0400
Subject: [R] Soft Question: Where to find this reference.
In-Reply-To: <CAEtAGeq-aVS=jjMjBV-uFBEvwkTcxXoo4tS+20YM6h6Pn_6fYw@mail.gmail.com>
References: <CAEtAGeq-aVS=jjMjBV-uFBEvwkTcxXoo4tS+20YM6h6Pn_6fYw@mail.gmail.com>
Message-ID: <99e7016c-d22b-d78b-f074-3ef6f49ece0f@gmail.com>

On 25/07/2016 6:48 AM, Justin Thong wrote:
> I notice a lot of r documentation refer to this reference below. I can't
> seem to find it anywhere.
> Does anyone have a link to point to where I can either view it or buy it?
>
>
> *Chambers, J. M., Freeny, A and Heiberger, R. M. (1992) Analysis of
> variance; designed experiments*
>

The complete versions of that reference say where:  "Chapter 5 of 
\emph{Statistical Models in S}
   eds J. M. Chambers and T. J. Hastie, Wadsworth & Brooks/Cole."

Academic libraries are likely to have copies of that book.  You can also 
buy copies on Amazon.  Google Books can give details.  The ISBN is 
0534167640.

Duncan Murdoch


From milujisb at gmail.com  Mon Jul 25 15:02:33 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Mon, 25 Jul 2016 15:02:33 +0200
Subject: [R] Country names from latitude and longitude
Message-ID: <CAMLwc7NkgT969QEdbV6NLTdXQf9VCSakMsCHVknQ1LRXCjz5Zw@mail.gmail.com>

I have the following data at 0.5 degree by 0.5 degree.

temp <- dput(head(ptsDF,10))
structure(list(longitude = c(-68.25, -67.75, -67.25, -68.25,
-67.75, -67.25, -71.25, -70.75, -69.25, -68.75), latitude = c(-54.75,
-54.75, -54.75, -54.25, -54.25, -54.25, -53.75, -53.75, -53.75,
-53.75), GDP = c(1.683046, 0.3212307, 0.0486207, 0.1223268, 0.0171909,
0.0062104, 0.22379, 0.1406729, 0.0030038, 0.0057422)), .Names =
c("longitude",
"latitude", "GDP"), row.names = c(4L, 17L, 30L, 43L, 56L, 69L,
82L, 95L, 108L, 121L), class = "data.frame")

I would like add the corresponding country names to each of the
coordinates. This is what I have done:

library(data.table)
library(rgdal)
library(reshape2)
library(dplyr)
library(tidyr)
library(lubridate)
library(maps)
library(countrycode)
library(ggplot2)
library(raster)

coord_cells <-temp [,c(1,2)]
pts_cells <-
SpatialPoints(coord_cells,proj4string=CRS(proj4string(worldmap)))
indices_cells <- over(pts_cells, worldmap,na.rm=TRUE)
foo_cells<-indices_cells[,c(3,5)] # Keep ISO3 and country names only
new_data <- cbind(foo_cells, temp)

However, I get a large number of NAs for coordinates which should have
corresponding countries. What am I doing wrong? Any suggestions? Thank you.

Sincerely,

Milu

	[[alternative HTML version deleted]]


From 99cuatla at gmail.com  Mon Jul 25 13:06:21 2016
From: 99cuatla at gmail.com (Oscar Moreno)
Date: Mon, 25 Jul 2016 07:06:21 -0400
Subject: [R] Soft Question: Where to find this reference.
In-Reply-To: <CAEtAGeq-aVS=jjMjBV-uFBEvwkTcxXoo4tS+20YM6h6Pn_6fYw@mail.gmail.com>
References: <CAEtAGeq-aVS=jjMjBV-uFBEvwkTcxXoo4tS+20YM6h6Pn_6fYw@mail.gmail.com>
Message-ID: <D9C00038-D363-445B-93B0-A183FE0516A5@gmail.com>

See book: Statistical Models in S (ISBN-10: 041283040X; ISBN-13: 978-0412830402) Chapter 5, p. 145.	

? Analysis of Variance; Designed Experiments, J. M. Chambers, A. E. Freeny and R. M. Heiberger, Statistical Models in S, J. M. Chambers and T. J. Hastie (editors), Wadsworth & Brooks/Cole, Pacific Grove, California, 1992.

> On Jul 25, 2016, at 06:48, Justin Thong <justinthong93 at gmail.com> wrote:
> 
> I notice a lot of r documentation refer to this reference below. I can't
> seem to find it anywhere.
> Does anyone have a link to point to where I can either view it or buy it?
> 
> 
> *Chambers, J. M., Freeny, A and Heiberger, R. M. (1992) Analysis of
> variance; designed experiments*
> 
> -- 
> Yours sincerely,
> Justin
> 
> *I check my email at 9AM and 4PM everyday*
> *If you have an EMERGENCY, contact me at +447938674419(UK) or
> +60125056192(Malaysia)*
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From abdoulayesar at gmail.com  Mon Jul 25 15:06:38 2016
From: abdoulayesar at gmail.com (Abdoulaye SARR)
Date: Mon, 25 Jul 2016 13:06:38 +0000
Subject: [R] probelm with xlab ylab and xaxp barplot
In-Reply-To: <EC5F7F05-0C9F-4E5B-9205-DCA81200BD29@me.com>
References: <C7042838-33C6-466B-8978-217A2E2320DC@gmail.com>
	<779771D7-EBEF-47EA-BE2C-4FF822E5D2DA@me.com>
	<CAN=6O0J_pG_cFWoq3dKuZ0rV84HemMdipC4U5J1wA1uJ94QQ6w@mail.gmail.com>
	<EC5F7F05-0C9F-4E5B-9205-DCA81200BD29@me.com>
Message-ID: <C230B1E5-3155-4407-A2C9-4D11DB7E9416@gmail.com>


>> Hi Marc and Others,


I am still struggling to have my slab and ylab displayed on a bar plot. Marc did useful advise on playing with mar settings. I tried may combinations and can?t have it work.

I paste the code I am suing hoping guidance on solving this issue.




## extract works for all time steps
d1<-read.nc(gp)

d2<-read.nc(er)

d3<-read.nc(me)

d4<-read.nc(ne)

d5<-read.nc(ar)

d6<-read.nc(cc)

d7<-read.nc(mr)

d8<-read.nc(ic)

z1<-d1$spei
z2<-d2$spei
z3<-d3$spei
z4<-d4$spei
z5<-d5$spei
z6<-d6$spei
z7<-d7$spei
z8<-d8$spei
#par(oma=c(2,2,2,2))  # all sides have 3 lines of space  

par(mar=rep(2,4))
#par(mar=c(5.1, 4.1, 2.1, 2.1))
#par(mai=c(1.02,0.82,0.82,0.42))
op <- par(oma=c(1,2,3,5))
#op <- par(oma=c(6,5,0,0))
par(mfrow=c(4,2))

line = 3

barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei", xaxp=c(181,2005,1), col=ifelse(z1>0,"green","brown"))

mtext("a")
barplot(z2,xlab="Years", ylab="spei",  ylim=c(-2,2), col=ifelse(z2>0,"green","brown"))
mtext("b")
barplot(z3, ylim=c(-2,2), xlab="Years", ylab="spei", col=ifelse(z3>0,"green","brown"))
mtext("c")
barplot(z4, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z4>0,"green","brown"))
mtext("d")
barplot(z5, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z5>0,"green","brown"))
mtext("e")
barplot(z6, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z6>0,"green","brown"))
mtext("f")
barplot(z7,xlab="Years", ylab="spei",  ylim=c(-2,2), col=ifelse(z7>0,"green","brown"))
mtext("g")
barplot(z8,  ylim=c(-2,2), xlab="Years", ylab="spei", col=ifelse(z8>0,"green","brown"))
mtext("h")
par(op)

Another solution with ggplot2 or lattice also welcome.


Best regards,

asarr
>> 
>> 
>> On Wed, Jul 20, 2016 at 6:03 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
>> 
>> > On Jul 20, 2016, at 4:00 AM, Abdoulaye SARR <abdoulayesar at gmail.com> wrote:
>> >
>> > I have the color of my bar plot displayed correctly but don?t have xlab,  ylab  and xaxp don?t show up.
>> >
>> > here is example of yearly data (25 years 1981-2005)
>> >> head(z1)
>> > [1] -0.1001726  0.2014272 -0.8556950  0.1920669 -0.8013520  1.3324949
>> >
>> > code to display values
>> >
>> > par(mar=rep(2,4))
>> > op <- par(oma=c(5,7,1,1))
>> > par(mfrow=c(4,2))
>> >
>> > line = 3
>> >
>> > barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei", xaxp=c(181,2005,1), col=ifelse(z1>0,"green","brown ?))
>> >
>> > hoe help on this issue
>> >
>> > Fipou
>> 
>> 
>> Hi,
>> 
>> First, a general comment, which is that barplots are typically good for displaying counts and percentages, not continuous data points or perhaps estimates of means, etc. Your values for z1 above, suggest that you might be better off just plotting the points on the y axis against the years on the x axis. That is, for example:
>> 
>>   plot(1981:2005, z1, col = ifelse(z1 > 0, "green", "brown"),
>>        ylab = "spei", xlab = "Years", pch = 19)
>> 
>> presuming that z1 has 25 values.
>> 
>> That being said, some additional notes to hopefully guide you here with barplot():
>> 
>> 1. You appear to be wanting to plot a matrix of 8 plots in a 4 row by 2 column matrix. That is fine, but note that changing the graphic parameters associated with the spacing of margins, etc. in a matrix don't always provide a result similar to what you might find in a single plot. I would start by not adjusting par(mar) and par(oma) from their default values to get an idea of what the plot looks like with default settings and then modify from there so that you can see how any adjustments affect the result. You may be adjusting the margins for each plot and the outer margins of the overall matrix in a manner that conflicts.
>> 
>> 
>> 2. In the case of a vertical barplot, the bars are not centered around integer values on the x axis, as they would be in say a boxplot. In the help for barplot() you will note that the Value section indicates that barplot returns a vector (by default) of the bar midpoints, which can then be used for annotation on the relevant axis. There are examples of the use of this on the barplot help page. Your values for 'xaxp' (which presumably has a typo for 1981, as 181) will not be correct here. Thus:
>> 
>>   MP <- barplot(z1, ...)
>> 
>> where 'MP' will contain the individual bar midpoints and then you can use code like:
>> 
>>   axis(1, at = MP, labels = 1981:2005, ...)
>> 
>> to place annotations below each bar. See ?axis as well as ?mtext for additional information on plot annotations.
>> 
>> Another option is to use the names.arg argument in barplot, to provide the names for each bar:
>> 
>>   barplot(z1, names.arg = 1981:2005, ...)
>> 
>> You will also likely have to adjust the font sizes for text spacing, as the defaults may be too large for all labels to display given the large number of bars. The cex* family of graphic parameters can be helpful. See the arguments in ?barplot and in ?par for more information.
>> 
>> Regards,
>> 
>> Marc Schwartz
>> 
>> 
>> <sample_code_sar.R><sample_bar_sar.pdf>
> 


	[[alternative HTML version deleted]]


From micallefpierre at hotmail.com  Mon Jul 25 12:20:20 2016
From: micallefpierre at hotmail.com (Pierre Micallef)
Date: Mon, 25 Jul 2016 10:20:20 +0000
Subject: [R] adding a data point on a boxplot
Message-ID: <AM4PR02MB1331C5149E307C2F0203BC4DD30D0@AM4PR02MB1331.eurprd02.prod.outlook.com>

Hi


Please could somebody offer some assistance with the following problem?

I am trying to use the boxplot function. As standard it displays (min, lower Q, median, Upper Q and Max). For each dotplot I would also like to add the current/most recent value from my data series as a data point. The goal being to graphically show where current values are compared to the above standard statistics.

I have removed the date columns from the data but it should be in descending order i.e. the last data point in each series should be the current value.

Happy for someone to amend the whole code if there is a smarter way of doing it.

Thanks for any help you can give with this.

See code below:

blpConnect()


data<-data.frame(LargeValue=bdh("DJUSVL Index", c("PX_LAST"), start.date=Sys.Date()-252*5),
                 LargeGrowth=bdh("DJUSGL Index", c("PX_LAST"), start.date=Sys.Date()-252*5))
data <- data[,!names(data) %in% c("LargeValue.date","LargeGrowth.date")]
boxplot(data, las = 2, names = c("Large Value"," Large Growth"), col = c("red","royalblue2"),main=toupper("Box Plot Value vs Growth"), Xlab="Index Level", range = 0, horizontal=TRUE)



Thanks
Pierre

	[[alternative HTML version deleted]]


From rohini.ramar at citi.com  Mon Jul 25 14:18:18 2016
From: rohini.ramar at citi.com (Ramar, Rohini )
Date: Mon, 25 Jul 2016 12:18:18 +0000
Subject: [R] Windows 10 Application Compatibility Check | FreeWare R
 Statistical Environment v3.2.2
Message-ID: <F7CCE7BE9001FB439190567BE5A9E2FE8DCB5A86@EXTXMB54.nam.nsroot.net>

Hello Team,

We are, Citi Application Readiness Team, need your assistance in order to gather info about below application compatibility and support for Win 10 as part of Window 10 Readiness initiative. CITI Bank has been using below "FreeWare R Statistical Environment v3.2.2"  software products currently on Win 7 operating system.

We would like to know whether the below listed application is compatible and supported even for Win 10 (64 Bit) or is there any other higher version of application which would be compatible for Win10. If you have not tested for Win 10, could you please provide us with a tentative date by when we can reach you.

Application Name : FreeWare R Statistical Environment v3.2.2


Note: Kindly re-direct this email to appropriate team if we reached you wrongly.


Regards,
Rohini R
Citi Architecture & Technology Engineering
Client Computing
Direct Phone #:+91 22 3346 1497
Email ID: rohini.ramar at citi.com<mailto:rohini.ramar at citi.com>



	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Mon Jul 25 16:28:12 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Mon, 25 Jul 2016 09:28:12 -0500
Subject: [R] probelm with xlab ylab and xaxp barplot
In-Reply-To: <C230B1E5-3155-4407-A2C9-4D11DB7E9416@gmail.com>
References: <C7042838-33C6-466B-8978-217A2E2320DC@gmail.com>
	<779771D7-EBEF-47EA-BE2C-4FF822E5D2DA@me.com>
	<CAN=6O0J_pG_cFWoq3dKuZ0rV84HemMdipC4U5J1wA1uJ94QQ6w@mail.gmail.com>
	<EC5F7F05-0C9F-4E5B-9205-DCA81200BD29@me.com>
	<C230B1E5-3155-4407-A2C9-4D11DB7E9416@gmail.com>
Message-ID: <1E3D23B5-C854-4C80-B802-F6CC2980280A@me.com>

Hi,

If your code below is a verbatim copy and paste, you still have the following two lines active:

  par(mar=rep(2,4))

and

  op <- par(oma=c(1,2,3,5))

Comment out both of those lines and then see what the result looks like.

As I noted before, try the plot **without any modifications** to the default margin values. Then adjust from there, which may require you to increase, not decrease, the values from their defaults in order to have room for your text.

The values you have for par(mar) above, for example, reduce the values to 2 for each side from the default, which is:

  c(5, 4, 4, 2) + 0.1.

So that alone will likely result in there not being enough room for your axis labels.

You may also have to create the barplot without any default annotation created by the function itself and then add it with ?axis, ?text and ?mtext. You may also have to reduce the size of the font itself, which is done via the cex* arguments to barplot() and the additional annotation functions mentioned in the prior sentence.

Regards,

Marc


> On Jul 25, 2016, at 8:06 AM, Abdoulaye SARR <abdoulayesar at gmail.com> wrote:
> 
> 
>>> Hi Marc and Others,
> 
> 
> I am still struggling to have my slab and ylab displayed on a bar plot. Marc did useful advise on playing with mar settings. I tried may combinations and can?t have it work.
> 
> I paste the code I am suing hoping guidance on solving this issue.
> 
> 
> 
> 
> ## extract works for all time steps
> d1<-read.nc(gp)
> 
> d2<-read.nc(er)
> 
> d3<-read.nc(me)
> 
> d4<-read.nc(ne)
> 
> d5<-read.nc(ar)
> 
> d6<-read.nc(cc)
> 
> d7<-read.nc(mr)
> 
> d8<-read.nc(ic)
> 
> z1<-d1$spei
> z2<-d2$spei
> z3<-d3$spei
> z4<-d4$spei
> z5<-d5$spei
> z6<-d6$spei
> z7<-d7$spei
> z8<-d8$spei
> #par(oma=c(2,2,2,2))  # all sides have 3 lines of space  
> 
> par(mar=rep(2,4))
> #par(mar=c(5.1, 4.1, 2.1, 2.1))
> #par(mai=c(1.02,0.82,0.82,0.42))
> op <- par(oma=c(1,2,3,5))
> #op <- par(oma=c(6,5,0,0))
> par(mfrow=c(4,2))
> 
> line = 3
> 
> barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei", xaxp=c(181,2005,1), col=ifelse(z1>0,"green","brown"))
> 
> mtext("a")
> barplot(z2,xlab="Years", ylab="spei",  ylim=c(-2,2), col=ifelse(z2>0,"green","brown"))
> mtext("b")
> barplot(z3, ylim=c(-2,2), xlab="Years", ylab="spei", col=ifelse(z3>0,"green","brown"))
> mtext("c")
> barplot(z4, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z4>0,"green","brown"))
> mtext("d")
> barplot(z5, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z5>0,"green","brown"))
> mtext("e")
> barplot(z6, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z6>0,"green","brown"))
> mtext("f")
> barplot(z7,xlab="Years", ylab="spei",  ylim=c(-2,2), col=ifelse(z7>0,"green","brown"))
> mtext("g")
> barplot(z8,  ylim=c(-2,2), xlab="Years", ylab="spei", col=ifelse(z8>0,"green","brown"))
> mtext("h")
> par(op)
> 
> Another solution with ggplot2 or lattice also welcome.
> 
> 
> Best regards,
> 
> asarr

<snip>


From dwinsemius at comcast.net  Mon Jul 25 18:16:05 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 25 Jul 2016 09:16:05 -0700
Subject: [R] adding a data point on a boxplot
In-Reply-To: <AM4PR02MB1331C5149E307C2F0203BC4DD30D0@AM4PR02MB1331.eurprd02.prod.outlook.com>
References: <AM4PR02MB1331C5149E307C2F0203BC4DD30D0@AM4PR02MB1331.eurprd02.prod.outlook.com>
Message-ID: <882EAC23-2A10-4FAE-AC07-E44E31D33291@comcast.net>


> On Jul 25, 2016, at 3:20 AM, Pierre Micallef <micallefpierre at hotmail.com> wrote:
> 
> Hi
> 
> 
> Please could somebody offer some assistance with the following problem?
> 
> I am trying to use the boxplot function. As standard it displays (min, lower Q, median, Upper Q and Max). For each dotplot I would also like to add the current/most recent value from my data series as a data point. The goal being to graphically show where current values are compared to the above standard statistics.
> 
> I have removed the date columns from the data but it should be in descending order i.e. the last data point in each series should be the current value.
> 
> Happy for someone to amend the whole code if there is a smarter way of doing it.
> 
> Thanks for any help you can give with this.
> 
> See code below:
> 
> blpConnect()
> 
> 
> data<-data.frame(LargeValue=bdh("DJUSVL Index", c("PX_LAST"), start.date=Sys.Date()-252*5),
>                 LargeGrowth=bdh("DJUSGL Index", c("PX_LAST"), start.date=Sys.Date()-252*5))
> data <- data[,!names(data) %in% c("LargeValue.date","LargeGrowth.date")]
> boxplot(data, las = 2, names = c("Large Value"," Large Growth"), col = c("red","royalblue2"),main=toupper("Box Plot Value vs Growth"), Xlab="Index Level", range = 0, horizontal=TRUE)
> 
> 
This should be very easy. (I'm not reviewing your code since it is obviously not reproducible yet. Presumably you would want to use tail( vectname, 1) to get the last point.)  

To plot a large red "point" (actually a circle) at 0 in the first "column" of the first example on the ?boxplot page, you would issue:

points(1, 0, cex=5, col="red")

-- 

David.


> 
> Thanks
> Pierre
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From marc_schwartz at me.com  Mon Jul 25 20:03:02 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Mon, 25 Jul 2016 13:03:02 -0500
Subject: [R] probelm with xlab ylab and xaxp barplot
In-Reply-To: <868300E7-D001-4669-88DD-EC27CD98C87B@gmail.com>
References: <C7042838-33C6-466B-8978-217A2E2320DC@gmail.com>
	<779771D7-EBEF-47EA-BE2C-4FF822E5D2DA@me.com>
	<CAN=6O0J_pG_cFWoq3dKuZ0rV84HemMdipC4U5J1wA1uJ94QQ6w@mail.gmail.com>
	<EC5F7F05-0C9F-4E5B-9205-DCA81200BD29@me.com>
	<C230B1E5-3155-4407-A2C9-4D11DB7E9416@gmail.com>
	<1E3D23B5-C854-4C80-B802-F6CC2980280A@me.com>
	<868300E7-D001-4669-88DD-EC27CD98C87B@gmail.com>
Message-ID: <0D47020C-786A-4B80-8B4E-F946CE2000B3@me.com>

Hi,

First, I noted again that you still have:

   xaxp=c(181,2005,1)

in the first barplot() call. Get rid of that, as barplot() does not use normal axis ranges for the bar midpoints.

Second, I do not see an indication that you are using the 'names.arg' argument in barplot(), which supplies the vector of text to place below each bar. If this is correct, then you want the basic barplot() call to look something like:

  barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei", 
          names.arg = 1981:2005,
          col=ifelse(z1>0,"green","brown"))

where names.arg on the second line is the vector of years from 1981 to 2005. If the data passed to barplot() have name attributes (for example, they are the result of using table() on a vector), those would be used, but I am guessing that your z* vectors are just numeric vectors without labels.

If you then need to further adjust the axes to make more room below the plots, adjust the first element of par(mar) until the default axes show.

For example: 

  par(mar = c(6, 4, 4, 2))

where 6 replaces the default 5 for the first element. You may need to go higher if the text is still not showing. If need be, increase it further slowly to view the impact. You can also change by less than a full integer (e.g. 5.5, 6.25, etc.) as you may need. At some point, you will overshoot with too much room and you can then back down slowly.

Using this approach, each plot will have a similar size as you adjust the margins and then look similar visually, presuming that the axis ranges are the same for each.

Regards,

Marc


> On Jul 25, 2016, at 11:56 AM, Abdoulaye SARR <abdoulayesar at gmail.com> wrote:
> 
> Hi Marc,
> 
> According to your guidance the labels are almost at the right place when adjusting mar values.
> 
> A remaining need is to have the x axis at least for the two bottom figures as date from 1981 to 2005. Do you think this is doable. Ylim is fine but how tots in this case xlim.
> 
> Best regards,
> 
> asarr
> Le 25 juil. 2016 ? 14:28, Marc Schwartz <marc_schwartz at me.com> a ?crit :
> 
>> Hi,
>> 
>> If your code below is a verbatim copy and paste, you still have the following two lines active:
>> 
>>  par(mar=rep(2,4))
>> 
>> and
>> 
>>  op <- par(oma=c(1,2,3,5))
>> 
>> Comment out both of those lines and then see what the result looks like.
>> 
>> As I noted before, try the plot **without any modifications** to the default margin values. Then adjust from there, which may require you to increase, not decrease, the values from their defaults in order to have room for your text.
>> 
>> The values you have for par(mar) above, for example, reduce the values to 2 for each side from the default, which is:
>> 
>>  c(5, 4, 4, 2) + 0.1.
>> 
>> So that alone will likely result in there not being enough room for your axis labels.
>> 
>> You may also have to create the barplot without any default annotation created by the function itself and then add it with ?axis, ?text and ?mtext. You may also have to reduce the size of the font itself, which is done via the cex* arguments to barplot() and the additional annotation functions mentioned in the prior sentence.
>> 
>> Regards,
>> 
>> Marc
>> 
>> 
>>> On Jul 25, 2016, at 8:06 AM, Abdoulaye SARR <abdoulayesar at gmail.com> wrote:
>>> 
>>> 
>>>>> Hi Marc and Others,
>>> 
>>> 
>>> I am still struggling to have my slab and ylab displayed on a bar plot. Marc did useful advise on playing with mar settings. I tried may combinations and can?t have it work.
>>> 
>>> I paste the code I am suing hoping guidance on solving this issue.
>>> 
>>> 
>>> 
>>> 
>>> ## extract works for all time steps
>>> d1<-read.nc(gp)
>>> 
>>> d2<-read.nc(er)
>>> 
>>> d3<-read.nc(me)
>>> 
>>> d4<-read.nc(ne)
>>> 
>>> d5<-read.nc(ar)
>>> 
>>> d6<-read.nc(cc)
>>> 
>>> d7<-read.nc(mr)
>>> 
>>> d8<-read.nc(ic)
>>> 
>>> z1<-d1$spei
>>> z2<-d2$spei
>>> z3<-d3$spei
>>> z4<-d4$spei
>>> z5<-d5$spei
>>> z6<-d6$spei
>>> z7<-d7$spei
>>> z8<-d8$spei
>>> #par(oma=c(2,2,2,2))  # all sides have 3 lines of space  
>>> 
>>> par(mar=rep(2,4))
>>> #par(mar=c(5.1, 4.1, 2.1, 2.1))
>>> #par(mai=c(1.02,0.82,0.82,0.42))
>>> op <- par(oma=c(1,2,3,5))
>>> #op <- par(oma=c(6,5,0,0))
>>> par(mfrow=c(4,2))
>>> 
>>> line = 3
>>> 
>>> barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei", xaxp=c(181,2005,1), col=ifelse(z1>0,"green","brown"))
>>> 
>>> mtext("a")
>>> barplot(z2,xlab="Years", ylab="spei",  ylim=c(-2,2), col=ifelse(z2>0,"green","brown"))
>>> mtext("b")
>>> barplot(z3, ylim=c(-2,2), xlab="Years", ylab="spei", col=ifelse(z3>0,"green","brown"))
>>> mtext("c")
>>> barplot(z4, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z4>0,"green","brown"))
>>> mtext("d")
>>> barplot(z5, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z5>0,"green","brown"))
>>> mtext("e")
>>> barplot(z6, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z6>0,"green","brown"))
>>> mtext("f")
>>> barplot(z7,xlab="Years", ylab="spei",  ylim=c(-2,2), col=ifelse(z7>0,"green","brown"))
>>> mtext("g")
>>> barplot(z8,  ylim=c(-2,2), xlab="Years", ylab="spei", col=ifelse(z8>0,"green","brown"))
>>> mtext("h")
>>> par(op)
>>> 
>>> Another solution with ggplot2 or lattice also welcome.
>>> 
>>> 
>>> Best regards,
>>> 
>>> asarr
>> 
>> <snip>
> 


From bogaso.christofer at gmail.com  Mon Jul 25 20:36:09 2016
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Tue, 26 Jul 2016 00:06:09 +0530
Subject: [R] Please assist me to download this data
Message-ID: <CA+dpOJmNRi_uYXh-nop9UULzkVHzabBpzzri2v8QaTTh_ZaBgA@mail.gmail.com>

Hi again,

I am trying to find some way to download all data historically from
this website "https://www.amfiindia.com/net-asset-value/nav-history".

Basically what I am trying to do is, I shall have a range of dates and
for each date I need to download entire dataset programmatically.

Really appreciate if experts here help.

Thanks,


From ulrik.stervbo at gmail.com  Mon Jul 25 21:23:11 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Mon, 25 Jul 2016 19:23:11 +0000
Subject: [R] Please assist me to download this data
In-Reply-To: <CA+dpOJmNRi_uYXh-nop9UULzkVHzabBpzzri2v8QaTTh_ZaBgA@mail.gmail.com>
References: <CA+dpOJmNRi_uYXh-nop9UULzkVHzabBpzzri2v8QaTTh_ZaBgA@mail.gmail.com>
Message-ID: <CAKVAULM1Yj11GJLc7UrS4ArYdJTjNUm7dPpTQiSnTCwyoDYpqQ@mail.gmail.com>

The easiest might be to download the entire dataset and filter it
appropriately. If I follow your link and press Download, I get the option
to "Download Complete NAV Report in Text Format" and will result in this:
http://portal.amfiindia.com/NAVReport.aspx?type=0

It is not the easiest format to get into R, and you might have to
pre-process it.

Best,
Ulrik

On Mon, 25 Jul 2016 at 20:37 Christofer Bogaso <bogaso.christofer at gmail.com>
wrote:

> Hi again,
>
> I am trying to find some way to download all data historically from
> this website "https://www.amfiindia.com/net-asset-value/nav-history".
>
> Basically what I am trying to do is, I shall have a range of dates and
> for each date I need to download entire dataset programmatically.
>
> Really appreciate if experts here help.
>
> Thanks,
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bogaso.christofer at gmail.com  Mon Jul 25 21:30:35 2016
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Tue, 26 Jul 2016 01:00:35 +0530
Subject: [R] Please assist me to download this data
In-Reply-To: <CAKVAULM1Yj11GJLc7UrS4ArYdJTjNUm7dPpTQiSnTCwyoDYpqQ@mail.gmail.com>
References: <CA+dpOJmNRi_uYXh-nop9UULzkVHzabBpzzri2v8QaTTh_ZaBgA@mail.gmail.com>
	<CAKVAULM1Yj11GJLc7UrS4ArYdJTjNUm7dPpTQiSnTCwyoDYpqQ@mail.gmail.com>
Message-ID: <CA+dpOJmXHsXxROpDpHqqDrihARWtUsutkGwTJ9GG9CcrWwmdtg@mail.gmail.com>

Hi Ulrik, Thanks for your reply. I am aware of that link and this is a
good option. However with this approach, I can not get Data
historically. I would like to create some TS in R for each MF there.

Any other idea?

Thanks,

On Tue, Jul 26, 2016 at 12:53 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
> The easiest might be to download the entire dataset and filter it
> appropriately. If I follow your link and press Download, I get the option to
> "Download Complete NAV Report in Text Format" and will result in this:
> http://portal.amfiindia.com/NAVReport.aspx?type=0
>
> It is not the easiest format to get into R, and you might have to
> pre-process it.
>
> Best,
> Ulrik
>
> On Mon, 25 Jul 2016 at 20:37 Christofer Bogaso <bogaso.christofer at gmail.com>
> wrote:
>>
>> Hi again,
>>
>> I am trying to find some way to download all data historically from
>> this website "https://www.amfiindia.com/net-asset-value/nav-history".
>>
>> Basically what I am trying to do is, I shall have a range of dates and
>> for each date I need to download entire dataset programmatically.
>>
>> Really appreciate if experts here help.
>>
>> Thanks,
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From ulrik.stervbo at gmail.com  Mon Jul 25 21:52:45 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Mon, 25 Jul 2016 19:52:45 +0000
Subject: [R] Please assist me to download this data
In-Reply-To: <CA+dpOJmXHsXxROpDpHqqDrihARWtUsutkGwTJ9GG9CcrWwmdtg@mail.gmail.com>
References: <CA+dpOJmNRi_uYXh-nop9UULzkVHzabBpzzri2v8QaTTh_ZaBgA@mail.gmail.com>
	<CAKVAULM1Yj11GJLc7UrS4ArYdJTjNUm7dPpTQiSnTCwyoDYpqQ@mail.gmail.com>
	<CA+dpOJmXHsXxROpDpHqqDrihARWtUsutkGwTJ9GG9CcrWwmdtg@mail.gmail.com>
Message-ID: <CAKVAULONNb=uO=e3jWKywirVySi6T3C3V32mLKRny_VzK+UJ3w@mail.gmail.com>

Hi Christofer,

If you can load all the data into R you don't need to query the website -
you simply filter the data by your dates.

I think that's the easiest solution.

Best wishes,
Ulrik

Christofer Bogaso <bogaso.christofer at gmail.com> schrieb am Mo., 25. Juli
2016 21:30:

> Hi Ulrik, Thanks for your reply. I am aware of that link and this is a
> good option. However with this approach, I can not get Data
> historically. I would like to create some TS in R for each MF there.
>
> Any other idea?
>
> Thanks,
>
> On Tue, Jul 26, 2016 at 12:53 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> wrote:
> > The easiest might be to download the entire dataset and filter it
> > appropriately. If I follow your link and press Download, I get the
> option to
> > "Download Complete NAV Report in Text Format" and will result in this:
> > http://portal.amfiindia.com/NAVReport.aspx?type=0
> >
> > It is not the easiest format to get into R, and you might have to
> > pre-process it.
> >
> > Best,
> > Ulrik
> >
> > On Mon, 25 Jul 2016 at 20:37 Christofer Bogaso <
> bogaso.christofer at gmail.com>
> > wrote:
> >>
> >> Hi again,
> >>
> >> I am trying to find some way to download all data historically from
> >> this website "https://www.amfiindia.com/net-asset-value/nav-history".
> >>
> >> Basically what I am trying to do is, I shall have a range of dates and
> >> for each date I need to download entire dataset programmatically.
> >>
> >> Really appreciate if experts here help.
> >>
> >> Thanks,
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bob at rudis.net  Mon Jul 25 22:23:08 2016
From: bob at rudis.net (boB Rudis)
Date: Mon, 25 Jul 2016 16:23:08 -0400
Subject: [R] Please assist me to download this data
In-Reply-To: <CAKVAULONNb=uO=e3jWKywirVySi6T3C3V32mLKRny_VzK+UJ3w@mail.gmail.com>
References: <CA+dpOJmNRi_uYXh-nop9UULzkVHzabBpzzri2v8QaTTh_ZaBgA@mail.gmail.com>
	<CAKVAULM1Yj11GJLc7UrS4ArYdJTjNUm7dPpTQiSnTCwyoDYpqQ@mail.gmail.com>
	<CA+dpOJmXHsXxROpDpHqqDrihARWtUsutkGwTJ9GG9CcrWwmdtg@mail.gmail.com>
	<CAKVAULONNb=uO=e3jWKywirVySi6T3C3V32mLKRny_VzK+UJ3w@mail.gmail.com>
Message-ID: <CAJ4QxaNPyitrZ4auOi3KyZafGqQjAsEC8BFpP9V75Bw0odsvcg@mail.gmail.com>

Valid parameters for the form would be super-helpful.

On Mon, Jul 25, 2016 at 3:52 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
> Hi Christofer,
>
> If you can load all the data into R you don't need to query the website -
> you simply filter the data by your dates.
>
> I think that's the easiest solution.
>
> Best wishes,
> Ulrik
>
> Christofer Bogaso <bogaso.christofer at gmail.com> schrieb am Mo., 25. Juli
> 2016 21:30:
>
>> Hi Ulrik, Thanks for your reply. I am aware of that link and this is a
>> good option. However with this approach, I can not get Data
>> historically. I would like to create some TS in R for each MF there.
>>
>> Any other idea?
>>
>> Thanks,
>>
>> On Tue, Jul 26, 2016 at 12:53 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>> wrote:
>> > The easiest might be to download the entire dataset and filter it
>> > appropriately. If I follow your link and press Download, I get the
>> option to
>> > "Download Complete NAV Report in Text Format" and will result in this:
>> > http://portal.amfiindia.com/NAVReport.aspx?type=0
>> >
>> > It is not the easiest format to get into R, and you might have to
>> > pre-process it.
>> >
>> > Best,
>> > Ulrik
>> >
>> > On Mon, 25 Jul 2016 at 20:37 Christofer Bogaso <
>> bogaso.christofer at gmail.com>
>> > wrote:
>> >>
>> >> Hi again,
>> >>
>> >> I am trying to find some way to download all data historically from
>> >> this website "https://www.amfiindia.com/net-asset-value/nav-history".
>> >>
>> >> Basically what I am trying to do is, I shall have a range of dates and
>> >> for each date I need to download entire dataset programmatically.
>> >>
>> >> Really appreciate if experts here help.
>> >>
>> >> Thanks,
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From abdoulayesar at gmail.com  Mon Jul 25 18:56:49 2016
From: abdoulayesar at gmail.com (Abdoulaye SARR)
Date: Mon, 25 Jul 2016 16:56:49 +0000
Subject: [R] probelm with xlab ylab and xaxp barplot
In-Reply-To: <1E3D23B5-C854-4C80-B802-F6CC2980280A@me.com>
References: <C7042838-33C6-466B-8978-217A2E2320DC@gmail.com>
	<779771D7-EBEF-47EA-BE2C-4FF822E5D2DA@me.com>
	<CAN=6O0J_pG_cFWoq3dKuZ0rV84HemMdipC4U5J1wA1uJ94QQ6w@mail.gmail.com>
	<EC5F7F05-0C9F-4E5B-9205-DCA81200BD29@me.com>
	<C230B1E5-3155-4407-A2C9-4D11DB7E9416@gmail.com>
	<1E3D23B5-C854-4C80-B802-F6CC2980280A@me.com>
Message-ID: <868300E7-D001-4669-88DD-EC27CD98C87B@gmail.com>

Hi Marc,

According to your guidance the labels are almost at the right place when adjusting mar values.

A remaining need is to have the x axis at least for the two bottom figures as date from 1981 to 2005. Do you think this is doable. Ylim is fine but how tots in this case xlim.

Best regards,

asarr
Le 25 juil. 2016 ? 14:28, Marc Schwartz <marc_schwartz at me.com> a ?crit :

> Hi,
> 
> If your code below is a verbatim copy and paste, you still have the following two lines active:
> 
>  par(mar=rep(2,4))
> 
> and
> 
>  op <- par(oma=c(1,2,3,5))
> 
> Comment out both of those lines and then see what the result looks like.
> 
> As I noted before, try the plot **without any modifications** to the default margin values. Then adjust from there, which may require you to increase, not decrease, the values from their defaults in order to have room for your text.
> 
> The values you have for par(mar) above, for example, reduce the values to 2 for each side from the default, which is:
> 
>  c(5, 4, 4, 2) + 0.1.
> 
> So that alone will likely result in there not being enough room for your axis labels.
> 
> You may also have to create the barplot without any default annotation created by the function itself and then add it with ?axis, ?text and ?mtext. You may also have to reduce the size of the font itself, which is done via the cex* arguments to barplot() and the additional annotation functions mentioned in the prior sentence.
> 
> Regards,
> 
> Marc
> 
> 
>> On Jul 25, 2016, at 8:06 AM, Abdoulaye SARR <abdoulayesar at gmail.com> wrote:
>> 
>> 
>>>> Hi Marc and Others,
>> 
>> 
>> I am still struggling to have my slab and ylab displayed on a bar plot. Marc did useful advise on playing with mar settings. I tried may combinations and can?t have it work.
>> 
>> I paste the code I am suing hoping guidance on solving this issue.
>> 
>> 
>> 
>> 
>> ## extract works for all time steps
>> d1<-read.nc(gp)
>> 
>> d2<-read.nc(er)
>> 
>> d3<-read.nc(me)
>> 
>> d4<-read.nc(ne)
>> 
>> d5<-read.nc(ar)
>> 
>> d6<-read.nc(cc)
>> 
>> d7<-read.nc(mr)
>> 
>> d8<-read.nc(ic)
>> 
>> z1<-d1$spei
>> z2<-d2$spei
>> z3<-d3$spei
>> z4<-d4$spei
>> z5<-d5$spei
>> z6<-d6$spei
>> z7<-d7$spei
>> z8<-d8$spei
>> #par(oma=c(2,2,2,2))  # all sides have 3 lines of space  
>> 
>> par(mar=rep(2,4))
>> #par(mar=c(5.1, 4.1, 2.1, 2.1))
>> #par(mai=c(1.02,0.82,0.82,0.42))
>> op <- par(oma=c(1,2,3,5))
>> #op <- par(oma=c(6,5,0,0))
>> par(mfrow=c(4,2))
>> 
>> line = 3
>> 
>> barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei", xaxp=c(181,2005,1), col=ifelse(z1>0,"green","brown"))
>> 
>> mtext("a")
>> barplot(z2,xlab="Years", ylab="spei",  ylim=c(-2,2), col=ifelse(z2>0,"green","brown"))
>> mtext("b")
>> barplot(z3, ylim=c(-2,2), xlab="Years", ylab="spei", col=ifelse(z3>0,"green","brown"))
>> mtext("c")
>> barplot(z4, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z4>0,"green","brown"))
>> mtext("d")
>> barplot(z5, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z5>0,"green","brown"))
>> mtext("e")
>> barplot(z6, xlab="Years", ylab="spei", ylim=c(-2,2), col=ifelse(z6>0,"green","brown"))
>> mtext("f")
>> barplot(z7,xlab="Years", ylab="spei",  ylim=c(-2,2), col=ifelse(z7>0,"green","brown"))
>> mtext("g")
>> barplot(z8,  ylim=c(-2,2), xlab="Years", ylab="spei", col=ifelse(z8>0,"green","brown"))
>> mtext("h")
>> par(op)
>> 
>> Another solution with ggplot2 or lattice also welcome.
>> 
>> 
>> Best regards,
>> 
>> asarr
> 
> <snip>


	[[alternative HTML version deleted]]


From kulm at cooper.edu  Mon Jul 25 18:51:16 2016
From: kulm at cooper.edu (kulm)
Date: Mon, 25 Jul 2016 12:51:16 -0400
Subject: [R] gcc error when configuring instillation
Message-ID: <57cf1116db4e3194006b87874446020e@cooper.edu>

Hello,

I am currently trying to install R in my directory of a shared linux
server.  I have downloaded the most recent source code tarball,
uncompressed it, but hit errors when trying to enter the ./configure
command.  The main error appears to focus on gcc as one of lines states:
"checking whether the C compiler works... no".  When reading the log I see:
"gcc: error: unrecognized command line option '-V'" and "gcc: fatal error:
no input files".  I have checked and I have f77 and gfortran both within my
path, and the gcc version is 4.8.2.  When trying to ./configure with the
--disable-openmp command I recieve the same error.  I have attached the
config.log to the email.  If anyone has any suggestion I would be most
grateful.

Thank you very much,
Scott Kulm

From mragonne at staffmail.ed.ac.uk  Mon Jul 25 16:45:33 2016
From: mragonne at staffmail.ed.ac.uk (Manon Ragonnet-Cronin)
Date: Mon, 25 Jul 2016 15:45:33 +0100
Subject: [R] brokerage function sna package raw.grp
Message-ID: <20160725154533.13037ac4iyrwufk8@www.staffmail.ed.ac.uk>



Dear all,

I am using the brokerage function in the sna package to look at  
liaisons between groups. The function returns raw and expected scores  
by vertex (raw.nli and exp.nli) and  the aggregate raw and expected  
scores (raw.gli and exp.gli). However, I am interested  in the scores  
for each group that I am studying  and only the *expected* scores per  
group (exp.grp) are given, not the raw scores by group. I thought  
maybe I'd be able to calculate the raw scores by sorting my vertices  
into groups and summing their raw scores within each group, but  
exp.grp does not seem to be the sum of exp.nli sorted by group.
I would be grateful for any help.

Thanks,
Manon Ragonnet,
Postdoctoral researcher, University of Edinburgh, UK.



-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From bogaso.christofer at gmail.com  Mon Jul 25 22:30:45 2016
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Tue, 26 Jul 2016 02:00:45 +0530
Subject: [R] Please assist me to download this data
In-Reply-To: <CAJ4QxaNPyitrZ4auOi3KyZafGqQjAsEC8BFpP9V75Bw0odsvcg@mail.gmail.com>
References: <CA+dpOJmNRi_uYXh-nop9UULzkVHzabBpzzri2v8QaTTh_ZaBgA@mail.gmail.com>
	<CAKVAULM1Yj11GJLc7UrS4ArYdJTjNUm7dPpTQiSnTCwyoDYpqQ@mail.gmail.com>
	<CA+dpOJmXHsXxROpDpHqqDrihARWtUsutkGwTJ9GG9CcrWwmdtg@mail.gmail.com>
	<CAKVAULONNb=uO=e3jWKywirVySi6T3C3V32mLKRny_VzK+UJ3w@mail.gmail.com>
	<CAJ4QxaNPyitrZ4auOi3KyZafGqQjAsEC8BFpP9V75Bw0odsvcg@mail.gmail.com>
Message-ID: <CA+dpOJnq8SkTqAwYz7Dv+3VoWeeQ21HGPuXSA0tJngW+oik3_Q@mail.gmail.com>

Just have tried this :

library(RCurl)
x = postForm("https://www.amfiindia.com/net-asset-value/nav-history",
'NAV Date' = "25-Jul-2016")

However doesnt look like I get valid data, because I still can get
result for some future date as well!!

Any other option I should try?

On Tue, Jul 26, 2016 at 1:53 AM, boB Rudis <bob at rudis.net> wrote:
> Valid parameters for the form would be super-helpful.
>
> On Mon, Jul 25, 2016 at 3:52 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>> Hi Christofer,
>>
>> If you can load all the data into R you don't need to query the website -
>> you simply filter the data by your dates.
>>
>> I think that's the easiest solution.
>>
>> Best wishes,
>> Ulrik
>>
>> Christofer Bogaso <bogaso.christofer at gmail.com> schrieb am Mo., 25. Juli
>> 2016 21:30:
>>
>>> Hi Ulrik, Thanks for your reply. I am aware of that link and this is a
>>> good option. However with this approach, I can not get Data
>>> historically. I would like to create some TS in R for each MF there.
>>>
>>> Any other idea?
>>>
>>> Thanks,
>>>
>>> On Tue, Jul 26, 2016 at 12:53 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>>> wrote:
>>> > The easiest might be to download the entire dataset and filter it
>>> > appropriately. If I follow your link and press Download, I get the
>>> option to
>>> > "Download Complete NAV Report in Text Format" and will result in this:
>>> > http://portal.amfiindia.com/NAVReport.aspx?type=0
>>> >
>>> > It is not the easiest format to get into R, and you might have to
>>> > pre-process it.
>>> >
>>> > Best,
>>> > Ulrik
>>> >
>>> > On Mon, 25 Jul 2016 at 20:37 Christofer Bogaso <
>>> bogaso.christofer at gmail.com>
>>> > wrote:
>>> >>
>>> >> Hi again,
>>> >>
>>> >> I am trying to find some way to download all data historically from
>>> >> this website "https://www.amfiindia.com/net-asset-value/nav-history".
>>> >>
>>> >> Basically what I am trying to do is, I shall have a range of dates and
>>> >> for each date I need to download entire dataset programmatically.
>>> >>
>>> >> Really appreciate if experts here help.
>>> >>
>>> >> Thanks,
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide
>>> >> http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From bbolker at gmail.com  Tue Jul 26 00:22:00 2016
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 25 Jul 2016 22:22:00 +0000
Subject: [R] glmmPQL crashes on inclusion of corSpatial object
References: <32c79e9b-4efb-74eb-e97a-c09aec1dc32d@gmail.com>
Message-ID: <loom.20160726T002003-692@post.gmane.org>

Patrick Johann Schratz <patrick.schratz <at> gmail.com> writes:

> 
> Link to data: <https://www.dropbox.com/s/yi3vf0bmqvydr8h/data.Rd?dl=0>  
> (1170 obs, 9 variables, .Rd file) [plain link in case sth goes wrong 
> with the hyperlink: https://www.dropbox.com/s/yi3vf0bmqvydr8h/data.Rd?dl=0]
> 

 By the way, this has been cross-posted both to StackOverflow
and to r-sig-mixed-models at r-project.org. Cross-posting between
R lists is explicitly deprecated; cross-posting between R lists
and StackOverflow is not *explicitly* deprecated anywhere, but is
probably a bad idea (tends to waste the time of answerers who don't
know the question has been commented on and/or answered already
elsewhere).  Try to pick the single best venue and stick to it.

  Ben Bolker


From dwinsemius at comcast.net  Tue Jul 26 00:22:25 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 25 Jul 2016 15:22:25 -0700
Subject: [R] Please assist me to download this data
In-Reply-To: <CA+dpOJnq8SkTqAwYz7Dv+3VoWeeQ21HGPuXSA0tJngW+oik3_Q@mail.gmail.com>
References: <CA+dpOJmNRi_uYXh-nop9UULzkVHzabBpzzri2v8QaTTh_ZaBgA@mail.gmail.com>
	<CAKVAULM1Yj11GJLc7UrS4ArYdJTjNUm7dPpTQiSnTCwyoDYpqQ@mail.gmail.com>
	<CA+dpOJmXHsXxROpDpHqqDrihARWtUsutkGwTJ9GG9CcrWwmdtg@mail.gmail.com>
	<CAKVAULONNb=uO=e3jWKywirVySi6T3C3V32mLKRny_VzK+UJ3w@mail.gmail.com>
	<CAJ4QxaNPyitrZ4auOi3KyZafGqQjAsEC8BFpP9V75Bw0odsvcg@mail.gmail.com>
	<CA+dpOJnq8SkTqAwYz7Dv+3VoWeeQ21HGPuXSA0tJngW+oik3_Q@mail.gmail.com>
Message-ID: <6903E848-3ED6-439B-BF0A-65CDCC4AD6AA@comcast.net>


> On Jul 25, 2016, at 1:30 PM, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Just have tried this :
> 
> library(RCurl)
> x = postForm("https://www.amfiindia.com/net-asset-value/nav-history",
> 'NAV Date' = "25-Jul-2016")
> 
> However doesnt look like I get valid data, because I still can get
> result for some future date as well!!
> 
> Any other option I should try?

This is not an R specific. It is simply reading HTML, XML, or JavaScript.

If you are unable to specify a particular set of values that delivers this during a human mediated interaction, then you should contact the owners of the webpage rather posting here. They have a "Contact Us" page.


-- 
David.


> 
> On Tue, Jul 26, 2016 at 1:53 AM, boB Rudis <bob at rudis.net> wrote:
>> Valid parameters for the form would be super-helpful.
>> 
>> On Mon, Jul 25, 2016 at 3:52 PM, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>>> Hi Christofer,
>>> 
>>> If you can load all the data into R you don't need to query the website -
>>> you simply filter the data by your dates.
>>> 
>>> I think that's the easiest solution.
>>> 
>>> Best wishes,
>>> Ulrik
>>> 
>>> Christofer Bogaso <bogaso.christofer at gmail.com> schrieb am Mo., 25. Juli
>>> 2016 21:30:
>>> 
>>>> Hi Ulrik, Thanks for your reply. I am aware of that link and this is a
>>>> good option. However with this approach, I can not get Data
>>>> historically. I would like to create some TS in R for each MF there.
>>>> 
>>>> Any other idea?
>>>> 
>>>> Thanks,
>>>> 
>>>> On Tue, Jul 26, 2016 at 12:53 AM, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>>>> wrote:
>>>>> The easiest might be to download the entire dataset and filter it
>>>>> appropriately. If I follow your link and press Download, I get the
>>>> option to
>>>>> "Download Complete NAV Report in Text Format" and will result in this:
>>>>> http://portal.amfiindia.com/NAVReport.aspx?type=0
>>>>> 
>>>>> It is not the easiest format to get into R, and you might have to
>>>>> pre-process it.
>>>>> 
>>>>> Best,
>>>>> Ulrik
>>>>> 
>>>>> On Mon, 25 Jul 2016 at 20:37 Christofer Bogaso <
>>>> bogaso.christofer at gmail.com>
>>>>> wrote:
>>>>>> 
>>>>>> Hi again,
>>>>>> 
>>>>>> I am trying to find some way to download all data historically from
>>>>>> this website "https://www.amfiindia.com/net-asset-value/nav-history".
>>>>>> 
>>>>>> Basically what I am trying to do is, I shall have a range of dates and
>>>>>> for each date I need to download entire dataset programmatically.
>>>>>> 
>>>>>> Really appreciate if experts here help.
>>>>>> 
>>>>>> Thanks,
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From chocold12 at gmail.com  Tue Jul 26 06:54:28 2016
From: chocold12 at gmail.com (lily li)
Date: Mon, 25 Jul 2016 22:54:28 -0600
Subject: [R] about netcdf files
Message-ID: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>

Hi all,

I have a problem in opening netcdf files. If one netcdf file contains
longitude, latitude, and daily precipitation. How to relate each
precipitation record to its associated location, and export them as csv
files? Thanks.

I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks for
any ideas.

	[[alternative HTML version deleted]]


From mapjacques at gmail.com  Tue Jul 26 04:49:37 2016
From: mapjacques at gmail.com (Maria Alice Jacques)
Date: Mon, 25 Jul 2016 23:49:37 -0300
Subject: [R] R version 3.2.5
Message-ID: <CAJ3zTkkeQnOoNEKA3NANCX_7zUgMmMS6_w_KSi4-1nbgnhWgqA@mail.gmail.com>

Good night,

How can I download and install R version 3.2.5 from cran mirror of
University of S?o Paulo, S?o Paulo?

Thanks for your support.

Maria Alice P. Jacques

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Jul 26 08:28:55 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 25 Jul 2016 23:28:55 -0700
Subject: [R] about netcdf files
In-Reply-To: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
Message-ID: <BFC0CACB-BD11-410D-B4DA-78EE3E7F90E9@comcast.net>

Please show code that allows potential responders to reply meaningfully.

-- 
David

> On Jul 25, 2016, at 9:54 PM, lily li <chocold12 at gmail.com> wrote:
> 
> Hi all,
> 
> I have a problem in opening netcdf files. If one netcdf file contains
> longitude, latitude, and daily precipitation. How to relate each
> precipitation record to its associated location, and export them as csv
> files? Thanks.
> 
> I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks for
> any ideas.
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From phaedrusv at gmail.com  Tue Jul 26 09:10:07 2016
From: phaedrusv at gmail.com (Andy Wolfe)
Date: Tue, 26 Jul 2016 08:10:07 +0100
Subject: [R] word stemming for corpus linguistics
Message-ID: <be27178e-5c35-b392-9471-645508768597@gmail.com>

Hi list

On a piece of work I'm doing in corpus linguistics, using a combo of 
texts by Gries "Quantitative Corpus Linguistics with R: A Practical 
Introduction" and Jockers "Text Analysis with R for Students of 
Literature", which are both really excellent by the way, I want to stem 
or lemmatize the words so that, for e.g., 'facilitating', 'facilitated', 
and 'facilitates' all become 'facilit'.

In text mining, using a combination of the packages 'tm' and 'SnowballC' 
this is feasible, but then I am finding that working with the DTM 
(document term matrix) becomes difficult for when I want to do 
concordance (or key word in context) analysis.

So, two questions:

(1) is there a package for R version 3.3.1 that can work with corpus 
linguistics? and/ or

(2) is there a way of doing concordance analysis using the tm package as 
part of the whole text mining process?

I appreciate any help. Thanks.

Andy


	[[alternative HTML version deleted]]


From loris.bennett at fu-berlin.de  Tue Jul 26 09:48:23 2016
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Tue, 26 Jul 2016 09:48:23 +0200
Subject: [R] R version 3.2.5
References: <CAJ3zTkkeQnOoNEKA3NANCX_7zUgMmMS6_w_KSi4-1nbgnhWgqA@mail.gmail.com>
Message-ID: <871t2gon5k.fsf@hornfels.zedat.fu-berlin.de>

Maria Alice Jacques <mapjacques at gmail.com> writes:

> Good night,
>
> How can I download and install R version 3.2.5 from cran mirror of
> University of S?o Paulo, S?o Paulo?
>
> Thanks for your support.
>
> Maria Alice P. Jacques

[snip (8 lines)]

On this page

https://cran.r-project.org/

you can click on 'Mirrors' and choose the one you want.  You can then
click on the appropriate 'Download R' link for your operating system.
Note you can only install binaries for an arbitrary old version if you
are using Windows (or possibly Mac OS X).  If you are using a
Linux-based distribution, there will often be only specific versions
available for the version of your particular distribution.  In this
case, you might have to install from source.

Regards

Loris

-- 
Dr. Loris Bennett (Mr.)
ZEDAT, Freie Universit?t Berlin         Email loris.bennett at fu-berlin.de


From paul.johnston at manchester.ac.uk  Tue Jul 26 09:50:21 2016
From: paul.johnston at manchester.ac.uk (Paul Johnston)
Date: Tue, 26 Jul 2016 07:50:21 +0000
Subject: [R] word stemming for corpus linguistics
In-Reply-To: <be27178e-5c35-b392-9471-645508768597@gmail.com>
References: <be27178e-5c35-b392-9471-645508768597@gmail.com>
Message-ID: <EE85AFC56BE9E84EB86F7739169F1BD6A2485107@MBXP15.ds.man.ac.uk>

Suggest look at http://www.inside-r.org/packages/cran/tm/docs/stemDocument



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Andy Wolfe
Sent: 26 July 2016 08:10
To: r-help at r-project.org
Subject: [R] word stemming for corpus linguistics

Hi list

On a piece of work I'm doing in corpus linguistics, using a combo of texts by Gries "Quantitative Corpus Linguistics with R: A Practical Introduction" and Jockers "Text Analysis with R for Students of Literature", which are both really excellent by the way, I want to stem or lemmatize the words so that, for e.g., 'facilitating', 'facilitated', and 'facilitates' all become 'facilit'.

In text mining, using a combination of the packages 'tm' and 'SnowballC' 
this is feasible, but then I am finding that working with the DTM (document term matrix) becomes difficult for when I want to do concordance (or key word in context) analysis.

So, two questions:

(1) is there a package for R version 3.3.1 that can work with corpus linguistics? and/ or

(2) is there a way of doing concordance analysis using the tm package as part of the whole text mining process?

I appreciate any help. Thanks.

Andy


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From phaedrusv at gmail.com  Tue Jul 26 10:13:51 2016
From: phaedrusv at gmail.com (Andy Wolfe)
Date: Tue, 26 Jul 2016 09:13:51 +0100
Subject: [R] word stemming for corpus linguistics
In-Reply-To: <EE85AFC56BE9E84EB86F7739169F1BD6A2485107@MBXP15.ds.man.ac.uk>
References: <be27178e-5c35-b392-9471-645508768597@gmail.com>
	<EE85AFC56BE9E84EB86F7739169F1BD6A2485107@MBXP15.ds.man.ac.uk>
Message-ID: <cfc318a0-002e-3dec-1638-88fdefddfdef@gmail.com>

Hi Paul

I have seen this - it's part of the tm package mentioned originally. So, 
I've tried it again and perhaps I'm using stemDocument incorrectly, but 
this is what I am doing:

# > library(tm)
Loading required package: NLP
 > text.v <- scan(file.choose(), what = 'char', sep = '\n')
Read 938 items
# >text.stem.v <- stemDocument(text.v, language = 'english')

But it isn't changing anything in the body of the text I'm passing to it 
- the words are unlemmatized/ unstemmed.

When I try using SnowballC, the error returned is that tm_map doesn't 
have a method to work with objects of class 'character'.

Again, the problem is that tm doesn't seem to allow for concordance 
analysis ... or perhaps it does and I just haven't figured out how to do 
it, so am happy to be shown some documentation on that process, and 
whether that is applied before or after the text is transformed into a 
DTM because searching on-line hasn't (yet) thrown anything back.

Thanks.
Andy


On 26/07/16 08:50, Paul Johnston wrote:
> Suggest look at http://www.inside-r.org/packages/cran/tm/docs/stemDocument
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Andy Wolfe
> Sent: 26 July 2016 08:10
> To: r-help at r-project.org
> Subject: [R] word stemming for corpus linguistics
>
> Hi list
>
> On a piece of work I'm doing in corpus linguistics, using a combo of texts by Gries "Quantitative Corpus Linguistics with R: A Practical Introduction" and Jockers "Text Analysis with R for Students of Literature", which are both really excellent by the way, I want to stem or lemmatize the words so that, for e.g., 'facilitating', 'facilitated', and 'facilitates' all become 'facilit'.
>
> In text mining, using a combination of the packages 'tm' and 'SnowballC'
> this is feasible, but then I am finding that working with the DTM (document term matrix) becomes difficult for when I want to do concordance (or key word in context) analysis.
>
> So, two questions:
>
> (1) is there a package for R version 3.3.1 that can work with corpus linguistics? and/ or
>
> (2) is there a way of doing concordance analysis using the tm package as part of the whole text mining process?
>
> I appreciate any help. Thanks.
>
> Andy
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maechler at stat.math.ethz.ch  Tue Jul 26 10:24:31 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 26 Jul 2016 10:24:31 +0200
Subject: [R] lm() silently drops NAs
In-Reply-To: <CABdHhvEAh4SYRKStO8tTZOAF=HXWmVwuioC+QkUaLJ04SEPcdg@mail.gmail.com>
References: <CABdHhvEAh4SYRKStO8tTZOAF=HXWmVwuioC+QkUaLJ04SEPcdg@mail.gmail.com>
Message-ID: <22423.7743.681107.446114@stat.math.ethz.ch>

I have been asked (in private)

    > Hi Martin,

    y <- c(1, 2, 3, NA, 4)
    x <- c(1, 2, 2, 1, 1)

    t.test(y ~ x)
    lm(y ~ x)

    > Normally, most R functions follow the principle that
    > "missings should never silently go missing". Do you have
    > any background on why these functions (and most model/test
    > function in stats, I presume) silently drop missing values?

And I think, the issue and an answer are important enough to be
public, hence this posting to R-help :

First note that in some sense it is not true that lm(y ~ x) silently drops 
NAs: Everybody who is taught about lm() is taught to look at
   summary( fm )  where  fm <- lm(y ~ x)

and that (for the above case)  "says"

 ' (1 observation deleted due to missingness) '

and so is not entirely silent.


This goes all back to the idea of having an 'na.action' argument
which may be a function (a very good idea, "functional
programming" in a double sense!)... which Chambers et al
introduced in The White Book (*1) and which I think to remember 
was quite a revolutionary idea; at least I had liked that very much
once I understood the beauty of passing functions as arguments
to other functions.
One problem already back then has been that we already had the
---much more primitive but often sufficient--- standard of an
'na.rm = FALSE' (i.e. default FALSE) argument.

This has been tought in all good classes/course about statistical
modeling with S and R ever since ... I had hoped ....
(but it seems I was too optimistic, .. or many students have too
 quickly forgotten what they were taught ..)
Notably the white book itself, and the MASS (*2) book do teach
this.. though possibly not loudly enough.

Two more decisions about this were made back then, as well:

  1) The default for na.action to be na.omit  (==> "silently dropping")

  2) na.action being governed by  options(na.action = ..)

'1)' may have been mostly "historical": I think it had been the behavior of
other "main stream" statistical packages back then (and now?) *and*
possibly more importantly, notably with the later (than white book = "S3")
advent of na.replace, you did want to keep the missing in your
data frame, for later analysis; e.g. drawing (-> "gaps" in
plots) so the NA *were* carried along and would not be
forgotten, something very important in most case.s.
   
and '2)' is something I personally no longer like very
much, as it is "killing" the functional paradigm.
OTOH, it has to be said in favor of that "session wide" / "global" setting
      options(na.action = *)
that indeed it depends on the specific data analysis, or even
the specific *phase* of a data analysis, *what* behavior of NA
treatment is desired.... and at the time it was thought smart
that all methods (also functions "deep down" called by
user-called functtions) would automatically use the "currently
desired" NA handling.

There have been recommendations (I don't know exactly where and
by whom) to always set

   options(na.action = na.fail)

in your global .First() or nowadays rather your  Rprofile, and
I assume that some of the CRAN packages and some of the "teaching
setups" would do that (and if you do that, the lm() and t.test()
calls above give an error). 

Of course, there's much more to say about this, quite a bit of which
has been published in scientific papers and books..

Martin Maechler
ETH Zurich and R Core Team

----------------

*1) "The White Book": Chambers and Hastie (1992):
 https://www.r-project.org/doc/bib/R-books_bib.html#R:Chambers+Hastie:1992

*2) "MASS - the book": Venables and Ripley (2002 (= 4th ed.)):
 https://www.r-project.org/doc/bib/R-books_bib.html#R:Venables+Ripley:2002


From skmagut at student.maseno.ac.ke  Tue Jul 26 10:27:49 2016
From: skmagut at student.maseno.ac.ke (Shadrack Magut)
Date: Tue, 26 Jul 2016 04:27:49 -0400
Subject: [R] knitr package error.
Message-ID: <CADVq9WVUbbpc2_LX9wjX+KzO8g6Uni1q01PN9BO_TYev0OBkbA@mail.gmail.com>

hello, someone help me sort out this error "## Error in summary(data):
object ?analysis? not found"" in using Knitr package in R. I imported the
data into R studio but as soon as i use any code involving the data i
imported in the Knitr environment within the latex document i encounter the
above error.Kindly help me sort out this error.

Attached to this mail is a word document containing the screenshot of my R
codes and output error.

Regards
shadrack.?

From jon.skoien at jrc.ec.europa.eu  Tue Jul 26 12:08:57 2016
From: jon.skoien at jrc.ec.europa.eu (Jon Skoien)
Date: Tue, 26 Jul 2016 12:08:57 +0200
Subject: [R] about netcdf files
In-Reply-To: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
Message-ID: <843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>

You could try with the brick function from the raster package.

bvar = brick(netcdfName)

This uses the ncdf4 functions for opening and reading the netcdf, but 
makes it easier to extract data for each day:

p1 = rasterToPoints(bvar[[1]])
and write p1 to csv.

Best,
Jon


On 7/26/2016 6:54 AM, lily li wrote:
> Hi all,
>
> I have a problem in opening netcdf files. If one netcdf file contains
> longitude, latitude, and daily precipitation. How to relate each
> precipitation record to its associated location, and export them as csv
> files? Thanks.
>
> I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks for
> any ideas.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Jon Olav Sk?ien
Joint Research Centre - European Commission
Institute for Space, Security & Migration
Disaster Risk Management Unit

Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY

jon.skoien at jrc.ec.europa.eu
Tel:  +39 0332 789205

Disclaimer: Views expressed in this email are those of the individual 
and do not necessarily represent official views of the European Commission.


From lorenzo.isella at gmail.com  Tue Jul 26 12:25:34 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Tue, 26 Jul 2016 12:25:34 +0200
Subject: [R] Visualization of a Convex Hull in R (Possibly with RGL)
Message-ID: <20160726102534.GA10914@chicca2>

Dear All,
I am not an expert about the calculation and visualization of convex
hulls, but I am trying to do something relatively simple.
Please consider the snippet at the end of the email.
The array pts  represents the position of (the centres of) a set of
spheres in 3D (whose radius is 0.5).
I found the geometry package which provides bindings to the qhull
(http://www.qhull.org/) library and allows me to calculate the convex
hull of pts and some interesting properties, like the area and the
volume of the convex hull.
That is all fine, but I would like to represent the results in 3D.
I would like to see a set of spheres with the given position in pts
and the radius R=0.5 and the convex hull going based on the
coordinates of pts.
Does anybody know how to achieve that?
Many thanks

Lorenzo



####################################################################


library(geometry)



pts <- read.table("test-agg.dat")

## names(pts) <- c("x", "y", "z")


pts <- structure(list(x = c(0.145138112090815, 0.880429452229448,
-1.66682740805663,
-0.955356943224252, 1.09396798448412, -2.63620653885452,
0.190270563436841,
2.77638842095489, -0.0914977922901252, -0.484087722062158,
0.674992784754569,
1.02571108118502, 2.78789154912298, -0.146882388586427,
-1.71507089246001,
-1.87886026272455, 6.61540228778138, 7.46818822627362,
4.80777628045963,
6.35487269602107, 4.70026626314108, 3.44366671298125,
2.92460648354883,
5.07053866161192, 7.2164343710271, 6.93292242981006, 7.84238243682145,
7.4321237311656, 9.59692827237335, 8.3454086671306, 6.574251622581,
6.93340304175759), y = c(-0.189309482569001, 1.67057595730283,
-0.995542605143325, 2.31599069771684, 1.79876139786291,
0.165886909346058,
2.70740725397259, 1.70535678189514, -0.819087329147786,
0.967935667739331,
-2.53860079551206, -1.60932884056828, -1.42050583991613,
-2.36883245674979,
0.191848867151458, -1.58255618338079, 1.98324724741419,
3.0095048680572,
1.97159273200079, 2.26459561200295, 0.4461804374566, 1.87939977307282,
1.98510457359889, -0.103495422088799, -1.32050185858493,
0.584580736435273,
-2.97562362406436, 0.106204178143333, -2.0843758994948,
-1.1492729389287,
-2.13797391772643, -4.45916649729404), z = c(-1.44428170702261,
-1.45742688370091, -1.70267889327056, -1.93637881287001,
-3.4452409532781,
-3.02436538816822, 0.114790290814684, -2.10208878117278,
2.07425243689128,
1.26652602551291, 1.39914827137784, -0.345006925422662,
0.596828021941431,
3.351773622867, 2.68408561840144, 3.97006405709929, 0.82767367646934,
-0.662142231346811, 1.68344957582882, 2.92819854377685,
0.386683699222387,
-0.220305098209874, 2.37510769001993, -1.51041233970289,
-0.707073219742548,
-1.24585080403725, -1.63914669343685, 0.683153891726357,
-1.26623658129696,
1.95073173465968, -2.94804638502708, -0.635785458903106)), .Names =
c("x",
"y", "z"), class = "data.frame", row.names = c(NA, -32L))

res<-convhulln(pts, options = "FA")

print("So far so good")


From lorenzo.isella at gmail.com  Tue Jul 26 12:26:47 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Tue, 26 Jul 2016 12:26:47 +0200
Subject: [R] Visualization of a Convex Hull in R (Possibly with RGL)
Message-ID: <20160726102647.GB10914@chicca2>

Dear All,
I am not an expert about the calculation and visualization of convex
hulls, but I am trying to do something relatively simple.
Please consider the snippet at the end of the email.
The array pts  represents the position of (the centres of) a set of
spheres in 3D (whose radius is 0.5).
I found the geometry package which provides bindings to the qhull
(http://www.qhull.org/) library and allows me to calculate the convex
hull of pts and some interesting properties, like the area and the
volume of the convex hull.
That is all fine, but I would like to represent the results in 3D.
I would like to see a set of spheres with the given position in pts
and the radius R=0.5 and the convex hull going based on the
coordinates of pts.
Does anybody know how to achieve that?
Many thanks

Lorenzo



####################################################################


library(geometry)


pts <- structure(list(x = c(0.145138112090815, 0.880429452229448,
-1.66682740805663,
-0.955356943224252, 1.09396798448412, -2.63620653885452,
0.190270563436841,
2.77638842095489, -0.0914977922901252, -0.484087722062158,
0.674992784754569,
1.02571108118502, 2.78789154912298, -0.146882388586427,
-1.71507089246001,
-1.87886026272455, 6.61540228778138, 7.46818822627362,
4.80777628045963,
6.35487269602107, 4.70026626314108, 3.44366671298125,
2.92460648354883,
5.07053866161192, 7.2164343710271, 6.93292242981006, 7.84238243682145,
7.4321237311656, 9.59692827237335, 8.3454086671306, 6.574251622581,
6.93340304175759), y = c(-0.189309482569001, 1.67057595730283,
-0.995542605143325, 2.31599069771684, 1.79876139786291,
0.165886909346058,
2.70740725397259, 1.70535678189514, -0.819087329147786,
0.967935667739331,
-2.53860079551206, -1.60932884056828, -1.42050583991613,
-2.36883245674979,
0.191848867151458, -1.58255618338079, 1.98324724741419,
3.0095048680572,
1.97159273200079, 2.26459561200295, 0.4461804374566, 1.87939977307282,
1.98510457359889, -0.103495422088799, -1.32050185858493,
0.584580736435273,
-2.97562362406436, 0.106204178143333, -2.0843758994948,
-1.1492729389287,
-2.13797391772643, -4.45916649729404), z = c(-1.44428170702261,
-1.45742688370091, -1.70267889327056, -1.93637881287001,
-3.4452409532781,
-3.02436538816822, 0.114790290814684, -2.10208878117278,
2.07425243689128,
1.26652602551291, 1.39914827137784, -0.345006925422662,
0.596828021941431,
3.351773622867, 2.68408561840144, 3.97006405709929, 0.82767367646934,
-0.662142231346811, 1.68344957582882, 2.92819854377685,
0.386683699222387,
-0.220305098209874, 2.37510769001993, -1.51041233970289,
-0.707073219742548,
-1.24585080403725, -1.63914669343685, 0.683153891726357,
-1.26623658129696,
1.95073173465968, -2.94804638502708, -0.635785458903106)), .Names =
c("x",
"y", "z"), class = "data.frame", row.names = c(NA, -32L))

res<-convhulln(pts, options = "FA")


From mdsumner at gmail.com  Tue Jul 26 12:48:22 2016
From: mdsumner at gmail.com (Michael Sumner)
Date: Tue, 26 Jul 2016 10:48:22 +0000
Subject: [R] Visualization of a Convex Hull in R (Possibly with RGL)
In-Reply-To: <20160726102534.GA10914@chicca2>
References: <20160726102534.GA10914@chicca2>
Message-ID: <CAAcGz99mdFhTcPE0JkBW6pGAJBjg010weTPCiXy+9JZuDjNOEA@mail.gmail.com>

On Tue, 26 Jul 2016 at 20:29 Lorenzo Isella <lorenzo.isella at gmail.com>
wrote:

> Dear All,
> I am not an expert about the calculation and visualization of convex
> hulls, but I am trying to do something relatively simple.
> Please consider the snippet at the end of the email.
> The array pts  represents the position of (the centres of) a set of
> spheres in 3D (whose radius is 0.5).
> I found the geometry package which provides bindings to the qhull
> (http://www.qhull.org/) library and allows me to calculate the convex
> hull of pts and some interesting properties, like the area and the
> volume of the convex hull.
> That is all fine, but I would like to represent the results in 3D.
> I would like to see a set of spheres with the given position in pts
> and the radius R=0.5 and the convex hull going based on the
> coordinates of pts.
> Does anybody know how to achieve that?
> Many thanks
>
> Lorenzo
>
>
>
> ####################################################################
>
>
> library(geometry)
>
>
>
> pts <- read.table("test-agg.dat")
>
> ## names(pts) <- c("x", "y", "z")
>
>
> pts <- structure(list(x = c(0.145138112090815, 0.880429452229448,
> -1.66682740805663,
> -0.955356943224252, 1.09396798448412, -2.63620653885452,
> 0.190270563436841,
> 2.77638842095489, -0.0914977922901252, -0.484087722062158,
> 0.674992784754569,
> 1.02571108118502, 2.78789154912298, -0.146882388586427,
> -1.71507089246001,
> -1.87886026272455, 6.61540228778138, 7.46818822627362,
> 4.80777628045963,
> 6.35487269602107, 4.70026626314108, 3.44366671298125,
> 2.92460648354883,
> 5.07053866161192, 7.2164343710271, 6.93292242981006, 7.84238243682145,
> 7.4321237311656, 9.59692827237335, 8.3454086671306, 6.574251622581,
> 6.93340304175759), y = c(-0.189309482569001, 1.67057595730283,
> -0.995542605143325, 2.31599069771684, 1.79876139786291,
> 0.165886909346058,
> 2.70740725397259, 1.70535678189514, -0.819087329147786,
> 0.967935667739331,
> -2.53860079551206, -1.60932884056828, -1.42050583991613,
> -2.36883245674979,
> 0.191848867151458, -1.58255618338079, 1.98324724741419,
> 3.0095048680572,
> 1.97159273200079, 2.26459561200295, 0.4461804374566, 1.87939977307282,
> 1.98510457359889, -0.103495422088799, -1.32050185858493,
> 0.584580736435273,
> -2.97562362406436, 0.106204178143333, -2.0843758994948,
> -1.1492729389287,
> -2.13797391772643, -4.45916649729404), z = c(-1.44428170702261,
> -1.45742688370091, -1.70267889327056, -1.93637881287001,
> -3.4452409532781,
> -3.02436538816822, 0.114790290814684, -2.10208878117278,
> 2.07425243689128,
> 1.26652602551291, 1.39914827137784, -0.345006925422662,
> 0.596828021941431,
> 3.351773622867, 2.68408561840144, 3.97006405709929, 0.82767367646934,
> -0.662142231346811, 1.68344957582882, 2.92819854377685,
> 0.386683699222387,
> -0.220305098209874, 2.37510769001993, -1.51041233970289,
> -0.707073219742548,
> -1.24585080403725, -1.63914669343685, 0.683153891726357,
> -1.26623658129696,
> 1.95073173465968, -2.94804638502708, -0.635785458903106)), .Names =
> c("x",
> "y", "z"), class = "data.frame", row.names = c(NA, -32L))
>
> res<-convhulln(pts, options = "FA")
>
>
In short

library(rgl)
bg3d("grey")
spheres3d(pts, radius = 0.5, col = "white")
## triangle functions generally expect triplets of points, one after another
## and hull/triangulation functions generally return arrays of indexes
## so transpose index of hull is the right order to draw triangles
triangles3d(pts[t(res$hull), ], col = "firebrick", alpha = 0.3)

(But, I wonder if you mean to find the convex hull based on the outer shell
of those spheres? )

Cheers, Mike.



> print("So far so good")
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Tue Jul 26 13:05:22 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 16:35:22 +0530
Subject: [R] Date Time in R
Message-ID: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>

Hi Team,

This scenario may have come across a number of times however i checked
nabble & SO and couldn't find a solution hence request assistance.

I have a date variable in my data-set eir. The class of this var was
character while i had read the file in r studio. Example of date - 05-30-16

To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y"). This
converts it to a date variable. However when i check few obs
with head(eir$date) all the results are <NA>.
I also need to create weekdays from this date variable but until i get this
resolved i cant find a weekday. For weekday i have used:
eir$week<- (eir$date)
eir$week<- weekdays(as.Date(eir$week))
class(eir$week)
eir$week<- as.factor(eir$week)
head(eir$week)

Head of this eir$week results again as expected in <NA> but shows Levels:
Friday Monday Saturday Sunday Thursday Tuesday Wednesday

Not sure what i should do here. Kindly suggest.

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Tue Jul 26 13:13:41 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 26 Jul 2016 07:13:41 -0400
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
Message-ID: <2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>

On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> Hi Team,
>
> This scenario may have come across a number of times however i checked
> nabble & SO and couldn't find a solution hence request assistance.
>
> I have a date variable in my data-set eir. The class of this var was
> character while i had read the file in r studio. Example of date - 05-30-16
>
> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y"). This
> converts it to a date variable. However when i check few obs
> with head(eir$date) all the results are <NA>.

I think you don't have character data like that, because I see

 > as.Date("05-30-16", "%m-%d-%y")
[1] "2016-05-30"

I'd guess eir$date is really a factor, because character data is 
frequently changed to factor automatically.  If that's the case, this 
should work for the conversion:

as.Date(as.character(eir$date), "%m-%d-%y")

If that doesn't work, you'll need to post something reproducible.

Duncan Murdoch

> I also need to create weekdays from this date variable but until i get this
> resolved i cant find a weekday. For weekday i have used:
> eir$week<- (eir$date)
> eir$week<- weekdays(as.Date(eir$week))
> class(eir$week)
> eir$week<- as.factor(eir$week)
> head(eir$week)
>
> Head of this eir$week results again as expected in <NA> but shows Levels:
> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>
> Not sure what i should do here. Kindly suggest.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From shivipmp82 at gmail.com  Tue Jul 26 13:42:32 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 17:12:32 +0530
Subject: [R] Date Time in R
In-Reply-To: <2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
Message-ID: <CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>

Thanks Duncan for the quick response. I will check again as you suggested.
If that doesn't work i will share a reproducible example.

Thanks again!!!!

On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>
>> Hi Team,
>>
>> This scenario may have come across a number of times however i checked
>> nabble & SO and couldn't find a solution hence request assistance.
>>
>> I have a date variable in my data-set eir. The class of this var was
>> character while i had read the file in r studio. Example of date -
>> 05-30-16
>>
>> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y"). This
>> converts it to a date variable. However when i check few obs
>> with head(eir$date) all the results are <NA>.
>>
>
> I think you don't have character data like that, because I see
>
> > as.Date("05-30-16", "%m-%d-%y")
> [1] "2016-05-30"
>
> I'd guess eir$date is really a factor, because character data is
> frequently changed to factor automatically.  If that's the case, this
> should work for the conversion:
>
> as.Date(as.character(eir$date), "%m-%d-%y")
>
> If that doesn't work, you'll need to post something reproducible.
>
> Duncan Murdoch
>
> I also need to create weekdays from this date variable but until i get this
>> resolved i cant find a weekday. For weekday i have used:
>> eir$week<- (eir$date)
>> eir$week<- weekdays(as.Date(eir$week))
>> class(eir$week)
>> eir$week<- as.factor(eir$week)
>> head(eir$week)
>>
>> Head of this eir$week results again as expected in <NA> but shows Levels:
>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>>
>> Not sure what i should do here. Kindly suggest.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>

	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Tue Jul 26 13:58:16 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 17:28:16 +0530
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
Message-ID: <CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>

Hello Again,

While i tried your solution as you suggested above it seems to be working.
Here is the output
temp<- dput(head(eir$date))
c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16")
however it still shows class(eir$date) as character and hence i cannot find
weekdays from this variable.

Sorry but i still dont understand in totality how R reads dates even though
have tried enough.

Regards, Shivi


On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:

> Thanks Duncan for the quick response. I will check again as you suggested.
> If that doesn't work i will share a reproducible example.
>
> Thanks again!!!!
>
> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>>
>>> Hi Team,
>>>
>>> This scenario may have come across a number of times however i checked
>>> nabble & SO and couldn't find a solution hence request assistance.
>>>
>>> I have a date variable in my data-set eir. The class of this var was
>>> character while i had read the file in r studio. Example of date -
>>> 05-30-16
>>>
>>> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y"). This
>>> converts it to a date variable. However when i check few obs
>>> with head(eir$date) all the results are <NA>.
>>>
>>
>> I think you don't have character data like that, because I see
>>
>> > as.Date("05-30-16", "%m-%d-%y")
>> [1] "2016-05-30"
>>
>> I'd guess eir$date is really a factor, because character data is
>> frequently changed to factor automatically.  If that's the case, this
>> should work for the conversion:
>>
>> as.Date(as.character(eir$date), "%m-%d-%y")
>>
>> If that doesn't work, you'll need to post something reproducible.
>>
>> Duncan Murdoch
>>
>> I also need to create weekdays from this date variable but until i get
>>> this
>>> resolved i cant find a weekday. For weekday i have used:
>>> eir$week<- (eir$date)
>>> eir$week<- weekdays(as.Date(eir$week))
>>> class(eir$week)
>>> eir$week<- as.factor(eir$week)
>>> head(eir$week)
>>>
>>> Head of this eir$week results again as expected in <NA> but shows Levels:
>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>>>
>>> Not sure what i should do here. Kindly suggest.
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Tue Jul 26 14:28:36 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 26 Jul 2016 07:28:36 -0500
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
Message-ID: <B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>

Hi, 

That eir$date might be a factor is irrelevant. There is an as.Date() method for factors, which does the factor to character coercion internally and then calls as.Date.character() on the result.

Using the example data below:

eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16", 
                           "05-30-16", "05-30-16", "05-30-16"))

> str(eir)
'data.frame':	6 obs. of  1 variable:
 $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1

> eir
      date
1 05-30-16
2 05-30-16
3 05-30-16
4 05-30-16
5 05-30-16
6 05-30-16

eir$date <- as.Date(eir$date, format = "%m-%d-%y")

> str(eir)
'data.frame':	6 obs. of  1 variable:
 $ date: Date, format: "2016-05-30" ...

> eir
        date
1 2016-05-30
2 2016-05-30
3 2016-05-30
4 2016-05-30
5 2016-05-30
6 2016-05-30

eir$days <- weekdays(eir$date)

> str(eir)
'data.frame':	6 obs. of  2 variables:
 $ date: Date, format: "2016-05-30" ...
 $ days: chr  "Monday" "Monday" "Monday" "Monday" ...

> eir
        date   days
1 2016-05-30 Monday
2 2016-05-30 Monday
3 2016-05-30 Monday
4 2016-05-30 Monday
5 2016-05-30 Monday
6 2016-05-30 Monday


I would check to be sure that you do not have any typos in your code.

Regards,

Marc Schwartz


> On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> 
> Hello Again,
> 
> While i tried your solution as you suggested above it seems to be working.
> Here is the output
> temp<- dput(head(eir$date))
> c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16")
> however it still shows class(eir$date) as character and hence i cannot find
> weekdays from this variable.
> 
> Sorry but i still dont understand in totality how R reads dates even though
> have tried enough.
> 
> Regards, Shivi
> 
> 
> On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> 
>> Thanks Duncan for the quick response. I will check again as you suggested.
>> If that doesn't work i will share a reproducible example.
>> 
>> Thanks again!!!!
>> 
>> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
>> wrote:
>> 
>>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>>> 
>>>> Hi Team,
>>>> 
>>>> This scenario may have come across a number of times however i checked
>>>> nabble & SO and couldn't find a solution hence request assistance.
>>>> 
>>>> I have a date variable in my data-set eir. The class of this var was
>>>> character while i had read the file in r studio. Example of date -
>>>> 05-30-16
>>>> 
>>>> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y"). This
>>>> converts it to a date variable. However when i check few obs
>>>> with head(eir$date) all the results are <NA>.
>>>> 
>>> 
>>> I think you don't have character data like that, because I see
>>> 
>>>> as.Date("05-30-16", "%m-%d-%y")
>>> [1] "2016-05-30"
>>> 
>>> I'd guess eir$date is really a factor, because character data is
>>> frequently changed to factor automatically.  If that's the case, this
>>> should work for the conversion:
>>> 
>>> as.Date(as.character(eir$date), "%m-%d-%y")
>>> 
>>> If that doesn't work, you'll need to post something reproducible.
>>> 
>>> Duncan Murdoch
>>> 
>>> I also need to create weekdays from this date variable but until i get
>>>> this
>>>> resolved i cant find a weekday. For weekday i have used:
>>>> eir$week<- (eir$date)
>>>> eir$week<- weekdays(as.Date(eir$week))
>>>> class(eir$week)
>>>> eir$week<- as.factor(eir$week)
>>>> head(eir$week)
>>>> 
>>>> Head of this eir$week results again as expected in <NA> but shows Levels:
>>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>>>> 
>>>> Not sure what i should do here. Kindly suggest.


From shivipmp82 at gmail.com  Tue Jul 26 14:41:46 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 18:11:46 +0530
Subject: [R] Date Time in R
In-Reply-To: <B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
Message-ID: <CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>

Thanks Marc for the help. this really helps.
I think there is some issue with the data saved in csv format for this
variable as when i checked:
str(eir$date)- this results in :-
Date[1:5327], format: NA NA NA NA NA.

Thanks again.

On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com> wrote:

> Hi,
>
> That eir$date might be a factor is irrelevant. There is an as.Date()
> method for factors, which does the factor to character coercion internally
> and then calls as.Date.character() on the result.
>
> Using the example data below:
>
> eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>                            "05-30-16", "05-30-16", "05-30-16"))
>
> > str(eir)
> 'data.frame':   6 obs. of  1 variable:
>  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>
> > eir
>       date
> 1 05-30-16
> 2 05-30-16
> 3 05-30-16
> 4 05-30-16
> 5 05-30-16
> 6 05-30-16
>
> eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>
> > str(eir)
> 'data.frame':   6 obs. of  1 variable:
>  $ date: Date, format: "2016-05-30" ...
>
> > eir
>         date
> 1 2016-05-30
> 2 2016-05-30
> 3 2016-05-30
> 4 2016-05-30
> 5 2016-05-30
> 6 2016-05-30
>
> eir$days <- weekdays(eir$date)
>
> > str(eir)
> 'data.frame':   6 obs. of  2 variables:
>  $ date: Date, format: "2016-05-30" ...
>  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>
> > eir
>         date   days
> 1 2016-05-30 Monday
> 2 2016-05-30 Monday
> 3 2016-05-30 Monday
> 4 2016-05-30 Monday
> 5 2016-05-30 Monday
> 6 2016-05-30 Monday
>
>
> I would check to be sure that you do not have any typos in your code.
>
> Regards,
>
> Marc Schwartz
>
>
> > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> >
> > Hello Again,
> >
> > While i tried your solution as you suggested above it seems to be
> working.
> > Here is the output
> > temp<- dput(head(eir$date))
> > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16")
> > however it still shows class(eir$date) as character and hence i cannot
> find
> > weekdays from this variable.
> >
> > Sorry but i still dont understand in totality how R reads dates even
> though
> > have tried enough.
> >
> > Regards, Shivi
> >
> >
> > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
> >
> >> Thanks Duncan for the quick response. I will check again as you
> suggested.
> >> If that doesn't work i will share a reproducible example.
> >>
> >> Thanks again!!!!
> >>
> >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> murdoch.duncan at gmail.com>
> >> wrote:
> >>
> >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> >>>
> >>>> Hi Team,
> >>>>
> >>>> This scenario may have come across a number of times however i checked
> >>>> nabble & SO and couldn't find a solution hence request assistance.
> >>>>
> >>>> I have a date variable in my data-set eir. The class of this var was
> >>>> character while i had read the file in r studio. Example of date -
> >>>> 05-30-16
> >>>>
> >>>> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y").
> This
> >>>> converts it to a date variable. However when i check few obs
> >>>> with head(eir$date) all the results are <NA>.
> >>>>
> >>>
> >>> I think you don't have character data like that, because I see
> >>>
> >>>> as.Date("05-30-16", "%m-%d-%y")
> >>> [1] "2016-05-30"
> >>>
> >>> I'd guess eir$date is really a factor, because character data is
> >>> frequently changed to factor automatically.  If that's the case, this
> >>> should work for the conversion:
> >>>
> >>> as.Date(as.character(eir$date), "%m-%d-%y")
> >>>
> >>> If that doesn't work, you'll need to post something reproducible.
> >>>
> >>> Duncan Murdoch
> >>>
> >>> I also need to create weekdays from this date variable but until i get
> >>>> this
> >>>> resolved i cant find a weekday. For weekday i have used:
> >>>> eir$week<- (eir$date)
> >>>> eir$week<- weekdays(as.Date(eir$week))
> >>>> class(eir$week)
> >>>> eir$week<- as.factor(eir$week)
> >>>> head(eir$week)
> >>>>
> >>>> Head of this eir$week results again as expected in <NA> but shows
> Levels:
> >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> >>>>
> >>>> Not sure what i should do here. Kindly suggest.
>
>

	[[alternative HTML version deleted]]


From paul.johnston at manchester.ac.uk  Tue Jul 26 15:05:35 2016
From: paul.johnston at manchester.ac.uk (Paul Johnston)
Date: Tue, 26 Jul 2016 13:05:35 +0000
Subject: [R] word stemming for corpus linguistics
In-Reply-To: <cfc318a0-002e-3dec-1638-88fdefddfdef@gmail.com>
References: <be27178e-5c35-b392-9471-645508768597@gmail.com>
	<EE85AFC56BE9E84EB86F7739169F1BD6A2485107@MBXP15.ds.man.ac.uk>
	<cfc318a0-002e-3dec-1638-88fdefddfdef@gmail.com>
Message-ID: <EE85AFC56BE9E84EB86F7739169F1BD6A248640E@MBXP15.ds.man.ac.uk>


Hi

I use the tm_map() with stemDocument used as an argument

Looking at a particular file before stemming

writeLines(as.character(data_mined_volatile[[1]]))

## The European Union is a "force for social injustice" which backs "the haves rather than the have-nots", Iain Duncan Smith has said.
## The ex-work and pensions secretary said "uncontrolled migration" drove down wages and increased the cost of living.
## He appealed to people "who may have done OK from the EU" to "think about the people that haven't".
## But Labour's Alan Johnson said the EU protected workers and stopped them from being "exploited".
## The former Labour home secretary accused the Leave campaign of dismissing such protections as "red tape".
## In other EU referendum campaign developments:
## Thirteen former US secretaries of state and defence and national security advisers, including Madeleine Albright and Leon Panetta, say in a letter to the Times that the UK's "place and influence" in the world would be diminished if it left the EU - and Europe would be "dangerously weakened"
## A British Chambers of Commerce survey suggests most business people back Remain but the gap with those backing Leave has narrowed.
## Five former heads of Nato claimed the UK would lose influence and "give succour to its enemies" by leaving the EU - claims dismissed as scaremongering by Boris Johnson
## Mr Corbyn is launching his party's battle bus, saying Labour votes will be crucial if the Remain side is to win
## The official Scottish campaign to keep the UK in the European Union is due to be launched in Edinburgh
## Mr Duncan Smith's speech came after he told the Sun Germany had a "de facto veto" over David Cameron's EU renegotiations, with Angela Merkel blocking the PM's plans for an "emergency brake" on EU migration.
## Downing Street said curbs it negotiated on in-work benefits for EU migrants were a "more effective" way forward.
## Follow the latest developments on BBC EU referendum live
## Laura Kuenssberg: Can Leave win over the have-nots


Now look at the same text after stemming

corpus <- data_mined_volatile
corpus <- tm_map(corpus,stemDocument)

writeLines(as.character(corpus[[1]]))

## The European Union is a "forc for social injustice" which back "the have rather than the have-nots", Iain Duncan Smith has said.
## The ex-work and pension secretari said "uncontrol migration" drove down wage and increas the cost of living.
## He appeal to peopl "who may have done OK from the EU" to "think about the peopl that haven't".
## But Labour Alan Johnson said the EU protect worker and stop them from be "exploited".
## The former Labour home secretari accus the Leav campaign of dismiss such protect as "red tape".
2
## In other EU referendum campaign developments:
## Thirteen former US secretari of state and defenc and nation secur advisers, includ Madelein Albright and Leon Panetta, say in a letter to the Time that the UK "place and influence" in the world would be diminish if it left the EU - and Europ would be "danger weakened"
## A British Chamber of Commerc survey suggest most busi peopl back Remain but the gap with those back Leav has narrowed.
## Five former head of Nato claim the UK would lose influenc and "give succour to it enemies" by leav the EU - claim dismiss as scaremong by Bori Johnson
## Mr Corbyn is launch his parti battl bus, say Labour vote will be crucial if the Remain side is to win
## The offici Scottish campaign to keep the UK in the European Union is due to be launch in Edinburgh
## Mr Duncan Smith speech came after he told the Sun Germani had a "de facto veto" over David Cameron EU renegotiations, with Angela Merkel block the PM plan for an "emerg brake" on EU migration.
## Down Street said curb it negoti on in-work benefit for EU migrant were a "more effective" way forward.
## Follow the latest develop on BBC EU referendum live
## Laura Kuenssberg: Can Leav win over the have-not

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Andy Wolfe
Sent: 26 July 2016 09:14
To: r-help at r-project.org
Subject: Re: [R] word stemming for corpus linguistics

Hi Paul

I have seen this - it's part of the tm package mentioned originally. So, I've tried it again and perhaps I'm using stemDocument incorrectly, but this is what I am doing:

# > library(tm)
Loading required package: NLP
 > text.v <- scan(file.choose(), what = 'char', sep = '\n') Read 938 items # >text.stem.v <- stemDocument(text.v, language = 'english')

But it isn't changing anything in the body of the text I'm passing to it
- the words are unlemmatized/ unstemmed.

When I try using SnowballC, the error returned is that tm_map doesn't have a method to work with objects of class 'character'.

Again, the problem is that tm doesn't seem to allow for concordance analysis ... or perhaps it does and I just haven't figured out how to do it, so am happy to be shown some documentation on that process, and whether that is applied before or after the text is transformed into a DTM because searching on-line hasn't (yet) thrown anything back.

Thanks.
Andy


On 26/07/16 08:50, Paul Johnston wrote:
> Suggest look at 
> http://www.inside-r.org/packages/cran/tm/docs/stemDocument
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Andy 
> Wolfe
> Sent: 26 July 2016 08:10
> To: r-help at r-project.org
> Subject: [R] word stemming for corpus linguistics
>
> Hi list
>
> On a piece of work I'm doing in corpus linguistics, using a combo of texts by Gries "Quantitative Corpus Linguistics with R: A Practical Introduction" and Jockers "Text Analysis with R for Students of Literature", which are both really excellent by the way, I want to stem or lemmatize the words so that, for e.g., 'facilitating', 'facilitated', and 'facilitates' all become 'facilit'.
>
> In text mining, using a combination of the packages 'tm' and 'SnowballC'
> this is feasible, but then I am finding that working with the DTM (document term matrix) becomes difficult for when I want to do concordance (or key word in context) analysis.
>
> So, two questions:
>
> (1) is there a package for R version 3.3.1 that can work with corpus 
> linguistics? and/ or
>
> (2) is there a way of doing concordance analysis using the tm package as part of the whole text mining process?
>
> I appreciate any help. Thanks.
>
> Andy
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From G.Maubach at gmx.de  Tue Jul 26 15:46:17 2016
From: G.Maubach at gmx.de (G.Maubach at gmx.de)
Date: Tue, 26 Jul 2016 15:46:17 +0200
Subject: [R] Error when installing packages
Message-ID: <trinity-88626132-f959-43b0-b249-bd6f2fe36816-1469540777226@3capp-gmx-bs76>

Hi All,

I try to install packages on Debian GNU Linux 8 (Kernel 3.16.0-4-amd64).

My sessionInfo() is

R version 3.3.1 (2016-06-21)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux 8 (jessie)

locale:
 [1] LC_CTYPE=de_DE.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=de_DE.UTF-8        LC_COLLATE=de_DE.UTF-8    
 [5] LC_MONETARY=de_DE.UTF-8    LC_MESSAGES=de_DE.UTF-8   
 [7] LC_PAPER=de_DE.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] tools_3.3.1

Installing the following packages

Warning in install.packages :
  packages ?excel.link?, ?installr? are not available (for R version 3.3.1)
Warning in install.packages :
  dependencies ?latticist?, ?graph?, ?RBGL?, ?pkgDepTools?, ?Rgraphviz? are not available
also installing the dependencies ?RCurl?, ?RWekajars?

results in the following messages:

(1)
* installing *source* package ?RCurl? ...
checking for curl-config... no
Cannot find curl-config

(2)
* installing *source* package ?RWekajars? ...
./configure: 1: ./configure: /usr/lib/jvm/default-java/jre/bin/java: not found
./configure: 50: test: -ge: unexpected operator
./configure: 51: test: -eq: unexpected operator
Need at least Java version 1.6/6.0.
ERROR: configuration failed for package ?RWekajars?

Annotation: I have openjdk-8-jre installed.

(3)
* installing *source* package ?cairoDevice? ...
ERROR: gtk+2. not found by pkg-config.
ERROR: configuration failed for package ?cairoDevice?

(4)
* installing *source* package ?rgdal? ...
configure: CC: gcc -std=gnu99
configure: CXX: g++
configure: rgdal: 1.1-10
checking for /usr/bin/svnversion... no
configure: svn revision: 622
checking for gdal-config... no
no
configure: error: gdal-config not found or not executable.
ERROR: configuration failed for package ?rgdal?

(5)
* installing *source* package ?rgeos? ...
configure: CC: gcc -std=gnu99
configure: CXX: g++
configure: rgeos: 0.3-19
checking for /usr/bin/svnversion... no
configure: svn revision: 524
checking for geos-config... no
no
configure: error: geos-config not found or not executable.
ERROR: configuration failed for package ?rgeos?

... and much more.

Do all these error messages have something in common?

How could I fix the installation?

Kind regards

Georg


From mlathouri at yahoo.gr  Tue Jul 26 15:47:58 2016
From: mlathouri at yahoo.gr (Maria Lathouri)
Date: Tue, 26 Jul 2016 13:47:58 +0000 (UTC)
Subject: [R] change the colour line in gamm4 plotting
References: <1888888338.8562589.1469540878724.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1888888338.8562589.1469540878724.JavaMail.yahoo@mail.yahoo.com>

Dear all 

I am stuck probably in a simple plotting question

My model is:model.1<-gamm4(y~s(x1, by=end.group)+Year+K1+k2+k3, data=.., random=~(1|WB_ID/Site_ID))
where y is my dependent variable, x1 is the smooth covariate and I use the by argument for the smooth term based on six different groups, and then the Year, k1, k2, k3 are explanatory fixed variables

I use the plot command to plot the model and I get six different plots:plot(model.1$gam, pages=1, xlab=" ", ylab=" ")
I would like to change the colour of the fitted and the standard error lines, from black which is the default to another colour; I tried to use the col=" " function, at least for the main line, in the plot command but it is not working. 

Any advice/suggestion is welcome.
Many thanks.Maria

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Jul 26 15:52:09 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 26 Jul 2016 06:52:09 -0700
Subject: [R] knitr package error.
In-Reply-To: <CADVq9WVUbbpc2_LX9wjX+KzO8g6Uni1q01PN9BO_TYev0OBkbA@mail.gmail.com>
References: <CADVq9WVUbbpc2_LX9wjX+KzO8g6Uni1q01PN9BO_TYev0OBkbA@mail.gmail.com>
Message-ID: <7E97B11E-4FD9-420A-B83A-569932037D9E@dcn.davis.ca.us>

The mailing list allows very few types of attachments through to limit virus problems. You need to learn how to convey your problem as a reproducible sequence of R statements to get clear assistance here. [1]

In this case,  you may be confused between the interactive working environment (variables) and the environment set up by knitr for generating the TeX document. You need to put every statement that is needed to create your variables in the knitr document, directly or by using the source or library functions. Variables you create interactively are not available to knitr. This is a key step to making your documents reproducible, much like your example code you should be sending here with your questions. 

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

-- 
Sent from my phone. Please excuse my brevity.

On July 26, 2016 1:27:49 AM PDT, Shadrack Magut <skmagut at student.maseno.ac.ke> wrote:
>hello, someone help me sort out this error "## Error in summary(data):
>object ?analysis? not found"" in using Knitr package in R. I imported
>the
>data into R studio but as soon as i use any code involving the data i
>imported in the Knitr environment within the latex document i encounter
>the
>above error.Kindly help me sort out this error.
>
>Attached to this mail is a word document containing the screenshot of
>my R
>codes and output error.
>
>Regards
>shadrack.?
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Tue Jul 26 16:28:17 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 26 Jul 2016 14:28:17 +0000
Subject: [R] Date Time in R
Message-ID: <38f2b35a2ac6465db2add0fbb16dab4b@exch-2p-mbx-t2.ads.tamu.edu>

What does this produce?

> readLines("YourCSVfilename.csv", n=5)

If the data are in Excel, the date format used in .csv files is not always in the same as the format used when viewing dates in the spreadsheet.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi Bhatia
Sent: Tuesday, July 26, 2016 7:42 AM
To: Marc Schwartz
Cc: R-help
Subject: Re: [R] Date Time in R

Thanks Marc for the help. this really helps.
I think there is some issue with the data saved in csv format for this
variable as when i checked:
str(eir$date)- this results in :-
Date[1:5327], format: NA NA NA NA NA.

Thanks again.

On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com> wrote:

> Hi,
>
> That eir$date might be a factor is irrelevant. There is an as.Date()
> method for factors, which does the factor to character coercion internally
> and then calls as.Date.character() on the result.
>
> Using the example data below:
>
> eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>                            "05-30-16", "05-30-16", "05-30-16"))
>
> > str(eir)
> 'data.frame':   6 obs. of  1 variable:
>  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>
> > eir
>       date
> 1 05-30-16
> 2 05-30-16
> 3 05-30-16
> 4 05-30-16
> 5 05-30-16
> 6 05-30-16
>
> eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>
> > str(eir)
> 'data.frame':   6 obs. of  1 variable:
>  $ date: Date, format: "2016-05-30" ...
>
> > eir
>         date
> 1 2016-05-30
> 2 2016-05-30
> 3 2016-05-30
> 4 2016-05-30
> 5 2016-05-30
> 6 2016-05-30
>
> eir$days <- weekdays(eir$date)
>
> > str(eir)
> 'data.frame':   6 obs. of  2 variables:
>  $ date: Date, format: "2016-05-30" ...
>  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>
> > eir
>         date   days
> 1 2016-05-30 Monday
> 2 2016-05-30 Monday
> 3 2016-05-30 Monday
> 4 2016-05-30 Monday
> 5 2016-05-30 Monday
> 6 2016-05-30 Monday
>
>
> I would check to be sure that you do not have any typos in your code.
>
> Regards,
>
> Marc Schwartz
>
>
> > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> >
> > Hello Again,
> >
> > While i tried your solution as you suggested above it seems to be
> working.
> > Here is the output
> > temp<- dput(head(eir$date))
> > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16")
> > however it still shows class(eir$date) as character and hence i cannot
> find
> > weekdays from this variable.
> >
> > Sorry but i still dont understand in totality how R reads dates even
> though
> > have tried enough.
> >
> > Regards, Shivi
> >
> >
> > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
> >
> >> Thanks Duncan for the quick response. I will check again as you
> suggested.
> >> If that doesn't work i will share a reproducible example.
> >>
> >> Thanks again!!!!
> >>
> >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> murdoch.duncan at gmail.com>
> >> wrote:
> >>
> >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> >>>
> >>>> Hi Team,
> >>>>
> >>>> This scenario may have come across a number of times however i checked
> >>>> nabble & SO and couldn't find a solution hence request assistance.
> >>>>
> >>>> I have a date variable in my data-set eir. The class of this var was
> >>>> character while i had read the file in r studio. Example of date -
> >>>> 05-30-16
> >>>>
> >>>> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y").
> This
> >>>> converts it to a date variable. However when i check few obs
> >>>> with head(eir$date) all the results are <NA>.
> >>>>
> >>>
> >>> I think you don't have character data like that, because I see
> >>>
> >>>> as.Date("05-30-16", "%m-%d-%y")
> >>> [1] "2016-05-30"
> >>>
> >>> I'd guess eir$date is really a factor, because character data is
> >>> frequently changed to factor automatically.  If that's the case, this
> >>> should work for the conversion:
> >>>
> >>> as.Date(as.character(eir$date), "%m-%d-%y")
> >>>
> >>> If that doesn't work, you'll need to post something reproducible.
> >>>
> >>> Duncan Murdoch
> >>>
> >>> I also need to create weekdays from this date variable but until i get
> >>>> this
> >>>> resolved i cant find a weekday. For weekday i have used:
> >>>> eir$week<- (eir$date)
> >>>> eir$week<- weekdays(as.Date(eir$week))
> >>>> class(eir$week)
> >>>> eir$week<- as.factor(eir$week)
> >>>> head(eir$week)
> >>>>
> >>>> Head of this eir$week results again as expected in <NA> but shows
> Levels:
> >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> >>>>
> >>>> Not sure what i should do here. Kindly suggest.
>
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From phaedrusv at gmail.com  Tue Jul 26 16:28:31 2016
From: phaedrusv at gmail.com (Andy Wolfe)
Date: Tue, 26 Jul 2016 15:28:31 +0100
Subject: [R] word stemming for corpus linguistics
In-Reply-To: <EE85AFC56BE9E84EB86F7739169F1BD6A248640E@MBXP15.ds.man.ac.uk>
References: <be27178e-5c35-b392-9471-645508768597@gmail.com>
	<EE85AFC56BE9E84EB86F7739169F1BD6A2485107@MBXP15.ds.man.ac.uk>
	<cfc318a0-002e-3dec-1638-88fdefddfdef@gmail.com>
	<EE85AFC56BE9E84EB86F7739169F1BD6A248640E@MBXP15.ds.man.ac.uk>
Message-ID: <14c42485-d8a8-3841-a1f8-0987427642ea@gmail.com>

Hi

Thanks for following up on this thread.

I've opted for this, albeit circuitous, route: use the tm package to 
stem the document and then use writeCorpus to write the stemmed document 
to disk, so that I can open it up and do the concordancing piece.

Many thanks - this'll do me fine until I come across a better (read, 
more elegant) solution.
Best
Andy


On 26/07/16 14:05, Paul Johnston wrote:
> Hi
>
> I use the tm_map() with stemDocument used as an argument
>
> Looking at a particular file before stemming
>
> writeLines(as.character(data_mined_volatile[[1]]))
>
> ## The European Union is a "force for social injustice" which backs "the haves rather than the have-nots", Iain Duncan Smith has said.
> ## The ex-work and pensions secretary said "uncontrolled migration" drove down wages and increased the cost of living.
> ## He appealed to people "who may have done OK from the EU" to "think about the people that haven't".
> ## But Labour's Alan Johnson said the EU protected workers and stopped them from being "exploited".
> ## The former Labour home secretary accused the Leave campaign of dismissing such protections as "red tape".
> ## In other EU referendum campaign developments:
> ## Thirteen former US secretaries of state and defence and national security advisers, including Madeleine Albright and Leon Panetta, say in a letter to the Times that the UK's "place and influence" in the world would be diminished if it left the EU - and Europe would be "dangerously weakened"
> ## A British Chambers of Commerce survey suggests most business people back Remain but the gap with those backing Leave has narrowed.
> ## Five former heads of Nato claimed the UK would lose influence and "give succour to its enemies" by leaving the EU - claims dismissed as scaremongering by Boris Johnson
> ## Mr Corbyn is launching his party's battle bus, saying Labour votes will be crucial if the Remain side is to win
> ## The official Scottish campaign to keep the UK in the European Union is due to be launched in Edinburgh
> ## Mr Duncan Smith's speech came after he told the Sun Germany had a "de facto veto" over David Cameron's EU renegotiations, with Angela Merkel blocking the PM's plans for an "emergency brake" on EU migration.
> ## Downing Street said curbs it negotiated on in-work benefits for EU migrants were a "more effective" way forward.
> ## Follow the latest developments on BBC EU referendum live
> ## Laura Kuenssberg: Can Leave win over the have-nots
>
>
> Now look at the same text after stemming
>
> corpus <- data_mined_volatile
> corpus <- tm_map(corpus,stemDocument)
>
> writeLines(as.character(corpus[[1]]))
>
> ## The European Union is a "forc for social injustice" which back "the have rather than the have-nots", Iain Duncan Smith has said.
> ## The ex-work and pension secretari said "uncontrol migration" drove down wage and increas the cost of living.
> ## He appeal to peopl "who may have done OK from the EU" to "think about the peopl that haven't".
> ## But Labour Alan Johnson said the EU protect worker and stop them from be "exploited".
> ## The former Labour home secretari accus the Leav campaign of dismiss such protect as "red tape".
> 2
> ## In other EU referendum campaign developments:
> ## Thirteen former US secretari of state and defenc and nation secur advisers, includ Madelein Albright and Leon Panetta, say in a letter to the Time that the UK "place and influence" in the world would be diminish if it left the EU - and Europ would be "danger weakened"
> ## A British Chamber of Commerc survey suggest most busi peopl back Remain but the gap with those back Leav has narrowed.
> ## Five former head of Nato claim the UK would lose influenc and "give succour to it enemies" by leav the EU - claim dismiss as scaremong by Bori Johnson
> ## Mr Corbyn is launch his parti battl bus, say Labour vote will be crucial if the Remain side is to win
> ## The offici Scottish campaign to keep the UK in the European Union is due to be launch in Edinburgh
> ## Mr Duncan Smith speech came after he told the Sun Germani had a "de facto veto" over David Cameron EU renegotiations, with Angela Merkel block the PM plan for an "emerg brake" on EU migration.
> ## Down Street said curb it negoti on in-work benefit for EU migrant were a "more effective" way forward.
> ## Follow the latest develop on BBC EU referendum live
> ## Laura Kuenssberg: Can Leav win over the have-not
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Andy Wolfe
> Sent: 26 July 2016 09:14
> To: r-help at r-project.org
> Subject: Re: [R] word stemming for corpus linguistics
>
> Hi Paul
>
> I have seen this - it's part of the tm package mentioned originally. So, I've tried it again and perhaps I'm using stemDocument incorrectly, but this is what I am doing:
>
> # > library(tm)
> Loading required package: NLP
>   > text.v <- scan(file.choose(), what = 'char', sep = '\n') Read 938 items # >text.stem.v <- stemDocument(text.v, language = 'english')
>
> But it isn't changing anything in the body of the text I'm passing to it
> - the words are unlemmatized/ unstemmed.
>
> When I try using SnowballC, the error returned is that tm_map doesn't have a method to work with objects of class 'character'.
>
> Again, the problem is that tm doesn't seem to allow for concordance analysis ... or perhaps it does and I just haven't figured out how to do it, so am happy to be shown some documentation on that process, and whether that is applied before or after the text is transformed into a DTM because searching on-line hasn't (yet) thrown anything back.
>
> Thanks.
> Andy
>
>
> On 26/07/16 08:50, Paul Johnston wrote:
>> Suggest look at
>> http://www.inside-r.org/packages/cran/tm/docs/stemDocument
>>
>>
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Andy
>> Wolfe
>> Sent: 26 July 2016 08:10
>> To: r-help at r-project.org
>> Subject: [R] word stemming for corpus linguistics
>>
>> Hi list
>>
>> On a piece of work I'm doing in corpus linguistics, using a combo of texts by Gries "Quantitative Corpus Linguistics with R: A Practical Introduction" and Jockers "Text Analysis with R for Students of Literature", which are both really excellent by the way, I want to stem or lemmatize the words so that, for e.g., 'facilitating', 'facilitated', and 'facilitates' all become 'facilit'.
>>
>> In text mining, using a combination of the packages 'tm' and 'SnowballC'
>> this is feasible, but then I am finding that working with the DTM (document term matrix) becomes difficult for when I want to do concordance (or key word in context) analysis.
>>
>> So, two questions:
>>
>> (1) is there a package for R version 3.3.1 that can work with corpus
>> linguistics? and/ or
>>
>> (2) is there a way of doing concordance analysis using the tm package as part of the whole text mining process?
>>
>> I appreciate any help. Thanks.
>>
>> Andy
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ulrik.stervbo at gmail.com  Tue Jul 26 16:29:07 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Tue, 26 Jul 2016 14:29:07 +0000
Subject: [R] Error when installing packages
In-Reply-To: <trinity-88626132-f959-43b0-b249-bd6f2fe36816-1469540777226@3capp-gmx-bs76>
References: <trinity-88626132-f959-43b0-b249-bd6f2fe36816-1469540777226@3capp-gmx-bs76>
Message-ID: <CAKVAULP8ROp53EdT8PK4StEqPKTgPz9MvFFxsxUrrizY6rg_xQ@mail.gmail.com>

Hi Georg,

excel.link and installr: as far as I can tell from CRAN, excel.link and
installr are for windows OS only
1-5: you are installing packages that are wrappers to system
function/programms. They must be present on your system first.

2: Maybe Weka is missing in the path? Is your java excutable really at
'/usr/lib/jvm/default-java/jre/bin/java'. Maybe you need to tell R where to
find Java

I always install things like Rgraphviz from the ubuntu repository.

Hope this helps,
Ulrik


On Tue, 26 Jul 2016 at 16:09 <G.Maubach at gmx.de> wrote:

> Hi All,
>
> I try to install packages on Debian GNU Linux 8 (Kernel 3.16.0-4-amd64).
>
> My sessionInfo() is
>
> R version 3.3.1 (2016-06-21)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Debian GNU/Linux 8 (jessie)
>
> locale:
>  [1] LC_CTYPE=de_DE.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=de_DE.UTF-8        LC_COLLATE=de_DE.UTF-8
>  [5] LC_MONETARY=de_DE.UTF-8    LC_MESSAGES=de_DE.UTF-8
>  [7] LC_PAPER=de_DE.UTF-8       LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] tools_3.3.1
>
> Installing the following packages
>
> Warning in install.packages :
>   packages ?excel.link?, ?installr? are not available (for R version 3.3.1)
> Warning in install.packages :
>   dependencies ?latticist?, ?graph?, ?RBGL?, ?pkgDepTools?, ?Rgraphviz?
> are not available
> also installing the dependencies ?RCurl?, ?RWekajars?
>
> results in the following messages:
>
> (1)
> * installing *source* package ?RCurl? ...
> checking for curl-config... no
> Cannot find curl-config
>
> (2)
> * installing *source* package ?RWekajars? ...
> ./configure: 1: ./configure: /usr/lib/jvm/default-java/jre/bin/java: not
> found
> ./configure: 50: test: -ge: unexpected operator
> ./configure: 51: test: -eq: unexpected operator
> Need at least Java version 1.6/6.0.
> ERROR: configuration failed for package ?RWekajars?
>
> Annotation: I have openjdk-8-jre installed.
>
> (3)
> * installing *source* package ?cairoDevice? ...
> ERROR: gtk+2. not found by pkg-config.
> ERROR: configuration failed for package ?cairoDevice?
>
> (4)
> * installing *source* package ?rgdal? ...
> configure: CC: gcc -std=gnu99
> configure: CXX: g++
> configure: rgdal: 1.1-10
> checking for /usr/bin/svnversion... no
> configure: svn revision: 622
> checking for gdal-config... no
> no
> configure: error: gdal-config not found or not executable.
> ERROR: configuration failed for package ?rgdal?
>
> (5)
> * installing *source* package ?rgeos? ...
> configure: CC: gcc -std=gnu99
> configure: CXX: g++
> configure: rgeos: 0.3-19
> checking for /usr/bin/svnversion... no
> configure: svn revision: 524
> checking for geos-config... no
> no
> configure: error: geos-config not found or not executable.
> ERROR: configuration failed for package ?rgeos?
>
> ... and much more.
>
> Do all these error messages have something in common?
>
> How could I fix the installation?
>
> Kind regards
>
> Georg
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Tue Jul 26 16:34:29 2016
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Tue, 26 Jul 2016 16:34:29 +0200
Subject: [R] Visualization of a Convex Hull in R (Possibly with RGL)
In-Reply-To: <CAAcGz99mdFhTcPE0JkBW6pGAJBjg010weTPCiXy+9JZuDjNOEA@mail.gmail.com>
References: <20160726102534.GA10914@chicca2>
	<CAAcGz99mdFhTcPE0JkBW6pGAJBjg010weTPCiXy+9JZuDjNOEA@mail.gmail.com>
Message-ID: <20160726143429.GC10914@chicca2>

On Tue, Jul 26, 2016 at 10:48:22AM +0000, Michael Sumner wrote:
>On Tue, 26 Jul 2016 at 20:29 Lorenzo Isella <lorenzo.isella at gmail.com>
>wrote:
>
>> Dear All,
>> I am not an expert about the calculation and visualization of convex
>> hulls, but I am trying to do something relatively simple.
>> Please consider the snippet at the end of the email.
>> The array pts  represents the position of (the centres of) a set of
>> spheres in 3D (whose radius is 0.5).
>> I found the geometry package which provides bindings to the qhull
>> (http://www.qhull.org/) library and allows me to calculate the convex
>> hull of pts and some interesting properties, like the area and the
>> volume of the convex hull.
>> That is all fine, but I would like to represent the results in 3D.
>> I would like to see a set of spheres with the given position in pts
>> and the radius R=0.5 and the convex hull going based on the
>> coordinates of pts.
>> Does anybody know how to achieve that?
>> Many thanks
>>
>> Lorenzo
>>
>>
>>
>> ####################################################################
>>
>>
>> library(geometry)
>>
>>
>>
>> pts <- read.table("test-agg.dat")
>>
>> ## names(pts) <- c("x", "y", "z")
>>
>>
>> pts <- structure(list(x = c(0.145138112090815, 0.880429452229448,
>> -1.66682740805663,
>> -0.955356943224252, 1.09396798448412, -2.63620653885452,
>> 0.190270563436841,
>> 2.77638842095489, -0.0914977922901252, -0.484087722062158,
>> 0.674992784754569,
>> 1.02571108118502, 2.78789154912298, -0.146882388586427,
>> -1.71507089246001,
>> -1.87886026272455, 6.61540228778138, 7.46818822627362,
>> 4.80777628045963,
>> 6.35487269602107, 4.70026626314108, 3.44366671298125,
>> 2.92460648354883,
>> 5.07053866161192, 7.2164343710271, 6.93292242981006, 7.84238243682145,
>> 7.4321237311656, 9.59692827237335, 8.3454086671306, 6.574251622581,
>> 6.93340304175759), y = c(-0.189309482569001, 1.67057595730283,
>> -0.995542605143325, 2.31599069771684, 1.79876139786291,
>> 0.165886909346058,
>> 2.70740725397259, 1.70535678189514, -0.819087329147786,
>> 0.967935667739331,
>> -2.53860079551206, -1.60932884056828, -1.42050583991613,
>> -2.36883245674979,
>> 0.191848867151458, -1.58255618338079, 1.98324724741419,
>> 3.0095048680572,
>> 1.97159273200079, 2.26459561200295, 0.4461804374566, 1.87939977307282,
>> 1.98510457359889, -0.103495422088799, -1.32050185858493,
>> 0.584580736435273,
>> -2.97562362406436, 0.106204178143333, -2.0843758994948,
>> -1.1492729389287,
>> -2.13797391772643, -4.45916649729404), z = c(-1.44428170702261,
>> -1.45742688370091, -1.70267889327056, -1.93637881287001,
>> -3.4452409532781,
>> -3.02436538816822, 0.114790290814684, -2.10208878117278,
>> 2.07425243689128,
>> 1.26652602551291, 1.39914827137784, -0.345006925422662,
>> 0.596828021941431,
>> 3.351773622867, 2.68408561840144, 3.97006405709929, 0.82767367646934,
>> -0.662142231346811, 1.68344957582882, 2.92819854377685,
>> 0.386683699222387,
>> -0.220305098209874, 2.37510769001993, -1.51041233970289,
>> -0.707073219742548,
>> -1.24585080403725, -1.63914669343685, 0.683153891726357,
>> -1.26623658129696,
>> 1.95073173465968, -2.94804638502708, -0.635785458903106)), .Names =
>> c("x",
>> "y", "z"), class = "data.frame", row.names = c(NA, -32L))
>>
>> res<-convhulln(pts, options = "FA")
>>
>>
>In short
>
>library(rgl)
>bg3d("grey")
>spheres3d(pts, radius = 0.5, col = "white")
>## triangle functions generally expect triplets of points, one after another
>## and hull/triangulation functions generally return arrays of indexes
>## so transpose index of hull is the right order to draw triangles
>triangles3d(pts[t(res$hull), ], col = "firebrick", alpha = 0.3)
>
>(But, I wonder if you mean to find the convex hull based on the outer shell
>of those spheres? )
>
>Cheers, Mike.
>



Dear Mike,
Thanks a lot, this is spot on.
Yep, if possible I would even prefer to have the convex hull based on
the outer shell of the spheres, but I wonder if that is easy to
achieve with a few lines of code.
Any ideas would be welcome.
Cheers

Lorenzo


From justinthong93 at gmail.com  Tue Jul 26 17:05:31 2016
From: justinthong93 at gmail.com (Justin Thong)
Date: Tue, 26 Jul 2016 16:05:31 +0100
Subject: [R] Linear Dependance of Model Matrix and How Fitted/ Sums of
	Squares Follow
Message-ID: <CAEtAGepWdWknos+JZQsMeCJtc7CAJzztubPx_ZxAY+Zv3ZPa9Q@mail.gmail.com>

Below is the covariates for a model ~x1+x2+x3+x4+x5+x6. I noticed that when
fitting this model that the coefficient x6 is unestimable.*Is this merely a
case that adding more columns to my model matrix will eventually lead to
linear dependance so the more terms I have in the model formulae the more
likely the model matrix becomes linearly dependant?*   I found that for
model formulae ~x1+x2+x3+x4+x5, all the coefficients are estimable so I
guess this example supports my statement.

But that being said, since not all coefficients are estimated then how does
R compute the fitted values and anova table. *Does it just ignore the
existence of x6 and consider the model to be ~x1+x2+x3+x4+x5? Or is there
something deeper that I do not understand.* Because the sums of squares and
fitted seem to be the same for model ~x1+x2+x3+x4+x5 as it is for
~x1+x2+x3+x4+x5+x6

However, this is not so clear cut for model with factors. Because factors
are only represented by a parameter for each level in the model matrix.
Consider factor F with 2 levels and G with 3 levels. The problem is that R
has a way of excluding certain rows from the anova table. Again, it can be
seen that it excludes the rows associated with the parameters which are not
estimable, but this is not absolutely clear in my mind.Look at small
example below for model ~F*G for two factors. As you can see, the
interaction parameters are not estimable ie F2:G2 and F2:G3. Now from what
I was told, F1 and G1 is contained within the (Intercept) parameter so
F1:G1, F1:G2, F2:G1 are not considered. You can see from the anova table
that the interaction row F:G is ignored. My main problem is why is it
ignored.
*Does that mean that if all the parameters (excluding the ones asasociated
with intercept) that is associated with a particular term is unestimable
then the row of that term in the anova table is ignored? How many
 unestimable parameters must there be for the row of a term to be
ignored? *Because
If the answer to the second question is to calculate fitted values and sums
of squares by ignoring unestimable parameters, then it means that the rows
of sums of squares disappear for a different reason other than
unestimability.

Sorry for the generally wordy question. I may not be thinking of it in the
correct manner and I would appreciate if anyone has an answer and perhaps
even some generalisations towards the use of QR decomposition.

(There is more code below this data)

 x1 x2 x3 x4 x5 x6
1   12  0  0  0  0  0
2   12  0  0  0  0  0
3   12  0  0  0  0  0
4   12  0  0  0  0  0
5    0 12  0  0  0  0
6    0 12  0  0  0  0
7    0 12  0  0  0  0
8    0 12  0  0  0  0
9    0  0 12  0  0  0
10   0  0 12  0  0  0
11   0  0 12  0  0  0
12   0  0 12  0  0  0
13   0  0  0 12  0  0
14   0  0  0 12  0  0
15   0  0  0 12  0  0
16   0  0  0 12  0  0
17   0  0  0  0 12  0
18   0  0  0  0 12  0
19   0  0  0  0 12  0
20   0  0  0  0 12  0
21   0  0  0  0  0 12
22   0  0  0  0  0 12
23   0  0  0  0  0 12
24   0  0  0  0  0 12
25   6  6  0  0  0  0
26   6  6  0  0  0  0
27   6  6  0  0  0  0
28   6  6  0  0  0  0
29   6  0  6  0  0  0
30   6  0  6  0  0  0
31   6  0  6  0  0  0
32   6  0  6  0  0  0
33   6  0  0  6  0  0
34   6  0  0  6  0  0
35   6  0  0  6  0  0
36   6  0  0  6  0  0
37   6  0  0  0  6  0
38   6  0  0  0  6  0
39   6  0  0  0  6  0
40   6  0  0  0  6  0
41   6  0  0  0  0  6
42   6  0  0  0  0  6
43   6  0  0  0  0  6
44   6  0  0  0  0  6
45   0  6  6  0  0  0
46   0  6  6  0  0  0
47   0  6  6  0  0  0
48   0  6  6  0  0  0
49   0  6  0  6  0  0
50   0  6  0  6  0  0
51   0  6  0  6  0  0
52   0  6  0  6  0  0
53   0  6  0  0  6  0
54   0  6  0  0  6  0
55   0  6  0  0  6  0
56   0  6  0  0  6  0
57   0  6  0  0  0  6
58   0  6  0  0  0  6
59   0  6  0  0  0  6
60   0  6  0  0  0  6
61   0  0  6  6  0  0
62   0  0  6  6  0  0
63   0  0  6  6  0  0
64   0  0  6  6  0  0
65   0  0  6  0  6  0
66   0  0  6  0  6  0
67   0  0  6  0  6  0
68   0  0  6  0  6  0
69   0  0  6  0  0  6
70   0  0  6  0  0  6
71   0  0  6  0  0  6
72   0  0  6  0  0  6
73   0  0  0  6  6  0
74   0  0  0  6  6  0
75   0  0  0  6  6  0
76   0  0  0  6  6  0
77   0  0  0  6  0  6
78   0  0  0  6  0  6
79   0  0  0  6  0  6
80   0  0  0  6  0  6
81   0  0  0  0  6  6
82   0  0  0  0  6  6
83   0  0  0  0  6  6
84   0  0  0  0  6  6
85   4  4  4  0  0  0
86   4  4  4  0  0  0
87   4  4  4  0  0  0
88   4  4  4  0  0  0
89   4  4  0  4  0  0
90   4  4  0  4  0  0
91   4  4  0  4  0  0
92   4  4  0  4  0  0
93   4  4  0  0  4  0
94   4  4  0  0  4  0
95   4  4  0  0  4  0
96   4  4  0  0  4  0
97   4  4  0  0  0  4
98   4  4  0  0  0  4
99   4  4  0  0  0  4
100  4  4  0  0  0  4
101  4  0  4  4  0  0
102  4  0  4  4  0  0
103  4  0  4  4  0  0
104  4  0  4  4  0  0
105  4  0  4  0  4  0
106  4  0  4  0  4  0
107  4  0  4  0  4  0
108  4  0  4  0  4  0
109  4  0  4  0  0  4
110  4  0  4  0  0  4
111  4  0  4  0  0  4
112  4  0  4  0  0  4
113  4  0  0  4  4  0
114  4  0  0  4  4  0
115  4  0  0  4  4  0
116  4  0  0  4  4  0
117  4  0  0  4  0  4
118  4  0  0  4  0  4
119  4  0  0  4  0  4
120  4  0  0  4  0  4
121  4  0  0  0  4  4
122  4  0  0  0  4  4
123  4  0  0  0  4  4
124  4  0  0  0  4  4
125  0  4  4  4  0  0
126  0  4  4  4  0  0
127  0  4  4  4  0  0
128  0  4  4  4  0  0
129  0  4  4  0  4  0
130  0  4  4  0  4  0
131  0  4  4  0  4  0
132  0  4  4  0  4  0
133  0  4  4  0  0  4
134  0  4  4  0  0  4
135  0  4  4  0  0  4
136  0  4  4  0  0  4
137  0  4  0  4  4  0
138  0  4  0  4  4  0
139  0  4  0  4  4  0
140  0  4  0  4  4  0
141  0  4  0  4  0  4
142  0  4  0  4  0  4
143  0  4  0  4  0  4
144  0  4  0  4  0  4
145  0  4  0  0  4  4
146  0  4  0  0  4  4
147  0  4  0  0  4  4
148  0  4  0  0  4  4
149  0  0  4  4  4  0
150  0  0  4  4  4  0
151  0  0  4  4  4  0
152  0  0  4  4  4  0
153  0  0  4  4  0  4
154  0  0  4  4  0  4
155  0  0  4  4  0  4
156  0  0  4  4  0  4
157  0  0  4  0  4  4
158  0  0  4  0  4  4
159  0  0  4  0  4  4
160  0  0  4  0  4  4
161  0  0  0  4  4  4
162  0  0  0  4  4  4
163  0  0  0  4  4  4
164  0  0  0  4  4  4

*F<- factor(c(rep(1,3),rep(2,3)))*
*G<- factor(c(rep(1,2),rep(2,2),rep(3,2)))*
*H<-F<- factor(c(rep(1,3),rep(2,3)))*
*y<-rnorm(6,2)*
*test3<-aov(y~F*G)*

*model.matrix(test3)*

 (Intercept) F2 G2 G3 F2:G2 F2:G3
1              1  0  0     0     0     0
2              1  0  0     0     0     0
3              1  0  1     0     0     0
4              1  1  1     0     1     0
5              1  1  0     1     0     1
6              1  1  0     1     0     1
attr(,"assign")
[1] 0 1 2 2 3 3
attr(,"contrasts")
attr(,"contrasts")$F
[1] "contr.treatment"

attr(,"contrasts")$G
[1] "contr.treatment"

*alias(test3)*

Model :
y ~ F * G

Complete :
      (Intercept) F2 G2 G3
F2:G2  0           1  0 -1
F2:G3  0           0  0  1

*summary(test3)*

                  Df Sum Sq Mean Sq F value Pr(>F)
F                1   0.0479  0.0479   0.059  0.830
G                2  0.9762  0.4881   0.604  0.624
Residuals   2  1.6175  0.8087
-- 
Yours sincerely,
Justin

*I check my email at 9AM and 4PM everyday*
*If you have an EMERGENCY, contact me at +447938674419
<%2B447938674419>(UK) or +60125056192 <%2B60125056192>(Malaysia)*

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Jul 26 17:25:03 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 26 Jul 2016 15:25:03 +0000
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
Message-ID: <4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>

Show us the output, don?t just tell us what you are seeing. If the dates are correct in the csv file, show us the structure of the data frame you created with read.csv() and show the command(s) you used to convert the character data to date format. The solution is likely to be simple if you will cut/paste the R console and not just describe what is happening.

David C

From: Shivi Bhatia [mailto:shivipmp82 at gmail.com]
Sent: Tuesday, July 26, 2016 10:08 AM
To: David L Carlson
Subject: Re: [R] Date Time in R

Hi David,
This gives the results accurately. The first line shows all the variable names and the rest shows all values stored for each of the variable. Here date is appearing as correct.

Thanks, Shivi

On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu<mailto:dcarlson at tamu.edu>> wrote:
What does this produce?

> readLines("YourCSVfilename.csv", n=5)

If the data are in Excel, the date format used in .csv files is not always in the same as the format used when viewing dates in the spreadsheet.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Shivi Bhatia
Sent: Tuesday, July 26, 2016 7:42 AM
To: Marc Schwartz
Cc: R-help
Subject: Re: [R] Date Time in R

Thanks Marc for the help. this really helps.
I think there is some issue with the data saved in csv format for this
variable as when i checked:
str(eir$date)- this results in :-
Date[1:5327], format: NA NA NA NA NA.

Thanks again.

On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com<mailto:marc_schwartz at me.com>> wrote:

> Hi,
>
> That eir$date might be a factor is irrelevant. There is an as.Date()
> method for factors, which does the factor to character coercion internally
> and then calls as.Date.character() on the result.
>
> Using the example data below:
>
> eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>                            "05-30-16", "05-30-16", "05-30-16"))
>
> > str(eir)
> 'data.frame':   6 obs. of  1 variable:
>  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>
> > eir
>       date
> 1 05-30-16
> 2 05-30-16
> 3 05-30-16
> 4 05-30-16
> 5 05-30-16
> 6 05-30-16
>
> eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>
> > str(eir)
> 'data.frame':   6 obs. of  1 variable:
>  $ date: Date, format: "2016-05-30" ...
>
> > eir
>         date
> 1 2016-05-30
> 2 2016-05-30
> 3 2016-05-30
> 4 2016-05-30
> 5 2016-05-30
> 6 2016-05-30
>
> eir$days <- weekdays(eir$date)
>
> > str(eir)
> 'data.frame':   6 obs. of  2 variables:
>  $ date: Date, format: "2016-05-30" ...
>  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>
> > eir
>         date   days
> 1 2016-05-30 Monday
> 2 2016-05-30 Monday
> 3 2016-05-30 Monday
> 4 2016-05-30 Monday
> 5 2016-05-30 Monday
> 6 2016-05-30 Monday
>
>
> I would check to be sure that you do not have any typos in your code.
>
> Regards,
>
> Marc Schwartz
>
>
> > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com<mailto:shivipmp82 at gmail.com>> wrote:
> >
> > Hello Again,
> >
> > While i tried your solution as you suggested above it seems to be
> working.
> > Here is the output
> > temp<- dput(head(eir$date))
> > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16")
> > however it still shows class(eir$date) as character and hence i cannot
> find
> > weekdays from this variable.
> >
> > Sorry but i still dont understand in totality how R reads dates even
> though
> > have tried enough.
> >
> > Regards, Shivi
> >
> >
> > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia <shivipmp82 at gmail.com<mailto:shivipmp82 at gmail.com>>
> wrote:
> >
> >> Thanks Duncan for the quick response. I will check again as you
> suggested.
> >> If that doesn't work i will share a reproducible example.
> >>
> >> Thanks again!!!!
> >>
> >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> murdoch.duncan at gmail.com<mailto:murdoch.duncan at gmail.com>>
> >> wrote:
> >>
> >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> >>>
> >>>> Hi Team,
> >>>>
> >>>> This scenario may have come across a number of times however i checked
> >>>> nabble & SO and couldn't find a solution hence request assistance.
> >>>>
> >>>> I have a date variable in my data-set eir. The class of this var was
> >>>> character while i had read the file in r studio. Example of date -
> >>>> 05-30-16
> >>>>
> >>>> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y").
> This
> >>>> converts it to a date variable. However when i check few obs
> >>>> with head(eir$date) all the results are <NA>.
> >>>>
> >>>
> >>> I think you don't have character data like that, because I see
> >>>
> >>>> as.Date("05-30-16", "%m-%d-%y")
> >>> [1] "2016-05-30"
> >>>
> >>> I'd guess eir$date is really a factor, because character data is
> >>> frequently changed to factor automatically.  If that's the case, this
> >>> should work for the conversion:
> >>>
> >>> as.Date(as.character(eir$date), "%m-%d-%y")
> >>>
> >>> If that doesn't work, you'll need to post something reproducible.
> >>>
> >>> Duncan Murdoch
> >>>
> >>> I also need to create weekdays from this date variable but until i get
> >>>> this
> >>>> resolved i cant find a weekday. For weekday i have used:
> >>>> eir$week<- (eir$date)
> >>>> eir$week<- weekdays(as.Date(eir$week))
> >>>> class(eir$week)
> >>>> eir$week<- as.factor(eir$week)
> >>>> head(eir$week)
> >>>>
> >>>> Head of this eir$week results again as expected in <NA> but shows
> Levels:
> >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> >>>>
> >>>> Not sure what i should do here. Kindly suggest.
>
>
        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help<https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccxorF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccxorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From NordlDJ at dshs.wa.gov  Tue Jul 26 17:31:32 2016
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Tue, 26 Jul 2016 15:31:32 +0000
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA2766309815C2@WAXMXOLYMB025.WAX.wa.lcl>

You still seem to be having problems, so where is the promised reproducible example?

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> Bhatia
> Sent: Tuesday, July 26, 2016 4:43 AM
> To: Duncan Murdoch
> Cc: r-help at r-project.org
> Subject: Re: [R] Date Time in R
> 
> Thanks Duncan for the quick response. I will check again as you suggested.
> If that doesn't work i will share a reproducible example.
> 
> Thanks again!!!!
> 
> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch
> <murdoch.duncan at gmail.com>
> wrote:
> 
> > On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> >
> >> Hi Team,
> >>
> >> This scenario may have come across a number of times however i
> >> checked nabble & SO and couldn't find a solution hence request
> assistance.
> >>
> >> I have a date variable in my data-set eir. The class of this var was
> >> character while i had read the file in r studio. Example of date -
> >> 05-30-16
> >>
> >> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y").
> >> This converts it to a date variable. However when i check few obs
> >> with head(eir$date) all the results are <NA>.
> >>
> >
> > I think you don't have character data like that, because I see
> >
> > > as.Date("05-30-16", "%m-%d-%y")
> > [1] "2016-05-30"
> >
> > I'd guess eir$date is really a factor, because character data is
> > frequently changed to factor automatically.  If that's the case, this
> > should work for the conversion:
> >
> > as.Date(as.character(eir$date), "%m-%d-%y")
> >
> > If that doesn't work, you'll need to post something reproducible.
> >
> > Duncan Murdoch
> >
> > I also need to create weekdays from this date variable but until i get
> > this
> >> resolved i cant find a weekday. For weekday i have used:
> >> eir$week<- (eir$date)
> >> eir$week<- weekdays(as.Date(eir$week))
> >> class(eir$week)
> >> eir$week<- as.factor(eir$week)
> >> head(eir$week)
> >>
> >> Head of this eir$week results again as expected in <NA> but shows
> Levels:
> >> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> >>
> >> Not sure what i should do here. Kindly suggest.
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From shivipmp82 at gmail.com  Tue Jul 26 17:45:46 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 21:15:46 +0530
Subject: [R] Date Time in R
In-Reply-To: <4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
Message-ID: <CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>

Hi David please see the code and some reproducible data:
eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
lubridate library to help with the dates:
install.packages("lubridate")
library(lubridate)
eir$date <- mdy(eir$date)
weekdays <- wdy(eir$date)
week names <- wdy(eir$date, label = TRUE)

This the output from the file:

structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
"Date"),

month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class = "factor"),

day = c(30L, 30L, 30L),

weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
"Thu", "Tue","Wed"), class = "factor"),

survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't know","No"),

 a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
"day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
"data.frame")

There are several other variables that i have removed which are not
relevant in this context.

On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu> wrote:

> Show us the output, don?t just tell us what you are seeing. If the dates
> are correct in the csv file, show us the structure of the data frame you
> created with read.csv() and show the command(s) you used to convert the
> character data to date format. The solution is likely to be simple if you
> will cut/paste the R console and not just describe what is happening.
>
>
>
> David C
>
>
>
> *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
> *Sent:* Tuesday, July 26, 2016 10:08 AM
> *To:* David L Carlson
>
> *Subject:* Re: [R] Date Time in R
>
>
>
> Hi David,
>
> This gives the results accurately. The first line shows all the variable
> names and the rest shows all values stored for each of the variable. Here
> date is appearing as correct.
>
>
>
> Thanks, Shivi
>
>
>
> On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
> wrote:
>
> What does this produce?
>
> > readLines("YourCSVfilename.csv", n=5)
>
> If the data are in Excel, the date format used in .csv files is not always
> in the same as the format used when viewing dates in the spreadsheet.
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> Bhatia
> Sent: Tuesday, July 26, 2016 7:42 AM
> To: Marc Schwartz
> Cc: R-help
> Subject: Re: [R] Date Time in R
>
> Thanks Marc for the help. this really helps.
> I think there is some issue with the data saved in csv format for this
> variable as when i checked:
> str(eir$date)- this results in :-
> Date[1:5327], format: NA NA NA NA NA.
>
> Thanks again.
>
> On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
> wrote:
>
> > Hi,
> >
> > That eir$date might be a factor is irrelevant. There is an as.Date()
> > method for factors, which does the factor to character coercion
> internally
> > and then calls as.Date.character() on the result.
> >
> > Using the example data below:
> >
> > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
> >                            "05-30-16", "05-30-16", "05-30-16"))
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  1 variable:
> >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
> >
> > > eir
> >       date
> > 1 05-30-16
> > 2 05-30-16
> > 3 05-30-16
> > 4 05-30-16
> > 5 05-30-16
> > 6 05-30-16
> >
> > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  1 variable:
> >  $ date: Date, format: "2016-05-30" ...
> >
> > > eir
> >         date
> > 1 2016-05-30
> > 2 2016-05-30
> > 3 2016-05-30
> > 4 2016-05-30
> > 5 2016-05-30
> > 6 2016-05-30
> >
> > eir$days <- weekdays(eir$date)
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  2 variables:
> >  $ date: Date, format: "2016-05-30" ...
> >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
> >
> > > eir
> >         date   days
> > 1 2016-05-30 Monday
> > 2 2016-05-30 Monday
> > 3 2016-05-30 Monday
> > 4 2016-05-30 Monday
> > 5 2016-05-30 Monday
> > 6 2016-05-30 Monday
> >
> >
> > I would check to be sure that you do not have any typos in your code.
> >
> > Regards,
> >
> > Marc Schwartz
> >
> >
> > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
> > >
> > > Hello Again,
> > >
> > > While i tried your solution as you suggested above it seems to be
> > working.
> > > Here is the output
> > > temp<- dput(head(eir$date))
> > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
> "05-30-16")
> > > however it still shows class(eir$date) as character and hence i cannot
> > find
> > > weekdays from this variable.
> > >
> > > Sorry but i still dont understand in totality how R reads dates even
> > though
> > > have tried enough.
> > >
> > > Regards, Shivi
> > >
> > >
> > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia <shivipmp82 at gmail.com>
> > wrote:
> > >
> > >> Thanks Duncan for the quick response. I will check again as you
> > suggested.
> > >> If that doesn't work i will share a reproducible example.
> > >>
> > >> Thanks again!!!!
> > >>
> > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> > murdoch.duncan at gmail.com>
> > >> wrote:
> > >>
> > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> > >>>
> > >>>> Hi Team,
> > >>>>
> > >>>> This scenario may have come across a number of times however i
> checked
> > >>>> nabble & SO and couldn't find a solution hence request assistance.
> > >>>>
> > >>>> I have a date variable in my data-set eir. The class of this var was
> > >>>> character while i had read the file in r studio. Example of date -
> > >>>> 05-30-16
> > >>>>
> > >>>> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y").
> > This
> > >>>> converts it to a date variable. However when i check few obs
> > >>>> with head(eir$date) all the results are <NA>.
> > >>>>
> > >>>
> > >>> I think you don't have character data like that, because I see
> > >>>
> > >>>> as.Date("05-30-16", "%m-%d-%y")
> > >>> [1] "2016-05-30"
> > >>>
> > >>> I'd guess eir$date is really a factor, because character data is
> > >>> frequently changed to factor automatically.  If that's the case, this
> > >>> should work for the conversion:
> > >>>
> > >>> as.Date(as.character(eir$date), "%m-%d-%y")
> > >>>
> > >>> If that doesn't work, you'll need to post something reproducible.
> > >>>
> > >>> Duncan Murdoch
> > >>>
> > >>> I also need to create weekdays from this date variable but until i
> get
> > >>>> this
> > >>>> resolved i cant find a weekday. For weekday i have used:
> > >>>> eir$week<- (eir$date)
> > >>>> eir$week<- weekdays(as.Date(eir$week))
> > >>>> class(eir$week)
> > >>>> eir$week<- as.factor(eir$week)
> > >>>> head(eir$week)
> > >>>>
> > >>>> Head of this eir$week results again as expected in <NA> but shows
> > Levels:
> > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> > >>>>
> > >>>> Not sure what i should do here. Kindly suggest.
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccxorF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccxorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Tue Jul 26 17:55:56 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 21:25:56 +0530
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
Message-ID: <CAB=p7SqF=uf3CH+UtarsnHQg1xjTOzMBsv_NZF0kSNc2HqLo8Q@mail.gmail.com>

Hi Team,

Please read wdy as wday it was mistakenly copied.


On Tue, Jul 26, 2016 at 9:15 PM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:

> Hi David please see the code and some reproducible data:
> eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
> lubridate library to help with the dates:
> install.packages("lubridate")
> library(lubridate)
> eir$date <- mdy(eir$date)
> weekdays <- wdy(eir$date)
> week names <- wdy(eir$date, label = TRUE)
>
> This the output from the file:
>
> structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
> "Date"),
>
> month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class =
> "factor"),
>
> day = c(30L, 30L, 30L),
>
> weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
> "Thu", "Tue","Wed"), class = "factor"),
>
> survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't know",
> "No"),
>
>  a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
> "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
> "data.frame")
>
> There are several other variables that i have removed which are not
> relevant in this context.
>
> On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu>
> wrote:
>
>> Show us the output, don?t just tell us what you are seeing. If the dates
>> are correct in the csv file, show us the structure of the data frame you
>> created with read.csv() and show the command(s) you used to convert the
>> character data to date format. The solution is likely to be simple if you
>> will cut/paste the R console and not just describe what is happening.
>>
>>
>>
>> David C
>>
>>
>>
>> *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>> *Sent:* Tuesday, July 26, 2016 10:08 AM
>> *To:* David L Carlson
>>
>> *Subject:* Re: [R] Date Time in R
>>
>>
>>
>> Hi David,
>>
>> This gives the results accurately. The first line shows all the variable
>> names and the rest shows all values stored for each of the variable. Here
>> date is appearing as correct.
>>
>>
>>
>> Thanks, Shivi
>>
>>
>>
>> On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
>> wrote:
>>
>> What does this produce?
>>
>> > readLines("YourCSVfilename.csv", n=5)
>>
>> If the data are in Excel, the date format used in .csv files is not
>> always in the same as the format used when viewing dates in the spreadsheet.
>>
>> -------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77840-4352
>>
>>
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
>> Bhatia
>> Sent: Tuesday, July 26, 2016 7:42 AM
>> To: Marc Schwartz
>> Cc: R-help
>> Subject: Re: [R] Date Time in R
>>
>> Thanks Marc for the help. this really helps.
>> I think there is some issue with the data saved in csv format for this
>> variable as when i checked:
>> str(eir$date)- this results in :-
>> Date[1:5327], format: NA NA NA NA NA.
>>
>> Thanks again.
>>
>> On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
>> wrote:
>>
>> > Hi,
>> >
>> > That eir$date might be a factor is irrelevant. There is an as.Date()
>> > method for factors, which does the factor to character coercion
>> internally
>> > and then calls as.Date.character() on the result.
>> >
>> > Using the example data below:
>> >
>> > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>> >                            "05-30-16", "05-30-16", "05-30-16"))
>> >
>> > > str(eir)
>> > 'data.frame':   6 obs. of  1 variable:
>> >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>> >
>> > > eir
>> >       date
>> > 1 05-30-16
>> > 2 05-30-16
>> > 3 05-30-16
>> > 4 05-30-16
>> > 5 05-30-16
>> > 6 05-30-16
>> >
>> > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>> >
>> > > str(eir)
>> > 'data.frame':   6 obs. of  1 variable:
>> >  $ date: Date, format: "2016-05-30" ...
>> >
>> > > eir
>> >         date
>> > 1 2016-05-30
>> > 2 2016-05-30
>> > 3 2016-05-30
>> > 4 2016-05-30
>> > 5 2016-05-30
>> > 6 2016-05-30
>> >
>> > eir$days <- weekdays(eir$date)
>> >
>> > > str(eir)
>> > 'data.frame':   6 obs. of  2 variables:
>> >  $ date: Date, format: "2016-05-30" ...
>> >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>> >
>> > > eir
>> >         date   days
>> > 1 2016-05-30 Monday
>> > 2 2016-05-30 Monday
>> > 3 2016-05-30 Monday
>> > 4 2016-05-30 Monday
>> > 5 2016-05-30 Monday
>> > 6 2016-05-30 Monday
>> >
>> >
>> > I would check to be sure that you do not have any typos in your code.
>> >
>> > Regards,
>> >
>> > Marc Schwartz
>> >
>> >
>> > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
>> wrote:
>> > >
>> > > Hello Again,
>> > >
>> > > While i tried your solution as you suggested above it seems to be
>> > working.
>> > > Here is the output
>> > > temp<- dput(head(eir$date))
>> > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
>> "05-30-16")
>> > > however it still shows class(eir$date) as character and hence i cannot
>> > find
>> > > weekdays from this variable.
>> > >
>> > > Sorry but i still dont understand in totality how R reads dates even
>> > though
>> > > have tried enough.
>> > >
>> > > Regards, Shivi
>> > >
>> > >
>> > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia <shivipmp82 at gmail.com>
>> > wrote:
>> > >
>> > >> Thanks Duncan for the quick response. I will check again as you
>> > suggested.
>> > >> If that doesn't work i will share a reproducible example.
>> > >>
>> > >> Thanks again!!!!
>> > >>
>> > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
>> > murdoch.duncan at gmail.com>
>> > >> wrote:
>> > >>
>> > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>> > >>>
>> > >>>> Hi Team,
>> > >>>>
>> > >>>> This scenario may have come across a number of times however i
>> checked
>> > >>>> nabble & SO and couldn't find a solution hence request assistance.
>> > >>>>
>> > >>>> I have a date variable in my data-set eir. The class of this var
>> was
>> > >>>> character while i had read the file in r studio. Example of date -
>> > >>>> 05-30-16
>> > >>>>
>> > >>>> To change this i have used eir$date<- as.Date(eir$date,
>> "%m-%d-%y").
>> > This
>> > >>>> converts it to a date variable. However when i check few obs
>> > >>>> with head(eir$date) all the results are <NA>.
>> > >>>>
>> > >>>
>> > >>> I think you don't have character data like that, because I see
>> > >>>
>> > >>>> as.Date("05-30-16", "%m-%d-%y")
>> > >>> [1] "2016-05-30"
>> > >>>
>> > >>> I'd guess eir$date is really a factor, because character data is
>> > >>> frequently changed to factor automatically.  If that's the case,
>> this
>> > >>> should work for the conversion:
>> > >>>
>> > >>> as.Date(as.character(eir$date), "%m-%d-%y")
>> > >>>
>> > >>> If that doesn't work, you'll need to post something reproducible.
>> > >>>
>> > >>> Duncan Murdoch
>> > >>>
>> > >>> I also need to create weekdays from this date variable but until i
>> get
>> > >>>> this
>> > >>>> resolved i cant find a weekday. For weekday i have used:
>> > >>>> eir$week<- (eir$date)
>> > >>>> eir$week<- weekdays(as.Date(eir$week))
>> > >>>> class(eir$week)
>> > >>>> eir$week<- as.factor(eir$week)
>> > >>>> head(eir$week)
>> > >>>>
>> > >>>> Head of this eir$week results again as expected in <NA> but shows
>> > Levels:
>> > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>> > >>>>
>> > >>>> Not sure what i should do here. Kindly suggest.
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccxorF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccxorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>
>

	[[alternative HTML version deleted]]


From tom at maladmin.com  Tue Jul 26 18:08:45 2016
From: tom at maladmin.com (Tom Wright)
Date: Tue, 26 Jul 2016 12:08:45 -0400
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
Message-ID: <cd847745387ec23289997617ec9e81af@mail.gmail.com>

Hi again Shiva,
I think what we need to see is the output from:

str(eid$date)

and perhaps
head(eid$date)

If you can send this information before doing any processing on the date
(i.e. before the as.Date() function) we may be able to help.



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi Bhatia
Sent: July 26, 2016 11:46 AM
To: David L Carlson <dcarlson at tamu.edu>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] Date Time in R

Hi David please see the code and some reproducible data:
eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
lubridate library to help with the dates:
install.packages("lubridate")
library(lubridate)
eir$date <- mdy(eir$date)
weekdays <- wdy(eir$date)
week names <- wdy(eir$date, label = TRUE)

This the output from the file:

structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
"Date"),

month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class = "factor"),

day = c(30L, 30L, 30L),

weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
"Thu", "Tue","Wed"), class = "factor"),

survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't know","No"),

 a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
"day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
"data.frame")

There are several other variables that i have removed which are not relevant
in this context.

On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu> wrote:

> Show us the output, don?t just tell us what you are seeing. If the
> dates are correct in the csv file, show us the structure of the data
> frame you created with read.csv() and show the command(s) you used to
> convert the character data to date format. The solution is likely to
> be simple if you will cut/paste the R console and not just describe what
> is happening.
>
>
>
> David C
>
>
>
> *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
> *Sent:* Tuesday, July 26, 2016 10:08 AM
> *To:* David L Carlson
>
> *Subject:* Re: [R] Date Time in R
>
>
>
> Hi David,
>
> This gives the results accurately. The first line shows all the
> variable names and the rest shows all values stored for each of the
> variable. Here date is appearing as correct.
>
>
>
> Thanks, Shivi
>
>
>
> On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
> wrote:
>
> What does this produce?
>
> > readLines("YourCSVfilename.csv", n=5)
>
> If the data are in Excel, the date format used in .csv files is not
> always in the same as the format used when viewing dates in the
> spreadsheet.
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> Bhatia
> Sent: Tuesday, July 26, 2016 7:42 AM
> To: Marc Schwartz
> Cc: R-help
> Subject: Re: [R] Date Time in R
>
> Thanks Marc for the help. this really helps.
> I think there is some issue with the data saved in csv format for this
> variable as when i checked:
> str(eir$date)- this results in :-
> Date[1:5327], format: NA NA NA NA NA.
>
> Thanks again.
>
> On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
> wrote:
>
> > Hi,
> >
> > That eir$date might be a factor is irrelevant. There is an as.Date()
> > method for factors, which does the factor to character coercion
> internally
> > and then calls as.Date.character() on the result.
> >
> > Using the example data below:
> >
> > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
> >                            "05-30-16", "05-30-16", "05-30-16"))
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  1 variable:
> >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
> >
> > > eir
> >       date
> > 1 05-30-16
> > 2 05-30-16
> > 3 05-30-16
> > 4 05-30-16
> > 5 05-30-16
> > 6 05-30-16
> >
> > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  1 variable:
> >  $ date: Date, format: "2016-05-30" ...
> >
> > > eir
> >         date
> > 1 2016-05-30
> > 2 2016-05-30
> > 3 2016-05-30
> > 4 2016-05-30
> > 5 2016-05-30
> > 6 2016-05-30
> >
> > eir$days <- weekdays(eir$date)
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  2 variables:
> >  $ date: Date, format: "2016-05-30" ...
> >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
> >
> > > eir
> >         date   days
> > 1 2016-05-30 Monday
> > 2 2016-05-30 Monday
> > 3 2016-05-30 Monday
> > 4 2016-05-30 Monday
> > 5 2016-05-30 Monday
> > 6 2016-05-30 Monday
> >
> >
> > I would check to be sure that you do not have any typos in your code.
> >
> > Regards,
> >
> > Marc Schwartz
> >
> >
> > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
> > >
> > > Hello Again,
> > >
> > > While i tried your solution as you suggested above it seems to be
> > working.
> > > Here is the output
> > > temp<- dput(head(eir$date))
> > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
> "05-30-16")
> > > however it still shows class(eir$date) as character and hence i
> > > cannot
> > find
> > > weekdays from this variable.
> > >
> > > Sorry but i still dont understand in totality how R reads dates
> > > even
> > though
> > > have tried enough.
> > >
> > > Regards, Shivi
> > >
> > >
> > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
> > > <shivipmp82 at gmail.com>
> > wrote:
> > >
> > >> Thanks Duncan for the quick response. I will check again as you
> > suggested.
> > >> If that doesn't work i will share a reproducible example.
> > >>
> > >> Thanks again!!!!
> > >>
> > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> > murdoch.duncan at gmail.com>
> > >> wrote:
> > >>
> > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> > >>>
> > >>>> Hi Team,
> > >>>>
> > >>>> This scenario may have come across a number of times however i
> checked
> > >>>> nabble & SO and couldn't find a solution hence request assistance.
> > >>>>
> > >>>> I have a date variable in my data-set eir. The class of this
> > >>>> var was character while i had read the file in r studio.
> > >>>> Example of date -
> > >>>> 05-30-16
> > >>>>
> > >>>> To change this i have used eir$date<- as.Date(eir$date,
> > >>>> "%m-%d-%y").
> > This
> > >>>> converts it to a date variable. However when i check few obs
> > >>>> with head(eir$date) all the results are <NA>.
> > >>>>
> > >>>
> > >>> I think you don't have character data like that, because I see
> > >>>
> > >>>> as.Date("05-30-16", "%m-%d-%y")
> > >>> [1] "2016-05-30"
> > >>>
> > >>> I'd guess eir$date is really a factor, because character data is
> > >>> frequently changed to factor automatically.  If that's the case,
> > >>> this should work for the conversion:
> > >>>
> > >>> as.Date(as.character(eir$date), "%m-%d-%y")
> > >>>
> > >>> If that doesn't work, you'll need to post something reproducible.
> > >>>
> > >>> Duncan Murdoch
> > >>>
> > >>> I also need to create weekdays from this date variable but until
> > >>> i
> get
> > >>>> this
> > >>>> resolved i cant find a weekday. For weekday i have used:
> > >>>> eir$week<- (eir$date)
> > >>>> eir$week<- weekdays(as.Date(eir$week))
> > >>>> class(eir$week)
> > >>>> eir$week<- as.factor(eir$week)
> > >>>> head(eir$week)
> > >>>>
> > >>>> Head of this eir$week results again as expected in <NA> but
> > >>>> shows
> > Levels:
> > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> > >>>>
> > >>>> Not sure what i should do here. Kindly suggest.
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
> ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
> orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
> hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
> xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Tue Jul 26 18:15:44 2016
From: h.wickham at gmail.com (Hadley Wickham)
Date: Tue, 26 Jul 2016 11:15:44 -0500
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
Message-ID: <CABdHhvE_aUwG2f-B+_t6KqmvHs6HVoqG_W5YUvpfs2Ek8xVwWg@mail.gmail.com>

I'd recommend reading up on how to create a minimal reproducible
example (e.g. http://r4ds.had.co.nz/exploratory-data-analysis.html).
It is unlikely anyone will be able to help you unless you can reliably
communicate _exactly_ what you're doing. Unlike human languages,
computer languages are extremely strict, and just one wrong character
can make a big difference.

Hadley

On Tue, Jul 26, 2016 at 10:45 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> Hi David please see the code and some reproducible data:
> eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
> lubridate library to help with the dates:
> install.packages("lubridate")
> library(lubridate)
> eir$date <- mdy(eir$date)
> weekdays <- wdy(eir$date)
> week names <- wdy(eir$date, label = TRUE)
>
> This the output from the file:
>
> structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
> "Date"),
>
> month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class = "factor"),
>
> day = c(30L, 30L, 30L),
>
> weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
> "Thu", "Tue","Wed"), class = "factor"),
>
> survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't know","No"),
>
>  a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
> "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
> "data.frame")
>
> There are several other variables that i have removed which are not
> relevant in this context.
>
> On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu> wrote:
>
>> Show us the output, don?t just tell us what you are seeing. If the dates
>> are correct in the csv file, show us the structure of the data frame you
>> created with read.csv() and show the command(s) you used to convert the
>> character data to date format. The solution is likely to be simple if you
>> will cut/paste the R console and not just describe what is happening.
>>
>>
>>
>> David C
>>
>>
>>
>> *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>> *Sent:* Tuesday, July 26, 2016 10:08 AM
>> *To:* David L Carlson
>>
>> *Subject:* Re: [R] Date Time in R
>>
>>
>>
>> Hi David,
>>
>> This gives the results accurately. The first line shows all the variable
>> names and the rest shows all values stored for each of the variable. Here
>> date is appearing as correct.
>>
>>
>>
>> Thanks, Shivi
>>
>>
>>
>> On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
>> wrote:
>>
>> What does this produce?
>>
>> > readLines("YourCSVfilename.csv", n=5)
>>
>> If the data are in Excel, the date format used in .csv files is not always
>> in the same as the format used when viewing dates in the spreadsheet.
>>
>> -------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77840-4352
>>
>>
>>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
>> Bhatia
>> Sent: Tuesday, July 26, 2016 7:42 AM
>> To: Marc Schwartz
>> Cc: R-help
>> Subject: Re: [R] Date Time in R
>>
>> Thanks Marc for the help. this really helps.
>> I think there is some issue with the data saved in csv format for this
>> variable as when i checked:
>> str(eir$date)- this results in :-
>> Date[1:5327], format: NA NA NA NA NA.
>>
>> Thanks again.
>>
>> On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
>> wrote:
>>
>> > Hi,
>> >
>> > That eir$date might be a factor is irrelevant. There is an as.Date()
>> > method for factors, which does the factor to character coercion
>> internally
>> > and then calls as.Date.character() on the result.
>> >
>> > Using the example data below:
>> >
>> > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>> >                            "05-30-16", "05-30-16", "05-30-16"))
>> >
>> > > str(eir)
>> > 'data.frame':   6 obs. of  1 variable:
>> >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>> >
>> > > eir
>> >       date
>> > 1 05-30-16
>> > 2 05-30-16
>> > 3 05-30-16
>> > 4 05-30-16
>> > 5 05-30-16
>> > 6 05-30-16
>> >
>> > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>> >
>> > > str(eir)
>> > 'data.frame':   6 obs. of  1 variable:
>> >  $ date: Date, format: "2016-05-30" ...
>> >
>> > > eir
>> >         date
>> > 1 2016-05-30
>> > 2 2016-05-30
>> > 3 2016-05-30
>> > 4 2016-05-30
>> > 5 2016-05-30
>> > 6 2016-05-30
>> >
>> > eir$days <- weekdays(eir$date)
>> >
>> > > str(eir)
>> > 'data.frame':   6 obs. of  2 variables:
>> >  $ date: Date, format: "2016-05-30" ...
>> >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>> >
>> > > eir
>> >         date   days
>> > 1 2016-05-30 Monday
>> > 2 2016-05-30 Monday
>> > 3 2016-05-30 Monday
>> > 4 2016-05-30 Monday
>> > 5 2016-05-30 Monday
>> > 6 2016-05-30 Monday
>> >
>> >
>> > I would check to be sure that you do not have any typos in your code.
>> >
>> > Regards,
>> >
>> > Marc Schwartz
>> >
>> >
>> > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
>> wrote:
>> > >
>> > > Hello Again,
>> > >
>> > > While i tried your solution as you suggested above it seems to be
>> > working.
>> > > Here is the output
>> > > temp<- dput(head(eir$date))
>> > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
>> "05-30-16")
>> > > however it still shows class(eir$date) as character and hence i cannot
>> > find
>> > > weekdays from this variable.
>> > >
>> > > Sorry but i still dont understand in totality how R reads dates even
>> > though
>> > > have tried enough.
>> > >
>> > > Regards, Shivi
>> > >
>> > >
>> > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia <shivipmp82 at gmail.com>
>> > wrote:
>> > >
>> > >> Thanks Duncan for the quick response. I will check again as you
>> > suggested.
>> > >> If that doesn't work i will share a reproducible example.
>> > >>
>> > >> Thanks again!!!!
>> > >>
>> > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
>> > murdoch.duncan at gmail.com>
>> > >> wrote:
>> > >>
>> > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>> > >>>
>> > >>>> Hi Team,
>> > >>>>
>> > >>>> This scenario may have come across a number of times however i
>> checked
>> > >>>> nabble & SO and couldn't find a solution hence request assistance.
>> > >>>>
>> > >>>> I have a date variable in my data-set eir. The class of this var was
>> > >>>> character while i had read the file in r studio. Example of date -
>> > >>>> 05-30-16
>> > >>>>
>> > >>>> To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y").
>> > This
>> > >>>> converts it to a date variable. However when i check few obs
>> > >>>> with head(eir$date) all the results are <NA>.
>> > >>>>
>> > >>>
>> > >>> I think you don't have character data like that, because I see
>> > >>>
>> > >>>> as.Date("05-30-16", "%m-%d-%y")
>> > >>> [1] "2016-05-30"
>> > >>>
>> > >>> I'd guess eir$date is really a factor, because character data is
>> > >>> frequently changed to factor automatically.  If that's the case, this
>> > >>> should work for the conversion:
>> > >>>
>> > >>> as.Date(as.character(eir$date), "%m-%d-%y")
>> > >>>
>> > >>> If that doesn't work, you'll need to post something reproducible.
>> > >>>
>> > >>> Duncan Murdoch
>> > >>>
>> > >>> I also need to create weekdays from this date variable but until i
>> get
>> > >>>> this
>> > >>>> resolved i cant find a weekday. For weekday i have used:
>> > >>>> eir$week<- (eir$date)
>> > >>>> eir$week<- weekdays(as.Date(eir$week))
>> > >>>> class(eir$week)
>> > >>>> eir$week<- as.factor(eir$week)
>> > >>>> head(eir$week)
>> > >>>>
>> > >>>> Head of this eir$week results again as expected in <NA> but shows
>> > Levels:
>> > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>> > >>>>
>> > >>>> Not sure what i should do here. Kindly suggest.
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccxorF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccxorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://hadley.nz


From shivipmp82 at gmail.com  Tue Jul 26 18:16:08 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 21:46:08 +0530
Subject: [R] Date Time in R
In-Reply-To: <cd847745387ec23289997617ec9e81af@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
	<cd847745387ec23289997617ec9e81af@mail.gmail.com>
Message-ID: <CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>

Hello Tom,

Please find the details: (i have changed name from eir to a1, rest all is
same)

str(a1$date)
 Factor w/ 32 levels "05-30-16","05-31-16",..: 1 1 1

head(a1$date)
05-30-16 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16
32 Levels: 05-30-16 05-31-16 06-01-16 06-02-16 06-03-16 06-04-16 06-05-16
06-06-16 06-07-16 06-08-16

Thanks again!!!

On Tue, Jul 26, 2016 at 9:38 PM, Tom Wright <tom at maladmin.com> wrote:

> Hi again Shiva,
> I think what we need to see is the output from:
>
> str(eid$date)
>
> and perhaps
> head(eid$date)
>
> If you can send this information before doing any processing on the date
> (i.e. before the as.Date() function) we may be able to help.
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> Bhatia
> Sent: July 26, 2016 11:46 AM
> To: David L Carlson <dcarlson at tamu.edu>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] Date Time in R
>
> Hi David please see the code and some reproducible data:
> eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
> lubridate library to help with the dates:
> install.packages("lubridate")
> library(lubridate)
> eir$date <- mdy(eir$date)
> weekdays <- wdy(eir$date)
> week names <- wdy(eir$date, label = TRUE)
>
> This the output from the file:
>
> structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
> "Date"),
>
> month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class =
> "factor"),
>
> day = c(30L, 30L, 30L),
>
> weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
> "Thu", "Tue","Wed"), class = "factor"),
>
> survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't know","No"),
>
>  a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
> "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
> "data.frame")
>
> There are several other variables that i have removed which are not
> relevant
> in this context.
>
> On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu>
> wrote:
>
> > Show us the output, don?t just tell us what you are seeing. If the
> > dates are correct in the csv file, show us the structure of the data
> > frame you created with read.csv() and show the command(s) you used to
> > convert the character data to date format. The solution is likely to
> > be simple if you will cut/paste the R console and not just describe what
> > is happening.
> >
> >
> >
> > David C
> >
> >
> >
> > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
> > *Sent:* Tuesday, July 26, 2016 10:08 AM
> > *To:* David L Carlson
> >
> > *Subject:* Re: [R] Date Time in R
> >
> >
> >
> > Hi David,
> >
> > This gives the results accurately. The first line shows all the
> > variable names and the rest shows all values stored for each of the
> > variable. Here date is appearing as correct.
> >
> >
> >
> > Thanks, Shivi
> >
> >
> >
> > On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
> > wrote:
> >
> > What does this produce?
> >
> > > readLines("YourCSVfilename.csv", n=5)
> >
> > If the data are in Excel, the date format used in .csv files is not
> > always in the same as the format used when viewing dates in the
> > spreadsheet.
> >
> > -------------------------------------
> > David L Carlson
> > Department of Anthropology
> > Texas A&M University
> > College Station, TX 77840-4352
> >
> >
> >
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> > Bhatia
> > Sent: Tuesday, July 26, 2016 7:42 AM
> > To: Marc Schwartz
> > Cc: R-help
> > Subject: Re: [R] Date Time in R
> >
> > Thanks Marc for the help. this really helps.
> > I think there is some issue with the data saved in csv format for this
> > variable as when i checked:
> > str(eir$date)- this results in :-
> > Date[1:5327], format: NA NA NA NA NA.
> >
> > Thanks again.
> >
> > On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
> > wrote:
> >
> > > Hi,
> > >
> > > That eir$date might be a factor is irrelevant. There is an as.Date()
> > > method for factors, which does the factor to character coercion
> > internally
> > > and then calls as.Date.character() on the result.
> > >
> > > Using the example data below:
> > >
> > > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
> > >                            "05-30-16", "05-30-16", "05-30-16"))
> > >
> > > > str(eir)
> > > 'data.frame':   6 obs. of  1 variable:
> > >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
> > >
> > > > eir
> > >       date
> > > 1 05-30-16
> > > 2 05-30-16
> > > 3 05-30-16
> > > 4 05-30-16
> > > 5 05-30-16
> > > 6 05-30-16
> > >
> > > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
> > >
> > > > str(eir)
> > > 'data.frame':   6 obs. of  1 variable:
> > >  $ date: Date, format: "2016-05-30" ...
> > >
> > > > eir
> > >         date
> > > 1 2016-05-30
> > > 2 2016-05-30
> > > 3 2016-05-30
> > > 4 2016-05-30
> > > 5 2016-05-30
> > > 6 2016-05-30
> > >
> > > eir$days <- weekdays(eir$date)
> > >
> > > > str(eir)
> > > 'data.frame':   6 obs. of  2 variables:
> > >  $ date: Date, format: "2016-05-30" ...
> > >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
> > >
> > > > eir
> > >         date   days
> > > 1 2016-05-30 Monday
> > > 2 2016-05-30 Monday
> > > 3 2016-05-30 Monday
> > > 4 2016-05-30 Monday
> > > 5 2016-05-30 Monday
> > > 6 2016-05-30 Monday
> > >
> > >
> > > I would check to be sure that you do not have any typos in your code.
> > >
> > > Regards,
> > >
> > > Marc Schwartz
> > >
> > >
> > > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> > wrote:
> > > >
> > > > Hello Again,
> > > >
> > > > While i tried your solution as you suggested above it seems to be
> > > working.
> > > > Here is the output
> > > > temp<- dput(head(eir$date))
> > > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
> > "05-30-16")
> > > > however it still shows class(eir$date) as character and hence i
> > > > cannot
> > > find
> > > > weekdays from this variable.
> > > >
> > > > Sorry but i still dont understand in totality how R reads dates
> > > > even
> > > though
> > > > have tried enough.
> > > >
> > > > Regards, Shivi
> > > >
> > > >
> > > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
> > > > <shivipmp82 at gmail.com>
> > > wrote:
> > > >
> > > >> Thanks Duncan for the quick response. I will check again as you
> > > suggested.
> > > >> If that doesn't work i will share a reproducible example.
> > > >>
> > > >> Thanks again!!!!
> > > >>
> > > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> > > murdoch.duncan at gmail.com>
> > > >> wrote:
> > > >>
> > > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> > > >>>
> > > >>>> Hi Team,
> > > >>>>
> > > >>>> This scenario may have come across a number of times however i
> > checked
> > > >>>> nabble & SO and couldn't find a solution hence request assistance.
> > > >>>>
> > > >>>> I have a date variable in my data-set eir. The class of this
> > > >>>> var was character while i had read the file in r studio.
> > > >>>> Example of date -
> > > >>>> 05-30-16
> > > >>>>
> > > >>>> To change this i have used eir$date<- as.Date(eir$date,
> > > >>>> "%m-%d-%y").
> > > This
> > > >>>> converts it to a date variable. However when i check few obs
> > > >>>> with head(eir$date) all the results are <NA>.
> > > >>>>
> > > >>>
> > > >>> I think you don't have character data like that, because I see
> > > >>>
> > > >>>> as.Date("05-30-16", "%m-%d-%y")
> > > >>> [1] "2016-05-30"
> > > >>>
> > > >>> I'd guess eir$date is really a factor, because character data is
> > > >>> frequently changed to factor automatically.  If that's the case,
> > > >>> this should work for the conversion:
> > > >>>
> > > >>> as.Date(as.character(eir$date), "%m-%d-%y")
> > > >>>
> > > >>> If that doesn't work, you'll need to post something reproducible.
> > > >>>
> > > >>> Duncan Murdoch
> > > >>>
> > > >>> I also need to create weekdays from this date variable but until
> > > >>> i
> > get
> > > >>>> this
> > > >>>> resolved i cant find a weekday. For weekday i have used:
> > > >>>> eir$week<- (eir$date)
> > > >>>> eir$week<- weekdays(as.Date(eir$week))
> > > >>>> class(eir$week)
> > > >>>> eir$week<- as.factor(eir$week)
> > > >>>> head(eir$week)
> > > >>>>
> > > >>>> Head of this eir$week results again as expected in <NA> but
> > > >>>> shows
> > > Levels:
> > > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> > > >>>>
> > > >>>> Not sure what i should do here. Kindly suggest.
> > >
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> > lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
> > ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
> > orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> > rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
> > hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
> > xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tom at maladmin.com  Tue Jul 26 18:28:10 2016
From: tom at maladmin.com (Tom Wright)
Date: Tue, 26 Jul 2016 12:28:10 -0400
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
	<cd847745387ec23289997617ec9e81af@mail.gmail.com>
	<CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>
Message-ID: <66ecc2246eb9b7638ee80a18b06541b5@mail.gmail.com>

So this looks correct so far.



# convert strings (or factors) to a date object

a1$date <- as.Date(a1$date, ?%m-%d-%y?)



# Note the date object is now displayed in the format %Y-%m-%d by default

# so no need for the mdy() or ymd() function



# extract the weekdays as number

weekdays <- wday(a1$date)



# note can?t have spaces in variable names

week_names <- wday(a1$date, label=TRUE)





*From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
*Sent:* July 26, 2016 12:16 PM
*To:* Tom Wright <tom at maladmin.com>
*Cc:* David L Carlson <dcarlson at tamu.edu>; r-help <r-help at r-project.org>
*Subject:* Re: [R] Date Time in R



Hello Tom,



Please find the details: (i have changed name from eir to a1, rest all is
same)



str(a1$date)

 Factor w/ 32 levels "05-30-16","05-31-16",..: 1 1 1



head(a1$date)

05-30-16 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16

32 Levels: 05-30-16 05-31-16 06-01-16 06-02-16 06-03-16 06-04-16 06-05-16
06-06-16 06-07-16 06-08-16



Thanks again!!!



On Tue, Jul 26, 2016 at 9:38 PM, Tom Wright <tom at maladmin.com> wrote:

Hi again Shiva,
I think what we need to see is the output from:

str(eid$date)

and perhaps
head(eid$date)

If you can send this information before doing any processing on the date
(i.e. before the as.Date() function) we may be able to help.



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi Bhatia
Sent: July 26, 2016 11:46 AM
To: David L Carlson <dcarlson at tamu.edu>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] Date Time in R

Hi David please see the code and some reproducible data:
eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
lubridate library to help with the dates:
install.packages("lubridate")
library(lubridate)
eir$date <- mdy(eir$date)
weekdays <- wdy(eir$date)
week names <- wdy(eir$date, label = TRUE)

This the output from the file:

structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
"Date"),

month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class = "factor"),

day = c(30L, 30L, 30L),

weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
"Thu", "Tue","Wed"), class = "factor"),

survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't know","No"),

 a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
"day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
"data.frame")

There are several other variables that i have removed which are not relevant
in this context.

On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu> wrote:

> Show us the output, don?t just tell us what you are seeing. If the
> dates are correct in the csv file, show us the structure of the data
> frame you created with read.csv() and show the command(s) you used to
> convert the character data to date format. The solution is likely to
> be simple if you will cut/paste the R console and not just describe what
> is happening.
>
>
>
> David C
>
>
>

> *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
> *Sent:* Tuesday, July 26, 2016 10:08 AM
> *To:* David L Carlson
>
> *Subject:* Re: [R] Date Time in R

>
>
>
> Hi David,
>
> This gives the results accurately. The first line shows all the
> variable names and the rest shows all values stored for each of the
> variable. Here date is appearing as correct.
>
>
>
> Thanks, Shivi
>
>
>
> On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
> wrote:
>
> What does this produce?
>
> > readLines("YourCSVfilename.csv", n=5)
>
> If the data are in Excel, the date format used in .csv files is not
> always in the same as the format used when viewing dates in the
> spreadsheet.
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> Bhatia
> Sent: Tuesday, July 26, 2016 7:42 AM
> To: Marc Schwartz
> Cc: R-help
> Subject: Re: [R] Date Time in R
>
> Thanks Marc for the help. this really helps.
> I think there is some issue with the data saved in csv format for this
> variable as when i checked:
> str(eir$date)- this results in :-
> Date[1:5327], format: NA NA NA NA NA.
>
> Thanks again.
>
> On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
> wrote:
>
> > Hi,
> >
> > That eir$date might be a factor is irrelevant. There is an as.Date()
> > method for factors, which does the factor to character coercion
> internally
> > and then calls as.Date.character() on the result.
> >
> > Using the example data below:
> >
> > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
> >                            "05-30-16", "05-30-16", "05-30-16"))
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  1 variable:
> >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
> >
> > > eir
> >       date
> > 1 05-30-16
> > 2 05-30-16
> > 3 05-30-16
> > 4 05-30-16
> > 5 05-30-16
> > 6 05-30-16
> >
> > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  1 variable:
> >  $ date: Date, format: "2016-05-30" ...
> >
> > > eir
> >         date
> > 1 2016-05-30
> > 2 2016-05-30
> > 3 2016-05-30
> > 4 2016-05-30
> > 5 2016-05-30
> > 6 2016-05-30
> >
> > eir$days <- weekdays(eir$date)
> >
> > > str(eir)
> > 'data.frame':   6 obs. of  2 variables:
> >  $ date: Date, format: "2016-05-30" ...
> >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
> >
> > > eir
> >         date   days
> > 1 2016-05-30 Monday
> > 2 2016-05-30 Monday
> > 3 2016-05-30 Monday
> > 4 2016-05-30 Monday
> > 5 2016-05-30 Monday
> > 6 2016-05-30 Monday
> >
> >
> > I would check to be sure that you do not have any typos in your code.
> >
> > Regards,
> >
> > Marc Schwartz
> >
> >
> > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
> > >
> > > Hello Again,
> > >
> > > While i tried your solution as you suggested above it seems to be
> > working.
> > > Here is the output
> > > temp<- dput(head(eir$date))
> > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
> "05-30-16")
> > > however it still shows class(eir$date) as character and hence i
> > > cannot
> > find
> > > weekdays from this variable.
> > >
> > > Sorry but i still dont understand in totality how R reads dates
> > > even
> > though
> > > have tried enough.
> > >
> > > Regards, Shivi
> > >
> > >
> > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
> > > <shivipmp82 at gmail.com>
> > wrote:
> > >
> > >> Thanks Duncan for the quick response. I will check again as you
> > suggested.
> > >> If that doesn't work i will share a reproducible example.
> > >>
> > >> Thanks again!!!!
> > >>
> > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> > murdoch.duncan at gmail.com>
> > >> wrote:
> > >>
> > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> > >>>
> > >>>> Hi Team,
> > >>>>
> > >>>> This scenario may have come across a number of times however i
> checked
> > >>>> nabble & SO and couldn't find a solution hence request assistance.
> > >>>>
> > >>>> I have a date variable in my data-set eir. The class of this
> > >>>> var was character while i had read the file in r studio.
> > >>>> Example of date -
> > >>>> 05-30-16
> > >>>>
> > >>>> To change this i have used eir$date<- as.Date(eir$date,
> > >>>> "%m-%d-%y").
> > This
> > >>>> converts it to a date variable. However when i check few obs
> > >>>> with head(eir$date) all the results are <NA>.
> > >>>>
> > >>>
> > >>> I think you don't have character data like that, because I see
> > >>>
> > >>>> as.Date("05-30-16", "%m-%d-%y")
> > >>> [1] "2016-05-30"
> > >>>
> > >>> I'd guess eir$date is really a factor, because character data is
> > >>> frequently changed to factor automatically.  If that's the case,
> > >>> this should work for the conversion:
> > >>>
> > >>> as.Date(as.character(eir$date), "%m-%d-%y")
> > >>>
> > >>> If that doesn't work, you'll need to post something reproducible.
> > >>>
> > >>> Duncan Murdoch
> > >>>
> > >>> I also need to create weekdays from this date variable but until
> > >>> i
> get
> > >>>> this
> > >>>> resolved i cant find a weekday. For weekday i have used:
> > >>>> eir$week<- (eir$date)
> > >>>> eir$week<- weekdays(as.Date(eir$week))
> > >>>> class(eir$week)
> > >>>> eir$week<- as.factor(eir$week)
> > >>>> head(eir$week)
> > >>>>
> > >>>> Head of this eir$week results again as expected in <NA> but
> > >>>> shows
> > Levels:
> > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> > >>>>
> > >>>> Not sure what i should do here. Kindly suggest.
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help

> <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
> ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
> orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
> hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
> xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>

> and provide commented, minimal, self-contained, reproducible code.
>
>
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Tue Jul 26 18:33:09 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 22:03:09 +0530
Subject: [R] Date Time in R
In-Reply-To: <66ecc2246eb9b7638ee80a18b06541b5@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
	<cd847745387ec23289997617ec9e81af@mail.gmail.com>
	<CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>
	<66ecc2246eb9b7638ee80a18b06541b5@mail.gmail.com>
Message-ID: <CAB=p7SrP1cp7XcZxJj1apCCQgq7J2TH6eZzAghRSF=Kc365UUA@mail.gmail.com>

Thanks Tom for the recommendation. This is now working.

Apologies if this sounds like a juvenile but i am not very good with dates
in R. What i initially did when i saw date as factor and then tried
converting with lot many transformation that is where it went bad.

But thanks to all this is really what i wanted.

On Tue, Jul 26, 2016 at 9:58 PM, Tom Wright <tom at maladmin.com> wrote:

> So this looks correct so far.
>
>
>
> # convert strings (or factors) to a date object
>
> a1$date <- as.Date(a1$date, ?%m-%d-%y?)
>
>
>
> # Note the date object is now displayed in the format %Y-%m-%d by default
>
> # so no need for the mdy() or ymd() function
>
>
>
> # extract the weekdays as number
>
> weekdays <- wday(a1$date)
>
>
>
> # note can?t have spaces in variable names
>
> week_names <- wday(a1$date, label=TRUE)
>
>
>
>
>
> *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
> *Sent:* July 26, 2016 12:16 PM
> *To:* Tom Wright <tom at maladmin.com>
> *Cc:* David L Carlson <dcarlson at tamu.edu>; r-help <r-help at r-project.org>
>
> *Subject:* Re: [R] Date Time in R
>
>
>
> Hello Tom,
>
>
>
> Please find the details: (i have changed name from eir to a1, rest all is
> same)
>
>
>
> str(a1$date)
>
>  Factor w/ 32 levels "05-30-16","05-31-16",..: 1 1 1
>
>
>
> head(a1$date)
>
> 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16
>
> 32 Levels: 05-30-16 05-31-16 06-01-16 06-02-16 06-03-16 06-04-16 06-05-16
> 06-06-16 06-07-16 06-08-16
>
>
>
> Thanks again!!!
>
>
>
> On Tue, Jul 26, 2016 at 9:38 PM, Tom Wright <tom at maladmin.com> wrote:
>
> Hi again Shiva,
> I think what we need to see is the output from:
>
> str(eid$date)
>
> and perhaps
> head(eid$date)
>
> If you can send this information before doing any processing on the date
> (i.e. before the as.Date() function) we may be able to help.
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> Bhatia
> Sent: July 26, 2016 11:46 AM
> To: David L Carlson <dcarlson at tamu.edu>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] Date Time in R
>
> Hi David please see the code and some reproducible data:
> eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
> lubridate library to help with the dates:
> install.packages("lubridate")
> library(lubridate)
> eir$date <- mdy(eir$date)
> weekdays <- wdy(eir$date)
> week names <- wdy(eir$date, label = TRUE)
>
> This the output from the file:
>
> structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
> "Date"),
>
> month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class =
> "factor"),
>
> day = c(30L, 30L, 30L),
>
> weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
> "Thu", "Tue","Wed"), class = "factor"),
>
> survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't know","No"),
>
>  a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
> "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
> "data.frame")
>
> There are several other variables that i have removed which are not
> relevant
> in this context.
>
> On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu>
> wrote:
>
> > Show us the output, don?t just tell us what you are seeing. If the
> > dates are correct in the csv file, show us the structure of the data
> > frame you created with read.csv() and show the command(s) you used to
> > convert the character data to date format. The solution is likely to
> > be simple if you will cut/paste the R console and not just describe what
> > is happening.
> >
> >
> >
> > David C
> >
> >
> >
>
> > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
> > *Sent:* Tuesday, July 26, 2016 10:08 AM
> > *To:* David L Carlson
> >
> > *Subject:* Re: [R] Date Time in R
>
> >
> >
> >
> > Hi David,
> >
> > This gives the results accurately. The first line shows all the
> > variable names and the rest shows all values stored for each of the
> > variable. Here date is appearing as correct.
> >
> >
> >
> > Thanks, Shivi
> >
> >
> >
> > On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
> > wrote:
> >
> > What does this produce?
> >
> > > readLines("YourCSVfilename.csv", n=5)
> >
> > If the data are in Excel, the date format used in .csv files is not
> > always in the same as the format used when viewing dates in the
> > spreadsheet.
> >
> > -------------------------------------
> > David L Carlson
> > Department of Anthropology
> > Texas A&M University
> > College Station, TX 77840-4352
> >
> >
> >
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> > Bhatia
> > Sent: Tuesday, July 26, 2016 7:42 AM
> > To: Marc Schwartz
> > Cc: R-help
> > Subject: Re: [R] Date Time in R
> >
> > Thanks Marc for the help. this really helps.
> > I think there is some issue with the data saved in csv format for this
> > variable as when i checked:
> > str(eir$date)- this results in :-
> > Date[1:5327], format: NA NA NA NA NA.
> >
> > Thanks again.
> >
> > On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
> > wrote:
> >
> > > Hi,
> > >
> > > That eir$date might be a factor is irrelevant. There is an as.Date()
> > > method for factors, which does the factor to character coercion
> > internally
> > > and then calls as.Date.character() on the result.
> > >
> > > Using the example data below:
> > >
> > > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
> > >                            "05-30-16", "05-30-16", "05-30-16"))
> > >
> > > > str(eir)
> > > 'data.frame':   6 obs. of  1 variable:
> > >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
> > >
> > > > eir
> > >       date
> > > 1 05-30-16
> > > 2 05-30-16
> > > 3 05-30-16
> > > 4 05-30-16
> > > 5 05-30-16
> > > 6 05-30-16
> > >
> > > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
> > >
> > > > str(eir)
> > > 'data.frame':   6 obs. of  1 variable:
> > >  $ date: Date, format: "2016-05-30" ...
> > >
> > > > eir
> > >         date
> > > 1 2016-05-30
> > > 2 2016-05-30
> > > 3 2016-05-30
> > > 4 2016-05-30
> > > 5 2016-05-30
> > > 6 2016-05-30
> > >
> > > eir$days <- weekdays(eir$date)
> > >
> > > > str(eir)
> > > 'data.frame':   6 obs. of  2 variables:
> > >  $ date: Date, format: "2016-05-30" ...
> > >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
> > >
> > > > eir
> > >         date   days
> > > 1 2016-05-30 Monday
> > > 2 2016-05-30 Monday
> > > 3 2016-05-30 Monday
> > > 4 2016-05-30 Monday
> > > 5 2016-05-30 Monday
> > > 6 2016-05-30 Monday
> > >
> > >
> > > I would check to be sure that you do not have any typos in your code.
> > >
> > > Regards,
> > >
> > > Marc Schwartz
> > >
> > >
> > > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> > wrote:
> > > >
> > > > Hello Again,
> > > >
> > > > While i tried your solution as you suggested above it seems to be
> > > working.
> > > > Here is the output
> > > > temp<- dput(head(eir$date))
> > > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
> > "05-30-16")
> > > > however it still shows class(eir$date) as character and hence i
> > > > cannot
> > > find
> > > > weekdays from this variable.
> > > >
> > > > Sorry but i still dont understand in totality how R reads dates
> > > > even
> > > though
> > > > have tried enough.
> > > >
> > > > Regards, Shivi
> > > >
> > > >
> > > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
> > > > <shivipmp82 at gmail.com>
> > > wrote:
> > > >
> > > >> Thanks Duncan for the quick response. I will check again as you
> > > suggested.
> > > >> If that doesn't work i will share a reproducible example.
> > > >>
> > > >> Thanks again!!!!
> > > >>
> > > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> > > murdoch.duncan at gmail.com>
> > > >> wrote:
> > > >>
> > > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> > > >>>
> > > >>>> Hi Team,
> > > >>>>
> > > >>>> This scenario may have come across a number of times however i
> > checked
> > > >>>> nabble & SO and couldn't find a solution hence request assistance.
> > > >>>>
> > > >>>> I have a date variable in my data-set eir. The class of this
> > > >>>> var was character while i had read the file in r studio.
> > > >>>> Example of date -
> > > >>>> 05-30-16
> > > >>>>
> > > >>>> To change this i have used eir$date<- as.Date(eir$date,
> > > >>>> "%m-%d-%y").
> > > This
> > > >>>> converts it to a date variable. However when i check few obs
> > > >>>> with head(eir$date) all the results are <NA>.
> > > >>>>
> > > >>>
> > > >>> I think you don't have character data like that, because I see
> > > >>>
> > > >>>> as.Date("05-30-16", "%m-%d-%y")
> > > >>> [1] "2016-05-30"
> > > >>>
> > > >>> I'd guess eir$date is really a factor, because character data is
> > > >>> frequently changed to factor automatically.  If that's the case,
> > > >>> this should work for the conversion:
> > > >>>
> > > >>> as.Date(as.character(eir$date), "%m-%d-%y")
> > > >>>
> > > >>> If that doesn't work, you'll need to post something reproducible.
> > > >>>
> > > >>> Duncan Murdoch
> > > >>>
> > > >>> I also need to create weekdays from this date variable but until
> > > >>> i
> > get
> > > >>>> this
> > > >>>> resolved i cant find a weekday. For weekday i have used:
> > > >>>> eir$week<- (eir$date)
> > > >>>> eir$week<- weekdays(as.Date(eir$week))
> > > >>>> class(eir$week)
> > > >>>> eir$week<- as.factor(eir$week)
> > > >>>> head(eir$week)
> > > >>>>
> > > >>>> Head of this eir$week results again as expected in <NA> but
> > > >>>> shows
> > > Levels:
> > > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> > > >>>>
> > > >>>> Not sure what i should do here. Kindly suggest.
> > >
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
>
> > <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> > lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
> > ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
> > orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> > rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
> > hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
> > xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
>
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From udhay.mallela at gmail.com  Tue Jul 26 18:02:03 2016
From: udhay.mallela at gmail.com (udhayakanth reddy)
Date: Tue, 26 Jul 2016 21:32:03 +0530
Subject: [R] R3.3.1 - Windows10 OS - Overriding default Legend title with
 user specified title
Message-ID: <CAPdUWFrdzi_mOBt4ZPVE0-1bxeT94He+yKoighewO15MnR9Qyg@mail.gmail.com>

Team - Could you please do the needful.

I have a below code using ggplot2() package. I am trying to plot between
the variables - 'Company Advertising' and 'Brand Revenue' of my data frame
'htmltable' , when the another variable 'Industry' is 'Luxury'; using
ggplot() function. I am using another variable of my data frame 'Brand
Value' as colour variable.

p<- ggplot(htmltable[htmltable$Industry='Luxury',],aes(x='CompanyAdvertising',y='BrandRevenue')

q <- p+geom_point(aes(color='BrandValue',size='BrandValue') +
geom_text(label='Brand')

r <- q+xlab("Company Advertisiment in Billions")+ylab("Brand Revenue
in Billions") +ggtitle("Luxury")

r+theme(plot.title=element_text(size=10,face='bold'),legend.key=element_rect(fill='light
blue'))

Here, I want to change my legend title from "BrandValue" to
"BrandValue in Billions". Please suggest.

I tried using labs parameter in the below statement. But it is
resulting in 2 legends.

r <- q+xlab("Company Advertisiment in Billions")+ylab("Brand Revenue
in Billions") +ggtitle("Luxury")+labs(colour="BrandValue in Billions")


Thanks,

Udhay

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Tue Jul 26 18:49:20 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 26 Jul 2016 09:49:20 -0700
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SrP1cp7XcZxJj1apCCQgq7J2TH6eZzAghRSF=Kc365UUA@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
	<cd847745387ec23289997617ec9e81af@mail.gmail.com>
	<CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>
	<66ecc2246eb9b7638ee80a18b06541b5@mail.gmail.com>
	<CAB=p7SrP1cp7XcZxJj1apCCQgq7J2TH6eZzAghRSF=Kc365UUA@mail.gmail.com>
Message-ID: <CAF8bMcb8jxHa0Fv2sp61x0c=xGmi8W-2MrNr5205cVDi4CRPuw@mail.gmail.com>

Can you show us what transformations you tried when
you saw the column called 'date' was a factor?

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Jul 26, 2016 at 9:33 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:

> Thanks Tom for the recommendation. This is now working.
>
> Apologies if this sounds like a juvenile but i am not very good with dates
> in R. What i initially did when i saw date as factor and then tried
> converting with lot many transformation that is where it went bad.
>
> But thanks to all this is really what i wanted.
>
> On Tue, Jul 26, 2016 at 9:58 PM, Tom Wright <tom at maladmin.com> wrote:
>
> > So this looks correct so far.
> >
> >
> >
> > # convert strings (or factors) to a date object
> >
> > a1$date <- as.Date(a1$date, ?%m-%d-%y?)
> >
> >
> >
> > # Note the date object is now displayed in the format %Y-%m-%d by default
> >
> > # so no need for the mdy() or ymd() function
> >
> >
> >
> > # extract the weekdays as number
> >
> > weekdays <- wday(a1$date)
> >
> >
> >
> > # note can?t have spaces in variable names
> >
> > week_names <- wday(a1$date, label=TRUE)
> >
> >
> >
> >
> >
> > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
> > *Sent:* July 26, 2016 12:16 PM
> > *To:* Tom Wright <tom at maladmin.com>
> > *Cc:* David L Carlson <dcarlson at tamu.edu>; r-help <r-help at r-project.org>
> >
> > *Subject:* Re: [R] Date Time in R
> >
> >
> >
> > Hello Tom,
> >
> >
> >
> > Please find the details: (i have changed name from eir to a1, rest all is
> > same)
> >
> >
> >
> > str(a1$date)
> >
> >  Factor w/ 32 levels "05-30-16","05-31-16",..: 1 1 1
> >
> >
> >
> > head(a1$date)
> >
> > 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16
> >
> > 32 Levels: 05-30-16 05-31-16 06-01-16 06-02-16 06-03-16 06-04-16 06-05-16
> > 06-06-16 06-07-16 06-08-16
> >
> >
> >
> > Thanks again!!!
> >
> >
> >
> > On Tue, Jul 26, 2016 at 9:38 PM, Tom Wright <tom at maladmin.com> wrote:
> >
> > Hi again Shiva,
> > I think what we need to see is the output from:
> >
> > str(eid$date)
> >
> > and perhaps
> > head(eid$date)
> >
> > If you can send this information before doing any processing on the date
> > (i.e. before the as.Date() function) we may be able to help.
> >
> >
> >
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> > Bhatia
> > Sent: July 26, 2016 11:46 AM
> > To: David L Carlson <dcarlson at tamu.edu>
> > Cc: r-help <r-help at r-project.org>
> > Subject: Re: [R] Date Time in R
> >
> > Hi David please see the code and some reproducible data:
> > eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
> > lubridate library to help with the dates:
> > install.packages("lubridate")
> > library(lubridate)
> > eir$date <- mdy(eir$date)
> > weekdays <- wdy(eir$date)
> > week names <- wdy(eir$date, label = TRUE)
> >
> > This the output from the file:
> >
> > structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
> > "Date"),
> >
> > month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class =
> > "factor"),
> >
> > day = c(30L, 30L, 30L),
> >
> > weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
> > "Thu", "Tue","Wed"), class = "factor"),
> >
> > survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't
> know","No"),
> >
> >  a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
> > "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
> > "data.frame")
> >
> > There are several other variables that i have removed which are not
> > relevant
> > in this context.
> >
> > On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu>
> > wrote:
> >
> > > Show us the output, don?t just tell us what you are seeing. If the
> > > dates are correct in the csv file, show us the structure of the data
> > > frame you created with read.csv() and show the command(s) you used to
> > > convert the character data to date format. The solution is likely to
> > > be simple if you will cut/paste the R console and not just describe
> what
> > > is happening.
> > >
> > >
> > >
> > > David C
> > >
> > >
> > >
> >
> > > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
> > > *Sent:* Tuesday, July 26, 2016 10:08 AM
> > > *To:* David L Carlson
> > >
> > > *Subject:* Re: [R] Date Time in R
> >
> > >
> > >
> > >
> > > Hi David,
> > >
> > > This gives the results accurately. The first line shows all the
> > > variable names and the rest shows all values stored for each of the
> > > variable. Here date is appearing as correct.
> > >
> > >
> > >
> > > Thanks, Shivi
> > >
> > >
> > >
> > > On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
> > > wrote:
> > >
> > > What does this produce?
> > >
> > > > readLines("YourCSVfilename.csv", n=5)
> > >
> > > If the data are in Excel, the date format used in .csv files is not
> > > always in the same as the format used when viewing dates in the
> > > spreadsheet.
> > >
> > > -------------------------------------
> > > David L Carlson
> > > Department of Anthropology
> > > Texas A&M University
> > > College Station, TX 77840-4352
> > >
> > >
> > >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
> > > Bhatia
> > > Sent: Tuesday, July 26, 2016 7:42 AM
> > > To: Marc Schwartz
> > > Cc: R-help
> > > Subject: Re: [R] Date Time in R
> > >
> > > Thanks Marc for the help. this really helps.
> > > I think there is some issue with the data saved in csv format for this
> > > variable as when i checked:
> > > str(eir$date)- this results in :-
> > > Date[1:5327], format: NA NA NA NA NA.
> > >
> > > Thanks again.
> > >
> > > On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
> > > wrote:
> > >
> > > > Hi,
> > > >
> > > > That eir$date might be a factor is irrelevant. There is an as.Date()
> > > > method for factors, which does the factor to character coercion
> > > internally
> > > > and then calls as.Date.character() on the result.
> > > >
> > > > Using the example data below:
> > > >
> > > > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
> > > >                            "05-30-16", "05-30-16", "05-30-16"))
> > > >
> > > > > str(eir)
> > > > 'data.frame':   6 obs. of  1 variable:
> > > >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
> > > >
> > > > > eir
> > > >       date
> > > > 1 05-30-16
> > > > 2 05-30-16
> > > > 3 05-30-16
> > > > 4 05-30-16
> > > > 5 05-30-16
> > > > 6 05-30-16
> > > >
> > > > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
> > > >
> > > > > str(eir)
> > > > 'data.frame':   6 obs. of  1 variable:
> > > >  $ date: Date, format: "2016-05-30" ...
> > > >
> > > > > eir
> > > >         date
> > > > 1 2016-05-30
> > > > 2 2016-05-30
> > > > 3 2016-05-30
> > > > 4 2016-05-30
> > > > 5 2016-05-30
> > > > 6 2016-05-30
> > > >
> > > > eir$days <- weekdays(eir$date)
> > > >
> > > > > str(eir)
> > > > 'data.frame':   6 obs. of  2 variables:
> > > >  $ date: Date, format: "2016-05-30" ...
> > > >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
> > > >
> > > > > eir
> > > >         date   days
> > > > 1 2016-05-30 Monday
> > > > 2 2016-05-30 Monday
> > > > 3 2016-05-30 Monday
> > > > 4 2016-05-30 Monday
> > > > 5 2016-05-30 Monday
> > > > 6 2016-05-30 Monday
> > > >
> > > >
> > > > I would check to be sure that you do not have any typos in your code.
> > > >
> > > > Regards,
> > > >
> > > > Marc Schwartz
> > > >
> > > >
> > > > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> > > wrote:
> > > > >
> > > > > Hello Again,
> > > > >
> > > > > While i tried your solution as you suggested above it seems to be
> > > > working.
> > > > > Here is the output
> > > > > temp<- dput(head(eir$date))
> > > > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
> > > "05-30-16")
> > > > > however it still shows class(eir$date) as character and hence i
> > > > > cannot
> > > > find
> > > > > weekdays from this variable.
> > > > >
> > > > > Sorry but i still dont understand in totality how R reads dates
> > > > > even
> > > > though
> > > > > have tried enough.
> > > > >
> > > > > Regards, Shivi
> > > > >
> > > > >
> > > > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
> > > > > <shivipmp82 at gmail.com>
> > > > wrote:
> > > > >
> > > > >> Thanks Duncan for the quick response. I will check again as you
> > > > suggested.
> > > > >> If that doesn't work i will share a reproducible example.
> > > > >>
> > > > >> Thanks again!!!!
> > > > >>
> > > > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
> > > > murdoch.duncan at gmail.com>
> > > > >> wrote:
> > > > >>
> > > > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
> > > > >>>
> > > > >>>> Hi Team,
> > > > >>>>
> > > > >>>> This scenario may have come across a number of times however i
> > > checked
> > > > >>>> nabble & SO and couldn't find a solution hence request
> assistance.
> > > > >>>>
> > > > >>>> I have a date variable in my data-set eir. The class of this
> > > > >>>> var was character while i had read the file in r studio.
> > > > >>>> Example of date -
> > > > >>>> 05-30-16
> > > > >>>>
> > > > >>>> To change this i have used eir$date<- as.Date(eir$date,
> > > > >>>> "%m-%d-%y").
> > > > This
> > > > >>>> converts it to a date variable. However when i check few obs
> > > > >>>> with head(eir$date) all the results are <NA>.
> > > > >>>>
> > > > >>>
> > > > >>> I think you don't have character data like that, because I see
> > > > >>>
> > > > >>>> as.Date("05-30-16", "%m-%d-%y")
> > > > >>> [1] "2016-05-30"
> > > > >>>
> > > > >>> I'd guess eir$date is really a factor, because character data is
> > > > >>> frequently changed to factor automatically.  If that's the case,
> > > > >>> this should work for the conversion:
> > > > >>>
> > > > >>> as.Date(as.character(eir$date), "%m-%d-%y")
> > > > >>>
> > > > >>> If that doesn't work, you'll need to post something reproducible.
> > > > >>>
> > > > >>> Duncan Murdoch
> > > > >>>
> > > > >>> I also need to create weekdays from this date variable but until
> > > > >>> i
> > > get
> > > > >>>> this
> > > > >>>> resolved i cant find a weekday. For weekday i have used:
> > > > >>>> eir$week<- (eir$date)
> > > > >>>> eir$week<- weekdays(as.Date(eir$week))
> > > > >>>> class(eir$week)
> > > > >>>> eir$week<- as.factor(eir$week)
> > > > >>>> head(eir$week)
> > > > >>>>
> > > > >>>> Head of this eir$week results again as expected in <NA> but
> > > > >>>> shows
> > > > Levels:
> > > > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
> > > > >>>>
> > > > >>>> Not sure what i should do here. Kindly suggest.
> > > >
> > > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> >
> > > <https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
> > > lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
> > > ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
> > > orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > <https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
> > > rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
> > > hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
> > > xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
> >
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Tue Jul 26 18:54:37 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 22:24:37 +0530
Subject: [R] Date Time in R
In-Reply-To: <CAF8bMcb8jxHa0Fv2sp61x0c=xGmi8W-2MrNr5205cVDi4CRPuw@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
	<cd847745387ec23289997617ec9e81af@mail.gmail.com>
	<CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>
	<66ecc2246eb9b7638ee80a18b06541b5@mail.gmail.com>
	<CAB=p7SrP1cp7XcZxJj1apCCQgq7J2TH6eZzAghRSF=Kc365UUA@mail.gmail.com>
	<CAF8bMcb8jxHa0Fv2sp61x0c=xGmi8W-2MrNr5205cVDi4CRPuw@mail.gmail.com>
Message-ID: <CAB=p7Sozs8z7Rr+OMnGGSQBF+m-6yZ5A8cSApHgEb5eXbb5VCA@mail.gmail.com>

Hi Bill,

I have mentioned the transformation all on the email above.

Regards, Shivi

On Tue, Jul 26, 2016 at 10:19 PM, William Dunlap <wdunlap at tibco.com> wrote:

> Can you show us what transformations you tried when
> you saw the column called 'date' was a factor?
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Tue, Jul 26, 2016 at 9:33 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
>
>> Thanks Tom for the recommendation. This is now working.
>>
>> Apologies if this sounds like a juvenile but i am not very good with dates
>> in R. What i initially did when i saw date as factor and then tried
>> converting with lot many transformation that is where it went bad.
>>
>> But thanks to all this is really what i wanted.
>>
>> On Tue, Jul 26, 2016 at 9:58 PM, Tom Wright <tom at maladmin.com> wrote:
>>
>> > So this looks correct so far.
>> >
>> >
>> >
>> > # convert strings (or factors) to a date object
>> >
>> > a1$date <- as.Date(a1$date, ?%m-%d-%y?)
>> >
>> >
>> >
>> > # Note the date object is now displayed in the format %Y-%m-%d by
>> default
>> >
>> > # so no need for the mdy() or ymd() function
>> >
>> >
>> >
>> > # extract the weekdays as number
>> >
>> > weekdays <- wday(a1$date)
>> >
>> >
>> >
>> > # note can?t have spaces in variable names
>> >
>> > week_names <- wday(a1$date, label=TRUE)
>> >
>> >
>> >
>> >
>> >
>> > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>> > *Sent:* July 26, 2016 12:16 PM
>> > *To:* Tom Wright <tom at maladmin.com>
>> > *Cc:* David L Carlson <dcarlson at tamu.edu>; r-help <r-help at r-project.org
>> >
>>
>> >
>> > *Subject:* Re: [R] Date Time in R
>> >
>> >
>> >
>> > Hello Tom,
>> >
>> >
>> >
>> > Please find the details: (i have changed name from eir to a1, rest all
>> is
>> > same)
>> >
>> >
>> >
>> > str(a1$date)
>> >
>> >  Factor w/ 32 levels "05-30-16","05-31-16",..: 1 1 1
>> >
>> >
>> >
>> > head(a1$date)
>> >
>> > 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16
>> >
>> > 32 Levels: 05-30-16 05-31-16 06-01-16 06-02-16 06-03-16 06-04-16
>> 06-05-16
>> > 06-06-16 06-07-16 06-08-16
>> >
>> >
>> >
>> > Thanks again!!!
>> >
>> >
>> >
>> > On Tue, Jul 26, 2016 at 9:38 PM, Tom Wright <tom at maladmin.com> wrote:
>> >
>> > Hi again Shiva,
>> > I think what we need to see is the output from:
>> >
>> > str(eid$date)
>> >
>> > and perhaps
>> > head(eid$date)
>> >
>> > If you can send this information before doing any processing on the date
>> > (i.e. before the as.Date() function) we may be able to help.
>> >
>> >
>> >
>> > -----Original Message-----
>> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
>> > Bhatia
>> > Sent: July 26, 2016 11:46 AM
>> > To: David L Carlson <dcarlson at tamu.edu>
>> > Cc: r-help <r-help at r-project.org>
>> > Subject: Re: [R] Date Time in R
>> >
>> > Hi David please see the code and some reproducible data:
>> > eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
>> > lubridate library to help with the dates:
>> > install.packages("lubridate")
>> > library(lubridate)
>> > eir$date <- mdy(eir$date)
>> > weekdays <- wdy(eir$date)
>> > week names <- wdy(eir$date, label = TRUE)
>> >
>> > This the output from the file:
>> >
>> > structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class =
>> > "Date"),
>> >
>> > month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class =
>> > "factor"),
>> >
>> > day = c(30L, 30L, 30L),
>> >
>> > weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat", "Sun",
>> > "Thu", "Tue","Wed"), class = "factor"),
>> >
>> > survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't
>> know","No"),
>> >
>> >  a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
>> > "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
>> > "data.frame")
>> >
>> > There are several other variables that i have removed which are not
>> > relevant
>> > in this context.
>> >
>> > On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu>
>> > wrote:
>> >
>> > > Show us the output, don?t just tell us what you are seeing. If the
>> > > dates are correct in the csv file, show us the structure of the data
>> > > frame you created with read.csv() and show the command(s) you used to
>> > > convert the character data to date format. The solution is likely to
>> > > be simple if you will cut/paste the R console and not just describe
>> what
>> > > is happening.
>> > >
>> > >
>> > >
>> > > David C
>> > >
>> > >
>> > >
>> >
>> > > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>> > > *Sent:* Tuesday, July 26, 2016 10:08 AM
>> > > *To:* David L Carlson
>> > >
>> > > *Subject:* Re: [R] Date Time in R
>> >
>> > >
>> > >
>> > >
>> > > Hi David,
>> > >
>> > > This gives the results accurately. The first line shows all the
>> > > variable names and the rest shows all values stored for each of the
>> > > variable. Here date is appearing as correct.
>> > >
>> > >
>> > >
>> > > Thanks, Shivi
>> > >
>> > >
>> > >
>> > > On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
>> > > wrote:
>> > >
>> > > What does this produce?
>> > >
>> > > > readLines("YourCSVfilename.csv", n=5)
>> > >
>> > > If the data are in Excel, the date format used in .csv files is not
>> > > always in the same as the format used when viewing dates in the
>> > > spreadsheet.
>> > >
>> > > -------------------------------------
>> > > David L Carlson
>> > > Department of Anthropology
>> > > Texas A&M University
>> > > College Station, TX 77840-4352
>> > >
>> > >
>> > >
>> > > -----Original Message-----
>> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
>> > > Bhatia
>> > > Sent: Tuesday, July 26, 2016 7:42 AM
>> > > To: Marc Schwartz
>> > > Cc: R-help
>> > > Subject: Re: [R] Date Time in R
>> > >
>> > > Thanks Marc for the help. this really helps.
>> > > I think there is some issue with the data saved in csv format for this
>> > > variable as when i checked:
>> > > str(eir$date)- this results in :-
>> > > Date[1:5327], format: NA NA NA NA NA.
>> > >
>> > > Thanks again.
>> > >
>> > > On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com>
>> > > wrote:
>> > >
>> > > > Hi,
>> > > >
>> > > > That eir$date might be a factor is irrelevant. There is an as.Date()
>> > > > method for factors, which does the factor to character coercion
>> > > internally
>> > > > and then calls as.Date.character() on the result.
>> > > >
>> > > > Using the example data below:
>> > > >
>> > > > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>> > > >                            "05-30-16", "05-30-16", "05-30-16"))
>> > > >
>> > > > > str(eir)
>> > > > 'data.frame':   6 obs. of  1 variable:
>> > > >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>> > > >
>> > > > > eir
>> > > >       date
>> > > > 1 05-30-16
>> > > > 2 05-30-16
>> > > > 3 05-30-16
>> > > > 4 05-30-16
>> > > > 5 05-30-16
>> > > > 6 05-30-16
>> > > >
>> > > > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>> > > >
>> > > > > str(eir)
>> > > > 'data.frame':   6 obs. of  1 variable:
>> > > >  $ date: Date, format: "2016-05-30" ...
>> > > >
>> > > > > eir
>> > > >         date
>> > > > 1 2016-05-30
>> > > > 2 2016-05-30
>> > > > 3 2016-05-30
>> > > > 4 2016-05-30
>> > > > 5 2016-05-30
>> > > > 6 2016-05-30
>> > > >
>> > > > eir$days <- weekdays(eir$date)
>> > > >
>> > > > > str(eir)
>> > > > 'data.frame':   6 obs. of  2 variables:
>> > > >  $ date: Date, format: "2016-05-30" ...
>> > > >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>> > > >
>> > > > > eir
>> > > >         date   days
>> > > > 1 2016-05-30 Monday
>> > > > 2 2016-05-30 Monday
>> > > > 3 2016-05-30 Monday
>> > > > 4 2016-05-30 Monday
>> > > > 5 2016-05-30 Monday
>> > > > 6 2016-05-30 Monday
>> > > >
>> > > >
>> > > > I would check to be sure that you do not have any typos in your
>> code.
>> > > >
>> > > > Regards,
>> > > >
>> > > > Marc Schwartz
>> > > >
>> > > >
>> > > > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
>> > > wrote:
>> > > > >
>> > > > > Hello Again,
>> > > > >
>> > > > > While i tried your solution as you suggested above it seems to be
>> > > > working.
>> > > > > Here is the output
>> > > > > temp<- dput(head(eir$date))
>> > > > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
>> > > "05-30-16")
>> > > > > however it still shows class(eir$date) as character and hence i
>> > > > > cannot
>> > > > find
>> > > > > weekdays from this variable.
>> > > > >
>> > > > > Sorry but i still dont understand in totality how R reads dates
>> > > > > even
>> > > > though
>> > > > > have tried enough.
>> > > > >
>> > > > > Regards, Shivi
>> > > > >
>> > > > >
>> > > > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
>> > > > > <shivipmp82 at gmail.com>
>> > > > wrote:
>> > > > >
>> > > > >> Thanks Duncan for the quick response. I will check again as you
>> > > > suggested.
>> > > > >> If that doesn't work i will share a reproducible example.
>> > > > >>
>> > > > >> Thanks again!!!!
>> > > > >>
>> > > > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
>> > > > murdoch.duncan at gmail.com>
>> > > > >> wrote:
>> > > > >>
>> > > > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>> > > > >>>
>> > > > >>>> Hi Team,
>> > > > >>>>
>> > > > >>>> This scenario may have come across a number of times however i
>> > > checked
>> > > > >>>> nabble & SO and couldn't find a solution hence request
>> assistance.
>> > > > >>>>
>> > > > >>>> I have a date variable in my data-set eir. The class of this
>> > > > >>>> var was character while i had read the file in r studio.
>> > > > >>>> Example of date -
>> > > > >>>> 05-30-16
>> > > > >>>>
>> > > > >>>> To change this i have used eir$date<- as.Date(eir$date,
>> > > > >>>> "%m-%d-%y").
>> > > > This
>> > > > >>>> converts it to a date variable. However when i check few obs
>> > > > >>>> with head(eir$date) all the results are <NA>.
>> > > > >>>>
>> > > > >>>
>> > > > >>> I think you don't have character data like that, because I see
>> > > > >>>
>> > > > >>>> as.Date("05-30-16", "%m-%d-%y")
>> > > > >>> [1] "2016-05-30"
>> > > > >>>
>> > > > >>> I'd guess eir$date is really a factor, because character data is
>> > > > >>> frequently changed to factor automatically.  If that's the case,
>> > > > >>> this should work for the conversion:
>> > > > >>>
>> > > > >>> as.Date(as.character(eir$date), "%m-%d-%y")
>> > > > >>>
>> > > > >>> If that doesn't work, you'll need to post something
>> reproducible.
>> > > > >>>
>> > > > >>> Duncan Murdoch
>> > > > >>>
>> > > > >>> I also need to create weekdays from this date variable but until
>> > > > >>> i
>> > > get
>> > > > >>>> this
>> > > > >>>> resolved i cant find a weekday. For weekday i have used:
>> > > > >>>> eir$week<- (eir$date)
>> > > > >>>> eir$week<- weekdays(as.Date(eir$week))
>> > > > >>>> class(eir$week)
>> > > > >>>> eir$week<- as.factor(eir$week)
>> > > > >>>> head(eir$week)
>> > > > >>>>
>> > > > >>>> Head of this eir$week results again as expected in <NA> but
>> > > > >>>> shows
>> > > > Levels:
>> > > > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>> > > > >>>>
>> > > > >>>> Not sure what i should do here. Kindly suggest.
>> > > >
>> > > >
>> > >
>> > >         [[alternative HTML version deleted]]
>> > >
>> > > ______________________________________________
>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> >
>> > > <
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>> > > lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
>> > > ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
>> > > orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
>> > > PLEASE do read the posting guide
>> > > http://www.R-project.org/posting-guide.html
>> > > <
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>> > > rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
>> > > hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
>> > > xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
>> >
>> > > and provide commented, minimal, self-contained, reproducible code.
>> > >
>> > >
>> > >
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Tue Jul 26 18:59:20 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Tue, 26 Jul 2016 22:29:20 +0530
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7Sozs8z7Rr+OMnGGSQBF+m-6yZ5A8cSApHgEb5eXbb5VCA@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
	<cd847745387ec23289997617ec9e81af@mail.gmail.com>
	<CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>
	<66ecc2246eb9b7638ee80a18b06541b5@mail.gmail.com>
	<CAB=p7SrP1cp7XcZxJj1apCCQgq7J2TH6eZzAghRSF=Kc365UUA@mail.gmail.com>
	<CAF8bMcb8jxHa0Fv2sp61x0c=xGmi8W-2MrNr5205cVDi4CRPuw@mail.gmail.com>
	<CAB=p7Sozs8z7Rr+OMnGGSQBF+m-6yZ5A8cSApHgEb5eXbb5VCA@mail.gmail.com>
Message-ID: <CAB=p7SqPruO2EGZ6of0p2pyeWOb_un9DNDR6tyRbE8gzaT098w@mail.gmail.com>

These are some of the codes will i have
eir$date<- as.Date(eir$date, "%d-%m-%Y")
class(eir$date)
eir$week<- (eir$date)
eir$week<- weekdays(as.Date(eir$week))
eir$week<- as.factor(eir$week)

On Tue, Jul 26, 2016 at 10:24 PM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:

> Hi Bill,
>
> I have mentioned the transformation all on the email above.
>
> Regards, Shivi
>
> On Tue, Jul 26, 2016 at 10:19 PM, William Dunlap <wdunlap at tibco.com>
> wrote:
>
>> Can you show us what transformations you tried when
>> you saw the column called 'date' was a factor?
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Tue, Jul 26, 2016 at 9:33 AM, Shivi Bhatia <shivipmp82 at gmail.com>
>> wrote:
>>
>>> Thanks Tom for the recommendation. This is now working.
>>>
>>> Apologies if this sounds like a juvenile but i am not very good with
>>> dates
>>> in R. What i initially did when i saw date as factor and then tried
>>> converting with lot many transformation that is where it went bad.
>>>
>>> But thanks to all this is really what i wanted.
>>>
>>> On Tue, Jul 26, 2016 at 9:58 PM, Tom Wright <tom at maladmin.com> wrote:
>>>
>>> > So this looks correct so far.
>>> >
>>> >
>>> >
>>> > # convert strings (or factors) to a date object
>>> >
>>> > a1$date <- as.Date(a1$date, ?%m-%d-%y?)
>>> >
>>> >
>>> >
>>> > # Note the date object is now displayed in the format %Y-%m-%d by
>>> default
>>> >
>>> > # so no need for the mdy() or ymd() function
>>> >
>>> >
>>> >
>>> > # extract the weekdays as number
>>> >
>>> > weekdays <- wday(a1$date)
>>> >
>>> >
>>> >
>>> > # note can?t have spaces in variable names
>>> >
>>> > week_names <- wday(a1$date, label=TRUE)
>>> >
>>> >
>>> >
>>> >
>>> >
>>> > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>>> > *Sent:* July 26, 2016 12:16 PM
>>> > *To:* Tom Wright <tom at maladmin.com>
>>> > *Cc:* David L Carlson <dcarlson at tamu.edu>; r-help <
>>> r-help at r-project.org>
>>>
>>> >
>>> > *Subject:* Re: [R] Date Time in R
>>> >
>>> >
>>> >
>>> > Hello Tom,
>>> >
>>> >
>>> >
>>> > Please find the details: (i have changed name from eir to a1, rest all
>>> is
>>> > same)
>>> >
>>> >
>>> >
>>> > str(a1$date)
>>> >
>>> >  Factor w/ 32 levels "05-30-16","05-31-16",..: 1 1 1
>>> >
>>> >
>>> >
>>> > head(a1$date)
>>> >
>>> > 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16
>>> >
>>> > 32 Levels: 05-30-16 05-31-16 06-01-16 06-02-16 06-03-16 06-04-16
>>> 06-05-16
>>> > 06-06-16 06-07-16 06-08-16
>>> >
>>> >
>>> >
>>> > Thanks again!!!
>>> >
>>> >
>>> >
>>> > On Tue, Jul 26, 2016 at 9:38 PM, Tom Wright <tom at maladmin.com> wrote:
>>> >
>>> > Hi again Shiva,
>>> > I think what we need to see is the output from:
>>> >
>>> > str(eid$date)
>>> >
>>> > and perhaps
>>> > head(eid$date)
>>> >
>>> > If you can send this information before doing any processing on the
>>> date
>>> > (i.e. before the as.Date() function) we may be able to help.
>>> >
>>> >
>>> >
>>> > -----Original Message-----
>>> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
>>> > Bhatia
>>> > Sent: July 26, 2016 11:46 AM
>>> > To: David L Carlson <dcarlson at tamu.edu>
>>> > Cc: r-help <r-help at r-project.org>
>>> > Subject: Re: [R] Date Time in R
>>> >
>>> > Hi David please see the code and some reproducible data:
>>> > eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
>>> > lubridate library to help with the dates:
>>> > install.packages("lubridate")
>>> > library(lubridate)
>>> > eir$date <- mdy(eir$date)
>>> > weekdays <- wdy(eir$date)
>>> > week names <- wdy(eir$date, label = TRUE)
>>> >
>>> > This the output from the file:
>>> >
>>> > structure(list(date = structure(c(NA_real_, NA_real_, NA_real_), class
>>> =
>>> > "Date"),
>>> >
>>> > month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class =
>>> > "factor"),
>>> >
>>> > day = c(30L, 30L, 30L),
>>> >
>>> > weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat",
>>> "Sun",
>>> > "Thu", "Tue","Wed"), class = "factor"),
>>> >
>>> > survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't
>>> know","No"),
>>> >
>>> >  a = c("05-30-16", "05-30-16", "05-30-16")), .Names = c("date","month",
>>> > "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
>>> > "data.frame")
>>> >
>>> > There are several other variables that i have removed which are not
>>> > relevant
>>> > in this context.
>>> >
>>> > On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu>
>>> > wrote:
>>> >
>>> > > Show us the output, don?t just tell us what you are seeing. If the
>>> > > dates are correct in the csv file, show us the structure of the data
>>> > > frame you created with read.csv() and show the command(s) you used to
>>> > > convert the character data to date format. The solution is likely to
>>> > > be simple if you will cut/paste the R console and not just describe
>>> what
>>> > > is happening.
>>> > >
>>> > >
>>> > >
>>> > > David C
>>> > >
>>> > >
>>> > >
>>> >
>>> > > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>>> > > *Sent:* Tuesday, July 26, 2016 10:08 AM
>>> > > *To:* David L Carlson
>>> > >
>>> > > *Subject:* Re: [R] Date Time in R
>>> >
>>> > >
>>> > >
>>> > >
>>> > > Hi David,
>>> > >
>>> > > This gives the results accurately. The first line shows all the
>>> > > variable names and the rest shows all values stored for each of the
>>> > > variable. Here date is appearing as correct.
>>> > >
>>> > >
>>> > >
>>> > > Thanks, Shivi
>>> > >
>>> > >
>>> > >
>>> > > On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu>
>>> > > wrote:
>>> > >
>>> > > What does this produce?
>>> > >
>>> > > > readLines("YourCSVfilename.csv", n=5)
>>> > >
>>> > > If the data are in Excel, the date format used in .csv files is not
>>> > > always in the same as the format used when viewing dates in the
>>> > > spreadsheet.
>>> > >
>>> > > -------------------------------------
>>> > > David L Carlson
>>> > > Department of Anthropology
>>> > > Texas A&M University
>>> > > College Station, TX 77840-4352
>>> > >
>>> > >
>>> > >
>>> > > -----Original Message-----
>>> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>> Shivi
>>> > > Bhatia
>>> > > Sent: Tuesday, July 26, 2016 7:42 AM
>>> > > To: Marc Schwartz
>>> > > Cc: R-help
>>> > > Subject: Re: [R] Date Time in R
>>> > >
>>> > > Thanks Marc for the help. this really helps.
>>> > > I think there is some issue with the data saved in csv format for
>>> this
>>> > > variable as when i checked:
>>> > > str(eir$date)- this results in :-
>>> > > Date[1:5327], format: NA NA NA NA NA.
>>> > >
>>> > > Thanks again.
>>> > >
>>> > > On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <marc_schwartz at me.com
>>> >
>>> > > wrote:
>>> > >
>>> > > > Hi,
>>> > > >
>>> > > > That eir$date might be a factor is irrelevant. There is an
>>> as.Date()
>>> > > > method for factors, which does the factor to character coercion
>>> > > internally
>>> > > > and then calls as.Date.character() on the result.
>>> > > >
>>> > > > Using the example data below:
>>> > > >
>>> > > > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>>> > > >                            "05-30-16", "05-30-16", "05-30-16"))
>>> > > >
>>> > > > > str(eir)
>>> > > > 'data.frame':   6 obs. of  1 variable:
>>> > > >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>>> > > >
>>> > > > > eir
>>> > > >       date
>>> > > > 1 05-30-16
>>> > > > 2 05-30-16
>>> > > > 3 05-30-16
>>> > > > 4 05-30-16
>>> > > > 5 05-30-16
>>> > > > 6 05-30-16
>>> > > >
>>> > > > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>>> > > >
>>> > > > > str(eir)
>>> > > > 'data.frame':   6 obs. of  1 variable:
>>> > > >  $ date: Date, format: "2016-05-30" ...
>>> > > >
>>> > > > > eir
>>> > > >         date
>>> > > > 1 2016-05-30
>>> > > > 2 2016-05-30
>>> > > > 3 2016-05-30
>>> > > > 4 2016-05-30
>>> > > > 5 2016-05-30
>>> > > > 6 2016-05-30
>>> > > >
>>> > > > eir$days <- weekdays(eir$date)
>>> > > >
>>> > > > > str(eir)
>>> > > > 'data.frame':   6 obs. of  2 variables:
>>> > > >  $ date: Date, format: "2016-05-30" ...
>>> > > >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>>> > > >
>>> > > > > eir
>>> > > >         date   days
>>> > > > 1 2016-05-30 Monday
>>> > > > 2 2016-05-30 Monday
>>> > > > 3 2016-05-30 Monday
>>> > > > 4 2016-05-30 Monday
>>> > > > 5 2016-05-30 Monday
>>> > > > 6 2016-05-30 Monday
>>> > > >
>>> > > >
>>> > > > I would check to be sure that you do not have any typos in your
>>> code.
>>> > > >
>>> > > > Regards,
>>> > > >
>>> > > > Marc Schwartz
>>> > > >
>>> > > >
>>> > > > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com>
>>> > > wrote:
>>> > > > >
>>> > > > > Hello Again,
>>> > > > >
>>> > > > > While i tried your solution as you suggested above it seems to be
>>> > > > working.
>>> > > > > Here is the output
>>> > > > > temp<- dput(head(eir$date))
>>> > > > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
>>> > > "05-30-16")
>>> > > > > however it still shows class(eir$date) as character and hence i
>>> > > > > cannot
>>> > > > find
>>> > > > > weekdays from this variable.
>>> > > > >
>>> > > > > Sorry but i still dont understand in totality how R reads dates
>>> > > > > even
>>> > > > though
>>> > > > > have tried enough.
>>> > > > >
>>> > > > > Regards, Shivi
>>> > > > >
>>> > > > >
>>> > > > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
>>> > > > > <shivipmp82 at gmail.com>
>>> > > > wrote:
>>> > > > >
>>> > > > >> Thanks Duncan for the quick response. I will check again as you
>>> > > > suggested.
>>> > > > >> If that doesn't work i will share a reproducible example.
>>> > > > >>
>>> > > > >> Thanks again!!!!
>>> > > > >>
>>> > > > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
>>> > > > murdoch.duncan at gmail.com>
>>> > > > >> wrote:
>>> > > > >>
>>> > > > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>>> > > > >>>
>>> > > > >>>> Hi Team,
>>> > > > >>>>
>>> > > > >>>> This scenario may have come across a number of times however i
>>> > > checked
>>> > > > >>>> nabble & SO and couldn't find a solution hence request
>>> assistance.
>>> > > > >>>>
>>> > > > >>>> I have a date variable in my data-set eir. The class of this
>>> > > > >>>> var was character while i had read the file in r studio.
>>> > > > >>>> Example of date -
>>> > > > >>>> 05-30-16
>>> > > > >>>>
>>> > > > >>>> To change this i have used eir$date<- as.Date(eir$date,
>>> > > > >>>> "%m-%d-%y").
>>> > > > This
>>> > > > >>>> converts it to a date variable. However when i check few obs
>>> > > > >>>> with head(eir$date) all the results are <NA>.
>>> > > > >>>>
>>> > > > >>>
>>> > > > >>> I think you don't have character data like that, because I see
>>> > > > >>>
>>> > > > >>>> as.Date("05-30-16", "%m-%d-%y")
>>> > > > >>> [1] "2016-05-30"
>>> > > > >>>
>>> > > > >>> I'd guess eir$date is really a factor, because character data
>>> is
>>> > > > >>> frequently changed to factor automatically.  If that's the
>>> case,
>>> > > > >>> this should work for the conversion:
>>> > > > >>>
>>> > > > >>> as.Date(as.character(eir$date), "%m-%d-%y")
>>> > > > >>>
>>> > > > >>> If that doesn't work, you'll need to post something
>>> reproducible.
>>> > > > >>>
>>> > > > >>> Duncan Murdoch
>>> > > > >>>
>>> > > > >>> I also need to create weekdays from this date variable but
>>> until
>>> > > > >>> i
>>> > > get
>>> > > > >>>> this
>>> > > > >>>> resolved i cant find a weekday. For weekday i have used:
>>> > > > >>>> eir$week<- (eir$date)
>>> > > > >>>> eir$week<- weekdays(as.Date(eir$week))
>>> > > > >>>> class(eir$week)
>>> > > > >>>> eir$week<- as.factor(eir$week)
>>> > > > >>>> head(eir$week)
>>> > > > >>>>
>>> > > > >>>> Head of this eir$week results again as expected in <NA> but
>>> > > > >>>> shows
>>> > > > Levels:
>>> > > > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>>> > > > >>>>
>>> > > > >>>> Not sure what i should do here. Kindly suggest.
>>> > > >
>>> > > >
>>> > >
>>> > >         [[alternative HTML version deleted]]
>>> > >
>>> > > ______________________________________________
>>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>>> >
>>> > > <
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>>> > >
>>> lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
>>> > >
>>> ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
>>> > > orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
>>> > > PLEASE do read the posting guide
>>> > > http://www.R-project.org/posting-guide.html
>>> > > <
>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>>> > >
>>> rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
>>> > >
>>> hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
>>> > > xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
>>> >
>>> > > and provide commented, minimal, self-contained, reproducible code.
>>> > >
>>> > >
>>> > >
>>> >
>>> >         [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> > http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>> >
>>> >
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From tom at maladmin.com  Tue Jul 26 21:44:06 2016
From: tom at maladmin.com (Tom Wright)
Date: Tue, 26 Jul 2016 15:44:06 -0400
Subject: [R] R3.3.1 - Windows10 OS - Overriding default Legend title
 with user specified title
In-Reply-To: <CAPdUWFrdzi_mOBt4ZPVE0-1bxeT94He+yKoighewO15MnR9Qyg@mail.gmail.com>
References: <CAPdUWFrdzi_mOBt4ZPVE0-1bxeT94He+yKoighewO15MnR9Qyg@mail.gmail.com>
Message-ID: <bcd08d1a145a266a13602d6e03ae840f@mail.gmail.com>

A quick google for "ggplot2 change legend text" turns up several hits.
This stackexchange question has several recipes.
http://stats.stackexchange.com/questions/5007/how-can-i-change-the-title-o
f-a-legend-in-ggplot2

including the correct use of the labs() function;
labs(aesthetic='BrandValue in Billions')

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
udhayakanth reddy
Sent: July 26, 2016 12:02 PM
To: r-help at r-project.org
Subject: [R] R3.3.1 - Windows10 OS - Overriding default Legend title with
user specified title

Team - Could you please do the needful.

I have a below code using ggplot2() package. I am trying to plot between
the variables - 'Company Advertising' and 'Brand Revenue' of my data frame
'htmltable' , when the another variable 'Industry' is 'Luxury'; using
ggplot() function. I am using another variable of my data frame 'Brand
Value' as colour variable.

p<-
ggplot(htmltable[htmltable$Industry='Luxury',],aes(x='CompanyAdvertising',
y='BrandRevenue')

q <- p+geom_point(aes(color='BrandValue',size='BrandValue') +
geom_text(label='Brand')

r <- q+xlab("Company Advertisiment in Billions")+ylab("Brand Revenue in
Billions") +ggtitle("Luxury")

r+theme(plot.title=element_text(size=10,face='bold'),legend.key=element_
r+rect(fill='light
blue'))

Here, I want to change my legend title from "BrandValue" to "BrandValue in
Billions". Please suggest.

I tried using labs parameter in the below statement. But it is resulting
in 2 legends.

r <- q+xlab("Company Advertisiment in Billions")+ylab("Brand Revenue in
Billions") +ggtitle("Luxury")+labs(colour="BrandValue in Billions")


Thanks,

Udhay

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Tue Jul 26 21:56:19 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 26 Jul 2016 12:56:19 -0700
Subject: [R] Date Time in R
In-Reply-To: <CAB=p7SqPruO2EGZ6of0p2pyeWOb_un9DNDR6tyRbE8gzaT098w@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
	<cd847745387ec23289997617ec9e81af@mail.gmail.com>
	<CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>
	<66ecc2246eb9b7638ee80a18b06541b5@mail.gmail.com>
	<CAB=p7SrP1cp7XcZxJj1apCCQgq7J2TH6eZzAghRSF=Kc365UUA@mail.gmail.com>
	<CAF8bMcb8jxHa0Fv2sp61x0c=xGmi8W-2MrNr5205cVDi4CRPuw@mail.gmail.com>
	<CAB=p7Sozs8z7Rr+OMnGGSQBF+m-6yZ5A8cSApHgEb5eXbb5VCA@mail.gmail.com>
	<CAB=p7SqPruO2EGZ6of0p2pyeWOb_un9DNDR6tyRbE8gzaT098w@mail.gmail.com>
Message-ID: <CAF8bMcaFGb+hzh5UJBOb9VbRTb8Z+u8yCeeN10-4e9b1YC68GQ@mail.gmail.com>

Your original mail said
   Example of date - 05-30-16

   To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y")

but by showing us the actual code I see you reversed the m and d and
capitalized the Y in your first attempt, causing NA's since your data was
not in that format.

The big mistake was overwriting the raw data with this mistakenly processed
data so subsequent attempts at processing it failed.

I recommend never overwriting the raw data - read the data into a
data.frame called, say, 'raw' and make a new data.frame called, say,
'cleaned' with all the transformations based on the raw data.  Iterate the
cleaning step until you have it right but don't alter the raw data.  Use
functions like head(), str(), plot(), and summary() to see if things look
basically ok.   The transform() function may help with that.  E.g.,

    raw <- read.table(text="date\n05-30-16\n06-01-16\n06-27-16",
header=TRUE)
    str(raw)
    #'data.frame':   3 obs. of  1 variable:
    #$ date: Factor w/ 3 levels "05-30-16","06-01-16",..: 1 2 3
    cleaned <- transform(raw, date=as.Date(date, format="%d-%m-%Y"))
    head(cleaned)
    #        date
    #1       <NA>
    #2 0016-01-06
    #3       <NA>
    cleaned <- transform(raw, date=as.Date(date, format="%d-%m-%y"))
    head(cleaned)
    #        date
    #1       <NA>
    #2 2016-01-06
    #3       <NA>
    cleaned <- transform(raw, date=as.Date(date, format="%m-%d-%y"))
    head(cleaned)
    #        date
    #1 2016-05-30
    #2 2016-06-01
    #3 2016-06-27
That last one looks ok, so move on to your next tranformation step.




Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Jul 26, 2016 at 9:59 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:

> These are some of the codes will i have
> eir$date<- as.Date(eir$date, "%d-%m-%Y")
> class(eir$date)
> eir$week<- (eir$date)
> eir$week<- weekdays(as.Date(eir$week))
> eir$week<- as.factor(eir$week)
>
> On Tue, Jul 26, 2016 at 10:24 PM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
>
>> Hi Bill,
>>
>> I have mentioned the transformation all on the email above.
>>
>> Regards, Shivi
>>
>> On Tue, Jul 26, 2016 at 10:19 PM, William Dunlap <wdunlap at tibco.com>
>> wrote:
>>
>>> Can you show us what transformations you tried when
>>> you saw the column called 'date' was a factor?
>>>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>> On Tue, Jul 26, 2016 at 9:33 AM, Shivi Bhatia <shivipmp82 at gmail.com>
>>> wrote:
>>>
>>>> Thanks Tom for the recommendation. This is now working.
>>>>
>>>> Apologies if this sounds like a juvenile but i am not very good with
>>>> dates
>>>> in R. What i initially did when i saw date as factor and then tried
>>>> converting with lot many transformation that is where it went bad.
>>>>
>>>> But thanks to all this is really what i wanted.
>>>>
>>>> On Tue, Jul 26, 2016 at 9:58 PM, Tom Wright <tom at maladmin.com> wrote:
>>>>
>>>> > So this looks correct so far.
>>>> >
>>>> >
>>>> >
>>>> > # convert strings (or factors) to a date object
>>>> >
>>>> > a1$date <- as.Date(a1$date, ?%m-%d-%y?)
>>>> >
>>>> >
>>>> >
>>>> > # Note the date object is now displayed in the format %Y-%m-%d by
>>>> default
>>>> >
>>>> > # so no need for the mdy() or ymd() function
>>>> >
>>>> >
>>>> >
>>>> > # extract the weekdays as number
>>>> >
>>>> > weekdays <- wday(a1$date)
>>>> >
>>>> >
>>>> >
>>>> > # note can?t have spaces in variable names
>>>> >
>>>> > week_names <- wday(a1$date, label=TRUE)
>>>> >
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>>>> > *Sent:* July 26, 2016 12:16 PM
>>>> > *To:* Tom Wright <tom at maladmin.com>
>>>> > *Cc:* David L Carlson <dcarlson at tamu.edu>; r-help <
>>>> r-help at r-project.org>
>>>>
>>>> >
>>>> > *Subject:* Re: [R] Date Time in R
>>>> >
>>>> >
>>>> >
>>>> > Hello Tom,
>>>> >
>>>> >
>>>> >
>>>> > Please find the details: (i have changed name from eir to a1, rest
>>>> all is
>>>> > same)
>>>> >
>>>> >
>>>> >
>>>> > str(a1$date)
>>>> >
>>>> >  Factor w/ 32 levels "05-30-16","05-31-16",..: 1 1 1
>>>> >
>>>> >
>>>> >
>>>> > head(a1$date)
>>>> >
>>>> > 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16
>>>> >
>>>> > 32 Levels: 05-30-16 05-31-16 06-01-16 06-02-16 06-03-16 06-04-16
>>>> 06-05-16
>>>> > 06-06-16 06-07-16 06-08-16
>>>> >
>>>> >
>>>> >
>>>> > Thanks again!!!
>>>> >
>>>> >
>>>> >
>>>> > On Tue, Jul 26, 2016 at 9:38 PM, Tom Wright <tom at maladmin.com> wrote:
>>>> >
>>>> > Hi again Shiva,
>>>> > I think what we need to see is the output from:
>>>> >
>>>> > str(eid$date)
>>>> >
>>>> > and perhaps
>>>> > head(eid$date)
>>>> >
>>>> > If you can send this information before doing any processing on the
>>>> date
>>>> > (i.e. before the as.Date() function) we may be able to help.
>>>> >
>>>> >
>>>> >
>>>> > -----Original Message-----
>>>> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Shivi
>>>> > Bhatia
>>>> > Sent: July 26, 2016 11:46 AM
>>>> > To: David L Carlson <dcarlson at tamu.edu>
>>>> > Cc: r-help <r-help at r-project.org>
>>>> > Subject: Re: [R] Date Time in R
>>>> >
>>>> > Hi David please see the code and some reproducible data:
>>>> > eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
>>>> > lubridate library to help with the dates:
>>>> > install.packages("lubridate")
>>>> > library(lubridate)
>>>> > eir$date <- mdy(eir$date)
>>>> > weekdays <- wdy(eir$date)
>>>> > week names <- wdy(eir$date, label = TRUE)
>>>> >
>>>> > This the output from the file:
>>>> >
>>>> > structure(list(date = structure(c(NA_real_, NA_real_, NA_real_),
>>>> class =
>>>> > "Date"),
>>>> >
>>>> > month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class =
>>>> > "factor"),
>>>> >
>>>> > day = c(30L, 30L, 30L),
>>>> >
>>>> > weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat",
>>>> "Sun",
>>>> > "Thu", "Tue","Wed"), class = "factor"),
>>>> >
>>>> > survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't
>>>> know","No"),
>>>> >
>>>> >  a = c("05-30-16", "05-30-16", "05-30-16")), .Names =
>>>> c("date","month",
>>>> > "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
>>>> > "data.frame")
>>>> >
>>>> > There are several other variables that i have removed which are not
>>>> > relevant
>>>> > in this context.
>>>> >
>>>> > On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu>
>>>> > wrote:
>>>> >
>>>> > > Show us the output, don?t just tell us what you are seeing. If the
>>>> > > dates are correct in the csv file, show us the structure of the data
>>>> > > frame you created with read.csv() and show the command(s) you used
>>>> to
>>>> > > convert the character data to date format. The solution is likely to
>>>> > > be simple if you will cut/paste the R console and not just describe
>>>> what
>>>> > > is happening.
>>>> > >
>>>> > >
>>>> > >
>>>> > > David C
>>>> > >
>>>> > >
>>>> > >
>>>> >
>>>> > > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>>>> > > *Sent:* Tuesday, July 26, 2016 10:08 AM
>>>> > > *To:* David L Carlson
>>>> > >
>>>> > > *Subject:* Re: [R] Date Time in R
>>>> >
>>>> > >
>>>> > >
>>>> > >
>>>> > > Hi David,
>>>> > >
>>>> > > This gives the results accurately. The first line shows all the
>>>> > > variable names and the rest shows all values stored for each of the
>>>> > > variable. Here date is appearing as correct.
>>>> > >
>>>> > >
>>>> > >
>>>> > > Thanks, Shivi
>>>> > >
>>>> > >
>>>> > >
>>>> > > On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <dcarlson at tamu.edu
>>>> >
>>>> > > wrote:
>>>> > >
>>>> > > What does this produce?
>>>> > >
>>>> > > > readLines("YourCSVfilename.csv", n=5)
>>>> > >
>>>> > > If the data are in Excel, the date format used in .csv files is not
>>>> > > always in the same as the format used when viewing dates in the
>>>> > > spreadsheet.
>>>> > >
>>>> > > -------------------------------------
>>>> > > David L Carlson
>>>> > > Department of Anthropology
>>>> > > Texas A&M University
>>>> > > College Station, TX 77840-4352
>>>> > >
>>>> > >
>>>> > >
>>>> > > -----Original Message-----
>>>> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>>> Shivi
>>>> > > Bhatia
>>>> > > Sent: Tuesday, July 26, 2016 7:42 AM
>>>> > > To: Marc Schwartz
>>>> > > Cc: R-help
>>>> > > Subject: Re: [R] Date Time in R
>>>> > >
>>>> > > Thanks Marc for the help. this really helps.
>>>> > > I think there is some issue with the data saved in csv format for
>>>> this
>>>> > > variable as when i checked:
>>>> > > str(eir$date)- this results in :-
>>>> > > Date[1:5327], format: NA NA NA NA NA.
>>>> > >
>>>> > > Thanks again.
>>>> > >
>>>> > > On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <
>>>> marc_schwartz at me.com>
>>>> > > wrote:
>>>> > >
>>>> > > > Hi,
>>>> > > >
>>>> > > > That eir$date might be a factor is irrelevant. There is an
>>>> as.Date()
>>>> > > > method for factors, which does the factor to character coercion
>>>> > > internally
>>>> > > > and then calls as.Date.character() on the result.
>>>> > > >
>>>> > > > Using the example data below:
>>>> > > >
>>>> > > > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>>>> > > >                            "05-30-16", "05-30-16", "05-30-16"))
>>>> > > >
>>>> > > > > str(eir)
>>>> > > > 'data.frame':   6 obs. of  1 variable:
>>>> > > >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>>>> > > >
>>>> > > > > eir
>>>> > > >       date
>>>> > > > 1 05-30-16
>>>> > > > 2 05-30-16
>>>> > > > 3 05-30-16
>>>> > > > 4 05-30-16
>>>> > > > 5 05-30-16
>>>> > > > 6 05-30-16
>>>> > > >
>>>> > > > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>>>> > > >
>>>> > > > > str(eir)
>>>> > > > 'data.frame':   6 obs. of  1 variable:
>>>> > > >  $ date: Date, format: "2016-05-30" ...
>>>> > > >
>>>> > > > > eir
>>>> > > >         date
>>>> > > > 1 2016-05-30
>>>> > > > 2 2016-05-30
>>>> > > > 3 2016-05-30
>>>> > > > 4 2016-05-30
>>>> > > > 5 2016-05-30
>>>> > > > 6 2016-05-30
>>>> > > >
>>>> > > > eir$days <- weekdays(eir$date)
>>>> > > >
>>>> > > > > str(eir)
>>>> > > > 'data.frame':   6 obs. of  2 variables:
>>>> > > >  $ date: Date, format: "2016-05-30" ...
>>>> > > >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>>>> > > >
>>>> > > > > eir
>>>> > > >         date   days
>>>> > > > 1 2016-05-30 Monday
>>>> > > > 2 2016-05-30 Monday
>>>> > > > 3 2016-05-30 Monday
>>>> > > > 4 2016-05-30 Monday
>>>> > > > 5 2016-05-30 Monday
>>>> > > > 6 2016-05-30 Monday
>>>> > > >
>>>> > > >
>>>> > > > I would check to be sure that you do not have any typos in your
>>>> code.
>>>> > > >
>>>> > > > Regards,
>>>> > > >
>>>> > > > Marc Schwartz
>>>> > > >
>>>> > > >
>>>> > > > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <shivipmp82 at gmail.com
>>>> >
>>>> > > wrote:
>>>> > > > >
>>>> > > > > Hello Again,
>>>> > > > >
>>>> > > > > While i tried your solution as you suggested above it seems to
>>>> be
>>>> > > > working.
>>>> > > > > Here is the output
>>>> > > > > temp<- dput(head(eir$date))
>>>> > > > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
>>>> > > "05-30-16")
>>>> > > > > however it still shows class(eir$date) as character and hence i
>>>> > > > > cannot
>>>> > > > find
>>>> > > > > weekdays from this variable.
>>>> > > > >
>>>> > > > > Sorry but i still dont understand in totality how R reads dates
>>>> > > > > even
>>>> > > > though
>>>> > > > > have tried enough.
>>>> > > > >
>>>> > > > > Regards, Shivi
>>>> > > > >
>>>> > > > >
>>>> > > > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
>>>> > > > > <shivipmp82 at gmail.com>
>>>> > > > wrote:
>>>> > > > >
>>>> > > > >> Thanks Duncan for the quick response. I will check again as you
>>>> > > > suggested.
>>>> > > > >> If that doesn't work i will share a reproducible example.
>>>> > > > >>
>>>> > > > >> Thanks again!!!!
>>>> > > > >>
>>>> > > > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
>>>> > > > murdoch.duncan at gmail.com>
>>>> > > > >> wrote:
>>>> > > > >>
>>>> > > > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>>>> > > > >>>
>>>> > > > >>>> Hi Team,
>>>> > > > >>>>
>>>> > > > >>>> This scenario may have come across a number of times however
>>>> i
>>>> > > checked
>>>> > > > >>>> nabble & SO and couldn't find a solution hence request
>>>> assistance.
>>>> > > > >>>>
>>>> > > > >>>> I have a date variable in my data-set eir. The class of this
>>>> > > > >>>> var was character while i had read the file in r studio.
>>>> > > > >>>> Example of date -
>>>> > > > >>>> 05-30-16
>>>> > > > >>>>
>>>> > > > >>>> To change this i have used eir$date<- as.Date(eir$date,
>>>> > > > >>>> "%m-%d-%y").
>>>> > > > This
>>>> > > > >>>> converts it to a date variable. However when i check few obs
>>>> > > > >>>> with head(eir$date) all the results are <NA>.
>>>> > > > >>>>
>>>> > > > >>>
>>>> > > > >>> I think you don't have character data like that, because I see
>>>> > > > >>>
>>>> > > > >>>> as.Date("05-30-16", "%m-%d-%y")
>>>> > > > >>> [1] "2016-05-30"
>>>> > > > >>>
>>>> > > > >>> I'd guess eir$date is really a factor, because character data
>>>> is
>>>> > > > >>> frequently changed to factor automatically.  If that's the
>>>> case,
>>>> > > > >>> this should work for the conversion:
>>>> > > > >>>
>>>> > > > >>> as.Date(as.character(eir$date), "%m-%d-%y")
>>>> > > > >>>
>>>> > > > >>> If that doesn't work, you'll need to post something
>>>> reproducible.
>>>> > > > >>>
>>>> > > > >>> Duncan Murdoch
>>>> > > > >>>
>>>> > > > >>> I also need to create weekdays from this date variable but
>>>> until
>>>> > > > >>> i
>>>> > > get
>>>> > > > >>>> this
>>>> > > > >>>> resolved i cant find a weekday. For weekday i have used:
>>>> > > > >>>> eir$week<- (eir$date)
>>>> > > > >>>> eir$week<- weekdays(as.Date(eir$week))
>>>> > > > >>>> class(eir$week)
>>>> > > > >>>> eir$week<- as.factor(eir$week)
>>>> > > > >>>> head(eir$week)
>>>> > > > >>>>
>>>> > > > >>>> Head of this eir$week results again as expected in <NA> but
>>>> > > > >>>> shows
>>>> > > > Levels:
>>>> > > > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>>>> > > > >>>>
>>>> > > > >>>> Not sure what i should do here. Kindly suggest.
>>>> > > >
>>>> > > >
>>>> > >
>>>> > >         [[alternative HTML version deleted]]
>>>> > >
>>>> > > ______________________________________________
>>>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>>>> >
>>>> > > <
>>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>>>> > >
>>>> lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
>>>> > >
>>>> ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
>>>> > > orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
>>>> > > PLEASE do read the posting guide
>>>> > > http://www.R-project.org/posting-guide.html
>>>> > > <
>>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>>>> > >
>>>> rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
>>>> > >
>>>> hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
>>>> > > xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
>>>> >
>>>> > > and provide commented, minimal, self-contained, reproducible code.
>>>> > >
>>>> > >
>>>> > >
>>>> >
>>>> >         [[alternative HTML version deleted]]
>>>> >
>>>> > ______________________________________________
>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>> > PLEASE do read the posting guide
>>>> > http://www.R-project.org/posting-guide.html
>>>> > and provide commented, minimal, self-contained, reproducible code.
>>>> >
>>>> >
>>>> >
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Tue Jul 26 21:59:32 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Wed, 27 Jul 2016 01:29:32 +0530
Subject: [R] Date Time in R
In-Reply-To: <CAF8bMcaFGb+hzh5UJBOb9VbRTb8Z+u8yCeeN10-4e9b1YC68GQ@mail.gmail.com>
References: <CAB=p7SqeiRf62RVF3gjEHFSsMqB=iV7BHW92jTHYSh9SRD7EDQ@mail.gmail.com>
	<2d9e8b29-458f-097c-e291-6dfb1cd776bb@gmail.com>
	<CAB=p7SoxuRAaYxAD4U=qbaUv67nG+rWgX75zqT9gu43Bqmijrg@mail.gmail.com>
	<CAB=p7SoPcyp6Az+0MSO_rk6YsHxQaVnO3VpgQ1jFnEGXrDk8TQ@mail.gmail.com>
	<B3FC73B7-52AD-421B-83DD-7D620082F938@me.com>
	<CAB=p7Son9kaJRZiZos5acMpvNmGXeH6gx4pP9qL800=JYjxs7g@mail.gmail.com>
	<bfa650fa27b94092aba8402b59ca13dc@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SoQuj3CzyyikUZNfbicSG0cYgO-86x4-HqnzSviAfQYLA@mail.gmail.com>
	<4e20fbfe32ed4901ae1af099c07e7eab@exch-2p-mbx-t2.ads.tamu.edu>
	<CAB=p7SqWc97g6wToG4FsS-JT7CfiKyXk3jaPJ0BFK_b9MZQaVQ@mail.gmail.com>
	<cd847745387ec23289997617ec9e81af@mail.gmail.com>
	<CAB=p7SrmuEbHS+ThefbCrtJvrjZTB8y67LPukHrXGm7Z8ZD5xA@mail.gmail.com>
	<66ecc2246eb9b7638ee80a18b06541b5@mail.gmail.com>
	<CAB=p7SrP1cp7XcZxJj1apCCQgq7J2TH6eZzAghRSF=Kc365UUA@mail.gmail.com>
	<CAF8bMcb8jxHa0Fv2sp61x0c=xGmi8W-2MrNr5205cVDi4CRPuw@mail.gmail.com>
	<CAB=p7Sozs8z7Rr+OMnGGSQBF+m-6yZ5A8cSApHgEb5eXbb5VCA@mail.gmail.com>
	<CAB=p7SqPruO2EGZ6of0p2pyeWOb_un9DNDR6tyRbE8gzaT098w@mail.gmail.com>
	<CAF8bMcaFGb+hzh5UJBOb9VbRTb8Z+u8yCeeN10-4e9b1YC68GQ@mail.gmail.com>
Message-ID: <CAB=p7SqXEZ2EEjQkUaQnYUCUoiejVyyNoLm=8MhrQg3LZwWKiQ@mail.gmail.com>

Thank you for all your tips. Will keep these checked.

Have a great day

On Wed, Jul 27, 2016 at 1:26 AM, William Dunlap <wdunlap at tibco.com> wrote:

> Your original mail said
>    Example of date - 05-30-16
>
>    To change this i have used eir$date<- as.Date(eir$date, "%m-%d-%y")
>
> but by showing us the actual code I see you reversed the m and d and
> capitalized the Y in your first attempt, causing NA's since your data was
> not in that format.
>
> The big mistake was overwriting the raw data with this mistakenly
> processed data so subsequent attempts at processing it failed.
>
> I recommend never overwriting the raw data - read the data into a
> data.frame called, say, 'raw' and make a new data.frame called, say,
> 'cleaned' with all the transformations based on the raw data.  Iterate the
> cleaning step until you have it right but don't alter the raw data.  Use
> functions like head(), str(), plot(), and summary() to see if things look
> basically ok.   The transform() function may help with that.  E.g.,
>
>     raw <- read.table(text="date\n05-30-16\n06-01-16\n06-27-16",
> header=TRUE)
>     str(raw)
>     #'data.frame':   3 obs. of  1 variable:
>     #$ date: Factor w/ 3 levels "05-30-16","06-01-16",..: 1 2 3
>     cleaned <- transform(raw, date=as.Date(date, format="%d-%m-%Y"))
>     head(cleaned)
>     #        date
>     #1       <NA>
>     #2 0016-01-06
>     #3       <NA>
>     cleaned <- transform(raw, date=as.Date(date, format="%d-%m-%y"))
>     head(cleaned)
>     #        date
>     #1       <NA>
>     #2 2016-01-06
>     #3       <NA>
>     cleaned <- transform(raw, date=as.Date(date, format="%m-%d-%y"))
>     head(cleaned)
>     #        date
>     #1 2016-05-30
>     #2 2016-06-01
>     #3 2016-06-27
> That last one looks ok, so move on to your next tranformation step.
>
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Tue, Jul 26, 2016 at 9:59 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
>
>> These are some of the codes will i have
>> eir$date<- as.Date(eir$date, "%d-%m-%Y")
>> class(eir$date)
>> eir$week<- (eir$date)
>> eir$week<- weekdays(as.Date(eir$week))
>> eir$week<- as.factor(eir$week)
>>
>> On Tue, Jul 26, 2016 at 10:24 PM, Shivi Bhatia <shivipmp82 at gmail.com>
>> wrote:
>>
>>> Hi Bill,
>>>
>>> I have mentioned the transformation all on the email above.
>>>
>>> Regards, Shivi
>>>
>>> On Tue, Jul 26, 2016 at 10:19 PM, William Dunlap <wdunlap at tibco.com>
>>> wrote:
>>>
>>>> Can you show us what transformations you tried when
>>>> you saw the column called 'date' was a factor?
>>>>
>>>> Bill Dunlap
>>>> TIBCO Software
>>>> wdunlap tibco.com
>>>>
>>>> On Tue, Jul 26, 2016 at 9:33 AM, Shivi Bhatia <shivipmp82 at gmail.com>
>>>> wrote:
>>>>
>>>>> Thanks Tom for the recommendation. This is now working.
>>>>>
>>>>> Apologies if this sounds like a juvenile but i am not very good with
>>>>> dates
>>>>> in R. What i initially did when i saw date as factor and then tried
>>>>> converting with lot many transformation that is where it went bad.
>>>>>
>>>>> But thanks to all this is really what i wanted.
>>>>>
>>>>> On Tue, Jul 26, 2016 at 9:58 PM, Tom Wright <tom at maladmin.com> wrote:
>>>>>
>>>>> > So this looks correct so far.
>>>>> >
>>>>> >
>>>>> >
>>>>> > # convert strings (or factors) to a date object
>>>>> >
>>>>> > a1$date <- as.Date(a1$date, ?%m-%d-%y?)
>>>>> >
>>>>> >
>>>>> >
>>>>> > # Note the date object is now displayed in the format %Y-%m-%d by
>>>>> default
>>>>> >
>>>>> > # so no need for the mdy() or ymd() function
>>>>> >
>>>>> >
>>>>> >
>>>>> > # extract the weekdays as number
>>>>> >
>>>>> > weekdays <- wday(a1$date)
>>>>> >
>>>>> >
>>>>> >
>>>>> > # note can?t have spaces in variable names
>>>>> >
>>>>> > week_names <- wday(a1$date, label=TRUE)
>>>>> >
>>>>> >
>>>>> >
>>>>> >
>>>>> >
>>>>> > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>>>>> > *Sent:* July 26, 2016 12:16 PM
>>>>> > *To:* Tom Wright <tom at maladmin.com>
>>>>> > *Cc:* David L Carlson <dcarlson at tamu.edu>; r-help <
>>>>> r-help at r-project.org>
>>>>>
>>>>> >
>>>>> > *Subject:* Re: [R] Date Time in R
>>>>> >
>>>>> >
>>>>> >
>>>>> > Hello Tom,
>>>>> >
>>>>> >
>>>>> >
>>>>> > Please find the details: (i have changed name from eir to a1, rest
>>>>> all is
>>>>> > same)
>>>>> >
>>>>> >
>>>>> >
>>>>> > str(a1$date)
>>>>> >
>>>>> >  Factor w/ 32 levels "05-30-16","05-31-16",..: 1 1 1
>>>>> >
>>>>> >
>>>>> >
>>>>> > head(a1$date)
>>>>> >
>>>>> > 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16 05-30-16
>>>>> >
>>>>> > 32 Levels: 05-30-16 05-31-16 06-01-16 06-02-16 06-03-16 06-04-16
>>>>> 06-05-16
>>>>> > 06-06-16 06-07-16 06-08-16
>>>>> >
>>>>> >
>>>>> >
>>>>> > Thanks again!!!
>>>>> >
>>>>> >
>>>>> >
>>>>> > On Tue, Jul 26, 2016 at 9:38 PM, Tom Wright <tom at maladmin.com>
>>>>> wrote:
>>>>> >
>>>>> > Hi again Shiva,
>>>>> > I think what we need to see is the output from:
>>>>> >
>>>>> > str(eid$date)
>>>>> >
>>>>> > and perhaps
>>>>> > head(eid$date)
>>>>> >
>>>>> > If you can send this information before doing any processing on the
>>>>> date
>>>>> > (i.e. before the as.Date() function) we may be able to help.
>>>>> >
>>>>> >
>>>>> >
>>>>> > -----Original Message-----
>>>>> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>>>> Shivi
>>>>> > Bhatia
>>>>> > Sent: July 26, 2016 11:46 AM
>>>>> > To: David L Carlson <dcarlson at tamu.edu>
>>>>> > Cc: r-help <r-help at r-project.org>
>>>>> > Subject: Re: [R] Date Time in R
>>>>> >
>>>>> > Hi David please see the code and some reproducible data:
>>>>> > eir$date<- as.Date(eir$date,format = "%m-%d-%y")  then i had used the
>>>>> > lubridate library to help with the dates:
>>>>> > install.packages("lubridate")
>>>>> > library(lubridate)
>>>>> > eir$date <- mdy(eir$date)
>>>>> > weekdays <- wdy(eir$date)
>>>>> > week names <- wdy(eir$date, label = TRUE)
>>>>> >
>>>>> > This the output from the file:
>>>>> >
>>>>> > structure(list(date = structure(c(NA_real_, NA_real_, NA_real_),
>>>>> class =
>>>>> > "Date"),
>>>>> >
>>>>> > month = structure(c(2L, 2L, 2L), .Label = c("Jun","May"), class =
>>>>> > "factor"),
>>>>> >
>>>>> > day = c(30L, 30L, 30L),
>>>>> >
>>>>> > weekday = structure(c(2L,2L, 2L), .Label = c("Fri", "Mon", "Sat",
>>>>> "Sun",
>>>>> > "Thu", "Tue","Wed"), class = "factor"),
>>>>> >
>>>>> > survey_rating = c(3L, 2L, 3L), query_status = c("Yes", "Don't
>>>>> know","No"),
>>>>> >
>>>>> >  a = c("05-30-16", "05-30-16", "05-30-16")), .Names =
>>>>> c("date","month",
>>>>> > "day", "weekday", "survey_rating"), row.names = c(NA, 3L), class =
>>>>> > "data.frame")
>>>>> >
>>>>> > There are several other variables that i have removed which are not
>>>>> > relevant
>>>>> > in this context.
>>>>> >
>>>>> > On Tue, Jul 26, 2016 at 8:55 PM, David L Carlson <dcarlson at tamu.edu>
>>>>> > wrote:
>>>>> >
>>>>> > > Show us the output, don?t just tell us what you are seeing. If the
>>>>> > > dates are correct in the csv file, show us the structure of the
>>>>> data
>>>>> > > frame you created with read.csv() and show the command(s) you used
>>>>> to
>>>>> > > convert the character data to date format. The solution is likely
>>>>> to
>>>>> > > be simple if you will cut/paste the R console and not just
>>>>> describe what
>>>>> > > is happening.
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> > > David C
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> >
>>>>> > > *From:* Shivi Bhatia [mailto:shivipmp82 at gmail.com]
>>>>> > > *Sent:* Tuesday, July 26, 2016 10:08 AM
>>>>> > > *To:* David L Carlson
>>>>> > >
>>>>> > > *Subject:* Re: [R] Date Time in R
>>>>> >
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> > > Hi David,
>>>>> > >
>>>>> > > This gives the results accurately. The first line shows all the
>>>>> > > variable names and the rest shows all values stored for each of the
>>>>> > > variable. Here date is appearing as correct.
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> > > Thanks, Shivi
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> > > On Tue, Jul 26, 2016 at 7:39 PM, David L Carlson <
>>>>> dcarlson at tamu.edu>
>>>>> > > wrote:
>>>>> > >
>>>>> > > What does this produce?
>>>>> > >
>>>>> > > > readLines("YourCSVfilename.csv", n=5)
>>>>> > >
>>>>> > > If the data are in Excel, the date format used in .csv files is not
>>>>> > > always in the same as the format used when viewing dates in the
>>>>> > > spreadsheet.
>>>>> > >
>>>>> > > -------------------------------------
>>>>> > > David L Carlson
>>>>> > > Department of Anthropology
>>>>> > > Texas A&M University
>>>>> > > College Station, TX 77840-4352
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> > > -----Original Message-----
>>>>> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>>>>> Shivi
>>>>> > > Bhatia
>>>>> > > Sent: Tuesday, July 26, 2016 7:42 AM
>>>>> > > To: Marc Schwartz
>>>>> > > Cc: R-help
>>>>> > > Subject: Re: [R] Date Time in R
>>>>> > >
>>>>> > > Thanks Marc for the help. this really helps.
>>>>> > > I think there is some issue with the data saved in csv format for
>>>>> this
>>>>> > > variable as when i checked:
>>>>> > > str(eir$date)- this results in :-
>>>>> > > Date[1:5327], format: NA NA NA NA NA.
>>>>> > >
>>>>> > > Thanks again.
>>>>> > >
>>>>> > > On Tue, Jul 26, 2016 at 5:58 PM, Marc Schwartz <
>>>>> marc_schwartz at me.com>
>>>>> > > wrote:
>>>>> > >
>>>>> > > > Hi,
>>>>> > > >
>>>>> > > > That eir$date might be a factor is irrelevant. There is an
>>>>> as.Date()
>>>>> > > > method for factors, which does the factor to character coercion
>>>>> > > internally
>>>>> > > > and then calls as.Date.character() on the result.
>>>>> > > >
>>>>> > > > Using the example data below:
>>>>> > > >
>>>>> > > > eir <- data.frame(date = c("05-30-16", "05-30-16", "05-30-16",
>>>>> > > >                            "05-30-16", "05-30-16", "05-30-16"))
>>>>> > > >
>>>>> > > > > str(eir)
>>>>> > > > 'data.frame':   6 obs. of  1 variable:
>>>>> > > >  $ date: Factor w/ 1 level "05-30-16": 1 1 1 1 1 1
>>>>> > > >
>>>>> > > > > eir
>>>>> > > >       date
>>>>> > > > 1 05-30-16
>>>>> > > > 2 05-30-16
>>>>> > > > 3 05-30-16
>>>>> > > > 4 05-30-16
>>>>> > > > 5 05-30-16
>>>>> > > > 6 05-30-16
>>>>> > > >
>>>>> > > > eir$date <- as.Date(eir$date, format = "%m-%d-%y")
>>>>> > > >
>>>>> > > > > str(eir)
>>>>> > > > 'data.frame':   6 obs. of  1 variable:
>>>>> > > >  $ date: Date, format: "2016-05-30" ...
>>>>> > > >
>>>>> > > > > eir
>>>>> > > >         date
>>>>> > > > 1 2016-05-30
>>>>> > > > 2 2016-05-30
>>>>> > > > 3 2016-05-30
>>>>> > > > 4 2016-05-30
>>>>> > > > 5 2016-05-30
>>>>> > > > 6 2016-05-30
>>>>> > > >
>>>>> > > > eir$days <- weekdays(eir$date)
>>>>> > > >
>>>>> > > > > str(eir)
>>>>> > > > 'data.frame':   6 obs. of  2 variables:
>>>>> > > >  $ date: Date, format: "2016-05-30" ...
>>>>> > > >  $ days: chr  "Monday" "Monday" "Monday" "Monday" ...
>>>>> > > >
>>>>> > > > > eir
>>>>> > > >         date   days
>>>>> > > > 1 2016-05-30 Monday
>>>>> > > > 2 2016-05-30 Monday
>>>>> > > > 3 2016-05-30 Monday
>>>>> > > > 4 2016-05-30 Monday
>>>>> > > > 5 2016-05-30 Monday
>>>>> > > > 6 2016-05-30 Monday
>>>>> > > >
>>>>> > > >
>>>>> > > > I would check to be sure that you do not have any typos in your
>>>>> code.
>>>>> > > >
>>>>> > > > Regards,
>>>>> > > >
>>>>> > > > Marc Schwartz
>>>>> > > >
>>>>> > > >
>>>>> > > > > On Jul 26, 2016, at 6:58 AM, Shivi Bhatia <
>>>>> shivipmp82 at gmail.com>
>>>>> > > wrote:
>>>>> > > > >
>>>>> > > > > Hello Again,
>>>>> > > > >
>>>>> > > > > While i tried your solution as you suggested above it seems to
>>>>> be
>>>>> > > > working.
>>>>> > > > > Here is the output
>>>>> > > > > temp<- dput(head(eir$date))
>>>>> > > > > c("05-30-16", "05-30-16", "05-30-16", "05-30-16", "05-30-16",
>>>>> > > "05-30-16")
>>>>> > > > > however it still shows class(eir$date) as character and hence i
>>>>> > > > > cannot
>>>>> > > > find
>>>>> > > > > weekdays from this variable.
>>>>> > > > >
>>>>> > > > > Sorry but i still dont understand in totality how R reads dates
>>>>> > > > > even
>>>>> > > > though
>>>>> > > > > have tried enough.
>>>>> > > > >
>>>>> > > > > Regards, Shivi
>>>>> > > > >
>>>>> > > > >
>>>>> > > > > On Tue, Jul 26, 2016 at 5:12 PM, Shivi Bhatia
>>>>> > > > > <shivipmp82 at gmail.com>
>>>>> > > > wrote:
>>>>> > > > >
>>>>> > > > >> Thanks Duncan for the quick response. I will check again as
>>>>> you
>>>>> > > > suggested.
>>>>> > > > >> If that doesn't work i will share a reproducible example.
>>>>> > > > >>
>>>>> > > > >> Thanks again!!!!
>>>>> > > > >>
>>>>> > > > >> On Tue, Jul 26, 2016 at 4:43 PM, Duncan Murdoch <
>>>>> > > > murdoch.duncan at gmail.com>
>>>>> > > > >> wrote:
>>>>> > > > >>
>>>>> > > > >>> On 26/07/2016 7:05 AM, Shivi Bhatia wrote:
>>>>> > > > >>>
>>>>> > > > >>>> Hi Team,
>>>>> > > > >>>>
>>>>> > > > >>>> This scenario may have come across a number of times
>>>>> however i
>>>>> > > checked
>>>>> > > > >>>> nabble & SO and couldn't find a solution hence request
>>>>> assistance.
>>>>> > > > >>>>
>>>>> > > > >>>> I have a date variable in my data-set eir. The class of this
>>>>> > > > >>>> var was character while i had read the file in r studio.
>>>>> > > > >>>> Example of date -
>>>>> > > > >>>> 05-30-16
>>>>> > > > >>>>
>>>>> > > > >>>> To change this i have used eir$date<- as.Date(eir$date,
>>>>> > > > >>>> "%m-%d-%y").
>>>>> > > > This
>>>>> > > > >>>> converts it to a date variable. However when i check few obs
>>>>> > > > >>>> with head(eir$date) all the results are <NA>.
>>>>> > > > >>>>
>>>>> > > > >>>
>>>>> > > > >>> I think you don't have character data like that, because I
>>>>> see
>>>>> > > > >>>
>>>>> > > > >>>> as.Date("05-30-16", "%m-%d-%y")
>>>>> > > > >>> [1] "2016-05-30"
>>>>> > > > >>>
>>>>> > > > >>> I'd guess eir$date is really a factor, because character
>>>>> data is
>>>>> > > > >>> frequently changed to factor automatically.  If that's the
>>>>> case,
>>>>> > > > >>> this should work for the conversion:
>>>>> > > > >>>
>>>>> > > > >>> as.Date(as.character(eir$date), "%m-%d-%y")
>>>>> > > > >>>
>>>>> > > > >>> If that doesn't work, you'll need to post something
>>>>> reproducible.
>>>>> > > > >>>
>>>>> > > > >>> Duncan Murdoch
>>>>> > > > >>>
>>>>> > > > >>> I also need to create weekdays from this date variable but
>>>>> until
>>>>> > > > >>> i
>>>>> > > get
>>>>> > > > >>>> this
>>>>> > > > >>>> resolved i cant find a weekday. For weekday i have used:
>>>>> > > > >>>> eir$week<- (eir$date)
>>>>> > > > >>>> eir$week<- weekdays(as.Date(eir$week))
>>>>> > > > >>>> class(eir$week)
>>>>> > > > >>>> eir$week<- as.factor(eir$week)
>>>>> > > > >>>> head(eir$week)
>>>>> > > > >>>>
>>>>> > > > >>>> Head of this eir$week results again as expected in <NA> but
>>>>> > > > >>>> shows
>>>>> > > > Levels:
>>>>> > > > >>>> Friday Monday Saturday Sunday Thursday Tuesday Wednesday
>>>>> > > > >>>>
>>>>> > > > >>>> Not sure what i should do here. Kindly suggest.
>>>>> > > >
>>>>> > > >
>>>>> > >
>>>>> > >         [[alternative HTML version deleted]]
>>>>> > >
>>>>> > > ______________________________________________
>>>>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> >
>>>>> > > <
>>>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mai
>>>>> > >
>>>>> lman_listinfo_r-2Dhelp&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZSh
>>>>> > >
>>>>> ld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0ccx
>>>>> > > orF0VRaQ&s=-2AvIVGKBvHIg4b3KBUCarOh98Mq95XbBM6rQR5o_Qw&e=>
>>>>> > > PLEASE do read the posting guide
>>>>> > > http://www.R-project.org/posting-guide.html
>>>>> > > <
>>>>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.o
>>>>> > >
>>>>> rg_posting-2Dguide.html&d=CwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZS
>>>>> > >
>>>>> hld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=s9RjgM0-LqObg32B_ODUoHMjaBJSYFn0cc
>>>>> > > xorF0VRaQ&s=Ldb33UyU9KMUXEsxxzwItyCFjZInukICij2jo66xaKg&e=>
>>>>> >
>>>>> > > and provide commented, minimal, self-contained, reproducible code.
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> >
>>>>> >         [[alternative HTML version deleted]]
>>>>> >
>>>>> > ______________________________________________
>>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> > PLEASE do read the posting guide
>>>>> > http://www.R-project.org/posting-guide.html
>>>>> > and provide commented, minimal, self-contained, reproducible code.
>>>>> >
>>>>> >
>>>>> >
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From careyshan at gmail.com  Tue Jul 26 22:11:24 2016
From: careyshan at gmail.com (Shane Carey)
Date: Tue, 26 Jul 2016 21:11:24 +0100
Subject: [R] Ocr
Message-ID: <CA+jRDxAC1ktoJd4N8tFGoBhBSQSbs074Kmxftq1j3_pFVbvLaQ@mail.gmail.com>

Hi,

Has anyone ever done any ocr in R?? I have some scanned images that I would
like to convert to text!!
Thanks


-- 
Le gach dea ghui,
Shane

	[[alternative HTML version deleted]]


From h.wickham at gmail.com  Tue Jul 26 22:26:11 2016
From: h.wickham at gmail.com (Hadley Wickham)
Date: Tue, 26 Jul 2016 15:26:11 -0500
Subject: [R] lm() silently drops NAs
In-Reply-To: <22423.7743.681107.446114@stat.math.ethz.ch>
References: <CABdHhvEAh4SYRKStO8tTZOAF=HXWmVwuioC+QkUaLJ04SEPcdg@mail.gmail.com>
	<22423.7743.681107.446114@stat.math.ethz.ch>
Message-ID: <CABdHhvGYoydrVDOS_Wn7BpV1isvWrP+cSUwcUOQ93BGeftYcBg@mail.gmail.com>

On Tue, Jul 26, 2016 at 3:24 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
> I have been asked (in private)

Martin was very polite to not share my name, but it was me :)

>     > Hi Martin,
>
>     y <- c(1, 2, 3, NA, 4)
>     x <- c(1, 2, 2, 1, 1)
>
>     t.test(y ~ x)
>     lm(y ~ x)
>
>     > Normally, most R functions follow the principle that
>     > "missings should never silently go missing". Do you have
>     > any background on why these functions (and most model/test
>     > function in stats, I presume) silently drop missing values?
>
> And I think, the issue and an answer are important enough to be
> public, hence this posting to R-help :
>
> First note that in some sense it is not true that lm(y ~ x) silently drops
> NAs: Everybody who is taught about lm() is taught to look at
>    summary( fm )  where  fm <- lm(y ~ x)

Good point - unfortunately the message was a bit subtle for me, and
don't remember being taught to look for it :(

> and that (for the above case)  "says"
>
>  ' (1 observation deleted due to missingness) '
>
> and so is not entirely silent.
>
>
> This goes all back to the idea of having an 'na.action' argument
> which may be a function (a very good idea, "functional
> programming" in a double sense!)... which Chambers et al
> introduced in The White Book (*1) and which I think to remember
> was quite a revolutionary idea; at least I had liked that very much
> once I understood the beauty of passing functions as arguments
> to other functions.
> One problem already back then has been that we already had the
> ---much more primitive but often sufficient--- standard of an
> 'na.rm = FALSE' (i.e. default FALSE) argument.
>
> This has been tought in all good classes/course about statistical
> modeling with S and R ever since ... I had hoped ....
> (but it seems I was too optimistic, .. or many students have too
>  quickly forgotten what they were taught ..)
> Notably the white book itself, and the MASS (*2) book do teach
> this.. though possibly not loudly enough.
>
> Two more decisions about this were made back then, as well:
>
>   1) The default for na.action to be na.omit  (==> "silently dropping")
>
>   2) na.action being governed by  options(na.action = ..)
>
> '1)' may have been mostly "historical": I think it had been the behavior of
> other "main stream" statistical packages back then (and now?) *and*
> possibly more importantly, notably with the later (than white book = "S3")
> advent of na.replace, you did want to keep the missing in your
> data frame, for later analysis; e.g. drawing (-> "gaps" in
> plots) so the NA *were* carried along and would not be
> forgotten, something very important in most case.s.
>
> and '2)' is something I personally no longer like very
> much, as it is "killing" the functional paradigm.
> OTOH, it has to be said in favor of that "session wide" / "global" setting
>       options(na.action = *)
> that indeed it depends on the specific data analysis, or even
> the specific *phase* of a data analysis, *what* behavior of NA
> treatment is desired.... and at the time it was thought smart
> that all methods (also functions "deep down" called by
> user-called functtions) would automatically use the "currently
> desired" NA handling.
>
> There have been recommendations (I don't know exactly where and
> by whom) to always set
>
>    options(na.action = na.fail)
>
> in your global .First() or nowadays rather your  Rprofile, and
> I assume that some of the CRAN packages and some of the "teaching
> setups" would do that (and if you do that, the lm() and t.test()
> calls above give an error).

I think that's a bit too strict for me, so I wrote my own:

na.warn <- function(object, ...) {
  missing <- complete.cases(object)
  if (any(missing)) {
    warning("Dropping ", sum(missing), " rows with missing values",
call. = FALSE)
  }

  na.exclude(object, ...)
}

That gives me the behaviour I want:

options(na.action = na.warn)

y <- c(1, 2, 3, NA, 4)
x <- c(1, 2, 2, 1, 1)
mod <- lm(y ~ x)
#> Warning: Dropping 4 rows with missing values

predict(mod)
#>   1   2   3   4   5
#> 2.5 2.5 2.5  NA 2.5
resid(mod)
#>    1    2    3    4    5
#> -1.5 -0.5  0.5   NA  1.5

To me, this would be the most sensible default behaviour, but I
realise it's too late to change without breaking many existing
expectations.

On a related note, I've never really understood why it's called
na.exclude - from my perspective it causes the _inclusion_ of missing
values in the predictions/residuals.

Thanks for the (as always!) informative response, Martin.

Hadley

-- 
http://hadley.nz


From murdoch.duncan at gmail.com  Tue Jul 26 22:30:31 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 26 Jul 2016 16:30:31 -0400
Subject: [R] Visualization of a Convex Hull in R (Possibly with RGL)
In-Reply-To: <20160726143429.GC10914@chicca2>
References: <20160726102534.GA10914@chicca2>
	<CAAcGz99mdFhTcPE0JkBW6pGAJBjg010weTPCiXy+9JZuDjNOEA@mail.gmail.com>
	<20160726143429.GC10914@chicca2>
Message-ID: <0a14b3b3-8360-9dc2-ff5c-54947fc3204d@gmail.com>

On 26/07/2016 10:34 AM, Lorenzo Isella wrote:
> On Tue, Jul 26, 2016 at 10:48:22AM +0000, Michael Sumner wrote:
>> On Tue, 26 Jul 2016 at 20:29 Lorenzo Isella <lorenzo.isella at gmail.com>
>> wrote:
>>
>>> Dear All,
>>> I am not an expert about the calculation and visualization of convex
>>> hulls, but I am trying to do something relatively simple.
>>> Please consider the snippet at the end of the email.
>>> The array pts  represents the position of (the centres of) a set of
>>> spheres in 3D (whose radius is 0.5).
>>> I found the geometry package which provides bindings to the qhull
>>> (http://www.qhull.org/) library and allows me to calculate the convex
>>> hull of pts and some interesting properties, like the area and the
>>> volume of the convex hull.
>>> That is all fine, but I would like to represent the results in 3D.
>>> I would like to see a set of spheres with the given position in pts
>>> and the radius R=0.5 and the convex hull going based on the
>>> coordinates of pts.
>>> Does anybody know how to achieve that?
>>> Many thanks
>>>
>>> Lorenzo
>>>
>>>
>>>
>>> ####################################################################
>>>
>>>
>>> library(geometry)
>>>
>>>
>>>
>>> pts <- read.table("test-agg.dat")
>>>
>>> ## names(pts) <- c("x", "y", "z")
>>>
>>>
>>> pts <- structure(list(x = c(0.145138112090815, 0.880429452229448,
>>> -1.66682740805663,
>>> -0.955356943224252, 1.09396798448412, -2.63620653885452,
>>> 0.190270563436841,
>>> 2.77638842095489, -0.0914977922901252, -0.484087722062158,
>>> 0.674992784754569,
>>> 1.02571108118502, 2.78789154912298, -0.146882388586427,
>>> -1.71507089246001,
>>> -1.87886026272455, 6.61540228778138, 7.46818822627362,
>>> 4.80777628045963,
>>> 6.35487269602107, 4.70026626314108, 3.44366671298125,
>>> 2.92460648354883,
>>> 5.07053866161192, 7.2164343710271, 6.93292242981006, 7.84238243682145,
>>> 7.4321237311656, 9.59692827237335, 8.3454086671306, 6.574251622581,
>>> 6.93340304175759), y = c(-0.189309482569001, 1.67057595730283,
>>> -0.995542605143325, 2.31599069771684, 1.79876139786291,
>>> 0.165886909346058,
>>> 2.70740725397259, 1.70535678189514, -0.819087329147786,
>>> 0.967935667739331,
>>> -2.53860079551206, -1.60932884056828, -1.42050583991613,
>>> -2.36883245674979,
>>> 0.191848867151458, -1.58255618338079, 1.98324724741419,
>>> 3.0095048680572,
>>> 1.97159273200079, 2.26459561200295, 0.4461804374566, 1.87939977307282,
>>> 1.98510457359889, -0.103495422088799, -1.32050185858493,
>>> 0.584580736435273,
>>> -2.97562362406436, 0.106204178143333, -2.0843758994948,
>>> -1.1492729389287,
>>> -2.13797391772643, -4.45916649729404), z = c(-1.44428170702261,
>>> -1.45742688370091, -1.70267889327056, -1.93637881287001,
>>> -3.4452409532781,
>>> -3.02436538816822, 0.114790290814684, -2.10208878117278,
>>> 2.07425243689128,
>>> 1.26652602551291, 1.39914827137784, -0.345006925422662,
>>> 0.596828021941431,
>>> 3.351773622867, 2.68408561840144, 3.97006405709929, 0.82767367646934,
>>> -0.662142231346811, 1.68344957582882, 2.92819854377685,
>>> 0.386683699222387,
>>> -0.220305098209874, 2.37510769001993, -1.51041233970289,
>>> -0.707073219742548,
>>> -1.24585080403725, -1.63914669343685, 0.683153891726357,
>>> -1.26623658129696,
>>> 1.95073173465968, -2.94804638502708, -0.635785458903106)), .Names =
>>> c("x",
>>> "y", "z"), class = "data.frame", row.names = c(NA, -32L))
>>>
>>> res<-convhulln(pts, options = "FA")
>>>
>>>
>> In short
>>
>> library(rgl)
>> bg3d("grey")
>> spheres3d(pts, radius = 0.5, col = "white")
>> ## triangle functions generally expect triplets of points, one after another
>> ## and hull/triangulation functions generally return arrays of indexes
>> ## so transpose index of hull is the right order to draw triangles
>> triangles3d(pts[t(res$hull), ], col = "firebrick", alpha = 0.3)
>>
>> (But, I wonder if you mean to find the convex hull based on the outer shell
>> of those spheres? )
>>
>> Cheers, Mike.
>>
>
>
>
> Dear Mike,
> Thanks a lot, this is spot on.
> Yep, if possible I would even prefer to have the convex hull based on
> the outer shell of the spheres, but I wonder if that is easy to
> achieve with a few lines of code.
> Any ideas would be welcome.

The convex hull of a set of spheres isn't a polyhedron (parts of the 
spheres will appear in it, and it will have curved edges).  Shapes like 
that are hard to draw in rgl.

Duncan Murdoch


From chocold12 at gmail.com  Tue Jul 26 21:07:53 2016
From: chocold12 at gmail.com (lily li)
Date: Tue, 26 Jul 2016 13:07:53 -0600
Subject: [R] about netcdf files
In-Reply-To: <843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
	<843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
Message-ID: <CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>

Thanks for your reply. But it says "Error in (function (classes, fdef,
mtable)):
unable to find an inherited method for function 'brick' for signature
'ncdf4' "

The dataset is attached. It contains daily precipitation data for 20 years,
within a rectangle, so that there are several grid points. I use the code
to open it, but don't know how to get csv files, while each file contains
continuous daily precipitation data for each grid cell.
pre1 = nc_open('sample_precip_daily.nc')
pre1
pre1_rd = ncvar_get(pre1, 'precipitation')
nc_close(pre1)

Thanks for your help.

On Tue, Jul 26, 2016 at 4:08 AM, Jon Skoien <jon.skoien at jrc.ec.europa.eu>
wrote:

> You could try with the brick function from the raster package.
>
> bvar = brick(netcdfName)
>
> This uses the ncdf4 functions for opening and reading the netcdf, but
> makes it easier to extract data for each day:
>
> p1 = rasterToPoints(bvar[[1]])
> and write p1 to csv.
>
> Best,
> Jon
>
>
>
> On 7/26/2016 6:54 AM, lily li wrote:
>
>> Hi all,
>>
>> I have a problem in opening netcdf files. If one netcdf file contains
>> longitude, latitude, and daily precipitation. How to relate each
>> precipitation record to its associated location, and export them as csv
>> files? Thanks.
>>
>> I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks for
>> any ideas.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> --
> Jon Olav Sk?ien
> Joint Research Centre - European Commission
> Institute for Space, Security & Migration
> Disaster Risk Management Unit
>
> Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
>
> jon.skoien at jrc.ec.europa.eu
> Tel:  +39 0332 789205
>
> Disclaimer: Views expressed in this email are those of the individual and
> do not necessarily represent official views of the European Commission.
>

From kolkata256 at gmail.com  Tue Jul 26 20:24:59 2016
From: kolkata256 at gmail.com (kolkata kolkata)
Date: Tue, 26 Jul 2016 14:24:59 -0400
Subject: [R] Time format issue
Message-ID: <CAPQRYa7WxN_0tAkbdyeOqw+u7pkPjsb9CAxcHMrbQW_xVGiE8A@mail.gmail.com>

Hello Everyone,

I have a file with time in the following format:

Time

27DEC11:00:30

27DEC11:01:30

27DEC11:02:00

??.

The time column is factor. I want to convert each time to the following
format:

20111211003000

20111211013000

20111211020000

(Year)(month)(date)(hr)(min)(sec)


Any suggestions in this regard

regards,

manik mitra

	[[alternative HTML version deleted]]


From h.wickham at gmail.com  Tue Jul 26 22:37:05 2016
From: h.wickham at gmail.com (Hadley Wickham)
Date: Tue, 26 Jul 2016 15:37:05 -0500
Subject: [R] lm() silently drops NAs
In-Reply-To: <CABdHhvGYoydrVDOS_Wn7BpV1isvWrP+cSUwcUOQ93BGeftYcBg@mail.gmail.com>
References: <CABdHhvEAh4SYRKStO8tTZOAF=HXWmVwuioC+QkUaLJ04SEPcdg@mail.gmail.com>
	<22423.7743.681107.446114@stat.math.ethz.ch>
	<CABdHhvGYoydrVDOS_Wn7BpV1isvWrP+cSUwcUOQ93BGeftYcBg@mail.gmail.com>
Message-ID: <CABdHhvFzOZA12qp-9TB9ZYKXurzmnbhyv1EkZi9W24-DHU97kA@mail.gmail.com>

> I think that's a bit too strict for me, so I wrote my own:
>
> na.warn <- function(object, ...) {
>   missing <- complete.cases(object)
>   if (any(missing)) {
>     warning("Dropping ", sum(missing), " rows with missing values",
> call. = FALSE)
>   }
>
>   na.exclude(object, ...)
> }

That should, of course, have been:

missing <- !complete.cases(object)

:-/

Hadley

-- 
http://hadley.nz


From roy.mendelssohn at noaa.gov  Tue Jul 26 22:52:42 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Tue, 26 Jul 2016 13:52:42 -0700
Subject: [R] about netcdf files
In-Reply-To: <CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
	<843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
	<CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
Message-ID: <C6173FB6-B6B0-471A-BC5B-3D819BBA847C@noaa.gov>

Hi Lily:

I doubt the mail-list would pass through the netcdf file. Instead, could you do the following, and post the results:

library(ncdf4
pre1 = nc_open('sample_precip_daily.nc')
str(pre1)
nc_close(pre1)

I have a feeling you haven't worked much with netcdf files. I will try to find a tutorial also to help you along.

Thanks,

-Roy

> On Jul 26, 2016, at 12:07 PM, lily li <chocold12 at gmail.com> wrote:
> 
> Thanks for your reply. But it says "Error in (function (classes, fdef,
> mtable)):
> unable to find an inherited method for function 'brick' for signature
> 'ncdf4' "
> 
> The dataset is attached. It contains daily precipitation data for 20 years,
> within a rectangle, so that there are several grid points. I use the code
> to open it, but don't know how to get csv files, while each file contains
> continuous daily precipitation data for each grid cell.
> pre1 = nc_open('sample_precip_daily.nc')
> pre1
> pre1_rd = ncvar_get(pre1, 'precipitation')
> nc_close(pre1)
> 
> Thanks for your help.
> 
> On Tue, Jul 26, 2016 at 4:08 AM, Jon Skoien <jon.skoien at jrc.ec.europa.eu>
> wrote:
> 
>> You could try with the brick function from the raster package.
>> 
>> bvar = brick(netcdfName)
>> 
>> This uses the ncdf4 functions for opening and reading the netcdf, but
>> makes it easier to extract data for each day:
>> 
>> p1 = rasterToPoints(bvar[[1]])
>> and write p1 to csv.
>> 
>> Best,
>> Jon
>> 
>> 
>> 
>> On 7/26/2016 6:54 AM, lily li wrote:
>> 
>>> Hi all,
>>> 
>>> I have a problem in opening netcdf files. If one netcdf file contains
>>> longitude, latitude, and daily precipitation. How to relate each
>>> precipitation record to its associated location, and export them as csv
>>> files? Thanks.
>>> 
>>> I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks for
>>> any ideas.
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> 
>> --
>> Jon Olav Sk?ien
>> Joint Research Centre - European Commission
>> Institute for Space, Security & Migration
>> Disaster Risk Management Unit
>> 
>> Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
>> 
>> jon.skoien at jrc.ec.europa.eu
>> Tel:  +39 0332 789205
>> 
>> Disclaimer: Views expressed in this email are those of the individual and
>> do not necessarily represent official views of the European Commission.
>> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From chocold12 at gmail.com  Tue Jul 26 23:00:07 2016
From: chocold12 at gmail.com (lily li)
Date: Tue, 26 Jul 2016 15:00:07 -0600
Subject: [R] about netcdf files
In-Reply-To: <C6173FB6-B6B0-471A-BC5B-3D819BBA847C@noaa.gov>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
	<843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
	<CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
	<C6173FB6-B6B0-471A-BC5B-3D819BBA847C@noaa.gov>
Message-ID: <CAN5afy9j-E5J30cXZGzvUhO0-CMFGLZO4LXftwFdn_DwRS=CwA@mail.gmail.com>

Here are the results. Yes, I tried to read netcdf files, but cannot grasp
the contents. Thanks for helping out.

> str(pre1)
List of 14
 $ filename   : chr "~/Downloads/sample_precip_daily.nc"
 $ writable   : logi FALSE
 $ id         : int 262144
 $ safemode   : logi FALSE
 $ format     : chr "NC_FORMAT_CLASSIC"
 $ is_GMT     : logi FALSE
 $ groups     :List of 1
  ..$ :List of 7
  .. ..$ id   : int 262144
  .. ..$ name : chr ""
  .. ..$ ndims: int 3
  .. ..$ nvars: int 4
  .. ..$ natts: int 48
  .. ..$ dimid: int [1:3(1d)] 0 1 2
  .. ..$ fqgn : chr ""
  .. ..- attr(*, "class")= chr "ncgroup4"
 $ fqgn2Rindex:List of 1
  ..$ : int 1
 $ ndims      : num 3
 $ natts      : num 48
 $ dim        :List of 3
  ..$ time:List of 11
  .. ..$ name         : chr "time"
  .. ..$ len          : int 7305
  .. ..$ unlim        : logi FALSE
  .. ..$ group_index  : int 1
  .. ..$ group_id     : int 262144
  .. ..$ id           : int 0
  .. ..$ dimvarid     :List of 5
  .. .. ..$ id         : int 1
  .. .. ..$ group_index: int 1
  .. .. ..$ group_id   : int 262144
  .. .. ..$ list_index : num -1
  .. .. ..$ isdimvar   : logi TRUE
  .. .. ..- attr(*, "class")= chr "ncid4"
  .. ..$ units        : chr "days since 1900-01-01 00:00:00"
  .. ..$ calendar     : chr "gregorian"
  .. ..$ vals         : num [1:7305(1d)] 38716 38717 38718 38719 38720 ...
  .. ..$ create_dimvar: logi TRUE
  .. ..- attr(*, "class")= chr "ncdim4"
  ..$ lat :List of 10
  .. ..$ name         : chr "lat"
  .. ..$ len          : int 5
  .. ..$ unlim        : logi FALSE
  .. ..$ group_index  : int 1
  .. ..$ group_id     : int 262144
  .. ..$ id           : int 1
  .. ..$ dimvarid     :List of 5
  .. .. ..$ id         : int 2
  .. .. ..$ group_index: int 1
  .. .. ..$ group_id   : int 262144
  .. .. ..$ list_index : num -1
  .. .. ..$ isdimvar   : logi TRUE
  .. .. ..- attr(*, "class")= chr "ncid4"
  .. ..$ units        : chr "degrees_north"
  .. ..$ vals         : num [1:5(1d)] 39.8 39.9 40 40 40.1
  .. ..$ create_dimvar: logi TRUE
  .. ..- attr(*, "class")= chr "ncdim4"
  ..$ lon :List of 10
  .. ..$ name         : chr "lon"
  .. ..$ len          : int 9
  .. ..$ unlim        : logi FALSE
  .. ..$ group_index  : int 1
  .. ..$ group_id     : int 262144
  .. ..$ id           : int 2
  .. ..$ dimvarid     :List of 5
  .. .. ..$ id         : int 3
  .. .. ..$ group_index: int 1
  .. .. ..$ group_id   : int 262144
  .. .. ..$ list_index : num -1
  .. .. ..$ isdimvar   : logi TRUE
  .. .. ..- attr(*, "class")= chr "ncid4"
  .. ..$ units        : chr "degrees_east"
  .. ..$ vals         : num [1:9(1d)] 254 254 254 254 255 ...
  .. ..$ create_dimvar: logi TRUE
  .. ..- attr(*, "class")= chr "ncdim4"
 $ unlimdimid : num -1
 $ nvars      : num 1
 $ var        :List of 1
  ..$ precipitation:List of 22
  .. ..$ id                :List of 5
  .. .. ..$ id         : num 0
  .. .. ..$ group_index: num -1
  .. .. ..$ group_id   : int 262144
  .. .. ..$ list_index : num 1
  .. .. ..$ isdimvar   : logi FALSE
  .. .. ..- attr(*, "class")= chr "ncid4"
  .. ..$ name              : chr "precipitation"
  .. ..$ ndims             : int 3
  .. ..$ natts             : int 9
  .. ..$ size              : int [1:3] 9 5 7305
  .. ..$ dimids            : int [1:3] 2 1 0
  .. ..$ prec              : chr "float"
  .. ..$ units             : chr "mm"
  .. ..$ longname          : chr "Precipitation"
  .. ..$ group_index       : int 1
  .. ..$ chunksizes        : logi NA
  .. ..$ storage           : num 1
  .. ..$ shuffle           : logi FALSE
  .. ..$ compression       : logi NA
  .. ..$ dims              : list()
  .. ..$ dim               :List of 3
  .. .. ..$ :List of 10
  .. .. .. ..$ name         : chr "lon"
  .. .. .. ..$ len          : int 9
  .. .. .. ..$ unlim        : logi FALSE
  .. .. .. ..$ group_index  : int 1
  .. .. .. ..$ group_id     : int 262144
  .. .. .. ..$ id           : int 2
  .. .. .. ..$ dimvarid     :List of 5
  .. .. .. .. ..$ id         : int 3
  .. .. .. .. ..$ group_index: int 1
  .. .. .. .. ..$ group_id   : int 262144
  .. .. .. .. ..$ list_index : num -1
  .. .. .. .. ..$ isdimvar   : logi TRUE
  .. .. .. .. ..- attr(*, "class")= chr "ncid4"
  .. .. .. ..$ units        : chr "degrees_east"
  .. .. .. ..$ vals         : num [1:9(1d)] 254 254 254 254 255 ...
  .. .. .. ..$ create_dimvar: logi TRUE
  .. .. .. ..- attr(*, "class")= chr "ncdim4"
  .. .. ..$ :List of 10
  .. .. .. ..$ name         : chr "lat"
  .. .. .. ..$ len          : int 5
  .. .. .. ..$ unlim        : logi FALSE
  .. .. .. ..$ group_index  : int 1
  .. .. .. ..$ group_id     : int 262144
  .. .. .. ..$ id           : int 1
  .. .. .. ..$ dimvarid     :List of 5
  .. .. .. .. ..$ id         : int 2
  .. .. .. .. ..$ group_index: int 1
  .. .. .. .. ..$ group_id   : int 262144
  .. .. .. .. ..$ list_index : num -1
  .. .. .. .. ..$ isdimvar   : logi TRUE
  .. .. .. .. ..- attr(*, "class")= chr "ncid4"
  .. .. .. ..$ units        : chr "degrees_north"
  .. .. .. ..$ vals         : num [1:5(1d)] 39.8 39.9 40 40 40.1
  .. .. .. ..$ create_dimvar: logi TRUE
  .. .. .. ..- attr(*, "class")= chr "ncdim4"
  .. .. ..$ :List of 11
  .. .. .. ..$ name         : chr "time"
  .. .. .. ..$ len          : int 7305
  .. .. .. ..$ unlim        : logi FALSE
  .. .. .. ..$ group_index  : int 1
  .. .. .. ..$ group_id     : int 262144
  .. .. .. ..$ id           : int 0
  .. .. .. ..$ dimvarid     :List of 5
  .. .. .. .. ..$ id         : int 1
  .. .. .. .. ..$ group_index: int 1
  .. .. .. .. ..$ group_id   : int 262144
  .. .. .. .. ..$ list_index : num -1
  .. .. .. .. ..$ isdimvar   : logi TRUE
  .. .. .. .. ..- attr(*, "class")= chr "ncid4"
  .. .. .. ..$ units        : chr "days since 1900-01-01 00:00:00"
  .. .. .. ..$ calendar     : chr "gregorian"
  .. .. .. ..$ vals         : num [1:7305(1d)] 38716 38717 38718 38719
38720 ...
  .. .. .. ..$ create_dimvar: logi TRUE
  .. .. .. ..- attr(*, "class")= chr "ncdim4"
  .. ..$ varsize           : int [1:3] 9 5 7305
  .. ..$ unlim             : logi FALSE
  .. ..$ make_missing_value: logi TRUE
  .. ..$ missval           : num -9999
  .. ..$ hasAddOffset      : logi FALSE
  .. ..$ hasScaleFact      : logi FALSE
  .. ..- attr(*, "class")= chr "ncvar4"
 - attr(*, "class")= chr "ncdf4"


On Tue, Jul 26, 2016 at 2:52 PM, Roy Mendelssohn - NOAA Federal <
roy.mendelssohn at noaa.gov> wrote:

> Hi Lily:
>
> I doubt the mail-list would pass through the netcdf file. Instead, could
> you do the following, and post the results:
>
> library(ncdf4
> pre1 = nc_open('sample_precip_daily.nc')
> str(pre1)
> nc_close(pre1)
>
> I have a feeling you haven't worked much with netcdf files. I will try to
> find a tutorial also to help you along.
>
> Thanks,
>
> -Roy
>
> > On Jul 26, 2016, at 12:07 PM, lily li <chocold12 at gmail.com> wrote:
> >
> > Thanks for your reply. But it says "Error in (function (classes, fdef,
> > mtable)):
> > unable to find an inherited method for function 'brick' for signature
> > 'ncdf4' "
> >
> > The dataset is attached. It contains daily precipitation data for 20
> years,
> > within a rectangle, so that there are several grid points. I use the code
> > to open it, but don't know how to get csv files, while each file contains
> > continuous daily precipitation data for each grid cell.
> > pre1 = nc_open('sample_precip_daily.nc')
> > pre1
> > pre1_rd = ncvar_get(pre1, 'precipitation')
> > nc_close(pre1)
> >
> > Thanks for your help.
> >
> > On Tue, Jul 26, 2016 at 4:08 AM, Jon Skoien <jon.skoien at jrc.ec.europa.eu
> >
> > wrote:
> >
> >> You could try with the brick function from the raster package.
> >>
> >> bvar = brick(netcdfName)
> >>
> >> This uses the ncdf4 functions for opening and reading the netcdf, but
> >> makes it easier to extract data for each day:
> >>
> >> p1 = rasterToPoints(bvar[[1]])
> >> and write p1 to csv.
> >>
> >> Best,
> >> Jon
> >>
> >>
> >>
> >> On 7/26/2016 6:54 AM, lily li wrote:
> >>
> >>> Hi all,
> >>>
> >>> I have a problem in opening netcdf files. If one netcdf file contains
> >>> longitude, latitude, and daily precipitation. How to relate each
> >>> precipitation record to its associated location, and export them as csv
> >>> files? Thanks.
> >>>
> >>> I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks
> for
> >>> any ideas.
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >> --
> >> Jon Olav Sk?ien
> >> Joint Research Centre - European Commission
> >> Institute for Space, Security & Migration
> >> Disaster Risk Management Unit
> >>
> >> Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
> >>
> >> jon.skoien at jrc.ec.europa.eu
> >> Tel:  +39 0332 789205
> >>
> >> Disclaimer: Views expressed in this email are those of the individual
> and
> >> do not necessarily represent official views of the European Commission.
> >>
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> **********************
> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new address and phone***
> 110 Shaffer Road
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK
> Jr.
>
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Tue Jul 26 23:04:10 2016
From: chocold12 at gmail.com (lily li)
Date: Tue, 26 Jul 2016 15:04:10 -0600
Subject: [R] about netcdf files
In-Reply-To: <C6173FB6-B6B0-471A-BC5B-3D819BBA847C@noaa.gov>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
	<843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
	<CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
	<C6173FB6-B6B0-471A-BC5B-3D819BBA847C@noaa.gov>
Message-ID: <CAN5afy-4PLWCfQYScQ7LJ3iWOnkE-FoGg_4c_-Gp5XWMQateCA@mail.gmail.com>

How to read continuous daily precipitation at each grid cell, and then
export to a csv file? Later, there should be several csv files, each
represents daily precipitation data for one grid cell. Thanks a lot.

On Tue, Jul 26, 2016 at 2:52 PM, Roy Mendelssohn - NOAA Federal <
roy.mendelssohn at noaa.gov> wrote:

> Hi Lily:
>
> I doubt the mail-list would pass through the netcdf file. Instead, could
> you do the following, and post the results:
>
> library(ncdf4
> pre1 = nc_open('sample_precip_daily.nc')
> str(pre1)
> nc_close(pre1)
>
> I have a feeling you haven't worked much with netcdf files. I will try to
> find a tutorial also to help you along.
>
> Thanks,
>
> -Roy
>
> > On Jul 26, 2016, at 12:07 PM, lily li <chocold12 at gmail.com> wrote:
> >
> > Thanks for your reply. But it says "Error in (function (classes, fdef,
> > mtable)):
> > unable to find an inherited method for function 'brick' for signature
> > 'ncdf4' "
> >
> > The dataset is attached. It contains daily precipitation data for 20
> years,
> > within a rectangle, so that there are several grid points. I use the code
> > to open it, but don't know how to get csv files, while each file contains
> > continuous daily precipitation data for each grid cell.
> > pre1 = nc_open('sample_precip_daily.nc')
> > pre1
> > pre1_rd = ncvar_get(pre1, 'precipitation')
> > nc_close(pre1)
> >
> > Thanks for your help.
> >
> > On Tue, Jul 26, 2016 at 4:08 AM, Jon Skoien <jon.skoien at jrc.ec.europa.eu
> >
> > wrote:
> >
> >> You could try with the brick function from the raster package.
> >>
> >> bvar = brick(netcdfName)
> >>
> >> This uses the ncdf4 functions for opening and reading the netcdf, but
> >> makes it easier to extract data for each day:
> >>
> >> p1 = rasterToPoints(bvar[[1]])
> >> and write p1 to csv.
> >>
> >> Best,
> >> Jon
> >>
> >>
> >>
> >> On 7/26/2016 6:54 AM, lily li wrote:
> >>
> >>> Hi all,
> >>>
> >>> I have a problem in opening netcdf files. If one netcdf file contains
> >>> longitude, latitude, and daily precipitation. How to relate each
> >>> precipitation record to its associated location, and export them as csv
> >>> files? Thanks.
> >>>
> >>> I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks
> for
> >>> any ideas.
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >> --
> >> Jon Olav Sk?ien
> >> Joint Research Centre - European Commission
> >> Institute for Space, Security & Migration
> >> Disaster Risk Management Unit
> >>
> >> Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
> >>
> >> jon.skoien at jrc.ec.europa.eu
> >> Tel:  +39 0332 789205
> >>
> >> Disclaimer: Views expressed in this email are those of the individual
> and
> >> do not necessarily represent official views of the European Commission.
> >>
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> **********************
> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new address and phone***
> 110 Shaffer Road
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK
> Jr.
>
>

	[[alternative HTML version deleted]]


From roy.mendelssohn at noaa.gov  Tue Jul 26 23:21:24 2016
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Tue, 26 Jul 2016 14:21:24 -0700
Subject: [R] about netcdf files
In-Reply-To: <CAN5afy9j-E5J30cXZGzvUhO0-CMFGLZO4LXftwFdn_DwRS=CwA@mail.gmail.com>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
	<843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
	<CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
	<C6173FB6-B6B0-471A-BC5B-3D819BBA847C@noaa.gov>
	<CAN5afy9j-E5J30cXZGzvUhO0-CMFGLZO4LXftwFdn_DwRS=CwA@mail.gmail.com>
Message-ID: <1CD3FB83-B4AE-4991-8E9F-E5F0969F5327@noaa.gov>

Hi Lily:

> On Jul 26, 2016, at 2:00 PM, lily li <chocold12 at gmail.com> wrote:
> 
> Here are the results. Yes, I tried to read netcdf files, but cannot grasp the contents. Thanks for helping out.
> 
> > str(pre1)

A guide to netcdf files and R can be found at https://www.image.ucar.edu/GSP/Software/Netcdf/, it would be worth your time to read through it.  Not certain why you want .csv files,  but I can give you a rough idea of how to read a slice into a variable in R,  it is then up to you to save the result as a .csv.

netcdf files have parameters, precipitation in this case, defined on a grid, in your case (lon, lat, time).  The file contains dimensions,  which gives the number of points in each dimension of the grid, and coordinate variables,  which give the values of that coordinate variable at each grid point.  So if I wanted to to subset at a certain lat, lon - I would first need to get those values

> lat <- ncvar_get(pre1, "lat")
> lon <- ncvar_get(pre1, "lon")
> 

I have no idea of contents of your file, but let's say you know there is a lat of 30, and a lon of 120, so I would have to find where they occur:

> myLat <- which(lat == 30)
> myLon <- which(lon == 120)

then you get the slice at that lat-lon for all time by telling ncvar_get where to start on the grid, and how many steps to take:

> myPrecip <- ncvar_get(pre1,"precipitation", start = c(myLat, myLon,1), count = c(1, 1, -1))

where the -1 value tells it to get all the values along the time dimension.  You now have that slice of the data stored in myPrecip.  I hope this gets you started, but i strongly urge you to read through the link above.

-Roy

> 
> On Tue, Jul 26, 2016 at 2:52 PM, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> Hi Lily:
> 
> I doubt the mail-list would pass through the netcdf file. Instead, could you do the following, and post the results:
> 
> library(ncdf4
> pre1 = nc_open('sample_precip_daily.nc')
> str(pre1)
> nc_close(pre1)
> 
> I have a feeling you haven't worked much with netcdf files. I will try to find a tutorial also to help you along.
> 
> Thanks,
> 
> -Roy
> 
> > On Jul 26, 2016, at 12:07 PM, lily li <chocold12 at gmail.com> wrote:
> >
> > Thanks for your reply. But it says "Error in (function (classes, fdef,
> > mtable)):
> > unable to find an inherited method for function 'brick' for signature
> > 'ncdf4' "
> >
> > The dataset is attached. It contains daily precipitation data for 20 years,
> > within a rectangle, so that there are several grid points. I use the code
> > to open it, but don't know how to get csv files, while each file contains
> > continuous daily precipitation data for each grid cell.
> > pre1 = nc_open('sample_precip_daily.nc')
> > pre1
> > pre1_rd = ncvar_get(pre1, 'precipitation')
> > nc_close(pre1)
> >
> > Thanks for your help.
> >
> > On Tue, Jul 26, 2016 at 4:08 AM, Jon Skoien <jon.skoien at jrc.ec.europa.eu>
> > wrote:
> >
> >> You could try with the brick function from the raster package.
> >>
> >> bvar = brick(netcdfName)
> >>
> >> This uses the ncdf4 functions for opening and reading the netcdf, but
> >> makes it easier to extract data for each day:
> >>
> >> p1 = rasterToPoints(bvar[[1]])
> >> and write p1 to csv.
> >>
> >> Best,
> >> Jon
> >>
> >>
> >>
> >> On 7/26/2016 6:54 AM, lily li wrote:
> >>
> >>> Hi all,
> >>>
> >>> I have a problem in opening netcdf files. If one netcdf file contains
> >>> longitude, latitude, and daily precipitation. How to relate each
> >>> precipitation record to its associated location, and export them as csv
> >>> files? Thanks.
> >>>
> >>> I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks for
> >>> any ideas.
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >> --
> >> Jon Olav Sk?ien
> >> Joint Research Centre - European Commission
> >> Institute for Space, Security & Migration
> >> Disaster Risk Management Unit
> >>
> >> Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
> >>
> >> jon.skoien at jrc.ec.europa.eu
> >> Tel:  +39 0332 789205
> >>
> >> Disclaimer: Views expressed in this email are those of the individual and
> >> do not necessarily represent official views of the European Commission.
> >>
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new address and phone***
> 110 Shaffer Road
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected"
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From dimitri.liakhovitski at gmail.com  Tue Jul 26 23:28:22 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 26 Jul 2016 17:28:22 -0400
Subject: [R] removing all non-numeric characters from a string, but not "."
Message-ID: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>

Hello!

I have a string x:
x <- c("x - 84", "y - 293.04", "z = 12.5")

I want to remove all the non-numeric stuff from it. The following works:
gsub("[^0-9]", "", x)

However, it strips my numbers of "."

Help - how could I do the same but leave the "." in?

Thanks a lot!

-- 
Dimitri Liakhovitski


From dwinsemius at comcast.net  Tue Jul 26 23:29:51 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 26 Jul 2016 14:29:51 -0700
Subject: [R] Time format issue
In-Reply-To: <CAPQRYa7WxN_0tAkbdyeOqw+u7pkPjsb9CAxcHMrbQW_xVGiE8A@mail.gmail.com>
References: <CAPQRYa7WxN_0tAkbdyeOqw+u7pkPjsb9CAxcHMrbQW_xVGiE8A@mail.gmail.com>
Message-ID: <D36FF0B2-D2A0-4D82-8829-786282C6016E@comcast.net>


> On Jul 26, 2016, at 11:24 AM, kolkata kolkata <kolkata256 at gmail.com> wrote:
> 
> Hello Everyone,
> 
> I have a file with time in the following format:
> 
> Time
> 
> 27DEC11:00:30
> 
> 27DEC11:01:30
> 
> 27DEC11:02:00
> 
> ??.
> 
> The time column is factor. I want to convert each time to the following
> format:
> 
> 20111211003000
> 
> 20111211013000
> 
> 20111211020000
> 
> (Year)(month)(date)(hr)(min)(sec)

> newTimeDt <- strptime(Time, format="%d%b%y:%H:%M")
> newTimeDt
[1] "2011-12-27 00:30:00 PST" "2011-12-27 01:30:00 PST" "2011-12-27 02:00:00 PST"
> newTimeDt <- as.POSIXct(Time, format="%d%b%y:%H:%M")
> newTimeDt
[1] "2011-12-27 00:30:00 PST" "2011-12-27 01:30:00 PST" "2011-12-27 02:00:00 PST"
> newOut <- format(newTimeDt, "%Y%m%d%H%M")
> newOut
[1] "201112270030" "201112270130" "201112270200"
> newOut <- format(newTimeDt, "%Y%m%d%H%M%S")
> newOut
[1] "20111227003000" "20111227013000" "20111227020000"

> 

> 
> Any suggestions in this regard
> 
> regards,
> 
> manik mitra
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From pdalgd at gmail.com  Tue Jul 26 23:30:31 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 26 Jul 2016 23:30:31 +0200
Subject: [R] lm() silently drops NAs
In-Reply-To: <CABdHhvGYoydrVDOS_Wn7BpV1isvWrP+cSUwcUOQ93BGeftYcBg@mail.gmail.com>
References: <CABdHhvEAh4SYRKStO8tTZOAF=HXWmVwuioC+QkUaLJ04SEPcdg@mail.gmail.com>
	<22423.7743.681107.446114@stat.math.ethz.ch>
	<CABdHhvGYoydrVDOS_Wn7BpV1isvWrP+cSUwcUOQ93BGeftYcBg@mail.gmail.com>
Message-ID: <AB5B2D91-FFC6-4142-88D1-D7BFBFA6BFCA@gmail.com>


> On 26 Jul 2016, at 22:26 , Hadley Wickham <h.wickham at gmail.com> wrote:
> 
> On Tue, Jul 26, 2016 at 3:24 AM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>> 
...
> To me, this would be the most sensible default behaviour, but I
> realise it's too late to change without breaking many existing
> expectations.

Probably. 

Re. the default choice, my recollection is that at the time the only choices available were na.omit and na.fail. S-PLUS was using na.fail for all the usual good reasons, but from a practical perspective, the consequence was that, since almost every data set has NA values,  you got an error unless you added na.action=na.omit to every single lm() call. And habitually typing na.action=na.omit doesn't really solve any of the issues with different models being fit to different subsets and all that. So the rationale for doing it differently in R was that it was better to get some probably meaningful output rather than to be certain of getting nothing. And, that was what the mainstream packages of the time were doing.

> On a related note, I've never really understood why it's called
> na.exclude - from my perspective it causes the _inclusion_ of missing
> values in the predictions/residuals.

I think the notion is that you exclude them from the analysis, but keep them around for the other purposes.

-pd

> 
> Thanks for the (as always!) informative response, Martin.
> 
> Hadley
> 
> -- 
> http://hadley.nz
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From marc_schwartz at me.com  Tue Jul 26 23:39:48 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 26 Jul 2016 16:39:48 -0500
Subject: [R] removing all non-numeric characters from a string,
	but not "."
In-Reply-To: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>
References: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>
Message-ID: <A14B60B2-1E6C-44C1-87F1-57CB3955DD91@me.com>


> On Jul 26, 2016, at 4:28 PM, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
> 
> Hello!
> 
> I have a string x:
> x <- c("x - 84", "y - 293.04", "z = 12.5")
> 
> I want to remove all the non-numeric stuff from it. The following works:
> gsub("[^0-9]", "", x)
> 
> However, it strips my numbers of "."
> 
> Help - how could I do the same but leave the "." in?
> 
> Thanks a lot!
> 
> -- 
> Dimitri Liakhovitski


> gsub("[^0-9\\.]", "", x)
[1] "84"     "293.04" "12.5"  

The period needs to be escaped since it otherwise has special meaning in the regex.

Regards,

Marc Schwartz


From dwinsemius at comcast.net  Tue Jul 26 23:40:22 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 26 Jul 2016 14:40:22 -0700
Subject: [R] removing all non-numeric characters from a string,
	but not "."
In-Reply-To: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>
References: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>
Message-ID: <64A07DAF-436A-4D5B-A41D-D7CC3BCADD8C@comcast.net>


> On Jul 26, 2016, at 2:28 PM, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
> 
> gsub("[^0-9]", "", x)


?regex

I think you might be bit embarrassed because it seems pretty obvious once you know that character class elements like "." don't need to be escaped so it's just this:

> gsub("[^0-9.]", "", x)
[1] "84"     "293.04" "12.5"  

You might want to add in some separator if you are processing expression this way.

> gsub("[^0-9., ]", "", gsub( "[-+*/]", " , ", x) )
[1] "  ,  84"              "  ,  293.04  ,  1200" "  12.5"  

-- 
David Winsemius
Alameda, CA, USA


From marc_schwartz at me.com  Tue Jul 26 23:45:07 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 26 Jul 2016 16:45:07 -0500
Subject: [R] removing all non-numeric characters from a string,
	but not "."
In-Reply-To: <A14B60B2-1E6C-44C1-87F1-57CB3955DD91@me.com>
References: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>
	<A14B60B2-1E6C-44C1-87F1-57CB3955DD91@me.com>
Message-ID: <934722B7-6451-480F-92BD-F222ECF03913@me.com>


> On Jul 26, 2016, at 4:39 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
> 
> 
>> On Jul 26, 2016, at 4:28 PM, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
>> 
>> Hello!
>> 
>> I have a string x:
>> x <- c("x - 84", "y - 293.04", "z = 12.5")
>> 
>> I want to remove all the non-numeric stuff from it. The following works:
>> gsub("[^0-9]", "", x)
>> 
>> However, it strips my numbers of "."
>> 
>> Help - how could I do the same but leave the "." in?
>> 
>> Thanks a lot!
>> 
>> -- 
>> Dimitri Liakhovitski
> 
> 
>> gsub("[^0-9\\.]", "", x)
> [1] "84"     "293.04" "12.5"  
> 
> The period needs to be escaped since it otherwise has special meaning in the regex.
> 
> Regards,
> 
> Marc Schwartz


Actually, let me correct my reply.

When in a character group, as is the case here, the period does not need to be escaped:

> gsub("[^0-9.]", "", x)
[1] "84"     "293.04" "12.5"  


Regards,

Marc


From pdalgd at gmail.com  Tue Jul 26 23:48:03 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 26 Jul 2016 23:48:03 +0200
Subject: [R] removing all non-numeric characters from a string,
	but not "."
In-Reply-To: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>
References: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>
Message-ID: <3F72F37C-E04E-4B14-A10C-D5E4314569DF@gmail.com>


> On 26 Jul 2016, at 23:28 , Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
> 
> Hello!
> 
> I have a string x:
> x <- c("x - 84", "y - 293.04", "z = 12.5")
> 
> I want to remove all the non-numeric stuff from it. The following works:
> gsub("[^0-9]", "", x)
> 
> However, it strips my numbers of "."
> 
> Help - how could I do the same but leave the "." in?

How about this?

> gsub("[^0-9.]", "", x)
[1] "84"     "293.04" "12.5"  

-pd

> 
> Thanks a lot!
> 
> -- 
> Dimitri Liakhovitski
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From dimitri.liakhovitski at gmail.com  Tue Jul 26 23:57:27 2016
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 26 Jul 2016 17:57:27 -0400
Subject: [R] removing all non-numeric characters from a string,
	but not "."
In-Reply-To: <3F72F37C-E04E-4B14-A10C-D5E4314569DF@gmail.com>
References: <CAN2xGJa8gwdjJ49boujQSQbebuotVxm=kDLbGg7=yZapG0rSHA@mail.gmail.com>
	<3F72F37C-E04E-4B14-A10C-D5E4314569DF@gmail.com>
Message-ID: <CAN2xGJYWJVXO8XGqtTS6xBCgXVZL+qTXL1vrUrnuY6D6kbomMg@mail.gmail.com>

Thank you very much, gentlemen!

On Tue, Jul 26, 2016 at 5:48 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>
>> On 26 Jul 2016, at 23:28 , Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
>>
>> Hello!
>>
>> I have a string x:
>> x <- c("x - 84", "y - 293.04", "z = 12.5")
>>
>> I want to remove all the non-numeric stuff from it. The following works:
>> gsub("[^0-9]", "", x)
>>
>> However, it strips my numbers of "."
>>
>> Help - how could I do the same but leave the "." in?
>
> How about this?
>
>> gsub("[^0-9.]", "", x)
> [1] "84"     "293.04" "12.5"
>
> -pd
>
>>
>> Thanks a lot!
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>



-- 
Dimitri Liakhovitski


From drjimlemon at gmail.com  Wed Jul 27 00:16:06 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 27 Jul 2016 08:16:06 +1000
Subject: [R] Ocr
In-Reply-To: <CA+jRDxAC1ktoJd4N8tFGoBhBSQSbs074Kmxftq1j3_pFVbvLaQ@mail.gmail.com>
References: <CA+jRDxAC1ktoJd4N8tFGoBhBSQSbs074Kmxftq1j3_pFVbvLaQ@mail.gmail.com>
Message-ID: <CA+8X3fXc3_NGhF3RX4jHCAfNmOjqcZDLr6E-j2UNaNyJuanLpQ@mail.gmail.com>

Hi Shane,
FreeOCR is a really good place to start.

http://www.paperfile.net/

Jim


On Wed, Jul 27, 2016 at 6:11 AM, Shane Carey <careyshan at gmail.com> wrote:
> Hi,
>
> Has anyone ever done any ocr in R?? I have some scanned images that I would
> like to convert to text!!
> Thanks
>
>
> --
> Le gach dea ghui,
> Shane
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From careyshan at gmail.com  Wed Jul 27 00:24:09 2016
From: careyshan at gmail.com (Shane Carey)
Date: Tue, 26 Jul 2016 23:24:09 +0100
Subject: [R] Ocr
In-Reply-To: <CA+8X3fXc3_NGhF3RX4jHCAfNmOjqcZDLr6E-j2UNaNyJuanLpQ@mail.gmail.com>
References: <CA+jRDxAC1ktoJd4N8tFGoBhBSQSbs074Kmxftq1j3_pFVbvLaQ@mail.gmail.com>
	<CA+8X3fXc3_NGhF3RX4jHCAfNmOjqcZDLr6E-j2UNaNyJuanLpQ@mail.gmail.com>
Message-ID: <CA+jRDxDJoqeHK-NbCpaBEPQeXqT654TmmZWzVVhdgxC797D9Vg@mail.gmail.com>

Cool, thanks Jim!!
I would love to be able to write my own script for this as I have many
images/ pdf's in a folder and would like to batch process them using an R
script!!
Thanks

On Tuesday, July 26, 2016, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Shane,
> FreeOCR is a really good place to start.
>
> http://www.paperfile.net/
>
> Jim
>
>
> On Wed, Jul 27, 2016 at 6:11 AM, Shane Carey <careyshan at gmail.com
> <javascript:;>> wrote:
> > Hi,
> >
> > Has anyone ever done any ocr in R?? I have some scanned images that I
> would
> > like to convert to text!!
> > Thanks
> >
> >
> > --
> > Le gach dea ghui,
> > Shane
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
Le gach dea ghui,
Shane

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Jul 27 00:39:55 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 27 Jul 2016 08:39:55 +1000
Subject: [R] Ocr
In-Reply-To: <CA+jRDxDJoqeHK-NbCpaBEPQeXqT654TmmZWzVVhdgxC797D9Vg@mail.gmail.com>
References: <CA+jRDxAC1ktoJd4N8tFGoBhBSQSbs074Kmxftq1j3_pFVbvLaQ@mail.gmail.com>
	<CA+8X3fXc3_NGhF3RX4jHCAfNmOjqcZDLr6E-j2UNaNyJuanLpQ@mail.gmail.com>
	<CA+jRDxDJoqeHK-NbCpaBEPQeXqT654TmmZWzVVhdgxC797D9Vg@mail.gmail.com>
Message-ID: <CA+8X3fUgVKXgPiikUE5Sh-Z+auz2iJxL89G6L1+yTCFRaX1zQQ@mail.gmail.com>

Hi Shane,
If you want to run OCR on the command line, the Tessaract engine is
probably the way to go. Harder to build and install, but you can call
it from an R session.

Jim


On Wed, Jul 27, 2016 at 8:24 AM, Shane Carey <careyshan at gmail.com> wrote:
> Cool, thanks Jim!!
> I would love to be able to write my own script for this as I have many
> images/ pdf's in a folder and would like to batch process them using an R
> script!!
> Thanks
>
>
> On Tuesday, July 26, 2016, Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Shane,
>> FreeOCR is a really good place to start.
>>
>> http://www.paperfile.net/
>>
>> Jim
>>
>>
>> On Wed, Jul 27, 2016 at 6:11 AM, Shane Carey <careyshan at gmail.com> wrote:
>> > Hi,
>> >
>> > Has anyone ever done any ocr in R?? I have some scanned images that I
>> > would
>> > like to convert to text!!
>> > Thanks
>> >
>> >
>> > --
>> > Le gach dea ghui,
>> > Shane
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Le gach dea ghui,
> Shane
>


From Achim.Zeileis at uibk.ac.at  Wed Jul 27 00:43:36 2016
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Wed, 27 Jul 2016 00:43:36 +0200 (CEST)
Subject: [R] Ocr
In-Reply-To: <CA+jRDxDJoqeHK-NbCpaBEPQeXqT654TmmZWzVVhdgxC797D9Vg@mail.gmail.com>
References: <CA+jRDxAC1ktoJd4N8tFGoBhBSQSbs074Kmxftq1j3_pFVbvLaQ@mail.gmail.com>
	<CA+8X3fXc3_NGhF3RX4jHCAfNmOjqcZDLr6E-j2UNaNyJuanLpQ@mail.gmail.com>
	<CA+jRDxDJoqeHK-NbCpaBEPQeXqT654TmmZWzVVhdgxC797D9Vg@mail.gmail.com>
Message-ID: <alpine.DEB.2.20.1607270040130.19804@paninaro>

On Wed, 27 Jul 2016, Shane Carey wrote:

> Cool, thanks Jim!!
> I would love to be able to write my own script for this as I have many
> images/ pdf's in a folder and would like to batch process them using an R
> script!!

The underlying engine is "tesseract" which is also available as a 
command-line tool and on other OSs. In principle, it is not hard to call 
it with a system() command and then readLines() the resulting text. 
However, it might be useful to play with the available options in the GUI 
first to see what works best for your images.

> Thanks
>
> On Tuesday, July 26, 2016, Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi Shane,
>> FreeOCR is a really good place to start.
>>
>> http://www.paperfile.net/
>>
>> Jim
>>
>>
>> On Wed, Jul 27, 2016 at 6:11 AM, Shane Carey <careyshan at gmail.com
>> <javascript:;>> wrote:
>>> Hi,
>>>
>>> Has anyone ever done any ocr in R?? I have some scanned images that I
>> would
>>> like to convert to text!!
>>> Thanks
>>>
>>>
>>> --
>>> Le gach dea ghui,
>>> Shane
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
>> more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> -- 
> Le gach dea ghui,
> Shane
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rbaer at atsu.edu  Wed Jul 27 00:49:13 2016
From: rbaer at atsu.edu (Robert Baer)
Date: Tue, 26 Jul 2016 17:49:13 -0500
Subject: [R] Windows 10 Application Compatibility Check | FreeWare R
 Statistical Environment v3.2.2
In-Reply-To: <F7CCE7BE9001FB439190567BE5A9E2FE8DCB5A86@EXTXMB54.nam.nsroot.net>
References: <F7CCE7BE9001FB439190567BE5A9E2FE8DCB5A86@EXTXMB54.nam.nsroot.net>
Message-ID: <dba03f60-720b-6649-37b9-c93fa1456b26@atsu.edu>

Runs fine on Windows 10 for me.


On 7/25/2016 7:18 AM, Ramar, Rohini wrote:
> Hello Team,
>
> We are, Citi Application Readiness Team, need your assistance in order to gather info about below application compatibility and support for Win 10 as part of Window 10 Readiness initiative. CITI Bank has been using below "FreeWare R Statistical Environment v3.2.2"  software products currently on Win 7 operating system.
>
> We would like to know whether the below listed application is compatible and supported even for Win 10 (64 Bit) or is there any other higher version of application which would be compatible for Win10. If you have not tested for Win 10, could you please provide us with a tentative date by when we can reach you.
>
> Application Name : FreeWare R Statistical Environment v3.2.2
>
>
> Note: Kindly re-direct this email to appropriate team if we reached you wrongly.
>
>
> Regards,
> Rohini R
> Citi Architecture & Technology Engineering
> Client Computing
> Direct Phone #:+91 22 3346 1497
> Email ID: rohini.ramar at citi.com<mailto:rohini.ramar at citi.com>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bob at rudis.net  Wed Jul 27 04:22:43 2016
From: bob at rudis.net (boB Rudis)
Date: Tue, 26 Jul 2016 22:22:43 -0400
Subject: [R] Ocr
In-Reply-To: <alpine.DEB.2.20.1607270040130.19804@paninaro>
References: <CA+jRDxAC1ktoJd4N8tFGoBhBSQSbs074Kmxftq1j3_pFVbvLaQ@mail.gmail.com>
	<CA+8X3fXc3_NGhF3RX4jHCAfNmOjqcZDLr6E-j2UNaNyJuanLpQ@mail.gmail.com>
	<CA+jRDxDJoqeHK-NbCpaBEPQeXqT654TmmZWzVVhdgxC797D9Vg@mail.gmail.com>
	<alpine.DEB.2.20.1607270040130.19804@paninaro>
Message-ID: <CAJ4QxaM8LMmo8EjjPBx9U7GXUJcuP-pB4c8HEw=URpeXgY=dMQ@mail.gmail.com>

https://cran.rstudio.com/web/packages/abbyyR/index.html

https://github.com/greenore/ocR

https://electricarchaeology.ca/2014/07/15/doing-ocr-within-r/

that was from a Google "r ocr" search. So, yes, there are options.

On Tue, Jul 26, 2016 at 6:43 PM, Achim Zeileis <Achim.Zeileis at uibk.ac.at> wrote:
> On Wed, 27 Jul 2016, Shane Carey wrote:
>
>> Cool, thanks Jim!!
>> I would love to be able to write my own script for this as I have many
>> images/ pdf's in a folder and would like to batch process them using an R
>> script!!
>
>
> The underlying engine is "tesseract" which is also available as a
> command-line tool and on other OSs. In principle, it is not hard to call it
> with a system() command and then readLines() the resulting text. However, it
> might be useful to play with the available options in the GUI first to see
> what works best for your images.
>
>
>> Thanks
>>
>> On Tuesday, July 26, 2016, Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>>> Hi Shane,
>>> FreeOCR is a really good place to start.
>>>
>>> http://www.paperfile.net/
>>>
>>> Jim
>>>
>>>
>>> On Wed, Jul 27, 2016 at 6:11 AM, Shane Carey <careyshan at gmail.com
>>> <javascript:;>> wrote:
>>>>
>>>> Hi,
>>>>
>>>> Has anyone ever done any ocr in R?? I have some scanned images that I
>>>
>>> would
>>>>
>>>> like to convert to text!!
>>>> Thanks
>>>>
>>>>
>>>> --
>>>> Le gach dea ghui,
>>>> Shane
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
>>>
>>> more, see
>>>>
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>
>>> http://www.R-project.org/posting-guide.html
>>>>
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>>
>> --
>> Le gach dea ghui,
>> Shane
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From A.Robinson at ms.unimelb.edu.au  Wed Jul 27 04:47:55 2016
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 27 Jul 2016 12:47:55 +1000
Subject: [R] lm() silently drops NAs
In-Reply-To: <AB5B2D91-FFC6-4142-88D1-D7BFBFA6BFCA@gmail.com>
References: <CABdHhvEAh4SYRKStO8tTZOAF=HXWmVwuioC+QkUaLJ04SEPcdg@mail.gmail.com>
	<22423.7743.681107.446114@stat.math.ethz.ch>
	<CABdHhvGYoydrVDOS_Wn7BpV1isvWrP+cSUwcUOQ93BGeftYcBg@mail.gmail.com>
	<AB5B2D91-FFC6-4142-88D1-D7BFBFA6BFCA@gmail.com>
Message-ID: <CAHyGmd61fgxHgH1LZ=U9DdmF+p0vKTj=-cX=6JM_GaxTmRV-KA@mail.gmail.com>

Agh.  I've argued elsewhere that the default behaviour should be to
fail, and the user should take the responsibility to explicitly handle
the missing values, even if that simply be by changing the argument.
Probably Peter and I have different experiences with the completeness
of datasets, but anything that encourages me not to think about what
I'm doing in that realm seems like a bad idea.

Best wishes,

Andrew

On 27 July 2016 at 07:30, peter dalgaard <pdalgd at gmail.com> wrote:
>
>> On 26 Jul 2016, at 22:26 , Hadley Wickham <h.wickham at gmail.com> wrote:
>>
>> On Tue, Jul 26, 2016 at 3:24 AM, Martin Maechler
>> <maechler at stat.math.ethz.ch> wrote:
>>>
> ...
>> To me, this would be the most sensible default behaviour, but I
>> realise it's too late to change without breaking many existing
>> expectations.
>
> Probably.
>
> Re. the default choice, my recollection is that at the time the only choices available were na.omit and na.fail. S-PLUS was using na.fail for all the usual good reasons, but from a practical perspective, the consequence was that, since almost every data set has NA values,  you got an error unless you added na.action=na.omit to every single lm() call. And habitually typing na.action=na.omit doesn't really solve any of the issues with different models being fit to different subsets and all that. So the rationale for doing it differently in R was that it was better to get some probably meaningful output rather than to be certain of getting nothing. And, that was what the mainstream packages of the time were doing.
>
>> On a related note, I've never really understood why it's called
>> na.exclude - from my perspective it causes the _inclusion_ of missing
>> values in the predictions/residuals.
>
> I think the notion is that you exclude them from the analysis, but keep them around for the other purposes.
>
> -pd
>
>>
>> Thanks for the (as always!) informative response, Martin.
>>
>> Hadley
>>
>> --
>> http://hadley.nz
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Andrew Robinson
Deputy Director, CEBRA, School of Biosciences
Reader & Associate Professor in Applied Statistics  Tel: (+61) 0403 138 955
School of Mathematics and Statistics                        Fax: +61-3-8344 4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au
Website: http://www.ms.unimelb.edu.au/~andrewpr

MSME: http://www.crcpress.com/product/isbn/9781439858028
FAwR: http://www.ms.unimelb.edu.au/~andrewpr/FAwR/
SPuR: http://www.ms.unimelb.edu.au/spuRs/


From drjimlemon at gmail.com  Wed Jul 27 06:49:11 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 27 Jul 2016 14:49:11 +1000
Subject: [R] change the colour line in gamm4 plotting
In-Reply-To: <1888888338.8562589.1469540878724.JavaMail.yahoo@mail.yahoo.com>
References: <1888888338.8562589.1469540878724.JavaMail.yahoo.ref@mail.yahoo.com>
	<1888888338.8562589.1469540878724.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+8X3fUqanQ0C_XJ9NNBNzEXV14PRG2smwAubLGn94RVd5w5cg@mail.gmail.com>

Hi Maria,
The "plot.gam" function doesn't use the "col" argument for lines, but
does change the color of points in the second example on the help
page. There doesn't seem to be an easy way to change the function to
get what you want.

Jim


On Tue, Jul 26, 2016 at 11:47 PM, Maria Lathouri via R-help
<r-help at r-project.org> wrote:
> Dear all
>
> I am stuck probably in a simple plotting question
>
> My model is:model.1<-gamm4(y~s(x1, by=end.group)+Year+K1+k2+k3, data=.., random=~(1|WB_ID/Site_ID))
> where y is my dependent variable, x1 is the smooth covariate and I use the by argument for the smooth term based on six different groups, and then the Year, k1, k2, k3 are explanatory fixed variables
>
> I use the plot command to plot the model and I get six different plots:plot(model.1$gam, pages=1, xlab=" ", ylab=" ")
> I would like to change the colour of the fitted and the standard error lines, from black which is the default to another colour; I tried to use the col=" " function, at least for the main line, in the plot command but it is not working.
>
> Any advice/suggestion is welcome.
> Many thanks.Maria
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From canamika at gmail.com  Wed Jul 27 05:07:28 2016
From: canamika at gmail.com (Anamika Chaudhuri)
Date: Tue, 26 Jul 2016 23:07:28 -0400
Subject: [R] Stack dataframes into a matrix
Message-ID: <CALv--daQYH4C90=C=1YPn-W-XJrb-vQHFW5BSqu=MtpqqCMe3g@mail.gmail.com>

I have 100 datasets with 20 rows and 2 columns in each dataset.
I am looking for help to produce x and y below as 1000 X 20 matrix and then
repeat that across 100 datasets using R

         library(MASS)
         library(car)
         set.seed(1234)
         library(mixtools)
         library(sp)

        for (k in 1:1){  # k IS THE NO OF DATASETS
        Y <- read.csv(file=paste0("MVNfreq",k,".csv"))

        Y<-as.matrix(Y)
        Y <- ifelse(Y==0,Y+.5,Y)


        Y1<-Y/60 # estimates of p

        #print(Y1)


sigma2<-matrix(c(var(Y1[,1]),cov(Y1[,1],Y1[,2]),cov(Y1[,1],Y1[,2]),var(Y1[,2])),2,2)

        rho<-sigma2[1,2]/sqrt(sigma2[1,1]*sigma2[2,2])
        mean(Y1[,1])
        mean(Y1[,2])

        #within<-matrix(data=0,nrow=20,ncol=1)

        for (rate3 in 1:20){
        rate<-Y1[i,]
        #print(rate)
        rate1<-rate/(1-rate)
        rate2<-log(rate1)

        Sigma11<-(1/(rate[1]*(1-rate[1]))^2)*sigma2[1,1]
        Sigma22<-(1/(rate[2]*(1-rate[2]))^2)*sigma2[2,2]

Sigma12<-(1/((rate[1]*(1-rate[1]))*(rate[2]*(1-rate[2]))))*sigma2[1,2]

        Sigma2<-matrix(c(Sigma11,Sigma12,Sigma12,Sigma22),2,2)

        rate3<-mvrnorm(1000, mu=c(rate2[1],rate2[2]), Sigma2)
        x<-exp(rate3[,1])/(1+exp(rate3[,1]))
        y<-exp(rate3[,2])/(1+exp(rate3[,2]))
        x<-as.data.frame(x)
        stack(x) # Need help to stack x into a single matrix
        print(x)
        print(y)
        }
        }

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul 27 08:46:26 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 26 Jul 2016 23:46:26 -0700
Subject: [R] Stack dataframes into a matrix
In-Reply-To: <CALv--daQYH4C90=C=1YPn-W-XJrb-vQHFW5BSqu=MtpqqCMe3g@mail.gmail.com>
References: <CALv--daQYH4C90=C=1YPn-W-XJrb-vQHFW5BSqu=MtpqqCMe3g@mail.gmail.com>
Message-ID: <0C584896-C750-4795-A6DC-E57CC14FDAFA@comcast.net>


> On Jul 26, 2016, at 8:07 PM, Anamika Chaudhuri <canamika at gmail.com> wrote:
> 
> I have 100 datasets with 20 rows and 2 columns in each dataset.
> I am looking for help to produce x and y below as 1000 X 20 matrix and then
> repeat that across 100 datasets using R
> 
>         library(MASS)
>         library(car)
>         set.seed(1234)
>         library(mixtools)
>         library(sp)
> 
>        for (k in 1:1){  # k IS THE NO OF DATASETS
>        Y <- read.csv(file=paste0("MVNfreq",k,".csv"))

So this is not reproducible but from the description seems like 

do.call( rbind, .... # the list of dataframes might "work" assuming column names are _all_ the same.

-- 
David.
> 
>        Y<-as.matrix(Y)
>        Y <- ifelse(Y==0,Y+.5,Y)
> 
> 
>        Y1<-Y/60 # estimates of p
> 
>        #print(Y1)
> 
> 
> sigma2<-matrix(c(var(Y1[,1]),cov(Y1[,1],Y1[,2]),cov(Y1[,1],Y1[,2]),var(Y1[,2])),2,2)
> 
>        rho<-sigma2[1,2]/sqrt(sigma2[1,1]*sigma2[2,2])
>        mean(Y1[,1])
>        mean(Y1[,2])
> 
>        #within<-matrix(data=0,nrow=20,ncol=1)
> 
>        for (rate3 in 1:20){
>        rate<-Y1[i,]
>        #print(rate)
>        rate1<-rate/(1-rate)
>        rate2<-log(rate1)
> 
>        Sigma11<-(1/(rate[1]*(1-rate[1]))^2)*sigma2[1,1]
>        Sigma22<-(1/(rate[2]*(1-rate[2]))^2)*sigma2[2,2]
> 
> Sigma12<-(1/((rate[1]*(1-rate[1]))*(rate[2]*(1-rate[2]))))*sigma2[1,2]
> 
>        Sigma2<-matrix(c(Sigma11,Sigma12,Sigma12,Sigma22),2,2)
> 
>        rate3<-mvrnorm(1000, mu=c(rate2[1],rate2[2]), Sigma2)
>        x<-exp(rate3[,1])/(1+exp(rate3[,1]))
>        y<-exp(rate3[,2])/(1+exp(rate3[,2]))
>        x<-as.data.frame(x)
>        stack(x) # Need help to stack x into a single matrix
>        print(x)
>        print(y)
>        }
>        }
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From careyshan at gmail.com  Wed Jul 27 08:57:32 2016
From: careyshan at gmail.com (Shane Carey)
Date: Wed, 27 Jul 2016 07:57:32 +0100
Subject: [R] Ocr
In-Reply-To: <CAJ4QxaM8LMmo8EjjPBx9U7GXUJcuP-pB4c8HEw=URpeXgY=dMQ@mail.gmail.com>
References: <CA+jRDxAC1ktoJd4N8tFGoBhBSQSbs074Kmxftq1j3_pFVbvLaQ@mail.gmail.com>
	<CA+8X3fXc3_NGhF3RX4jHCAfNmOjqcZDLr6E-j2UNaNyJuanLpQ@mail.gmail.com>
	<CA+jRDxDJoqeHK-NbCpaBEPQeXqT654TmmZWzVVhdgxC797D9Vg@mail.gmail.com>
	<alpine.DEB.2.20.1607270040130.19804@paninaro>
	<CAJ4QxaM8LMmo8EjjPBx9U7GXUJcuP-pB4c8HEw=URpeXgY=dMQ@mail.gmail.com>
Message-ID: <CA+jRDxC9Vw04QB33Wu7-9ZnmTvO4fJ5Wjq573UhwEQEg06x0KQ@mail.gmail.com>

Yep, I seen that!! Just don't know which one is best!! Any who, I'll try
them out and see how it goes!! I'm sure I'll be back looking for help!!!
Rhanks

On Wednesday, July 27, 2016, boB Rudis <bob at rudis.net> wrote:

> https://cran.rstudio.com/web/packages/abbyyR/index.html
>
> https://github.com/greenore/ocR
>
> https://electricarchaeology.ca/2014/07/15/doing-ocr-within-r/
>
> that was from a Google "r ocr" search. So, yes, there are options.
>
> On Tue, Jul 26, 2016 at 6:43 PM, Achim Zeileis <Achim.Zeileis at uibk.ac.at
> <javascript:;>> wrote:
> > On Wed, 27 Jul 2016, Shane Carey wrote:
> >
> >> Cool, thanks Jim!!
> >> I would love to be able to write my own script for this as I have many
> >> images/ pdf's in a folder and would like to batch process them using an
> R
> >> script!!
> >
> >
> > The underlying engine is "tesseract" which is also available as a
> > command-line tool and on other OSs. In principle, it is not hard to call
> it
> > with a system() command and then readLines() the resulting text.
> However, it
> > might be useful to play with the available options in the GUI first to
> see
> > what works best for your images.
> >
> >
> >> Thanks
> >>
> >> On Tuesday, July 26, 2016, Jim Lemon <drjimlemon at gmail.com
> <javascript:;>> wrote:
> >>
> >>> Hi Shane,
> >>> FreeOCR is a really good place to start.
> >>>
> >>> http://www.paperfile.net/
> >>>
> >>> Jim
> >>>
> >>>
> >>> On Wed, Jul 27, 2016 at 6:11 AM, Shane Carey <careyshan at gmail.com
> <javascript:;>
> >>> <javascript:;>> wrote:
> >>>>
> >>>> Hi,
> >>>>
> >>>> Has anyone ever done any ocr in R?? I have some scanned images that I
> >>>
> >>> would
> >>>>
> >>>> like to convert to text!!
> >>>> Thanks
> >>>>
> >>>>
> >>>> --
> >>>> Le gach dea ghui,
> >>>> Shane
> >>>>
> >>>>         [[alternative HTML version deleted]]
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org <javascript:;> <javascript:;> mailing list --
> To UNSUBSCRIBE and
> >>>
> >>> more, see
> >>>>
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>
> >>> http://www.R-project.org/posting-guide.html
> >>>>
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >>
> >>
> >> --
> >> Le gach dea ghui,
> >> Shane
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
Le gach dea ghui,
Shane

	[[alternative HTML version deleted]]


From jon.skoien at jrc.ec.europa.eu  Wed Jul 27 09:10:44 2016
From: jon.skoien at jrc.ec.europa.eu (Jon Skoien)
Date: Wed, 27 Jul 2016 09:10:44 +0200
Subject: [R] about netcdf files
In-Reply-To: <CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
	<843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
	<CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
Message-ID: <f2713316-fd1c-0c2c-26c3-b4dd2371ff85@jrc.ec.europa.eu>

I think you get the error because you passed the object from nc_open(), 
you should rather pass the filename of the netCDF, so:

pre = brick("sample_precip_daily.nc")
 From the test-file, this gives you a RasterBrick-object with 5*9 pixels 
and 7305 layers.
Then you can extract data.frames of each layer with:
dframe = rasterToPoints(pre[[1]])
where 1 is the first layer, or time series with:
tseries = pre[3,4]
where 3 and 4 are the pixels in x and y-direction.

To write dframe to csv:
write.csv(dframe, filename = "csvfile.csv")
Check the function for more options.

Hope this helps you a bit further.
Jon

On 7/26/2016 9:07 PM, lily li wrote:
> Thanks for your reply. But it says "Error in (function (classes, fdef, 
> mtable)):
> unable to find an inherited method for function 'brick' for signature 
> 'ncdf4' "
>
> The dataset is attached. It contains daily precipitation data for 20 
> years, within a rectangle, so that there are several grid points. I 
> use the code to open it, but don't know how to get csv files, while 
> each file contains continuous daily precipitation data for each grid cell.
> pre1 = nc_open('sample_precip_daily.nc <http://sample_precip_daily.nc>')
> pre1
> pre1_rd = ncvar_get(pre1, 'precipitation')
> nc_close(pre1)
>
> Thanks for your help.
>
> On Tue, Jul 26, 2016 at 4:08 AM, Jon Skoien 
> <jon.skoien at jrc.ec.europa.eu <mailto:jon.skoien at jrc.ec.europa.eu>> wrote:
>
>     You could try with the brick function from the raster package.
>
>     bvar = brick(netcdfName)
>
>     This uses the ncdf4 functions for opening and reading the netcdf,
>     but makes it easier to extract data for each day:
>
>     p1 = rasterToPoints(bvar[[1]])
>     and write p1 to csv.
>
>     Best,
>     Jon
>
>
>
>     On 7/26/2016 6:54 AM, lily li wrote:
>
>         Hi all,
>
>         I have a problem in opening netcdf files. If one netcdf file
>         contains
>         longitude, latitude, and daily precipitation. How to relate each
>         precipitation record to its associated location, and export
>         them as csv
>         files? Thanks.
>
>         I just use nc_open(), ncvar_get(), but it is not very helpful.
>         Thanks for
>         any ideas.
>
>                 [[alternative HTML version deleted]]
>
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing
>         list -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         and provide commented, minimal, self-contained, reproducible code.
>
>
>     -- 
>     Jon Olav Sk?ien
>     Joint Research Centre - European Commission
>     Institute for Space, Security & Migration
>     Disaster Risk Management Unit
>
>     Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
>
>     jon.skoien at jrc.ec.europa.eu <mailto:jon.skoien at jrc.ec.europa.eu>
>     Tel: +39 0332 789205 <tel:%2B39%200332%20789205>
>
>     Disclaimer: Views expressed in this email are those of the
>     individual and do not necessarily represent official views of the
>     European Commission.
>
>

-- 
Jon Olav Sk?ien
Joint Research Centre - European Commission
Institute for Space, Security & Migration
Disaster Risk Management Unit

Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY

jon.skoien at jrc.ec.europa.eu
Tel:  +39 0332 789205

Disclaimer: Views expressed in this email are those of the individual and do not necessarily represent official views of the European Commission.


	[[alternative HTML version deleted]]


From faradj.g at gmail.com  Wed Jul 27 10:50:06 2016
From: faradj.g at gmail.com (Faradj Koliev)
Date: Wed, 27 Jul 2016 10:50:06 +0200
Subject: [R] Likelihood ratio test in porl (MASS)
Message-ID: <37B73D17-E0F3-44E9-BD37-A15C213E32D9@gmail.com>

Dear all, 

A quick question: Let?s say I have a full and a restricted model that looks something like this: 

Full<- polr(Y ~ X1+X2+X3+X4, data=data, Hess = TRUE,  method="logistic?) # ordered logistic regression 

Restricted<- polr(Y ~ X1+X2+X3, data=data, Hess = TRUE,  method="logistic?) # ordered logistic regression 

I wanted to conduct the F-test (using aov command) in order to determine whether the information from the X4 variable statistically improves our understanding of Y. 
However, I?ve been told that the likelihood ratio test is a better alternative. So, I would like to conduct the LR test. In rms package this is easy -- lrest(Full, Restricted) ? I?m just curious how to perform the same using polr. Thanks!
	[[alternative HTML version deleted]]


From Achim.Zeileis at uibk.ac.at  Wed Jul 27 11:39:05 2016
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Wed, 27 Jul 2016 11:39:05 +0200 (CEST)
Subject: [R] Likelihood ratio test in porl (MASS)
In-Reply-To: <37B73D17-E0F3-44E9-BD37-A15C213E32D9@gmail.com>
References: <37B73D17-E0F3-44E9-BD37-A15C213E32D9@gmail.com>
Message-ID: <alpine.DEB.2.20.1607271137450.20494@paninaro>

On Wed, 27 Jul 2016, Faradj Koliev wrote:

> Dear all, 
>
> A quick question: Let?s say I have a full and a restricted model that looks something like this: 
>
> Full<- polr(Y ~ X1+X2+X3+X4, data=data, Hess = TRUE,  method="logistic?) # ordered logistic regression 
>
> Restricted<- polr(Y ~ X1+X2+X3, data=data, Hess = TRUE,  method="logistic?) # ordered logistic regression 
>
> I wanted to conduct the F-test (using aov command) in order to determine whether the information from the X4 variable statistically improves our understanding of Y. 
> However, I?ve been told that the likelihood ratio test is a better alternative. So, I would like to conduct the LR test. In rms package this is easy -- lrest(Full, Restricted) ? I?m just curious how to perform the same using polr. Thanks!

One generic possibility to conduct the likelihood ratio test is the 
lrtest() function in package "lmtest", i.e.,

library("lmtest")
lrtest(Restricted, Full)

> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From peter.anthoni at kit.edu  Wed Jul 27 13:13:57 2016
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Wed, 27 Jul 2016 11:13:57 +0000
Subject: [R] Aggregate matrix in a 2 by 2 manor
In-Reply-To: <CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
	<CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
Message-ID: <E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>

Hi all,

I need to aggregate some matrix data (1440x720) to a lower dimension (720x360) for lots of years and variables

I can do double for loop, but that will be slow. Anybody know a quicker way?

here an example with a smaller matrix size:

tst=matrix(1:(8*4),ncol=8,nrow=4)
tst_2x2=matrix(NA,ncol=4,nrow=2)
nx=2
ny=2
for(ilon in seq(1,8,nx)) {
  for (ilat in seq(1,4,ny)) {
    ilon_2x2=1+(ilon-1)/nx
    ilat_2x2=1+(ilat-1)/ny
    tst_2x2[ilat_2x2,ilon_2x2] = mean(tst[ilat+0:1,ilon+0:1])
  }
}

tst
tst_2x2

> tst
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    5    9   13   17   21   25   29
[2,]    2    6   10   14   18   22   26   30
[3,]    3    7   11   15   19   23   27   31
[4,]    4    8   12   16   20   24   28   32

> tst_2x2
     [,1] [,2] [,3] [,4]
[1,]  3.5 11.5 19.5 27.5
[2,]  5.5 13.5 21.5 29.5


I though a cast to 3d-array might do the trick and apply over the new dimension, but that does not work, since it casts the data along the row.
> matrix(apply(array(tst,dim=c(nx,ny,8)),3,mean),nrow=nrow(tst)/ny)
     [,1] [,2] [,3] [,4]
[1,]  2.5 10.5 18.5 26.5
[2,]  6.5 14.5 22.5 30.5


cheers
Peter


From jfox at mcmaster.ca  Wed Jul 27 13:35:23 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Wed, 27 Jul 2016 11:35:23 +0000
Subject: [R] Likelihood ratio test in porl (MASS)
In-Reply-To: <37B73D17-E0F3-44E9-BD37-A15C213E32D9@gmail.com>
References: <37B73D17-E0F3-44E9-BD37-A15C213E32D9@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC8365468FC@FHSDB2D11-2.csu.mcmaster.ca>

Dear Faradj Koliev,

There is an anova() method for "polr" objects that computes LR chisquare tests for nested models, so a short answer to your question is anova(Full, Restricted).

The question, however, seems to reflect some misunderstandings. First aov() fits linear analysis-of-variance models, which assume normally distributed errors. These are different from the ordinal regression models, such as the proportional-odds model, fit by polr(). For the former, F-tests *are* LR tests; for the latter, F-tests aren't appropriate.

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox




> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Faradj Koliev
> Sent: July 27, 2016 4:50 AM
> To: r-help at r-project.org
> Subject: [R] Likelihood ratio test in porl (MASS)
> 
> Dear all,
> 
> A quick question: Let?s say I have a full and a restricted model that looks
> something like this:
> 
> Full<- polr(Y ~ X1+X2+X3+X4, data=data, Hess = TRUE,  method="logistic?) #
> ordered logistic regression
> 
> Restricted<- polr(Y ~ X1+X2+X3, data=data, Hess = TRUE,  method="logistic?) #
> ordered logistic regression
> 
> I wanted to conduct the F-test (using aov command) in order to determine
> whether the information from the X4 variable statistically improves our
> understanding of Y.
> However, I?ve been told that the likelihood ratio test is a better alternative. So,
> I would like to conduct the LR test. In rms package this is easy -- lrest(Full,
> Restricted) ? I?m just curious how to perform the same using polr. Thanks!
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From sezenismail at gmail.com  Wed Jul 27 14:03:58 2016
From: sezenismail at gmail.com (Ismail SEZEN)
Date: Wed, 27 Jul 2016 15:03:58 +0300
Subject: [R] Time format issue
In-Reply-To: <D36FF0B2-D2A0-4D82-8829-786282C6016E@comcast.net>
References: <CAPQRYa7WxN_0tAkbdyeOqw+u7pkPjsb9CAxcHMrbQW_xVGiE8A@mail.gmail.com>
	<D36FF0B2-D2A0-4D82-8829-786282C6016E@comcast.net>
Message-ID: <2E3CE2D5-728E-4E50-BB68-D91A3EE4A14E@gmail.com>


>> The time column is factor. I want to convert each time to the following
>> format:
>> 
>> 20111211003000
>> 
>> 20111211013000
>> 
>> 20111211020000
>> 
>> (Year)(month)(date)(hr)(min)(sec)
> 
>> newTimeDt <- strptime(Time, format="%d%b%y:%H:%M")
>> newTimeDt
> [1] "2011-12-27 00:30:00 PST" "2011-12-27 01:30:00 PST" "2011-12-27 02:00:00 PST"
>> newTimeDt <- as.POSIXct(Time, format="%d%b%y:%H:%M")
>> newTimeDt
> [1] "2011-12-27 00:30:00 PST" "2011-12-27 01:30:00 PST" "2011-12-27 02:00:00 PST"
>> newOut <- format(newTimeDt, "%Y%m%d%H%M")
>> newOut
> [1] "201112270030" "201112270130" "201112270200"
>> newOut <- format(newTimeDt, "%Y%m%d%H%M%S")
>> newOut
> [1] "20111227003000" "20111227013000" "20111227020000"

Addition to David, if you use functions similar to read.csv or read.table to read dates from a file, you have to set 

stringsAsFactors = T

in function parameters or set data.frame column as character seperately.


	[[alternative HTML version deleted]]


From faradj.g at gmail.com  Wed Jul 27 14:52:38 2016
From: faradj.g at gmail.com (Faradj Koliev)
Date: Wed, 27 Jul 2016 14:52:38 +0200
Subject: [R] Likelihood ratio test in porl (MASS)
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC8365468FC@FHSDB2D11-2.csu.mcmaster.ca>
References: <37B73D17-E0F3-44E9-BD37-A15C213E32D9@gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC8365468FC@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <43207C55-BFDA-43FF-8B82-AD95A9C1E08D@gmail.com>

Dear Achim Zeileis, dear John Fox,

Thank you for your time! Both worked well. 

lrtest(Restrict, Full)

  #Df  LogLik Df  Chisq Pr(>Chisq)    
1  27 -882.00                         
2  28 -866.39  1 31.212  2.313e-08 ***


anova(Restrict, Full)

  Resid. df Resid. Dev   Test    Df LR stat.      Pr(Chi)
1      2121   1763.999                                   
2      2120   1732.787 1 vs 2     1 31.21204 2.313266e-08



And both seems to reject the null hypothesis.  Thanks again! 

Best, 
Faradj








> 27 jul 2016 kl. 13:35 skrev Fox, John <jfox at mcmaster.ca>:
> 
> Dear Faradj Koliev,
> 
> There is an anova() method for "polr" objects that computes LR chisquare tests for nested models, so a short answer to your question is anova(Full, Restricted).
> 
> The question, however, seems to reflect some misunderstandings. First aov() fits linear analysis-of-variance models, which assume normally distributed errors. These are different from the ordinal regression models, such as the proportional-odds model, fit by polr(). For the former, F-tests *are* LR tests; for the latter, F-tests aren't appropriate.
> 
> I hope this helps,
> John
> 
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> Web: socserv.mcmaster.ca/jfox
> 
> 
> 
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Faradj Koliev
>> Sent: July 27, 2016 4:50 AM
>> To: r-help at r-project.org
>> Subject: [R] Likelihood ratio test in porl (MASS)
>> 
>> Dear all,
>> 
>> A quick question: Let?s say I have a full and a restricted model that looks
>> something like this:
>> 
>> Full<- polr(Y ~ X1+X2+X3+X4, data=data, Hess = TRUE,  method="logistic?) #
>> ordered logistic regression
>> 
>> Restricted<- polr(Y ~ X1+X2+X3, data=data, Hess = TRUE,  method="logistic?) #
>> ordered logistic regression
>> 
>> I wanted to conduct the F-test (using aov command) in order to determine
>> whether the information from the X4 variable statistically improves our
>> understanding of Y.
>> However, I?ve been told that the likelihood ratio test is a better alternative. So,
>> I would like to conduct the LR test. In rms package this is easy -- lrest(Full,
>> Restricted) ? I?m just curious how to perform the same using polr. Thanks!
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From srivibish at gmail.com  Wed Jul 27 15:14:08 2016
From: srivibish at gmail.com (sri vathsan)
Date: Wed, 27 Jul 2016 18:44:08 +0530
Subject: [R] Reducing execution time
Message-ID: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>

Hi,

I created list of 3 combination numbers (mycombos, around 3 lakh
combinations) and counting the occurrence of those combination in another
list. This comparision list (mylist) is having around 8000 records.I am
using the following code.

myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
  sum(sapply(myList, function(j) {
    sum(!is.na(match(c(myCombos[i,]), j)))})==3)})

The above code takes very long time to execute and is there any other
effecting method which will reduce the time.
-- 

Regards,
Srivathsan.K

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed Jul 27 15:28:37 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 27 Jul 2016 15:28:37 +0200
Subject: [R] Reducing execution time
In-Reply-To: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
Message-ID: <CAJuCY5wUcW3AiCbVowSPNmuD3Zw02M6OQRTQELSKf2n4_JKk0A@mail.gmail.com>

A reproducible example makes your problem easier to understand.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-07-27 15:14 GMT+02:00 sri vathsan <srivibish at gmail.com>:

> Hi,
>
> I created list of 3 combination numbers (mycombos, around 3 lakh
> combinations) and counting the occurrence of those combination in another
> list. This comparision list (mylist) is having around 8000 records.I am
> using the following code.
>
> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
>   sum(sapply(myList, function(j) {
>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
>
> The above code takes very long time to execute and is there any other
> effecting method which will reduce the time.
> --
>
> Regards,
> Srivathsan.K
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Jul 27 15:30:19 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 27 Jul 2016 06:30:19 -0700
Subject: [R] Reducing execution time
In-Reply-To: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
Message-ID: <CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>

Not entirely sure I understand, but match() is already vectorized, so you
should be able to lose the supply(). This would speed things up a lot.
Please re-read ?match *carefully* .

Bert

On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com> wrote:

Hi,

I created list of 3 combination numbers (mycombos, around 3 lakh
combinations) and counting the occurrence of those combination in another
list. This comparision list (mylist) is having around 8000 records.I am
using the following code.

myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
  sum(sapply(myList, function(j) {
    sum(!is.na(match(c(myCombos[i,]), j)))})==3)})

The above code takes very long time to execute and is there any other
effecting method which will reduce the time.
--

Regards,
Srivathsan.K

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From frainj at gmail.com  Wed Jul 27 16:47:59 2016
From: frainj at gmail.com (John C Frain)
Date: Wed, 27 Jul 2016 15:47:59 +0100
Subject: [R] Windows 10 Application Compatibility Check | FreeWare R
 Statistical Environment v3.2.2
In-Reply-To: <dba03f60-720b-6649-37b9-c93fa1456b26@atsu.edu>
References: <F7CCE7BE9001FB439190567BE5A9E2FE8DCB5A86@EXTXMB54.nam.nsroot.net>
	<dba03f60-720b-6649-37b9-c93fa1456b26@atsu.edu>
Message-ID: <CAHrK515L7GqZSsvDzOaE+s5hcuuMahbwP34hXY919KX50oarRg@mail.gmail.com>

When I first graduated some 50 years ago I worked in the Department of
Finance. On small piece of my work involved getting CPI data from the CSO
and doing some calculations. A specific person in the CSO (Central
Statistics Office) usually supplied this information and this fact was
always recorded on the file. On one occasion I could not contact that
person and I visited the department library and extracted the information
myself. When my boss saw that I had not contacted the CSO I was instructed
to contact them and repeat the calculation. It was explained to me that if
I contacted the CSO we could blame someone outside the Department of
Finance if they supplied the information.

It appears that Citi Architecture & Technology Engineering are behaving in
a similar way.  Are they trying to find some excuse to deny their users
access to R on the grounds that it is not "properly" supported. This does
happen in large organisations.

If any one knows of some organisation that is willing to provide support to
Citi (for a respectable fee) there might be a bit of money to be made.

John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

On 26 July 2016 at 23:49, Robert Baer <rbaer at atsu.edu> wrote:

> Runs fine on Windows 10 for me.
>
>
> On 7/25/2016 7:18 AM, Ramar, Rohini wrote:
>
>> Hello Team,
>>
>> We are, Citi Application Readiness Team, need your assistance in order to
>> gather info about below application compatibility and support for Win 10 as
>> part of Window 10 Readiness initiative. CITI Bank has been using below
>> "FreeWare R Statistical Environment v3.2.2"  software products currently on
>> Win 7 operating system.
>>
>> We would like to know whether the below listed application is compatible
>> and supported even for Win 10 (64 Bit) or is there any other higher version
>> of application which would be compatible for Win10. If you have not tested
>> for Win 10, could you please provide us with a tentative date by when we
>> can reach you.
>>
>> Application Name : FreeWare R Statistical Environment v3.2.2
>>
>>
>> Note: Kindly re-direct this email to appropriate team if we reached you
>> wrongly.
>>
>>
>> Regards,
>> Rohini R
>> Citi Architecture & Technology Engineering
>> Client Computing
>> Direct Phone #:+91 22 3346 1497
>> Email ID: rohini.ramar at citi.com<mailto:rohini.ramar at citi.com>
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From srivibish at gmail.com  Wed Jul 27 16:47:42 2016
From: srivibish at gmail.com (sri vathsan)
Date: Wed, 27 Jul 2016 20:17:42 +0530
Subject: [R] Reducing execution time
In-Reply-To: <CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
	<CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
Message-ID: <CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>

Hi,

Apologizes for the less information.

Basically, myCombos is a matrix with 3 variables which is a triplet that is
a combination of 79 codes. There are around 3lakh combination as such and
it looks like below.

V1 V2 V3
65 23 77
77 34 65
55 34 23
23 77 34
34 65 55

Each triplet will compare in a list (mylist) having 8177 elements which
will looks like below.

77,65,34,23,55
65,23,77,65,55,34
77,34,65
55,78,56
98,23,77,65,34

Now I want to count the no of occurrence of the triplet in the above list.
I.e., the triplet 65 23 77 is seen 3 times in the list. So my output looks
like below

V1 V2 V3 Freq
65 23 77  3
77 34 65  4
55 34 23  2

I hope, I made it clear this time.


On Wed, Jul 27, 2016 at 7:00 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Not entirely sure I understand, but match() is already vectorized, so you
> should be able to lose the supply(). This would speed things up a lot.
> Please re-read ?match *carefully* .
>
> Bert
>
> On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com> wrote:
>
> Hi,
>
> I created list of 3 combination numbers (mycombos, around 3 lakh
> combinations) and counting the occurrence of those combination in another
> list. This comparision list (mylist) is having around 8000 records.I am
> using the following code.
>
> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
>   sum(sapply(myList, function(j) {
>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
>
> The above code takes very long time to execute and is there any other
> effecting method which will reduce the time.
> --
>
> Regards,
> Srivathsan.K
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


-- 

Regards,
Srivathsan.K
Phone : 9600165206

	[[alternative HTML version deleted]]


From jon.skoien at jrc.ec.europa.eu  Wed Jul 27 17:31:49 2016
From: jon.skoien at jrc.ec.europa.eu (Jon Skoien)
Date: Wed, 27 Jul 2016 17:31:49 +0200
Subject: [R] about netcdf files
In-Reply-To: <CAN5afy8AP-NC5duOKdJmR89SRkJbBtWPCQ_RHcu2k8UPT4sZcw@mail.gmail.com>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
	<843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
	<CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
	<f2713316-fd1c-0c2c-26c3-b4dd2371ff85@jrc.ec.europa.eu>
	<CAN5afy8AP-NC5duOKdJmR89SRkJbBtWPCQ_RHcu2k8UPT4sZcw@mail.gmail.com>
Message-ID: <0bd462fb-72d2-a5a0-73f9-023b1278e377@jrc.ec.europa.eu>

Hi Lily,

You can ignore the first three lines, they are just for information.
Which version of raster do you have? I see that the attempt to read the 
crs-variable was not wrapped in a try-call in earlier versions. 
Upgrading might help.

Best,
Jon


On 7/27/2016 5:13 PM, lily li wrote:
> Hi Jon,
>
> I still have problems. The brick function does not work well and I 
> don't know where to check the problem. I put the error message below, 
> thanks.
>
> > pre = brick("~/Downloads/sample_precip_daily.nc 
> <http://sample_precip_daily.nc>")
> [1] "vobjtovarid4: error #F: I could not find the requsted var (or 
> dimvar) in the file!"
> [1] "var (or dimvar) name: crs"
> [1] "file name: /.../Downloads/sample_precip_daily.nc 
> <http://sample_precip_daily.nc>"
> Error in vobjtovarid4(nc, varid, allowdimvar = TRUE, verbose = verbose) :
>   Variable not found
>
>
> On Wed, Jul 27, 2016 at 1:10 AM, Jon Skoien 
> <jon.skoien at jrc.ec.europa.eu <mailto:jon.skoien at jrc.ec.europa.eu>> wrote:
>
>     I think you get the error because you passed the object from
>     nc_open(), you should rather pass the filename of the netCDF, so:
>
>     pre = brick("sample_precip_daily.nc <http://sample_precip_daily.nc>")
>     From the test-file, this gives you a RasterBrick-object with 5*9
>     pixels and 7305 layers.
>     Then you can extract data.frames of each layer with:
>     dframe = rasterToPoints(pre[[1]])
>     where 1 is the first layer, or time series with:
>     tseries = pre[3,4]
>     where 3 and 4 are the pixels in x and y-direction.
>
>     To write dframe to csv:
>     write.csv(dframe, filename = "csvfile.csv")
>     Check the function for more options.
>
>     Hope this helps you a bit further.
>     Jon
>
>
>     On 7/26/2016 9:07 PM, lily li wrote:
>>     Thanks for your reply. But it says "Error in (function (classes,
>>     fdef, mtable)):
>>     unable to find an inherited method for function 'brick' for
>>     signature 'ncdf4' "
>>
>>     The dataset is attached. It contains daily precipitation data for
>>     20 years, within a rectangle, so that there are several grid
>>     points. I use the code to open it, but don't know how to get csv
>>     files, while each file contains continuous daily precipitation
>>     data for each grid cell.
>>     pre1 = nc_open('sample_precip_daily.nc
>>     <http://sample_precip_daily.nc>')
>>     pre1
>>     pre1_rd = ncvar_get(pre1, 'precipitation')
>>     nc_close(pre1)
>>
>>     Thanks for your help.
>>
>>     On Tue, Jul 26, 2016 at 4:08 AM, Jon Skoien
>>     <jon.skoien at jrc.ec.europa.eu
>>     <mailto:jon.skoien at jrc.ec.europa.eu>> wrote:
>>
>>         You could try with the brick function from the raster package.
>>
>>         bvar = brick(netcdfName)
>>
>>         This uses the ncdf4 functions for opening and reading the
>>         netcdf, but makes it easier to extract data for each day:
>>
>>         p1 = rasterToPoints(bvar[[1]])
>>         and write p1 to csv.
>>
>>         Best,
>>         Jon
>>
>>
>>
>>         On 7/26/2016 6:54 AM, lily li wrote:
>>
>>             Hi all,
>>
>>             I have a problem in opening netcdf files. If one netcdf
>>             file contains
>>             longitude, latitude, and daily precipitation. How to
>>             relate each
>>             precipitation record to its associated location, and
>>             export them as csv
>>             files? Thanks.
>>
>>             I just use nc_open(), ncvar_get(), but it is not very
>>             helpful. Thanks for
>>             any ideas.
>>
>>                     [[alternative HTML version deleted]]
>>
>>             ______________________________________________
>>             R-help at r-project.org <mailto:R-help at r-project.org>
>>             mailing list -- To UNSUBSCRIBE and more, see
>>             https://stat.ethz.ch/mailman/listinfo/r-help
>>             PLEASE do read the posting guide
>>             http://www.R-project.org/posting-guide.html
>>             and provide commented, minimal, self-contained,
>>             reproducible code.
>>
>>
>>         -- 
>>         Jon Olav Sk?ien
>>         Joint Research Centre - European Commission
>>         Institute for Space, Security & Migration
>>         Disaster Risk Management Unit
>>
>>         Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
>>
>>         jon.skoien at jrc.ec.europa.eu <mailto:jon.skoien at jrc.ec.europa.eu>
>>         Tel: +39 0332 789205 <tel:%2B39%200332%20789205>
>>
>>         Disclaimer: Views expressed in this email are those of the
>>         individual and do not necessarily represent official views of
>>         the European Commission.
>>
>>
>
>     -- 
>     Jon Olav Sk?ien
>     Joint Research Centre - European Commission
>     Institute for Space, Security & Migration
>     Disaster Risk Management Unit
>
>     Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
>
>     jon.skoien at jrc.ec.europa.eu <mailto:jon.skoien at jrc.ec.europa.eu>
>     Tel:+39 0332 789205 <tel:%2B39%200332%20789205>
>
>     Disclaimer: Views expressed in this email are those of the individual and do not necessarily represent official views of the European Commission.
>
>

-- 
Jon Olav Sk?ien
Joint Research Centre - European Commission
Institute for Space, Security & Migration
Disaster Risk Management Unit

Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY

jon.skoien at jrc.ec.europa.eu
Tel:  +39 0332 789205

Disclaimer: Views expressed in this email are those of the individual and do not necessarily represent official views of the European Commission.


	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Wed Jul 27 17:32:17 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 27 Jul 2016 11:32:17 -0400
Subject: [R] Reducing execution time
In-Reply-To: <CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
	<CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
	<CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>
Message-ID: <CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>

Hi,

It's really a good idea to use dput() or some other reproducible way
to provide data. I had to guess as to what your data looked like.

It appears that order doesn't matter?

Given than, here's one approach:

combs <- structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L, 34L,
34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
"V2", "V3"), class = "data.frame", row.names = c(NA, -5L))

dat <- list(
c(77,65,34,23,55),
c(65,23,77,65,55,34),
c(77,34,65),
c(55,78,56),
c(98,23,77,65,34))


sapply(seq_len(nrow(combs)), function(i)sum(sapply(dat,
function(j)all(combs[i,] %in% j))))

On a dataset of comparable time to yours, it takes me under a minute and a half.

> combs <- combs[rep(1:nrow(combs), length=100), ]
> dat <- dat[rep(1:length(dat), length=10000)]
>
> dim(combs)
[1] 100   3
> length(dat)
[1] 10000
>
> system.time(test <- sapply(seq_len(nrow(combs)), function(i)sum(sapply(dat, function(j)all(combs[i,] %in% j)))))
   user  system elapsed
 86.380   0.006  86.391




On Wed, Jul 27, 2016 at 10:47 AM, sri vathsan <srivibish at gmail.com> wrote:
> Hi,
>
> Apologizes for the less information.
>
> Basically, myCombos is a matrix with 3 variables which is a triplet that is
> a combination of 79 codes. There are around 3lakh combination as such and
> it looks like below.
>
> V1 V2 V3
> 65 23 77
> 77 34 65
> 55 34 23
> 23 77 34
> 34 65 55
>
> Each triplet will compare in a list (mylist) having 8177 elements which
> will looks like below.
>
> 77,65,34,23,55
> 65,23,77,65,55,34
> 77,34,65
> 55,78,56
> 98,23,77,65,34
>
> Now I want to count the no of occurrence of the triplet in the above list.
> I.e., the triplet 65 23 77 is seen 3 times in the list. So my output looks
> like below
>
> V1 V2 V3 Freq
> 65 23 77  3
> 77 34 65  4
> 55 34 23  2
>
> I hope, I made it clear this time.
>
>
> On Wed, Jul 27, 2016 at 7:00 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
>> Not entirely sure I understand, but match() is already vectorized, so you
>> should be able to lose the supply(). This would speed things up a lot.
>> Please re-read ?match *carefully* .
>>
>> Bert
>>
>> On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com> wrote:
>>
>> Hi,
>>
>> I created list of 3 combination numbers (mycombos, around 3 lakh
>> combinations) and counting the occurrence of those combination in another
>> list. This comparision list (mylist) is having around 8000 records.I am
>> using the following code.
>>
>> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
>>   sum(sapply(myList, function(j) {
>>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
>>
>> The above code takes very long time to execute and is there any other
>> effecting method which will reduce the time.
>> --
>>
>> Regards,
>> Srivathsan.K
>>


From marc_schwartz at me.com  Wed Jul 27 18:09:54 2016
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 27 Jul 2016 11:09:54 -0500
Subject: [R] Windows 10 Application Compatibility Check | FreeWare R
 Statistical Environment v3.2.2
In-Reply-To: <CAHrK515L7GqZSsvDzOaE+s5hcuuMahbwP34hXY919KX50oarRg@mail.gmail.com>
References: <F7CCE7BE9001FB439190567BE5A9E2FE8DCB5A86@EXTXMB54.nam.nsroot.net>
	<dba03f60-720b-6649-37b9-c93fa1456b26@atsu.edu>
	<CAHrK515L7GqZSsvDzOaE+s5hcuuMahbwP34hXY919KX50oarRg@mail.gmail.com>
Message-ID: <DCAE57CB-2445-4AEF-92D7-893F18D60019@me.com>

Hi,

With the caveat that I speak for myself only, a few comments:

1. Given that Citi appears to have been running R on Windows 7 for some period of time, I am not sure that they are looking for reasons to deny users access to R. They are simply looking for some indication that R will run on Windows 10.

2. There is nothing presently in the relevant R for Windows FAQ:

  https://cran.r-project.org/bin/windows/base/rw-FAQ.html#Does-R-run-under-Windows-Vista_003f

that explicitly indicates that R will run on Windows 10. At some point, presumably, that would be updated to reflect more recent experience by R Core.

3. Besides Robert's comment below on his experience (n of 1), there are other comments that I see via a Google search that would suggest that his experience is not unique (e.g. https://rpubs.com/kartykr/rrproject) and there are threads in the R-Help archives from last September:

  https://stat.ethz.ch/pipermail/r-help/2015-September/431809.html
  https://stat.ethz.ch/pipermail/r-help/2015-September/431825.html

that also support the notion that R will run on Windows 10.

4. If Citi, wants to pursue a more "commercial-like" approach to paid support for R, there are commercial versions of R available, which I shall not mention here, but a quick Google search would avail them of more details. 

5. Lastly and perhaps most importantly, R is not Freeware. R (as made available by the R Foundation) is free open source software (FOSS) and is distributed under relevant open source licenses. The label "Freeware", which typically refers to proprietary software, only means that the binary application itself is free of charge. It does not mean that the source code for the application is also available. There is a very large functional and philosophical difference between FOSS and Freeware.

Regards,

Marc Schwartz


> On Jul 27, 2016, at 9:47 AM, John C Frain <frainj at gmail.com> wrote:
> 
> When I first graduated some 50 years ago I worked in the Department of
> Finance. On small piece of my work involved getting CPI data from the CSO
> and doing some calculations. A specific person in the CSO (Central
> Statistics Office) usually supplied this information and this fact was
> always recorded on the file. On one occasion I could not contact that
> person and I visited the department library and extracted the information
> myself. When my boss saw that I had not contacted the CSO I was instructed
> to contact them and repeat the calculation. It was explained to me that if
> I contacted the CSO we could blame someone outside the Department of
> Finance if they supplied the information.
> 
> It appears that Citi Architecture & Technology Engineering are behaving in
> a similar way.  Are they trying to find some excuse to deny their users
> access to R on the grounds that it is not "properly" supported. This does
> happen in large organisations.
> 
> If any one knows of some organisation that is willing to provide support to
> Citi (for a respectable fee) there might be a bit of money to be made.
> 
> John C Frain
> 3 Aranleigh Park
> Rathfarnham
> Dublin 14
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.html
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
> 
> On 26 July 2016 at 23:49, Robert Baer <rbaer at atsu.edu> wrote:
> 
>> Runs fine on Windows 10 for me.
>> 
>> 
>> On 7/25/2016 7:18 AM, Ramar, Rohini wrote:
>> 
>>> Hello Team,
>>> 
>>> We are, Citi Application Readiness Team, need your assistance in order to
>>> gather info about below application compatibility and support for Win 10 as
>>> part of Window 10 Readiness initiative. CITI Bank has been using below
>>> "FreeWare R Statistical Environment v3.2.2"  software products currently on
>>> Win 7 operating system.
>>> 
>>> We would like to know whether the below listed application is compatible
>>> and supported even for Win 10 (64 Bit) or is there any other higher version
>>> of application which would be compatible for Win10. If you have not tested
>>> for Win 10, could you please provide us with a tentative date by when we
>>> can reach you.
>>> 
>>> Application Name : FreeWare R Statistical Environment v3.2.2
>>> 
>>> 
>>> Note: Kindly re-direct this email to appropriate team if we reached you
>>> wrongly.
>>> 
>>> 
>>> Regards,
>>> Rohini R
>>> Citi Architecture & Technology Engineering
>>> Client Computing
>>> Direct Phone #:+91 22 3346 1497
>>> Email ID: rohini.ramar at citi.com<mailto:rohini.ramar at citi.com>


From dcarlson at tamu.edu  Wed Jul 27 18:08:32 2016
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 27 Jul 2016 16:08:32 +0000
Subject: [R] Aggregate matrix in a 2 by 2 manor
In-Reply-To: <E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
	<CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
	<E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>
Message-ID: <f2247c74794a41ad8a70f85d2c85df92@exch-2p-mbx-t2.ads.tamu.edu>

This should be faster. It uses apply() across the blocks. 

> ilon <- seq(1,8,nx)
> ilat <- seq(1,4,ny)
> cells <- as.matrix(expand.grid(ilat, ilon))
> blocks <- apply(cells, 1, function(x) tst[x[1]:(x[1]+1), x[2]:(x[2]+1)])
> block.means <- colMeans(blocks)
> tst_2x2 <- matrix(block.means, 2, 4)
> tst_2x2
     [,1] [,2] [,3] [,4]
[1,]  3.5 11.5 19.5 27.5
[2,]  5.5 13.5 21.5 29.5

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-poject.org] On Behalf Of Anthoni, Peter (IMK)
Sent: Wednesday, July 27, 2016 6:14 AM
To: r-help at r-project.org
Subject: [R] Aggregate matrix in a 2 by 2 manor

Hi all,

I need to aggregate some matrix data (1440x720) to a lower dimension (720x360) for lots of years and variables

I can do double for loop, but that will be slow. Anybody know a quicker way?

here an example with a smaller matrix size:

tst=matrix(1:(8*4),ncol=8,nrow=4)
tst_2x2=matrix(NA,ncol=4,nrow=2)
nx=2
ny=2
for(ilon in seq(1,8,nx)) {
  for (ilat in seq(1,4,ny)) {
    ilon_2x2=1+(ilon-1)/nx
    ilat_2x2=1+(ilat-1)/ny
    tst_2x2[ilat_2x2,ilon_2x2] = mean(tst[ilat+0:1,ilon+0:1])
  }
}

tst
tst_2x2

> tst
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    5    9   13   17   21   25   29
[2,]    2    6   10   14   18   22   26   30
[3,]    3    7   11   15   19   23   27   31
[4,]    4    8   12   16   20   24   28   32

> tst_2x2
     [,1] [,2] [,3] [,4]
[1,]  3.5 11.5 19.5 27.5
[2,]  5.5 13.5 21.5 29.5


I though a cast to 3d-array might do the trick and apply over the new dimension, but that does not work, since it casts the data along the row.
> matrix(apply(array(tst,dim=c(nx,ny,8)),3,mean),nrow=nrow(tst)/ny)
     [,1] [,2] [,3] [,4]
[1,]  2.5 10.5 18.5 26.5
[2,]  6.5 14.5 22.5 30.5


cheers
Peter

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From chocold12 at gmail.com  Wed Jul 27 18:46:50 2016
From: chocold12 at gmail.com (lily li)
Date: Wed, 27 Jul 2016 10:46:50 -0600
Subject: [R] about netcdf files
In-Reply-To: <0bd462fb-72d2-a5a0-73f9-023b1278e377@jrc.ec.europa.eu>
References: <CAN5afy_MqYxSGuoqEwKHEn3N9X7q9XYJDHAga-e1M8dR30Q2FA@mail.gmail.com>
	<843d58d8-c1c9-a1b4-0480-3f727c55bff1@jrc.ec.europa.eu>
	<CAN5afy9NeYPbG_B=KH3+pp2Dg9ytt9W5MJ+ApjFTRK2bVThSRA@mail.gmail.com>
	<f2713316-fd1c-0c2c-26c3-b4dd2371ff85@jrc.ec.europa.eu>
	<CAN5afy8AP-NC5duOKdJmR89SRkJbBtWPCQ_RHcu2k8UPT4sZcw@mail.gmail.com>
	<0bd462fb-72d2-a5a0-73f9-023b1278e377@jrc.ec.europa.eu>
Message-ID: <CAN5afy-OqV9X0vMXaPLq6hHfQ8npoo7PGZdrn5Rsw74d0w0QPA@mail.gmail.com>

Hi Jon,

The versions are: raster_2.5-2 sp_1.2-3


On Wed, Jul 27, 2016 at 9:31 AM, Jon Skoien <jon.skoien at jrc.ec.europa.eu>
wrote:

> Hi Lily,
>
> You can ignore the first three lines, they are just for information.
> Which version of raster do you have? I see that the attempt to read the
> crs-variable was not wrapped in a try-call in earlier versions. Upgrading
> might help.
>
> Best,
> Jon
>
>
>
> On 7/27/2016 5:13 PM, lily li wrote:
>
> Hi Jon,
>
> I still have problems. The brick function does not work well and I don't
> know where to check the problem. I put the error message below, thanks.
>
> > pre = brick("~/Downloads/sample_precip_daily.nc")
> [1] "vobjtovarid4: error #F: I could not find the requsted var (or dimvar)
> in the file!"
> [1] "var (or dimvar) name: crs"
> [1] "file name: /.../Downloads/sample_precip_daily.nc"
> Error in vobjtovarid4(nc, varid, allowdimvar = TRUE, verbose = verbose) :
>   Variable not found
>
>
> On Wed, Jul 27, 2016 at 1:10 AM, Jon Skoien <jon.skoien at jrc.ec.europa.eu>
> wrote:
>
>> I think you get the error because you passed the object from nc_open(),
>> you should rather pass the filename of the netCDF, so:
>>
>> pre = brick("sample_precip_daily.nc")
>> From the test-file, this gives you a RasterBrick-object with 5*9 pixels
>> and 7305 layers.
>> Then you can extract data.frames of each layer with:
>> dframe = rasterToPoints(pre[[1]])
>> where 1 is the first layer, or time series with:
>> tseries = pre[3,4]
>> where 3 and 4 are the pixels in x and y-direction.
>>
>> To write dframe to csv:
>> write.csv(dframe, filename = "csvfile.csv")
>> Check the function for more options.
>>
>> Hope this helps you a bit further.
>> Jon
>>
>>
>> On 7/26/2016 9:07 PM, lily li wrote:
>>
>> Thanks for your reply. But it says "Error in (function (classes, fdef,
>> mtable)):
>> unable to find an inherited method for function 'brick' for signature
>> 'ncdf4' "
>>
>> The dataset is attached. It contains daily precipitation data for 20
>> years, within a rectangle, so that there are several grid points. I use the
>> code to open it, but don't know how to get csv files, while each file
>> contains continuous daily precipitation data for each grid cell.
>> pre1 = nc_open('sample_precip_daily.nc')
>> pre1
>> pre1_rd = ncvar_get(pre1, 'precipitation')
>> nc_close(pre1)
>>
>> Thanks for your help.
>>
>> On Tue, Jul 26, 2016 at 4:08 AM, Jon Skoien <
>> <jon.skoien at jrc.ec.europa.eu>jon.skoien at jrc.ec.europa.eu> wrote:
>>
>>> You could try with the brick function from the raster package.
>>>
>>> bvar = brick(netcdfName)
>>>
>>> This uses the ncdf4 functions for opening and reading the netcdf, but
>>> makes it easier to extract data for each day:
>>>
>>> p1 = rasterToPoints(bvar[[1]])
>>> and write p1 to csv.
>>>
>>> Best,
>>> Jon
>>>
>>>
>>>
>>> On 7/26/2016 6:54 AM, lily li wrote:
>>>
>>>> Hi all,
>>>>
>>>> I have a problem in opening netcdf files. If one netcdf file contains
>>>> longitude, latitude, and daily precipitation. How to relate each
>>>> precipitation record to its associated location, and export them as csv
>>>> files? Thanks.
>>>>
>>>> I just use nc_open(), ncvar_get(), but it is not very helpful. Thanks
>>>> for
>>>> any ideas.
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> <http://www.R-project.org/posting-guide.html>
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>> --
>>> Jon Olav Sk?ien
>>> Joint Research Centre - European Commission
>>> Institute for Space, Security & Migration
>>> Disaster Risk Management Unit
>>>
>>> Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
>>>
>>> jon.skoien at jrc.ec.europa.eu
>>> Tel:  +39 0332 789205 <%2B39%200332%20789205>
>>>
>>> Disclaimer: Views expressed in this email are those of the individual
>>> and do not necessarily represent official views of the European Commission.
>>>
>>
>>
>> --
>> Jon Olav Sk?ien
>> Joint Research Centre - European Commission
>> Institute for Space, Security & Migration
>> Disaster Risk Management Unit
>>
>> Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
>> jon.skoien at jrc.ec.europa.eu
>> Tel:  +39 0332 789205
>>
>> Disclaimer: Views expressed in this email are those of the individual and do not necessarily represent official views of the European Commission.
>>
>>
>
> --
> Jon Olav Sk?ien
> Joint Research Centre - European Commission
> Institute for Space, Security & Migration
> Disaster Risk Management Unit
>
> Via E. Fermi 2749, TP 122,  I-21027 Ispra (VA), ITALY
> jon.skoien at jrc.ec.europa.eu
> Tel:  +39 0332 789205
>
> Disclaimer: Views expressed in this email are those of the individual and do not necessarily represent official views of the European Commission.
>
>

	[[alternative HTML version deleted]]


From abdoulayesar at gmail.com  Wed Jul 27 09:41:45 2016
From: abdoulayesar at gmail.com (Abdoulaye Sarr)
Date: Wed, 27 Jul 2016 07:41:45 +0000
Subject: [R] probelm with xlab ylab and xaxp barplot
In-Reply-To: <0D47020C-786A-4B80-8B4E-F946CE2000B3@me.com>
References: <C7042838-33C6-466B-8978-217A2E2320DC@gmail.com>
	<779771D7-EBEF-47EA-BE2C-4FF822E5D2DA@me.com>
	<CAN=6O0J_pG_cFWoq3dKuZ0rV84HemMdipC4U5J1wA1uJ94QQ6w@mail.gmail.com>
	<EC5F7F05-0C9F-4E5B-9205-DCA81200BD29@me.com>
	<C230B1E5-3155-4407-A2C9-4D11DB7E9416@gmail.com>
	<1E3D23B5-C854-4C80-B802-F6CC2980280A@me.com>
	<868300E7-D001-4669-88DD-EC27CD98C87B@gmail.com>
	<0D47020C-786A-4B80-8B4E-F946CE2000B3@me.com>
Message-ID: <CAN=6O0+3aVGzTL7TAmFa=MYOSNR09jEGna0DJP+n8FaUPKpenw@mail.gmail.com>

Hi Marc,

I have something very close now, and think to leave it like that.

Thank you so much for your help to solve the problem.

Cheers,

asarr

On Mon, Jul 25, 2016 at 6:03 PM, Marc Schwartz <marc_schwartz at me.com> wrote:

> Hi,
>
> First, I noted again that you still have:
>
>    xaxp=c(181,2005,1)
>
> in the first barplot() call. Get rid of that, as barplot() does not use
> normal axis ranges for the bar midpoints.
>
> Second, I do not see an indication that you are using the 'names.arg'
> argument in barplot(), which supplies the vector of text to place below
> each bar. If this is correct, then you want the basic barplot() call to
> look something like:
>
>   barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei",
>           names.arg = 1981:2005,
>           col=ifelse(z1>0,"green","brown"))
>
> where names.arg on the second line is the vector of years from 1981 to
> 2005. If the data passed to barplot() have name attributes (for example,
> they are the result of using table() on a vector), those would be used, but
> I am guessing that your z* vectors are just numeric vectors without labels.
>
> If you then need to further adjust the axes to make more room below the
> plots, adjust the first element of par(mar) until the default axes show.
>
> For example:
>
>   par(mar = c(6, 4, 4, 2))
>
> where 6 replaces the default 5 for the first element. You may need to go
> higher if the text is still not showing. If need be, increase it further
> slowly to view the impact. You can also change by less than a full integer
> (e.g. 5.5, 6.25, etc.) as you may need. At some point, you will overshoot
> with too much room and you can then back down slowly.
>
> Using this approach, each plot will have a similar size as you adjust the
> margins and then look similar visually, presuming that the axis ranges are
> the same for each.
>
> Regards,
>
> Marc
>
>
> > On Jul 25, 2016, at 11:56 AM, Abdoulaye SARR <abdoulayesar at gmail.com>
> wrote:
> >
> > Hi Marc,
> >
> > According to your guidance the labels are almost at the right place when
> adjusting mar values.
> >
> > A remaining need is to have the x axis at least for the two bottom
> figures as date from 1981 to 2005. Do you think this is doable. Ylim is
> fine but how tots in this case xlim.
> >
> > Best regards,
> >
> > asarr
> > Le 25 juil. 2016 ? 14:28, Marc Schwartz <marc_schwartz at me.com> a ?crit :
> >
> >> Hi,
> >>
> >> If your code below is a verbatim copy and paste, you still have the
> following two lines active:
> >>
> >>  par(mar=rep(2,4))
> >>
> >> and
> >>
> >>  op <- par(oma=c(1,2,3,5))
> >>
> >> Comment out both of those lines and then see what the result looks like.
> >>
> >> As I noted before, try the plot **without any modifications** to the
> default margin values. Then adjust from there, which may require you to
> increase, not decrease, the values from their defaults in order to have
> room for your text.
> >>
> >> The values you have for par(mar) above, for example, reduce the values
> to 2 for each side from the default, which is:
> >>
> >>  c(5, 4, 4, 2) + 0.1.
> >>
> >> So that alone will likely result in there not being enough room for
> your axis labels.
> >>
> >> You may also have to create the barplot without any default annotation
> created by the function itself and then add it with ?axis, ?text and
> ?mtext. You may also have to reduce the size of the font itself, which is
> done via the cex* arguments to barplot() and the additional annotation
> functions mentioned in the prior sentence.
> >>
> >> Regards,
> >>
> >> Marc
> >>
> >>
> >>> On Jul 25, 2016, at 8:06 AM, Abdoulaye SARR <abdoulayesar at gmail.com>
> wrote:
> >>>
> >>>
> >>>>> Hi Marc and Others,
> >>>
> >>>
> >>> I am still struggling to have my slab and ylab displayed on a bar
> plot. Marc did useful advise on playing with mar settings. I tried may
> combinations and can?t have it work.
> >>>
> >>> I paste the code I am suing hoping guidance on solving this issue.
> >>>
> >>>
> >>>
> >>>
> >>> ## extract works for all time steps
> >>> d1<-read.nc(gp)
> >>>
> >>> d2<-read.nc(er)
> >>>
> >>> d3<-read.nc(me)
> >>>
> >>> d4<-read.nc(ne)
> >>>
> >>> d5<-read.nc(ar)
> >>>
> >>> d6<-read.nc(cc)
> >>>
> >>> d7<-read.nc(mr)
> >>>
> >>> d8<-read.nc(ic)
> >>>
> >>> z1<-d1$spei
> >>> z2<-d2$spei
> >>> z3<-d3$spei
> >>> z4<-d4$spei
> >>> z5<-d5$spei
> >>> z6<-d6$spei
> >>> z7<-d7$spei
> >>> z8<-d8$spei
> >>> #par(oma=c(2,2,2,2))  # all sides have 3 lines of space
> >>>
> >>> par(mar=rep(2,4))
> >>> #par(mar=c(5.1, 4.1, 2.1, 2.1))
> >>> #par(mai=c(1.02,0.82,0.82,0.42))
> >>> op <- par(oma=c(1,2,3,5))
> >>> #op <- par(oma=c(6,5,0,0))
> >>> par(mfrow=c(4,2))
> >>>
> >>> line = 3
> >>>
> >>> barplot(z1, ylim=c(-2,2), xlab="Years", ylab="spei",
> xaxp=c(181,2005,1), col=ifelse(z1>0,"green","brown"))
> >>>
> >>> mtext("a")
> >>> barplot(z2,xlab="Years", ylab="spei",  ylim=c(-2,2),
> col=ifelse(z2>0,"green","brown"))
> >>> mtext("b")
> >>> barplot(z3, ylim=c(-2,2), xlab="Years", ylab="spei",
> col=ifelse(z3>0,"green","brown"))
> >>> mtext("c")
> >>> barplot(z4, xlab="Years", ylab="spei", ylim=c(-2,2),
> col=ifelse(z4>0,"green","brown"))
> >>> mtext("d")
> >>> barplot(z5, xlab="Years", ylab="spei", ylim=c(-2,2),
> col=ifelse(z5>0,"green","brown"))
> >>> mtext("e")
> >>> barplot(z6, xlab="Years", ylab="spei", ylim=c(-2,2),
> col=ifelse(z6>0,"green","brown"))
> >>> mtext("f")
> >>> barplot(z7,xlab="Years", ylab="spei",  ylim=c(-2,2),
> col=ifelse(z7>0,"green","brown"))
> >>> mtext("g")
> >>> barplot(z8,  ylim=c(-2,2), xlab="Years", ylab="spei",
> col=ifelse(z8>0,"green","brown"))
> >>> mtext("h")
> >>> par(op)
> >>>
> >>> Another solution with ggplot2 or lattice also welcome.
> >>>
> >>>
> >>> Best regards,
> >>>
> >>> asarr
> >>
> >> <snip>
> >
>
>

	[[alternative HTML version deleted]]


From arthur.stilben at gmail.com  Wed Jul 27 15:28:57 2016
From: arthur.stilben at gmail.com (Arthur Stilben)
Date: Wed, 27 Jul 2016 10:28:57 -0300
Subject: [R] How to use fuzzy_partition with fuzzy_trapezoid
Message-ID: <CAE9uMN-em1soEwN6BpXyQvRiLe9NgvwKFCkFM5WP3Qc9jpdcrA@mail.gmail.com>

Hello, guys!

I tried to do that:

> teste = fuzzy_partition( varnames = c( "a", "b" ), FUN = fuzzy_trapezoid, corners = c( 0, 1, 2, 3), corners = c(4, 5, 6, 7))
Error in FUN(i, ...) :
  argumento formal "corners" corresponde a m?ltiplos argumentos especificados


So, how can I use fuzzy_partitions to generate multiples trapezoids?

-- 
Arthur Rodrigues Stilben
Geoinform?tica - LENEP
(22) 2765-6555


From luis.cota at db.com  Wed Jul 27 16:41:52 2016
From: luis.cota at db.com (Luis Cota)
Date: Wed, 27 Jul 2016 14:41:52 +0000
Subject: [R] Have any of you succesfully used VS Code with R? How have you
 set this up?
Message-ID: <BE59AEC726BE7847B8478D60A11338C60C7FD5CF@UCUSDC1PWXMR006.us.db.com>

I've been keen to use either VSCode or Atom and have not had much success. Any suggestions?

Regards,

- Luis

____________________________________________________





---
This communication may contain confidential and/or privileged information. If you are not the intended recipient (or have received this communication in error) please notify the sender immediately and destroy this communication. Any unauthorized copying, disclosure or distribution of the material in this communication is strictly forbidden.

Deutsche Bank does not render legal or tax advice, and the information contained in this communication should not be regarded as such.


From Erich.Striessnig at wu.ac.at  Wed Jul 27 17:18:07 2016
From: Erich.Striessnig at wu.ac.at (Striessnig, Erich)
Date: Wed, 27 Jul 2016 15:18:07 +0000
Subject: [R] How to set up a dynamic panel data model with pgmm from plm
	pacakge?
Message-ID: <edad7649cded46e791a56e54279fb230@mbx8.ad.wu-wien.ac.at>

\n<<



Hi,



I am trying to set up a dynamic panel data model using the pgmm function from the plm package. The formula that I want to estimate is something like this:



[y(t) - y(t-1)] ~ [x(t) - x(t-1)] + [x(t-1) - y(t-1)]



where the lagged value of the dependent variable should be instrumented by its 4th and 5th lag.

Can anyone please tell me which one of the five formulas in the R-code below does that?





###########################

library(plm)



value.x <- rnorm(336)

value.y <- rnorm(336)

diff.xy <- value.x - value.y

my.data <- data.frame(expand.grid(ids=1:28,times=1:12),value.x,value.y,diff.xy)



form1 <- diff(value.y) ~ diff(value.x) + lag(I(value.x-value.y),1) | lag(value.y,4:5)

form2 <- diff(value.y) ~ diff(value.x) + lag(I(value.x-value.y),4:5) | lag(value.y,4:5)

form3 <- diff(value.y) ~ diff(value.x) + diff.xy | lag(diff.xy,4:5)

form4 <- diff(value.y) ~ diff(value.x) + lag(diff.xy,1) | lag(diff.xy,4:5)

form5 <- diff(value.y) ~ diff(value.x) + lag(diff.xy,1) | lag(I(value.x-value.y),4:5)



mod1 <- pgmm(formula=form1,data=my.data,index=c('ids','times'),

             effect="individual", model="twosteps")

summary(mod1)

###########################



Kind regards,

Erich







>>\n\n \n<<Write your query here, using your example code to illustrate>> \n<<End with your name and affiliation>>\n\n\n\n

--please do not edit the information below--



R Version:

platform = x86_64-w64-mingw32

arch = x86_64

os = mingw32

system = x86_64, mingw32

status =

 major = 3

minor = 2.5

year = 2016

month = 04

day = 14

svn rev = 70478

language = R

version.string = R version 3.2.5 (2016-04-14)

nickname = Very, Very Secure Dishes



Windows 7 x64 (build 7601) Service Pack 1



Locale:

LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252



Search Path:

.GlobalEnv, package:plm, package:TinnRcom, package:Hmisc, package:ggplot2, package:Formula, package:survival, package:lattice,

package:grid, package:R2HTML, package:formatR, package:svSocket, package:stats, package:graphics, package:grDevices, package:utils,

package:datasets, package:methods, SciViews:TempEnv, Autoloads, package:base

	[[alternative HTML version deleted]]


From stefan.kruger at gmail.com  Wed Jul 27 17:20:22 2016
From: stefan.kruger at gmail.com (Stefan Kruger)
Date: Wed, 27 Jul 2016 16:20:22 +0100
Subject: [R] Reduce woes
Message-ID: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>

Hi -

I'm new to R.

In other functional languages I'm familiar with you can often seed a call
to reduce() with a custom accumulator. Here's an example in Elixir:

map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
Enum.count(v), nil) end)
# %{"one" => 2, "three" => 1, "two" => 2}

In R-terms that's reducing a list of vectors to become a new list mapping
the names to the vector lengths.

Even in JavaScript, you can do similar things:

list = { one: [1, 1], three: [3], two: [2, 2] };
var result = Object.keys(list).reduceRight(function (acc, item) {
  acc[item] = list[item].length;
  return acc;
}, {});
// result == { two: 2, three: 1, one: 2 }

In R, from what I can gather, Reduce() is restricted such that any init
value you feed it is required to be of the same type as the elements of the
vector you're reducing -- so I can't build up. So whilst I can do, say

> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
[1] 111

I can't use Reduce to build up a list, vector or data frame?

What am I missing?

Many thanks for any pointers,

Stefan



-- 
Stefan Kruger <stefan.kruger at gmail.com>

	[[alternative HTML version deleted]]


From canamika at gmail.com  Wed Jul 27 17:56:33 2016
From: canamika at gmail.com (Anamika Chaudhuri)
Date: Wed, 27 Jul 2016 11:56:33 -0400
Subject: [R] Stack dataframes into a matrix
In-Reply-To: <0C584896-C750-4795-A6DC-E57CC14FDAFA@comcast.net>
References: <CALv--daQYH4C90=C=1YPn-W-XJrb-vQHFW5BSqu=MtpqqCMe3g@mail.gmail.com>
	<0C584896-C750-4795-A6DC-E57CC14FDAFA@comcast.net>
Message-ID: <CALv--da_4moaJsyi+y=Y9Ly3RJbw2-a-PkRASSktt0ugq_=RJw@mail.gmail.com>

Hi David,

Thanks for your response. rbind doesnot seem to work.
Here is a reproducible example

Y<-matrix(1:40,ncol=2)
Y1<-Y/60 # estimates of p

#print(Y1)

sigma2<-
matrix(c(var(Y1[,1]),cov(Y1[,1],Y1[,2]),cov(Y1[,1],Y1[,2]),var(Y1[,2])),2,2)
#print(sigma2)

rho<-sigma2[1,2]/sqrt(sigma2[1,1]*sigma2[2,2])
#rho

mean(Y1[,1])
mean(Y1[,2])

#within<-matrix(data=0,nrow=20,ncol=1)

for (rate3 in 1:20){

rate<-Y1[i,]
#print(rate)


rate1<-rate/(1-rate)


rate2<-log(rate1)


Sigma11<-(1/(rate[1]*(1-rate[1]))^2)*sigma2[1,1]
Sigma22<-(1/(rate[2]*(1-rate[2]))^2)*sigma2[2,2]
Sigma12<-(1/((rate[1]*(1-rate[1]))*(rate[2]*(1-rate[2]))))*sigma2[1,2]

Sigma2<-matrix(c(Sigma11,Sigma12,Sigma12,Sigma22),2,2)
#print(Sigma2)

rate3<-mvrnorm(1000, mu=c(rate2[1],rate2[2]), Sigma2)

#print(rate3)

x<-exp(rate3[,1])/(1+exp(rate3[,1]))
y<-exp(rate3[,2])/(1+exp(rate3[,2]))
print(x) # Need to be able to stack the x's to produce one matrix
}

Thanks
Anamika

On Wed, Jul 27, 2016 at 2:46 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jul 26, 2016, at 8:07 PM, Anamika Chaudhuri <canamika at gmail.com>
> wrote:
> >
> > I have 100 datasets with 20 rows and 2 columns in each dataset.
> > I am looking for help to produce x and y below as 1000 X 20 matrix and
> then
> > repeat that across 100 datasets using R
> >
> >         library(MASS)
> >         library(car)
> >         set.seed(1234)
> >         library(mixtools)
> >         library(sp)
> >
> >        for (k in 1:1){  # k IS THE NO OF DATASETS
> >        Y <- read.csv(file=paste0("MVNfreq",k,".csv"))
>
> So this is not reproducible but from the description seems like
>
> do.call( rbind, .... # the list of dataframes might "work" assuming column
> names are _all_ the same.
>
> --
> David.
> >
> >        Y<-as.matrix(Y)
> >        Y <- ifelse(Y==0,Y+.5,Y)
> >
> >
> >        Y1<-Y/60 # estimates of p
> >
> >        #print(Y1)
> >
> >
> >
> sigma2<-matrix(c(var(Y1[,1]),cov(Y1[,1],Y1[,2]),cov(Y1[,1],Y1[,2]),var(Y1[,2])),2,2)
> >
> >        rho<-sigma2[1,2]/sqrt(sigma2[1,1]*sigma2[2,2])
> >        mean(Y1[,1])
> >        mean(Y1[,2])
> >
> >        #within<-matrix(data=0,nrow=20,ncol=1)
> >
> >        for (rate3 in 1:20){
> >        rate<-Y1[i,]
> >        #print(rate)
> >        rate1<-rate/(1-rate)
> >        rate2<-log(rate1)
> >
> >        Sigma11<-(1/(rate[1]*(1-rate[1]))^2)*sigma2[1,1]
> >        Sigma22<-(1/(rate[2]*(1-rate[2]))^2)*sigma2[2,2]
> >
> > Sigma12<-(1/((rate[1]*(1-rate[1]))*(rate[2]*(1-rate[2]))))*sigma2[1,2]
> >
> >        Sigma2<-matrix(c(Sigma11,Sigma12,Sigma12,Sigma22),2,2)
> >
> >        rate3<-mvrnorm(1000, mu=c(rate2[1],rate2[2]), Sigma2)
> >        x<-exp(rate3[,1])/(1+exp(rate3[,1]))
> >        y<-exp(rate3[,2])/(1+exp(rate3[,2]))
> >        x<-as.data.frame(x)
> >        stack(x) # Need help to stack x into a single matrix
> >        print(x)
> >        print(y)
> >        }
> >        }
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From peter.langfelder at gmail.com  Wed Jul 27 19:22:56 2016
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Wed, 27 Jul 2016 10:22:56 -0700
Subject: [R] Reduce woes
In-Reply-To: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
Message-ID: <CA+hbrhUBozCG8ct+NUnF9yttfCJc4Tx-v9XkBt7dpTYbr3jTtg@mail.gmail.com>

If you have a simple list of vectors (call it lst), use

lengths = sapply(lst, length)

In general, you may want to look at functions lapply and sapply which
apply a function over a list, in this case the function length().

Peter

On Wed, Jul 27, 2016 at 8:20 AM, Stefan Kruger <stefan.kruger at gmail.com> wrote:
> Hi -
>
> I'm new to R.
>
> In other functional languages I'm familiar with you can often seed a call
> to reduce() with a custom accumulator. Here's an example in Elixir:
>
> map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
> map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
> Enum.count(v), nil) end)
> # %{"one" => 2, "three" => 1, "two" => 2}
>
> In R-terms that's reducing a list of vectors to become a new list mapping
> the names to the vector lengths.
>
> Even in JavaScript, you can do similar things:
>
> list = { one: [1, 1], three: [3], two: [2, 2] };
> var result = Object.keys(list).reduceRight(function (acc, item) {
>   acc[item] = list[item].length;
>   return acc;
> }, {});
> // result == { two: 2, three: 1, one: 2 }
>
> In R, from what I can gather, Reduce() is restricted such that any init
> value you feed it is required to be of the same type as the elements of the
> vector you're reducing -- so I can't build up. So whilst I can do, say
>
>> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
> [1] 111
>
> I can't use Reduce to build up a list, vector or data frame?
>
> What am I missing?
>
> Many thanks for any pointers,
>
> Stefan
>
>
>
> --
> Stefan Kruger <stefan.kruger at gmail.com>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From srivibish at gmail.com  Wed Jul 27 19:29:38 2016
From: srivibish at gmail.com (sri vathsan)
Date: Wed, 27 Jul 2016 22:59:38 +0530
Subject: [R] Reducing execution time
In-Reply-To: <CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
	<CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
	<CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>
	<CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>
Message-ID: <CAGO7QoPDawUd2ztr8aOfNBOvvCN2wG21wGmiBGsf5kJSduHdVw@mail.gmail.com>

Hi,

Thanks for the solution. But I am afraid that after running this code still
it takes more time. It has been an hour and still it is executing. I
understand the delay because each triplet has to compare almost 9000
elements.

Regards,
Sri

On Wed, Jul 27, 2016 at 9:02 PM, Sarah Goslee <sarah.goslee at gmail.com>
wrote:

> Hi,
>
> It's really a good idea to use dput() or some other reproducible way
> to provide data. I had to guess as to what your data looked like.
>
> It appears that order doesn't matter?
>
> Given than, here's one approach:
>
> combs <- structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L, 34L,
> 34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
> "V2", "V3"), class = "data.frame", row.names = c(NA, -5L))
>
> dat <- list(
> c(77,65,34,23,55),
> c(65,23,77,65,55,34),
> c(77,34,65),
> c(55,78,56),
> c(98,23,77,65,34))
>
>
> sapply(seq_len(nrow(combs)), function(i)sum(sapply(dat,
> function(j)all(combs[i,] %in% j))))
>
> On a dataset of comparable time to yours, it takes me under a minute and a
> half.
>
> > combs <- combs[rep(1:nrow(combs), length=100), ]
> > dat <- dat[rep(1:length(dat), length=10000)]
> >
> > dim(combs)
> [1] 100   3
> > length(dat)
> [1] 10000
> >
> > system.time(test <- sapply(seq_len(nrow(combs)),
> function(i)sum(sapply(dat, function(j)all(combs[i,] %in% j)))))
>    user  system elapsed
>  86.380   0.006  86.391
>
>
>
>
> On Wed, Jul 27, 2016 at 10:47 AM, sri vathsan <srivibish at gmail.com> wrote:
> > Hi,
> >
> > Apologizes for the less information.
> >
> > Basically, myCombos is a matrix with 3 variables which is a triplet that
> is
> > a combination of 79 codes. There are around 3lakh combination as such and
> > it looks like below.
> >
> > V1 V2 V3
> > 65 23 77
> > 77 34 65
> > 55 34 23
> > 23 77 34
> > 34 65 55
> >
> > Each triplet will compare in a list (mylist) having 8177 elements which
> > will looks like below.
> >
> > 77,65,34,23,55
> > 65,23,77,65,55,34
> > 77,34,65
> > 55,78,56
> > 98,23,77,65,34
> >
> > Now I want to count the no of occurrence of the triplet in the above
> list.
> > I.e., the triplet 65 23 77 is seen 3 times in the list. So my output
> looks
> > like below
> >
> > V1 V2 V3 Freq
> > 65 23 77  3
> > 77 34 65  4
> > 55 34 23  2
> >
> > I hope, I made it clear this time.
> >
> >
> > On Wed, Jul 27, 2016 at 7:00 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >
> >> Not entirely sure I understand, but match() is already vectorized, so
> you
> >> should be able to lose the supply(). This would speed things up a lot.
> >> Please re-read ?match *carefully* .
> >>
> >> Bert
> >>
> >> On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com> wrote:
> >>
> >> Hi,
> >>
> >> I created list of 3 combination numbers (mycombos, around 3 lakh
> >> combinations) and counting the occurrence of those combination in
> another
> >> list. This comparision list (mylist) is having around 8000 records.I am
> >> using the following code.
> >>
> >> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
> >>   sum(sapply(myList, function(j) {
> >>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
> >>
> >> The above code takes very long time to execute and is there any other
> >> effecting method which will reduce the time.
> >> --
> >>
> >> Regards,
> >> Srivathsan.K
> >>
>



-- 

Regards,
Srivathsan.K
Phone : 9600165206

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Wed Jul 27 19:49:16 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 27 Jul 2016 13:49:16 -0400
Subject: [R] Reducing execution time
In-Reply-To: <CAGO7QoPDawUd2ztr8aOfNBOvvCN2wG21wGmiBGsf5kJSduHdVw@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
	<CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
	<CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>
	<CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>
	<CAGO7QoPDawUd2ztr8aOfNBOvvCN2wG21wGmiBGsf5kJSduHdVw@mail.gmail.com>
Message-ID: <CAM_vju=+q_4i7BnOn3cr2OtJ=H6gyrJw=vrjnLPUu=px=zcB0Q@mail.gmail.com>

You said you had 79 triplets and 8000 records.

When I compared 100 triplets to 10000 records it took 86 seconds.

So obviously there is something you're not telling us about the format
of your data.

If you use dput() to provide actual examples, you will get better
results than if we on Rhelp have to guess. Because we tend to guess in
ways that make the most sense after extensive R experience, and that's
probably not what you have.

Sarah

On Wed, Jul 27, 2016 at 1:29 PM, sri vathsan <srivibish at gmail.com> wrote:
> Hi,
>
> Thanks for the solution. But I am afraid that after running this code still
> it takes more time. It has been an hour and still it is executing. I
> understand the delay because each triplet has to compare almost 9000
> elements.
>
> Regards,
> Sri
>
> On Wed, Jul 27, 2016 at 9:02 PM, Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
>>
>> Hi,
>>
>> It's really a good idea to use dput() or some other reproducible way
>> to provide data. I had to guess as to what your data looked like.
>>
>> It appears that order doesn't matter?
>>
>> Given than, here's one approach:
>>
>> combs <- structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L, 34L,
>> 34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
>> "V2", "V3"), class = "data.frame", row.names = c(NA, -5L))
>>
>> dat <- list(
>> c(77,65,34,23,55),
>> c(65,23,77,65,55,34),
>> c(77,34,65),
>> c(55,78,56),
>> c(98,23,77,65,34))
>>
>>
>> sapply(seq_len(nrow(combs)), function(i)sum(sapply(dat,
>> function(j)all(combs[i,] %in% j))))
>>
>> On a dataset of comparable time to yours, it takes me under a minute and a
>> half.
>>
>> > combs <- combs[rep(1:nrow(combs), length=100), ]
>> > dat <- dat[rep(1:length(dat), length=10000)]
>> >
>> > dim(combs)
>> [1] 100   3
>> > length(dat)
>> [1] 10000
>> >
>> > system.time(test <- sapply(seq_len(nrow(combs)),
>> > function(i)sum(sapply(dat, function(j)all(combs[i,] %in% j)))))
>>    user  system elapsed
>>  86.380   0.006  86.391
>>
>>
>>
>>
>> On Wed, Jul 27, 2016 at 10:47 AM, sri vathsan <srivibish at gmail.com> wrote:
>> > Hi,
>> >
>> > Apologizes for the less information.
>> >
>> > Basically, myCombos is a matrix with 3 variables which is a triplet that
>> > is
>> > a combination of 79 codes. There are around 3lakh combination as such
>> > and
>> > it looks like below.
>> >
>> > V1 V2 V3
>> > 65 23 77
>> > 77 34 65
>> > 55 34 23
>> > 23 77 34
>> > 34 65 55
>> >
>> > Each triplet will compare in a list (mylist) having 8177 elements which
>> > will looks like below.
>> >
>> > 77,65,34,23,55
>> > 65,23,77,65,55,34
>> > 77,34,65
>> > 55,78,56
>> > 98,23,77,65,34
>> >
>> > Now I want to count the no of occurrence of the triplet in the above
>> > list.
>> > I.e., the triplet 65 23 77 is seen 3 times in the list. So my output
>> > looks
>> > like below
>> >
>> > V1 V2 V3 Freq
>> > 65 23 77  3
>> > 77 34 65  4
>> > 55 34 23  2
>> >
>> > I hope, I made it clear this time.
>> >
>> >
>> > On Wed, Jul 27, 2016 at 7:00 PM, Bert Gunter <bgunter.4567 at gmail.com>
>> > wrote:
>> >
>> >> Not entirely sure I understand, but match() is already vectorized, so
>> >> you
>> >> should be able to lose the supply(). This would speed things up a lot.
>> >> Please re-read ?match *carefully* .
>> >>
>> >> Bert
>> >>
>> >> On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com> wrote:
>> >>
>> >> Hi,
>> >>
>> >> I created list of 3 combination numbers (mycombos, around 3 lakh
>> >> combinations) and counting the occurrence of those combination in
>> >> another
>> >> list. This comparision list (mylist) is having around 8000 records.I am
>> >> using the following code.
>> >>
>> >> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
>> >>   sum(sapply(myList, function(j) {
>> >>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
>> >>
>> >> The above code takes very long time to execute and is there any other
>> >> effecting method which will reduce the time.
>> >> --
>> >>
>> >> Regards,
>> >> Srivathsan.K
>> >>
>
>
>
>


From istazahn at gmail.com  Wed Jul 27 20:02:57 2016
From: istazahn at gmail.com (Ista Zahn)
Date: Wed, 27 Jul 2016 14:02:57 -0400
Subject: [R] Have any of you succesfully used VS Code with R? How have
 you set this up?
In-Reply-To: <BE59AEC726BE7847B8478D60A11338C60C7FD5CF@UCUSDC1PWXMR006.us.db.com>
References: <BE59AEC726BE7847B8478D60A11338C60C7FD5CF@UCUSDC1PWXMR006.us.db.com>
Message-ID: <CA+vqiLEjJsmx5B5Enb3VPxWKX7Zj+_ojmLE+MMXCPt+Dr-WUQQ@mail.gmail.com>

Hi Luis,

What features do you want? What did you try? What was missing?

I've not used VSCode, but for Atom check out
https://atom.io/packages/repl, https://atom.io/packages/language-r,
https://atom.io/packages/autocomplete-r, and possibly
https://atom.io/packages/hydrogen

Best,
Ista

On Wed, Jul 27, 2016 at 10:41 AM, Luis Cota <luis.cota at db.com> wrote:
> I've been keen to use either VSCode or Atom and have not had much success. Any suggestions?
>
> Regards,
>
> - Luis
>
> ____________________________________________________
>
>
>
>
>
> ---
> This communication may contain confidential and/or privileged information. If you are not the intended recipient (or have received this communication in error) please notify the sender immediately and destroy this communication. Any unauthorized copying, disclosure or distribution of the material in this communication is strictly forbidden.
>
> Deutsche Bank does not render legal or tax advice, and the information contained in this communication should not be regarded as such.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From chocold12 at gmail.com  Wed Jul 27 20:17:46 2016
From: chocold12 at gmail.com (lily li)
Date: Wed, 27 Jul 2016 12:17:46 -0600
Subject: [R] how to create column names for the matrix
Message-ID: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>

Hi all,

I want to ask that how to create column names for a matrix. For example,
the matrix below, the column names should be: 1-A, 1-B, 1-C, 1-D, 2-A, 2-B,
2-C, 2-D, 3-A, etc. Thanks for your help.

chars = c('A','B','C','D')
matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
k = 0
for(i in seq(1:length(1:5))){
  for(j in seq(1:length(chars))){
    k = k+1
    matrix1[,k] = c(1:100)[k]
  }
}

	[[alternative HTML version deleted]]


From srivibish at gmail.com  Wed Jul 27 20:27:42 2016
From: srivibish at gmail.com (sri vathsan)
Date: Wed, 27 Jul 2016 23:57:42 +0530
Subject: [R] Reducing execution time
In-Reply-To: <CAM_vju=+q_4i7BnOn3cr2OtJ=H6gyrJw=vrjnLPUu=px=zcB0Q@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
	<CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
	<CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>
	<CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>
	<CAGO7QoPDawUd2ztr8aOfNBOvvCN2wG21wGmiBGsf5kJSduHdVw@mail.gmail.com>
	<CAM_vju=+q_4i7BnOn3cr2OtJ=H6gyrJw=vrjnLPUu=px=zcB0Q@mail.gmail.com>
Message-ID: <CAGO7QoPWuAp-pLuALS-r8DDBmdGSYvsn3So_bB9g8PVYr_xhMQ@mail.gmail.com>

Hi,

It is not a just 79 triplets. As I said, there are 79 codes. I am making
triplets out of that 79 codes and matching the triplets in the list.

Please find the dput of the data below.

> dput(head(newd,10))
structure(list(uniq_id = c("1", "2", "3", "4", "5", "6", "7",
"8", "9", "10"), hi = c("11,  22,  84,  85,  108,  111", "18,  84,  85,
87,  122,  134",
"2,  18,  22", "18,  108,  122,  134,  176", "19,  85,  87,  100,  107",
"79,  85,  111", "11,  88,  108", "19,  88,  96", "19,  85,  96",
"19,  100,  103")), .Names = c("uniq_id", "hi"), row.names = c(NA,
-10L), class = c("tbl_df", "tbl", "data.frame"))
>

I am trying to count the frequency of the triplets in the above data using
the below code.

# split column into a list
myList <- strsplit(newd$hi, split=",")
# get all pairwise combinations
myCombos <- t(combn(unique(unlist(myList)), 3))
# count the instances where the pair is present
myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
  sum(sapply(myList, function(j) {
    sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
#final matrix
final <- cbind(matrix(as.integer(myCombos), nrow(myCombos)), myCounts)

I hope I made my point clear. Please let me know if I miss anything.

Regards,
Sri




On Wed, Jul 27, 2016 at 11:19 PM, Sarah Goslee <sarah.goslee at gmail.com>
wrote:

> You said you had 79 triplets and 8000 records.
>
> When I compared 100 triplets to 10000 records it took 86 seconds.
>
> So obviously there is something you're not telling us about the format
> of your data.
>
> If you use dput() to provide actual examples, you will get better
> results than if we on Rhelp have to guess. Because we tend to guess in
> ways that make the most sense after extensive R experience, and that's
> probably not what you have.
>
> Sarah
>
> On Wed, Jul 27, 2016 at 1:29 PM, sri vathsan <srivibish at gmail.com> wrote:
> > Hi,
> >
> > Thanks for the solution. But I am afraid that after running this code
> still
> > it takes more time. It has been an hour and still it is executing. I
> > understand the delay because each triplet has to compare almost 9000
> > elements.
> >
> > Regards,
> > Sri
> >
> > On Wed, Jul 27, 2016 at 9:02 PM, Sarah Goslee <sarah.goslee at gmail.com>
> > wrote:
> >>
> >> Hi,
> >>
> >> It's really a good idea to use dput() or some other reproducible way
> >> to provide data. I had to guess as to what your data looked like.
> >>
> >> It appears that order doesn't matter?
> >>
> >> Given than, here's one approach:
> >>
> >> combs <- structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L,
> 34L,
> >> 34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
> >> "V2", "V3"), class = "data.frame", row.names = c(NA, -5L))
> >>
> >> dat <- list(
> >> c(77,65,34,23,55),
> >> c(65,23,77,65,55,34),
> >> c(77,34,65),
> >> c(55,78,56),
> >> c(98,23,77,65,34))
> >>
> >>
> >> sapply(seq_len(nrow(combs)), function(i)sum(sapply(dat,
> >> function(j)all(combs[i,] %in% j))))
> >>
> >> On a dataset of comparable time to yours, it takes me under a minute
> and a
> >> half.
> >>
> >> > combs <- combs[rep(1:nrow(combs), length=100), ]
> >> > dat <- dat[rep(1:length(dat), length=10000)]
> >> >
> >> > dim(combs)
> >> [1] 100   3
> >> > length(dat)
> >> [1] 10000
> >> >
> >> > system.time(test <- sapply(seq_len(nrow(combs)),
> >> > function(i)sum(sapply(dat, function(j)all(combs[i,] %in% j)))))
> >>    user  system elapsed
> >>  86.380   0.006  86.391
> >>
> >>
> >>
> >>
> >> On Wed, Jul 27, 2016 at 10:47 AM, sri vathsan <srivibish at gmail.com>
> wrote:
> >> > Hi,
> >> >
> >> > Apologizes for the less information.
> >> >
> >> > Basically, myCombos is a matrix with 3 variables which is a triplet
> that
> >> > is
> >> > a combination of 79 codes. There are around 3lakh combination as such
> >> > and
> >> > it looks like below.
> >> >
> >> > V1 V2 V3
> >> > 65 23 77
> >> > 77 34 65
> >> > 55 34 23
> >> > 23 77 34
> >> > 34 65 55
> >> >
> >> > Each triplet will compare in a list (mylist) having 8177 elements
> which
> >> > will looks like below.
> >> >
> >> > 77,65,34,23,55
> >> > 65,23,77,65,55,34
> >> > 77,34,65
> >> > 55,78,56
> >> > 98,23,77,65,34
> >> >
> >> > Now I want to count the no of occurrence of the triplet in the above
> >> > list.
> >> > I.e., the triplet 65 23 77 is seen 3 times in the list. So my output
> >> > looks
> >> > like below
> >> >
> >> > V1 V2 V3 Freq
> >> > 65 23 77  3
> >> > 77 34 65  4
> >> > 55 34 23  2
> >> >
> >> > I hope, I made it clear this time.
> >> >
> >> >
> >> > On Wed, Jul 27, 2016 at 7:00 PM, Bert Gunter <bgunter.4567 at gmail.com>
> >> > wrote:
> >> >
> >> >> Not entirely sure I understand, but match() is already vectorized, so
> >> >> you
> >> >> should be able to lose the supply(). This would speed things up a
> lot.
> >> >> Please re-read ?match *carefully* .
> >> >>
> >> >> Bert
> >> >>
> >> >> On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com> wrote:
> >> >>
> >> >> Hi,
> >> >>
> >> >> I created list of 3 combination numbers (mycombos, around 3 lakh
> >> >> combinations) and counting the occurrence of those combination in
> >> >> another
> >> >> list. This comparision list (mylist) is having around 8000 records.I
> am
> >> >> using the following code.
> >> >>
> >> >> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
> >> >>   sum(sapply(myList, function(j) {
> >> >>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
> >> >>
> >> >> The above code takes very long time to execute and is there any other
> >> >> effecting method which will reduce the time.
> >> >> --
> >> >>
> >> >> Regards,
> >> >> Srivathsan.K
> >> >>
> >
> >
> >
> >
>



-- 

Regards,
Srivathsan.K
Phone : 9600165206

	[[alternative HTML version deleted]]


From hpages at fredhutch.org  Wed Jul 27 20:40:14 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 27 Jul 2016 11:40:14 -0700
Subject: [R] how to create column names for the matrix
In-Reply-To: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
References: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
Message-ID: <2d425ac2-5b3f-e0e4-cd85-9f2f4967b6a4@fredhutch.org>

Hi,

On 07/27/2016 11:17 AM, lily li wrote:
> Hi all,
>
> I want to ask that how to create column names for a matrix. For example,
> the matrix below, the column names should be: 1-A, 1-B, 1-C, 1-D, 2-A, 2-B,
> 2-C, 2-D, 3-A, etc. Thanks for your help.
>
> chars = c('A','B','C','D')
> matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
> k = 0
> for(i in seq(1:length(1:5))){
>   for(j in seq(1:length(chars))){
>     k = k+1
>     matrix1[,k] = c(1:100)[k]
>   }
> }

Also how could you possibly use such level of code obfuscation to
perform such simple initialization of your matrix?

My 1st advice would be that you slow down and take the time to compare
seq(1:length(1:5)) with 1:length(1:5) with 1:5. It will be a great
learning experience!

As for initializing your matrix, what about doing

   ncol <- 5 * length(chars)
   matrix1 <- matrix(seq_len(ncol), nrow=100, ncol=ncol, byrow=TRUE)

instead?

Then set the colnames with:

   colnames(matrix1) <- paste(rep(1:5, length(chars)), chars, sep="-")

Cheers,
H.

>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From hpages at fredhutch.org  Wed Jul 27 20:42:23 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 27 Jul 2016 11:42:23 -0700
Subject: [R] how to create column names for the matrix
In-Reply-To: <2d425ac2-5b3f-e0e4-cd85-9f2f4967b6a4@fredhutch.org>
References: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
	<2d425ac2-5b3f-e0e4-cd85-9f2f4967b6a4@fredhutch.org>
Message-ID: <8724ee03-5baf-87d0-1a6f-4e6063fd5b80@fredhutch.org>

On 07/27/2016 11:40 AM, Herv? Pag?s wrote:
> Hi,
>
> On 07/27/2016 11:17 AM, lily li wrote:
>> Hi all,
>>
>> I want to ask that how to create column names for a matrix. For example,
>> the matrix below, the column names should be: 1-A, 1-B, 1-C, 1-D, 2-A,
>> 2-B,
>> 2-C, 2-D, 3-A, etc. Thanks for your help.
>>
>> chars = c('A','B','C','D')
>> matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
>> k = 0
>> for(i in seq(1:length(1:5))){
>>   for(j in seq(1:length(chars))){
>>     k = k+1
>>     matrix1[,k] = c(1:100)[k]
>>   }
>> }
>
> Also how could you possibly use such level of code obfuscation to
> perform such simple initialization of your matrix?
>
> My 1st advice would be that you slow down and take the time to compare
> seq(1:length(1:5)) with 1:length(1:5) with 1:5. It will be a great
> learning experience!
>
> As for initializing your matrix, what about doing
>
>   ncol <- 5 * length(chars)
>   matrix1 <- matrix(seq_len(ncol), nrow=100, ncol=ncol, byrow=TRUE)
>
> instead?
>
> Then set the colnames with:
>
>   colnames(matrix1) <- paste(rep(1:5, length(chars)), chars, sep="-")

or maybe

   colnames(matrix1) <- paste(rep(1:5, each=length(chars)), chars, sep="-")

is what you're after.

H.

>
> Cheers,
> H.
>
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From chocold12 at gmail.com  Wed Jul 27 20:45:06 2016
From: chocold12 at gmail.com (lily li)
Date: Wed, 27 Jul 2016 12:45:06 -0600
Subject: [R] how to create column names for the matrix
In-Reply-To: <8724ee03-5baf-87d0-1a6f-4e6063fd5b80@fredhutch.org>
References: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
	<2d425ac2-5b3f-e0e4-cd85-9f2f4967b6a4@fredhutch.org>
	<8724ee03-5baf-87d0-1a6f-4e6063fd5b80@fredhutch.org>
Message-ID: <CAN5afy_mUVWdF71kUiHMDrp78AXZagOvxfgBBs6z=p+w1h+7hw@mail.gmail.com>

Thanks. I shorten a more complex matrix to this example, but use the
original structure of the code. The original matrix has all characters
instead of 1:5.


On Wed, Jul 27, 2016 at 12:42 PM, Herv? Pag?s <hpages at fredhutch.org> wrote:

> On 07/27/2016 11:40 AM, Herv? Pag?s wrote:
>
>> Hi,
>>
>> On 07/27/2016 11:17 AM, lily li wrote:
>>
>>> Hi all,
>>>
>>> I want to ask that how to create column names for a matrix. For example,
>>> the matrix below, the column names should be: 1-A, 1-B, 1-C, 1-D, 2-A,
>>> 2-B,
>>> 2-C, 2-D, 3-A, etc. Thanks for your help.
>>>
>>> chars = c('A','B','C','D')
>>> matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
>>> k = 0
>>> for(i in seq(1:length(1:5))){
>>>   for(j in seq(1:length(chars))){
>>>     k = k+1
>>>     matrix1[,k] = c(1:100)[k]
>>>   }
>>> }
>>>
>>
>> Also how could you possibly use such level of code obfuscation to
>> perform such simple initialization of your matrix?
>>
>> My 1st advice would be that you slow down and take the time to compare
>> seq(1:length(1:5)) with 1:length(1:5) with 1:5. It will be a great
>> learning experience!
>>
>> As for initializing your matrix, what about doing
>>
>>   ncol <- 5 * length(chars)
>>   matrix1 <- matrix(seq_len(ncol), nrow=100, ncol=ncol, byrow=TRUE)
>>
>> instead?
>>
>> Then set the colnames with:
>>
>>   colnames(matrix1) <- paste(rep(1:5, length(chars)), chars, sep="-")
>>
>
> or maybe
>
>   colnames(matrix1) <- paste(rep(1:5, each=length(chars)), chars, sep="-")
>
> is what you're after.
>
> H.
>
>
>
>> Cheers,
>> H.
>>
>>
>>>     [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
> --
> Herv? Pag?s
>
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
>
> E-mail: hpages at fredhutch.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Wed Jul 27 20:49:51 2016
From: chocold12 at gmail.com (lily li)
Date: Wed, 27 Jul 2016 12:49:51 -0600
Subject: [R] how to create column names for the matrix
In-Reply-To: <2d425ac2-5b3f-e0e4-cd85-9f2f4967b6a4@fredhutch.org>
References: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
	<2d425ac2-5b3f-e0e4-cd85-9f2f4967b6a4@fredhutch.org>
Message-ID: <CAN5afy-eC+ZRjcosZ7NDqc_fVpspNi3B3fzQ0c+CZC=3MPt-Xw@mail.gmail.com>

If replace 1:5 to char2 = c('east','west','south','north','central'), how
to put on column names with the original structure? Thanks again.


On Wed, Jul 27, 2016 at 12:40 PM, Herv? Pag?s <hpages at fredhutch.org> wrote:

> Hi,
>
>
> On 07/27/2016 11:17 AM, lily li wrote:
>
>> Hi all,
>>
>> I want to ask that how to create column names for a matrix. For example,
>> the matrix below, the column names should be: 1-A, 1-B, 1-C, 1-D, 2-A,
>> 2-B,
>> 2-C, 2-D, 3-A, etc. Thanks for your help.
>>
>> chars = c('A','B','C','D')
>> matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
>> k = 0
>> for(i in seq(1:length(1:5))){
>>   for(j in seq(1:length(chars))){
>>     k = k+1
>>     matrix1[,k] = c(1:100)[k]
>>   }
>> }
>>
>
> Also how could you possibly use such level of code obfuscation to
> perform such simple initialization of your matrix?
>
> My 1st advice would be that you slow down and take the time to compare
> seq(1:length(1:5)) with 1:length(1:5) with 1:5. It will be a great
> learning experience!
>
> As for initializing your matrix, what about doing
>
>   ncol <- 5 * length(chars)
>   matrix1 <- matrix(seq_len(ncol), nrow=100, ncol=ncol, byrow=TRUE)
>
> instead?
>
> Then set the colnames with:
>
>   colnames(matrix1) <- paste(rep(1:5, length(chars)), chars, sep="-")
>
> Cheers,
> H.
>
>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> --
> Herv? Pag?s
>
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
>
> E-mail: hpages at fredhutch.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
>

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Wed Jul 27 20:50:01 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Wed, 27 Jul 2016 19:50:01 +0100
Subject: [R] how to create column names for the matrix
In-Reply-To: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
Message-ID: <20160727195001.Horde.084hXpMISeYpV7iC9PkEhUz@mail.sapo.pt>

Hello,

Try

chars = c('A','B','C','D')
matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
k = 0
for(i in 1:length(1:5)){? # or more simply just for(i in 1:5)
? for(j in 1:length(chars)){
??? k = k+1
??? matrix1[,k] = k
? }
}
matrix1
tmp <- expand.grid(chars, 1:5, stringsAsFactors = FALSE)
nms <- paste(tmp[[2]], tmp[[1]], sep = "-")
rm(tmp)
colnames(matrix1) <- nms

head(matrix1)

Hope this helps,

Rui Barradas
?

Citando lily li <chocold12 at gmail.com>:

> Hi all,
>
> I want to ask that how to create column names for a matrix. For example,
> the matrix below, the column names should be: 1-A, 1-B, 1-C, 1-D, 2-A, 2-B,
> 2-C, 2-D, 3-A, etc. Thanks for your help.
>
> chars = c('A','B','C','D')
> matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
> k = 0
> for(i in seq(1:length(1:5))){
> for(j in seq(1:length(chars))){
> ? ?k = k+1
> ? ?matrix1[,k] = c(1:100)[k]
> }
> }
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From hpages at fredhutch.org  Wed Jul 27 20:53:17 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 27 Jul 2016 11:53:17 -0700
Subject: [R] how to create column names for the matrix
In-Reply-To: <CAN5afy_mUVWdF71kUiHMDrp78AXZagOvxfgBBs6z=p+w1h+7hw@mail.gmail.com>
References: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
	<2d425ac2-5b3f-e0e4-cd85-9f2f4967b6a4@fredhutch.org>
	<8724ee03-5baf-87d0-1a6f-4e6063fd5b80@fredhutch.org>
	<CAN5afy_mUVWdF71kUiHMDrp78AXZagOvxfgBBs6z=p+w1h+7hw@mail.gmail.com>
Message-ID: <2696b6e3-4407-c2d3-fd78-42fe7337bbcb@fredhutch.org>

I see. But please understand that initializing the values is not the
same as setting the colnames. How providing this almost-impossible-
to-read initialization code helps with respect to your question which
is about setting the colnames?

I know people often asked you to show the code in your previous
questions on this site. They're right: showing the code helps.
But only the code that is relevant to your question. Code that is
not relevant to your question is only distracting and confusing.

H.

On 07/27/2016 11:45 AM, lily li wrote:
> Thanks. I shorten a more complex matrix to this example, but use the
> original structure of the code. The original matrix has all characters
> instead of 1:5.
>
>
> On Wed, Jul 27, 2016 at 12:42 PM, Herv? Pag?s <hpages at fredhutch.org
> <mailto:hpages at fredhutch.org>> wrote:
>
>     On 07/27/2016 11:40 AM, Herv? Pag?s wrote:
>
>         Hi,
>
>         On 07/27/2016 11:17 AM, lily li wrote:
>
>             Hi all,
>
>             I want to ask that how to create column names for a matrix.
>             For example,
>             the matrix below, the column names should be: 1-A, 1-B, 1-C,
>             1-D, 2-A,
>             2-B,
>             2-C, 2-D, 3-A, etc. Thanks for your help.
>
>             chars = c('A','B','C','D')
>             matrix1 = matrix(nrow = length(1:100), ncol =
>             length(1:5)*length(chars))
>             k = 0
>             for(i in seq(1:length(1:5))){
>               for(j in seq(1:length(chars))){
>                 k = k+1
>                 matrix1[,k] = c(1:100)[k]
>               }
>             }
>
>
>         Also how could you possibly use such level of code obfuscation to
>         perform such simple initialization of your matrix?
>
>         My 1st advice would be that you slow down and take the time to
>         compare
>         seq(1:length(1:5)) with 1:length(1:5) with 1:5. It will be a great
>         learning experience!
>
>         As for initializing your matrix, what about doing
>
>           ncol <- 5 * length(chars)
>           matrix1 <- matrix(seq_len(ncol), nrow=100, ncol=ncol, byrow=TRUE)
>
>         instead?
>
>         Then set the colnames with:
>
>           colnames(matrix1) <- paste(rep(1:5, length(chars)), chars,
>         sep="-")
>
>
>     or maybe
>
>       colnames(matrix1) <- paste(rep(1:5, each=length(chars)), chars,
>     sep="-")
>
>     is what you're after.
>
>     H.
>
>
>
>         Cheers,
>         H.
>
>
>                 [[alternative HTML version deleted]]
>
>             ______________________________________________
>             R-help at r-project.org <mailto:R-help at r-project.org> mailing
>             list -- To UNSUBSCRIBE and more, see
>             https://stat.ethz.ch/mailman/listinfo/r-help
>             PLEASE do read the posting guide
>             http://www.R-project.org/posting-guide.html
>             and provide commented, minimal, self-contained, reproducible
>             code.
>
>
>
>     --
>     Herv? Pag?s
>
>     Program in Computational Biology
>     Division of Public Health Sciences
>     Fred Hutchinson Cancer Research Center
>     1100 Fairview Ave. N, M1-B514
>     P.O. Box 19024
>     Seattle, WA 98109-1024
>
>     E-mail: hpages at fredhutch.org <mailto:hpages at fredhutch.org>
>     Phone:  (206) 667-5791 <tel:%28206%29%20667-5791>
>     Fax:    (206) 667-1319 <tel:%28206%29%20667-1319>
>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From hpages at fredhutch.org  Wed Jul 27 20:58:32 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 27 Jul 2016 11:58:32 -0700
Subject: [R] how to create column names for the matrix
In-Reply-To: <CAN5afy-eC+ZRjcosZ7NDqc_fVpspNi3B3fzQ0c+CZC=3MPt-Xw@mail.gmail.com>
References: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
	<2d425ac2-5b3f-e0e4-cd85-9f2f4967b6a4@fredhutch.org>
	<CAN5afy-eC+ZRjcosZ7NDqc_fVpspNi3B3fzQ0c+CZC=3MPt-Xw@mail.gmail.com>
Message-ID: <9f103b04-13e8-1f36-6f5e-e29b1c907068@fredhutch.org>

On 07/27/2016 11:49 AM, lily li wrote:
> If replace 1:5 to char2 = c('east','west','south','north','central'),
> how to put on column names with the original structure? Thanks again.

Have you tried to actually "replace 1:5 with char2", literally?

You need to try things before you ask. That's the only way to learn!

H.

>
>
> On Wed, Jul 27, 2016 at 12:40 PM, Herv? Pag?s <hpages at fredhutch.org
> <mailto:hpages at fredhutch.org>> wrote:
>
>     Hi,
>
>
>     On 07/27/2016 11:17 AM, lily li wrote:
>
>         Hi all,
>
>         I want to ask that how to create column names for a matrix. For
>         example,
>         the matrix below, the column names should be: 1-A, 1-B, 1-C,
>         1-D, 2-A, 2-B,
>         2-C, 2-D, 3-A, etc. Thanks for your help.
>
>         chars = c('A','B','C','D')
>         matrix1 = matrix(nrow = length(1:100), ncol =
>         length(1:5)*length(chars))
>         k = 0
>         for(i in seq(1:length(1:5))){
>           for(j in seq(1:length(chars))){
>             k = k+1
>             matrix1[,k] = c(1:100)[k]
>           }
>         }
>
>
>     Also how could you possibly use such level of code obfuscation to
>     perform such simple initialization of your matrix?
>
>     My 1st advice would be that you slow down and take the time to compare
>     seq(1:length(1:5)) with 1:length(1:5) with 1:5. It will be a great
>     learning experience!
>
>     As for initializing your matrix, what about doing
>
>       ncol <- 5 * length(chars)
>       matrix1 <- matrix(seq_len(ncol), nrow=100, ncol=ncol, byrow=TRUE)
>
>     instead?
>
>     Then set the colnames with:
>
>       colnames(matrix1) <- paste(rep(1:5, length(chars)), chars, sep="-")
>
>     Cheers,
>     H.
>
>
>                 [[alternative HTML version deleted]]
>
>         ______________________________________________
>         R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>         -- To UNSUBSCRIBE and more, see
>         https://stat.ethz.ch/mailman/listinfo/r-help
>         PLEASE do read the posting guide
>         http://www.R-project.org/posting-guide.html
>         and provide commented, minimal, self-contained, reproducible code.
>
>
>     --
>     Herv? Pag?s
>
>     Program in Computational Biology
>     Division of Public Health Sciences
>     Fred Hutchinson Cancer Research Center
>     1100 Fairview Ave. N, M1-B514
>     P.O. Box 19024
>     Seattle, WA 98109-1024
>
>     E-mail: hpages at fredhutch.org <mailto:hpages at fredhutch.org>
>     Phone:  (206) 667-5791 <tel:%28206%29%20667-5791>
>     Fax:    (206) 667-1319 <tel:%28206%29%20667-1319>
>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From jdnewmil at dcn.davis.ca.us  Wed Jul 27 21:02:46 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 27 Jul 2016 12:02:46 -0700
Subject: [R] Aggregate matrix in a 2 by 2 manor
In-Reply-To: <f2247c74794a41ad8a70f85d2c85df92@exch-2p-mbx-t2.ads.tamu.edu>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
	<CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
	<E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>
	<f2247c74794a41ad8a70f85d2c85df92@exch-2p-mbx-t2.ads.tamu.edu>
Message-ID: <ED550726-6ACD-487A-959A-2DB8706B76F4@dcn.davis.ca.us>

An alternative (more compact, not necessarily faster, because apply is still a for loop inside):

f <- function( m, nx, ny ) {
  # redefine the dimensions of my
  a <- array( m
             , dim = c( ny
                    , nrow( m ) %/% ny
                    , ncol( m ) %/% nx )
            )
  # apply mean over dim 1
  apply( a, c( 2, 3 ), FUN=mean )
}
f( tst, nx, ny )
                    
-- 
Sent from my phone. Please excuse my brevity.

On July 27, 2016 9:08:32 AM PDT, David L Carlson <dcarlson at tamu.edu> wrote:
>This should be faster. It uses apply() across the blocks. 
>
>> ilon <- seq(1,8,nx)
>> ilat <- seq(1,4,ny)
>> cells <- as.matrix(expand.grid(ilat, ilon))
>> blocks <- apply(cells, 1, function(x) tst[x[1]:(x[1]+1),
>x[2]:(x[2]+1)])
>> block.means <- colMeans(blocks)
>> tst_2x2 <- matrix(block.means, 2, 4)
>> tst_2x2
>     [,1] [,2] [,3] [,4]
>[1,]  3.5 11.5 19.5 27.5
>[2,]  5.5 13.5 21.5 29.5
>
>-------------------------------------
>David L Carlson
>Department of Anthropology
>Texas A&M University
>College Station, TX 77840-4352
>
>
>
>-----Original Message-----
>From: R-help [mailto:r-help-bounces at r-poject.org] On Behalf Of Anthoni,
>Peter (IMK)
>Sent: Wednesday, July 27, 2016 6:14 AM
>To: r-help at r-project.org
>Subject: [R] Aggregate matrix in a 2 by 2 manor
>
>Hi all,
>
>I need to aggregate some matrix data (1440x720) to a lower dimension
>(720x360) for lots of years and variables
>
>I can do double for loop, but that will be slow. Anybody know a quicker
>way?
>
>here an example with a smaller matrix size:
>
>tst=matrix(1:(8*4),ncol=8,nrow=4)
>tst_2x2=matrix(NA,ncol=4,nrow=2)
>nx=2
>ny=2
>for(ilon in seq(1,8,nx)) {
>  for (ilat in seq(1,4,ny)) {
>    ilon_2x2=1+(ilon-1)/nx
>    ilat_2x2=1+(ilat-1)/ny
>    tst_2x2[ilat_2x2,ilon_2x2] = mean(tst[ilat+0:1,ilon+0:1])
>  }
>}
>
>tst
>tst_2x2
>
>> tst
>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
>[1,]    1    5    9   13   17   21   25   29
>[2,]    2    6   10   14   18   22   26   30
>[3,]    3    7   11   15   19   23   27   31
>[4,]    4    8   12   16   20   24   28   32
>
>> tst_2x2
>     [,1] [,2] [,3] [,4]
>[1,]  3.5 11.5 19.5 27.5
>[2,]  5.5 13.5 21.5 29.5
>
>
>I though a cast to 3d-array might do the trick and apply over the new
>dimension, but that does not work, since it casts the data along the
>row.
>> matrix(apply(array(tst,dim=c(nx,ny,8)),3,mean),nrow=nrow(tst)/ny)
>     [,1] [,2] [,3] [,4]
>[1,]  2.5 10.5 18.5 26.5
>[2,]  6.5 14.5 22.5 30.5
>
>
>cheers
>Peter
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Wed Jul 27 21:11:19 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 27 Jul 2016 12:11:19 -0700
Subject: [R] Stack dataframes into a matrix
In-Reply-To: <CALv--da_4moaJsyi+y=Y9Ly3RJbw2-a-PkRASSktt0ugq_=RJw@mail.gmail.com>
References: <CALv--daQYH4C90=C=1YPn-W-XJrb-vQHFW5BSqu=MtpqqCMe3g@mail.gmail.com>
	<0C584896-C750-4795-A6DC-E57CC14FDAFA@comcast.net>
	<CALv--da_4moaJsyi+y=Y9Ly3RJbw2-a-PkRASSktt0ugq_=RJw@mail.gmail.com>
Message-ID: <17DE0FC6-603E-4C67-8CDA-CEB19D803789@comcast.net>


> On Jul 27, 2016, at 8:56 AM, Anamika Chaudhuri <canamika at gmail.com> wrote:
> 
> 
> Y<-matrix(1:40,ncol=2)
> Y1<-Y/60 # estimates of p
> 
> #print(Y1)
> 
> sigma2<- matrix(c(var(Y1[,1]),cov(Y1[,1],Y1[,2]),cov(Y1[,1],Y1[,2]),var(Y1[,2])),2,2)
> #print(sigma2)
> 
> rho<-sigma2[1,2]/sqrt(sigma2[1,1]*sigma2[2,2])
> #rho
> 
> mean(Y1[,1])
> mean(Y1[,2])
> 
> #within<-matrix(data=0,nrow=20,ncol=1)
> 
> for (rate3 in 1:20){
> 
> rate<-Y1[i,]
> #print(rate)
> 
> 
> rate1<-rate/(1-rate)
> 
> 
> rate2<-log(rate1)
> 
> 
> Sigma11<-(1/(rate[1]*(1-rate[1]))^2)*sigma2[1,1]
> Sigma22<-(1/(rate[2]*(1-rate[2]))^2)*sigma2[2,2]
> Sigma12<-(1/((rate[1]*(1-rate[1]))*(rate[2]*(1-rate[2]))))*sigma2[1,2]
> 
> Sigma2<-matrix(c(Sigma11,Sigma12,Sigma12,Sigma22),2,2)
> #print(Sigma2)
> 
> rate3<-mvrnorm(1000, mu=c(rate2[1],rate2[2]), Sigma2)
> 
> #print(rate3)
> 
> x<-exp(rate3[,1])/(1+exp(rate3[,1]))
> y<-exp(rate3[,2])/(1+exp(rate3[,2]))
> print(x) # Need to be able to stack the x's to produce one matrix
> }

Not reproducible. 
No loading of package for mvnorm (probably MASS) and suspect that you meant the loop to be over 'i' rather than 'rate3' since Y1[i,] throws an error. If my guesses are correct then perhaps:

require(MASS)
X <- matrix(NA, nrow=20, ncol=1000)  # predimentsion with NA's
Y<-matrix(1:40,ncol=2)
Y1<-Y/60 
sigma2<- matrix(c(var(Y1[,1]),cov(Y1[,1],Y1[,2]),cov(Y1[,1],Y1[,2]),var(Y1[,2])),2,2)
rho<-sigma2[1,2]/sqrt(sigma2[1,1]*sigma2[2,2])
mean(Y1[,1])
mean(Y1[,2])
for (rate3 in 1:20){

rate<-Y1[rate3,]
rate1<-rate/(1-rate)
rate2<-log(rate1)
Sigma11<-(1/(rate[1]*(1-rate[1]))^2)*sigma2[1,1]
Sigma22<-(1/(rate[2]*(1-rate[2]))^2)*sigma2[2,2]
Sigma12<-(1/((rate[1]*(1-rate[1]))*(rate[2]*(1-rate[2]))))*sigma2[1,2]

Sigma2<-matrix(c(Sigma11,Sigma12,Sigma12,Sigma22),2,2)
rate3<-mvrnorm(1000, mu=c(rate2[1],rate2[2]), Sigma2)
x<-exp(rate3[,1])/(1+exp(rate3[,1])) 

# this is not really "stacking" 
# because your code was overwriting x values each time through the loop.
 X[ rate3 , ] <- x   # save to single row at a time

y<-exp(rate3[,2])/(1+exp(rate3[,2]))  # not sure what you wanted to do with these
# print(x)    # and DO NOT PRINT 1000 item vectors to screen!
}


David Winsemius
Alameda, CA, USA


From roundsjeremiah at gmail.com  Wed Jul 27 21:17:22 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Wed, 27 Jul 2016 12:17:22 -0700
Subject: [R] Reducing execution time
In-Reply-To: <CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
	<CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
	<CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>
	<CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>
Message-ID: <CAOjnRsYwLLRGSir3vi6ni5gr9GeLAO7dp1hgZOEaQNvqUgg9hQ@mail.gmail.com>

If I understood the request this is the same programming  task as counting
words in a document and counting character sequences in a string or
matching bytes in byte arrays (though you don't want to go down that far)
 You can do something like what follows.  There are also vectorized greps
in stringr.

combs = structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L, 34L,
34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
"V2", "V3"), class = "data.frame", row.names = c(NA, -5L))

dat = list(
c(77,65,34,23,55, 65,23,77, 44),
c(65,23,77,65,55,34, 77, 34,65, 10),
c(77,34,65),
c(55,78,56),
c(98,23,77,65,34, 65, 23, 77, 34))


words = unlist(apply(combs, 1 , function(d) paste(as.character(d),
collapse=" ")))
dat = lapply(dat, function(d) paste( as.character(d), collapse= " "))
doc = paste(dat, collapse = " ## ") # just some arbitrary separator
character that isn't in your words
counts = sapply(words, function(w) length(grep(w, doc)))
names(counts) = words
counts
cbind(combs, data.frame(N = counts))



On Wed, Jul 27, 2016 at 8:32 AM, Sarah Goslee <sarah.goslee at gmail.com>
wrote:

> Hi,
>
> It's really a good idea to use dput() or some other reproducible way
> to provide data. I had to guess as to what your data looked like.
>
> It appears that order doesn't matter?
>
> Given than, here's one approach:
>
> combs <- structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L, 34L,
> 34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
> "V2", "V3"), class = "data.frame", row.names = c(NA, -5L))
>
> dat <- list(
> c(77,65,34,23,55),
> c(65,23,77,65,55,34),
> c(77,34,65),
> c(55,78,56),
> c(98,23,77,65,34))
>
>
> sapply(seq_len(nrow(combs)), function(i)sum(sapply(dat,
> function(j)all(combs[i,] %in% j))))
>
> On a dataset of comparable time to yours, it takes me under a minute and a
> half.
>
> > combs <- combs[rep(1:nrow(combs), length=100), ]
> > dat <- dat[rep(1:length(dat), length=10000)]
> >
> > dim(combs)
> [1] 100   3
> > length(dat)
> [1] 10000
> >
> > system.time(test <- sapply(seq_len(nrow(combs)),
> function(i)sum(sapply(dat, function(j)all(combs[i,] %in% j)))))
>    user  system elapsed
>  86.380   0.006  86.391
>
>
>
>
> On Wed, Jul 27, 2016 at 10:47 AM, sri vathsan <srivibish at gmail.com> wrote:
> > Hi,
> >
> > Apologizes for the less information.
> >
> > Basically, myCombos is a matrix with 3 variables which is a triplet that
> is
> > a combination of 79 codes. There are around 3lakh combination as such and
> > it looks like below.
> >
> > V1 V2 V3
> > 65 23 77
> > 77 34 65
> > 55 34 23
> > 23 77 34
> > 34 65 55
> >
> > Each triplet will compare in a list (mylist) having 8177 elements which
> > will looks like below.
> >
> > 77,65,34,23,55
> > 65,23,77,65,55,34
> > 77,34,65
> > 55,78,56
> > 98,23,77,65,34
> >
> > Now I want to count the no of occurrence of the triplet in the above
> list.
> > I.e., the triplet 65 23 77 is seen 3 times in the list. So my output
> looks
> > like below
> >
> > V1 V2 V3 Freq
> > 65 23 77  3
> > 77 34 65  4
> > 55 34 23  2
> >
> > I hope, I made it clear this time.
> >
> >
> > On Wed, Jul 27, 2016 at 7:00 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >
> >> Not entirely sure I understand, but match() is already vectorized, so
> you
> >> should be able to lose the supply(). This would speed things up a lot.
> >> Please re-read ?match *carefully* .
> >>
> >> Bert
> >>
> >> On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com> wrote:
> >>
> >> Hi,
> >>
> >> I created list of 3 combination numbers (mycombos, around 3 lakh
> >> combinations) and counting the occurrence of those combination in
> another
> >> list. This comparision list (mylist) is having around 8000 records.I am
> >> using the following code.
> >>
> >> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
> >>   sum(sapply(myList, function(j) {
> >>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
> >>
> >> The above code takes very long time to execute and is there any other
> >> effecting method which will reduce the time.
> >> --
> >>
> >> Regards,
> >> Srivathsan.K
> >>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From roundsjeremiah at gmail.com  Wed Jul 27 21:26:35 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Wed, 27 Jul 2016 12:26:35 -0700
Subject: [R] Reducing execution time
In-Reply-To: <CAGO7QoPWuAp-pLuALS-r8DDBmdGSYvsn3So_bB9g8PVYr_xhMQ@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
	<CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
	<CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>
	<CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>
	<CAGO7QoPDawUd2ztr8aOfNBOvvCN2wG21wGmiBGsf5kJSduHdVw@mail.gmail.com>
	<CAM_vju=+q_4i7BnOn3cr2OtJ=H6gyrJw=vrjnLPUu=px=zcB0Q@mail.gmail.com>
	<CAGO7QoPWuAp-pLuALS-r8DDBmdGSYvsn3So_bB9g8PVYr_xhMQ@mail.gmail.com>
Message-ID: <CAOjnRsYifP=D_aeYhZdMK6vo0b-P6r1Xf7x4_vSp0KTxEbJ5BA@mail.gmail.com>

Correction to my code. I created a "doc" variable because I was thinking of
doing something faster, but I never did the change.  grep needed to work on
the original source "dat" to be used for counting.

 Fixed:

combs = structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L, 34L,
34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
"V2", "V3"), class = "data.frame", row.names = c(NA, -5L))

dat = list(
c(77,65,34,23,55, 65,23,77, 44),
c(65,23,77,65,55,34, 77, 34,65, 10),
c(77,34,65),
c(55,78,56),
c(98,23,77,65,34, 65, 23, 77, 34))


words = unlist(apply(combs, 1 , function(d) paste(as.character(d),
collapse=" ")))
dat = lapply(dat, function(d) paste( as.character(d), collapse= " "))
#doc = paste(dat, collapse = " ## ") # just some arbitrary separator
character that isn't in your words
counts = sapply(words, function(w) length(grep(w, dat)))
names(counts) = words
counts
cbind(combs, data.frame(N = counts))


On Wed, Jul 27, 2016 at 11:27 AM, sri vathsan <srivibish at gmail.com> wrote:

> Hi,
>
> It is not a just 79 triplets. As I said, there are 79 codes. I am making
> triplets out of that 79 codes and matching the triplets in the list.
>
> Please find the dput of the data below.
>
> > dput(head(newd,10))
> structure(list(uniq_id = c("1", "2", "3", "4", "5", "6", "7",
> "8", "9", "10"), hi = c("11,  22,  84,  85,  108,  111", "18,  84,  85,
> 87,  122,  134",
> "2,  18,  22", "18,  108,  122,  134,  176", "19,  85,  87,  100,  107",
> "79,  85,  111", "11,  88,  108", "19,  88,  96", "19,  85,  96",
> "19,  100,  103")), .Names = c("uniq_id", "hi"), row.names = c(NA,
> -10L), class = c("tbl_df", "tbl", "data.frame"))
> >
>
> I am trying to count the frequency of the triplets in the above data using
> the below code.
>
> # split column into a list
> myList <- strsplit(newd$hi, split=",")
> # get all pairwise combinations
> myCombos <- t(combn(unique(unlist(myList)), 3))
> # count the instances where the pair is present
> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
>   sum(sapply(myList, function(j) {
>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
> #final matrix
> final <- cbind(matrix(as.integer(myCombos), nrow(myCombos)), myCounts)
>
> I hope I made my point clear. Please let me know if I miss anything.
>
> Regards,
> Sri
>
>
>
>
> On Wed, Jul 27, 2016 at 11:19 PM, Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
>
> > You said you had 79 triplets and 8000 records.
> >
> > When I compared 100 triplets to 10000 records it took 86 seconds.
> >
> > So obviously there is something you're not telling us about the format
> > of your data.
> >
> > If you use dput() to provide actual examples, you will get better
> > results than if we on Rhelp have to guess. Because we tend to guess in
> > ways that make the most sense after extensive R experience, and that's
> > probably not what you have.
> >
> > Sarah
> >
> > On Wed, Jul 27, 2016 at 1:29 PM, sri vathsan <srivibish at gmail.com>
> wrote:
> > > Hi,
> > >
> > > Thanks for the solution. But I am afraid that after running this code
> > still
> > > it takes more time. It has been an hour and still it is executing. I
> > > understand the delay because each triplet has to compare almost 9000
> > > elements.
> > >
> > > Regards,
> > > Sri
> > >
> > > On Wed, Jul 27, 2016 at 9:02 PM, Sarah Goslee <sarah.goslee at gmail.com>
> > > wrote:
> > >>
> > >> Hi,
> > >>
> > >> It's really a good idea to use dput() or some other reproducible way
> > >> to provide data. I had to guess as to what your data looked like.
> > >>
> > >> It appears that order doesn't matter?
> > >>
> > >> Given than, here's one approach:
> > >>
> > >> combs <- structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L,
> > 34L,
> > >> 34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
> > >> "V2", "V3"), class = "data.frame", row.names = c(NA, -5L))
> > >>
> > >> dat <- list(
> > >> c(77,65,34,23,55),
> > >> c(65,23,77,65,55,34),
> > >> c(77,34,65),
> > >> c(55,78,56),
> > >> c(98,23,77,65,34))
> > >>
> > >>
> > >> sapply(seq_len(nrow(combs)), function(i)sum(sapply(dat,
> > >> function(j)all(combs[i,] %in% j))))
> > >>
> > >> On a dataset of comparable time to yours, it takes me under a minute
> > and a
> > >> half.
> > >>
> > >> > combs <- combs[rep(1:nrow(combs), length=100), ]
> > >> > dat <- dat[rep(1:length(dat), length=10000)]
> > >> >
> > >> > dim(combs)
> > >> [1] 100   3
> > >> > length(dat)
> > >> [1] 10000
> > >> >
> > >> > system.time(test <- sapply(seq_len(nrow(combs)),
> > >> > function(i)sum(sapply(dat, function(j)all(combs[i,] %in% j)))))
> > >>    user  system elapsed
> > >>  86.380   0.006  86.391
> > >>
> > >>
> > >>
> > >>
> > >> On Wed, Jul 27, 2016 at 10:47 AM, sri vathsan <srivibish at gmail.com>
> > wrote:
> > >> > Hi,
> > >> >
> > >> > Apologizes for the less information.
> > >> >
> > >> > Basically, myCombos is a matrix with 3 variables which is a triplet
> > that
> > >> > is
> > >> > a combination of 79 codes. There are around 3lakh combination as
> such
> > >> > and
> > >> > it looks like below.
> > >> >
> > >> > V1 V2 V3
> > >> > 65 23 77
> > >> > 77 34 65
> > >> > 55 34 23
> > >> > 23 77 34
> > >> > 34 65 55
> > >> >
> > >> > Each triplet will compare in a list (mylist) having 8177 elements
> > which
> > >> > will looks like below.
> > >> >
> > >> > 77,65,34,23,55
> > >> > 65,23,77,65,55,34
> > >> > 77,34,65
> > >> > 55,78,56
> > >> > 98,23,77,65,34
> > >> >
> > >> > Now I want to count the no of occurrence of the triplet in the above
> > >> > list.
> > >> > I.e., the triplet 65 23 77 is seen 3 times in the list. So my output
> > >> > looks
> > >> > like below
> > >> >
> > >> > V1 V2 V3 Freq
> > >> > 65 23 77  3
> > >> > 77 34 65  4
> > >> > 55 34 23  2
> > >> >
> > >> > I hope, I made it clear this time.
> > >> >
> > >> >
> > >> > On Wed, Jul 27, 2016 at 7:00 PM, Bert Gunter <
> bgunter.4567 at gmail.com>
> > >> > wrote:
> > >> >
> > >> >> Not entirely sure I understand, but match() is already vectorized,
> so
> > >> >> you
> > >> >> should be able to lose the supply(). This would speed things up a
> > lot.
> > >> >> Please re-read ?match *carefully* .
> > >> >>
> > >> >> Bert
> > >> >>
> > >> >> On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com>
> wrote:
> > >> >>
> > >> >> Hi,
> > >> >>
> > >> >> I created list of 3 combination numbers (mycombos, around 3 lakh
> > >> >> combinations) and counting the occurrence of those combination in
> > >> >> another
> > >> >> list. This comparision list (mylist) is having around 8000
> records.I
> > am
> > >> >> using the following code.
> > >> >>
> > >> >> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
> > >> >>   sum(sapply(myList, function(j) {
> > >> >>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
> > >> >>
> > >> >> The above code takes very long time to execute and is there any
> other
> > >> >> effecting method which will reduce the time.
> > >> >> --
> > >> >>
> > >> >> Regards,
> > >> >> Srivathsan.K
> > >> >>
> > >
> > >
> > >
> > >
> >
>
>
>
> --
>
> Regards,
> Srivathsan.K
> Phone : 9600165206
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul 27 21:35:34 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 27 Jul 2016 12:35:34 -0700
Subject: [R] Reduce woes
In-Reply-To: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
Message-ID: <32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>


> On Jul 27, 2016, at 8:20 AM, Stefan Kruger <stefan.kruger at gmail.com> wrote:
> 
> Hi -
> 
> I'm new to R.
> 
> In other functional languages I'm familiar with you can often seed a call
> to reduce() with a custom accumulator. Here's an example in Elixir:
> 
> map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
> map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
> Enum.count(v), nil) end)
> # %{"one" => 2, "three" => 1, "two" => 2}
> 
> In R-terms that's reducing a list of vectors to become a new list mapping
> the names to the vector lengths.
> 
> Even in JavaScript, you can do similar things:
> 
> list = { one: [1, 1], three: [3], two: [2, 2] };
> var result = Object.keys(list).reduceRight(function (acc, item) {
>  acc[item] = list[item].length;
>  return acc;
> }, {});
> // result == { two: 2, three: 1, one: 2 }
> 
> In R, from what I can gather, Reduce() is restricted such that any init
> value you feed it is required to be of the same type as the elements of the
> vector you're reducing -- so I can't build up. So whilst I can do, say
> 
>> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
> [1] 111
> 
> I can't use Reduce to build up a list, vector or data frame?
> 
> What am I missing?
> 
> Many thanks for any pointers,

This builds a list:

> Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96, accumulate=TRUE)
[[1]]
[1] 96

[[2]]
[1] 96  1

[[3]]
[1] 96  1  2

[[4]]
[1] 96  1  2  3

[[5]]
[1] 96  1  2  3  4

[[6]]
[1] 96  1  2  3  4  5

But you are not saying what you want. The other examples were doing something with names but you provided no names for the R example.

This would return a list of named vectors:

> Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))  }, c(1,2,3,4,5), 96, accumulate=TRUE)
[[1]]
[1] 96

[[2]]
 1  2 
96  1 

[[3]]
 1  2  3 
96  1  2 

[[4]]
 1  2  3  4 
96  1  2  3 

[[5]]
 1  2  3  4  5 
96  1  2  3  4 

[[6]]
 1  2  3  4  5  6 
96  1  2  3  4  5 




> Stefan
> 
> 
> 
> -- 
> Stefan Kruger <stefan.kruger at gmail.com>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From chocold12 at gmail.com  Wed Jul 27 21:36:39 2016
From: chocold12 at gmail.com (lily li)
Date: Wed, 27 Jul 2016 13:36:39 -0600
Subject: [R] how to create column names for the matrix
In-Reply-To: <20160727195001.Horde.084hXpMISeYpV7iC9PkEhUz@mail.sapo.pt>
References: <CAN5afy9VgOS3Cxo7ATn5kvifqWkHVnVUPK3M8TjbirP4rKBiWg@mail.gmail.com>
	<20160727195001.Horde.084hXpMISeYpV7iC9PkEhUz@mail.sapo.pt>
Message-ID: <CAN5afy-q8PChw6oC=N6swzmtnkStY9tg=SDYHENHya6_LsZ_0g@mail.gmail.com>

Thanks, this works. I thought the code should put in the for loop, but it
turns out an extra line.

On Wed, Jul 27, 2016 at 12:50 PM, <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Try
>
>
> chars = c('A','B','C','D')
> matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
> k = 0
> for(i in 1:length(1:5)){  # or more simply just for(i in 1:5)
>   for(j in 1:length(chars)){
>     k = k+1
>     matrix1[,k] = k
>   }
> }
> matrix1
> tmp <- expand.grid(chars, 1:5, stringsAsFactors = FALSE)
> nms <- paste(tmp[[2]], tmp[[1]], sep = "-")
> rm(tmp)
> colnames(matrix1) <- nms
>
> head(matrix1)
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> Citando lily li <chocold12 at gmail.com>:
>
> Hi all,
>
> I want to ask that how to create column names for a matrix. For example,
> the matrix below, the column names should be: 1-A, 1-B, 1-C, 1-D, 2-A, 2-B,
> 2-C, 2-D, 3-A, etc. Thanks for your help.
>
> chars = c('A','B','C','D')
> matrix1 = matrix(nrow = length(1:100), ncol = length(1:5)*length(chars))
> k = 0
> for(i in seq(1:length(1:5))){
> for(j in seq(1:length(chars))){
>    k = k+1
>    matrix1[,k] = c(1:100)[k]
> }
> }
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.htmland provide commented,
> minimal, self-contained, reproducible code.
>
>
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul 27 21:40:07 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 27 Jul 2016 12:40:07 -0700
Subject: [R] Time format issue
In-Reply-To: <2E3CE2D5-728E-4E50-BB68-D91A3EE4A14E@gmail.com>
References: <CAPQRYa7WxN_0tAkbdyeOqw+u7pkPjsb9CAxcHMrbQW_xVGiE8A@mail.gmail.com>
	<D36FF0B2-D2A0-4D82-8829-786282C6016E@comcast.net>
	<2E3CE2D5-728E-4E50-BB68-D91A3EE4A14E@gmail.com>
Message-ID: <81CBD880-F468-41FB-A3C1-0F46F919C9C7@comcast.net>


> On Jul 27, 2016, at 5:03 AM, Ismail SEZEN <sezenismail at gmail.com> wrote:
> 
> 
>>> The time column is factor. I want to convert each time to the following
>>> format:
>>> 
>>> 20111211003000
>>> 
>>> 20111211013000
>>> 
>>> 20111211020000
>>> 
>>> (Year)(month)(date)(hr)(min)(sec)
>> 
>>> newTimeDt <- strptime(Time, format="%d%b%y:%H:%M")
>>> newTimeDt
>> [1] "2011-12-27 00:30:00 PST" "2011-12-27 01:30:00 PST" "2011-12-27 02:00:00 PST"
>>> newTimeDt <- as.POSIXct(Time, format="%d%b%y:%H:%M")
>>> newTimeDt
>> [1] "2011-12-27 00:30:00 PST" "2011-12-27 01:30:00 PST" "2011-12-27 02:00:00 PST"
>>> newOut <- format(newTimeDt, "%Y%m%d%H%M")
>>> newOut
>> [1] "201112270030" "201112270130" "201112270200"
>>> newOut <- format(newTimeDt, "%Y%m%d%H%M%S")
>>> newOut
>> [1] "20111227003000" "20111227013000" "20111227020000"
> 
> Addition to David, if you use functions similar to read.csv or read.table to read dates from a file, you have to set 
> 

I am not opposed to using stringsAsFactors=FALSE at the time of data input, but since there is an as.POSIXct.factor function it was not needed here.


> stringsAsFactors = T

Wrong: It would need to be FALSE to be anything other than the default. And you would not be reading Dates but rather character values.

> 
> in function parameters or set data.frame column as character seperately.

I did test my code on a factor vector.


-- 

David Winsemius
Alameda, CA, USA


From frainj at gmail.com  Wed Jul 27 22:16:28 2016
From: frainj at gmail.com (John C Frain)
Date: Wed, 27 Jul 2016 21:16:28 +0100
Subject: [R] Windows 10 Application Compatibility Check | FreeWare R
 Statistical Environment v3.2.2
In-Reply-To: <DCAE57CB-2445-4AEF-92D7-893F18D60019@me.com>
References: <F7CCE7BE9001FB439190567BE5A9E2FE8DCB5A86@EXTXMB54.nam.nsroot.net>
	<dba03f60-720b-6649-37b9-c93fa1456b26@atsu.edu>
	<CAHrK515L7GqZSsvDzOaE+s5hcuuMahbwP34hXY919KX50oarRg@mail.gmail.com>
	<DCAE57CB-2445-4AEF-92D7-893F18D60019@me.com>
Message-ID: <CAHrK516mknMMh7jzXozx2rkHzMjsi=pP3Zfx0gOtZH9j6gBEXw@mail.gmail.com>

Sorry if my earlier remarks were unfair to City but their request reminded
me of some problems that I had with using R, similar software and the
obstacles placed in my way by an IT Department many years ago. If it is of
any help I have R installed on a Desktop and two laptops (running 64-bit
Windows 10) and one very modest Tablet (running 32-bit Windows 10). Perhaps
I have been lucky as I have not encountered any problems.

Rstudio clearly supports Windows 10 (See supported packages on
https://www.rstudio.com/products/rstudio/download2/).  Microsoft have
"Microsoft R Open" (formerly Revolution R) available at
https://mran.revolutionanalytics.com/. These are based on a fixed CRAN
repository date. The version available there is said to be compatible
with Windows
7.0 (SP1), 8.1, 10 and Windows Server? 2008 R2 (SP1), 2012 (64-bit versions
only).


John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

On 27 July 2016 at 17:09, Marc Schwartz <marc_schwartz at me.com> wrote:

> Hi,
>
> With the caveat that I speak for myself only, a few comments:
>
> 1. Given that Citi appears to have been running R on Windows 7 for some
> period of time, I am not sure that they are looking for reasons to deny
> users access to R. They are simply looking for some indication that R will
> run on Windows 10.
>
> 2. There is nothing presently in the relevant R for Windows FAQ:
>
>
> https://cran.r-project.org/bin/windows/base/rw-FAQ.html#Does-R-run-under-Windows-Vista_003f
>
> that explicitly indicates that R will run on Windows 10. At some point,
> presumably, that would be updated to reflect more recent experience by R
> Core.
>
> 3. Besides Robert's comment below on his experience (n of 1), there are
> other comments that I see via a Google search that would suggest that his
> experience is not unique (e.g. https://rpubs.com/kartykr/rrproject) and
> there are threads in the R-Help archives from last September:
>
>   https://stat.ethz.ch/pipermail/r-help/2015-September/431809.html
>   https://stat.ethz.ch/pipermail/r-help/2015-September/431825.html
>
> that also support the notion that R will run on Windows 10.
>
> 4. If Citi, wants to pursue a more "commercial-like" approach to paid
> support for R, there are commercial versions of R available, which I shall
> not mention here, but a quick Google search would avail them of more
> details.
>
> 5. Lastly and perhaps most importantly, R is not Freeware. R (as made
> available by the R Foundation) is free open source software (FOSS) and is
> distributed under relevant open source licenses. The label "Freeware",
> which typically refers to proprietary software, only means that the binary
> application itself is free of charge. It does not mean that the source code
> for the application is also available. There is a very large functional and
> philosophical difference between FOSS and Freeware.
>
> Regards,
>
> Marc Schwartz
>
>
> > On Jul 27, 2016, at 9:47 AM, John C Frain <frainj at gmail.com> wrote:
> >
> > When I first graduated some 50 years ago I worked in the Department of
> > Finance. On small piece of my work involved getting CPI data from the CSO
> > and doing some calculations. A specific person in the CSO (Central
> > Statistics Office) usually supplied this information and this fact was
> > always recorded on the file. On one occasion I could not contact that
> > person and I visited the department library and extracted the information
> > myself. When my boss saw that I had not contacted the CSO I was
> instructed
> > to contact them and repeat the calculation. It was explained to me that
> if
> > I contacted the CSO we could blame someone outside the Department of
> > Finance if they supplied the information.
> >
> > It appears that Citi Architecture & Technology Engineering are behaving
> in
> > a similar way.  Are they trying to find some excuse to deny their users
> > access to R on the grounds that it is not "properly" supported. This does
> > happen in large organisations.
> >
> > If any one knows of some organisation that is willing to provide support
> to
> > Citi (for a respectable fee) there might be a bit of money to be made.
> >
> > John C Frain
> > 3 Aranleigh Park
> > Rathfarnham
> > Dublin 14
> > Ireland
> > www.tcd.ie/Economics/staff/frainj/home.html
> > mailto:frainj at tcd.ie
> > mailto:frainj at gmail.com
> >
> > On 26 July 2016 at 23:49, Robert Baer <rbaer at atsu.edu> wrote:
> >
> >> Runs fine on Windows 10 for me.
> >>
> >>
> >> On 7/25/2016 7:18 AM, Ramar, Rohini wrote:
> >>
> >>> Hello Team,
> >>>
> >>> We are, Citi Application Readiness Team, need your assistance in order
> to
> >>> gather info about below application compatibility and support for Win
> 10 as
> >>> part of Window 10 Readiness initiative. CITI Bank has been using below
> >>> "FreeWare R Statistical Environment v3.2.2"  software products
> currently on
> >>> Win 7 operating system.
> >>>
> >>> We would like to know whether the below listed application is
> compatible
> >>> and supported even for Win 10 (64 Bit) or is there any other higher
> version
> >>> of application which would be compatible for Win10. If you have not
> tested
> >>> for Win 10, could you please provide us with a tentative date by when
> we
> >>> can reach you.
> >>>
> >>> Application Name : FreeWare R Statistical Environment v3.2.2
> >>>
> >>>
> >>> Note: Kindly re-direct this email to appropriate team if we reached you
> >>> wrongly.
> >>>
> >>>
> >>> Regards,
> >>> Rohini R
> >>> Citi Architecture & Technology Engineering
> >>> Client Computing
> >>> Direct Phone #:+91 22 3346 1497
> >>> Email ID: rohini.ramar at citi.com<mailto:rohini.ramar at citi.com>
>
>

	[[alternative HTML version deleted]]


From justinthong93 at gmail.com  Wed Jul 27 22:34:05 2016
From: justinthong93 at gmail.com (Justin Thong)
Date: Wed, 27 Jul 2016 21:34:05 +0100
Subject: [R] Reference for aov()
In-Reply-To: <03376F5C-47DD-4259-8791-9110D857A53E@gmail.com>
References: <CAEtAGepf2Ake07BzsGrak-J6=vTxTpkPL=V=825thBRkoUjnQA@mail.gmail.com>
	<03376F5C-47DD-4259-8791-9110D857A53E@gmail.com>
Message-ID: <CAEtAGer+5ppeixOa9pfhfuDt9XRhJVMV7WpxpGYm0+g80_eu9Q@mail.gmail.com>

Hi Peter,


Thank you for your good answer. I am sorry for the late reply.

*An ortogonalized model matrix generates a decomposition of the model space
into orthogonal subspaces corresponding to the terms of the model.
Projections onto each of the subspaces are easily worked out.  E.g., for a
two-way analysis (Y~row+col+row:col) you can decompose the model effect as
a row effect, a column effect, and an interaction effect. This allows
straightforward calculation of the sums of squares of the ANOVA table. As
you are probably well aware, if the design is unbalanced, the results will
depend on the order of terms -- col + row + col:row gives a different
result.*

It may be a stupid question. How are projections of each sums of squares
easily worked out and how does the sums of squares follow easily? Does it
matter that certain parameters of the model are not estimated. R appears to
just give a sums of squares despite some of the parameters being
non-estimable.

Thank you





On 14 July 2016 at 09:50, peter dalgaard <pdalgd at gmail.com> wrote:

> I am not aware of a detailed documentation of this beyond the actual
> source code.
> However, the principles are fairly straightforward, except that the rules
> for constructing the design matrix from the model formula can be a bit
> arcane at times.
>
> The two main tools are the design matrix constructor (model.matrix) and a
> Gram-Schmidt type ortogonalization of its columns (the latter is called a
> QR decomposition in R, which it is, but there are several algorithms for
> QR, and the linear models codes depend on the QR algorithm being based on
> orthogonalization - so LINPACK works and LAPACK doesn't).
>
> An ortogonalized model matrix generates a decomposition of the model space
> into orthogonal subspaces corresponding to the terms of the model.
> Projections onto each of the subspaces are easily worked out.  E.g., for a
> two-way analysis (Y~row+col+row:col) you can decompose the model effect as
> a row effect, a column effect, and an interaction effect. This allows
> straightforward calculation of the sums of squares of the ANOVA table. As
> you are probably well aware, if the design is unbalanced, the results will
> depend on the order of terms -- col + row + col:row gives a different
> result.
>
> What aov() does is that it first decomposes the observations according to
> the Error() term, forming the error strata, then fits the systematic part
> of the model to each stratum in turn. In the nice cases, each term of the
> model will be estimable in exactly one stratum, and part of the aov() logic
> is to detect and remove unestimable terms. E.g., if you have a balanced two
> way layout, say individual x treatment, the variable gender is a subfactor
> of individual, so Y ~ gender * treatment + Error(individual/treatment), the
> gender effect is estimated in the individual stratum, whereas treatment and
> gender:treatment are estimated in the individual:treatment stratum.
>
> It should be noted that it is very hard to interpret the results of aov()
> unless the Error() part of the model corresponds to a balanced experimental
> design. Or put more sharply: The model implied by the decomposition into
> error strata becomes nonsensical otherwise. If you do have a balanced
> design, the error strata reduce to simple combinations of means and
> observation, so the aov() algorithm is quite inefficient, but to my
> knowledge nobody has bothered to try and do better.
>
> -pd
>
> > On 13 Jul 2016, at 18:18 , Justin Thong <justinthong93 at gmail.com> wrote:
> >
> > Hi
> >
> > *I have been looking for a reference to explain how R uses the aov
> > command(at a deeper level)*. More specifically, how R reads the formulae
> > and R computes the sums of squares. I am not interested in understanding
> > what the difference of Type 1,2,3 sum of squares are. I am more
> interested
> > in finding out about how R computes ~x1:x2:x3  or how R computes ~A:x1
> > emphasizing sequential nature of the way it computes, and models even
> more
> > complicated than this.
> >
> > Yours sincerely,
> > Justin
> >
> > *I check my email at 9AM and 4PM everyday*
> > *If you have an EMERGENCY, contact me at +447938674419(UK) or
> > +60125056192(Malaysia)*
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>


-- 
Yours sincerely,
Justin

*I check my email at 9AM and 4PM everyday*
*If you have an EMERGENCY, contact me at +447938674419(UK) or
+60125056192(Malaysia)*

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Wed Jul 27 22:41:46 2016
From: hannah.hlx at gmail.com (li li)
Date: Wed, 27 Jul 2016 16:41:46 -0400
Subject: [R] Help on improving an algorithm
Message-ID: <CAHLnndbxr272957gi9XWcrCJP_TdTB4PLKxj7VL36Fa=iTtZoQ@mail.gmail.com>

Hi all,
 I have four matrix tmp_a, tmp_b, tmp_c and tmp_d whose dimensions are
shown as below.
I want to take one row from each of these matrices and then put the four
selected rows into a matrix.
I want to count the number of such matrices for which the vector of row sum
is less than or equal to (8,8,8,8,8) (componentwise).
Below is the code I use now. Is there a way to make this more efficient and
faster?
 Thanks for the help in advance.

> dim(tmp_a);dim(tmp_b); dim(tmp_c); dim(tmp_d)[1] 252   6
[1] 126   6
[1] 126   6
[1] 126   6


p <- 0
for (i in 1:dim(tmp_a)[1]){
    for (j in 1:dim(tmp_b)[1]){
        for (k in 1:dim(tmp_c)[1]){
            for (l in 1:dim(tmp_c)[1]){
            conti <- rbind(tmp_a[i,], tmp_b[j,], tmp_c[k,],tmp_d[l,])
            if (sum(apply(conti,1,sum)>8)==0)
                p <- p+1
                print(p)}}}}

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Wed Jul 27 23:01:37 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Wed, 27 Jul 2016 14:01:37 -0700
Subject: [R] font size in graphs...can R read Windows settings?
Message-ID: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>

Hi All,
I am putting together a package that (among other things) draws some nice
graphs for users. I place some explanatory text on figs using "text" and
"mtext". But the size of the text depends on the Windows display settings:
Smaller (100%), medium (125%) or larger (150%) (In Windows 7... Control
panel | Appearance and personalization | Display | Make text and other
items smaller or larger). If I create figs that look good with one setting,
the text is too big (or too small) if another setting is used in Windows.
If I know the Windows setting, I can use cex in R to make the right sized
labels, but if I don't know a user's Windows display size setting...is
there a way for R to read the setting? Or is there another way to control
label size so that text labels on graphs look good regardless of WIndows
display size setting?

Many thanks for Ideas,

-Dan
-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From sbigelow at jonesctr.org  Wed Jul 27 23:02:01 2016
From: sbigelow at jonesctr.org (Seth Bigelow)
Date: Wed, 27 Jul 2016 17:02:01 -0400
Subject: [R] Lattice barchart legend with panel.barchart
Message-ID: <CAEryiFuL0AyZwej6FwmwW66AUpON8Rz8sH44m1=02Q718NUVvQ@mail.gmail.com>

I have constructed a barchart that requires a panel call, but the panel
reduces the facsimiles of bars in the legend to small colored circles. You
can see this behavior in the following example:

Titan <- as.data.frame(Titanic)

titanpanel <- function(x,y,...){
panel.barchart(x,y,...)
}

barchart(Class~Freq|Sex + Age, Titan,
groups=Survived,
panel = titanpanel,
stack=TRUE, layout=c(4,1),
auto.key=list(title="Survived", columns=2))

...if you comment out the panel and run the barchart statement you will see
nice blocks displayed in the legend. Is there any easy way to retain these
blocks with panel.barchart?


-- 
Seth W. Bigelow, Ph.D.
Assistant Scientist of Forest Ecology
Joseph W. Jones Ecological Research Center
Newton, GA

	[[alternative HTML version deleted]]


From paul at stat.auckland.ac.nz  Wed Jul 27 23:24:57 2016
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Thu, 28 Jul 2016 09:24:57 +1200
Subject: [R] [FORGED]  Lattice barchart legend with panel.barchart
In-Reply-To: <CAEryiFuL0AyZwej6FwmwW66AUpON8Rz8sH44m1=02Q718NUVvQ@mail.gmail.com>
References: <CAEryiFuL0AyZwej6FwmwW66AUpON8Rz8sH44m1=02Q718NUVvQ@mail.gmail.com>
Message-ID: <a390f286-5da7-1a8f-a1fb-fc83f49d1860@stat.auckland.ac.nz>

Hi

Try this ...

barchart(Class~Freq|Sex + Age, Titan,
          groups=Survived,
          panel = titanpanel,
          stack=TRUE, layout=c(4,1),
          key=simpleKey(title="Survived", text=levels(Titan$Survived),
                        rectangles=TRUE, points=FALSE, columns=2))

Paul

On 28/07/16 09:02, Seth Bigelow wrote:
> I have constructed a barchart that requires a panel call, but the panel
> reduces the facsimiles of bars in the legend to small colored circles. You
> can see this behavior in the following example:
>
> Titan <- as.data.frame(Titanic)
>
> titanpanel <- function(x,y,...){
> panel.barchart(x,y,...)
> }
>
> barchart(Class~Freq|Sex + Age, Titan,
> groups=Survived,
> panel = titanpanel,
> stack=TRUE, layout=c(4,1),
> auto.key=list(title="Survived", columns=2))
>
> ...if you comment out the panel and run the barchart statement you will see
> nice blocks displayed in the legend. Is there any easy way to retain these
> blocks with panel.barchart?
>
>

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From pdalgd at gmail.com  Wed Jul 27 23:27:32 2016
From: pdalgd at gmail.com (pdalgd at gmail.com)
Date: Wed, 27 Jul 2016 23:27:32 +0200
Subject: [R] Reference for aov()
In-Reply-To: <CAEtAGep0ZeR1GvJCn6gZEs94QVEEWiQ5khWCgLofubosUnPLnQ@mail.gmail.com>
References: <CAEtAGepf2Ake07BzsGrak-J6=vTxTpkPL=V=825thBRkoUjnQA@mail.gmail.com>
	<03376F5C-47DD-4259-8791-9110D857A53E@gmail.com>
	<CAEtAGer+5ppeixOa9pfhfuDt9XRhJVMV7WpxpGYm0+g80_eu9Q@mail.gmail.com>
	<CAEtAGep0ZeR1GvJCn6gZEs94QVEEWiQ5khWCgLofubosUnPLnQ@mail.gmail.com>
Message-ID: <3D1D2048-ACB9-4876-B00D-032C83012AFF@gmail.com>

In general, some assembly is required. This stuff doesn't quit fit in a one page email. You may want to start playing around with model.matrix and qr() to see the light.

One tricky bit is how much you know already. There is some risk for me of having to rewrite a linear algebra textbook...

I'll assume that you know the betahat=(X'X)^{-1}X'y formula and the corresponding projection yhat = X (X'X)^{-1}X'y. Now assume that X is orthogonal, then X'X is the identity matrix so the whole thing becomes X X'Y = sum(x_i x_i'y) and the (uncorrected) sum of squares for regression is sum((x_i' y)^2). 

If you have an arbitrary design matrix X and orthgonalize it successively, then you have say Xtilde with the property that Xtilde is orthogonal and that, for any k, the first k columns of Xtilde spans the same subspace as the first k columns of X. This means that the projections onto the first k columns of X and the corresponding regression SS might as well be based on Xtilde and the simpler formulas for the orthogonal case. This also goes for differences between projections onto the first k1 and k2 columns.

In connection with stratified models, it happens that some terms of the systematic part are not estimable in all strata so e.g. in a paired design on individual subjects, you cannot estimate a gender effect from the within-subject stratum. This looks at differences within the same subject so a gender main effect cancels out. Technically, what happens is that the design matrix for the systematic model is projected (columnwise) onto each stratum and one or more of the resulting matrices may be singular, this is detected numerically and the corresponding terms are removed from the model. The difficulty here is to detect the aliasing reliably and get the DF right.

-pd



> On 27 Jul 2016, at 22:34 , Justin Thong <justinthong93 at gmail.com> wrote:
> 
> Hi Peter, 
> 
> (EDIT)
> Thank you for your good answer. I am sorry for the late reply.
> 
> An ortogonalized model matrix generates a decomposition of the model space into orthogonal subspaces corresponding to the terms of the model. Projections onto each of the subspaces are easily worked out.  E.g., for a two-way analysis (Y~row+col+row:col) you can decompose the model effect as a row effect, a column effect, and an interaction effect. This allows straightforward calculation of the sums of squares of the ANOVA table. As you are probably well aware, if the design is unbalanced, the results will depend on the order of terms -- col + row + col:row gives a different result.
> 
> It may be a stupid question. How are projections of each subspace easily worked out and how does the sums of squares follow easily? Does it matter that certain parameters of the model are not estimated. R appears to just give a sums of squares despite some of the parameters being non-estimable.
> 
> Thank you 
> 
> On 27 July 2016 at 21:34, Justin Thong <justinthong93 at gmail.com> wrote:
> Hi Peter, 
> 
> 
> Thank you for your good answer. I am sorry for the late reply.
> 
> An ortogonalized model matrix generates a decomposition of the model space into orthogonal subspaces corresponding to the terms of the model. Projections onto each of the subspaces are easily worked out.  E.g., for a two-way analysis (Y~row+col+row:col) you can decompose the model effect as a row effect, a column effect, and an interaction effect. This allows straightforward calculation of the sums of squares of the ANOVA table. As you are probably well aware, if the design is unbalanced, the results will depend on the order of terms -- col + row + col:row gives a different result.
> 
> It may be a stupid question. How are projections of each sums of squares easily worked out and how does the sums of squares follow easily? Does it matter that certain parameters of the model are not estimated. R appears to just give a sums of squares despite some of the parameters being non-estimable.
> 
> Thank you 
> 
> 
> 
> 
> 
> On 14 July 2016 at 09:50, peter dalgaard <pdalgd at gmail.com> wrote:
> I am not aware of a detailed documentation of this beyond the actual source code.
> However, the principles are fairly straightforward, except that the rules for constructing the design matrix from the model formula can be a bit arcane at times.
> 
> The two main tools are the design matrix constructor (model.matrix) and a Gram-Schmidt type ortogonalization of its columns (the latter is called a QR decomposition in R, which it is, but there are several algorithms for QR, and the linear models codes depend on the QR algorithm being based on orthogonalization - so LINPACK works and LAPACK doesn't).
> 
> An ortogonalized model matrix generates a decomposition of the model space into orthogonal subspaces corresponding to the terms of the model. Projections onto each of the subspaces are easily worked out.  E.g., for a two-way analysis (Y~row+col+row:col) you can decompose the model effect as a row effect, a column effect, and an interaction effect. This allows straightforward calculation of the sums of squares of the ANOVA table. As you are probably well aware, if the design is unbalanced, the results will depend on the order of terms -- col + row + col:row gives a different result.
> 
> What aov() does is that it first decomposes the observations according to the Error() term, forming the error strata, then fits the systematic part of the model to each stratum in turn. In the nice cases, each term of the model will be estimable in exactly one stratum, and part of the aov() logic is to detect and remove unestimable terms. E.g., if you have a balanced two way layout, say individual x treatment, the variable gender is a subfactor of individual, so Y ~ gender * treatment + Error(individual/treatment), the gender effect is estimated in the individual stratum, whereas treatment and gender:treatment are estimated in the individual:treatment stratum.
> 
> It should be noted that it is very hard to interpret the results of aov() unless the Error() part of the model corresponds to a balanced experimental design. Or put more sharply: The model implied by the decomposition into error strata becomes nonsensical otherwise. If you do have a balanced design, the error strata reduce to simple combinations of means and observation, so the aov() algorithm is quite inefficient, but to my knowledge nobody has bothered to try and do better.
> 
> -pd
> 
> > On 13 Jul 2016, at 18:18 , Justin Thong <justinthong93 at gmail.com> wrote:
> >
> > Hi
> >
> > *I have been looking for a reference to explain how R uses the aov
> > command(at a deeper level)*. More specifically, how R reads the formulae
> > and R computes the sums of squares. I am not interested in understanding
> > what the difference of Type 1,2,3 sum of squares are. I am more interested
> > in finding out about how R computes ~x1:x2:x3  or how R computes ~A:x1
> > emphasizing sequential nature of the way it computes, and models even more
> > complicated than this.
> >
> > Yours sincerely,
> > Justin
> >
> > *I check my email at 9AM and 4PM everyday*
> > *If you have an EMERGENCY, contact me at +447938674419(UK) or
> > +60125056192(Malaysia)*
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> -- 
> Yours sincerely,
> Justin
> 
> I check my email at 9AM and 4PM everyday
> If you have an EMERGENCY, contact me at +447938674419(UK) or +60125056192(Malaysia)  
> 
> 
> 
> 
> -- 
> Yours sincerely,
> Justin
> 
> I check my email at 9AM and 4PM everyday
> If you have an EMERGENCY, contact me at +447938674419(UK) or +60125056192(Malaysia)  
> 


From jdnewmil at dcn.davis.ca.us  Wed Jul 27 23:46:09 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 27 Jul 2016 14:46:09 -0700
Subject: [R] font size in graphs...can R read Windows settings?
In-Reply-To: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>
References: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>
Message-ID: <5133B33E-DF59-4360-88E1-7478B879D6E1@dcn.davis.ca.us>

What if it is being used on a platform other than Windows? This problem is more challenging than you seem to think it is. 

I would suggest including a kind of "par" argument to your function that lets the user change your defaults.
-- 
Sent from my phone. Please excuse my brevity.

On July 27, 2016 2:01:37 PM PDT, "Dalthorp, Daniel" <ddalthorp at usgs.gov> wrote:
>Hi All,
>I am putting together a package that (among other things) draws some
>nice
>graphs for users. I place some explanatory text on figs using "text"
>and
>"mtext". But the size of the text depends on the Windows display
>settings:
>Smaller (100%), medium (125%) or larger (150%) (In Windows 7... Control
>panel | Appearance and personalization | Display | Make text and other
>items smaller or larger). If I create figs that look good with one
>setting,
>the text is too big (or too small) if another setting is used in
>Windows.
>If I know the Windows setting, I can use cex in R to make the right
>sized
>labels, but if I don't know a user's Windows display size setting...is
>there a way for R to read the setting? Or is there another way to
>control
>label size so that text labels on graphs look good regardless of
>WIndows
>display size setting?
>
>Many thanks for Ideas,
>
>-Dan


From dwinsemius at comcast.net  Thu Jul 28 00:26:37 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 27 Jul 2016 15:26:37 -0700
Subject: [R] Aggregate matrix in a 2 by 2 manor
In-Reply-To: <ED550726-6ACD-487A-959A-2DB8706B76F4@dcn.davis.ca.us>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
	<CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
	<E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>
	<f2247c74794a41ad8a70f85d2c85df92@exch-2p-mbx-t2.ads.tamu.edu>
	<ED550726-6ACD-487A-959A-2DB8706B76F4@dcn.davis.ca.us>
Message-ID: <506368AE-3234-425B-97B5-DD3B13094237@comcast.net>


> On Jul 27, 2016, at 12:02 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> An alternative (more compact, not necessarily faster, because apply is still a for loop inside):
> 
> f <- function( m, nx, ny ) {
>  # redefine the dimensions of my
>  a <- array( m
>             , dim = c( ny
>                    , nrow( m ) %/% ny
>                    , ncol( m ) %/% nx )
>            )
>  # apply mean over dim 1
>  apply( a, c( 2, 3 ), FUN=mean )
> }
> f( tst, nx, ny )

Here's an apparently loopless strategy, although I suspect the code for interaction (and maybe tapply as well?) uses a loop.


tst_2X2 <- matrix(NA, ,ncol=4,nrow=2)

 tst_2x2[] <- tapply( tst, interaction( (row(tst)+1) %/% 2, (col(tst)+1) %/% 2 ), mean)

 tst_2x2

     [,1] [,2] [,3] [,4]
[1,]  3.5 11.5 19.5 27.5
[2,]  5.5 13.5 21.5 29.5

-- 
David.


> 
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On July 27, 2016 9:08:32 AM PDT, David L Carlson <dcarlson at tamu.edu> wrote:
>> This should be faster. It uses apply() across the blocks. 
>> 
>>> ilon <- seq(1,8,nx)
>>> ilat <- seq(1,4,ny)
>>> cells <- as.matrix(expand.grid(ilat, ilon))
>>> blocks <- apply(cells, 1, function(x) tst[x[1]:(x[1]+1),
>> x[2]:(x[2]+1)])
>>> block.means <- colMeans(blocks)
>>> tst_2x2 <- matrix(block.means, 2, 4)
>>> tst_2x2
>>    [,1] [,2] [,3] [,4]
>> [1,]  3.5 11.5 19.5 27.5
>> [2,]  5.5 13.5 21.5 29.5
>> 
>> -------------------------------------
>> David L Carlson
>> Department of Anthropology
>> Texas A&M University
>> College Station, TX 77840-4352
>> 
>> 
>> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-poject.org] On Behalf Of Anthoni,
>> Peter (IMK)
>> Sent: Wednesday, July 27, 2016 6:14 AM
>> To: r-help at r-project.org
>> Subject: [R] Aggregate matrix in a 2 by 2 manor
>> 
>> Hi all,
>> 
>> I need to aggregate some matrix data (1440x720) to a lower dimension
>> (720x360) for lots of years and variables
>> 
>> I can do double for loop, but that will be slow. Anybody know a quicker
>> way?
>> 
>> here an example with a smaller matrix size:
>> 
>> tst=matrix(1:(8*4),ncol=8,nrow=4)
>> tst_2x2=matrix(NA,ncol=4,nrow=2)
>> nx=2
>> ny=2
>> for(ilon in seq(1,8,nx)) {
>> for (ilat in seq(1,4,ny)) {
>>   ilon_2x2=1+(ilon-1)/nx
>>   ilat_2x2=1+(ilat-1)/ny
>>   tst_2x2[ilat_2x2,ilon_2x2] = mean(tst[ilat+0:1,ilon+0:1])
>> }
>> }
>> 
>> tst
>> tst_2x2
>> 
>>> tst
>>    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
>> [1,]    1    5    9   13   17   21   25   29
>> [2,]    2    6   10   14   18   22   26   30
>> [3,]    3    7   11   15   19   23   27   31
>> [4,]    4    8   12   16   20   24   28   32
>> 
>>> tst_2x2
>>    [,1] [,2] [,3] [,4]
>> [1,]  3.5 11.5 19.5 27.5
>> [2,]  5.5 13.5 21.5 29.5
>> 
>> 
>> I though a cast to 3d-array might do the trick and apply over the new
>> dimension, but that does not work, since it casts the data along the
>> row.
>>> matrix(apply(array(tst,dim=c(nx,ny,8)),3,mean),nrow=nrow(tst)/ny)
>>    [,1] [,2] [,3] [,4]
>> [1,]  2.5 10.5 18.5 26.5
>> [2,]  6.5 14.5 22.5 30.5
>> 
>> 
>> cheers
>> Peter
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From sbigelow at jonesctr.org  Thu Jul 28 03:00:26 2016
From: sbigelow at jonesctr.org (Seth Bigelow)
Date: Wed, 27 Jul 2016 21:00:26 -0400
Subject: [R] [FORGED]  Lattice barchart legend with panel.barchart
In-Reply-To: <a390f286-5da7-1a8f-a1fb-fc83f49d1860@stat.auckland.ac.nz>
References: <CAEryiFuL0AyZwej6FwmwW66AUpON8Rz8sH44m1=02Q718NUVvQ@mail.gmail.com>
	<a390f286-5da7-1a8f-a1fb-fc83f49d1860@stat.auckland.ac.nz>
Message-ID: <CAEryiFs=EcFAbb1SwHa+Qafs+dmroRTTos_NRQSAu8+SjN3eOg@mail.gmail.com>

Works great, thank you Paul!

On Wed, Jul 27, 2016 at 5:24 PM, Paul Murrell <paul at stat.auckland.ac.nz>
wrote:

> Hi
>
> Try this ...
>
> barchart(Class~Freq|Sex + Age, Titan,
>          groups=Survived,
>          panel = titanpanel,
>          stack=TRUE, layout=c(4,1),
>          key=simpleKey(title="Survived", text=levels(Titan$Survived),
>                        rectangles=TRUE, points=FALSE, columns=2))
>
> Paul
>
> On 28/07/16 09:02, Seth Bigelow wrote:
>
>> I have constructed a barchart that requires a panel call, but the panel
>> reduces the facsimiles of bars in the legend to small colored circles. You
>> can see this behavior in the following example:
>>
>> Titan <- as.data.frame(Titanic)
>>
>> titanpanel <- function(x,y,...){
>> panel.barchart(x,y,...)
>> }
>>
>> barchart(Class~Freq|Sex + Age, Titan,
>> groups=Survived,
>> panel = titanpanel,
>> stack=TRUE, layout=c(4,1),
>> auto.key=list(title="Survived", columns=2))
>>
>> ...if you comment out the panel and run the barchart statement you will
>> see
>> nice blocks displayed in the legend. Is there any easy way to retain these
>> blocks with panel.barchart?
>>
>>
>>
> --
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
>



-- 
Seth W. Bigelow, Ph.D.
Assistant Scientist of Forest Ecology
Joseph W. Jones Ecological Research Center
Newton, GA
(229)-734-4706 x-270

	[[alternative HTML version deleted]]


From dulcalma at bigpond.com  Thu Jul 28 05:26:24 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Thu, 28 Jul 2016 13:26:24 +1000
Subject: [R] Lattice barchart legend with panel.barchart
In-Reply-To: <CAEryiFuL0AyZwej6FwmwW66AUpON8Rz8sH44m1=02Q718NUVvQ@mail.gmail.com>
References: <CAEryiFuL0AyZwej6FwmwW66AUpON8Rz8sH44m1=02Q718NUVvQ@mail.gmail.com>
Message-ID: <000601d1e87f$d1e8a370$75b9ea50$@bigpond.com>

Hi

Following Pauls reply you may like to increase the panel space available by
using useOuterStrips which makes the smaller proportions easier to see and
compare
If you also wanted to change colours auto.key is the better option. Colours
are a bit garish 
See
 ?xyplot
and 
panel.barchart

library(latticeExtra) 

useOuterStrips(
barchart(Class~Freq|Sex + Age, Titan,
          groups=Survived,
          panel = titanpanel,
          par.settings = list(strip.background = list(col = "transparent"),
                              superpose.polygon= list(col = c("red","blue"),
                                                      border =
c("red","blue"))),
          stack=TRUE, layout=c(4,1),
          auto.key=list(title="Survived", text=levels(Titan$Survived),
                        rectangles=TRUE, points=FALSE, columns=2)
)
)

Regards

Duncan
	
Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Seth Bigelow
Sent: Thursday, 28 July 2016 07:02
To: r-help at r-project.org
Subject: [R] Lattice barchart legend with panel.barchart

I have constructed a barchart that requires a panel call, but the panel
reduces the facsimiles of bars in the legend to small colored circles. You
can see this behavior in the following example:

Titan <- as.data.frame(Titanic)

titanpanel <- function(x,y,...){
panel.barchart(x,y,...)
}

barchart(Class~Freq|Sex + Age, Titan,
groups=Survived,
panel = titanpanel,
stack=TRUE, layout=c(4,1),
auto.key=list(title="Survived", columns=2))

...if you comment out the panel and run the barchart statement you will see
nice blocks displayed in the legend. Is there any easy way to retain these
blocks with panel.barchart?


-- 
Seth W. Bigelow, Ph.D.
Assistant Scientist of Forest Ecology
Joseph W. Jones Ecological Research Center
Newton, GA

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Thu Jul 28 05:43:20 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Thu, 28 Jul 2016 13:43:20 +1000
Subject: [R] font size in graphs...can R read Windows settings?
In-Reply-To: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>
References: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>
Message-ID: <000801d1e882$2f6e5f10$8e4b1d30$@bigpond.com>

Hi  Dan

For devices png, pdf, postscript and ? others the pointsize argument
controls the font size which is modified by cex

For lattice there are the settings in trellis.par.get()

trellis.par.get()$fontsize
$text
[1] 12

$points
[1] 8

which you can set and there is no real need to change font size except if
you need to change main.
trellis.par.get()$grid.pars  are the settings for grid elements if used  eg
text

these could be set globally by trellis.par.set() or individually with
argument par.settings eg
xyplot(y ~ x, data = datm,
             par.settings = list(strip.background = list(col =
"transparent"),
                                                 fontsize = list(text = 16,
 
points = 12),  # large size;  need to refine   
                                                superpose.polygon= list(col
= c("red","blue"),
 
border = c("red","blue"))),              
            type = "b")

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dalthorp,
Daniel
Sent: Thursday, 28 July 2016 07:02
To: r-help at R-project.org (r-help at r-project.org)
Subject: [R] font size in graphs...can R read Windows settings?

Hi All,
I am putting together a package that (among other things) draws some nice
graphs for users. I place some explanatory text on figs using "text" and
"mtext". But the size of the text depends on the Windows display settings:
Smaller (100%), medium (125%) or larger (150%) (In Windows 7... Control
panel | Appearance and personalization | Display | Make text and other
items smaller or larger). If I create figs that look good with one setting,
the text is too big (or too small) if another setting is used in Windows.
If I know the Windows setting, I can use cex in R to make the right sized
labels, but if I don't know a user's Windows display size setting...is
there a way for R to read the setting? Or is there another way to control
label size so that text labels on graphs look good regardless of WIndows
display size setting?

Many thanks for Ideas,

-Dan
-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ramkishore31 at gmail.com  Thu Jul 28 07:37:45 2016
From: ramkishore31 at gmail.com (Ramkishore Swaminathan)
Date: Wed, 27 Jul 2016 22:37:45 -0700
Subject: [R] Example for multivariate hawkes distribution
Message-ID: <CAHG51U2kR7oLP-ED=2KAo_O0X_6gHXg48rxkgfgKMEb3zh35PA@mail.gmail.com>

Hi,

I have a list of N categories that a user can click on. Lets say there are
K such users totally. I have the past 3 months data which tells which user
has clicked on which category on which date for how many times. For ex -
{20th June 2016 : [10,15,12,15]} this dict is for a particular user and
says on 20th June he clicked on categories 10,12 once and 15 twice.

Given this data, I want to use a Multivariate Hawkes distribution to model
this, so that I can predict which categories a user will click on based on
the past categories(same and different categories) that have been clicked.

I have already looked at a number of examples.
http://jheusser.github.io/2013/09/08/hawkes.html uses a univariate Hawkes
distribution using ptproc package. ptproc however, doesn't exist now(not
able to install it in R studio).

Using the hawkes package,

lambda0<-c(0.2,0.2)
alpha<-matrix(c(0.5,0,0,0.5),byrow=TRUE,nrow=2)
beta<-c(0.7,0.7)
history<-simulateHawkes(lambda0,alpha,beta,3600)
l<-likelihoodHawkes(lambda0,alpha,beta,history)

This computes the likelihood for some random initialization of parameters.
How do I find the best parameters by using EM algorithm that maximizes
likelihood here for Multivariate Hawkes distribution ?

I want to feed some random initialization of the mean, alpha and beta
parameters and want the model to perform Maximum Likelihood estimation
using EM algorithm for Multivariate Hawkes distribution to find the best
values of parameters and return it.

Can you explain how its done using which package with an example?

Any help is appreciated!
Thanks!

Ramkishore

	[[alternative HTML version deleted]]


From jepusto at gmail.com  Wed Jul 27 16:29:16 2016
From: jepusto at gmail.com (James Pustejovsky)
Date: Wed, 27 Jul 2016 09:29:16 -0500
Subject: [R] [R-pkgs] new package clubSandwich: Cluster-Robust (Sandwich)
 Variance Estimators with Small-Sample Corrections
Message-ID: <CAFUVuJwJ1-Uk5KJHYv0L+FEVk=WKwe2yPvL9C=wxCoy_WzhgKw@mail.gmail.com>

Dear R users:

I'm happy to announce the first CRAN release of the clubSandwich package:

https://cran.r-project.org/web/packages/clubSandwich

clubSandwich provides several variants of the cluster-robust variance
estimator for ordinary and weighted least squares linear regression models,
including the bias-reduced linearization estimator of Bell and McCaffrey
(2002). The package includes functions for estimating the
variance-covariance matrix and for testing single- and multiple-contrast
hypotheses based on Wald test statistics. The hypothesis tests incorporate
small-sample corrections that lead to more accurate rejection rates when
the number of clusters is small or the design is unbalanced/leveraged.
Tests of single regression coefficients use Satterthwaite or saddle-point
corrections. Tests of multiple-contrast hypotheses use an approximation to
Hotelling's T-squared distribution. Methods are provided for a variety of
fitted models, including lm(), plm() (from package 'plm'), gls() and lme()
(from 'nlme'), robu() (from 'robumeta'), and rma.uni() and rma.mv() (from
'metafor').

The package includes two vignettes that demonstrate its use for estimation
of panel data models and meta-regression models.

Bug reports, suggestions, and feature requests are welcome
at https://github.com/jepusto/clubSandwich

Cheers,
James


___________________________________________
James Pustejovsky
Assistant Professor, Quantitative Methods Program
Educational Psychology Department
The University of Texas at Austin
http://jepusto.github.io/

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From bbagy81 at gmail.com  Thu Jul 28 08:54:58 2016
From: bbagy81 at gmail.com (=?utf-8?B?67CV7Z2s6rWt?=)
Date: Thu, 28 Jul 2016 00:54:58 -0600
Subject: [R] Please advice me about nls.lm
Message-ID: <8B239067-0C98-4ED0-B970-C61BEE390F4E@gmail.com>

Dear Sir/Madam


Dear

Hello, I am a student in South Korea, and I would like to analysis genome converge and fit curve using nls.lm in R. But I am a very beginner user R. So this is not easy for me. I try to find  and implement many example and tutorial. But it dose not help for me.

And I finally find expert like you. I think this is great opportunity for me to learn high technology. 
I want to do this using ?FT0011.txt?, and finally I want to see like "011.pdf?. This figure is from Prism. But I can not use this. Because I need max and min value from fitted curve. But this result does not show about those value. More important this is that I am not sure it is correct. I can't totally understand about par and fn.


I apologies if I am rude. English is not familiar to me. I needed brave for sending email for you.  I believe I could get advice from you.



Thank for your time, you have a great day.

Sincerely,

Hee kuk Park,



________________________________








From pdalgd at gmail.com  Thu Jul 28 11:15:58 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 28 Jul 2016 11:15:58 +0200
Subject: [R] Please advice me about nls.lm
In-Reply-To: <8B239067-0C98-4ED0-B970-C61BEE390F4E@gmail.com>
References: <8B239067-0C98-4ED0-B970-C61BEE390F4E@gmail.com>
Message-ID: <1B4FC5E2-A98A-4F80-8BC1-921DCC8EBC86@gmail.com>

I think you need someone who speaks your own language. Have a look at 

https://www.facebook.com/groups/KoreaRUsers/

- Peter D.


> On 28 Jul 2016, at 08:54 , ??? <bbagy81 at gmail.com> wrote:
> 
> Dear Sir/Madam
> 
> 
> Dear
> 
> Hello, I am a student in South Korea, and I would like to analysis genome converge and fit curve using nls.lm in R. But I am a very beginner user R. So this is not easy for me. I try to find  and implement many example and tutorial. But it dose not help for me.
> 
> And I finally find expert like you. I think this is great opportunity for me to learn high technology. 
> I want to do this using ?FT0011.txt?, and finally I want to see like "011.pdf?. This figure is from Prism. But I can not use this. Because I need max and min value from fitted curve. But this result does not show about those value. More important this is that I am not sure it is correct. I can't totally understand about par and fn.
> 
> 
> I apologies if I am rude. English is not familiar to me. I needed brave for sending email for you.  I believe I could get advice from you.
> 
> 
> 
> Thank for your time, you have a great day.
> 
> Sincerely,
> 
> Hee kuk Park,
> 
> 
> 
> ________________________________
> 
> 
> 
> 
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From G.Maubach at weinwolf.de  Thu Jul 28 12:32:17 2016
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Thu, 28 Jul 2016 12:32:17 +0200
Subject: [R] Spread data.frame on 2 variables
Message-ID: <OFEF3872D0.FF96D82C-ONC1257FFE.00390EC0-C1257FFE.0039ED76@lotus.hawesko.de>

Hi All,

I need to spread a data.frame on 2 variables, e. g. "channel" and "unit".

If I do it in two steps spreads keeps all cases that does not look like 
the one before although it contains the same values for a specific case.

Here is what I have right now:

-- cut --

test1$dummy <- 1
test2 <- spread(data = test1, key = 'channel', value = "dummy")
test2
cat("First spread is OK!")

test2$dummy <- 1
test3 <- spread(data = test2, key = 'unit', value = 'dummy')

test1
# test2
test3
warning(paste0("Second spread is not OK cause spread does not merge 
cases\n",
               "with CustID 700 and 800 into one case,\n",
               "cause they have values on different variables,\n",
               "although the corresponding values of the cases with",
               "custID 700 and 800 are missing."))

cat("What I would like to have is:\n")
target4 <- structure(list(custID = c(100, 200, 300, 500, 600, 700, 800, 
900),
  `10` = c(1, NA, NA, NA, NA, NA, NA, NA),
  `20` = c(1, NA, NA, NA, NA, NA, NA, NA), 
  `30` = c(NA, NA, NA, NA, NA, NA, 1, 1),
  `40` = c(NA, NA, NA, NA, 1, NA, 1, 1),
  `50` = c(NA, NA, 1, NA, NA, NA, 1, 1), 
  `60` = c(NA, NA, NA, NA, NA, 1, NA, NA),
  `70` = c(NA, NA, NA, NA, NA, 1, NA, NA), 
  `99` = c(NA, 1, NA, 1, NA, NA, NA, NA), 
  `1000` = c(1, NA, NA, NA, NA, NA, 1, 1), 
  `2000` = c(NA, NA, NA, NA, 1, 1, 1, NA),
  `3000` = c(NA, NA, 1, NA, NA, 1, NA, NA),
  `4000` = c(NA, NA, 1, NA, NA, NA, NA, NA),
  `6000` = c(NA, NA, NA, NA, 1, NA, NA, NA),
  `9999` = c(NA, 1, NA, 1, NA, NA, NA, NA)),
.Names = c("custID",
 "10",  "20",  "30",  "40",  "50",  "60",  "70",  "99", 
 "1000",  "2000",  "3000",  "4000",  "6000",  "9999"),
row.names = c(NA, 8L), class = "data.frame")

target4

cat("What would be a proper way to create target4 from test1?")

-- cut --

What would be the proper way to create target4 from test1?

Kind regards

Georg


From stefan.kruger at gmail.com  Thu Jul 28 12:55:54 2016
From: stefan.kruger at gmail.com (Stefan Kruger)
Date: Thu, 28 Jul 2016 11:55:54 +0100
Subject: [R] Reduce woes
In-Reply-To: <32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
Message-ID: <CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>

David - many thanks for your response.

What I tried to do was to turn

data <- list(one = c(1, 1), three = c(3), two = c(2, 2))

into

result <- list(one = 2, three = 1, two = 2)

that is creating a new list which has the same names as the first, but
where the values are the vector lengths.

I know there are many other (and better) trivial ways of achieving this -
my aim is less the task itself, and more figuring out if this can be done
using Reduce() in the fashion I showed in the other examples I gave. It's a
building block of doing map-filter-reduce type pipelines that I'd like to
understand how to do in R.

Fumbling in the dark, I tried:

Reduce(function(acc, item) { setNames(c(acc, length(data[item])), item },
names(data), accumulate=TRUE)

but setNames sets all the names, not adding one - and acc is still a
vector, not a list.

It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()' aim at
doing what I'd like to do - but I've not been able to figure out quite how.

Thanks

Stefan



On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net> wrote:

>
> > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <stefan.kruger at gmail.com>
> wrote:
> >
> > Hi -
> >
> > I'm new to R.
> >
> > In other functional languages I'm familiar with you can often seed a call
> > to reduce() with a custom accumulator. Here's an example in Elixir:
> >
> > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
> > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
> > Enum.count(v), nil) end)
> > # %{"one" => 2, "three" => 1, "two" => 2}
> >
> > In R-terms that's reducing a list of vectors to become a new list mapping
> > the names to the vector lengths.
> >
> > Even in JavaScript, you can do similar things:
> >
> > list = { one: [1, 1], three: [3], two: [2, 2] };
> > var result = Object.keys(list).reduceRight(function (acc, item) {
> >  acc[item] = list[item].length;
> >  return acc;
> > }, {});
> > // result == { two: 2, three: 1, one: 2 }
> >
> > In R, from what I can gather, Reduce() is restricted such that any init
> > value you feed it is required to be of the same type as the elements of
> the
> > vector you're reducing -- so I can't build up. So whilst I can do, say
> >
> >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
> > [1] 111
> >
> > I can't use Reduce to build up a list, vector or data frame?
> >
> > What am I missing?
> >
> > Many thanks for any pointers,
>
> This builds a list:
>
> > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
> accumulate=TRUE)
> [[1]]
> [1] 96
>
> [[2]]
> [1] 96  1
>
> [[3]]
> [1] 96  1  2
>
> [[4]]
> [1] 96  1  2  3
>
> [[5]]
> [1] 96  1  2  3  4
>
> [[6]]
> [1] 96  1  2  3  4  5
>
> But you are not saying what you want. The other examples were doing
> something with names but you provided no names for the R example.
>
> This would return a list of named vectors:
>
> > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))  },
> c(1,2,3,4,5), 96, accumulate=TRUE)
> [[1]]
> [1] 96
>
> [[2]]
>  1  2
> 96  1
>
> [[3]]
>  1  2  3
> 96  1  2
>
> [[4]]
>  1  2  3  4
> 96  1  2  3
>
> [[5]]
>  1  2  3  4  5
> 96  1  2  3  4
>
> [[6]]
>  1  2  3  4  5  6
> 96  1  2  3  4  5
>
>
>
>
> > Stefan
> >
> >
> >
> > --
> > Stefan Kruger <stefan.kruger at gmail.com>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>


-- 
Stefan Kruger <stefan.kruger at gmail.com>

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Thu Jul 28 13:00:52 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Thu, 28 Jul 2016 11:00:52 +0000
Subject: [R] Spread data.frame on 2 variables
In-Reply-To: <OFEF3872D0.FF96D82C-ONC1257FFE.00390EC0-C1257FFE.0039ED76@lotus.hawesko.de>
References: <OFEF3872D0.FF96D82C-ONC1257FFE.00390EC0-C1257FFE.0039ED76@lotus.hawesko.de>
Message-ID: <CAKVAULNG9E89v+gP0iTgX+03uw4SdnW4kKxvHD7yqkYk=zaYww@mail.gmail.com>

Hi Georg,

it's hard to tell without a reproducible example.

Should spread really merge elements? Does spread know anything about
CustID? Maybe you need to make a useful key of the CustIDs first and spread
on that?

Maybe I'm all off, because I'm really just guessing.

Best,
Ulrik

On Thu, 28 Jul 2016 at 12:36 <G.Maubach at weinwolf.de> wrote:

> Hi All,
>
> I need to spread a data.frame on 2 variables, e. g. "channel" and "unit".
>
> If I do it in two steps spreads keeps all cases that does not look like
> the one before although it contains the same values for a specific case.
>
> Here is what I have right now:
>
> -- cut --
>
> test1$dummy <- 1
> test2 <- spread(data = test1, key = 'channel', value = "dummy")
> test2
> cat("First spread is OK!")
>
> test2$dummy <- 1
> test3 <- spread(data = test2, key = 'unit', value = 'dummy')
>
> test1
> # test2
> test3
> warning(paste0("Second spread is not OK cause spread does not merge
> cases\n",
>                "with CustID 700 and 800 into one case,\n",
>                "cause they have values on different variables,\n",
>                "although the corresponding values of the cases with",
>                "custID 700 and 800 are missing."))
>
> cat("What I would like to have is:\n")
> target4 <- structure(list(custID = c(100, 200, 300, 500, 600, 700, 800,
> 900),
>   `10` = c(1, NA, NA, NA, NA, NA, NA, NA),
>   `20` = c(1, NA, NA, NA, NA, NA, NA, NA),
>   `30` = c(NA, NA, NA, NA, NA, NA, 1, 1),
>   `40` = c(NA, NA, NA, NA, 1, NA, 1, 1),
>   `50` = c(NA, NA, 1, NA, NA, NA, 1, 1),
>   `60` = c(NA, NA, NA, NA, NA, 1, NA, NA),
>   `70` = c(NA, NA, NA, NA, NA, 1, NA, NA),
>   `99` = c(NA, 1, NA, 1, NA, NA, NA, NA),
>   `1000` = c(1, NA, NA, NA, NA, NA, 1, 1),
>   `2000` = c(NA, NA, NA, NA, 1, 1, 1, NA),
>   `3000` = c(NA, NA, 1, NA, NA, 1, NA, NA),
>   `4000` = c(NA, NA, 1, NA, NA, NA, NA, NA),
>   `6000` = c(NA, NA, NA, NA, 1, NA, NA, NA),
>   `9999` = c(NA, 1, NA, 1, NA, NA, NA, NA)),
> .Names = c("custID",
>  "10",  "20",  "30",  "40",  "50",  "60",  "70",  "99",
>  "1000",  "2000",  "3000",  "4000",  "6000",  "9999"),
> row.names = c(NA, 8L), class = "data.frame")
>
> target4
>
> cat("What would be a proper way to create target4 from test1?")
>
> -- cut --
>
> What would be the proper way to create target4 from test1?
>
> Kind regards
>
> Georg
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ulrik.stervbo at gmail.com  Thu Jul 28 13:03:52 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Thu, 28 Jul 2016 11:03:52 +0000
Subject: [R] Reduce woes
In-Reply-To: <CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
Message-ID: <CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>

Hi Stefan,

in that case,lapply(data, length) should do the trick.

Best wishes,
Ulrik

On Thu, 28 Jul 2016 at 12:57 Stefan Kruger <stefan.kruger at gmail.com> wrote:

> David - many thanks for your response.
>
> What I tried to do was to turn
>
> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>
> into
>
> result <- list(one = 2, three = 1, two = 2)
>
> that is creating a new list which has the same names as the first, but
> where the values are the vector lengths.
>
> I know there are many other (and better) trivial ways of achieving this -
> my aim is less the task itself, and more figuring out if this can be done
> using Reduce() in the fashion I showed in the other examples I gave. It's a
> building block of doing map-filter-reduce type pipelines that I'd like to
> understand how to do in R.
>
> Fumbling in the dark, I tried:
>
> Reduce(function(acc, item) { setNames(c(acc, length(data[item])), item },
> names(data), accumulate=TRUE)
>
> but setNames sets all the names, not adding one - and acc is still a
> vector, not a list.
>
> It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()' aim at
> doing what I'd like to do - but I've not been able to figure out quite how.
>
> Thanks
>
> Stefan
>
>
>
> On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net> wrote:
>
> >
> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <stefan.kruger at gmail.com>
> > wrote:
> > >
> > > Hi -
> > >
> > > I'm new to R.
> > >
> > > In other functional languages I'm familiar with you can often seed a
> call
> > > to reduce() with a custom accumulator. Here's an example in Elixir:
> > >
> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
> > > Enum.count(v), nil) end)
> > > # %{"one" => 2, "three" => 1, "two" => 2}
> > >
> > > In R-terms that's reducing a list of vectors to become a new list
> mapping
> > > the names to the vector lengths.
> > >
> > > Even in JavaScript, you can do similar things:
> > >
> > > list = { one: [1, 1], three: [3], two: [2, 2] };
> > > var result = Object.keys(list).reduceRight(function (acc, item) {
> > >  acc[item] = list[item].length;
> > >  return acc;
> > > }, {});
> > > // result == { two: 2, three: 1, one: 2 }
> > >
> > > In R, from what I can gather, Reduce() is restricted such that any init
> > > value you feed it is required to be of the same type as the elements of
> > the
> > > vector you're reducing -- so I can't build up. So whilst I can do, say
> > >
> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
> > > [1] 111
> > >
> > > I can't use Reduce to build up a list, vector or data frame?
> > >
> > > What am I missing?
> > >
> > > Many thanks for any pointers,
> >
> > This builds a list:
> >
> > > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
> > accumulate=TRUE)
> > [[1]]
> > [1] 96
> >
> > [[2]]
> > [1] 96  1
> >
> > [[3]]
> > [1] 96  1  2
> >
> > [[4]]
> > [1] 96  1  2  3
> >
> > [[5]]
> > [1] 96  1  2  3  4
> >
> > [[6]]
> > [1] 96  1  2  3  4  5
> >
> > But you are not saying what you want. The other examples were doing
> > something with names but you provided no names for the R example.
> >
> > This would return a list of named vectors:
> >
> > > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))  },
> > c(1,2,3,4,5), 96, accumulate=TRUE)
> > [[1]]
> > [1] 96
> >
> > [[2]]
> >  1  2
> > 96  1
> >
> > [[3]]
> >  1  2  3
> > 96  1  2
> >
> > [[4]]
> >  1  2  3  4
> > 96  1  2  3
> >
> > [[5]]
> >  1  2  3  4  5
> > 96  1  2  3  4
> >
> > [[6]]
> >  1  2  3  4  5  6
> > 96  1  2  3  4  5
> >
> >
> >
> >
> > > Stefan
> > >
> > >
> > >
> > > --
> > > Stefan Kruger <stefan.kruger at gmail.com>
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
>
>
> --
> Stefan Kruger <stefan.kruger at gmail.com>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Joanne.Thomas at jbarisk.com  Thu Jul 28 13:19:00 2016
From: Joanne.Thomas at jbarisk.com (Joanne Thomas)
Date: Thu, 28 Jul 2016 11:19:00 +0000
Subject: [R] SegFault when trying to install an inhouse package
Message-ID: <AM4PR05MB18910BD08CA72567E5B6CEB5FC000@AM4PR05MB1891.eurprd05.prod.outlook.com>

Hi,

Apologies if something similar has come up but I have been unable to find it in the archive.

The company I work for has an issue with a package they've developed called ESVP. This is dependent on OpenSM, which I'm told is already somewhat delicate with respect to calls on rJava. When trying to install ESVP on a number of Scientific Linux 7.2 servers (with R 3.3.0), I get the following results:

> install.packages("/data/Jupiter/GFES/Packages/ESVP_1.2.2.tar.gz", repo=NULL, type="source")
 Installing package into ?/home/joannethomas at jbanorthwest.co.uk/R/x86_64-redhat-linux-gnu-library/3.3?
(as ?lib? is unspecified)
 * installing *source* package ?ESVP? ...
 ** libs
 g++ -m64 -I/usr/include/R -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/Rcpp/include"   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c GreatCircleDistances.cpp -o GreatCircleDistances.o
 g++ -m64 -I/usr/include/R -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/Rcpp/include"   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c RcppExports.cpp -o RcppExports.o
 g++ -m64 -shared -L/usr/lib64/R/lib -Wl,-z,relro -o ESVP.so GreatCircleDistances.o RcppExports.o -L/usr/lib64/R/lib -lR
 installing to /home/joannethomas at jbanorthwest.co.uk/R/x86_64-redhat-linux-gnu-library/3.3/ESVP/libs
 ** R
 ** preparing package for lazy loading
 Warning in rgl.init(initValue, onlyNULL) :
   RGL: unable to open X11 display
 Warning: 'rgl_init' failed, running with rgl.useNULL = TRUE
 /usr/lib64/R/bin/INSTALL: line 34: 51703 Done                    echo 'tools:::.install_packages()'
      51704 Segmentation fault      (core dumped) | R_DEFAULT_PACKAGES= LC_COLLATE=C "${R_HOME}/bin/R" $myArgs --slave --args ${args}
 Warning in install.packages :
   installation of package ?/data/Jupiter/GFES/Packages/ESVP_1.2.2.tar.gz? had non-zero exit status

We also have some servers running R 3.2.3, the package installs, although the X11 warning also occurs:

> install.packages("/data/GFES/Packages/ESVP_1.2.2.tar.gz", repo=NULL, type="source")
 Installing package into ?/home/jbanorthwest.co.uk/joannethomas/R/x86_64-redhat-linux-gnu-library/3.2?
(as ?lib? is unspecified)
 * installing *source* package ?ESVP? ...
 ** libs
 g++ -m64 -I/usr/include/R -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/Rcpp/include"   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c GreatCircleDistances.cpp -o GreatCircleDistances.o
 g++ -m64 -I/usr/include/R -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/Rcpp/include"   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c RcppExports.cpp -o RcppExports.o
 g++ -m64 -shared -L/usr/lib64/R/lib -Wl,-z,relro -o ESVP.so GreatCircleDistances.o RcppExports.o -L/usr/lib64/R/lib -lR
 installing to /home/jbanorthwest.co.uk/joannethomas/R/x86_64-redhat-linux-gnu-library/3.2/ESVP/libs
 ** R
 ** preparing package for lazy loading
 Note: the specification for S3 class ?family? in package ?MatrixModels? seems equivalent to one from package ?lme4?: not turning on duplicate class definitions for this class.
 No protocol specified
 Warning in rgl.init(initValue, onlyNULL) :
   RGL: unable to open X11 display
 Warning: 'rgl_init' failed, running with rgl.useNULL = TRUE
 ** help
 *** installing help indices
   converting help for package ?ESVP?
    finding HTML links ... done
     CheckContinuity                         html
     CheckEventDur                           html
     CheckEventFreq                          html
     CheckEventSeas                          html
     CheckEventSpCov                         html
     CheckEventWaitTime                      html
     CheckMarDistr                           html
     CheckMaxInt                             html
     CheckMaxRp                              html
     CheckNumExceed                          html
     CheckPairDep                            html
     CheckRrm                                html
     CheckRrmAtt                             html
     CheckRrmFit                             html
     CheckTailDep                            html
     CheckTcDist                             html
     CheckTcLife                             html
     CheckTcSpDistr                          html
     CheckTcTrack                            html
     CheckTimeSeries                         html
     CheckTrigDens                           html
     ESVP                                    html
     PlotCatchment                           html
     PlotNearestNeighbours                   html
     PlotTransProb                           html
     RunAll                                  html
     VerifyEventDef                          html
 ** building package indices
 ** testing if installed package can be loaded
 Note: the specification for S3 class ?family? in package ?MatrixModels? seems equivalent to one from package ?lme4?: not turning on duplicate class definitions for this class.
 No protocol specified
 Warning in rgl.init(initValue, onlyNULL) :
   RGL: unable to open X11 display
 Warning: 'rgl_init' failed, running with rgl.useNULL = TRUE
 * DONE (ESVP)

Aside from R, installed Linux packages are the same versions and the library of R packages is identical.

Does anyone have any suggestions on what is causing the segfault (a jump to an invalid memory address).

Thanks,
Jo

?

Find out more about us here: www.jbarisk.com<http://www.jbarisk.com/> and follow us on Twitter @JBARisk<http://twitter.com/JBARisk> and LinkedIn<https://www.linkedin.com/company/2370847?trk=tyah&trkInfo=clickedVertical%3Acompany%2CclickedEntityId%3A2370847%2Cidx%3A2-1-2%2CtarId%3A1447414259786%2Ctas%3AJBA%20RISK%20MANAGEMENT>

The JBA Group supports the JBA Trust.

All JBA Risk Management's email messages contain confidential information and are intended only for the individual(s) named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail.
Please notify the sender immediately by email if you have received this email by mistake and delete this email from your system.


JBA Risk Management Limited is registered in England, company number 07732946, South Barn, Broughton Hall, Skipton, North Yorkshire, BD23 3AE, Telephone: +441756799919



From stefan.kruger at gmail.com  Thu Jul 28 14:09:51 2016
From: stefan.kruger at gmail.com (Stefan Kruger)
Date: Thu, 28 Jul 2016 13:09:51 +0100
Subject: [R] Reduce woes
In-Reply-To: <CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
	<CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>
Message-ID: <CAG7vnkyKwkXY-5J1svBK=xot_iDdrdBq5+ryUzAqahg0TAKGEQ@mail.gmail.com>

Ulrik - many thanks for your reply.

I'm aware of many simple solutions as the one you suggest, both iterative
and functional style - but I'm trying to learn how to bend Reduce() for the
purpose of using it in more complex processing tasks. What I'm trying to
work out is how to have the accumulator in Reduce not be the same type as
the elements of the vector/list being reduced - ideally it could be an S3
instance, list, vector, or data frame.

Here's a more realistic example (in Elixir, sorry)

Given two lists:

1. data: maps an id string to a vector of revision strings
2. dict: maps known id/revision pairs as a string to true (or 1)

find the items in data not already in dict, returned as a named list.

```elixir
data = %{
    "id1" => ["rev1.1", "rev1.2"],
    "id2" => ["rev2.1"],
    "id3" => ["rev3.1", "rev3.2", "rev3.3"]
}

dict = %{
    "id1/rev1.1" => 1,
    "id1/rev1.2" => 1,
    "id3/rev3.1" => 1
}

# Find the items in data not already in dict. Return as a grouped map

Map.keys(data)
    |> Enum.flat_map(fn id -> Enum.map(data[id], fn rev -> {id, rev} end)
end)
    |> Enum.filter(fn {id, rev} -> !Dict.has_key?(dict, "#{id}/#{rev}") end)
    |> Enum.reduce(%{}, fn ({k, v}, d) -> Map.update(d, k, [v], &[v|&1])
end)
```




On 28 July 2016 at 12:03, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:

> Hi Stefan,
>
> in that case,lapply(data, length) should do the trick.
>
> Best wishes,
> Ulrik
>
> On Thu, 28 Jul 2016 at 12:57 Stefan Kruger <stefan.kruger at gmail.com>
> wrote:
>
>> David - many thanks for your response.
>>
>> What I tried to do was to turn
>>
>> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>>
>> into
>>
>> result <- list(one = 2, three = 1, two = 2)
>>
>> that is creating a new list which has the same names as the first, but
>> where the values are the vector lengths.
>>
>> I know there are many other (and better) trivial ways of achieving this -
>> my aim is less the task itself, and more figuring out if this can be done
>> using Reduce() in the fashion I showed in the other examples I gave. It's
>> a
>> building block of doing map-filter-reduce type pipelines that I'd like to
>> understand how to do in R.
>>
>> Fumbling in the dark, I tried:
>>
>> Reduce(function(acc, item) { setNames(c(acc, length(data[item])), item },
>> names(data), accumulate=TRUE)
>>
>> but setNames sets all the names, not adding one - and acc is still a
>> vector, not a list.
>>
>> It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()' aim at
>> doing what I'd like to do - but I've not been able to figure out quite
>> how.
>>
>> Thanks
>>
>> Stefan
>>
>>
>>
>> On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net> wrote:
>>
>> >
>> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <stefan.kruger at gmail.com>
>> > wrote:
>> > >
>> > > Hi -
>> > >
>> > > I'm new to R.
>> > >
>> > > In other functional languages I'm familiar with you can often seed a
>> call
>> > > to reduce() with a custom accumulator. Here's an example in Elixir:
>> > >
>> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
>> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
>> > > Enum.count(v), nil) end)
>> > > # %{"one" => 2, "three" => 1, "two" => 2}
>> > >
>> > > In R-terms that's reducing a list of vectors to become a new list
>> mapping
>> > > the names to the vector lengths.
>> > >
>> > > Even in JavaScript, you can do similar things:
>> > >
>> > > list = { one: [1, 1], three: [3], two: [2, 2] };
>> > > var result = Object.keys(list).reduceRight(function (acc, item) {
>> > >  acc[item] = list[item].length;
>> > >  return acc;
>> > > }, {});
>> > > // result == { two: 2, three: 1, one: 2 }
>> > >
>> > > In R, from what I can gather, Reduce() is restricted such that any
>> init
>> > > value you feed it is required to be of the same type as the elements
>> of
>> > the
>> > > vector you're reducing -- so I can't build up. So whilst I can do, say
>> > >
>> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
>> > > [1] 111
>> > >
>> > > I can't use Reduce to build up a list, vector or data frame?
>> > >
>> > > What am I missing?
>> > >
>> > > Many thanks for any pointers,
>> >
>> > This builds a list:
>> >
>> > > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
>> > accumulate=TRUE)
>> > [[1]]
>> > [1] 96
>> >
>> > [[2]]
>> > [1] 96  1
>> >
>> > [[3]]
>> > [1] 96  1  2
>> >
>> > [[4]]
>> > [1] 96  1  2  3
>> >
>> > [[5]]
>> > [1] 96  1  2  3  4
>> >
>> > [[6]]
>> > [1] 96  1  2  3  4  5
>> >
>> > But you are not saying what you want. The other examples were doing
>> > something with names but you provided no names for the R example.
>> >
>> > This would return a list of named vectors:
>> >
>> > > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))  },
>> > c(1,2,3,4,5), 96, accumulate=TRUE)
>> > [[1]]
>> > [1] 96
>> >
>> > [[2]]
>> >  1  2
>> > 96  1
>> >
>> > [[3]]
>> >  1  2  3
>> > 96  1  2
>> >
>> > [[4]]
>> >  1  2  3  4
>> > 96  1  2  3
>> >
>> > [[5]]
>> >  1  2  3  4  5
>> > 96  1  2  3  4
>> >
>> > [[6]]
>> >  1  2  3  4  5  6
>> > 96  1  2  3  4  5
>> >
>> >
>> >
>> >
>> > > Stefan
>> > >
>> > >
>> > >
>> > > --
>> > > Stefan Kruger <stefan.kruger at gmail.com>
>> > >
>> > >       [[alternative HTML version deleted]]
>> > >
>> > > ______________________________________________
>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > > and provide commented, minimal, self-contained, reproducible code.
>> >
>> > David Winsemius
>> > Alameda, CA, USA
>> >
>> >
>>
>>
>> --
>> Stefan Kruger <stefan.kruger at gmail.com>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


-- 
Stefan Kruger <stefan.kruger at gmail.com>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Thu Jul 28 14:12:13 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 28 Jul 2016 08:12:13 -0400
Subject: [R] SegFault when trying to install an inhouse package
In-Reply-To: <AM4PR05MB18910BD08CA72567E5B6CEB5FC000@AM4PR05MB1891.eurprd05.prod.outlook.com>
References: <AM4PR05MB18910BD08CA72567E5B6CEB5FC000@AM4PR05MB1891.eurprd05.prod.outlook.com>
Message-ID: <94b81445-fba5-4684-4990-70fbacbb895f@gmail.com>

On 28/07/2016 7:19 AM, Joanne Thomas wrote:
> Hi,
>
> Apologies if something similar has come up but I have been unable to find it in the archive.
>
> The company I work for has an issue with a package they've developed called ESVP. This is dependent on OpenSM, which I'm told is already somewhat delicate with respect to calls on rJava. When trying to install ESVP on a number of Scientific Linux 7.2 servers (with R 3.3.0), I get the following results:
>
> > install.packages("/data/Jupiter/GFES/Packages/ESVP_1.2.2.tar.gz", repo=NULL, type="source")
>   Installing package into ?/home/joannethomas at jbanorthwest.co.uk/R/x86_64-redhat-linux-gnu-library/3.3?
> (as ?lib? is unspecified)
>   * installing *source* package ?ESVP? ...
>   ** libs
>   g++ -m64 -I/usr/include/R -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/Rcpp/include"   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c GreatCircleDistances.cpp -o GreatCircleDistances.o
>   g++ -m64 -I/usr/include/R -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/Rcpp/include"   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c RcppExports.cpp -o RcppExports.o
>   g++ -m64 -shared -L/usr/lib64/R/lib -Wl,-z,relro -o ESVP.so GreatCircleDistances.o RcppExports.o -L/usr/lib64/R/lib -lR
>   installing to /home/joannethomas at jbanorthwest.co.uk/R/x86_64-redhat-linux-gnu-library/3.3/ESVP/libs
>   ** R
>   ** preparing package for lazy loading
>   Warning in rgl.init(initValue, onlyNULL) :
>     RGL: unable to open X11 display
>   Warning: 'rgl_init' failed, running with rgl.useNULL = TRUE
>   /usr/lib64/R/bin/INSTALL: line 34: 51703 Done                    echo 'tools:::.install_packages()'
>        51704 Segmentation fault      (core dumped) | R_DEFAULT_PACKAGES= LC_COLLATE=C "${R_HOME}/bin/R" $myArgs --slave --args ${args}
>   Warning in install.packages :
>     installation of package ?/data/Jupiter/GFES/Packages/ESVP_1.2.2.tar.gz? had non-zero exit status
>
> We also have some servers running R 3.2.3, the package installs, although the X11 warning also occurs:
>
> > install.packages("/data/GFES/Packages/ESVP_1.2.2.tar.gz", repo=NULL, type="source")
>   Installing package into ?/home/jbanorthwest.co.uk/joannethomas/R/x86_64-redhat-linux-gnu-library/3.2?
> (as ?lib? is unspecified)
>   * installing *source* package ?ESVP? ...
>   ** libs
>   g++ -m64 -I/usr/include/R -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/Rcpp/include"   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c GreatCircleDistances.cpp -o GreatCircleDistances.o
>   g++ -m64 -I/usr/include/R -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/Rcpp/include"   -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c RcppExports.cpp -o RcppExports.o
>   g++ -m64 -shared -L/usr/lib64/R/lib -Wl,-z,relro -o ESVP.so GreatCircleDistances.o RcppExports.o -L/usr/lib64/R/lib -lR
>   installing to /home/jbanorthwest.co.uk/joannethomas/R/x86_64-redhat-linux-gnu-library/3.2/ESVP/libs
>   ** R
>   ** preparing package for lazy loading
>   Note: the specification for S3 class ?family? in package ?MatrixModels? seems equivalent to one from package ?lme4?: not turning on duplicate class definitions for this class.
>   No protocol specified
>   Warning in rgl.init(initValue, onlyNULL) :
>     RGL: unable to open X11 display
>   Warning: 'rgl_init' failed, running with rgl.useNULL = TRUE
>   ** help
>   *** installing help indices
>     converting help for package ?ESVP?
>      finding HTML links ... done
>       CheckContinuity                         html
>       CheckEventDur                           html
>       CheckEventFreq                          html
>       CheckEventSeas                          html
>       CheckEventSpCov                         html
>       CheckEventWaitTime                      html
>       CheckMarDistr                           html
>       CheckMaxInt                             html
>       CheckMaxRp                              html
>       CheckNumExceed                          html
>       CheckPairDep                            html
>       CheckRrm                                html
>       CheckRrmAtt                             html
>       CheckRrmFit                             html
>       CheckTailDep                            html
>       CheckTcDist                             html
>       CheckTcLife                             html
>       CheckTcSpDistr                          html
>       CheckTcTrack                            html
>       CheckTimeSeries                         html
>       CheckTrigDens                           html
>       ESVP                                    html
>       PlotCatchment                           html
>       PlotNearestNeighbours                   html
>       PlotTransProb                           html
>       RunAll                                  html
>       VerifyEventDef                          html
>   ** building package indices
>   ** testing if installed package can be loaded
>   Note: the specification for S3 class ?family? in package ?MatrixModels? seems equivalent to one from package ?lme4?: not turning on duplicate class definitions for this class.
>   No protocol specified
>   Warning in rgl.init(initValue, onlyNULL) :
>     RGL: unable to open X11 display
>   Warning: 'rgl_init' failed, running with rgl.useNULL = TRUE
>   * DONE (ESVP)
>
> Aside from R, installed Linux packages are the same versions and the library of R packages is identical.
>
> Does anyone have any suggestions on what is causing the segfault (a jump to an invalid memory address).

You can avoid the rgl.init problems by setting the environment variable 
RGL_USE_NULL to TRUE before running anything.  That should help on your 
3.2.3 servers.  It's possible that the segfault on the other system is 
related, but I'd guess it's something else.  Tracking down segfaults can 
be hard:  you'll need to understand how to look at the core dump, and do 
some low level debugging.

Duncan Murdoch
> Thanks,
> Jo
>
> ?
>
> Find out more about us here: www.jbarisk.com<http://www.jbarisk.com/> and follow us on Twitter @JBARisk<http://twitter.com/JBARisk> and LinkedIn<https://www.linkedin.com/company/2370847?trk=tyah&trkInfo=clickedVertical%3Acompany%2CclickedEntityId%3A2370847%2Cidx%3A2-1-2%2CtarId%3A1447414259786%2Ctas%3AJBA%20RISK%20MANAGEMENT>
>
> The JBA Group supports the JBA Trust.
>
> All JBA Risk Management's email messages contain confidential information and are intended only for the individual(s) named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail.
> Please notify the sender immediately by email if you have received this email by mistake and delete this email from your system.
>
>
> JBA Risk Management Limited is registered in England, company number 07732946, South Barn, Broughton Hall, Skipton, North Yorkshire, BD23 3AE, Telephone: +441756799919
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ulrik.stervbo at gmail.com  Thu Jul 28 14:21:55 2016
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Thu, 28 Jul 2016 12:21:55 +0000
Subject: [R] Spread data.frame on 2 variables
In-Reply-To: <OF2307C174.58155947-ONC1257FFE.0040A46E-C1257FFE.00415453@lotus.hawesko.de>
References: <OFEF3872D0.FF96D82C-ONC1257FFE.00390EC0-C1257FFE.0039ED76@lotus.hawesko.de>
	<CAKVAULNG9E89v+gP0iTgX+03uw4SdnW4kKxvHD7yqkYk=zaYww@mail.gmail.com>
	<OF2307C174.58155947-ONC1257FFE.0040A46E-C1257FFE.00415453@lotus.hawesko.de>
Message-ID: <CAKVAULMUPn0pcMMaurYD-n9H2wwXXfvdNR4niGOBDcDGFs0mLw@mail.gmail.com>

Hi Georg,

it is difficult to figure out what happens between your expectation and the
outcome if we cannot see a minimal dataset.

Based on your description I did this

library(tidyr)
library(dplyr)

test_df <- data_frame(channel = LETTERS[1:5], unit = letters[1:5], custID =
c(1:5), dummy = 1)
test_df %>% spread(channel, dummy) %>% mutate(dummy = 1) %>% spread(unit,
dummy)

which seems to be working fine as I get wide data. If a combination is
missing in the long form it will also be missing in the wide form. Maybe
you are looking for something like this:

channel_wide <- test_df  %>% select(channel, custID) %>% spread(channel,
custID)
unit_wide <- test_df  %>% select(unit, custID) %>% spread(unit, custID)
bind_cols(channel_wide, unit_wide)

Apologies for the HTML - it's gmail

Best wishes,
Ulrik

On Thu, 28 Jul 2016 at 13:54 <G.Maubach at weinwolf.de> wrote:

> Hi Ulrik,
>
> I have included a reproducable example. I ran the code and it did exactly
> what I wanted to show you.
>
> You are right: the solution shall merge cases in the end cause the values
> on the variables are either missing or the same.
>
> Example 1: Values are the same
> If you look at 6 and 7 and variable 70 the value is 1 in both cases. This
> is in this context the same information and cases 6 and 7 with custID can
> be merged to 1 for variable 70.
>
> Example 2: Values are missing and not missing
> If you look at cases 8 and 9 the value for case 8 at variable 40, 50 and
> 2000 is missing whereas the variables 40, 50 and 2000 have all 1 for case
> 9. Case 8 and 9 could be merged together cause the missing values are
> overwritten what is correct in this case.
>
> The solution I am looking for is to transform the data from long into wide
> form and keep all but missing value information.
>
> Did I explain my problem in a comprehensible way? Are there any further
> questions?
>
> Kind regards
>
> Georg
>
>
>
>
>
> Von:    Ulrik Stervbo <ulrik.stervbo at gmail.com>
> An:     G.Maubach at weinwolf.de, r-help at r-project.org,
> Datum:  28.07.2016 12:59
> Betreff:        Re: [R] Spread data.frame on 2 variables
>
>
>
> Hi Georg,
>
> it's hard to tell without a reproducible example.
>
> Should spread really merge elements? Does spread know anything about
> CustID? Maybe you need to make a useful key of the CustIDs first and
> spread on that?
>
> Maybe I'm all off, because I'm really just guessing.
>
> Best,
> Ulrik
>
> On Thu, 28 Jul 2016 at 12:36 <G.Maubach at weinwolf.de> wrote:
> Hi All,
>
> I need to spread a data.frame on 2 variables, e. g. "channel" and "unit".
>
> If I do it in two steps spreads keeps all cases that does not look like
> the one before although it contains the same values for a specific case.
>
> Here is what I have right now:
>
> -- cut --
>
> test1$dummy <- 1
> test2 <- spread(data = test1, key = 'channel', value = "dummy")
> test2
> cat("First spread is OK!")
>
> test2$dummy <- 1
> test3 <- spread(data = test2, key = 'unit', value = 'dummy')
>
> test1
> # test2
> test3
> warning(paste0("Second spread is not OK cause spread does not merge
> cases\n",
>                "with CustID 700 and 800 into one case,\n",
>                "cause they have values on different variables,\n",
>                "although the corresponding values of the cases with",
>                "custID 700 and 800 are missing."))
>
> cat("What I would like to have is:\n")
> target4 <- structure(list(custID = c(100, 200, 300, 500, 600, 700, 800,
> 900),
>   `10` = c(1, NA, NA, NA, NA, NA, NA, NA),
>   `20` = c(1, NA, NA, NA, NA, NA, NA, NA),
>   `30` = c(NA, NA, NA, NA, NA, NA, 1, 1),
>   `40` = c(NA, NA, NA, NA, 1, NA, 1, 1),
>   `50` = c(NA, NA, 1, NA, NA, NA, 1, 1),
>   `60` = c(NA, NA, NA, NA, NA, 1, NA, NA),
>   `70` = c(NA, NA, NA, NA, NA, 1, NA, NA),
>   `99` = c(NA, 1, NA, 1, NA, NA, NA, NA),
>   `1000` = c(1, NA, NA, NA, NA, NA, 1, 1),
>   `2000` = c(NA, NA, NA, NA, 1, 1, 1, NA),
>   `3000` = c(NA, NA, 1, NA, NA, 1, NA, NA),
>   `4000` = c(NA, NA, 1, NA, NA, NA, NA, NA),
>   `6000` = c(NA, NA, NA, NA, 1, NA, NA, NA),
>   `9999` = c(NA, 1, NA, 1, NA, NA, NA, NA)),
> .Names = c("custID",
>  "10",  "20",  "30",  "40",  "50",  "60",  "70",  "99",
>  "1000",  "2000",  "3000",  "4000",  "6000",  "9999"),
> row.names = c(NA, 8L), class = "data.frame")
>
> target4
>
> cat("What would be a proper way to create target4 from test1?")
>
> -- cut --
>
> What would be the proper way to create target4 from test1?
>
> Kind regards
>
> Georg
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From arthur.stilben at gmail.com  Thu Jul 28 15:16:48 2016
From: arthur.stilben at gmail.com (Arthur Stilben)
Date: Thu, 28 Jul 2016 10:16:48 -0300
Subject: [R] Fuzzy variable universe
Message-ID: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>

Hello, everyone!

Is there a way to set universe group for each fuzzy_variable? I try to do this:

> teste = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 ) ), universe = seq( from = 0, to= 10, by = 0.1 ) )


But when I print teste:

> teste
A fuzzy variable with values: a, universe


So, how can I set universe on fuzzy_variable?

-- 
Arthur Rodrigues Stilben
Geoinform?tica - LENEP
(22) 2765-6555


From roger.bos at rothschild.com  Thu Jul 28 15:14:28 2016
From: roger.bos at rothschild.com (Bos, Roger)
Date: Thu, 28 Jul 2016 13:14:28 +0000
Subject: [R] problems reading XML type file from ishares website
Message-ID: <0765308CD028654885F30322557308D82008D92B@NYCSM0208.rth.ad.rothschild.com>

The ishares website has the S&P 500 stocks you can download as a XLS file, which opens fine in Excel, but I am not able to open it in R due to what seems to be invalid XML formatting.   I tried using XLConnect and XML as shown below.  Does anyone know a workaround or can point out what I am doing wrong.  Here is my reproducible code:

temp <- "https://www.ishares.com/us/239726/fund-download.dl"
fname <- "ivv.xls"
download.file(url = temp, destfile = fname)
readWorksheetFromFile(fname)
library(XML)
xmlfile <- xmlTreeParse(fname)

09:06:17 > readWorksheetFromFile(fname)
Error: InvalidFormatException (Java): Your InputStream was neither an OLE2 stream, nor an OOXML stream
09:06:17 > library(XML)
09:06:25 > xmlfile <- xmlTreeParse(fname)
Opening and ending tag mismatch: Style line 14 and Style
Error: 1: Opening and ending tag mismatch: Style line 14 and Style


Thanks in advance, Roger







This message and any attachments are for the intended recipient's use only.

This message may contain confidential, proprietary or legally privileged

information. No right to confidential or privileged treatment

of this message is waived or lost by an error in transmission.

If you have received this message in error, please immediately

notify the sender by e-mail, delete the message, any attachments and all

copies from your system and destroy any hard copies.  You must

not, directly or indirectly, use, disclose, distribute,

print or copy any part of this message or any attachments if you are not

the intended recipient.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Thu Jul 28 16:23:48 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 28 Jul 2016 07:23:48 -0700
Subject: [R] Fuzzy variable universe
In-Reply-To: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>
References: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>
Message-ID: <C1119D79-8AA3-422E-99CA-3F08EE3B91FF@dcn.davis.ca.us>

This appears to be a question about a contributed package, though you have not specified which one (so your example code is not reproducible). 

Be warned that I have never seen discussion of fuzzy logic on this list, so any help you get here is likely to be from someone reading the documentation for you. Please be sure to read it carefully yourself first, and read about reproducibility and support for contributed packages in the Posting Guide. 
-- 
Sent from my phone. Please excuse my brevity.

On July 28, 2016 6:16:48 AM PDT, Arthur Stilben <arthur.stilben at gmail.com> wrote:
>Hello, everyone!
>
>Is there a way to set universe group for each fuzzy_variable? I try to
>do this:
>
>> teste = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3
>) ), universe = seq( from = 0, to= 10, by = 0.1 ) )
>
>
>But when I print teste:
>
>> teste
>A fuzzy variable with values: a, universe
>
>
>So, how can I set universe on fuzzy_variable?
>
>-- 
>Arthur Rodrigues Stilben
>Geoinform?tica - LENEP
>(22) 2765-6555
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Thu Jul 28 16:33:49 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 28 Jul 2016 07:33:49 -0700
Subject: [R] problems reading XML type file from ishares website
In-Reply-To: <0765308CD028654885F30322557308D82008D92B@NYCSM0208.rth.ad.rothschild.com>
References: <0765308CD028654885F30322557308D82008D92B@NYCSM0208.rth.ad.rothschild.com>
Message-ID: <6BEAB4B9-2051-4125-B619-59408C11BFB1@dcn.davis.ca.us>

XLS has nothing to do with XML. The shift from XLS to XLSX/XLSM formats was where XML was introduced. You might occasionally find mislabelled files that seem to work anyway, but there is a significant difference inside true XLS files. 

Use a package designed to handle your data format. There are a few, and most seem to require external software support  (e.g. Perl or Java or Windows OS), so you have to decide what overhead support headaches you can tolerate. 
-- 
Sent from my phone. Please excuse my brevity.

On July 28, 2016 6:14:28 AM PDT, "Bos, Roger" <roger.bos at rothschild.com> wrote:
>The ishares website has the S&P 500 stocks you can download as a XLS
>file, which opens fine in Excel, but I am not able to open it in R due
>to what seems to be invalid XML formatting.   I tried using XLConnect
>and XML as shown below.  Does anyone know a workaround or can point out
>what I am doing wrong.  Here is my reproducible code:
>
>temp <- "https://www.ishares.com/us/239726/fund-download.dl"
>fname <- "ivv.xls"
>download.file(url = temp, destfile = fname)
>readWorksheetFromFile(fname)
>library(XML)
>xmlfile <- xmlTreeParse(fname)
>
>09:06:17 > readWorksheetFromFile(fname)
>Error: InvalidFormatException (Java): Your InputStream was neither an
>OLE2 stream, nor an OOXML stream
>09:06:17 > library(XML)
>09:06:25 > xmlfile <- xmlTreeParse(fname)
>Opening and ending tag mismatch: Style line 14 and Style
>Error: 1: Opening and ending tag mismatch: Style line 14 and Style
>
>
>Thanks in advance, Roger
>
>
>
>
>
>
>
>This message and any attachments are for the intended recipient's use
>only.
>
>This message may contain confidential, proprietary or legally
>privileged
>
>information. No right to confidential or privileged treatment
>
>of this message is waived or lost by an error in transmission.
>
>If you have received this message in error, please immediately
>
>notify the sender by e-mail, delete the message, any attachments and
>all
>
>copies from your system and destroy any hard copies.  You must
>
>not, directly or indirectly, use, disclose, distribute,
>
>print or copy any part of this message or any attachments if you are
>not
>
>the intended recipient.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From nika.susac at gmail.com  Thu Jul 28 16:40:06 2016
From: nika.susac at gmail.com (=?UTF-8?Q?Nika_Su=C5=A1ac?=)
Date: Thu, 28 Jul 2016 16:40:06 +0200
Subject: [R] Estimators for non-normal data
In-Reply-To: <22421.57329.286791.309542@stat.math.ethz.ch>
References: <CAMmS1Wh8_b1yPeUwoLp=BiKyxNyiVxtJJb=+X+L7joepy87t7Q@mail.gmail.com>
	<22421.57329.286791.309542@stat.math.ethz.ch>
Message-ID: <CAMmS1WhM=zDrj93Tm89Ka4OYAdQnoB8d3rQ5SF_w_4M8gxfAZw@mail.gmail.com>

Yes, I meant confirmatory factor analysis, I'm sorry if I wasn't clear. I
was doing my analyses in lavaan and saw that there are several robust
options (MLM, MLMVS, MLMV, MLF, MLR), but I wasn't sure about their
specifics, so that was what I was actually asking about.
I will definitely consult the links you sent me, thank you!

2016-07-25 11:46 GMT+02:00 Martin Maechler <maechler at stat.math.ethz.ch>:

> >>>>> Nika Su?ac <nika.susac at gmail.com>
> >>>>>     on Sat, 23 Jul 2016 19:39:48 +0200 writes:
>
>     > Hi!  I have non-normal data (items are continuous on a
>     > 9-point scale, but not normally distributed) and I want to
>     > conduct cfa. Which of the estimators available in lavaan
>     > do you recommend me to use?  Thanks in advance!
>
> I think you want *robust* statistical methods then.
>
> Robust factor analysis (if 'cfa' means something like that) is
> somewhat prominent topic.
> Simple approaches will already be available by using
> MASS::cov.rob() for a robust covariance matrix which you then
> can pass to other methods.
>
> For more (and more modern) methods and approaches,
>
> - for R packages, I'd recommend you consult the CRAN task view
>   about robust statistical methods,
>         https://cran.r-project.org/web/views/Robust.html
>   notably the section on 'Multivariate Analysis'
>
> - for more specific help and expertise, I strongly recommend
>   the dedicated  R-SIG-robust mailing list (instead of R-help),
>    --> https://stat.ethz.ch/mailman/listinfo/r-sig-robust
>
> Best regards,
> Martin Maechler, ETH Zurich
>
>     >   [[alternative HTML version deleted]]
>

	[[alternative HTML version deleted]]


From ramkishore31 at gmail.com  Thu Jul 28 16:45:30 2016
From: ramkishore31 at gmail.com (Ramkishore Swaminathan)
Date: Thu, 28 Jul 2016 07:45:30 -0700
Subject: [R] EM algorithm for maximizing the likelihood of Multivariate
	Hawkes process
Message-ID: <CAHG51U1-P5Qq2kAHOpyDJE+7cTAaULhLxgatodWLsvMEMy=HQg@mail.gmail.com>

I am trying to model data with multivariate Hawkes distribution. Take the
below example. I am able to compute likelihood but don't know how to
maximize it.

library(hawkes)
lambda0 <- c(0.2,0.2)
alpha   <- matrix(c(0.5,0,0,0.5),byrow=TRUE,nrow=2)
beta    <- c(0.7,0.7)
history <- simulateHawkes(lambda0,alpha,beta,3600)
l       <- likelihoodHawkes(lambda0,alpha,beta,history)

How do I maximize this likelihood so that I can find the best lambda, alpha
and beta parameters?

I am not able to find any library or function calls for doing this.

Thanks for the help.

	[[alternative HTML version deleted]]


From macqueen1 at llnl.gov  Thu Jul 28 18:16:56 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 28 Jul 2016 16:16:56 +0000
Subject: [R] Help on improving an algorithm
In-Reply-To: <CAHLnndbxr272957gi9XWcrCJP_TdTB4PLKxj7VL36Fa=iTtZoQ@mail.gmail.com>
References: <CAHLnndbxr272957gi9XWcrCJP_TdTB4PLKxj7VL36Fa=iTtZoQ@mail.gmail.com>
Message-ID: <D3BF78E2.17FE2B%macqueen1@llnl.gov>

This is a good example to illustrate that R is a vectorized language, and
programming concepts that would work in fortran, C, or any of many other
languages are not always effective in R.

The vectorized method below has produced the same value for 'p' in every
example I?ve tried, and is much faster.

Note that the desired calculation looks at every combination of one row
from each of four matrices, i.e., 126^4 combinations (a large number).
This suggests the use of expand.grid().

I don't see any need to actually construct the 'conti' matrix. Only its
row sums are needed, and those can be calculated directly from the rows of
the input matrices.

The vectorized method makes using 126 rows tractable. It still takes a
while on my machine, but it's tolerable. The original method takes longer
than I'm willing to wait. I used a smaller number of rows for testing.

## number of rows in each input matrix
nr <- 126
nr <- 11
## number of columns in each input matrix
nc <- 6

## simulate input data, in order to create
## a reproducible example
xr <- 0:2
tmpa <- matrix( sample(xr, nr*nc, replace=TRUE) , ncol=nc)
tmpb <- matrix( sample(xr, nr*nc, replace=TRUE) , ncol=nc)
tmpc <- matrix( sample(xr, nr*nc, replace=TRUE) , ncol=nc)
tmpd <- matrix( sample(xr, nr*nc, replace=TRUE) , ncol=nc)

cat('method 2 (vectorized)\n')
t2 <- system.time({
  rsa <- rowSums(tmpa)
  rsb <- rowSums(tmpb)
  rsc <- rowSums(tmpc)
  rsd <- rowSums(tmpd)

  bigr <- expand.grid(rsa, rsb, rsc, rsd)

  p2 <- sum( rowSums( bigr > 8) == 0)
  print(p2)
})

cat('original method (looping)\n')

torig <- system.time({
  p <- 0
  for (i in 1:nrow(tmpa)) {
    for (j in 1:nrow(tmpb)) {
      for (k in 1:nrow(tmpc)) {
        for (l in 1:nrow(tmpd)) {
          conti <- rbind(tmpa[i,], tmpb[j,], tmpc[k,],tmpd[l,])
          if (sum(apply(conti,1,sum)>8)==0) p <- p+1
          ##        print(p)
        }
      }
    }
  }

print(p)
})

cat('\n')
print(t2)
print(torig)


A couple of minor points:
  dim(tmp_a)[1] can be replaced by nrow(tmp_a)
  in the original code, apply() can be replaced by rowSums()
It might be faster if rowSums() was replaced by .rowSums(); I haven't
tried that.

-Don

 
-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 7/27/16, 1:41 PM, "R-help on behalf of li li"
<r-help-bounces at r-project.org on behalf of hannah.hlx at gmail.com> wrote:

>Hi all,
> I have four matrix tmp_a, tmp_b, tmp_c and tmp_d whose dimensions are
>shown as below.
>I want to take one row from each of these matrices and then put the four
>selected rows into a matrix.
>I want to count the number of such matrices for which the vector of row
>sum
>is less than or equal to (8,8,8,8,8) (componentwise).
>Below is the code I use now. Is there a way to make this more efficient
>and
>faster?
> Thanks for the help in advance.
>
>> dim(tmp_a);dim(tmp_b); dim(tmp_c); dim(tmp_d)[1] 252   6
>[1] 126   6
>[1] 126   6
>[1] 126   6
>
>
>p <- 0
>for (i in 1:dim(tmp_a)[1]){
>    for (j in 1:dim(tmp_b)[1]){
>        for (k in 1:dim(tmp_c)[1]){
>            for (l in 1:dim(tmp_c)[1]){
>            conti <- rbind(tmp_a[i,], tmp_b[j,], tmp_c[k,],tmp_d[l,])
>            if (sum(apply(conti,1,sum)>8)==0)
>                p <- p+1
>                print(p)}}}}
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Thu Jul 28 18:22:04 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 28 Jul 2016 09:22:04 -0700
Subject: [R] Reduce woes
In-Reply-To: <CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
Message-ID: <CAF8bMcbeF+aq529gJ_gWmaf3w8Yn25PVrwYGBP2y9nOBo67m=Q@mail.gmail.com>

I am not as familiar with those other languages as I should be, but
in both examples aren't you using 'map' to call 'reduce' on each
component of a list?  In R the basic mapping function is lapply.

  > data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
  > rData <- lapply(X=data,
FUN=function(dataElement)Reduce(f=function(acc,item)acc+1L, x=dataElement,
init=0L))
  > str(rData)
  List of 3
   $ one  : int 2
   $ three: int 1
   $ two  : int 2



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 28, 2016 at 3:55 AM, Stefan Kruger <stefan.kruger at gmail.com>
wrote:

> David - many thanks for your response.
>
> What I tried to do was to turn
>
> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>
> into
>
> result <- list(one = 2, three = 1, two = 2)
>
> that is creating a new list which has the same names as the first, but
> where the values are the vector lengths.
>
> I know there are many other (and better) trivial ways of achieving this -
> my aim is less the task itself, and more figuring out if this can be done
> using Reduce() in the fashion I showed in the other examples I gave. It's a
> building block of doing map-filter-reduce type pipelines that I'd like to
> understand how to do in R.
>
> Fumbling in the dark, I tried:
>
> Reduce(function(acc, item) { setNames(c(acc, length(data[item])), item },
> names(data), accumulate=TRUE)
>
> but setNames sets all the names, not adding one - and acc is still a
> vector, not a list.
>
> It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()' aim at
> doing what I'd like to do - but I've not been able to figure out quite how.
>
> Thanks
>
> Stefan
>
>
>
> On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net> wrote:
>
> >
> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <stefan.kruger at gmail.com>
> > wrote:
> > >
> > > Hi -
> > >
> > > I'm new to R.
> > >
> > > In other functional languages I'm familiar with you can often seed a
> call
> > > to reduce() with a custom accumulator. Here's an example in Elixir:
> > >
> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
> > > Enum.count(v), nil) end)
> > > # %{"one" => 2, "three" => 1, "two" => 2}
> > >
> > > In R-terms that's reducing a list of vectors to become a new list
> mapping
> > > the names to the vector lengths.
> > >
> > > Even in JavaScript, you can do similar things:
> > >
> > > list = { one: [1, 1], three: [3], two: [2, 2] };
> > > var result = Object.keys(list).reduceRight(function (acc, item) {
> > >  acc[item] = list[item].length;
> > >  return acc;
> > > }, {});
> > > // result == { two: 2, three: 1, one: 2 }
> > >
> > > In R, from what I can gather, Reduce() is restricted such that any init
> > > value you feed it is required to be of the same type as the elements of
> > the
> > > vector you're reducing -- so I can't build up. So whilst I can do, say
> > >
> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
> > > [1] 111
> > >
> > > I can't use Reduce to build up a list, vector or data frame?
> > >
> > > What am I missing?
> > >
> > > Many thanks for any pointers,
> >
> > This builds a list:
> >
> > > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
> > accumulate=TRUE)
> > [[1]]
> > [1] 96
> >
> > [[2]]
> > [1] 96  1
> >
> > [[3]]
> > [1] 96  1  2
> >
> > [[4]]
> > [1] 96  1  2  3
> >
> > [[5]]
> > [1] 96  1  2  3  4
> >
> > [[6]]
> > [1] 96  1  2  3  4  5
> >
> > But you are not saying what you want. The other examples were doing
> > something with names but you provided no names for the R example.
> >
> > This would return a list of named vectors:
> >
> > > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))  },
> > c(1,2,3,4,5), 96, accumulate=TRUE)
> > [[1]]
> > [1] 96
> >
> > [[2]]
> >  1  2
> > 96  1
> >
> > [[3]]
> >  1  2  3
> > 96  1  2
> >
> > [[4]]
> >  1  2  3  4
> > 96  1  2  3
> >
> > [[5]]
> >  1  2  3  4  5
> > 96  1  2  3  4
> >
> > [[6]]
> >  1  2  3  4  5  6
> > 96  1  2  3  4  5
> >
> >
> >
> >
> > > Stefan
> > >
> > >
> > >
> > > --
> > > Stefan Kruger <stefan.kruger at gmail.com>
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
>
>
> --
> Stefan Kruger <stefan.kruger at gmail.com>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From arthur.stilben at gmail.com  Thu Jul 28 20:11:59 2016
From: arthur.stilben at gmail.com (Arthur Rodrigues Stilben)
Date: Thu, 28 Jul 2016 15:11:59 -0300
Subject: [R] Fuzzy variable universe
In-Reply-To: <C1119D79-8AA3-422E-99CA-3F08EE3B91FF@dcn.davis.ca.us>
References: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>
	<C1119D79-8AA3-422E-99CA-3F08EE3B91FF@dcn.davis.ca.us>
Message-ID: <579A4AEF.2030508@gmail.com>

Sorry, I forgot to mention:

 > install.packages("sets")
...
 > library(sets)
 > teste = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 
) ), universe = seq( from = 0, to = 10, by = 0.1 ) )
 > teste
A fuzzy variable with values: a, universe

The ideia is to set the universe group for the fuzzy variable, but it 
didn't work.

PS.: I'm newbie here, so I apologize for some mistakes :P.

Att,

Em 28-07-2016 11:23, Jeff Newmiller escreveu:
> This appears to be a question about a contributed package, though you have not specified which one (so your example code is not reproducible).
>
> Be warned that I have never seen discussion of fuzzy logic on this list, so any help you get here is likely to be from someone reading the documentation for you. Please be sure to read it carefully yourself first, and read about reproducibility and support for contributed packages in the Posting Guide.

-- 
Arthur Rodrigues Stilben


From jdnewmil at dcn.davis.ca.us  Thu Jul 28 20:20:23 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 28 Jul 2016 11:20:23 -0700
Subject: [R] problems reading XML type file from ishares website
In-Reply-To: <0765308CD028654885F30322557308D82008DB41@NYCSM0208.rth.ad.rothschild.com>
References: <0765308CD028654885F30322557308D82008D92B@NYCSM0208.rth.ad.rothschild.com>
	<6BEAB4B9-2051-4125-B619-59408C11BFB1@dcn.davis.ca.us>
	<0765308CD028654885F30322557308D82008DB41@NYCSM0208.rth.ad.rothschild.com>
Message-ID: <85B8F049-BD52-43DA-8288-000E7F68C252@dcn.davis.ca.us>

Please keep the list included in the thread (e.g. reply-all?).

I looked at the file and agree that it looks like xml with a utf8 byte order mark and Unix line endings, which means it is not XLS and it is not XLSX (which is a zipped directory of xml files with DOS line endings). Excel complains but manages to open the file if it has the XLS extension,  but I am not aware that any of the usual R Excel packages will understand this file. 

The byte order mark can be addressed by opening the file with encoding="UTF-8-BOM", but as you mentioned originally the XML structure is still broken (c.f. the error message about the Style ending tag). Line 16 seems to use /Style rather than /ss:Style. Maybe

library(XML)
txt <- readLines( fname, encoding="UTF-8-BOM" )
txt <- sub( "</Style>", "</ss:Style>", txt )
fnamenobom  <- "nobom.xml"
xmlfile  <- xmlTreeParse( "nobom.xml" )

-- 
Sent from my phone. Please excuse my brevity.

On July 28, 2016 8:26:44 AM PDT, "Bos, Roger" <roger.bos at rothschild.com> wrote:
>Jeff,
>
>Thanks for your suggestions.  I mentioned XLS because that is the
>extension the ishares website provides.  I have tried many packages
>such as xml, xml2, XLConnect, and readxl.  I am not even sure what data
>format the file is, but I looks to me like XML and the extension is
>XLS.  If you have the names of specific packages you think I should
>try, that would be very helpful.
>
>Thanks,
>
>Roger
>
>
>
>
>
>***************************************************************
>This message and any attachments are for the intended recipient's use
>only.
>This message may contain confidential, proprietary or legally
>privileged
>information. No right to confidential or privileged treatment
>of this message is waived or lost by an error in transmission.
>If you have received this message in error, please immediately
>notify the sender by e-mail, delete the message, any attachments and
>all
>copies from your system and destroy any hard copies.  You must
>not, directly or indirectly, use, disclose, distribute,
>print or copy any part of this message or any attachments if you are
>not
>the intended recipient.
>
>
>-----Original Message-----
>From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us]
>Sent: Thursday, July 28, 2016 10:34 AM
>To: Bos, Roger; r-help at r-project.org
>Subject: Re: [R] problems reading XML type file from ishares website
>
>XLS has nothing to do with XML. The shift from XLS to XLSX/XLSM formats
>was where XML was introduced. You might occasionally find mislabelled
>files that seem to work anyway, but there is a significant difference
>inside true XLS files.
>
>Use a package designed to handle your data format. There are a few, and
>most seem to require external software support  (e.g. Perl or Java or
>Windows OS), so you have to decide what overhead support headaches you
>can tolerate.
>--
>Sent from my phone. Please excuse my brevity.
>
>On July 28, 2016 6:14:28 AM PDT, "Bos, Roger"
><roger.bos at rothschild.com> wrote:
>>The ishares website has the S&P 500 stocks you can download as a XLS
>>file, which opens fine in Excel, but I am not able to open it in R due
>>to what seems to be invalid XML formatting.   I tried using XLConnect
>>and XML as shown below.  Does anyone know a workaround or can point
>out
>>what I am doing wrong.  Here is my reproducible code:
>>
>>temp <- "https://www.ishares.com/us/239726/fund-download.dl"
>>fname <- "ivv.xls"
>>download.file(url = temp, destfile = fname)
>>readWorksheetFromFile(fname)
>>library(XML)
>>xmlfile <- xmlTreeParse(fname)
>>
>>09:06:17 > readWorksheetFromFile(fname)
>>Error: InvalidFormatException (Java): Your InputStream was neither an
>>OLE2 stream, nor an OOXML stream
>>09:06:17 > library(XML)
>>09:06:25 > xmlfile <- xmlTreeParse(fname) Opening and ending tag
>>mismatch: Style line 14 and Style
>>Error: 1: Opening and ending tag mismatch: Style line 14 and Style
>>
>>
>>Thanks in advance, Roger
>>
>>
>>
>>
>>
>>
>>
>>This message and any attachments are for the intended recipient's use
>>only.
>>
>>This message may contain confidential, proprietary or legally
>>privileged
>>
>>information. No right to confidential or privileged treatment
>>
>>of this message is waived or lost by an error in transmission.
>>
>>If you have received this message in error, please immediately
>>
>>notify the sender by e-mail, delete the message, any attachments and
>>all
>>
>>copies from your system and destroy any hard copies.  You must
>>
>>not, directly or indirectly, use, disclose, distribute,
>>
>>print or copy any part of this message or any attachments if you are
>>not
>>
>>the intended recipient.
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.


From sarah.goslee at gmail.com  Thu Jul 28 20:25:43 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 28 Jul 2016 14:25:43 -0400
Subject: [R] Fuzzy variable universe
In-Reply-To: <579A4AEF.2030508@gmail.com>
References: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>
	<C1119D79-8AA3-422E-99CA-3F08EE3B91FF@dcn.davis.ca.us>
	<579A4AEF.2030508@gmail.com>
Message-ID: <CAM_vjun+yfcNkv6WnimzndfmZYNuOoEvfmwtOnYEjqqDcm-KfQ@mail.gmail.com>

As Jeff suggested, I read the help for you.

Based on the examples, you need:


     ## set universe
     sets_options("universe", seq(from = 0, to = 10, by = 0.1))
     teste2 = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 ) ) )

comparing
plot(teste) # complete with Warning
and
plot(teste2)
makes me think this did what you wanted. At least, it did something.

Sarah

On Thu, Jul 28, 2016 at 2:11 PM, Arthur Rodrigues Stilben
<arthur.stilben at gmail.com> wrote:
> Sorry, I forgot to mention:
>
>> install.packages("sets")
> ...
>> library(sets)
>> teste = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 ) ),
>> universe = seq( from = 0, to = 10, by = 0.1 ) )
>> teste
> A fuzzy variable with values: a, universe
>
> The ideia is to set the universe group for the fuzzy variable, but it didn't
> work.
>
> PS.: I'm newbie here, so I apologize for some mistakes :P.
>
> Att,
>
> Em 28-07-2016 11:23, Jeff Newmiller escreveu:
>>
>> This appears to be a question about a contributed package, though you have
>> not specified which one (so your example code is not reproducible).
>>
>> Be warned that I have never seen discussion of fuzzy logic on this list,
>> so any help you get here is likely to be from someone reading the
>> documentation for you. Please be sure to read it carefully yourself first,
>> and read about reproducibility and support for contributed packages in the
>> Posting Guide.
>
>
> --
> Arthur Rodrigues Stilben
>


From bgunter.4567 at gmail.com  Thu Jul 28 20:31:44 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 28 Jul 2016 11:31:44 -0700
Subject: [R] Fuzzy variable universe
In-Reply-To: <579A4AEF.2030508@gmail.com>
References: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>
	<C1119D79-8AA3-422E-99CA-3F08EE3B91FF@dcn.davis.ca.us>
	<579A4AEF.2030508@gmail.com>
Message-ID: <CAGxFJbTm_aUcBp91ikxiOXe4j6K=v-frFHtnaZV1NBr1KAdeUw@mail.gmail.com>

Below.

-- Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Jul 28, 2016 at 11:11 AM, Arthur Rodrigues Stilben
<arthur.stilben at gmail.com> wrote:
> Sorry, I forgot to mention:
>
>> install.packages("sets")
> ...
>> library(sets)
>> teste = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 ) ),
>> universe = seq( from = 0, to = 10, by = 0.1 ) )
>> teste
> A fuzzy variable with values: a, universe
>
> The ideia is to set the universe group for the fuzzy variable, but it didn't
> work.
>
> PS.: I'm newbie here, so I apologize for some mistakes :P.

Have you read the posting guide linked below? If not, please do so
before posting (and apologizing) further. If so, then you'll soon get
the hang of it. Most important: stay civil. We all do and say dumb
things from time to time.

Cheers,
Bert


>
> Att,
>
> Em 28-07-2016 11:23, Jeff Newmiller escreveu:
>>
>> This appears to be a question about a contributed package, though you have
>> not specified which one (so your example code is not reproducible).
>>
>> Be warned that I have never seen discussion of fuzzy logic on this list,
>> so any help you get here is likely to be from someone reading the
>> documentation for you. Please be sure to read it carefully yourself first,
>> and read about reproducibility and support for contributed packages in the
>> Posting Guide.
>
>
> --
> Arthur Rodrigues Stilben
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hannah.hlx at gmail.com  Thu Jul 28 20:34:58 2016
From: hannah.hlx at gmail.com (li li)
Date: Thu, 28 Jul 2016 14:34:58 -0400
Subject: [R] Help on improving an algorithm
In-Reply-To: <D3BF78E2.17FE2B%macqueen1@llnl.gov>
References: <CAHLnndbxr272957gi9XWcrCJP_TdTB4PLKxj7VL36Fa=iTtZoQ@mail.gmail.com>
	<D3BF78E2.17FE2B%macqueen1@llnl.gov>
Message-ID: <CAHLnndZfYXH_6tB8ROUYtgmB4gcwkt5uFuZ=TO+iecaa8_1foQ@mail.gmail.com>

Thanks very much Don! You alternative method does make it much faster.
However I think I made a mistake in my previous coding. Sorry.

Instead of checking whether each row sum is larger than 8, what I wanted is
to
check whether each column sum is lager than 8. See the code below.

So in this case, I should still try to use expand.grid? How to use
expand.grid if each element is a row instead of a single number or
character?

Thanks for you help.


> dim(tmp_a);dim(tmp_b); dim(tmp_c); dim(tmp_d)[1] 252   6
[1] 126   6
[1] 126   6
[1] 126   6


p <- 0
for (i in 1:dim(tmp_a)[1]){
    for (j in 1:dim(tmp_b)[1]){
        for (k in 1:dim(tmp_c)[1]){
            for (l in 1:dim(tmp_c)[1]){
            conti <- rbind(tmp_a[i,], tmp_b[j,], tmp_c[k,],tmp_d[l,])

* if (sum(apply(conti,2,sum)>8)==0)*                p <- p+1
                print(p)}}}}


2016-07-28 12:16 GMT-04:00 MacQueen, Don <macqueen1 at llnl.gov>:

> This is a good example to illustrate that R is a vectorized language, and
> programming concepts that would work in fortran, C, or any of many other
> languages are not always effective in R.
>
> The vectorized method below has produced the same value for 'p' in every
> example I?ve tried, and is much faster.
>
> Note that the desired calculation looks at every combination of one row
> from each of four matrices, i.e., 126^4 combinations (a large number).
> This suggests the use of expand.grid().
>
> I don't see any need to actually construct the 'conti' matrix. Only its
> row sums are needed, and those can be calculated directly from the rows of
> the input matrices.
>
> The vectorized method makes using 126 rows tractable. It still takes a
> while on my machine, but it's tolerable. The original method takes longer
> than I'm willing to wait. I used a smaller number of rows for testing.
>
> ## number of rows in each input matrix
> nr <- 126
> nr <- 11
> ## number of columns in each input matrix
> nc <- 6
>
> ## simulate input data, in order to create
> ## a reproducible example
> xr <- 0:2
> tmpa <- matrix( sample(xr, nr*nc, replace=TRUE) , ncol=nc)
> tmpb <- matrix( sample(xr, nr*nc, replace=TRUE) , ncol=nc)
> tmpc <- matrix( sample(xr, nr*nc, replace=TRUE) , ncol=nc)
> tmpd <- matrix( sample(xr, nr*nc, replace=TRUE) , ncol=nc)
>
> cat('method 2 (vectorized)\n')
> t2 <- system.time({
>   rsa <- rowSums(tmpa)
>   rsb <- rowSums(tmpb)
>   rsc <- rowSums(tmpc)
>   rsd <- rowSums(tmpd)
>
>   bigr <- expand.grid(rsa, rsb, rsc, rsd)
>
>   p2 <- sum( rowSums( bigr > 8) == 0)
>   print(p2)
> })
>
> cat('original method (looping)\n')
>
> torig <- system.time({
>   p <- 0
>   for (i in 1:nrow(tmpa)) {
>     for (j in 1:nrow(tmpb)) {
>       for (k in 1:nrow(tmpc)) {
>         for (l in 1:nrow(tmpd)) {
>           conti <- rbind(tmpa[i,], tmpb[j,], tmpc[k,],tmpd[l,])
>           if (sum(apply(conti,1,sum)>8)==0) p <- p+1
>           ##        print(p)
>         }
>       }
>     }
>   }
>
> print(p)
> })
>
> cat('\n')
> print(t2)
> print(torig)
>
>
> A couple of minor points:
>   dim(tmp_a)[1] can be replaced by nrow(tmp_a)
>   in the original code, apply() can be replaced by rowSums()
> It might be faster if rowSums() was replaced by .rowSums(); I haven't
> tried that.
>
> -Don
>
>
> --
> Don MacQueen
>
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
>
>
>
>
>
> On 7/27/16, 1:41 PM, "R-help on behalf of li li"
> <r-help-bounces at r-project.org on behalf of hannah.hlx at gmail.com> wrote:
>
> >Hi all,
> > I have four matrix tmp_a, tmp_b, tmp_c and tmp_d whose dimensions are
> >shown as below.
> >I want to take one row from each of these matrices and then put the four
> >selected rows into a matrix.
> >I want to count the number of such matrices for which the vector of row
> >sum
> >is less than or equal to (8,8,8,8,8) (componentwise).
> >Below is the code I use now. Is there a way to make this more efficient
> >and
> >faster?
> > Thanks for the help in advance.
> >
> >> dim(tmp_a);dim(tmp_b); dim(tmp_c); dim(tmp_d)[1] 252   6
> >[1] 126   6
> >[1] 126   6
> >[1] 126   6
> >
> >
> >p <- 0
> >for (i in 1:dim(tmp_a)[1]){
> >    for (j in 1:dim(tmp_b)[1]){
> >        for (k in 1:dim(tmp_c)[1]){
> >            for (l in 1:dim(tmp_c)[1]){
> >            conti <- rbind(tmp_a[i,], tmp_b[j,], tmp_c[k,],tmp_d[l,])
> >            if (sum(apply(conti,1,sum)>8)==0)
> >                p <- p+1
> >                print(p)}}}}
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From arthur.stilben at gmail.com  Thu Jul 28 20:46:36 2016
From: arthur.stilben at gmail.com (Arthur Rodrigues Stilben)
Date: Thu, 28 Jul 2016 15:46:36 -0300
Subject: [R] Fuzzy variable universe
In-Reply-To: <579A52B1.1000101@gmail.com>
References: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>
	<C1119D79-8AA3-422E-99CA-3F08EE3B91FF@dcn.davis.ca.us>
	<579A4AEF.2030508@gmail.com>
	<CAM_vjun+yfcNkv6WnimzndfmZYNuOoEvfmwtOnYEjqqDcm-KfQ@mail.gmail.com>
	<579A52B1.1000101@gmail.com>
Message-ID: <579A530C.6080708@gmail.com>



Em 28-07-2016 15:45, Arthur Rodrigues Stilben escreveu:
> Sarah,
>
> First of all, thanks for reply.
>
> Second, It really works, but in fact I would to like to set 
> individuals universe groups for each fuzzy_variable. Something like this:
>
> > test1 = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 
> ) ), universe = seq( from = 0, to = 10, by = 0.1 ) )
> > test2 = fuzzy_variable( b = fuzzy_trapezoid( corners = c( 4, 5, 6, 7 
> ) ), universe = seq( from = 0, to = 5, by = 0.1 ) )
>
> Em 28-07-2016 15:25, Sarah Goslee escreveu:
>> As Jeff suggested, I read the help for you.
>>
>> Based on the examples, you need:
>>
>>
>>       ## set universe
>>       sets_options("universe", seq(from = 0, to = 10, by = 0.1))
>>       teste2 = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 
>> 1, 2, 3 ) ) )
>>
>> comparing
>> plot(teste) # complete with Warning
>> and
>> plot(teste2)
>> makes me think this did what you wanted. At least, it did something.
>>
>> Sarah
>>
>> On Thu, Jul 28, 2016 at 2:11 PM, Arthur Rodrigues Stilben
>> <arthur.stilben at gmail.com> wrote:
>>> Sorry, I forgot to mention:
>>>
>>>> install.packages("sets")
>>> ...
>>>> library(sets)
>>>> teste = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 
>>>> 3 ) ),
>>>> universe = seq( from = 0, to = 10, by = 0.1 ) )
>>>> teste
>>> A fuzzy variable with values: a, universe
>>>
>>> The ideia is to set the universe group for the fuzzy variable, but 
>>> it didn't
>>> work.
>>>
>>> PS.: I'm newbie here, so I apologize for some mistakes :P.
>>>
>>> Att,
>>>
>>> Em 28-07-2016 11:23, Jeff Newmiller escreveu:
>>>> This appears to be a question about a contributed package, though 
>>>> you have
>>>> not specified which one (so your example code is not reproducible).
>>>>
>>>> Be warned that I have never seen discussion of fuzzy logic on this 
>>>> list,
>>>> so any help you get here is likely to be from someone reading the
>>>> documentation for you. Please be sure to read it carefully yourself 
>>>> first,
>>>> and read about reproducibility and support for contributed packages 
>>>> in the
>>>> Posting Guide.
>>>
>>> -- 
>>> Arthur Rodrigues Stilben
>>>
>

-- 
Arthur Rodrigues Stilben


From arthur.stilben at gmail.com  Thu Jul 28 20:47:53 2016
From: arthur.stilben at gmail.com (Arthur Rodrigues Stilben)
Date: Thu, 28 Jul 2016 15:47:53 -0300
Subject: [R] Fuzzy variable universe
In-Reply-To: <CAGxFJbTm_aUcBp91ikxiOXe4j6K=v-frFHtnaZV1NBr1KAdeUw@mail.gmail.com>
References: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>
	<C1119D79-8AA3-422E-99CA-3F08EE3B91FF@dcn.davis.ca.us>
	<579A4AEF.2030508@gmail.com>
	<CAGxFJbTm_aUcBp91ikxiOXe4j6K=v-frFHtnaZV1NBr1KAdeUw@mail.gmail.com>
Message-ID: <579A5359.8040100@gmail.com>

Thanks for reply, Bert.

Em 28-07-2016 15:31, Bert Gunter escreveu:
> Below.
>
> -- Bert
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Jul 28, 2016 at 11:11 AM, Arthur Rodrigues Stilben
> <arthur.stilben at gmail.com> wrote:
>> Sorry, I forgot to mention:
>>
>>> install.packages("sets")
>> ...
>>> library(sets)
>>> teste = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 ) ),
>>> universe = seq( from = 0, to = 10, by = 0.1 ) )
>>> teste
>> A fuzzy variable with values: a, universe
>>
>> The ideia is to set the universe group for the fuzzy variable, but it didn't
>> work.
>>
>> PS.: I'm newbie here, so I apologize for some mistakes :P.
> Have you read the posting guide linked below? If not, please do so
> before posting (and apologizing) further. If so, then you'll soon get
> the hang of it. Most important: stay civil. We all do and say dumb
> things from time to time.
>
> Cheers,
> Bert
>
>
>> Att,
>>
>> Em 28-07-2016 11:23, Jeff Newmiller escreveu:
>>> This appears to be a question about a contributed package, though you have
>>> not specified which one (so your example code is not reproducible).
>>>
>>> Be warned that I have never seen discussion of fuzzy logic on this list,
>>> so any help you get here is likely to be from someone reading the
>>> documentation for you. Please be sure to read it carefully yourself first,
>>> and read about reproducibility and support for contributed packages in the
>>> Posting Guide.
>>
>> --
>> Arthur Rodrigues Stilben
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Arthur Rodrigues Stilben


From sarah.goslee at gmail.com  Thu Jul 28 20:50:48 2016
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 28 Jul 2016 14:50:48 -0400
Subject: [R] Fuzzy variable universe
In-Reply-To: <579A530C.6080708@gmail.com>
References: <CAE9uMN8DAg9U8mJe3XB8+U6EzecumGS3y8GW74-uhxAypFx6rQ@mail.gmail.com>
	<C1119D79-8AA3-422E-99CA-3F08EE3B91FF@dcn.davis.ca.us>
	<579A4AEF.2030508@gmail.com>
	<CAM_vjun+yfcNkv6WnimzndfmZYNuOoEvfmwtOnYEjqqDcm-KfQ@mail.gmail.com>
	<579A52B1.1000101@gmail.com> <579A530C.6080708@gmail.com>
Message-ID: <CAM_vjunimoHHECWOm+R1qNckxb7UNfvHW-PtBoeJPxMX0a-dcA@mail.gmail.com>

On Thu, Jul 28, 2016 at 2:46 PM, Arthur Rodrigues Stilben
<arthur.stilben at gmail.com> wrote:
>
>
> Em 28-07-2016 15:45, Arthur Rodrigues Stilben escreveu:
>>
>> Sarah,
>>
>> First of all, thanks for reply.
>>
>> Second, It really works, but in fact I would to like to set individuals
>> universe groups for each fuzzy_variable. Something like this:
>>
>> > test1 = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 )
>> > ), universe = seq( from = 0, to = 10, by = 0.1 ) )
>> > test2 = fuzzy_variable( b = fuzzy_trapezoid( corners = c( 4, 5, 6, 7 )
>> > ), universe = seq( from = 0, to = 5, by = 0.1 ) )

You have to do it as two steps. fuzzy_variable() doesn't accept a
universe argument.

sets_options("universe", seq(from = 0, to = 10, by = 0.1))
test1 = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 )))

sets_options("universe",  seq( from = 0, to = 5, by = 0.1 ))
test2 = fuzzy_variable( b = fuzzy_trapezoid( corners = c( 4, 5, 6, 7 )))

But there's no reason you can't set it as many times as you want.

>> Em 28-07-2016 15:25, Sarah Goslee escreveu:
>>>
>>> As Jeff suggested, I read the help for you.
>>>
>>> Based on the examples, you need:
>>>
>>>
>>>       ## set universe
>>>       sets_options("universe", seq(from = 0, to = 10, by = 0.1))
>>>       teste2 = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2,
>>> 3 ) ) )
>>>
>>> comparing
>>> plot(teste) # complete with Warning
>>> and
>>> plot(teste2)
>>> makes me think this did what you wanted. At least, it did something.
>>>
>>> Sarah
>>>
>>> On Thu, Jul 28, 2016 at 2:11 PM, Arthur Rodrigues Stilben
>>> <arthur.stilben at gmail.com> wrote:
>>>>
>>>> Sorry, I forgot to mention:
>>>>
>>>>> install.packages("sets")
>>>>
>>>> ...
>>>>>
>>>>> library(sets)
>>>>> teste = fuzzy_variable( a = fuzzy_trapezoid( corners = c( 0, 1, 2, 3 )
>>>>> ),
>>>>> universe = seq( from = 0, to = 10, by = 0.1 ) )
>>>>> teste
>>>>
>>>> A fuzzy variable with values: a, universe
>>>>
>>>> The ideia is to set the universe group for the fuzzy variable, but it
>>>> didn't
>>>> work.
>>>>
>>>> PS.: I'm newbie here, so I apologize for some mistakes :P.
>>>>
>>>> Att,
>>>>
>>>> Em 28-07-2016 11:23, Jeff Newmiller escreveu:
>>>>>
>>>>> This appears to be a question about a contributed package, though you
>>>>> have
>>>>> not specified which one (so your example code is not reproducible).
>>>>>
>>>>> Be warned that I have never seen discussion of fuzzy logic on this
>>>>> list,
>>>>> so any help you get here is likely to be from someone reading the
>>>>> documentation for you. Please be sure to read it carefully yourself
>>>>> first,
>>>>> and read about reproducibility and support for contributed packages in
>>>>> the
>>>>> Posting Guide.
>>>>
>>>>
>>>> --
>>>> Arthur Rodrigues Stilben
>>>>


From jdnewmil at dcn.davis.ca.us  Thu Jul 28 20:55:17 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 28 Jul 2016 11:55:17 -0700
Subject: [R] problems reading XML type file from ishares website
In-Reply-To: <85B8F049-BD52-43DA-8288-000E7F68C252@dcn.davis.ca.us>
References: <0765308CD028654885F30322557308D82008D92B@NYCSM0208.rth.ad.rothschild.com>
	<6BEAB4B9-2051-4125-B619-59408C11BFB1@dcn.davis.ca.us>
	<0765308CD028654885F30322557308D82008DB41@NYCSM0208.rth.ad.rothschild.com>
	<85B8F049-BD52-43DA-8288-000E7F68C252@dcn.davis.ca.us>
Message-ID: <422D9653-527E-4C08-B75D-37B53F348A6C@dcn.davis.ca.us>

Er, I failed to include the step to write the repaired data to a file...

fnamenobom  <- "nobom.xml"
cat( paste( txt, collapse="\n" ), file=fnamenobom )
xmlfile  <- xmlTreeParse( fnamenobom )

-- 
Sent from my phone. Please excuse my brevity.

On July 28, 2016 11:20:23 AM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>Please keep the list included in the thread (e.g. reply-all?).
>
>I looked at the file and agree that it looks like xml with a utf8 byte
>order mark and Unix line endings, which means it is not XLS and it is
>not XLSX (which is a zipped directory of xml files with DOS line
>endings). Excel complains but manages to open the file if it has the
>XLS extension,  but I am not aware that any of the usual R Excel
>packages will understand this file. 
>
>The byte order mark can be addressed by opening the file with
>encoding="UTF-8-BOM", but as you mentioned originally the XML structure
>is still broken (c.f. the error message about the Style ending tag).
>Line 16 seems to use /Style rather than /ss:Style. Maybe
>
>library(XML)
>txt <- readLines( fname, encoding="UTF-8-BOM" )
>txt <- sub( "</Style>", "</ss:Style>", txt )
>fnamenobom  <- "nobom.xml"
>xmlfile  <- xmlTreeParse( "nobom.xml" )


From cbenjami at BTBOCES.ORG  Thu Jul 28 20:52:04 2016
From: cbenjami at BTBOCES.ORG (Courtney Benjamin)
Date: Thu, 28 Jul 2016 18:52:04 +0000
Subject: [R] How to Display Value Labels in R Outputs?
Message-ID: <1469731922615.78838@BTBOCES.ORG>

Hello R Experts,

I am using the Survey Package in R to do some initial descriptive stats for my dissertation. With the outputs for both the svymean and the barplot, I would like the value labels to be displayed for the variable-it would make the descriptive statistics much easier to interpret. Instead of the output labels of: F1RTRCC1, F1RTRCC2-I would like to see the value labels of "academic" and "occupational" to be displayed.

How do I go about making this happen? I am including a minimal reproducible example with a small subset of my actual data:

https://drive.google.com/file/d/0B5fHCR5TGRjaZ29wNzR0cF9YRXc/view?usp=sharing


Any help is greatly appreciated. My only experience thus far is with SPSS and I have a feeling that the reason the variable value labels are not appearing is due to either the way I read the dataset into R: elsq1ch<-read.table (file="els-Q1-04-21-16.dat", header = TRUE, sep = "\t", quote = "\"", dec =".") or I am not specifying some detail that is required to manually assign labels to the values of the variables.?


Courtney Benjamin

Broome-Tioga BOCES

Automotive Technology II Teacher

Located at Gault Toyota

Doctoral Candidate-Educational Theory & Practice

State University of New York at Binghamton

cbenjami at btboces.org<mailto:cbenjami at btboces.org>

607-763-8633

	[[alternative HTML version deleted]]


From roundsjeremiah at gmail.com  Thu Jul 28 21:14:50 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Thu, 28 Jul 2016 12:14:50 -0700
Subject: [R] Reduce woes
In-Reply-To: <CAG7vnkyKwkXY-5J1svBK=xot_iDdrdBq5+ryUzAqahg0TAKGEQ@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
	<CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>
	<CAG7vnkyKwkXY-5J1svBK=xot_iDdrdBq5+ryUzAqahg0TAKGEQ@mail.gmail.com>
Message-ID: <CAOjnRsZ8nODDbCe4Xns8AqSo8KDukh97AzQOkemAxpK=i0ATDA@mail.gmail.com>

Re:
"What I'm trying to
work out is how to have the accumulator in Reduce not be the same type as
the elements of the vector/list being reduced - ideally it could be an S3
instance, list, vector, or data frame."

Pretty sure that is not true.  See code that follows.  I would never solve
this task in this way though so no comment on the use of Reduce for what
you described.  (Note the accumulation of "functions" in a list is just a
demo of possibilities).  You could accumulate in an environment too and
potentially gain a lot of copy efficiency.


lookup = list()
lookup[[as.character(1)]] = function() print("1")
lookup[[as.character(2)]] = function() print("2")
lookup[[as.character(3)]] = function() print("3")

data = list(c(1,2), c(1,4), c(3,3), c(2,30))


r = Reduce(function(acc, item) {
append(acc, list(lookup[[as.character(min(item))]]))
}, data,list())
r
for(f in r) f()


On Thu, Jul 28, 2016 at 5:09 AM, Stefan Kruger <stefan.kruger at gmail.com>
wrote:

> Ulrik - many thanks for your reply.
>
> I'm aware of many simple solutions as the one you suggest, both iterative
> and functional style - but I'm trying to learn how to bend Reduce() for the
> purpose of using it in more complex processing tasks. What I'm trying to
> work out is how to have the accumulator in Reduce not be the same type as
> the elements of the vector/list being reduced - ideally it could be an S3
> instance, list, vector, or data frame.
>
> Here's a more realistic example (in Elixir, sorry)
>
> Given two lists:
>
> 1. data: maps an id string to a vector of revision strings
> 2. dict: maps known id/revision pairs as a string to true (or 1)
>
> find the items in data not already in dict, returned as a named list.
>
> ```elixir
> data = %{
>     "id1" => ["rev1.1", "rev1.2"],
>     "id2" => ["rev2.1"],
>     "id3" => ["rev3.1", "rev3.2", "rev3.3"]
> }
>
> dict = %{
>     "id1/rev1.1" => 1,
>     "id1/rev1.2" => 1,
>     "id3/rev3.1" => 1
> }
>
> # Find the items in data not already in dict. Return as a grouped map
>
> Map.keys(data)
>     |> Enum.flat_map(fn id -> Enum.map(data[id], fn rev -> {id, rev} end)
> end)
>     |> Enum.filter(fn {id, rev} -> !Dict.has_key?(dict, "#{id}/#{rev}")
> end)
>     |> Enum.reduce(%{}, fn ({k, v}, d) -> Map.update(d, k, [v], &[v|&1])
> end)
> ```
>
>
>
>
> On 28 July 2016 at 12:03, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>
> > Hi Stefan,
> >
> > in that case,lapply(data, length) should do the trick.
> >
> > Best wishes,
> > Ulrik
> >
> > On Thu, 28 Jul 2016 at 12:57 Stefan Kruger <stefan.kruger at gmail.com>
> > wrote:
> >
> >> David - many thanks for your response.
> >>
> >> What I tried to do was to turn
> >>
> >> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
> >>
> >> into
> >>
> >> result <- list(one = 2, three = 1, two = 2)
> >>
> >> that is creating a new list which has the same names as the first, but
> >> where the values are the vector lengths.
> >>
> >> I know there are many other (and better) trivial ways of achieving this
> -
> >> my aim is less the task itself, and more figuring out if this can be
> done
> >> using Reduce() in the fashion I showed in the other examples I gave.
> It's
> >> a
> >> building block of doing map-filter-reduce type pipelines that I'd like
> to
> >> understand how to do in R.
> >>
> >> Fumbling in the dark, I tried:
> >>
> >> Reduce(function(acc, item) { setNames(c(acc, length(data[item])), item
> },
> >> names(data), accumulate=TRUE)
> >>
> >> but setNames sets all the names, not adding one - and acc is still a
> >> vector, not a list.
> >>
> >> It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()' aim at
> >> doing what I'd like to do - but I've not been able to figure out quite
> >> how.
> >>
> >> Thanks
> >>
> >> Stefan
> >>
> >>
> >>
> >> On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >>
> >> >
> >> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <stefan.kruger at gmail.com
> >
> >> > wrote:
> >> > >
> >> > > Hi -
> >> > >
> >> > > I'm new to R.
> >> > >
> >> > > In other functional languages I'm familiar with you can often seed a
> >> call
> >> > > to reduce() with a custom accumulator. Here's an example in Elixir:
> >> > >
> >> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
> >> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
> >> > > Enum.count(v), nil) end)
> >> > > # %{"one" => 2, "three" => 1, "two" => 2}
> >> > >
> >> > > In R-terms that's reducing a list of vectors to become a new list
> >> mapping
> >> > > the names to the vector lengths.
> >> > >
> >> > > Even in JavaScript, you can do similar things:
> >> > >
> >> > > list = { one: [1, 1], three: [3], two: [2, 2] };
> >> > > var result = Object.keys(list).reduceRight(function (acc, item) {
> >> > >  acc[item] = list[item].length;
> >> > >  return acc;
> >> > > }, {});
> >> > > // result == { two: 2, three: 1, one: 2 }
> >> > >
> >> > > In R, from what I can gather, Reduce() is restricted such that any
> >> init
> >> > > value you feed it is required to be of the same type as the elements
> >> of
> >> > the
> >> > > vector you're reducing -- so I can't build up. So whilst I can do,
> say
> >> > >
> >> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
> >> > > [1] 111
> >> > >
> >> > > I can't use Reduce to build up a list, vector or data frame?
> >> > >
> >> > > What am I missing?
> >> > >
> >> > > Many thanks for any pointers,
> >> >
> >> > This builds a list:
> >> >
> >> > > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
> >> > accumulate=TRUE)
> >> > [[1]]
> >> > [1] 96
> >> >
> >> > [[2]]
> >> > [1] 96  1
> >> >
> >> > [[3]]
> >> > [1] 96  1  2
> >> >
> >> > [[4]]
> >> > [1] 96  1  2  3
> >> >
> >> > [[5]]
> >> > [1] 96  1  2  3  4
> >> >
> >> > [[6]]
> >> > [1] 96  1  2  3  4  5
> >> >
> >> > But you are not saying what you want. The other examples were doing
> >> > something with names but you provided no names for the R example.
> >> >
> >> > This would return a list of named vectors:
> >> >
> >> > > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))  },
> >> > c(1,2,3,4,5), 96, accumulate=TRUE)
> >> > [[1]]
> >> > [1] 96
> >> >
> >> > [[2]]
> >> >  1  2
> >> > 96  1
> >> >
> >> > [[3]]
> >> >  1  2  3
> >> > 96  1  2
> >> >
> >> > [[4]]
> >> >  1  2  3  4
> >> > 96  1  2  3
> >> >
> >> > [[5]]
> >> >  1  2  3  4  5
> >> > 96  1  2  3  4
> >> >
> >> > [[6]]
> >> >  1  2  3  4  5  6
> >> > 96  1  2  3  4  5
> >> >
> >> >
> >> >
> >> >
> >> > > Stefan
> >> > >
> >> > >
> >> > >
> >> > > --
> >> > > Stefan Kruger <stefan.kruger at gmail.com>
> >> > >
> >> > >       [[alternative HTML version deleted]]
> >> > >
> >> > > ______________________________________________
> >> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > > PLEASE do read the posting guide
> >> > http://www.R-project.org/posting-guide.html
> >> > > and provide commented, minimal, self-contained, reproducible code.
> >> >
> >> > David Winsemius
> >> > Alameda, CA, USA
> >> >
> >> >
> >>
> >>
> >> --
> >> Stefan Kruger <stefan.kruger at gmail.com>
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
>
>
> --
> Stefan Kruger <stefan.kruger at gmail.com>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From roundsjeremiah at gmail.com  Thu Jul 28 21:19:32 2016
From: roundsjeremiah at gmail.com (jeremiah rounds)
Date: Thu, 28 Jul 2016 12:19:32 -0700
Subject: [R] Reduce woes
In-Reply-To: <CAOjnRsZ8nODDbCe4Xns8AqSo8KDukh97AzQOkemAxpK=i0ATDA@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
	<CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>
	<CAG7vnkyKwkXY-5J1svBK=xot_iDdrdBq5+ryUzAqahg0TAKGEQ@mail.gmail.com>
	<CAOjnRsZ8nODDbCe4Xns8AqSo8KDukh97AzQOkemAxpK=i0ATDA@mail.gmail.com>
Message-ID: <CAOjnRsYoLQbhPTePt9J-th4O56GOEDroSB-NZ+qwmnGdveyr2A@mail.gmail.com>

Basically using Reduce as an lapply in that example, but I think that was
caused by how people started talking about things in the first place =) But
the point is the accumulator can be anything as far as I can tell.

On Thu, Jul 28, 2016 at 12:14 PM, jeremiah rounds <roundsjeremiah at gmail.com>
wrote:

> Re:
> "What I'm trying to
> work out is how to have the accumulator in Reduce not be the same type as
> the elements of the vector/list being reduced - ideally it could be an S3
> instance, list, vector, or data frame."
>
> Pretty sure that is not true.  See code that follows.  I would never solve
> this task in this way though so no comment on the use of Reduce for what
> you described.  (Note the accumulation of "functions" in a list is just a
> demo of possibilities).  You could accumulate in an environment too and
> potentially gain a lot of copy efficiency.
>
>
> lookup = list()
> lookup[[as.character(1)]] = function() print("1")
> lookup[[as.character(2)]] = function() print("2")
> lookup[[as.character(3)]] = function() print("3")
>
> data = list(c(1,2), c(1,4), c(3,3), c(2,30))
>
>
> r = Reduce(function(acc, item) {
> append(acc, list(lookup[[as.character(min(item))]]))
> }, data,list())
> r
> for(f in r) f()
>
>
> On Thu, Jul 28, 2016 at 5:09 AM, Stefan Kruger <stefan.kruger at gmail.com>
> wrote:
>
>> Ulrik - many thanks for your reply.
>>
>> I'm aware of many simple solutions as the one you suggest, both iterative
>> and functional style - but I'm trying to learn how to bend Reduce() for
>> the
>> purpose of using it in more complex processing tasks. What I'm trying to
>> work out is how to have the accumulator in Reduce not be the same type as
>> the elements of the vector/list being reduced - ideally it could be an S3
>> instance, list, vector, or data frame.
>>
>> Here's a more realistic example (in Elixir, sorry)
>>
>> Given two lists:
>>
>> 1. data: maps an id string to a vector of revision strings
>> 2. dict: maps known id/revision pairs as a string to true (or 1)
>>
>> find the items in data not already in dict, returned as a named list.
>>
>> ```elixir
>> data = %{
>>     "id1" => ["rev1.1", "rev1.2"],
>>     "id2" => ["rev2.1"],
>>     "id3" => ["rev3.1", "rev3.2", "rev3.3"]
>> }
>>
>> dict = %{
>>     "id1/rev1.1" => 1,
>>     "id1/rev1.2" => 1,
>>     "id3/rev3.1" => 1
>> }
>>
>> # Find the items in data not already in dict. Return as a grouped map
>>
>> Map.keys(data)
>>     |> Enum.flat_map(fn id -> Enum.map(data[id], fn rev -> {id, rev} end)
>> end)
>>     |> Enum.filter(fn {id, rev} -> !Dict.has_key?(dict, "#{id}/#{rev}")
>> end)
>>     |> Enum.reduce(%{}, fn ({k, v}, d) -> Map.update(d, k, [v], &[v|&1])
>> end)
>> ```
>>
>>
>>
>>
>> On 28 July 2016 at 12:03, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>>
>> > Hi Stefan,
>> >
>> > in that case,lapply(data, length) should do the trick.
>> >
>> > Best wishes,
>> > Ulrik
>> >
>> > On Thu, 28 Jul 2016 at 12:57 Stefan Kruger <stefan.kruger at gmail.com>
>> > wrote:
>> >
>> >> David - many thanks for your response.
>> >>
>> >> What I tried to do was to turn
>> >>
>> >> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>> >>
>> >> into
>> >>
>> >> result <- list(one = 2, three = 1, two = 2)
>> >>
>> >> that is creating a new list which has the same names as the first, but
>> >> where the values are the vector lengths.
>> >>
>> >> I know there are many other (and better) trivial ways of achieving
>> this -
>> >> my aim is less the task itself, and more figuring out if this can be
>> done
>> >> using Reduce() in the fashion I showed in the other examples I gave.
>> It's
>> >> a
>> >> building block of doing map-filter-reduce type pipelines that I'd like
>> to
>> >> understand how to do in R.
>> >>
>> >> Fumbling in the dark, I tried:
>> >>
>> >> Reduce(function(acc, item) { setNames(c(acc, length(data[item])), item
>> },
>> >> names(data), accumulate=TRUE)
>> >>
>> >> but setNames sets all the names, not adding one - and acc is still a
>> >> vector, not a list.
>> >>
>> >> It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()' aim
>> at
>> >> doing what I'd like to do - but I've not been able to figure out quite
>> >> how.
>> >>
>> >> Thanks
>> >>
>> >> Stefan
>> >>
>> >>
>> >>
>> >> On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net>
>> wrote:
>> >>
>> >> >
>> >> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <
>> stefan.kruger at gmail.com>
>> >> > wrote:
>> >> > >
>> >> > > Hi -
>> >> > >
>> >> > > I'm new to R.
>> >> > >
>> >> > > In other functional languages I'm familiar with you can often seed
>> a
>> >> call
>> >> > > to reduce() with a custom accumulator. Here's an example in Elixir:
>> >> > >
>> >> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
>> >> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
>> >> > > Enum.count(v), nil) end)
>> >> > > # %{"one" => 2, "three" => 1, "two" => 2}
>> >> > >
>> >> > > In R-terms that's reducing a list of vectors to become a new list
>> >> mapping
>> >> > > the names to the vector lengths.
>> >> > >
>> >> > > Even in JavaScript, you can do similar things:
>> >> > >
>> >> > > list = { one: [1, 1], three: [3], two: [2, 2] };
>> >> > > var result = Object.keys(list).reduceRight(function (acc, item) {
>> >> > >  acc[item] = list[item].length;
>> >> > >  return acc;
>> >> > > }, {});
>> >> > > // result == { two: 2, three: 1, one: 2 }
>> >> > >
>> >> > > In R, from what I can gather, Reduce() is restricted such that any
>> >> init
>> >> > > value you feed it is required to be of the same type as the
>> elements
>> >> of
>> >> > the
>> >> > > vector you're reducing -- so I can't build up. So whilst I can do,
>> say
>> >> > >
>> >> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
>> >> > > [1] 111
>> >> > >
>> >> > > I can't use Reduce to build up a list, vector or data frame?
>> >> > >
>> >> > > What am I missing?
>> >> > >
>> >> > > Many thanks for any pointers,
>> >> >
>> >> > This builds a list:
>> >> >
>> >> > > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
>> >> > accumulate=TRUE)
>> >> > [[1]]
>> >> > [1] 96
>> >> >
>> >> > [[2]]
>> >> > [1] 96  1
>> >> >
>> >> > [[3]]
>> >> > [1] 96  1  2
>> >> >
>> >> > [[4]]
>> >> > [1] 96  1  2  3
>> >> >
>> >> > [[5]]
>> >> > [1] 96  1  2  3  4
>> >> >
>> >> > [[6]]
>> >> > [1] 96  1  2  3  4  5
>> >> >
>> >> > But you are not saying what you want. The other examples were doing
>> >> > something with names but you provided no names for the R example.
>> >> >
>> >> > This would return a list of named vectors:
>> >> >
>> >> > > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))  },
>> >> > c(1,2,3,4,5), 96, accumulate=TRUE)
>> >> > [[1]]
>> >> > [1] 96
>> >> >
>> >> > [[2]]
>> >> >  1  2
>> >> > 96  1
>> >> >
>> >> > [[3]]
>> >> >  1  2  3
>> >> > 96  1  2
>> >> >
>> >> > [[4]]
>> >> >  1  2  3  4
>> >> > 96  1  2  3
>> >> >
>> >> > [[5]]
>> >> >  1  2  3  4  5
>> >> > 96  1  2  3  4
>> >> >
>> >> > [[6]]
>> >> >  1  2  3  4  5  6
>> >> > 96  1  2  3  4  5
>> >> >
>> >> >
>> >> >
>> >> >
>> >> > > Stefan
>> >> > >
>> >> > >
>> >> > >
>> >> > > --
>> >> > > Stefan Kruger <stefan.kruger at gmail.com>
>> >> > >
>> >> > >       [[alternative HTML version deleted]]
>> >> > >
>> >> > > ______________________________________________
>> >> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > > PLEASE do read the posting guide
>> >> > http://www.R-project.org/posting-guide.html
>> >> > > and provide commented, minimal, self-contained, reproducible code.
>> >> >
>> >> > David Winsemius
>> >> > Alameda, CA, USA
>> >> >
>> >> >
>> >>
>> >>
>> >> --
>> >> Stefan Kruger <stefan.kruger at gmail.com>
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>>
>>
>> --
>> Stefan Kruger <stefan.kruger at gmail.com>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Thu Jul 28 21:51:03 2016
From: chocold12 at gmail.com (lily li)
Date: Thu, 28 Jul 2016 13:51:03 -0600
Subject: [R] about file name
Message-ID: <CAN5afy9PYcZ56-n0mrYvcJD77dobqxCPKyLTtogYFyUAU3Th6g@mail.gmail.com>

Hi R users,

I have a string for example 'X35.84375_.100.71875', and I have another
dataframe df that I want to export with the transformed string name
'35.84375_-100.71875' with no extension. How to do this in R? Thanks for
your help.

a = 'X35.84375_.100.71875'
write.table(df, file='', row.names=F, col.names=F)

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Thu Jul 28 22:38:34 2016
From: jholtman at gmail.com (jim holtman)
Date: Thu, 28 Jul 2016 16:38:34 -0400
Subject: [R] about file name
In-Reply-To: <CAN5afy9PYcZ56-n0mrYvcJD77dobqxCPKyLTtogYFyUAU3Th6g@mail.gmail.com>
References: <CAN5afy9PYcZ56-n0mrYvcJD77dobqxCPKyLTtogYFyUAU3Th6g@mail.gmail.com>
Message-ID: <CAAxdm-5YNiARoW0Aeu6vqoGA2TPhvj451P_SA3TBCdy5-JL37w@mail.gmail.com>

just strip off the first character:

> a
[1] "X35.84375_.100.71875"
> a.new <- sub("^.", '', a)
> a.new
[1] "35.84375_.100.71875"
>


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jul 28, 2016 at 3:51 PM, lily li <chocold12 at gmail.com> wrote:

> Hi R users,
>
> I have a string for example 'X35.84375_.100.71875', and I have another
> dataframe df that I want to export with the transformed string name
> '35.84375_-100.71875' with no extension. How to do this in R? Thanks for
> your help.
>
> a = 'X35.84375_.100.71875'
> write.table(df, file='', row.names=F, col.names=F)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chocold12 at gmail.com  Thu Jul 28 22:39:33 2016
From: chocold12 at gmail.com (lily li)
Date: Thu, 28 Jul 2016 14:39:33 -0600
Subject: [R] about file name
In-Reply-To: <CAAxdm-5YNiARoW0Aeu6vqoGA2TPhvj451P_SA3TBCdy5-JL37w@mail.gmail.com>
References: <CAN5afy9PYcZ56-n0mrYvcJD77dobqxCPKyLTtogYFyUAU3Th6g@mail.gmail.com>
	<CAAxdm-5YNiARoW0Aeu6vqoGA2TPhvj451P_SA3TBCdy5-JL37w@mail.gmail.com>
Message-ID: <CAN5afy8FADbvQRURsJgRg8sJdb+5j-uM_NAjQVCrexgsdCnjSw@mail.gmail.com>

Thanks, but how to get the string like this:
"35.84375_-100.71875" use the minus sign instead of dot.

On Thu, Jul 28, 2016 at 2:38 PM, jim holtman <jholtman at gmail.com> wrote:

> just strip off the first character:
>
> > a
> [1] "X35.84375_.100.71875"
> > a.new <- sub("^.", '', a)
> > a.new
> [1] "35.84375_.100.71875"
> >
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Thu, Jul 28, 2016 at 3:51 PM, lily li <chocold12 at gmail.com> wrote:
>
>> Hi R users,
>>
>> I have a string for example 'X35.84375_.100.71875', and I have another
>> dataframe df that I want to export with the transformed string name
>> '35.84375_-100.71875' with no extension. How to do this in R? Thanks for
>> your help.
>>
>> a = 'X35.84375_.100.71875'
>> write.table(df, file='', row.names=F, col.names=F)
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From gangchen6 at gmail.com  Thu Jul 28 22:40:16 2016
From: gangchen6 at gmail.com (Gang Chen)
Date: Thu, 28 Jul 2016 16:40:16 -0400
Subject: [R] Subtraction with aggregate
Message-ID: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>

With the following data in data.frame:

subject   QM    emotion     yi
  s1   75.1017   neutral  -75.928276
  s2  -47.3512   neutral -178.295990
  s3  -68.9016   neutral -134.753906
  s1   17.2099  negative -104.168312
  s2  -53.1114  negative -182.373474
  s3  -33.0322  negative -137.420410

I can obtain the average between the two emotions with

mydata <- read.table('clipboard', header=TRUE)
aggregate(mydata[,c('yi', 'QM')], by=list(subject=mydata$subject), mean)

My question is, what is a nice way to get the difference between the
two emotions?

Thanks,
Gang


From jholtman at gmail.com  Thu Jul 28 22:43:31 2016
From: jholtman at gmail.com (jim holtman)
Date: Thu, 28 Jul 2016 16:43:31 -0400
Subject: [R] about file name
In-Reply-To: <CAN5afy8FADbvQRURsJgRg8sJdb+5j-uM_NAjQVCrexgsdCnjSw@mail.gmail.com>
References: <CAN5afy9PYcZ56-n0mrYvcJD77dobqxCPKyLTtogYFyUAU3Th6g@mail.gmail.com>
	<CAAxdm-5YNiARoW0Aeu6vqoGA2TPhvj451P_SA3TBCdy5-JL37w@mail.gmail.com>
	<CAN5afy8FADbvQRURsJgRg8sJdb+5j-uM_NAjQVCrexgsdCnjSw@mail.gmail.com>
Message-ID: <CAAxdm-6hpydiztenCV+RZ1jVfDoNdKHU2dvpm4G5aTynm+gwyA@mail.gmail.com>

add another step: (need to learn about regular expressions)

> a
[1] "X35.84375_.100.71875"
> a.new <- sub("^.", '', a)
> a.new
[1] "35.84375_.100.71875"
> sub("_.", "_-", a.new)
[1] "35.84375_-100.71875"
>


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jul 28, 2016 at 4:39 PM, lily li <chocold12 at gmail.com> wrote:

> Thanks, but how to get the string like this:
> "35.84375_-100.71875" use the minus sign instead of dot.
>
> On Thu, Jul 28, 2016 at 2:38 PM, jim holtman <jholtman at gmail.com> wrote:
>
>> just strip off the first character:
>>
>> > a
>> [1] "X35.84375_.100.71875"
>> > a.new <- sub("^.", '', a)
>> > a.new
>> [1] "35.84375_.100.71875"
>> >
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Thu, Jul 28, 2016 at 3:51 PM, lily li <chocold12 at gmail.com> wrote:
>>
>>> Hi R users,
>>>
>>> I have a string for example 'X35.84375_.100.71875', and I have another
>>> dataframe df that I want to export with the transformed string name
>>> '35.84375_-100.71875' with no extension. How to do this in R? Thanks for
>>> your help.
>>>
>>> a = 'X35.84375_.100.71875'
>>> write.table(df, file='', row.names=F, col.names=F)
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Thu Jul 28 22:46:02 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Thu, 28 Jul 2016 21:46:02 +0100
Subject: [R] about file name
In-Reply-To: <CAN5afy8FADbvQRURsJgRg8sJdb+5j-uM_NAjQVCrexgsdCnjSw@mail.gmail.com>
References: <CAN5afy9PYcZ56-n0mrYvcJD77dobqxCPKyLTtogYFyUAU3Th6g@mail.gmail.com>
	<CAAxdm-5YNiARoW0Aeu6vqoGA2TPhvj451P_SA3TBCdy5-JL37w@mail.gmail.com>
	<CAN5afy8FADbvQRURsJgRg8sJdb+5j-uM_NAjQVCrexgsdCnjSw@mail.gmail.com>
Message-ID: <20160728214602.Horde.t0R_iXzoJXpLTxBTIIf3EUR@mail.sapo.pt>

Hello,

Just use ?sub.

x <- "35.84375_.100.71875"
y <- sub("_\\.", "_-", x)

Hope this helps,

Rui Barradas
?

Citando lily li <chocold12 at gmail.com>:

> Thanks, but how to get the string like this:
> "35.84375_-100.71875" use the minus sign instead of dot.
>
> On Thu, Jul 28, 2016 at 2:38 PM, jim holtman <jholtman at gmail.com> wrote:
>> just strip off the first character:
>>
>> a
>> [1] "X35.84375_.100.71875"
>> a.new <- sub("^.", '', a)
>> a.new
>> [1] "35.84375_.100.71875"
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Thu, Jul 28, 2016 at 3:51 PM, lily li <chocold12 at gmail.com> wrote:
>>> Hi R users,
>>>
>>> I have a string for example 'X35.84375_.100.71875', and I have another
>>> dataframe df that I want to export with the transformed string name
>>> '35.84375_-100.71875' with no extension. How to do this in R? Thanks for
>>> your help.
>>>
>>> a = 'X35.84375_.100.71875'
>>> write.table(df, file='', row.names=F, col.names=F)
>>>
>>> ? ? ? ? [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Thu Jul 28 22:53:10 2016
From: jholtman at gmail.com (jim holtman)
Date: Thu, 28 Jul 2016 16:53:10 -0400
Subject: [R] Subtraction with aggregate
In-Reply-To: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>
References: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>
Message-ID: <CAAxdm-6z_Uk12Q1EJVcHKBHNkENt3cCgY91zuGSjZUee_9vSSA@mail.gmail.com>

Not sure what you mean by "nice way", but here is a dplyr solution:

> library(dplyr)
> mydata <- read.table(text = "subject   QM    emotion     yi
+    s1   75.1017   neutral  -75.928276
+    s2  -47.3512   neutral -178.295990
+    s3  -68.9016   neutral -134.753906
+    s1   17.2099  negative -104.168312
+    s2  -53.1114  negative -182.373474
+    s3  -33.0322  negative -137.420410", header = TRUE)
> agg <- mydata %>%
+         group_by(subject) %>%
+         summarise(QM = mean(QM),
+                   yi = mean(yi)
+                   )
>
>
> agg
# A tibble: 3 x 3
  subject       QM         yi
   <fctr>    <dbl>      <dbl>
1      s1  46.1558  -90.04829
2      s2 -50.2313 -180.33473
3      s3 -50.9669 -136.08716



Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jul 28, 2016 at 4:40 PM, Gang Chen <gangchen6 at gmail.com> wrote:

> With the following data in data.frame:
>
> subject   QM    emotion     yi
>   s1   75.1017   neutral  -75.928276
>   s2  -47.3512   neutral -178.295990
>   s3  -68.9016   neutral -134.753906
>   s1   17.2099  negative -104.168312
>   s2  -53.1114  negative -182.373474
>   s3  -33.0322  negative -137.420410
>
> I can obtain the average between the two emotions with
>
> mydata <- read.table('clipboard', header=TRUE)
> aggregate(mydata[,c('yi', 'QM')], by=list(subject=mydata$subject), mean)
>
> My question is, what is a nice way to get the difference between the
> two emotions?
>
> Thanks,
> Gang
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Thu Jul 28 23:16:37 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 28 Jul 2016 14:16:37 -0700
Subject: [R] Subtraction with aggregate
In-Reply-To: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>
References: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>
Message-ID: <7297AA6B-9C7A-4DD9-9FDE-1BD4F11B2B6A@dcn.davis.ca.us>

What represents the difference when multiple values are present? sd?
-- 
Sent from my phone. Please excuse my brevity.

On July 28, 2016 1:40:16 PM PDT, Gang Chen <gangchen6 at gmail.com> wrote:
>With the following data in data.frame:
>
>subject   QM    emotion     yi
>  s1   75.1017   neutral  -75.928276
>  s2  -47.3512   neutral -178.295990
>  s3  -68.9016   neutral -134.753906
>  s1   17.2099  negative -104.168312
>  s2  -53.1114  negative -182.373474
>  s3  -33.0322  negative -137.420410
>
>I can obtain the average between the two emotions with
>
>mydata <- read.table('clipboard', header=TRUE)
>aggregate(mydata[,c('yi', 'QM')], by=list(subject=mydata$subject),
>mean)
>
>My question is, what is a nice way to get the difference between the
>two emotions?
>
>Thanks,
>Gang
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From gangchen6 at gmail.com  Thu Jul 28 23:21:58 2016
From: gangchen6 at gmail.com (Gang Chen)
Date: Thu, 28 Jul 2016 17:21:58 -0400
Subject: [R] Subtraction with aggregate
In-Reply-To: <CAAxdm-6z_Uk12Q1EJVcHKBHNkENt3cCgY91zuGSjZUee_9vSSA@mail.gmail.com>
References: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>
	<CAAxdm-6z_Uk12Q1EJVcHKBHNkENt3cCgY91zuGSjZUee_9vSSA@mail.gmail.com>
Message-ID: <CAHmzXO5r5GJDdwGVvXUQhPnbH1f+DH30tPMfpUOqLuzf9Os4NQ@mail.gmail.com>

Hi Jim and Jeff,

Thanks for the quick help!

Sorry I didn't state the question clearly: I want the difference
between 'neutral' and 'negative' for each subject. And another person
offered a solution for it:

aggregate(cbind(QM, yi) ~ subject, data = mydata, FUN = diff)


On Thu, Jul 28, 2016 at 4:53 PM, jim holtman <jholtman at gmail.com> wrote:
> Not sure what you mean by "nice way", but here is a dplyr solution:
>
>> library(dplyr)
>> mydata <- read.table(text = "subject   QM    emotion     yi
> +    s1   75.1017   neutral  -75.928276
> +    s2  -47.3512   neutral -178.295990
> +    s3  -68.9016   neutral -134.753906
> +    s1   17.2099  negative -104.168312
> +    s2  -53.1114  negative -182.373474
> +    s3  -33.0322  negative -137.420410", header = TRUE)
>> agg <- mydata %>%
> +         group_by(subject) %>%
> +         summarise(QM = mean(QM),
> +                   yi = mean(yi)
> +                   )
>>
>>
>> agg
> # A tibble: 3 x 3
>   subject       QM         yi
>    <fctr>    <dbl>      <dbl>
> 1      s1  46.1558  -90.04829
> 2      s2 -50.2313 -180.33473
> 3      s3 -50.9669 -136.08716
>
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Thu, Jul 28, 2016 at 4:40 PM, Gang Chen <gangchen6 at gmail.com> wrote:
>>
>> With the following data in data.frame:
>>
>> subject   QM    emotion     yi
>>   s1   75.1017   neutral  -75.928276
>>   s2  -47.3512   neutral -178.295990
>>   s3  -68.9016   neutral -134.753906
>>   s1   17.2099  negative -104.168312
>>   s2  -53.1114  negative -182.373474
>>   s3  -33.0322  negative -137.420410
>>
>> I can obtain the average between the two emotions with
>>
>> mydata <- read.table('clipboard', header=TRUE)
>> aggregate(mydata[,c('yi', 'QM')], by=list(subject=mydata$subject), mean)
>>
>> My question is, what is a nice way to get the difference between the
>> two emotions?
>>
>> Thanks,
>> Gang
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From jholtman at gmail.com  Fri Jul 29 00:49:22 2016
From: jholtman at gmail.com (jim holtman)
Date: Thu, 28 Jul 2016 18:49:22 -0400
Subject: [R] Subtraction with aggregate
In-Reply-To: <CAHmzXO5r5GJDdwGVvXUQhPnbH1f+DH30tPMfpUOqLuzf9Os4NQ@mail.gmail.com>
References: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>
	<CAAxdm-6z_Uk12Q1EJVcHKBHNkENt3cCgY91zuGSjZUee_9vSSA@mail.gmail.com>
	<CAHmzXO5r5GJDdwGVvXUQhPnbH1f+DH30tPMfpUOqLuzf9Os4NQ@mail.gmail.com>
Message-ID: <CAAxdm-7LP-D6BRLBssmqHF=khZNLie8v6-uUEs9=9B1+kHXz4g@mail.gmail.com>

One thing to watch out for are there always two samples (one of each type)
for each subject?  You had better sort by the emotion to make sure that
when you do the difference, it is always with the data in the same
order.    Here is an example of some of these cases where they are ignored:


> library(dplyr)
> mydata <- read.table(text = "subject   QM    emotion     yi
+  s0  123  neutral   321  # only one sample
+  s5  123 neutral 321   # three samples
+  s5  321 negative  345
+  s5  345 what  1234
+  s6 456 neutral 567   # two emotions the same
+  s6 567 neutral 123
+    s1   75.1017   neutral  -75.928276
+    s2  -47.3512   neutral -178.295990
+    s3  -68.9016   neutral -134.753906
+    s1   17.2099  negative -104.168312
+    s2  -53.1114  negative -182.373474
+    s3  -33.0322  negative -137.420410", header = TRUE, as.is = TRUE)
>
> agg <- mydata %>%
+         arrange(desc(emotion)) %>%  # sort
+         group_by(subject) %>%
+         filter(n() == 2 && emotion[1L] != emotion[2L]) %>%  # test for 2
emotions that are different
+         summarise(QM = QM[1L] - QM[2L],
+                   yi = yi[1L] - yi[2L]
+                   )
>
>
> agg
# A tibble: 3 x 3
  subject       QM        yi
    <chr>    <dbl>     <dbl>
1      s1  57.8918 28.240036
2      s2   5.7602  4.077484
3      s3 -35.8694  2.666504


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jul 28, 2016 at 5:21 PM, Gang Chen <gangchen6 at gmail.com> wrote:

> Hi Jim and Jeff,
>
> Thanks for the quick help!
>
> Sorry I didn't state the question clearly: I want the difference
> between 'neutral' and 'negative' for each subject. And another person
> offered a solution for it:
>
> aggregate(cbind(QM, yi) ~ subject, data = mydata, FUN = diff)
>
>
> On Thu, Jul 28, 2016 at 4:53 PM, jim holtman <jholtman at gmail.com> wrote:
> > Not sure what you mean by "nice way", but here is a dplyr solution:
> >
> >> library(dplyr)
> >> mydata <- read.table(text = "subject   QM    emotion     yi
> > +    s1   75.1017   neutral  -75.928276
> > +    s2  -47.3512   neutral -178.295990
> > +    s3  -68.9016   neutral -134.753906
> > +    s1   17.2099  negative -104.168312
> > +    s2  -53.1114  negative -182.373474
> > +    s3  -33.0322  negative -137.420410", header = TRUE)
> >> agg <- mydata %>%
> > +         group_by(subject) %>%
> > +         summarise(QM = mean(QM),
> > +                   yi = mean(yi)
> > +                   )
> >>
> >>
> >> agg
> > # A tibble: 3 x 3
> >   subject       QM         yi
> >    <fctr>    <dbl>      <dbl>
> > 1      s1  46.1558  -90.04829
> > 2      s2 -50.2313 -180.33473
> > 3      s3 -50.9669 -136.08716
> >
> >
> >
> > Jim Holtman
> > Data Munger Guru
> >
> > What is the problem that you are trying to solve?
> > Tell me what you want to do, not how you want to do it.
> >
> > On Thu, Jul 28, 2016 at 4:40 PM, Gang Chen <gangchen6 at gmail.com> wrote:
> >>
> >> With the following data in data.frame:
> >>
> >> subject   QM    emotion     yi
> >>   s1   75.1017   neutral  -75.928276
> >>   s2  -47.3512   neutral -178.295990
> >>   s3  -68.9016   neutral -134.753906
> >>   s1   17.2099  negative -104.168312
> >>   s2  -53.1114  negative -182.373474
> >>   s3  -33.0322  negative -137.420410
> >>
> >> I can obtain the average between the two emotions with
> >>
> >> mydata <- read.table('clipboard', header=TRUE)
> >> aggregate(mydata[,c('yi', 'QM')], by=list(subject=mydata$subject), mean)
> >>
> >> My question is, what is a nice way to get the difference between the
> >> two emotions?
> >>
> >> Thanks,
> >> Gang
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
>

	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Fri Jul 29 01:22:20 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Thu, 28 Jul 2016 16:22:20 -0700
Subject: [R] font size in graphs...can R read Windows settings?
In-Reply-To: <000801d1e882$2f6e5f10$8e4b1d30$@bigpond.com>
References: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>
	<000801d1e882$2f6e5f10$8e4b1d30$@bigpond.com>
Message-ID: <CAJeYpE-nGCuJuS3zyNsP5MSEqZRxb579fVw2LKb=b-o4PMLStQ@mail.gmail.com>

Thanks, Duncan. This is close to what I was looking for. But I'm not using
lattice. And the fontsize$text and fontsize$points are independent of
display settings in Windows (screen resolution and 'size of objects').

I need to make my graphs [esp. placement and size of text(), mtext()] look
good regardless of display settings.

Here's what I'm working with at the moment...

When package is loaded, I store the default graph size ('din') which is
correlated with OS display parameters. Then, I simply multiple my current
'cex' values by the stored 'din' divided by 7 (which is the 'din'
associated with the display parameters that I created the figs in
initially). It seems to work just fine for my text() and mtext() text
sizes.

But some of my algorithms for placing text and subfigs in margins still
need fixing...combinations of 'plt' and 'usr' seem to be working, but it's
tedious.

-Dan

On Wed, Jul 27, 2016 at 8:43 PM, Duncan Mackay <dulcalma at bigpond.com> wrote:

> Hi  Dan
>
> For devices png, pdf, postscript and ? others the pointsize argument
> controls the font size which is modified by cex
>
> For lattice there are the settings in trellis.par.get()
>
> trellis.par.get()$fontsize
> $text
> [1] 12
>
> $points
> [1] 8
>
> which you can set and there is no real need to change font size except if
> you need to change main.
> trellis.par.get()$grid.pars  are the settings for grid elements if used  eg
> text
>
> these could be set globally by trellis.par.set() or individually with
> argument par.settings eg
> xyplot(y ~ x, data = datm,
>              par.settings = list(strip.background = list(col =
> "transparent"),
>                                                  fontsize = list(text = 16,
>
> points = 12),  # large size;  need to refine
>                                                 superpose.polygon= list(col
> = c("red","blue"),
>
> border = c("red","blue"))),
>             type = "b")
>
> Regards
>
> Duncan
>
> Duncan Mackay
> Department of Agronomy and Soil Science
> University of New England
> Armidale NSW 2351
> Email: home: mackay at northnet.com.au
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dalthorp,
> Daniel
> Sent: Thursday, 28 July 2016 07:02
> To: r-help at R-project.org (r-help at r-project.org)
> Subject: [R] font size in graphs...can R read Windows settings?
>
> Hi All,
> I am putting together a package that (among other things) draws some nice
> graphs for users. I place some explanatory text on figs using "text" and
> "mtext". But the size of the text depends on the Windows display settings:
> Smaller (100%), medium (125%) or larger (150%) (In Windows 7... Control
> panel | Appearance and personalization | Display | Make text and other
> items smaller or larger). If I create figs that look good with one setting,
> the text is too big (or too small) if another setting is used in Windows.
> If I know the Windows setting, I can use cex in R to make the right sized
> labels, but if I don't know a user's Windows display size setting...is
> there a way for R to read the setting? Or is there another way to control
> label size so that text labels on graphs look good regardless of WIndows
> display size setting?
>
> Many thanks for Ideas,
>
> -Dan
> --
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From stochastickang at gmail.com  Fri Jul 29 01:38:26 2016
From: stochastickang at gmail.com (Steven Kang)
Date: Fri, 29 Jul 2016 09:38:26 +1000
Subject: [R] Converting string to data frame
Message-ID: <CAKu3pN4Kz-TyN_rmdieoYJcQZm0uAzCX1xPP+737JsbexOywBw@mail.gmail.com>

Hi R users,

I would like to convert a string into a data frame by creating a separator
(ie pipe) between each potential fields (then using *read.table* function).

ie. Here is the dummy input data for illustration
(4 x 5)

Date                      Type                      Description
                In            Out        Net

1/1/2016              Share                    Share margin 1234
600         100         500

1/1/2016              Bond                     Govnt LTM
                                                0

3/1/2016                                              JPY RTOP123
                500                         500

5/1/2016              Cash                      Margin C123
                                50           -500

The following *dat* object was read from PDF file and stored as a string
(which requires to be converted to data frame)


> *dat*

[1] ?1/01/2016?  Share                          Share margin
1234                   600  100  500?

[2]  ?1/01/2016?     Bond                       Govt LTM
                                                0?

[3]  ?3/01/2016?                                       JPY appre RTOP124
               500          500?

[4]  ?5/01/2016?  Cash                           Margin
call                                             50  -50?


> *class(dat)*

[1] ?character?

Are there any effective ways (ie functions) to insert a pipe as a separator
between the fields (including the empty field) like the following desired
outcome?

[1]  ?1/01/2016? | Share|Share margin 1234 |600 | 100 | 500?

[2]  ?1/01/2016? | Bond |Govt LTM |||0?

[3]  ?3/01/2016? |             |JPY appre RTOP124 |500 ||500?

[4]  ?5/01/2016? | Cash  |Margin call ||50 |-50?

I was trying *gsub *function to insert a pipe between the fields and
everything appeared to be fine except when the pipe wasn?t inserted as
intended for the empty fields (ie as there are 5 fields, there should be 5
pipes, but this isn?t the case for records with empty field).

Any suggestion would be much appreciated.


Thanks.

Steven

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Jul 29 02:05:18 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 28 Jul 2016 17:05:18 -0700
Subject: [R] Converting string to data frame
In-Reply-To: <CAKu3pN4Kz-TyN_rmdieoYJcQZm0uAzCX1xPP+737JsbexOywBw@mail.gmail.com>
References: <CAKu3pN4Kz-TyN_rmdieoYJcQZm0uAzCX1xPP+737JsbexOywBw@mail.gmail.com>
Message-ID: <CAGxFJbTkkPmRhba8_xqNkJ4nW8QPwk=Uzxkw+8SJpW=gW+W7Fw@mail.gmail.com>

Although others may be better able to decipher your messed up html
post than I, you are more likely to get a helpful response if you
**follow the posting guide** and post in plain text only, using
?dput() to input example data in a form that makes it easy for R
helpers to input your data, scrutinize, and respond.

Possibly useful comment: did you double escape the "|" symbol in your
gsub() call? ("|" must be escaped with a "\" ; but the "\" must be
escaped by a "\" for R to interpret as as a "\" instead of the first
part of an escape code such as e.g.  "\t" for tab).  Example:

> gsub("[[:blank:]]+","\\|","a  b")

[1] "a|b"


Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Jul 28, 2016 at 4:38 PM, Steven Kang <stochastickang at gmail.com> wrote:
> Hi R users,
>
> I would like to convert a string into a data frame by creating a separator
> (ie pipe) between each potential fields (then using *read.table* function).
>
> ie. Here is the dummy input data for illustration
> (4 x 5)
>
> Date                      Type                      Description
>                 In            Out        Net
>
> 1/1/2016              Share                    Share margin 1234
> 600         100         500
>
> 1/1/2016              Bond                     Govnt LTM
>                                                 0
>
> 3/1/2016                                              JPY RTOP123
>                 500                         500
>
> 5/1/2016              Cash                      Margin C123
>                                 50           -500
>
> The following *dat* object was read from PDF file and stored as a string
> (which requires to be converted to data frame)
>
>
>> *dat*
>
> [1] ?1/01/2016?  Share                          Share margin
> 1234                   600  100  500?
>
> [2]  ?1/01/2016?     Bond                       Govt LTM
>                                                 0?
>
> [3]  ?3/01/2016?                                       JPY appre RTOP124
>                500          500?
>
> [4]  ?5/01/2016?  Cash                           Margin
> call                                             50  -50?
>
>
>> *class(dat)*
>
> [1] ?character?
>
> Are there any effective ways (ie functions) to insert a pipe as a separator
> between the fields (including the empty field) like the following desired
> outcome?
>
> [1]  ?1/01/2016? | Share|Share margin 1234 |600 | 100 | 500?
>
> [2]  ?1/01/2016? | Bond |Govt LTM |||0?
>
> [3]  ?3/01/2016? |             |JPY appre RTOP124 |500 ||500?
>
> [4]  ?5/01/2016? | Cash  |Margin call ||50 |-50?
>
> I was trying *gsub *function to insert a pipe between the fields and
> everything appeared to be fine except when the pipe wasn?t inserted as
> intended for the empty fields (ie as there are 5 fields, there should be 5
> pipes, but this isn?t the case for records with empty field).
>
> Any suggestion would be much appreciated.
>
>
> Thanks.
>
> Steven
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Fri Jul 29 02:07:31 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 29 Jul 2016 10:07:31 +1000
Subject: [R] Subtraction with aggregate
In-Reply-To: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>
References: <CAHmzXO4HXQKZ4N+b8RvobCcef3NJEFk6jCUT_MvKYVp-puQxqg@mail.gmail.com>
Message-ID: <CA+8X3fUE7w1n_YDz1Nx3GOa0CH9y71Q1W=Yhv_Y_pthwPGNwOA@mail.gmail.com>

Hi Gang,
This is one way:

gangdat<-read.table(text="subject   QM    emotion     yi
  s1   75.1017   neutral  -75.928276
  s2  -47.3512   neutral -178.295990
  s3  -68.9016   neutral -134.753906
  s1   17.2099  negative -104.168312
  s2  -53.1114  negative -182.373474
  s3  -33.0322  negative -137.420410",
 header=TRUE)
library(prettyR)
gangstretch<-stretch_df(gangdat,"subject",c("emotion","yi"))
gangstretch$emodiff<-gangstretch$yi_2-gangstretch$yi_1
gangstretch$emodiff

Jim


On Fri, Jul 29, 2016 at 6:40 AM, Gang Chen <gangchen6 at gmail.com> wrote:
> With the following data in data.frame:
>
> subject   QM    emotion     yi
>   s1   75.1017   neutral  -75.928276
>   s2  -47.3512   neutral -178.295990
>   s3  -68.9016   neutral -134.753906
>   s1   17.2099  negative -104.168312
>   s2  -53.1114  negative -182.373474
>   s3  -33.0322  negative -137.420410
>
> I can obtain the average between the two emotions with
>
> mydata <- read.table('clipboard', header=TRUE)
> aggregate(mydata[,c('yi', 'QM')], by=list(subject=mydata$subject), mean)
>
> My question is, what is a nice way to get the difference between the
> two emotions?
>
> Thanks,
> Gang
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jun.shen.ut at gmail.com  Fri Jul 29 02:51:47 2016
From: jun.shen.ut at gmail.com (Jun Shen)
Date: Thu, 28 Jul 2016 20:51:47 -0400
Subject: [R] How to pass na.rm=T to a user defined function
Message-ID: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>

Dear list,

I write a small function to calculate multiple stats on multiple variables
and export in a format exactly the way I want. Everything seems fine until
NA appears in the data.

Here is my function:

do.stats <- function(data, stats.func, summary.var)
            as.data.frame(signif(sapply(stats.func,function(func)
mapply(func,data[summary.var])),3))

A test dataset:
test <-
data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))

a command like the following
do.stats(test, stats.func=c('mean','sd','median','min','max'),
summary.var=c('CL','V1', 'V2','ALPHA'))

gives me

         mean    sd  median   min  max
CL     0.1030 0.917  0.0363 -2.32 2.47
V1    -0.0545 1.070 -0.2120 -2.21 2.70
V2     0.0600 1.000  0.0621 -2.80 2.62
ALPHA -0.0113 0.919  0.0284 -2.35 2.31


However if I have a NA in the data
test$CL[1] <- NA

The same command run gives me
         mean    sd  median   min  max
CL        * NA    NA      NA    NA   NA*
V1    -0.0545 1.070 -0.2120 -2.21 2.70
V2     0.0600 1.000  0.0621 -2.80 2.62
ALPHA -0.0113 0.919  0.0284 -2.35 2.31

I know this is because those functions (mean, sd etc.) all have
na.rm=F by default. How can I

pass na.rm=T to all these functions without manually redefining those
stats functions

Appreciate any comment.

Thanks for your help.


Jun

	[[alternative HTML version deleted]]


From dulcalma at bigpond.com  Fri Jul 29 03:15:04 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Fri, 29 Jul 2016 11:15:04 +1000
Subject: [R] font size in graphs...can R read Windows settings?
In-Reply-To: <CAJeYpE-nGCuJuS3zyNsP5MSEqZRxb579fVw2LKb=b-o4PMLStQ@mail.gmail.com>
References: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>
	<000801d1e882$2f6e5f10$8e4b1d30$@bigpond.com>
	<CAJeYpE-nGCuJuS3zyNsP5MSEqZRxb579fVw2LKb=b-o4PMLStQ@mail.gmail.com>
Message-ID: <000c01d1e936$a36a8d30$ea3fa790$@bigpond.com>

Hi Dan

 

This is one of  the features/problems of proportional graphing ? getting it exactly right.

 

Anyway good graphs usually take time

 

I wonder if aspect in both base and lattice may be of use.

Also I have a default par settings for base graphics where I sometimes change the mai command for multiple graphs and rarely the font size.

 

par(mfrow = c(1,1),

    las = 1,

    mai = c(0.85, 0.85, 0.32, 0.12),

    font.main = 1,

    cex.main = 1.0,

    cex.lab  = 1.0,

    cex.axis = 0.9)

 

Duncan

 

From: Dalthorp, Daniel [mailto:ddalthorp at usgs.gov] 
Sent: Friday, 29 July 2016 09:22
To: Duncan Mackay
Cc: R
Subject: Re: [R] font size in graphs...can R read Windows settings?

 

Thanks, Duncan. This is close to what I was looking for. But I'm not using lattice. And the fontsize$text and fontsize$points are independent of display settings in Windows (screen resolution and 'size of objects').

 

I need to make my graphs [esp. placement and size of text(), mtext()] look good regardless of display settings.

 

Here's what I'm working with at the moment...

 

When package is loaded, I store the default graph size ('din') which is correlated with OS display parameters. Then, I simply multiple my current 'cex' values by the stored 'din' divided by 7 (which is the 'din' associated with the display parameters that I created the figs in initially). It seems to work just fine for my text() and mtext() text sizes. 

 

But some of my algorithms for placing text and subfigs in margins still need fixing...combinations of 'plt' and 'usr' seem to be working, but it's tedious.

 

-Dan

 

On Wed, Jul 27, 2016 at 8:43 PM, Duncan Mackay <dulcalma at bigpond.com> wrote:

Hi  Dan

For devices png, pdf, postscript and ? others the pointsize argument
controls the font size which is modified by cex

For lattice there are the settings in trellis.par.get()

trellis.par.get()$fontsize
$text
[1] 12

$points
[1] 8

which you can set and there is no real need to change font size except if
you need to change main.
trellis.par.get()$grid.pars  are the settings for grid elements if used  eg
text

these could be set globally by trellis.par.set() or individually with
argument par.settings eg
xyplot(y ~ x, data = datm,
             par.settings = list(strip.background = list(col =
"transparent"),
                                                 fontsize = list(text = 16,

points = 12),  # large size;  need to refine
                                                superpose.polygon= list(col
= c("red","blue"),

border = c("red","blue"))),
            type = "b")

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dalthorp,
Daniel
Sent: Thursday, 28 July 2016 07:02
To: r-help at R-project.org (r-help at r-project.org)
Subject: [R] font size in graphs...can R read Windows settings?

Hi All,
I am putting together a package that (among other things) draws some nice
graphs for users. I place some explanatory text on figs using "text" and
"mtext". But the size of the text depends on the Windows display settings:
Smaller (100%), medium (125%) or larger (150%) (In Windows 7... Control
panel | Appearance and personalization | Display | Make text and other
items smaller or larger). If I create figs that look good with one setting,
the text is too big (or too small) if another setting is used in Windows.
If I know the Windows setting, I can use cex in R to make the right sized
labels, but if I don't know a user's Windows display size setting...is
there a way for R to read the setting? Or is there another way to control
label size so that text labels on graphs look good regardless of WIndows
display size setting?

Many thanks for Ideas,

-Dan
--
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.







-- 

Dan Dalthorp, PhD

USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way 
Corvallis, OR 97331 
ph: 541-750-0953
ddalthorp at usgs.gov


	[[alternative HTML version deleted]]


From ddalthorp at usgs.gov  Fri Jul 29 03:49:29 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Thu, 28 Jul 2016 18:49:29 -0700
Subject: [R] font size in graphs...can R read Windows settings?
In-Reply-To: <000c01d1e936$a36a8d30$ea3fa790$@bigpond.com>
References: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>
	<000801d1e882$2f6e5f10$8e4b1d30$@bigpond.com>
	<CAJeYpE-nGCuJuS3zyNsP5MSEqZRxb579fVw2LKb=b-o4PMLStQ@mail.gmail.com>
	<000c01d1e936$a36a8d30$ea3fa790$@bigpond.com>
Message-ID: <CAJeYpE9s=QZoQmM_1JvhA2FsN4+YuMrG8KMtzBCkvuYcFY-6fA@mail.gmail.com>

The trouble is getting the figs to look right for different users who
happen to have different display settings. Nearly all my users will be on
MS Windows, and, for the parameter set you gave, the figs will look
different, depending on which "display size" parameter value is active and
the screen resolution (for Windows 7 --- Control Panel | Appearance and
Personalization | Display | Make text and other items larger or smaller).

For example, the following gives different look (for text(), mtext(), axis
labels, etc.) "smaller" than for "larger" or for fine resolution vs. coarse
resolution. Getting it wrong makes the figs look sloppy and hard to read.
If I knew users' display parameters ahead of time, I could adjust the
drawing algorithms using par parameters, but I don't know the display
parameters. 'din' values at time of package attachment (and after closing
of open devices) gives a reasonable proxy in a lot of cases but not always.

par(mfrow = c(1,1),
     las = 1,
     mai = c(0.85, 0.85, 0.32, 0.12),
     font.main = 1,
     cex.main = 1.0,
     cex.lab  = 1.0,
     cex.axis = 0.9)
plot(0,0)
plot(0,0, type='n')
text(0,0, 'junk')
mtext(side = 3, line = .5, "more junk")


On Thu, Jul 28, 2016 at 6:15 PM, Duncan Mackay <dulcalma at bigpond.com> wrote:

> Hi Dan
>
>
>
> This is one of  the features/problems of proportional graphing ? getting
> it exactly right.
>
>
>
> Anyway good graphs usually take time
>
>
>
> I wonder if aspect in both base and lattice may be of use.
>
> Also I have a default par settings for base graphics where I sometimes
> change the mai command for multiple graphs and rarely the font size.
>
>
>
> par(mfrow = c(1,1),
>
>     las = 1,
>
>     mai = c(0.85, 0.85, 0.32, 0.12),
>
>     font.main = 1,
>
>     cex.main = 1.0,
>
>     cex.lab  = 1.0,
>
>     cex.axis = 0.9)
>
>
>
> Duncan
>
>
>
> *From:* Dalthorp, Daniel [mailto:ddalthorp at usgs.gov]
> *Sent:* Friday, 29 July 2016 09:22
> *To:* Duncan Mackay
> *Cc:* R
> *Subject:* Re: [R] font size in graphs...can R read Windows settings?
>
>
>
> Thanks, Duncan. This is close to what I was looking for. But I'm not using
> lattice. And the fontsize$text and fontsize$points are independent of
> display settings in Windows (screen resolution and 'size of objects').
>
>
>
> I need to make my graphs [esp. placement and size of text(), mtext()] look
> good regardless of display settings.
>
>
>
> Here's what I'm working with at the moment...
>
>
>
> When package is loaded, I store the default graph size ('din') which is
> correlated with OS display parameters. Then, I simply multiple my current
> 'cex' values by the stored 'din' divided by 7 (which is the 'din'
> associated with the display parameters that I created the figs in
> initially). It seems to work just fine for my text() and mtext() text
> sizes.
>
>
>
> But some of my algorithms for placing text and subfigs in margins still
> need fixing...combinations of 'plt' and 'usr' seem to be working, but it's
> tedious.
>
>
>
> -Dan
>
>
>
> On Wed, Jul 27, 2016 at 8:43 PM, Duncan Mackay <dulcalma at bigpond.com>
> wrote:
>
> Hi  Dan
>
> For devices png, pdf, postscript and ? others the pointsize argument
> controls the font size which is modified by cex
>
> For lattice there are the settings in trellis.par.get()
>
> trellis.par.get()$fontsize
> $text
> [1] 12
>
> $points
> [1] 8
>
> which you can set and there is no real need to change font size except if
> you need to change main.
> trellis.par.get()$grid.pars  are the settings for grid elements if used  eg
> text
>
> these could be set globally by trellis.par.set() or individually with
> argument par.settings eg
> xyplot(y ~ x, data = datm,
>              par.settings = list(strip.background = list(col =
> "transparent"),
>                                                  fontsize = list(text = 16,
>
> points = 12),  # large size;  need to refine
>                                                 superpose.polygon= list(col
> = c("red","blue"),
>
> border = c("red","blue"))),
>             type = "b")
>
> Regards
>
> Duncan
>
> Duncan Mackay
> Department of Agronomy and Soil Science
> University of New England
> Armidale NSW 2351
> Email: home: mackay at northnet.com.au
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dalthorp,
> Daniel
> Sent: Thursday, 28 July 2016 07:02
> To: r-help at R-project.org (r-help at r-project.org)
> Subject: [R] font size in graphs...can R read Windows settings?
>
> Hi All,
> I am putting together a package that (among other things) draws some nice
> graphs for users. I place some explanatory text on figs using "text" and
> "mtext". But the size of the text depends on the Windows display settings:
> Smaller (100%), medium (125%) or larger (150%) (In Windows 7... Control
> panel | Appearance and personalization | Display | Make text and other
> items smaller or larger). If I create figs that look good with one setting,
> the text is too big (or too small) if another setting is used in Windows.
> If I know the Windows setting, I can use cex in R to make the right sized
> labels, but if I don't know a user's Windows display size setting...is
> there a way for R to read the setting? Or is there another way to control
> label size so that text labels on graphs look good regardless of WIndows
> display size setting?
>
> Many thanks for Ideas,
>
> -Dan
> --
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
> --
>
> Dan Dalthorp, PhD
>
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
>



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Jul 29 04:22:43 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 28 Jul 2016 19:22:43 -0700
Subject: [R] How to pass na.rm=T to a user defined function
In-Reply-To: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>
References: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>
Message-ID: <7AAA037F-BE78-403E-81B6-27767830947B@dcn.davis.ca.us>

Why not remove it yourself before passing it to those functions? 
-- 
Sent from my phone. Please excuse my brevity.

On July 28, 2016 5:51:47 PM PDT, Jun Shen <jun.shen.ut at gmail.com> wrote:
>Dear list,
>
>I write a small function to calculate multiple stats on multiple
>variables
>and export in a format exactly the way I want. Everything seems fine
>until
>NA appears in the data.
>
>Here is my function:
>
>do.stats <- function(data, stats.func, summary.var)
>            as.data.frame(signif(sapply(stats.func,function(func)
>mapply(func,data[summary.var])),3))
>
>A test dataset:
>test <-
>data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
>
>a command like the following
>do.stats(test, stats.func=c('mean','sd','median','min','max'),
>summary.var=c('CL','V1', 'V2','ALPHA'))
>
>gives me
>
>         mean    sd  median   min  max
>CL     0.1030 0.917  0.0363 -2.32 2.47
>V1    -0.0545 1.070 -0.2120 -2.21 2.70
>V2     0.0600 1.000  0.0621 -2.80 2.62
>ALPHA -0.0113 0.919  0.0284 -2.35 2.31
>
>
>However if I have a NA in the data
>test$CL[1] <- NA
>
>The same command run gives me
>         mean    sd  median   min  max
>CL        * NA    NA      NA    NA   NA*
>V1    -0.0545 1.070 -0.2120 -2.21 2.70
>V2     0.0600 1.000  0.0621 -2.80 2.62
>ALPHA -0.0113 0.919  0.0284 -2.35 2.31
>
>I know this is because those functions (mean, sd etc.) all have
>na.rm=F by default. How can I
>
>pass na.rm=T to all these functions without manually redefining those
>stats functions
>
>Appreciate any comment.
>
>Thanks for your help.
>
>
>Jun
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jun.shen.ut at gmail.com  Fri Jul 29 04:37:31 2016
From: jun.shen.ut at gmail.com (Jun Shen)
Date: Thu, 28 Jul 2016 22:37:31 -0400
Subject: [R] How to pass na.rm=T to a user defined function
In-Reply-To: <7AAA037F-BE78-403E-81B6-27767830947B@dcn.davis.ca.us>
References: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>
	<7AAA037F-BE78-403E-81B6-27767830947B@dcn.davis.ca.us>
Message-ID: <CAMCXXmooBSueHbCeA8NbYB7qzWHO+fLZxjiWCUdAWgraJJTHJw@mail.gmail.com>

Because in reality the NA may appear in one variable but not others. For
example for ID=1, CL may be NA but not for others, For ID=2, V1 may be NA
etc. To keep all the IDs and all the variables in one data frame, it's
inevitable to see some NA

On Thu, Jul 28, 2016 at 10:22 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Why not remove it yourself before passing it to those functions?
> --
> Sent from my phone. Please excuse my brevity.
>
> On July 28, 2016 5:51:47 PM PDT, Jun Shen <jun.shen.ut at gmail.com> wrote:
> >Dear list,
> >
> >I write a small function to calculate multiple stats on multiple
> >variables
> >and export in a format exactly the way I want. Everything seems fine
> >until
> >NA appears in the data.
> >
> >Here is my function:
> >
> >do.stats <- function(data, stats.func, summary.var)
> >            as.data.frame(signif(sapply(stats.func,function(func)
> >mapply(func,data[summary.var])),3))
> >
> >A test dataset:
> >test <-
>
> >data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
> >
> >a command like the following
> >do.stats(test, stats.func=c('mean','sd','median','min','max'),
> >summary.var=c('CL','V1', 'V2','ALPHA'))
> >
> >gives me
> >
> >         mean    sd  median   min  max
> >CL     0.1030 0.917  0.0363 -2.32 2.47
> >V1    -0.0545 1.070 -0.2120 -2.21 2.70
> >V2     0.0600 1.000  0.0621 -2.80 2.62
> >ALPHA -0.0113 0.919  0.0284 -2.35 2.31
> >
> >
> >However if I have a NA in the data
> >test$CL[1] <- NA
> >
> >The same command run gives me
> >         mean    sd  median   min  max
> >CL        * NA    NA      NA    NA   NA*
> >V1    -0.0545 1.070 -0.2120 -2.21 2.70
> >V2     0.0600 1.000  0.0621 -2.80 2.62
> >ALPHA -0.0113 0.919  0.0284 -2.35 2.31
> >
> >I know this is because those functions (mean, sd etc.) all have
> >na.rm=F by default. How can I
> >
> >pass na.rm=T to all these functions without manually redefining those
> >stats functions
> >
> >Appreciate any comment.
> >
> >Thanks for your help.
> >
> >
> >Jun
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From roslinaump at gmail.com  Fri Jul 29 05:12:19 2016
From: roslinaump at gmail.com (roslinazairimah zakaria)
Date: Fri, 29 Jul 2016 11:12:19 +0800
Subject: [R] Extract data
Message-ID: <CANTvJZJSWc2o9UhFJNbdyYoCSJwZ_7U6Ko9ekV=Kot8NvnLngw@mail.gmail.com>

Dear r-users,

I would like to extract year from 2009 to 2014 with the corresponding month
and rain amount.

I tried this:
aggbalok_mth[aggbalok_mth$year == 2009:2014, ]  but some of the data is
missing.

> dput(aggbalok_mth[aggbalok_mth$year == 2009:2014, ] )
structure(list(month = c(1, 7, 2, 8, 3, 9, 4, 10, 5, 11, 6, 12
), year = c(2009, 2009, 2010, 2010, 2011, 2011, 2012, 2012, 2013,
2013, 2014, 2014), x = c(424.6, 59.5, 6, 54.6, 387.9, 236.1,
160.3, 162.5, 102.8, 139.5, 293.3, 39)), .Names = c("month",
"year", "x"), row.names = c(7L, 13L, 20L, 26L, 33L, 39L, 46L,
52L, 59L, 65L, 72L, 78L), class = "data.frame")
Warning message:
In aggbalok_mth$year == 2009:2014 :
  longer object length is not a multiple of shorter object length


Here is the my data:

structure(list(month = c(7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,
6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8,
9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3,
4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
11), year = c(2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009,
2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010,
2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010,
2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011,
2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012,
2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,
2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,
2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015,
2015, 2015, 2015, 2015), x = c(0, 168.7, 203, 149.3, 299.9, 570.7,
424.6, 52.6, 407.7, 210.3, 459.8, 249.2, 59.5, 310.4, 182.7,
433.3, 161, 560.5, 197.5, 6, 68.9, 170.4, 117, 271.2, 133.5,
54.6, 145.5, 122.5, 460.9, 708, 646.7, 58.8, 387.9, 42.9, 190.7,
133.3, 131.7, 158.4, 236.1, 412.9, 462.5, 713.8, 437.1, 140.1,
311.4, 160.3, 202.4, 58.8, 134.7, 120.4, 206.9, 162.5, 68.5,
1025.9, 229.5, 331, 9, 51.4, 102.8, 162.9, 157.2, 32.6, 103.9,
158.7, 139.5, 1371.2, 221.5, 6.1, 19.1, 11, 87.7, 293.3, 87.3,
184, 69.5, 231, 448.2, 39, 19.3, 3.9, 53.8, 141.9, 325, 53.5,
133.3, 321.1, 77.6, 156.5, 2.2)), .Names = c("month", "year",
"x"), row.names = c(NA, -89L), class = "data.frame")

Thank you very much for any help given.
-- 
*Dr. Roslinazairimah Binti Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Deputy Dean (Academic & Student Affairs)
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Jul 29 05:56:57 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 29 Jul 2016 13:56:57 +1000
Subject: [R] Extract data
In-Reply-To: <CANTvJZJSWc2o9UhFJNbdyYoCSJwZ_7U6Ko9ekV=Kot8NvnLngw@mail.gmail.com>
References: <CANTvJZJSWc2o9UhFJNbdyYoCSJwZ_7U6Ko9ekV=Kot8NvnLngw@mail.gmail.com>
Message-ID: <CA+8X3fWOCe5ERKFVOhmOchjZd_aNN3-KW1bh7Pv28OqOq0kj=g@mail.gmail.com>

Hi Roslina,
Try this:

 aggbalok_mth[aggbalok_mth$year %in% 2009:2014,]

Jim


On Fri, Jul 29, 2016 at 1:12 PM, roslinazairimah zakaria
<roslinaump at gmail.com> wrote:
> Dear r-users,
>
> I would like to extract year from 2009 to 2014 with the corresponding month
> and rain amount.
>
> I tried this:
> aggbalok_mth[aggbalok_mth$year == 2009:2014, ]  but some of the data is
> missing.
>
>> dput(aggbalok_mth[aggbalok_mth$year == 2009:2014, ] )
> structure(list(month = c(1, 7, 2, 8, 3, 9, 4, 10, 5, 11, 6, 12
> ), year = c(2009, 2009, 2010, 2010, 2011, 2011, 2012, 2012, 2013,
> 2013, 2014, 2014), x = c(424.6, 59.5, 6, 54.6, 387.9, 236.1,
> 160.3, 162.5, 102.8, 139.5, 293.3, 39)), .Names = c("month",
> "year", "x"), row.names = c(7L, 13L, 20L, 26L, 33L, 39L, 46L,
> 52L, 59L, 65L, 72L, 78L), class = "data.frame")
> Warning message:
> In aggbalok_mth$year == 2009:2014 :
>   longer object length is not a multiple of shorter object length
>
>
> Here is the my data:
>
> structure(list(month = c(7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,
> 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8,
> 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3,
> 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
> 11), year = c(2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009,
> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010,
> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010,
> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011,
> 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012,
> 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,
> 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,
> 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015,
> 2015, 2015, 2015, 2015), x = c(0, 168.7, 203, 149.3, 299.9, 570.7,
> 424.6, 52.6, 407.7, 210.3, 459.8, 249.2, 59.5, 310.4, 182.7,
> 433.3, 161, 560.5, 197.5, 6, 68.9, 170.4, 117, 271.2, 133.5,
> 54.6, 145.5, 122.5, 460.9, 708, 646.7, 58.8, 387.9, 42.9, 190.7,
> 133.3, 131.7, 158.4, 236.1, 412.9, 462.5, 713.8, 437.1, 140.1,
> 311.4, 160.3, 202.4, 58.8, 134.7, 120.4, 206.9, 162.5, 68.5,
> 1025.9, 229.5, 331, 9, 51.4, 102.8, 162.9, 157.2, 32.6, 103.9,
> 158.7, 139.5, 1371.2, 221.5, 6.1, 19.1, 11, 87.7, 293.3, 87.3,
> 184, 69.5, 231, 448.2, 39, 19.3, 3.9, 53.8, 141.9, 325, 53.5,
> 133.3, 321.1, 77.6, 156.5, 2.2)), .Names = c("month", "year",
> "x"), row.names = c(NA, -89L), class = "data.frame")
>
> Thank you very much for any help given.
> --
> *Dr. Roslinazairimah Binti Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
>
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Deputy Dean (Academic & Student Affairs)
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From roslinaump at gmail.com  Fri Jul 29 06:36:08 2016
From: roslinaump at gmail.com (roslinazairimah zakaria)
Date: Fri, 29 Jul 2016 12:36:08 +0800
Subject: [R] Extract data
In-Reply-To: <CA+8X3fWOCe5ERKFVOhmOchjZd_aNN3-KW1bh7Pv28OqOq0kj=g@mail.gmail.com>
References: <CANTvJZJSWc2o9UhFJNbdyYoCSJwZ_7U6Ko9ekV=Kot8NvnLngw@mail.gmail.com>
	<CA+8X3fWOCe5ERKFVOhmOchjZd_aNN3-KW1bh7Pv28OqOq0kj=g@mail.gmail.com>
Message-ID: <CANTvJZ+tgyOOXnDXyStWC8DjJovh5QdknW0AZc8Q+wMp3FadFQ@mail.gmail.com>

Thank you very much Jim.  It works beautifully.

Best regards,



On Fri, Jul 29, 2016 at 11:56 AM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Roslina,
> Try this:
>
>  aggbalok_mth[aggbalok_mth$year %in% 2009:2014,]
>
> Jim
>
>
> On Fri, Jul 29, 2016 at 1:12 PM, roslinazairimah zakaria
> <roslinaump at gmail.com> wrote:
> > Dear r-users,
> >
> > I would like to extract year from 2009 to 2014 with the corresponding
> month
> > and rain amount.
> >
> > I tried this:
> > aggbalok_mth[aggbalok_mth$year == 2009:2014, ]  but some of the data is
> > missing.
> >
> >> dput(aggbalok_mth[aggbalok_mth$year == 2009:2014, ] )
> > structure(list(month = c(1, 7, 2, 8, 3, 9, 4, 10, 5, 11, 6, 12
> > ), year = c(2009, 2009, 2010, 2010, 2011, 2011, 2012, 2012, 2013,
> > 2013, 2014, 2014), x = c(424.6, 59.5, 6, 54.6, 387.9, 236.1,
> > 160.3, 162.5, 102.8, 139.5, 293.3, 39)), .Names = c("month",
> > "year", "x"), row.names = c(7L, 13L, 20L, 26L, 33L, 39L, 46L,
> > 52L, 59L, 65L, 72L, 78L), class = "data.frame")
> > Warning message:
> > In aggbalok_mth$year == 2009:2014 :
> >   longer object length is not a multiple of shorter object length
> >
> >
> > Here is the my data:
> >
> > structure(list(month = c(7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,
> > 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
> > 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8,
> > 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3,
> > 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
> > 11), year = c(2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009,
> > 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010,
> > 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010,
> > 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011,
> > 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012,
> > 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,
> > 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,
> > 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015,
> > 2015, 2015, 2015, 2015), x = c(0, 168.7, 203, 149.3, 299.9, 570.7,
> > 424.6, 52.6, 407.7, 210.3, 459.8, 249.2, 59.5, 310.4, 182.7,
> > 433.3, 161, 560.5, 197.5, 6, 68.9, 170.4, 117, 271.2, 133.5,
> > 54.6, 145.5, 122.5, 460.9, 708, 646.7, 58.8, 387.9, 42.9, 190.7,
> > 133.3, 131.7, 158.4, 236.1, 412.9, 462.5, 713.8, 437.1, 140.1,
> > 311.4, 160.3, 202.4, 58.8, 134.7, 120.4, 206.9, 162.5, 68.5,
> > 1025.9, 229.5, 331, 9, 51.4, 102.8, 162.9, 157.2, 32.6, 103.9,
> > 158.7, 139.5, 1371.2, 221.5, 6.1, 19.1, 11, 87.7, 293.3, 87.3,
> > 184, 69.5, 231, 448.2, 39, 19.3, 3.9, 53.8, 141.9, 325, 53.5,
> > 133.3, 321.1, 77.6, 156.5, 2.2)), .Names = c("month", "year",
> > "x"), row.names = c(NA, -89L), class = "data.frame")
> >
> > Thank you very much for any help given.
> > --
> > *Dr. Roslinazairimah Binti Zakaria*
> > *Tel: +609-5492370; Fax. No.+609-5492766*
> >
> > *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> > roslinaump at gmail.com <roslinaump at gmail.com>*
> > Deputy Dean (Academic & Student Affairs)
> > Faculty of Industrial Sciences & Technology
> > University Malaysia Pahang
> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>



-- 
*Dr. Roslinazairimah Binti Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Deputy Dean (Academic & Student Affairs)
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Jul 29 08:13:54 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 28 Jul 2016 23:13:54 -0700
Subject: [R] EM algorithm for maximizing the likelihood of
	Multivariate	Hawkes process
In-Reply-To: <CAHG51U1-P5Qq2kAHOpyDJE+7cTAaULhLxgatodWLsvMEMy=HQg@mail.gmail.com>
References: <CAHG51U1-P5Qq2kAHOpyDJE+7cTAaULhLxgatodWLsvMEMy=HQg@mail.gmail.com>
Message-ID: <41EE2BD3-9041-467B-A7CF-3A71D49C76D4@dcn.davis.ca.us>

How did you try to find the answer before posting? Some possibilities might be go ogling [1] or perusing CRAN to find [2]...

Note that HTML tends to mangle code... please follow the Posting Guide and send plain text email to this list.

[1] http://bfy.tw/6y3o
[2] https://cran.r-project.org/web/views/Optimization.html
-- 
Sent from my phone. Please excuse my brevity.

On July 28, 2016 7:45:30 AM PDT, Ramkishore Swaminathan <ramkishore31 at gmail.com> wrote:
>I am trying to model data with multivariate Hawkes distribution. Take
>the
>below example. I am able to compute likelihood but don't know how to
>maximize it.
>
>library(hawkes)
>lambda0 <- c(0.2,0.2)
>alpha   <- matrix(c(0.5,0,0,0.5),byrow=TRUE,nrow=2)
>beta    <- c(0.7,0.7)
>history <- simulateHawkes(lambda0,alpha,beta,3600)
>l       <- likelihoodHawkes(lambda0,alpha,beta,history)
>
>How do I maximize this likelihood so that I can find the best lambda,
>alpha
>and beta parameters?
>
>I am not able to find any library or function calls for doing this.
>
>Thanks for the help.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Jul 29 08:29:49 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 28 Jul 2016 23:29:49 -0700
Subject: [R] How to pass na.rm=T to a user defined function
In-Reply-To: <CAMCXXmooBSueHbCeA8NbYB7qzWHO+fLZxjiWCUdAWgraJJTHJw@mail.gmail.com>
References: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>
	<7AAA037F-BE78-403E-81B6-27767830947B@dcn.davis.ca.us>
	<CAMCXXmooBSueHbCeA8NbYB7qzWHO+fLZxjiWCUdAWgraJJTHJw@mail.gmail.com>
Message-ID: <51EAF573-5B10-43BC-8E83-1EA614FA415C@comcast.net>


> On Jul 28, 2016, at 7:37 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> 
> Because in reality the NA may appear in one variable but not others. For
> example for ID=1, CL may be NA but not for others, For ID=2, V1 may be NA
> etc. To keep all the IDs and all the variables in one data frame, it's
> inevitable to see some NA

That doesn't seem to acknowledge Newmiller's advice. In particular this would have seemed to an obvious response to that suggestion:

do.stats <- function(data, stats.func, summary.var)
          as.data.frame(signif(sapply(stats.func,function(func)
mapply( func,  na.omit( data[summary.var]) )), 3))


And please also heed the advice in the Posting Guide to use plain text.

-- 
David.



> 
> On Thu, Jul 28, 2016 at 10:22 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> 
>> Why not remove it yourself before passing it to those functions?
>> --
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 28, 2016 5:51:47 PM PDT, Jun Shen <jun.shen.ut at gmail.com> wrote:
>>> Dear list,
>>> 
>>> I write a small function to calculate multiple stats on multiple
>>> variables
>>> and export in a format exactly the way I want. Everything seems fine
>>> until
>>> NA appears in the data.
>>> 
>>> Here is my function:
>>> 
>>> do.stats <- function(data, stats.func, summary.var)
>>>           as.data.frame(signif(sapply(stats.func,function(func)
>>> mapply(func,data[summary.var])),3))
>>> 
>>> A test dataset:
>>> test <-
>> 
>>> data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
>>> 
>>> a command like the following
>>> do.stats(test, stats.func=c('mean','sd','median','min','max'),
>>> summary.var=c('CL','V1', 'V2','ALPHA'))
>>> 
>>> gives me
>>> 
>>>        mean    sd  median   min  max
>>> CL     0.1030 0.917  0.0363 -2.32 2.47
>>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
>>> V2     0.0600 1.000  0.0621 -2.80 2.62
>>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
>>> 
>>> 
>>> However if I have a NA in the data
>>> test$CL[1] <- NA
>>> 
>>> The same command run gives me
>>>        mean    sd  median   min  max
>>> CL        * NA    NA      NA    NA   NA*
>>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
>>> V2     0.0600 1.000  0.0621 -2.80 2.62
>>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
>>> 
>>> I know this is because those functions (mean, sd etc.) all have
>>> na.rm=F by default. How can I
>>> 
>>> pass na.rm=T to all these functions without manually redefining those
>>> stats functions
>>> 
>>> Appreciate any comment.
>>> 
>>> Thanks for your help.
>>> 
>>> 
>>> Jun
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ross.chapman at ecogeonomix.com  Fri Jul 29 08:37:03 2016
From: ross.chapman at ecogeonomix.com (ross.chapman at ecogeonomix.com)
Date: Fri, 29 Jul 2016 16:37:03 +1000
Subject: [R] cpquery problem
Message-ID: <00e101d1e963$9e770790$db6516b0$@ecogeonomix.com>

Hi all

 

I have a problem with the cpquery function in the bnlearn package.

 

I have constructed a hybrid network (using a mix of continuous and discrete
variables).

 

The network is named "fitted".

 

I am interested in predicting the probability of observing a value greater
that a particular threshold in a continuous variable (ABW) from evidence
presented in a mix of continuous and discrete variables.

 

One of the continuous variables (EST) takes 3 values, x, y and z.

 

I find that the cpquery gives very plausible results if the EST is set to
equal x.

 

For example:

 

> cpquery(fitted,event=(ABW>=11), evidence=eval(parse(text="(EST=='x' & TR>9
& BU>15819 &  RF>2989)")),n=10^6)

[1] 0.7471088

 

While inverting the threshold for ABW gives: 

 

> cpquery(fitted,event=(ABW<=11), evidence=eval(parse(text="(EST=='x' & TR>9
& BU>15819 &  RF>2989)")),n=10^6)
[1] 0.2587795

 

However, if I replace EST=='x' with EST=='z' or EST=='y' I get 0 probability
of obtaining a value for ABW that is either greater or less than the
threshold.

 

For example:

 

> cpquery(fitted,event=(ABW>=11), evidence=eval(parse(text="(EST=='y' & TR>9
& BU>15819 &  RF>2989)")),n=10^6)

[1] 0

 

and 

 

> cpquery(fitted,event=(ABW<=11), evidence=eval(parse(text="(EST=='y' & TR>9
& BU>15819 &  RF>2989)")),n=10^6)

[1] 0

 

I do not understand why classes y and z are returning 0 probabilities for
these queries.  

 

My own knowledge from the data is that these classes should both typically
return a value for ABW that is very much higher than the threshold value.

 

Can you help me understand why my cpquery code is not giving the anticipated
results for x and y? 

 

Many thanks

 

Ross

 


	[[alternative HTML version deleted]]


From srivibish at gmail.com  Fri Jul 29 09:16:50 2016
From: srivibish at gmail.com (sri vathsan)
Date: Fri, 29 Jul 2016 12:46:50 +0530
Subject: [R] Reducing execution time
In-Reply-To: <CAOjnRsYifP=D_aeYhZdMK6vo0b-P6r1Xf7x4_vSp0KTxEbJ5BA@mail.gmail.com>
References: <CAGO7QoNjZ=CCND=WkQ=0bm1UOCX8iYVEEcYC_8JcZ2rUxuGbnw@mail.gmail.com>
	<CAGxFJbSZh50fj6WHCDBtrYyXqwOQ+rtWUZfFkG=0TuGSzZw22A@mail.gmail.com>
	<CAGO7QoMDcgNR9MpNXxsiFd_gcojebVxFe8Qu+FYP3xCR21kVLQ@mail.gmail.com>
	<CAM_vjunxJqAxjCtzMpoyAYBWBydS0XvsZBXJnm6WN04xctqz+Q@mail.gmail.com>
	<CAGO7QoPDawUd2ztr8aOfNBOvvCN2wG21wGmiBGsf5kJSduHdVw@mail.gmail.com>
	<CAM_vju=+q_4i7BnOn3cr2OtJ=H6gyrJw=vrjnLPUu=px=zcB0Q@mail.gmail.com>
	<CAGO7QoPWuAp-pLuALS-r8DDBmdGSYvsn3So_bB9g8PVYr_xhMQ@mail.gmail.com>
	<CAOjnRsYifP=D_aeYhZdMK6vo0b-P6r1Xf7x4_vSp0KTxEbJ5BA@mail.gmail.com>
Message-ID: <CAGO7QoNA6W2drTOFm=VwSkOkEo-_Toe-3ABa4QewA3CmSj-0Tw@mail.gmail.com>

Hi,

Thanks for the response. Unfortunately this did not solve my problem and
may be the way I represented my data would be the problem. I am not sure
that I can give a link for the data which will give a clear representation.
If that is not a proper way, I have to follow my original method.

Regards,
Sri

On Thu, Jul 28, 2016 at 12:56 AM, jeremiah rounds <roundsjeremiah at gmail.com>
wrote:

> Correction to my code. I created a "doc" variable because I was thinking
> of doing something faster, but I never did the change.  grep needed to work
> on the original source "dat" to be used for counting.
>
>  Fixed:
>
> combs = structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L, 34L,
> 34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
> "V2", "V3"), class = "data.frame", row.names = c(NA, -5L))
>
> dat = list(
> c(77,65,34,23,55, 65,23,77, 44),
> c(65,23,77,65,55,34, 77, 34,65, 10),
> c(77,34,65),
> c(55,78,56),
> c(98,23,77,65,34, 65, 23, 77, 34))
>
>
> words = unlist(apply(combs, 1 , function(d) paste(as.character(d),
> collapse=" ")))
> dat = lapply(dat, function(d) paste( as.character(d), collapse= " "))
> #doc = paste(dat, collapse = " ## ") # just some arbitrary separator
> character that isn't in your words
> counts = sapply(words, function(w) length(grep(w, dat)))
> names(counts) = words
> counts
> cbind(combs, data.frame(N = counts))
>
>
> On Wed, Jul 27, 2016 at 11:27 AM, sri vathsan <srivibish at gmail.com> wrote:
>
>> Hi,
>>
>> It is not a just 79 triplets. As I said, there are 79 codes. I am making
>> triplets out of that 79 codes and matching the triplets in the list.
>>
>> Please find the dput of the data below.
>>
>> > dput(head(newd,10))
>> structure(list(uniq_id = c("1", "2", "3", "4", "5", "6", "7",
>> "8", "9", "10"), hi = c("11,  22,  84,  85,  108,  111", "18,  84,  85,
>> 87,  122,  134",
>> "2,  18,  22", "18,  108,  122,  134,  176", "19,  85,  87,  100,  107",
>> "79,  85,  111", "11,  88,  108", "19,  88,  96", "19,  85,  96",
>> "19,  100,  103")), .Names = c("uniq_id", "hi"), row.names = c(NA,
>> -10L), class = c("tbl_df", "tbl", "data.frame"))
>> >
>>
>> I am trying to count the frequency of the triplets in the above data using
>> the below code.
>>
>> # split column into a list
>> myList <- strsplit(newd$hi, split=",")
>> # get all pairwise combinations
>> myCombos <- t(combn(unique(unlist(myList)), 3))
>> # count the instances where the pair is present
>> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
>>   sum(sapply(myList, function(j) {
>>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
>> #final matrix
>> final <- cbind(matrix(as.integer(myCombos), nrow(myCombos)), myCounts)
>>
>> I hope I made my point clear. Please let me know if I miss anything.
>>
>> Regards,
>> Sri
>>
>>
>>
>>
>> On Wed, Jul 27, 2016 at 11:19 PM, Sarah Goslee <sarah.goslee at gmail.com>
>> wrote:
>>
>> > You said you had 79 triplets and 8000 records.
>> >
>> > When I compared 100 triplets to 10000 records it took 86 seconds.
>> >
>> > So obviously there is something you're not telling us about the format
>> > of your data.
>> >
>> > If you use dput() to provide actual examples, you will get better
>> > results than if we on Rhelp have to guess. Because we tend to guess in
>> > ways that make the most sense after extensive R experience, and that's
>> > probably not what you have.
>> >
>> > Sarah
>> >
>> > On Wed, Jul 27, 2016 at 1:29 PM, sri vathsan <srivibish at gmail.com>
>> wrote:
>> > > Hi,
>> > >
>> > > Thanks for the solution. But I am afraid that after running this code
>> > still
>> > > it takes more time. It has been an hour and still it is executing. I
>> > > understand the delay because each triplet has to compare almost 9000
>> > > elements.
>> > >
>> > > Regards,
>> > > Sri
>> > >
>> > > On Wed, Jul 27, 2016 at 9:02 PM, Sarah Goslee <sarah.goslee at gmail.com
>> >
>> > > wrote:
>> > >>
>> > >> Hi,
>> > >>
>> > >> It's really a good idea to use dput() or some other reproducible way
>> > >> to provide data. I had to guess as to what your data looked like.
>> > >>
>> > >> It appears that order doesn't matter?
>> > >>
>> > >> Given than, here's one approach:
>> > >>
>> > >> combs <- structure(list(V1 = c(65L, 77L, 55L, 23L, 34L), V2 = c(23L,
>> > 34L,
>> > >> 34L, 77L, 65L), V3 = c(77L, 65L, 23L, 34L, 55L)), .Names = c("V1",
>> > >> "V2", "V3"), class = "data.frame", row.names = c(NA, -5L))
>> > >>
>> > >> dat <- list(
>> > >> c(77,65,34,23,55),
>> > >> c(65,23,77,65,55,34),
>> > >> c(77,34,65),
>> > >> c(55,78,56),
>> > >> c(98,23,77,65,34))
>> > >>
>> > >>
>> > >> sapply(seq_len(nrow(combs)), function(i)sum(sapply(dat,
>> > >> function(j)all(combs[i,] %in% j))))
>> > >>
>> > >> On a dataset of comparable time to yours, it takes me under a minute
>> > and a
>> > >> half.
>> > >>
>> > >> > combs <- combs[rep(1:nrow(combs), length=100), ]
>> > >> > dat <- dat[rep(1:length(dat), length=10000)]
>> > >> >
>> > >> > dim(combs)
>> > >> [1] 100   3
>> > >> > length(dat)
>> > >> [1] 10000
>> > >> >
>> > >> > system.time(test <- sapply(seq_len(nrow(combs)),
>> > >> > function(i)sum(sapply(dat, function(j)all(combs[i,] %in% j)))))
>> > >>    user  system elapsed
>> > >>  86.380   0.006  86.391
>> > >>
>> > >>
>> > >>
>> > >>
>> > >> On Wed, Jul 27, 2016 at 10:47 AM, sri vathsan <srivibish at gmail.com>
>> > wrote:
>> > >> > Hi,
>> > >> >
>> > >> > Apologizes for the less information.
>> > >> >
>> > >> > Basically, myCombos is a matrix with 3 variables which is a triplet
>> > that
>> > >> > is
>> > >> > a combination of 79 codes. There are around 3lakh combination as
>> such
>> > >> > and
>> > >> > it looks like below.
>> > >> >
>> > >> > V1 V2 V3
>> > >> > 65 23 77
>> > >> > 77 34 65
>> > >> > 55 34 23
>> > >> > 23 77 34
>> > >> > 34 65 55
>> > >> >
>> > >> > Each triplet will compare in a list (mylist) having 8177 elements
>> > which
>> > >> > will looks like below.
>> > >> >
>> > >> > 77,65,34,23,55
>> > >> > 65,23,77,65,55,34
>> > >> > 77,34,65
>> > >> > 55,78,56
>> > >> > 98,23,77,65,34
>> > >> >
>> > >> > Now I want to count the no of occurrence of the triplet in the
>> above
>> > >> > list.
>> > >> > I.e., the triplet 65 23 77 is seen 3 times in the list. So my
>> output
>> > >> > looks
>> > >> > like below
>> > >> >
>> > >> > V1 V2 V3 Freq
>> > >> > 65 23 77  3
>> > >> > 77 34 65  4
>> > >> > 55 34 23  2
>> > >> >
>> > >> > I hope, I made it clear this time.
>> > >> >
>> > >> >
>> > >> > On Wed, Jul 27, 2016 at 7:00 PM, Bert Gunter <
>> bgunter.4567 at gmail.com>
>> > >> > wrote:
>> > >> >
>> > >> >> Not entirely sure I understand, but match() is already
>> vectorized, so
>> > >> >> you
>> > >> >> should be able to lose the supply(). This would speed things up a
>> > lot.
>> > >> >> Please re-read ?match *carefully* .
>> > >> >>
>> > >> >> Bert
>> > >> >>
>> > >> >> On Jul 27, 2016 6:15 AM, "sri vathsan" <srivibish at gmail.com>
>> wrote:
>> > >> >>
>> > >> >> Hi,
>> > >> >>
>> > >> >> I created list of 3 combination numbers (mycombos, around 3 lakh
>> > >> >> combinations) and counting the occurrence of those combination in
>> > >> >> another
>> > >> >> list. This comparision list (mylist) is having around 8000
>> records.I
>> > am
>> > >> >> using the following code.
>> > >> >>
>> > >> >> myCounts <- sapply(1:nrow(myCombos), FUN=function(i) {
>> > >> >>   sum(sapply(myList, function(j) {
>> > >> >>     sum(!is.na(match(c(myCombos[i,]), j)))})==3)})
>> > >> >>
>> > >> >> The above code takes very long time to execute and is there any
>> other
>> > >> >> effecting method which will reduce the time.
>> > >> >> --
>> > >> >>
>> > >> >> Regards,
>> > >> >> Srivathsan.K
>> > >> >>
>> > >
>> > >
>> > >
>> > >
>> >
>>
>>
>>
>> --
>>
>> Regards,
>> Srivathsan.K
>> Phone : 9600165206
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 

Regards,
Srivathsan.K
Phone : 9600165206

	[[alternative HTML version deleted]]


From stefan.kruger at gmail.com  Fri Jul 29 10:37:32 2016
From: stefan.kruger at gmail.com (Stefan Kruger)
Date: Fri, 29 Jul 2016 09:37:32 +0100
Subject: [R] Reduce woes
In-Reply-To: <CAOjnRsYoLQbhPTePt9J-th4O56GOEDroSB-NZ+qwmnGdveyr2A@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
	<CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>
	<CAG7vnkyKwkXY-5J1svBK=xot_iDdrdBq5+ryUzAqahg0TAKGEQ@mail.gmail.com>
	<CAOjnRsZ8nODDbCe4Xns8AqSo8KDukh97AzQOkemAxpK=i0ATDA@mail.gmail.com>
	<CAOjnRsYoLQbhPTePt9J-th4O56GOEDroSB-NZ+qwmnGdveyr2A@mail.gmail.com>
Message-ID: <CAG7vnky2boUP9t1nrYfPbWjNSUd6DBvX8cNfvO3hPAK0xCDu_Q@mail.gmail.com>

Jeremiah -

neat - that's one step closer, but one small thing I still don't understand:

> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
> r = Reduce(function(acc, item) { append(acc, setNames(length(item),
names(item))) }, data, list())
> str(r)
List of 3
 $ : int 2
 $ : int 1
 $ : int 2

I wanted the names to remain, but it seems like the "data" parameter loses
its names when consumed by the Reduce()? If I print "item" inside the
reducing function, it's not got the names. I'm probably missing some
central tenet of R here.

As to your comment of this being lapply() implemented by Reduce() - as I
understand lapply()  (or map() in other functional languages), it's limited
to returning a list/vector of the same length as the original. Consider
this contrived example:

> r = Reduce(function(acc, item) { if (length(item) > 1) {append(acc,
setNames(length(item), names(item)))} }, data, list())
> str(r)
 int 2
> r
[1] 2

I don't think you could achieve that with lapply()?

Thanks

Stefan


On 28 July 2016 at 20:19, jeremiah rounds <roundsjeremiah at gmail.com> wrote:

> Basically using Reduce as an lapply in that example, but I think that was
> caused by how people started talking about things in the first place =) But
> the point is the accumulator can be anything as far as I can tell.
>
> On Thu, Jul 28, 2016 at 12:14 PM, jeremiah rounds <
> roundsjeremiah at gmail.com> wrote:
>
>> Re:
>> "What I'm trying to
>> work out is how to have the accumulator in Reduce not be the same type as
>> the elements of the vector/list being reduced - ideally it could be an S3
>> instance, list, vector, or data frame."
>>
>> Pretty sure that is not true.  See code that follows.  I would never
>> solve this task in this way though so no comment on the use of Reduce for
>> what you described.  (Note the accumulation of "functions" in a list is
>> just a demo of possibilities).  You could accumulate in an environment too
>> and potentially gain a lot of copy efficiency.
>>
>>
>> lookup = list()
>> lookup[[as.character(1)]] = function() print("1")
>> lookup[[as.character(2)]] = function() print("2")
>> lookup[[as.character(3)]] = function() print("3")
>>
>> data = list(c(1,2), c(1,4), c(3,3), c(2,30))
>>
>>
>> r = Reduce(function(acc, item) {
>> append(acc, list(lookup[[as.character(min(item))]]))
>> }, data,list())
>> r
>> for(f in r) f()
>>
>>
>> On Thu, Jul 28, 2016 at 5:09 AM, Stefan Kruger <stefan.kruger at gmail.com>
>> wrote:
>>
>>> Ulrik - many thanks for your reply.
>>>
>>> I'm aware of many simple solutions as the one you suggest, both iterative
>>> and functional style - but I'm trying to learn how to bend Reduce() for
>>> the
>>> purpose of using it in more complex processing tasks. What I'm trying to
>>> work out is how to have the accumulator in Reduce not be the same type as
>>> the elements of the vector/list being reduced - ideally it could be an S3
>>> instance, list, vector, or data frame.
>>>
>>> Here's a more realistic example (in Elixir, sorry)
>>>
>>> Given two lists:
>>>
>>> 1. data: maps an id string to a vector of revision strings
>>> 2. dict: maps known id/revision pairs as a string to true (or 1)
>>>
>>> find the items in data not already in dict, returned as a named list.
>>>
>>> ```elixir
>>> data = %{
>>>     "id1" => ["rev1.1", "rev1.2"],
>>>     "id2" => ["rev2.1"],
>>>     "id3" => ["rev3.1", "rev3.2", "rev3.3"]
>>> }
>>>
>>> dict = %{
>>>     "id1/rev1.1" => 1,
>>>     "id1/rev1.2" => 1,
>>>     "id3/rev3.1" => 1
>>> }
>>>
>>> # Find the items in data not already in dict. Return as a grouped map
>>>
>>> Map.keys(data)
>>>     |> Enum.flat_map(fn id -> Enum.map(data[id], fn rev -> {id, rev} end)
>>> end)
>>>     |> Enum.filter(fn {id, rev} -> !Dict.has_key?(dict, "#{id}/#{rev}")
>>> end)
>>>     |> Enum.reduce(%{}, fn ({k, v}, d) -> Map.update(d, k, [v], &[v|&1])
>>> end)
>>> ```
>>>
>>>
>>>
>>>
>>> On 28 July 2016 at 12:03, Ulrik Stervbo <ulrik.stervbo at gmail.com> wrote:
>>>
>>> > Hi Stefan,
>>> >
>>> > in that case,lapply(data, length) should do the trick.
>>> >
>>> > Best wishes,
>>> > Ulrik
>>> >
>>> > On Thu, 28 Jul 2016 at 12:57 Stefan Kruger <stefan.kruger at gmail.com>
>>> > wrote:
>>> >
>>> >> David - many thanks for your response.
>>> >>
>>> >> What I tried to do was to turn
>>> >>
>>> >> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>>> >>
>>> >> into
>>> >>
>>> >> result <- list(one = 2, three = 1, two = 2)
>>> >>
>>> >> that is creating a new list which has the same names as the first, but
>>> >> where the values are the vector lengths.
>>> >>
>>> >> I know there are many other (and better) trivial ways of achieving
>>> this -
>>> >> my aim is less the task itself, and more figuring out if this can be
>>> done
>>> >> using Reduce() in the fashion I showed in the other examples I gave.
>>> It's
>>> >> a
>>> >> building block of doing map-filter-reduce type pipelines that I'd
>>> like to
>>> >> understand how to do in R.
>>> >>
>>> >> Fumbling in the dark, I tried:
>>> >>
>>> >> Reduce(function(acc, item) { setNames(c(acc, length(data[item])),
>>> item },
>>> >> names(data), accumulate=TRUE)
>>> >>
>>> >> but setNames sets all the names, not adding one - and acc is still a
>>> >> vector, not a list.
>>> >>
>>> >> It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()' aim
>>> at
>>> >> doing what I'd like to do - but I've not been able to figure out quite
>>> >> how.
>>> >>
>>> >> Thanks
>>> >>
>>> >> Stefan
>>> >>
>>> >>
>>> >>
>>> >> On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net>
>>> wrote:
>>> >>
>>> >> >
>>> >> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <
>>> stefan.kruger at gmail.com>
>>> >> > wrote:
>>> >> > >
>>> >> > > Hi -
>>> >> > >
>>> >> > > I'm new to R.
>>> >> > >
>>> >> > > In other functional languages I'm familiar with you can often
>>> seed a
>>> >> call
>>> >> > > to reduce() with a custom accumulator. Here's an example in
>>> Elixir:
>>> >> > >
>>> >> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
>>> >> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
>>> >> > > Enum.count(v), nil) end)
>>> >> > > # %{"one" => 2, "three" => 1, "two" => 2}
>>> >> > >
>>> >> > > In R-terms that's reducing a list of vectors to become a new list
>>> >> mapping
>>> >> > > the names to the vector lengths.
>>> >> > >
>>> >> > > Even in JavaScript, you can do similar things:
>>> >> > >
>>> >> > > list = { one: [1, 1], three: [3], two: [2, 2] };
>>> >> > > var result = Object.keys(list).reduceRight(function (acc, item) {
>>> >> > >  acc[item] = list[item].length;
>>> >> > >  return acc;
>>> >> > > }, {});
>>> >> > > // result == { two: 2, three: 1, one: 2 }
>>> >> > >
>>> >> > > In R, from what I can gather, Reduce() is restricted such that any
>>> >> init
>>> >> > > value you feed it is required to be of the same type as the
>>> elements
>>> >> of
>>> >> > the
>>> >> > > vector you're reducing -- so I can't build up. So whilst I can
>>> do, say
>>> >> > >
>>> >> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
>>> >> > > [1] 111
>>> >> > >
>>> >> > > I can't use Reduce to build up a list, vector or data frame?
>>> >> > >
>>> >> > > What am I missing?
>>> >> > >
>>> >> > > Many thanks for any pointers,
>>> >> >
>>> >> > This builds a list:
>>> >> >
>>> >> > > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
>>> >> > accumulate=TRUE)
>>> >> > [[1]]
>>> >> > [1] 96
>>> >> >
>>> >> > [[2]]
>>> >> > [1] 96  1
>>> >> >
>>> >> > [[3]]
>>> >> > [1] 96  1  2
>>> >> >
>>> >> > [[4]]
>>> >> > [1] 96  1  2  3
>>> >> >
>>> >> > [[5]]
>>> >> > [1] 96  1  2  3  4
>>> >> >
>>> >> > [[6]]
>>> >> > [1] 96  1  2  3  4  5
>>> >> >
>>> >> > But you are not saying what you want. The other examples were doing
>>> >> > something with names but you provided no names for the R example.
>>> >> >
>>> >> > This would return a list of named vectors:
>>> >> >
>>> >> > > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))
>>> },
>>> >> > c(1,2,3,4,5), 96, accumulate=TRUE)
>>> >> > [[1]]
>>> >> > [1] 96
>>> >> >
>>> >> > [[2]]
>>> >> >  1  2
>>> >> > 96  1
>>> >> >
>>> >> > [[3]]
>>> >> >  1  2  3
>>> >> > 96  1  2
>>> >> >
>>> >> > [[4]]
>>> >> >  1  2  3  4
>>> >> > 96  1  2  3
>>> >> >
>>> >> > [[5]]
>>> >> >  1  2  3  4  5
>>> >> > 96  1  2  3  4
>>> >> >
>>> >> > [[6]]
>>> >> >  1  2  3  4  5  6
>>> >> > 96  1  2  3  4  5
>>> >> >
>>> >> >
>>> >> >
>>> >> >
>>> >> > > Stefan
>>> >> > >
>>> >> > >
>>> >> > >
>>> >> > > --
>>> >> > > Stefan Kruger <stefan.kruger at gmail.com>
>>> >> > >
>>> >> > >       [[alternative HTML version deleted]]
>>> >> > >
>>> >> > > ______________________________________________
>>> >> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> > > https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> > > PLEASE do read the posting guide
>>> >> > http://www.R-project.org/posting-guide.html
>>> >> > > and provide commented, minimal, self-contained, reproducible code.
>>> >> >
>>> >> > David Winsemius
>>> >> > Alameda, CA, USA
>>> >> >
>>> >> >
>>> >>
>>> >>
>>> >> --
>>> >> Stefan Kruger <stefan.kruger at gmail.com>
>>> >>
>>> >>         [[alternative HTML version deleted]]
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide
>>> >> http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.
>>> >>
>>> >
>>>
>>>
>>> --
>>> Stefan Kruger <stefan.kruger at gmail.com>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>


-- 
Stefan Kruger <stefan.kruger at gmail.com>

	[[alternative HTML version deleted]]


From dialvac-r at yahoo.de  Fri Jul 29 10:52:04 2016
From: dialvac-r at yahoo.de (Alain D.)
Date: Fri, 29 Jul 2016 10:52:04 +0200 (CEST)
Subject: [R] store result of loop in df
Message-ID: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>

Dear list, 

I have a dataframe df:

df<-data.frame(x=c(5,32,18,3,17), n=c(11,200,432,20,60))

Now I want to run n=nrow binom.test() with x being the number of success and n
the number of trials and then store the results as a new VAR in df.

I tried 

  for (i in 1:nrow(df)){
  df$VAR<-(binom.test(df[i,1],df[i,2],0.065))$estimate[[1]]
} 

but bin does only contain the last result. 

What is wrong with my code? Can anyone help?

Thank you in advance.

Alain


From drjimlemon at gmail.com  Fri Jul 29 10:52:31 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 29 Jul 2016 18:52:31 +1000
Subject: [R] Extract data
In-Reply-To: <CANTvJZKu9Ge6bT1SaXD4+=9Bmckg57Qwy_UMQ4qdaTNoX+Jtnw@mail.gmail.com>
References: <CANTvJZJSWc2o9UhFJNbdyYoCSJwZ_7U6Ko9ekV=Kot8NvnLngw@mail.gmail.com>
	<CA+8X3fWOCe5ERKFVOhmOchjZd_aNN3-KW1bh7Pv28OqOq0kj=g@mail.gmail.com>
	<CANTvJZ+tgyOOXnDXyStWC8DjJovh5QdknW0AZc8Q+wMp3FadFQ@mail.gmail.com>
	<CANTvJZKgQyqgus3eurkS9uP=YOSq9udx4kM5XvyhRkvitwE+8g@mail.gmail.com>
	<CANTvJZKu9Ge6bT1SaXD4+=9Bmckg57Qwy_UMQ4qdaTNoX+Jtnw@mail.gmail.com>
Message-ID: <CA+8X3fXV=JrLrJUq7WU0t5NVLE8VNr3KSQ9ee_c7x+T8DDVHJw@mail.gmail.com>

Hi Roslina,
This may be what you want:

sum_balok<-
 as.vector(by(aggbalok_2009_2014$x,aggbalok_2009_2014$year,sum,na.rm=TRUE))
cbind(year=2009:2014,sum_balok)

I don't have the data for the other measures, but you could calculate
the sums as you did below and then add them to the cbind arguments.

Jim


On Fri, Jul 29, 2016 at 5:51 PM, roslinazairimah zakaria
<roslinaump at gmail.com> wrote:
> I tried this:
> dt1 <- aggbalok_mth[aggbalok_mth$year %in% 2009:2014,]
> dt2 <- agggambang_mth[agggambang_mth$year %in% 2009:2014,]
> dt3 <- aggsgsoi_mth[aggsgsoi_mth$year %in% 2009:2014,]
> dt4 <- aggjpsphg_mth[aggjpsphg_mth$year %in% 2009:2014,]
> dt5 <- aggpbesar_mth[aggpbesar_mth$year %in% 2009:2014,]
> dt6 <- aggpmanis_mth[aggpmanis_mth$year %in% 2009:2014,]
>
> ## Yearly Sum
> dt_year <- cbind(aggregate(dt1[,3], by=dt1[,c(2,2)],FUN=sum, na.rm=TRUE),
>       aggregate(dt2[,3], by=dt2[,c(2,2)],FUN=sum, na.rm=TRUE),
>       aggregate(dt3[,3], by=dt3[,c(2,2)],FUN=sum, na.rm=TRUE),
>       aggregate(dt4[,3], by=dt4[,c(2,2)],FUN=sum, na.rm=TRUE),
>       aggregate(dt5[,3], by=dt5[,c(2,2)],FUN=sum, na.rm=TRUE),
>       aggregate(dt6[,3], by=dt6[,c(2,2)],FUN=sum, na.rm=TRUE))
>
> However the output is not good.
> dput(dt_year)
> structure(list(year = c(2009, 2010, 2011, 2012, 2013, 2014),
>     year.1 = c(2009, 2010, 2011, 2012, 2013, 2014), x = c(3511.6,
>     2456, 3575.7, 3029, 2849.7, 1697.7), year = c(2009, 2010,
>     2011, 2012, 2013, 2014), year.1 = c(2009, 2010, 2011, 2012,
>     2013, 2014), x = c(3040.3, 2209.1, 3210.3, 3403.8, 3449.3,
>     2070.4), year = c(2009, 2010, 2011, 2012, 2013, 2014), year.1 = c(2009,
>     2010, 2011, 2012, 2013, 2014), x = c(3657.1, 2059.2, 2862.4,
>     3221.2, 3136.8, 3255.1), year = c(2009, 2010, 2011, 2012,
>     2013, 2014), year.1 = c(2009, 2010, 2011, 2012, 2013, 2014
>     ), x = c(2726.4, 2266.8, 2249.1, 3093.5, 1513, 3087.5), year = c(2009,
>     2010, 2011, 2012, 2013, 2014), year.1 = c(2009, 2010, 2011,
>     2012, 2013, 2014), x = c(3194.9, 1879.6, 1861.4, 3080, 1648.4,
>     3012.3), year = c(2009, 2010, 2011, 2012, 2013, 2014), year.1 = c(2009,
>     2010, 2011, 2012, 2013, 2014), x = c(2697.9, 1639.7, 2107.8,
>     2479.7, 3078.8, 2288.7)), .Names = c("year", "year.1", "x",
> "year", "year.1", "x", "year", "year.1", "x", "year", "year.1",
> "x", "year", "year.1", "x", "year", "year.1", "x"), row.names = c(NA,
> -6L), class = "data.frame")
>
> Thank you.
>
> On Fri, Jul 29, 2016 at 2:30 PM, roslinazairimah zakaria
> <roslinaump at gmail.com> wrote:
>>
>> I have one more question, how do I get the sum for the years.  Thank you.
>>
>> On Fri, Jul 29, 2016 at 12:36 PM, roslinazairimah zakaria
>> <roslinaump at gmail.com> wrote:
>>>
>>> Thank you very much Jim.  It works beautifully.
>>>
>>> Best regards,
>>>
>>>
>>>
>>> On Fri, Jul 29, 2016 at 11:56 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
>>>>
>>>> Hi Roslina,
>>>> Try this:
>>>>
>>>>  aggbalok_mth[aggbalok_mth$year %in% 2009:2014,]
>>>>
>>>> Jim
>>>>
>>>>
>>>> On Fri, Jul 29, 2016 at 1:12 PM, roslinazairimah zakaria
>>>> <roslinaump at gmail.com> wrote:
>>>> > Dear r-users,
>>>> >
>>>> > I would like to extract year from 2009 to 2014 with the corresponding
>>>> > month
>>>> > and rain amount.
>>>> >
>>>> > I tried this:
>>>> > aggbalok_mth[aggbalok_mth$year == 2009:2014, ]  but some of the data
>>>> > is
>>>> > missing.
>>>> >
>>>> >> dput(aggbalok_mth[aggbalok_mth$year == 2009:2014, ] )
>>>> > structure(list(month = c(1, 7, 2, 8, 3, 9, 4, 10, 5, 11, 6, 12
>>>> > ), year = c(2009, 2009, 2010, 2010, 2011, 2011, 2012, 2012, 2013,
>>>> > 2013, 2014, 2014), x = c(424.6, 59.5, 6, 54.6, 387.9, 236.1,
>>>> > 160.3, 162.5, 102.8, 139.5, 293.3, 39)), .Names = c("month",
>>>> > "year", "x"), row.names = c(7L, 13L, 20L, 26L, 33L, 39L, 46L,
>>>> > 52L, 59L, 65L, 72L, 78L), class = "data.frame")
>>>> > Warning message:
>>>> > In aggbalok_mth$year == 2009:2014 :
>>>> >   longer object length is not a multiple of shorter object length
>>>> >
>>>> >
>>>> > Here is the my data:
>>>> >
>>>> > structure(list(month = c(7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,
>>>> > 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
>>>> > 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8,
>>>> > 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3,
>>>> > 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
>>>> > 11), year = c(2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009,
>>>> > 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010,
>>>> > 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010,
>>>> > 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011,
>>>> > 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012,
>>>> > 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,
>>>> > 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,
>>>> > 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015,
>>>> > 2015, 2015, 2015, 2015), x = c(0, 168.7, 203, 149.3, 299.9, 570.7,
>>>> > 424.6, 52.6, 407.7, 210.3, 459.8, 249.2, 59.5, 310.4, 182.7,
>>>> > 433.3, 161, 560.5, 197.5, 6, 68.9, 170.4, 117, 271.2, 133.5,
>>>> > 54.6, 145.5, 122.5, 460.9, 708, 646.7, 58.8, 387.9, 42.9, 190.7,
>>>> > 133.3, 131.7, 158.4, 236.1, 412.9, 462.5, 713.8, 437.1, 140.1,
>>>> > 311.4, 160.3, 202.4, 58.8, 134.7, 120.4, 206.9, 162.5, 68.5,
>>>> > 1025.9, 229.5, 331, 9, 51.4, 102.8, 162.9, 157.2, 32.6, 103.9,
>>>> > 158.7, 139.5, 1371.2, 221.5, 6.1, 19.1, 11, 87.7, 293.3, 87.3,
>>>> > 184, 69.5, 231, 448.2, 39, 19.3, 3.9, 53.8, 141.9, 325, 53.5,
>>>> > 133.3, 321.1, 77.6, 156.5, 2.2)), .Names = c("month", "year",
>>>> > "x"), row.names = c(NA, -89L), class = "data.frame")
>>>> >
>>>> > Thank you very much for any help given.
>>>> > --
>>>> > *Dr. Roslinazairimah Binti Zakaria*
>>>> > *Tel: +609-5492370; Fax. No.+609-5492766*
>>>> >
>>>> > *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
>>>> > roslinaump at gmail.com <roslinaump at gmail.com>*
>>>> > Deputy Dean (Academic & Student Affairs)
>>>> > Faculty of Industrial Sciences & Technology
>>>> > University Malaysia Pahang
>>>> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>>> >
>>>> >         [[alternative HTML version deleted]]
>>>> >
>>>> > ______________________________________________
>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>> > PLEASE do read the posting guide
>>>> > http://www.R-project.org/posting-guide.html
>>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>>
>>> --
>>> Dr. Roslinazairimah Binti Zakaria
>>> Tel: +609-5492370; Fax. No.+609-5492766
>>> Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
>>> Deputy Dean (Academic & Student Affairs)
>>> Faculty of Industrial Sciences & Technology
>>> University Malaysia Pahang
>>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>>
>>
>>
>>
>> --
>> Dr. Roslinazairimah Binti Zakaria
>> Tel: +609-5492370; Fax. No.+609-5492766
>> Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
>> Deputy Dean (Academic & Student Affairs)
>> Faculty of Industrial Sciences & Technology
>> University Malaysia Pahang
>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>
>
>
>
> --
> Dr. Roslinazairimah Binti Zakaria
> Tel: +609-5492370; Fax. No.+609-5492766
> Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
> Deputy Dean (Academic & Student Affairs)
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia


From bhh at xs4all.nl  Fri Jul 29 11:10:22 2016
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 29 Jul 2016 11:10:22 +0200
Subject: [R] store result of loop in df
In-Reply-To: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
References: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
Message-ID: <ADC49C17-0F0E-4D56-944F-C62434E4E9EF@xs4all.nl>


> On 29 Jul 2016, at 10:52, Alain D. via R-help <r-help at r-project.org> wrote:
> 
> Dear list, 
> 
> I have a dataframe df:
> 
> df<-data.frame(x=c(5,32,18,3,17), n=c(11,200,432,20,60))
> 
> Now I want to run n=nrow binom.test() with x being the number of success and n
> the number of trials and then store the results as a new VAR in df.
> 
> I tried 
> 
>  for (i in 1:nrow(df)){
>  df$VAR<-(binom.test(df[i,1],df[i,2],0.065))$estimate[[1]]
> } 
> 
> but bin does only contain the last result. 
> 
> What is wrong with my code? Can anyone help?
> 

How about

for (i in 1:nrow(df)){
 df$VAR[i] <- (binom.test(df[i,1],df[i,2],0.065))$estimate[[1]]
} 

Berend Hasselman


From drjimlemon at gmail.com  Fri Jul 29 11:19:07 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 29 Jul 2016 19:19:07 +1000
Subject: [R] store result of loop in df
In-Reply-To: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
References: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
Message-ID: <CA+8X3fVqvvpTDb+_JE08SR1Qmf_-SdEg+rYtBH4y7NXD92qHNg@mail.gmail.com>

Hi Alain,
You are probably storing the result, replicated five times, in df$VAR.
Each cycle of the loop replaces the last value with the current value.
If you really want the entire output of binom.test in the last column:

multi.binom.test<-function(xs,ns) {
  reslist<-list()
 for(i in 1:length(xs)) reslist[[i]]<-binom.test(xs[i],ns[i])
 return(reslist)
}
df$VAR<-multi.binom.test(df$x,df$n)

I suspect that you probably want only the p.value element.

Jim


On Fri, Jul 29, 2016 at 6:52 PM, Alain D. via R-help
<r-help at r-project.org> wrote:
> Dear list,
>
> I have a dataframe df:
>
> df<-data.frame(x=c(5,32,18,3,17), n=c(11,200,432,20,60))
>
> Now I want to run n=nrow binom.test() with x being the number of success and n
> the number of trials and then store the results as a new VAR in df.
>
> I tried
>
>   for (i in 1:nrow(df)){
>   df$VAR<-(binom.test(df[i,1],df[i,2],0.065))$estimate[[1]]
> }
>
> but bin does only contain the last result.
>
> What is wrong with my code? Can anyone help?
>
> Thank you in advance.
>
> Alain
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Fri Jul 29 11:20:30 2016
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 29 Jul 2016 21:20:30 +1200
Subject: [R] [FORGED]  store result of loop in df
In-Reply-To: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
References: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
Message-ID: <29fdcaf4-68bd-37b3-84b0-d797d57700fb@auckland.ac.nz>

On 29/07/16 20:52, Alain D. via R-help wrote:
> Dear list,
>
> I have a dataframe df:
>
> df<-data.frame(x=c(5,32,18,3,17), n=c(11,200,432,20,60))
>
> Now I want to run n=nrow binom.test() with x being the number of success and n
> the number of trials and then store the results as a new VAR in df.
>
> I tried
>
>   for (i in 1:nrow(df)){
>   df$VAR<-(binom.test(df[i,1],df[i,2],0.065))$estimate[[1]]
> }
>
> but bin does only contain the last result.
>
> What is wrong with my code? Can anyone help?

Try:

for (i in 1:nrow(df)){
   df$VAR[i] <- binom.test(df[i,1],df[i,2],0.065)$estimate[[1]]
}

(Actually I was amazed that this works without initializing df$VAR, but 
it *does* work!  There is no limit to the wondrous nature of R!)

BTW -- calling your data frame "df" is a bad idea.  There is a built-in
function named "df", and on occasion the result of naming some other 
object "df" can be mysterious errors accompanied by opaque error messages.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ivan.calandra at univ-reims.fr  Fri Jul 29 11:32:29 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Fri, 29 Jul 2016 11:32:29 +0200
Subject: [R] store result of loop in df
In-Reply-To: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
References: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
Message-ID: <c2217d37-7525-c2e0-8c26-0b7649b260bd@univ-reims.fr>

Dear Alain,

The problem is that you save the results of each iteration in df$VAR. So 
obviously, you overwrite df$VAR at each iteration.

What you need to do it to create an empty vector that contains the right 
number of elements and then iteratively fill this list. You can then 
combine df and that vector

That would do it:
results <- vector(mode="numeric", length=nrow(df))
for (i in seq_along(results)){
     results[i] <- binom.test(df[i,1],df[i,2],0.065)$estimate[[1]]

}
df$VAR1 <- results

You could also use apply():
foo <- function(x) binom.test(x[[1]],x[[2]],0.065)$estimate
df$VAR2 <- apply(df[,1:2], 1, FUN=foo)

identical(df$VAR1, df$VAR2)

HTH,
Ivan

--
Ivan Calandra, PhD
Scientific Mediator
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
--
https://www.researchgate.net/profile/Ivan_Calandra
https://publons.com/author/705639/

Le 29/07/2016 ? 10:52, Alain D. via R-help a ?crit :
> Dear list,
>
> I have a dataframe df:
>
> df<-data.frame(x=c(5,32,18,3,17), n=c(11,200,432,20,60))
>
> Now I want to run n=nrow binom.test() with x being the number of success and n
> the number of trials and then store the results as a new VAR in df.
>
> I tried
>
>    for (i in 1:nrow(df)){
>    df$VAR<-(binom.test(df[i,1],df[i,2],0.065))$estimate[[1]]
> }
>
> but bin does only contain the last result.
>
> What is wrong with my code? Can anyone help?
>
> Thank you in advance.
>
> Alain
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ivan.calandra at univ-reims.fr  Fri Jul 29 11:33:53 2016
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Fri, 29 Jul 2016 11:33:53 +0200
Subject: [R] [FORGED] store result of loop in df
In-Reply-To: <29fdcaf4-68bd-37b3-84b0-d797d57700fb@auckland.ac.nz>
References: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
	<29fdcaf4-68bd-37b3-84b0-d797d57700fb@auckland.ac.nz>
Message-ID: <ca7d4959-c24a-e19f-6179-94a5b6bbfa33@univ-reims.fr>


Le 29/07/2016 ? 11:20, Rolf Turner a ?crit :
> On 29/07/16 20:52, Alain D. via R-help wrote:
>> Dear list,
>>
>> I have a dataframe df:
>>
>> df<-data.frame(x=c(5,32,18,3,17), n=c(11,200,432,20,60))
>>
>> Now I want to run n=nrow binom.test() with x being the number of 
>> success and n
>> the number of trials and then store the results as a new VAR in df.
>>
>> I tried
>>
>>   for (i in 1:nrow(df)){
>>   df$VAR<-(binom.test(df[i,1],df[i,2],0.065))$estimate[[1]]
>> }
>>
>> but bin does only contain the last result.
>>
>> What is wrong with my code? Can anyone help?
>
> Try:
>
> for (i in 1:nrow(df)){
>   df$VAR[i] <- binom.test(df[i,1],df[i,2],0.065)$estimate[[1]]
> }
>
> (Actually I was amazed that this works without initializing df$VAR, 
> but it *does* work!  There is no limit to the wondrous nature of R!)
I was so sure that it wouldn't work that I haven't even tried!
>
> BTW -- calling your data frame "df" is a bad idea.  There is a built-in
> function named "df", and on occasion the result of naming some other 
> object "df" can be mysterious errors accompanied by opaque error 
> messages.
>
> cheers,
>
> Rolf Turner
>


From marco.scutari at gmail.com  Fri Jul 29 11:34:58 2016
From: marco.scutari at gmail.com (Marco Scutari)
Date: Fri, 29 Jul 2016 10:34:58 +0100
Subject: [R] cpquery problem
In-Reply-To: <00e101d1e963$9e770790$db6516b0$@ecogeonomix.com>
References: <00e101d1e963$9e770790$db6516b0$@ecogeonomix.com>
Message-ID: <CA+RJqXX5R7M_pJaCBLeNKYYjyJyuJhRNK5TAS2LPmWB3d7W3Sw@mail.gmail.com>

Hi Ross,

 first, I have a side question: is there a particular reason why you
are using parse(eval()) in your queries? I know sometimes there is no
other solution if you only use exported functions, but you should try
not to. It makes for brittle code that breaks easily depending on how
variables are scoped.

On 29 July 2016 at 07:37,  <ross.chapman at ecogeonomix.com> wrote:
> However, if I replace EST=='x' with EST=='z' or EST=='y' I get 0 probability
> of obtaining a value for ABW that is either greater or less than the
> threshold.
>
> For example:
>
>> cpquery(fitted,event=(ABW>=11), evidence=eval(parse(text="(EST=='y' & TR>9
> & BU>15819 &  RF>2989)")),n=10^6)
>
> [1] 0
>
> and
>
>> cpquery(fitted,event=(ABW<=11), evidence=eval(parse(text="(EST=='y' & TR>9
> & BU>15819 &  RF>2989)")),n=10^6)
>
> [1] 0

>From this output, my guess is that the evidence has probability
(exactly or close to) zero. You can check by running cpdist(): if the
evidence has probability zero, no random samples will be returned.
Turning on the debugging output in cpquery() should also highlight
what the problem is.

If that turns out to be the case, switch from the default method =
"ls" to method = "lw" in cpdist(). (Note that the syntax changes
slightly, check the documentation for examples.)

> My own knowledge from the data is that these classes should both typically
> return a value for ABW that is very much higher than the threshold value.

That may be, but much depends on the specific sample the model was
fitted from. How does the fitted network look like?

Cheers,
    Marco

-- 
Marco Scutari, Ph.D.
Lecturer in Statistics, Department of Statistics
University of Oxford, United Kingdom


From f_j_rod at hotmail.com  Fri Jul 29 12:52:05 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Fri, 29 Jul 2016 12:52:05 +0200
Subject: [R] Strange message after reading multiple scripts from one folder
Message-ID: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>

Dear list,
 
I have one folder named "scripts_JMbayes", wich contains 10 R scripts.
I can read them properly by doing:
 
> pathnames <- list.files(pattern="[.]R", path="Mydir/scripts_JMbayes", full.names = TRUE)
> sapply(pathnames, USE.NAMES = FALSE, FUN = source,)
 
However, R generates the following message:
 
        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
value   ?     ?     ?     ?     ?     ?     ?     ?     ?     ?         
visible FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 
What does it mean and what should I change to avoid this message?
Any help would be appreciated!
 
Best,
 
Frank

 		 	   		  
	[[alternative HTML version deleted]]


From jholtman at gmail.com  Fri Jul 29 14:26:09 2016
From: jholtman at gmail.com (jim holtman)
Date: Fri, 29 Jul 2016 08:26:09 -0400
Subject: [R] Strange message after reading multiple scripts from one
	folder
In-Reply-To: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>
References: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>
Message-ID: <CAAxdm-7PVDoQaNmDYPsYXob2kxUTozxEKDpHT2vPRmk1D2Mh=w@mail.gmail.com>

Hard to tell without seeing the scripts.  Do you have a matrix in your
scripts that have "value" and "visible" as row names?  You probably have
some statement that is causing output and so the problem is "your" as to
how to avoid the message.  So look at your scripts to see if anything
refers to either "value" or "visible", and then you might find the cause of
your problem.


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Fri, Jul 29, 2016 at 6:52 AM, Frank S. <f_j_rod at hotmail.com> wrote:

> Dear list,
>
> I have one folder named "scripts_JMbayes", wich contains 10 R scripts.
> I can read them properly by doing:
>
> > pathnames <- list.files(pattern="[.]R", path="Mydir/scripts_JMbayes",
> full.names = TRUE)
> > sapply(pathnames, USE.NAMES = FALSE, FUN = source,)
>
> However, R generates the following message:
>
>         [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
> value   ?     ?     ?     ?     ?     ?     ?     ?     ?     ?
> visible FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
>
> What does it mean and what should I change to avoid this message?
> Any help would be appreciated!
>
> Best,
>
> Frank
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Fri Jul 29 14:44:20 2016
From: jfox at mcmaster.ca (Fox, John)
Date: Fri, 29 Jul 2016 12:44:20 +0000
Subject: [R] Strange message after reading multiple scripts from one
 folder
In-Reply-To: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>
References: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC836547612@FHSDB2D11-2.csu.mcmaster.ca>

Dear Frank,

What you see isn't a "message" but the result returned by sapply(). The ?s indicate that sapply() didn't know what to do with the corresponding element. In an individual use of source(), the result, a 2-element list, is returned invisibly, so you don't see it. 

To see what's going on, try 

res <- lapply(pathnames, FUN = source)
str(res)

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank S.
> Sent: July 29, 2016 6:52 AM
> To: r-help at r-project.org
> Subject: [R] Strange message after reading multiple scripts from one folder
> 
> Dear list,
> 
> I have one folder named "scripts_JMbayes", wich contains 10 R scripts.
> I can read them properly by doing:
> 
> > pathnames <- list.files(pattern="[.]R", path="Mydir/scripts_JMbayes",
> > full.names = TRUE) sapply(pathnames, USE.NAMES = FALSE, FUN = source,)
> 
> However, R generates the following message:
> 
>         [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
> value   ?     ?     ?     ?     ?     ?     ?     ?     ?     ?
> visible FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
> 
> What does it mean and what should I change to avoid this message?
> Any help would be appreciated!
> 
> Best,
> 
> Frank
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Keith.Jewell at campdenbri.co.uk  Fri Jul 29 14:47:57 2016
From: Keith.Jewell at campdenbri.co.uk (Keith Jewell)
Date: Fri, 29 Jul 2016 13:47:57 +0100
Subject: [R] Strange message after reading multiple scripts from one
	folder
In-Reply-To: <CAAxdm-7PVDoQaNmDYPsYXob2kxUTozxEKDpHT2vPRmk1D2Mh=w@mail.gmail.com>
References: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>
	<CAAxdm-7PVDoQaNmDYPsYXob2kxUTozxEKDpHT2vPRmk1D2Mh=w@mail.gmail.com>
Message-ID: <nnfj9t$nji$1@ger.gmane.org>

I can't immediately see it in the help text but it seems that source 
returns a list with two named elements; value and visible.

I surmise that it is returned using withVisible (qv).

KJ

On 29/07/2016 13:26, jim holtman wrote:
> Hard to tell without seeing the scripts.  Do you have a matrix in your
> scripts that have "value" and "visible" as row names?  You probably have
> some statement that is causing output and so the problem is "your" as to
> how to avoid the message.  So look at your scripts to see if anything
> refers to either "value" or "visible", and then you might find the cause of
> your problem.
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Fri, Jul 29, 2016 at 6:52 AM, Frank S. <f_j_rod at hotmail.com> wrote:
>
>> Dear list,
>>
>> I have one folder named "scripts_JMbayes", wich contains 10 R scripts.
>> I can read them properly by doing:
>>
>>> pathnames <- list.files(pattern="[.]R", path="Mydir/scripts_JMbayes",
>> full.names = TRUE)
>>> sapply(pathnames, USE.NAMES = FALSE, FUN = source,)
>>
>> However, R generates the following message:
>>
>>          [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
>> value   ?     ?     ?     ?     ?     ?     ?     ?     ?     ?
>> visible FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
>>
>> What does it mean and what should I change to avoid this message?
>> Any help would be appreciated!
>>
>> Best,
>>
>> Frank
>>
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> 	[[alternative HTML version deleted]]
>


From roger.bos at rothschild.com  Fri Jul 29 15:51:54 2016
From: roger.bos at rothschild.com (Bos, Roger)
Date: Fri, 29 Jul 2016 13:51:54 +0000
Subject: [R] problems reading XML type file from ishares website
In-Reply-To: <422D9653-527E-4C08-B75D-37B53F348A6C@dcn.davis.ca.us>
References: <0765308CD028654885F30322557308D82008D92B@NYCSM0208.rth.ad.rothschild.com>
	<6BEAB4B9-2051-4125-B619-59408C11BFB1@dcn.davis.ca.us>
	<0765308CD028654885F30322557308D82008DB41@NYCSM0208.rth.ad.rothschild.com>
	<85B8F049-BD52-43DA-8288-000E7F68C252@dcn.davis.ca.us>
	<422D9653-527E-4C08-B75D-37B53F348A6C@dcn.davis.ca.us>
Message-ID: <0765308CD028654885F30322557308D82008E0EE@NYCSM0208.rth.ad.rothschild.com>

Jeff,

Thanks so much for your help.  I feel pretty confident in saying that there is no way I could have figured out how to open that file (in R) by myself.  It was hard enough to get the data I needed once I could read the file.  In case anyone on the list is interested, here is a working solution to download the S&P 500 weights, though I am sure there is a better way than mine:

  library(XML)
  temp <- "https://www.ishares.com/us/239726/fund-download.dl"
  fname <- "C:/pit/" %+% etf %+% "_FundHoldings.xls"
  download.file(url = temp, destfile = fname)
  txt <- readLines(fname, encoding="UTF-8-BOM" )
  txt <- sub( "</Style>", "</ss:Style>", txt )
  fnamenobom  <- "nobom.xml"
  cat( paste( txt, collapse="\n" ), file=fnamenobom )
  xmlfile  <- xmlTreeParse(fnamenobom)
  xmltop = xmlRoot(xmlfile)
  xml_data <- xmlToList(xmlfile)

  datadate <- unlist(xml_data[["Worksheet"]][[1]])[1]
  d <- list()
  for (ii in 10:length(xml_data[["Worksheet"]])) {
    rd <- unlist(xml_data[["Worksheet"]][[ii]])
    d[[ii]] <- data.frame(Symbol=rd[1], weight=as.numeric(rd[10]), ISIN=rd[31])
  }
  out <- rbindlist(d)
  out$datadate <- datadate
  out

Thanks,

Roger





***************************************************************
This message and any attachments are for the intended recipient's use only.
This message may contain confidential, proprietary or legally privileged
information. No right to confidential or privileged treatment
of this message is waived or lost by an error in transmission.
If you have received this message in error, please immediately
notify the sender by e-mail, delete the message, any attachments and all
copies from your system and destroy any hard copies.  You must
not, directly or indirectly, use, disclose, distribute,
print or copy any part of this message or any attachments if you are not
the intended recipient.


-----Original Message-----
From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us]
Sent: Thursday, July 28, 2016 2:55 PM
To: Bos, Roger; R-help
Subject: Re: [R] problems reading XML type file from ishares website

Er, I failed to include the step to write the repaired data to a file...

fnamenobom  <- "nobom.xml"
cat( paste( txt, collapse="\n" ), file=fnamenobom ) xmlfile  <- xmlTreeParse( fnamenobom )

--
Sent from my phone. Please excuse my brevity.

On July 28, 2016 11:20:23 AM PDT, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>Please keep the list included in the thread (e.g. reply-all?).
>
>I looked at the file and agree that it looks like xml with a utf8 byte
>order mark and Unix line endings, which means it is not XLS and it is
>not XLSX (which is a zipped directory of xml files with DOS line
>endings). Excel complains but manages to open the file if it has the
>XLS extension,  but I am not aware that any of the usual R Excel
>packages will understand this file.
>
>The byte order mark can be addressed by opening the file with
>encoding="UTF-8-BOM", but as you mentioned originally the XML structure
>is still broken (c.f. the error message about the Style ending tag).
>Line 16 seems to use /Style rather than /ss:Style. Maybe
>
>library(XML)
>txt <- readLines( fname, encoding="UTF-8-BOM" ) txt <- sub( "</Style>",
>"</ss:Style>", txt ) fnamenobom  <- "nobom.xml"
>xmlfile  <- xmlTreeParse( "nobom.xml" )


From morrisk10 at xavier.edu  Fri Jul 29 15:44:42 2016
From: morrisk10 at xavier.edu (Morris, Kathryn)
Date: Fri, 29 Jul 2016 13:44:42 +0000
Subject: [R] metafor estimates using mods and subset do not match
Message-ID: <CO2PR01MB21818F8F8047919D7AD70CF7C3010@CO2PR01MB2181.prod.exchangelabs.com>

I am running a meta-analysis using metafor and getting what seem to be conflicting results.

#analysis with species moderator 
cropMeta.species<-rma(cropyi, cropvi, data=dataCropMeta, mods=~dataCropMeta$species - 1, method="HE")

The output for the analysis using species as a moderator shows estimates for the seven species in my data set, and makes sense.

#subset analysis with B oleraceae
cropMeta.species.b.oleracea<-rma(cropyi, cropvi, data=dataCropMeta, subset=(species=="B. oleracea"), method="HE")

Then when I start to plot the results and use separate analyses for each species in order to get the coef and variances for plotting the subgroups I get different estimates. The estimates for most subgroups are almost identical in both analyses, but one of them is 5.922 in the analysis with species as a moderator, and 7.0139 in the subset analysis for only that species.

Why are the estimates different? If they should be different, then which analysis is producing the correct estimates?

I'm happy to provide the data and R scripts if that would help.

many thanks,
Kathryn  
 
 
---------------------------------

 Dr. Kathryn Morris
 Assistant Professor, Biology
 Xavier University
 3800 Victory Parkway
 Cincinnati, OH
 45207

 Office: (513) 745-3554
      

From wdunlap at tibco.com  Fri Jul 29 16:54:43 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 29 Jul 2016 07:54:43 -0700
Subject: [R] Reduce woes
In-Reply-To: <CAG7vnky2boUP9t1nrYfPbWjNSUd6DBvX8cNfvO3hPAK0xCDu_Q@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
	<CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>
	<CAG7vnkyKwkXY-5J1svBK=xot_iDdrdBq5+ryUzAqahg0TAKGEQ@mail.gmail.com>
	<CAOjnRsZ8nODDbCe4Xns8AqSo8KDukh97AzQOkemAxpK=i0ATDA@mail.gmail.com>
	<CAOjnRsYoLQbhPTePt9J-th4O56GOEDroSB-NZ+qwmnGdveyr2A@mail.gmail.com>
	<CAG7vnky2boUP9t1nrYfPbWjNSUd6DBvX8cNfvO3hPAK0xCDu_Q@mail.gmail.com>
Message-ID: <CAF8bMcZRAJngoECE31QKQnF6frgJc5n_gXAK7XxQ5kTsHB8C+Q@mail.gmail.com>

Reduce (like lapply) apparently uses the [[ operator to
extract components from the list given to it. X[[i]] does
not attach names(X)[i] to its output (where would it put it?).
Hence your se

To help understand what these functions are doing try
putting print statements in your test functions:
> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
> r <- Reduce(function(acc, item) { cat("acc="); str(acc) ; cat("item=");
str(item); length(item) }, data, init=list())
acc= list()
item= num [1:2] 1 1
acc= int 2
item= num 3
acc= int 1
item= num [1:2] 2 2
> data2 <- list(one = c(oneA=1, onB=1), three = c(threeA=3), two =
c(twoA=2, twoB=2))
> r <- Reduce(function(acc, item) { cat("acc="); str(acc) ; cat("item=");
str(item); length(item) }, data2, init=list())
acc= list()
item= Named num [1:2] 1 1
 - attr(*, "names")= chr [1:2] "oneA" "onB"
acc= int 2
item= Named num 3
 - attr(*, "names")= chr "threeA"
acc= int 1
item= Named num [1:2] 2 2
 - attr(*, "names")= chr [1:2] "twoA" "twoB"


I still don't understand why you want Reduce to to lapply's
job.   Reduce maps many to one and lapply maps many to
many.



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Jul 29, 2016 at 1:37 AM, Stefan Kruger <stefan.kruger at gmail.com>
wrote:

> Jeremiah -
>
> neat - that's one step closer, but one small thing I still don't
> understand:
>
> > data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
> > r = Reduce(function(acc, item) { append(acc, setNames(length(item),
> names(item))) }, data, list())
> > str(r)
> List of 3
>  $ : int 2
>  $ : int 1
>  $ : int 2
>
> I wanted the names to remain, but it seems like the "data" parameter loses
> its names when consumed by the Reduce()? If I print "item" inside the
> reducing function, it's not got the names. I'm probably missing some
> central tenet of R here.
>
> As to your comment of this being lapply() implemented by Reduce() - as I
> understand lapply()  (or map() in other functional languages), it's limited
> to returning a list/vector of the same length as the original. Consider
> this contrived example:
>
> > r = Reduce(function(acc, item) { if (length(item) > 1) {append(acc,
> setNames(length(item), names(item)))} }, data, list())
> > str(r)
>  int 2
> > r
> [1] 2
>
> I don't think you could achieve that with lapply()?
>
> Thanks
>
> Stefan
>
>
> On 28 July 2016 at 20:19, jeremiah rounds <roundsjeremiah at gmail.com>
> wrote:
>
> > Basically using Reduce as an lapply in that example, but I think that was
> > caused by how people started talking about things in the first place =)
> But
> > the point is the accumulator can be anything as far as I can tell.
> >
> > On Thu, Jul 28, 2016 at 12:14 PM, jeremiah rounds <
> > roundsjeremiah at gmail.com> wrote:
> >
> >> Re:
> >> "What I'm trying to
> >> work out is how to have the accumulator in Reduce not be the same type
> as
> >> the elements of the vector/list being reduced - ideally it could be an
> S3
> >> instance, list, vector, or data frame."
> >>
> >> Pretty sure that is not true.  See code that follows.  I would never
> >> solve this task in this way though so no comment on the use of Reduce
> for
> >> what you described.  (Note the accumulation of "functions" in a list is
> >> just a demo of possibilities).  You could accumulate in an environment
> too
> >> and potentially gain a lot of copy efficiency.
> >>
> >>
> >> lookup = list()
> >> lookup[[as.character(1)]] = function() print("1")
> >> lookup[[as.character(2)]] = function() print("2")
> >> lookup[[as.character(3)]] = function() print("3")
> >>
> >> data = list(c(1,2), c(1,4), c(3,3), c(2,30))
> >>
> >>
> >> r = Reduce(function(acc, item) {
> >> append(acc, list(lookup[[as.character(min(item))]]))
> >> }, data,list())
> >> r
> >> for(f in r) f()
> >>
> >>
> >> On Thu, Jul 28, 2016 at 5:09 AM, Stefan Kruger <stefan.kruger at gmail.com
> >
> >> wrote:
> >>
> >>> Ulrik - many thanks for your reply.
> >>>
> >>> I'm aware of many simple solutions as the one you suggest, both
> iterative
> >>> and functional style - but I'm trying to learn how to bend Reduce() for
> >>> the
> >>> purpose of using it in more complex processing tasks. What I'm trying
> to
> >>> work out is how to have the accumulator in Reduce not be the same type
> as
> >>> the elements of the vector/list being reduced - ideally it could be an
> S3
> >>> instance, list, vector, or data frame.
> >>>
> >>> Here's a more realistic example (in Elixir, sorry)
> >>>
> >>> Given two lists:
> >>>
> >>> 1. data: maps an id string to a vector of revision strings
> >>> 2. dict: maps known id/revision pairs as a string to true (or 1)
> >>>
> >>> find the items in data not already in dict, returned as a named list.
> >>>
> >>> ```elixir
> >>> data = %{
> >>>     "id1" => ["rev1.1", "rev1.2"],
> >>>     "id2" => ["rev2.1"],
> >>>     "id3" => ["rev3.1", "rev3.2", "rev3.3"]
> >>> }
> >>>
> >>> dict = %{
> >>>     "id1/rev1.1" => 1,
> >>>     "id1/rev1.2" => 1,
> >>>     "id3/rev3.1" => 1
> >>> }
> >>>
> >>> # Find the items in data not already in dict. Return as a grouped map
> >>>
> >>> Map.keys(data)
> >>>     |> Enum.flat_map(fn id -> Enum.map(data[id], fn rev -> {id, rev}
> end)
> >>> end)
> >>>     |> Enum.filter(fn {id, rev} -> !Dict.has_key?(dict, "#{id}/#{rev}")
> >>> end)
> >>>     |> Enum.reduce(%{}, fn ({k, v}, d) -> Map.update(d, k, [v],
> &[v|&1])
> >>> end)
> >>> ```
> >>>
> >>>
> >>>
> >>>
> >>> On 28 July 2016 at 12:03, Ulrik Stervbo <ulrik.stervbo at gmail.com>
> wrote:
> >>>
> >>> > Hi Stefan,
> >>> >
> >>> > in that case,lapply(data, length) should do the trick.
> >>> >
> >>> > Best wishes,
> >>> > Ulrik
> >>> >
> >>> > On Thu, 28 Jul 2016 at 12:57 Stefan Kruger <stefan.kruger at gmail.com>
> >>> > wrote:
> >>> >
> >>> >> David - many thanks for your response.
> >>> >>
> >>> >> What I tried to do was to turn
> >>> >>
> >>> >> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
> >>> >>
> >>> >> into
> >>> >>
> >>> >> result <- list(one = 2, three = 1, two = 2)
> >>> >>
> >>> >> that is creating a new list which has the same names as the first,
> but
> >>> >> where the values are the vector lengths.
> >>> >>
> >>> >> I know there are many other (and better) trivial ways of achieving
> >>> this -
> >>> >> my aim is less the task itself, and more figuring out if this can be
> >>> done
> >>> >> using Reduce() in the fashion I showed in the other examples I gave.
> >>> It's
> >>> >> a
> >>> >> building block of doing map-filter-reduce type pipelines that I'd
> >>> like to
> >>> >> understand how to do in R.
> >>> >>
> >>> >> Fumbling in the dark, I tried:
> >>> >>
> >>> >> Reduce(function(acc, item) { setNames(c(acc, length(data[item])),
> >>> item },
> >>> >> names(data), accumulate=TRUE)
> >>> >>
> >>> >> but setNames sets all the names, not adding one - and acc is still a
> >>> >> vector, not a list.
> >>> >>
> >>> >> It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()'
> aim
> >>> at
> >>> >> doing what I'd like to do - but I've not been able to figure out
> quite
> >>> >> how.
> >>> >>
> >>> >> Thanks
> >>> >>
> >>> >> Stefan
> >>> >>
> >>> >>
> >>> >>
> >>> >> On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net>
> >>> wrote:
> >>> >>
> >>> >> >
> >>> >> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <
> >>> stefan.kruger at gmail.com>
> >>> >> > wrote:
> >>> >> > >
> >>> >> > > Hi -
> >>> >> > >
> >>> >> > > I'm new to R.
> >>> >> > >
> >>> >> > > In other functional languages I'm familiar with you can often
> >>> seed a
> >>> >> call
> >>> >> > > to reduce() with a custom accumulator. Here's an example in
> >>> Elixir:
> >>> >> > >
> >>> >> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
> >>> >> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
> >>> >> > > Enum.count(v), nil) end)
> >>> >> > > # %{"one" => 2, "three" => 1, "two" => 2}
> >>> >> > >
> >>> >> > > In R-terms that's reducing a list of vectors to become a new
> list
> >>> >> mapping
> >>> >> > > the names to the vector lengths.
> >>> >> > >
> >>> >> > > Even in JavaScript, you can do similar things:
> >>> >> > >
> >>> >> > > list = { one: [1, 1], three: [3], two: [2, 2] };
> >>> >> > > var result = Object.keys(list).reduceRight(function (acc, item)
> {
> >>> >> > >  acc[item] = list[item].length;
> >>> >> > >  return acc;
> >>> >> > > }, {});
> >>> >> > > // result == { two: 2, three: 1, one: 2 }
> >>> >> > >
> >>> >> > > In R, from what I can gather, Reduce() is restricted such that
> any
> >>> >> init
> >>> >> > > value you feed it is required to be of the same type as the
> >>> elements
> >>> >> of
> >>> >> > the
> >>> >> > > vector you're reducing -- so I can't build up. So whilst I can
> >>> do, say
> >>> >> > >
> >>> >> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
> >>> >> > > [1] 111
> >>> >> > >
> >>> >> > > I can't use Reduce to build up a list, vector or data frame?
> >>> >> > >
> >>> >> > > What am I missing?
> >>> >> > >
> >>> >> > > Many thanks for any pointers,
> >>> >> >
> >>> >> > This builds a list:
> >>> >> >
> >>> >> > > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
> >>> >> > accumulate=TRUE)
> >>> >> > [[1]]
> >>> >> > [1] 96
> >>> >> >
> >>> >> > [[2]]
> >>> >> > [1] 96  1
> >>> >> >
> >>> >> > [[3]]
> >>> >> > [1] 96  1  2
> >>> >> >
> >>> >> > [[4]]
> >>> >> > [1] 96  1  2  3
> >>> >> >
> >>> >> > [[5]]
> >>> >> > [1] 96  1  2  3  4
> >>> >> >
> >>> >> > [[6]]
> >>> >> > [1] 96  1  2  3  4  5
> >>> >> >
> >>> >> > But you are not saying what you want. The other examples were
> doing
> >>> >> > something with names but you provided no names for the R example.
> >>> >> >
> >>> >> > This would return a list of named vectors:
> >>> >> >
> >>> >> > > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))
> >>> },
> >>> >> > c(1,2,3,4,5), 96, accumulate=TRUE)
> >>> >> > [[1]]
> >>> >> > [1] 96
> >>> >> >
> >>> >> > [[2]]
> >>> >> >  1  2
> >>> >> > 96  1
> >>> >> >
> >>> >> > [[3]]
> >>> >> >  1  2  3
> >>> >> > 96  1  2
> >>> >> >
> >>> >> > [[4]]
> >>> >> >  1  2  3  4
> >>> >> > 96  1  2  3
> >>> >> >
> >>> >> > [[5]]
> >>> >> >  1  2  3  4  5
> >>> >> > 96  1  2  3  4
> >>> >> >
> >>> >> > [[6]]
> >>> >> >  1  2  3  4  5  6
> >>> >> > 96  1  2  3  4  5
> >>> >> >
> >>> >> >
> >>> >> >
> >>> >> >
> >>> >> > > Stefan
> >>> >> > >
> >>> >> > >
> >>> >> > >
> >>> >> > > --
> >>> >> > > Stefan Kruger <stefan.kruger at gmail.com>
> >>> >> > >
> >>> >> > >       [[alternative HTML version deleted]]
> >>> >> > >
> >>> >> > > ______________________________________________
> >>> >> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> >>> >> > > https://stat.ethz.ch/mailman/listinfo/r-help
> >>> >> > > PLEASE do read the posting guide
> >>> >> > http://www.R-project.org/posting-guide.html
> >>> >> > > and provide commented, minimal, self-contained, reproducible
> code.
> >>> >> >
> >>> >> > David Winsemius
> >>> >> > Alameda, CA, USA
> >>> >> >
> >>> >> >
> >>> >>
> >>> >>
> >>> >> --
> >>> >> Stefan Kruger <stefan.kruger at gmail.com>
> >>> >>
> >>> >>         [[alternative HTML version deleted]]
> >>> >>
> >>> >> ______________________________________________
> >>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> >> PLEASE do read the posting guide
> >>> >> http://www.R-project.org/posting-guide.html
> >>> >> and provide commented, minimal, self-contained, reproducible code.
> >>> >>
> >>> >
> >>>
> >>>
> >>> --
> >>> Stefan Kruger <stefan.kruger at gmail.com>
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >>
> >
>
>
> --
> Stefan Kruger <stefan.kruger at gmail.com>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Jul 29 17:12:22 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 29 Jul 2016 08:12:22 -0700
Subject: [R] store result of loop in df
In-Reply-To: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
References: <1558342546.632945.e1cc5721-5916-4f10-b14f-1f6ef4b3b2b4.open-xchange@com4.strato.de>
Message-ID: <CAGxFJbTgnC5asgTk6b=rXyDKJXHJLdveP32rsLug+TyBeEywMA@mail.gmail.com>

What's wrong with:

result <- with(df, x/n)  ? Do I misunderstand?

(and what's the p = .065 or [[1]] have to do with the estimated prob,
as you are not actually doing any test?)

Finally, if you really do this sort of thing, ?mapply (and ?with) can
be useful; e.g. you could have rewritten your loop as:

> with(df,mapply(function(...)binom.test(...)$estimate,x,n,MoreArgs = list(p=.065)))

probability of success probability of success probability of success
            0.45454545             0.16000000             0.04166667
probability of success probability of success
            0.15000000             0.28333333


Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jul 29, 2016 at 1:52 AM, Alain D. via R-help
<r-help at r-project.org> wrote:
> Dear list,
>
> I have a dataframe df:
>
> df<-data.frame(x=c(5,32,18,3,17), n=c(11,200,432,20,60))
>
> Now I want to run n=nrow binom.test() with x being the number of success and n
> the number of trials and then store the results as a new VAR in df.
>
> I tried
>
>   for (i in 1:nrow(df)){
>   df$VAR<-(binom.test(df[i,1],df[i,2],0.065))$estimate[[1]]
> }
>
> but bin does only contain the last result.
>
> What is wrong with my code? Can anyone help?
>
> Thank you in advance.
>
> Alain
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Gaurang.Mehta at royallondon.com  Fri Jul 29 17:14:29 2016
From: Gaurang.Mehta at royallondon.com (Mehta, Gaurang)
Date: Fri, 29 Jul 2016 16:14:29 +0100
Subject: [R] Using lapply or mapply
Message-ID: <3BB657B92C75C74385A95FAA7C6B17161F0D472E9F@VRTPRDEXM02.royallondongroup.com>

Hi Team,
I am new to using apply function in R.
I want to find out the empirical quantiles of all items in a list.


## Fitting Kernal Density function
Emp_Marginals<-apply(M_Diff_Final,2,kde)
### Simulated variables
Sim_Cop<-abs(rmvnorm(10000,mean=apply(M_Diff_Final,2,mean),sigma=cov(M_Diff_Final),method="chol"))
####Now I am trying to use mapply but no help

# Sim_Quantiles<-matrix(mapply(qkde,Sim_Cop,fhat=Emp_Marginals,SIMPLIFY = F),nrow=10,byrow=T)
#### Matrix Def

Sim_Quantiles<-matrix(0,nrow=nrow(Sim_Cop),ncol=ncol(Sim_Cop))

### The following code works but I want to avoid using for loops
for (j in 1:ncol(Sim_Quantiles)){
    for (i in 1:nrow(Sim_Cop)){
   Sim_Quantiles[i,j]<-qkde(Sim_Cop[i,j],Emp_Marginals[[j]])
    }
  }

Thanks and Regards,
Gaurang Mehta

This email is intended for the person or company named and access by anyone else is unauthorised. If you are not the person or company named, please delete this email and notify the sender.

The information in this email, including any attachments, may be confidential or legally privileged (meaning that its disclosure is protected in law). Its unauthorised disclosure, copying, distribution or use is prohibited and may be unlawful.

Email communications sent over the internet are not guaranteed to be secure or virus-free and such messages are potentially at risk.  The Royal London Group accepts no liability for any claims arising from use of the internet to transmit messages by or to any company within the Royal London Group.

The Royal London Group consists of The Royal London Mutual Insurance Society Limited and its subsidiaries.

The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority and provides life assurance and pensions.

Registered in England and Wales number 99064.

Registered office: 55 Gracechurch Street, London, EC3V 0RL.

In the Republic of Ireland: The Royal London Mutual Insurance Society Limited is authorised by the Prudential Regulation Authority in the UK and is regulated by the Central Bank of Ireland for conduct of business rules.


	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Jul 29 17:18:03 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 29 Jul 2016 08:18:03 -0700
Subject: [R] metafor estimates using mods and subset do not match
In-Reply-To: <CO2PR01MB21818F8F8047919D7AD70CF7C3010@CO2PR01MB2181.prod.exchangelabs.com>
References: <CO2PR01MB21818F8F8047919D7AD70CF7C3010@CO2PR01MB2181.prod.exchangelabs.com>
Message-ID: <CAGxFJbQ978NjhYL4HCXMfHv4zdJ4NL+n+6BMij3xSpJjmvTicg@mail.gmail.com>

I think you should consult with a local statistician. Generally
speaking, statistical questions like this tend to be OT here, and you
appear to be sufficiently confused about the statistical issues that
online posts would not be sufficient.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Jul 29, 2016 at 6:44 AM, Morris, Kathryn <morrisk10 at xavier.edu> wrote:
> I am running a meta-analysis using metafor and getting what seem to be conflicting results.
>
> #analysis with species moderator
> cropMeta.species<-rma(cropyi, cropvi, data=dataCropMeta, mods=~dataCropMeta$species - 1, method="HE")
>
> The output for the analysis using species as a moderator shows estimates for the seven species in my data set, and makes sense.
>
> #subset analysis with B oleraceae
> cropMeta.species.b.oleracea<-rma(cropyi, cropvi, data=dataCropMeta, subset=(species=="B. oleracea"), method="HE")
>
> Then when I start to plot the results and use separate analyses for each species in order to get the coef and variances for plotting the subgroups I get different estimates. The estimates for most subgroups are almost identical in both analyses, but one of them is 5.922 in the analysis with species as a moderator, and 7.0139 in the subset analysis for only that species.
>
> Why are the estimates different? If they should be different, then which analysis is producing the correct estimates?
>
> I'm happy to provide the data and R scripts if that would help.
>
> many thanks,
> Kathryn
>
>
> ---------------------------------
>
>  Dr. Kathryn Morris
>  Assistant Professor, Biology
>  Xavier University
>  3800 Victory Parkway
>  Cincinnati, OH
>  45207
>
>  Office: (513) 745-3554
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From lists at dewey.myzen.co.uk  Fri Jul 29 17:33:27 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 29 Jul 2016 16:33:27 +0100
Subject: [R] metafor estimates using mods and subset do not match
In-Reply-To: <CO2PR01MB21818F8F8047919D7AD70CF7C3010@CO2PR01MB2181.prod.exchangelabs.com>
References: <CO2PR01MB21818F8F8047919D7AD70CF7C3010@CO2PR01MB2181.prod.exchangelabs.com>
Message-ID: <f5725cf7-9771-bb28-b4c9-fc403f2a70d4@dewey.myzen.co.uk>

Dear Kathryn

I think that the author of metafor has addressed this

http://www.metafor-project.org/doku.php/tips:comp_two_independent_estimates

The other tips on that site are well worth reading too.

On 29/07/2016 14:44, Morris, Kathryn wrote:
> I am running a meta-analysis using metafor and getting what seem to be conflicting results.
>
> #analysis with species moderator
> cropMeta.species<-rma(cropyi, cropvi, data=dataCropMeta, mods=~dataCropMeta$species - 1, method="HE")
>
> The output for the analysis using species as a moderator shows estimates for the seven species in my data set, and makes sense.
>
> #subset analysis with B oleraceae
> cropMeta.species.b.oleracea<-rma(cropyi, cropvi, data=dataCropMeta, subset=(species=="B. oleracea"), method="HE")
>
> Then when I start to plot the results and use separate analyses for each species in order to get the coef and variances for plotting the subgroups I get different estimates. The estimates for most subgroups are almost identical in both analyses, but one of them is 5.922 in the analysis with species as a moderator, and 7.0139 in the subset analysis for only that species.
>
> Why are the estimates different? If they should be different, then which analysis is producing the correct estimates?
>
> I'm happy to provide the data and R scripts if that would help.
>
> many thanks,
> Kathryn
>
>
> ---------------------------------
>
>  Dr. Kathryn Morris
>  Assistant Professor, Biology
>  Xavier University
>  3800 Victory Parkway
>  Cincinnati, OH
>  45207
>
>  Office: (513) 745-3554
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From stefan.kruger at gmail.com  Fri Jul 29 17:43:16 2016
From: stefan.kruger at gmail.com (Stefan Kruger)
Date: Fri, 29 Jul 2016 16:43:16 +0100
Subject: [R] Reduce woes
In-Reply-To: <CAF8bMcZRAJngoECE31QKQnF6frgJc5n_gXAK7XxQ5kTsHB8C+Q@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
	<CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>
	<CAG7vnkyKwkXY-5J1svBK=xot_iDdrdBq5+ryUzAqahg0TAKGEQ@mail.gmail.com>
	<CAOjnRsZ8nODDbCe4Xns8AqSo8KDukh97AzQOkemAxpK=i0ATDA@mail.gmail.com>
	<CAOjnRsYoLQbhPTePt9J-th4O56GOEDroSB-NZ+qwmnGdveyr2A@mail.gmail.com>
	<CAG7vnky2boUP9t1nrYfPbWjNSUd6DBvX8cNfvO3hPAK0xCDu_Q@mail.gmail.com>
	<CAF8bMcZRAJngoECE31QKQnF6frgJc5n_gXAK7XxQ5kTsHB8C+Q@mail.gmail.com>
Message-ID: <CAG7vnkwiygX59pAXRoSHs6LizSDomEJsmGjqpeiN6K+Zdu=BKQ@mail.gmail.com>

>> I still don't understand why you want Reduce to to lapply's
>> job.   Reduce maps many to one and lapply maps many to
>> many.

Say you want to map a function over a subset of a vector or list? With the
generalised version of Reduce you map many-to-one, but the one can be a
'complex' structure. lapply() and friends not only map many-to-many, but
X-to-X - the resulting list will be the same length as the source. This
frequently gets used in Elixir, Erlang, Haskell etc as a means of
processing a pipeline or stream - start with a vector, select a subset
based on some predicate, turn this subset into an entirely different
object/list/

In iterative-fashion pseudo code

source = list(c(1,2,3,4), c(8,7,6,5,4,3,7), c(5,4))
result = { }
foreach (item in source) {
    if (length(item) > 2) {
        result[generate_some_name()] = length(item)
    }
}

That's and example of what I want to do. It maps many (a subset of the
vectors in source) to one (the result named list). It's a map-filter - but
even more general than your typical map-filter in that you can change the
data structure - e.g. map a function over a vector, use a subset of the
results, and turn those into a list or S3 object.


Stefan



On 29 July 2016 at 15:54, William Dunlap <wdunlap at tibco.com> wrote:

> Reduce (like lapply) apparently uses the [[ operator to
> extract components from the list given to it. X[[i]] does
> not attach names(X)[i] to its output (where would it put it?).
> Hence your se
>
> To help understand what these functions are doing try
> putting print statements in your test functions:
> > data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
> > r <- Reduce(function(acc, item) { cat("acc="); str(acc) ; cat("item=");
> str(item); length(item) }, data, init=list())
> acc= list()
> item= num [1:2] 1 1
> acc= int 2
> item= num 3
> acc= int 1
> item= num [1:2] 2 2
> > data2 <- list(one = c(oneA=1, onB=1), three = c(threeA=3), two =
> c(twoA=2, twoB=2))
> > r <- Reduce(function(acc, item) { cat("acc="); str(acc) ; cat("item=");
> str(item); length(item) }, data2, init=list())
> acc= list()
> item= Named num [1:2] 1 1
>  - attr(*, "names")= chr [1:2] "oneA" "onB"
> acc= int 2
> item= Named num 3
>  - attr(*, "names")= chr "threeA"
> acc= int 1
> item= Named num [1:2] 2 2
>  - attr(*, "names")= chr [1:2] "twoA" "twoB"
>
>
> I still don't understand why you want Reduce to to lapply's
> job.   Reduce maps many to one and lapply maps many to
> many.
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Fri, Jul 29, 2016 at 1:37 AM, Stefan Kruger <stefan.kruger at gmail.com>
> wrote:
>
>> Jeremiah -
>>
>> neat - that's one step closer, but one small thing I still don't
>> understand:
>>
>> > data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>> > r = Reduce(function(acc, item) { append(acc, setNames(length(item),
>> names(item))) }, data, list())
>> > str(r)
>> List of 3
>>  $ : int 2
>>  $ : int 1
>>  $ : int 2
>>
>> I wanted the names to remain, but it seems like the "data" parameter loses
>> its names when consumed by the Reduce()? If I print "item" inside the
>> reducing function, it's not got the names. I'm probably missing some
>> central tenet of R here.
>>
>> As to your comment of this being lapply() implemented by Reduce() - as I
>> understand lapply()  (or map() in other functional languages), it's
>> limited
>> to returning a list/vector of the same length as the original. Consider
>> this contrived example:
>>
>> > r = Reduce(function(acc, item) { if (length(item) > 1) {append(acc,
>> setNames(length(item), names(item)))} }, data, list())
>> > str(r)
>>  int 2
>> > r
>> [1] 2
>>
>> I don't think you could achieve that with lapply()?
>>
>> Thanks
>>
>> Stefan
>>
>>
>> On 28 July 2016 at 20:19, jeremiah rounds <roundsjeremiah at gmail.com>
>> wrote:
>>
>> > Basically using Reduce as an lapply in that example, but I think that
>> was
>> > caused by how people started talking about things in the first place =)
>> But
>> > the point is the accumulator can be anything as far as I can tell.
>> >
>> > On Thu, Jul 28, 2016 at 12:14 PM, jeremiah rounds <
>> > roundsjeremiah at gmail.com> wrote:
>> >
>> >> Re:
>> >> "What I'm trying to
>> >> work out is how to have the accumulator in Reduce not be the same type
>> as
>> >> the elements of the vector/list being reduced - ideally it could be an
>> S3
>> >> instance, list, vector, or data frame."
>> >>
>> >> Pretty sure that is not true.  See code that follows.  I would never
>> >> solve this task in this way though so no comment on the use of Reduce
>> for
>> >> what you described.  (Note the accumulation of "functions" in a list is
>> >> just a demo of possibilities).  You could accumulate in an environment
>> too
>> >> and potentially gain a lot of copy efficiency.
>> >>
>> >>
>> >> lookup = list()
>> >> lookup[[as.character(1)]] = function() print("1")
>> >> lookup[[as.character(2)]] = function() print("2")
>> >> lookup[[as.character(3)]] = function() print("3")
>> >>
>> >> data = list(c(1,2), c(1,4), c(3,3), c(2,30))
>> >>
>> >>
>> >> r = Reduce(function(acc, item) {
>> >> append(acc, list(lookup[[as.character(min(item))]]))
>> >> }, data,list())
>> >> r
>> >> for(f in r) f()
>> >>
>> >>
>> >> On Thu, Jul 28, 2016 at 5:09 AM, Stefan Kruger <
>> stefan.kruger at gmail.com>
>> >> wrote:
>> >>
>> >>> Ulrik - many thanks for your reply.
>> >>>
>> >>> I'm aware of many simple solutions as the one you suggest, both
>> iterative
>> >>> and functional style - but I'm trying to learn how to bend Reduce()
>> for
>> >>> the
>> >>> purpose of using it in more complex processing tasks. What I'm trying
>> to
>> >>> work out is how to have the accumulator in Reduce not be the same
>> type as
>> >>> the elements of the vector/list being reduced - ideally it could be
>> an S3
>> >>> instance, list, vector, or data frame.
>> >>>
>> >>> Here's a more realistic example (in Elixir, sorry)
>> >>>
>> >>> Given two lists:
>> >>>
>> >>> 1. data: maps an id string to a vector of revision strings
>> >>> 2. dict: maps known id/revision pairs as a string to true (or 1)
>> >>>
>> >>> find the items in data not already in dict, returned as a named list.
>> >>>
>> >>> ```elixir
>> >>> data = %{
>> >>>     "id1" => ["rev1.1", "rev1.2"],
>> >>>     "id2" => ["rev2.1"],
>> >>>     "id3" => ["rev3.1", "rev3.2", "rev3.3"]
>> >>> }
>> >>>
>> >>> dict = %{
>> >>>     "id1/rev1.1" => 1,
>> >>>     "id1/rev1.2" => 1,
>> >>>     "id3/rev3.1" => 1
>> >>> }
>> >>>
>> >>> # Find the items in data not already in dict. Return as a grouped map
>> >>>
>> >>> Map.keys(data)
>> >>>     |> Enum.flat_map(fn id -> Enum.map(data[id], fn rev -> {id, rev}
>> end)
>> >>> end)
>> >>>     |> Enum.filter(fn {id, rev} -> !Dict.has_key?(dict,
>> "#{id}/#{rev}")
>> >>> end)
>> >>>     |> Enum.reduce(%{}, fn ({k, v}, d) -> Map.update(d, k, [v],
>> &[v|&1])
>> >>> end)
>> >>> ```
>> >>>
>> >>>
>> >>>
>> >>>
>> >>> On 28 July 2016 at 12:03, Ulrik Stervbo <ulrik.stervbo at gmail.com>
>> wrote:
>> >>>
>> >>> > Hi Stefan,
>> >>> >
>> >>> > in that case,lapply(data, length) should do the trick.
>> >>> >
>> >>> > Best wishes,
>> >>> > Ulrik
>> >>> >
>> >>> > On Thu, 28 Jul 2016 at 12:57 Stefan Kruger <stefan.kruger at gmail.com
>> >
>> >>> > wrote:
>> >>> >
>> >>> >> David - many thanks for your response.
>> >>> >>
>> >>> >> What I tried to do was to turn
>> >>> >>
>> >>> >> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>> >>> >>
>> >>> >> into
>> >>> >>
>> >>> >> result <- list(one = 2, three = 1, two = 2)
>> >>> >>
>> >>> >> that is creating a new list which has the same names as the first,
>> but
>> >>> >> where the values are the vector lengths.
>> >>> >>
>> >>> >> I know there are many other (and better) trivial ways of achieving
>> >>> this -
>> >>> >> my aim is less the task itself, and more figuring out if this can
>> be
>> >>> done
>> >>> >> using Reduce() in the fashion I showed in the other examples I
>> gave.
>> >>> It's
>> >>> >> a
>> >>> >> building block of doing map-filter-reduce type pipelines that I'd
>> >>> like to
>> >>> >> understand how to do in R.
>> >>> >>
>> >>> >> Fumbling in the dark, I tried:
>> >>> >>
>> >>> >> Reduce(function(acc, item) { setNames(c(acc, length(data[item])),
>> >>> item },
>> >>> >> names(data), accumulate=TRUE)
>> >>> >>
>> >>> >> but setNames sets all the names, not adding one - and acc is still
>> a
>> >>> >> vector, not a list.
>> >>> >>
>> >>> >> It looks like 'lambda.tools.fold()' and possibly 'purrr.reduce()'
>> aim
>> >>> at
>> >>> >> doing what I'd like to do - but I've not been able to figure out
>> quite
>> >>> >> how.
>> >>> >>
>> >>> >> Thanks
>> >>> >>
>> >>> >> Stefan
>> >>> >>
>> >>> >>
>> >>> >>
>> >>> >> On 27 July 2016 at 20:35, David Winsemius <dwinsemius at comcast.net>
>> >>> wrote:
>> >>> >>
>> >>> >> >
>> >>> >> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <
>> >>> stefan.kruger at gmail.com>
>> >>> >> > wrote:
>> >>> >> > >
>> >>> >> > > Hi -
>> >>> >> > >
>> >>> >> > > I'm new to R.
>> >>> >> > >
>> >>> >> > > In other functional languages I'm familiar with you can often
>> >>> seed a
>> >>> >> call
>> >>> >> > > to reduce() with a custom accumulator. Here's an example in
>> >>> Elixir:
>> >>> >> > >
>> >>> >> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
>> >>> >> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) -> Map.update(acc, k,
>> >>> >> > > Enum.count(v), nil) end)
>> >>> >> > > # %{"one" => 2, "three" => 1, "two" => 2}
>> >>> >> > >
>> >>> >> > > In R-terms that's reducing a list of vectors to become a new
>> list
>> >>> >> mapping
>> >>> >> > > the names to the vector lengths.
>> >>> >> > >
>> >>> >> > > Even in JavaScript, you can do similar things:
>> >>> >> > >
>> >>> >> > > list = { one: [1, 1], three: [3], two: [2, 2] };
>> >>> >> > > var result = Object.keys(list).reduceRight(function (acc,
>> item) {
>> >>> >> > >  acc[item] = list[item].length;
>> >>> >> > >  return acc;
>> >>> >> > > }, {});
>> >>> >> > > // result == { two: 2, three: 1, one: 2 }
>> >>> >> > >
>> >>> >> > > In R, from what I can gather, Reduce() is restricted such that
>> any
>> >>> >> init
>> >>> >> > > value you feed it is required to be of the same type as the
>> >>> elements
>> >>> >> of
>> >>> >> > the
>> >>> >> > > vector you're reducing -- so I can't build up. So whilst I can
>> >>> do, say
>> >>> >> > >
>> >>> >> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5), 96)
>> >>> >> > > [1] 111
>> >>> >> > >
>> >>> >> > > I can't use Reduce to build up a list, vector or data frame?
>> >>> >> > >
>> >>> >> > > What am I missing?
>> >>> >> > >
>> >>> >> > > Many thanks for any pointers,
>> >>> >> >
>> >>> >> > This builds a list:
>> >>> >> >
>> >>> >> > > Reduce(function(acc, item) { c(acc , item) }, c(1,2,3,4,5), 96,
>> >>> >> > accumulate=TRUE)
>> >>> >> > [[1]]
>> >>> >> > [1] 96
>> >>> >> >
>> >>> >> > [[2]]
>> >>> >> > [1] 96  1
>> >>> >> >
>> >>> >> > [[3]]
>> >>> >> > [1] 96  1  2
>> >>> >> >
>> >>> >> > [[4]]
>> >>> >> > [1] 96  1  2  3
>> >>> >> >
>> >>> >> > [[5]]
>> >>> >> > [1] 96  1  2  3  4
>> >>> >> >
>> >>> >> > [[6]]
>> >>> >> > [1] 96  1  2  3  4  5
>> >>> >> >
>> >>> >> > But you are not saying what you want. The other examples were
>> doing
>> >>> >> > something with names but you provided no names for the R example.
>> >>> >> >
>> >>> >> > This would return a list of named vectors:
>> >>> >> >
>> >>> >> > > Reduce(function(acc, item) { setNames( c(acc,item), 1:(item+1))
>> >>> },
>> >>> >> > c(1,2,3,4,5), 96, accumulate=TRUE)
>> >>> >> > [[1]]
>> >>> >> > [1] 96
>> >>> >> >
>> >>> >> > [[2]]
>> >>> >> >  1  2
>> >>> >> > 96  1
>> >>> >> >
>> >>> >> > [[3]]
>> >>> >> >  1  2  3
>> >>> >> > 96  1  2
>> >>> >> >
>> >>> >> > [[4]]
>> >>> >> >  1  2  3  4
>> >>> >> > 96  1  2  3
>> >>> >> >
>> >>> >> > [[5]]
>> >>> >> >  1  2  3  4  5
>> >>> >> > 96  1  2  3  4
>> >>> >> >
>> >>> >> > [[6]]
>> >>> >> >  1  2  3  4  5  6
>> >>> >> > 96  1  2  3  4  5
>> >>> >> >
>> >>> >> >
>> >>> >> >
>> >>> >> >
>> >>> >> > > Stefan
>> >>> >> > >
>> >>> >> > >
>> >>> >> > >
>> >>> >> > > --
>> >>> >> > > Stefan Kruger <stefan.kruger at gmail.com>
>> >>> >> > >
>> >>> >> > >       [[alternative HTML version deleted]]
>> >>> >> > >
>> >>> >> > > ______________________________________________
>> >>> >> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>> >>> >> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> >> > > PLEASE do read the posting guide
>> >>> >> > http://www.R-project.org/posting-guide.html
>> >>> >> > > and provide commented, minimal, self-contained, reproducible
>> code.
>> >>> >> >
>> >>> >> > David Winsemius
>> >>> >> > Alameda, CA, USA
>> >>> >> >
>> >>> >> >
>> >>> >>
>> >>> >>
>> >>> >> --
>> >>> >> Stefan Kruger <stefan.kruger at gmail.com>
>> >>> >>
>> >>> >>         [[alternative HTML version deleted]]
>> >>> >>
>> >>> >> ______________________________________________
>> >>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> >> PLEASE do read the posting guide
>> >>> >> http://www.R-project.org/posting-guide.html
>> >>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>> >>
>> >>> >
>> >>>
>> >>>
>> >>> --
>> >>> Stefan Kruger <stefan.kruger at gmail.com>
>> >>>
>> >>>         [[alternative HTML version deleted]]
>> >>>
>> >>> ______________________________________________
>> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> PLEASE do read the posting guide
>> >>> http://www.R-project.org/posting-guide.html
>> >>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>
>> >>
>> >
>>
>>
>> --
>> Stefan Kruger <stefan.kruger at gmail.com>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 
Stefan Kruger <stefan.kruger at gmail.com>

	[[alternative HTML version deleted]]


From macqueen1 at llnl.gov  Fri Jul 29 17:54:20 2016
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 29 Jul 2016 15:54:20 +0000
Subject: [R] Strange message after reading multiple scripts from one
 folder
In-Reply-To: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>
References: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>
Message-ID: <D3C0C81A.1808CF%macqueen1@llnl.gov>

For what it's worth (perhaps little...), I would normally do

for (pn in pathnames) source(pn)

It's clearer to read and won't return a strange value. I doubt there will
be a noticeable difference in speed. It can easily be extended to be more
informative, as in
for (pn in pathnames) {
  cat('--- now sourcing',pn,'---\n')
  source(pn)
}

One could also introduce error trapping using try() in this version.

As far as surpassing the message is concerned, one option would be to put
your sapply() expression inside invisible(). Not sure what will happen in
that case if any of the scripts fail with an error.

> 10*2
[1] 20
> invisible(10*2)

>

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 7/29/16, 3:52 AM, "R-help on behalf of Frank S."
<r-help-bounces at r-project.org on behalf of f_j_rod at hotmail.com> wrote:

>Dear list,
> 
>I have one folder named "scripts_JMbayes", wich contains 10 R scripts.
>I can read them properly by doing:
> 
>> pathnames <- list.files(pattern="[.]R", path="Mydir/scripts_JMbayes",
>>full.names = TRUE)
>> sapply(pathnames, USE.NAMES = FALSE, FUN = source,)
> 
>However, R generates the following message:
> 
>        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
>value   ?     ?     ?     ?     ?     ?     ?     ?     ?     ?
>visible FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
> 
>What does it mean and what should I change to avoid this message?
>Any help would be appreciated!
> 
>Best,
> 
>Frank
>
> 		 	   		  
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From f_j_rod at hotmail.com  Fri Jul 29 18:12:06 2016
From: f_j_rod at hotmail.com (Frank S.)
Date: Fri, 29 Jul 2016 18:12:06 +0200
Subject: [R] Strange message after reading multiple scripts from one
 folder
In-Reply-To: <D3C0C81A.1808CF%macqueen1@llnl.gov>
References: <BLU183-W66150D16E5B1FB060CB8FEBA010@phx.gbl>,
	<D3C0C81A.1808CF%macqueen1@llnl.gov>
Message-ID: <BLU183-W555CD7D840CAAC17CC20A1BA010@phx.gbl>

Dear Keith, Jim, John and Don:
 
Thanks a bunch for your quick replies! They have helped me a lot in order to understand the problem I had in the code. As all of you pointed out, the root of problem consisted in having a list with two named elements. Furthermore, it is very interesting the use of invisible() statement. In particular, the solutions proposed by John Fox and Don MacQueen work correctly.
 
Thank you very much!!
 
Sincerely,
 
Frank S.
 
> From: macqueen1 at llnl.gov
> To: f_j_rod at hotmail.com; r-help at r-project.org
> Subject: Re: [R] Strange message after reading multiple scripts from one folder
> Date: Fri, 29 Jul 2016 15:54:20 +0000
> 
> For what it's worth (perhaps little...), I would normally do
> 
> for (pn in pathnames) source(pn)
> 
> It's clearer to read and won't return a strange value. I doubt there will
> be a noticeable difference in speed. It can easily be extended to be more
> informative, as in
> for (pn in pathnames) {
>   cat('--- now sourcing',pn,'---\n')
>   source(pn)
> }
> 
> One could also introduce error trapping using try() in this version.
> 
> As far as surpassing the message is concerned, one option would be to put
> your sapply() expression inside invisible(). Not sure what will happen in
> that case if any of the scripts fail with an error.
> 
> > 10*2
> [1] 20
> > invisible(10*2)
> 
> >
> 
> -Don
> 
> -- 
> Don MacQueen
> 
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
> 
> 
> 
> 
> 
> On 7/29/16, 3:52 AM, "R-help on behalf of Frank S."
> <r-help-bounces at r-project.org on behalf of f_j_rod at hotmail.com> wrote:
> 
> >Dear list,
> > 
> >I have one folder named "scripts_JMbayes", wich contains 10 R scripts.
> >I can read them properly by doing:
> > 
> >> pathnames <- list.files(pattern="[.]R", path="Mydir/scripts_JMbayes",
> >>full.names = TRUE)
> >> sapply(pathnames, USE.NAMES = FALSE, FUN = source,)
> > 
> >However, R generates the following message:
> > 
> >        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
> >value   ?     ?     ?     ?     ?     ?     ?     ?     ?     ?
> >visible FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
> > 
> >What does it mean and what should I change to avoid this message?
> >Any help would be appreciated!
> > 
> >Best,
> > 
> >Frank
> >
> > 		 	   		  
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> 
 		 	   		  
	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Fri Jul 29 18:53:19 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Fri, 29 Jul 2016 22:23:19 +0530
Subject: [R] WordCloud Does not display in RMD File in R Studio
Message-ID: <CAB=p7Sqs6JqiHp1=vU-b=ZP7j5EuQX50i=bm4WxY+n=ztOb80Q@mail.gmail.com>

Dear Team,

I have created a text mining solution for one of my business owners.

I am sharing with his using RMD via HTML. All is working ok however the
WordCloud does not populate any output in RMD file.

Few months back when this happened on R studio i searched and used
dev.off() to shut down any concurrent running session but this also does
not help.

Other plots that i have created such ggplot and barpplot they are being
displayed. similarly i can show wordcloud also in R Studio but not on RMD.

Not sure what i am doing wrong as i only get a message staing NULL when i
run wordcloud.
Plz find the code for wordcloud if that helps:
word0<- names(frequency)
png("p3_sa_para.png", 640, 480)


wordcloud(word0,frequency,random.order =F,
colors = brewer.pal(8, "Dark2"),scale = c(5.5,0.5),min.freq = 100,
max.words = Inf)

	[[alternative HTML version deleted]]


From xie at yihui.name  Fri Jul 29 20:15:17 2016
From: xie at yihui.name (Yihui Xie)
Date: Fri, 29 Jul 2016 13:15:17 -0500
Subject: [R] WordCloud Does not display in RMD File in R Studio
In-Reply-To: <CAB=p7Sqs6JqiHp1=vU-b=ZP7j5EuQX50i=bm4WxY+n=ztOb80Q@mail.gmail.com>
References: <CAB=p7Sqs6JqiHp1=vU-b=ZP7j5EuQX50i=bm4WxY+n=ztOb80Q@mail.gmail.com>
Message-ID: <CANROs4ekQVvQFX3-hmvhmqmrC6G+m=TcTsqvjgw9cgjgX6dQMw@mail.gmail.com>

Typically you don't need to open the png() device manually. Try
comment out that line.

BTW, I'm not sure which wordcloud package you were using, but this one
is the best one I have seen: https://github.com/Lchiffon/wordcloud2

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Fri, Jul 29, 2016 at 11:53 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:
> Dear Team,
>
> I have created a text mining solution for one of my business owners.
>
> I am sharing with his using RMD via HTML. All is working ok however the
> WordCloud does not populate any output in RMD file.
>
> Few months back when this happened on R studio i searched and used
> dev.off() to shut down any concurrent running session but this also does
> not help.
>
> Other plots that i have created such ggplot and barpplot they are being
> displayed. similarly i can show wordcloud also in R Studio but not on RMD.
>
> Not sure what i am doing wrong as i only get a message staing NULL when i
> run wordcloud.
> Plz find the code for wordcloud if that helps:
> word0<- names(frequency)
> png("p3_sa_para.png", 640, 480)
>
>
> wordcloud(word0,frequency,random.order =F,
> colors = brewer.pal(8, "Dark2"),scale = c(5.5,0.5),min.freq = 100,
> max.words = Inf)
>


From cryan at binghamton.edu  Fri Jul 29 20:30:39 2016
From: cryan at binghamton.edu (Christopher W. Ryan)
Date: Fri, 29 Jul 2016 14:30:39 -0400
Subject: [R] what happened to inside-r? [possibly OT]
Message-ID: <579BA0CF.9050406@binghamton.edu>

This might be a bit off-topic, but up until recently (a day or so ago?) 
I loved using inside-r.org as a quick and easy way to access help pages 
on R commands. Took me to what I needed without any fuss. Now that URL 
redirects to the "Microsoft R Application Network"?  Looks to be 
something related to Revolution R.  What happened?

Thanks.

--Chris Ryan


From shivipmp82 at gmail.com  Fri Jul 29 20:30:36 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Sat, 30 Jul 2016 00:00:36 +0530
Subject: [R] WordCloud Does not display in RMD File in R Studio
In-Reply-To: <CANROs4ekQVvQFX3-hmvhmqmrC6G+m=TcTsqvjgw9cgjgX6dQMw@mail.gmail.com>
References: <CAB=p7Sqs6JqiHp1=vU-b=ZP7j5EuQX50i=bm4WxY+n=ztOb80Q@mail.gmail.com>
	<CANROs4ekQVvQFX3-hmvhmqmrC6G+m=TcTsqvjgw9cgjgX6dQMw@mail.gmail.com>
Message-ID: <CAB=p7SoQLKWEA3AXyV0NLsJJ3t5vsTVU9Bfb2gBjB5c_dWZOdQ@mail.gmail.com>

Thanks for the suggestion. even without png isnt not working.

I will check this new package. Thank you.

On Fri, Jul 29, 2016 at 11:45 PM, Yihui Xie <xie at yihui.name> wrote:

> Typically you don't need to open the png() device manually. Try
> comment out that line.
>
> BTW, I'm not sure which wordcloud package you were using, but this one
> is the best one I have seen: https://github.com/Lchiffon/wordcloud2
>
> Regards,
> Yihui
> --
> Yihui Xie <xieyihui at gmail.com>
> Web: http://yihui.name
>
>
> On Fri, Jul 29, 2016 at 11:53 AM, Shivi Bhatia <shivipmp82 at gmail.com>
> wrote:
> > Dear Team,
> >
> > I have created a text mining solution for one of my business owners.
> >
> > I am sharing with his using RMD via HTML. All is working ok however the
> > WordCloud does not populate any output in RMD file.
> >
> > Few months back when this happened on R studio i searched and used
> > dev.off() to shut down any concurrent running session but this also does
> > not help.
> >
> > Other plots that i have created such ggplot and barpplot they are being
> > displayed. similarly i can show wordcloud also in R Studio but not on
> RMD.
> >
> > Not sure what i am doing wrong as i only get a message staing NULL when i
> > run wordcloud.
> > Plz find the code for wordcloud if that helps:
> > word0<- names(frequency)
> > png("p3_sa_para.png", 640, 480)
> >
> >
> > wordcloud(word0,frequency,random.order =F,
> > colors = brewer.pal(8, "Dark2"),scale = c(5.5,0.5),min.freq = 100,
> > max.words = Inf)
> >
>

	[[alternative HTML version deleted]]


From shivipmp82 at gmail.com  Fri Jul 29 21:37:16 2016
From: shivipmp82 at gmail.com (Shivi Bhatia)
Date: Sat, 30 Jul 2016 01:07:16 +0530
Subject: [R] WordCloud Does not display in RMD File in R Studio
In-Reply-To: <CAB=p7SoQLKWEA3AXyV0NLsJJ3t5vsTVU9Bfb2gBjB5c_dWZOdQ@mail.gmail.com>
References: <CAB=p7Sqs6JqiHp1=vU-b=ZP7j5EuQX50i=bm4WxY+n=ztOb80Q@mail.gmail.com>
	<CANROs4ekQVvQFX3-hmvhmqmrC6G+m=TcTsqvjgw9cgjgX6dQMw@mail.gmail.com>
	<CAB=p7SoQLKWEA3AXyV0NLsJJ3t5vsTVU9Bfb2gBjB5c_dWZOdQ@mail.gmail.com>
Message-ID: <CAB=p7SrY-_ZxHyydnXx0uDg2LY4o37aOiLLhmFtztmqch_pS=Q@mail.gmail.com>

Hi Yihui,

Seems like the issue was with the wordcloud. I used wordcloud2 and it
worked handsomely.

& thanks for suggesting wordcloud2 seems far better than wordcloud.

tc.

On Sat, Jul 30, 2016 at 12:00 AM, Shivi Bhatia <shivipmp82 at gmail.com> wrote:

> Thanks for the suggestion. even without png isnt not working.
>
> I will check this new package. Thank you.
>
> On Fri, Jul 29, 2016 at 11:45 PM, Yihui Xie <xie at yihui.name> wrote:
>
>> Typically you don't need to open the png() device manually. Try
>> comment out that line.
>>
>> BTW, I'm not sure which wordcloud package you were using, but this one
>> is the best one I have seen: https://github.com/Lchiffon/wordcloud2
>>
>> Regards,
>> Yihui
>> --
>> Yihui Xie <xieyihui at gmail.com>
>> Web: http://yihui.name
>>
>>
>> On Fri, Jul 29, 2016 at 11:53 AM, Shivi Bhatia <shivipmp82 at gmail.com>
>> wrote:
>> > Dear Team,
>> >
>> > I have created a text mining solution for one of my business owners.
>> >
>> > I am sharing with his using RMD via HTML. All is working ok however the
>> > WordCloud does not populate any output in RMD file.
>> >
>> > Few months back when this happened on R studio i searched and used
>> > dev.off() to shut down any concurrent running session but this also does
>> > not help.
>> >
>> > Other plots that i have created such ggplot and barpplot they are being
>> > displayed. similarly i can show wordcloud also in R Studio but not on
>> RMD.
>> >
>> > Not sure what i am doing wrong as i only get a message staing NULL when
>> i
>> > run wordcloud.
>> > Plz find the code for wordcloud if that helps:
>> > word0<- names(frequency)
>> > png("p3_sa_para.png", 640, 480)
>> >
>> >
>> > wordcloud(word0,frequency,random.order =F,
>> > colors = brewer.pal(8, "Dark2"),scale = c(5.5,0.5),min.freq = 100,
>> > max.words = Inf)
>> >
>>
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jul 29 21:54:08 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 29 Jul 2016 12:54:08 -0700
Subject: [R] what happened to inside-r? [possibly OT]
In-Reply-To: <579BA0CF.9050406@binghamton.edu>
References: <579BA0CF.9050406@binghamton.edu>
Message-ID: <8D2B3139-903F-4140-B68C-B72533ECE9DF@comcast.net>

Been off the grid for the last year? MS bought Revolution R.


Sent from my iPhone

> On Jul 29, 2016, at 11:30 AM, Christopher W. Ryan <cryan at binghamton.edu> wrote:
> 
> This might be a bit off-topic, but up until recently (a day or so ago?) I loved using inside-r.org as a quick and easy way to access help pages on R commands. Took me to what I needed without any fuss. Now that URL redirects to the "Microsoft R Application Network"?  Looks to be something related to Revolution R.  What happened?
> 
> Thanks.
> 
> --Chris Ryan
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cryan at binghamton.edu  Fri Jul 29 22:07:31 2016
From: cryan at binghamton.edu (Christopher W. Ryan)
Date: Fri, 29 Jul 2016 16:07:31 -0400
Subject: [R] I'm getting confused with notation and terminology in output
 from weibull parametric survival model from survreg()
Message-ID: <579BB783.6090408@binghamton.edu>

I'm trying to run a Weibull parametric survival model for recurrent 
event data, with subject-specific frailties, using survreg() in the 
survival package, and I'm having trouble understanding the output and 
its notation, and how that translates to some of the books I am using as 
references (DF Moore, Applied Survival Analysis Using R; and Kleinbaum 
and Klein, Survival Analysis A Self-Learning Text). I understand there 
are different notations for different ways of parameterizing a Weibull 
or a gamma distribution, and perhaps that's where I am getting hung up. 
I also may be confusing "scale" for the Weibull distribution of the 
survial times with "scale" for the gamma distribution of the frailties.

My ultimate goal is to display example survival curves: say, one for 
"typically frail" subjects, one for "extra-frail" subjects, and one for 
"not-so-frail" subjects. I'd like to get estimated "frailty" for each of 
my subjects; or at least the distribution of those frailties. Do I need 
the parameters of the gamma distribution to do that? If so, how do I 
extract them? Or are they readily available in the survreg object?

Here is what I have tried so far:

## create some data similar to my real data, in which
## vast majority of subjects had no event
id <- c(1:600, rep(601:630, each=1), rep(631:650, each=2), rep(651:656, 
each=3), rep(677:679, each=4), rep(680, 5))
time <- c(rpois(lambda=800, 600), rpois(lambda=600, length(601:630)), 
rpois(lambda=600, length(631:650)*2), rpois(lambda=600, 
length(651:656)*3), rpois(lambda=600, length(677:679)*4), 
rpois(lambda=600, 5))
event <- c(rep(0, 600), rep(1, (length(id) - 600)))
dd <- data.frame(id=id, time=time, event=event)
dd.2 <- dd[order(id, time), ]
str(dd.2)
table(table(dd.2$id))
# time until censoring, for those without events
summary(subset(dd.2, event==0, select=time))

library(survival)
Surv.1 <- Surv(time, event)

# model without frailties
model.1 <- survreg(Surv.1 ~ 1, data=dd.2, dist="weibull")

# add frailty term
model.2 <- survreg(Surv.1 ~ 1 + frailty(id), data=dd.2, dist="weibull")

# should be same as above line
model.2.b <- survreg(Surv.1 ~ 1 + frailty(id, distribution="gamma"), 
data=dd.2, dist="weibull")

# I don't know if this is the right way to go about it
a.scale <- model.2$scale
var.X <- model.2$history$frailty$theta
s.shape <- sqrt(var.X/a.scale)

gamma.frail.x <- function(a,s,q){ 1/((s^a) * gamma(a)) * (q^(a-1) * 
exp(-(q/s))) }
q <- seq(0.1, 10, by=0.2)

maybe.my.frailties <- gamma.frail.x(a.scale, s.shape, q)))
plot(density(maybe.my.frailties))

## end code


Or, would I be better off changing tactics and using frailtypack?

Thanks for any help.  Session info is below, in case it is relevant.

--Chris Ryan



 > sessionInfo()
R version 3.3.1 (2016-06-21)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] survival_2.39-5

loaded via a namespace (and not attached):
[1] compiler_3.3.1  Matrix_1.2-6    tools_3.3.1     splines_3.3.1
[5] grid_3.3.1      lattice_0.20-33


From ddalthorp at usgs.gov  Fri Jul 29 22:18:56 2016
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Fri, 29 Jul 2016 13:18:56 -0700
Subject: [R] I'm getting confused with notation and terminology in
 output from weibull parametric survival model from survreg()
In-Reply-To: <579BB783.6090408@binghamton.edu>
References: <579BB783.6090408@binghamton.edu>
Message-ID: <CAJeYpE-4syO4DOx7LtHtmKd=Xg5jamZAHCJw8afcF6P1csg_Jg@mail.gmail.com>

The parameterization for Weibull in the 'survival' package corresponds to
base R's dweibull, etc. suite as 1/scale --> shape and exp(coef[1]) -->
scale

On Fri, Jul 29, 2016 at 1:07 PM, Christopher W. Ryan <cryan at binghamton.edu>
wrote:

> I'm trying to run a Weibull parametric survival model for recurrent event
> data, with subject-specific frailties, using survreg() in the survival
> package, and I'm having trouble understanding the output and its notation,
> and how that translates to some of the books I am using as references (DF
> Moore, Applied Survival Analysis Using R; and Kleinbaum and Klein, Survival
> Analysis A Self-Learning Text). I understand there are different notations
> for different ways of parameterizing a Weibull or a gamma distribution, and
> perhaps that's where I am getting hung up. I also may be confusing "scale"
> for the Weibull distribution of the survial times with "scale" for the
> gamma distribution of the frailties.
>
> My ultimate goal is to display example survival curves: say, one for
> "typically frail" subjects, one for "extra-frail" subjects, and one for
> "not-so-frail" subjects. I'd like to get estimated "frailty" for each of my
> subjects; or at least the distribution of those frailties. Do I need the
> parameters of the gamma distribution to do that? If so, how do I extract
> them? Or are they readily available in the survreg object?
>
> Here is what I have tried so far:
>
> ## create some data similar to my real data, in which
> ## vast majority of subjects had no event
> id <- c(1:600, rep(601:630, each=1), rep(631:650, each=2), rep(651:656,
> each=3), rep(677:679, each=4), rep(680, 5))
> time <- c(rpois(lambda=800, 600), rpois(lambda=600, length(601:630)),
> rpois(lambda=600, length(631:650)*2), rpois(lambda=600, length(651:656)*3),
> rpois(lambda=600, length(677:679)*4), rpois(lambda=600, 5))
> event <- c(rep(0, 600), rep(1, (length(id) - 600)))
> dd <- data.frame(id=id, time=time, event=event)
> dd.2 <- dd[order(id, time), ]
> str(dd.2)
> table(table(dd.2$id))
> # time until censoring, for those without events
> summary(subset(dd.2, event==0, select=time))
>
> library(survival)
> Surv.1 <- Surv(time, event)
>
> # model without frailties
> model.1 <- survreg(Surv.1 ~ 1, data=dd.2, dist="weibull")
>
> # add frailty term
> model.2 <- survreg(Surv.1 ~ 1 + frailty(id), data=dd.2, dist="weibull")
>
> # should be same as above line
> model.2.b <- survreg(Surv.1 ~ 1 + frailty(id, distribution="gamma"),
> data=dd.2, dist="weibull")
>
> # I don't know if this is the right way to go about it
> a.scale <- model.2$scale
> var.X <- model.2$history$frailty$theta
> s.shape <- sqrt(var.X/a.scale)
>
> gamma.frail.x <- function(a,s,q){ 1/((s^a) * gamma(a)) * (q^(a-1) *
> exp(-(q/s))) }
> q <- seq(0.1, 10, by=0.2)
>
> maybe.my.frailties <- gamma.frail.x(a.scale, s.shape, q)))
> plot(density(maybe.my.frailties))
>
> ## end code
>
>
> Or, would I be better off changing tactics and using frailtypack?
>
> Thanks for any help.  Session info is below, in case it is relevant.
>
> --Chris Ryan
>
>
>
> > sessionInfo()
> R version 3.3.1 (2016-06-21)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
> locale:
> [1] LC_COLLATE=English_United States.1252
> [2] LC_CTYPE=English_United States.1252
> [3] LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C
> [5] LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] survival_2.39-5
>
> loaded via a namespace (and not attached):
> [1] compiler_3.3.1  Matrix_1.2-6    tools_3.3.1     splines_3.3.1
> [5] grid_3.3.1      lattice_0.20-33
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Jul 29 23:06:40 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 29 Jul 2016 14:06:40 -0700
Subject: [R] Reduce woes
In-Reply-To: <CAG7vnkwiygX59pAXRoSHs6LizSDomEJsmGjqpeiN6K+Zdu=BKQ@mail.gmail.com>
References: <CAG7vnkwrzkFiDcekL71hA4PbneszyCX4k0Akj2+1X-KbvTWaDA@mail.gmail.com>
	<32E02587-D727-46B0-9978-6DBC90209DEC@comcast.net>
	<CAG7vnkx3nD8n7kRk0FtqjTxGz44TNHT9h+kHx+E+ao6g1GUTDw@mail.gmail.com>
	<CAKVAULMxngqm8YNp_4FautnQUOTKFuPB43BBrtXWXxR=sQTixg@mail.gmail.com>
	<CAG7vnkyKwkXY-5J1svBK=xot_iDdrdBq5+ryUzAqahg0TAKGEQ@mail.gmail.com>
	<CAOjnRsZ8nODDbCe4Xns8AqSo8KDukh97AzQOkemAxpK=i0ATDA@mail.gmail.com>
	<CAOjnRsYoLQbhPTePt9J-th4O56GOEDroSB-NZ+qwmnGdveyr2A@mail.gmail.com>
	<CAG7vnky2boUP9t1nrYfPbWjNSUd6DBvX8cNfvO3hPAK0xCDu_Q@mail.gmail.com>
	<CAF8bMcZRAJngoECE31QKQnF6frgJc5n_gXAK7XxQ5kTsHB8C+Q@mail.gmail.com>
	<CAG7vnkwiygX59pAXRoSHs6LizSDomEJsmGjqpeiN6K+Zdu=BKQ@mail.gmail.com>
Message-ID: <002B3B27-01C3-4898-A9CD-4E02CA4633E0@dcn.davis.ca.us>

Having experienced some frustration myself when I first started with R many years ago, I can relate to your apparent frustration. However, if you would like to succeed in using R I strongly recommend learning R and not trying to write Haskell or Erlang or C or Fortran or any other language when writing in R. I am sure there are many things R could do better, and once you understand how R actually works you might even be in a position to contribute some improvements. But thinking in those other languages with an R interpreter on front of you is going to just make you more frustrated. 

For one thing, everything in R is a vector... even lists. Appending to a list is not O(1) as it would be for a linked list. Thus it is preferred to find algorithms that pre-allocate memory for results. Map (lapply) is 1:1 to encourage that.  Reduce is N:1 because it is simpler that way. Use Map to    make a grouping vector that you can use to select which elements you want to process and then map over that subset of your input data or aggregate over the whole thing.

Also, names are attributes of the list vector... one name per element.  Not all list operations maintain that attribute so you often have to explicitly copy names from source to destination. 

Oh and "source" is a common base R function... and so it is generally advised to not re-use common names in the global environment.
-- 
Sent from my phone. Please excuse my brevity.

On July 29, 2016 8:43:16 AM PDT, Stefan Kruger <stefan.kruger at gmail.com> wrote:
>>> I still don't understand why you want Reduce to to lapply's
>>> job.   Reduce maps many to one and lapply maps many to
>>> many.
>
>Say you want to map a function over a subset of a vector or list? With
>the
>generalised version of Reduce you map many-to-one, but the one can be a
>'complex' structure. lapply() and friends not only map many-to-many,
>but
>X-to-X - the resulting list will be the same length as the source. This
>frequently gets used in Elixir, Erlang, Haskell etc as a means of
>processing a pipeline or stream - start with a vector, select a subset
>based on some predicate, turn this subset into an entirely different
>object/list/
>
>In iterative-fashion pseudo code
>
>source = list(c(1,2,3,4), c(8,7,6,5,4,3,7), c(5,4))
>result = { }
>foreach (item in source) {
>    if (length(item) > 2) {
>        result[generate_some_name()] = length(item)
>    }
>}
>
>That's and example of what I want to do. It maps many (a subset of the
>vectors in source) to one (the result named list). It's a map-filter -
>but
>even more general than your typical map-filter in that you can change
>the
>data structure - e.g. map a function over a vector, use a subset of the
>results, and turn those into a list or S3 object.
>
>
>Stefan
>
>
>
>On 29 July 2016 at 15:54, William Dunlap <wdunlap at tibco.com> wrote:
>
>> Reduce (like lapply) apparently uses the [[ operator to
>> extract components from the list given to it. X[[i]] does
>> not attach names(X)[i] to its output (where would it put it?).
>> Hence your se
>>
>> To help understand what these functions are doing try
>> putting print statements in your test functions:
>> > data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>> > r <- Reduce(function(acc, item) { cat("acc="); str(acc) ;
>cat("item=");
>> str(item); length(item) }, data, init=list())
>> acc= list()
>> item= num [1:2] 1 1
>> acc= int 2
>> item= num 3
>> acc= int 1
>> item= num [1:2] 2 2
>> > data2 <- list(one = c(oneA=1, onB=1), three = c(threeA=3), two =
>> c(twoA=2, twoB=2))
>> > r <- Reduce(function(acc, item) { cat("acc="); str(acc) ;
>cat("item=");
>> str(item); length(item) }, data2, init=list())
>> acc= list()
>> item= Named num [1:2] 1 1
>>  - attr(*, "names")= chr [1:2] "oneA" "onB"
>> acc= int 2
>> item= Named num 3
>>  - attr(*, "names")= chr "threeA"
>> acc= int 1
>> item= Named num [1:2] 2 2
>>  - attr(*, "names")= chr [1:2] "twoA" "twoB"
>>
>>
>> I still don't understand why you want Reduce to to lapply's
>> job.   Reduce maps many to one and lapply maps many to
>> many.
>>
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Fri, Jul 29, 2016 at 1:37 AM, Stefan Kruger
><stefan.kruger at gmail.com>
>> wrote:
>>
>>> Jeremiah -
>>>
>>> neat - that's one step closer, but one small thing I still don't
>>> understand:
>>>
>>> > data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>>> > r = Reduce(function(acc, item) { append(acc,
>setNames(length(item),
>>> names(item))) }, data, list())
>>> > str(r)
>>> List of 3
>>>  $ : int 2
>>>  $ : int 1
>>>  $ : int 2
>>>
>>> I wanted the names to remain, but it seems like the "data" parameter
>loses
>>> its names when consumed by the Reduce()? If I print "item" inside
>the
>>> reducing function, it's not got the names. I'm probably missing some
>>> central tenet of R here.
>>>
>>> As to your comment of this being lapply() implemented by Reduce() -
>as I
>>> understand lapply()  (or map() in other functional languages), it's
>>> limited
>>> to returning a list/vector of the same length as the original.
>Consider
>>> this contrived example:
>>>
>>> > r = Reduce(function(acc, item) { if (length(item) > 1)
>{append(acc,
>>> setNames(length(item), names(item)))} }, data, list())
>>> > str(r)
>>>  int 2
>>> > r
>>> [1] 2
>>>
>>> I don't think you could achieve that with lapply()?
>>>
>>> Thanks
>>>
>>> Stefan
>>>
>>>
>>> On 28 July 2016 at 20:19, jeremiah rounds <roundsjeremiah at gmail.com>
>>> wrote:
>>>
>>> > Basically using Reduce as an lapply in that example, but I think
>that
>>> was
>>> > caused by how people started talking about things in the first
>place =)
>>> But
>>> > the point is the accumulator can be anything as far as I can tell.
>>> >
>>> > On Thu, Jul 28, 2016 at 12:14 PM, jeremiah rounds <
>>> > roundsjeremiah at gmail.com> wrote:
>>> >
>>> >> Re:
>>> >> "What I'm trying to
>>> >> work out is how to have the accumulator in Reduce not be the same
>type
>>> as
>>> >> the elements of the vector/list being reduced - ideally it could
>be an
>>> S3
>>> >> instance, list, vector, or data frame."
>>> >>
>>> >> Pretty sure that is not true.  See code that follows.  I would
>never
>>> >> solve this task in this way though so no comment on the use of
>Reduce
>>> for
>>> >> what you described.  (Note the accumulation of "functions" in a
>list is
>>> >> just a demo of possibilities).  You could accumulate in an
>environment
>>> too
>>> >> and potentially gain a lot of copy efficiency.
>>> >>
>>> >>
>>> >> lookup = list()
>>> >> lookup[[as.character(1)]] = function() print("1")
>>> >> lookup[[as.character(2)]] = function() print("2")
>>> >> lookup[[as.character(3)]] = function() print("3")
>>> >>
>>> >> data = list(c(1,2), c(1,4), c(3,3), c(2,30))
>>> >>
>>> >>
>>> >> r = Reduce(function(acc, item) {
>>> >> append(acc, list(lookup[[as.character(min(item))]]))
>>> >> }, data,list())
>>> >> r
>>> >> for(f in r) f()
>>> >>
>>> >>
>>> >> On Thu, Jul 28, 2016 at 5:09 AM, Stefan Kruger <
>>> stefan.kruger at gmail.com>
>>> >> wrote:
>>> >>
>>> >>> Ulrik - many thanks for your reply.
>>> >>>
>>> >>> I'm aware of many simple solutions as the one you suggest, both
>>> iterative
>>> >>> and functional style - but I'm trying to learn how to bend
>Reduce()
>>> for
>>> >>> the
>>> >>> purpose of using it in more complex processing tasks. What I'm
>trying
>>> to
>>> >>> work out is how to have the accumulator in Reduce not be the
>same
>>> type as
>>> >>> the elements of the vector/list being reduced - ideally it could
>be
>>> an S3
>>> >>> instance, list, vector, or data frame.
>>> >>>
>>> >>> Here's a more realistic example (in Elixir, sorry)
>>> >>>
>>> >>> Given two lists:
>>> >>>
>>> >>> 1. data: maps an id string to a vector of revision strings
>>> >>> 2. dict: maps known id/revision pairs as a string to true (or 1)
>>> >>>
>>> >>> find the items in data not already in dict, returned as a named
>list.
>>> >>>
>>> >>> ```elixir
>>> >>> data = %{
>>> >>>     "id1" => ["rev1.1", "rev1.2"],
>>> >>>     "id2" => ["rev2.1"],
>>> >>>     "id3" => ["rev3.1", "rev3.2", "rev3.3"]
>>> >>> }
>>> >>>
>>> >>> dict = %{
>>> >>>     "id1/rev1.1" => 1,
>>> >>>     "id1/rev1.2" => 1,
>>> >>>     "id3/rev3.1" => 1
>>> >>> }
>>> >>>
>>> >>> # Find the items in data not already in dict. Return as a
>grouped map
>>> >>>
>>> >>> Map.keys(data)
>>> >>>     |> Enum.flat_map(fn id -> Enum.map(data[id], fn rev -> {id,
>rev}
>>> end)
>>> >>> end)
>>> >>>     |> Enum.filter(fn {id, rev} -> !Dict.has_key?(dict,
>>> "#{id}/#{rev}")
>>> >>> end)
>>> >>>     |> Enum.reduce(%{}, fn ({k, v}, d) -> Map.update(d, k, [v],
>>> &[v|&1])
>>> >>> end)
>>> >>> ```
>>> >>>
>>> >>>
>>> >>>
>>> >>>
>>> >>> On 28 July 2016 at 12:03, Ulrik Stervbo
><ulrik.stervbo at gmail.com>
>>> wrote:
>>> >>>
>>> >>> > Hi Stefan,
>>> >>> >
>>> >>> > in that case,lapply(data, length) should do the trick.
>>> >>> >
>>> >>> > Best wishes,
>>> >>> > Ulrik
>>> >>> >
>>> >>> > On Thu, 28 Jul 2016 at 12:57 Stefan Kruger
><stefan.kruger at gmail.com
>>> >
>>> >>> > wrote:
>>> >>> >
>>> >>> >> David - many thanks for your response.
>>> >>> >>
>>> >>> >> What I tried to do was to turn
>>> >>> >>
>>> >>> >> data <- list(one = c(1, 1), three = c(3), two = c(2, 2))
>>> >>> >>
>>> >>> >> into
>>> >>> >>
>>> >>> >> result <- list(one = 2, three = 1, two = 2)
>>> >>> >>
>>> >>> >> that is creating a new list which has the same names as the
>first,
>>> but
>>> >>> >> where the values are the vector lengths.
>>> >>> >>
>>> >>> >> I know there are many other (and better) trivial ways of
>achieving
>>> >>> this -
>>> >>> >> my aim is less the task itself, and more figuring out if this
>can
>>> be
>>> >>> done
>>> >>> >> using Reduce() in the fashion I showed in the other examples
>I
>>> gave.
>>> >>> It's
>>> >>> >> a
>>> >>> >> building block of doing map-filter-reduce type pipelines that
>I'd
>>> >>> like to
>>> >>> >> understand how to do in R.
>>> >>> >>
>>> >>> >> Fumbling in the dark, I tried:
>>> >>> >>
>>> >>> >> Reduce(function(acc, item) { setNames(c(acc,
>length(data[item])),
>>> >>> item },
>>> >>> >> names(data), accumulate=TRUE)
>>> >>> >>
>>> >>> >> but setNames sets all the names, not adding one - and acc is
>still
>>> a
>>> >>> >> vector, not a list.
>>> >>> >>
>>> >>> >> It looks like 'lambda.tools.fold()' and possibly
>'purrr.reduce()'
>>> aim
>>> >>> at
>>> >>> >> doing what I'd like to do - but I've not been able to figure
>out
>>> quite
>>> >>> >> how.
>>> >>> >>
>>> >>> >> Thanks
>>> >>> >>
>>> >>> >> Stefan
>>> >>> >>
>>> >>> >>
>>> >>> >>
>>> >>> >> On 27 July 2016 at 20:35, David Winsemius
><dwinsemius at comcast.net>
>>> >>> wrote:
>>> >>> >>
>>> >>> >> >
>>> >>> >> > > On Jul 27, 2016, at 8:20 AM, Stefan Kruger <
>>> >>> stefan.kruger at gmail.com>
>>> >>> >> > wrote:
>>> >>> >> > >
>>> >>> >> > > Hi -
>>> >>> >> > >
>>> >>> >> > > I'm new to R.
>>> >>> >> > >
>>> >>> >> > > In other functional languages I'm familiar with you can
>often
>>> >>> seed a
>>> >>> >> call
>>> >>> >> > > to reduce() with a custom accumulator. Here's an example
>in
>>> >>> Elixir:
>>> >>> >> > >
>>> >>> >> > > map = %{"one" => [1, 1], "three" => [3], "two" => [2, 2]}
>>> >>> >> > > map |> Enum.reduce(%{}, fn ({k,v}, acc) ->
>Map.update(acc, k,
>>> >>> >> > > Enum.count(v), nil) end)
>>> >>> >> > > # %{"one" => 2, "three" => 1, "two" => 2}
>>> >>> >> > >
>>> >>> >> > > In R-terms that's reducing a list of vectors to become a
>new
>>> list
>>> >>> >> mapping
>>> >>> >> > > the names to the vector lengths.
>>> >>> >> > >
>>> >>> >> > > Even in JavaScript, you can do similar things:
>>> >>> >> > >
>>> >>> >> > > list = { one: [1, 1], three: [3], two: [2, 2] };
>>> >>> >> > > var result = Object.keys(list).reduceRight(function (acc,
>>> item) {
>>> >>> >> > >  acc[item] = list[item].length;
>>> >>> >> > >  return acc;
>>> >>> >> > > }, {});
>>> >>> >> > > // result == { two: 2, three: 1, one: 2 }
>>> >>> >> > >
>>> >>> >> > > In R, from what I can gather, Reduce() is restricted such
>that
>>> any
>>> >>> >> init
>>> >>> >> > > value you feed it is required to be of the same type as
>the
>>> >>> elements
>>> >>> >> of
>>> >>> >> > the
>>> >>> >> > > vector you're reducing -- so I can't build up. So whilst
>I can
>>> >>> do, say
>>> >>> >> > >
>>> >>> >> > >> Reduce(function(acc, item) { acc + item }, c(1,2,3,4,5),
>96)
>>> >>> >> > > [1] 111
>>> >>> >> > >
>>> >>> >> > > I can't use Reduce to build up a list, vector or data
>frame?
>>> >>> >> > >
>>> >>> >> > > What am I missing?
>>> >>> >> > >
>>> >>> >> > > Many thanks for any pointers,
>>> >>> >> >
>>> >>> >> > This builds a list:
>>> >>> >> >
>>> >>> >> > > Reduce(function(acc, item) { c(acc , item) },
>c(1,2,3,4,5), 96,
>>> >>> >> > accumulate=TRUE)
>>> >>> >> > [[1]]
>>> >>> >> > [1] 96
>>> >>> >> >
>>> >>> >> > [[2]]
>>> >>> >> > [1] 96  1
>>> >>> >> >
>>> >>> >> > [[3]]
>>> >>> >> > [1] 96  1  2
>>> >>> >> >
>>> >>> >> > [[4]]
>>> >>> >> > [1] 96  1  2  3
>>> >>> >> >
>>> >>> >> > [[5]]
>>> >>> >> > [1] 96  1  2  3  4
>>> >>> >> >
>>> >>> >> > [[6]]
>>> >>> >> > [1] 96  1  2  3  4  5
>>> >>> >> >
>>> >>> >> > But you are not saying what you want. The other examples
>were
>>> doing
>>> >>> >> > something with names but you provided no names for the R
>example.
>>> >>> >> >
>>> >>> >> > This would return a list of named vectors:
>>> >>> >> >
>>> >>> >> > > Reduce(function(acc, item) { setNames( c(acc,item),
>1:(item+1))
>>> >>> },
>>> >>> >> > c(1,2,3,4,5), 96, accumulate=TRUE)
>>> >>> >> > [[1]]
>>> >>> >> > [1] 96
>>> >>> >> >
>>> >>> >> > [[2]]
>>> >>> >> >  1  2
>>> >>> >> > 96  1
>>> >>> >> >
>>> >>> >> > [[3]]
>>> >>> >> >  1  2  3
>>> >>> >> > 96  1  2
>>> >>> >> >
>>> >>> >> > [[4]]
>>> >>> >> >  1  2  3  4
>>> >>> >> > 96  1  2  3
>>> >>> >> >
>>> >>> >> > [[5]]
>>> >>> >> >  1  2  3  4  5
>>> >>> >> > 96  1  2  3  4
>>> >>> >> >
>>> >>> >> > [[6]]
>>> >>> >> >  1  2  3  4  5  6
>>> >>> >> > 96  1  2  3  4  5
>>> >>> >> >
>>> >>> >> >
>>> >>> >> >
>>> >>> >> >
>>> >>> >> > > Stefan
>>> >>> >> > >
>>> >>> >> > >
>>> >>> >> > >
>>> >>> >> > > --
>>> >>> >> > > Stefan Kruger <stefan.kruger at gmail.com>
>>> >>> >> > >
>>> >>> >> > >       [[alternative HTML version deleted]]
>>> >>> >> > >
>>> >>> >> > > ______________________________________________
>>> >>> >> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>more,
>>> see
>>> >>> >> > > https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>> >> > > PLEASE do read the posting guide
>>> >>> >> > http://www.R-project.org/posting-guide.html
>>> >>> >> > > and provide commented, minimal, self-contained,
>reproducible
>>> code.
>>> >>> >> >
>>> >>> >> > David Winsemius
>>> >>> >> > Alameda, CA, USA
>>> >>> >> >
>>> >>> >> >
>>> >>> >>
>>> >>> >>
>>> >>> >> --
>>> >>> >> Stefan Kruger <stefan.kruger at gmail.com>
>>> >>> >>
>>> >>> >>         [[alternative HTML version deleted]]
>>> >>> >>
>>> >>> >> ______________________________________________
>>> >>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>>> >>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>> >> PLEASE do read the posting guide
>>> >>> >> http://www.R-project.org/posting-guide.html
>>> >>> >> and provide commented, minimal, self-contained, reproducible
>code.
>>> >>> >>
>>> >>> >
>>> >>>
>>> >>>
>>> >>> --
>>> >>> Stefan Kruger <stefan.kruger at gmail.com>
>>> >>>
>>> >>>         [[alternative HTML version deleted]]
>>> >>>
>>> >>> ______________________________________________
>>> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>> PLEASE do read the posting guide
>>> >>> http://www.R-project.org/posting-guide.html
>>> >>> and provide commented, minimal, self-contained, reproducible
>code.
>>> >>>
>>> >>
>>> >>
>>> >
>>>
>>>
>>> --
>>> Stefan Kruger <stefan.kruger at gmail.com>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>


From dulcalma at bigpond.com  Sat Jul 30 01:49:46 2016
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Sat, 30 Jul 2016 09:49:46 +1000
Subject: [R] font size in graphs...can R read Windows settings?
In-Reply-To: <CAJeYpE9s=QZoQmM_1JvhA2FsN4+YuMrG8KMtzBCkvuYcFY-6fA@mail.gmail.com>
References: <CAJeYpE8vGzFXB0eot1vc1sKCrFMA6Bm-0YkiNdfL+9skqqFb3A@mail.gmail.com>
	<000801d1e882$2f6e5f10$8e4b1d30$@bigpond.com>
	<CAJeYpE-nGCuJuS3zyNsP5MSEqZRxb579fVw2LKb=b-o4PMLStQ@mail.gmail.com>
	<000c01d1e936$a36a8d30$ea3fa790$@bigpond.com>
	<CAJeYpE9s=QZoQmM_1JvhA2FsN4+YuMrG8KMtzBCkvuYcFY-6fA@mail.gmail.com>
Message-ID: <000001d1e9f3$e39ac1f0$aad045d0$@bigpond.com>

Hi

 

There are some System commands that grab a computers settings to suit your purpose but I do not know what they are. 

May be tikz or Java or python based packages will improve things but I do not know them at all

 

Regards

 

Duncan

 

From: Dalthorp, Daniel [mailto:ddalthorp at usgs.gov] 
Sent: Friday, 29 July 2016 11:49
To: Duncan Mackay
Cc: R
Subject: Re: [R] font size in graphs...can R read Windows settings?

 

The trouble is getting the figs to look right for different users who happen to have different display settings. Nearly all my users will be on MS Windows, and, for the parameter set you gave, the figs will look different, depending on which "display size" parameter value is active and the screen resolution (for Windows 7 --- Control Panel | Appearance and Personalization | Display | Make text and other items larger or smaller). 

 

For example, the following gives different look (for text(), mtext(), axis labels, etc.) "smaller" than for "larger" or for fine resolution vs. coarse resolution. Getting it wrong makes the figs look sloppy and hard to read. If I knew users' display parameters ahead of time, I could adjust the drawing algorithms using par parameters, but I don't know the display parameters. 'din' values at time of package attachment (and after closing of open devices) gives a reasonable proxy in a lot of cases but not always.

 

par(mfrow = c(1,1),

     las = 1,

     mai = c(0.85, 0.85, 0.32, 0.12),

     font.main = 1,

     cex.main = 1.0,

     cex.lab  = 1.0,

     cex.axis = 0.9)

plot(0,0)

plot(0,0, type='n')

text(0,0, 'junk')

mtext(side = 3, line = .5, "more junk")

 

 

On Thu, Jul 28, 2016 at 6:15 PM, Duncan Mackay <dulcalma at bigpond.com> wrote:

Hi Dan

 

This is one of  the features/problems of proportional graphing ? getting it exactly right.

 

Anyway good graphs usually take time

 

I wonder if aspect in both base and lattice may be of use.

Also I have a default par settings for base graphics where I sometimes change the mai command for multiple graphs and rarely the font size.

 

par(mfrow = c(1,1),

    las = 1,

    mai = c(0.85, 0.85, 0.32, 0.12),

    font.main = 1,

    cex.main = 1.0,

    cex.lab  = 1.0,

    cex.axis = 0.9)

 

Duncan

 

From: Dalthorp, Daniel [mailto:ddalthorp at usgs.gov] 
Sent: Friday, 29 July 2016 09:22
To: Duncan Mackay
Cc: R
Subject: Re: [R] font size in graphs...can R read Windows settings?

 

Thanks, Duncan. This is close to what I was looking for. But I'm not using lattice. And the fontsize$text and fontsize$points are independent of display settings in Windows (screen resolution and 'size of objects').

 

I need to make my graphs [esp. placement and size of text(), mtext()] look good regardless of display settings.

 

Here's what I'm working with at the moment...

 

When package is loaded, I store the default graph size ('din') which is correlated with OS display parameters. Then, I simply multiple my current 'cex' values by the stored 'din' divided by 7 (which is the 'din' associated with the display parameters that I created the figs in initially). It seems to work just fine for my text() and mtext() text sizes. 

 

But some of my algorithms for placing text and subfigs in margins still need fixing...combinations of 'plt' and 'usr' seem to be working, but it's tedious.

 

-Dan

 

On Wed, Jul 27, 2016 at 8:43 PM, Duncan Mackay <dulcalma at bigpond.com> wrote:

Hi  Dan

For devices png, pdf, postscript and ? others the pointsize argument
controls the font size which is modified by cex

For lattice there are the settings in trellis.par.get()

trellis.par.get()$fontsize
$text
[1] 12

$points
[1] 8

which you can set and there is no real need to change font size except if
you need to change main.
trellis.par.get()$grid.pars  are the settings for grid elements if used  eg
text

these could be set globally by trellis.par.set() or individually with
argument par.settings eg
xyplot(y ~ x, data = datm,
             par.settings = list(strip.background = list(col =
"transparent"),
                                                 fontsize = list(text = 16,

points = 12),  # large size;  need to refine
                                                superpose.polygon= list(col
= c("red","blue"),

border = c("red","blue"))),
            type = "b")

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dalthorp,
Daniel
Sent: Thursday, 28 July 2016 07:02
To: r-help at R-project.org (r-help at r-project.org)
Subject: [R] font size in graphs...can R read Windows settings?

Hi All,
I am putting together a package that (among other things) draws some nice
graphs for users. I place some explanatory text on figs using "text" and
"mtext". But the size of the text depends on the Windows display settings:
Smaller (100%), medium (125%) or larger (150%) (In Windows 7... Control
panel | Appearance and personalization | Display | Make text and other
items smaller or larger). If I create figs that look good with one setting,
the text is too big (or too small) if another setting is used in Windows.
If I know the Windows setting, I can use cex in R to make the right sized
labels, but if I don't know a user's Windows display size setting...is
there a way for R to read the setting? Or is there another way to control
label size so that text labels on graphs look good regardless of WIndows
display size setting?

Many thanks for Ideas,

-Dan
--
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.







-- 

Dan Dalthorp, PhD

USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way 
Corvallis, OR 97331 
ph: 541-750-0953
ddalthorp at usgs.gov







-- 

Dan Dalthorp, PhD

USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way 
Corvallis, OR 97331 
ph: 541-750-0953
ddalthorp at usgs.gov


	[[alternative HTML version deleted]]


From jun.shen.ut at gmail.com  Sat Jul 30 02:08:11 2016
From: jun.shen.ut at gmail.com (Jun Shen)
Date: Fri, 29 Jul 2016 20:08:11 -0400
Subject: [R] How to pass na.rm=T to a user defined function
In-Reply-To: <51EAF573-5B10-43BC-8E83-1EA614FA415C@comcast.net>
References: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>
	<7AAA037F-BE78-403E-81B6-27767830947B@dcn.davis.ca.us>
	<CAMCXXmooBSueHbCeA8NbYB7qzWHO+fLZxjiWCUdAWgraJJTHJw@mail.gmail.com>
	<51EAF573-5B10-43BC-8E83-1EA614FA415C@comcast.net>
Message-ID: <CAMCXXmpb6XT10azrOmsqOWOmCCAXu=n3y150jGR-8h-1-afXhg@mail.gmail.com>

Thanks Jeff/David for the reply. I wasn't clear in the previous message.
the problem of using na.omit is it will omit the whole row where there is
at least one NA, even when some variables do have non-NA values.

For example: let's define a new function
N <- function(x) length(x[!is.na(x)])

test <-
data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
test$CL[1] <- NA

do.stats(test, stats.func=c('mean','sd','median','min','max','N'),
summary.var=c('CL','V1', 'V2','ALPHA'))

gives

         mean    sd  median   min  max  N
CL    -0.0232 0.918 -0.0786 -2.14 3.14 99
V1    -0.0410 0.936 -0.1160 -2.86 2.67 99
V2    -0.1760 0.978 -0.1490 -2.31 2.15 99
ALPHA -0.1380 0.960 -0.2160 -2.41 2.20 99

there is one non-missing value in V1,V2 and ALPHA is omitted.



On Fri, Jul 29, 2016 at 2:29 AM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jul 28, 2016, at 7:37 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> >
> > Because in reality the NA may appear in one variable but not others. For
> > example for ID=1, CL may be NA but not for others, For ID=2, V1 may be NA
> > etc. To keep all the IDs and all the variables in one data frame, it's
> > inevitable to see some NA
>
> That doesn't seem to acknowledge Newmiller's advice. In particular this
> would have seemed to an obvious response to that suggestion:
>
> do.stats <- function(data, stats.func, summary.var)
>           as.data.frame(signif(sapply(stats.func,function(func)
> mapply( func,  na.omit( data[summary.var]) )), 3))
>
>
> And please also heed the advice in the Posting Guide to use plain text.
>
> --
> David.
>
>
>
> >
> > On Thu, Jul 28, 2016 at 10:22 PM, Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us>
> > wrote:
> >
> >> Why not remove it yourself before passing it to those functions?
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On July 28, 2016 5:51:47 PM PDT, Jun Shen <jun.shen.ut at gmail.com>
> wrote:
> >>> Dear list,
> >>>
> >>> I write a small function to calculate multiple stats on multiple
> >>> variables
> >>> and export in a format exactly the way I want. Everything seems fine
> >>> until
> >>> NA appears in the data.
> >>>
> >>> Here is my function:
> >>>
> >>> do.stats <- function(data, stats.func, summary.var)
> >>>           as.data.frame(signif(sapply(stats.func,function(func)
> >>> mapply(func,data[summary.var])),3))
> >>>
> >>> A test dataset:
> >>> test <-
> >>
> >>>
> data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
> >>>
> >>> a command like the following
> >>> do.stats(test, stats.func=c('mean','sd','median','min','max'),
> >>> summary.var=c('CL','V1', 'V2','ALPHA'))
> >>>
> >>> gives me
> >>>
> >>>        mean    sd  median   min  max
> >>> CL     0.1030 0.917  0.0363 -2.32 2.47
> >>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
> >>> V2     0.0600 1.000  0.0621 -2.80 2.62
> >>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
> >>>
> >>>
> >>> However if I have a NA in the data
> >>> test$CL[1] <- NA
> >>>
> >>> The same command run gives me
> >>>        mean    sd  median   min  max
> >>> CL        * NA    NA      NA    NA   NA*
> >>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
> >>> V2     0.0600 1.000  0.0621 -2.80 2.62
> >>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
> >>>
> >>> I know this is because those functions (mean, sd etc.) all have
> >>> na.rm=F by default. How can I
> >>>
> >>> pass na.rm=T to all these functions without manually redefining those
> >>> stats functions
> >>>
> >>> Appreciate any comment.
> >>>
> >>> Thanks for your help.
> >>>
> >>>
> >>> Jun
> >>>
> >>>      [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Jul 30 02:52:45 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 29 Jul 2016 17:52:45 -0700
Subject: [R] How to pass na.rm=T to a user defined function
In-Reply-To: <CAMCXXmpb6XT10azrOmsqOWOmCCAXu=n3y150jGR-8h-1-afXhg@mail.gmail.com>
References: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>
	<7AAA037F-BE78-403E-81B6-27767830947B@dcn.davis.ca.us>
	<CAMCXXmooBSueHbCeA8NbYB7qzWHO+fLZxjiWCUdAWgraJJTHJw@mail.gmail.com>
	<51EAF573-5B10-43BC-8E83-1EA614FA415C@comcast.net>
	<CAMCXXmpb6XT10azrOmsqOWOmCCAXu=n3y150jGR-8h-1-afXhg@mail.gmail.com>
Message-ID: <8869FF7D-67E7-45F0-B581-E848226C8C16@comcast.net>


> On Jul 29, 2016, at 5:08 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> 
> Thanks Jeff/David for the reply. I wasn't clear in the previous message. the problem of using na.omit is it will omit the whole row where there is at least one NA, even when some variables do have non-NA values. 

Did you actually run the example I offered,  or did you just guess at what would happen and complained? When applied only to a vector there is no such thing as a "column". 

What you are describing would only have happened if `na.omit` were applied to an object that was a dataframe. That was not what was offered in the example.

-- 
David.
> 
> For example: let's define a new function
> N <- function(x) length(x[!is.na(x)])
> 
> test <- data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
> test$CL[1] <- NA
> 
> do.stats(test, stats.func=c('mean','sd','median','min','max','N'), summary.var=c('CL','V1', 'V2','ALPHA'))
> 
> gives
> 
>          mean    sd  median   min  max  N
> CL    -0.0232 0.918 -0.0786 -2.14 3.14 99
> V1    -0.0410 0.936 -0.1160 -2.86 2.67 99
> V2    -0.1760 0.978 -0.1490 -2.31 2.15 99
> ALPHA -0.1380 0.960 -0.2160 -2.41 2.20 99
> 
> 
> there is one non-missing value in V1,V2 and ALPHA is omitted.
> 
> 
> On Fri, Jul 29, 2016 at 2:29 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > On Jul 28, 2016, at 7:37 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> >
> > Because in reality the NA may appear in one variable but not others. For
> > example for ID=1, CL may be NA but not for others, For ID=2, V1 may be NA
> > etc. To keep all the IDs and all the variables in one data frame, it's
> > inevitable to see some NA
> 
> That doesn't seem to acknowledge Newmiller's advice. In particular this would have seemed to an obvious response to that suggestion:
> 
> do.stats <- function(data, stats.func, summary.var)
>           as.data.frame(signif(sapply(stats.func,function(func)
> mapply( func,  na.omit( data[summary.var]) )), 3))
> 
> 
> And please also heed the advice in the Posting Guide to use plain text.
> 
> --
> David.
> 
> 
> 
> >
> > On Thu, Jul 28, 2016 at 10:22 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> > wrote:
> >
> >> Why not remove it yourself before passing it to those functions?
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On July 28, 2016 5:51:47 PM PDT, Jun Shen <jun.shen.ut at gmail.com> wrote:
> >>> Dear list,
> >>>
> >>> I write a small function to calculate multiple stats on multiple
> >>> variables
> >>> and export in a format exactly the way I want. Everything seems fine
> >>> until
> >>> NA appears in the data.
> >>>
> >>> Here is my function:
> >>>
> >>> do.stats <- function(data, stats.func, summary.var)
> >>>           as.data.frame(signif(sapply(stats.func,function(func)
> >>> mapply(func,data[summary.var])),3))
> >>>
> >>> A test dataset:
> >>> test <-
> >>
> >>> data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
> >>>
> >>> a command like the following
> >>> do.stats(test, stats.func=c('mean','sd','median','min','max'),
> >>> summary.var=c('CL','V1', 'V2','ALPHA'))
> >>>
> >>> gives me
> >>>
> >>>        mean    sd  median   min  max
> >>> CL     0.1030 0.917  0.0363 -2.32 2.47
> >>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
> >>> V2     0.0600 1.000  0.0621 -2.80 2.62
> >>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
> >>>
> >>>
> >>> However if I have a NA in the data
> >>> test$CL[1] <- NA
> >>>
> >>> The same command run gives me
> >>>        mean    sd  median   min  max
> >>> CL        * NA    NA      NA    NA   NA*
> >>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
> >>> V2     0.0600 1.000  0.0621 -2.80 2.62
> >>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
> >>>
> >>> I know this is because those functions (mean, sd etc.) all have
> >>> na.rm=F by default. How can I
> >>>
> >>> pass na.rm=T to all these functions without manually redefining those
> >>> stats functions
> >>>
> >>> Appreciate any comment.
> >>>
> >>> Thanks for your help.
> >>>
> >>>
> >>> Jun
> >>>
> >>>      [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Sat Jul 30 03:00:00 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 29 Jul 2016 18:00:00 -0700
Subject: [R] How to pass na.rm=T to a user defined function
In-Reply-To: <8869FF7D-67E7-45F0-B581-E848226C8C16@comcast.net>
References: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>
	<7AAA037F-BE78-403E-81B6-27767830947B@dcn.davis.ca.us>
	<CAMCXXmooBSueHbCeA8NbYB7qzWHO+fLZxjiWCUdAWgraJJTHJw@mail.gmail.com>
	<51EAF573-5B10-43BC-8E83-1EA614FA415C@comcast.net>
	<CAMCXXmpb6XT10azrOmsqOWOmCCAXu=n3y150jGR-8h-1-afXhg@mail.gmail.com>
	<8869FF7D-67E7-45F0-B581-E848226C8C16@comcast.net>
Message-ID: <3CF7557E-93E9-46D6-93C3-D6A40A514DA4@comcast.net>


> On Jul 29, 2016, at 5:52 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Jul 29, 2016, at 5:08 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
>> 
>> Thanks Jeff/David for the reply. I wasn't clear in the previous message. the problem of using na.omit is it will omit the whole row where there is at least one NA, even when some variables do have non-NA values. 
> 
> Did you actually run the example I offered,  or did you just guess at what would happen and complained? When applied only to a vector there is no such thing as a "column". 
> 
> What you are describing would only have happened if `na.omit` were applied to an object that was a dataframe. That was not what was offered in the example.

And then I looked at the code again and realized you were not looping over the columns as I thought was happening. So what you wnat is:

do.stats <- function(data, stats.func, summary.var)
         as.data.frame(signif(sapply(stats.func,function(func)
mapply( func, lapply( data[summary.var], na.omit) )), 3))

-- 
David


> 
> -- 
> David.
>> 
>> For example: let's define a new function
>> N <- function(x) length(x[!is.na(x)])
>> 
>> test <- data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
>> test$CL[1] <- NA
>> 
>> do.stats(test, stats.func=c('mean','sd','median','min','max','N'), summary.var=c('CL','V1', 'V2','ALPHA'))
>> 
>> gives
>> 
>>         mean    sd  median   min  max  N
>> CL    -0.0232 0.918 -0.0786 -2.14 3.14 99
>> V1    -0.0410 0.936 -0.1160 -2.86 2.67 99
>> V2    -0.1760 0.978 -0.1490 -2.31 2.15 99
>> ALPHA -0.1380 0.960 -0.2160 -2.41 2.20 99
>> 
>> 
>> there is one non-missing value in V1,V2 and ALPHA is omitted.
>> 
>> 
>> On Fri, Jul 29, 2016 at 2:29 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>>> On Jul 28, 2016, at 7:37 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
>>> 
>>> Because in reality the NA may appear in one variable but not others. For
>>> example for ID=1, CL may be NA but not for others, For ID=2, V1 may be NA
>>> etc. To keep all the IDs and all the variables in one data frame, it's
>>> inevitable to see some NA
>> 
>> That doesn't seem to acknowledge Newmiller's advice. In particular this would have seemed to an obvious response to that suggestion:
>> 
>> do.stats <- function(data, stats.func, summary.var)
>>          as.data.frame(signif(sapply(stats.func,function(func)
>> mapply( func,  na.omit( data[summary.var]) )), 3))
>> 
>> 
>> And please also heed the advice in the Posting Guide to use plain text.
>> 
>> --
>> David.
>> 
>> 
>> 
>>> 
>>> On Thu, Jul 28, 2016 at 10:22 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>>> wrote:
>>> 
>>>> Why not remove it yourself before passing it to those functions?
>>>> --
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On July 28, 2016 5:51:47 PM PDT, Jun Shen <jun.shen.ut at gmail.com> wrote:
>>>>> Dear list,
>>>>> 
>>>>> I write a small function to calculate multiple stats on multiple
>>>>> variables
>>>>> and export in a format exactly the way I want. Everything seems fine
>>>>> until
>>>>> NA appears in the data.
>>>>> 
>>>>> Here is my function:
>>>>> 
>>>>> do.stats <- function(data, stats.func, summary.var)
>>>>>          as.data.frame(signif(sapply(stats.func,function(func)
>>>>> mapply(func,data[summary.var])),3))
>>>>> 
>>>>> A test dataset:
>>>>> test <-
>>>> 
>>>>> data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
>>>>> 
>>>>> a command like the following
>>>>> do.stats(test, stats.func=c('mean','sd','median','min','max'),
>>>>> summary.var=c('CL','V1', 'V2','ALPHA'))
>>>>> 
>>>>> gives me
>>>>> 
>>>>>       mean    sd  median   min  max
>>>>> CL     0.1030 0.917  0.0363 -2.32 2.47
>>>>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
>>>>> V2     0.0600 1.000  0.0621 -2.80 2.62
>>>>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
>>>>> 
>>>>> 
>>>>> However if I have a NA in the data
>>>>> test$CL[1] <- NA
>>>>> 
>>>>> The same command run gives me
>>>>>       mean    sd  median   min  max
>>>>> CL        * NA    NA      NA    NA   NA*
>>>>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
>>>>> V2     0.0600 1.000  0.0621 -2.80 2.62
>>>>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
>>>>> 
>>>>> I know this is because those functions (mean, sd etc.) all have
>>>>> na.rm=F by default. How can I
>>>>> 
>>>>> pass na.rm=T to all these functions without manually redefining those
>>>>> stats functions
>>>>> 
>>>>> Appreciate any comment.
>>>>> 
>>>>> Thanks for your help.
>>>>> 
>>>>> 
>>>>> Jun
>>>>> 
>>>>>     [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
>> 
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jun.shen.ut at gmail.com  Sat Jul 30 04:46:39 2016
From: jun.shen.ut at gmail.com (Jun Shen)
Date: Fri, 29 Jul 2016 22:46:39 -0400
Subject: [R] How to pass na.rm=T to a user defined function
In-Reply-To: <3CF7557E-93E9-46D6-93C3-D6A40A514DA4@comcast.net>
References: <CAMCXXmqh=JbeJyh3iPTzPw8bQ4UhV9F3TGs19-+mbPKVUWieiQ@mail.gmail.com>
	<7AAA037F-BE78-403E-81B6-27767830947B@dcn.davis.ca.us>
	<CAMCXXmooBSueHbCeA8NbYB7qzWHO+fLZxjiWCUdAWgraJJTHJw@mail.gmail.com>
	<51EAF573-5B10-43BC-8E83-1EA614FA415C@comcast.net>
	<CAMCXXmpb6XT10azrOmsqOWOmCCAXu=n3y150jGR-8h-1-afXhg@mail.gmail.com>
	<8869FF7D-67E7-45F0-B581-E848226C8C16@comcast.net>
	<3CF7557E-93E9-46D6-93C3-D6A40A514DA4@comcast.net>
Message-ID: <CAMCXXmrR5Zvkmxg4dXbTkq1uT0v3gexyegu3=jo5ZnJebOuSRQ@mail.gmail.com>

Thanks David.This is working perfectly!

On Fri, Jul 29, 2016 at 9:00 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Jul 29, 2016, at 5:52 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >
> >> On Jul 29, 2016, at 5:08 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> >>
> >> Thanks Jeff/David for the reply. I wasn't clear in the previous
> message. the problem of using na.omit is it will omit the whole row where
> there is at least one NA, even when some variables do have non-NA values.
> >
> > Did you actually run the example I offered,  or did you just guess at
> what would happen and complained? When applied only to a vector there is no
> such thing as a "column".
> >
> > What you are describing would only have happened if `na.omit` were
> applied to an object that was a dataframe. That was not what was offered in
> the example.
>
> And then I looked at the code again and realized you were not looping over
> the columns as I thought was happening. So what you wnat is:
>
> do.stats <- function(data, stats.func, summary.var)
>          as.data.frame(signif(sapply(stats.func,function(func)
> mapply( func, lapply( data[summary.var], na.omit) )), 3))
>
> --
> David
>
>
> >
> > --
> > David.
> >>
> >> For example: let's define a new function
> >> N <- function(x) length(x[!is.na(x)])
> >>
> >> test <-
> data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
> >> test$CL[1] <- NA
> >>
> >> do.stats(test, stats.func=c('mean','sd','median','min','max','N'),
> summary.var=c('CL','V1', 'V2','ALPHA'))
> >>
> >> gives
> >>
> >>         mean    sd  median   min  max  N
> >> CL    -0.0232 0.918 -0.0786 -2.14 3.14 99
> >> V1    -0.0410 0.936 -0.1160 -2.86 2.67 99
> >> V2    -0.1760 0.978 -0.1490 -2.31 2.15 99
> >> ALPHA -0.1380 0.960 -0.2160 -2.41 2.20 99
> >>
> >>
> >> there is one non-missing value in V1,V2 and ALPHA is omitted.
> >>
> >>
> >> On Fri, Jul 29, 2016 at 2:29 AM, David Winsemius <
> dwinsemius at comcast.net> wrote:
> >>
> >>> On Jul 28, 2016, at 7:37 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> >>>
> >>> Because in reality the NA may appear in one variable but not others.
> For
> >>> example for ID=1, CL may be NA but not for others, For ID=2, V1 may be
> NA
> >>> etc. To keep all the IDs and all the variables in one data frame, it's
> >>> inevitable to see some NA
> >>
> >> That doesn't seem to acknowledge Newmiller's advice. In particular this
> would have seemed to an obvious response to that suggestion:
> >>
> >> do.stats <- function(data, stats.func, summary.var)
> >>          as.data.frame(signif(sapply(stats.func,function(func)
> >> mapply( func,  na.omit( data[summary.var]) )), 3))
> >>
> >>
> >> And please also heed the advice in the Posting Guide to use plain text.
> >>
> >> --
> >> David.
> >>
> >>
> >>
> >>>
> >>> On Thu, Jul 28, 2016 at 10:22 PM, Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us>
> >>> wrote:
> >>>
> >>>> Why not remove it yourself before passing it to those functions?
> >>>> --
> >>>> Sent from my phone. Please excuse my brevity.
> >>>>
> >>>> On July 28, 2016 5:51:47 PM PDT, Jun Shen <jun.shen.ut at gmail.com>
> wrote:
> >>>>> Dear list,
> >>>>>
> >>>>> I write a small function to calculate multiple stats on multiple
> >>>>> variables
> >>>>> and export in a format exactly the way I want. Everything seems fine
> >>>>> until
> >>>>> NA appears in the data.
> >>>>>
> >>>>> Here is my function:
> >>>>>
> >>>>> do.stats <- function(data, stats.func, summary.var)
> >>>>>          as.data.frame(signif(sapply(stats.func,function(func)
> >>>>> mapply(func,data[summary.var])),3))
> >>>>>
> >>>>> A test dataset:
> >>>>> test <-
> >>>>
> >>>>>
> data.frame(ID=1:100,CL=rnorm(100),V1=rnorm(100),V2=rnorm(100),ALPHA=rnorm(100))
> >>>>>
> >>>>> a command like the following
> >>>>> do.stats(test, stats.func=c('mean','sd','median','min','max'),
> >>>>> summary.var=c('CL','V1', 'V2','ALPHA'))
> >>>>>
> >>>>> gives me
> >>>>>
> >>>>>       mean    sd  median   min  max
> >>>>> CL     0.1030 0.917  0.0363 -2.32 2.47
> >>>>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
> >>>>> V2     0.0600 1.000  0.0621 -2.80 2.62
> >>>>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
> >>>>>
> >>>>>
> >>>>> However if I have a NA in the data
> >>>>> test$CL[1] <- NA
> >>>>>
> >>>>> The same command run gives me
> >>>>>       mean    sd  median   min  max
> >>>>> CL        * NA    NA      NA    NA   NA*
> >>>>> V1    -0.0545 1.070 -0.2120 -2.21 2.70
> >>>>> V2     0.0600 1.000  0.0621 -2.80 2.62
> >>>>> ALPHA -0.0113 0.919  0.0284 -2.35 2.31
> >>>>>
> >>>>> I know this is because those functions (mean, sd etc.) all have
> >>>>> na.rm=F by default. How can I
> >>>>>
> >>>>> pass na.rm=T to all these functions without manually redefining those
> >>>>> stats functions
> >>>>>
> >>>>> Appreciate any comment.
> >>>>>
> >>>>> Thanks for your help.
> >>>>>
> >>>>>
> >>>>> Jun
> >>>>>
> >>>>>     [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>>
> >>>
> >>>      [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >> David Winsemius
> >> Alameda, CA, USA
> >>
> >>
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sat Jul 30 14:21:10 2016
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 30 Jul 2016 22:21:10 +1000
Subject: [R] How to Display Value Labels in R Outputs?
In-Reply-To: <1469731922615.78838@BTBOCES.ORG>
References: <1469731922615.78838@BTBOCES.ORG>
Message-ID: <CA+8X3fV4-PfS7VYF+RLnPcDhxf2dpHvC_XUCDmfWeWnms8-gyA@mail.gmail.com>

Hi Courtney,
I haven't seen any answers to your question, and perhaps it is because
others, like I, were unable to open the file you attached. The
uninformative labels you are getting may be the names of values or the
character value of factors. Is there a sample data set from the
original file that you read in?

Jim


On Fri, Jul 29, 2016 at 4:52 AM, Courtney Benjamin <cbenjami at btboces.org> wrote:
> Hello R Experts,
>
> I am using the Survey Package in R to do some initial descriptive stats for my dissertation. With the outputs for both the svymean and the barplot, I would like the value labels to be displayed for the variable-it would make the descriptive statistics much easier to interpret. Instead of the output labels of: F1RTRCC1, F1RTRCC2-I would like to see the value labels of "academic" and "occupational" to be displayed.
>
> How do I go about making this happen? I am including a minimal reproducible example with a small subset of my actual data:
>
> https://drive.google.com/file/d/0B5fHCR5TGRjaZ29wNzR0cF9YRXc/view?usp=sharing
>
>
> Any help is greatly appreciated. My only experience thus far is with SPSS and I have a feeling that the reason the variable value labels are not appearing is due to either the way I read the dataset into R: elsq1ch<-read.table (file="els-Q1-04-21-16.dat", header = TRUE, sep = "\t", quote = "\"", dec =".") or I am not specifying some detail that is required to manually assign labels to the values of the variables.?
>
>
> Courtney Benjamin
>
> Broome-Tioga BOCES
>
> Automotive Technology II Teacher
>
> Located at Gault Toyota
>
> Doctoral Candidate-Educational Theory & Practice
>
> State University of New York at Binghamton
>
> cbenjami at btboces.org<mailto:cbenjami at btboces.org>
>
> 607-763-8633
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From neal at walfield.org  Sat Jul 30 18:58:19 2016
From: neal at walfield.org (Neal H. Walfield)
Date: Sat, 30 Jul 2016 18:58:19 +0200
Subject: [R] %in% with matrix of lists
Message-ID: <877fc3hxlg.wl-neal@walfield.org>

I have a matrix of lists.  Something along the lists of (but much
bigger than):

  x = array(dim=c(2, 2), data=list())
  x[1,1] = list(1:5)
  x[2,1] = list(6:9)
  x[1,2] = list(10:13)
  x[2,2] = list(14:16)

Each list contains a number of observations/ground truth for a
particular state.  That is, state <1,1> has observations <1,2,3,4,5>.
States may have a different number of observations (which is why I'm
not using an array).

I have another numeric matrix with the same dimensions and I want to
check if its values occur.  Something along the lines of:

 y = array(dim=c(2, 2), data=c(1, 10, 11, 20))

I want to do:

 y %in% x

But it doesn't work as hoped.  (I expect: c(T, F, T, F).)

I realize that I can do this with sapply, but I was hoping for a
faster / smarter solution.  Any ideas?

Thanks!

:) Neal


From bgunter.4567 at gmail.com  Sat Jul 30 19:28:42 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 30 Jul 2016 10:28:42 -0700
Subject: [R] %in% with matrix of lists
In-Reply-To: <877fc3hxlg.wl-neal@walfield.org>
References: <877fc3hxlg.wl-neal@walfield.org>
Message-ID: <CAGxFJbQHn0JTioSMFFua418mamZxT9VwQuvQ=KnD4QueWJO7rQ@mail.gmail.com>

Bottom line: No, I dont see any vectorized way to do this.

However, the following may offer some slight improvement over your approach.

1. Why do you need to store these as arrays, which are merely vectors
with a "dim" attribute?

## convert to vectors (a list is also a vector):

dim(x) <- NULL; dim(y) <- NULL

2. ... and use mapply() instead of sapply() (not clear to me  *how*
you mean to use sapply() anyway)

> mapply("%in%",y,x)  ## might be slightly faster to use match() directly

[1]  TRUE FALSE  TRUE FALSE  ## NOT c(T,F,T,F)


Whether this makes any noticeable difference seems doubtful, however.

There may be special packages that do offer a real improvement, so you
should search. rseek.org is a good R search engine, although google
often does well also.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Jul 30, 2016 at 9:58 AM, Neal H. Walfield <neal at walfield.org> wrote:
> I have a matrix of lists.  Something along the lists of (but much
> bigger than):
>
>   x = array(dim=c(2, 2), data=list())
>   x[1,1] = list(1:5)
>   x[2,1] = list(6:9)
>   x[1,2] = list(10:13)
>   x[2,2] = list(14:16)
>
> Each list contains a number of observations/ground truth for a
> particular state.  That is, state <1,1> has observations <1,2,3,4,5>.
> States may have a different number of observations (which is why I'm
> not using an array).
>
> I have another numeric matrix with the same dimensions and I want to
> check if its values occur.  Something along the lines of:
>
>  y = array(dim=c(2, 2), data=c(1, 10, 11, 20))
>
> I want to do:
>
>  y %in% x
>
> But it doesn't work as hoped.  (I expect: c(T, F, T, F).)
>
> I realize that I can do this with sapply, but I was hoping for a
> faster / smarter solution.  Any ideas?
>
> Thanks!
>
> :) Neal
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From neal at walfield.org  Sat Jul 30 19:39:54 2016
From: neal at walfield.org (Neal H. Walfield)
Date: Sat, 30 Jul 2016 19:39:54 +0200
Subject: [R] %in% with matrix of lists
In-Reply-To: <CAGxFJbQHn0JTioSMFFua418mamZxT9VwQuvQ=KnD4QueWJO7rQ@mail.gmail.com>
References: <877fc3hxlg.wl-neal@walfield.org>
	<CAGxFJbQHn0JTioSMFFua418mamZxT9VwQuvQ=KnD4QueWJO7rQ@mail.gmail.com>
Message-ID: <8760rnhvo5.wl-neal@walfield.org>

On Sat, 30 Jul 2016 19:28:42 +0200,
Bert Gunter wrote:
> Bottom line: No, I dont see any vectorized way to do this.
> 
> However, the following may offer some slight improvement over your approach.
> 
> 1. Why do you need to store these as arrays, which are merely vectors
> with a "dim" attribute?
> 
> ## convert to vectors (a list is also a vector):
> 
> dim(x) <- NULL; dim(y) <- NULL

I apologize that my example was not minimal.  I have the data stored
in an array for other reasons.

> 2. ... and use mapply() instead of sapply() (not clear to me  *how*
> you mean to use sapply() anyway)

Sorry, I meant (and I'm using) mapply.

> > mapply("%in%",y,x)  ## might be slightly faster to use match() directly
> 
> [1]  TRUE FALSE  TRUE FALSE  ## NOT c(T,F,T,F)

I'm not sure what you mean by NOT here.  You get the same answer as I
do, as far as I can see.


Thanks for your help!

:) Neal


From ruipbarradas at sapo.pt  Sat Jul 30 19:42:54 2016
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Sat, 30 Jul 2016 18:42:54 +0100
Subject: [R] %in% with matrix of lists
In-Reply-To: <877fc3hxlg.wl-neal@walfield.org>
Message-ID: <20160730184254.Horde.XyoiMxcU86U_eXwn5d7vAPZ@mail.sapo.pt>

Hello,

I don't have a solution but see the difference between the two loops below.

for(j in 1:2)
?? ?for(i in 1:2){
?? ??? ?cat(y[i,j], "\n")
?? ??? ?cat(x[i,j][[1]], "\n")
?? ??? ?cat(y[i,j] %in% x[i,j], "\n", "\n")
?? ?}

for(j in 1:2)
?? ?for(i in 1:2){
?? ??? ?cat(y[i,j], "\n")
?? ??? ?cat(x[i,j][[1]], "\n")
?? ??? ?cat(y[i,j] %in% x[i,j][[1]], "\n", "\n")
?? ?}

So apparently y %in% x is equivalent to the first, but you want the second.
Note that the following doesn't work either.

y %in% x[[1]]
[1]? TRUE FALSE FALSE FALSE

Rui Barradas

?

Citando Neal H. Walfield <neal at walfield.org>:

> I have a matrix of lists.? Something along the lists of (but much
> bigger than):
>
> x = array(dim=c(2, 2), data=list())
> x[1,1] = list(1:5)
> x[2,1] = list(6:9)
> x[1,2] = list(10:13)
> x[2,2] = list(14:16)
>
> Each list contains a number of observations/ground truth for a
> particular state.? That is, state <1,1> has observations <1,2,3,4,5>.
> States may have a different number of observations (which is why I'm
> not using an array).
>
> I have another numeric matrix with the same dimensions and I want to
> check if its values occur.? Something along the lines of:
>
> y = array(dim=c(2, 2), data=c(1, 10, 11, 20))
>
> I want to do:
>
> y %in% x
>
> But it doesn't work as hoped.? (I expect: c(T, F, T, F).)
>
> I realize that I can do this with sapply, but I was hoping for a
> faster / smarter solution.? Any ideas?
>
> Thanks!
>
> :) Neal
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From peter.anthoni at kit.edu  Sat Jul 30 20:06:16 2016
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Sat, 30 Jul 2016 18:06:16 +0000
Subject: [R] Aggregate matrix in a 2 by 2 manor
In-Reply-To: <506368AE-3234-425B-97B5-DD3B13094237@comcast.net>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
	<CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
	<E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>
	<f2247c74794a41ad8a70f85d2c85df92@exch-2p-mbx-t2.ads.tamu.edu>
	<ED550726-6ACD-487A-959A-2DB8706B76F4@dcn.davis.ca.us>
	<506368AE-3234-425B-97B5-DD3B13094237@comcast.net>
Message-ID: <A5E0D447-2136-4A69-AC3A-E2514BEFE29F@kit.edu>

Hi all,

thanks for the suggestions, I did some timing tests, see below.
Unfortunately the aggregate.nx.ny.array.apply, does not produce the expected result.
So the fastest seems to be the aggregate.nx.ny.expand.grid, though the double for loop is not that much slower.

many thanks
Peter

> tst=matrix(1:(1440*360),ncol=1440,nrow=360)
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.forloop(tst,2,2,mean,na.rm=T)})
   user  system elapsed 
 11.227   0.073  11.371 
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.interaction(tst,2,2,mean,na.rm=T)})
   user  system elapsed 
 26.354   0.475  26.880 
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.expand.grid(tst,2,2,mean,na.rm=T)})
   user  system elapsed 
  9.683   0.055   9.763 
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.array.apply(tst,2,2,mean,na.rm=T)})
   user  system elapsed 
  7.693   0.055   7.800 

> tst.small=matrix(1:(8*4),ncol=8,nrow=4)
> aggregate.nx.ny.forloop = function(data,nx=2,ny=2, FUN=mean,...) 
+ {
+   nlon=nrow(data)
+   nlat=ncol(data)
+   newdata=matrix(NA,nrow=nlon/nx,ncol=nlat/ny)
+   dim(newdata)
+   for(ilon in seq(1,nlon,nx)) {
+     for(ilat in seq(1,nlat,ny)) {
+       ilon_new=1+(ilon-1)/nx
+       ilat_new=1+(ilat-1)/ny
+       newdata[ilon_new,ilat_new] = FUN(data[ilon+0:1,ilat+0:1],...)
+     }
+   }
+   newdata
+ }
> aggregate.nx.ny.forloop(tst.small)
     [,1] [,2] [,3] [,4]
[1,]  3.5 11.5 19.5 27.5
[2,]  5.5 13.5 21.5 29.5
> 
> aggregate.nx.ny.interaction = function(data,nx=2,ny=2, FUN=mean,...) 
+ {
+   
+   nlon=nrow(data)
+   nlat=ncol(data)
+   newdata=matrix(NA,nrow=nlon/nx,ncol=nlat/ny)
+   newdata[] <- tapply( data, interaction( (row(data)+1) %/% 2, (col(data)+1) %/% 2 ), FUN, ...)
+   newdata
+ }
> aggregate.nx.ny.interaction(tst.small)
     [,1] [,2] [,3] [,4]
[1,]  3.5 11.5 19.5 27.5
[2,]  5.5 13.5 21.5 29.5
> 
> aggregate.nx.ny.expand.grid = function(data,nx=2,ny=2, FUN=mean,...) 
+ {
+   ilon <- seq(1,ncol(data),nx)
+   ilat <- seq(1,nrow(data),ny)
+   cells <- as.matrix(expand.grid(ilat, ilon))
+   blocks <- apply(cells, 1, function(x) data[x[1]:(x[1]+1),x[2]:(x[2]+1)])
+   block.means <- colMeans(blocks)
+   matrix(block.means, nrow(data)/ny, ncol(data)/nx)
+ }
> aggregate.nx.ny.expand.grid(tst.small)
     [,1] [,2] [,3] [,4]
[1,]  3.5 11.5 19.5 27.5
[2,]  5.5 13.5 21.5 29.5
> 
> aggregate.nx.ny.array.apply = function(data,nx=2,ny=2, FUN=mean,...) {
+   a <- array(data, dim = c(ny, nrow( data ) %/% ny, ncol( data ) %/% nx))
+   apply( a, c(2, 3), FUN, ... )
+ }
> aggregate.nx.ny.array.apply(tst.small)
     [,1] [,2] [,3] [,4]
[1,]  1.5  5.5  9.5 13.5
[2,]  3.5  7.5 11.5 15.5



> On 28 Jul 2016, at 00:26, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Jul 27, 2016, at 12:02 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>> 
>> An alternative (more compact, not necessarily faster, because apply is still a for loop inside):
>> 
>> f <- function( m, nx, ny ) {
>> # redefine the dimensions of my
>> a <- array( m
>>            , dim = c( ny
>>                   , nrow( m ) %/% ny
>>                   , ncol( m ) %/% nx )
>>           )
>> # apply mean over dim 1
>> apply( a, c( 2, 3 ), FUN=mean )
>> }
>> f( tst, nx, ny )
> 
> Here's an apparently loopless strategy, although I suspect the code for interaction (and maybe tapply as well?) uses a loop.
> 
> 
> tst_2X2 <- matrix(NA, ,ncol=4,nrow=2)
> 
> tst_2x2[] <- tapply( tst, interaction( (row(tst)+1) %/% 2, (col(tst)+1) %/% 2 ), mean)
> 
> tst_2x2
> 
>     [,1] [,2] [,3] [,4]
> [1,]  3.5 11.5 19.5 27.5
> [2,]  5.5 13.5 21.5 29.5
> 
> -- 
> David.
> 
> 
>> 
>> -- 
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 27, 2016 9:08:32 AM PDT, David L Carlson <dcarlson at tamu.edu> wrote:
>>> This should be faster. It uses apply() across the blocks. 
>>> 
>>>> ilon <- seq(1,8,nx)
>>>> ilat <- seq(1,4,ny)
>>>> cells <- as.matrix(expand.grid(ilat, ilon))
>>>> blocks <- apply(cells, 1, function(x) tst[x[1]:(x[1]+1),
>>> x[2]:(x[2]+1)])
>>>> block.means <- colMeans(blocks)
>>>> tst_2x2 <- matrix(block.means, 2, 4)
>>>> tst_2x2
>>>   [,1] [,2] [,3] [,4]
>>> [1,]  3.5 11.5 19.5 27.5
>>> [2,]  5.5 13.5 21.5 29.5
>>> 
>>> -------------------------------------
>>> David L Carlson
>>> Department of Anthropology
>>> Texas A&M University
>>> College Station, TX 77840-4352
>>> 
>>> 
>>> 
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-poject.org] On Behalf Of Anthoni,
>>> Peter (IMK)
>>> Sent: Wednesday, July 27, 2016 6:14 AM
>>> To: r-help at r-project.org
>>> Subject: [R] Aggregate matrix in a 2 by 2 manor
>>> 
>>> Hi all,
>>> 
>>> I need to aggregate some matrix data (1440x720) to a lower dimension
>>> (720x360) for lots of years and variables
>>> 
>>> I can do double for loop, but that will be slow. Anybody know a quicker
>>> way?
>>> 
>>> here an example with a smaller matrix size:
>>> 
>>> tst=matrix(1:(8*4),ncol=8,nrow=4)
>>> tst_2x2=matrix(NA,ncol=4,nrow=2)
>>> nx=2
>>> ny=2
>>> for(ilon in seq(1,8,nx)) {
>>> for (ilat in seq(1,4,ny)) {
>>>  ilon_2x2=1+(ilon-1)/nx
>>>  ilat_2x2=1+(ilat-1)/ny
>>>  tst_2x2[ilat_2x2,ilon_2x2] = mean(tst[ilat+0:1,ilon+0:1])
>>> }
>>> }
>>> 
>>> tst
>>> tst_2x2
>>> 
>>>> tst
>>>   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
>>> [1,]    1    5    9   13   17   21   25   29
>>> [2,]    2    6   10   14   18   22   26   30
>>> [3,]    3    7   11   15   19   23   27   31
>>> [4,]    4    8   12   16   20   24   28   32
>>> 
>>>> tst_2x2
>>>   [,1] [,2] [,3] [,4]
>>> [1,]  3.5 11.5 19.5 27.5
>>> [2,]  5.5 13.5 21.5 29.5
>>> 
>>> 
>>> I though a cast to 3d-array might do the trick and apply over the new
>>> dimension, but that does not work, since it casts the data along the
>>> row.
>>>> matrix(apply(array(tst,dim=c(nx,ny,8)),3,mean),nrow=nrow(tst)/ny)
>>>   [,1] [,2] [,3] [,4]
>>> [1,]  2.5 10.5 18.5 26.5
>>> [2,]  6.5 14.5 22.5 30.5
>>> 
>>> 
>>> cheers
>>> Peter
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 


From jdnewmil at dcn.davis.ca.us  Sat Jul 30 20:35:40 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 30 Jul 2016 11:35:40 -0700
Subject: [R] %in% with matrix of lists
In-Reply-To: <8760rnhvo5.wl-neal@walfield.org>
References: <877fc3hxlg.wl-neal@walfield.org>
	<CAGxFJbQHn0JTioSMFFua418mamZxT9VwQuvQ=KnD4QueWJO7rQ@mail.gmail.com>
	<8760rnhvo5.wl-neal@walfield.org>
Message-ID: <F933A574-3583-4D4F-BFE8-C8EC413B74C8@dcn.davis.ca.us>

>> [1]  TRUE FALSE  TRUE FALSE  ## NOT c(T,F,T,F)
>
>I'm not sure what you mean by NOT here.  You get the same answer as I
>do, as far as I can see.
>

# valid R
T <- FALSE
# invalid R
TRUE <- FALSE

It is much much safer and clearer to use TRUE/FALSE than T/F. So

c( TRUE, FALSE,  TRUE, FALSE ) may not always be the same as c(T,F,T,F).

I recommend reading the R Inferno to learn about this other such pitfalls. 
-- 
Sent from my phone. Please excuse my brevity.


From neal at walfield.org  Sat Jul 30 20:38:03 2016
From: neal at walfield.org (Neal H. Walfield)
Date: Sat, 30 Jul 2016 20:38:03 +0200
Subject: [R] %in% with matrix of lists
In-Reply-To: <F933A574-3583-4D4F-BFE8-C8EC413B74C8@dcn.davis.ca.us>
References: <877fc3hxlg.wl-neal@walfield.org>
	<CAGxFJbQHn0JTioSMFFua418mamZxT9VwQuvQ=KnD4QueWJO7rQ@mail.gmail.com>
	<8760rnhvo5.wl-neal@walfield.org>
	<F933A574-3583-4D4F-BFE8-C8EC413B74C8@dcn.davis.ca.us>
Message-ID: <8737mrhsz8.wl-neal@walfield.org>

On Sat, 30 Jul 2016 20:35:40 +0200,
Jeff Newmiller wrote:
> 
> >> [1]  TRUE FALSE  TRUE FALSE  ## NOT c(T,F,T,F)
> >
> >I'm not sure what you mean by NOT here.  You get the same answer as I
> >do, as far as I can see.
> >
> 
> # valid R
> T <- FALSE
> # invalid R
> TRUE <- FALSE
> 
> It is much much safer and clearer to use TRUE/FALSE than T/F. So
> 
> c( TRUE, FALSE,  TRUE, FALSE ) may not always be the same as c(T,F,T,F).

I see.  I was just trying to create a minimal example.  I'll try to be
more vigorous next time!

> I recommend reading the R Inferno to learn about this other such pitfalls. 

Thanks for the tip!


From maechler at stat.math.ethz.ch  Sat Jul 30 21:06:25 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 30 Jul 2016 21:06:25 +0200
Subject: [R] lm() silently drops NAs
In-Reply-To: <AB5B2D91-FFC6-4142-88D1-D7BFBFA6BFCA@gmail.com>
References: <CABdHhvEAh4SYRKStO8tTZOAF=HXWmVwuioC+QkUaLJ04SEPcdg@mail.gmail.com>
	<22423.7743.681107.446114@stat.math.ethz.ch>
	<CABdHhvGYoydrVDOS_Wn7BpV1isvWrP+cSUwcUOQ93BGeftYcBg@mail.gmail.com>
	<AB5B2D91-FFC6-4142-88D1-D7BFBFA6BFCA@gmail.com>
Message-ID: <22428.64177.706198.219373@stat.math.ethz.ch>

>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>     on Tue, 26 Jul 2016 23:30:31 +0200 writes:

    >> On 26 Jul 2016, at 22:26 , Hadley Wickham
    >> <h.wickham at gmail.com> wrote:
    >> 
    >> On Tue, Jul 26, 2016 at 3:24 AM, Martin Maechler
    >> <maechler at stat.math.ethz.ch> wrote:
    >>> 
    > ...
    >> To me, this would be the most sensible default behaviour,
    >> but I realise it's too late to change without breaking
    >> many existing expectations.

    > Probably.

Well,.... yes, Hadley's proposal would change too many default outputs;
Still, I'd tend think we at least should contemplate changing
the default (mildly).  Before doing that we could at least provide one
or a few new  na.* functions which people could try using  as their own
defaults, using options(na.action = *) __ or __ as I read in the
white book (see below) by

      attr(<data.frame>, "na.action") <-  na.<myfavorite>

That is actually something that I have not seen advertised much,
but seems very reasonable to me.

It often does make sense to set a dataset-specific NA handling.

I agree (with Hadley's implicit statement) that 'na.exclude' is
often much more reasonable than 'na.omit' and that was the
reason it was invented *after* the white book, IIRC, for S-PLUS,
not S; and quite advertized at the time (at least by some people
to whom I listened ...)


If we'd change the default na.action , we'd probably would not want
to go  as far as a version of  Hadley's (corrected in 2nd e-mail) 

   na.warn <- function(object, ...) {
     missing <- !complete.cases(object)
     if (any(missing)) {
	warning("Dropping ", sum(missing), " rows with missing values", call. = FALSE)
     }
     na.exclude(object, ...)
   }

[the warning probably needs to be a bit smarter: you don't always have
 "rows", sometimes it's "entries" in a vector]

but *providing* something like the above, possibly rather named
along the line of

	'na.warn.and.keep'

and provide a  'na.warn.and.omit'  (which I would call
'na.warn', as after all it would be very close to the current R
default, na.omit, but would warn additionally..




    > Re. the default choice, my recollection is that at the
    > time the only choices available were na.omit and na.omit.

    > S-PLUS was using na.fail for all the usual good
    > reasons, but from a practical perspective, the consequence
    > was that, since almost every data set has NA values, you
    > got an error unless you added na.action=na.omit to every
    > single lm() call. And habitually typing na.action=na.omit
    > doesn't really solve any of the issues with different
    > models being fit to different subsets and all that. 

yes, indeed; you are right:

- The white book (p. 112-113) indeed explains that 'na.fail' is
  (i.e., was there) the default.

- an important example of where na.omit was not good enough, at
  the time was stepwise regression (where the number of complete
  cases may depend on the current subset of predictor
  variables), or also bootstrapping and crossvalidation.

    > So the
    > rationale for doing it differently in R was that it was
    > better to get some probably meaningful output rather than
    > to be certain of getting nothing. And, that was what the
    > mainstream packages of the time were doing.

    >> On a related note, I've never really understood why it's
    >> called na.exclude - from my perspective it causes the
    >> _inclusion_ of missing values in the
    >> predictions/residuals.

    > I think the notion is that you exclude them from the
    > analysis, but keep them around for the other purposes.

    > -pd

    >> 
    >> Thanks for the (as always!) informative response, Martin.
    >> 
    >> Hadley
    >> 
    >> -- 
    >> http://hadley.nz
    >> 
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide
    >> http://www.R-project.org/posting-guide.html and provide
    >> commented, minimal, self-contained, reproducible code.

    > -- 
    > Peter Dalgaard, Professor, Center for Statistics,
    > Copenhagen Business School Solbjerg Plads 3, 2000
    > Frederiksberg, Denmark Phone: (+45)38153501 Office: A 4.23
    > Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com


From bgunter.4567 at gmail.com  Sat Jul 30 22:40:24 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 30 Jul 2016 13:40:24 -0700
Subject: [R] %in% with matrix of lists
In-Reply-To: <8737mrhsz8.wl-neal@walfield.org>
References: <877fc3hxlg.wl-neal@walfield.org>
	<CAGxFJbQHn0JTioSMFFua418mamZxT9VwQuvQ=KnD4QueWJO7rQ@mail.gmail.com>
	<8760rnhvo5.wl-neal@walfield.org>
	<F933A574-3583-4D4F-BFE8-C8EC413B74C8@dcn.davis.ca.us>
	<8737mrhsz8.wl-neal@walfield.org>
Message-ID: <CAGxFJbRO_jyGtTqk7jUxJHch7sCJi_jRB3UvtrHCsjw-L-LW4g@mail.gmail.com>

Below.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Jul 30, 2016 at 11:38 AM, Neal H. Walfield <neal at walfield.org> wrote:
> On Sat, 30 Jul 2016 20:35:40 +0200,
> Jeff Newmiller wrote:
>>
>> >> [1]  TRUE FALSE  TRUE FALSE  ## NOT c(T,F,T,F)
>> >
>> >I'm not sure what you mean by NOT here.  You get the same answer as I
>> >do, as far as I can see.
>> >
>>
>> # valid R
>> T <- FALSE
>> # invalid R
>> TRUE <- FALSE
>>
>> It is much much safer and clearer to use TRUE/FALSE than T/F. So
>>
>> c( TRUE, FALSE,  TRUE, FALSE ) may not always be the same as c(T,F,T,F).
>
> I see.  I was just trying to create a minimal example.  I'll try to be
> more vigorous next time!

... and more rigorous, too, I hope. ;-)

-- Bert



>
>> I recommend reading the R Inferno to learn about this other such pitfalls.
>
> Thanks for the tip!


From jdnewmil at dcn.davis.ca.us  Sat Jul 30 23:02:37 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 30 Jul 2016 14:02:37 -0700
Subject: [R] Aggregate matrix in a 2 by 2 manor
In-Reply-To: <A5E0D447-2136-4A69-AC3A-E2514BEFE29F@kit.edu>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
	<CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
	<E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>
	<f2247c74794a41ad8a70f85d2c85df92@exch-2p-mbx-t2.ads.tamu.edu>
	<ED550726-6ACD-487A-959A-2DB8706B76F4@dcn.davis.ca.us>
	<506368AE-3234-425B-97B5-DD3B13094237@comcast.net>
	<A5E0D447-2136-4A69-AC3A-E2514BEFE29F@kit.edu>
Message-ID: <8CAC8B1A-89AC-449F-B85A-923F18606333@dcn.davis.ca.us>

For the record, the array.apply code can be fixed as below, but then it is slower than the expand.grid version. 

aggregate.nx.ny.array.apply <- function(dta,nx=2,ny=2, FUN=mean,...)
{
   a <- array(dta, dim = c(ny, nrow( dta ) %/% ny, nx, ncol( dta ) %/% nx))
  apply( a, c(2, 4), FUN, ... )
}

-- 
Sent from my phone. Please excuse my brevity.

On July 30, 2016 11:06:16 AM PDT, "Anthoni, Peter (IMK)" <peter.anthoni at kit.edu> wrote:
>Hi all,
>
>thanks for the suggestions, I did some timing tests, see below.
>Unfortunately the aggregate.nx.ny.array.apply, does not produce the
>expected result.
>So the fastest seems to be the aggregate.nx.ny.expand.grid, though the
>double for loop is not that much slower.
>
>many thanks
>Peter
>
>> tst=matrix(1:(1440*360),ncol=1440,nrow=360)
>> system.time( {for(i in 1:10)
>tst_2x2=aggregate.nx.ny.forloop(tst,2,2,mean,na.rm=T)})
>   user  system elapsed 
> 11.227   0.073  11.371 
>> system.time( {for(i in 1:10)
>tst_2x2=aggregate.nx.ny.interaction(tst,2,2,mean,na.rm=T)})
>   user  system elapsed 
> 26.354   0.475  26.880 
>> system.time( {for(i in 1:10)
>tst_2x2=aggregate.nx.ny.expand.grid(tst,2,2,mean,na.rm=T)})
>   user  system elapsed 
>  9.683   0.055   9.763 
>> system.time( {for(i in 1:10)
>tst_2x2=aggregate.nx.ny.array.apply(tst,2,2,mean,na.rm=T)})
>   user  system elapsed 
>  7.693   0.055   7.800 
>
>> tst.small=matrix(1:(8*4),ncol=8,nrow=4)
>> aggregate.nx.ny.forloop = function(data,nx=2,ny=2, FUN=mean,...) 
>+ {
>+   nlon=nrow(data)
>+   nlat=ncol(data)
>+   newdata=matrix(NA,nrow=nlon/nx,ncol=nlat/ny)
>+   dim(newdata)
>+   for(ilon in seq(1,nlon,nx)) {
>+     for(ilat in seq(1,nlat,ny)) {
>+       ilon_new=1+(ilon-1)/nx
>+       ilat_new=1+(ilat-1)/ny
>+       newdata[ilon_new,ilat_new] = FUN(data[ilon+0:1,ilat+0:1],...)
>+     }
>+   }
>+   newdata
>+ }
>> aggregate.nx.ny.forloop(tst.small)
>     [,1] [,2] [,3] [,4]
>[1,]  3.5 11.5 19.5 27.5
>[2,]  5.5 13.5 21.5 29.5
>> 
>> aggregate.nx.ny.interaction = function(data,nx=2,ny=2, FUN=mean,...) 
>+ {
>+   
>+   nlon=nrow(data)
>+   nlat=ncol(data)
>+   newdata=matrix(NA,nrow=nlon/nx,ncol=nlat/ny)
>+   newdata[] <- tapply( data, interaction( (row(data)+1) %/% 2,
>(col(data)+1) %/% 2 ), FUN, ...)
>+   newdata
>+ }
>> aggregate.nx.ny.interaction(tst.small)
>     [,1] [,2] [,3] [,4]
>[1,]  3.5 11.5 19.5 27.5
>[2,]  5.5 13.5 21.5 29.5
>> 
>> aggregate.nx.ny.expand.grid = function(data,nx=2,ny=2, FUN=mean,...) 
>+ {
>+   ilon <- seq(1,ncol(data),nx)
>+   ilat <- seq(1,nrow(data),ny)
>+   cells <- as.matrix(expand.grid(ilat, ilon))
>+   blocks <- apply(cells, 1, function(x)
>data[x[1]:(x[1]+1),x[2]:(x[2]+1)])
>+   block.means <- colMeans(blocks)
>+   matrix(block.means, nrow(data)/ny, ncol(data)/nx)
>+ }
>> aggregate.nx.ny.expand.grid(tst.small)
>     [,1] [,2] [,3] [,4]
>[1,]  3.5 11.5 19.5 27.5
>[2,]  5.5 13.5 21.5 29.5
>> 
>> aggregate.nx.ny.array.apply = function(data,nx=2,ny=2, FUN=mean,...)
>{
>+   a <- array(data, dim = c(ny, nrow( data ) %/% ny, ncol( data ) %/%
>nx))
>+   apply( a, c(2, 3), FUN, ... )
>+ }
>> aggregate.nx.ny.array.apply(tst.small)
>     [,1] [,2] [,3] [,4]
>[1,]  1.5  5.5  9.5 13.5
>[2,]  3.5  7.5 11.5 15.5
>
>
>
>> On 28 Jul 2016, at 00:26, David Winsemius <dwinsemius at comcast.net>
>wrote:
>> 
>> 
>>> On Jul 27, 2016, at 12:02 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>>> 
>>> An alternative (more compact, not necessarily faster, because apply
>is still a for loop inside):
>>> 
>>> f <- function( m, nx, ny ) {
>>> # redefine the dimensions of my
>>> a <- array( m
>>>            , dim = c( ny
>>>                   , nrow( m ) %/% ny
>>>                   , ncol( m ) %/% nx )
>>>           )
>>> # apply mean over dim 1
>>> apply( a, c( 2, 3 ), FUN=mean )
>>> }
>>> f( tst, nx, ny )
>> 
>> Here's an apparently loopless strategy, although I suspect the code
>for interaction (and maybe tapply as well?) uses a loop.
>> 
>> 
>> tst_2X2 <- matrix(NA, ,ncol=4,nrow=2)
>> 
>> tst_2x2[] <- tapply( tst, interaction( (row(tst)+1) %/% 2,
>(col(tst)+1) %/% 2 ), mean)
>> 
>> tst_2x2
>> 
>>     [,1] [,2] [,3] [,4]
>> [1,]  3.5 11.5 19.5 27.5
>> [2,]  5.5 13.5 21.5 29.5
>> 
>> -- 
>> David.
>> 
>> 
>>> 
>>> -- 
>>> Sent from my phone. Please excuse my brevity.
>>> 
>>> On July 27, 2016 9:08:32 AM PDT, David L Carlson <dcarlson at tamu.edu>
>wrote:
>>>> This should be faster. It uses apply() across the blocks. 
>>>> 
>>>>> ilon <- seq(1,8,nx)
>>>>> ilat <- seq(1,4,ny)
>>>>> cells <- as.matrix(expand.grid(ilat, ilon))
>>>>> blocks <- apply(cells, 1, function(x) tst[x[1]:(x[1]+1),
>>>> x[2]:(x[2]+1)])
>>>>> block.means <- colMeans(blocks)
>>>>> tst_2x2 <- matrix(block.means, 2, 4)
>>>>> tst_2x2
>>>>   [,1] [,2] [,3] [,4]
>>>> [1,]  3.5 11.5 19.5 27.5
>>>> [2,]  5.5 13.5 21.5 29.5
>>>> 
>>>> -------------------------------------
>>>> David L Carlson
>>>> Department of Anthropology
>>>> Texas A&M University
>>>> College Station, TX 77840-4352
>>>> 
>>>> 
>>>> 
>>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-poject.org] On Behalf Of
>Anthoni,
>>>> Peter (IMK)
>>>> Sent: Wednesday, July 27, 2016 6:14 AM
>>>> To: r-help at r-project.org
>>>> Subject: [R] Aggregate matrix in a 2 by 2 manor
>>>> 
>>>> Hi all,
>>>> 
>>>> I need to aggregate some matrix data (1440x720) to a lower
>dimension
>>>> (720x360) for lots of years and variables
>>>> 
>>>> I can do double for loop, but that will be slow. Anybody know a
>quicker
>>>> way?
>>>> 
>>>> here an example with a smaller matrix size:
>>>> 
>>>> tst=matrix(1:(8*4),ncol=8,nrow=4)
>>>> tst_2x2=matrix(NA,ncol=4,nrow=2)
>>>> nx=2
>>>> ny=2
>>>> for(ilon in seq(1,8,nx)) {
>>>> for (ilat in seq(1,4,ny)) {
>>>>  ilon_2x2=1+(ilon-1)/nx
>>>>  ilat_2x2=1+(ilat-1)/ny
>>>>  tst_2x2[ilat_2x2,ilon_2x2] = mean(tst[ilat+0:1,ilon+0:1])
>>>> }
>>>> }
>>>> 
>>>> tst
>>>> tst_2x2
>>>> 
>>>>> tst
>>>>   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
>>>> [1,]    1    5    9   13   17   21   25   29
>>>> [2,]    2    6   10   14   18   22   26   30
>>>> [3,]    3    7   11   15   19   23   27   31
>>>> [4,]    4    8   12   16   20   24   28   32
>>>> 
>>>>> tst_2x2
>>>>   [,1] [,2] [,3] [,4]
>>>> [1,]  3.5 11.5 19.5 27.5
>>>> [2,]  5.5 13.5 21.5 29.5
>>>> 
>>>> 
>>>> I though a cast to 3d-array might do the trick and apply over the
>new
>>>> dimension, but that does not work, since it casts the data along
>the
>>>> row.
>>>>> matrix(apply(array(tst,dim=c(nx,ny,8)),3,mean),nrow=nrow(tst)/ny)
>>>>   [,1] [,2] [,3] [,4]
>>>> [1,]  2.5 10.5 18.5 26.5
>>>> [2,]  6.5 14.5 22.5 30.5
>>>> 
>>>> 
>>>> cheers
>>>> Peter
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>>


From roslinaump at gmail.com  Sun Jul 31 04:53:40 2016
From: roslinaump at gmail.com (roslinazairimah zakaria)
Date: Sun, 31 Jul 2016 10:53:40 +0800
Subject: [R] lapply
Message-ID: <CANTvJZKLaDTqYWcA=x4MZ14hAqJ8R=OnGV_xL=+pe63omSYg5Q@mail.gmail.com>

Dear r-users,

I would like to use lapply for the following task:

## Kolmogorov-Smirnov
ks.test(stn_all[,1][stn_all[,1] > 0],stn_all_gen[,1][stn_all_gen[,1] > 0])
ks.test(stn_all[,2][stn_all[,2] > 0],stn_all_gen[,2][stn_all_gen[,2] > 0])
ks.test(stn_all[,3][stn_all[,3] > 0],stn_all_gen[,3][stn_all_gen[,3] > 0])
ks.test(stn_all[,4][stn_all[,4] > 0],stn_all_gen[,4][stn_all_gen[,4] > 0])
ks.test(stn_all[,5][stn_all[,5] > 0],stn_all_gen[,5][stn_all_gen[,5] > 0])
ks.test(stn_all[,6][stn_all[,6] > 0],stn_all_gen[,6][stn_all_gen[,6] > 0])

 I would like to conduct the Kolmogorov Smirnov goodness of fit tests.

Is it possible?


Thank you very much.
-- 
*Dr. Roslinazairimah Binti Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Deputy Dean (Academic & Student Affairs)
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From roslinaump at gmail.com  Sun Jul 31 05:17:31 2016
From: roslinaump at gmail.com (roslinazairimah zakaria)
Date: Sun, 31 Jul 2016 11:17:31 +0800
Subject: [R] Extract data
In-Reply-To: <CA+8X3fXV=JrLrJUq7WU0t5NVLE8VNr3KSQ9ee_c7x+T8DDVHJw@mail.gmail.com>
References: <CANTvJZJSWc2o9UhFJNbdyYoCSJwZ_7U6Ko9ekV=Kot8NvnLngw@mail.gmail.com>
	<CA+8X3fWOCe5ERKFVOhmOchjZd_aNN3-KW1bh7Pv28OqOq0kj=g@mail.gmail.com>
	<CANTvJZ+tgyOOXnDXyStWC8DjJovh5QdknW0AZc8Q+wMp3FadFQ@mail.gmail.com>
	<CANTvJZKgQyqgus3eurkS9uP=YOSq9udx4kM5XvyhRkvitwE+8g@mail.gmail.com>
	<CANTvJZKu9Ge6bT1SaXD4+=9Bmckg57Qwy_UMQ4qdaTNoX+Jtnw@mail.gmail.com>
	<CA+8X3fXV=JrLrJUq7WU0t5NVLE8VNr3KSQ9ee_c7x+T8DDVHJw@mail.gmail.com>
Message-ID: <CANTvJZ+1E+hQR86U+8fEhHUUA1Rs1=kWNpXckVGbMpORbxSYWw@mail.gmail.com>

Thank you so much Jim.

Here is the code.

sum_balok   <- as.vector(by(dt1$x,dt1$year,sum,na.rm=TRUE))
sum_gambang <- as.vector(by(dt2$x,dt2$year,sum,na.rm=TRUE))
sum_sgsoi   <- as.vector(by(dt3$x,dt3$year,sum,na.rm=TRUE))
sum_jpsphg  <- as.vector(by(dt4$x,dt4$year,sum,na.rm=TRUE))
sum_pbesar  <- as.vector(by(dt5$x,dt5$year,sum,na.rm=TRUE))
sum_pmanis  <- as.vector(by(dt6$x,dt6$year,sum,na.rm=TRUE))

sum_year <- cbind(sum_balok, sum_gambang, sum_sgsoi, sum_jpsphg,
sum_pbesar, sum_pmanis)
dimnames(sum_year) = list( 2009:2014,
c("Balok","Gambang","SgSoi","JPSPhg","PayaBesar","PulauManis")) # column
names
rbind(sum_year, colMeans(sum_year))

Best regards

On Fri, Jul 29, 2016 at 4:52 PM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Roslina,
> This may be what you want:
>
> sum_balok<-
>  as.vector(by(aggbalok_2009_2014$x,aggbalok_2009_2014$year,sum,na.rm=TRUE))
> cbind(year=2009:2014,sum_balok)
>
> I don't have the data for the other measures, but you could calculate
> the sums as you did below and then add them to the cbind arguments.
>
> Jim
>
>
> On Fri, Jul 29, 2016 at 5:51 PM, roslinazairimah zakaria
> <roslinaump at gmail.com> wrote:
> > I tried this:
> > dt1 <- aggbalok_mth[aggbalok_mth$year %in% 2009:2014,]
> > dt2 <- agggambang_mth[agggambang_mth$year %in% 2009:2014,]
> > dt3 <- aggsgsoi_mth[aggsgsoi_mth$year %in% 2009:2014,]
> > dt4 <- aggjpsphg_mth[aggjpsphg_mth$year %in% 2009:2014,]
> > dt5 <- aggpbesar_mth[aggpbesar_mth$year %in% 2009:2014,]
> > dt6 <- aggpmanis_mth[aggpmanis_mth$year %in% 2009:2014,]
> >
> > ## Yearly Sum
> > dt_year <- cbind(aggregate(dt1[,3], by=dt1[,c(2,2)],FUN=sum, na.rm=TRUE),
> >       aggregate(dt2[,3], by=dt2[,c(2,2)],FUN=sum, na.rm=TRUE),
> >       aggregate(dt3[,3], by=dt3[,c(2,2)],FUN=sum, na.rm=TRUE),
> >       aggregate(dt4[,3], by=dt4[,c(2,2)],FUN=sum, na.rm=TRUE),
> >       aggregate(dt5[,3], by=dt5[,c(2,2)],FUN=sum, na.rm=TRUE),
> >       aggregate(dt6[,3], by=dt6[,c(2,2)],FUN=sum, na.rm=TRUE))
> >
> > However the output is not good.
> > dput(dt_year)
> > structure(list(year = c(2009, 2010, 2011, 2012, 2013, 2014),
> >     year.1 = c(2009, 2010, 2011, 2012, 2013, 2014), x = c(3511.6,
> >     2456, 3575.7, 3029, 2849.7, 1697.7), year = c(2009, 2010,
> >     2011, 2012, 2013, 2014), year.1 = c(2009, 2010, 2011, 2012,
> >     2013, 2014), x = c(3040.3, 2209.1, 3210.3, 3403.8, 3449.3,
> >     2070.4), year = c(2009, 2010, 2011, 2012, 2013, 2014), year.1 =
> c(2009,
> >     2010, 2011, 2012, 2013, 2014), x = c(3657.1, 2059.2, 2862.4,
> >     3221.2, 3136.8, 3255.1), year = c(2009, 2010, 2011, 2012,
> >     2013, 2014), year.1 = c(2009, 2010, 2011, 2012, 2013, 2014
> >     ), x = c(2726.4, 2266.8, 2249.1, 3093.5, 1513, 3087.5), year =
> c(2009,
> >     2010, 2011, 2012, 2013, 2014), year.1 = c(2009, 2010, 2011,
> >     2012, 2013, 2014), x = c(3194.9, 1879.6, 1861.4, 3080, 1648.4,
> >     3012.3), year = c(2009, 2010, 2011, 2012, 2013, 2014), year.1 =
> c(2009,
> >     2010, 2011, 2012, 2013, 2014), x = c(2697.9, 1639.7, 2107.8,
> >     2479.7, 3078.8, 2288.7)), .Names = c("year", "year.1", "x",
> > "year", "year.1", "x", "year", "year.1", "x", "year", "year.1",
> > "x", "year", "year.1", "x", "year", "year.1", "x"), row.names = c(NA,
> > -6L), class = "data.frame")
> >
> > Thank you.
> >
> > On Fri, Jul 29, 2016 at 2:30 PM, roslinazairimah zakaria
> > <roslinaump at gmail.com> wrote:
> >>
> >> I have one more question, how do I get the sum for the years.  Thank
> you.
> >>
> >> On Fri, Jul 29, 2016 at 12:36 PM, roslinazairimah zakaria
> >> <roslinaump at gmail.com> wrote:
> >>>
> >>> Thank you very much Jim.  It works beautifully.
> >>>
> >>> Best regards,
> >>>
> >>>
> >>>
> >>> On Fri, Jul 29, 2016 at 11:56 AM, Jim Lemon <drjimlemon at gmail.com>
> wrote:
> >>>>
> >>>> Hi Roslina,
> >>>> Try this:
> >>>>
> >>>>  aggbalok_mth[aggbalok_mth$year %in% 2009:2014,]
> >>>>
> >>>> Jim
> >>>>
> >>>>
> >>>> On Fri, Jul 29, 2016 at 1:12 PM, roslinazairimah zakaria
> >>>> <roslinaump at gmail.com> wrote:
> >>>> > Dear r-users,
> >>>> >
> >>>> > I would like to extract year from 2009 to 2014 with the
> corresponding
> >>>> > month
> >>>> > and rain amount.
> >>>> >
> >>>> > I tried this:
> >>>> > aggbalok_mth[aggbalok_mth$year == 2009:2014, ]  but some of the data
> >>>> > is
> >>>> > missing.
> >>>> >
> >>>> >> dput(aggbalok_mth[aggbalok_mth$year == 2009:2014, ] )
> >>>> > structure(list(month = c(1, 7, 2, 8, 3, 9, 4, 10, 5, 11, 6, 12
> >>>> > ), year = c(2009, 2009, 2010, 2010, 2011, 2011, 2012, 2012, 2013,
> >>>> > 2013, 2014, 2014), x = c(424.6, 59.5, 6, 54.6, 387.9, 236.1,
> >>>> > 160.3, 162.5, 102.8, 139.5, 293.3, 39)), .Names = c("month",
> >>>> > "year", "x"), row.names = c(7L, 13L, 20L, 26L, 33L, 39L, 46L,
> >>>> > 52L, 59L, 65L, 72L, 78L), class = "data.frame")
> >>>> > Warning message:
> >>>> > In aggbalok_mth$year == 2009:2014 :
> >>>> >   longer object length is not a multiple of shorter object length
> >>>> >
> >>>> >
> >>>> > Here is the my data:
> >>>> >
> >>>> > structure(list(month = c(7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,
> >>>> > 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
> >>>> > 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8,
> >>>> > 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3,
> >>>> > 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
> >>>> > 11), year = c(2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009,
> >>>> > 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010,
> >>>> > 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010,
> >>>> > 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011,
> >>>> > 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012,
> >>>> > 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,
> >>>> > 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,
> >>>> > 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015,
> >>>> > 2015, 2015, 2015, 2015), x = c(0, 168.7, 203, 149.3, 299.9, 570.7,
> >>>> > 424.6, 52.6, 407.7, 210.3, 459.8, 249.2, 59.5, 310.4, 182.7,
> >>>> > 433.3, 161, 560.5, 197.5, 6, 68.9, 170.4, 117, 271.2, 133.5,
> >>>> > 54.6, 145.5, 122.5, 460.9, 708, 646.7, 58.8, 387.9, 42.9, 190.7,
> >>>> > 133.3, 131.7, 158.4, 236.1, 412.9, 462.5, 713.8, 437.1, 140.1,
> >>>> > 311.4, 160.3, 202.4, 58.8, 134.7, 120.4, 206.9, 162.5, 68.5,
> >>>> > 1025.9, 229.5, 331, 9, 51.4, 102.8, 162.9, 157.2, 32.6, 103.9,
> >>>> > 158.7, 139.5, 1371.2, 221.5, 6.1, 19.1, 11, 87.7, 293.3, 87.3,
> >>>> > 184, 69.5, 231, 448.2, 39, 19.3, 3.9, 53.8, 141.9, 325, 53.5,
> >>>> > 133.3, 321.1, 77.6, 156.5, 2.2)), .Names = c("month", "year",
> >>>> > "x"), row.names = c(NA, -89L), class = "data.frame")
> >>>> >
> >>>> > Thank you very much for any help given.
> >>>> > --
> >>>> > *Dr. Roslinazairimah Binti Zakaria*
> >>>> > *Tel: +609-5492370; Fax. No.+609-5492766*
> >>>> >
> >>>> > *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> >>>> > roslinaump at gmail.com <roslinaump at gmail.com>*
> >>>> > Deputy Dean (Academic & Student Affairs)
> >>>> > Faculty of Industrial Sciences & Technology
> >>>> > University Malaysia Pahang
> >>>> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
> >>>> >
> >>>> >         [[alternative HTML version deleted]]
> >>>> >
> >>>> > ______________________________________________
> >>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> > PLEASE do read the posting guide
> >>>> > http://www.R-project.org/posting-guide.html
> >>>> > and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> >>>
> >>>
> >>> --
> >>> Dr. Roslinazairimah Binti Zakaria
> >>> Tel: +609-5492370; Fax. No.+609-5492766
> >>> Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
> >>> Deputy Dean (Academic & Student Affairs)
> >>> Faculty of Industrial Sciences & Technology
> >>> University Malaysia Pahang
> >>> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
> >>
> >>
> >>
> >>
> >> --
> >> Dr. Roslinazairimah Binti Zakaria
> >> Tel: +609-5492370; Fax. No.+609-5492766
> >> Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
> >> Deputy Dean (Academic & Student Affairs)
> >> Faculty of Industrial Sciences & Technology
> >> University Malaysia Pahang
> >> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
> >
> >
> >
> >
> > --
> > Dr. Roslinazairimah Binti Zakaria
> > Tel: +609-5492370; Fax. No.+609-5492766
> > Email: roslinazairimah at ump.edu.my; roslinaump at gmail.com
> > Deputy Dean (Academic & Student Affairs)
> > Faculty of Industrial Sciences & Technology
> > University Malaysia Pahang
> > Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
>



-- 
*Dr. Roslinazairimah Binti Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Deputy Dean (Academic & Student Affairs)
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From marine.regis at hotmail.fr  Sun Jul 31 05:45:35 2016
From: marine.regis at hotmail.fr (Marine Regis)
Date: Sun, 31 Jul 2016 03:45:35 +0000
Subject: [R] Replace any values in a data frame based on another data frame
Message-ID: <AMSPR07MB470760784289AF6793EB6C9E2030@AMSPR07MB470.eurprd07.prod.outlook.com>

Hello,

I have two data frames with different sizes but with the same number of columns.

> df1 <- data.frame(col1 = c(1:6), col2 = c(rep("a", 3), rep("b", 3)), col3 = c(rep("AA", 2), rep("BB", 2), rep("CC", 2)), col4=c(1,8,6,9,7,6))

> df1

  col1 col2 col3 col4

1    1    a   AA    1

2    2    a   AA    8

3    3    a   BB    6

4    4    b   BB    9

5    5    b   CC    7

6    6    b   CC    6



> df2< - data.frame(col1 = c(1,3,5,6), col2 = c(rep("a", 2), rep("b", 2)), col3 = c(rep("AA", 1), rep("EE", 1), rep("FF", 1), rep("CC", 1)), col4=c(1,8,5,9))



> df2

  col1 col2 col3 col4

1    1    a   AA    1

2    3    a   EE    8

3    5    b   FF    5

4    6    b   CC    9





Based on col1 and col2, how can I replace any values in col3 and col4 of df1 which don't match with those of df2 by the values of df2



In this example, the result will be:



> df1

   col1 col2 col3 col4

1    1    a   AA    1

2    2    a   AA    8

3    3    a   EE    8

4    4    b   BB    9

5    5    b   FF    5

6    6    b   CC    9



Thanks a lot for your help.

Have a nice day

Marine

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sun Jul 31 08:13:04 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 30 Jul 2016 23:13:04 -0700 (PDT)
Subject: [R] Aggregate matrix in a 2 by 2 manor
In-Reply-To: <8CAC8B1A-89AC-449F-B85A-923F18606333@dcn.davis.ca.us>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
	<CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
	<E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>
	<f2247c74794a41ad8a70f85d2c85df92@exch-2p-mbx-t2.ads.tamu.edu>
	<ED550726-6ACD-487A-959A-2DB8706B76F4@dcn.davis.ca.us>
	<506368AE-3234-425B-97B5-DD3B13094237@comcast.net>
	<A5E0D447-2136-4A69-AC3A-E2514BEFE29F@kit.edu>
	<8CAC8B1A-89AC-449F-B85A-923F18606333@dcn.davis.ca.us>
Message-ID: <alpine.BSF.2.00.1607302306560.22689@pedal.dcn.davis.ca.us>

If you don't need all that FUN flexibility, you can get this done way 
faster with the aperm and colMeans functions:

tst <- matrix( seq.int( 1440 * 360 )
              , ncol = 1440
              , nrow = 360
              )
tst.small <- matrix( seq.int( 8 * 4 )
                    , ncol = 8
                    , nrow = 4
                    )
aggregate.nx.ny.expand.grid <- function( dta, nx = 2, ny = 2, FUN = mean, ... )
{
   ilon <- seq( 1, ncol( dta ), nx )
   ilat <- seq( 1, nrow( dta ), ny )
   cells <- as.matrix( expand.grid( ilat, ilon ) )
   blocks <- apply( cells
                  , 1
                  , function( x ) dta[ x[ 1 ]:( x[ 1 ] + 1 ), x[ 2 ]:( x[ 2 ] + 1 ) ] )
   block.means <- colMeans( blocks )
   matrix( block.means
         , nrow( dta ) / ny
         , ncol( dta ) / nx
         )
}

aggregate.nx.ny.array.apply <- function( dta, nx = 2, ny = 2, FUN = mean, ... ) {
   a <- array( dta
             , dim = c( ny
                      , nrow( dta ) %/% ny
                      , nx
                      , ncol( dta ) %/% nx
                      )
             )
   apply( a, c( 2, 4 ), FUN, ... )
}

aggregate.nx.ny.array.aperm.mean <- function( dta, nx = 2, ny = 2, ... ) {
   # number of rows in result
   nnr <- nrow( dta ) %/% ny
   # number of columns in result
   nnc <- ncol( dta ) %/% nx
   # number of values to take mean of
   nxny <- nx * ny
   # describe existing layout of values in dta as 4-d array
   a1 <- array( dta, dim = c( ny, nnr, nx, nnc ) )
   # swap data in dimensions 2 and 3
   a2 <- aperm( a1, c( 1, 3, 2, 4 ) )
   # treat first two dimensions as column vectors, remaining as columns
   a3 <- matrix( a2, nrow = nxny )
   # fast calculation of column means
   v <- colMeans( a3, ... )
   # reframe result vector as a matrix
   matrix( v, ncol = nnc )
}

aggregate.nx.ny.array.aperm.apply <- function( dta, nx = 2, ny = 2, FUN = mean, ... ) {
   # number of rows in result
   nnr <- nrow( dta ) %/% ny
   # number of columns in result
   nnc <- ncol( dta ) %/% nx
   # number of values to apply FUN to
   nxny <- nx * ny
   # describe existing layout of values in dta as 4-d array
   a1 <- array( dta, dim = c( ny, nnr, nx, nnc ) )
   # swap data in dimensions 2 and 3
   a2 <- aperm( a1, c( 1, 3, 2, 4 ) )
   # treat first two dimensions as column vectors, remaining as columns
   a3 <- matrix( a2, nrow = nxny )
   # apply FUN to column vectors
   v <- apply( a3, 2, FUN = FUN, ... )
   matrix( v, ncol = nnc )
}
test1 <- aggregate.nx.ny.expand.grid( tst )
test2 <- aggregate.nx.ny.array.apply( tst )
test3 <- aggregate.nx.ny.array.aperm.mean( tst )
test4 <- aggregate.nx.ny.array.aperm.apply( tst )
library(microbenchmark)
microbenchmark(
   aggregate.nx.ny.expand.grid( tst, 2, 2, mean, na.rm = TRUE )
, aggregate.nx.ny.array.apply( tst, 2, 2, mean, na.rm = TRUE )
, aggregate.nx.ny.array.aperm.mean( tst, 2, 2, na.rm = TRUE )
, aggregate.nx.ny.array.aperm.apply( tst, 2, 2, mean, na.rm = TRUE )
)
#Unit: milliseconds
#                                                             expr        min
#       aggregate.nx.ny.expand.grid(tst, 2, 2, mean, na.rm = TRUE) 628.528322
#       aggregate.nx.ny.array.apply(tst, 2, 2, mean, na.rm = TRUE) 846.883314
#        aggregate.nx.ny.array.aperm.mean(tst, 2, 2, na.rm = TRUE)   8.904369
# aggregate.nx.ny.array.aperm.apply(tst, 2, 2, mean, na.rm = TRUE) 619.691851
#         lq       mean     median        uq      max neval cld
# 675.470967  916.39630  778.54090  873.9754 2452.695   100  b
# 920.831966 1126.94691 1000.33830 1094.9233 3412.639   100   c
#   9.191747   21.98528   10.30099   15.9169  158.687   100 a
# 733.246331  936.73359  757.58383  844.2016 2824.557   100  b


On Sat, 30 Jul 2016, Jeff Newmiller wrote:

> For the record, the array.apply code can be fixed as below, but then it is slower than the expand.grid version.
>
> aggregate.nx.ny.array.apply <- function(dta,nx=2,ny=2, FUN=mean,...)
> {
>   a <- array(dta, dim = c(ny, nrow( dta ) %/% ny, nx, ncol( dta ) %/% nx))
>  apply( a, c(2, 4), FUN, ... )
> }
>
> -- 
> Sent from my phone. Please excuse my brevity.
>
> On July 30, 2016 11:06:16 AM PDT, "Anthoni, Peter (IMK)" <peter.anthoni at kit.edu> wrote:
>> Hi all,
>>
>> thanks for the suggestions, I did some timing tests, see below.
>> Unfortunately the aggregate.nx.ny.array.apply, does not produce the
>> expected result.
>> So the fastest seems to be the aggregate.nx.ny.expand.grid, though the
>> double for loop is not that much slower.
>>
>> many thanks
>> Peter
>>
>>> tst=matrix(1:(1440*360),ncol=1440,nrow=360)
>>> system.time( {for(i in 1:10)
>> tst_2x2=aggregate.nx.ny.forloop(tst,2,2,mean,na.rm=T)})
>>   user  system elapsed
>> 11.227   0.073  11.371
>>> system.time( {for(i in 1:10)
>> tst_2x2=aggregate.nx.ny.interaction(tst,2,2,mean,na.rm=T)})
>>   user  system elapsed
>> 26.354   0.475  26.880
>>> system.time( {for(i in 1:10)
>> tst_2x2=aggregate.nx.ny.expand.grid(tst,2,2,mean,na.rm=T)})
>>   user  system elapsed
>>  9.683   0.055   9.763
>>> system.time( {for(i in 1:10)
>> tst_2x2=aggregate.nx.ny.array.apply(tst,2,2,mean,na.rm=T)})
>>   user  system elapsed
>>  7.693   0.055   7.800
>>
>>> tst.small=matrix(1:(8*4),ncol=8,nrow=4)
>>> aggregate.nx.ny.forloop = function(data,nx=2,ny=2, FUN=mean,...)
>> + {
>> +   nlon=nrow(data)
>> +   nlat=ncol(data)
>> +   newdata=matrix(NA,nrow=nlon/nx,ncol=nlat/ny)
>> +   dim(newdata)
>> +   for(ilon in seq(1,nlon,nx)) {
>> +     for(ilat in seq(1,nlat,ny)) {
>> +       ilon_new=1+(ilon-1)/nx
>> +       ilat_new=1+(ilat-1)/ny
>> +       newdata[ilon_new,ilat_new] = FUN(data[ilon+0:1,ilat+0:1],...)
>> +     }
>> +   }
>> +   newdata
>> + }
>>> aggregate.nx.ny.forloop(tst.small)
>>     [,1] [,2] [,3] [,4]
>> [1,]  3.5 11.5 19.5 27.5
>> [2,]  5.5 13.5 21.5 29.5
>>>
>>> aggregate.nx.ny.interaction = function(data,nx=2,ny=2, FUN=mean,...)
>> + {
>> +
>> +   nlon=nrow(data)
>> +   nlat=ncol(data)
>> +   newdata=matrix(NA,nrow=nlon/nx,ncol=nlat/ny)
>> +   newdata[] <- tapply( data, interaction( (row(data)+1) %/% 2,
>> (col(data)+1) %/% 2 ), FUN, ...)
>> +   newdata
>> + }
>>> aggregate.nx.ny.interaction(tst.small)
>>     [,1] [,2] [,3] [,4]
>> [1,]  3.5 11.5 19.5 27.5
>> [2,]  5.5 13.5 21.5 29.5
>>>
>>> aggregate.nx.ny.expand.grid = function(data,nx=2,ny=2, FUN=mean,...)
>> + {
>> +   ilon <- seq(1,ncol(data),nx)
>> +   ilat <- seq(1,nrow(data),ny)
>> +   cells <- as.matrix(expand.grid(ilat, ilon))
>> +   blocks <- apply(cells, 1, function(x)
>> data[x[1]:(x[1]+1),x[2]:(x[2]+1)])
>> +   block.means <- colMeans(blocks)
>> +   matrix(block.means, nrow(data)/ny, ncol(data)/nx)
>> + }
>>> aggregate.nx.ny.expand.grid(tst.small)
>>     [,1] [,2] [,3] [,4]
>> [1,]  3.5 11.5 19.5 27.5
>> [2,]  5.5 13.5 21.5 29.5
>>>
>>> aggregate.nx.ny.array.apply = function(data,nx=2,ny=2, FUN=mean,...)
>> {
>> +   a <- array(data, dim = c(ny, nrow( data ) %/% ny, ncol( data ) %/%
>> nx))
>> +   apply( a, c(2, 3), FUN, ... )
>> + }
>>> aggregate.nx.ny.array.apply(tst.small)
>>     [,1] [,2] [,3] [,4]
>> [1,]  1.5  5.5  9.5 13.5
>> [2,]  3.5  7.5 11.5 15.5
>>
>>
>>
>>> On 28 Jul 2016, at 00:26, David Winsemius <dwinsemius at comcast.net>
>> wrote:
>>>
>>>
>>>> On Jul 27, 2016, at 12:02 PM, Jeff Newmiller
>> <jdnewmil at dcn.davis.ca.us> wrote:
>>>>
>>>> An alternative (more compact, not necessarily faster, because apply
>> is still a for loop inside):
>>>>
>>>> f <- function( m, nx, ny ) {
>>>> # redefine the dimensions of my
>>>> a <- array( m
>>>>            , dim = c( ny
>>>>                   , nrow( m ) %/% ny
>>>>                   , ncol( m ) %/% nx )
>>>>           )
>>>> # apply mean over dim 1
>>>> apply( a, c( 2, 3 ), FUN=mean )
>>>> }
>>>> f( tst, nx, ny )
>>>
>>> Here's an apparently loopless strategy, although I suspect the code
>> for interaction (and maybe tapply as well?) uses a loop.
>>>
>>>
>>> tst_2X2 <- matrix(NA, ,ncol=4,nrow=2)
>>>
>>> tst_2x2[] <- tapply( tst, interaction( (row(tst)+1) %/% 2,
>> (col(tst)+1) %/% 2 ), mean)
>>>
>>> tst_2x2
>>>
>>>     [,1] [,2] [,3] [,4]
>>> [1,]  3.5 11.5 19.5 27.5
>>> [2,]  5.5 13.5 21.5 29.5
>>>
>>> --
>>> David.
>>>
>>>
>>>>
>>>> --
>>>> Sent from my phone. Please excuse my brevity.
>>>>
>>>> On July 27, 2016 9:08:32 AM PDT, David L Carlson <dcarlson at tamu.edu>
>> wrote:
>>>>> This should be faster. It uses apply() across the blocks.
>>>>>
>>>>>> ilon <- seq(1,8,nx)
>>>>>> ilat <- seq(1,4,ny)
>>>>>> cells <- as.matrix(expand.grid(ilat, ilon))
>>>>>> blocks <- apply(cells, 1, function(x) tst[x[1]:(x[1]+1),
>>>>> x[2]:(x[2]+1)])
>>>>>> block.means <- colMeans(blocks)
>>>>>> tst_2x2 <- matrix(block.means, 2, 4)
>>>>>> tst_2x2
>>>>>   [,1] [,2] [,3] [,4]
>>>>> [1,]  3.5 11.5 19.5 27.5
>>>>> [2,]  5.5 13.5 21.5 29.5
>>>>>
>>>>> -------------------------------------
>>>>> David L Carlson
>>>>> Department of Anthropology
>>>>> Texas A&M University
>>>>> College Station, TX 77840-4352
>>>>>
>>>>>
>>>>>
>>>>> -----Original Message-----
>>>>> From: R-help [mailto:r-help-bounces at r-poject.org] On Behalf Of
>> Anthoni,
>>>>> Peter (IMK)
>>>>> Sent: Wednesday, July 27, 2016 6:14 AM
>>>>> To: r-help at r-project.org
>>>>> Subject: [R] Aggregate matrix in a 2 by 2 manor
>>>>>
>>>>> Hi all,
>>>>>
>>>>> I need to aggregate some matrix data (1440x720) to a lower
>> dimension
>>>>> (720x360) for lots of years and variables
>>>>>
>>>>> I can do double for loop, but that will be slow. Anybody know a
>> quicker
>>>>> way?
>>>>>
>>>>> here an example with a smaller matrix size:
>>>>>
>>>>> tst=matrix(1:(8*4),ncol=8,nrow=4)
>>>>> tst_2x2=matrix(NA,ncol=4,nrow=2)
>>>>> nx=2
>>>>> ny=2
>>>>> for(ilon in seq(1,8,nx)) {
>>>>> for (ilat in seq(1,4,ny)) {
>>>>>  ilon_2x2=1+(ilon-1)/nx
>>>>>  ilat_2x2=1+(ilat-1)/ny
>>>>>  tst_2x2[ilat_2x2,ilon_2x2] = mean(tst[ilat+0:1,ilon+0:1])
>>>>> }
>>>>> }
>>>>>
>>>>> tst
>>>>> tst_2x2
>>>>>
>>>>>> tst
>>>>>   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
>>>>> [1,]    1    5    9   13   17   21   25   29
>>>>> [2,]    2    6   10   14   18   22   26   30
>>>>> [3,]    3    7   11   15   19   23   27   31
>>>>> [4,]    4    8   12   16   20   24   28   32
>>>>>
>>>>>> tst_2x2
>>>>>   [,1] [,2] [,3] [,4]
>>>>> [1,]  3.5 11.5 19.5 27.5
>>>>> [2,]  5.5 13.5 21.5 29.5
>>>>>
>>>>>
>>>>> I though a cast to 3d-array might do the trick and apply over the
>> new
>>>>> dimension, but that does not work, since it casts the data along
>> the
>>>>> row.
>>>>>> matrix(apply(array(tst,dim=c(nx,ny,8)),3,mean),nrow=nrow(tst)/ny)
>>>>>   [,1] [,2] [,3] [,4]
>>>>> [1,]  2.5 10.5 18.5 26.5
>>>>> [2,]  6.5 14.5 22.5 30.5
>>>>>
>>>>>
>>>>> cheers
>>>>> Peter
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From bgunter.4567 at gmail.com  Sun Jul 31 08:15:37 2016
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 30 Jul 2016 23:15:37 -0700
Subject: [R] Replace any values in a data frame based on another data
	frame
In-Reply-To: <AMSPR07MB470760784289AF6793EB6C9E2030@AMSPR07MB470.eurprd07.prod.outlook.com>
References: <AMSPR07MB470760784289AF6793EB6C9E2030@AMSPR07MB470.eurprd07.prod.outlook.com>
Message-ID: <CAGxFJbTHAp1__O01-ceznrgDMubDMG9EVFb-xCY4SxPVAQUk7g@mail.gmail.com>

Marine:

Thanks for the reproducible example. I would not have fooled with this
otherwise!

1. First note that your specification that you wish to replace only
those values in col3 and col4 of df1 that don't match with those of
df2 is irrelevant: if you replace those that do match you don't change
them. This means that you want to replace the rows of df1 for which
the first 2 columns of df1 match those of df2 with the corresponding
rows of df2.

2. You need to understand and deal with factors to do this. By
default, non-numeric columns will be read into a data frame as
factors, which is what happens in your example. You need to first
convert all such columns to character to make things work properly (or
at least, I had to doing it as shown below).

Also note that there was a typo in your code: "df2< -"  should be
"df2<-" (you inserted an extra space).

Anyway, here is one way to go about it, but I suggest that you wait
for other solutions before accepting it, as I would not be surprised
if there were better ways. However, this should be reasonably fast, as
everything is vectorized.

> df1 <- data.frame(col1 = c(1:6), col2 = c(rep("a", 3), rep("b", 3)), col3 = c(rep("AA", 2), rep("BB", 2), rep("CC", 2)), col4=c(1,8,6,9,7,6))
>
> df2<- data.frame(col1 = c(1,3,5,6), col2 = c(rep("a", 2), rep("b", 2)), col3 = c(rep("AA", 1), rep("EE", 1), rep("FF", 1), rep("CC", 1)), col4=c(1,8,5,9))
>
> ## convert factors to character vectors
> df1[,3] <- as.character(df1[,3])
> df2[,3] <- as.character(df2[,3])
>
> ## concatenate first 2 columns of both df's to identify rows for matching
> first <- with(df1,paste0(col1,col2))
> second <- with(df2,paste0(col1,col2))
>
> ## now find the rows of the df1 that match those of df2
> wh <-match(first,second)
>
> ## Use indexing to substitute
> df1[!is.na(wh),] <- df2[na.omit(wh),]
>
> df1
  col1 col2 col3 col4
1    1    a   AA    1
2    2    a   AA    8
3    3    a   EE    8
4    4    b   BB    9
5    5    b   FF    5
6    6    b   CC    9

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Jul 30, 2016 at 8:45 PM, Marine Regis <marine.regis at hotmail.fr> wrote:
> Hello,
>
> I have two data frames with different sizes but with the same number of columns.
>
>> df1 <- data.frame(col1 = c(1:6), col2 = c(rep("a", 3), rep("b", 3)), col3 = c(rep("AA", 2), rep("BB", 2), rep("CC", 2)), col4=c(1,8,6,9,7,6))
>
>> df1
>
>   col1 col2 col3 col4
>
> 1    1    a   AA    1
>
> 2    2    a   AA    8
>
> 3    3    a   BB    6
>
> 4    4    b   BB    9
>
> 5    5    b   CC    7
>
> 6    6    b   CC    6
>
>
>
>> df2< - data.frame(col1 = c(1,3,5,6), col2 = c(rep("a", 2), rep("b", 2)), col3 = c(rep("AA", 1), rep("EE", 1), rep("FF", 1), rep("CC", 1)), col4=c(1,8,5,9))
>
>
>
>> df2
>
>   col1 col2 col3 col4
>
> 1    1    a   AA    1
>
> 2    3    a   EE    8
>
> 3    5    b   FF    5
>
> 4    6    b   CC    9
>
>
>
>
>
> Based on col1 and col2, how can I replace any values in col3 and col4 of df1 which don't match with those of df2 by the values of df2
>
>
>
> In this example, the result will be:
>
>
>
>> df1
>
>    col1 col2 col3 col4
>
> 1    1    a   AA    1
>
> 2    2    a   AA    8
>
> 3    3    a   EE    8
>
> 4    4    b   BB    9
>
> 5    5    b   FF    5
>
> 6    6    b   CC    9
>
>
>
> Thanks a lot for your help.
>
> Have a nice day
>
> Marine
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sun Jul 31 08:38:15 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 30 Jul 2016 23:38:15 -0700 (PDT)
Subject: [R] Replace any values in a data frame based on another data
 frame
In-Reply-To: <AMSPR07MB470760784289AF6793EB6C9E2030@AMSPR07MB470.eurprd07.prod.outlook.com>
References: <AMSPR07MB470760784289AF6793EB6C9E2030@AMSPR07MB470.eurprd07.prod.outlook.com>
Message-ID: <alpine.BSF.2.00.1607302333400.22689@pedal.dcn.davis.ca.us>

Your use of HTML email corrupted your example slightly, but I was able to 
fix it. Please follow the Posting Guide and set your emails to Plain Text 
mode when posting to this mailing list in the future.

Here is one way:

# you have to be careful about mucking with factors
# convert columns to factors after you have finished
# changing values in them
# start by not creating factors in the first place
df1 <- data.frame( col1 = 1:6
                  , col2 = c( rep( "a", 3 )
                            , rep( "b", 3 )
                            )
                  , col3 = c( rep( "AA", 2 )
                            , rep( "BB", 2 )
                            , rep( "CC", 2 )
                            )
                  , col4 = c( 1, 8, 6, 9, 7, 6 )
                  , stringsAsFactors = FALSE
                  )

df2 <- data.frame( col1 = c( 1, 3, 5, 6 )
                  , col2 = c( rep( "a", 2 )
                            , rep( "b", 2 )
                            )
                  , col3 = c( rep( "AA", 1 )
                            , rep( "EE", 1 )
                            , rep( "FF", 1 )
                            , rep( "CC", 1 )
                            )
                  , col4 = c( 1, 8, 5, 9 )
                  , stringsAsFactors = FALSE
                  )

df3 <- merge( df1, df2, by = c( "col1", "col2" ), all = TRUE )
idx <- !is.na( df3$col3.y )
df3$col3.x[ idx ] <- df3$col3.y[ idx ]
df3$col4.x[ idx ] <- df3$col4.y[ idx ]
df3$col3.y <- df3$col4.y <- NULL
names( df3 )[ 3:4 ] <- c( "col3", "col4" )
df3

On Sun, 31 Jul 2016, Marine Regis wrote:

> Hello,
>
> I have two data frames with different sizes but with the same number of columns.
>
>> df1 <- data.frame(col1 = c(1:6), col2 = c(rep("a", 3), rep("b", 3)), col3 = c(rep("AA", 2), rep("BB", 2), rep("CC", 2)), col4=c(1,8,6,9,7,6))
>
>> df1
>
>  col1 col2 col3 col4
>
> 1    1    a   AA    1
>
> 2    2    a   AA    8
>
> 3    3    a   BB    6
>
> 4    4    b   BB    9
>
> 5    5    b   CC    7
>
> 6    6    b   CC    6
>
>
>
>> df2< - data.frame(col1 = c(1,3,5,6), col2 = c(rep("a", 2), rep("b", 2)), col3 = c(rep("AA", 1), rep("EE", 1), rep("FF", 1), rep("CC", 1)), col4=c(1,8,5,9))
>
>
>
>> df2
>
>  col1 col2 col3 col4
>
> 1    1    a   AA    1
>
> 2    3    a   EE    8
>
> 3    5    b   FF    5
>
> 4    6    b   CC    9
>
>
>
>
>
> Based on col1 and col2, how can I replace any values in col3 and col4 of df1 which don't match with those of df2 by the values of df2
>
>
>
> In this example, the result will be:
>
>
>
>> df1
>
>   col1 col2 col3 col4
>
> 1    1    a   AA    1
>
> 2    2    a   AA    8
>
> 3    3    a   EE    8
>
> 4    4    b   BB    9
>
> 5    5    b   FF    5
>
> 6    6    b   CC    9
>
>
>
> Thanks a lot for your help.
>
> Have a nice day
>
> Marine
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From peter.anthoni at kit.edu  Sun Jul 31 09:31:40 2016
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Sun, 31 Jul 2016 07:31:40 +0000
Subject: [R] Aggregate matrix in a 2 by 2 manor
In-Reply-To: <alpine.BSF.2.00.1607302306560.22689@pedal.dcn.davis.ca.us>
References: <CAMfArMoO09nhV8DzrfngKC1n2pg+GSi0j8BE8R49bUshPqpGWg@mail.gmail.com>
	<CAF8bMcaxV1H91zD1M=xqR59g+i90AiHMs-cZJr8qH7SLWM2c_g@mail.gmail.com>
	<E7BE67A1-68D3-4CD8-BC7F-F53CE5239207@kit.edu>
	<f2247c74794a41ad8a70f85d2c85df92@exch-2p-mbx-t2.ads.tamu.edu>
	<ED550726-6ACD-487A-959A-2DB8706B76F4@dcn.davis.ca.us>
	<506368AE-3234-425B-97B5-DD3B13094237@comcast.net>
	<A5E0D447-2136-4A69-AC3A-E2514BEFE29F@kit.edu>
	<8CAC8B1A-89AC-449F-B85A-923F18606333@dcn.davis.ca.us>
	<alpine.BSF.2.00.1607302306560.22689@pedal.dcn.davis.ca.us>
Message-ID: <C6B9B623-7D5B-42C7-BB3C-B480070B0874@kit.edu>

Hi Jeff,

many thanks, that one is the Speedy Gonzalles out of all. Can also do some FUN stuff.

aggregate.nx.ny.array.aperm <- function( dta, nx = 2, ny = 2, FUN=colMeans, ... ) {
 # number of rows in result
 nnr <- nrow( dta ) %/% ny
 # number of columns in result
 nnc <- ncol( dta ) %/% nx
 # number of values to take mean of
 nxny <- nx * ny
 # describe existing layout of values in dta as 4-d array
 a1 <- array( dta, dim = c( ny, nnr, nx, nnc ) )
 # swap data in dimensions 2 and 3
 a2 <- aperm( a1, c( 1, 3, 2, 4 ) )
 # treat first two dimensions as column vectors, remaining as columns
 a3 <- matrix( a2, nrow = nxny )
 # fast calculation of column means
 v <- FUN( a3, ... )
 # reframe result vector as a matrix
 matrix( v, ncol = nnc )
}


> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.forloop(tst,2,2,mean,na.rm=T)})
   user  system elapsed 
 14.003   0.271  14.663 
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.interaction(tst,2,2,mean,na.rm=T)})
   user  system elapsed 
 32.686   1.175  35.012 
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.expand.grid(tst,2,2,mean,na.rm=T)})
   user  system elapsed 
  9.590   0.197   9.951 
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.array.apply(tst,2,2,mean,na.rm=T)})
   user  system elapsed 
  8.391   0.174   8.737 
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.array.aperm(tst,2,2,colMeans,na.rm=T)})
   user  system elapsed 
  0.195   0.019   0.216 
> system.time( {for(i in 1:10) tst_2x2=aggregate.nx.ny.array.aperm(tst,2,2,colSums,na.rm=T)})
   user  system elapsed 
  0.169   0.017   0.188 


> aggregate.nx.ny.array.aperm(tst.small,FUN=colMeans)
     [,1] [,2] [,3] [,4]
[1,]  3.5 11.5 19.5 27.5
[2,]  5.5 13.5 21.5 29.5
> aggregate.nx.ny.array.aperm(tst.small,FUN=colSums)
     [,1] [,2] [,3] [,4]
[1,]   14   46   78  110
[2,]   22   54   86  118
> aggregate.nx.ny.forloop(tst.small,FUN=sum)
     [,1] [,2] [,3] [,4]
[1,]   14   46   78  110
[2,]   22   54   86  118

cheers
Peter



> On 31 Jul 2016, at 08:13, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> If you don't need all that FUN flexibility, you can get this done way faster with the aperm and colMeans functions:
> 
> tst <- matrix( seq.int( 1440 * 360 )
>             , ncol = 1440
>             , nrow = 360
>             )
> tst.small <- matrix( seq.int( 8 * 4 )
>                   , ncol = 8
>                   , nrow = 4
>                   )
> aggregate.nx.ny.expand.grid <- function( dta, nx = 2, ny = 2, FUN = mean, ... )
> {
>  ilon <- seq( 1, ncol( dta ), nx )
>  ilat <- seq( 1, nrow( dta ), ny )
>  cells <- as.matrix( expand.grid( ilat, ilon ) )
>  blocks <- apply( cells
>                 , 1
>                 , function( x ) dta[ x[ 1 ]:( x[ 1 ] + 1 ), x[ 2 ]:( x[ 2 ] + 1 ) ] )
>  block.means <- colMeans( blocks )
>  matrix( block.means
>        , nrow( dta ) / ny
>        , ncol( dta ) / nx
>        )
> }
> 
> aggregate.nx.ny.array.apply <- function( dta, nx = 2, ny = 2, FUN = mean, ... ) {
>  a <- array( dta
>            , dim = c( ny
>                     , nrow( dta ) %/% ny
>                     , nx
>                     , ncol( dta ) %/% nx
>                     )
>            )
>  apply( a, c( 2, 4 ), FUN, ... )
> }
> 
> aggregate.nx.ny.array.aperm.mean <- function( dta, nx = 2, ny = 2, ... ) {
>  # number of rows in result
>  nnr <- nrow( dta ) %/% ny
>  # number of columns in result
>  nnc <- ncol( dta ) %/% nx
>  # number of values to take mean of
>  nxny <- nx * ny
>  # describe existing layout of values in dta as 4-d array
>  a1 <- array( dta, dim = c( ny, nnr, nx, nnc ) )
>  # swap data in dimensions 2 and 3
>  a2 <- aperm( a1, c( 1, 3, 2, 4 ) )
>  # treat first two dimensions as column vectors, remaining as columns
>  a3 <- matrix( a2, nrow = nxny )
>  # fast calculation of column means
>  v <- colMeans( a3, ... )
>  # reframe result vector as a matrix
>  matrix( v, ncol = nnc )
> }
> 
> aggregate.nx.ny.array.aperm.apply <- function( dta, nx = 2, ny = 2, FUN = mean, ... ) {
>  # number of rows in result
>  nnr <- nrow( dta ) %/% ny
>  # number of columns in result
>  nnc <- ncol( dta ) %/% nx
>  # number of values to apply FUN to
>  nxny <- nx * ny
>  # describe existing layout of values in dta as 4-d array
>  a1 <- array( dta, dim = c( ny, nnr, nx, nnc ) )
>  # swap data in dimensions 2 and 3
>  a2 <- aperm( a1, c( 1, 3, 2, 4 ) )
>  # treat first two dimensions as column vectors, remaining as columns
>  a3 <- matrix( a2, nrow = nxny )
>  # apply FUN to column vectors
>  v <- apply( a3, 2, FUN = FUN, ... )
>  matrix( v, ncol = nnc )
> }
> test1 <- aggregate.nx.ny.expand.grid( tst )
> test2 <- aggregate.nx.ny.array.apply( tst )
> test3 <- aggregate.nx.ny.array.aperm.mean( tst )
> test4 <- aggregate.nx.ny.array.aperm.apply( tst )
> library(microbenchmark)
> microbenchmark(
>  aggregate.nx.ny.expand.grid( tst, 2, 2, mean, na.rm = TRUE )
> , aggregate.nx.ny.array.apply( tst, 2, 2, mean, na.rm = TRUE )
> , aggregate.nx.ny.array.aperm.mean( tst, 2, 2, na.rm = TRUE )
> , aggregate.nx.ny.array.aperm.apply( tst, 2, 2, mean, na.rm = TRUE )
> )
> #Unit: milliseconds
> #                                                             expr        min
> #       aggregate.nx.ny.expand.grid(tst, 2, 2, mean, na.rm = TRUE) 628.528322
> #       aggregate.nx.ny.array.apply(tst, 2, 2, mean, na.rm = TRUE) 846.883314
> #        aggregate.nx.ny.array.aperm.mean(tst, 2, 2, na.rm = TRUE)   8.904369
> # aggregate.nx.ny.array.aperm.apply(tst, 2, 2, mean, na.rm = TRUE) 619.691851
> #         lq       mean     median        uq      max neval cld
> # 675.470967  916.39630  778.54090  873.9754 2452.695   100  b
> # 920.831966 1126.94691 1000.33830 1094.9233 3412.639   100   c
> #   9.191747   21.98528   10.30099   15.9169  158.687   100 a
> # 733.246331  936.73359  757.58383  844.2016 2824.557   100  b
> 
> 
> On Sat, 30 Jul 2016, Jeff Newmiller wrote:
> 
>> For the record, the array.apply code can be fixed as below, but then it is slower than the expand.grid version.
>> 
>> aggregate.nx.ny.array.apply <- function(dta,nx=2,ny=2, FUN=mean,...)
>> {
>>  a <- array(dta, dim = c(ny, nrow( dta ) %/% ny, nx, ncol( dta ) %/% nx))
>> apply( a, c(2, 4), FUN, ... )
>> }
>> 
>> -- 
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 30, 2016 11:06:16 AM PDT, "Anthoni, Peter (IMK)" <peter.anthoni at kit.edu> wrote:
>>> Hi all,
>>> 
>>> thanks for the suggestions, I did some timing tests, see below.
>>> Unfortunately the aggregate.nx.ny.array.apply, does not produce the
>>> expected result.
>>> So the fastest seems to be the aggregate.nx.ny.expand.grid, though the
>>> double for loop is not that much slower.
>>> 
>>> many thanks
>>> Peter
>>> 
>>>> tst=matrix(1:(1440*360),ncol=1440,nrow=360)
>>>> system.time( {for(i in 1:10)
>>> tst_2x2=aggregate.nx.ny.forloop(tst,2,2,mean,na.rm=T)})
>>>  user  system elapsed
>>> 11.227   0.073  11.371
>>>> system.time( {for(i in 1:10)
>>> tst_2x2=aggregate.nx.ny.interaction(tst,2,2,mean,na.rm=T)})
>>>  user  system elapsed
>>> 26.354   0.475  26.880
>>>> system.time( {for(i in 1:10)
>>> tst_2x2=aggregate.nx.ny.expand.grid(tst,2,2,mean,na.rm=T)})
>>>  user  system elapsed
>>> 9.683   0.055   9.763
>>>> system.time( {for(i in 1:10)
>>> tst_2x2=aggregate.nx.ny.array.apply(tst,2,2,mean,na.rm=T)})
>>>  user  system elapsed
>>> 7.693   0.055   7.800
>>> 
>>>> tst.small=matrix(1:(8*4),ncol=8,nrow=4)
>>>> aggregate.nx.ny.forloop = function(data,nx=2,ny=2, FUN=mean,...)
>>> + {
>>> +   nlon=nrow(data)
>>> +   nlat=ncol(data)
>>> +   newdata=matrix(NA,nrow=nlon/nx,ncol=nlat/ny)
>>> +   dim(newdata)
>>> +   for(ilon in seq(1,nlon,nx)) {
>>> +     for(ilat in seq(1,nlat,ny)) {
>>> +       ilon_new=1+(ilon-1)/nx
>>> +       ilat_new=1+(ilat-1)/ny
>>> +       newdata[ilon_new,ilat_new] = FUN(data[ilon+0:1,ilat+0:1],...)
>>> +     }
>>> +   }
>>> +   newdata
>>> + }
>>>> aggregate.nx.ny.forloop(tst.small)
>>>    [,1] [,2] [,3] [,4]
>>> [1,]  3.5 11.5 19.5 27.5
>>> [2,]  5.5 13.5 21.5 29.5
>>>> 
>>>> aggregate.nx.ny.interaction = function(data,nx=2,ny=2, FUN=mean,...)
>>> + {
>>> +
>>> +   nlon=nrow(data)
>>> +   nlat=ncol(data)
>>> +   newdata=matrix(NA,nrow=nlon/nx,ncol=nlat/ny)
>>> +   newdata[] <- tapply( data, interaction( (row(data)+1) %/% 2,
>>> (col(data)+1) %/% 2 ), FUN, ...)
>>> +   newdata
>>> + }
>>>> aggregate.nx.ny.interaction(tst.small)
>>>    [,1] [,2] [,3] [,4]
>>> [1,]  3.5 11.5 19.5 27.5
>>> [2,]  5.5 13.5 21.5 29.5
>>>> 
>>>> aggregate.nx.ny.expand.grid = function(data,nx=2,ny=2, FUN=mean,...)
>>> + {
>>> +   ilon <- seq(1,ncol(data),nx)
>>> +   ilat <- seq(1,nrow(data),ny)
>>> +   cells <- as.matrix(expand.grid(ilat, ilon))
>>> +   blocks <- apply(cells, 1, function(x)
>>> data[x[1]:(x[1]+1),x[2]:(x[2]+1)])
>>> +   block.means <- colMeans(blocks)
>>> +   matrix(block.means, nrow(data)/ny, ncol(data)/nx)
>>> + }
>>>> aggregate.nx.ny.expand.grid(tst.small)
>>>    [,1] [,2] [,3] [,4]
>>> [1,]  3.5 11.5 19.5 27.5
>>> [2,]  5.5 13.5 21.5 29.5
>>>> 
>>>> aggregate.nx.ny.array.apply = function(data,nx=2,ny=2, FUN=mean,...)
>>> {
>>> +   a <- array(data, dim = c(ny, nrow( data ) %/% ny, ncol( data ) %/%
>>> nx))
>>> +   apply( a, c(2, 3), FUN, ... )
>>> + }
>>>> aggregate.nx.ny.array.apply(tst.small)
>>>    [,1] [,2] [,3] [,4]
>>> [1,]  1.5  5.5  9.5 13.5
>>> [2,]  3.5  7.5 11.5 15.5
>>> 
>>> 
>>> 
>>>> On 28 Jul 2016, at 00:26, David Winsemius <dwinsemius at comcast.net>
>>> wrote:
>>>> 
>>>> 
>>>>> On Jul 27, 2016, at 12:02 PM, Jeff Newmiller
>>> <jdnewmil at dcn.davis.ca.us> wrote:
>>>>> 
>>>>> An alternative (more compact, not necessarily faster, because apply
>>> is still a for loop inside):
>>>>> 
>>>>> f <- function( m, nx, ny ) {
>>>>> # redefine the dimensions of my
>>>>> a <- array( m
>>>>>           , dim = c( ny
>>>>>                  , nrow( m ) %/% ny
>>>>>                  , ncol( m ) %/% nx )
>>>>>          )
>>>>> # apply mean over dim 1
>>>>> apply( a, c( 2, 3 ), FUN=mean )
>>>>> }
>>>>> f( tst, nx, ny )
>>>> 
>>>> Here's an apparently loopless strategy, although I suspect the code
>>> for interaction (and maybe tapply as well?) uses a loop.
>>>> 
>>>> 
>>>> tst_2X2 <- matrix(NA, ,ncol=4,nrow=2)
>>>> 
>>>> tst_2x2[] <- tapply( tst, interaction( (row(tst)+1) %/% 2,
>>> (col(tst)+1) %/% 2 ), mean)
>>>> 
>>>> tst_2x2
>>>> 
>>>>    [,1] [,2] [,3] [,4]
>>>> [1,]  3.5 11.5 19.5 27.5
>>>> [2,]  5.5 13.5 21.5 29.5
>>>> 
>>>> --
>>>> David.
>>>> 
>>>> 
>>>>> 
>>>>> --
>>>>> Sent from my phone. Please excuse my brevity.
>>>>> 
>>>>> On July 27, 2016 9:08:32 AM PDT, David L Carlson <dcarlson at tamu.edu>
>>> wrote:
>>>>>> This should be faster. It uses apply() across the blocks.
>>>>>> 
>>>>>>> ilon <- seq(1,8,nx)
>>>>>>> ilat <- seq(1,4,ny)
>>>>>>> cells <- as.matrix(expand.grid(ilat, ilon))
>>>>>>> blocks <- apply(cells, 1, function(x) tst[x[1]:(x[1]+1),
>>>>>> x[2]:(x[2]+1)])
>>>>>>> block.means <- colMeans(blocks)
>>>>>>> tst_2x2 <- matrix(block.means, 2, 4)
>>>>>>> tst_2x2
>>>>>>  [,1] [,2] [,3] [,4]
>>>>>> [1,]  3.5 11.5 19.5 27.5
>>>>>> [2,]  5.5 13.5 21.5 29.5
>>>>>> 
>>>>>> -------------------------------------
>>>>>> David L Carlson
>>>>>> Department of Anthropology
>>>>>> Texas A&M University
>>>>>> College Station, TX 77840-4352
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> -----Original Message-----
>>>>>> From: R-help [mailto:r-help-bounces at r-poject.org] On Behalf Of
>>> Anthoni,
>>>>>> Peter (IMK)
>>>>>> Sent: Wednesday, July 27, 2016 6:14 AM
>>>>>> To: r-help at r-project.org
>>>>>> Subject: [R] Aggregate matrix in a 2 by 2 manor
>>>>>> 
>>>>>> Hi all,
>>>>>> 
>>>>>> I need to aggregate some matrix data (1440x720) to a lower
>>> dimension
>>>>>> (720x360) for lots of years and variables
>>>>>> 
>>>>>> I can do double for loop, but that will be slow. Anybody know a
>>> quicker
>>>>>> way?
>>>>>> 
>>>>>> here an example with a smaller matrix size:
>>>>>> 
>>>>>> tst=matrix(1:(8*4),ncol=8,nrow=4)
>>>>>> tst_2x2=matrix(NA,ncol=4,nrow=2)
>>>>>> nx=2
>>>>>> ny=2
>>>>>> for(ilon in seq(1,8,nx)) {
>>>>>> for (ilat in seq(1,4,ny)) {
>>>>>> ilon_2x2=1+(ilon-1)/nx
>>>>>> ilat_2x2=1+(ilat-1)/ny
>>>>>> tst_2x2[ilat_2x2,ilon_2x2] = mean(tst[ilat+0:1,ilon+0:1])
>>>>>> }
>>>>>> }
>>>>>> 
>>>>>> tst
>>>>>> tst_2x2
>>>>>> 
>>>>>>> tst
>>>>>>  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
>>>>>> [1,]    1    5    9   13   17   21   25   29
>>>>>> [2,]    2    6   10   14   18   22   26   30
>>>>>> [3,]    3    7   11   15   19   23   27   31
>>>>>> [4,]    4    8   12   16   20   24   28   32
>>>>>> 
>>>>>>> tst_2x2
>>>>>>  [,1] [,2] [,3] [,4]
>>>>>> [1,]  3.5 11.5 19.5 27.5
>>>>>> [2,]  5.5 13.5 21.5 29.5
>>>>>> 
>>>>>> 
>>>>>> I though a cast to 3d-array might do the trick and apply over the
>>> new
>>>>>> dimension, but that does not work, since it casts the data along
>>> the
>>>>>> row.
>>>>>>> matrix(apply(array(tst,dim=c(nx,ny,8)),3,mean),nrow=nrow(tst)/ny)
>>>>>>  [,1] [,2] [,3] [,4]
>>>>>> [1,]  2.5 10.5 18.5 26.5
>>>>>> [2,]  6.5 14.5 22.5 30.5
>>>>>> 
>>>>>> 
>>>>>> cheers
>>>>>> Peter
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> David Winsemius
>>>> Alameda, CA, USA
>>>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                      Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------


From ross.chapman at ecogeonomix.com  Sun Jul 31 10:11:16 2016
From: ross.chapman at ecogeonomix.com (Ross Chapman)
Date: Sun, 31 Jul 2016 18:11:16 +1000
Subject: [R] cpquery problem
Message-ID: <9a0d19b8a05d0ae5c8676569bbfe96d2.squirrel@ecogeonomix.com>

Hi Marco

Thanks for your prompt reply.

First, I have been using the parse(eval()) convention because I saw it
used in some example code for running cpquery, but am happy to drop this
practice.

I have tried running the cpquery in the debug mode, and found that it
typically returns the following for instances where the conditional
probability is returned as 0:

   > event matches 0 samples out of 0 (p = 0)

Am I right in understanding that the Monte Carlo sampling has been unable
to create any cases that match the query?  If so, why would this be if the
evidence used is very typical of an average case in the data used to train
the network?

Also, I have run predict on this network and get very good correlations
between the predicted and actual observations (r-squared 0.8 - 0.9).  Why
would it be that the network can return near perfect predictions can be so
good for a test set while the conditional probabilities remain at zero for
when exploring the same data set?

I think that I must be missing something in my deployment of the package
or the interpretation of the output.

Many thanks for your help.

Ross

On Fri, July 29, 2016 7:34 pm, Marco Scutari wrote:
> Hi Ross,
>
>
> first, I have a side question: is there a particular reason why you are
> using parse(eval()) in your queries? I know sometimes there is no other
> solution if you only use exported functions, but you should try not to. It
> makes for brittle code that breaks easily depending on how variables are
> scoped.
>
> On 29 July 2016 at 07:37,  <ross.chapman at ecogeonomix.com> wrote:
>
>> However, if I replace EST=='x' with EST=='z' or EST=='y' I get 0
>> probability of obtaining a value for ABW that is either greater or less
>> than the threshold.
>>
>> For example:
>>
>>
>>> cpquery(fitted,event=(ABW>=11), evidence=eval(parse(text="(EST=='y' &
>>> TR>9
>>>
>> & BU>15819 &  RF>2989)")),n=10^6)
>>
>>
>> [1] 0
>>
>>
>> and
>>
>>> cpquery(fitted,event=(ABW<=11), evidence=eval(parse(text="(EST=='y' &
>>> TR>9
>>>
>> & BU>15819 &  RF>2989)")),n=10^6)
>>
>>
>> [1] 0
>>
>
> From this output, my guess is that the evidence has probability
> (exactly or close to) zero. You can check by running cpdist(): if the
> evidence has probability zero, no random samples will be returned. Turning
> on the debugging output in cpquery() should also highlight what the
> problem is.
>
> If that turns out to be the case, switch from the default method =
> "ls" to method = "lw" in cpdist(). (Note that the syntax changes
> slightly, check the documentation for examples.)
>
>> My own knowledge from the data is that these classes should both
>> typically return a value for ABW that is very much higher than the
>> threshold value.
>
> That may be, but much depends on the specific sample the model was
> fitted from. How does the fitted network look like?
>
> Cheers,
> Marco
>
>
> --
> Marco Scutari, Ph.D.
> Lecturer in Statistics, Department of Statistics
> University of Oxford, United Kingdom
>
>


From dwinsemius at comcast.net  Sun Jul 31 18:06:53 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 31 Jul 2016 09:06:53 -0700
Subject: [R] lapply
In-Reply-To: <CANTvJZKLaDTqYWcA=x4MZ14hAqJ8R=OnGV_xL=+pe63omSYg5Q@mail.gmail.com>
References: <CANTvJZKLaDTqYWcA=x4MZ14hAqJ8R=OnGV_xL=+pe63omSYg5Q@mail.gmail.com>
Message-ID: <98B93864-4A04-450E-85D1-07B5114D3104@comcast.net>


> On Jul 30, 2016, at 7:53 PM, roslinazairimah zakaria <roslinaump at gmail.com> wrote:
> 
> Dear r-users,
> 
> I would like to use lapply for the following task:
> 
> ## Kolmogorov-Smirnov
> ks.test(stn_all[,1][stn_all[,1] > 0],stn_all_gen[,1][stn_all_gen[,1] > 0])
> ks.test(stn_all[,2][stn_all[,2] > 0],stn_all_gen[,2][stn_all_gen[,2] > 0])
> ks.test(stn_all[,3][stn_all[,3] > 0],stn_all_gen[,3][stn_all_gen[,3] > 0])
> ks.test(stn_all[,4][stn_all[,4] > 0],stn_all_gen[,4][stn_all_gen[,4] > 0])
> ks.test(stn_all[,5][stn_all[,5] > 0],stn_all_gen[,5][stn_all_gen[,5] > 0])
> ks.test(stn_all[,6][stn_all[,6] > 0],stn_all_gen[,6][stn_all_gen[,6] > 0])
> 
> I would like to conduct the Kolmogorov Smirnov goodness of fit tests.

Either use `lapply` over the column indices or (probably more cleanly) use `mapply` over the two dataframes.

-- 
David
> 
> Is it possible?
> 
> 
> Thank you very much.
> -- 
> *Dr. Roslinazairimah Binti Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
> 
> *Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
> roslinaump at gmail.com <roslinaump at gmail.com>*
> Deputy Dean (Academic & Student Affairs)
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From samsad.afrin at gmail.com  Sun Jul 31 10:54:30 2016
From: samsad.afrin at gmail.com (Samsad Afrin Himi)
Date: Sun, 31 Jul 2016 10:54:30 +0200
Subject: [R] Arcsine Tranformation.
Message-ID: <CAGzhwenRdy1rrPhO7+XAzwgvxxjPUnYdrfc_DS6xicx350FqHA@mail.gmail.com>

Dear R-Team,

How can I do arcsine tzransformation in R?  My data is proportional score.

Could you please help me out?

Best,
Samsad

	[[alternative HTML version deleted]]


From bhaskar.kolkata at gmail.com  Sun Jul 31 17:07:06 2016
From: bhaskar.kolkata at gmail.com (Bhaskar Mitra)
Date: Sun, 31 Jul 2016 11:07:06 -0400
Subject: [R] Help in plotting
Message-ID: <CAEGXkYWXQEuJhBnsG89KNWvd831OLEcBNCa51uVMz1yob7t9AA@mail.gmail.com>

Hello Everyone,


I have a data frame with 2 columns as shown at the end of this mail. I want
to plot the data in column A;
 however I want the  data-points in column A to be of different color based
on conditions in column B.
i.e all data in column A corresponding to value 0 in column B should be
red.
Similarly all values in column A corresponding to value 2 in column B
should be green.

Any suggestions or help in this regard.

Best,
Bhaskar





A       B
12      0
11      1
23      2
34      0
45      1
67      3
?         ..

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sun Jul 31 19:10:45 2016
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 31 Jul 2016 10:10:45 -0700 (PDT)
Subject: [R] Arcsine Tranformation.
In-Reply-To: <CAGzhwenRdy1rrPhO7+XAzwgvxxjPUnYdrfc_DS6xicx350FqHA@mail.gmail.com>
References: <CAGzhwenRdy1rrPhO7+XAzwgvxxjPUnYdrfc_DS6xicx350FqHA@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1607310944300.47406@pedal.dcn.davis.ca.us>

On Sun, 31 Jul 2016, Samsad Afrin Himi wrote:

> Dear R-Team,
>
> How can I do arcsine tzransformation in R?  My data is proportional score.

?asin

> Could you please help me out?

This is such a simple task that it is difficult to tell what very basic 
aspects of R you need help with. If you don't know what a data 
transformation is, or what it is for, then what should be obvious might 
not be [1] and this is not a stats theory forum [2]. And if you have no 
clue how to put data into R or perform basic calculations with it then you 
should be reading the "Introduction to R" [3] or taking a course. And if 
you are now taking a course, then you should be asking your instructor for 
assistance per the Posting Guide (no homework on this list).

If this is not homework and you understand the theory and have a basic 
grasp of R syntax then you can clarify what you are stuck on by providing 
a reproducible example [4] of the R code that gets you to where you are 
stuck, including both sample data and code. Then we can narrow down the 
scope of discussion. Don't forget to switch your email program to plain 
text so your hard work (yes, asking questions by email isn't easy) does 
not get damaged by HTML formatting.

[1] https://en.wikipedia.org/wiki/Data_transformation_(statistics)
[2] e.g. stats.stackexchange.com
[3] https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf
[4] http://adv-r.had.co.nz/Reproducibility.html

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From dwinsemius at comcast.net  Sun Jul 31 19:29:23 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 31 Jul 2016 10:29:23 -0700
Subject: [R] Help in plotting
In-Reply-To: <CAEGXkYWXQEuJhBnsG89KNWvd831OLEcBNCa51uVMz1yob7t9AA@mail.gmail.com>
References: <CAEGXkYWXQEuJhBnsG89KNWvd831OLEcBNCa51uVMz1yob7t9AA@mail.gmail.com>
Message-ID: <27B0E59A-2ECC-4B42-BB8F-92175180CCA3@comcast.net>


> On Jul 31, 2016, at 8:07 AM, Bhaskar Mitra <bhaskar.kolkata at gmail.com> wrote:
> 
> Hello Everyone,
> 
> 
> I have a data frame with 2 columns as shown at the end of this mail. I want
> to plot the data in column A;
> however I want the  data-points in column A to be of different color based
> on conditions in column B.
> i.e all data in column A corresponding to value 0 in column B should be
> red.
> Similarly all values in column A corresponding to value 2 in column B
> should be green.
> 
> Any suggestions or help in this regard.
> 
> Best,
> Bhaskar
> 
> 
> 
> 
> 
> A       B
> 12      0
> 11      1
> 23      2
> 34      0
> 45      1
> 67      3
> ?         ..

This should do it:

plot(1:length(dfrm$A), dfrm$A, col = c("red", "blue", "green") [ 1+dfrm$B] )

I suggest reading more at: 

?palette   # ..... and follow the links to other relevant help pages.

> 	[[alternative HTML version deleted]]

And I suggest you read the Posting Guide more carefully.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


