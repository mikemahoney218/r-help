From jwd @end|ng |rom @urewe@t@net  Fri Mar  1 20:53:28 2019
From: jwd @end|ng |rom @urewe@t@net (John)
Date: Fri, 1 Mar 2019 11:53:28 -0800
Subject: [R] Which dependency list to build first?
In-Reply-To: <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
References: <alpine.LNX.2.20.1902241104040.31842@salmo.appl-ecosys.com>
 <23669.4832.639467.519820@stat.math.ethz.ch>
 <alpine.LNX.2.20.1902260532430.16864@salmo.appl-ecosys.com>
Message-ID: <20190301115328.267dcbc1@Draco.localdomain>

On Tue, 26 Feb 2019 05:47:38 -0800 (PST)
Rich Shepard <rshepard at appl-ecosys.com> wrote:

> /usr/lib/R/library/later/libs/later.so: undefined symbol:
> __atomic_fetch_add_8

Stackoverflow may help at least in locating the trouble.  A web search
indicates the error is not unknown in other contexts than R.  

JWDougherty


From w|n@ton @end|ng |rom r@tud|o@com  Fri Mar  1 05:12:04 2019
From: w|n@ton @end|ng |rom r@tud|o@com (Winston Chang)
Date: Thu, 28 Feb 2019 22:12:04 -0600
Subject: [R] profvis function parse_rprof not being loaded
In-Reply-To: <dc3afd85-01c0-8c45-9f08-96b1480451cb@gmail.com>
References: <CAN9eD7mvBUTqxbdkPwoF=FkcaEbih5PwdOyE7RKRDxF=uEOc2Q@mail.gmail.com>
 <dc3afd85-01c0-8c45-9f08-96b1480451cb@gmail.com>
Message-ID: <CANTAc-5unNnLKfBGyUNTTHpfPqRs1mny0c+W94qVq8qjf=yGSg@mail.gmail.com>

Thanks for the heads up - I've updated the development version of profvis
to export parse_rprof.

-Winston

On Thu, Feb 28, 2019 at 5:14 AM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 27/02/2019 9:42 p.m., nevil amos wrote:
> > I have loaded the profvis library  but the function parse_ rprof()  is
> > absent.
> > below is the session info show the absence of the function ( which is
> > listed  in the package help for the current version.)
> >
>
> Looks as though they forgot to export it.  You can get to it using
> profvis:::parse_rprof, but the maintainer (who is cc'd) might want to
> fix this.
>
> Duncan Murdoch
>
> >
> > Documentation for package ?profvis? version 0.3.5
> >
> > DESCRIPTION file.
> > Help Pages
> >
> > parse_rprof Parse Rprof output file for use with profvis
> > pause Pause an R process
> > print.profvis Print a profvis object
> > profvis Profile an R expression and visualize profiling data
> > profvisOutput Widget output function for use in Shiny
> > renderProfvis Widget render function for use in Shiny
> >
> >
> > Rsession info:
> >
> >
> >> library(profvis)
> >> parse_rprof()
> > Error in parse_rprof() : could not find function "parse_rprof"
> >> ls("package:profvis")
> > [1] "pause"         "profvis"       "profvisOutput" "renderProfvis"
> >> sessionInfo()
> > R version 3.5.2 (2018-12-20)
> > Platform: x86_64-w64-mingw32/x64 (64-bit)
> > Running under: Windows 7 x64 (build 7601) Service Pack 1
> >
> > Matrix products: default
> >
> > locale:
> > [1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252
> > LC_MONETARY=English_Australia.1252
> > [4] LC_NUMERIC=C                       LC_TIME=English_Australia.1252
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > other attached packages:
> > [1] profvis_0.3.5
> >
> > loaded via a namespace (and not attached):
> >   [1] htmlwidgets_1.3 compiler_3.5.2  magrittr_1.5    htmltools_0.3.6
> > tools_3.5.2     yaml_2.2.0      Rcpp_1.0.0      stringi_1.2.4
> >   [9] stringr_1.3.1   digest_0.6.18
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>

	[[alternative HTML version deleted]]


From jrkr|de@u @end|ng |rom gm@||@com  Sat Mar  2 15:11:45 2019
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Sat, 2 Mar 2019 09:11:45 -0500
Subject: [R] R installation Ubuntu 10.04 missing dependency
Message-ID: <CAKZQJMBQN7YnQ4sZikSYByjJZ5_D2QApfH7v82_9pqr20rOHiQ@mail.gmail.com>

To upgrade to R.3.5.2 from 3.4.4 I have been following the
instructions at
https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/
.

I seem to have the repository properly connected and verified.

I am getting an error:
---------------------------------------------------------------------------------------------------
john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help resolve the situation:

The following packages have unmet dependencies:
 r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable
               Recommends: r-base-dev but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
-------------------------------------------------------------------------------------------------
Earlier I was getting another error
r-base-core : Depends: libpng12-0 but it is not installable
but that was cured by installing libpng12-0 via the Ubuntu Software app.

I cannot find what appears to be an installable version of libreadlines6.

Ubuntu reports that the up-to-date version of libreadlines7.

Should I consider this an R issue or an Ubuntu issue?

In any case, has anyone encountered the problem and beaten it?

Thanks

-- 
John Kane
Kingston ON Canada


From D@rren@D@ny|uk @end|ng |rom @d6@bc@c@  Sat Mar  2 23:27:55 2019
From: D@rren@D@ny|uk @end|ng |rom @d6@bc@c@ (Darren Danyluk)
Date: Sat, 2 Mar 2019 22:27:55 +0000
Subject: [R] Remove Even Number from A Vector
Message-ID: <YQXPR01MB2743BF5157B0C75811B6587498770@YQXPR01MB2743.CANPRD01.PROD.OUTLOOK.COM>

Hello,

I found this email when looking for some help with R Studio.  It's actually my daughter who is looking for help.

It sounds like she is working with the very basics of this software, and her task is to write the code which would result in the extraction of "odd" data from a dataset of restaurant sales.

This is a shot in the dark...please ignore if my question makes little or no sense.  I have no working knowledge of R software.

Thanks.

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sun Mar  3 07:43:12 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 3 Mar 2019 17:43:12 +1100
Subject: [R] Remove Even Number from A Vector
In-Reply-To: <YQXPR01MB2743BF5157B0C75811B6587498770@YQXPR01MB2743.CANPRD01.PROD.OUTLOOK.COM>
References: <YQXPR01MB2743BF5157B0C75811B6587498770@YQXPR01MB2743.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <CA+8X3fW8JmeQPQD4+sFX4hDiw9UCnLhsibPPjnrP0Bk+q+gDkQ@mail.gmail.com>

Hi Darren,
You're probably looking for the %% (remainder) operator:

x<-1:10
# get odd numbers
x[as.logical(x%%2)]
# get even numbers
x[!(x%%2)]

Jim

On Sun, Mar 3, 2019 at 4:10 PM Darren Danyluk <Darren.Danyluk at sd6.bc.ca> wrote:
>
> Hello,
>
> I found this email when looking for some help with R Studio.  It's actually my daughter who is looking for help.
>
> It sounds like she is working with the very basics of this software, and her task is to write the code which would result in the extraction of "odd" data from a dataset of restaurant sales.
>
> This is a shot in the dark...please ignore if my question makes little or no sense.  I have no working knowledge of R software.
>
> Thanks.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Sun Mar  3 09:44:08 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sun, 3 Mar 2019 11:44:08 +0300
Subject: [R] Remove Even Number from A Vector
In-Reply-To: <YQXPR01MB2743BF5157B0C75811B6587498770@YQXPR01MB2743.CANPRD01.PROD.OUTLOOK.COM>
References: <YQXPR01MB2743BF5157B0C75811B6587498770@YQXPR01MB2743.CANPRD01.PROD.OUTLOOK.COM>
Message-ID: <20190303114408.5b56e8b6@Tarkus>

Hi Darren,

On Sat, 2 Mar 2019 22:27:55 +0000
Darren Danyluk <Darren.Danyluk at sd6.bc.ca> wrote:

> It sounds like she is working with the very basics of this software,
> and her task is to write the code which would result in the
> extraction of "odd" data from a dataset of restaurant sales.

Not a native English speaker here; what exactly do you mean by "odd" in
this case?

If I ignore the "subject" field, it looks like your daughter should be
looking for outlier detection methods. For example, https://rseek.org/
offers some good results for "outlier detection".

-- 
Best regards,
Ivan


From er|cjberger @end|ng |rom gm@||@com  Sun Mar  3 10:13:23 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sun, 3 Mar 2019 11:13:23 +0200
Subject: [R] R installation Ubuntu 10.04 missing dependency
In-Reply-To: <CAKZQJMBQN7YnQ4sZikSYByjJZ5_D2QApfH7v82_9pqr20rOHiQ@mail.gmail.com>
References: <CAKZQJMBQN7YnQ4sZikSYByjJZ5_D2QApfH7v82_9pqr20rOHiQ@mail.gmail.com>
Message-ID: <CAGgJW77QHNPn2WYu9-E80dKVu2_Oh0-XTa1wYWa9on+VUwkSyg@mail.gmail.com>

Hi John,
Is the subject line of your question correct? Ubuntu 10.04?
If that is not a typo, that could be contributing to your problem. I
believe that the current stable release of Ubuntu is 18.04.
According to the following link ubuntu 10.04 was released almost 10 years
ago and was "retired" about 5 years ago.

http://fridge.ubuntu.com/2015/03/18/ubuntu-10-04-lucid-lynx-reaches-end-of-life-on-april-30-2015/

HTH,
Eric


On Sun, Mar 3, 2019 at 7:10 AM John Kane <jrkrideau at gmail.com> wrote:

> To upgrade to R.3.5.2 from 3.4.4 I have been following the
> instructions at
>
> https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/
> .
>
> I seem to have the repository properly connected and verified.
>
> I am getting an error:
>
> ---------------------------------------------------------------------------------------------------
> john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended
> Reading package lists... Done
> Building dependency tree
> Reading state information... Done
> Some packages could not be installed. This may mean that you have
> requested an impossible situation or if you are using the unstable
> distribution that some required packages have not yet been created
> or been moved out of Incoming.
> The following information may help resolve the situation:
>
> The following packages have unmet dependencies:
>  r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable
>                Recommends: r-base-dev but it is not going to be installed
> E: Unable to correct problems, you have held broken packages.
>
> -------------------------------------------------------------------------------------------------
> Earlier I was getting another error
> r-base-core : Depends: libpng12-0 but it is not installable
> but that was cured by installing libpng12-0 via the Ubuntu Software app.
>
> I cannot find what appears to be an installable version of libreadlines6.
>
> Ubuntu reports that the up-to-date version of libreadlines7.
>
> Should I consider this an R issue or an Ubuntu issue?
>
> In any case, has anyone encountered the problem and beaten it?
>
> Thanks
>
> --
> John Kane
> Kingston ON Canada
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun Mar  3 12:07:10 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 3 Mar 2019 06:07:10 -0500
Subject: [R] Remove Even Number from A Vector
In-Reply-To: <20190303114408.5b56e8b6@Tarkus>
References: <YQXPR01MB2743BF5157B0C75811B6587498770@YQXPR01MB2743.CANPRD01.PROD.OUTLOOK.COM>
 <20190303114408.5b56e8b6@Tarkus>
Message-ID: <62bf1736-233d-e09f-3e13-5aeb0ac64766@gmail.com>

On 03/03/2019 3:44 a.m., Ivan Krylov wrote:
> Hi Darren,
> 
> On Sat, 2 Mar 2019 22:27:55 +0000
> Darren Danyluk <Darren.Danyluk at sd6.bc.ca> wrote:
> 
>> It sounds like she is working with the very basics of this software,
>> and her task is to write the code which would result in the
>> extraction of "odd" data from a dataset of restaurant sales.
> 
> Not a native English speaker here; what exactly do you mean by "odd" in
> this case?

"Odd" numbers have a remainder of 1 when divided by 2; "even" numbers 
are multiples of 2.

Duncan Murdoch

> 
> If I ignore the "subject" field, it looks like your daughter should be
> looking for outlier detection methods. For example, https://rseek.org/
> offers some good results for "outlier detection".


From jrkr|de@u @end|ng |rom y@hoo@c@  Sun Mar  3 12:20:33 2019
From: jrkr|de@u @end|ng |rom y@hoo@c@ (John Kane)
Date: Sun, 3 Mar 2019 11:20:33 +0000 (UTC)
Subject: [R] R installation Ubuntu 18.04 missing dependency libreadline6
References: <1289123265.8389549.1551612033163.ref@mail.yahoo.com>
Message-ID: <1289123265.8389549.1551612033163@mail.yahoo.com>


 NOTE. This is a re-post of a message of Saturday 2018-03-03 sent with an incorrect header. 

To upgrade to R.3.5.2 from 3.4.4 I have been following the instructions at https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/ .

I seem to have the repository properly connected and verified.

I am getting an error: --------------------------------------------------------------------------------------------------- john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help resolve the situation:

The following packages have unmet dependencies: r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable Recommends: r-base-dev but it is not going to be installed E: Unable to correct problems, you have held broken packages. ------------------------------------------------------------------------------------------------- Earlier I was getting another error r-base-core : Depends: libpng12-0 but it is not installable but that was cured by installing libpng12-0 via the Ubuntu Software app.

I cannot find what appears to be an installable version of libreadlines6.

Ubuntu reports that the up-to-date version of libreadlines7.

Should I consider this an R issue or an Ubuntu issue?

In any case, has anyone encountered the problem and beaten it?

Thanks


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Mar  3 13:33:29 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 3 Mar 2019 12:33:29 +0000
Subject: [R] R installation Ubuntu 18.04 missing dependency libreadline6
In-Reply-To: <1289123265.8389549.1551612033163@mail.yahoo.com>
References: <1289123265.8389549.1551612033163.ref@mail.yahoo.com>
 <1289123265.8389549.1551612033163@mail.yahoo.com>
Message-ID: <77e2b9a1-1ebd-88a2-5a4d-baed65136b67@sapo.pt>

Hello,

I never installed R on Ubuntu like you did it.
I follow this:

sudo apt-get update
sudo apt-get install r-base
sudo apt-get install r-base-dev


And it works at the first try. See [1]
The problems I [always] have are with missing Ubuntu libs needed by 
contributed packages, not with base R.

[1] https://cran.r-project.org/bin/linux/ubuntu/README

Hope this helps,

Rui Barradas


?s 11:20 de 03/03/2019, John Kane via R-help escreveu:
> 
>   NOTE. This is a re-post of a message of Saturday 2018-03-03 sent with an incorrect header.
> 
> To upgrade to R.3.5.2 from 3.4.4 I have been following the instructions at https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/ .
> 
> I seem to have the repository properly connected and verified.
> 
> I am getting an error: --------------------------------------------------------------------------------------------------- john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help resolve the situation:
> 
> The following packages have unmet dependencies: r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable Recommends: r-base-dev but it is not going to be installed E: Unable to correct problems, you have held broken packages. ------------------------------------------------------------------------------------------------- Earlier I was getting another error r-base-core : Depends: libpng12-0 but it is not installable but that was cured by installing libpng12-0 via the Ubuntu Software app.
> 
> I cannot find what appears to be an installable version of libreadlines6.
> 
> Ubuntu reports that the up-to-date version of libreadlines7.
> 
> Should I consider this an R issue or an Ubuntu issue?
> 
> In any case, has anyone encountered the problem and beaten it?
> 
> Thanks
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From |@t@z@hn @end|ng |rom gm@||@com  Sun Mar  3 19:16:39 2019
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Sun, 3 Mar 2019 13:16:39 -0500
Subject: [R] R installation Ubuntu 18.04 missing dependency libreadline6
In-Reply-To: <1289123265.8389549.1551612033163@mail.yahoo.com>
References: <1289123265.8389549.1551612033163.ref@mail.yahoo.com>
 <1289123265.8389549.1551612033163@mail.yahoo.com>
Message-ID: <CA+vqiLHJ0yxOj7jZmimbjrxgJG3W5gSc=uk05fkgpnHTCqNB+g@mail.gmail.com>

Hi John,

The official instructions at
https://cran.r-project.org/bin/linux/ubuntu/ work on a fresh
ubuntu:bionic from dockerhub. This suggests that the issue is due to
the configuration of your local system rather than with any problem
with either R or ubuntu. My guess is that you've been reading too many
blog posts and adding too many ppa's to your system, which can lead to
package conflicts. Unfortunately in that case it's hard for people to
help you remotely, since we don't know what you've done to your
system.

Best,
Ista

On Sun, Mar 3, 2019 at 6:20 AM John Kane via R-help
<r-help at r-project.org> wrote:
>
>
>  NOTE. This is a re-post of a message of Saturday 2018-03-03 sent with an incorrect header.
>
> To upgrade to R.3.5.2 from 3.4.4 I have been following the instructions at https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/ .
>
> I seem to have the repository properly connected and verified.
>
> I am getting an error: --------------------------------------------------------------------------------------------------- john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help resolve the situation:
>
> The following packages have unmet dependencies: r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable Recommends: r-base-dev but it is not going to be installed E: Unable to correct problems, you have held broken packages. ------------------------------------------------------------------------------------------------- Earlier I was getting another error r-base-core : Depends: libpng12-0 but it is not installable but that was cured by installing libpng12-0 via the Ubuntu Software app.
>
> I cannot find what appears to be an installable version of libreadlines6.
>
> Ubuntu reports that the up-to-date version of libreadlines7.
>
> Should I consider this an R issue or an Ubuntu issue?
>
> In any case, has anyone encountered the problem and beaten it?
>
> Thanks
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom y@hoo@c@  Sun Mar  3 20:08:04 2019
From: jrkr|de@u @end|ng |rom y@hoo@c@ (John Kane)
Date: Sun, 3 Mar 2019 19:08:04 +0000 (UTC)
Subject: [R] R installation Ubuntu 18.04 missing dependency libreadline6
In-Reply-To: <CA+vqiLHJ0yxOj7jZmimbjrxgJG3W5gSc=uk05fkgpnHTCqNB+g@mail.gmail.com>
References: <1289123265.8389549.1551612033163.ref@mail.yahoo.com>
 <1289123265.8389549.1551612033163@mail.yahoo.com>
 <CA+vqiLHJ0yxOj7jZmimbjrxgJG3W5gSc=uk05fkgpnHTCqNB+g@mail.gmail.com>
Message-ID: <1900561567.8514643.1551640084544@mail.yahoo.com>

 Thanks Ista,
Interestingly enough, I have only one repository as far as I can see. I was a bit amazed.
I think I have a faulty Ubuntu installation and will have to reinstall. What fun
?
    On Sunday, March 3, 2019, 1:16:52 p.m. EST, Ista Zahn <istazahn at gmail.com> wrote:  
 
 Hi John,

The official instructions at
https://cran.r-project.org/bin/linux/ubuntu/ work on a fresh
ubuntu:bionic from dockerhub. This suggests that the issue is due to
the configuration of your local system rather than with any problem
with either R or ubuntu. My guess is that you've been reading too many
blog posts and adding too many ppa's to your system, which can lead to
package conflicts. Unfortunately in that case it's hard for people to
help you remotely, since we don't know what you've done to your
system.

Best,
Ista

On Sun, Mar 3, 2019 at 6:20 AM John Kane via R-help
<r-help at r-project.org> wrote:
>
>
>? NOTE. This is a re-post of a message of Saturday 2018-03-03 sent with an incorrect header.
>
> To upgrade to R.3.5.2 from 3.4.4 I have been following the instructions at https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/ .
>
> I seem to have the repository properly connected and verified.
>
> I am getting an error: --------------------------------------------------------------------------------------------------- john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help resolve the situation:
>
> The following packages have unmet dependencies: r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable Recommends: r-base-dev but it is not going to be installed E: Unable to correct problems, you have held broken packages. ------------------------------------------------------------------------------------------------- Earlier I was getting another error r-base-core : Depends: libpng12-0 but it is not installable but that was cured by installing libpng12-0 via the Ubuntu Software app.
>
> I cannot find what appears to be an installable version of libreadlines6.
>
> Ubuntu reports that the up-to-date version of libreadlines7.
>
> Should I consider this an R issue or an Ubuntu issue?
>
> In any case, has anyone encountered the problem and beaten it?
>
> Thanks
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.  
	[[alternative HTML version deleted]]


From |@t@z@hn @end|ng |rom gm@||@com  Sun Mar  3 21:59:01 2019
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Sun, 3 Mar 2019 15:59:01 -0500
Subject: [R] R installation Ubuntu 18.04 missing dependency libreadline6
In-Reply-To: <1900561567.8514643.1551640084544@mail.yahoo.com>
References: <1289123265.8389549.1551612033163.ref@mail.yahoo.com>
 <1289123265.8389549.1551612033163@mail.yahoo.com>
 <CA+vqiLHJ0yxOj7jZmimbjrxgJG3W5gSc=uk05fkgpnHTCqNB+g@mail.gmail.com>
 <1900561567.8514643.1551640084544@mail.yahoo.com>
Message-ID: <CA+vqiLFzWmL2-6khqDPTUFknhJ7CSwU6NGg3LVoAiAneSAer4Q@mail.gmail.com>

Hi John,

This is not the place, but if you post your /etc/apt/sources.list to
r-sig-debian I might be able to help you sort it out.

Best,
Ista


On Sun, Mar 3, 2019 at 2:08 PM John Kane <jrkrideau at yahoo.ca> wrote:
>
> Thanks Ista,
>
> Interestingly enough, I have only one repository as far as I can see. I was a bit amazed.
>
> I think I have a faulty Ubuntu installation and will have to reinstall. What fun
>
>
>
> On Sunday, March 3, 2019, 1:16:52 p.m. EST, Ista Zahn <istazahn at gmail.com> wrote:
>
>
> Hi John,
>
> The official instructions at
> https://cran.r-project.org/bin/linux/ubuntu/ work on a fresh
> ubuntu:bionic from dockerhub. This suggests that the issue is due to
> the configuration of your local system rather than with any problem
> with either R or ubuntu. My guess is that you've been reading too many
> blog posts and adding too many ppa's to your system, which can lead to
> package conflicts. Unfortunately in that case it's hard for people to
> help you remotely, since we don't know what you've done to your
> system.
>
> Best,
> Ista
>
> On Sun, Mar 3, 2019 at 6:20 AM John Kane via R-help
> <r-help at r-project.org> wrote:
> >
> >
> >  NOTE. This is a re-post of a message of Saturday 2018-03-03 sent with an incorrect header.
> >
> > To upgrade to R.3.5.2 from 3.4.4 I have been following the instructions at https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/ .
> >
> > I seem to have the repository properly connected and verified.
> >
> > I am getting an error: --------------------------------------------------------------------------------------------------- john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help resolve the situation:
> >
> > The following packages have unmet dependencies: r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable Recommends: r-base-dev but it is not going to be installed E: Unable to correct problems, you have held broken packages. ------------------------------------------------------------------------------------------------- Earlier I was getting another error r-base-core : Depends: libpng12-0 but it is not installable but that was cured by installing libpng12-0 via the Ubuntu Software app.
> >
> > I cannot find what appears to be an installable version of libreadlines6.
> >
> > Ubuntu reports that the up-to-date version of libreadlines7.
> >
> > Should I consider this an R issue or an Ubuntu issue?
> >
> > In any case, has anyone encountered the problem and beaten it?
> >
> > Thanks
>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom y@hoo@c@  Sun Mar  3 22:24:03 2019
From: jrkr|de@u @end|ng |rom y@hoo@c@ (John Kane)
Date: Sun, 3 Mar 2019 21:24:03 +0000 (UTC)
Subject: [R] R installation Ubuntu 18.04 missing dependency libreadline6
In-Reply-To: <CA+vqiLFzWmL2-6khqDPTUFknhJ7CSwU6NGg3LVoAiAneSAer4Q@mail.gmail.com>
References: <1289123265.8389549.1551612033163.ref@mail.yahoo.com>
 <1289123265.8389549.1551612033163@mail.yahoo.com>
 <CA+vqiLHJ0yxOj7jZmimbjrxgJG3W5gSc=uk05fkgpnHTCqNB+g@mail.gmail.com>
 <1900561567.8514643.1551640084544@mail.yahoo.com>
 <CA+vqiLFzWmL2-6khqDPTUFknhJ7CSwU6NGg3LVoAiAneSAer4Q@mail.gmail.com>
Message-ID: <1328781229.8580979.1551648243195@mail.yahoo.com>

 Will do. Thanks.?
    On Sunday, March 3, 2019, 3:59:14 p.m. EST, Ista Zahn <istazahn at gmail.com> wrote:  
 
 Hi John,

This is not the place, but if you post your /etc/apt/sources.list to
r-sig-debian I might be able to help you sort it out.

Best,
Ista


On Sun, Mar 3, 2019 at 2:08 PM John Kane <jrkrideau at yahoo.ca> wrote:
>
> Thanks Ista,
>
> Interestingly enough, I have only one repository as far as I can see. I was a bit amazed.
>
> I think I have a faulty Ubuntu installation and will have to reinstall. What fun
>
>
>
> On Sunday, March 3, 2019, 1:16:52 p.m. EST, Ista Zahn <istazahn at gmail.com> wrote:
>
>
> Hi John,
>
> The official instructions at
> https://cran.r-project.org/bin/linux/ubuntu/ work on a fresh
> ubuntu:bionic from dockerhub. This suggests that the issue is due to
> the configuration of your local system rather than with any problem
> with either R or ubuntu. My guess is that you've been reading too many
> blog posts and adding too many ppa's to your system, which can lead to
> package conflicts. Unfortunately in that case it's hard for people to
> help you remotely, since we don't know what you've done to your
> system.
>
> Best,
> Ista
>
> On Sun, Mar 3, 2019 at 6:20 AM John Kane via R-help
> <r-help at r-project.org> wrote:
> >
> >
> >? NOTE. This is a re-post of a message of Saturday 2018-03-03 sent with an incorrect header.
> >
> > To upgrade to R.3.5.2 from 3.4.4 I have been following the instructions at https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/ .
> >
> > I seem to have the repository properly connected and verified.
> >
> > I am getting an error: --------------------------------------------------------------------------------------------------- john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help resolve the situation:
> >
> > The following packages have unmet dependencies: r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable Recommends: r-base-dev but it is not going to be installed E: Unable to correct problems, you have held broken packages. ------------------------------------------------------------------------------------------------- Earlier I was getting another error r-base-core : Depends: libpng12-0 but it is not installable but that was cured by installing libpng12-0 via the Ubuntu Software app.
> >
> > I cannot find what appears to be an installable version of libreadlines6.
> >
> > Ubuntu reports that the up-to-date version of libreadlines7.
> >
> > Should I consider this an R issue or an Ubuntu issue?
> >
> > In any case, has anyone encountered the problem and beaten it?
> >
> > Thanks
>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.  
	[[alternative HTML version deleted]]


From jrkr|de@u @end|ng |rom y@hoo@c@  Sun Mar  3 22:55:08 2019
From: jrkr|de@u @end|ng |rom y@hoo@c@ (John Kane)
Date: Sun, 3 Mar 2019 21:55:08 +0000 (UTC)
Subject: [R] R installation Ubuntu 18.04 missing dependency libreadline6
In-Reply-To: <1900561567.8514643.1551640084544@mail.yahoo.com>
References: <1289123265.8389549.1551612033163.ref@mail.yahoo.com>
 <1289123265.8389549.1551612033163@mail.yahoo.com>
 <CA+vqiLHJ0yxOj7jZmimbjrxgJG3W5gSc=uk05fkgpnHTCqNB+g@mail.gmail.com>
 <1900561567.8514643.1551640084544@mail.yahoo.com>
Message-ID: <1834287529.8590031.1551650108951@mail.yahoo.com>

 
Cancel my mailing that source.list. to r-sig-debian.
?I took one last look at the sources.list file and realised that I had the same repository repeated. I had read it as being commented out but it was not. Delete one line and I was fine.?
Nothing like blindness! I must have scanned that thing at least 3 or 4 times and totally missed it.??Your offer was gratefully received. I had not realised how often I use R in a day even just as a quick calculator.

Thanks to everyone for the help. I was just about ready to reinstall Ubuntu and not looking forward to 2 or 3 days of installations and customising.?
    On Sunday, March 3, 2019, 2:08:04 p.m. EST, John Kane <jrkrideau at yahoo.ca> wrote:  
 
  Thanks Ista,
Interestingly enough, I have only one repository as far as I can see. I was a bit amazed.
I think I have a faulty Ubuntu installation and will have to reinstall. What fun
?
    On Sunday, March 3, 2019, 1:16:52 p.m. EST, Ista Zahn <istazahn at gmail.com> wrote:  
 
 Hi John,

The official instructions at
https://cran.r-project.org/bin/linux/ubuntu/ work on a fresh
ubuntu:bionic from dockerhub. This suggests that the issue is due to
the configuration of your local system rather than with any problem
with either R or ubuntu. My guess is that you've been reading too many
blog posts and adding too many ppa's to your system, which can lead to
package conflicts. Unfortunately in that case it's hard for people to
help you remotely, since we don't know what you've done to your
system.

Best,
Ista

On Sun, Mar 3, 2019 at 6:20 AM John Kane via R-help
<r-help at r-project.org> wrote:
>
>
>? NOTE. This is a re-post of a message of Saturday 2018-03-03 sent with an incorrect header.
>
> To upgrade to R.3.5.2 from 3.4.4 I have been following the instructions at https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/ .
>
> I seem to have the repository properly connected and verified.
>
> I am getting an error: --------------------------------------------------------------------------------------------------- john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help resolve the situation:
>
> The following packages have unmet dependencies: r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable Recommends: r-base-dev but it is not going to be installed E: Unable to correct problems, you have held broken packages. ------------------------------------------------------------------------------------------------- Earlier I was getting another error r-base-core : Depends: libpng12-0 but it is not installable but that was cured by installing libpng12-0 via the Ubuntu Software app.
>
> I cannot find what appears to be an installable version of libreadlines6.
>
> Ubuntu reports that the up-to-date version of libreadlines7.
>
> Should I consider this an R issue or an Ubuntu issue?
>
> In any case, has anyone encountered the problem and beaten it?
>
> Thanks
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.    
	[[alternative HTML version deleted]]


From phii m@iii@g oii phiiipsmith@c@  Mon Mar  4 04:14:55 2019
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Sun, 03 Mar 2019 22:14:55 -0500
Subject: [R] Tidyverse data frame conversion from monthly to annual
Message-ID: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>

I have a data frame in which the first column is a sequence of monthly 
dates and the other columns are variables. There are a great many 
variables. I want to create another data frame similar to the first one, 
but with annual values instead of monthly, created by summing the months 
within each year.

I am able to do this as shown in this reprex:

library(tidyverse)
REF_DATE <- seq(as.Date("2000/1/1"),by="month",length.out=36)
set.seed(57)
df <- data.frame(REF_DATE,
                  x=sample(1:100,size=36),
                  y=sample(1:100,size=36),
                  z=sample(1:100,size=36),
                  Year=year(REF_DATE))
df1 <- df %>%
   group_by(Year) %>%
   summarise(x_a=sum(x),y_a=sum(y),z_a=sum(z)) %>%
   ungroup()

However, while this works for the simple case with only three variables, 
I actually have many more than three, so I am looking for a more general 
approach. I have no clue as to how to proceed. Any advice will be much 
appreciated.

Philip


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Mar  4 04:21:38 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 03 Mar 2019 19:21:38 -0800
Subject: [R] Tidyverse data frame conversion from monthly to annual
In-Reply-To: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>
References: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>
Message-ID: <EC997F64-9572-41EC-AF9B-F7FE7134A5A0@dcn.davis.ca.us>

?summarise_all

See the examples.

On March 3, 2019 7:14:55 PM PST, phil at philipsmith.ca wrote:
>I have a data frame in which the first column is a sequence of monthly 
>dates and the other columns are variables. There are a great many 
>variables. I want to create another data frame similar to the first
>one, 
>but with annual values instead of monthly, created by summing the
>months 
>within each year.
>
>I am able to do this as shown in this reprex:
>
>library(tidyverse)
>REF_DATE <- seq(as.Date("2000/1/1"),by="month",length.out=36)
>set.seed(57)
>df <- data.frame(REF_DATE,
>                  x=sample(1:100,size=36),
>                  y=sample(1:100,size=36),
>                  z=sample(1:100,size=36),
>                  Year=year(REF_DATE))
>df1 <- df %>%
>   group_by(Year) %>%
>   summarise(x_a=sum(x),y_a=sum(y),z_a=sum(z)) %>%
>   ungroup()
>
>However, while this works for the simple case with only three
>variables, 
>I actually have many more than three, so I am looking for a more
>general 
>approach. I have no clue as to how to proceed. Any advice will be much 
>appreciated.
>
>Philip
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From pro|@@m|t@m|tt@| @end|ng |rom gm@||@com  Mon Mar  4 04:24:19 2019
From: pro|@@m|t@m|tt@| @end|ng |rom gm@||@com (Amit Mittal)
Date: Mon, 4 Mar 2019 08:54:19 +0530
Subject: [R] Tidyverse data frame conversion from monthly to annual
In-Reply-To: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>
References: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>
Message-ID: <5c7c9a63.1c69fb81.ae13f.07a7@mx.google.com>

Try using time series objects in xts. Should be easy. At worst once the xts object is automatically arranged by date you have to note the row numbers for each year period and I think data would be ok to handle unless it is more than 30-40 years when you would look up other xts options. Xts objects would automatically take the date column and use them as row ids so there is obviously another simple loop you can generate for summing up . There would also be easy functions once you start with xts. 


Best Regards
Amit
+91 7899381263






Please request Skype as available?
5th Year FPM (Ph.D.) in Finance and Accounting Area
Indian Institute of Management, Lucknow, (U.P.) 226013 India
http://bit.ly/2A2PhD
AEA Job profile : http://bit.ly/AEAamit
FMA 2 page profile : http://bit.ly/FMApdf2p
SSRN top10% downloaded since July 2017:?http://ssrn.com/author=2665511

From: phil at philipsmith.ca
Sent: 04 March 2019 08:45
To: r-help at r-project.org
Subject: [R] Tidyverse data frame conversion from monthly to annual

I have a data frame in which the first column is a sequence of monthly 
dates and the other columns are variables. There are a great many 
variables. I want to create another data frame similar to the first one, 
but with annual values instead of monthly, created by summing the months 
within each year.

I am able to do this as shown in this reprex:

library(tidyverse)
REF_DATE <- seq(as.Date("2000/1/1"),by="month",length.out=36)
set.seed(57)
df <- data.frame(REF_DATE,
                  x=sample(1:100,size=36),
                  y=sample(1:100,size=36),
                  z=sample(1:100,size=36),
                  Year=year(REF_DATE))
df1 <- df %>%
   group_by(Year) %>%
   summarise(x_a=sum(x),y_a=sum(y),z_a=sum(z)) %>%
   ungroup()

However, while this works for the simple case with only three variables, 
I actually have many more than three, so I am looking for a more general 
approach. I have no clue as to how to proceed. Any advice will be much 
appreciated.

Philip

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From phii m@iii@g oii phiiipsmith@c@  Mon Mar  4 04:38:19 2019
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Sun, 03 Mar 2019 22:38:19 -0500
Subject: [R] Tidyverse data frame conversion from monthly to annual
In-Reply-To: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>
References: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>
Message-ID: <374a2fa728f6f75fe574b6464c455ef9@philipsmith.ca>

summarise_all() does the trick. Thanks very much for the help.

Philip

On 2019-03-03 22:14, phil at philipsmith.ca wrote:
> I have a data frame in which the first column is a sequence of monthly
> dates and the other columns are variables. There are a great many
> variables. I want to create another data frame similar to the first
> one, but with annual values instead of monthly, created by summing the
> months within each year.
> 
> I am able to do this as shown in this reprex:
> 
> library(tidyverse)
> REF_DATE <- seq(as.Date("2000/1/1"),by="month",length.out=36)
> set.seed(57)
> df <- data.frame(REF_DATE,
>                  x=sample(1:100,size=36),
>                  y=sample(1:100,size=36),
>                  z=sample(1:100,size=36),
>                  Year=year(REF_DATE))
> df1 <- df %>%
>   group_by(Year) %>%
>   summarise(x_a=sum(x),y_a=sum(y),z_a=sum(z)) %>%
>   ungroup()
> 
> However, while this works for the simple case with only three
> variables, I actually have many more than three, so I am looking for a
> more general approach. I have no clue as to how to proceed. Any advice
> will be much appreciated.
> 
> Philip


From oreg @end|ng |rom huj|@@c@||  Sun Mar  3 10:30:45 2019
From: oreg @end|ng |rom huj|@@c@|| (Shaul Oreg)
Date: Sun, 3 Mar 2019 11:30:45 +0200
Subject: [R] Multilevel moderated mediation
Message-ID: <7aec766e7c0c9356f07df9e4ee094b2d@mail.gmail.com>

Hello,



I am trying to run a *multilevel moderated mediation model in R*, with data
nested in three levels (children, within classes, within schools). All of
my variables are at the individual level, but I still need to account for
the nested nature of the data.

In separate analyses of mediation (using the mediation package) and of
moderation (using the lme4 package) I find evidence for indirect effects of
the predictor on the outcome, and evidence that my moderator moderates the
effects of the predictor on the mediator. But I?d also like to test if the
moderator moderates the indirect effect of the predictor on the outcome. Is
there a way to do this in R?



Thank you,



Shaul Oreg

	[[alternative HTML version deleted]]


From @@u|@we@ver@70 @end|ng |rom gm@||@com  Sun Mar  3 11:19:49 2019
From: @@u|@we@ver@70 @end|ng |rom gm@||@com (Saul Weaver)
Date: Sun, 3 Mar 2019 12:19:49 +0200
Subject: [R] Multilevel models
Message-ID: <CAAxD7FdH9R5RuGo5WQ33W5M=PJC5Dn6xvDJAVgKH9x2gstz5eg@mail.gmail.com>

Hello,

I have data with workers within departments. I am interested in testing the
effects of peers' satisfaction on employees' productivity. To assess peer
satisfaction, I calculate, for each employee, the average satisfaction of
the employees' peers within the department. In other words, I calculate the
average satisfaction in the department, while excluding the focal employee.
I'm not sure about the level of this variable, because on the one hand, it
is unique for each employee, but on the other hand, the values of this
variable across employees are not independent of each other. How would I
account for this issue in R?

Thank you,

S Weaver

	[[alternative HTML version deleted]]


From jrkr|de@u @end|ng |rom gm@||@com  Sun Mar  3 14:10:58 2019
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Sun, 3 Mar 2019 08:10:58 -0500
Subject: [R] R installation Ubuntu 18.04 missing dependency libreadline6
In-Reply-To: <77e2b9a1-1ebd-88a2-5a4d-baed65136b67@sapo.pt>
References: <1289123265.8389549.1551612033163.ref@mail.yahoo.com>
 <1289123265.8389549.1551612033163@mail.yahoo.com>
 <77e2b9a1-1ebd-88a2-5a4d-baed65136b67@sapo.pt>
Message-ID: <CAKZQJMDYicRdcgW2LHQLcnkYjxwdas7aqRgOYD3huyRpxuuC_Q@mail.gmail.com>

Thanks Rui,
Depending on how I try to install I get a different error. I am
starting to think that somehow the Ubuntu installation may be faulty.
I used a ISO that I had burned about a year ago.  I had no problem
then with a shared installation but who knows.

sudo apt-get update
sudo apt-get install r-base

results in
------------------------------------------------------------------------------------
john at jonh-T510:~$ sudo apt-get install r-base
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help resolve the situation:

The following packages have unmet dependencies:
 r-base : Depends: r-base-core (>= 3.5.2-1xenial) but it is not going
to be installed
          Depends: r-recommended (= 3.5.2-1xenial) but it is not going
to be installed
          Recommends: r-base-html but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
------------------------------------------------------------------------------

I downloaded the R-3.5.2.tar.gz file and tried compiling.
Results
------------------------------------------------------------------------------
 checking whether PCRE support suffices... yes
checking for pcre2-config... no
checking for curl-config... no
checking curl/curl.h usability... no
checking curl/curl.h presence... no
checking for curl/curl.h... no
configure: error: libcurl >= 7.22.0 library and headers are required
with support for https
-------------------------------------------------------------------------------
Symantic seems to feel that libcurs3  7.58.0-2ubuntu2 and
libcurl3-gnutls 7.58.0-2ubuntu3.6 are installed. Maybe I need to
install a few more flavours?

On Sun, 3 Mar 2019 at 07:33, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> I never installed R on Ubuntu like you did it.
> I follow this:
>
> sudo apt-get update
> sudo apt-get install r-base
> sudo apt-get install r-base-dev
>
>
> And it works at the first try. See [1]
> The problems I [always] have are with missing Ubuntu libs needed by
> contributed packages, not with base R.
>
> [1] https://cran.r-project.org/bin/linux/ubuntu/README
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 11:20 de 03/03/2019, John Kane via R-help escreveu:
> >
> >   NOTE. This is a re-post of a message of Saturday 2018-03-03 sent with an incorrect header.
> >
> > To upgrade to R.3.5.2 from 3.4.4 I have been following the instructions at https://www.r-bloggers.com/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/ .
> >
> > I seem to have the repository properly connected and verified.
> >
> > I am getting an error: --------------------------------------------------------------------------------------------------- john at jonh-T510:~$ sudo apt install r-base r-base-core r-recommended Reading package lists... Done Building dependency tree Reading state information... Done Some packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help resolve the situation:
> >
> > The following packages have unmet dependencies: r-base-core : Depends: libreadline6 (>= 6.0) but it is not installable Recommends: r-base-dev but it is not going to be installed E: Unable to correct problems, you have held broken packages. ------------------------------------------------------------------------------------------------- Earlier I was getting another error r-base-core : Depends: libpng12-0 but it is not installable but that was cured by installing libpng12-0 via the Ubuntu Software app.
> >
> > I cannot find what appears to be an installable version of libreadlines6.
> >
> > Ubuntu reports that the up-to-date version of libreadlines7.
> >
> > Should I consider this an R issue or an Ubuntu issue?
> >
> > In any case, has anyone encountered the problem and beaten it?
> >
> > Thanks
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From petr@p|k@| @end|ng |rom prechez@@cz  Mon Mar  4 08:23:29 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 4 Mar 2019 07:23:29 +0000
Subject: [R] Tidyverse data frame conversion from monthly to annual
In-Reply-To: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>
References: <7bc154f23968b20a3055c8a34160a4f9@philipsmith.ca>
Message-ID: <362c0a681d5a412087162aec3f41789d@SRVEXCHCM1302.precheza.cz>

Hi

aggregate is another option.

aggregate(df[, 2:4], list(df$Year), sum)

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of
> phil at philipsmith.ca
> Sent: Monday, March 4, 2019 4:15 AM
> To: r-help at r-project.org
> Subject: [R] Tidyverse data frame conversion from monthly to annual
>
> I have a data frame in which the first column is a sequence of monthly dates
> and the other columns are variables. There are a great many variables. I want
> to create another data frame similar to the first one, but with annual values
> instead of monthly, created by summing the months within each year.
>
> I am able to do this as shown in this reprex:
>
> library(tidyverse)
> REF_DATE <- seq(as.Date("2000/1/1"),by="month",length.out=36)
> set.seed(57)
> df <- data.frame(REF_DATE,
>                   x=sample(1:100,size=36),
>                   y=sample(1:100,size=36),
>                   z=sample(1:100,size=36),
>                   Year=year(REF_DATE))
> df1 <- df %>%
>    group_by(Year) %>%
>    summarise(x_a=sum(x),y_a=sum(y),z_a=sum(z)) %>%
>    ungroup()
>
> However, while this works for the simple case with only three variables, I
> actually have many more than three, so I am looking for a more general
> approach. I have no clue as to how to proceed. Any advice will be much
> appreciated.
>
> Philip
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From j|ox @end|ng |rom mcm@@ter@c@  Mon Mar  4 14:45:47 2019
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Mon, 4 Mar 2019 13:45:47 +0000
Subject: [R] Multilevel models
In-Reply-To: <20657_1551672062_x24412EW029184_CAAxD7FdH9R5RuGo5WQ33W5M=PJC5Dn6xvDJAVgKH9x2gstz5eg@mail.gmail.com>
References: <20657_1551672062_x24412EW029184_CAAxD7FdH9R5RuGo5WQ33W5M=PJC5Dn6xvDJAVgKH9x2gstz5eg@mail.gmail.com>
Message-ID: <A9EF0A15-83F7-4EF9-A274-290933309EF0@mcmaster.ca>

Dear Saul,

The most commonly used mixed-effect models software in R, in the lme4 and nlme packages, use the Laird-Ware form of the model, which isn't explicitly hierarchical. That is, higher-level variables are simply invariant within groups and appear in the model formula in the same manner as individual-level variables. So there's no problem -- just specify the model as you normally would.

By the way, you're more likely to get responses about mixed models if you post to the R-sig-mixed-models list <https://stat.ethz.ch/mailman/listinfo/r-sig-mixed-models> rather than to the more general R-help list.

I hope this helps,
 John

  -------------------------------------------------
  John Fox, Professor Emeritus
  McMaster University
  Hamilton, Ontario, Canada
  Web: http::/socserv.mcmaster.ca/jfox

> On Mar 3, 2019, at 5:19 AM, Saul Weaver <saul.weaver.70 at gmail.com> wrote:
> 
> Hello,
> 
> I have data with workers within departments. I am interested in testing the
> effects of peers' satisfaction on employees' productivity. To assess peer
> satisfaction, I calculate, for each employee, the average satisfaction of
> the employees' peers within the department. In other words, I calculate the
> average satisfaction in the department, while excluding the focal employee.
> I'm not sure about the level of this variable, because on the one hand, it
> is unique for each employee, but on the other hand, the values of this
> variable across employees are not independent of each other. How would I
> account for this issue in R?
> 
> Thank you,
> 
> S Weaver
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@rce|o|@|@ @end|ng |rom gm@||@com  Mon Mar  4 18:41:29 2019
From: m@rce|o|@|@ @end|ng |rom gm@||@com (Marcelo Laia)
Date: Mon, 4 Mar 2019 14:41:29 -0300
Subject: [R] Error in Cairo::Cairo(file = imgName, unit = "in", dpi = dpi,
 width = w, : Failed to create Cairo backend!
Message-ID: <20190304174129.GE21454@localhost>

Hi,

I'm trying to do a MetaboAnalystR [1]'s analysis with a large dataset. All works
great except PlotHeatMap function. This functions plot two type of image output:
"overview" and "detail". In "overview" mode, we can do plot the image in png or
pdf. However, in this mode, we do not could see the heatmap genes labels [2]. If
I try to "detail" mode, in pdf graphics device, an output image [3] is
generated. However, it wasn't opened in acroread, evince. It is only viewed in
xpdf. If I try to "detail" mode, in png graphics device, I got the error:

Error in Cairo::Cairo(file = imgName, unit = "in", dpi = dpi, width = w, :
Failed to create Cairo backend!

I figured out that this error is not MetaboAnalystR related. Maybe it is related
with Cairo package/library.

Someone already/yet having had this issue? Are there workaround for that?

Best Wishes!

-- 
Marcelo


From m@rce|o|@|@ @end|ng |rom gm@||@com  Mon Mar  4 19:25:15 2019
From: m@rce|o|@|@ @end|ng |rom gm@||@com (Marcelo Laia)
Date: Mon, 4 Mar 2019 15:25:15 -0300
Subject: [R] 
 Error in Cairo::Cairo(file = imgName, unit = "in", dpi = dpi,
 width = w, : Failed to create Cairo backend!
In-Reply-To: <20190304174129.GE21454@localhost>
References: <20190304174129.GE21454@localhost>
Message-ID: <20190304182515.GF21454@localhost>

1. https://github.com/jsychong/MetaboAnalystR
2. https://www.dropbox.com/s/rchhnjg0gziwr2l/heatmap_1_dpi72.png?dl=0
3. https://www.dropbox.com/s/rchhnjg0gziwr2l/heatmap_1_dpi72.png?dl=0

On 04/03/19 at 02:41, Marcelo Laia wrote:
> Hi,
> 
> I'm trying to do a MetaboAnalystR [1]'s analysis with a large dataset. All works
> great except PlotHeatMap function. This functions plot two type of image output:
> "overview" and "detail". In "overview" mode, we can do plot the image in png or
> pdf. However, in this mode, we do not could see the heatmap genes labels [2]. If
> I try to "detail" mode, in pdf graphics device, an output image [3] is
> generated. However, it wasn't opened in acroread, evince. It is only viewed in
> xpdf. If I try to "detail" mode, in png graphics device, I got the error:
> 
> Error in Cairo::Cairo(file = imgName, unit = "in", dpi = dpi, width = w, :
> Failed to create Cairo backend!
> 
> I figured out that this error is not MetaboAnalystR related. Maybe it is related
> with Cairo package/library.
> 
> Someone already/yet having had this issue? Are there workaround for that?
> 
> Best Wishes!
> 
> -- 
> Marcelo


-- 
Marcelo


From p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz  Tue Mar  5 01:58:26 2019
From: p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz (Paul Murrell)
Date: Tue, 5 Mar 2019 13:58:26 +1300
Subject: [R] [FORGED] R cairo_pdf function does not respect plotting
 boundaries
In-Reply-To: <CAO0anGmxEwS2VtAqT-i6701QrkFCL90cOneB+c_y3uJ_40aG9w@mail.gmail.com>
References: <CAO0anGmxEwS2VtAqT-i6701QrkFCL90cOneB+c_y3uJ_40aG9w@mail.gmail.com>
Message-ID: <581bf016-0b51-977b-200d-bbeef95f9046@stat.auckland.ac.nz>

Hi

(cc'ed to r-devel where further discussion should probably take place)

Thanks Lee.  I see that problem.

There is a "+ 1" in the Cairo device code for setting the clipping region
(https://github.com/wch/r-source/blob/ba600867f2a94e46cf9eb75dc8b37f12b08a4561/src/library/grDevices/src/cairo/cairoFns.c#L156) 


Remove the "+ 1" and the problem goes away (for your example at least).

The comment on the line above that code suggests that the "+ 1" was 
modelled on the X11 device code, but X11 deals in integer pixels and 
Cairo (at the API level) does not, so it would seem that the "+ 1" is 
just unnecessary.

However, I have a slight nagging worry that we have been here before, so 
I would like to do some more testing before committing that change.

Paul

On 1/03/19 8:13 AM, Lee Steven Kelvin wrote:
> Hello all,
> 
> When producing a plot in R using the cairo_pdf device, the resultant plot does not respect the plotting boundaries. Lines and shaded regions will spill over the lower x-axis and the right-side y-axis (sides 1 and 4). I would like to know if it is possible to fix this behaviour when using 'cairo_pdf' in R?
> 
> As an example, see the image at this web link: https://i.stack.imgur.com/0lfZd.png
> 
> This image is a screenshot of a PDF file constructed using the following minimum working example code:
> 
> cairo_pdf(file="test.pdf", width=0.5, height=0.5)
> par("mar"=c(0.25,0.25,0.25,0.25))
> plot(NA, xlim=c(0,1), ylim=c(0,1), axes=FALSE)
> polygon(x=c(-1,-1,2,2), y=c(-1,2,2,-1), density=5, col="green3", lwd=10)
> abline(h=0.25, col="red", lwd=5)
> abline(h=0.75, col="hotpink", lwd=5, lend=1)
> abline(v=0.25, col="blue", lwd=5)
> abline(v=0.75, col="cyan", lwd=5, lend=1)
> box()
> dev.off()
> 
> Here I'm plotting a shaded region in green using 'polygon', with boundaries that lie outside the plot. I'm also drawing two sets of horizontal/vertical lines using 'abline'. The first in each pair uses standard rounded line caps, whilst the second in each pair uses butt line caps.
> 
> As you can see, the shading lines and the default rounded-end ablines all extend beyond the plotting region along the lower and right-hand side axes. Only when using 'lend=1' am I able to contain the ablines to the plotting region. I know of no such fix for the shading lines however.
> 
> I would naively expect the R plotting region to be respected, and for it to be impossible to plot outside of this region unless explicitly specified by the user.
> 
> I have tested this on the other cairo devices (SVG and PS), and also reproduce the same behaviour, indicating that this is an issue with the cairo graphics API, or its implementation within R.
> 
> This behaviour does not occur when using the standard R 'pdf' graphics device. I would switch to 'pdf' in general, however, 'cairo_pdf' has several advantages over 'pdf', notably, reduced output file sizes on occasion and support for a larger array of UTF-8 characters, so ideally I would prefer to use cairo_pdf.
> 
> I should note that I have also posted this message on StackOverflow at this web link: https://stackoverflow.com/questions/54892809/r-cairo-pdf-function-does-not-respect-plotting-boundaries
> 
> Thank you in advance for any insights into this issue.
> 
> Sincerely,
> Lee Kelvin
> 
> 
> 
> --
> Dr Lee Kelvin
> Department of Physics
> UC Davis
> One Shields Avenue
> Davis, CA 95616
> USA
> 
> Ph: +1 (530) 752-1500
> Fax: +1 (530) 752-4717
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From |@ke|v|n @end|ng |rom ucd@v|@@edu  Tue Mar  5 08:22:29 2019
From: |@ke|v|n @end|ng |rom ucd@v|@@edu (Lee Steven Kelvin)
Date: Tue, 5 Mar 2019 07:22:29 +0000
Subject: [R] R cairo_pdf function does not respect plotting boundaries
In-Reply-To: <581bf016-0b51-977b-200d-bbeef95f9046@stat.auckland.ac.nz>
References: <CAO0anGmxEwS2VtAqT-i6701QrkFCL90cOneB+c_y3uJ_40aG9w@mail.gmail.com>
 <581bf016-0b51-977b-200d-bbeef95f9046@stat.auckland.ac.nz>
Message-ID: <CAO0anGmLgRSFJWaRmYbAnKL-8EwaZXyUjBCu+dBnLMV39wTwGA@mail.gmail.com>

Hi Paul,

Great, thank you for looking in to this, and I'm glad that you're able to reproduce it at your end too.

From your reply, I'm happy that it seems like the fix may be fairly trivial, but I understand the necessity for caution.

If there's anything else I can do to help, please do let me know.

Thank you again,
Best,
Lee


On Monday, 4 March 2019, Paul Murrell <paul at stat.auckland.ac.nz<mailto:paul at stat.auckland.ac.nz>> wrote:
Hi

(cc'ed to r-devel where further discussion should probably take place)

Thanks Lee.  I see that problem.

There is a "+ 1" in the Cairo device code for setting the clipping region
(https://github.com/wch/r-source/blob/ba600867f2a94e46cf9eb75dc8b37f12b08a4561/src/library/grDevices/src/cairo/cairoFns.c#L156)

Remove the "+ 1" and the problem goes away (for your example at least).

The comment on the line above that code suggests that the "+ 1" was modelled on the X11 device code, but X11 deals in integer pixels and Cairo (at the API level) does not, so it would seem that the "+ 1" is just unnecessary.

However, I have a slight nagging worry that we have been here before, so I would like to do some more testing before committing that change.

Paul

On 1/03/19 8:13 AM, Lee Steven Kelvin wrote:
Hello all,

When producing a plot in R using the cairo_pdf device, the resultant plot does not respect the plotting boundaries. Lines and shaded regions will spill over the lower x-axis and the right-side y-axis (sides 1 and 4). I would like to know if it is possible to fix this behaviour when using 'cairo_pdf' in R?

As an example, see the image at this web link: https://i.stack.imgur.com/0lfZd.png

This image is a screenshot of a PDF file constructed using the following minimum working example code:

cairo_pdf(file="test.pdf", width=0.5, height=0.5)
par("mar"=c(0.25,0.25,0.25,0.25))
plot(NA, xlim=c(0,1), ylim=c(0,1), axes=FALSE)
polygon(x=c(-1,-1,2,2), y=c(-1,2,2,-1), density=5, col="green3", lwd=10)
abline(h=0.25, col="red", lwd=5)
abline(h=0.75, col="hotpink", lwd=5, lend=1)
abline(v=0.25, col="blue", lwd=5)
abline(v=0.75, col="cyan", lwd=5, lend=1)
box()
dev.off()

Here I'm plotting a shaded region in green using 'polygon', with boundaries that lie outside the plot. I'm also drawing two sets of horizontal/vertical lines using 'abline'. The first in each pair uses standard rounded line caps, whilst the second in each pair uses butt line caps.

As you can see, the shading lines and the default rounded-end ablines all extend beyond the plotting region along the lower and right-hand side axes. Only when using 'lend=1' am I able to contain the ablines to the plotting region. I know of no such fix for the shading lines however.

I would naively expect the R plotting region to be respected, and for it to be impossible to plot outside of this region unless explicitly specified by the user.

I have tested this on the other cairo devices (SVG and PS), and also reproduce the same behaviour, indicating that this is an issue with the cairo graphics API, or its implementation within R.

This behaviour does not occur when using the standard R 'pdf' graphics device. I would switch to 'pdf' in general, however, 'cairo_pdf' has several advantages over 'pdf', notably, reduced output file sizes on occasion and support for a larger array of UTF-8 characters, so ideally I would prefer to use cairo_pdf.

I should note that I have also posted this message on StackOverflow at this web link: https://stackoverflow.com/questions/54892809/r-cairo-pdf-function-does-not-respect-plotting-boundaries

Thank you in advance for any insights into this issue.

Sincerely,
Lee Kelvin



--
Dr Lee Kelvin
Department of Physics
UC Davis
One Shields Avenue
Davis, CA 95616
USA

Ph: +1 (530) 752-1500
Fax: +1 (530) 752-4717


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


--
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz<mailto:paul at stat.auckland.ac.nz>
http://www.stat.auckland.ac.nz/~paul/


--
Dr Lee Kelvin
Department of Physics
UC Davis
One Shields Avenue
Davis, CA 95616
USA

Ph: +1 (530) 752-1500
Fax: +1 (530) 752-4717


	[[alternative HTML version deleted]]


From e@@w|ek @end|ng |rom gm@||@com  Tue Mar  5 17:18:20 2019
From: e@@w|ek @end|ng |rom gm@||@com (Ek Esawi)
Date: Tue, 5 Mar 2019 11:18:20 -0500
Subject: [R] Function doesn't work inside loop but works outside
Message-ID: <CA+ZkTxvnPKAzn0EFYfyLA4HB6Xyw8iQ-mSz8bpznTRP=KJthrA@mail.gmail.com>

Hi All,

I am using xlsx package to extract and clean data from an Excel
Workbook. I ran into a strange behavior that I don?t understand. The
gsub doesn?t work inside the loop but does outside the loop as shown
on my code.. Tried to Google for help but nothing came up.

My code loads and reads data from sheets in the workbook as a list of
data frames and assign them names. I wanted to replace the numbers
with spaces inside each part of the description column on each data
frame using gsub.

Example data:
Date        description       number
12/12/12  AAAA234BBB    1
1/3/12      cccc65bb35ff      2
2/7/13      234abababab     3

I want to have the description column to be like this.
               AAAA BBB
               Cccc bb ff
                  abababab

My code

MyFile <- "C:/Users/name/Documents/Testing2.xlsx"
MyWBook <- loadWorkbook(MyFile)
MySNames <- list(names(getSheets (MyWBook)))
NumSheets <- length(getSheets(MyWBook))

for (i in 1:NumSheets) {
  MySNames[[i]]
<-read.xlsx(MyFile,i,as.data.frame=TRUE,header=TRUE,keepFormulas=FALSE,stringsAsFactors=FALSE)
  gsub("'|-|[0-9]","",MySNames[[i]]$Description)
}

The gsub function above doesn?t work, but when I tried the function
outside the loops, as shown below, it worked.
gsub("'|-|[0-9]","",MySNames[[2]]$Description)


Thanks  in advance--EK


From pd@|gd @end|ng |rom gm@||@com  Tue Mar  5 17:36:54 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Tue, 5 Mar 2019 17:36:54 +0100
Subject: [R] Function doesn't work inside loop but works outside
In-Reply-To: <CA+ZkTxvnPKAzn0EFYfyLA4HB6Xyw8iQ-mSz8bpznTRP=KJthrA@mail.gmail.com>
References: <CA+ZkTxvnPKAzn0EFYfyLA4HB6Xyw8iQ-mSz8bpznTRP=KJthrA@mail.gmail.com>
Message-ID: <3E9DA29C-FAD5-42DC-9471-2FBE43517FC1@gmail.com>

You need a print() around the gsub(...) when inside a loop.

-pd

> On 5 Mar 2019, at 17:18 , Ek Esawi <esawiek at gmail.com> wrote:
> 
> Hi All,
> 
> I am using xlsx package to extract and clean data from an Excel
> Workbook. I ran into a strange behavior that I don?t understand. The
> gsub doesn?t work inside the loop but does outside the loop as shown
> on my code.. Tried to Google for help but nothing came up.
> 
> My code loads and reads data from sheets in the workbook as a list of
> data frames and assign them names. I wanted to replace the numbers
> with spaces inside each part of the description column on each data
> frame using gsub.
> 
> Example data:
> Date        description       number
> 12/12/12  AAAA234BBB    1
> 1/3/12      cccc65bb35ff      2
> 2/7/13      234abababab     3
> 
> I want to have the description column to be like this.
>               AAAA BBB
>               Cccc bb ff
>                  abababab
> 
> My code
> 
> MyFile <- "C:/Users/name/Documents/Testing2.xlsx"
> MyWBook <- loadWorkbook(MyFile)
> MySNames <- list(names(getSheets (MyWBook)))
> NumSheets <- length(getSheets(MyWBook))
> 
> for (i in 1:NumSheets) {
>  MySNames[[i]]
> <-read.xlsx(MyFile,i,as.data.frame=TRUE,header=TRUE,keepFormulas=FALSE,stringsAsFactors=FALSE)
>  gsub("'|-|[0-9]","",MySNames[[i]]$Description)
> }
> 
> The gsub function above doesn?t work, but when I tried the function
> outside the loops, as shown below, it worked.
> gsub("'|-|[0-9]","",MySNames[[2]]$Description)
> 
> 
> Thanks  in advance--EK
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From e@@w|ek @end|ng |rom gm@||@com  Tue Mar  5 17:49:15 2019
From: e@@w|ek @end|ng |rom gm@||@com (Ek Esawi)
Date: Tue, 5 Mar 2019 11:49:15 -0500
Subject: [R] Function doesn't work inside loop but works outside
In-Reply-To: <3E9DA29C-FAD5-42DC-9471-2FBE43517FC1@gmail.com>
References: <CA+ZkTxvnPKAzn0EFYfyLA4HB6Xyw8iQ-mSz8bpznTRP=KJthrA@mail.gmail.com>
 <3E9DA29C-FAD5-42DC-9471-2FBE43517FC1@gmail.com>
Message-ID: <CA+ZkTxtQUw34vhnu5vbDUp-RtfKztmFf=yWRpQgURDUY1NUEiw@mail.gmail.com>

Thank you Peter. That's a dumb question on my part! At least i should
have known that i need an assignment statement.

Thanks again--EK

On Tue, Mar 5, 2019 at 11:36 AM peter dalgaard <pdalgd at gmail.com> wrote:
>
> You need a print() around the gsub(...) when inside a loop.
>
> -pd
>
> > On 5 Mar 2019, at 17:18 , Ek Esawi <esawiek at gmail.com> wrote:
> >
> > Hi All,
> >
> > I am using xlsx package to extract and clean data from an Excel
> > Workbook. I ran into a strange behavior that I don?t understand. The
> > gsub doesn?t work inside the loop but does outside the loop as shown
> > on my code.. Tried to Google for help but nothing came up.
> >
> > My code loads and reads data from sheets in the workbook as a list of
> > data frames and assign them names. I wanted to replace the numbers
> > with spaces inside each part of the description column on each data
> > frame using gsub.
> >
> > Example data:
> > Date        description       number
> > 12/12/12  AAAA234BBB    1
> > 1/3/12      cccc65bb35ff      2
> > 2/7/13      234abababab     3
> >
> > I want to have the description column to be like this.
> >               AAAA BBB
> >               Cccc bb ff
> >                  abababab
> >
> > My code
> >
> > MyFile <- "C:/Users/name/Documents/Testing2.xlsx"
> > MyWBook <- loadWorkbook(MyFile)
> > MySNames <- list(names(getSheets (MyWBook)))
> > NumSheets <- length(getSheets(MyWBook))
> >
> > for (i in 1:NumSheets) {
> >  MySNames[[i]]
> > <-read.xlsx(MyFile,i,as.data.frame=TRUE,header=TRUE,keepFormulas=FALSE,stringsAsFactors=FALSE)
> >  gsub("'|-|[0-9]","",MySNames[[i]]$Description)
> > }
> >
> > The gsub function above doesn?t work, but when I tried the function
> > outside the loops, as shown below, it worked.
> > gsub("'|-|[0-9]","",MySNames[[2]]$Description)
> >
> >
> > Thanks  in advance--EK
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>


From k@m@||k@r@y333 @end|ng |rom gm@||@com  Tue Mar  5 19:04:41 2019
From: k@m@||k@r@y333 @end|ng |rom gm@||@com (Kamalika Ray)
Date: Tue, 5 Mar 2019 23:34:41 +0530
Subject: [R] Unable to install ggplot2
Message-ID: <CAFPXQ692erLUN76+2G3N43oWBtdTw27ScFHNO7A46T0oxyb1iQ@mail.gmail.com>

Hi,
 I have been trying to install the ggplot2 package but I am unable to do
so. My Mac OS version is 10.7.4 and I have downloaded the R-Studio-1.1.463.
I have attached the screenshot of the error message which appears.

Please help!

Thank you,
Kamalika
India

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2019-03-04 at 10.48.52 PM.png
Type: image/png
Size: 288891 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190305/f7349051/attachment.png>

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Mar  5 21:23:38 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 05 Mar 2019 12:23:38 -0800
Subject: [R] Unable to install ggplot2
In-Reply-To: <CAFPXQ692erLUN76+2G3N43oWBtdTw27ScFHNO7A46T0oxyb1iQ@mail.gmail.com>
References: <CAFPXQ692erLUN76+2G3N43oWBtdTw27ScFHNO7A46T0oxyb1iQ@mail.gmail.com>
Message-ID: <1CE82BEA-6A3A-403F-945B-26C48DB5B027@dcn.davis.ca.us>

Please post the text version of the error in the future... your picture is almost unreadable. Also, if it is actually important that you are using RStudio then your question probably doesn't belong here. Also, if the problem is a faulty contributed package then you will need to contact the package maintainer as the Posting Guide mentioned below says.

I don't know why the dependency is not being handled correctly, but my suggestion would be to install the rlang package first, and once that is installed try installing ggplot2. Read the errors... it says there is a problem with the rlang package.

On March 5, 2019 10:04:41 AM PST, Kamalika Ray <kamalikaray333 at gmail.com> wrote:
>Hi,
>I have been trying to install the ggplot2 package but I am unable to do
>so. My Mac OS version is 10.7.4 and I have downloaded the
>R-Studio-1.1.463.
>I have attached the screenshot of the error message which appears.
>
>Please help!
>
>Thank you,
>Kamalika
>India

-- 
Sent from my phone. Please excuse my brevity.


From |n@h @end|ng |rom b|@vt@edu  Wed Mar  6 03:26:47 2019
From: |n@h @end|ng |rom b|@vt@edu (Ina Hoeschele)
Date: Tue, 5 Mar 2019 21:26:47 -0500
Subject: [R] glmmPQL for logistic kernel machine regression model
Message-ID: <CAC6SyKSgmO_AEaExNMYdzwtp5t3hc2J7xJ2ZLOs6PY0F9Qy5Ww@mail.gmail.com>

 Hi, I am trying to fit a logistic kernel machine regression model (with a
Gaussian kernel) in R using glmmPQL. Does anyone have experience with this
and maybe can provide some example code?
Thanks, Ina

	[[alternative HTML version deleted]]


From e|@@eg@rrenc @end|ng |rom gm@||@com  Wed Mar  6 10:16:11 2019
From: e|@@eg@rrenc @end|ng |rom gm@||@com (David Bars)
Date: Wed, 6 Mar 2019 10:16:11 +0100
Subject: [R] Benjamini-Hochberg (BH) Q value (false discovery rate)
Message-ID: <CABsq2cn98puKEzWRePE1SpUFPWM-xur2OLv47mFxRY+_NC3-Vg@mail.gmail.com>

Dear everybody,

I'm using stats package (version 3.5.2) and in detail their p.adjust()
function in order to control the false discovery rate in multiple
comparisons. In particular I used the Benjamini-Hochberg method.

Nevertheless,  the help of the stats package indicates that the adjusting
method is based on:
Benjamini, Y., and Hochberg, Y. (1995). Controlling the false discovery
rate: a practical and powerful approach to multiple testing. Journal of the
Royal Statistical Society Series B, 57, 289?300.
http://www.jstor.org/stable/2346101.

But, in the function mentioned above (p.adjust()) there are no possibiity
to define the Q value (the false discovery rate). I understand that the Q
value is calculated automatically but through which method???

For example, in another package (sgof v.2.3 also deposited on CRAN)
indicates that the the false discovery rate is estimated by the method
developed by:

Dalmasso C, Broet P and Moreau T (2005). A simple procedure for estimating
the false discovery rate. *Bioinformatics* 21:660--668.


Many thanks for your help,

David Bars.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Mar  6 13:32:06 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 6 Mar 2019 12:32:06 +0000
Subject: [R] inconsistency in nls output....
Message-ID: <SL2P216MB009134B81754FD1516678F13C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members,
                             I have the following nls output:

 Formula: YLf13 ~ (d + e * ((XL)^(1/3)) + f * log(LM3 + 18.81))

Parameters:
    Estimate Std. Error t value Pr(>|t|)
d  5.892e-09  8.644e-10   6.817 2.06e-11 ***
e -6.585e-09  5.518e-10 -11.934  < 2e-16 ***
f  1.850e-10  2.295e-10   0.806     0.42
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 9.57e-10 on 677 degrees of freedom

Number of iterations to convergence: 2
Achieved convergence tolerance: 3.973e-08

------
Residual sum of squares: 6.2e-16

------
t-based confidence interval:
           2.5%         97.5%
d  4.195378e-09  7.589714e-09
e -7.668142e-09 -5.501342e-09
f -2.655647e-10  6.354852e-10

------
Correlation matrix:
           d             e             f
d  1.0000000 -6.202339e-01 -7.832539e-01
e -0.6202339  1.000000e+00 -2.127301e-05
f -0.7832539 -2.127301e-05  1.000000e+00


if I let XL = 1.1070513 and LM3 = 0.3919 , and consider the coeffs as given above, the right hand side of the above equation is negative.
But YLf13 is always positive! How is this possible? Am I interpreting the result of the nls output properly?  Should I interpret the coeffs differently? I have done hours of thinking over the above problem but couldn't find any results...

I cannot provide the full values of YLf13, XL and LM3 due to IPR issues....please cooperate......however, if the only way to solve the problem is to give these values, I would indeed give them.....

Also forgive me if there is a minor mistake in my calculations... or a typo....

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Mar  6 13:48:26 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 6 Mar 2019 12:48:26 +0000
Subject: [R] Fw: inconsistency in nls output....
In-Reply-To: <SL2P216MB009134B81754FD1516678F13C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB009134B81754FD1516678F13C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <SL2P216MB0091DE106FD2088939C0C5A6C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members,
                            with reference to the attached message:

I think I have found out the problem:
YLf13 has the structure:
YLf13 <- a*exp(-1000*LM1); LM1 is another vector.

most of the YLf13 vector is getting populated with zeros, I think, because of the very low value of exp(-1000*LM1). Is there any method in R wherein I can work with these very low values?

Or is the problem not related to the structure of YLf13?

very many thanks for your time and effort...
yours sincerely,
AKSHAY M KULKARNI


________________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
Sent: Wednesday, March 6, 2019 6:02 PM
To: R help Mailing  list
Subject: [R] inconsistency in nls output....

dear members,
                             I have the following nls output:

 Formula: YLf13 ~ (d + e * ((XL)^(1/3)) + f * log(LM3 + 18.81))

Parameters:
    Estimate Std. Error t value Pr(>|t|)
d  5.892e-09  8.644e-10   6.817 2.06e-11 ***
e -6.585e-09  5.518e-10 -11.934  < 2e-16 ***
f  1.850e-10  2.295e-10   0.806     0.42
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 9.57e-10 on 677 degrees of freedom

Number of iterations to convergence: 2
Achieved convergence tolerance: 3.973e-08

------
Residual sum of squares: 6.2e-16

------
t-based confidence interval:
           2.5%         97.5%
d  4.195378e-09  7.589714e-09
e -7.668142e-09 -5.501342e-09
f -2.655647e-10  6.354852e-10

------
Correlation matrix:
           d             e             f
d  1.0000000 -6.202339e-01 -7.832539e-01
e -0.6202339  1.000000e+00 -2.127301e-05
f -0.7832539 -2.127301e-05  1.000000e+00


if I let XL = 1.1070513 and LM3 = 0.3919 , and consider the coeffs as given above, the right hand side of the above equation is negative.
But YLf13 is always positive! How is this possible? Am I interpreting the result of the nls output properly?  Should I interpret the coeffs differently? I have done hours of thinking over the above problem but couldn't find any results...

I cannot provide the full values of YLf13, XL and LM3 due to IPR issues....please cooperate......however, if the only way to solve the problem is to give these values, I would indeed give them.....

Also forgive me if there is a minor mistake in my calculations... or a typo....

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ATT00001.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190306/72a93b6b/attachment.txt>

From dc@r|@on @end|ng |rom t@mu@edu  Wed Mar  6 16:36:49 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Wed, 6 Mar 2019 15:36:49 +0000
Subject: [R] Benjamini-Hochberg (BH) Q value (false discovery rate)
In-Reply-To: <CABsq2cn98puKEzWRePE1SpUFPWM-xur2OLv47mFxRY+_NC3-Vg@mail.gmail.com>
References: <CABsq2cn98puKEzWRePE1SpUFPWM-xur2OLv47mFxRY+_NC3-Vg@mail.gmail.com>
Message-ID: <c59ff93718a945f180a8c37e7a958798@tamu.edu>

R is open source so you can look at the code. 
In this case just typing the function name gets it:

> p.adjust
function (p, method = p.adjust.methods, n = length(p)) 
{
    method <- match.arg(method)
    if (method == "fdr") 
        method <- "BH"
    . . . . . Stuff deleted . . . .  
# The code for Benjamini and Hochberg:
    }, BH = {
        i <- lp:1L
        o <- order(p, decreasing = TRUE)
        ro <- order(o)
        pmin(1, cummin(n/i * p[o]))[ro]

# A quick example
p <- c(0.7723, 0.1992, 0.1821, 0.6947, 0.0932, 0.1484, 0.0006, 0.0004)
round(p.adjust(p, method="BH"), 4)
#  [1] 0.7723  0.2656  0.2656  0.7723  0.2485  0.2656  0.0024  0.0024

n <- length(p)
i <- n:1L
o <- order(p, decreasing=TRUE)    # Order from largest to smallest
ro <- order(o)     # Reverse the ordering to return the adjusted values
pmin(1, cummin(n/i * p[o]))[ro]   # Compute the adjusted value

The adjustment orders the p-values from largest to smallest and multiplies them by n/i, in this case

> n/i
[1] 1.000000 1.142857 1.333333 1.600000 2.000000 2.666667 4.000000 8.000000

So the largest is multiplied by 1 and the smallest by 8.

Then cummin() takes the cumulative minimum so that the multiplication does not change the decreasing order of the values. In this example the last value is the same as the second to last instead of 8 times the original value (.0032). The pmin() function ensures that the adjusted value never exceeds 1.

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of David Bars
Sent: Wednesday, March 6, 2019 3:16 AM
To: r-help at r-project.org
Subject: [R] Benjamini-Hochberg (BH) Q value (false discovery rate)

Dear everybody,

I'm using stats package (version 3.5.2) and in detail their p.adjust()
function in order to control the false discovery rate in multiple
comparisons. In particular I used the Benjamini-Hochberg method.

Nevertheless,  the help of the stats package indicates that the adjusting
method is based on:
Benjamini, Y., and Hochberg, Y. (1995). Controlling the false discovery
rate: a practical and powerful approach to multiple testing. Journal of the
Royal Statistical Society Series B, 57, 289?300.
http://www.jstor.org/stable/2346101.

But, in the function mentioned above (p.adjust()) there are no possibiity
to define the Q value (the false discovery rate). I understand that the Q
value is calculated automatically but through which method???

For example, in another package (sgof v.2.3 also deposited on CRAN)
indicates that the the false discovery rate is estimated by the method
developed by:

Dalmasso C, Broet P and Moreau T (2005). A simple procedure for estimating
the false discovery rate. *Bioinformatics* 21:660--668.


Many thanks for your help,

David Bars.

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From pro|jcn@@h @end|ng |rom gm@||@com  Wed Mar  6 18:10:59 2019
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 6 Mar 2019 12:10:59 -0500
Subject: [R] Fw: inconsistency in nls output....
In-Reply-To: <SL2P216MB0091DE106FD2088939C0C5A6C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB009134B81754FD1516678F13C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <SL2P216MB0091DE106FD2088939C0C5A6C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <940078a8-fade-3e1a-510a-e9cb178a4642@gmail.com>

nls() is a Model T Ford trying to drive on the Interstate. The code
is quite old and uses approximations that work well when the user
provides a reasonable problem, but in cases where there are mixed large
and small numbers like yours could get into trouble.

Duncan Murdoch and I prepared the nlsr package to address some
of the weaknesses (in particular we try to use analytic derivatives).

The output of nlsr also gives the singular values of the Jacobian, though
I suspect many R users will have to do some work to interpret those.

You haven't provided a reproducible example. That's almost always the
way to get definitive answers. Otherwise we're guessing as to the issue.

JN

On 2019-03-06 7:48 a.m., akshay kulkarni wrote:
> dear members,
>                             with reference to the attached message:
> 
> I think I have found out the problem:
> YLf13 has the structure:
> YLf13 <- a*exp(-1000*LM1); LM1 is another vector.
> 
> most of the YLf13 vector is getting populated with zeros, I think, because of the very low value of exp(-1000*LM1). Is there any method in R wherein I can work with these very low values?
> 
> Or is the problem not related to the structure of YLf13?
> 
> very many thanks for your time and effort...
> yours sincerely,
> AKSHAY M KULKARNI
> 
> 
> ________________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
> Sent: Wednesday, March 6, 2019 6:02 PM
> To: R help Mailing  list
> Subject: [R] inconsistency in nls output....
> 
> dear members,
>                              I have the following nls output:
> 
>  Formula: YLf13 ~ (d + e * ((XL)^(1/3)) + f * log(LM3 + 18.81))
> 
> Parameters:
>     Estimate Std. Error t value Pr(>|t|)
> d  5.892e-09  8.644e-10   6.817 2.06e-11 ***
> e -6.585e-09  5.518e-10 -11.934  < 2e-16 ***
> f  1.850e-10  2.295e-10   0.806     0.42
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 9.57e-10 on 677 degrees of freedom
> 
> Number of iterations to convergence: 2
> Achieved convergence tolerance: 3.973e-08
> 
> ------
> Residual sum of squares: 6.2e-16
> 
> ------
> t-based confidence interval:
>            2.5%         97.5%
> d  4.195378e-09  7.589714e-09
> e -7.668142e-09 -5.501342e-09
> f -2.655647e-10  6.354852e-10
> 
> ------
> Correlation matrix:
>            d             e             f
> d  1.0000000 -6.202339e-01 -7.832539e-01
> e -0.6202339  1.000000e+00 -2.127301e-05
> f -0.7832539 -2.127301e-05  1.000000e+00
> 
> 
> if I let XL = 1.1070513 and LM3 = 0.3919 , and consider the coeffs as given above, the right hand side of the above equation is negative.
> But YLf13 is always positive! How is this possible? Am I interpreting the result of the nls output properly?  Should I interpret the coeffs differently? I have done hours of thinking over the above problem but couldn't find any results...
> 
> I cannot provide the full values of YLf13, XL and LM3 due to IPR issues....please cooperate......however, if the only way to solve the problem is to give these values, I would indeed give them.....
> 
> Also forgive me if there is a minor mistake in my calculations... or a typo....
> 
> very many thanks for your time and effort....
> yours sincerely,
> AKSHAY M KULKARNI
> 
>         [[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From t@v@r|ch@norberto @end|ng |rom gm@||@com  Wed Mar  6 19:22:44 2019
From: t@v@r|ch@norberto @end|ng |rom gm@||@com (Norberto Hernandez)
Date: Wed, 6 Mar 2019 12:22:44 -0600
Subject: [R] Unable to install ggplot2
In-Reply-To: <1CE82BEA-6A3A-403F-945B-26C48DB5B027@dcn.davis.ca.us>
References: <CAFPXQ692erLUN76+2G3N43oWBtdTw27ScFHNO7A46T0oxyb1iQ@mail.gmail.com>
 <1CE82BEA-6A3A-403F-945B-26C48DB5B027@dcn.davis.ca.us>
Message-ID: <CADRcKWztusm61UZCV5=d+eaJ6PP81V=Bp2RjPVodqDFdVWN3Nw@mail.gmail.com>

I have the same issue with ggplot2 and the rlang package, you need to
have the most updated version of the rlang library in order to get
installed ggplot2

Regards
Norberto

El mar., 5 mar. 2019 a las 14:24, Jeff Newmiller
(<jdnewmil at dcn.davis.ca.us>) escribi?:
>
> Please post the text version of the error in the future... your picture is almost unreadable. Also, if it is actually important that you are using RStudio then your question probably doesn't belong here. Also, if the problem is a faulty contributed package then you will need to contact the package maintainer as the Posting Guide mentioned below says.
>
> I don't know why the dependency is not being handled correctly, but my suggestion would be to install the rlang package first, and once that is installed try installing ggplot2. Read the errors... it says there is a problem with the rlang package.
>
> On March 5, 2019 10:04:41 AM PST, Kamalika Ray <kamalikaray333 at gmail.com> wrote:
> >Hi,
> >I have been trying to install the ggplot2 package but I am unable to do
> >so. My Mac OS version is 10.7.4 and I have downloaded the
> >R-Studio-1.1.463.
> >I have attached the screenshot of the error message which appears.
> >
> >Please help!
> >
> >Thank you,
> >Kamalika
> >India
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From reichm@@j m@iii@g oii sbcgiob@i@@et  Wed Mar  6 22:44:02 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Wed, 6 Mar 2019 15:44:02 -0600
Subject: [R] Zoom In/Out  maps library
Message-ID: <001801d4d465$b7ac1300$27043900$@sbcglobal.net>

R Help

Anyone know if I can add a zoom In/Out function to the maps available via the "maps" library? Or do I need to use a different mapping library?

world.map <- map_data("world")

ggplot(data = world.map) +
  geom_polygon(mapping = aes(x=long, y=lat, group=group))

Jeff


From roy@mende|@@ohn @end|ng |rom no@@@gov  Wed Mar  6 23:12:06 2019
From: roy@mende|@@ohn @end|ng |rom no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 6 Mar 2019 14:12:06 -0800
Subject: [R] Zoom In/Out  maps library
In-Reply-To: <C5024B8F-F2E1-4371-ABC5-DD93D60D70A6@gmail.com>
References: <001801d4d465$b7ac1300$27043900$@sbcglobal.net>
 <C5024B8F-F2E1-4371-ABC5-DD93D60D70A6@gmail.com>
Message-ID: <737DB709-95E6-4F99-88C3-3F2AB2085B4E@noaa.gov>

world.map <- maps::map("world", plot = FALSE, fill = TRUE)
p <- sf:: st_as_sf(world.map, coords = c('x', 'y'))
map view::map view(p)


HTH,

-Roy
> On Mar 6, 2019, at 2:10 PM, rmendelss gmail <rmendelss at gmail.com> wrote:
> 
> world.map <- maps::map("world", plot = FALSE, fill = TRUE)
> p <- sf:: st_as_sf(world.map, coords = c('x', 'y'))
> map view::map view(p)
> 
> HTH,
> 
> -Roy
> 
>> On Mar 6, 2019, at 1:44 PM, reichmanj at sbcglobal.net wrote:
>> 
>> R Help
>> 
>> Anyone know if I can add a zoom In/Out function to the maps available via the "maps" library? Or do I need to use a different mapping library?
>> 
>> world.map <- map_data("world")
>> 
>> ggplot(data = world.map) +
>> geom_polygon(mapping = aes(x=long, y=lat, group=group))
>> 
>> Jeff
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From roy@mende|@@ohn @end|ng |rom no@@@gov  Wed Mar  6 23:22:37 2019
From: roy@mende|@@ohn @end|ng |rom no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 6 Mar 2019 14:22:37 -0800
Subject: [R] Zoom In/Out  maps library
In-Reply-To: <737DB709-95E6-4F99-88C3-3F2AB2085B4E@noaa.gov>
References: <001801d4d465$b7ac1300$27043900$@sbcglobal.net>
 <C5024B8F-F2E1-4371-ABC5-DD93D60D70A6@gmail.com>
 <737DB709-95E6-4F99-88C3-3F2AB2085B4E@noaa.gov>
Message-ID: <FC58799A-8166-4631-887F-D4355C450FAF@noaa.gov>

Or if you prefer plotly:

world.map <- maps::map("world", plot = FALSE, fill = TRUE)
p <- sf:: st_as_sf(world.map, coords = c('x', 'y'))
plotly::ggplotly(
ggplot2::ggplot(data = p) + ggplot2::geom_sf()
)

> On Mar 6, 2019, at 2:12 PM, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> 
> world.map <- maps::map("world", plot = FALSE, fill = TRUE)
> p <- sf:: st_as_sf(world.map, coords = c('x', 'y'))
> map view::map view(p)
> 
> 
> HTH,
> 
> -Roy
>> On Mar 6, 2019, at 2:10 PM, rmendelss gmail <rmendelss at gmail.com> wrote:
>> 
>> world.map <- maps::map("world", plot = FALSE, fill = TRUE)
>> p <- sf:: st_as_sf(world.map, coords = c('x', 'y'))
>> map view::map view(p)
>> 
>> HTH,
>> 
>> -Roy
>> 
>>> On Mar 6, 2019, at 1:44 PM, reichmanj at sbcglobal.net wrote:
>>> 
>>> R Help
>>> 
>>> Anyone know if I can add a zoom In/Out function to the maps available via the "maps" library? Or do I need to use a different mapping library?
>>> 
>>> world.map <- map_data("world")
>>> 
>>> ggplot(data = world.map) +
>>> geom_polygon(mapping = aes(x=long, y=lat, group=group))
>>> 
>>> Jeff
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected" 
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From reichm@@j m@iii@g oii sbcgiob@i@@et  Wed Mar  6 23:36:23 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Wed, 6 Mar 2019 16:36:23 -0600
Subject: [R] Zoom In/Out  maps library
In-Reply-To: <C5024B8F-F2E1-4371-ABC5-DD93D60D70A6@gmail.com>
References: <001801d4d465$b7ac1300$27043900$@sbcglobal.net>
 <C5024B8F-F2E1-4371-ABC5-DD93D60D70A6@gmail.com>
Message-ID: <001f01d4d46d$07a78590$16f690b0$@sbcglobal.net>

Roy

Thank you - that's helpful.  Going to have to read up on sf and mapview
library. Those are new ones.  Then to add a point feature layer (lat/long)
where would I insert that?

Library(maps)
Library(sf) # simple features
Library(mapview)

world.map <- maps::map("world", plot = FALSE, fill = TRUE) 
p <- sf::st_as_sf(world.map, coords = c('x', 'y')) 
mapview::mapview(p, legend=FALSE)



-----Original Message-----
From: rmendelss gmail <rmendelss at gmail.com> 
Sent: Wednesday, March 6, 2019 4:11 PM
To: reichmanj at sbcglobal.net
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] Zoom In/Out maps library

world.map <- maps::map("world", plot = FALSE, fill = TRUE) p <- sf::
st_as_sf(world.map, coords = c('x', 'y')) map view::map view(p)

HTH,

-Roy

> On Mar 6, 2019, at 1:44 PM, reichmanj at sbcglobal.net wrote:
> 
> R Help
> 
> Anyone know if I can add a zoom In/Out function to the maps available via
the "maps" library? Or do I need to use a different mapping library?
> 
> world.map <- map_data("world")
> 
> ggplot(data = world.map) +
>  geom_polygon(mapping = aes(x=long, y=lat, group=group))
> 
> Jeff
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From roy@mende|@@ohn @end|ng |rom no@@@gov  Wed Mar  6 23:48:12 2019
From: roy@mende|@@ohn @end|ng |rom no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 6 Mar 2019 14:48:12 -0800
Subject: [R] Zoom In/Out  maps library
In-Reply-To: <001f01d4d46d$07a78590$16f690b0$@sbcglobal.net>
References: <001801d4d465$b7ac1300$27043900$@sbcglobal.net>
 <C5024B8F-F2E1-4371-ABC5-DD93D60D70A6@gmail.com>
 <001f01d4d46d$07a78590$16f690b0$@sbcglobal.net>
Message-ID: <BF376E66-1848-4D17-AE58-CD5012500F04@noaa.gov>

see https://r-spatial.github.io/mapview/index.html

The main thing is the data types that map view supports,  so you must have a raster or an spatial object like an "sf" object.   So points would have to also be an sf object and the two combined  (sf has commands to do this) or perhaps you can do ti directly in mapview,  I haven't played with it much.

The plotly example I sent works with ggplot2,  so if you know how to build up the map in ggplot2 you can try that,  though again as far as I can see plotly for maps needs sf objects.

Mainly sent you the links to get you started.  I haven't played with either much,  just know that they exist and zoom maps.

HTH,

-Roy

> On Mar 6, 2019, at 2:36 PM, <reichmanj at sbcglobal.net> <reichmanj at sbcglobal.net> wrote:
> 
> Roy
> 
> Thank you - that's helpful.  Going to have to read up on sf and mapview
> library. Those are new ones.  Then to add a point feature layer (lat/long)
> where would I insert that?
> 
> Library(maps)
> Library(sf) # simple features
> Library(mapview)
> 
> world.map <- maps::map("world", plot = FALSE, fill = TRUE) 
> p <- sf::st_as_sf(world.map, coords = c('x', 'y')) 
> mapview::mapview(p, legend=FALSE)
> 
> 
> 
> -----Original Message-----
> From: rmendelss gmail <rmendelss at gmail.com> 
> Sent: Wednesday, March 6, 2019 4:11 PM
> To: reichmanj at sbcglobal.net
> Cc: R help Mailing list <r-help at r-project.org>
> Subject: Re: [R] Zoom In/Out maps library
> 
> world.map <- maps::map("world", plot = FALSE, fill = TRUE) p <- sf::
> st_as_sf(world.map, coords = c('x', 'y')) map view::map view(p)
> 
> HTH,
> 
> -Roy
> 
>> On Mar 6, 2019, at 1:44 PM, reichmanj at sbcglobal.net wrote:
>> 
>> R Help
>> 
>> Anyone know if I can add a zoom In/Out function to the maps available via
> the "maps" library? Or do I need to use a different mapping library?
>> 
>> world.map <- map_data("world")
>> 
>> ggplot(data = world.map) +
>> geom_polygon(mapping = aes(x=long, y=lat, group=group))
>> 
>> Jeff
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From roy@mende|@@ohn @end|ng |rom no@@@gov  Wed Mar  6 23:53:01 2019
From: roy@mende|@@ohn @end|ng |rom no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Wed, 6 Mar 2019 14:53:01 -0800
Subject: [R] Zoom In/Out  maps library
In-Reply-To: <BF376E66-1848-4D17-AE58-CD5012500F04@noaa.gov>
References: <001801d4d465$b7ac1300$27043900$@sbcglobal.net>
 <C5024B8F-F2E1-4371-ABC5-DD93D60D70A6@gmail.com>
 <001f01d4d46d$07a78590$16f690b0$@sbcglobal.net>
 <BF376E66-1848-4D17-AE58-CD5012500F04@noaa.gov>
Message-ID: <03A62E7B-8B27-42BA-9775-54E63E8301E4@noaa.gov>

Also,  I forgot that tmap can do interactive maps, see:

https://geocompr.robinlovelace.net/adv-map.html#interactive-maps

-Roy

> On Mar 6, 2019, at 2:48 PM, Roy Mendelssohn - NOAA Federal <roy.mendelssohn at noaa.gov> wrote:
> 
> see https://r-spatial.github.io/mapview/index.html
> 
> The main thing is the data types that map view supports,  so you must have a raster or an spatial object like an "sf" object.   So points would have to also be an sf object and the two combined  (sf has commands to do this) or perhaps you can do ti directly in mapview,  I haven't played with it much.
> 
> The plotly example I sent works with ggplot2,  so if you know how to build up the map in ggplot2 you can try that,  though again as far as I can see plotly for maps needs sf objects.
> 
> Mainly sent you the links to get you started.  I haven't played with either much,  just know that they exist and zoom maps.
> 
> HTH,
> 
> -Roy
> 
>> On Mar 6, 2019, at 2:36 PM, <reichmanj at sbcglobal.net> <reichmanj at sbcglobal.net> wrote:
>> 
>> Roy
>> 
>> Thank you - that's helpful.  Going to have to read up on sf and mapview
>> library. Those are new ones.  Then to add a point feature layer (lat/long)
>> where would I insert that?
>> 
>> Library(maps)
>> Library(sf) # simple features
>> Library(mapview)
>> 
>> world.map <- maps::map("world", plot = FALSE, fill = TRUE) 
>> p <- sf::st_as_sf(world.map, coords = c('x', 'y')) 
>> mapview::mapview(p, legend=FALSE)
>> 
>> 
>> 
>> -----Original Message-----
>> From: rmendelss gmail <rmendelss at gmail.com> 
>> Sent: Wednesday, March 6, 2019 4:11 PM
>> To: reichmanj at sbcglobal.net
>> Cc: R help Mailing list <r-help at r-project.org>
>> Subject: Re: [R] Zoom In/Out maps library
>> 
>> world.map <- maps::map("world", plot = FALSE, fill = TRUE) p <- sf::
>> st_as_sf(world.map, coords = c('x', 'y')) map view::map view(p)
>> 
>> HTH,
>> 
>> -Roy
>> 
>>> On Mar 6, 2019, at 1:44 PM, reichmanj at sbcglobal.net wrote:
>>> 
>>> R Help
>>> 
>>> Anyone know if I can add a zoom In/Out function to the maps available via
>> the "maps" library? Or do I need to use a different mapping library?
>>> 
>>> world.map <- map_data("world")
>>> 
>>> ggplot(data = world.map) +
>>> geom_polygon(mapping = aes(x=long, y=lat, group=group))
>>> 
>>> Jeff
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> **********************
> "The contents of this message do not reflect any position of the U.S. Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> ***Note new street address***
> 110 McAllister Way
> Santa Cruz, CA 95060
> Phone: (831)-420-3666
> Fax: (831) 420-3980
> e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/
> 
> "Old age and treachery will overcome youth and skill."
> "From those who have been given much, much will be expected" 
> "the arc of the moral universe is long, but it bends toward justice" -MLK Jr.
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From pd@|gd @end|ng |rom gm@||@com  Thu Mar  7 00:36:17 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 7 Mar 2019 00:36:17 +0100
Subject: [R] Unable to install ggplot2
In-Reply-To: <CADRcKWztusm61UZCV5=d+eaJ6PP81V=Bp2RjPVodqDFdVWN3Nw@mail.gmail.com>
References: <CAFPXQ692erLUN76+2G3N43oWBtdTw27ScFHNO7A46T0oxyb1iQ@mail.gmail.com>
 <1CE82BEA-6A3A-403F-945B-26C48DB5B027@dcn.davis.ca.us>
 <CADRcKWztusm61UZCV5=d+eaJ6PP81V=Bp2RjPVodqDFdVWN3Nw@mail.gmail.com>
Message-ID: <DA653D83-87CE-4B3C-B683-F3D264988A54@gmail.com>

Also, R seems to be version 3.2.x i.e. 3-4 years old. Earliest rlang is anno 2017 as far as I can tell.

-pd

> On 6 Mar 2019, at 19:22 , Norberto Hernandez <tavarich.norberto at gmail.com> wrote:
> 
> I have the same issue with ggplot2 and the rlang package, you need to
> have the most updated version of the rlang library in order to get
> installed ggplot2
> 
> Regards
> Norberto
> 
> El mar., 5 mar. 2019 a las 14:24, Jeff Newmiller
> (<jdnewmil at dcn.davis.ca.us>) escribi?:
>> 
>> Please post the text version of the error in the future... your picture is almost unreadable. Also, if it is actually important that you are using RStudio then your question probably doesn't belong here. Also, if the problem is a faulty contributed package then you will need to contact the package maintainer as the Posting Guide mentioned below says.
>> 
>> I don't know why the dependency is not being handled correctly, but my suggestion would be to install the rlang package first, and once that is installed try installing ggplot2. Read the errors... it says there is a problem with the rlang package.
>> 
>> On March 5, 2019 10:04:41 AM PST, Kamalika Ray <kamalikaray333 at gmail.com> wrote:
>>> Hi,
>>> I have been trying to install the ggplot2 package but I am unable to do
>>> so. My Mac OS version is 10.7.4 and I have downloaded the
>>> R-Studio-1.1.463.
>>> I have attached the screenshot of the error message which appears.
>>> 
>>> Please help!
>>> 
>>> Thank you,
>>> Kamalika
>>> India
>> 
>> --
>> Sent from my phone. Please excuse my brevity.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From j@p@rk4 @end|ng |rom u|c@edu  Thu Mar  7 00:37:58 2019
From: j@p@rk4 @end|ng |rom u|c@edu (Sparks, John)
Date: Wed, 6 Mar 2019 23:37:58 +0000
Subject: [R] strucchange Graph By Week and xts error
Message-ID: <DM6PR13MB2522707B1D4B36BE197CFFD3FA730@DM6PR13MB2522.namprd13.prod.outlook.com>

Hi R Helpers,

I am doing some work at identifying change points in time series data.  A very nice example is given in the R Bloggers post

https://www.r-bloggers.com/a-look-at-strucchange-and-segmented/

The data for the aswan dam in that example is yearly.  My data is weekly.  I ran the code switching the data for the analysis to my data and it worked, but the scale of the line chart is not sensible.  I have 225 weekly observations and the x-axis of the line graph shows numbers from 0 to over 1500.  The information on the ts object is

Start=1
End=1569
Frequency=0.1428...

I can't share the data because it is proprietary.

Wanting to be a good member of the list, I attempted to put weekly increments on the Nile data so I could reproduce the x axis of the chart with the axis scale that I am seeing.  Unfortunately, in doing so I got another error that I don't understand.



library(strucchange)
library(lubridate)
library(xts)

# example from R-Blog runs fine
data(?Nile?)
plot(Nile)
bp.nile <- breakpoints(Nile ~ 1)
ci.nile <- confint(bp.nile, breaks = 1)
lines(ci.nile)

#problem comes in here
dfNile<-data.frame(Nile)
dfNile$week<-seq(ymd('2012-01-01'),ymd('2013-11-30'),by='weeks')
tsNile<-as.xts(x=dfNile[,-2],order.by=dfNile$week)

Error in xts(x.mat, order.by = order.by, frequency = frequency(x), ...) :
  formal argument "order.by" matched by multiple actual arguments


Can somebody help me to put together the ts object with weeks so that I can demonstrate the problem with the scale on the x-axis and then try to get some help with that original problem?

Much appreciated.
--John Sparks








	[[alternative HTML version deleted]]


From h@w|ckh@m @end|ng |rom gm@||@com  Thu Mar  7 00:55:36 2019
From: h@w|ckh@m @end|ng |rom gm@||@com (Hadley Wickham)
Date: Wed, 6 Mar 2019 17:55:36 -0600
Subject: [R] Unable to install ggplot2
In-Reply-To: <DA653D83-87CE-4B3C-B683-F3D264988A54@gmail.com>
References: <CAFPXQ692erLUN76+2G3N43oWBtdTw27ScFHNO7A46T0oxyb1iQ@mail.gmail.com>
 <1CE82BEA-6A3A-403F-945B-26C48DB5B027@dcn.davis.ca.us>
 <CADRcKWztusm61UZCV5=d+eaJ6PP81V=Bp2RjPVodqDFdVWN3Nw@mail.gmail.com>
 <DA653D83-87CE-4B3C-B683-F3D264988A54@gmail.com>
Message-ID: <CABdHhvHOnhrTM6AnOgc8RKDS9nHYZchrc0oeMi5w0Q+DgPyijA@mail.gmail.com>

rlang works with R 3.1 and up, but it does require compilation from
source, which I suspect is the root cause of this problem.

Hadley

On Wed, Mar 6, 2019 at 5:36 PM peter dalgaard <pdalgd at gmail.com> wrote:
>
> Also, R seems to be version 3.2.x i.e. 3-4 years old. Earliest rlang is anno 2017 as far as I can tell.
>
> -pd
>
> > On 6 Mar 2019, at 19:22 , Norberto Hernandez <tavarich.norberto at gmail.com> wrote:
> >
> > I have the same issue with ggplot2 and the rlang package, you need to
> > have the most updated version of the rlang library in order to get
> > installed ggplot2
> >
> > Regards
> > Norberto
> >
> > El mar., 5 mar. 2019 a las 14:24, Jeff Newmiller
> > (<jdnewmil at dcn.davis.ca.us>) escribi?:
> >>
> >> Please post the text version of the error in the future... your picture is almost unreadable. Also, if it is actually important that you are using RStudio then your question probably doesn't belong here. Also, if the problem is a faulty contributed package then you will need to contact the package maintainer as the Posting Guide mentioned below says.
> >>
> >> I don't know why the dependency is not being handled correctly, but my suggestion would be to install the rlang package first, and once that is installed try installing ggplot2. Read the errors... it says there is a problem with the rlang package.
> >>
> >> On March 5, 2019 10:04:41 AM PST, Kamalika Ray <kamalikaray333 at gmail.com> wrote:
> >>> Hi,
> >>> I have been trying to install the ggplot2 package but I am unable to do
> >>> so. My Mac OS version is 10.7.4 and I have downloaded the
> >>> R-Studio-1.1.463.
> >>> I have attached the screenshot of the error message which appears.
> >>>
> >>> Please help!
> >>>
> >>> Thank you,
> >>> Kamalika
> >>> India
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://hadley.nz


From drj|m|emon @end|ng |rom gm@||@com  Thu Mar  7 01:01:48 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 7 Mar 2019 11:01:48 +1100
Subject: [R] strucchange Graph By Week and xts error
In-Reply-To: <DM6PR13MB2522707B1D4B36BE197CFFD3FA730@DM6PR13MB2522.namprd13.prod.outlook.com>
References: <DM6PR13MB2522707B1D4B36BE197CFFD3FA730@DM6PR13MB2522.namprd13.prod.outlook.com>
Message-ID: <CA+8X3fXga2CLusvVM2xOZ7Es0L_f4g+jY9v1ywEedUhTcEfhAA@mail.gmail.com>

Hi John,
You seem to have 1569 days of data, so perhaps you can get around your
axis problem like this:

plot(Nile,xaxt="n",xlab="Week")
...
axis(1,at=seq(0,200,50),labels=seq(0,200,50)*7)
(untested)

Jim

On Thu, Mar 7, 2019 at 10:46 AM Sparks, John <jspark4 at uic.edu> wrote:
>
> Hi R Helpers,
>
> I am doing some work at identifying change points in time series data.  A very nice example is given in the R Bloggers post
>
> https://www.r-bloggers.com/a-look-at-strucchange-and-segmented/
>
> The data for the aswan dam in that example is yearly.  My data is weekly.  I ran the code switching the data for the analysis to my data and it worked, but the scale of the line chart is not sensible.  I have 225 weekly observations and the x-axis of the line graph shows numbers from 0 to over 1500.  The information on the ts object is
>
> Start=1
> End=1569
> Frequency=0.1428...
>
> I can't share the data because it is proprietary.
>
> Wanting to be a good member of the list, I attempted to put weekly increments on the Nile data so I could reproduce the x axis of the chart with the axis scale that I am seeing.  Unfortunately, in doing so I got another error that I don't understand.
>
>
>
> library(strucchange)
> library(lubridate)
> library(xts)
>
> # example from R-Blog runs fine
> data(?Nile?)
> plot(Nile)
> bp.nile <- breakpoints(Nile ~ 1)
> ci.nile <- confint(bp.nile, breaks = 1)
> lines(ci.nile)
>
> #problem comes in here
> dfNile<-data.frame(Nile)
> dfNile$week<-seq(ymd('2012-01-01'),ymd('2013-11-30'),by='weeks')
> tsNile<-as.xts(x=dfNile[,-2],order.by=dfNile$week)
>
> Error in xts(x.mat, order.by = order.by, frequency = frequency(x), ...) :
>   formal argument "order.by" matched by multiple actual arguments
>
>
> Can somebody help me to put together the ts object with weeks so that I can demonstrate the problem with the scale on the x-axis and then try to get some help with that original problem?
>
> Much appreciated.
> --John Sparks
>
>
>
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Ach|m@Ze||e|@ @end|ng |rom u|bk@@c@@t  Thu Mar  7 01:11:44 2019
From: Ach|m@Ze||e|@ @end|ng |rom u|bk@@c@@t (Achim Zeileis)
Date: Thu, 7 Mar 2019 01:11:44 +0100 (CET)
Subject: [R] strucchange Graph By Week and xts error
In-Reply-To: <DM6PR13MB2522707B1D4B36BE197CFFD3FA730@DM6PR13MB2522.namprd13.prod.outlook.com>
References: <DM6PR13MB2522707B1D4B36BE197CFFD3FA730@DM6PR13MB2522.namprd13.prod.outlook.com>
Message-ID: <alpine.DEB.2.21.1903070102200.693@paninaro>

On Thu, 7 Mar 2019, Sparks, John wrote:

> Hi R Helpers,
>
> I am doing some work at identifying change points in time series data. 
> A very nice example is given in the R Bloggers post
>
> https://www.r-bloggers.com/a-look-at-strucchange-and-segmented/
>
> The data for the aswan dam in that example is yearly.  My data is 
> weekly.  I ran the code switching the data for the analysis to my data 
> and it worked, but the scale of the line chart is not sensible.  I have 
> 225 weekly observations and the x-axis of the line graph shows numbers 
> from 0 to over 1500.  The information on the ts object is

Unfortunately, breakpoints() can only deal automatically with "ts" time 
series not with zoo/xts/... So either you can squeeze your data onto a 
regular "ts" grid which may work in the case of weekly data. Or you need 
to handle the time index "by hand". See

https://stackoverflow.com/questions/43243548/strucchange-not-reporting-breakdates/43267082#43267082

for an example for this.

As for the as.xts() error below. This is because dfNile[, -2] is still a 
"ts" object and then as.xts() sets up "order.by" for you.

Either you use xts() rather than as.xts() or you make the first column in 
the data.frame "numeric" rather than "ts", e.g., by starting the 
transformation with:

dfNile<-data.frame(as.numeric(Nile))


> Start=1
> End=1569
> Frequency=0.1428...
>
> I can't share the data because it is proprietary.
>
> Wanting to be a good member of the list, I attempted to put weekly 
> increments on the Nile data so I could reproduce the x axis of the chart 
> with the axis scale that I am seeing.  Unfortunately, in doing so I got 
> another error that I don't understand.
>
>
>
> library(strucchange)
> library(lubridate)
> library(xts)
>
> # example from R-Blog runs fine
> data(???Nile???)
> plot(Nile)
> bp.nile <- breakpoints(Nile ~ 1)
> ci.nile <- confint(bp.nile, breaks = 1)
> lines(ci.nile)
>
> #problem comes in here
> dfNile<-data.frame(Nile)
> dfNile$week<-seq(ymd('2012-01-01'),ymd('2013-11-30'),by='weeks')
> tsNile<-as.xts(x=dfNile[,-2],order.by=dfNile$week)
>
> Error in xts(x.mat, order.by = order.by, frequency = frequency(x), ...) :
>  formal argument "order.by" matched by multiple actual arguments
>
>
> Can somebody help me to put together the ts object with weeks so that I can demonstrate the problem with the scale on the x-axis and then try to get some help with that original problem?
>
> Much appreciated.
> --John Sparks
>
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
>


From j@p@rk4 @end|ng |rom u|c@edu  Thu Mar  7 01:46:57 2019
From: j@p@rk4 @end|ng |rom u|c@edu (Sparks, John)
Date: Thu, 7 Mar 2019 00:46:57 +0000
Subject: [R] strucchange Graph By Week and xts error
In-Reply-To: <alpine.DEB.2.21.1903070102200.693@paninaro>
References: <DM6PR13MB2522707B1D4B36BE197CFFD3FA730@DM6PR13MB2522.namprd13.prod.outlook.com>,
 <alpine.DEB.2.21.1903070102200.693@paninaro>
Message-ID: <DM6PR13MB252239EE83CE21DE520375DDFA4C0@DM6PR13MB2522.namprd13.prod.outlook.com>

Thanks to Achim's direction I now have a re-producible example.

The code below creates a ts object.  The x scale of the last graph runs from 0 to 700.

So just need a way to get that scale to show the weeks (or some summary of them).

Thanks a bunch.
--JJS


library(strucchange)
library(xts)
library(lubridate)

#rm(list=ls())


data("Nile")
class(Nile)
plot(Nile)
bp.nile <- breakpoints(Nile ~ 1)
ci.nile <- confint(bp.nile, breaks = 1)
lines(ci.nile)


dfNile<-data.frame(as.numeric(Nile))
dfNile$week<-seq(ymd('2012-01-01'),ymd('2013-11-30'),by='weeks')
tsNile <- as.xts(x = dfNile[, -2], order.by = dfNile$week)
tsNile<-as.ts(tsNile)


plot(tsNile)
bp.tsNile <- breakpoints(tsNile ~ 1)
ci.tsNile <- confint(bp.tsNile, breaks = 1)
lines(ci.tsNile)





________________________________
From: Achim Zeileis <Achim.Zeileis at uibk.ac.at>
Sent: Wednesday, March 6, 2019 6:11 PM
To: Sparks, John
Cc: r-help at r-project.org
Subject: Re: [R] strucchange Graph By Week and xts error

On Thu, 7 Mar 2019, Sparks, John wrote:

> Hi R Helpers,
>
> I am doing some work at identifying change points in time series data.
> A very nice example is given in the R Bloggers post
>
> https://www.r-bloggers.com/a-look-at-strucchange-and-segmented/
>
> The data for the aswan dam in that example is yearly.  My data is
> weekly.  I ran the code switching the data for the analysis to my data
> and it worked, but the scale of the line chart is not sensible.  I have
> 225 weekly observations and the x-axis of the line graph shows numbers
> from 0 to over 1500.  The information on the ts object is

Unfortunately, breakpoints() can only deal automatically with "ts" time
series not with zoo/xts/... So either you can squeeze your data onto a
regular "ts" grid which may work in the case of weekly data. Or you need
to handle the time index "by hand". See

https://stackoverflow.com/questions/43243548/strucchange-not-reporting-breakdates/43267082#43267082

for an example for this.

As for the as.xts() error below. This is because dfNile[, -2] is still a
"ts" object and then as.xts() sets up "order.by" for you.

Either you use xts() rather than as.xts() or you make the first column in
the data.frame "numeric" rather than "ts", e.g., by starting the
transformation with:

dfNile<-data.frame(as.numeric(Nile))


> Start=1
> End=1569
> Frequency=0.1428...
>
> I can't share the data because it is proprietary.
>
> Wanting to be a good member of the list, I attempted to put weekly
> increments on the Nile data so I could reproduce the x axis of the chart
> with the axis scale that I am seeing.  Unfortunately, in doing so I got
> another error that I don't understand.
>
>
>
> library(strucchange)
> library(lubridate)
> library(xts)
>
> # example from R-Blog runs fine
> data(???Nile???)
> plot(Nile)
> bp.nile <- breakpoints(Nile ~ 1)
> ci.nile <- confint(bp.nile, breaks = 1)
> lines(ci.nile)
>
> #problem comes in here
> dfNile<-data.frame(Nile)
> dfNile$week<-seq(ymd('2012-01-01'),ymd('2013-11-30'),by='weeks')
> tsNile<-as.xts(x=dfNile[,-2],order.by=dfNile$week)
>
> Error in xts(x.mat, order.by = order.by, frequency = frequency(x), ...) :
>  formal argument "order.by" matched by multiple actual arguments
>
>
> Can somebody help me to put together the ts object with weeks so that I can demonstrate the problem with the scale on the x-axis and then try to get some help with that original problem?
>
> Much appreciated.
> --John Sparks
>
>
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
>

	[[alternative HTML version deleted]]


From Ach|m@Ze||e|@ @end|ng |rom u|bk@@c@@t  Thu Mar  7 02:06:16 2019
From: Ach|m@Ze||e|@ @end|ng |rom u|bk@@c@@t (Achim Zeileis)
Date: Thu, 7 Mar 2019 02:06:16 +0100 (CET)
Subject: [R] strucchange Graph By Week and xts error
In-Reply-To: <DM6PR13MB252239EE83CE21DE520375DDFA4C0@DM6PR13MB2522.namprd13.prod.outlook.com>
References: <DM6PR13MB2522707B1D4B36BE197CFFD3FA730@DM6PR13MB2522.namprd13.prod.outlook.com>,
 <alpine.DEB.2.21.1903070102200.693@paninaro>
 <DM6PR13MB252239EE83CE21DE520375DDFA4C0@DM6PR13MB2522.namprd13.prod.outlook.com>
Message-ID: <alpine.DEB.2.21.1903070159341.693@paninaro>

On Thu, 7 Mar 2019, Sparks, John wrote:

> Thanks to Achim's direction I now have a re-producible example.
>
> The code below creates a ts object.  The x scale of the last graph runs 
> from 0 to 700.

Yes, and hence breakpoints() re-uses that scaling. As I wrote in my 
previous mail you either have to squeeze your data into a regular grid of 
52 weekly observations per year or you have to keep track of the time 
index yourself. See below.

> So just need a way to get that scale to show the weeks (or some summary 
> of them).
>
> Thanks a bunch.
> --JJS
>
>
> library(strucchange)
> library(xts)
> library(lubridate)
>
> #rm(list=ls())
>
>
> data("Nile")
> class(Nile)
> plot(Nile)
> bp.nile <- breakpoints(Nile ~ 1)
> ci.nile <- confint(bp.nile, breaks = 1)
> lines(ci.nile)
>
>
> dfNile<-data.frame(as.numeric(Nile))
> dfNile$week<-seq(ymd('2012-01-01'),ymd('2013-11-30'),by='weeks')
> tsNile <- as.xts(x = dfNile[, -2], order.by = dfNile$week)
> tsNile<-as.ts(tsNile)
>
>
> plot(tsNile)
> bp.tsNile <- breakpoints(tsNile ~ 1)
> ci.tsNile <- confint(bp.tsNile, breaks = 1)
> lines(ci.tsNile)

If you want to use your own non-ts time scale, you can use "xts" (as you 
do above) or "zoo" (as I do below) or keep thing in a plain "data.frame" 
(or similar). Then you just have to index the times with the breakpoints 
or their confidence intervals respectively:

## zoo series
x <- zoo(Nile, seq(ymd('2012-01-01'),ymd('2013-11-30'),by='weeks'))

## breakpoints and confidence intervals
bp <- breakpoints(x ~ 1)
ci <- confint(bp, breaks = 1)

## map time index
cix <- time(x)[ci$confint]

## visualize
plot(x)
abline(v = cix[2], lty = 2)
arrows(cix[1], min(x), cix[3], min(x),
   col = 2, angle = 90, length = 0.05, code = 3)

Above, the $confint is a vector. If it is a matrix (due to more than one 
breakpoint) the code needs to be tweaked to make cix also a matrix and 
then use cix[,i] rather than cix[i] for i = 1, 2, 3.

>
>
>
>
> ________________________________
> From: Achim Zeileis <Achim.Zeileis at uibk.ac.at>
> Sent: Wednesday, March 6, 2019 6:11 PM
> To: Sparks, John
> Cc: r-help at r-project.org
> Subject: Re: [R] strucchange Graph By Week and xts error
>
> On Thu, 7 Mar 2019, Sparks, John wrote:
>
>> Hi R Helpers,
>>
>> I am doing some work at identifying change points in time series data.
>> A very nice example is given in the R Bloggers post
>>
>> https://www.r-bloggers.com/a-look-at-strucchange-and-segmented/
>>
>> The data for the aswan dam in that example is yearly.  My data is
>> weekly.  I ran the code switching the data for the analysis to my data
>> and it worked, but the scale of the line chart is not sensible.  I have
>> 225 weekly observations and the x-axis of the line graph shows numbers
>> from 0 to over 1500.  The information on the ts object is
>
> Unfortunately, breakpoints() can only deal automatically with "ts" time
> series not with zoo/xts/... So either you can squeeze your data onto a
> regular "ts" grid which may work in the case of weekly data. Or you need
> to handle the time index "by hand". See
>
> https://stackoverflow.com/questions/43243548/strucchange-not-reporting-breakdates/43267082#43267082
>
> for an example for this.
>
> As for the as.xts() error below. This is because dfNile[, -2] is still a
> "ts" object and then as.xts() sets up "order.by" for you.
>
> Either you use xts() rather than as.xts() or you make the first column in
> the data.frame "numeric" rather than "ts", e.g., by starting the
> transformation with:
>
> dfNile<-data.frame(as.numeric(Nile))
>
>
>> Start=1
>> End=1569
>> Frequency=0.1428...
>>
>> I can't share the data because it is proprietary.
>>
>> Wanting to be a good member of the list, I attempted to put weekly
>> increments on the Nile data so I could reproduce the x axis of the chart
>> with the axis scale that I am seeing.  Unfortunately, in doing so I got
>> another error that I don't understand.
>>
>>
>>
>> library(strucchange)
>> library(lubridate)
>> library(xts)
>>
>> # example from R-Blog runs fine
>> data(???Nile???)
>> plot(Nile)
>> bp.nile <- breakpoints(Nile ~ 1)
>> ci.nile <- confint(bp.nile, breaks = 1)
>> lines(ci.nile)
>>
>> #problem comes in here
>> dfNile<-data.frame(Nile)
>> dfNile$week<-seq(ymd('2012-01-01'),ymd('2013-11-30'),by='weeks')
>> tsNile<-as.xts(x=dfNile[,-2],order.by=dfNile$week)
>>
>> Error in xts(x.mat, order.by = order.by, frequency = frequency(x), ...) :
>>  formal argument "order.by" matched by multiple actual arguments
>>
>>
>> Can somebody help me to put together the ts object with weeks so that I can demonstrate the problem with the scale on the x-axis and then try to get some help with that original problem?
>>
>> Much appreciated.
>> --John Sparks
>>
>>
>>
>>
>>
>>
>>
>>
>>        [[alternative HTML version deleted]]
>>
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Mar  7 05:29:02 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 7 Mar 2019 04:29:02 +0000
Subject: [R] Fw: inconsistency in nls output....
In-Reply-To: <940078a8-fade-3e1a-510a-e9cb178a4642@gmail.com>
References: <SL2P216MB009134B81754FD1516678F13C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <SL2P216MB0091DE106FD2088939C0C5A6C8730@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <940078a8-fade-3e1a-510a-e9cb178a4642@gmail.com>
Message-ID: <SL2P216MB009163F2FE7E2C7CE1F2F392C84C0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear JN,
                Thanks for the reply. I will consider using the nlsr package. But for now I make did with reducing the exponent. It is working for me.

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

________________________________
From: J C Nash <profjcnash at gmail.com>
Sent: Wednesday, March 6, 2019 10:40 PM
To: akshay kulkarni; R help Mailing list
Subject: Re: [R] Fw: inconsistency in nls output....

nls() is a Model T Ford trying to drive on the Interstate. The code
is quite old and uses approximations that work well when the user
provides a reasonable problem, but in cases where there are mixed large
and small numbers like yours could get into trouble.

Duncan Murdoch and I prepared the nlsr package to address some
of the weaknesses (in particular we try to use analytic derivatives).

The output of nlsr also gives the singular values of the Jacobian, though
I suspect many R users will have to do some work to interpret those.

You haven't provided a reproducible example. That's almost always the
way to get definitive answers. Otherwise we're guessing as to the issue.

JN

On 2019-03-06 7:48 a.m., akshay kulkarni wrote:
> dear members,
>                             with reference to the attached message:
>
> I think I have found out the problem:
> YLf13 has the structure:
> YLf13 <- a*exp(-1000*LM1); LM1 is another vector.
>
> most of the YLf13 vector is getting populated with zeros, I think, because of the very low value of exp(-1000*LM1). Is there any method in R wherein I can work with these very low values?
>
> Or is the problem not related to the structure of YLf13?
>
> very many thanks for your time and effort...
> yours sincerely,
> AKSHAY M KULKARNI
>
>
> ________________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
> Sent: Wednesday, March 6, 2019 6:02 PM
> To: R help Mailing  list
> Subject: [R] inconsistency in nls output....
>
> dear members,
>                              I have the following nls output:
>
>  Formula: YLf13 ~ (d + e * ((XL)^(1/3)) + f * log(LM3 + 18.81))
>
> Parameters:
>     Estimate Std. Error t value Pr(>|t|)
> d  5.892e-09  8.644e-10   6.817 2.06e-11 ***
> e -6.585e-09  5.518e-10 -11.934  < 2e-16 ***
> f  1.850e-10  2.295e-10   0.806     0.42
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Residual standard error: 9.57e-10 on 677 degrees of freedom
>
> Number of iterations to convergence: 2
> Achieved convergence tolerance: 3.973e-08
>
> ------
> Residual sum of squares: 6.2e-16
>
> ------
> t-based confidence interval:
>            2.5%         97.5%
> d  4.195378e-09  7.589714e-09
> e -7.668142e-09 -5.501342e-09
> f -2.655647e-10  6.354852e-10
>
> ------
> Correlation matrix:
>            d             e             f
> d  1.0000000 -6.202339e-01 -7.832539e-01
> e -0.6202339  1.000000e+00 -2.127301e-05
> f -0.7832539 -2.127301e-05  1.000000e+00
>
>
> if I let XL = 1.1070513 and LM3 = 0.3919 , and consider the coeffs as given above, the right hand side of the above equation is negative.
> But YLf13 is always positive! How is this possible? Am I interpreting the result of the nls output properly?  Should I interpret the coeffs differently? I have done hours of thinking over the above problem but couldn't find any results...
>
> I cannot provide the full values of YLf13, XL and LM3 due to IPR issues....please cooperate......however, if the only way to solve the problem is to give these values, I would indeed give them.....
>
> Also forgive me if there is a minor mistake in my calculations... or a typo....
>
> very many thanks for your time and effort....
> yours sincerely,
> AKSHAY M KULKARNI
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rmende|@@ @end|ng |rom gm@||@com  Wed Mar  6 23:10:34 2019
From: rmende|@@ @end|ng |rom gm@||@com (rmendelss gmail)
Date: Wed, 6 Mar 2019 14:10:34 -0800
Subject: [R] Zoom In/Out  maps library
In-Reply-To: <001801d4d465$b7ac1300$27043900$@sbcglobal.net>
References: <001801d4d465$b7ac1300$27043900$@sbcglobal.net>
Message-ID: <C5024B8F-F2E1-4371-ABC5-DD93D60D70A6@gmail.com>

world.map <- maps::map("world", plot = FALSE, fill = TRUE)
p <- sf:: st_as_sf(world.map, coords = c('x', 'y'))
map view::map view(p)

HTH,

-Roy

> On Mar 6, 2019, at 1:44 PM, reichmanj at sbcglobal.net wrote:
> 
> R Help
> 
> Anyone know if I can add a zoom In/Out function to the maps available via the "maps" library? Or do I need to use a different mapping library?
> 
> world.map <- map_data("world")
> 
> ggplot(data = world.map) +
>  geom_polygon(mapping = aes(x=long, y=lat, group=group))
> 
> Jeff
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @n@b@v|z@deh @end|ng |rom ucd@v|@@edu  Wed Mar  6 23:29:41 2019
From: @n@b@v|z@deh @end|ng |rom ucd@v|@@edu (Aryana Nabavizadeh)
Date: Wed, 6 Mar 2019 14:29:41 -0800
Subject: [R] R Coding Help
Message-ID: <D5D75EBC-AF0B-4279-91D4-26B098AA0ADE@ucdavis.edu>

Hello,
 I uploaded a square matrix to R and followed the exact coding, however, the coding is incorrect. I was wondering if you could take a look at my coding to see where the issue is. Attached below is my square matrix and a screenshot of the coding on R. However, I did not add the labels to the columns and rows as I had issues with the header as I would upload the matrix (meaning that I used the file labeled phdtext.csv) . However, I also do not know if I should keep the labels or not (and instead use the file PHDfullmatrix2.csv). The point of doing this was to generate a knowledge structure graph with the words as the nodes and the distances from one-another representing the similarity of association of the two words (the shorter the distance, the more the two words were associated with one another). At the end of this, I would get the eccentricity values and was wondering if I could get help with where I went wrong. Attached below as well is the associated link that I followed with instructions.

http://www.wouterspekkink.org/r/mds/gephi/2015/12/15/a-simple-example-of-mds-using-r-and-gephi.html


Thank you,

Aryana Nabavizadeh

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2019-03-06 at 2.18.03 PM.png
Type: image/png
Size: 279950 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190306/6848e0dd/attachment.png>

-------------- next part --------------


-------------- next part --------------


From |@b|o@|@non| @end|ng |rom pop@o@|t  Thu Mar  7 12:33:58 2019
From: |@b|o@|@non| @end|ng |rom pop@o@|t (Fabio Fanoni)
Date: Thu, 7 Mar 2019 11:33:58 +0000
Subject: [R] gls() error after R upgrade
Message-ID: <c996a8ec4b4644a4ab33167bf6359513@popso.it>

Dear all,

I have been using the package nlme for about a year

Recently, following an update of R, in some situations the gls() function does not work.


The following lines of code

require(nlme)
db<-read.csv2("G:\\Dati.csv",as.is=TRUE)
f<-"DELTA_INVNORM_STD ~ 0+ ITOD + EU_CH"
gls(as.formula(f),correlation=corARMA(p=2,q=3,fixed=FALSE),data=db,weights=varExp(),method="ML")



with the old version of R returns the following output:

Generalized least squares fit by maximum likelihood
  Model: as.formula(f)
  Data: db
  Log-likelihood: 12.11128

Coefficients:
      ITOD      EU_CH
-0.1053291 -0.4406068

Correlation Structure: ARMA(2,3)
Formula: ~1
 Parameter estimate(s):
      Phi1       Phi2     Theta1     Theta2     Theta3
 0.6485554 -0.1750409  0.9450232  0.9566344  0.9566288
Variance function:
Structure: Exponential of variance covariate
Formula: ~fitted(.)
 Parameter estimates:
        expon
-0.0007047246
Degrees of freedom: 71 total; 69 residual
Residual standard error: 0.7740528



while with the new one version of R ends with the following error message

Error in `coef<-.corARMA`(`*tmp*`, value = value[parMap[, i]]) :
  NA/NaN/Inf in foreign function call (arg 1)




Here are the parameters of the old version of R



> R.version

               _

platform       i386-w64-mingw32

arch           i386

os             mingw32

system         i386, mingw32

status

major          3

minor          3.2

year           2016

month          10

day            31

svn rev        71607

language       R

version.string R version 3.3.2 (2016-10-31)

nickname       Sincere Pumpkin Patch



> sessionInfo()

R version 3.3.2 (2016-10-31)

Platform: i386-w64-mingw32/i386 (32-bit)

Running under: Windows 10 x64 (build 16299)



locale:

[1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252    LC_MONETARY=Italian_Italy.1252

[4] LC_NUMERIC=C                   LC_TIME=Italian_Italy.1252



attached base packages:

[1] stats     graphics  grDevices utils     datasets  methods   base



other attached packages:

[1] nlme_3.1-128



loaded via a namespace (and not attached):

[1] grid_3.3.2      lattice_0.20-34




and the correspondents of the new one

> R.version
               _
platform       i386-w64-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          3
minor          5.2
year           2018
month          12
day            20
svn rev        75870
language       R
version.string R version 3.5.2 (2018-12-20)
nickname       Eggshell Igloo

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows 10 x64 (build 16299)

Matrix products: default

locale:
[1] LC_COLLATE=Italian_Italy.1252  LC_CTYPE=Italian_Italy.1252
[3] LC_MONETARY=Italian_Italy.1252 LC_NUMERIC=C
[5] LC_TIME=Italian_Italy.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] nlme_3.1-137

loaded via a namespace (and not attached):
[1] compiler_3.5.2  grid_3.5.2      lattice_0.20-38


The data used are shown below


I wonder if someone could help me to solve this problem.

Best regards,

Fabio Fanoni


DELTA_INVNORM_STD;ITOD;EU_CH
-1,01888891391635;1,11744160507933;0,164921569515933
-1,7287582158078;1,33122211515022;0,192399322297369
-1,92084919508607;1,42620153798496;0,119394703266418
-1,07481132504896;1,40851069525448;-0,21451386962765
0,368325144516397;1,143582080184;-0,541418306765893
1,60232300735139;0,877753890833926;-0,540071359447086
1,79762928248747;0,585114281768911;-0,457099318942592
0,91995670533225;0,300270733467356;-0,292232806294851
-0,0262660321428139;0,20146037523289;-0,236469135573377
-1,02121152912805;0,0419782544708825;-0,345437279939175
-1,10672650470506;0,0574913198102401;-0,366584376685452
-1,39178681241528;0,33582796272111;-0,222730255141816
-1,1454072922613;0,119188584353956;-0,00950828000562807
-0,958672816825354;-0,0667970836192611;0,4570747351185
-1,32272038017045;-0,307712992787705;0,958678420940093
-0,907400017348963;-0,781894397724971;1,3426934842003
-1,24178782808971;-0,502123817789113;1,78543551198139
-0,727279530248652;-0,198729005124588;1,65424271463061
-0,663683090224516;-0,00292614483254617;1,29016249027665
-0,870662613154294;0,284152373447782;0,852269477692287
-0,740956260890321;0,212558271927232;0,359286261984589
-0,875175106507846;0,16921613723138;0,301636857743261
-0,159691021207123;0,344902273564796;0,406160071512065
0,485359221230746;0,405193753056144;0,545299871378597
0,91806657022351;0,51772409209882;0,664639525858513
0,94182403298868;0,624398008842597;0,721750147939642
0,772810225693767;0,669380547864261;0,746938090683181
0,454079892235428;0,829860577818214;0,875841077996076
0,216198540864624;0,85936757871922;1,06778126286636
-0,390013295840405;0,820301653758161;1,32356681853436
-1,08957286663887;0,735286971796754;1,51483352772482
-1,7181964072478;0,54188741408352;1,59807495122772
-2,14142427321141;0,3080114833751;1,30188093962788
-1,73631132303729;0,242276386254216;0,821154955431617
-1,14832298161027;-0,168559611137128;0,390400772122712
-0,199657255063557;-0,673990227176314;-0,425580724204736
0,637711021315926;-1,26311924357155;-0,777807797606142
1,13372564859946;-2,22697608948676;-1,02793616079627
1,3387228312938;-2,67326030450369;-1,25476231880355
1,13410628583832;-2,41425486942835;-0,769187327492251
1,20371911407773;-1,73317141026889;-0,4868668898436
1,04245681382429;-0,627677467262121;-0,517038541303494
1,19122635858486;0,318264079861633;-0,902669938921763
1,10644115835792;0,53399823108225;-1,58826681113873
0,657027862693417;0,566924775110004;-2,16826290930735
0,391546504792817;0,382793958142103;-2,37380727704873
-0,121513593301095;0,113508212335312;-2,28814134594747
0,090587410027589;-0,10456301293743;-1,92150191643467
0,293363258062962;-0,599038773090656;-1,5279235122598
0,490646403474033;-1,16804784867814;-1,10646327070376
0,663013157660536;-1,68022387383074;-0,281457211581005
0,48006697458688;-2,17535440458421;0,0140633243390067
0,33992207153128;-2,15751594621565;0,415319331867849
0,485131796838891;-1,91164287702336;0,74289724468591
0,645057199360022;-1,64072137764755;0,715150100824384
0,830719890823037;-1,20839230550334;0,886616664628225
1,00060688160202;-0,799350150825283;0,78775063121476
0,799516770753564;-0,474799224784852;0,620190215847712
0,884527141039385;-0,111321336688718;0,400772282236831
0,77592825475446;0,41873781137028;0,214623979323192
0,897761905934407;0,618216183184323;-0,371702774880583
1,23087946317063;0,86282014652723;-1,04275259746654
1,1730433788867;1,03592979384644;-1,50933562067236
1,18413315011977;0,843466359597336;-1,89254251877396
0,522306442129412;0,786432900675309;-1,199941514842
-0,423472231576842;0,534354713733984;-0,258289703958632
-1,02160314495135;0,445296924001273;0,367637345059225
-1,2194743130469;0,791437804698057;0,831122380946011
-0,682787491324478;0,735393577499657;0,636488297397258
-0,277058822726022;0,988210808618591;0,362382895620209
-0,0283252883031908;1,18581655608866;0,466771415869385



AVVERTENZE LEGALI - I contenuti di questo messaggio, proveniente da un indirizzo di posta elettronica aziendale della Banca Popolare di Sondrio, e gli eventuali allegati possono essere letti e utilizzati, per esigenze lavorative, da chi opera alle dipendenze o per conto della stessa, o ? comunque cointeressato nell'inerente relazione d'affari. Le dichiarazioni, ivi contenute, non impegnano contrattualmente la Banca Popolare di Sondrio se non nei limiti di quanto eventualmente previsto in accordi opportunamente formalizzati. Se il messaggio ? stato ricevuto per errore ce ne scusiamo, pregando di segnalare ci? al mittente e poi di distruggerlo senza farne alcun uso poich? l'utilizzo senza averne diritto ? vietato dalla legge e potrebbe costituire reato.

DATI SOCIETARI - BANCA POPOLARE DI SONDRIO - Societ? cooperativa per azioni - Fondata nel 1871
Sede sociale e direzione generale: I - 23100 SONDRIO SO - piazza Garibaldi, 16
Indirizzo Internet: http:[doppiabarra]www[punto]popso[punto]it - E-mail: info[chiocciola]popso[punto]it
Iscritta al Registro delle Imprese di Sondrio al n. 00053810149, all'Albo delle Banche al n. 842, all'Albo delle Societ? Cooperative al n. A160536 Capogruppo del Gruppo bancario Banca Popolare di Sondrio, iscritto all'Albo dei Gruppi bancari al n. 5696.0 Aderente al Fondo Interbancario di Tutela dei Depositi e al Fondo Nazionale di Garanzia - Codice fiscale e partita IVA: 00053810149
Capitale sociale: EUR 1.360.157.331 - Riserve: EUR 947.325.264 (Dati approvati dall'Assemblea dei soci del 29 aprile 2017).

N.B. I "filtri antivirus" e "antispam" in uso su molti sistemi di posta elettronica possono talvolta ritardare o impedire, in tutto o in parte, il recapito dei messaggi. In tali casi, salvo verifica di avvenuta ricezione, pu? essere necessario modificare i contenuti o le modalit? d'invio.


	[[alternative HTML version deleted]]


From g|uc||||@ @end|ng |rom gm@||@com  Thu Mar  7 00:05:20 2019
From: g|uc||||@ @end|ng |rom gm@||@com (Giuseppe Cillis)
Date: Thu, 7 Mar 2019 00:05:20 +0100
Subject: [R] Sankey Diagram
Message-ID: <CAE43vpj3jqDDt4ggfEYJy39dpZvw89dkqoSAy0PROF43VdaLTg@mail.gmail.com>

Hi,
I have a .csv hat is organized in this way:

code88;code13;sup_88_13
1;1;77.67576251
1;2;10.1
1;3;2.4
1;4;0.0
1;5;0.2
1;6;0.1
..............
I would create a simple Sankey diagram, how can I do it? I have little
experience with R unfortunately !
thanks

Giuseppe

	[[alternative HTML version deleted]]


From jrkr|de@u @end|ng |rom gm@||@com  Thu Mar  7 21:20:16 2019
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Thu, 7 Mar 2019 15:20:16 -0500
Subject: [R] R Coding Help
In-Reply-To: <D5D75EBC-AF0B-4279-91D4-26B098AA0ADE@ucdavis.edu>
References: <D5D75EBC-AF0B-4279-91D4-26B098AA0ADE@ucdavis.edu>
Message-ID: <CAKZQJMDfZM+tj7DXYvW7NyPEJC1-RE5VfOJtwWSmZvv241dsQA@mail.gmail.com>

Thanks for the screenshot but it is not really all that useful. We
really need the actual code and some sample data.

Have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
or  http://adv-r.had.co.nz/Reproducibility.html for some suggesitions.

Providing the sample data in dput() format is the best way to supply it.

On Thu, 7 Mar 2019 at 13:19, Aryana Nabavizadeh
<anabavizadeh at ucdavis.edu> wrote:
>
> Hello,
>  I uploaded a square matrix to R and followed the exact coding, however, the coding is incorrect. I was wondering if you could take a look at my coding to see where the issue is. Attached below is my square matrix and a screenshot of the coding on R. However, I did not add the labels to the columns and rows as I had issues with the header as I would upload the matrix (meaning that I used the file labeled phdtext.csv) . However, I also do not know if I should keep the labels or not (and instead use the file PHDfullmatrix2.csv). The point of doing this was to generate a knowledge structure graph with the words as the nodes and the distances from one-another representing the similarity of association of the two words (the shorter the distance, the more the two words were associated with one another). At the end of this, I would get the eccentricity values and was wondering if I could get help with where I went wrong. Attached below as well is the associated link that I followed with instructions.
>
> http://www.wouterspekkink.org/r/mds/gephi/2015/12/15/a-simple-example-of-mds-using-r-and-gephi.html
>
>
> Thank you,
>
> Aryana Nabavizadeh
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From jrkr|de@u @end|ng |rom gm@||@com  Thu Mar  7 21:44:40 2019
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Thu, 7 Mar 2019 15:44:40 -0500
Subject: [R] Sankey Diagram
In-Reply-To: <CAE43vpj3jqDDt4ggfEYJy39dpZvw89dkqoSAy0PROF43VdaLTg@mail.gmail.com>
References: <CAE43vpj3jqDDt4ggfEYJy39dpZvw89dkqoSAy0PROF43VdaLTg@mail.gmail.com>
Message-ID: <CAKZQJMDhjsyFLDeCKcyLAB0DCTtt2p=a+3tr-E3+7PANs3D1Ng@mail.gmail.com>

I would suggest googling "Sankey Diagrams R statistics" which should
provide some suggestions such as
https://www.displayr.com/how-to-create-sankey-diagrams-from-tables-using-r/
or https://plot.ly/r/sankey-diagram/  or perhaps
https://www.r-bloggers.com/creating-custom-sankey-diagrams-using-r/ ?

If you are very new to R I would suggest searching out some R
tutorials or basic introductions to R. A good place to start is the
Contributed list at R Cran (https://cran.r-project.org/).

For asking questions on R -help you might want to have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
or http://adv-r.had.co.nz/Reproducibility.html .

In particular, the advice about providing sample data in dput() format
is important.


On Thu, 7 Mar 2019 at 15:33, Giuseppe Cillis <giucillis at gmail.com> wrote:
>
> Hi,
> I have a .csv hat is organized in this way:
>
> code88;code13;sup_88_13
> 1;1;77.67576251
> 1;2;10.1
> 1;3;2.4
> 1;4;0.0
> 1;5;0.2
> 1;6;0.1
> ..............
> I would create a simple Sankey diagram, how can I do it? I have little
> experience with R unfortunately !
> thanks
>
> Giuseppe
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
John Kane
Kingston ON Canada


From |n@h @end|ng |rom b|@vt@edu  Thu Mar  7 22:03:55 2019
From: |n@h @end|ng |rom b|@vt@edu (Ina Hoeschele)
Date: Thu, 7 Mar 2019 16:03:55 -0500
Subject: [R] glmmPQL(MASS), logistic kernel machine regression,
 Gaussian process
Message-ID: <CAC6SyKSAYypbd5K+1qn5Yci1=wrGQRdw54C40A1i2z+0BOT06g@mail.gmail.com>

 Hi, I am trying to fit a logistic kernel machine regression model (with a
Gaussian kernel) in R using glmmPQL. Does anyone have experience with this
and maybe can provide some example code?
Thanks, Ina

	[[alternative HTML version deleted]]


From dc@r|@on @end|ng |rom t@mu@edu  Thu Mar  7 22:04:25 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Thu, 7 Mar 2019 21:04:25 +0000
Subject: [R] R Coding Help
In-Reply-To: <CAKZQJMDfZM+tj7DXYvW7NyPEJC1-RE5VfOJtwWSmZvv241dsQA@mail.gmail.com>
References: <D5D75EBC-AF0B-4279-91D4-26B098AA0ADE@ucdavis.edu>
 <CAKZQJMDfZM+tj7DXYvW7NyPEJC1-RE5VfOJtwWSmZvv241dsQA@mail.gmail.com>
Message-ID: <54061e28253b4c0db1b85bdfb8019cdd@tamu.edu>

We can see a few errors in what you have given us, but not enough to solve your problem as John indicates.

Looking at your Environment window, you have a data frame "phdtext" which is displayed in the upper left window. That appears to be a symmetrical 17 x 17 matrix so it is not clear why you are trying to create another one with the as.matrix(read.table()) command. It is not working because phDMatrix is empty as is phdDist.

Your first visible error message involves trying to install package vegan without using "vegan", i.e. install.packages("vegan"). That was not fatal because you were able to use library(vegan) to load it (i.e. you had already installed vegan). 

Based on these hints, the following code MIGHT work:

phdDist <- as.dist(phdtext)
phdMDS <- metaMDS(phdDist)

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of John Kane
Sent: Thursday, March 7, 2019 2:20 PM
To: Aryana Nabavizadeh <anabavizadeh at ucdavis.edu>
Cc: R. Help Mailing List <r-help at r-project.org>
Subject: Re: [R] R Coding Help

Thanks for the screenshot but it is not really all that useful. We
really need the actual code and some sample data.

Have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
or  http://adv-r.had.co.nz/Reproducibility.html for some suggesitions.

Providing the sample data in dput() format is the best way to supply it.

On Thu, 7 Mar 2019 at 13:19, Aryana Nabavizadeh
<anabavizadeh at ucdavis.edu> wrote:
>
> Hello,
>  I uploaded a square matrix to R and followed the exact coding, however, the coding is incorrect. I was wondering if you could take a look at my coding to see where the issue is. Attached below is my square matrix and a screenshot of the coding on R. However, I did not add the labels to the columns and rows as I had issues with the header as I would upload the matrix (meaning that I used the file labeled phdtext.csv) . However, I also do not know if I should keep the labels or not (and instead use the file PHDfullmatrix2.csv). The point of doing this was to generate a knowledge structure graph with the words as the nodes and the distances from one-another representing the similarity of association of the two words (the shorter the distance, the more the two words were associated with one another). At the end of this, I would get the eccentricity values and was wondering if I could get help with where I went wrong. Attached below as well is the associated link that I followed with i
 nstructions.
>
> http://www.wouterspekkink.org/r/mds/gephi/2015/12/15/a-simple-example-of-mds-using-r-and-gephi.html
>
>
> Thank you,
>
> Aryana Nabavizadeh
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Fri Mar  8 13:39:15 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Fri, 8 Mar 2019 12:39:15 +0000
Subject: [R] rounding off problem.....
Message-ID: <SL2P216MB0091AFFB591A566E21768361C84D0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members....
                                 here is a piece of my code:

> tail(YLf14,15)
 [1] 5.706871e-217 2.563877e-218 2.823295e-218 2.694622e-222 1.777409e-226
 [6] 1.134403e-201 5.269464e-215 2.272121e-219 2.794970e-223 1.630978e-187
[11] 1.721529e-213 5.859815e-178 4.842612e-222 1.333685e-193 1.256051e-174
> YLf16 <- YLf14 + 0.001
> tail(YLf16,15)
 [1] 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001
[13] 0.001 0.001 0.001

Is there any way to avoid the rounding off of YLf16 to 0.001, and take exact values?

very many thanks for your time and effort......
yours sincerely,
AKSHAY M KULKARNI


	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Fri Mar  8 14:01:26 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 8 Mar 2019 15:01:26 +0200
Subject: [R] rounding off problem.....
In-Reply-To: <SL2P216MB0091AFFB591A566E21768361C84D0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091AFFB591A566E21768361C84D0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAGgJW74uiUOfEWdH_K0ezQ9WvPVT3vfAPEuWCEHXDbVM_=pZqA@mail.gmail.com>

"Is there any way ..."
Two quick answers:
1. using base R functions and data types the answer is 'no' - a double
(i.e. numeric) contains about 15 significant digits.
    So 5.678e-100 is fine but   0.01 + 5.678e-100 will keep the
.0100000000000000  as the significant digits and "drop" the digits 80 or so
places further to the right.
2. there are packages that provide for arbitrary precision calculations
    see R CRAN package Rmpfr
     https://cran.r-project.org/web/packages/Rmpfr/vignettes/Rmpfr-pkg.pdf

HTH,
Eric

On Fri, Mar 8, 2019 at 2:39 PM akshay kulkarni <akshay_e4 at hotmail.com>
wrote:

> dear members....
>                                  here is a piece of my code:
>
> > tail(YLf14,15)
>  [1] 5.706871e-217 2.563877e-218 2.823295e-218 2.694622e-222 1.777409e-226
>  [6] 1.134403e-201 5.269464e-215 2.272121e-219 2.794970e-223 1.630978e-187
> [11] 1.721529e-213 5.859815e-178 4.842612e-222 1.333685e-193 1.256051e-174
> > YLf16 <- YLf14 + 0.001
> > tail(YLf16,15)
>  [1] 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001
> 0.001
> [13] 0.001 0.001 0.001
>
> Is there any way to avoid the rounding off of YLf16 to 0.001, and take
> exact values?
>
> very many thanks for your time and effort......
> yours sincerely,
> AKSHAY M KULKARNI
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kev|n@thorpe @end|ng |rom utoronto@c@  Fri Mar  8 14:04:41 2019
From: kev|n@thorpe @end|ng |rom utoronto@c@ (Kevin Thorpe)
Date: Fri, 8 Mar 2019 13:04:41 +0000
Subject: [R] rounding off problem.....
Message-ID: <6B00EBAF-B936-4FD2-94D4-A66E20488F6C@utoronto.ca>

I'm no expert in R internals or floating point computation, however, two things come to mind.

First, I suspect the exact value is stored. It is just the printing that looks rounded. That is likely because 0.001 completely dominates the rest. To print in full precision, you would need over 200 digits for some of your values.

Second, you may be pushing the limits of precision. It seems to me your original values are indistinguishable from zero. If they really represent materially different values, you might want to rescale them to improve computational reliability. 

-- 
Kevin E. Thorpe
Head of Biostatistics,  Applied Health Research Centre (AHRC)
Li Ka Shing Knowledge Institute of St. Michael's
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016
 

?On 2019-03-08, 7:39 AM, "R-help on behalf of akshay kulkarni" <r-help-bounces at r-project.org on behalf of akshay_e4 at hotmail.com> wrote:

    dear members....
                                     here is a piece of my code:
    
    > tail(YLf14,15)
     [1] 5.706871e-217 2.563877e-218 2.823295e-218 2.694622e-222 1.777409e-226
     [6] 1.134403e-201 5.269464e-215 2.272121e-219 2.794970e-223 1.630978e-187
    [11] 1.721529e-213 5.859815e-178 4.842612e-222 1.333685e-193 1.256051e-174
    > YLf16 <- YLf14 + 0.001
    > tail(YLf16,15)
     [1] 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001
    [13] 0.001 0.001 0.001
    
    Is there any way to avoid the rounding off of YLf16 to 0.001, and take exact values?
    
    very many thanks for your time and effort......
    yours sincerely,
    AKSHAY M KULKARNI
    
    
    	[[alternative HTML version deleted]]
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From @||v@no @end|ng |rom ue|@br  Fri Mar  8 18:51:20 2019
From: @||v@no @end|ng |rom ue|@br (Silvano Cesar da Costa)
Date: Fri, 8 Mar 2019 14:51:20 -0300
Subject: [R] Common elements
Message-ID: <CAMaZ2WBBObtfdh4y+KOJipQRC7aXh-5_cZ3psv3nFRHGi6RRBQ@mail.gmail.com>

Hi,

I have a dataset with ten columns, but I need extract only lines that has
common elements.
The columns are:

      Animal Mother
 [1,]   1143    430
 [2,]   1144    134
 [3,]   1146      3
 [4,]   1147    151
 [5,]   1150    230
 [6,]   1156    290
 [7,]   1157    227
 [8,]   1159    757
 [9,]   1160      3
[10,]   1161    236
[11,]   1162    231
[12,]   1164    132
[13,]   1165    420
[14,]   1168    290
[15,]   1169    229
[16,]   1172    425
[17,]   1173    134
[18,]   1174    234
[19,]   1175    233
[20,]   1178    239
[21,]   1179    757
[22,]   1180    236
[23,]   1185    420
[24,]   1186    389
[25,]   1190    425
[26,]   1192    235

How can I do this?

Thanks.

Prof. Dr. Silvano Cesar da Costa
Universidade Estadual de Londrina
Centro de Ci?ncias Exatas
Departamento de Estat?stica

Fone: (43) 3371-4346

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Mar  8 19:31:41 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 08 Mar 2019 10:31:41 -0800
Subject: [R] Common elements
In-Reply-To: <CAMaZ2WBBObtfdh4y+KOJipQRC7aXh-5_cZ3psv3nFRHGi6RRBQ@mail.gmail.com>
References: <CAMaZ2WBBObtfdh4y+KOJipQRC7aXh-5_cZ3psv3nFRHGi6RRBQ@mail.gmail.com>
Message-ID: <635CFF9A-1DC3-4727-8245-25FA8C6D9558@dcn.davis.ca.us>

This request is quite unclear. Can you make a reproducible example [1][2][3] and provide data that represents the expected result corresponding to the input data? The dput function is much much better than tabular form for this purpose.

For one thing, you mention ten columns, but they are not shown?

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

[2] http://adv-r.had.co.nz/Reproducibility.html

[3] https://cran.r-project.org/web/packages/reprex/index.html (read the vignette)

On March 8, 2019 9:51:20 AM PST, Silvano Cesar da Costa <silvano at uel.br> wrote:
>Hi,
>
>I have a dataset with ten columns, but I need extract only lines that
>has
>common elements.
>The columns are:
>
>      Animal Mother
> [1,]   1143    430
> [2,]   1144    134
> [3,]   1146      3
> [4,]   1147    151
> [5,]   1150    230
> [6,]   1156    290
> [7,]   1157    227
> [8,]   1159    757
> [9,]   1160      3
>[10,]   1161    236
>[11,]   1162    231
>[12,]   1164    132
>[13,]   1165    420
>[14,]   1168    290
>[15,]   1169    229
>[16,]   1172    425
>[17,]   1173    134
>[18,]   1174    234
>[19,]   1175    233
>[20,]   1178    239
>[21,]   1179    757
>[22,]   1180    236
>[23,]   1185    420
>[24,]   1186    389
>[25,]   1190    425
>[26,]   1192    235
>
>How can I do this?
>
>Thanks.
>
>Prof. Dr. Silvano Cesar da Costa
>Universidade Estadual de Londrina
>Centro de Ci?ncias Exatas
>Departamento de Estat?stica
>
>Fone: (43) 3371-4346
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From HDor@n @end|ng |rom @|r@org  Fri Mar  8 19:42:08 2019
From: HDor@n @end|ng |rom @|r@org (Doran, Harold)
Date: Fri, 8 Mar 2019 18:42:08 +0000
Subject: [R] Common elements
In-Reply-To: <CAMaZ2WBBObtfdh4y+KOJipQRC7aXh-5_cZ3psv3nFRHGi6RRBQ@mail.gmail.com>
References: <CAMaZ2WBBObtfdh4y+KOJipQRC7aXh-5_cZ3psv3nFRHGi6RRBQ@mail.gmail.com>
Message-ID: <D8A821A1.576BB%hdoran@air.org>

Do you mean like this?

tmp <- data.frame(v1 = c(1,2,1,4,2), v2 = c(10, 11, 10, 14, 11))
vals <- paste(tmp$v1, tmp$v2, sep ='')
tmp[which(vals %in% vals[duplicated(vals)]),]



On 3/8/19, 12:51 PM, "Silvano Cesar da Costa" <silvano at uel.br> wrote:

>Hi,
>
>I have a dataset with ten columns, but I need extract only lines that has
>common elements.
>The columns are:
>
>      Animal Mother
> [1,]   1143    430
> [2,]   1144    134
> [3,]   1146      3
> [4,]   1147    151
> [5,]   1150    230
> [6,]   1156    290
> [7,]   1157    227
> [8,]   1159    757
> [9,]   1160      3
>[10,]   1161    236
>[11,]   1162    231
>[12,]   1164    132
>[13,]   1165    420
>[14,]   1168    290
>[15,]   1169    229
>[16,]   1172    425
>[17,]   1173    134
>[18,]   1174    234
>[19,]   1175    233
>[20,]   1178    239
>[21,]   1179    757
>[22,]   1180    236
>[23,]   1185    420
>[24,]   1186    389
>[25,]   1190    425
>[26,]   1192    235
>
>How can I do this?
>
>Thanks.
>
>Prof. Dr. Silvano Cesar da Costa
>Universidade Estadual de Londrina
>Centro de Ci?ncias Exatas
>Departamento de Estat?stica
>
>Fone: (43) 3371-4346
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Mar  8 20:37:42 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 Mar 2019 19:37:42 +0000
Subject: [R] Common elements
In-Reply-To: <CAMaZ2WBBObtfdh4y+KOJipQRC7aXh-5_cZ3psv3nFRHGi6RRBQ@mail.gmail.com>
References: <CAMaZ2WBBObtfdh4y+KOJipQRC7aXh-5_cZ3psv3nFRHGi6RRBQ@mail.gmail.com>
Message-ID: <6d9277ec-0b78-704f-9601-5aefeb0b3b20@sapo.pt>

Hello,

If I understand correctly, here are two different ways of doing what you 
want.

1) base R.

i <- duplicated(df1[[2]])
j <- duplicated(df1[[2]], fromLast = TRUE)
res <- df1[i | j, ]

res[order(res[[2]]), ]    # not strictly needed


2) with package dplyr. If you do not want to order by Mother,
delete the last %>%, at the end of the line and the next line, 
arrange(Mother).

library(dplyr)

df1 %>% group_by(Mother) %>% filter(n() > 1) %>%
   arrange(Mother)


#------------ dataset ------------------
# Note that your data seems to be a matrix,
# read.table creates data.frames

df1 <- read.table(text = "
       Animal Mother
[1,]   1143    430
[2,]   1144    134
[3,]   1146      3
[4,]   1147    151
[5,]   1150    230
[6,]   1156    290
[7,]   1157    227
[8,]   1159    757
[9,]   1160      3
[10,]   1161    236
[11,]   1162    231
[12,]   1164    132
[13,]   1165    420
[14,]   1168    290
[15,]   1169    229
[16,]   1172    425
[17,]   1173    134
[18,]   1174    234
[19,]   1175    233
[20,]   1178    239
[21,]   1179    757
[22,]   1180    236
[23,]   1185    420
[24,]   1186    389
[25,]   1190    425
[26,]   1192    235
", header = TRUE)
row.names(df1) <- NULL



Hope this helps,

Rui Barradas


?s 17:51 de 08/03/2019, Silvano Cesar da Costa escreveu:
> Hi,
> 
> I have a dataset with ten columns, but I need extract only lines that has
> common elements.
> The columns are:
> 
>        Animal Mother
>   [1,]   1143    430
>   [2,]   1144    134
>   [3,]   1146      3
>   [4,]   1147    151
>   [5,]   1150    230
>   [6,]   1156    290
>   [7,]   1157    227
>   [8,]   1159    757
>   [9,]   1160      3
> [10,]   1161    236
> [11,]   1162    231
> [12,]   1164    132
> [13,]   1165    420
> [14,]   1168    290
> [15,]   1169    229
> [16,]   1172    425
> [17,]   1173    134
> [18,]   1174    234
> [19,]   1175    233
> [20,]   1178    239
> [21,]   1179    757
> [22,]   1180    236
> [23,]   1185    420
> [24,]   1186    389
> [25,]   1190    425
> [26,]   1192    235
> 
> How can I do this?
> 
> Thanks.
> 
> Prof. Dr. Silvano Cesar da Costa
> Universidade Estadual de Londrina
> Centro de Ci?ncias Exatas
> Departamento de Estat?stica
> 
> Fone: (43) 3371-4346
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From or|o|eb@|t|more @end|ng |rom gm@||@com  Fri Mar  8 23:53:41 2019
From: or|o|eb@|t|more @end|ng |rom gm@||@com (Adrian Johnson)
Date: Fri, 8 Mar 2019 17:53:41 -0500
Subject: [R] pheatmap - clustering of select columns
Message-ID: <CAL2fYnNNmmXfA=tx74uP9xYO3LX8zYD8G6_3DRKu03OLOXa9Pg@mail.gmail.com>

Dear group,
In pheatmap, is it possible to cluster select colmns.
For example, in a matrix of 10 rows and 20 columns. I want to cluster
only those columns 1-10 and cluster rows. Similarly, retaining the
same row clustering resulted from clustering of columns 1-10, I want
to enforce clustering of 11-20 and not clustering rows.
Is it possible. Currently I use cluster_cols=TRUE, but this clusters
all columns.
Thanks for your help

Adrian.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Mar  9 06:25:40 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 9 Mar 2019 05:25:40 +0000
Subject: [R] rounding off problem.....
In-Reply-To: <CAGgJW74uiUOfEWdH_K0ezQ9WvPVT3vfAPEuWCEHXDbVM_=pZqA@mail.gmail.com>
References: <SL2P216MB0091AFFB591A566E21768361C84D0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <CAGgJW74uiUOfEWdH_K0ezQ9WvPVT3vfAPEuWCEHXDbVM_=pZqA@mail.gmail.com>
Message-ID: <SL2P216MB0091935E4216505F45359E0CC84E0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear EB,
                 I checked the Rmpfr package. Thats exactly what I needed....! Thanks a lot.

very many thanks for your time and effort,
yours sincerely,

AKSHAY M KULKARNI


________________________________
From: Eric Berger <ericjberger at gmail.com>
Sent: Friday, March 8, 2019 6:31 PM
To: akshay kulkarni
Cc: R help Mailing list
Subject: Re: [R] rounding off problem.....

"Is there any way ..."
Two quick answers:
1. using base R functions and data types the answer is 'no' - a double (i.e. numeric) contains about 15 significant digits.
    So 5.678e-100 is fine but   0.01 + 5.678e-100 will keep the .0100000000000000  as the significant digits and "drop" the digits 80 or so places further to the right.
2. there are packages that provide for arbitrary precision calculations
    see R CRAN package Rmpfr
     https://cran.r-project.org/web/packages/Rmpfr/vignettes/Rmpfr-pkg.pdf

HTH,
Eric

On Fri, Mar 8, 2019 at 2:39 PM akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>> wrote:
dear members....
                                 here is a piece of my code:

> tail(YLf14,15)
 [1] 5.706871e-217 2.563877e-218 2.823295e-218 2.694622e-222 1.777409e-226
 [6] 1.134403e-201 5.269464e-215 2.272121e-219 2.794970e-223 1.630978e-187
[11] 1.721529e-213 5.859815e-178 4.842612e-222 1.333685e-193 1.256051e-174
> YLf16 <- YLf14 + 0.001
> tail(YLf16,15)
 [1] 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001
[13] 0.001 0.001 0.001

Is there any way to avoid the rounding off of YLf16 to 0.001, and take exact values?

very many thanks for your time and effort......
yours sincerely,
AKSHAY M KULKARNI


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From eu|erg@u@@r|em@nn @end|ng |rom gm@||@com  Sun Mar 10 02:36:17 2019
From: eu|erg@u@@r|em@nn @end|ng |rom gm@||@com (David Goldsmith)
Date: Sat, 9 Mar 2019 17:36:17 -0800
Subject: [R] Q re: logical indexing with is.na
Message-ID: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>

Hi!  Newbie (self-)learning R using P. Dalgaard's "Intro Stats w/ R"; not
new to statistics (have had grad-level courses and work experience in
statistics) or vectorized programming syntax (have extensive experience
with MatLab, Python/NumPy, and IDL, and even a smidgen--a long time ago--of
experience w/ S-plus).

In exploring the use of is.na in the context of logical indexing, I've come
across the following puzzling-to-me result:

> y; !is.na(y[1:3]); y[!is.na(y[1:3])]
[1]  0.3534253 -1.6731597         NA -0.2079209
[1]  TRUE  TRUE FALSE
[1]  0.3534253 -1.6731597 -0.2079209

As you can see, y is a four element vector, the third element of which is
NA; the next line gives what I would expect--T T F--because the first two
elements are not NA but the third element is.  The third line is what
confuses me: why is the result not the two element vector consisting of
simply the first two elements of the vector (or, if vectorized indexing in
R is implemented to return a vector the same length as the logical index
vector, which appears to be the case, at least the first two elements and
then either NA or NaN in the third slot, where the logical indexing vector
is FALSE): why does the implementation "go looking" for an element whose
index in the "original" vector, 4, is larger than BOTH the largest index
specified in the inner-most subsetting index AND the size of the resulting
indexing vector?  (Note: at first I didn't even understand why the result
wasn't simply

0.3534253 -1.6731597         NA

but then I realized that the third logical index being FALSE, there was no
reason for *any* element to be there; but if there is, due to some
overriding rule regarding the length of the result relative to the length
of the indexer, shouldn't it revert back to *something* that indicates the
"FALSE"ness of that indexing element?)

Thanks!

DLG

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS:
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK:
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] ISwR_2.0-7

loaded via a namespace (and not attached):
[1] compiler_3.5.2 tools_3.5.2

	[[alternative HTML version deleted]]


From rmh @end|ng |rom temp|e@edu  Sun Mar 10 03:30:03 2019
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Sat, 9 Mar 2019 21:30:03 -0500
Subject: [R] Q re: logical indexing with is.na
In-Reply-To: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
References: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
Message-ID: <CAGx1TMBPQxCuDQ37ZmnO+=JwyaKjFznu-5t0UB_nWe7m+h0E_g@mail.gmail.com>

>From ?Arithmetic
the elements of shorter
     vectors are recycled as necessary (with a ?warning? when they are
     recycled only _fractionally_).

> tmp <- !is.na(y[1:3])
> tmp
[1]  TRUE  TRUE FALSE
> c(tmp, tmp)
[1]  TRUE  TRUE FALSE  TRUE  TRUE FALSE
> c(tmp, tmp)[1:4]
[1]  TRUE  TRUE FALSE  TRUE
>  y[c(tmp, tmp)[1:4]]
[1]  0.3534253 -1.6731597 -0.2079209
>

The behavior is as documented.  I am surprised that there is no
warning about partial recycling.

On Sat, Mar 9, 2019 at 9:03 PM David Goldsmith
<eulergaussriemann at gmail.com> wrote:
>
> Hi!  Newbie (self-)learning R using P. Dalgaard's "Intro Stats w/ R"; not
> new to statistics (have had grad-level courses and work experience in
> statistics) or vectorized programming syntax (have extensive experience
> with MatLab, Python/NumPy, and IDL, and even a smidgen--a long time ago--of
> experience w/ S-plus).
>
> In exploring the use of is.na in the context of logical indexing, I've come
> across the following puzzling-to-me result:
>
> > y; !is.na(y[1:3]); y[!is.na(y[1:3])]
> [1]  0.3534253 -1.6731597         NA -0.2079209
> [1]  TRUE  TRUE FALSE
> [1]  0.3534253 -1.6731597 -0.2079209
>
> As you can see, y is a four element vector, the third element of which is
> NA; the next line gives what I would expect--T T F--because the first two
> elements are not NA but the third element is.  The third line is what
> confuses me: why is the result not the two element vector consisting of
> simply the first two elements of the vector (or, if vectorized indexing in
> R is implemented to return a vector the same length as the logical index
> vector, which appears to be the case, at least the first two elements and
> then either NA or NaN in the third slot, where the logical indexing vector
> is FALSE): why does the implementation "go looking" for an element whose
> index in the "original" vector, 4, is larger than BOTH the largest index
> specified in the inner-most subsetting index AND the size of the resulting
> indexing vector?  (Note: at first I didn't even understand why the result
> wasn't simply
>
> 0.3534253 -1.6731597         NA
>
> but then I realized that the third logical index being FALSE, there was no
> reason for *any* element to be there; but if there is, due to some
> overriding rule regarding the length of the result relative to the length
> of the indexer, shouldn't it revert back to *something* that indicates the
> "FALSE"ness of that indexing element?)
>
> Thanks!
>
> DLG
>
> > sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS High Sierra 10.13.6
>
> Matrix products: default
> BLAS:
> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
> LAPACK:
> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] ISwR_2.0-7
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.2 tools_3.5.2
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Mar 10 03:57:05 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 10 Mar 2019 15:57:05 +1300
Subject: [R] [FORGED]  Q re: logical indexing with is.na
In-Reply-To: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
References: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
Message-ID: <937cc22c-386d-da86-f8ac-0d6876afbb8d@auckland.ac.nz>

On 3/10/19 2:36 PM, David Goldsmith wrote:
> Hi!  Newbie (self-)learning R using P. Dalgaard's "Intro Stats w/ R"; not
> new to statistics (have had grad-level courses and work experience in
> statistics) or vectorized programming syntax (have extensive experience
> with MatLab, Python/NumPy, and IDL, and even a smidgen--a long time ago--of
> experience w/ S-plus).
> 
> In exploring the use of is.na in the context of logical indexing, I've come
> across the following puzzling-to-me result:
> 
>> y; !is.na(y[1:3]); y[!is.na(y[1:3])]
> [1]  0.3534253 -1.6731597         NA -0.2079209
> [1]  TRUE  TRUE FALSE
> [1]  0.3534253 -1.6731597 -0.2079209
> 
> As you can see, y is a four element vector, the third element of which is
> NA; the next line gives what I would expect--T T F--because the first two
> elements are not NA but the third element is.  The third line is what
> confuses me: why is the result not the two element vector consisting of
> simply the first two elements of the vector (or, if vectorized indexing in
> R is implemented to return a vector the same length as the logical index
> vector, which appears to be the case, at least the first two elements and
> then either NA or NaN in the third slot, where the logical indexing vector
> is FALSE): why does the implementation "go looking" for an element whose
> index in the "original" vector, 4, is larger than BOTH the largest index
> specified in the inner-most subsetting index AND the size of the resulting
> indexing vector?  (Note: at first I didn't even understand why the result
> wasn't simply
> 
> 0.3534253 -1.6731597         NA
> 
> but then I realized that the third logical index being FALSE, there was no
> reason for *any* element to be there; but if there is, due to some
> overriding rule regarding the length of the result relative to the length
> of the indexer, shouldn't it revert back to *something* that indicates the
> "FALSE"ness of that indexing element?)
> 
> Thanks!

It happens because R is eco-concious and re-cycles. :-)

Try:

ok <- c(TRUE,TRUE,FALSE)
(1:4)[ok]

In general in R if there is an operation involving two vectors then
the shorter one gets recycled to provide sufficiently many entries to 
match those of the longer vector.

This in the foregoing example the first entry of "ok" gets used again,
to make a length 4 vector to match up with 1:4.  The result is the same 
as (1:4)[c(TRUE,TRUE,FALSE,TRUE)].

If you did (1:7)[ok] you'd get the same result as that from
(1:7)[c(TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE)] i.e. "ok" gets
recycled 2 and 1/3 times.

Try 10*(1:3) + 1:4, 10*(1:3) + 1:5, 10*(1:3) + 1:6 .

Note that in the first two instances you get warnings, but in the third
you don't, since 6 is an integer multiple of 3.

Why aren't there warnings when logical indexing is used?  I guess 
because it would be annoying.  Maybe.

Note that integer indices get recycled too, but the recycling is limited 
so as not to produce redundancies.  So

(1:4)[1:3] just (sensibly) gives

[1] 1 2 3

and *not*

[1] 1 2 3 1

Perhaps a bit subtle, but it gives what you'd actually *want* rather 
than being pedantic about rules with a result that you wouldn't want.

cheers,

Rolf Turner

P.S.  If you do

y[1:3][!is.na(y[1:3])]

i.e. if you're careful to match the length of the vector and the that of 
the indices, you get what you initially expected.

R. T.

P^2.S.  To the younger and wiser heads on this list:  the help on "[" 
does not mention that the index vectors can be logical.  I couldn't find 
anything about logical indexing in the R help files.  Is something 
missing here, or am I just not looking in the right place?

R. T.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Mar 10 06:07:17 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 Mar 2019 21:07:17 -0800
Subject: [R] [FORGED]  Q re: logical indexing with is.na
In-Reply-To: <937cc22c-386d-da86-f8ac-0d6876afbb8d@auckland.ac.nz>
References: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
 <937cc22c-386d-da86-f8ac-0d6876afbb8d@auckland.ac.nz>
Message-ID: <43C651D9-D213-4D70-B2A8-D6A8C7D14DAD@dcn.davis.ca.us>

Regarding the mention of logical indexing, under ?Extract I see:

For?[-indexing only:?i,?j,?...?can be logical vectors, indicating elements/slices to select. Such vectors are recycled if necessary to match the corresponding extent.?i,?j,?...?can also be negative integers, indicating elements/slices to leave out of the selection.

On March 9, 2019 6:57:05 PM PST, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>On 3/10/19 2:36 PM, David Goldsmith wrote:
>> Hi!  Newbie (self-)learning R using P. Dalgaard's "Intro Stats w/ R";
>not
>> new to statistics (have had grad-level courses and work experience in
>> statistics) or vectorized programming syntax (have extensive
>experience
>> with MatLab, Python/NumPy, and IDL, and even a smidgen--a long time
>ago--of
>> experience w/ S-plus).
>> 
>> In exploring the use of is.na in the context of logical indexing,
>I've come
>> across the following puzzling-to-me result:
>> 
>>> y; !is.na(y[1:3]); y[!is.na(y[1:3])]
>> [1]  0.3534253 -1.6731597         NA -0.2079209
>> [1]  TRUE  TRUE FALSE
>> [1]  0.3534253 -1.6731597 -0.2079209
>> 
>> As you can see, y is a four element vector, the third element of
>which is
>> NA; the next line gives what I would expect--T T F--because the first
>two
>> elements are not NA but the third element is.  The third line is what
>> confuses me: why is the result not the two element vector consisting
>of
>> simply the first two elements of the vector (or, if vectorized
>indexing in
>> R is implemented to return a vector the same length as the logical
>index
>> vector, which appears to be the case, at least the first two elements
>and
>> then either NA or NaN in the third slot, where the logical indexing
>vector
>> is FALSE): why does the implementation "go looking" for an element
>whose
>> index in the "original" vector, 4, is larger than BOTH the largest
>index
>> specified in the inner-most subsetting index AND the size of the
>resulting
>> indexing vector?  (Note: at first I didn't even understand why the
>result
>> wasn't simply
>> 
>> 0.3534253 -1.6731597         NA
>> 
>> but then I realized that the third logical index being FALSE, there
>was no
>> reason for *any* element to be there; but if there is, due to some
>> overriding rule regarding the length of the result relative to the
>length
>> of the indexer, shouldn't it revert back to *something* that
>indicates the
>> "FALSE"ness of that indexing element?)
>> 
>> Thanks!
>
>It happens because R is eco-concious and re-cycles. :-)
>
>Try:
>
>ok <- c(TRUE,TRUE,FALSE)
>(1:4)[ok]
>
>In general in R if there is an operation involving two vectors then
>the shorter one gets recycled to provide sufficiently many entries to 
>match those of the longer vector.
>
>This in the foregoing example the first entry of "ok" gets used again,
>to make a length 4 vector to match up with 1:4.  The result is the same
>
>as (1:4)[c(TRUE,TRUE,FALSE,TRUE)].
>
>If you did (1:7)[ok] you'd get the same result as that from
>(1:7)[c(TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE)] i.e. "ok" gets
>recycled 2 and 1/3 times.
>
>Try 10*(1:3) + 1:4, 10*(1:3) + 1:5, 10*(1:3) + 1:6 .
>
>Note that in the first two instances you get warnings, but in the third
>you don't, since 6 is an integer multiple of 3.
>
>Why aren't there warnings when logical indexing is used?  I guess 
>because it would be annoying.  Maybe.
>
>Note that integer indices get recycled too, but the recycling is
>limited 
>so as not to produce redundancies.  So
>
>(1:4)[1:3] just (sensibly) gives
>
>[1] 1 2 3
>
>and *not*
>
>[1] 1 2 3 1
>
>Perhaps a bit subtle, but it gives what you'd actually *want* rather 
>than being pedantic about rules with a result that you wouldn't want.
>
>cheers,
>
>Rolf Turner
>
>P.S.  If you do
>
>y[1:3][!is.na(y[1:3])]
>
>i.e. if you're careful to match the length of the vector and the that
>of 
>the indices, you get what you initially expected.
>
>R. T.
>
>P^2.S.  To the younger and wiser heads on this list:  the help on "[" 
>does not mention that the index vectors can be logical.  I couldn't
>find 
>anything about logical indexing in the R help files.  Is something 
>missing here, or am I just not looking in the right place?
>
>R. T.

-- 
Sent from my phone. Please excuse my brevity.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Mar 10 06:56:31 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 10 Mar 2019 18:56:31 +1300
Subject: [R] [FORGED] Q re: logical indexing with is.na
In-Reply-To: <43C651D9-D213-4D70-B2A8-D6A8C7D14DAD@dcn.davis.ca.us>
References: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
 <937cc22c-386d-da86-f8ac-0d6876afbb8d@auckland.ac.nz>
 <43C651D9-D213-4D70-B2A8-D6A8C7D14DAD@dcn.davis.ca.us>
Message-ID: <d0e4cc79-0f37-6f43-dde9-8c96335e685f@auckland.ac.nz>


On 3/10/19 6:07 PM, Jeff Newmiller wrote:

> Regarding the mention of logical indexing, under ?Extract I see:
> 
> For [-indexing only: i, j, ... can be logical vectors, indicating
> elements/slices to select. Such vectors are recycled if necessary to
> match the corresponding extent. i, j, ... can also be negative
> integers, indicating elements/slices to leave out of the selection.

Dang!  It was staring me in the face all the time, and I didn't see it!
Grrrrrr.

Thanks Jeff.

cheers,

Rolf


-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From eu|erg@u@@r|em@nn @end|ng |rom gm@||@com  Sun Mar 10 07:15:54 2019
From: eu|erg@u@@r|em@nn @end|ng |rom gm@||@com (David Goldsmith)
Date: Sat, 9 Mar 2019 22:15:54 -0800
Subject: [R] [FORGED] Q re: logical indexing with is.na
In-Reply-To: <43C651D9-D213-4D70-B2A8-D6A8C7D14DAD@dcn.davis.ca.us>
References: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
 <937cc22c-386d-da86-f8ac-0d6876afbb8d@auckland.ac.nz>
 <43C651D9-D213-4D70-B2A8-D6A8C7D14DAD@dcn.davis.ca.us>
Message-ID: <CA+_2Pn3Qfk9uXmiUm_qApYzDHzyPnQ-kAs4Cfzx-OV5xWFpNaw@mail.gmail.com>

Thanks, all.  I had read about recycling, but I guess I didn't fully
appreciate all the "weirdness" it might produce. :/

With this explained, I'm going to ask a follow-up, which is only
contextually related: the impetus for this discovery was checking "corner
cases" to determine if all(x[!is.na(x)]==y[!is.na(y)]) would suffice to
determine equality of two vectors containing NA's.  Between the above
result; my related discovery that this indexing preserves relative
positional info but not absolute positional info; and the performance
penalty when comparing long vectors that may be unequal "early on";  I've
concluded that--if it (can be made to) "short circuit"--it would probably
be better to use an implicit loop.  So that's my Q: will (or can) an
implicit loop (be made to) "exit early" if a specified condition is met
before all indices have been checked?

Thanks again!

DLG

On Sat, Mar 9, 2019 at 9:07 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Regarding the mention of logical indexing, under ?Extract I see:
>
> For [-indexing only: i, j, ... can be logical vectors, indicating
> elements/slices to select. Such vectors are recycled if necessary to match
> the corresponding extent. i, j, ... can also be negative integers,
> indicating elements/slices to leave out of the selection.
>
> On March 9, 2019 6:57:05 PM PST, Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
> >On 3/10/19 2:36 PM, David Goldsmith wrote:
> >> Hi!  Newbie (self-)learning R using P. Dalgaard's "Intro Stats w/ R";
> >not
> >> new to statistics (have had grad-level courses and work experience in
> >> statistics) or vectorized programming syntax (have extensive
> >experience
> >> with MatLab, Python/NumPy, and IDL, and even a smidgen--a long time
> >ago--of
> >> experience w/ S-plus).
> >>
> >> In exploring the use of is.na in the context of logical indexing,
> >I've come
> >> across the following puzzling-to-me result:
> >>
> >>> y; !is.na(y[1:3]); y[!is.na(y[1:3])]
> >> [1]  0.3534253 -1.6731597         NA -0.2079209
> >> [1]  TRUE  TRUE FALSE
> >> [1]  0.3534253 -1.6731597 -0.2079209
> >>
> >> As you can see, y is a four element vector, the third element of
> >which is
> >> NA; the next line gives what I would expect--T T F--because the first
> >two
> >> elements are not NA but the third element is.  The third line is what
> >> confuses me: why is the result not the two element vector consisting
> >of
> >> simply the first two elements of the vector (or, if vectorized
> >indexing in
> >> R is implemented to return a vector the same length as the logical
> >index
> >> vector, which appears to be the case, at least the first two elements
> >and
> >> then either NA or NaN in the third slot, where the logical indexing
> >vector
> >> is FALSE): why does the implementation "go looking" for an element
> >whose
> >> index in the "original" vector, 4, is larger than BOTH the largest
> >index
> >> specified in the inner-most subsetting index AND the size of the
> >resulting
> >> indexing vector?  (Note: at first I didn't even understand why the
> >result
> >> wasn't simply
> >>
> >> 0.3534253 -1.6731597         NA
> >>
> >> but then I realized that the third logical index being FALSE, there
> >was no
> >> reason for *any* element to be there; but if there is, due to some
> >> overriding rule regarding the length of the result relative to the
> >length
> >> of the indexer, shouldn't it revert back to *something* that
> >indicates the
> >> "FALSE"ness of that indexing element?)
> >>
> >> Thanks!
> >
> >It happens because R is eco-concious and re-cycles. :-)
> >
> >Try:
> >
> >ok <- c(TRUE,TRUE,FALSE)
> >(1:4)[ok]
> >
> >In general in R if there is an operation involving two vectors then
> >the shorter one gets recycled to provide sufficiently many entries to
> >match those of the longer vector.
> >
> >This in the foregoing example the first entry of "ok" gets used again,
> >to make a length 4 vector to match up with 1:4.  The result is the same
> >
> >as (1:4)[c(TRUE,TRUE,FALSE,TRUE)].
> >
> >If you did (1:7)[ok] you'd get the same result as that from
> >(1:7)[c(TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE)] i.e. "ok" gets
> >recycled 2 and 1/3 times.
> >
> >Try 10*(1:3) + 1:4, 10*(1:3) + 1:5, 10*(1:3) + 1:6 .
> >
> >Note that in the first two instances you get warnings, but in the third
> >you don't, since 6 is an integer multiple of 3.
> >
> >Why aren't there warnings when logical indexing is used?  I guess
> >because it would be annoying.  Maybe.
> >
> >Note that integer indices get recycled too, but the recycling is
> >limited
> >so as not to produce redundancies.  So
> >
> >(1:4)[1:3] just (sensibly) gives
> >
> >[1] 1 2 3
> >
> >and *not*
> >
> >[1] 1 2 3 1
> >
> >Perhaps a bit subtle, but it gives what you'd actually *want* rather
> >than being pedantic about rules with a result that you wouldn't want.
> >
> >cheers,
> >
> >Rolf Turner
> >
> >P.S.  If you do
> >
> >y[1:3][!is.na(y[1:3])]
> >
> >i.e. if you're careful to match the length of the vector and the that
> >of
> >the indices, you get what you initially expected.
> >
> >R. T.
> >
> >P^2.S.  To the younger and wiser heads on this list:  the help on "["
> >does not mention that the index vectors can be logical.  I couldn't
> >find
> >anything about logical indexing in the R help files.  Is something
> >missing here, or am I just not looking in the right place?
> >
> >R. T.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun Mar 10 12:46:00 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 10 Mar 2019 07:46:00 -0400
Subject: [R] [FORGED] Q re: logical indexing with is.na
In-Reply-To: <CA+_2Pn3Qfk9uXmiUm_qApYzDHzyPnQ-kAs4Cfzx-OV5xWFpNaw@mail.gmail.com>
References: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
 <937cc22c-386d-da86-f8ac-0d6876afbb8d@auckland.ac.nz>
 <43C651D9-D213-4D70-B2A8-D6A8C7D14DAD@dcn.davis.ca.us>
 <CA+_2Pn3Qfk9uXmiUm_qApYzDHzyPnQ-kAs4Cfzx-OV5xWFpNaw@mail.gmail.com>
Message-ID: <f7973391-9807-8912-ba31-ca8a37a808db@gmail.com>

On 10/03/2019 1:15 a.m., David Goldsmith wrote:
> Thanks, all.  I had read about recycling, but I guess I didn't fully
> appreciate all the "weirdness" it might produce. :/
> 
> With this explained, I'm going to ask a follow-up, which is only
> contextually related: the impetus for this discovery was checking "corner
> cases" to determine if all(x[!is.na(x)]==y[!is.na(y)]) would suffice to
> determine equality of two vectors containing NA's.  Between the above
> result; my related discovery that this indexing preserves relative
> positional info but not absolute positional info; and the performance
> penalty when comparing long vectors that may be unequal "early on";  I've
> concluded that--if it (can be made to) "short circuit"--it would probably
> be better to use an implicit loop.  So that's my Q: will (or can) an
> implicit loop (be made to) "exit early" if a specified condition is met
> before all indices have been checked?

You could use the identical() function.  When I have vectors of length 1 
million, all(x == y) takes about 3 milliseconds when the difference is 
in the last value, 2 milliseconds when it comes first.  identical(x, y) 
takes about 5 milliseconds when the difference comes last, but 0.006 
milliseconds when it comes first.  Of course, all(x == y) and 
identical(x, y) do slightly different tests:  read the docs!

Duncan Murdoch
> 
> Thanks again!
> 
> DLG
> 
> On Sat, Mar 9, 2019 at 9:07 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> 
>> Regarding the mention of logical indexing, under ?Extract I see:
>>
>> For [-indexing only: i, j, ... can be logical vectors, indicating
>> elements/slices to select. Such vectors are recycled if necessary to match
>> the corresponding extent. i, j, ... can also be negative integers,
>> indicating elements/slices to leave out of the selection.
>>
>> On March 9, 2019 6:57:05 PM PST, Rolf Turner <r.turner at auckland.ac.nz>
>> wrote:
>>> On 3/10/19 2:36 PM, David Goldsmith wrote:
>>>> Hi!  Newbie (self-)learning R using P. Dalgaard's "Intro Stats w/ R";
>>> not
>>>> new to statistics (have had grad-level courses and work experience in
>>>> statistics) or vectorized programming syntax (have extensive
>>> experience
>>>> with MatLab, Python/NumPy, and IDL, and even a smidgen--a long time
>>> ago--of
>>>> experience w/ S-plus).
>>>>
>>>> In exploring the use of is.na in the context of logical indexing,
>>> I've come
>>>> across the following puzzling-to-me result:
>>>>
>>>>> y; !is.na(y[1:3]); y[!is.na(y[1:3])]
>>>> [1]  0.3534253 -1.6731597         NA -0.2079209
>>>> [1]  TRUE  TRUE FALSE
>>>> [1]  0.3534253 -1.6731597 -0.2079209
>>>>
>>>> As you can see, y is a four element vector, the third element of
>>> which is
>>>> NA; the next line gives what I would expect--T T F--because the first
>>> two
>>>> elements are not NA but the third element is.  The third line is what
>>>> confuses me: why is the result not the two element vector consisting
>>> of
>>>> simply the first two elements of the vector (or, if vectorized
>>> indexing in
>>>> R is implemented to return a vector the same length as the logical
>>> index
>>>> vector, which appears to be the case, at least the first two elements
>>> and
>>>> then either NA or NaN in the third slot, where the logical indexing
>>> vector
>>>> is FALSE): why does the implementation "go looking" for an element
>>> whose
>>>> index in the "original" vector, 4, is larger than BOTH the largest
>>> index
>>>> specified in the inner-most subsetting index AND the size of the
>>> resulting
>>>> indexing vector?  (Note: at first I didn't even understand why the
>>> result
>>>> wasn't simply
>>>>
>>>> 0.3534253 -1.6731597         NA
>>>>
>>>> but then I realized that the third logical index being FALSE, there
>>> was no
>>>> reason for *any* element to be there; but if there is, due to some
>>>> overriding rule regarding the length of the result relative to the
>>> length
>>>> of the indexer, shouldn't it revert back to *something* that
>>> indicates the
>>>> "FALSE"ness of that indexing element?)
>>>>
>>>> Thanks!
>>>
>>> It happens because R is eco-concious and re-cycles. :-)
>>>
>>> Try:
>>>
>>> ok <- c(TRUE,TRUE,FALSE)
>>> (1:4)[ok]
>>>
>>> In general in R if there is an operation involving two vectors then
>>> the shorter one gets recycled to provide sufficiently many entries to
>>> match those of the longer vector.
>>>
>>> This in the foregoing example the first entry of "ok" gets used again,
>>> to make a length 4 vector to match up with 1:4.  The result is the same
>>>
>>> as (1:4)[c(TRUE,TRUE,FALSE,TRUE)].
>>>
>>> If you did (1:7)[ok] you'd get the same result as that from
>>> (1:7)[c(TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE)] i.e. "ok" gets
>>> recycled 2 and 1/3 times.
>>>
>>> Try 10*(1:3) + 1:4, 10*(1:3) + 1:5, 10*(1:3) + 1:6 .
>>>
>>> Note that in the first two instances you get warnings, but in the third
>>> you don't, since 6 is an integer multiple of 3.
>>>
>>> Why aren't there warnings when logical indexing is used?  I guess
>>> because it would be annoying.  Maybe.
>>>
>>> Note that integer indices get recycled too, but the recycling is
>>> limited
>>> so as not to produce redundancies.  So
>>>
>>> (1:4)[1:3] just (sensibly) gives
>>>
>>> [1] 1 2 3
>>>
>>> and *not*
>>>
>>> [1] 1 2 3 1
>>>
>>> Perhaps a bit subtle, but it gives what you'd actually *want* rather
>>> than being pedantic about rules with a result that you wouldn't want.
>>>
>>> cheers,
>>>
>>> Rolf Turner
>>>
>>> P.S.  If you do
>>>
>>> y[1:3][!is.na(y[1:3])]
>>>
>>> i.e. if you're careful to match the length of the vector and the that
>>> of
>>> the indices, you get what you initially expected.
>>>
>>> R. T.
>>>
>>> P^2.S.  To the younger and wiser heads on this list:  the help on "["
>>> does not mention that the index vectors can be logical.  I couldn't
>>> find
>>> anything about logical indexing in the R help files.  Is something
>>> missing here, or am I just not looking in the right place?
>>>
>>> R. T.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pro|jcn@@h @end|ng |rom gm@||@com  Sun Mar 10 19:57:24 2019
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Sun, 10 Mar 2019 14:57:24 -0400
Subject: [R] rgl.postscript hang
Message-ID: <5c73ac19-8b70-d9cb-3461-86f2e102fbc9@gmail.com>

I've found some other reports that resemble this, but none seem to have a clear
indication of the source of the hangup. This post raises a non-critical issue, but
it would be nice to understand the problem and possibly fix it, or else have an
error message generated. As Duncan has posted in one of the items, rgl.snapshot is
recommended, and it worked for me, but I originally wanted a pdf so tried rgl.postscript
before I saw his comment that the latter may give problems. Indeed, users may not
realize that the message in the documentation

   "This function is a wrapper for the GL2PS library by Christophe Geuzaine,
    and has the same limitations as that library: not all OpenGL features are
    supported, and some are only supported in some formats. See the reference for
    full details."

can imply a "hang" rather than a failure that leaves R (or RStudio) running. Or indeed
crashes them, which it may do in other systems. I used the Linux "kill" command after
using "ps ax" to find which processes were running R or RStudio.

The relevant previous posts are

https://stackoverflow.com/questions/23242846/exporting-rgl-snapshot-and-rgl-postscript-fails

https://support.rstudio.com/hc/en-us/community/posts/200669327-rgl-postscript-hangs

http://r.789695.n4.nabble.com/rgl-postscript-failure-when-saving-a-scene-td4177241.html

My code that hangs is as follows with the rgl.snapshot line commented and the rgl.postscript
one uncommented. The script displays an image box with the correct graph, and seems to output
a partial preamble to the eps file before hanging. This occurs in either R or RStudio.

Unless someone has a true solution, I suggest offline communications until we have an
understanding of the problem. I'm posting mainly to try to put a box around the issue to
help others avoid it.

Best,

JN

---------------
# candlestick function
# J C Nash 2011-2-3
cstick.f<-function(x,alpha=100){
  x<-as.vector(x)
  r2<-crossprod(x)
  f<-as.double(r2+alpha/r2)
  return(f)
}
x <- (-100:100)/5
y <- x
library(rgl)

z <- matrix(NA, length(x), length(y))
for (i in seq_along(x)) {
    for (j in seq_along(y)) {
        z[i,j] <- cstick.f(c(x[i],y[j]))
        if (is.infinite(z[i,j])) {z[i,j] <- 1000}
    }
}
persp3d(x, y, z, col='red')
par3d( windowRect=c( 0,0,500,500 ) )
# rgl.postscript("candlestick3d.eps","eps")
rgl.snapshot("candlestick3d.png","png")
rgl.pop()
----------------

System had following setup

 sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 19.1

Matrix products: default
BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so

locale:
 [1] LC_CTYPE=en_CA.UTF-8       LC_NUMERIC=C               LC_TIME=en_CA.UTF-8        LC_COLLATE=en_CA.UTF-8
LC_MONETARY=en_CA.UTF-8
 [6] LC_MESSAGES=en_CA.UTF-8    LC_PAPER=en_CA.UTF-8       LC_NAME=C                  LC_ADDRESS=C
LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] rgl_0.99.16

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.0              digest_0.6.18           later_0.8.0             mime_0.6                R6_2.4.0
     jsonlite_1.6
 [7] xtable_1.8-3            magrittr_1.5            evaluate_0.13           miniUI_0.1.1.1          promises_1.0.1
     rmarkdown_1.11
[13] webshot_0.5.1           tools_3.5.2             manipulateWidget_0.10.0 htmlwidgets_1.3         crosstalk_1.0.0
     shiny_1.2.0
[19] httpuv_1.4.5.1          xfun_0.5                yaml_2.2.0              compiler_3.5.2          htmltools_0.3.6
     knitr_1.21
>

I got the same behaviour on another machine running Linux Mint 18.3.


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun Mar 10 21:34:16 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 10 Mar 2019 16:34:16 -0400
Subject: [R] rgl.postscript hang
In-Reply-To: <5c73ac19-8b70-d9cb-3461-86f2e102fbc9@gmail.com>
References: <5c73ac19-8b70-d9cb-3461-86f2e102fbc9@gmail.com>
Message-ID: <403be42a-f349-3ba5-df76-df9ab3d96235@gmail.com>

I suspect it isn't actually hung, it is just very, very slow.  If I 
change the line

x <- (-100:100)/5

to

x <- (-20:20)/5

then it works almost instantly.  At

x <- (-50:50)/5

it is slow (16 seconds); replacing the 50 with 60, 70 or 80 yields 29, 
46, or 120 seconds respectively. So far I haven't been patient enough to 
know what

x <- (-90:90)/5

would take.  This doesn't really make sense:  with

x <- (-n:n)/5

it will need to sort (2n + 1)^2 objects, but that should take something 
proportional to n^2 log(n) time, and the time seems to be growing faster 
than that.

Duncan Murdoch

On 10/03/2019 2:57 p.m., J C Nash wrote:
> I've found some other reports that resemble this, but none seem to have a clear
> indication of the source of the hangup. This post raises a non-critical issue, but
> it would be nice to understand the problem and possibly fix it, or else have an
> error message generated. As Duncan has posted in one of the items, rgl.snapshot is
> recommended, and it worked for me, but I originally wanted a pdf so tried rgl.postscript
> before I saw his comment that the latter may give problems. Indeed, users may not
> realize that the message in the documentation
> 
>     "This function is a wrapper for the GL2PS library by Christophe Geuzaine,
>      and has the same limitations as that library: not all OpenGL features are
>      supported, and some are only supported in some formats. See the reference for
>      full details."
> 
> can imply a "hang" rather than a failure that leaves R (or RStudio) running. Or indeed
> crashes them, which it may do in other systems. I used the Linux "kill" command after
> using "ps ax" to find which processes were running R or RStudio.
> 
> The relevant previous posts are
> 
> https://stackoverflow.com/questions/23242846/exporting-rgl-snapshot-and-rgl-postscript-fails
> 
> https://support.rstudio.com/hc/en-us/community/posts/200669327-rgl-postscript-hangs
> 
> http://r.789695.n4.nabble.com/rgl-postscript-failure-when-saving-a-scene-td4177241.html
> 
> My code that hangs is as follows with the rgl.snapshot line commented and the rgl.postscript
> one uncommented. The script displays an image box with the correct graph, and seems to output
> a partial preamble to the eps file before hanging. This occurs in either R or RStudio.
> 
> Unless someone has a true solution, I suggest offline communications until we have an
> understanding of the problem. I'm posting mainly to try to put a box around the issue to
> help others avoid it.
> 
> Best,
> 
> JN
> 
> ---------------
> # candlestick function
> # J C Nash 2011-2-3
> cstick.f<-function(x,alpha=100){
>    x<-as.vector(x)
>    r2<-crossprod(x)
>    f<-as.double(r2+alpha/r2)
>    return(f)
> }
> x <- (-100:100)/5
> y <- x
> library(rgl)
> 
> z <- matrix(NA, length(x), length(y))
> for (i in seq_along(x)) {
>      for (j in seq_along(y)) {
>          z[i,j] <- cstick.f(c(x[i],y[j]))
>          if (is.infinite(z[i,j])) {z[i,j] <- 1000}
>      }
> }
> persp3d(x, y, z, col='red')
> par3d( windowRect=c( 0,0,500,500 ) )
> # rgl.postscript("candlestick3d.eps","eps")
> rgl.snapshot("candlestick3d.png","png")
> rgl.pop()
> ----------------
> 
> System had following setup
> 
>   sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Linux Mint 19.1
> 
> Matrix products: default
> BLAS: /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
> LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so
> 
> locale:
>   [1] LC_CTYPE=en_CA.UTF-8       LC_NUMERIC=C               LC_TIME=en_CA.UTF-8        LC_COLLATE=en_CA.UTF-8
> LC_MONETARY=en_CA.UTF-8
>   [6] LC_MESSAGES=en_CA.UTF-8    LC_PAPER=en_CA.UTF-8       LC_NAME=C                  LC_ADDRESS=C
> LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] rgl_0.99.16
> 
> loaded via a namespace (and not attached):
>   [1] Rcpp_1.0.0              digest_0.6.18           later_0.8.0             mime_0.6                R6_2.4.0
>       jsonlite_1.6
>   [7] xtable_1.8-3            magrittr_1.5            evaluate_0.13           miniUI_0.1.1.1          promises_1.0.1
>       rmarkdown_1.11
> [13] webshot_0.5.1           tools_3.5.2             manipulateWidget_0.10.0 htmlwidgets_1.3         crosstalk_1.0.0
>       shiny_1.2.0
> [19] httpuv_1.4.5.1          xfun_0.5                yaml_2.2.0              compiler_3.5.2          htmltools_0.3.6
>       knitr_1.21
>>
> 
> I got the same behaviour on another machine running Linux Mint 18.3.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bow|ewongg @end|ng |rom gm@||@com  Sun Mar 10 21:54:37 2019
From: bow|ewongg @end|ng |rom gm@||@com (wong bowie)
Date: Sun, 10 Mar 2019 20:54:37 +0000
Subject: [R] How to change the number of bins?
Message-ID: <CA+gbZiDdxpKoEeyzhW+e3oWVgVNLuCcA_C8FMrYr-28ios_R2g@mail.gmail.com>

I wish to calculate the weight of evidence of a variable x, which is
positively skewed, with over 6000 of the observations are 999 but only 200
range from 1-27. I used the code,

?IV<-create_infotables(data=Test[,-1],y="class",bins=10)?

However, no matter what number I used in bins parameter, I can only get 2
bins, [1,27] and [999,999]. Is there any way I can look into the [1,27]
closely because they represent a lot? The output from R is shown below,

Table$pdays
    pdays        N   Percent     WOE       IV
    1 [1,27]    243  0.03807584  2.6743166 0.5267751
    2 [999,999] 6139 0.96192416 -0.2230081 0.5707022

Thank you very much!!

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Mar 10 23:48:29 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 10 Mar 2019 15:48:29 -0700
Subject: [R] How to change the number of bins?
In-Reply-To: <CA+gbZiDdxpKoEeyzhW+e3oWVgVNLuCcA_C8FMrYr-28ios_R2g@mail.gmail.com>
References: <CA+gbZiDdxpKoEeyzhW+e3oWVgVNLuCcA_C8FMrYr-28ios_R2g@mail.gmail.com>
Message-ID: <df35febe-041f-7f0e-076e-804229a5a80f@comcast.net>

Seems rather likely that 999 is not really a measured value but rather 
is a missing value indicator.


-- 

David.

On 3/10/19 1:54 PM, wong bowie wrote:
> I wish to calculate the weight of evidence of a variable x, which is
> positively skewed, with over 6000 of the observations are 999 but only 200
> range from 1-27. I used the code,
>
> ?IV<-create_infotables(data=Test[,-1],y="class",bins=10)?
>
> However, no matter what number I used in bins parameter, I can only get 2
> bins, [1,27] and [999,999]. Is there any way I can look into the [1,27]
> closely because they represent a lot? The output from R is shown below,
>
> Table$pdays
>      pdays        N   Percent     WOE       IV
>      1 [1,27]    243  0.03807584  2.6743166 0.5267751
>      2 [999,999] 6139 0.96192416 -0.2230081 0.5707022
>
> Thank you very much!!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bow|ewongg @end|ng |rom gm@||@com  Mon Mar 11 01:29:26 2019
From: bow|ewongg @end|ng |rom gm@||@com (wong bowie)
Date: Mon, 11 Mar 2019 00:29:26 +0000
Subject: [R] How to change the number of bins?
In-Reply-To: <df35febe-041f-7f0e-076e-804229a5a80f@comcast.net>
References: <CA+gbZiDdxpKoEeyzhW+e3oWVgVNLuCcA_C8FMrYr-28ios_R2g@mail.gmail.com>
 <df35febe-041f-7f0e-076e-804229a5a80f@comcast.net>
Message-ID: <CA+gbZiCtat-Eiq5Jr1SOjCPhOA5W3JvM+MWn8k2fGw+bktjy8A@mail.gmail.com>

You are right. Actually this variable represents the number of day passed
after contacting a client, 999 means the client has never been contacted.

But I am not supposed to change the value, am I?

David Winsemius <dwinsemius at comcast.net> ? 2019?3?10? ?? ??10:48???

> Seems rather likely that 999 is not really a measured value but rather
> is a missing value indicator.
>
>
> --
>
> David.
>
> On 3/10/19 1:54 PM, wong bowie wrote:
> > I wish to calculate the weight of evidence of a variable x, which is
> > positively skewed, with over 6000 of the observations are 999 but only
> 200
> > range from 1-27. I used the code,
> >
> > ?IV<-create_infotables(data=Test[,-1],y="class",bins=10)?
> >
> > However, no matter what number I used in bins parameter, I can only get 2
> > bins, [1,27] and [999,999]. Is there any way I can look into the [1,27]
> > closely because they represent a lot? The output from R is shown below,
> >
> > Table$pdays
> >      pdays        N   Percent     WOE       IV
> >      1 [1,27]    243  0.03807584  2.6743166 0.5267751
> >      2 [999,999] 6139 0.96192416 -0.2230081 0.5707022
> >
> > Thank you very much!!
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dc@r|@on @end|ng |rom t@mu@edu  Mon Mar 11 01:50:39 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Mon, 11 Mar 2019 00:50:39 +0000
Subject: [R] [R-sig-eco] AIC in Biomod 2
Message-ID: <c8c1bbc0a554404e9080b4c8c4d77e12@tamu.edu>

I searched the manual for package biomed2 and found 6 references to "AIC". Perhaps that is your answer.

David L. Carlson
Department of Anthropology
Texas A&M University

-----Original Message-----
From: R-sig-ecology [mailto:r-sig-ecology-bounces at r-project.org] On Behalf Of Lara Silva
Sent: Saturday, March 9, 2019 5:32 PM
To: r-sig-ecology at r-project.org
Subject: [R-sig-eco] AIC in Biomod 2

Hello

Is there any way to calculate the AIC in Biomod 2?

Thanks,

Lara

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-ecology mailing list
R-sig-ecology at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-ecology


From dw|n@em|u@ @end|ng |rom comc@@t@net  Mon Mar 11 02:39:27 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 10 Mar 2019 18:39:27 -0700
Subject: [R] How to change the number of bins?
In-Reply-To: <CA+gbZiCtat-Eiq5Jr1SOjCPhOA5W3JvM+MWn8k2fGw+bktjy8A@mail.gmail.com>
References: <CA+gbZiDdxpKoEeyzhW+e3oWVgVNLuCcA_C8FMrYr-28ios_R2g@mail.gmail.com>
 <df35febe-041f-7f0e-076e-804229a5a80f@comcast.net>
 <CA+gbZiCtat-Eiq5Jr1SOjCPhOA5W3JvM+MWn8k2fGw+bktjy8A@mail.gmail.com>
Message-ID: <59c7bef0-fcd9-0342-89f3-7ff7555ac797@comcast.net>


On 3/10/19 5:29 PM, wong bowie wrote:
> You are right. Actually this variable represents the number of day 
> passed after contacting a client, 999 means the client has never been 
> contacted.
>
> But I am not supposed to change the value, am I?


I certainly would. SAS allows one to specify a value such as 999 to be 
missing but R needs to have it changed to NA

is.na(Table$pdays) <- Table$pdays == 999


-- 

David


>
> David Winsemius <dwinsemius at comcast.net 
> <mailto:dwinsemius at comcast.net>> ? 2019?3?10? ?? ??10:48???
>
>     Seems rather likely that 999 is not really a measured value but
>     rather
>     is a missing value indicator.
>
>
>     -- 
>
>     David.
>
>     On 3/10/19 1:54 PM, wong bowie wrote:
>     > I wish to calculate the weight of evidence of a variable x, which is
>     > positively skewed, with over 6000 of the observations are 999
>     but only 200
>     > range from 1-27. I used the code,
>     >
>     > ?IV<-create_infotables(data=Test[,-1],y="class",bins=10)?
>     >
>     > However, no matter what number I used in bins parameter, I can
>     only get 2
>     > bins, [1,27] and [999,999]. Is there any way I can look into the
>     [1,27]
>     > closely because they represent a lot? The output from R is shown
>     below,
>     >
>     > Table$pdays
>     >? ? ? pdays? ? ? ? N? ?Percent? ? ?WOE? ? ? ?IV
>     >? ? ? 1 [1,27]? ? 243? 0.03807584? 2.6743166 0.5267751
>     >? ? ? 2 [999,999] 6139 0.96192416 -0.2230081 0.5707022
>     >
>     > Thank you very much!!
>     >
>     >? ? ? ?[[alternative HTML version deleted]]
>     >
>     > ______________________________________________
>     > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>     > https://stat.ethz.ch/mailman/listinfo/r-help
>     > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Mon Mar 11 05:20:58 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 11 Mar 2019 15:20:58 +1100
Subject: [R] How to change the number of bins?
In-Reply-To: <CA+gbZiDdxpKoEeyzhW+e3oWVgVNLuCcA_C8FMrYr-28ios_R2g@mail.gmail.com>
References: <CA+gbZiDdxpKoEeyzhW+e3oWVgVNLuCcA_C8FMrYr-28ios_R2g@mail.gmail.com>
Message-ID: <CA+8X3fUR4NrhYM26AE7XPpnWWdg3afCHtmBVxOH9JjWo_vYDUA@mail.gmail.com>

Hi Bowie,
As David suggested, you can substitute the R missing value (NA) for
999 (probably an SPSS missing  value). If you don't want to change it,
you could probably just subset your data like this:

V<-create_infotables(data=Test[Test[n] != 999,-1],y="class",bins=10)

where "n" is the column number in Test of the variable of interest.

Jim

On Mon, Mar 11, 2019 at 9:45 AM wong bowie <bowiewongg at gmail.com> wrote:
>
> I wish to calculate the weight of evidence of a variable x, which is
> positively skewed, with over 6000 of the observations are 999 but only 200
> range from 1-27. I used the code,
>
> ?IV<-create_infotables(data=Test[,-1],y="class",bins=10)?
>
> However, no matter what number I used in bins parameter, I can only get 2
> bins, [1,27] and [999,999]. Is there any way I can look into the [1,27]
> closely because they represent a lot? The output from R is shown below,
>
> Table$pdays
>     pdays        N   Percent     WOE       IV
>     1 [1,27]    243  0.03807584  2.6743166 0.5267751
>     2 [999,999] 6139 0.96192416 -0.2230081 0.5707022
>
> Thank you very much!!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Mon Mar 11 05:39:29 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 10 Mar 2019 21:39:29 -0700
Subject: [R] How to change the number of bins?
In-Reply-To: <CA+8X3fUR4NrhYM26AE7XPpnWWdg3afCHtmBVxOH9JjWo_vYDUA@mail.gmail.com>
References: <CA+gbZiDdxpKoEeyzhW+e3oWVgVNLuCcA_C8FMrYr-28ios_R2g@mail.gmail.com>
 <CA+8X3fUR4NrhYM26AE7XPpnWWdg3afCHtmBVxOH9JjWo_vYDUA@mail.gmail.com>
Message-ID: <CAGxFJbS4h5ywBqKX5Fhxw4O1Hz-UyTQvorrBJh5_uftFi_ccfQ@mail.gmail.com>

You are asking the wrong question. The right  question is, "why are so many
values missing?" Is it because they were censored, not reported for some
reason, due to instrument failure,...?  Until you answer that question, any
analysis you do is garbage.

I strongly recommend you consult a competent data analyst.

Bert

On Sun, Mar 10, 2019, 9:21 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Bowie,
> As David suggested, you can substitute the R missing value (NA) for
> 999 (probably an SPSS missing  value). If you don't want to change it,
> you could probably just subset your data like this:
>
> V<-create_infotables(data=Test[Test[n] != 999,-1],y="class",bins=10)
>
> where "n" is the column number in Test of the variable of interest.
>
> Jim
>
> On Mon, Mar 11, 2019 at 9:45 AM wong bowie <bowiewongg at gmail.com> wrote:
> >
> > I wish to calculate the weight of evidence of a variable x, which is
> > positively skewed, with over 6000 of the observations are 999 but only
> 200
> > range from 1-27. I used the code,
> >
> > ?IV<-create_infotables(data=Test[,-1],y="class",bins=10)?
> >
> > However, no matter what number I used in bins parameter, I can only get 2
> > bins, [1,27] and [999,999]. Is there any way I can look into the [1,27]
> > closely because they represent a lot? The output from R is shown below,
> >
> > Table$pdays
> >     pdays        N   Percent     WOE       IV
> >     1 [1,27]    243  0.03807584  2.6743166 0.5267751
> >     2 [999,999] 6139 0.96192416 -0.2230081 0.5707022
> >
> > Thank you very much!!
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Mon Mar 11 07:58:14 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 11 Mar 2019 06:58:14 +0000
Subject: [R] [FORGED] Q re: logical indexing with is.na
In-Reply-To: <CA+_2Pn3Qfk9uXmiUm_qApYzDHzyPnQ-kAs4Cfzx-OV5xWFpNaw@mail.gmail.com>
References: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
 <937cc22c-386d-da86-f8ac-0d6876afbb8d@auckland.ac.nz>
 <43C651D9-D213-4D70-B2A8-D6A8C7D14DAD@dcn.davis.ca.us>
 <CA+_2Pn3Qfk9uXmiUm_qApYzDHzyPnQ-kAs4Cfzx-OV5xWFpNaw@mail.gmail.com>
Message-ID: <a809f1025687450c9e5ebfa0a8e703cc@SRVEXCHCM1302.precheza.cz>

Hi

Do you want something like this?
> x <- c(1,2,NA, 3, 4, 5, NA, 6,7,8, NA, NA, 9,10)
> y <- c(1,2,NA, NA, 3, 4, 5, 6, NA, 7,8, NA, NA, 9,10)
> identical(x[which(!is.na(x))], y[which(!is.na(y))])
[1] TRUE

If I expect NA and want to extract or compare something, I tend to use which to select only non NA elements.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of David Goldsmith
> Sent: Sunday, March 10, 2019 7:16 AM
> Cc: r-help at r-project.org
> Subject: Re: [R] [FORGED] Q re: logical indexing with is.na
>
> Thanks, all.  I had read about recycling, but I guess I didn't fully appreciate all
> the "weirdness" it might produce. :/
>
> With this explained, I'm going to ask a follow-up, which is only contextually
> related: the impetus for this discovery was checking "corner cases" to
> determine if all(x[!is.na(x)]==y[!is.na(y)]) would suffice to determine equality of
> two vectors containing NA's.  Between the above result; my related discovery
> that this indexing preserves relative positional info but not absolute positional
> info; and the performance penalty when comparing long vectors that may be
> unequal "early on";  I've concluded that--if it (can be made to) "short circuit"--it
> would probably be better to use an implicit loop.  So that's my Q: will (or can)
> an implicit loop (be made to) "exit early" if a specified condition is met before
> all indices have been checked?
>
> Thanks again!
>
> DLG
>
> On Sat, Mar 9, 2019 at 9:07 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
> > Regarding the mention of logical indexing, under ?Extract I see:
> >
> > For [-indexing only: i, j, ... can be logical vectors, indicating
> > elements/slices to select. Such vectors are recycled if necessary to
> > match the corresponding extent. i, j, ... can also be negative
> > integers, indicating elements/slices to leave out of the selection.
> >
> > On March 9, 2019 6:57:05 PM PST, Rolf Turner <r.turner at auckland.ac.nz>
> > wrote:
> > >On 3/10/19 2:36 PM, David Goldsmith wrote:
> > >> Hi!  Newbie (self-)learning R using P. Dalgaard's "Intro Stats w/
> > >> R";
> > >not
> > >> new to statistics (have had grad-level courses and work experience
> > >> in
> > >> statistics) or vectorized programming syntax (have extensive
> > >experience
> > >> with MatLab, Python/NumPy, and IDL, and even a smidgen--a long time
> > >ago--of
> > >> experience w/ S-plus).
> > >>
> > >> In exploring the use of is.na in the context of logical indexing,
> > >I've come
> > >> across the following puzzling-to-me result:
> > >>
> > >>> y; !is.na(y[1:3]); y[!is.na(y[1:3])]
> > >> [1]  0.3534253 -1.6731597         NA -0.2079209
> > >> [1]  TRUE  TRUE FALSE
> > >> [1]  0.3534253 -1.6731597 -0.2079209
> > >>
> > >> As you can see, y is a four element vector, the third element of
> > >which is
> > >> NA; the next line gives what I would expect--T T F--because the
> > >> first
> > >two
> > >> elements are not NA but the third element is.  The third line is
> > >> what confuses me: why is the result not the two element vector
> > >> consisting
> > >of
> > >> simply the first two elements of the vector (or, if vectorized
> > >indexing in
> > >> R is implemented to return a vector the same length as the logical
> > >index
> > >> vector, which appears to be the case, at least the first two
> > >> elements
> > >and
> > >> then either NA or NaN in the third slot, where the logical indexing
> > >vector
> > >> is FALSE): why does the implementation "go looking" for an element
> > >whose
> > >> index in the "original" vector, 4, is larger than BOTH the largest
> > >index
> > >> specified in the inner-most subsetting index AND the size of the
> > >resulting
> > >> indexing vector?  (Note: at first I didn't even understand why the
> > >result
> > >> wasn't simply
> > >>
> > >> 0.3534253 -1.6731597         NA
> > >>
> > >> but then I realized that the third logical index being FALSE, there
> > >was no
> > >> reason for *any* element to be there; but if there is, due to some
> > >> overriding rule regarding the length of the result relative to the
> > >length
> > >> of the indexer, shouldn't it revert back to *something* that
> > >indicates the
> > >> "FALSE"ness of that indexing element?)
> > >>
> > >> Thanks!
> > >
> > >It happens because R is eco-concious and re-cycles. :-)
> > >
> > >Try:
> > >
> > >ok <- c(TRUE,TRUE,FALSE)
> > >(1:4)[ok]
> > >
> > >In general in R if there is an operation involving two vectors then
> > >the shorter one gets recycled to provide sufficiently many entries to
> > >match those of the longer vector.
> > >
> > >This in the foregoing example the first entry of "ok" gets used
> > >again, to make a length 4 vector to match up with 1:4.  The result is
> > >the same
> > >
> > >as (1:4)[c(TRUE,TRUE,FALSE,TRUE)].
> > >
> > >If you did (1:7)[ok] you'd get the same result as that from
> > >(1:7)[c(TRUE,TRUE,FALSE,TRUE,TRUE,FALSE,TRUE)] i.e. "ok" gets
> > >recycled 2 and 1/3 times.
> > >
> > >Try 10*(1:3) + 1:4, 10*(1:3) + 1:5, 10*(1:3) + 1:6 .
> > >
> > >Note that in the first two instances you get warnings, but in the
> > >third you don't, since 6 is an integer multiple of 3.
> > >
> > >Why aren't there warnings when logical indexing is used?  I guess
> > >because it would be annoying.  Maybe.
> > >
> > >Note that integer indices get recycled too, but the recycling is
> > >limited so as not to produce redundancies.  So
> > >
> > >(1:4)[1:3] just (sensibly) gives
> > >
> > >[1] 1 2 3
> > >
> > >and *not*
> > >
> > >[1] 1 2 3 1
> > >
> > >Perhaps a bit subtle, but it gives what you'd actually *want* rather
> > >than being pedantic about rules with a result that you wouldn't want.
> > >
> > >cheers,
> > >
> > >Rolf Turner
> > >
> > >P.S.  If you do
> > >
> > >y[1:3][!is.na(y[1:3])]
> > >
> > >i.e. if you're careful to match the length of the vector and the that
> > >of the indices, you get what you initially expected.
> > >
> > >R. T.
> > >
> > >P^2.S.  To the younger and wiser heads on this list:  the help on "["
> > >does not mention that the index vectors can be logical.  I couldn't
> > >find anything about logical indexing in the R help files.  Is
> > >something missing here, or am I just not looking in the right place?
> > >
> > >R. T.
> >
> > --
> > Sent from my phone. Please excuse my brevity.
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From pd@me@ @end|ng |rom cb@@dk  Mon Mar 11 09:35:49 2019
From: pd@me@ @end|ng |rom cb@@dk (Peter Dalgaard)
Date: Mon, 11 Mar 2019 08:35:49 +0000
Subject: [R] R 3.5.3 is released
Message-ID: <9FF9937D-3F09-4AB2-8502-E622266A440F@cbs.dk>

The build system rolled up R-3.5.3.tar.gz (codename "Great Truth") this morning.

The list below details the changes in this release. This is the wrap-up release for the 3.5.x series, so actually, not much has happened.

You can get the source code from

http://cran.r-project.org/src/base/R-3/R-3.5.3.tar.gz

or wait for it to be mirrored at a CRAN site nearer to you.

Binaries for various platforms will appear in due course.


For the R Core Team,

Peter Dalgaard


These are the checksums (md5 and SHA-256) for the freshly created files, in case you wish
to check that they are uncorrupted:

MD5 (AUTHORS) = b9c44f9f78cab3184ad9898bebc854b4
MD5 (COPYING) = eb723b61539feef013de476e68b5c50a
MD5 (COPYING.LIB) = a6f89e2100d9b6cdffcea4f398e37343
MD5 (FAQ) = 3bba37aa1dd06de3f781200a8081302f
MD5 (INSTALL) = 7893f754308ca31f1ccf62055090ad7b
MD5 (NEWS) = ab129f42b1d5ca25122db6b1bda0fcd9
MD5 (NEWS.0) = bfcd7c147251b5474d96848c6f57e5a8
MD5 (NEWS.1) = eb78c4d053ec9c32b815cf0c2ebea801
MD5 (NEWS.2) = 591dcf615162127f904e4e461f330ce9
MD5 (R-latest.tar.gz) = 525e902dd331c387f271692a1537aff8
MD5 (README) = f468f281c919665e276a1b691decbbe6
MD5 (RESOURCES) = 529223fd3ffef95731d0a87353108435
MD5 (THANKS) = 08158353102084599797db8c9ccf8e2a
MD5 (VERSION-INFO.dcf) = cb61b0eb560efcbbec47128abf3fb761
MD5 (R-3/R-3.5.3.tar.gz) = 525e902dd331c387f271692a1537aff8

2cde824a7b18958e5f06b391c801c8288be0f84fa8934b7ddefef23c67e60c09  AUTHORS
e6d6a009505e345fe949e1310334fcb0747f28dae2856759de102ab66b722cb4  COPYING
6095e9ffa777dd22839f7801aa845b31c9ed07f3d6bf8a26dc5d2dec8ccc0ef3  COPYING.LIB
98df47801c33cc4f4a4de98447cb2bd40e09c0920195f540a981ceed874714f2  FAQ
f87461be6cbaecc4dce44ac58e5bd52364b0491ccdadaf846cb9b452e9550f31  INSTALL
4c6eb7cd9d8f4c1858a8f853698d2954d42d5d8b71c5c4d20bc6bd970f034bfe  NEWS
4e21b62f515b749f80997063fceab626d7258c7d650e81a662ba8e0640f12f62  NEWS.0
12b30c724117b1b2b11484673906a6dcd48a361f69fc420b36194f9218692d01  NEWS.1
ca04f78ffe54afa326fe3ed40e7e1411aca0000ed2fa5ead97ddf51c6aa5b7bc  NEWS.2
2bfa37b7bd709f003d6b8a172ddfb6d03ddd2d672d6096439523039f7a8e678c  R-latest.tar.gz
2fdd3e90f23f32692d4b3a0c0452f2c219a10882033d1774f8cadf25886c3ddc  README
408737572ecc6e1135fdb2cf7a9dbb1a6cb27967c757f1771b8c39d1fd2f1ab9  RESOURCES
2d2e85e85574c4430951f6b070c08cd5aff1602abfd1bb162bed6d89c436b11f  THANKS
1ce83e7a843f95e8b0d5abf6ced7426cc337cc607f9862f99d46a7d05793ac15  VERSION-INFO.dcf
2bfa37b7bd709f003d6b8a172ddfb6d03ddd2d672d6096439523039f7a8e678c  R-3/R-3.5.3.tar.gz

This is the relevant part of the NEWS file:

CHANGES IN R 3.5.3:

  INSTALLATION on a UNIX-ALIKE:

    * Detection of flags for C++98/11/14/17 has been improved: in
      particular if CXX??STD is set, it is tried first with no
      additional flags.

  PACKAGE INSTALLATION:

    * New macro F_VISIBILITY as an alternative to F77_VISIBILITY.  This
      will become the preferred form in R 3.6.0.

  BUG FIXES:

    * writeLines(readLines(fnam), fnam) now works as expected, thanks
      to Peter Meissner's PR#17528.

    * setClassUnion() no longer warns, but uses message() for now, when
      encountering "non local" subclasses of class members.

    * stopifnot(exprs = T) no longer fails.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From j@vedbtk111 @end|ng |rom gm@||@com  Mon Mar 11 14:18:52 2019
From: j@vedbtk111 @end|ng |rom gm@||@com (javed khan)
Date: Mon, 11 Mar 2019 14:18:52 +0100
Subject: [R] About wilcox test
Message-ID: <CAJhui+tfOUGWg1ZVYxyZMT-foCHR+-qmih0Y01++n2LjN1DbKQ@mail.gmail.com>

Hi,

I have two set of data in excel:
A column( 16.38, -31, -16.77, 127, -57, 23.44 and so on)
B column ( -12, -59.23, -44, 34.23, 55.5, -12.12 and so on)

I run the wilcox test as :

wilcox.test(A , B, data = mydata, paired = FALSE)

I got always the p value very high, like 0.60

Even I make changes in the data, it gives me 0.7, 0.4 etc which is too high
than 0.05 and can not thus reject the null hypothesis.

What could be the problem as I know there is difference in the data?

Regards

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Mon Mar 11 18:06:25 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Mon, 11 Mar 2019 13:06:25 -0400
Subject: [R] About wilcox test
In-Reply-To: <CAJhui+tfOUGWg1ZVYxyZMT-foCHR+-qmih0Y01++n2LjN1DbKQ@mail.gmail.com>
References: <CAJhui+tfOUGWg1ZVYxyZMT-foCHR+-qmih0Y01++n2LjN1DbKQ@mail.gmail.com>
Message-ID: <7ABEC2EE-A68B-4D08-A788-B94E49670316@malonequantitative.com>

First, I'm pretty sure this is a statistics question, not an R question.

But you're asserting there is a difference in the data. The statistical test is telling you that this apparent difference is within the range of chance variation. There's not a contradiction there, unless you mean there is a statistically significant difference with the Wilcox test in some other computation method.

Pat

?On 3/11/19, 12:47 PM, "R-help on behalf of javed khan" <r-help-bounces at r-project.org on behalf of javedbtk111 at gmail.com> wrote:

    Hi,
    
    I have two set of data in excel:
    A column( 16.38, -31, -16.77, 127, -57, 23.44 and so on)
    B column ( -12, -59.23, -44, 34.23, 55.5, -12.12 and so on)
    
    I run the wilcox test as :
    
    wilcox.test(A , B, data = mydata, paired = FALSE)
    
    I got always the p value very high, like 0.60
    
    Even I make changes in the data, it gives me 0.7, 0.4 etc which is too high
    than 0.05 and can not thus reject the null hypothesis.
    
    What could be the problem as I know there is difference in the data?
    
    Regards
    
    	[[alternative HTML version deleted]]
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From |zm|r||g @end|ng |rom m@||@n|h@gov  Mon Mar 11 18:11:48 2019
From: |zm|r||g @end|ng |rom m@||@n|h@gov (Izmirlian, Grant (NIH/NCI) [E])
Date: Mon, 11 Mar 2019 17:11:48 +0000
Subject: [R] Q re: logical indexing with is.na
In-Reply-To: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
References: <CA+_2Pn3BYiJZsKmu4-TXrXuWr0xecdp4g2=b=Mxtb3RPe1BBjA@mail.gmail.com>
Message-ID: <BL0PR0901MB24024A2A40546368532E9600E5480@BL0PR0901MB2402.namprd09.prod.outlook.com>

logical indexing requires the logical index to be of the same length as the vector being indexed. If it is not, then the index
is wrapped to be of sufficient length. The result on line 3 is
y[c(TRUE, TRUE, FALSE, TRUE)] where the last TRUE was
originally the first component of !is.na(y[1:3])


Grant Izmirlian, Ph.D.
Mathematical Statistician
izmirlig at mail.nih.gov

Delivery Address:
9609 Medical Center Dr, RM 5E130
Rockville MD 20850

Postal Address:
BG 9609 RM 5E130 MSC 9789
9609 Medical Center Dr
Bethesda, MD 20892-9789

 ofc:  240-276-7025
 cell: 240-888-7367
  fax: 240-276-7845


________________________________
From: David Goldsmith <eulergaussriemann at gmail.com>
Sent: Saturday, March 9, 2019 8:36 PM
To: r-help at r-project.org
Subject: [R] Q re: logical indexing with is.na

Hi!  Newbie (self-)learning R using P. Dalgaard's "Intro Stats w/ R"; not
new to statistics (have had grad-level courses and work experience in
statistics) or vectorized programming syntax (have extensive experience
with MatLab, Python/NumPy, and IDL, and even a smidgen--a long time ago--of
experience w/ S-plus).

In exploring the use of is.na in the context of logical indexing, I've come
across the following puzzling-to-me result:

> y; !is.na(y[1:3]); y[!is.na(y[1:3])]
[1]  0.3534253 -1.6731597         NA -0.2079209
[1]  TRUE  TRUE FALSE
[1]  0.3534253 -1.6731597 -0.2079209

As you can see, y is a four element vector, the third element of which is
NA; the next line gives what I would expect--T T F--because the first two
elements are not NA but the third element is.  The third line is what
confuses me: why is the result not the two element vector consisting of
simply the first two elements of the vector (or, if vectorized indexing in
R is implemented to return a vector the same length as the logical index
vector, which appears to be the case, at least the first two elements and
then either NA or NaN in the third slot, where the logical indexing vector
is FALSE): why does the implementation "go looking" for an element whose
index in the "original" vector, 4, is larger than BOTH the largest index
specified in the inner-most subsetting index AND the size of the resulting
indexing vector?  (Note: at first I didn't even understand why the result
wasn't simply

0.3534253 -1.6731597         NA

but then I realized that the third logical index being FALSE, there was no
reason for *any* element to be there; but if there is, due to some
overriding rule regarding the length of the result relative to the length
of the indexer, shouldn't it revert back to *something* that indicates the
"FALSE"ness of that indexing element?)

Thanks!

DLG

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS:
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK:
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] ISwR_2.0-7

loaded via a namespace (and not attached):
[1] compiler_3.5.2 tools_3.5.2

        [[alternative HTML version deleted]]



	[[alternative HTML version deleted]]


From @eb@@t|en@b|hore| @end|ng |rom cogn|gencorp@com  Mon Mar 11 20:26:07 2019
From: @eb@@t|en@b|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Mon, 11 Mar 2019 15:26:07 -0400 (EDT)
Subject: [R] Stratifying data with xyplot
Message-ID: <1505236375.928997.1552332367691.JavaMail.zimbra@cognigencorp.com>

Hi, 

I am a big user/fan of the lattice package for plotting. As far as I know, lattice only offers one method to stratify data within a xyplot panel, using the groups arguments. 
A contrario, the ggplot package allow users to use different variables for coloring, setting the symbols, the line types or the size of symbols. This frequently comes handy.
My question is whether any work has been done in the lattice ecosystem to reproduce this functionality? If so, I would greatly appreciate any pointers to the appropriate package documentation. 

Thank you

Sebastien


From kw@@t@t @end|ng |rom gm@||@com  Mon Mar 11 21:57:34 2019
From: kw@@t@t @end|ng |rom gm@||@com (Kevin Wright)
Date: Mon, 11 Mar 2019 15:57:34 -0500
Subject: [R] Stratifying data with xyplot
In-Reply-To: <1505236375.928997.1552332367691.JavaMail.zimbra@cognigencorp.com>
References: <1505236375.928997.1552332367691.JavaMail.zimbra@cognigencorp.com>
Message-ID: <CAKFxdiSQ7j5RAEPqDCxSjNGbQvoHRpHhnMavWWE=2aPzP5TW4w@mail.gmail.com>

See the examples here:
https://www.stat.ubc.ca/~jenny/STAT545A/block10_latticeNittyGritty.html


On Mon, Mar 11, 2019 at 2:26 PM Sebastien Bihorel <
sebastien.bihorel at cognigencorp.com> wrote:

> Hi,
>
> I am a big user/fan of the lattice package for plotting. As far as I know,
> lattice only offers one method to stratify data within a xyplot panel,
> using the groups arguments.
> A contrario, the ggplot package allow users to use different variables for
> coloring, setting the symbols, the line types or the size of symbols. This
> frequently comes handy.
> My question is whether any work has been done in the lattice ecosystem to
> reproduce this functionality? If so, I would greatly appreciate any
> pointers to the appropriate package documentation.
>
> Thank you
>
> Sebastien
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Kevin Wright

	[[alternative HTML version deleted]]


From p@@c@|@n|k|@u@ @end|ng |rom |eu@uzh@ch  Mon Mar 11 19:57:47 2019
From: p@@c@|@n|k|@u@ @end|ng |rom |eu@uzh@ch (Pascal A. Niklaus)
Date: Mon, 11 Mar 2019 19:57:47 +0100
Subject: [R] Global counter in library
Message-ID: <dc18e824-b2f9-e795-6c23-ddbf2ce2b00a@ieu.uzh.ch>

I am trying to implement a global counter in a library. This counter 
resides in an environment local to that package, and is incremented by 
calling a function within this package.

Problems arise when I use parallelization. I protected the increment 
operation with a lock so that this operation effectively should be 
atomic. However, the environment local to the package seems not to be 
shared among the threads that use the functions in that package (not in 
the sense that a change would be visible in the other threads).

How is such a global counter best implemented?

Here is a basic sketch of what I tried:


library(parallel)

library(flock)

## begin of code in package

tmp <- tempfile()
.localstuff <- new.env()
.localstuff$N <- 0
.localstuff$tmp <- tempfile()

incr <- function()
{
 ??? h <- lock(.localstuff$tmp)
 ??? .localstuff$N <- .localstuff$N + 1
 ??? unlock(h)
}

## end code in library

invisible(mclapply(1:10, function(i) incr()))

cat("N =", .localstuff$N)


From @yen @end|ng |rom hqu@edu@cn  Tue Mar 12 16:05:23 2019
From: @yen @end|ng |rom hqu@edu@cn (Steven Yen)
Date: Tue, 12 Mar 2019 23:05:23 +0800
Subject: [R] Installing package into...
Message-ID: <75f326bc-2307-4ca9-dc2c-9691a71170aa@hqu.edu.cn>

I install package using either the command line or Tools -> Install 
packages... (in RStudio) and get a non-fatal message saying...
Installing package into ?C:/Users/xuhaer/Documents/R/win-library/3.5?
(as ?lib? is unspecified)
I know it is not a fatal message. But, is there a way to do a cleaner 
installation without getting such message? Thanks.

-- 
syen at hqu.edu.cn


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Mar 12 17:20:52 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 12 Mar 2019 12:20:52 -0400
Subject: [R] Installing package into...
In-Reply-To: <75f326bc-2307-4ca9-dc2c-9691a71170aa@hqu.edu.cn>
References: <75f326bc-2307-4ca9-dc2c-9691a71170aa@hqu.edu.cn>
Message-ID: <4c298bd7-e142-533a-e1eb-929a782fbf1e@gmail.com>

On 12/03/2019 11:05 a.m., Steven Yen wrote:
> I install package using either the command line or Tools -> Install
> packages... (in RStudio) and get a non-fatal message saying...
> Installing package into ?C:/Users/xuhaer/Documents/R/win-library/3.5?
> (as ?lib? is unspecified)
> I know it is not a fatal message. But, is there a way to do a cleaner
> installation without getting such message? Thanks.
> 

Have you tried specifying the library?  Then R wouldn't tell you its guess.

Duncan Murdoch


From tg@77m @end|ng |rom y@hoo@com  Tue Mar 12 17:23:22 2019
From: tg@77m @end|ng |rom y@hoo@com (Thomas Subia)
Date: Tue, 12 Mar 2019 16:23:22 +0000 (UTC)
Subject: [R] About wilcox test
References: <131699924.4210357.1552407802081.ref@mail.yahoo.com>
Message-ID: <131699924.4210357.1552407802081@mail.yahoo.com>

Javid wrote: 

"I have two set of data in excel:
A column( 16.38, -31, -16.77, 127, -57, 23.44 and so on)
B column ( -12, -59.23, -44, 34.23, 55.5, -12.12 and so on)

I run the wilcox test as :

wilcox.test(A , B, data = mydata, paired = FALSE)

I got always the p value very high, like 0.60

Even I make changes in the data, it gives me 0.7, 0.4 etc which is too high
than 0.05 and can not thus reject the null hypothesis.

What could be the problem as I know there is difference in the data?"
How did you conclude there is a difference in the data? 2-sample t-test?Some details and a data set would be helpful for someone to investigate this further.
Thomas SubiaStatisticianIMG Precision



	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Tue Mar 12 17:24:30 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Tue, 12 Mar 2019 16:24:30 +0000
Subject: [R] Installing package into...
In-Reply-To: <75f326bc-2307-4ca9-dc2c-9691a71170aa@hqu.edu.cn>
References: <75f326bc-2307-4ca9-dc2c-9691a71170aa@hqu.edu.cn>
Message-ID: <768b2824-06f1-81d8-a552-eb11dd038cfa@dewey.myzen.co.uk>

Dear Steven

If you use install,packages() from within R there is a lib argument. I 
do not use RStudio so not sure how it works there.

Michael

On 12/03/2019 15:05, Steven Yen wrote:
> I install package using either the command line or Tools -> Install
> packages... (in RStudio) and get a non-fatal message saying...
> Installing package into ?C:/Users/xuhaer/Documents/R/win-library/3.5?
> (as ?lib? is unspecified)
> I know it is not a fatal message. But, is there a way to do a cleaner
> installation without getting such message? Thanks.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Mar 12 17:35:51 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 12 Mar 2019 12:35:51 -0400
Subject: [R] Global counter in library
In-Reply-To: <dc18e824-b2f9-e795-6c23-ddbf2ce2b00a@ieu.uzh.ch>
References: <dc18e824-b2f9-e795-6c23-ddbf2ce2b00a@ieu.uzh.ch>
Message-ID: <80d6c1c0-0da0-b35c-ddfa-47dcfa8c8141@gmail.com>

On 11/03/2019 2:57 p.m., Pascal A. Niklaus wrote:
> I am trying to implement a global counter in a library. This counter
> resides in an environment local to that package, and is incremented by
> calling a function within this package.
> 
> Problems arise when I use parallelization. I protected the increment
> operation with a lock so that this operation effectively should be
> atomic. However, the environment local to the package seems not to be
> shared among the threads that use the functions in that package (not in
> the sense that a change would be visible in the other threads).
> 
> How is such a global counter best implemented?

Use the Rdsm package.  See ?Rdsm::Rdsm for help.

Duncan Murdoch

> 
> Here is a basic sketch of what I tried:
> 
> 
> library(parallel)
> 
> library(flock)
> 
> ## begin of code in package
> 
> tmp <- tempfile()
> .localstuff <- new.env()
> .localstuff$N <- 0
> .localstuff$tmp <- tempfile()
> 
> incr <- function()
> {
>   ??? h <- lock(.localstuff$tmp)
>   ??? .localstuff$N <- .localstuff$N + 1
>   ??? unlock(h)
> }
> 
> ## end code in library
> 
> invisible(mclapply(1:10, function(i) incr()))
> 
> cat("N =", .localstuff$N)


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Mar 12 17:40:07 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 12 Mar 2019 16:40:07 +0000
Subject: [R] Installing package into...
In-Reply-To: <768b2824-06f1-81d8-a552-eb11dd038cfa@dewey.myzen.co.uk>
References: <75f326bc-2307-4ca9-dc2c-9691a71170aa@hqu.edu.cn>
 <768b2824-06f1-81d8-a552-eb11dd038cfa@dewey.myzen.co.uk>
Message-ID: <af115d29-81e2-1cf1-56a7-87c34304980f@sapo.pt>

Hello,

Yes, the lib argument works in RStudio.

Hope this helps,

Rui Barradas

?s 16:24 de 12/03/2019, Michael Dewey escreveu:
> Dear Steven
> 
> If you use install,packages() from within R there is a lib argument. I 
> do not use RStudio so not sure how it works there.
> 
> Michael
> 
> On 12/03/2019 15:05, Steven Yen wrote:
>> I install package using either the command line or Tools -> Install
>> packages... (in RStudio) and get a non-fatal message saying...
>> Installing package into ?C:/Users/xuhaer/Documents/R/win-library/3.5?
>> (as ?lib? is unspecified)
>> I know it is not a fatal message. But, is there a way to do a cleaner
>> installation without getting such message? Thanks.
>>
>


From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Mar 12 17:40:18 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 12 Mar 2019 09:40:18 -0700
Subject: [R] About wilcox test
In-Reply-To: <131699924.4210357.1552407802081@mail.yahoo.com>
References: <131699924.4210357.1552407802081.ref@mail.yahoo.com>
 <131699924.4210357.1552407802081@mail.yahoo.com>
Message-ID: <86df5675-2028-411a-9a9e-b18c3895c5f4@comcast.net>


On 3/12/19 9:23 AM, Thomas Subia via R-help wrote:
> Javid wrote:
>
> "I have two set of data in excel:
> A column( 16.38, -31, -16.77, 127, -57, 23.44 and so on)
> B column ( -12, -59.23, -44, 34.23, 55.5, -12.12 and so on)


If those are in an excel spreadsheet than you need to indicate exactly 
how they? became `mydata`. And then you need to read the help page. The 
Usage section indicates that there are two forms. The first which you 
are in effect choosing has the first two arguments as data objects. The 
second version which appear to desire has a formula object as it first 
argument. Try:

This would be a way to use the first form:


 ?with( mydata, wilcox.test(A,? B,? paired = FALSE))

And this would succeed for the second form under the assumption that it 
is a dataframe with only two columns of numeric data:


 ?wilcox.test(values~ ind, data=stack(mydata), paired = FALSE)

Please read the Posting guide and in the future post in plain text.

David

>
> I run the wilcox test as :
>
> wilcox.test(A , B, data = mydata, paired = FALSE)
>
> I got always the p value very high, like 0.60
>
> Even I make changes in the data, it gives me 0.7, 0.4 etc which is too high
> than 0.05 and can not thus reject the null hypothesis.
>
> What could be the problem as I know there is difference in the data?"
> How did you conclude there is a difference in the data? 2-sample t-test?Some details and a data set would be helpful for someone to investigate this further.
> Thomas SubiaStatisticianIMG Precision
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From b@mrut| @end|ng |rom hotm@||@com  Tue Mar 12 20:34:14 2019
From: b@mrut| @end|ng |rom hotm@||@com (SMRUTI BULSARI)
Date: Tue, 12 Mar 2019 19:34:14 +0000
Subject: [R] Issue with t.test
Message-ID: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>

I am writing this e-mail as I have come across one issue while performing t-test using R. If the function t.test(?) is used, a p-value of 1 is obtained for a calculated t-value greater than 1.96. If the t-value is greater than 1.96, the null hypothesis is rejected. However, if the p-value is greater than 0.05, one may say that there is not enough evidence to reject the null hypothesis. Thus, a t-value of greater than 1.96 and a p-value of 1 leaves the user of R software confused on whether to reject or not to reject the null. Moreover, how can p-value of stochastic process be exactly equal to 1? Can you please guide me on whom should I report this issue to? Or can you please forward this issue to the appropriate person / team?

Thanking you,

Sincerely,

Smruti Bulsari

Assistant Professor
Department of Human Resource Development
Veer Narmad South Gujarat University
Surat - 395 007
(Gujarat) INDIA

Ph:+91-261-2256071 (Ext. 204)

Alternate E-mail: sbbulsari at vnsgu.ac.in

LinkedIn Profile: https://www.linkedin.com/in/smruti-bulsari-0a30b312/

Blog Posts: https://wordpress.com/posts/smrutibulsari.wordpress.com



	[[alternative HTML version deleted]]


From 538280 @end|ng |rom gm@||@com  Tue Mar 12 21:06:31 2019
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Tue, 12 Mar 2019 14:06:31 -0600
Subject: [R] Issue with t.test
In-Reply-To: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>
References: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>
Message-ID: <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>

Can you show us an example with the data that you are using and the
output from t.test.

A t-value of 1.96 is not an automatic rejection.  It depends on alpha
and the degrees of freedom.  Even if we set alpha at 0.05, 1.96 should
not give a p-value less than 0.05 with finite degrees of freedom.

This also depends on the arguments to t.test.  If the alternative is
set to "Less" then a t value of 1.96 would give a p-value close to
0.975 for high degrees of freedom.

The only time I have seen t.test give a p-value of 1 is when the data
mean exactly equals the null hypothesis mean and the alternative is
the default of two.sided.

For us to diagnose what is going on we need to see your command (how
you are calling t.test) and what data you are using.

On Tue, Mar 12, 2019 at 1:49 PM SMRUTI BULSARI <bsmruti at hotmail.com> wrote:
>
> I am writing this e-mail as I have come across one issue while performing t-test using R. If the function t.test(?) is used, a p-value of 1 is obtained for a calculated t-value greater than 1.96. If the t-value is greater than 1.96, the null hypothesis is rejected. However, if the p-value is greater than 0.05, one may say that there is not enough evidence to reject the null hypothesis. Thus, a t-value of greater than 1.96 and a p-value of 1 leaves the user of R software confused on whether to reject or not to reject the null. Moreover, how can p-value of stochastic process be exactly equal to 1? Can you please guide me on whom should I report this issue to? Or can you please forward this issue to the appropriate person / team?
>
> Thanking you,
>
> Sincerely,
>
> Smruti Bulsari
>
> Assistant Professor
> Department of Human Resource Development
> Veer Narmad South Gujarat University
> Surat - 395 007
> (Gujarat) INDIA
>
> Ph:+91-261-2256071 (Ext. 204)
>
> Alternate E-mail: sbbulsari at vnsgu.ac.in
>
> LinkedIn Profile: https://www.linkedin.com/in/smruti-bulsari-0a30b312/
>
> Blog Posts: https://wordpress.com/posts/smrutibulsari.wordpress.com
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From p@@c@|@n|k|@u@ @end|ng |rom |eu@uzh@ch  Tue Mar 12 20:13:34 2019
From: p@@c@|@n|k|@u@ @end|ng |rom |eu@uzh@ch (Pascal A. Niklaus)
Date: Tue, 12 Mar 2019 20:13:34 +0100
Subject: [R] Global counter in library
In-Reply-To: <80d6c1c0-0da0-b35c-ddfa-47dcfa8c8141@gmail.com>
References: <dc18e824-b2f9-e795-6c23-ddbf2ce2b00a@ieu.uzh.ch>
 <80d6c1c0-0da0-b35c-ddfa-47dcfa8c8141@gmail.com>
Message-ID: <2a4256d0-16c4-ad11-d804-4edb4d2f0bdd@ieu.uzh.ch>

Thanks for the hint. I think I will use a big.matrix [with 1 element 
only ;-) ] and a mutex to ensure access is thread-safe.

Pascal


On 12.03.19 17:35, Duncan Murdoch wrote:
> On 11/03/2019 2:57 p.m., Pascal A. Niklaus wrote:
>> I am trying to implement a global counter in a library. This counter
>> resides in an environment local to that package, and is incremented by
>> calling a function within this package.
>>
>> Problems arise when I use parallelization. I protected the increment
>> operation with a lock so that this operation effectively should be
>> atomic. However, the environment local to the package seems not to be
>> shared among the threads that use the functions in that package (not in
>> the sense that a change would be visible in the other threads).
>>
>> How is such a global counter best implemented?
>
> Use the Rdsm package.? See ?Rdsm::Rdsm for help.
>
> Duncan Murdoch
>
>>
>> Here is a basic sketch of what I tried:
>>
>>
>> library(parallel)
>>
>> library(flock)
>>
>> ## begin of code in package
>>
>> tmp <- tempfile()
>> .localstuff <- new.env()
>> .localstuff$N <- 0
>> .localstuff$tmp <- tempfile()
>>
>> incr <- function()
>> {
>> ? ??? h <- lock(.localstuff$tmp)
>> ? ??? .localstuff$N <- .localstuff$N + 1
>> ? ??? unlock(h)
>> }
>>
>> ## end code in library
>>
>> invisible(mclapply(1:10, function(i) incr()))
>>
>> cat("N =", .localstuff$N)


From r@turner @end|ng |rom @uck|@nd@@c@nz  Tue Mar 12 21:38:24 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 13 Mar 2019 09:38:24 +1300
Subject: [R] [FORGED] Re:  Issue with t.test
In-Reply-To: <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>
References: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>
 <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>
Message-ID: <a43c2498-40f5-090e-b8fb-22724da41383@auckland.ac.nz>

On 13/03/19 9:06 AM, Greg Snow wrote:

<SNIP>

> The only time I have seen t.test give a p-value of 1 is when the
> data mean exactly equals the null hypothesis mean and the alternative
> is the default of two.sided.

<SNIP>

Doesn't have to be *exact* equality.  Just close!

E.g.:

set.seed(42)
x <- runif(10)
mew <- 0.63626
mew==mean(x) # FALSE
t.test(x,mu=mew)

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From b@mrut| @end|ng |rom hotm@||@com  Wed Mar 13 11:47:27 2019
From: b@mrut| @end|ng |rom hotm@||@com (SMRUTI BULSARI)
Date: Wed, 13 Mar 2019 10:47:27 +0000
Subject: [R] Issue with t.test
In-Reply-To: <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>
References: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>,
 <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>
Message-ID: <PS2PR02MB2839F67B6327A80D90A9A243A94A0@PS2PR02MB2839.apcprd02.prod.outlook.com>

Dear Greg Snow / Rolf Turner:

I must thank you very much for your very prompt and a detailed response to my query on t.test command.

I am attaching herewith, a file in csv format, containing the column vector with on which I am performing the one-sample t-test using t.test. I understand that 1.96 is not an automatic rejection of null. However, at alpha = 0.05 and degrees of freedom = 99 (100 observations minus 1), one may consider rejecting the null at t-value greater than 1.96 for a two-tailed test and an absolute value of 1.64 for upper and lower tailed tests. I am performing one-sample t-test with a hypothesized mean of 50 and the actual mean works out to 59.96753. I would also like to bring to your notice that while, I perform a two-tailed t-test using the following command:

t.test(X,mu=50,alternative='two.sided',conf.level=0.95)

I am getting a p-value < 2.2e-16.

If I use the same command with alternative = ?greater?, as follows:

t.test(X,mu=50,alternative='greater',conf.level=0.95)

I still get a p-value <2.2e-16

However, if I use the command with alternative = ?less?, as follows:

t.test(X,mu=50,alternative='less',conf.level=0.95)

I am getting a p-value = 1 and therefore, this query. As you mention in your e-mail, I even thought that 2.2e-16 is to be deducted from 1 to get the p-value for the ?less? and perhaps that seems to be a explanation to it.

I am still attaching the R script file and the data file for your perusal.

Thanking you once again.

Kindest regards,

Smruti Bulsari

LinkedIn Profile: https://www.linkedin.com/in/smruti-bulsari-0a30b312/

Blog Posts: https://wordpress.com/posts/smrutibulsari.wordpress.com


________________________________
From: Greg Snow <538280 at gmail.com>
Sent: 13 March 2019 01:36
To: SMRUTI BULSARI
Cc: r-help at R-project.org
Subject: Re: [R] Issue with t.test

Can you show us an example with the data that you are using and the
output from t.test.

A t-value of 1.96 is not an automatic rejection.  It depends on alpha
and the degrees of freedom.  Even if we set alpha at 0.05, 1.96 should
not give a p-value less than 0.05 with finite degrees of freedom.

This also depends on the arguments to t.test.  If the alternative is
set to "Less" then a t value of 1.96 would give a p-value close to
0.975 for high degrees of freedom.

The only time I have seen t.test give a p-value of 1 is when the data
mean exactly equals the null hypothesis mean and the alternative is
the default of two.sided.

For us to diagnose what is going on we need to see your command (how
you are calling t.test) and what data you are using.

On Tue, Mar 12, 2019 at 1:49 PM SMRUTI BULSARI <bsmruti at hotmail.com> wrote:
>
> I am writing this e-mail as I have come across one issue while performing t-test using R. If the function t.test(?) is used, a p-value of 1 is obtained for a calculated t-value greater than 1.96. If the t-value is greater than 1.96, the null hypothesis is rejected. However, if the p-value is greater than 0.05, one may say that there is not enough evidence to reject the null hypothesis. Thus, a t-value of greater than 1.96 and a p-value of 1 leaves the user of R software confused on whether to reject or not to reject the null. Moreover, how can p-value of stochastic process be exactly equal to 1? Can you please guide me on whom should I report this issue to? Or can you please forward this issue to the appropriate person / team?
>
> Thanking you,
>
> Sincerely,
>
> Smruti Bulsari
>
> Assistant Professor
> Department of Human Resource Development
> Veer Narmad South Gujarat University
> Surat - 395 007
> (Gujarat) INDIA
>
> Ph:+91-261-2256071 (Ext. 204)
>
> Alternate E-mail: sbbulsari at vnsgu.ac.in
>
> LinkedIn Profile: https://www.linkedin.com/in/smruti-bulsari-0a30b312/
>
> Blog Posts: https://wordpress.com/posts/smrutibulsari.wordpress.com
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com

From d@n|e|@j@ck@on1 @end|ng |rom @@tr@zenec@@com  Wed Mar 13 12:06:53 2019
From: d@n|e|@j@ck@on1 @end|ng |rom @@tr@zenec@@com (Jackson, Daniel)
Date: Wed, 13 Mar 2019 11:06:53 +0000
Subject: [R] Rank ANCOVA
Message-ID: <VI1PR04MB5488035B17D6AB450F6349B9B34A0@VI1PR04MB5488.eurprd04.prod.outlook.com>

Hi Frank and Dennis

I am in a similar situation but I would prefer to use a proportional odds model. 2 questions.


  1.  Has rank ancova been implemented in R now, despite its short comings?
  2.  Where has it been shown to yield unreliable analyses? I would like this evidence (which I believe but I would like to convince others!).

Thanks, Dan
________________________________

AstraZeneca UK Limited is a company incorporated in Engl...{{dropped:16}}


From drj|m|emon @end|ng |rom gm@||@com  Wed Mar 13 21:55:19 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 14 Mar 2019 07:55:19 +1100
Subject: [R] Issue with t.test
In-Reply-To: <PS2PR02MB2839F67B6327A80D90A9A243A94A0@PS2PR02MB2839.apcprd02.prod.outlook.com>
References: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>
 <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>
 <PS2PR02MB2839F67B6327A80D90A9A243A94A0@PS2PR02MB2839.apcprd02.prod.outlook.com>
Message-ID: <CA+8X3fWZDrxr4a0DbyVV-LfsSM4GZdaw_mEFSFYtc2tkX_CCSQ@mail.gmail.com>

Hi Smruti,
In the example in question, you are probably doing something like this:

# I didn't see any attachment for the data
X<-rnorm(99,mean=59.96753)
t.test(X,mu=50,alternative="less")

       One Sample t-test

data:  X
t = 101.29, df = 98, p-value = 1
alternative hypothesis: true mean is less than 50
95 percent confidence interval:
    -Inf 60.07022
sample estimates:
mean of x
59.90779

What this probably means is since the lower confidence interval for
the test is undefined, the program returns a p-value of 1 as an
estimate, much like the p-value<2.2e-16 for the other tests.

> 1-2e-16
[1] 1

Neither of these values are exact, but indicate that the null
hypothesis should be accepted or rejected respectively.

Jim


From jessy@white m@iii@g oii b2bs@iesd@t@@com  Wed Mar 13 18:21:32 2019
From: jessy@white m@iii@g oii b2bs@iesd@t@@com (jessy@white m@iii@g oii b2bs@iesd@t@@com)
Date: Wed, 13 Mar 2019 22:51:32 +0530
Subject: [R] The Travel Goods Show-Business Emails
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAAKoKoFp+p4xNugk6wWOYfngBAEF4T6PYsR1KqBwzL8tPSXsAAAAAAQkAAEYAAAAAAAAAqgqgWn6njE26CTrBY5h+eAcAQXhPo9ixHUqoHDMvy09JewAAAABqYwAAQXhPo9ixHUqoHDMvy09JewAAA2S43gAAAQAAAAA=@b2bsalesdata.com>

Hi,

I am following up to check if you are interested in acquiring the Attendees
List.

The Travel Goods Show-2019
Las Vegas, USA

26-28 Mar 2019 

Counts: 3,500

Let me know if you are interested so that I can send you the cost &
additional details.


Waiting for your response.

Thanks & Regards,
Jessy White- Business Analyst

 


	[[alternative HTML version deleted]]


From |e|y@de||m@ @end|ng |rom cp@co@|d  Wed Mar 13 21:51:37 2019
From: |e|y@de||m@ @end|ng |rom cp@co@|d (Greenlight Financial Services)
Date: Thu, 14 Mar 2019 03:51:37 +0700 (WIT)
Subject: [R] Loan Offer
Message-ID: <79734747.2713804.1552510297527.JavaMail.root@jktzmb02.cp.co.id>


Do you need a Loan @ 2% P.A? Mail us your: Names,Home Add,Mob No,Email id,Amount Needed,Loan Duration,Occupation :If you are interested kindly contact us via email for more details.


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Thu Mar 14 11:31:40 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Thu, 14 Mar 2019 11:31:40 +0100
Subject: [R] Display common color scale on multiple scatter3D plots
Message-ID: <CAMk+s2RoveSZgRJXbgAFb08AGp7pHcAf_VXxwLV9VBg866kP-w@mail.gmail.com>

Dear all,
I am trying to display multivariate data using the library plot3D. I
have 3 variables that go on the axis and a fourth that I would like to
display as a color shade. However, the scale differs between plots
because the data I am using varies.
Would be possible to maintain a single scale so that the different
points will have the same shade on different plots? This would be like
giving the same axis scale on different plots.

For example, I got the following:
>>>>
set.seed(50)
x = runif(10, 6, 18)
y = runif(10,  5,  7)
z = runif(10, 0.7, 3.14)
k = runif(10, 0.5, 1.2)
w = runif(10, min(z, k), max(z,k))
df = my.data <- data.frame(x, y, z, k, w)

library("plot3D")
X = df$x
Y = df$y
Z = df$z
K = df$k
W = df$w
scatter3D(X, Y, Z, col.var = Z, pch = 16, cex = 2)
scatter3D(X, Y, K, col.var = K, pch = 16, cex = 2)
<<<

In these plots the scale varies (0.6-0.9 in the first plot, 1.5-2.5 in
the second); I tried by creating a common scale with the variable W
which ranges from the min and max of the fourth levels represented by
the variables Z and K, but the result is the same:
scatter3D(X, Y, Z, col.var = W, pch = 16, cex = 2)
scatter3D(X, Y, K, col.var = W, pch = 16, cex = 2)

Essentially, I would like the variables Z and K to be shaded according
to a common scale so that I could evaluate the shades of the two plots
directly.
Would that be possible?
Thank you.

-- 
Best regards,
Luigi


From pd@|gd @end|ng |rom gm@||@com  Thu Mar 14 11:37:14 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 14 Mar 2019 11:37:14 +0100
Subject: [R] Rank ANCOVA
In-Reply-To: <VI1PR04MB5488035B17D6AB450F6349B9B34A0@VI1PR04MB5488.eurprd04.prod.outlook.com>
References: <VI1PR04MB5488035B17D6AB450F6349B9B34A0@VI1PR04MB5488.eurprd04.prod.outlook.com>
Message-ID: <E41334E9-F755-4EBA-98FC-472DEE4D8D76@gmail.com>

1. This is the R-help mailing list, not "Frank and Dennis"
2. You seem to be referring to two posts from July 2010 by Frank Harrell and Dennis Fisher. 

-pd

> On 13 Mar 2019, at 12:06 , Jackson, Daniel <daniel.jackson1 at astrazeneca.com> wrote:
> 
> Hi Frank and Dennis
> 
> I am in a similar situation but I would prefer to use a proportional odds model. 2 questions.
> 
> 
>  1.  Has rank ancova been implemented in R now, despite its short comings?
>  2.  Where has it been shown to yield unreliable analyses? I would like this evidence (which I believe but I would like to convince others!).
> 
> Thanks, Dan
> ________________________________
> 
> AstraZeneca UK Limited is a company incorporated in =\...{{dropped:26}}


From pb@rro@ @end|ng |rom u@|g@pt  Thu Mar 14 12:29:56 2019
From: pb@rro@ @end|ng |rom u@|g@pt (Pedro Conte de Barros)
Date: Thu, 14 Mar 2019 11:29:56 +0000
Subject: [R] Sorting vector based on pairs of comparisons
Message-ID: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>

Dear All,

This should be a quite established algorithm, but I have been searching 
for a couple days already without finding any satisfactory solution.

I have a matrix defining pairs of Smaller-Larger arbitrary character 
values, like below

Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")

Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF"

matComp <- cbind(Smaller, Larger)

so that matComp looks like this

 ???? Smaller Larger
[1,] "ASD"?? "SDR"
[2,] "DFE"?? "EDF"
[3,] "ASD"?? "KLM"
[4,] "SDR"?? "KLM"
[5,] "EDF"?? "SDR"
[6,] "ASD"?? "EDF"

This matrix establishes six pairs of "larger than" relationships that 
can be used to sort the unique values in the matrix,

 > unique(as.vector(matComp))
[1] "ASD" "DFE" "SDR" "EDF" "KLM"

Specifically, I would like to get this:

sorted <- c("ASD", "DFE", "EDF", "SDR", "KLM")

or, equally valid (my matrix does not have the full information):

sorted <- c("DFE", "ASD", "EDF", "SDR", "KLM")

Preferably, I would get the different combinations of the unique values 
that satisfy the "larger than" conditions in the matrix...


I am sure this is a trivial problem, but I could not find any algorithm 
to solve it.

Any help would be highly appreciated


From rmh @end|ng |rom temp|e@edu  Thu Mar 14 13:04:04 2019
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Thu, 14 Mar 2019 08:04:04 -0400
Subject: [R] Sorting vector based on pairs of comparisons
In-Reply-To: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
Message-ID: <CAGx1TMBa9LmYGYxpKJuw+_aUMDsVqEz=0RqV7jp98L7M98-VbA@mail.gmail.com>

Try this. Anything that appears only in Smaller is candidate for smallest.
Among those, order is arbitrary.

Anything that appears only in Larger is a candidate for largest. Among
those order is arbitrary.

Remove rows of matComp containing the already classified items.  Repeat
with the smaller set.

On Thu, Mar 14, 2019 at 07:30 Pedro Conte de Barros <pbarros at ualg.pt> wrote:

> Dear All,
>
> This should be a quite established algorithm, but I have been searching
> for a couple days already without finding any satisfactory solution.
>
> I have a matrix defining pairs of Smaller-Larger arbitrary character
> values, like below
>
> Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")
>
> Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF"
>
> matComp <- cbind(Smaller, Larger)
>
> so that matComp looks like this
>
>       Smaller Larger
> [1,] "ASD"   "SDR"
> [2,] "DFE"   "EDF"
> [3,] "ASD"   "KLM"
> [4,] "SDR"   "KLM"
> [5,] "EDF"   "SDR"
> [6,] "ASD"   "EDF"
>
> This matrix establishes six pairs of "larger than" relationships that
> can be used to sort the unique values in the matrix,
>
>  > unique(as.vector(matComp))
> [1] "ASD" "DFE" "SDR" "EDF" "KLM"
>
> Specifically, I would like to get this:
>
> sorted <- c("ASD", "DFE", "EDF", "SDR", "KLM")
>
> or, equally valid (my matrix does not have the full information):
>
> sorted <- c("DFE", "ASD", "EDF", "SDR", "KLM")
>
> Preferably, I would get the different combinations of the unique values
> that satisfy the "larger than" conditions in the matrix...
>
>
> I am sure this is a trivial problem, but I could not find any algorithm
> to solve it.
>
> Any help would be highly appreciated
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu Mar 14 14:09:48 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 Mar 2019 06:09:48 -0700
Subject: [R] Sorting vector based on pairs of comparisons
In-Reply-To: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
Message-ID: <CAGxFJbQO8G4gRogj57Cats1u+2ejUehoZg_XXz-cy9QvQEGN6A@mail.gmail.com>

This cannot be done unless transitivity is guaranteed. Is it?

S   L

a  b
b  c
c  a

Bert


On Thu, Mar 14, 2019, 4:30 AM Pedro Conte de Barros <pbarros at ualg.pt> wrote:

> Dear All,
>
> This should be a quite established algorithm, but I have been searching
> for a couple days already without finding any satisfactory solution.
>
> I have a matrix defining pairs of Smaller-Larger arbitrary character
> values, like below
>
> Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")
>
> Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF"
>
> matComp <- cbind(Smaller, Larger)
>
> so that matComp looks like this
>
>       Smaller Larger
> [1,] "ASD"   "SDR"
> [2,] "DFE"   "EDF"
> [3,] "ASD"   "KLM"
> [4,] "SDR"   "KLM"
> [5,] "EDF"   "SDR"
> [6,] "ASD"   "EDF"
>
> This matrix establishes six pairs of "larger than" relationships that
> can be used to sort the unique values in the matrix,
>
>  > unique(as.vector(matComp))
> [1] "ASD" "DFE" "SDR" "EDF" "KLM"
>
> Specifically, I would like to get this:
>
> sorted <- c("ASD", "DFE", "EDF", "SDR", "KLM")
>
> or, equally valid (my matrix does not have the full information):
>
> sorted <- c("DFE", "ASD", "EDF", "SDR", "KLM")
>
> Preferably, I would get the different combinations of the unique values
> that satisfy the "larger than" conditions in the matrix...
>
>
> I am sure this is a trivial problem, but I could not find any algorithm
> to solve it.
>
> Any help would be highly appreciated
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pb@rro@ @end|ng |rom u@|g@pt  Thu Mar 14 15:14:19 2019
From: pb@rro@ @end|ng |rom u@|g@pt (Pedro Conte de Barros)
Date: Thu, 14 Mar 2019 14:14:19 +0000
Subject: [R] Sorting vector based on pairs of comparisons
In-Reply-To: <CAGxFJbQO8G4gRogj57Cats1u+2ejUehoZg_XXz-cy9QvQEGN6A@mail.gmail.com>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
 <CAGxFJbQO8G4gRogj57Cats1u+2ejUehoZg_XXz-cy9QvQEGN6A@mail.gmail.com>
Message-ID: <49f98eb8-ce7d-17c3-b7f7-961b15969412@ualg.pt>

Thanks for this.

Yes, this is checked before trying to process this.

Pedro

On 14/03/2019 14.09, Bert Gunter wrote:
This cannot be done unless transitivity is guaranteed. Is it?

S   L

a  b
b  c
c  a

Bert


On Thu, Mar 14, 2019, 4:30 AM Pedro Conte de Barros <pbarros at ualg.pt<mailto:pbarros at ualg.pt>> wrote:
Dear All,

This should be a quite established algorithm, but I have been searching
for a couple days already without finding any satisfactory solution.

I have a matrix defining pairs of Smaller-Larger arbitrary character
values, like below

Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")

Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF"

matComp <- cbind(Smaller, Larger)

so that matComp looks like this

      Smaller Larger
[1,] "ASD"   "SDR"
[2,] "DFE"   "EDF"
[3,] "ASD"   "KLM"
[4,] "SDR"   "KLM"
[5,] "EDF"   "SDR"
[6,] "ASD"   "EDF"

This matrix establishes six pairs of "larger than" relationships that
can be used to sort the unique values in the matrix,

 > unique(as.vector(matComp))
[1] "ASD" "DFE" "SDR" "EDF" "KLM"

Specifically, I would like to get this:

sorted <- c("ASD", "DFE", "EDF", "SDR", "KLM")

or, equally valid (my matrix does not have the full information):

sorted <- c("DFE", "ASD", "EDF", "SDR", "KLM")

Preferably, I would get the different combinations of the unique values
that satisfy the "larger than" conditions in the matrix...


I am sure this is a trivial problem, but I could not find any algorithm
to solve it.

Any help would be highly appreciated

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Thu Mar 14 16:07:11 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 14 Mar 2019 08:07:11 -0700
Subject: [R] Sorting vector based on pairs of comparisons
In-Reply-To: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
Message-ID: <CAF8bMcb8gnUzWzbGx69G7OpOJFu=0wK697TnuFKUT0-fghcu9w@mail.gmail.com>

This is called topological sorting in some circles.  The function below
will give you one ordering that is consistent with the contraints but not
all possible orderings.  I couldn't find such a function in core R so I
wrote one a while back based on Kahn's algorithm, as described in Wikipedia.

> Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")
> Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF")
> matComp <- cbind(Smaller, Larger)
> sortTopologically(matComp, unique(as.vector(matComp)))
[1] "ASD" "DFE" "EDF" "SDR" "KLM"

Bill Dunlap
TIBCO Software
wdunlap tibco.com

sortTopologically <- function(edgeMatrix, V)
{
    # edgeMatrix is 2-column matrix which describes a partial
    #   ordering of a set of vertices. The first column is the 'from'
vertex,
    #   the second the 'to' vertex.
    # V is the vector of all the vertices in the graph.
    #
    # Return a vector, L, consisting of the vertices in
    #   V in an order consistent with the partial ordering
    #   described by edgeMatrix.
    # Throw an error if such an ordering is not possible.
    #
    # Use Kahn's algorithm (
https://en.wikipedia.org/wiki/Topological_sorting).
    #
    # Note that disconnected vertices will not be mentioned in edgeMatrix,
    #   but will be in V.
    stopifnot(is.matrix(edgeMatrix),
              ncol(edgeMatrix)==2,
              !any(is.na(edgeMatrix)),
              !any(is.na(V)),
              all(as.vector(edgeMatrix) %in% V))
    L <- V[0] # match the type of the input
    S <- setdiff(V, edgeMatrix[, 2])
    V <- setdiff(V, S)
    while(length(S) > 0) {
        n <- S[1]
        # cat("Adding", n, "to L", "\n")
        L <- c(L, n)
        S <- S[-1]
        mRow <- edgeMatrix[,1] == n
        edgeMatrix <- edgeMatrix[ !mRow, , drop=FALSE ]
        S <- c(S, setdiff(V, edgeMatrix[,2]))
        V <- setdiff(V, S)
    }
    if (nrow(edgeMatrix) > 0) {
        stop("There are cycles in the dependency graph")
    }
    L
}


On Thu, Mar 14, 2019 at 4:30 AM Pedro Conte de Barros <pbarros at ualg.pt>
wrote:

> Dear All,
>
> This should be a quite established algorithm, but I have been searching
> for a couple days already without finding any satisfactory solution.
>
> I have a matrix defining pairs of Smaller-Larger arbitrary character
> values, like below
>
> Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")
>
> Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF"
>
> matComp <- cbind(Smaller, Larger)
>
> so that matComp looks like this
>
>       Smaller Larger
> [1,] "ASD"   "SDR"
> [2,] "DFE"   "EDF"
> [3,] "ASD"   "KLM"
> [4,] "SDR"   "KLM"
> [5,] "EDF"   "SDR"
> [6,] "ASD"   "EDF"
>
> This matrix establishes six pairs of "larger than" relationships that
> can be used to sort the unique values in the matrix,
>
>  > unique(as.vector(matComp))
> [1] "ASD" "DFE" "SDR" "EDF" "KLM"
>
> Specifically, I would like to get this:
>
> sorted <- c("ASD", "DFE", "EDF", "SDR", "KLM")
>
> or, equally valid (my matrix does not have the full information):
>
> sorted <- c("DFE", "ASD", "EDF", "SDR", "KLM")
>
> Preferably, I would get the different combinations of the unique values
> that satisfy the "larger than" conditions in the matrix...
>
>
> I am sure this is a trivial problem, but I could not find any algorithm
> to solve it.
>
> Any help would be highly appreciated
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From d@n|e|@j@ck@on1 @end|ng |rom @@tr@zenec@@com  Thu Mar 14 11:59:01 2019
From: d@n|e|@j@ck@on1 @end|ng |rom @@tr@zenec@@com (Jackson, Daniel)
Date: Thu, 14 Mar 2019 10:59:01 +0000
Subject: [R] Rank ANCOVA
In-Reply-To: <E41334E9-F755-4EBA-98FC-472DEE4D8D76@gmail.com>
References: <VI1PR04MB5488035B17D6AB450F6349B9B34A0@VI1PR04MB5488.eurprd04.prod.outlook.com>
 <E41334E9-F755-4EBA-98FC-472DEE4D8D76@gmail.com>
Message-ID: <AM0PR04MB5473E4482020C7F473D466CBB34B0@AM0PR04MB5473.eurprd04.prod.outlook.com>

Thanks I am now in personal communication with Frank anyway!

Dan

-----Original Message-----
From: peter dalgaard <pdalgd at gmail.com>
Sent: 14 March 2019 10:37
To: Jackson, Daniel <daniel.jackson1 at astrazeneca.com>
Cc: r-help at r-project.org
Subject: Re: [R] Rank ANCOVA

1. This is the R-help mailing list, not "Frank and Dennis"
2. You seem to be referring to two posts from July 2010 by Frank Harrell and Dennis Fisher.

-pd

> On 13 Mar 2019, at 12:06 , Jackson, Daniel <daniel.jackson1 at astrazeneca.com> wrote:
>
> Hi Frank and Dennis
>
> I am in a similar situation but I would prefer to use a proportional odds model. 2 questions.
>
>
>  1.  Has rank ancova been implemented in R now, despite its short comings?
>  2.  Where has it been shown to yield unreliable analyses? I would like this evidence (which I believe but I would like to convince others!).
>
> Thanks, Dan
> ________________________________
>
> AstraZeneca UK Limited is a company incorporated in
> Engl...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com









________________________________


AstraZeneca UK Limited is a company incorporated in England and Wales with registered number:03674842 and its registered office at 1 Francis Crick Avenue, Cambridge Biomedical Campus, Cambridge, CB2 0AA.

This e-mail and its attachments are intended for the above named recipient only and may contain confidential and privileged information. If they have come to you in error, you must not copy or show them to anyone; instead, please reply to this e-mail, highlighting the error to the sender and then immediately delete the message. For information about how AstraZeneca UK Limited and its affiliates may process information, personal data and monitor communications, please see our privacy notice at www.astrazeneca.com<https://www.astrazeneca.com>


From juiie@@i@croix m@iii@g oii stude@t@uiiege@be  Thu Mar 14 15:44:07 2019
From: juiie@@i@croix m@iii@g oii stude@t@uiiege@be (juiie@@i@croix m@iii@g oii stude@t@uiiege@be)
Date: Thu, 14 Mar 2019 15:44:07 +0100 (CET)
Subject: [R] Unobtainable source code of specific functions
Message-ID: <2060206265.62847167.1552574647288.JavaMail.zimbra@student.uliege.be>

Dear all,

I currently work on a translation of a cost surface function from R to Python for my dissertation at university. In this context, I have to understand the "AccCost" method (https://rdrr.io/cran/gdistance/src/R/accCost.R). This method calls the function "shortest.paths" among other functions. This is precisely the function that computes the cost surface so I found the source code with "getAnywhere(shortest.paths())", but my problem is that this code calls other functions that cannot be accessed with getAnywhere or on the web... These functions are "C_R_igraph_finalizer", "C_R_igraph_get_graph_id" and "C_R_igraph_shortest_paths". My question is therefore the following, how or where could I find these source code ? (Note : I have the package Igraph installed)

Thanks a lot for the help,

Julien Lacroix


From @eb@@t|en@b|hore| @end|ng |rom cogn|gencorp@com  Thu Mar 14 19:53:12 2019
From: @eb@@t|en@b|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Thu, 14 Mar 2019 14:53:12 -0400 (EDT)
Subject: [R] How to list recursive package dependency prior to
 installation/upgrade of a package
Message-ID: <637332606.1077018.1552589592526.JavaMail.zimbra@cognigencorp.com>

Hi

Is there an elegant way to recursive list all dependencies of a package prior to its installation or upgrade?

I am particularly interested in finding which of the packages currently installed in my test/production environment would require an upgrade prior to actual installation/upgrade of a package.

Can the packrat package help with this?

Thank you


From ro@||n@ump @end|ng |rom gm@||@com  Thu Mar 14 20:06:28 2019
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Fri, 15 Mar 2019 03:06:28 +0800
Subject: [R] Extract data of special character
Message-ID: <CANTvJZKN1ce+SY9saGO1KW0EdyhSJzA8Ow8xy3pcfNgX1U5UvQ@mail.gmail.com>

Hi r-users,

I have these data and I would like to count (frequency) how many of
ATTRIBUTE related to TRAITS.

> dput(dd)
structure(list(ID = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L), .Label = c("AA11063",
"AA11127", "AA15194", "CB13086", "EA15058"), class = "factor"),
    NAME = structure(c(5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 4L, 4L, 4L, 4L, 4L,
    4L, 4L, 4L, 4L, 4L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("FIONA ANAK
GASING",
    "JAMES WONG NGUONG DEK", "KOH BEE FONG", "NUR EZREEN BINTI JASNI",
    "SIOW SIN YANG"), class = "factor"), FACULTY = structure(c(1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
    1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 3L,
    3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 2L,
    2L, 2L, 2L, 2L), .Label = c("FKASA", "FKEE", "FSKKP"), class =
"factor"),
    ATTRIBUTE = structure(c(7L, 9L, 1L, 5L, 6L, 4L, 3L, 2L, 8L,
    10L, 7L, 9L, 1L, 5L, 6L, 4L, 3L, 2L, 8L, 10L, 7L, 9L, 1L,
    5L, 6L, 4L, 3L, 2L, 8L, 10L, 7L, 9L, 1L, 5L, 6L, 4L, 3L,
    2L, 8L, 10L, 7L, 9L, 1L, 5L, 6L, 4L, 3L, 2L, 8L, 10L), .Label =
c("COMMUNICATION",
    "CREATIVITY", "ENTREPRENEURIAL SKILLS", "INFORMATION MANAGEMENT AND
LIFELONG LEARNING",
    "LEADERSHIP AND TEAMWORK SKILLS", "PROBLEM SOLVING", "SOCIAL SKILLS",
    "UNITY AND PATRIOTISM", "VALUES, ATTITUDES AND PROFESSIONALISM",
    "VOLUNTEERISM"), class = "factor"), POINT = structure(c(3L,
    20L, 21L, 3L, 12L, 25L, 19L, 12L, 11L, 21L, 11L, 11L, 26L,
    7L, 5L, 16L, 12L, 4L, 16L, 27L, 4L, 16L, 12L, 8L, 4L, 4L,
    19L, 19L, 25L, 15L, 6L, 1L, 14L, 9L, 14L, 4L, 15L, 22L, 23L,
    19L, 4L, 2L, 17L, 10L, 18L, 16L, 12L, 24L, 13L, 4L), .Label =
c("1,030",
    "1,160", "10", "120", "140", "2,100", "2,140", "2,380", "2,390",
    "2,440", "20", "240", "320", "340", "360", "40", "400", "440",
    "480", "520", "60", "600", "700", "720", "80", "810", "840"
    ), class = "factor"), SCORE = c(2L, 35L, 6L, 1L, 48L, 16L,
    96L, 24L, 2L, 6L, 4L, 1L, 81L, 100L, 28L, 8L, 48L, 12L, 4L,
    84L, 24L, 3L, 24L, 100L, 24L, 24L, 96L, 48L, 8L, 36L, 100L,
    69L, 34L, 100L, 68L, 24L, 72L, 60L, 70L, 48L, 24L, 77L, 40L,
    100L, 88L, 8L, 48L, 72L, 32L, 12L), TRAIT = structure(c(5L,
    5L, 5L, 3L, 4L, 5L, 2L, 5L, 5L, 5L, 5L, 5L, 2L, 2L, 5L, 5L,
    4L, 5L, 5L, 2L, 5L, 5L, 5L, 2L, 5L, 5L, 2L, 4L, 5L, 5L, 2L,
    6L, 5L, 2L, 6L, 5L, 1L, 6L, 1L, 4L, 5L, 1L, 4L, 2L, 2L, 5L,
    4L, 1L, 5L, 5L), .Label = c("ACHIEVER", "CONQUEROR", "INTROVERT",
    "NOVICE", "SEEKER", "SURVIVOR"), class = "factor")), class =
"data.frame", row.names = c(NA,-50L))

Thank you so much for any help given.
-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Mar 14 20:07:39 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 14 Mar 2019 22:07:39 +0300
Subject: [R] Unobtainable source code of specific functions
In-Reply-To: <2060206265.62847167.1552574647288.JavaMail.zimbra@student.uliege.be>
References: <2060206265.62847167.1552574647288.JavaMail.zimbra@student.uliege.be>
Message-ID: <20190314220739.696240ec@Tarkus>

On Thu, 14 Mar 2019 15:44:07 +0100 (CET)
julien.lacroix at student.uliege.be wrote:

> My question is therefore the following, how or where could I find
> these source code ? (Note : I have the package Igraph installed)

Functions wrapped in .Call() are implemented in a compiled language,
with corresponding sources in the src/ directory of a package.

For example, the call to C_R_igraph_shortest_paths seems to end up in
https://github.com/igraph/igraph/blob/6faf5610b8ab0175466e93a8a1d1981987976a38/src/structural_properties.c#L460

-- 
Best regards,
Ivan


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Mar 14 20:27:12 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 14 Mar 2019 22:27:12 +0300
Subject: [R] Extract data of special character
In-Reply-To: <CANTvJZKN1ce+SY9saGO1KW0EdyhSJzA8Ow8xy3pcfNgX1U5UvQ@mail.gmail.com>
References: <CANTvJZKN1ce+SY9saGO1KW0EdyhSJzA8Ow8xy3pcfNgX1U5UvQ@mail.gmail.com>
Message-ID: <20190314222712.08f63571@Tarkus>

On Fri, 15 Mar 2019 03:06:28 +0800
roslinazairimah zakaria <roslinaump at gmail.com> wrote:

> how many of ATTRIBUTE related to TRAITS.

The table() function can be used to count occurrences of each
combination of factor levels. Does extracting the two columns by
dd[,c('ATTRIBUTE','TRAIT')] and passing the result to table() help?

-- 
Best regards,
Ivan


From wdun|@p @end|ng |rom t|bco@com  Thu Mar 14 20:50:58 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 14 Mar 2019 12:50:58 -0700
Subject: [R] How to list recursive package dependency prior to
 installation/upgrade of a package
In-Reply-To: <637332606.1077018.1552589592526.JavaMail.zimbra@cognigencorp.com>
References: <637332606.1077018.1552589592526.JavaMail.zimbra@cognigencorp.com>
Message-ID: <CAF8bMcbVfiBp0tHB4rg0DG67dUZnrSa2D+pGG2mNQvWibx2izA@mail.gmail.com>

> tools::package_dependencies("lme4")
$lme4
 [1] "Matrix"    "methods"   "stats"     "graphics"  "grid"      "splines"
 [7] "utils"     "parallel"  "MASS"      "lattice"   "boot"      "nlme"
[13] "minqa"     "nloptr"    "Rcpp"      "RcppEigen"

> tools::package_dependencies("lme4", recursive=TRUE)
$lme4
 [1] "Matrix"    "methods"   "stats"     "graphics"  "grid"      "splines"
 [7] "utils"     "parallel"  "MASS"      "lattice"   "boot"      "nlme"
[13] "minqa"     "nloptr"    "Rcpp"      "RcppEigen" "grDevices"

Use reverse=TRUE to list packages that depend on the given package.
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Mar 14, 2019 at 12:09 PM Sebastien Bihorel <
sebastien.bihorel at cognigencorp.com> wrote:

> Hi
>
> Is there an elegant way to recursive list all dependencies of a package
> prior to its installation or upgrade?
>
> I am particularly interested in finding which of the packages currently
> installed in my test/production environment would require an upgrade prior
> to actual installation/upgrade of a package.
>
> Can the packrat package help with this?
>
> Thank you
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @eb@@t|en@b|hore| @end|ng |rom cogn|gencorp@com  Thu Mar 14 21:03:56 2019
From: @eb@@t|en@b|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Thu, 14 Mar 2019 16:03:56 -0400 (EDT)
Subject: [R] How to list recursive package dependency prior to
 installation/upgrade of a package
In-Reply-To: <CAF8bMcbVfiBp0tHB4rg0DG67dUZnrSa2D+pGG2mNQvWibx2izA@mail.gmail.com>
References: <637332606.1077018.1552589592526.JavaMail.zimbra@cognigencorp.com>
 <CAF8bMcbVfiBp0tHB4rg0DG67dUZnrSa2D+pGG2mNQvWibx2izA@mail.gmail.com>
Message-ID: <218245296.1081806.1552593836776.JavaMail.zimbra@cognigencorp.com>

That is great! 

Is there a way to know version required in the dependent packages? 


From: "William Dunlap" <wdunlap at tibco.com> 
To: "Sebastien Bihorel" <sebastien.bihorel at cognigencorp.com> 
Cc: r-help at r-project.org 
Sent: Thursday, March 14, 2019 3:50:58 PM 
Subject: Re: [R] How to list recursive package dependency prior to installation/upgrade of a package 

> tools::package_dependencies("lme4") 
$lme4 
[1] "Matrix" "methods" "stats" "graphics" "grid" "splines" 
[7] "utils" "parallel" "MASS" "lattice" "boot" "nlme" 
[13] "minqa" "nloptr" "Rcpp" "RcppEigen" 

> tools::package_dependencies("lme4", recursive=TRUE) 
$lme4 
[1] "Matrix" "methods" "stats" "graphics" "grid" "splines" 
[7] "utils" "parallel" "MASS" "lattice" "boot" "nlme" 
[13] "minqa" "nloptr" "Rcpp" "RcppEigen" "grDevices" 

Use reverse=TRUE to list packages that depend on the given package. 
Bill Dunlap 
TIBCO Software 
wdunlap [ http://tibco.com/ | tibco.com ] 


On Thu, Mar 14, 2019 at 12:09 PM Sebastien Bihorel < [ mailto:sebastien.bihorel at cognigencorp.com | sebastien.bihorel at cognigencorp.com ] > wrote: 


Hi 

Is there an elegant way to recursive list all dependencies of a package prior to its installation or upgrade? 

I am particularly interested in finding which of the packages currently installed in my test/production environment would require an upgrade prior to actual installation/upgrade of a package. 

Can the packrat package help with this? 

Thank you 

______________________________________________ 
[ mailto:R-help at r-project.org | R-help at r-project.org ] mailing list -- To UNSUBSCRIBE and more, see 
[ https://stat.ethz.ch/mailman/listinfo/r-help | https://stat.ethz.ch/mailman/listinfo/r-help ] 
PLEASE do read the posting guide [ http://www.r-project.org/posting-guide.html | http://www.R-project.org/posting-guide.html ] 
and provide commented, minimal, self-contained, reproducible code. 





	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Thu Mar 14 21:16:36 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 14 Mar 2019 13:16:36 -0700
Subject: [R] How to list recursive package dependency prior to
 installation/upgrade of a package
In-Reply-To: <218245296.1081806.1552593836776.JavaMail.zimbra@cognigencorp.com>
References: <637332606.1077018.1552589592526.JavaMail.zimbra@cognigencorp.com>
 <CAF8bMcbVfiBp0tHB4rg0DG67dUZnrSa2D+pGG2mNQvWibx2izA@mail.gmail.com>
 <218245296.1081806.1552593836776.JavaMail.zimbra@cognigencorp.com>
Message-ID: <CAF8bMcadPuvCA_w2dB9Dy78cWSiqLMfixBT10MJL7Pn+7k=8SQ@mail.gmail.com>

available.packages() and installed.packages() map package names to version
and a lot of other things.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Mar 14, 2019 at 1:03 PM Sebastien Bihorel <
sebastien.bihorel at cognigencorp.com> wrote:

> That is great!
>
> Is there a way to know version required in the dependent packages?
>
> ------------------------------
> *From: *"William Dunlap" <wdunlap at tibco.com>
> *To: *"Sebastien Bihorel" <sebastien.bihorel at cognigencorp.com>
> *Cc: *r-help at r-project.org
> *Sent: *Thursday, March 14, 2019 3:50:58 PM
> *Subject: *Re: [R] How to list recursive package dependency prior to
> installation/upgrade of a package
>
> > tools::package_dependencies("lme4")
> $lme4
>  [1] "Matrix"    "methods"   "stats"     "graphics"  "grid"
> "splines"
>  [7] "utils"     "parallel"  "MASS"      "lattice"   "boot"      "nlme"
>
> [13] "minqa"     "nloptr"    "Rcpp"      "RcppEigen"
>
> > tools::package_dependencies("lme4", recursive=TRUE)
> $lme4
>  [1] "Matrix"    "methods"   "stats"     "graphics"  "grid"
> "splines"
>  [7] "utils"     "parallel"  "MASS"      "lattice"   "boot"      "nlme"
>
> [13] "minqa"     "nloptr"    "Rcpp"      "RcppEigen" "grDevices"
>
> Use reverse=TRUE to list packages that depend on the given package.
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Thu, Mar 14, 2019 at 12:09 PM Sebastien Bihorel <
> sebastien.bihorel at cognigencorp.com> wrote:
>
>> Hi
>>
>> Is there an elegant way to recursive list all dependencies of a package
>> prior to its installation or upgrade?
>>
>> I am particularly interested in finding which of the packages currently
>> installed in my test/production environment would require an upgrade prior
>> to actual installation/upgrade of a package.
>>
>> Can the packrat package help with this?
>>
>> Thank you
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Mar 14 22:51:53 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 14 Mar 2019 21:51:53 +0000
Subject: [R] Extract data of special character
In-Reply-To: <20190314222712.08f63571@Tarkus>
References: <CANTvJZKN1ce+SY9saGO1KW0EdyhSJzA8Ow8xy3pcfNgX1U5UvQ@mail.gmail.com>
 <20190314222712.08f63571@Tarkus>
Message-ID: <618625b8-5f9b-51c4-dc77-2657141950a8@sapo.pt>

Hello,

Or more simple,

xtabs( ~ ATTRIBUTE + TRAIT, dd)


Hope this helps,

Rui Barradas

?s 19:27 de 14/03/2019, Ivan Krylov escreveu:
> On Fri, 15 Mar 2019 03:06:28 +0800
> roslinazairimah zakaria <roslinaump at gmail.com> wrote:
> 
>> how many of ATTRIBUTE related to TRAITS.
> 
> The table() function can be used to count occurrences of each
> combination of factor levels. Does extracting the two columns by
> dd[,c('ATTRIBUTE','TRAIT')] and passing the result to table() help?
>


From drj|m|emon @end|ng |rom gm@||@com  Thu Mar 14 23:36:09 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 15 Mar 2019 09:36:09 +1100
Subject: [R] Display common color scale on multiple scatter3D plots
In-Reply-To: <CAMk+s2RoveSZgRJXbgAFb08AGp7pHcAf_VXxwLV9VBg866kP-w@mail.gmail.com>
References: <CAMk+s2RoveSZgRJXbgAFb08AGp7pHcAf_VXxwLV9VBg866kP-w@mail.gmail.com>
Message-ID: <CA+8X3fV35MjUwxDOyBckncGS6MFFgJ4dqUP+0dPYc5BQeq89eQ@mail.gmail.com>

Hi Luigi,
Upon careful reading of the help page, you can do it with scatter3D:

scatter3D(X, Y, Z, col.var = Z, pch = 16, cex = 2,clim=c(0.5,3))
scatter3D(X, Y, K, col.var = K, pch = 16, cex = 2,clim=c(0.5,3))

Jim

On Thu, Mar 14, 2019 at 9:32 PM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>
> Dear all,
> I am trying to display multivariate data using the library plot3D. I
> have 3 variables that go on the axis and a fourth that I would like to
> display as a color shade. However, the scale differs between plots
> because the data I am using varies.
> Would be possible to maintain a single scale so that the different
> points will have the same shade on different plots? This would be like
> giving the same axis scale on different plots.
>
> For example, I got the following:
> >>>>
> set.seed(50)
> x = runif(10, 6, 18)
> y = runif(10,  5,  7)
> z = runif(10, 0.7, 3.14)
> k = runif(10, 0.5, 1.2)
> w = runif(10, min(z, k), max(z,k))
> df = my.data <- data.frame(x, y, z, k, w)
>
> library("plot3D")
> X = df$x
> Y = df$y
> Z = df$z
> K = df$k
> W = df$w
> scatter3D(X, Y, Z, col.var = Z, pch = 16, cex = 2)
> scatter3D(X, Y, K, col.var = K, pch = 16, cex = 2)
> <<<
>
> In these plots the scale varies (0.6-0.9 in the first plot, 1.5-2.5 in
> the second); I tried by creating a common scale with the variable W
> which ranges from the min and max of the fourth levels represented by
> the variables Z and K, but the result is the same:
> scatter3D(X, Y, Z, col.var = W, pch = 16, cex = 2)
> scatter3D(X, Y, K, col.var = W, pch = 16, cex = 2)
>
> Essentially, I would like the variables Z and K to be shaded according
> to a common scale so that I could evaluate the shades of the two plots
> directly.
> Would that be possible?
> Thank you.
>
> --
> Best regards,
> Luigi
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ro@||n@ump @end|ng |rom gm@||@com  Fri Mar 15 00:33:50 2019
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Fri, 15 Mar 2019 07:33:50 +0800
Subject: [R] Extract data of special character
In-Reply-To: <618625b8-5f9b-51c4-dc77-2657141950a8@sapo.pt>
References: <CANTvJZKN1ce+SY9saGO1KW0EdyhSJzA8Ow8xy3pcfNgX1U5UvQ@mail.gmail.com>
 <20190314222712.08f63571@Tarkus>
 <618625b8-5f9b-51c4-dc77-2657141950a8@sapo.pt>
Message-ID: <CANTvJZKiT_6GruXeTUEmEFDA1YC-+6+GC4+v7gxHHBT1HKHJ7g@mail.gmail.com>

Yes, it does.

table(dd$ATTRIBUTE, dd$TRAIT) #or
xtabs( ~ ATTRIBUTE + TRAIT, dd)

Thank you so much Rui and Ivan.

On Fri, Mar 15, 2019 at 5:51 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Or more simple,
>
> xtabs( ~ ATTRIBUTE + TRAIT, dd)
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 19:27 de 14/03/2019, Ivan Krylov escreveu:
> > On Fri, 15 Mar 2019 03:06:28 +0800
> > roslinazairimah zakaria <roslinaump at gmail.com> wrote:
> >
> >> how many of ATTRIBUTE related to TRAITS.
> >
> > The table() function can be used to count occurrences of each
> > combination of factor levels. Does extracting the two columns by
> > dd[,c('ATTRIBUTE','TRAIT')] and passing the result to table() help?
> >
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From ro@||n@ump @end|ng |rom gm@||@com  Fri Mar 15 01:06:52 2019
From: ro@||n@ump @end|ng |rom gm@||@com (roslinazairimah zakaria)
Date: Fri, 15 Mar 2019 08:06:52 +0800
Subject: [R] Extract data of special character
In-Reply-To: <618625b8-5f9b-51c4-dc77-2657141950a8@sapo.pt>
References: <CANTvJZKN1ce+SY9saGO1KW0EdyhSJzA8Ow8xy3pcfNgX1U5UvQ@mail.gmail.com>
 <20190314222712.08f63571@Tarkus>
 <618625b8-5f9b-51c4-dc77-2657141950a8@sapo.pt>
Message-ID: <CANTvJZJmY1Oms0-BehmE-jAh25EcjFuQ8_MR728ikM4OsFP0rg@mail.gmail.com>

Hi Rui and Ivan,
Yes both works well.

table(dd$ATTRIBUTE, dd$TRAIT) #or
xtabs( ~ ATTRIBUTE + TRAIT, dd)

I have another question,I also want to extract all the PCT_SCORE to draw
box plot. I have tried to draw box of PCT_SCORE for all attribute scores.

boxplot(dd$PCT_SCORE, ylim=c(0,100),
        main="Cohort 2015:Distribution of Score % of all attributes",
        xlab="Score(%) of all attribute",horizontal=TRUE,
cex.axis=0.8,cex.main=0.8, cex.lab=0.8, col="lightblue" )

However, I want to draw boxplot for each individual score of the
attributes. That is to extract data of the sub-attribute eg. COMMUNICATION,
CREATIVITY, ENTREPRENEURIAL SKILLS and etc.

Thank you.



On Fri, Mar 15, 2019 at 5:51 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Or more simple,
>
> xtabs( ~ ATTRIBUTE + TRAIT, dd)
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 19:27 de 14/03/2019, Ivan Krylov escreveu:
> > On Fri, 15 Mar 2019 03:06:28 +0800
> > roslinazairimah zakaria <roslinaump at gmail.com> wrote:
> >
> >> how many of ATTRIBUTE related to TRAITS.
> >
> > The table() function can be used to count occurrences of each
> > combination of factor levels. Does extracting the two columns by
> > dd[,c('ATTRIBUTE','TRAIT')] and passing the result to table() help?
> >
>


-- 
*Roslinazairimah Zakaria*
*Tel: +609-5492370; Fax. No.+609-5492766*

*Email: roslinazairimah at ump.edu.my <roslinazairimah at ump.edu.my>;
roslinaump at gmail.com <roslinaump at gmail.com>*
Faculty of Industrial Sciences & Technology
University Malaysia Pahang
Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri Mar 15 06:49:42 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 15 Mar 2019 16:49:42 +1100
Subject: [R] Sorting vector based on pairs of comparisons
In-Reply-To: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
Message-ID: <CA+8X3fVVMJCZEvjGOfAXWxgjPBHa_K+VUM3ik1921xsnA-V-dg@mail.gmail.com>

Hi Pedro,
This looks too simple to me, but it seems to work:

swap<-function(x,i1,i2) {
 tmp<-x[i1]
 x[i1]<-x[i2]
 x[i2]<-tmp
 return(x)
}
mpo<-function(x) {
 L<-unique(as.vector(x))
 for(i in 1:nrow(x)) {
  i1<-which(L==x[i,1])
  i2<-which(L==x[i,2])
  if(i2<i1) L<-swap(L,i1,i2)
 }
 return(L)
}
mpo(matComp)

Jim

On Thu, Mar 14, 2019 at 10:30 PM Pedro Conte de Barros <pbarros at ualg.pt> wrote:
>
> Dear All,
>
> This should be a quite established algorithm, but I have been searching
> for a couple days already without finding any satisfactory solution.
>
> I have a matrix defining pairs of Smaller-Larger arbitrary character
> values, like below
>
> Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")
>
> Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF"
>
> matComp <- cbind(Smaller, Larger)
>
> so that matComp looks like this
>
>       Smaller Larger
> [1,] "ASD"   "SDR"
> [2,] "DFE"   "EDF"
> [3,] "ASD"   "KLM"
> [4,] "SDR"   "KLM"
> [5,] "EDF"   "SDR"
> [6,] "ASD"   "EDF"
>
> This matrix establishes six pairs of "larger than" relationships that
> can be used to sort the unique values in the matrix,
>
>  > unique(as.vector(matComp))
> [1] "ASD" "DFE" "SDR" "EDF" "KLM"
>
> Specifically, I would like to get this:
>
> sorted <- c("ASD", "DFE", "EDF", "SDR", "KLM")
>
> or, equally valid (my matrix does not have the full information):
>
> sorted <- c("DFE", "ASD", "EDF", "SDR", "KLM")
>
> Preferably, I would get the different combinations of the unique values
> that satisfy the "larger than" conditions in the matrix...
>
>
> I am sure this is a trivial problem, but I could not find any algorithm
> to solve it.
>
> Any help would be highly appreciated
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Mar 15 08:53:14 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 15 Mar 2019 00:53:14 -0700
Subject: [R] Sorting vector based on pairs of comparisons
In-Reply-To: <CA+8X3fVVMJCZEvjGOfAXWxgjPBHa_K+VUM3ik1921xsnA-V-dg@mail.gmail.com>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
 <CA+8X3fVVMJCZEvjGOfAXWxgjPBHa_K+VUM3ik1921xsnA-V-dg@mail.gmail.com>
Message-ID: <CAGxFJbSN_uKU-cSaMsQ+QavaQv6DUzxJ=UGGfA470hQ_XrOo-A@mail.gmail.com>

If I understand correctly, the answer is a topological sort.

Here is an explanation

https://davidurbina.blog/on-partial-order-total-order-and-the-topological-sort/

This was found by a simple web search on
"Convert partial ordering to total ordering"
Btw.  Please use search engines before posting here.

Bert


On Thu, Mar 14, 2019, 10:50 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Pedro,
> This looks too simple to me, but it seems to work:
>
> swap<-function(x,i1,i2) {
>  tmp<-x[i1]
>  x[i1]<-x[i2]
>  x[i2]<-tmp
>  return(x)
> }
> mpo<-function(x) {
>  L<-unique(as.vector(x))
>  for(i in 1:nrow(x)) {
>   i1<-which(L==x[i,1])
>   i2<-which(L==x[i,2])
>   if(i2<i1) L<-swap(L,i1,i2)
>  }
>  return(L)
> }
> mpo(matComp)
>
> Jim
>
> On Thu, Mar 14, 2019 at 10:30 PM Pedro Conte de Barros <pbarros at ualg.pt>
> wrote:
> >
> > Dear All,
> >
> > This should be a quite established algorithm, but I have been searching
> > for a couple days already without finding any satisfactory solution.
> >
> > I have a matrix defining pairs of Smaller-Larger arbitrary character
> > values, like below
> >
> > Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")
> >
> > Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF"
> >
> > matComp <- cbind(Smaller, Larger)
> >
> > so that matComp looks like this
> >
> >       Smaller Larger
> > [1,] "ASD"   "SDR"
> > [2,] "DFE"   "EDF"
> > [3,] "ASD"   "KLM"
> > [4,] "SDR"   "KLM"
> > [5,] "EDF"   "SDR"
> > [6,] "ASD"   "EDF"
> >
> > This matrix establishes six pairs of "larger than" relationships that
> > can be used to sort the unique values in the matrix,
> >
> >  > unique(as.vector(matComp))
> > [1] "ASD" "DFE" "SDR" "EDF" "KLM"
> >
> > Specifically, I would like to get this:
> >
> > sorted <- c("ASD", "DFE", "EDF", "SDR", "KLM")
> >
> > or, equally valid (my matrix does not have the full information):
> >
> > sorted <- c("DFE", "ASD", "EDF", "SDR", "KLM")
> >
> > Preferably, I would get the different combinations of the unique values
> > that satisfy the "larger than" conditions in the matrix...
> >
> >
> > I am sure this is a trivial problem, but I could not find any algorithm
> > to solve it.
> >
> > Any help would be highly appreciated
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pb@rro@ @end|ng |rom u@|g@pt  Fri Mar 15 09:46:21 2019
From: pb@rro@ @end|ng |rom u@|g@pt (Pedro Conte de Barros)
Date: Fri, 15 Mar 2019 08:46:21 +0000
Subject: [R] Sorting vector based on pairs of comparisons
In-Reply-To: <CAGxFJbSN_uKU-cSaMsQ+QavaQv6DUzxJ=UGGfA470hQ_XrOo-A@mail.gmail.com>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
 <CA+8X3fVVMJCZEvjGOfAXWxgjPBHa_K+VUM3ik1921xsnA-V-dg@mail.gmail.com>
 <CAGxFJbSN_uKU-cSaMsQ+QavaQv6DUzxJ=UGGfA470hQ_XrOo-A@mail.gmail.com>
Message-ID: <f2929172-64e7-40b0-2854-4622571d8450@ualg.pt>

Thanks Bert. Excellent reference, I learned a lot from it!

Just a note: I did use search engines for at least 2 days before posting. BUT as often happens, I did not use the right keywords. I tried several variants of "Convert ordered pairs to sorted", "Sort vector on paired comparisons" and about anything else I could think of round "paired comparisons". But the problem was, I did not know what was the correct terminology to use for this search, that was why I had to ask.

This is where humans excel versus computer search engines - find things based on imprecise specifications- and thanks again for pointing me in the right direction.

Best,

Pedro

On 15/03/2019 08.53, Bert Gunter wrote:
If I understand correctly, the answer is a topological sort.

Here is an explanation

https://davidurbina.blog/on-partial-order-total-order-and-the-topological-sort/

This was found by a simple web search on
"Convert partial ordering to total ordering"
Btw.  Please use search engines before posting here.

Bert


On Thu, Mar 14, 2019, 10:50 PM Jim Lemon <drjimlemon at gmail.com<mailto:drjimlemon at gmail.com>> wrote:
Hi Pedro,
This looks too simple to me, but it seems to work:

swap<-function(x,i1,i2) {
 tmp<-x[i1]
 x[i1]<-x[i2]
 x[i2]<-tmp
 return(x)
}
mpo<-function(x) {
 L<-unique(as.vector(x))
 for(i in 1:nrow(x)) {
  i1<-which(L==x[i,1])
  i2<-which(L==x[i,2])
  if(i2<i1) L<-swap(L,i1,i2)
 }
 return(L)
}
mpo(matComp)

Jim

On Thu, Mar 14, 2019 at 10:30 PM Pedro Conte de Barros <pbarros at ualg.pt<mailto:pbarros at ualg.pt>> wrote:
>
> Dear All,
>
> This should be a quite established algorithm, but I have been searching
> for a couple days already without finding any satisfactory solution.
>
> I have a matrix defining pairs of Smaller-Larger arbitrary character
> values, like below
>
> Smaller <- c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")
>
> Larger <- c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF"
>
> matComp <- cbind(Smaller, Larger)
>
> so that matComp looks like this
>
>       Smaller Larger
> [1,] "ASD"   "SDR"
> [2,] "DFE"   "EDF"
> [3,] "ASD"   "KLM"
> [4,] "SDR"   "KLM"
> [5,] "EDF"   "SDR"
> [6,] "ASD"   "EDF"
>
> This matrix establishes six pairs of "larger than" relationships that
> can be used to sort the unique values in the matrix,
>
>  > unique(as.vector(matComp))
> [1] "ASD" "DFE" "SDR" "EDF" "KLM"
>
> Specifically, I would like to get this:
>
> sorted <- c("ASD", "DFE", "EDF", "SDR", "KLM")
>
> or, equally valid (my matrix does not have the full information):
>
> sorted <- c("DFE", "ASD", "EDF", "SDR", "KLM")
>
> Preferably, I would get the different combinations of the unique values
> that satisfy the "larger than" conditions in the matrix...
>
>
> I am sure this is a trivial problem, but I could not find any algorithm
> to solve it.
>
> Any help would be highly appreciated
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From |r@nk|eboytje @end|ng |rom hotm@||@com  Fri Mar 15 10:09:17 2019
From: |r@nk|eboytje @end|ng |rom hotm@||@com (Frank van Berkum)
Date: Fri, 15 Mar 2019 09:09:17 +0000
Subject: [R] [mgcv] Memory issues with bam() on computer cluster
In-Reply-To: <mailman.353852.1.1551783602.4534.r-help@r-project.org>
References: <mailman.353852.1.1551783602.4534.r-help@r-project.org>
Message-ID: <AM0PR0302MB3458D8EF91DD48219756AC27BC440@AM0PR0302MB3458.eurprd03.prod.outlook.com>

Dear Community,

In our current research we are trying to fit Generalized Additive Models to a large dataset. We are using the package mgcv in R.

Our dataset contains about 22 million records with less than 20 risk factors for each observation, so in our case n>>p. The dataset covers the period 2006 until 2011, and we analyse both the complete dataset and datasets in which we leave out a single year. The latter part is done to analyse robustness of the results. We understand k-fold cross validation may seem more appropriate, but out approach is closer to what is done in practice (how will one additional year of information affect your estimates?).

We use the function bam as advocated in Wood et al. (2017), and we apply the following options: bam(?, discrete=TRUE, chunk.size=10000, gc.level=1). We run these analyses on a computer cluster (see https://userinfo.surfsara.nl/systems/lisa/description for details), and the job is allocated to a node within the computer cluster. A node has at least 16 cores and 64Gb memory.

We had expected 64Gb of memory to be sufficient for these analyses, especially since the bam function is built specifically for large datasets. However, when applying this function to the different datasets described above with different regression specifications (different risk factors included in the linear predictor), we sometimes obtain errors of the following form.

Error in XWyd(G$Xd, w, z, G$kd, G$ks, G$ts, G$dt, G$v, G$qc, G$drop, ar.stop,  :

  'Calloc' could not allocate memory (22624897 of 8 bytes)

Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> XWyd

Execution halted

Warning message:

system call failed: Cannot allocate memory

Error in Xbd(G$Xd, coef, G$kd, G$ks, G$ts, G$dt, G$v, G$qc, G$drop) :

  'Calloc' could not allocate memory (18590685 of 8 bytes)

Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> Xbd

Execution halted

Warning message:

system call failed: Cannot allocate memory

Error: cannot allocate vector of size 1.7 Gb

Timing stopped at: 2 0.556 4.831

Error in system.time(oo <- .C(C_XWXd0, XWX = as.double(rep(0, (pt + nt)^2)),  :

  'Calloc' could not allocate memory (55315650 of 24 bytes)

Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> XWXd -> system.time -> .C

Timing stopped at: 1.056 1.396 2.459

Execution halted

Warning message:

system call failed: Cannot allocate memory

The errors seem to arise at different stages in the optimization process. We have analysed whether these errors disappear if different settings are used (different chunk.size, different gc.level), but this does not resolve our problem. Also, the errors occur on different datasets when using different settings, and even when using the same settings it is possible that an error that occurred on dataset X in one run it does not necessarily occur on dataset X in a different run. When using the discrete=TRUE option, optimization can be parallelized, but we have chosen to not employ this feature to ensure memory does not have to be shared between parallel processes.

Naturally I cannot share our dataset with you which makes the problem difficult to analyse. However, based on your collective knowledge, could you pinpoint us to where the problem may occur? Is it something within the C-code used within the package (as the last error seems to indicate), or is it related to the computer cluster?

Any help or insights is much appreciated.

Kind regards,

Frank

	[[alternative HTML version deleted]]


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Fri Mar 15 11:26:25 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Fri, 15 Mar 2019 11:26:25 +0100
Subject: [R] Display common color scale on multiple scatter3D plots
In-Reply-To: <CA+8X3fV35MjUwxDOyBckncGS6MFFgJ4dqUP+0dPYc5BQeq89eQ@mail.gmail.com>
References: <CAMk+s2RoveSZgRJXbgAFb08AGp7pHcAf_VXxwLV9VBg866kP-w@mail.gmail.com>
 <CA+8X3fV35MjUwxDOyBckncGS6MFFgJ4dqUP+0dPYc5BQeq89eQ@mail.gmail.com>
Message-ID: <CAMk+s2QS4zcF55FsJNYG3hg=FygFrh=N25JGQOeabS1h3VyJpw@mail.gmail.com>

Thank you!
I even bought a manual on this and such feature was not there. Case solved.
Regards,
Luigi

On Thu, Mar 14, 2019 at 11:36 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Luigi,
> Upon careful reading of the help page, you can do it with scatter3D:
>
> scatter3D(X, Y, Z, col.var = Z, pch = 16, cex = 2,clim=c(0.5,3))
> scatter3D(X, Y, K, col.var = K, pch = 16, cex = 2,clim=c(0.5,3))
>
> Jim
>
> On Thu, Mar 14, 2019 at 9:32 PM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
> >
> > Dear all,
> > I am trying to display multivariate data using the library plot3D. I
> > have 3 variables that go on the axis and a fourth that I would like to
> > display as a color shade. However, the scale differs between plots
> > because the data I am using varies.
> > Would be possible to maintain a single scale so that the different
> > points will have the same shade on different plots? This would be like
> > giving the same axis scale on different plots.
> >
> > For example, I got the following:
> > >>>>
> > set.seed(50)
> > x = runif(10, 6, 18)
> > y = runif(10,  5,  7)
> > z = runif(10, 0.7, 3.14)
> > k = runif(10, 0.5, 1.2)
> > w = runif(10, min(z, k), max(z,k))
> > df = my.data <- data.frame(x, y, z, k, w)
> >
> > library("plot3D")
> > X = df$x
> > Y = df$y
> > Z = df$z
> > K = df$k
> > W = df$w
> > scatter3D(X, Y, Z, col.var = Z, pch = 16, cex = 2)
> > scatter3D(X, Y, K, col.var = K, pch = 16, cex = 2)
> > <<<
> >
> > In these plots the scale varies (0.6-0.9 in the first plot, 1.5-2.5 in
> > the second); I tried by creating a common scale with the variable W
> > which ranges from the min and max of the fourth levels represented by
> > the variables Z and K, but the result is the same:
> > scatter3D(X, Y, Z, col.var = W, pch = 16, cex = 2)
> > scatter3D(X, Y, K, col.var = W, pch = 16, cex = 2)
> >
> > Essentially, I would like the variables Z and K to be shaded according
> > to a common scale so that I could evaluate the shades of the two plots
> > directly.
> > Would that be possible?
> > Thank you.
> >
> > --
> > Best regards,
> > Luigi
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.



-- 
Best regards,
Luigi


From drj|m|emon @end|ng |rom gm@||@com  Fri Mar 15 11:39:29 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 15 Mar 2019 21:39:29 +1100
Subject: [R] Sorting vector based on pairs of comparisons
In-Reply-To: <f2929172-64e7-40b0-2854-4622571d8450@ualg.pt>
References: <ce8f0350-f042-94e6-d304-2e475b9e6a3e@ualg.pt>
 <CA+8X3fVVMJCZEvjGOfAXWxgjPBHa_K+VUM3ik1921xsnA-V-dg@mail.gmail.com>
 <CAGxFJbSN_uKU-cSaMsQ+QavaQv6DUzxJ=UGGfA470hQ_XrOo-A@mail.gmail.com>
 <f2929172-64e7-40b0-2854-4622571d8450@ualg.pt>
Message-ID: <CA+8X3fXn62-NBREUJJJQGRF8Koz=qX5zDfzSmHsR9xb7p4uMNg@mail.gmail.com>

Hi Bert,
Good reference and David Urbina's example showed that a simple swap
was position dependent. The reason I pursued this is that it seems
more efficient to sequentially apply the precedence rules to the
arbitrarily sorted elements of the vector than to go through the
directed graph approach. By changing the simple position swap to an
insertion of the out-of-sequence element before its precedence pair,
both examples work correctly and adding precedence rules to the
initial list also produces a correct result. The output of the
examples below is a bit verbose, as I added a printout of the vector
at each step, ending with a logical indicating whether repos (element
reposition) was called.

repos<-function(x,i1,i2) {
 tmp<-x[i2]
 x<-x[-i2]
 if(i1 > 1) return(c(x[1:(i1-1)],tmp,x[i1:length(x)]))
 else return(c(tmp,x))
}
mpo<-function(x) {
 L<-unique(as.vector(x))
 for(i in 1:nrow(x)) {
  i1<-which(L==x[i,1])
  i2<-which(L==x[i,2])
  if(i2<i1) L<-repos(L,i1,i2)
  cat(L,i2<i1,"\n")
 }
 cat("\n")
 return(L)
}
# Pedro's example
Smaller<-c("ASD", "DFE", "ASD", "SDR", "EDF", "ASD")
Larger<-c("SDR", "EDF", "KLM", "KLM", "SDR", "EDF")
matComp<-cbind(Smaller,Larger)
mpo(matComp)
# David Urbina's example
priors<-c("A","B","C","C","D","E","E","F","G")
posts<-c("E","H","A","D","E","B","F","G","H")
dinnerMat<-cbind(priors,posts)
mpo(dinnerMat)
# add the condition that the taquitos must precede the guacamole
dinnerMat<-rbind(dinnerMat,c("G","B"))
mpo(dinnerMat)

To echo David Urbina's disclaimer, I would like to know if I have made
any mistakes.

Jim

> On 15/03/2019 08.53, Bert Gunter wrote:
> If I understand correctly, the answer is a topological sort.
>
> Here is an explanation
>
> https://davidurbina.blog/on-partial-order-total-order-and-the-topological-sort/
>
> This was found by a simple web search on
> "Convert partial ordering to total ordering"
> Btw.  Please use search engines before posting here.
>
> Bert


From kry|ov@r00t @end|ng |rom gm@||@com  Fri Mar 15 11:50:19 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 15 Mar 2019 13:50:19 +0300
Subject: [R] Extract data of special character
In-Reply-To: <CANTvJZJmY1Oms0-BehmE-jAh25EcjFuQ8_MR728ikM4OsFP0rg@mail.gmail.com>
References: <CANTvJZKN1ce+SY9saGO1KW0EdyhSJzA8Ow8xy3pcfNgX1U5UvQ@mail.gmail.com>
 <20190314222712.08f63571@Tarkus>
 <618625b8-5f9b-51c4-dc77-2657141950a8@sapo.pt>
 <CANTvJZJmY1Oms0-BehmE-jAh25EcjFuQ8_MR728ikM4OsFP0rg@mail.gmail.com>
Message-ID: <20190315135019.1810bd0e@trisector>

? Fri, 15 Mar 2019 08:06:52 +0800
roslinazairimah zakaria <roslinaump at gmail.com> ?????:

> I want to draw boxplot for each individual score of the
> attributes.

You mean, a box per every possible ATTRIBUTE value? This is easily
doable with the bwplot() function from library(lattice).

-- 
Best regards,
Ivan


From @@chre|b @end|ng |rom u@|bert@@c@  Fri Mar 15 01:43:28 2019
From: @@chre|b @end|ng |rom u@|bert@@c@ (Stefan Schreiber)
Date: Thu, 14 Mar 2019 18:43:28 -0600
Subject: [R] density vs. mass for discrete probability functions
Message-ID: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>

Dear R users,

While experimenting with the dbinom() function and reading its
documentation (?dbinom) it reads that "dbinom gives the density" but
shouldn't it be called "mass" instead of "density"? I assume that it
has something to do with keeping the function for "density" consistent
across discrete and continuous probability functions - but I am not
sure and was hoping someone could clarify?

Furthermore the help file for dbinom() function references a link
(http://www.herine.net/stat/software/dbinom.html) but it doesn't seem
to land where it should. Maybe this could be updated?

Thank you,
Stefan


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Mar 15 13:02:51 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 15 Mar 2019 12:02:51 +0000
Subject: [R] Extract data of special character
In-Reply-To: <CANTvJZJmY1Oms0-BehmE-jAh25EcjFuQ8_MR728ikM4OsFP0rg@mail.gmail.com>
References: <CANTvJZKN1ce+SY9saGO1KW0EdyhSJzA8Ow8xy3pcfNgX1U5UvQ@mail.gmail.com>
 <20190314222712.08f63571@Tarkus>
 <618625b8-5f9b-51c4-dc77-2657141950a8@sapo.pt>
 <CANTvJZJmY1Oms0-BehmE-jAh25EcjFuQ8_MR728ikM4OsFP0rg@mail.gmail.com>
Message-ID: <31185563-faff-f512-1e8e-5a12f747bf9e@sapo.pt>

Hello,

Something like this?


old_par <- par(mar = par("mar") + c(5, 0, -2, 0))
boxplot(SCORE ~ ATTRIBUTE, dd, cex.axis = 0.6, las = 2)
par(old_par)


Hope this helps,

Rui Barradas

?s 00:06 de 15/03/2019, roslinazairimah zakaria escreveu:
> Hi Rui and Ivan,
> Yes both works well.
> 
> table(dd$ATTRIBUTE, dd$TRAIT) #or
> xtabs( ~ ATTRIBUTE + TRAIT, dd)
> 
> I have another question,I also want to extract all the PCT_SCORE to draw 
> box plot. I have tried to draw box?of PCT_SCORE for all attribute scores.
> 
> boxplot(dd$PCT_SCORE, ylim=c(0,100),
>  ? ? ? ? main="Cohort 2015:Distribution of Score % of all attributes",
>  ? ? ? ? xlab="Score(%) of all attribute",horizontal=TRUE, 
> cex.axis=0.8,cex.main=0.8, cex.lab=0.8, col="lightblue" )
> 
> However, I want to draw boxplot for each individual score of the 
> attributes. That is to extract data of the sub-attribute eg. 
> COMMUNICATION, CREATIVITY, ENTREPRENEURIAL SKILLS and etc.
> 
> Thank you.
> 
> 
> 
> On Fri, Mar 15, 2019 at 5:51 AM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
>     Hello,
> 
>     Or more simple,
> 
>     xtabs( ~ ATTRIBUTE + TRAIT, dd)
> 
> 
>     Hope this helps,
> 
>     Rui Barradas
> 
>     ?s 19:27 de 14/03/2019, Ivan Krylov escreveu:
>      > On Fri, 15 Mar 2019 03:06:28 +0800
>      > roslinazairimah zakaria <roslinaump at gmail.com
>     <mailto:roslinaump at gmail.com>> wrote:
>      >
>      >> how many of ATTRIBUTE related to TRAITS.
>      >
>      > The table() function can be used to count occurrences of each
>      > combination of factor levels. Does extracting the two columns by
>      > dd[,c('ATTRIBUTE','TRAIT')] and passing the result to table() help?
>      >
> 
> 
> 
> -- 
> *Roslinazairimah Zakaria*
> *Tel: +609-5492370; Fax. No.+609-5492766*
> *Email: roslinazairimah at ump.edu.my <mailto:roslinazairimah at ump.edu.my>; 
> roslinaump at gmail.com <mailto:roslinaump at gmail.com>
> *
> Faculty of Industrial Sciences & Technology
> University Malaysia Pahang
> Lebuhraya Tun Razak, 26300 Gambang, Pahang, Malaysia


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Fri Mar 15 13:04:10 2019
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Fri, 15 Mar 2019 07:04:10 -0500
Subject: [R] density vs. mass for discrete probability functions
In-Reply-To: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
References: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
Message-ID: <ea93f7b3-5def-5551-9b16-60b5910d990e@effectivedefense.org>



On 2019-03-14 19:43, Stefan Schreiber wrote:
> Dear R users,
>
> While experimenting with the dbinom() function and reading its
> documentation (?dbinom) it reads that "dbinom gives the density" but
> shouldn't it be called "mass" instead of "density"? I assume that it
> has something to do with keeping the function for "density" consistent
> across discrete and continuous probability functions - but I am not
> sure and was hoping someone could clarify?


 ????? The Wikipedia article on "Probability density function" gives the 
"Formal definition" that, "the density of [a random variable] with 
respect to a reference measure ... is the Radon?Nikodym derivative".


 ????? This sounds bazaar to people who haven't studied 
measure-theoretic probability, but it allows a unified treatment of 
continuous and discrete probabilities and to others that are 
combinations and neither.? The "reference measure" for a discrete 
probability distribution is the "counting measure", which supports the 
use of the word "density" in this context being equivalent to "mass".? 
For continuous distributions, the "reference measure" is routinely taken 
to be the "improper prior" that assigns measure 1 to any unit interval 
on the real line.


 ????? Does that make it clear as mud?


 ????? Spencer Graves


https://en.wikipedia.org/wiki/Probability_density_function
>
> Furthermore the help file for dbinom() function references a link
> (http://www.herine.net/stat/software/dbinom.html) but it doesn't seem
> to land where it should. Maybe this could be updated?
>
> Thank you,
> Stefan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @|mon@wood @end|ng |rom b@th@edu  Fri Mar 15 13:31:31 2019
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Fri, 15 Mar 2019 12:31:31 +0000
Subject: [R] [mgcv] Memory issues with bam() on computer cluster
In-Reply-To: <AM0PR0302MB3458D8EF91DD48219756AC27BC440@AM0PR0302MB3458.eurprd03.prod.outlook.com>
References: <mailman.353852.1.1551783602.4534.r-help@r-project.org>
 <AM0PR0302MB3458D8EF91DD48219756AC27BC440@AM0PR0302MB3458.eurprd03.prod.outlook.com>
Message-ID: <d8e2643a-d960-0d86-4296-f0c7fcf149cb@bath.edu>

Can you supply the results of sessionInfo() please, and the full bam 
call that causes this.

best,

Simon (mgcv maintainer)

On 15/03/2019 09:09, Frank van Berkum wrote:
> Dear Community,
>
> In our current research we are trying to fit Generalized Additive Models to a large dataset. We are using the package mgcv in R.
>
> Our dataset contains about 22 million records with less than 20 risk factors for each observation, so in our case n>>p. The dataset covers the period 2006 until 2011, and we analyse both the complete dataset and datasets in which we leave out a single year. The latter part is done to analyse robustness of the results. We understand k-fold cross validation may seem more appropriate, but out approach is closer to what is done in practice (how will one additional year of information affect your estimates?).
>
> We use the function bam as advocated in Wood et al. (2017), and we apply the following options: bam(?, discrete=TRUE, chunk.size=10000, gc.level=1). We run these analyses on a computer cluster (see https://userinfo.surfsara.nl/systems/lisa/description for details), and the job is allocated to a node within the computer cluster. A node has at least 16 cores and 64Gb memory.
>
> We had expected 64Gb of memory to be sufficient for these analyses, especially since the bam function is built specifically for large datasets. However, when applying this function to the different datasets described above with different regression specifications (different risk factors included in the linear predictor), we sometimes obtain errors of the following form.
>
> Error in XWyd(G$Xd, w, z, G$kd, G$ks, G$ts, G$dt, G$v, G$qc, G$drop, ar.stop,  :
>
>    'Calloc' could not allocate memory (22624897 of 8 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> XWyd
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> Error in Xbd(G$Xd, coef, G$kd, G$ks, G$ts, G$dt, G$v, G$qc, G$drop) :
>
>    'Calloc' could not allocate memory (18590685 of 8 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> Xbd
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> Error: cannot allocate vector of size 1.7 Gb
>
> Timing stopped at: 2 0.556 4.831
>
> Error in system.time(oo <- .C(C_XWXd0, XWX = as.double(rep(0, (pt + nt)^2)),  :
>
>    'Calloc' could not allocate memory (55315650 of 24 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> XWXd -> system.time -> .C
>
> Timing stopped at: 1.056 1.396 2.459
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> The errors seem to arise at different stages in the optimization process. We have analysed whether these errors disappear if different settings are used (different chunk.size, different gc.level), but this does not resolve our problem. Also, the errors occur on different datasets when using different settings, and even when using the same settings it is possible that an error that occurred on dataset X in one run it does not necessarily occur on dataset X in a different run. When using the discrete=TRUE option, optimization can be parallelized, but we have chosen to not employ this feature to ensure memory does not have to be shared between parallel processes.
>
> Naturally I cannot share our dataset with you which makes the problem difficult to analyse. However, based on your collective knowledge, could you pinpoint us to where the problem may occur? Is it something within the C-code used within the package (as the last error seems to indicate), or is it related to the computer cluster?
>
> Any help or insights is much appreciated.
>
> Kind regards,
>
> Frank
>
> 	[[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Simon Wood, School of Mathematics, University of Bristol, BS8 1TW UK
https://people.maths.bris.ac.uk/~sw15190/


	[[alternative HTML version deleted]]


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Fri Mar 15 13:48:36 2019
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Fri, 15 Mar 2019 18:18:36 +0530
Subject: [R] Stratifying data with xyplot
In-Reply-To: <CAKFxdiSQ7j5RAEPqDCxSjNGbQvoHRpHhnMavWWE=2aPzP5TW4w@mail.gmail.com>
References: <1505236375.928997.1552332367691.JavaMail.zimbra@cognigencorp.com>
 <CAKFxdiSQ7j5RAEPqDCxSjNGbQvoHRpHhnMavWWE=2aPzP5TW4w@mail.gmail.com>
Message-ID: <CADfFDC7EbPc+s3--8OV-apYyf4aH1GR4Tpx2Os_CM5z4irW39Q@mail.gmail.com>

On Tue, Mar 12, 2019 at 2:28 AM Kevin Wright <kw.stat at gmail.com> wrote:
>
> See the examples here:
> https://www.stat.ubc.ca/~jenny/STAT545A/block10_latticeNittyGritty.html

Excellent reference. The only improvement I could think of is to abuse
the non-standard evaluation of 'groups' to avoid repeating the name of
the dataset, which would go something like

xyplot(lifeExp ~ gdpPercap | factor(year), yDat, aspect = 2/3,
       grid = TRUE, scales = list(x = list(log = 10, equispaced.log = FALSE)),
       col = jDarkGray, pch = jPch,
       groups = list(cex = sqrt(pop/pi) / jCexDivisor,
                     fill = color),
       panel = function(x, y, ..., groups, subscripts) {
           panel.xyplot(x, y,
                        cex = groups$cex[subscripts],
                        fill = groups$fill[subscripts], ...)
         })

Unfortunately, this doesn't work because prepanel.default.xyplot()
tries to be too smart and assumes that 'groups' is a factor. A
workaround is to override the default prepanel function; e.g.,

lattice.options(prepanel.default.xyplot =
                function(x, y, ...) list(xlim = extendrange(range(x)),
                                         ylim = extendrange(range(y))))

I will try to fix prepanel.default.xyplot() for the next update of lattice.

-Deepayan


>
> On Mon, Mar 11, 2019 at 2:26 PM Sebastien Bihorel <
> sebastien.bihorel at cognigencorp.com> wrote:
>
> > Hi,
> >
> > I am a big user/fan of the lattice package for plotting. As far as I know,
> > lattice only offers one method to stratify data within a xyplot panel,
> > using the groups arguments.
> > A contrario, the ggplot package allow users to use different variables for
> > coloring, setting the symbols, the line types or the size of symbols. This
> > frequently comes handy.
> > My question is whether any work has been done in the lattice ecosystem to
> > reproduce this functionality? If so, I would greatly appreciate any
> > pointers to the appropriate package documentation.
> >
> > Thank you
> >
> > Sebastien
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Kevin Wright
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From B|||@Po||ng @end|ng |rom ze||@@com  Fri Mar 15 14:29:52 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Fri, 15 Mar 2019 13:29:52 +0000
Subject: [R] 3.5.3
Message-ID: <BN7PR02MB50738C9B757B729D42D16314EA440@BN7PR02MB5073.namprd02.prod.outlook.com>

Good morning.

This may have already been asked, if so my appologies.

I want to upgrade to 3.5.3 however there is an alternative called patched.

Do I want it or the basic 3.5.3?

Thank you.

https://www.google.com/search?safe=strict&rlz=1C1GCEB_enUS821US821&ei=naWLXLuuAoOb5wKp6K_oBA&q=difference+between+r+3.5.3+and+3.5.3+patched&oq=difference+between+r+3.5.3+and+3.5.3+patched&gs_l=psy-ab.3...3568.18782..19030...1.0..0.133.3101.34j4......0....1..gws-wiz.......0i7i30j33i10j33i299j33i22i29i30j33i160.nyMRzRN9ebw

WHP



Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From pd@|gd @end|ng |rom gm@||@com  Fri Mar 15 14:37:25 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 15 Mar 2019 14:37:25 +0100
Subject: [R] density vs. mass for discrete probability functions
In-Reply-To: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
References: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
Message-ID: <098EE006-5A68-4145-B5B2-3A1D5DEC295D@gmail.com>

Mathematically, you can bring discrete and continuous distributions on a common footing by defining probability functions as densities wrt. counting measure. You don't really need Radon-Nikodym derivatives to understand the idea, just the fact that sums can be interpreted as integrals wrt counting measure, hence sum_{x in A} f(x) and int_A f(x) dx are essentially the same concept.

-pd

> On 15 Mar 2019, at 01:43 , Stefan Schreiber <sschreib at ualberta.ca> wrote:
> 
> Dear R users,
> 
> While experimenting with the dbinom() function and reading its
> documentation (?dbinom) it reads that "dbinom gives the density" but
> shouldn't it be called "mass" instead of "density"? I assume that it
> has something to do with keeping the function for "density" consistent
> across discrete and continuous probability functions - but I am not
> sure and was hoping someone could clarify?
> 
> Furthermore the help file for dbinom() function references a link
> (http://www.herine.net/stat/software/dbinom.html) but it doesn't seem
> to land where it should. Maybe this could be updated?
> 
> Thank you,
> Stefan
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pd@|gd @end|ng |rom gm@||@com  Fri Mar 15 14:40:50 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 15 Mar 2019 14:40:50 +0100
Subject: [R] 3.5.3
In-Reply-To: <BN7PR02MB50738C9B757B729D42D16314EA440@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB50738C9B757B729D42D16314EA440@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <CB740514-F5FC-4821-8FB7-866D4E69CFBB@gmail.com>

You generally only want the patched version if it is known to fix something that you need. As 3.5.3 is intended as the wrap-up release of 3.5.x, it is quite likely that the two will only ever differ by name.

-pd

> On 15 Mar 2019, at 14:29 , Bill Poling <Bill.Poling at zelis.com> wrote:
> 
> Good morning.
> 
> This may have already been asked, if so my appologies.
> 
> I want to upgrade to 3.5.3 however there is an alternative called patched.
> 
> Do I want it or the basic 3.5.3?
> 
> Thank you.
> 
> https://www.google.com/search?safe=strict&rlz=1C1GCEB_enUS821US821&ei=naWLXLuuAoOb5wKp6K_oBA&q=difference+between+r+3.5.3+and+3.5.3+patched&oq=difference+between+r+3.5.3+and+3.5.3+patched&gs_l=psy-ab.3...3568.18782..19030...1.0..0.133.3101.34j4......0....1..gws-wiz.......0i7i30j33i10j33i299j33i22i29i30j33i160.nyMRzRN9ebw
> 
> WHP
> 
> 
> 
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Mar 15 14:54:50 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 15 Mar 2019 06:54:50 -0700
Subject: [R] 3.5.3
In-Reply-To: <BN7PR02MB50738C9B757B729D42D16314EA440@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB50738C9B757B729D42D16314EA440@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <FFFAE9B2-AD7A-4A2B-AED0-57D664AF0913@dcn.davis.ca.us>

You really need to get into better touch with your feelings, man... how are we supposed to know what you want? ;-)

In general you should be using the main release unless you know there is a bug in it that affects you and has specifically been addressed in the patched version. The patched version may have also introduced new bugs and has in general been tested less than the main release version.

But hey... pick your own poison... maybe you want to contribute more bug reports per week... patched would be just your thing!

On March 15, 2019 6:29:52 AM PDT, Bill Poling <Bill.Poling at zelis.com> wrote:
>Good morning.
>
>This may have already been asked, if so my appologies.
>
>I want to upgrade to 3.5.3 however there is an alternative called
>patched.
>
>Do I want it or the basic 3.5.3?
>
>Thank you.
>
>https://www.google.com/search?safe=strict&rlz=1C1GCEB_enUS821US821&ei=naWLXLuuAoOb5wKp6K_oBA&q=difference+between+r+3.5.3+and+3.5.3+patched&oq=difference+between+r+3.5.3+and+3.5.3+patched&gs_l=psy-ab.3...3568.18782..19030...1.0..0.133.3101.34j4......0....1..gws-wiz.......0i7i30j33i10j33i299j33i22i29i30j33i160.nyMRzRN9ebw
>
>WHP
>
>
>
>Confidentiality Notice This message is sent from Zelis.
>...{{dropped:13}}
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Fri Mar 15 15:31:57 2019
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Fri, 15 Mar 2019 09:31:57 -0500
Subject: [R] 3.5.3
In-Reply-To: <FFFAE9B2-AD7A-4A2B-AED0-57D664AF0913@dcn.davis.ca.us>
References: <BN7PR02MB50738C9B757B729D42D16314EA440@BN7PR02MB5073.namprd02.prod.outlook.com>
 <FFFAE9B2-AD7A-4A2B-AED0-57D664AF0913@dcn.davis.ca.us>
Message-ID: <847c6f44-eae8-5e85-bb26-da95bd7ac6ea@effectivedefense.org>



On 2019-03-15 08:54, Jeff Newmiller wrote:
> You really need to get into better touch with your feelings, man... how are we supposed to know what you want? ;-)
>
> In general you should be using the main release unless you know there is a bug in it that affects you and has specifically been addressed in the patched version. The patched version may have also introduced new bugs and has in general been tested less than the main release version.
>
> But hey... pick your own poison... maybe you want to contribute more bug reports per week... patched would be just your thing!


 ????? A "fortune"?? sg
>
> On March 15, 2019 6:29:52 AM PDT, Bill Poling <Bill.Poling at zelis.com> wrote:
>> Good morning.
>>
>> This may have already been asked, if so my appologies.
>>
>> I want to upgrade to 3.5.3 however there is an alternative called
>> patched.
>>
>> Do I want it or the basic 3.5.3?
>>
>> Thank you.
>>
>> https://www.google.com/search?safe=strict&rlz=1C1GCEB_enUS821US821&ei=naWLXLuuAoOb5wKp6K_oBA&q=difference+between+r+3.5.3+and+3.5.3+patched&oq=difference+between+r+3.5.3+and+3.5.3+patched&gs_l=psy-ab.3...3568.18782..19030...1.0..0.133.3101.34j4......0....1..gws-wiz.......0i7i30j33i10j33i299j33i22i29i30j33i160.nyMRzRN9ebw
>>
>> WHP
>>
>>
>>
>> Confidentiality Notice This message is sent from Zelis.
>> ...{{dropped:13}}
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Fri Mar 15 15:36:45 2019
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Fri, 15 Mar 2019 09:36:45 -0500
Subject: [R] density vs. mass for discrete probability functions
In-Reply-To: <098EE006-5A68-4145-B5B2-3A1D5DEC295D@gmail.com>
References: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
 <098EE006-5A68-4145-B5B2-3A1D5DEC295D@gmail.com>
Message-ID: <09e35373-2cb6-a891-9db1-b275a8baf87f@effectivedefense.org>



On 2019-03-15 08:37, peter dalgaard wrote:
> Mathematically, you can bring discrete and continuous distributions on a common footing by defining probability functions as densities wrt. counting measure. You don't really need Radon-Nikodym derivatives to understand the idea, just the fact that sums can be interpreted as integrals wrt counting measure, hence sum_{x in A} f(x) and int_A f(x) dx are essentially the same concept.


 ????? Correct.? That's for clearing up my "mud".? sg
> -pd
>
>> On 15 Mar 2019, at 01:43 , Stefan Schreiber <sschreib at ualberta.ca> wrote:
>>
>> Dear R users,
>>
>> While experimenting with the dbinom() function and reading its
>> documentation (?dbinom) it reads that "dbinom gives the density" but
>> shouldn't it be called "mass" instead of "density"? I assume that it
>> has something to do with keeping the function for "density" consistent
>> across discrete and continuous probability functions - but I am not
>> sure and was hoping someone could clarify?
>>
>> Furthermore the help file for dbinom() function references a link
>> (http://www.herine.net/stat/software/dbinom.html) but it doesn't seem
>> to land where it should. Maybe this could be updated?
>>
>> Thank you,
>> Stefan
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From B|||@Po||ng @end|ng |rom ze||@@com  Fri Mar 15 20:45:27 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Fri, 15 Mar 2019 19:45:27 +0000
Subject: [R] Help with gsub function
Message-ID: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>

Good afternoon.

sessionInfo()
#R version 3.5.3 (2019-03-11)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows >= 8 x64 (build 9200)

I am using gsub function to remove a hyphen in a 9 character column of values in order to convert it to integer.

Works fine except where the second segment has a leading 0, then it is eliminating the 0

Example "73-0700090" becomes " 73700090"
                 "77-0633896" becomes "77633896"

Is there a remedy for this?

tb2a$TID2 <- gsub(tb2a$TID, pattern="-[0-0]{0,7}", replacement = "")

head(tb2a$TID,n=10)
 [1] "11-1352310" "45-2711804" "35-6001540" "77-0633896" "62-1762545" "61-1029768" "73-0700090" "47-0376604" "47-0486026" "38-3833117"
> head(tb2a$TID2,n=10)
 [1] "111352310" "452711804" "356001540" "77633896"  "621762545" "611029768" "73700090"  "47376604"  "47486026"  "383833117"

I have googled the problem and have not found a solution.

http://www.endmemo.com/program/R/gsub.php
http://r.789695.n4.nabble.com/extracting-characters-from-string-td3298971.html


Thank you.

WHP

Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}


From kry|ov@r00t @end|ng |rom gm@||@com  Fri Mar 15 20:49:02 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 15 Mar 2019 22:49:02 +0300
Subject: [R] Help with gsub function
In-Reply-To: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <20190315224902.172d5e32@Tarkus>

On Fri, 15 Mar 2019 19:45:27 +0000
Bill Poling <Bill.Poling at zelis.com> wrote:

Hello Bill,

> tb2a$TID2 <- gsub(tb2a$TID, pattern="-[0-0]{0,7}", replacement = "")

Is the pattern supposed to mean something besides the "-" you want to
remove? For the problem you describe, pattern="-" should be enough. It
should locate all hyphens in the string and replace them with empty
strings, i.e. remove them.

-- 
Best regards,
Ivan


From peter@|@ng|e|der @end|ng |rom gm@||@com  Fri Mar 15 20:52:41 2019
From: peter@|@ng|e|der @end|ng |rom gm@||@com (Peter Langfelder)
Date: Fri, 15 Mar 2019 12:52:41 -0700
Subject: [R] Help with gsub function
In-Reply-To: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <CA+hbrhW30ubMtSn3uDz9Vb9Env1b84gYFGn_yZu-vVzM3W=ahQ@mail.gmail.com>

If you want to remove just the hyphen, why not do

sub("-", "", tb2a$TID)

sub("-", "", "73-017323")
[1] "73017323"

Am I missing something?

Peter

On Fri, Mar 15, 2019 at 12:46 PM Bill Poling <Bill.Poling at zelis.com> wrote:
>
> Good afternoon.
>
> sessionInfo()
> #R version 3.5.3 (2019-03-11)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows >= 8 x64 (build 9200)
>
> I am using gsub function to remove a hyphen in a 9 character column of values in order to convert it to integer.
>
> Works fine except where the second segment has a leading 0, then it is eliminating the 0
>
> Example "73-0700090" becomes " 73700090"
>                  "77-0633896" becomes "77633896"
>
> Is there a remedy for this?
>
> tb2a$TID2 <- gsub(tb2a$TID, pattern="-[0-0]{0,7}", replacement = "")
>
> head(tb2a$TID,n=10)
>  [1] "11-1352310" "45-2711804" "35-6001540" "77-0633896" "62-1762545" "61-1029768" "73-0700090" "47-0376604" "47-0486026" "38-3833117"
> > head(tb2a$TID2,n=10)
>  [1] "111352310" "452711804" "356001540" "77633896"  "621762545" "611029768" "73700090"  "47376604"  "47486026"  "383833117"
>
> I have googled the problem and have not found a solution.
>
> http://www.endmemo.com/program/R/gsub.php
> http://r.789695.n4.nabble.com/extracting-characters-from-string-td3298971.html
>
>
> Thank you.
>
> WHP
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Mar 15 21:00:05 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 15 Mar 2019 13:00:05 -0700
Subject: [R] Help with gsub function
In-Reply-To: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <C923414B-6C57-4CE4-B2A5-DD5641B28B4A@dcn.davis.ca.us>

Your pattern seems ... way overboard? Why not

gsub("-", "", tb2a$TID)

On March 15, 2019 12:45:27 PM PDT, Bill Poling <Bill.Poling at zelis.com> wrote:
>Good afternoon.
>
>sessionInfo()
>#R version 3.5.3 (2019-03-11)
>#Platform: x86_64-w64-mingw32/x64 (64-bit)
>#Running under: Windows >= 8 x64 (build 9200)
>
>I am using gsub function to remove a hyphen in a 9 character column of
>values in order to convert it to integer.
>
>Works fine except where the second segment has a leading 0, then it is
>eliminating the 0
>
>Example "73-0700090" becomes " 73700090"
>                 "77-0633896" becomes "77633896"
>
>Is there a remedy for this?
>
>tb2a$TID2 <- 
>
>head(tb2a$TID,n=10)
>[1] "11-1352310" "45-2711804" "35-6001540" "77-0633896" "62-1762545"
>"61-1029768" "73-0700090" "47-0376604" "47-0486026" "38-3833117"
>> head(tb2a$TID2,n=10)
>[1] "111352310" "452711804" "356001540" "77633896"  "621762545"
>"611029768" "73700090"  "47376604"  "47486026"  "383833117"
>
>I have googled the problem and have not found a solution.
>
>http://www.endmemo.com/program/R/gsub.php
>http://r.789695.n4.nabble.com/extracting-characters-from-string-td3298971.html
>
>
>Thank you.
>
>WHP
>
>Confidentiality Notice This message is sent from Zelis.
>...{{dropped:13}}
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From JLucke m@iii@g oii ri@@buii@io@edu  Fri Mar 15 21:39:03 2019
From: JLucke m@iii@g oii ri@@buii@io@edu (JLucke m@iii@g oii ri@@buii@io@edu)
Date: Fri, 15 Mar 2019 16:39:03 -0400
Subject: [R] density vs. mass for discrete probability functions
In-Reply-To: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
References: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
Message-ID: <OF95F944BF.D08218CC-ON852583BE.00711BE4-852583BE.0071706C@ria.buffalo.edu>

Stefan---

Under the measure-theoretic approach to probability, discrete & continuous 
probability densities follow the same underlying mathematical principles.
Check any text on measure-theoretic probability theory.

---JFL





Stefan Schreiber <sschreib at ualberta.ca> 
Sent by: "R-help" <r-help-bounces at r-project.org>
03/14/2019 08:43 PM

To
r-help at r-project.org, 
cc

Subject
[R] density vs. mass for discrete probability functions






Dear R users,

While experimenting with the dbinom() function and reading its
documentation (?dbinom) it reads that "dbinom gives the density" but
shouldn't it be called "mass" instead of "density"? I assume that it
has something to do with keeping the function for "density" consistent
across discrete and continuous probability functions - but I am not
sure and was hoping someone could clarify?

Furthermore the help file for dbinom() function references a link
(http://www.herine.net/stat/software/dbinom.html) but it doesn't seem
to land where it should. Maybe this could be updated?

Thank you,
Stefan

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From e@@w|ek @end|ng |rom gm@||@com  Sat Mar 16 06:40:01 2019
From: e@@w|ek @end|ng |rom gm@||@com (Ek Esawi)
Date: Sat, 16 Mar 2019 01:40:01 -0400
Subject: [R] Split a DF on Date column for each single year
Message-ID: <CA+ZkTxs12dyB5woWhbJ6oZgokaBpYzDZxMxsb+wPdpnvXchozA@mail.gmail.com>

Hi All?

I have a data frame with over 13000 rows and 4 columns. A mini data
frame is given at the bottom. I want to split the data frame into
lists each corresponds to single year which ranges from 1990 to 2018).
I wanted to use the split function, but it requires a vector of the
same length as MyDate which contains many multiples of each year.
Any help is highly appreciated.

I want the following results:
List 1990
MyDate MyNo MyDes
1990
1990
1990
?...
List 2000
2000
2000
2000
?...
List 2001
2001
2001
2001
2001
?...
List 2018
2018
2018
2018
?...

Sample data frame

mydf <- data.frame(MyDate=c("1990-01-01","1990-04-07","2000-04-05","2018-01-04"),MyNo=c(1,2,3,4),MyDes=c("AA","BB","CC","DD"))


EK


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Mar 16 06:52:47 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 15 Mar 2019 22:52:47 -0700
Subject: [R] Split a DF on Date column for each single year
In-Reply-To: <CA+ZkTxs12dyB5woWhbJ6oZgokaBpYzDZxMxsb+wPdpnvXchozA@mail.gmail.com>
References: <CA+ZkTxs12dyB5woWhbJ6oZgokaBpYzDZxMxsb+wPdpnvXchozA@mail.gmail.com>
Message-ID: <BF4206FE-112F-4EF0-852C-CAA62075CF74@dcn.davis.ca.us>

Couldn't you just use the substr function to pull the year out yourself to make the grouping column?

On March 15, 2019 10:40:01 PM PDT, Ek Esawi <esawiek at gmail.com> wrote:
>Hi All?
>
>I have a data frame with over 13000 rows and 4 columns. A mini data
>frame is given at the bottom. I want to split the data frame into
>lists each corresponds to single year which ranges from 1990 to 2018).
>I wanted to use the split function, but it requires a vector of the
>same length as MyDate which contains many multiples of each year.
>Any help is highly appreciated.
>
>I want the following results:
>List 1990
>MyDate MyNo MyDes
>1990
>1990
>1990
>?...
>List 2000
>2000
>2000
>2000
>?...
>List 2001
>2001
>2001
>2001
>2001
>?...
>List 2018
>2018
>2018
>2018
>?...
>
>Sample data frame
>
>mydf <-
>data.frame(MyDate=c("1990-01-01","1990-04-07","2000-04-05","2018-01-04"),MyNo=c(1,2,3,4),MyDes=c("AA","BB","CC","DD"))
>
>
>EK
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From R@|ner@Schuerm@nn @end|ng |rom gmx@net  Sat Mar 16 07:05:32 2019
From: R@|ner@Schuerm@nn @end|ng |rom gmx@net (Rainer Schuermann)
Date: Sat, 16 Mar 2019 07:05:32 +0100
Subject: [R] Split a DF on Date column for each single year
In-Reply-To: <CA+ZkTxs12dyB5woWhbJ6oZgokaBpYzDZxMxsb+wPdpnvXchozA@mail.gmail.com>
References: <CA+ZkTxs12dyB5woWhbJ6oZgokaBpYzDZxMxsb+wPdpnvXchozA@mail.gmail.com>
Message-ID: <1772488.kR93HETb1O@nizza>

In your sample data.frame, MyDate and MyDes are factors; is that what you want?
rs

On Samstag, 16. M?rz 2019 01:40:01 CET Ek Esawi wrote:
> Hi All?
> 
> I have a data frame with over 13000 rows and 4 columns. A mini data
> frame is given at the bottom. I want to split the data frame into
> lists each corresponds to single year which ranges from 1990 to 2018).
> I wanted to use the split function, but it requires a vector of the
> same length as MyDate which contains many multiples of each year.
> Any help is highly appreciated.
> 
> I want the following results:
> List 1990
> MyDate MyNo MyDes
> 1990
> 1990
> 1990
> ?...
> List 2000
> 2000
> 2000
> 2000
> ?...
> List 2001
> 2001
> 2001
> 2001
> 2001
> ?...
> List 2018
> 2018
> 2018
> 2018
> ?...
> 
> Sample data frame
> 
> mydf <- data.frame(MyDate=c("1990-01-01","1990-04-07","2000-04-05","2018-01-04"),MyNo=c(1,2,3,4),MyDes=c("AA","BB","CC","DD"))
> 
> 
> EK
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From e@@w|ek @end|ng |rom gm@||@com  Sat Mar 16 07:39:21 2019
From: e@@w|ek @end|ng |rom gm@||@com (Ek Esawi)
Date: Sat, 16 Mar 2019 02:39:21 -0400
Subject: [R] Split a DF on Date column for each single year
In-Reply-To: <BF4206FE-112F-4EF0-852C-CAA62075CF74@dcn.davis.ca.us>
References: <CA+ZkTxs12dyB5woWhbJ6oZgokaBpYzDZxMxsb+wPdpnvXchozA@mail.gmail.com>
 <BF4206FE-112F-4EF0-852C-CAA62075CF74@dcn.davis.ca.us>
Message-ID: <CA+ZkTxv_0xWczcYqp0NNQ9r92YozFZDY_C3vRq9iNXHPx7wi2g@mail.gmail.com>

Thank you Jeff and Rainer. I will try Jeff's idea using the sub
string. function to extract the year and split on that.

Thanks again to both--EK

On Sat, Mar 16, 2019 at 1:52 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> Couldn't you just use the substr function to pull the year out yourself to make the grouping column?
>
> On March 15, 2019 10:40:01 PM PDT, Ek Esawi <esawiek at gmail.com> wrote:
> >Hi All?
> >
> >I have a data frame with over 13000 rows and 4 columns. A mini data
> >frame is given at the bottom. I want to split the data frame into
> >lists each corresponds to single year which ranges from 1990 to 2018).
> >I wanted to use the split function, but it requires a vector of the
> >same length as MyDate which contains many multiples of each year.
> >Any help is highly appreciated.
> >
> >I want the following results:
> >List 1990
> >MyDate MyNo MyDes
> >1990
> >1990
> >1990
> >?...
> >List 2000
> >2000
> >2000
> >2000
> >?...
> >List 2001
> >2001
> >2001
> >2001
> >2001
> >?...
> >List 2018
> >2018
> >2018
> >2018
> >?...
> >
> >Sample data frame
> >
> >mydf <-
> >data.frame(MyDate=c("1990-01-01","1990-04-07","2000-04-05","2018-01-04"),MyNo=c(1,2,3,4),MyDes=c("AA","BB","CC","DD"))
> >
> >
> >EK
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From B|||@Po||ng @end|ng |rom ze||@@com  Sat Mar 16 09:01:30 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Sat, 16 Mar 2019 08:01:30 +0000
Subject: [R] Help with gsub function
In-Reply-To: <CA+hbrhW30ubMtSn3uDz9Vb9Env1b84gYFGn_yZu-vVzM3W=ahQ@mail.gmail.com>
References: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
 <CA+hbrhW30ubMtSn3uDz9Vb9Env1b84gYFGn_yZu-vVzM3W=ahQ@mail.gmail.com>
Message-ID: <BN7PR02MB50737403EAEACE1A8D996921EA450@BN7PR02MB5073.namprd02.prod.outlook.com>

Good morning Peter, yes that works fine. My attempt was based on a google search that looked promising but was obviously more complicated than it needed to be.

Thank you.

WHP

From: Peter Langfelder <peter.langfelder at gmail.com>
Sent: Friday, March 15, 2019 3:53 PM
To: Bill Poling <Bill.Poling at zelis.com>
Cc: r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Help with gsub function

If you want to remove just the hyphen, why not do

sub("-", "", tb2a$TID)

sub("-", "", "73-017323")
[1] "73017323"

Am I missing something?

Peter

On Fri, Mar 15, 2019 at 12:46 PM Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>
> Good afternoon.
>
> sessionInfo()
> #R version 3.5.3 (2019-03-11)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows >= 8 x64 (build 9200)
>
> I am using gsub function to remove a hyphen in a 9 character column of values in order to convert it to integer.
>
> Works fine except where the second segment has a leading 0, then it is eliminating the 0
>
> Example "73-0700090" becomes " 73700090"
> "77-0633896" becomes "77633896"
>
> Is there a remedy for this?
>
> tb2a$TID2 <- gsub(tb2a$TID, pattern="-[0-0]{0,7}", replacement = "")
>
> head(tb2a$TID,n=10)
> [1] "11-1352310" "45-2711804" "35-6001540" "77-0633896" "62-1762545" "61-1029768" "73-0700090" "47-0376604" "47-0486026" "38-3833117"
> > head(tb2a$TID2,n=10)
> [1] "111352310" "452711804" "356001540" "77633896" "621762545" "611029768" "73700090" "47376604" "47486026" "383833117"
>
> I have googled the problem and have not found a solution.
>
> http://www.endmemo.com/program/R/gsub.php
> http://r.789695.n4.nabble.com/extracting-characters-from-string-td3298971.html
>
>
> Thank you.
>
> WHP
>
> Confidentiality Notice This message is sent from Zelis. ...{{dropped:13}}
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From @@chre|b @end|ng |rom u@|bert@@c@  Fri Mar 15 15:21:24 2019
From: @@chre|b @end|ng |rom u@|bert@@c@ (Stefan Schreiber)
Date: Fri, 15 Mar 2019 08:21:24 -0600
Subject: [R] density vs. mass for discrete probability functions
In-Reply-To: <098EE006-5A68-4145-B5B2-3A1D5DEC295D@gmail.com>
References: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
 <098EE006-5A68-4145-B5B2-3A1D5DEC295D@gmail.com>
Message-ID: <CAPK=JisKQpPcscD1fobKtyiTA17YPDXQ5NSn2d+kMSEdSkb6-g@mail.gmail.com>

Thank you Peter and Spencer. That clears things up. Also since no one
responded the second part of my question, I'm still wondering if it was
noted that there is a hyperlink in the dbinom help file (?dbinom) that
isn't directing correctly?

Stefan

On Fri, Mar 15, 2019, 07:37 peter dalgaard, <pdalgd at gmail.com> wrote:

> Mathematically, you can bring discrete and continuous distributions on a
> common footing by defining probability functions as densities wrt. counting
> measure. You don't really need Radon-Nikodym derivatives to understand the
> idea, just the fact that sums can be interpreted as integrals wrt counting
> measure, hence sum_{x in A} f(x) and int_A f(x) dx are essentially the same
> concept.
>
> -pd
>
> > On 15 Mar 2019, at 01:43 , Stefan Schreiber <sschreib at ualberta.ca>
> wrote:
> >
> > Dear R users,
> >
> > While experimenting with the dbinom() function and reading its
> > documentation (?dbinom) it reads that "dbinom gives the density" but
> > shouldn't it be called "mass" instead of "density"? I assume that it
> > has something to do with keeping the function for "density" consistent
> > across discrete and continuous probability functions - but I am not
> > sure and was hoping someone could clarify?
> >
> > Furthermore the help file for dbinom() function references a link
> > (http://www.herine.net/stat/software/dbinom.html) but it doesn't seem
> > to land where it should. Maybe this could be updated?
> >
> > Thank you,
> > Stefan
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Mar 16 18:47:57 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 16 Mar 2019 17:47:57 +0000
Subject: [R] density vs. mass for discrete probability functions
In-Reply-To: <CAPK=JisKQpPcscD1fobKtyiTA17YPDXQ5NSn2d+kMSEdSkb6-g@mail.gmail.com>
References: <CAPK=JitMrcOpZBHzgo53r7f+ewFwsGc=ocrnSSyg4A-Fn-Zamw@mail.gmail.com>
 <098EE006-5A68-4145-B5B2-3A1D5DEC295D@gmail.com>
 <CAPK=JisKQpPcscD1fobKtyiTA17YPDXQ5NSn2d+kMSEdSkb6-g@mail.gmail.com>
Message-ID: <a32c65a0-9969-1abe-157f-45eafb08882d@sapo.pt>

Hello,

Yes, there is even an old discussion on this on r-devel, dated August, 
10 2013.
See [1].


[1] 
https://r-project.markmail.org/search/?q=broken-link-in-docs-for-Binormial-functions#query:broken-link-in-docs-for-Binormial-functions+page:1+mid:rf6tbiokcdyai6el+state:results


Hope this helps,



Rui Barradas

?s 14:21 de 15/03/2019, Stefan Schreiber escreveu:
> Thank you Peter and Spencer. That clears things up. Also since no one
> responded the second part of my question, I'm still wondering if it was
> noted that there is a hyperlink in the dbinom help file (?dbinom) that
> isn't directing correctly?
> 
> Stefan
> 
> On Fri, Mar 15, 2019, 07:37 peter dalgaard, <pdalgd at gmail.com> wrote:
> 
>> Mathematically, you can bring discrete and continuous distributions on a
>> common footing by defining probability functions as densities wrt. counting
>> measure. You don't really need Radon-Nikodym derivatives to understand the
>> idea, just the fact that sums can be interpreted as integrals wrt counting
>> measure, hence sum_{x in A} f(x) and int_A f(x) dx are essentially the same
>> concept.
>>
>> -pd
>>
>>> On 15 Mar 2019, at 01:43 , Stefan Schreiber <sschreib at ualberta.ca>
>> wrote:
>>>
>>> Dear R users,
>>>
>>> While experimenting with the dbinom() function and reading its
>>> documentation (?dbinom) it reads that "dbinom gives the density" but
>>> shouldn't it be called "mass" instead of "density"? I assume that it
>>> has something to do with keeping the function for "density" consistent
>>> across discrete and continuous probability functions - but I am not
>>> sure and was hoping someone could clarify?
>>>
>>> Furthermore the help file for dbinom() function references a link
>>> (http://www.herine.net/stat/software/dbinom.html) but it doesn't seem
>>> to land where it should. Maybe this could be updated?
>>>
>>> Thank you,
>>> Stefan
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From r@i@1290 m@iii@g oii @im@com  Sat Mar 16 18:50:46 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sat, 16 Mar 2019 17:50:46 +0000 (UTC)
Subject: [R] Problems trying to make a time series using NetCDF file
References: <917481836.5460861.1552758646989.ref@mail.yahoo.com>
Message-ID: <917481836.5460861.1552758646989@mail.yahoo.com>

Hi there,
I am using climate model data in attempt to create a time series for a specific location. Getting longitude and latitude is fine. However, I am receiving the following error when using the "ncvar_get" function in trying to derive the "time" dimension:
Error in nc$dim[[idobj$list_index]] : 
  attempt to select more than one element in get1index <real>
What could be causing this?
-This is what I have done so far:
#CanESM2 plotting for specified yearncfname<-"MaxPrecCCCMACanESM2rcp45.nc"
Prec<-raster(ncfname)
print(Prec)
Model<-nc_open(ncfname)longitude<-ncvar_get(Model, "lon") #Works fine
latitude<-ncvar_get(Model, "lat") #Works fine
time<-ncvar_get(Model, "time") #Error in nc$dim[[idobj$list_index]] : attempt to select more than one element in get1index <real>


Here is the structure of the file after using "nc_open":
File MaxPrecCCCMACanESM2rcp45.nc (NC_FORMAT_NETCDF4):

     3 variables (excluding dimension variables):
        double onedaymax[lon,lat,time]   (Contiguous storage)  
            units: mm/day
        double fivedaymax[lon,lat,time]   (Contiguous storage)  
            units: mm/day
        short Year[time]   (Contiguous storage)  

     3 dimensions:
        time  Size:95
        lat  Size:64
            units: degree North
        lon  Size:128
            units: degree East
What could be causing this error??
Thanks in advance, and I look forward to your response!
	[[alternative HTML version deleted]]


From che|@e@rh|nton @end|ng |rom gm@||@com  Sat Mar 16 23:09:22 2019
From: che|@e@rh|nton @end|ng |rom gm@||@com (Chelsea Hinton)
Date: Sat, 16 Mar 2019 18:09:22 -0400
Subject: [R] Modularity Analysis in R, bipartite or rnetcarto package?
Message-ID: <CAA3E8s3u9NNKkep+4UKT+U2iqzBNTRuYGrZABkWhrjGEnn79DQ@mail.gmail.com>

Hello,

I have constructed a plant-pollinator network and would like to conduct a
modularity analysis using the algorithm developed by Guimera and Amaral
(2005) and the thresholds assigned by Olesen et al. (2007). In researching
functions to accomplish this in R, I found the netcarto function in
'rnetcarto' package AND the czvalues function in the 'bipartite' package.
Both functions state they use the previously mentioned formulae and
thresholds. I am hoping to gather insight on which one is best to use. The
results produced by both functions are close in values but given the
assigned thresholds (c>0.62, z>2.5 determine whether the lower trophic
level species can be connectors or hubs) make a huge difference in my
conclusions if they are right around these thresholds. Can anyone provide
insight into which function is best suited for this particular analysis?
Please send response to chelsearhinton at gmail.com

Thank you,

Chelsea Hinton
Graduate Assistant
Department of Biological Sciences
Eastern Kentucky University

	[[alternative HTML version deleted]]


From k|m@j@cob@en @end|ng |rom gm@||@com  Sat Mar 16 19:03:06 2019
From: k|m@j@cob@en @end|ng |rom gm@||@com (Kim Jacobsen)
Date: Sat, 16 Mar 2019 18:03:06 +0000
Subject: [R] plot.xmean.ordinaly vs plot() in package "rms"
Message-ID: <CAHn6EJ3LF286PCWaGPJ0tdk8wXSwejmJrsygzWtHeL2qyoUErw@mail.gmail.com>

Would anyone be able to explain what the difference is between
plot.xmean.ordinaly and plot() in the "rms" package? (for the purposes of
testing the proportional odds assumption in ordinal models). In the package
document (https://cran.r-project.org/web/packages/rms/rms.pdf) they seem
both to be used interchangeably.

Thank you!

-- 
Kim Solve Jacobsen
PhD candidate, Wildlife Conservation Research Unit
University of Oxford
Recanati-Kaplan Centre
OX13 5QL Oxford, UK

[image: Image result for oxford logo]

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Mar 17 02:33:59 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 16 Mar 2019 18:33:59 -0700
Subject: [R] Problems trying to make a time series using NetCDF file
In-Reply-To: <917481836.5460861.1552758646989@mail.yahoo.com>
References: <917481836.5460861.1552758646989.ref@mail.yahoo.com>
 <917481836.5460861.1552758646989@mail.yahoo.com>
Message-ID: <334E8ED6-1F27-4D48-B449-845298C1686F@dcn.davis.ca.us>

You might have better luck on the R-sig-spatial mailing list. If you had provided a reproducible example ("reprex") then I might have tried to answer the question but I don't use these tools often enough to grasp the source of your problem based only on diagnostics. And even though they might have a better chance of seeing  the issue with only diagnostics, I am sure they would appreciate a reprex too.

On March 16, 2019 10:50:46 AM PDT, rain1290--- via R-help <r-help at r-project.org> wrote:
>Hi there,
>I am using climate model data in attempt to create a time series for a
>specific location. Getting longitude and latitude is fine. However, I
>am receiving the following error when using the "ncvar_get" function in
>trying to derive the "time" dimension:
>Error in nc$dim[[idobj$list_index]] : 
>  attempt to select more than one element in get1index <real>
>What could be causing this?
>-This is what I have done so far:
>#CanESM2 plotting for specified
>yearncfname<-"MaxPrecCCCMACanESM2rcp45.nc"
>Prec<-raster(ncfname)
>print(Prec)
>Model<-nc_open(ncfname)longitude<-ncvar_get(Model, "lon") #Works fine
>latitude<-ncvar_get(Model, "lat") #Works fine
>time<-ncvar_get(Model, "time") #Error in nc$dim[[idobj$list_index]] :
>attempt to select more than one element in get1index <real>
>
>
>Here is the structure of the file after using "nc_open":
>File MaxPrecCCCMACanESM2rcp45.nc (NC_FORMAT_NETCDF4):
>
>     3 variables (excluding dimension variables):
>        double onedaymax[lon,lat,time]   (Contiguous storage)  
>            units: mm/day
>        double fivedaymax[lon,lat,time]   (Contiguous storage)  
>            units: mm/day
>        short Year[time]   (Contiguous storage)  
>
>     3 dimensions:
>        time  Size:95
>        lat  Size:64
>            units: degree North
>        lon  Size:128
>            units: degree East
>What could be causing this error??
>Thanks in advance, and I look forward to your response!
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From dp|erce @end|ng |rom uc@d@edu  Sun Mar 17 03:02:50 2019
From: dp|erce @end|ng |rom uc@d@edu (David W. Pierce)
Date: Sat, 16 Mar 2019 19:02:50 -0700
Subject: [R] Problems trying to make a time series using NetCDF file
In-Reply-To: <917481836.5460861.1552758646989@mail.yahoo.com>
References: <917481836.5460861.1552758646989.ref@mail.yahoo.com>
 <917481836.5460861.1552758646989@mail.yahoo.com>
Message-ID: <CAL+Zad8y0nsd86KH1eD_y1Yu6WahiX939dfyx6tiXbDAyHF0zg@mail.gmail.com>

Hi there,

the reason it's doing this is because there is no variable named "time" in
the file. Obviously I need to patch the ncdf4 package so that it issues an/
understandable error message in this case instead of dying with a weird R
error message. That is probably happening because there is a dimension
named "time", but no variable name "time".

Perhaps what you meant to do instead is get the "Year" variable, which is,
indeed, a variable in the file:

Year <- ncvar_get( Model, "Year" )

Regards,

--Dave


On Sat, Mar 16, 2019 at 5:08 PM rain1290--- via R-help <r-help at r-project.org>
wrote:

> Hi there,
> I am using climate model data in attempt to create a time series for a
> specific location. Getting longitude and latitude is fine. However, I am
> receiving the following error when using the "ncvar_get" function in trying
> to derive the "time" dimension:
> Error in nc$dim[[idobj$list_index]] :
>   attempt to select more than one element in get1index <real>
> What could be causing this?
> -This is what I have done so far:
> #CanESM2 plotting for specified yearncfname<-"MaxPrecCCCMACanESM2rcp45.nc"
> Prec<-raster(ncfname)
> print(Prec)
> Model<-nc_open(ncfname)longitude<-ncvar_get(Model, "lon") #Works fine
> latitude<-ncvar_get(Model, "lat") #Works fine
> time<-ncvar_get(Model, "time") #Error in nc$dim[[idobj$list_index]] :
> attempt to select more than one element in get1index <real>
>
>
> Here is the structure of the file after using "nc_open":
> File MaxPrecCCCMACanESM2rcp45.nc (NC_FORMAT_NETCDF4):
>
>      3 variables (excluding dimension variables):
>         double onedaymax[lon,lat,time]   (Contiguous storage)
>             units: mm/day
>         double fivedaymax[lon,lat,time]   (Contiguous storage)
>             units: mm/day
>         short Year[time]   (Contiguous storage)
>
>      3 dimensions:
>         time  Size:95
>         lat  Size:64
>             units: degree North
>         lon  Size:128
>             units: degree East
> What could be causing this error?
> Thanks in advance, and I look forward to your response!
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
David W. Pierce
Division of Climate, Atmospheric Science, and Physical Oceanography
Mail Stop 0230
Scripps Institution of Oceanography, La Jolla, California, USA
(858) 534-8276 (voice)  /  (858) 534-8561 (fax)    dpierce at ucsd.edu

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Mar 17 03:08:29 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 16 Mar 2019 19:08:29 -0700
Subject: [R] plot.xmean.ordinaly vs plot() in package "rms"
In-Reply-To: <CAHn6EJ3LF286PCWaGPJ0tdk8wXSwejmJrsygzWtHeL2qyoUErw@mail.gmail.com>
References: <CAHn6EJ3LF286PCWaGPJ0tdk8wXSwejmJrsygzWtHeL2qyoUErw@mail.gmail.com>
Message-ID: <031F9B89-F684-4735-A9B2-2AAD35942C91@dcn.davis.ca.us>

Read up on S3 object orientation[1]. If you have an object x of class "xmean.ordinaly" then writing

plot(x)

will end up invoking the plot.xmean.ordinaly function rather than the plot.default function in base graphics. This is broadly true throughout R.

[1] http://adv-r.had.co.nz/S3.html

On March 16, 2019 11:03:06 AM PDT, Kim Jacobsen <kimsjacobsen at gmail.com> wrote:
>Would anyone be able to explain what the difference is between
>plot.xmean.ordinaly and plot() in the "rms" package? (for the purposes
>of
>testing the proportional odds assumption in ordinal models). In the
>package
>document (https://cran.r-project.org/web/packages/rms/rms.pdf) they
>seem
>both to be used interchangeably.
>
>Thank you!

-- 
Sent from my phone. Please excuse my brevity.


From B|||@Po||ng @end|ng |rom ze||@@com  Sun Mar 17 14:34:05 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Sun, 17 Mar 2019 13:34:05 +0000
Subject: [R] Help with gsub function
In-Reply-To: <C923414B-6C57-4CE4-B2A5-DD5641B28B4A@dcn.davis.ca.us>
References: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
 <C923414B-6C57-4CE4-B2A5-DD5641B28B4A@dcn.davis.ca.us>
Message-ID: <BN7PR02MB50739701E8D3462985A91D79EA460@BN7PR02MB5073.namprd02.prod.outlook.com>

Yep, thank you Jeff, consequence of the first url I landed on asking how to do it and rushing off.

All set now.

Appreciate your help.

WHP

From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Friday, March 15, 2019 4:00 PM
To: r-help at r-project.org; Bill Poling <Bill.Poling at zelis.com>; r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Help with gsub function

Your pattern seems ... way overboard? Why not

gsub("-", "", tb2a$TID)

On March 15, 2019 12:45:27 PM PDT, Bill Poling <mailto:Bill.Poling at zelis.com> wrote:
>Good afternoon.
>
>sessionInfo()
>#R version 3.5.3 (2019-03-11)
>#Platform: x86_64-w64-mingw32/x64 (64-bit)
>#Running under: Windows >= 8 x64 (build 9200)
>
>I am using gsub function to remove a hyphen in a 9 character column of
>values in order to convert it to integer.
>
>Works fine except where the second segment has a leading 0, then it is
>eliminating the 0
>
>Example "73-0700090" becomes " 73700090"
> "77-0633896" becomes "77633896"
>
>Is there a remedy for this?
>
>tb2a$TID2 <-
>
>head(tb2a$TID,n=10)
>[1] "11-1352310" "45-2711804" "35-6001540" "77-0633896" "62-1762545"
>"61-1029768" "73-0700090" "47-0376604" "47-0486026" "38-3833117"
>> head(tb2a$TID2,n=10)
>[1] "111352310" "452711804" "356001540" "77633896" "621762545"
>"611029768" "73700090" "47376604" "47486026" "383833117"
>
>I have googled the problem and have not found a solution.
>
>http://www.endmemo.com/program/R/gsub.php
>http://r.789695.n4.nabble.com/extracting-characters-from-string-td3298971.html
>
>
>Thank you.
>
>WHP
>
>Confidentiality Notice This message is sent from Zelis.
>...{{dropped:13}}
>
>______________________________________________
>mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

Confidentiality Notice This message is sent from Zelis. This transmission may contain information which is privileged and confidential and is intended for the personal and confidential use of the named recipient only. Such information may be protected by applicable State and Federal laws from this disclosure or unauthorized use. If the reader of this message is not the intended recipient, or the employee or agent responsible for delivering the message to the intended recipient, you are hereby notified that any disclosure, review, discussion, copying, or taking any action in reliance on the contents of this transmission is strictly prohibited. If you have received this transmission in error, please contact the sender immediately. Zelis, 2018.

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Mar 17 15:38:33 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 17 Mar 2019 07:38:33 -0700
Subject: [R] plot.xmean.ordinaly vs plot() in package "rms"
In-Reply-To: <CAHn6EJ2=jLakgdMazEfwvUQ=X0JbjhjoEL4os3iR8wvFeMO2Kw@mail.gmail.com>
References: <CAHn6EJ3LF286PCWaGPJ0tdk8wXSwejmJrsygzWtHeL2qyoUErw@mail.gmail.com>
 <031F9B89-F684-4735-A9B2-2AAD35942C91@dcn.davis.ca.us>
 <CAHn6EJ2=jLakgdMazEfwvUQ=X0JbjhjoEL4os3iR8wvFeMO2Kw@mail.gmail.com>
Message-ID: <7D856F8B-80C7-48BB-BF37-BF17D0204A5F@dcn.davis.ca.us>

Please keep the mailing list included in the thread.

I can't tell if you do understand and are just being sloppy, or if you are completely confused, because xmean.ordinaly() and plot.xmean.ordinaly() are two completely different symbols in R.

As for being "safe"... you may choose to be specific or not, but plot and plot.xmean.ordinaly are both equally "safe" to call, and being too specific can cause problems sometimes as well.

On March 17, 2019 6:40:10 AM PDT, Kim Jacobsen <kimsjacobsen at gmail.com> wrote:
>Dear Jeff,
>
>Thank you so much! So if I understand the S3 object documents
>correctly,
>the xmean.ordinaly() command and plot() command are interchangeable as
>long
>as x is an object x of class "xmean.ordinaly"? So would I be right to
>think
>that I might as well just xmean.ordinaly() to be safe?
>
>Many thanks,
>
>
>
>On Sun, 17 Mar 2019 at 02:08, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> Read up on S3 object orientation[1]. If you have an object x of class
>> "xmean.ordinaly" then writing
>>
>> plot(x)
>>
>> will end up invoking the plot.xmean.ordinaly function rather than the
>> plot.default function in base graphics. This is broadly true
>throughout R.
>>
>> [1] http://adv-r.had.co.nz/S3.html
>>
>> On March 16, 2019 11:03:06 AM PDT, Kim Jacobsen
><kimsjacobsen at gmail.com>
>> wrote:
>> >Would anyone be able to explain what the difference is between
>> >plot.xmean.ordinaly and plot() in the "rms" package? (for the
>purposes
>> >of
>> >testing the proportional odds assumption in ordinal models). In the
>> >package
>> >document (https://cran.r-project.org/web/packages/rms/rms.pdf) they
>> >seem
>> >both to be used interchangeably.
>> >
>> >Thank you!
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From r@i@1290 m@iii@g oii @im@com  Sun Mar 17 23:53:47 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sun, 17 Mar 2019 22:53:47 +0000 (UTC)
Subject: [R] Weighted spatial averages across grid cells in NetCDF file
References: <370687267.5819070.1552863227388.ref@mail.yahoo.com>
Message-ID: <370687267.5819070.1552863227388@mail.yahoo.com>

Hi there,
I am currently working on a project that involves climate model data stored in a NetCDF file. I am currently trying to calculate "weighted" spatial annual "global" averages for precipitation. I need to do this for each of the 95 years of global precipitation data that I have. The idea would be to somehow apply weights to each grid cell by using the cosine of its latitude (which means latitude grid cells at the equator would have a weight of 1 (i.e. the cosine of 0 degrees is 1), and the poles would have a value of 1 (as the cosine of 90 is 1)). Then, I would be in a position to calculate annual weighted averages based on averaging each grid cell.?
I have an idea how to do this conceptually, but I am not sure where to begin writing a script in R to apply the weights across all grid cells and then average these for each of the 95 years. I would greatly appreciate any help with this, or any resources that may be helpful!!!
At the very least, I have opened the .nc file and read-in the NetCDF variables, as shown below:
ncfname<-"MaxPrecCCCMACanESM2rcp45.nc"
Prec<-raster(ncfname)
print(Prec)
Model<-nc_open(ncfname)
get<-ncvar_get(Model,"onedaymax")longitude<-ncvar_get(Model, "lon")
latitude<-ncvar_get(Model, "lat")
Year<-ncvar_get(Model, "Year")

Also, if it helps, here is what the .nc file contains:

3 variables (excluding dimension variables):
        double onedaymax[lon,lat,time]   (Contiguous storage)  
            units: mm/day
        double fivedaymax[lon,lat,time]   (Contiguous storage)  
            units: mm/day
        short Year[time]   (Contiguous storage)  

     3 dimensions:
        time  Size:95
        lat  Size:64
            units: degree North
        lon  Size:128
            units: degree East
Again, any assistance would be extremely valuable with this! I look forward to your response!
	[[alternative HTML version deleted]]


From S@E|||@on @end|ng |rom LGCGroup@com  Mon Mar 18 13:31:50 2019
From: S@E|||@on @end|ng |rom LGCGroup@com (S Ellison)
Date: Mon, 18 Mar 2019 12:31:50 +0000
Subject: [R] Help with gsub function
In-Reply-To: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
References: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
Message-ID: <5b1c2ac3402b43aa97e9b86439d7bbf6@GBDCVPEXC04.corp.lgc-group.com>

> tb2a$TID2 <- gsub(tb2a$TID, pattern="-[0-0]{0,7}", replacement = "")

Just to add something on why this didn't work ...

It looks like you were trying to match a hyphen followed by a number up to seven digits. by mistake(?) you gave the digit range as [0-0] so it would repmatch a hyphen followed by between none and seven zeroes. When it met "-0" it matched that.
And because it was gsub, it replaced what it matched.
If you'd given it the right digit range it would have replaced the whole of the number.

If you _really_ wanted to do that kind of thing (control the following pattern), you'd have needed something like (untested)
gsub("-([0-0]{0,7})", "\\1", tb2a$TID)

#The () means 'remember this bit"; the "\\1" means "put the first thing you remember here". And it needs to be "\\1" because that becomes "\1" for the grep parser.


Steve E


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From B|||@Po||ng @end|ng |rom ze||@@com  Mon Mar 18 15:04:33 2019
From: B|||@Po||ng @end|ng |rom ze||@@com (Bill Poling)
Date: Mon, 18 Mar 2019 14:04:33 +0000
Subject: [R] Help with gsub function
In-Reply-To: <5b1c2ac3402b43aa97e9b86439d7bbf6@GBDCVPEXC04.corp.lgc-group.com>
References: <BN7PR02MB5073FEF9B6661497FEF0786BEA440@BN7PR02MB5073.namprd02.prod.outlook.com>
 <5b1c2ac3402b43aa97e9b86439d7bbf6@GBDCVPEXC04.corp.lgc-group.com>
Message-ID: <BN7PR02MB5073152187561F8B08D45038EA470@BN7PR02MB5073.namprd02.prod.outlook.com>

Good morning Steve. Terrific, so kind of you to follow-up.

I will add that to my ever growing R bag of tips and tricks.

Cheers.

WHP

William H. Poling, Ph.D., MPH | Manager, Revenue Development
Data Intelligence & Analytics
Zelis Healthcare


-----Original Message-----
From: S Ellison <S.Ellison at LGCGroup.com>
Sent: Monday, March 18, 2019 8:32 AM
To: Bill Poling <Bill.Poling at zelis.com>; r-help (r-help at r-project.org) <r-help at r-project.org>
Subject: RE: Help with gsub function

> tb2a$TID2 <- gsub(tb2a$TID, pattern="-[0-0]{0,7}", replacement = "")

Just to add something on why this didn't work ...

It looks like you were trying to match a hyphen followed by a number up to seven digits. by mistake(?) you gave the digit range as [0-0] so it would repmatch a hyphen followed by between none and seven zeroes. When it met "-0" it matched that.
And because it was gsub, it replaced what it matched.
If you'd given it the right digit range it would have replaced the whole of the number.

If you _really_ wanted to do that kind of thing (control the following pattern), you'd have needed something like (untested) gsub("-([0-0]{0,7})", "\\1", tb2a$TID)

#The () means 'remember this bit"; the "\\1" means "put the first thing you remember here". And it needs to be "\\1" because that becomes "\1" for the grep parser.


Steve E


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:22}}


From ev@n@cooch @end|ng |rom gm@||@com  Mon Mar 18 00:39:54 2019
From: ev@n@cooch @end|ng |rom gm@||@com (Evan Cooch)
Date: Sun, 17 Mar 2019 19:39:54 -0400
Subject: [R] looking for 'tied rows' in dataframe
Message-ID: <4444b3d0-9fe1-dc11-5ddb-c454eb1ab12a@gmail.com>

Suppose I have the following sort of structure:

test <- matrix(c(2,1,1,2,2,2),3,2,byrow=T)

What I need to be able to do is (i) find the maximum value for each row, 
(ii) find the column containing the max, but (iii) if the maximum value 
is a tie (in this case, all numbers of the row are the same value), then 
I want which.max (presumably, a tweaked version of what which.max does) 
to reurn a T for the row where all values are the same.

Parts (i) and (ii) seem easy enough:

apply(test,1,max)? --- gives me the maximum values
apply(test,1,which.max) --- gives me the column

But, standard which.max doesn't handles ties/duplicates in a way that 
serves my need. It defaults to returning the first column containing the 
maximum value.

What I'd like to end up with is, ultimately, something where 
apply(test,1,which.max) yields 1,2,T? (rather than 1,2,1).

So, a function which does what which.max currently does if the elements 
of the row differ, but which returns a T (or some such) if in fact the 
row values are all the same.

I've tried a bunch of things, to know avail. Closest I got was to use a 
function to test for whether or not a vector

isUnique <- function(vector){
 ???????????????? return(!any(duplicated(vector)))
 ??????????? }

which returns TRUE if values of vector all unique. So

apply(test,1,isUnique)

returns

[1]? TRUE? TRUE FALSE

but I'm stuck beyond this.? Suggestions/pointers to the obvious welcome.

Thanks in advance.


From ev@n@cooch @end|ng |rom gm@||@com  Mon Mar 18 01:00:27 2019
From: ev@n@cooch @end|ng |rom gm@||@com (Evan Cooch)
Date: Sun, 17 Mar 2019 20:00:27 -0400
Subject: [R] looking for 'tied rows' in dataframe
In-Reply-To: <4444b3d0-9fe1-dc11-5ddb-c454eb1ab12a@gmail.com>
References: <4444b3d0-9fe1-dc11-5ddb-c454eb1ab12a@gmail.com>
Message-ID: <746f42e7-54a3-d8db-77f0-e73893510baa@gmail.com>

Got relatively close - below:

On 3/17/2019 7:39 PM, Evan Cooch wrote:
> Suppose I have the following sort of structure:
>
> test <- matrix(c(2,1,1,2,2,2),3,2,byrow=T)
>
> What I need to be able to do is (i) find the maximum value for each 
> row, (ii) find the column containing the max, but (iii) if the maximum 
> value is a tie (in this case, all numbers of the row are the same 
> value), then I want which.max (presumably, a tweaked version of what 
> which.max does) to reurn a T for the row where all values are the same.
>
> Parts (i) and (ii) seem easy enough:
>
> apply(test,1,max)? --- gives me the maximum values
> apply(test,1,which.max) --- gives me the column
>
> But, standard which.max doesn't handles ties/duplicates in a way that 
> serves my need. It defaults to returning the first column containing 
> the maximum value.
>
> What I'd like to end up with is, ultimately, something where 
> apply(test,1,which.max) yields 1,2,T? (rather than 1,2,1).
>
> So, a function which does what which.max currently does if the 
> elements of the row differ, but which returns a T (or some such) if in 
> fact the row values are all the same.
>
> I've tried a bunch of things, to know avail. Closest I got was to use 
> a function to test for whether or not a vector
>
> isUnique <- function(vector){
> ???????????????? return(!any(duplicated(vector)))
> ??????????? }
>
> which returns TRUE if values of vector all unique. So
>
> apply(test,1,isUnique)
>
> returns
>
> [1]? TRUE? TRUE FALSE
>
> but I'm stuck beyond this. 

The following gets me pretty close,

test_new <- test
test_new[which(apply(test,1,isUnique)==FALSE),] <- 'T'

but is clunky.


From ev@n@cooch @end|ng |rom gm@||@com  Mon Mar 18 01:17:13 2019
From: ev@n@cooch @end|ng |rom gm@||@com (Evan Cooch)
Date: Sun, 17 Mar 2019 20:17:13 -0400
Subject: [R] looking for 'tied rows' in dataframe
In-Reply-To: <746f42e7-54a3-d8db-77f0-e73893510baa@gmail.com>
References: <4444b3d0-9fe1-dc11-5ddb-c454eb1ab12a@gmail.com>
 <746f42e7-54a3-d8db-77f0-e73893510baa@gmail.com>
Message-ID: <d560fef6-a0c8-0802-91f5-afc3efcd3204@gmail.com>

Solved --

hold=apply(test,1,which.max)
 ??? hold[apply(test,1,isUnique)==FALSE] <- 'T'

Now, all I need to do is figure out how to get <- 'T' from turning 
everything in the matrix to a string.


On 3/17/2019 8:00 PM, Evan Cooch wrote:
> Got relatively close - below:
>
> On 3/17/2019 7:39 PM, Evan Cooch wrote:
>> Suppose I have the following sort of structure:
>>
>> test <- matrix(c(2,1,1,2,2,2),3,2,byrow=T)
>>
>> What I need to be able to do is (i) find the maximum value for each 
>> row, (ii) find the column containing the max, but (iii) if the 
>> maximum value is a tie (in this case, all numbers of the row are the 
>> same value), then I want which.max (presumably, a tweaked version of 
>> what which.max does) to reurn a T for the row where all values are 
>> the same.
>>
>> Parts (i) and (ii) seem easy enough:
>>
>> apply(test,1,max)? --- gives me the maximum values
>> apply(test,1,which.max) --- gives me the column
>>
>> But, standard which.max doesn't handles ties/duplicates in a way that 
>> serves my need. It defaults to returning the first column containing 
>> the maximum value.
>>
>> What I'd like to end up with is, ultimately, something where 
>> apply(test,1,which.max) yields 1,2,T? (rather than 1,2,1).
>>
>> So, a function which does what which.max currently does if the 
>> elements of the row differ, but which returns a T (or some such) if 
>> in fact the row values are all the same.
>>
>> I've tried a bunch of things, to know avail. Closest I got was to use 
>> a function to test for whether or not a vector
>>
>> isUnique <- function(vector){
>> ???????????????? return(!any(duplicated(vector)))
>> ??????????? }
>>
>> which returns TRUE if values of vector all unique. So
>>
>> apply(test,1,isUnique)
>>
>> returns
>>
>> [1]? TRUE? TRUE FALSE
>>
>> but I'm stuck beyond this. 
>
> The following gets me pretty close,
>
> test_new <- test
> test_new[which(apply(test,1,isUnique)==FALSE),] <- 'T'
>
> but is clunky.
>
>
>
>
>
>


From w||||@m@@one@ @end|ng |rom ndorm@@ox@@c@uk  Mon Mar 18 17:35:17 2019
From: w||||@m@@one@ @end|ng |rom ndorm@@ox@@c@uk (William Sones)
Date: Mon, 18 Mar 2019 16:35:17 +0000
Subject: [R] ?grid::grid-package doesn't find the grid-package help page
Message-ID: <CWLP265MB1571527D3D40C0FEAA13A808E7470@CWLP265MB1571.GBRP265.PROD.OUTLOOK.COM>

Hi

I've entered the command "?grid::grid-package" on two computers over here and they both direct me to the Arithmetic {base} site (http://127.0.0.1:30753/library/base/html/Arithmetic.html).

Shouldn't this command direct me to the grid-package {grid} site (something like https://stat.ethz.ch/R-manual/R-devel/library/grid/html/grid-package.html)?

This has been performed on R versions 3.5.2 and 3.5.3 (Windows).

I couldn't find mention of this in Bugzilla, so thought it best to confirm whether this warrants documentation as a bug, or whether alternative methods are preferred for addressing similar (minor) issues.

Thanks

Will

William Sones
Medical Statistician

Email: william.sones at ndorms.ox.ac.uk<mailto:william.sones at ndorms.ox.ac.uk>
Tel: 01865 227887

Oxford Clinical Trials Research Unit
Botnar Research Centre
Nuffield Department of Orthopaedics, Rheumatology & Musculoskeletal Sciences
University of Oxford
Windmill Road
Oxford, OX3 7LD


	[[alternative HTML version deleted]]


From btupper @end|ng |rom b|ge|ow@org  Mon Mar 18 17:37:01 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Mon, 18 Mar 2019 12:37:01 -0400
Subject: [R] looking for 'tied rows' in dataframe
In-Reply-To: <d560fef6-a0c8-0802-91f5-afc3efcd3204@gmail.com>
References: <4444b3d0-9fe1-dc11-5ddb-c454eb1ab12a@gmail.com>
 <746f42e7-54a3-d8db-77f0-e73893510baa@gmail.com>
 <d560fef6-a0c8-0802-91f5-afc3efcd3204@gmail.com>
Message-ID: <A1693598-E049-4E83-A7F1-79B959B31C4F@bigelow.org>

Hi,

Might you replaced 'T' with a numeric value that signals the TRUE case without rumpling your matrix?  0 might be a good choice as it is never an index for a 1-based indexing system.

hold=apply(test,1,which.max)
hold[apply(test,1,isUnique)==FALSE] <- 0
hold
[1] 1 2 0
 


> On Mar 17, 2019, at 8:17 PM, Evan Cooch <evan.cooch at gmail.com> wrote:
> 
> Solved --
> 
> hold=apply(test,1,which.max)
>     hold[apply(test,1,isUnique)==FALSE] <- 'T'
> 
> Now, all I need to do is figure out how to get <- 'T' from turning everything in the matrix to a string.
> 
> 
> On 3/17/2019 8:00 PM, Evan Cooch wrote:
>> Got relatively close - below:
>> 
>> On 3/17/2019 7:39 PM, Evan Cooch wrote:
>>> Suppose I have the following sort of structure:
>>> 
>>> test <- matrix(c(2,1,1,2,2,2),3,2,byrow=T)
>>> 
>>> What I need to be able to do is (i) find the maximum value for each row, (ii) find the column containing the max, but (iii) if the maximum value is a tie (in this case, all numbers of the row are the same value), then I want which.max (presumably, a tweaked version of what which.max does) to reurn a T for the row where all values are the same.
>>> 
>>> Parts (i) and (ii) seem easy enough:
>>> 
>>> apply(test,1,max)  --- gives me the maximum values
>>> apply(test,1,which.max) --- gives me the column
>>> 
>>> But, standard which.max doesn't handles ties/duplicates in a way that serves my need. It defaults to returning the first column containing the maximum value.
>>> 
>>> What I'd like to end up with is, ultimately, something where apply(test,1,which.max) yields 1,2,T  (rather than 1,2,1).
>>> 
>>> So, a function which does what which.max currently does if the elements of the row differ, but which returns a T (or some such) if in fact the row values are all the same.
>>> 
>>> I've tried a bunch of things, to know avail. Closest I got was to use a function to test for whether or not a vector
>>> 
>>> isUnique <- function(vector){
>>>                  return(!any(duplicated(vector)))
>>>             }
>>> 
>>> which returns TRUE if values of vector all unique. So
>>> 
>>> apply(test,1,isUnique)
>>> 
>>> returns
>>> 
>>> [1]  TRUE  TRUE FALSE
>>> 
>>> but I'm stuck beyond this. 
>> 
>> The following gets me pretty close,
>> 
>> test_new <- test
>> test_new[which(apply(test,1,isUnique)==FALSE),] <- 'T'
>> 
>> but is clunky.
>> 
>> 
>> 
>> 
>> 
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Mar 18 18:00:35 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 18 Mar 2019 10:00:35 -0700
Subject: [R] ?grid::grid-package doesn't find the grid-package help page
In-Reply-To: <CWLP265MB1571527D3D40C0FEAA13A808E7470@CWLP265MB1571.GBRP265.PROD.OUTLOOK.COM>
References: <CWLP265MB1571527D3D40C0FEAA13A808E7470@CWLP265MB1571.GBRP265.PROD.OUTLOOK.COM>
Message-ID: <B5A08121-A86A-4048-A0EF-9CC52B6646B5@dcn.davis.ca.us>

Try

?grid::`grid-package`

The "-" is not a legal character in a bare symbol.

On March 18, 2019 9:35:17 AM PDT, William Sones <william.sones at ndorms.ox.ac.uk> wrote:
>Hi
>
>I've entered the command "?grid::grid-package" on two computers over
>here and they both direct me to the Arithmetic {base} site
>(http://127.0.0.1:30753/library/base/html/Arithmetic.html).
>
>Shouldn't this command direct me to the grid-package {grid} site
>(something like
>https://stat.ethz.ch/R-manual/R-devel/library/grid/html/grid-package.html)?
>
>This has been performed on R versions 3.5.2 and 3.5.3 (Windows).
>
>I couldn't find mention of this in Bugzilla, so thought it best to
>confirm whether this warrants documentation as a bug, or whether
>alternative methods are preferred for addressing similar (minor)
>issues.
>
>Thanks
>
>Will
>
>William Sones
>Medical Statistician
>
>Email:
>william.sones at ndorms.ox.ac.uk<mailto:william.sones at ndorms.ox.ac.uk>
>Tel: 01865 227887
>
>Oxford Clinical Trials Research Unit
>Botnar Research Centre
>Nuffield Department of Orthopaedics, Rheumatology & Musculoskeletal
>Sciences
>University of Oxford
>Windmill Road
>Oxford, OX3 7LD
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Mar 18 20:45:04 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 18 Mar 2019 15:45:04 -0400
Subject: [R] ?grid::grid-package doesn't find the grid-package help page
In-Reply-To: <B5A08121-A86A-4048-A0EF-9CC52B6646B5@dcn.davis.ca.us>
References: <CWLP265MB1571527D3D40C0FEAA13A808E7470@CWLP265MB1571.GBRP265.PROD.OUTLOOK.COM>
 <B5A08121-A86A-4048-A0EF-9CC52B6646B5@dcn.davis.ca.us>
Message-ID: <432f36be-65e1-19b2-3d45-b52ce65149a8@gmail.com>

On 18/03/2019 1:00 p.m., Jeff Newmiller wrote:
> Try
> 
> ?grid::`grid-package`
> 
> The "-" is not a legal character in a bare symbol.

Or

package?grid::grid

which makes use of the rarely used "type" argument to "?".

Duncan Murdoch

> 
> On March 18, 2019 9:35:17 AM PDT, William Sones <william.sones at ndorms.ox.ac.uk> wrote:
>> Hi
>>
>> I've entered the command "?grid::grid-package" on two computers over
>> here and they both direct me to the Arithmetic {base} site
>> (http://127.0.0.1:30753/library/base/html/Arithmetic.html).
>>
>> Shouldn't this command direct me to the grid-package {grid} site
>> (something like
>> https://stat.ethz.ch/R-manual/R-devel/library/grid/html/grid-package.html)?
>>
>> This has been performed on R versions 3.5.2 and 3.5.3 (Windows).
>>
>> I couldn't find mention of this in Bugzilla, so thought it best to
>> confirm whether this warrants documentation as a bug, or whether
>> alternative methods are preferred for addressing similar (minor)
>> issues.
>>
>> Thanks
>>
>> Will
>>
>> William Sones
>> Medical Statistician
>>
>> Email:
>> william.sones at ndorms.ox.ac.uk<mailto:william.sones at ndorms.ox.ac.uk>
>> Tel: 01865 227887
>>
>> Oxford Clinical Trials Research Unit
>> Botnar Research Centre
>> Nuffield Department of Orthopaedics, Rheumatology & Musculoskeletal
>> Sciences
>> University of Oxford
>> Windmill Road
>> Oxford, OX3 7LD
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From w||||@m@@one@ @end|ng |rom ndorm@@ox@@c@uk  Mon Mar 18 18:45:53 2019
From: w||||@m@@one@ @end|ng |rom ndorm@@ox@@c@uk (William Sones)
Date: Mon, 18 Mar 2019 17:45:53 +0000
Subject: [R] ?grid::grid-package doesn't find the grid-package help page
In-Reply-To: <B5A08121-A86A-4048-A0EF-9CC52B6646B5@dcn.davis.ca.us>
References: <CWLP265MB1571527D3D40C0FEAA13A808E7470@CWLP265MB1571.GBRP265.PROD.OUTLOOK.COM>
 <B5A08121-A86A-4048-A0EF-9CC52B6646B5@dcn.davis.ca.us>
Message-ID: <CWLP265MB1571EA89A8BC4D9039B32643E7470@CWLP265MB1571.GBRP265.PROD.OUTLOOK.COM>

To quote Homer Simpson "Doh!".

Thank you Jeff. ?grid::`grid-package`  works perfectly.

Thanks

Will

-----Original Message-----
From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.ca.us] 
Sent: 18 March 2019 17:01
To: r-help at r-project.org; William Sones <william.sones at ndorms.ox.ac.uk>; r-help at R-project.org
Subject: Re: [R] ?grid::grid-package doesn't find the grid-package help page

Try

?grid::`grid-package`

The "-" is not a legal character in a bare symbol.

On March 18, 2019 9:35:17 AM PDT, William Sones <william.sones at ndorms.ox.ac.uk> wrote:
>Hi
>
>I've entered the command "?grid::grid-package" on two computers over 
>here and they both direct me to the Arithmetic {base} site 
>(http://127.0.0.1:30753/library/base/html/Arithmetic.html).
>
>Shouldn't this command direct me to the grid-package {grid} site 
>(something like 
>https://stat.ethz.ch/R-manual/R-devel/library/grid/html/grid-package.html)?
>
>This has been performed on R versions 3.5.2 and 3.5.3 (Windows).
>
>I couldn't find mention of this in Bugzilla, so thought it best to 
>confirm whether this warrants documentation as a bug, or whether 
>alternative methods are preferred for addressing similar (minor) 
>issues.
>
>Thanks
>
>Will
>
>William Sones
>Medical Statistician
>
>Email:
>william.sones at ndorms.ox.ac.uk<mailto:william.sones at ndorms.ox.ac.uk>
>Tel: 01865 227887
>
>Oxford Clinical Trials Research Unit
>Botnar Research Centre
>Nuffield Department of Orthopaedics, Rheumatology & Musculoskeletal 
>Sciences University of Oxford Windmill Road Oxford, OX3 7LD
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.

From motyoc@k@ @end|ng |rom y@hoo@com  Tue Mar 19 12:06:22 2019
From: motyoc@k@ @end|ng |rom y@hoo@com (Andras Farkas)
Date: Tue, 19 Mar 2019 11:06:22 +0000 (UTC)
Subject: [R] data frame solution
References: <1314762859.7288924.1552993582101.ref@mail.yahoo.com>
Message-ID: <1314762859.7288924.1552993582101@mail.yahoo.com>

Hello All,

wonder if you have thoughts on a clever solution for this code:



df? ? ? ?<- data.frame(a = c(6,1), b = c(1000,1200), c =c(-1,3))?

#the caveat here is that the number of rows for df can be anything from 1 row to in the hundreds. I kept it to 2 to have minimal reproducible

t<-seq(-5,24,0.1) #min(t) will always be <=df$c[1], which is the value that is always going to equal to min(df$c)

times1 <- c(rbind(df$c[1],df$c[1]+df$a[1]),max(t)) #length of times1 will always be 3, see times2 is of length 4

input1? ?<- c(rbind(df$b[1]/df$a[1],rep(0,length(df$b[1]))),0)?#length of input1 will always be 3, see input2 is of length 4

out1 <-data.frame(t,ifelse(t>=times1[1]&t<times1[2],input1[1],ifelse(t>=times1[2]&t<times1[3],input1[2],input1[3])))

times2 <- c(times1[1],rbind(df$c[2],df$c[2]+df$a[2]),max(t)) #note 1st value of times2, (or for times3, times4,.. if more rows in df) is times1[1], which will always be < times2[1]?(or times3[1], times4[1],.. if more rows in df)?

input2? ?<- c(0,rbind(df$b[2]/df$a[2],rep(0,length(df$b[2]))),0)?#note 1st value of input2??(or for input3, input4,.. if more rows in df)? is 0, which will always be 0 (or?for all input2-n other then for input1 as that will be as above?,.. if more rows in df)

out2 <-data.frame(t,ifelse(t>=times2[1]&t<times2[2],input2[1],ifelse(t>=times2[2]&t<times2[3],input2[2],ifelse(t>=times2[3]&t<times2[4],input2[3],input2[3]))))

result<-data.frame(t,out1[,2]+out2[,2])

so if I did it all manually then for row 3 in df I would calculate out3 and so on... Would like to be able to do this with a clean function solution that allows for different row numbers in df...

as always your help is appreciated?

thanks? ? ? ? ? ? ? ? ?

Andras?


From @k@h@y_e4 @end|ng |rom hotm@||@com  Tue Mar 19 13:13:20 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Tue, 19 Mar 2019 12:13:20 +0000
Subject: [R] problem with nlsLM function
Message-ID: <SL2P216MB00917C5389DFC4F8C2192EC0C8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members,
                    I am getting the "singular gradient error" when I use nls for a function of two variables:
> formulaDH5
HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) + (A * sin(w * HM3 + a) +
    C)

HM1 is the response variable, and HM2 and HM3 are predictors.

The problem is I get the same error even when I use nlsLM(in the minpack.lm package):

> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 0.43143, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 0.94))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates
> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 1, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 0.94))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates
> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 1, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 2))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates

I came to know that nlsLM converges when nls throws a singular gradient error. What is happening above? Can the problem get  solved if I use nls.lm function(in the minpack.lm package) instead?

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Tue Mar 19 13:26:09 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Tue, 19 Mar 2019 12:26:09 +0000
Subject: [R] Fw: problem with nlsLM function
In-Reply-To: <SL2P216MB00917C5389DFC4F8C2192EC0C8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB00917C5389DFC4F8C2192EC0C8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <SL2P216MB00915CC9D0AA921B3E83B44CC8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>


dear members,
                            also,I can provide HM1,HM2 and HM3 if needed....

________________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
Sent: Tuesday, March 19, 2019 5:43 PM
To: R help Mailing  list
Subject: [R] problem with nlsLM function

dear members,
                    I am getting the "singular gradient error" when I use nls for a function of two variables:
> formulaDH5
HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) + (A * sin(w * HM3 + a) +
    C)

HM1 is the response variable, and HM2 and HM3 are predictors.

The problem is I get the same error even when I use nlsLM(in the minpack.lm package):

> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 0.43143, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 0.94))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates
> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 1, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 0.94))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates
> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 1, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 2))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates

I came to know that nlsLM converges when nls throws a singular gradient error. What is happening above? Can the problem get  solved if I use nls.lm function(in the minpack.lm package) instead?

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Mar 19 13:31:13 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 19 Mar 2019 08:31:13 -0400
Subject: [R] Fw: problem with nlsLM function
In-Reply-To: <SL2P216MB00915CC9D0AA921B3E83B44CC8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB00917C5389DFC4F8C2192EC0C8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <SL2P216MB00915CC9D0AA921B3E83B44CC8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <a18ffcb0-b314-300a-646f-631635c7e433@gmail.com>

On 19/03/2019 8:26 a.m., akshay kulkarni wrote:
> 
> dear members,
>                              also,I can provide HM1,HM2 and HM3 if needed....
> 
> ________________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
> Sent: Tuesday, March 19, 2019 5:43 PM
> To: R help Mailing  list
> Subject: [R] problem with nlsLM function
> 
> dear members,
>                      I am getting the "singular gradient error" when I use nls for a function of two variables:
>> formulaDH5
> HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) + (A * sin(w * HM3 + a) +
>      C)
> 
> HM1 is the response variable, and HM2 and HM3 are predictors.
> 
> The problem is I get the same error even when I use nlsLM(in the minpack.lm package):
> 
>> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 0.43143, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 0.94))

You have "a" twice in your start list.  That's bound to cause trouble...

Duncan Murdoch


> Error in nlsModel(formula, mf, start, wts) :
>    singular gradient matrix at initial parameter estimates
>> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 1, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 0.94))
> Error in nlsModel(formula, mf, start, wts) :
>    singular gradient matrix at initial parameter estimates
>> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 1, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 2))
> Error in nlsModel(formula, mf, start, wts) :
>    singular gradient matrix at initial parameter estimates
> 
> I came to know that nlsLM converges when nls throws a singular gradient error. What is happening above? Can the problem get  solved if I use nls.lm function(in the minpack.lm package) instead?
> 
> very many thanks for your time and effort....
> yours sincerely,
> AKSHAY M KULKARNI
> 
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @k@h@y_e4 @end|ng |rom hotm@||@com  Tue Mar 19 13:49:19 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Tue, 19 Mar 2019 12:49:19 +0000
Subject: [R] Fw: problem with nlsLM function
In-Reply-To: <a18ffcb0-b314-300a-646f-631635c7e433@gmail.com>
References: <SL2P216MB00917C5389DFC4F8C2192EC0C8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <SL2P216MB00915CC9D0AA921B3E83B44CC8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <a18ffcb0-b314-300a-646f-631635c7e433@gmail.com>
Message-ID: <SL2P216MB0091AE5F0458CE21B2375ED5C8400@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

Dear duncan,
                        Sorry to bother you with such a silly mistake!!!!!!!! I didn,t notice it!!!!!
Sent: Tuesday, March 19, 2019 6:01 PM
To: akshay kulkarni; R help Mailing list
Subject: Re: [R] Fw: problem with nlsLM function

On 19/03/2019 8:26 a.m., akshay kulkarni wrote:
>
> dear members,
>                              also,I can provide HM1,HM2 and HM3 if needed....
>
> ________________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
> Sent: Tuesday, March 19, 2019 5:43 PM
> To: R help Mailing  list
> Subject: [R] problem with nlsLM function
>
> dear members,
>                      I am getting the "singular gradient error" when I use nls for a function of two variables:
>> formulaDH5
> HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) + (A * sin(w * HM3 + a) +
>      C)
>
> HM1 is the response variable, and HM2 and HM3 are predictors.
>
> The problem is I get the same error even when I use nlsLM(in the minpack.lm package):
>
>> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 0.43143, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 0.94))

You have "a" twice in your start list.  That's bound to cause trouble...

Duncan Murdoch


> Error in nlsModel(formula, mf, start, wts) :
>    singular gradient matrix at initial parameter estimates
>> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 1, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 0.94))
> Error in nlsModel(formula, mf, start, wts) :
>    singular gradient matrix at initial parameter estimates
>> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 1, b = 2,A = 0.09,w = 0.8,a = 0.01,C = 2))
> Error in nlsModel(formula, mf, start, wts) :
>    singular gradient matrix at initial parameter estimates
>
> I came to know that nlsLM converges when nls throws a singular gradient error. What is happening above? Can the problem get  solved if I use nls.lm function(in the minpack.lm package) instead?
>
> very many thanks for your time and effort....
> yours sincerely,
> AKSHAY M KULKARNI
>
>          [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


	[[alternative HTML version deleted]]


From w||||@m@@one@ @end|ng |rom ndorm@@ox@@c@uk  Tue Mar 19 11:29:28 2019
From: w||||@m@@one@ @end|ng |rom ndorm@@ox@@c@uk (William Sones)
Date: Tue, 19 Mar 2019 10:29:28 +0000
Subject: [R] ?grid::grid-package doesn't find the grid-package help page
In-Reply-To: <432f36be-65e1-19b2-3d45-b52ce65149a8@gmail.com>
References: <CWLP265MB1571527D3D40C0FEAA13A808E7470@CWLP265MB1571.GBRP265.PROD.OUTLOOK.COM>
 <B5A08121-A86A-4048-A0EF-9CC52B6646B5@dcn.davis.ca.us>
 <432f36be-65e1-19b2-3d45-b52ce65149a8@gmail.com>
Message-ID: <CWLP265MB15716B9B6ECC8895DE49E461E7400@CWLP265MB1571.GBRP265.PROD.OUTLOOK.COM>

Wow! I've never come across this approach before. This will keep me reading for a good few days. 

Thanks Duncan!

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com] 
Sent: 18 March 2019 19:45
To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>; r-help at r-project.org; William Sones <william.sones at ndorms.ox.ac.uk>
Subject: Re: [R] ?grid::grid-package doesn't find the grid-package help page

On 18/03/2019 1:00 p.m., Jeff Newmiller wrote:
> Try
> 
> ?grid::`grid-package`
> 
> The "-" is not a legal character in a bare symbol.

Or

package?grid::grid

which makes use of the rarely used "type" argument to "?".

Duncan Murdoch

> 
> On March 18, 2019 9:35:17 AM PDT, William Sones <william.sones at ndorms.ox.ac.uk> wrote:
>> Hi
>>
>> I've entered the command "?grid::grid-package" on two computers over 
>> here and they both direct me to the Arithmetic {base} site 
>> (http://127.0.0.1:30753/library/base/html/Arithmetic.html).
>>
>> Shouldn't this command direct me to the grid-package {grid} site 
>> (something like 
>> https://stat.ethz.ch/R-manual/R-devel/library/grid/html/grid-package.html)?
>>
>> This has been performed on R versions 3.5.2 and 3.5.3 (Windows).
>>
>> I couldn't find mention of this in Bugzilla, so thought it best to 
>> confirm whether this warrants documentation as a bug, or whether 
>> alternative methods are preferred for addressing similar (minor) 
>> issues.
>>
>> Thanks
>>
>> Will
>>
>> William Sones
>> Medical Statistician
>>
>> Email:
>> william.sones at ndorms.ox.ac.uk<mailto:william.sones at ndorms.ox.ac.uk>
>> Tel: 01865 227887
>>
>> Oxford Clinical Trials Research Unit
>> Botnar Research Centre
>> Nuffield Department of Orthopaedics, Rheumatology & Musculoskeletal 
>> Sciences University of Oxford Windmill Road Oxford, OX3 7LD
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 


From rtch|ruk@ @end|ng |rom gm@||@com  Tue Mar 19 15:49:22 2019
From: rtch|ruk@ @end|ng |rom gm@||@com (Rayt Chiruka)
Date: Tue, 19 Mar 2019 16:49:22 +0200
Subject: [R] Obtaining individual parameters from a sitar growth model.
Message-ID: <CAGtiZ8wy3_uhASnn4AiLLzckFhd4uK=oKJGQHBwr+5jyTa55jQ@mail.gmail.com>

I want to obtain the individual biological parameters (ie age at takeoff,
final height velocity at takeoff) after a sitar model. (I do understand a
sitar model fits a mean curve for the population).



I have managed to obtain the parameters for one individual using the code
below,





library(sitar)

data(heights)

library(tidyverse)

m1 <- sitar(x=age, y=height, id=id, data=heights, df=5)



#velocity at takeoff for individual 1



(vel_attoff<-getPeakTrough(plot_v(m1, subset=age < 14 & id==1) ,
peak=FALSE))





What I want is to produce a dataframe with the id and the velocity at
takeoff for each individual in the dataset.



I have tried to do this



for (i in seq_along(heights[1])){

                print(getPeakTrough(plot_v(m1, subset=age < 14 & id==i) ,
peak=FALSE))

}



But it prints only one x and y  value



Any assistance will be greatly appreciated.


-- 
R T CHIRUKA
University of Fort Hare
Statistics Department
Box X 1314
Alice
5700
South Africa

	[[alternative HTML version deleted]]


From rtch|ruk@ @end|ng |rom gm@||@com  Tue Mar 19 16:17:23 2019
From: rtch|ruk@ @end|ng |rom gm@||@com (Rayt Chiruka)
Date: Tue, 19 Mar 2019 17:17:23 +0200
Subject: [R] help with sitar growth model
Message-ID: <CAGtiZ8wJULXP31dU6V=Nwyji5EH5Aq1quKr_Cd4WWY2DmWf6PQ@mail.gmail.com>

I want to obtain the individual biological parameters (ie age at takeoff,
final height velocity at takeoff) after a sitar model. (I do understand a
sitar model fits a mean curve for the population).

I have managed to obtain the parameters for one individual using the code
below,

      library(sitar)
      data(heights)
      library(tidyverse)

      m1 <- sitar(x=age, y=height, id=id, data=heights, df=5)

      #velocity at takeoff for individual 1

     (vel_attoff<-getPeakTrough(plot_v(m1, subset=age < 14 &id==1) ,
peak=FALSE))

What I want is to produce a dataframe with the id and the velocity at
takeoff for each individual in the dataset.
I have tried to do this

      for (i in seq_along(heights[1])){
       print(getPeakTrough(plot_v(m1, subset=age < 14 & id==i) ,
peak=FALSE))
        }

But it prints only one x and y  value

Any assistance will be greatly appreciated

-- 
R T CHIRUKA
University of Fort Hare
Statistics Department
Box X 1314
Alice
5700
South Africa

	[[alternative HTML version deleted]]


From j@vedbtk111 @end|ng |rom gm@||@com  Tue Mar 19 17:47:24 2019
From: j@vedbtk111 @end|ng |rom gm@||@com (javed khan)
Date: Tue, 19 Mar 2019 17:47:24 +0100
Subject: [R] Fwd: high p values
In-Reply-To: <CAJhui+tJH10VqvEw7duww8FkEcOCX_swO3OYs1ZqbMo-Sm98fg@mail.gmail.com>
References: <CAJhui+tJH10VqvEw7duww8FkEcOCX_swO3OYs1ZqbMo-Sm98fg@mail.gmail.com>
Message-ID: <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>

Hi

This is my function:

wilcox.test(A,B, data = data, paired = FALSE)

It gives me high p value, though the median of A column is 6900 and B
column is 3500.

Why it gives p value high if there is a difference in the median?

Regards

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Tue Mar 19 17:52:17 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Tue, 19 Mar 2019 12:52:17 -0400
Subject: [R] Fwd: high p values
In-Reply-To: <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
References: <CAJhui+tJH10VqvEw7duww8FkEcOCX_swO3OYs1ZqbMo-Sm98fg@mail.gmail.com>
 <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
Message-ID: <E69E6B1D-0CBB-460F-A0A3-79B7E68A79FD@malonequantitative.com>

We've had this conversation.

A) This is off-topic for R-Help. Your question is about the statistical test, not about the R coding.

B) A difference in sample statistics, whether or not it "looks" large, is not sufficient for statistical significance.

?On 3/19/19, 12:48 PM, "R-help on behalf of javed khan" <r-help-bounces at r-project.org on behalf of javedbtk111 at gmail.com> wrote:

    Hi
    
    This is my function:
    
    wilcox.test(A,B, data = data, paired = FALSE)
    
    It gives me high p value, though the median of A column is 6900 and B
    column is 3500.
    
    Why it gives p value high if there is a difference in the median?
    
    Regards
    
    	[[alternative HTML version deleted]]
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From ev@n@cooch @end|ng |rom gm@||@com  Tue Mar 19 17:57:51 2019
From: ev@n@cooch @end|ng |rom gm@||@com (Evan Cooch)
Date: Tue, 19 Mar 2019 12:57:51 -0400
Subject: [R] looking for 'tied rows' in dataframe
In-Reply-To: <A1693598-E049-4E83-A7F1-79B959B31C4F@bigelow.org>
References: <4444b3d0-9fe1-dc11-5ddb-c454eb1ab12a@gmail.com>
 <746f42e7-54a3-d8db-77f0-e73893510baa@gmail.com>
 <d560fef6-a0c8-0802-91f5-afc3efcd3204@gmail.com>
 <A1693598-E049-4E83-A7F1-79B959B31C4F@bigelow.org>
Message-ID: <8f609b41-7914-f6f1-6121-ca491cdb0b71@gmail.com>

Good suggestion, and for my purposes, will solve the problem. Thanks!

On 3/18/2019 12:37 PM, Ben Tupper wrote:
> Hi,
>
> Might you replaced 'T' with a numeric value that signals the TRUE case without rumpling your matrix?  0 might be a good choice as it is never an index for a 1-based indexing system.
>
> hold=apply(test,1,which.max)
> hold[apply(test,1,isUnique)==FALSE] <- 0
> hold
> [1] 1 2 0
>   
>
>
>> On Mar 17, 2019, at 8:17 PM, Evan Cooch <evan.cooch at gmail.com> wrote:
>>
>> Solved --
>>
>> hold=apply(test,1,which.max)
>>      hold[apply(test,1,isUnique)==FALSE] <- 'T'
>>
>> Now, all I need to do is figure out how to get <- 'T' from turning everything in the matrix to a string.
>>
>>
>> On 3/17/2019 8:00 PM, Evan Cooch wrote:
>>> Got relatively close - below:
>>>
>>> On 3/17/2019 7:39 PM, Evan Cooch wrote:
>>>> Suppose I have the following sort of structure:
>>>>
>>>> test <- matrix(c(2,1,1,2,2,2),3,2,byrow=T)
>>>>
>>>> What I need to be able to do is (i) find the maximum value for each row, (ii) find the column containing the max, but (iii) if the maximum value is a tie (in this case, all numbers of the row are the same value), then I want which.max (presumably, a tweaked version of what which.max does) to reurn a T for the row where all values are the same.
>>>>
>>>> Parts (i) and (ii) seem easy enough:
>>>>
>>>> apply(test,1,max)  --- gives me the maximum values
>>>> apply(test,1,which.max) --- gives me the column
>>>>
>>>> But, standard which.max doesn't handles ties/duplicates in a way that serves my need. It defaults to returning the first column containing the maximum value.
>>>>
>>>> What I'd like to end up with is, ultimately, something where apply(test,1,which.max) yields 1,2,T  (rather than 1,2,1).
>>>>
>>>> So, a function which does what which.max currently does if the elements of the row differ, but which returns a T (or some such) if in fact the row values are all the same.
>>>>
>>>> I've tried a bunch of things, to know avail. Closest I got was to use a function to test for whether or not a vector
>>>>
>>>> isUnique <- function(vector){
>>>>                   return(!any(duplicated(vector)))
>>>>              }
>>>>
>>>> which returns TRUE if values of vector all unique. So
>>>>
>>>> apply(test,1,isUnique)
>>>>
>>>> returns
>>>>
>>>> [1]  TRUE  TRUE FALSE
>>>>
>>>> but I'm stuck beyond this.
>>> The following gets me pretty close,
>>>
>>> test_new <- test
>>> test_new[which(apply(test,1,isUnique)==FALSE),] <- 'T'
>>>
>>> but is clunky.
>>>
>>>
>>>
>>>
>>>
>>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
>
> Ecological Forecasting: https://eco.bigelow.org/
>
>
>
>
>
>


	[[alternative HTML version deleted]]


From S@E|||@on @end|ng |rom LGCGroup@com  Tue Mar 19 18:06:43 2019
From: S@E|||@on @end|ng |rom LGCGroup@com (S Ellison)
Date: Tue, 19 Mar 2019 17:06:43 +0000
Subject: [R] Fwd: high p values
In-Reply-To: <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
References: <CAJhui+tJH10VqvEw7duww8FkEcOCX_swO3OYs1ZqbMo-Sm98fg@mail.gmail.com>
 <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
Message-ID: <9e37e45abdd247dcb725ad2003a1e2ce@GBDCVPEXC04.corp.lgc-group.com>

> This is my function:
> 
> wilcox.test(A,B, data = data, paired = FALSE)
> 
> It gives me high p value, though the median of A column is 6900 and B
> column is 3500.
> 
> Why it gives p value high if there is a difference in the median?

Perhaps becuase a) because you are testing the wrong data or b) there isn't a significant difference 

a) You are probably not using the data you think you are. Check ?wilcox.test; the 'data' argument is specific to the formula method. That needs a formula as the first argument, not a numeric vector. What you've done is apply the default, and 'data' has been ignored. So A and B are whatever was lying around in your current environment, not what is in 'data'.  ('data' is a terrible name for a data frame, by the way, as 'data' is an R function). 

After that:
- How many data points do you have in each group?
- How much do the two groups overlap?

If the answers are 'not many' or 'lots' (in that order), and especially if both apply, you can't expect a significant test result.

S Ellison


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From ph|| @end|ng |rom pr|com@com@@u  Tue Mar 19 18:42:24 2019
From: ph|| @end|ng |rom pr|com@com@@u (Philip Rhoades)
Date: Wed, 20 Mar 2019 04:42:24 +1100
Subject: [R] A general question about using Bayes' Theorem for calculating
 the probability of The End of Human Technological Civilisation
Message-ID: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>

People,

I have only a general statistics understanding and have never actually 
used Bayes' Theorem for any real-world problem.  My interest lies in 
developing some statistical approach for addressing the subject above 
and it seems to me that BT is what I should be looking at?  However, 
what I am specifically interested in is how such a work-up would be 
developed for a year-on-year situation eg:

I think it is likely that TEHTC could be triggered by a multi-gigaton 
release of methane from the Arctic Ocean and the Siberian Permafrost in 
any Northern Hemisphere Summer from now on (multiple physical and 
non-physical, human positive feedback loops would then kick in).

So, say my estimate (Bayesian Prior) is that for this coming (2019) NHS 
the chance of this triggering NOT occurring is x%.  The manipulation is 
then done to calculate the posterior for 2019 - but for every successive 
year (given the state of the world), isn't it true that the chance of a 
triggering NOT occurring in the NHS MUST go down? - ie it is just an 
argument about the scale of the change from year to year?

It seems to be that the posterior for one year becomes the prior for the 
next year?  Once the prior gets small enough people won't bother with 
the calculations anyway . .

Does anyone know of any existing work on this topic?  I want to write a 
plain-English doc about it but I want to have the stats clear in my head 
. .

Thanks,

Phil.
-- 
Philip Rhoades

PO Box 896
Cowra  NSW  2794
Australia
E-mail:  phil at pricom.com.au


From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Mar 19 18:59:31 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 19 Mar 2019 10:59:31 -0700
Subject: [R] 
 A general question about using Bayes' Theorem for calculating
 the probability of The End of Human Technological Civilisation
In-Reply-To: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
References: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
Message-ID: <1f11f634-3ee5-45dd-f9b6-7cd7d8644fa0@comcast.net>

Rhelp is not a forum for discussions of statistics. Instead it is for 
persons who have specific questions about the use of R.

Please read the list info page where you started the subscription 
process. And do read the Posting Guide. Both these are linked at the 
bottom of this response.

There are Web accessible forums that are set up to statistics.

-- 

David.

On 3/19/19 10:42 AM, Philip Rhoades wrote:
> People,
>
> I have only a general statistics understanding and have never actually 
> used Bayes' Theorem for any real-world problem.? My interest lies in 
> developing some statistical approach for addressing the subject above 
> and it seems to me that BT is what I should be looking at?? However, 
> what I am specifically interested in is how such a work-up would be 
> developed for a year-on-year situation eg:
>
> I think it is likely that TEHTC could be triggered by a multi-gigaton 
> release of methane from the Arctic Ocean and the Siberian Permafrost 
> in any Northern Hemisphere Summer from now on (multiple physical and 
> non-physical, human positive feedback loops would then kick in).
>
> So, say my estimate (Bayesian Prior) is that for this coming (2019) 
> NHS the chance of this triggering NOT occurring is x%.? The 
> manipulation is then done to calculate the posterior for 2019 - but 
> for every successive year (given the state of the world), isn't it 
> true that the chance of a triggering NOT occurring in the NHS MUST go 
> down? - ie it is just an argument about the scale of the change from 
> year to year?
>
> It seems to be that the posterior for one year becomes the prior for 
> the next year?? Once the prior gets small enough people won't bother 
> with the calculations anyway . .
>
> Does anyone know of any existing work on this topic?? I want to write 
> a plain-English doc about it but I want to have the stats clear in my 
> head . .
>
> Thanks,
>
> Phil.


From ev@n@cooch @end|ng |rom gm@||@com  Tue Mar 19 19:06:22 2019
From: ev@n@cooch @end|ng |rom gm@||@com (Evan Cooch)
Date: Tue, 19 Mar 2019 14:06:22 -0400
Subject: [R] 
 A general question about using Bayes' Theorem for calculating
 the probability of The End of Human Technological Civilisation
In-Reply-To: <1f11f634-3ee5-45dd-f9b6-7cd7d8644fa0@comcast.net>
References: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
 <1f11f634-3ee5-45dd-f9b6-7cd7d8644fa0@comcast.net>
Message-ID: <2abe22b5-5997-8591-28a6-bd8237c95a36@gmail.com>

Just curious -- if R-help is a moderated list (which? in theory , it is 
-- my posts have been 'modertated', to the degree that they aren't 
released to the list until someone approves them), and if these 
'statistics discussion' questions are inappropriate to the mission (as 
described), then...why isn't the 'moderator' (him/her/they) blocking on 
submission?

On 3/19/2019 1:59 PM, David Winsemius wrote:
> Rhelp is not a forum for discussions of statistics. Instead it is for 
> persons who have specific questions about the use of R.
>
> Please read the list info page where you started the subscription 
> process. And do read the Posting Guide. Both these are linked at the 
> bottom of this response.
>
> There are Web accessible forums that are set up to statistics.
>


From wdun|@p @end|ng |rom t|bco@com  Tue Mar 19 19:22:00 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Tue, 19 Mar 2019 11:22:00 -0700
Subject: [R] Fwd: high p values
In-Reply-To: <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
References: <CAJhui+tJH10VqvEw7duww8FkEcOCX_swO3OYs1ZqbMo-Sm98fg@mail.gmail.com>
 <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
Message-ID: <CAF8bMcavPZNOqgDQLdEavrxoXDFre9XVjW=d8oCE2G13SzdC0Q@mail.gmail.com>

Any reasonable test of whether two samples differ should be scale and
location invariant.  E.g., if you measure temperature it should not matter
if you units are degrees Fahrenheit or micro-Kelvins.  Thus saying the
medians are 3500 and 6200 is equivalent to saying they are 100.035 and
100.062: it does not tell use how different the samples are.  You need to
consider how much overlap there is.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, Mar 19, 2019 at 9:48 AM javed khan <javedbtk111 at gmail.com> wrote:

> Hi
>
> This is my function:
>
> wilcox.test(A,B, data = data, paired = FALSE)
>
> It gives me high p value, though the median of A column is 6900 and B
> column is 3500.
>
> Why it gives p value high if there is a difference in the median?
>
> Regards
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rc_@chw@rtz @end|ng |rom me@com  Tue Mar 19 19:43:35 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Tue, 19 Mar 2019 14:43:35 -0400
Subject: [R] 
 A general question about using Bayes' Theorem for calculating
 the probability of The End of Human Technological Civilisation
In-Reply-To: <2abe22b5-5997-8591-28a6-bd8237c95a36@gmail.com>
References: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
 <1f11f634-3ee5-45dd-f9b6-7cd7d8644fa0@comcast.net>
 <2abe22b5-5997-8591-28a6-bd8237c95a36@gmail.com>
Message-ID: <EF2839A8-7563-452E-858B-9A9A40706F22@me.com>



> On Mar 19, 2019, at 2:06 PM, Evan Cooch <evan.cooch at gmail.com> wrote:
> 
> Just curious -- if R-help is a moderated list (which  in theory , it is -- my posts have been 'modertated', to the degree that they aren't released to the list until someone approves them), and if these 'statistics discussion' questions are inappropriate to the mission (as described), then...why isn't the 'moderator' (him/her/they) blocking on submission?
> 
> On 3/19/2019 1:59 PM, David Winsemius wrote:
>> Rhelp is not a forum for discussions of statistics. Instead it is for persons who have specific questions about the use of R.
>> 
>> Please read the list info page where you started the subscription process. And do read the Posting Guide. Both these are linked at the bottom of this response.
>> 
>> There are Web accessible forums that are set up to statistics.
>> 
> 

Evan,

While I cannot speak for the R-Help moderators, which is a 'larger' group, I am a co-moderator for R-Devel.

The initial moderation occurs when someone who has not subscribed to the list sends a post. The list software captures the post and sends the moderators a notification that there is a post from a non-subscriber requiring manual review.

If the post is not relevant to the specific R list and should be sent to another R list, where it is better suited given the focus of the topic, the post will be rejected and the poster informed of the reason. If it is not truly R related, per se, then a recommendation to send the post to StackExchange or similar will be send back to the poster, with a rejection of the post.

Once a sender's e-mail account has been approved to post, which generally means that they have both subscribed to the list in question and have sent at least one relevant post to the list, future posts are typically no longer moderated.

It is possible that once in a while, a moderator will miss something in the post content and approve it going to the list, but that should be a rare event.

A search of the R-Help archives suggests that Philip has posted previously, going to back at least 2011, which is likely why this particular post managed to not be moderated.

Regards,

Marc Schwartz


From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Mar 19 19:47:56 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 19 Mar 2019 11:47:56 -0700
Subject: [R] 
 A general question about using Bayes' Theorem for calculating
 the probability of The End of Human Technological Civilisation
In-Reply-To: <2abe22b5-5997-8591-28a6-bd8237c95a36@gmail.com>
References: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
 <1f11f634-3ee5-45dd-f9b6-7cd7d8644fa0@comcast.net>
 <2abe22b5-5997-8591-28a6-bd8237c95a36@gmail.com>
Message-ID: <337d0660-3430-0deb-8929-9b22d71b6c58@comcast.net>

Actually the list is not moderated in the usual sense of the word. If 
you subscribe, your posts are not moderated. Only your first posting 
after subscription would be moderated, but for the purpose of preventing 
persons with obvious spamming goals.

And there are several different moderators. If I had seen that posting, 
I might have rejected it.

-- 

David.


On 3/19/19 11:06 AM, Evan Cooch wrote:
> Just curious -- if R-help is a moderated list (which? in theory , it 
> is -- my posts have been 'modertated', to the degree that they aren't 
> released to the list until someone approves them), and if these 
> 'statistics discussion' questions are inappropriate to the mission (as 
> described), then...why isn't the 'moderator' (him/her/they) blocking 
> on submission?
>
> On 3/19/2019 1:59 PM, David Winsemius wrote:
>> Rhelp is not a forum for discussions of statistics. Instead it is for 
>> persons who have specific questions about the use of R.
>>
>> Please read the list info page where you started the subscription 
>> process. And do read the Posting Guide. Both these are linked at the 
>> bottom of this response.
>>
>> There are Web accessible forums that are set up to statistics.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Mar 19 20:49:03 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 19 Mar 2019 12:49:03 -0700
Subject: [R] 
 A general question about using Bayes' Theorem for calculating
 the probability of The End of Human Technological Civilisation
In-Reply-To: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
References: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
Message-ID: <4AD7E9F6-768D-409D-9FB8-5785A2DDE182@dcn.davis.ca.us>

Highly off topic. Try StackOverflow.

On March 19, 2019 10:42:24 AM PDT, Philip Rhoades <phil at pricom.com.au> wrote:
>People,
>
>I have only a general statistics understanding and have never actually 
>used Bayes' Theorem for any real-world problem.  My interest lies in 
>developing some statistical approach for addressing the subject above 
>and it seems to me that BT is what I should be looking at?  However, 
>what I am specifically interested in is how such a work-up would be 
>developed for a year-on-year situation eg:
>
>I think it is likely that TEHTC could be triggered by a multi-gigaton 
>release of methane from the Arctic Ocean and the Siberian Permafrost in
>
>any Northern Hemisphere Summer from now on (multiple physical and 
>non-physical, human positive feedback loops would then kick in).
>
>So, say my estimate (Bayesian Prior) is that for this coming (2019) NHS
>
>the chance of this triggering NOT occurring is x%.  The manipulation is
>
>then done to calculate the posterior for 2019 - but for every
>successive 
>year (given the state of the world), isn't it true that the chance of a
>
>triggering NOT occurring in the NHS MUST go down? - ie it is just an 
>argument about the scale of the change from year to year?
>
>It seems to be that the posterior for one year becomes the prior for
>the 
>next year?  Once the prior gets small enough people won't bother with 
>the calculations anyway . .
>
>Does anyone know of any existing work on this topic?  I want to write a
>
>plain-English doc about it but I want to have the stats clear in my
>head 
>. .
>
>Thanks,
>
>Phil.

-- 
Sent from my phone. Please excuse my brevity.


From drj|m|emon @end|ng |rom gm@||@com  Tue Mar 19 23:25:17 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 20 Mar 2019 09:25:17 +1100
Subject: [R] Fwd: high p values
In-Reply-To: <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
References: <CAJhui+tJH10VqvEw7duww8FkEcOCX_swO3OYs1ZqbMo-Sm98fg@mail.gmail.com>
 <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
Message-ID: <CA+8X3fX_YqoArgFnT7xa_r1pR4hcoB_BCiM8tesmeLwj87EjHw@mail.gmail.com>

Hi Javed,
Easy.

A<-c(2000,2100,2300,2400,6900,7000,7040,7050,7060)
median(A)
[1] 6900
B<-c(3300,3350,3400,3450,3500,7000,7100,7200,7300)
median(B)
[1] 3500
wilcox.test(A,B,paired=FALSE)

       Wilcoxon rank sum test with continuity correction

data:  A and B
W = 26.5, p-value = 0.233
alternative hypothesis: true location shift is not equal to 0

Jim

On Wed, Mar 20, 2019 at 3:48 AM javed khan <javedbtk111 at gmail.com> wrote:
>
> Hi
>
> This is my function:
>
> wilcox.test(A,B, data = data, paired = FALSE)
>
> It gives me high p value, though the median of A column is 6900 and B
> column is 3500.
>
> Why it gives p value high if there is a difference in the median?
>
> Regards
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Mar 19 23:47:57 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 19 Mar 2019 15:47:57 -0700
Subject: [R] plot.xmean.ordinaly vs plot() in package "rms"
In-Reply-To: <CAHn6EJ17OO6KMFU9caPq8h1SkGOy_6v8LMrEbmPj-ymuD-steg@mail.gmail.com>
References: <CAHn6EJ3LF286PCWaGPJ0tdk8wXSwejmJrsygzWtHeL2qyoUErw@mail.gmail.com>
 <031F9B89-F684-4735-A9B2-2AAD35942C91@dcn.davis.ca.us>
 <CAHn6EJ2=jLakgdMazEfwvUQ=X0JbjhjoEL4os3iR8wvFeMO2Kw@mail.gmail.com>
 <7D856F8B-80C7-48BB-BF37-BF17D0204A5F@dcn.davis.ca.us>
 <CAHn6EJ17OO6KMFU9caPq8h1SkGOy_6v8LMrEbmPj-ymuD-steg@mail.gmail.com>
Message-ID: <C0C4E053-E4BF-4CB7-919A-86B3BD8CA2A7@dcn.davis.ca.us>

No. Do not call plot.xmean.ordinaly() if the argument is not of class xmean.ordinaly, because that function assumes that it is such an object.

That is one reason why it is better to call plot() than to be more specific.

On March 19, 2019 3:29:11 PM PDT, Kim Jacobsen <kimsjacobsen at gmail.com> wrote:
>Mailing list now included (apologies, first time I post anything so not
>quite sure how it works).
>
>You are quite right, it was a typo. I meant to write that
>plot.xmean.ordinaly(). So please let me correct my last statement: the
>plot.xmean.ordinaly() command and plot() command are interchangeable as
>long as x is an object x of class "xmean.ordinaly", and
>plot.xmean.ordinaly() is best used if the object is not of class
>"xmean.ordinaly" or if you are unsure what class it it. Is this a
>correct
>encapsulation?
>
>
>On Sun, 17 Mar 2019 at 14:38, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> Please keep the mailing list included in the thread.
>>
>> I can't tell if you do understand and are just being sloppy, or if
>you are
>> completely confused, because xmean.ordinaly() and
>plot.xmean.ordinaly() are
>> two completely different symbols in R.
>>
>> As for being "safe"... you may choose to be specific or not, but plot
>and
>> plot.xmean.ordinaly are both equally "safe" to call, and being too
>specific
>> can cause problems sometimes as well.
>>
>> On March 17, 2019 6:40:10 AM PDT, Kim Jacobsen
><kimsjacobsen at gmail.com>
>> wrote:
>> >Dear Jeff,
>> >
>> >Thank you so much! So if I understand the S3 object documents
>> >correctly,
>> >the xmean.ordinaly() command and plot() command are interchangeable
>as
>> >long
>> >as x is an object x of class "xmean.ordinaly"? So would I be right
>to
>> >think
>> >that I might as well just xmean.ordinaly() to be safe?
>> >
>> >Many thanks,
>> >
>> >
>> >
>> >On Sun, 17 Mar 2019 at 02:08, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>> >wrote:
>> >
>> >> Read up on S3 object orientation[1]. If you have an object x of
>class
>> >> "xmean.ordinaly" then writing
>> >>
>> >> plot(x)
>> >>
>> >> will end up invoking the plot.xmean.ordinaly function rather than
>the
>> >> plot.default function in base graphics. This is broadly true
>> >throughout R.
>> >>
>> >> [1] http://adv-r.had.co.nz/S3.html
>> >>
>> >> On March 16, 2019 11:03:06 AM PDT, Kim Jacobsen
>> ><kimsjacobsen at gmail.com>
>> >> wrote:
>> >> >Would anyone be able to explain what the difference is between
>> >> >plot.xmean.ordinaly and plot() in the "rms" package? (for the
>> >purposes
>> >> >of
>> >> >testing the proportional odds assumption in ordinal models). In
>the
>> >> >package
>> >> >document (https://cran.r-project.org/web/packages/rms/rms.pdf)
>they
>> >> >seem
>> >> >both to be used interchangeably.
>> >> >
>> >> >Thank you!
>> >>
>> >> --
>> >> Sent from my phone. Please excuse my brevity.
>> >>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From m@rc_@chw@rtz @end|ng |rom me@com  Wed Mar 20 00:16:59 2019
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Tue, 19 Mar 2019 19:16:59 -0400
Subject: [R] high p values
In-Reply-To: <CA+8X3fX_YqoArgFnT7xa_r1pR4hcoB_BCiM8tesmeLwj87EjHw@mail.gmail.com>
References: <CAJhui+tJH10VqvEw7duww8FkEcOCX_swO3OYs1ZqbMo-Sm98fg@mail.gmail.com>
 <CAJhui+vu9Dy2T7nVmc8S58woLijgA7hLiXt_8WG5odcKHs57tw@mail.gmail.com>
 <CA+8X3fX_YqoArgFnT7xa_r1pR4hcoB_BCiM8tesmeLwj87EjHw@mail.gmail.com>
Message-ID: <621F47AE-AE3A-44CE-9CF1-627D93DCD9A8@me.com>

Hi,

Since folks are taking the time to point out some subtle issues here, taking an example from the UCLA Stats web site:

https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-why-is-the-mann-whitney-significant-when-the-medians-are-equal/

Grp1 <- rep(c(-2, 0, 5), each = 20)
Grp2 <- rep(c(-1, 0, 10), each = 20)

> Grp1
 [1] -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2  0  0
[23]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  5  5  5
[45]  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5
> Grp2
 [1] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0
[23]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 10 10 10
[45] 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10

> median(Grp1)
[1] 0
> median(Grp2)
[1] 0

> wilcox.test(Grp1, Grp2)

	Wilcoxon rank sum test with continuity correction

data:  Grp1 and Grp2
W = 1400, p-value = 0.03096
alternative hypothesis: true location shift is not equal to 0


So, in contrast to the original problem, here is an example where you have equal medians, but a significant test result.

The key concept is that the Wilcoxon Rank Sum test is not strictly a test of differences in medians. That is, the null hypothesis for the test is not that the medians are equal, and you are either accepting or rejecting that null. 

Javed, I would suggest spending some time with a good tutorial on non-parametric statistics.

Regards,

Marc Schwartz


> On Mar 19, 2019, at 6:25 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Hi Javed,
> Easy.
> 
> A<-c(2000,2100,2300,2400,6900,7000,7040,7050,7060)
> median(A)
> [1] 6900
> B<-c(3300,3350,3400,3450,3500,7000,7100,7200,7300)
> median(B)
> [1] 3500
> wilcox.test(A,B,paired=FALSE)
> 
>       Wilcoxon rank sum test with continuity correction
> 
> data:  A and B
> W = 26.5, p-value = 0.233
> alternative hypothesis: true location shift is not equal to 0
> 
> Jim
> 
> On Wed, Mar 20, 2019 at 3:48 AM javed khan <javedbtk111 at gmail.com> wrote:
>> 
>> Hi
>> 
>> This is my function:
>> 
>> wilcox.test(A,B, data = data, paired = FALSE)
>> 
>> It gives me high p value, though the median of A column is 6900 and B
>> column is 3500.
>> 
>> Why it gives p value high if there is a difference in the median?
>> 
>> Regards


From dw|n@em|u@ @end|ng |rom comc@@t@net  Wed Mar 20 02:38:49 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 19 Mar 2019 18:38:49 -0700
Subject: [R] 
 A general question about using Bayes' Theorem for calculating
 the probability of The End of Human Technological Civilisation
In-Reply-To: <4AD7E9F6-768D-409D-9FB8-5785A2DDE182@dcn.davis.ca.us>
References: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
 <4AD7E9F6-768D-409D-9FB8-5785A2DDE182@dcn.davis.ca.us>
Message-ID: <dbc28a46-7f37-3609-345a-dcc087ebffab@comcast.net>


On 3/19/19 12:49 PM, Jeff Newmiller wrote:
> Highly off topic. Try StackOverflow.


As it stands it's off-topic for SO. (You would just be making more work 
for those of us who know the rules but need 4 close votes for 
migration.)? Better would be immediately posting at CrossValidated.com 
(i.e., stats.stackexchange.com)

-- 

David.

>
> On March 19, 2019 10:42:24 AM PDT, Philip Rhoades <phil at pricom.com.au> wrote:
>> People,
>>
>> I have only a general statistics understanding and have never actually
>> used Bayes' Theorem for any real-world problem.  My interest lies in
>> developing some statistical approach for addressing the subject above
>> and it seems to me that BT is what I should be looking at?  However,
>> what I am specifically interested in is how such a work-up would be
>> developed for a year-on-year situation eg:
>>
>> I think it is likely that TEHTC could be triggered by a multi-gigaton
>> release of methane from the Arctic Ocean and the Siberian Permafrost in
>>
>> any Northern Hemisphere Summer from now on (multiple physical and
>> non-physical, human positive feedback loops would then kick in).
>>
>> So, say my estimate (Bayesian Prior) is that for this coming (2019) NHS
>>
>> the chance of this triggering NOT occurring is x%.  The manipulation is
>>
>> then done to calculate the posterior for 2019 - but for every
>> successive
>> year (given the state of the world), isn't it true that the chance of a
>>
>> triggering NOT occurring in the NHS MUST go down? - ie it is just an
>> argument about the scale of the change from year to year?
>>
>> It seems to be that the posterior for one year becomes the prior for
>> the
>> next year?  Once the prior gets small enough people won't bother with
>> the calculations anyway . .
>>
>> Does anyone know of any existing work on this topic?  I want to write a
>>
>> plain-English doc about it but I want to have the stats clear in my
>> head
>> . .
>>
>> Thanks,
>>
>> Phil.


From ph|| @end|ng |rom pr|com@com@@u  Wed Mar 20 07:58:06 2019
From: ph|| @end|ng |rom pr|com@com@@u (Philip Rhoades)
Date: Wed, 20 Mar 2019 17:58:06 +1100
Subject: [R] 
 A general question about using Bayes' Theorem for calculating
 the probability of The End of Human Technological Civilisation
In-Reply-To: <dbc28a46-7f37-3609-345a-dcc087ebffab@comcast.net>
References: <ddf584b7355117de3ff38f6590d5a18e@pricom.com.au>
 <4AD7E9F6-768D-409D-9FB8-5785A2DDE182@dcn.davis.ca.us>
 <dbc28a46-7f37-3609-345a-dcc087ebffab@comcast.net>
Message-ID: <b9b2acc1139e7b2c4fcfe954e6c07d4d@pricom.com.au>

David,


On 2019-03-20 12:38, David Winsemius wrote:
> On 3/19/19 12:49 PM, Jeff Newmiller wrote:
>> Highly off topic. Try StackOverflow.
>> 
> As it stands it's off-topic for SO. (You would just be making more
> work for those of us who know the rules but need 4 close votes for
> migration.)? Better would be immediately posting at CrossValidated.com
> (i.e., stats.stackexchange.com)


Thanks - I will check that out . .

P.


> --
> 
> David.
> 
>> 
>> On March 19, 2019 10:42:24 AM PDT, Philip Rhoades <phil at pricom.com.au> 
>> wrote:
>>> People,
>>> 
>>> I have only a general statistics understanding and have never 
>>> actually
>>> used Bayes' Theorem for any real-world problem.  My interest lies in
>>> developing some statistical approach for addressing the subject above
>>> and it seems to me that BT is what I should be looking at?  However,
>>> what I am specifically interested in is how such a work-up would be
>>> developed for a year-on-year situation eg:
>>> 
>>> I think it is likely that TEHTC could be triggered by a multi-gigaton
>>> release of methane from the Arctic Ocean and the Siberian Permafrost 
>>> in
>>> 
>>> any Northern Hemisphere Summer from now on (multiple physical and
>>> non-physical, human positive feedback loops would then kick in).
>>> 
>>> So, say my estimate (Bayesian Prior) is that for this coming (2019) 
>>> NHS
>>> 
>>> the chance of this triggering NOT occurring is x%.  The manipulation 
>>> is
>>> 
>>> then done to calculate the posterior for 2019 - but for every
>>> successive
>>> year (given the state of the world), isn't it true that the chance of 
>>> a
>>> 
>>> triggering NOT occurring in the NHS MUST go down? - ie it is just an
>>> argument about the scale of the change from year to year?
>>> 
>>> It seems to be that the posterior for one year becomes the prior for
>>> the
>>> next year?  Once the prior gets small enough people won't bother with
>>> the calculations anyway . .
>>> 
>>> Does anyone know of any existing work on this topic?  I want to write 
>>> a
>>> 
>>> plain-English doc about it but I want to have the stats clear in my
>>> head
>>> . .
>>> 
>>> Thanks,
>>> 
>>> Phil.

-- 
Philip Rhoades

PO Box 896
Cowra  NSW  2794
Australia
E-mail:  phil at pricom.com.au


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Mar 20 09:02:45 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 20 Mar 2019 08:02:45 +0000
Subject: [R] problem with nlsLM.....
Message-ID: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members,
                             Yesterday, Duncan identified a silly mistake in an nls call...in the list of starting values, one of the names of the variable was getting repeated. But I am experiencing the same problem again, with a different formula:

> formulaDH3
HM1 ~ (a + (b * ((HM2 + 0.3)^(1/3)))) * (c * log(HM3 + 27))

here HM1 is the response variable, and HM2 and HM3 are predictors.....

> nonlin_modDH3 <- nls(formulaDH3, start = list(a = 0.43143, b = 0.68173,c = 0.02954))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates
> nonlin_modDH3 <- nlsLM(formulaDH3, start = list(a = 0.43143, b = 0.68173,c = 0.02954))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates

I am using nlsLM function from the minpack.lm package which says that it will converge when nls fails with singular gradient matrix error...


Is there again a silly mistake(pardon me again if there is one!), or is the problem serious? If it is serious, any pointers towards a solution?

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Mar 20 09:20:05 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 20 Mar 2019 08:20:05 +0000
Subject: [R] Fw: problem with nlsLM.....
In-Reply-To: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <SL2P216MB0091E7E47082786F87FDA0D6C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members,
                             I think Duncan's  solution is not working...I've checked the function again:

> formulaDH5 <- as.formula(HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) + (A*sin(w*HM3 + c) + C))
> nonlin_modDH5 <- nls(formulaDH5, start = list(a = 0.43143, b = 0.68173,A = 0.09,w = 0.8,c = 0.01,C = 0.94))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates
> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 0.43143, b = 0.68173,A = 0.09,w = 0.8,c = 0.01,C = 0.94))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates


I've checked the Internet  for a method of getting the starting values, but they are not comprehensive....any resources for how to find the starting values?

> dput(HM1)
c(1.01484280343579, 0.977004104656811, 1.02448653468064, 1.01562102015533,
0.968359321714289, 0.99105072887847, 0.99384582083167, 1.00579414727333,
0.967593120445675, 1.04694148058855, 1.03832347454026, 0.971040156128456,
0.976967848281943, 0.990386686964446, 1.02257020722784, 0.994529161072552,
1.05361176882747, 0.988021514764882, 0.998909692369571, 1.04933819895509,
0.988354009297047, 0.878584020586706, 1.01438895222572, 1.01522203155842,
0.999979601913003, 0.975862934766315, 1.03948445565434, 1.02493339899053,
0.972267168470219, 0.971001051421653, 1.03066167719129, 0.967898970631469,
0.976606662010519, 0.990061873979734, 1.03398666689317, 1.05491455733517,
0.995997711923663, 0.991213393857838, 0.9903216916016, 1.06850794640807,
1.01345738121581, 0.877847194198055, 0.992124544661245, 1.00927272338804,
0.988196514422814, 0.987504905313256, 0.990274516132497, 1.01484624802751,
0.9800176904083, 0.969321725342799, 1.07407427451262, 1.02023130729371,
0.975827723120998, 0.988260886231393, 1.05116598585815, 0.977996171738906,
0.985118207891832, 0.98243346941999, 0.991380612684958, 1.00960741365585,
1.03062446605116, 1.04176358759364, 1.02494798972349, 1.04645592406625,
0.999998705820515, 0.972161829098397, 0.991313521356105, 1.0211561076261,
0.971257832217016, 1.00935026461508, 0.997140402835885, 1.00719609467891,
0.97483865743955, 0.987491380127905, 1.0643888147255, 1.00804564512905,
0.989366232219398, 0.942042386899482, 1.0140328534339, 0.98812919092058,
1.04584882138055, 0.970586857498767, 1.0098232311784, 1.02237732533005,
0.99306513943649, 0.963808401790738, 1.03577231048959, 1.00540101137939,
0.970458918298484, 0.973460294926879, 0.992676651187501, 1.04079931972326,
0.988928168736541, 0.986376934855979, 1.04117941173535, 1.01252015337269,
0.920578394310677, 0.98933093765412, 0.98116698970429, 0.995498900493761,
0.96463760451942, 1.1059686605887, 1.03366353582116, 0.981256131441973,
1.0152599452654, 0.995362080418312, 0.997276311537458, 0.943438355295013,
1.04025117202718, 1.03102737097366, 1.02096902615856, 1.02506560388494,
0.959364103040633, 0.987217067188943, 1.00454456275356, 1.03517749036747,
1.0148342900634, 0.974518005568706, 1.02515903830848, 1.00435361604303,
0.973400954462685, 0.981702992961201, 1.0155969247203, 1.01064154947471,
0.957131570492054, 0.88504625471737, 1.01849907662046, 1.01503806358198,
0.987004149653273, 0.974485857983481, 0.987572979379099, 1.0037529762248,
0.976026658183349, 0.983176178747953, 1.03812943411666, 1.01828203593425,
0.984139742592733, 1.00377243020886, 1.02118652563454, 1.01115776483144,
0.98387406692053, 0.985919715423209, 0.989550590454508, 1.01845373071313,
1.00394660346757, 0.949076679285431, 1.06094051230739, 0.981940494629297,
0.982812578115724, 0.929673163266673, 1.05359335975355, 1.03498999963467,
0.989218608625026, 0.994412680504384, 1.03162650392014, 1.04359975620911,
0.96938669600183, 0.987150178669349, 0.990503131271205, 1.01120231626779,
0.977334529946535, 0.966013274319284, 1.04966722359666, 1.04745672849702,
0.960799643211444, 0.866626315091455, 1.01316731305781, 1.02919067036911,
0.988567285725018, 0.978702298855537, 1.0186604054056, 1.01398864226215,
0.985492431198792, 1.03982326960775, 1.02287049736119, 1.02357106122821,
0.977519287403578, 0.993818572820706, 1.01139405704906, 1.02683235508704,
0.943370973180458, 0.970153991236375, 0.970746163945049, 1.00996886938921,
0.970599365922215, 1.03230385881209, 1.02072718512322, 1.0161024503161,
0.987718207880278, 0.987512809150475, 1.04108232940019, 1.0153758143276,
1.03916475100072, 0.959776942350879, 1.01739288375256, 1.05438479662213,
0.970479916939806, 0.979826128477137, 0.982800613999074, 0.977050857426291,
0.977474025608337, 0.972905352371901, 1.02191626900472, 1.02737043327345,
0.987565251408109, 0.93131925605196, 0.991469304659888, 0.949708678221412,
0.948662234856045, 0.938158485016718, 1.02342896552764, 1.01869899424866,
1.01244484998263, 0.990637832491416, 1.04591108144934, 0.951244278685872,
0.977265525807748, 0.942206977147219, 1.09440128638859, 1.02935716108991,
0.978428723706514, 1.08589930790608, 1.0015813231405, 1.07512822987239,
0.976356469419301, 0.972775004620642, 1.00954673593815, 0.992551447194369,
0.985819189451289, 0.975310750212573, 1.05999783258221, 1.07815688799274,
0.959648862885262, 1.03418767541976, 1.07207529899183, 1.04949264142055,
0.966872387358102, 0.963564695272614, 1.01310836328613, 0.996622968927382,
1.00202486063801, 0.948215304098709, 1.02845448262427, 0.987124642586768,
0.999269328077687, 0.964674178364545, 0.991913103743918, 0.970556592778505,
0.979495816235886, 0.949160643807912, 0.999611005677085, 1.01697727168167,
0.966362681248743, 0.938703012004999, 1.01394761650376, 1.011369485146,
0.975929406359934, 0.97111480362557, 1.0581474202687, 1.03965938660391,
0.990447147628825, 0.967286377976687, 0.968162001740811, 1.00573452177125,
0.974628505381728, 0.983183640545729, 1.08185959707149, 0.976533333333333,
0.932582771965921, 1.02205682784386, 1.02162002635618, 1.00634335266967,
1.03045302554561, 1.05015537832235, 1.0238800474742, 1.0151188247761,
0.951176244898346, 0.937999477475998, 0.986664690881963, 1.00889938014614,
0.972546365957459, 0.937560562249233, 0.953227957137494, 0.964284336810156,
0.977909815090698, 0.988068102557503, 1.04429493287078, 1.01803069781624,
0.957632272450841, 0.971991500790857, 0.98209395086599, 1.02815448263624,
0.947227198465786, 0.941538843776026, 1.05745885548346, 1.01671992454154,
0.983992269183969, 0.951511089781983, 1.0110983382201, 1.00912039359684,
1.14741321154181, 0.985344569298617, 0.986725720078838, 1.03117619992574,
1.21391930217405, 0.997217690041687, 1.07387278335981, 1.02108776576674,
1.10313006343474, 1.10905015571117, 1.03999508931616, 1.03976033898847,
1.02524837903366, 0.984818521675578, 1.04179889608739, 0.975154632188378,
0.969607959579886, 1.02781726918172, 0.999963290601752, 1.01515310515696,
1.00520167771997, 0.984550628692601, 1.01902810624565, 1.00685219196689,
0.982173452231062, 0.999629285915004, 0.970558368481076, 0.995716235126211,
1.01314341676203, 1.02075675497478, 1.01128668171558, 1.05844449406008,
1.02510935720045, 0.983720171349822, 0.844396082698583, 1.05810846093782,
0.99048977314373, 0.971262209252963, 0.968530578900656, 1.04638759101296,
1.1799017507928, 0.898448146333193, 1.01160891745375, 1.01132611427775,
0.973840647644128, 1.04683923857471, 0.990344793003927, 1.0068683076688,
0.992798241837764, 0.99368375299494, 1.01200003722717, 1.0294016781563,
1.01547984207734, 1.04807175686468, 0.981737356425208, 1.06169292627851,
1.00518291863648, 0.976808854545251, 0.999321592083517, 0.97527078961014,
0.904376968942001, 0.987807226191329, 1.0350293488462, 1.02802944623052,
0.963692701708712, 0.998027399233401, 1.01762747994825, 1.0361812777154,
0.988298193013531, 1.04174794125517, 0.970839140069407, 1.0011387710754,
0.961691863428911, 1.02272120871699, 1.00519180537531, 0.999331326197155,
0.988055521308447, 1.00078630058504, 1.03915754537212, 0.971719181925134,
0.947846525645955, 0.955178087494908, 1.0019107391621, 1.01160524115586,
0.984958748948305, 0.967579294820259, 1.00552471207069, 1.04518978636097,
1.01810784117456, 0.993552343701494, 1.03535983577691, 0.982973964981781,
0.976175417305875, 0.971173421374951, 1.02622192154044, 1.06169292627851,
0.980316131595686, 0.98802873807095, 1.02522486330609, 1.01920704714191,
0.987730950969463, 0.972320506142784, 1.02950031603837, 1.01481027236895,
1.05066348592531, 0.941412086703545, 1.00426249434292, 0.997599618540051,
0.9727707483253, 0.987822018689403, 1.00496006171741, 1.00365488022554,
0.982021737672154, 0.946014836456916, 0.977232083157459, 1.04246004468132,
0.981228463109654, 0.971195860055527, 0.957569129533581, 0.987312165664453,
0.955583610052133, 0.991173211015633, 0.988434021882053, 0.94971029297998,
1.02260060019826, 0.986729432665294, 1.00044978044626, 1.00283798105021,
0.97876379280085, 0.948623031823408, 1.02317511219612, 1.02274213461243,
0.952138762776323, 0.974014522860041, 1.02874293788325, 1.02108205408977,
0.950582622331241, 0.986098109021331, 1.00098916530354, 1.05435029523389,
0.999921440746985, 0.964778188342829, 1.02087753056688, 1.0295810738595,
0.981181990342875, 0.94981916198993, 1.00828646284387, 1.01127872412662,
0.976192285243012, 0.95032633282931, 1.02884118341175, 1.01603776783957,
0.952947066257398, 0.988924105443658, 1.02727453593468, 1.02944328284254,
0.970561666318556, 0.982859453938812, 0.995886713501412, 1.04330648532137,
0.991844865771659, 0.99159250467833, 1.0332159700053, 0.993864541664301,
0.952716253755034, 0.960889912140351, 0.945669021854222, 0.985578531230794,
0.975053113912155, 0.999552576905685, 1.02526821151208, 1.00450298718514,
0.948193972002236, 0.99358065264941, 1.04855364680554, 0.982181523974446,
0.957570494265717, 0.989249667650167, 1.01928030167592, 0.977392376116208,
0.98989833978368, 0.950312758756191, 1.01179069272545, 1.03630963397181,
0.997439452068598, 0.981793158114732, 0.98560918506727, 1.0497573745934,
1.06729226949522, 0.987962794653476, 1.05655507163022, 1.01896087310511,
0.985627836611195, 1.03148215915346, 1.07268555224398, 1.00116324087534,
1.0353274520202, 0.965843323254256, 0.981731445105197, 1.01927876056569,
0.999117162412819, 0.984262994383997, 1.0058994935294, 1.03100765642408,
0.970000664632288, 0.974667000854514, 1.02683721041632, 1.00224180239507,
0.976488446884133, 0.989129579180466, 1.01111944121172, 1.01081759149299,
0.990822520380867, 0.951849309560414, 1.0533258785109, 1.02605855087945,
0.993203883495146, 1.01231349444546, 1.05040134097269, 0.988195090931879,
0.998324259112233, 0.955268105597824, 1.01179535756103, 1.00923640076821,
1.01018973127051, 0.97727388908476, 1.0163531272719, 1.03445181115965,
0.922709820364792, 1.05034425545701, 0.971929096725562, 1.01600521659268,
0.989307124797035, 0.966327502991904, 1.0148486097639, 1.00804453317389,
0.986688556832374, 1.01100519377607, 1.07022911567444, 1.01751745478207,
1.00395256916996, 1.02666291685486, 1.04306886282915, 0.976284796774505,
0.873912939480404, 0.985956634379203, 0.966005821428092, 1.01911378235808,
0.989971040708023, 0.948518684869746, 0.964578663605958, 1.00130118816861,
1.14681217078448, 0.989198271209915, 0.975831372743337, 0.87134009583841,
0.999320371256267, 1.04366317607907, 1.0941742251269, 1.04912747347822,
0.873169695550468, 0.9946703855144, 1.02687904508206, 1.01557590029223,
0.979039928979583, 0.978199700239161, 0.988038704848986, 0.944583404359117,
0.97831517569251, 0.9881631354812, 0.990553852862955, 1.05139702349736,
0.987315661692795, 0.973936995140175, 1.06038641236987, 1.0651525599012,
0.997891074790299, 0.979657304627723, 0.976638433723023, 0.981157058049022,
0.974773098042751, 1.04814547433216, 1.02327857487465, 1.01725074945885,
0.978364366421386, 1.02726007295919, 1.02997589351422, 1.0208112362723,
0.965389721633011, 1.01152741033802, 1.00352090319123, 0.981151102088714,
0.986010324656192, 0.97990659165448, 0.990503768576242, 0.985301104774446,
0.976384165205197, 0.983887706958686, 0.992885571845564, 0.985757622165226,
0.946874017209482, 0.990962963812142, 1.04319324396693, 1.07354098057006,
0.984597785175012, 0.97845443949194, 0.963960215777945, 1.06584156194012,
1.15623372835712, 0.94928101059165, 1.05512096334013, 1.00899261027951,
1.00760894032175, 0.989021593753401, 1.01442722705518, 1.04495346859144,
1.00095646335122, 0.958907195434402, 1.02045280866442, 1.00659622664478,
0.980590749755976, 1.01197688939839, 0.998468538022796, 1.040625279257,
0.934164344256759, 0.992160161968429, 0.998594942355232, 1.0532781805963,
0.974144103277039, 0.946253316413415, 1.00623102828924, 0.988787044999963,
0.987541467770869, 0.999140303075891, 1.05429436081972, 1.01617267860198,
1.17044068684611, 1.05663112916056, 1.01681282884482, 1.02872692218644,
1.00738468152863, 1.0222081131894, 1.01817103171172, 1.01222250929532,
0.97386289237006, 0.987701353725335, 1.00418624697519, 1.0183108729199,
0.956852868616883, 1.05440980534023, 1.01562402108019, 1.00083274000183,
0.945641654423597, 0.966367379950251, 1.02020174816679, 1.01151664724635,
0.940072052642951, 0.991027873457432, 0.984568985852697, 1.02373049431651,
0.985887377562884, 1.05498951399906, 0.98672427011774, 1.02570399134917,
1.06987898201568, 1.02648218241568, 0.971175525661877, 1.03211886190237,
0.992647697074835, 0.986388969656675, 1.02097305537141, 1.04518978636097,
0.987300416455667, 0.985182286987375, 1.02373433952904, 1.01376295257605,
0.992398195656577, 0.983765510339449, 1.06154013507033, 1.01312319643703
)


> dput(HM3)
c(0.196622289085315, -0.0436031046458981, 0.000820037203846464,
0.392758326108918, 0.0445839547736504, 0.223969403187679, -0.070152338130932,
0.197471312884591, 0.0498910549108242, -0.620322188497616, 0.488037411730214,
0.0461937792271113, 0.0051066586622384, 0.413877974790452, -0.154637441642314,
0.229802002983503, 0.509904610124243, 0.126452664534026, 0.583037470359474,
0.470621010682895, -0.00578078533090776, -0.457074922218691,
-0.0506356375273661, -0.767302691971105, -0.000822439375515026,
-0.170080770417037, -0.0638533091192526, 0.218492017287635, -0.088210288825341,
-0.389219867379597, 1.23355646106213, -1.78773586807154, 0.0117401974737664,
-0.154882548871051, -2.11547741687211, -0.0638533091192526, -8.18190789706224,
0.282699360997887, 0.0383693451361519, -0.0638533091192526, 0.832793868600698,
0.894379977535782, -0.302329357989944, 0.573489523345283, -0.223540700260161,
0.229802002983503, 0.0239731624923989, 0.556201388601996, -0.112552578411774,
-0.0438764575701239, 0.470621010682895, -4.92500180132548, 0.00309129784132211,
0.172407876390051, 0.229802002983503, 0.061799194890529, -0.000864150216291216,
-0.0683834224257835, -1.42874041532869, 0.654009227783557, -0.620322188497616,
0.272034153890376, -0.00558557185457871, 0.229802002983503, -1.53118563423569e-05,
0.470621010682895, -0.736608540541868, -1.47747441666374, -0.00291475050109709,
-1.56966101463791, 0.362334298169537, -1.41867286052432, -0.0062864890624406,
-0.223090980170635, 0.229802002983503, 0, 0.0147973092972513,
0.757637727251206, 0.332227874554666, -0.620322188497616, -0.0638533091192526,
-0.0638533091192526, 1.59477829009842, -0.247635059505396, -0.0638533091192526,
0.384123364351151, -0.909542286753924, -1.61643290186366, -0.00969493151693274,
-0.0638533091192526, 0.335128597413319, 0.229802002983503, -3.36005853184083,
-1.77772450660144, -0.223359097040026, -4.75082975711953, 0.545441153017038,
-0.0181047188208818, 0.229802002983503, 0.718505786757723, -0.199075726922584,
0.0779145211225284, 0.0981591683724576, -0.0638533091192526,
0.771585236053205, -0.124873344480422, -0.143455357050914, -0.982192613814533,
0.840325570227771, -0.0639597669819456, 0.272309718636627, 0.470621010682895,
0.000149945344883234, 0.0179651303218593, 0.586470204231886,
1.12661379530659, 0.587538739887084, -0.389219867379597, 0.231816913894395,
-2.32942853843292, 0.229802002983503, 0.470621010682895, 7.7137219916985,
3.14411367083572, 0.0371415305209129, 0.109258089720235, 0.0744990417275174,
0.978949955731298, 0.0046627432182197, 0.0778534035355464, -0.258075504911111,
0.428481177165365, 0.470621010682895, 0.0118295358531435, 0.601929630798962,
0.677233238299559, -0.620322188497616, 0.901650947775265, 0.229802002983503,
0.114385813284502, -0.287236365991277, 0.00362108510216535, 0.54836513813854,
-0.360900820378516, 0.901102331983843, 0.303232126663717, -0.389219867379597,
-0.389219867379597, -0.0638533091192526, 0.0172621246134547,
-0.0638533091192526, 0.036790744537595, 0.0267711437261105, 0.229802002983503,
2.31402345437609, 0.229802002983503, -0.620322188497616, 0.470621010682895,
-0.0638533091192526, 0, -0.0638533091192526, 0.683308812634821,
0.598730940691975, 0.229802002983503, 0.697360153425382, 0.00772365161682687,
-4.74600966429007, 0.593589435095411, -0.152099577506429, 0.0064938252361837,
-0.108940122157535, 0.0514396263381645, 1.10452888209261, 0.229802002983503,
-0.0638533091192526, 0, 0.0362637715949993, -2.79167649513235,
0.0221979095188955, 0.122798837911884, -3.07802860403649, 0.887963749928486,
-0.0638533091192526, -2.59080601794078, -0.0638533091192526,
-0.0720032601992142, 0.188653367126112, -0.390900634446388, 0.00626262322890335,
0.10449726079375, 0.190234594305548, 0.229802002983503, 0.470621010682895,
0.441447072301397, 0.710575973379983, 0.424723018059659, 0.0242933124111495,
-0.0638533091192526, 0.221839916416526, 0.470621010682895, -0.0108657434161119,
0.734169111179744, 0.465629851897761, 0.0252290916908343, -0.0261736586703599,
0.470621010682895, 0.00872967160561454, 0.820007851307147, -0.0638533091192526,
0.162601634884078, 0.15876421059534, 0.276805180633414, -0.386980840708979,
0.10228170638373, 0.0199458385530109, 0.470621010682895, 0.360613568422605,
0.0398196823378306, -0.389219867379597, -0.00269176292387149,
0.553663817786709, 0.00803266728255501, 0.470621010682895, -0.389219867379597,
0.0531985700905557, -0.00966390653254687, 0.948851286517476,
0, -0.0889303986002686, -0.289482205789089, 0.470621010682895,
-0.620322188497616, -1.33981685878991, 0.0183827591576367, 0.011135182844728,
-0.0638533091192526, 2.32607985155626, -0.138083520720171, 0.00471455153770216,
0.158058381916342, 0.00450504854498247, 0.0789824876681455, 0.0704699239866158,
-2.19086590383429, 0.0809155162971262, -0.0638533091192526, 0.0329404929656065,
0.0281676116282727, -0.389219867379597, 0.012790919597435, -0.00704771101800125,
0.0403529797537856, 0.0558406758822688, 0.164832137315436, -0.165567865123191,
-0.11103429903157, -0.0355024172803945, 9.48260587090123e-05,
0.470621010682895, -0.0495357136064147, 0.229802002983503, 0.0637860580674806,
-0.620322188497616, -0.0226657756569641, -0.389219867379597,
-0.316625748304681, 0.229802002983503, 0, 0.364124775102099,
-0.0310987139252845, 0.492424057922772, -0.224535737231339, -0.133435578504748,
0.229802002983503, 0.184389526960601, -3.97960671764283, 0.223470481648489,
-0.620322188497616, 1.57339312469411, -3.97007919079201, 0.028292556618311,
-0.0208634007145496, 0.253055735744466, -0.0638533091192526,
-1.93884051014153, 0.0301349400625612, -0.0638533091192526, 0.000295057056987376,
0, 0.286160612098488, 0.101207913446722, 0.374419900839882, -0.106217410937095,
0.229802002983503, 2.31067381143729, 2.09201022549168, 0.00385297198146129,
0.0512938203984638, -1.74049624153457, 0.426702921643898, -0.0638533091192526,
0.018935028523515, 0.23126745462646, 0.00767373530818363, 0.229802002983503,
3.84024898682852, -0.0638533091192526, 0.389584248100691, 0.0449003257302091,
1.02228506672615, 0.229802002983503, -0.0638533091192526, -0.0829134652872339,
3.50596545804498, -0.389219867379597, 0.9170368554337, -0.0638533091192526,
0.393117195150408, 0.228802985129773, 0.380899253549764, 0.232617830598094,
-0.389219867379597, 0.0459161417216973, 1.31722638132631, -0.151301202089443,
-0.116990991708309, 0.927131393095724, 0.287762794679405, 0.53782819173468,
-0.389219867379597, 0, 0.778671412967675, -0.389219867379597,
-0.0683403974328925, 0, 0.229802002983503, 0.473035927661744,
-0.620322188497616, 0.994155024290692, 0.229802002983503, 0.470621010682895,
-0.194201456791898, 0.735175033053973, 0, 8.3894395202509, -2.71421951365695,
1.63476338054133, 0, -0.358533993792374, -0.180900357181005,
0.464964929465083, 0.828110705634323, 0.290014059095831, 0.0265807736784206,
0.913325706925416, -0.620322188497616, 0.482569085367064, 0,
0.999426501553857, -0.0638533091192526, -0.894190901951266, 0.0608976332239153,
-0.389219867379597, 0.000920175777920006, 0.0444241832790267,
-0.389219867379597, 1.77775509259178, 0.00731201645980821, 0,
-0.620322188497616, 0.229802002983503, 5.07464022484589, 1.48600912617683,
0.037030406316896, 5.19861898546793, 0.614580107047209, 0.140671508556198,
-0.0638533091192526, 0.470621010682895, 1.44599607789379, 0.229802002983503,
0.0337669838054181, -0.339507807487096, -0.227604499761264, -0.0126140839785072,
0.329583322374931, -0.348480271857373, 0.229802002983503, 0.220851780815745,
-0.00466286305694716, 0.470621010682895, 1.94004188027446, 0.00769634829635375,
0.229802002983503, 0.229802002983503, -0.0638533091192526, 0.00196933273638366,
0.362886861307995, -0.010895368171175, 0.00517992475169629, 0.0121875763733234,
0.229802002983503, -0.389219867379597, -0.000556322059116982,
0.229802002983503, 0.00260711445755056, -0.58609877956866, 0,
0.173845846564558, 0.050042474875399, -0.608356727049781, 1.29406201978428,
-0.0207947874733201, 0.00589643228835565, 0.720668560955405,
-0.0638533091192526, 0.0325689306223551, -0.620322188497616,
-0.389219867379597, -0.0638533091192526, -0.620322188497616,
0.0384932081048732, -0.0349291605765668, 0.00798065241516178,
-0.134723236994354, -0.0638533091192526, 0.281152548562933, 0.785134308633961,
0.00911501343278954, 0.0290625580516694, 0.229802002983503, 0.0364502381509222,
0.0200267754432485, 0.470621010682895, -2.30392713054924, 0.0133977174531081,
0.0313438687192373, -0.453629796591262, -0.444456614744397, 0.470621010682895,
-0.00419309963021769, 0.229802002983503, 0.781666390558601, 0.263209713357162,
-0.00288720581609739, 0.0157812058748433, 0.377266856994834,
0.211102246795362, -0.0244674117557579, 0.502934845080243, 0.470621010682895,
-0.620322188497616, 0.119508324298399, 0.68156448564652, -0.389219867379597,
0.344388633359758, 0.541519544456095, 0.348203898915403, -0.617865773083779,
-0.620322188497616, -0.389219867379597, -0.630469325697215, -0.389219867379597,
-0.00185171176208817, 0.470621010682895, -0.0638533091192526,
0.0384110765023374, -0.476268477810135, 0.0275459003440332, -0.377613497242655,
0.480114279740138, 0.208177472628734, 0.0388621060580001, 0.314810488790625,
-0.389219867379597, 0.622343433254446, 0.229802002983503, 0,
0.0116064532794852, 0.640880718848922, 0.0159256710348949, 0.0294218642201815,
-0.11922554805783, 0.470621010682895, 0.0719964703043177, 0.176673405801693,
-0.216160070123196, -0.620322188497616, 0.263287565296575, 0.334468720690456,
-0.389219867379597, -0.389219867379597, 0, -0.0638533091192526,
-0.0638533091192526, 0.328005934417359, 1.98345092754637, -0.0638533091192526,
-0.322569885555602, 0.343583272797935, -0.0131035846556284, -0.0638533091192526,
0.332049938227108, 0.229802002983503, -0.184471568949188, 0.0738083137554319,
0.295094806935004, 2.00072915211246, 0.0524057350651284, -0.394994587233133,
0.11082820395266, 0.0470433275207449, -0.0244714107059427, -0.19392136026585,
-0.389219867379597, 1.54447845601115, 0, 1.50461847711675, 0.470621010682895,
-2.19230689436821, -2.15372326763204, 0.0602893383146939, -0.350076259362557,
-0.389219867379597, -0.0491349223300808, -0.347427839125497,
-0.125793370606986, -0.389219867379597, 0.218492017287635, -0.389219867379597,
-2.19203559916863, 0.470621010682895, -0.0756276343860818, -0.620322188497616,
0.039618360679447, 0.229802002983503, 0.342229794086586, 1.22342842687211,
-0.0638533091192526, -0.389219867379597, 0, -0.121360237566266,
0.470621010682895, 0.203522177713664, -2.66081323726908, 0.164981226545032,
0.208989371873146, 0.178977127763616, 0.00534834375159016, 0.0259183781780568,
-0.0638533091192526, -0.620322188497616, 3.77342257919255, 0.077694414226668,
-0.0638533091192526, -2.76001789665019, 0.93498899883197, 0.229802002983503,
0.229802002983503, -0.189889234701173, -0.878128024239223, 0.229802002983503,
-0.971246200836784, 0.00238384747429768, -0.620322188497616,
0.440261892604354, 3.76354782684596, 0.470621010682895, 3.86172131986622,
0.575298756860043, 12.9023010859722, 0.470621010682895, 0.378503973417637,
0.474891582933229, 0, -0.389219867379597, 0.520087612011093,
0.00160600701629905, -1.70670322718237, 0.229802002983503, -0.0349116284595025,
-0.389219867379597, 0.229802002983503, -0.0638533091192526, 0.0138641669555371,
-1.76999545498462, 0.470621010682895, -0.0638533091192526, 0.470621010682895,
0.568835467760944, 0.196290116211879, 0.00657090150018764, 1.15108667563071,
0.229802002983503, 1.39616453144586, -0.0847185084207546, -0.063387618790934,
0.470621010682895, 0.111214507604595, -0.0859882232308096, 0.229802002983503,
0.00453833599472268, 0.334589717726727, 0.470621010682895, -0.390600326717378,
0.254343160688718, -0.951335732362531, -0.0638533091192526, -0.389219867379597,
0.185995171841162, -0.0638533091192526, 0.0161178324221534, -0.702070844948495,
-0.466435025160672, 0.470621010682895, 0.470621010682895, -0.383876151221017,
-0.0638533091192526, -0.0638533091192526, 0.470621010682895,
-0.389219867379597, -0.0123155655016255, -0.386709341729674,
0.229802002983503, -0.620322188497616, 0.187872106293471, 1.0998198175547,
0.229802002983503, -0.0111456829865045, -0.620322188497616, -0.620322188497616,
0.0484181771965215, 0.29713373197861, 1.03416257683562, 0.470621010682895,
-1.14286511426204, -0.620322188497616, -0.620322188497616, 3.57860778510592,
0.229802002983503, -0.62520946847561, 0.290707988341369, 0.0924839451700828,
-0.292199030679553, 0.275232409230032, -0.0884019891746591, -0.613845322077624,
0.470621010682895, 0.27263770155515, 0.229802002983503, 0.470621010682895,
-0.616577036861926, -0.0638533091192526, 0.229802002983503, 0.103227620279027,
0.000325827725882409, -0.389219867379597, -0.0213033086486025,
1.1517268134498, -0.0027549233829829, -5.20036700047062, -0.389219867379597,
1.38009140442044, 0.188112123387496, -0.389219867379597, -0.0715310034898503,
0.229802002983503, -1.42113754281828, 0.0671280483594289, -0.866540256758301,
-0.389219867379597, -0.389219867379597, 0.112435457850701, -0.162158911168545,
0.470621010682895, 0.470621010682895, 0.0211915910476394, -0.00153721649859708,
-0.0638533091192526, -0.102776844490052)

> dput(HM2)
c(0.016497438441392, -0.0121005433105642, 0.0340103138875006,
0.0393128837416979, 0.144718671661076, 0.0775923152157973, 0.000894372752462123,
0.0129080340603581, -0.0022179741964203, 0.0540058630486591,
0.0383234745402589, -0.0142841740056228, -0.0113383036972718,
0.00764362896437113, 0.022570207227843, 0.0892462240318431, 0.060498120257716,
0.0432525311181986, 0.00222557516581898, 0.0493381989550945,
-0.0116459907029527, -0.112480015005373, 0.0182691285457092,
0.0248597234825647, 0.00754701511666922, 0.0471476327719148,
0.0625841102244406, 0.0303698631973417, 0.00781124380799404,
-0.0168950573556397, 0.0473901805181147, 0.00740505106540693,
-0.0152639817574294, 0.00916841478852549, 0.0409730632910991,
0.0625866632066982, 0.00664005076885597, 0.0103946833001679,
0.0243728672440622, 0.0722309706115829, 0.0174699387206196, -0.088714507487996,
-0.00504535647738195, 0.00927272338803572, -0.00707755664004029,
0.209001332673238, 0.0280975011236682, 0.0185107628508926, -0.0152895488479284,
0.0293166958720994, 0.0839281669393395, 0.0350355281775659, -0.0214691529922399,
-0.00698847060167527, 0.069853381162298, 0.00880702674287574,
0.0440111442331915, 0.0576762465158807, 0.0180025302908308, 0.0104372279684426,
0.0399937793788931, 0.0484415593089806, 0.0359627022827372, 0.0610406756211853,
0.0400425377047152, 0.00783749255154968, 0.0033626359803225,
0.0253846624900874, 0.028956317299215, 0.0220327419434368, 0.0055491218235435,
0.021359789760335, 0.0053563472471669, 0.00770764128276149, 0.120606533813112,
0.0338481150043749, 0.00480652155775551, -0.0208260153687132,
0.0264346940874102, 0.0254170849175835, 0.0549828722223003, -0.0197153554154837,
0.0146319132316291, 0.0223773253300461, 0.046744336162787, 0.0439389840326429,
0.0730303072697907, 0.0157399337823011, 0.0189220332073482, 0.0332902036920697,
-0.00629466834494764, 0.0674012192688983, 0.0200657055355507,
0.0101015050127517, 0.0425927774526406, 0.0282767872201141, 0.00648039037553307,
0.0515174537352363, 0.128342038159934, 0.0432170296083376, 0.01871577325763,
0.122157724187454, 0.0336635358211635, 0.0225239126708406, 0.0183439669703353,
0.134384074630845, 0.00984528984402488, -0.049573212443543, 0.0459003250172428,
0.0354638054442889, 0.0209690261585554, 0.0252976241664904, -0.0211582204487595,
0.00564472281613363, 0.0148258737510351, 0.0428454717776024,
0.0315492077820927, 0.0628515089974484, 0.034599813272583, 0.00983450218873532,
0.0790661896510682, 0.076097511515163, 0.015596924720301, 0.010641549474715,
0.00626447132858588, -0.0752885625254798, 0.0263105739632706,
0.0336726716607009, 0.0873459584037158, 0.0137508559242068, -0.00705208890843943,
0.0673463675870221, -0.000264192225152525, 0.0181803111819752,
0.0415821927667122, 0.0258248658300624, 0.0697171115138401, 0.0609029360829605,
0.0252388531172213, 0.0120346601757736, 0.0231724225476939, 0.00664147935356364,
0.00364229190085414, 0.0221357760157817, 0.00873744091784354,
-0.0243757212939979, 0.0609405123073909, -0.0110205054092022,
0.0570746855530399, -0.0541885933947211, 0.0790835216830754,
0.0443624834606028, 0.010803474486455, 0.012658784733822, 0.0407444149396382,
0.0758344977136341, 0.0249648665726016, 0.0706224364244773, 0.0617623493482699,
0.0157667711676134, 0.159594212558186, 0.10429461374287, 0.0575594583605458,
0.0629746059562333, 0.187704355999386, -0.121475803700469, 0.0316944060343089,
0.0446672218032357, 0.0144595666502411, 0.0581713078328564, 0.0240501430003371,
0.0169003110123663, -0.00441905663049872, 0.0398232696077516,
0.0488599473848877, 0.0333193570494318, -0.0165383461076467,
0.0908431754234104, 0.0145316828198416, 0.0268323550870429, -0.00807316790584186,
0.0643886942067867, 0.0132605944827886, 0.0225125532638365, -0.029400634077785,
0.0470539042942522, 0.0266903807836804, 0.0216184350463828, -0.00196619725935374,
-0.00896821424676157, 0.0471292741982513, 0.0153758143276018,
0.0391647510007153, 0.0638918758798112, 0.0350540963584708, 0.0621376260090577,
-0.0235965026923011, 0.0722625556919611, -0.00136537610871872,
0.00876826451613888, 0.0314000163087439, 0.0327764509794023,
0.0416905332713813, 0.0287500712769455, 0.0363887020395213, -0.00500079481628151,
0.0284155664844784, -0.0252462616989099, 0.0448399729520494,
0.000185492290551008, 0.0334348196455053, 0.0875709173367935,
0.0351964196451633, 0.232571060368655, 0.1090264053299, 0.0617423514625135,
0.0768976100544436, -0.0546347871751927, 0.0996882491247678,
0.0381052389461968, 0.0652248201643501, 0.0858993079060812, 0.0666492513259647,
0.0751282298723881, -0.0192055466287929, 0.0869162270419438,
0.0162427091867147, 0.0351089781252477, 0.0226340139536196, 0.0652987163108841,
0.0779638975412273, 0.117722278377793, -0.0394478774616255, 0.0438529807975176,
0.0881967320593799, 0.0588214648998437, -0.0184036676567495,
0.0140741349441621, 0.0161750696830995, 0.0118833913020066, 0.00937605882553054,
-0.0455051243509685, 0.0418377741899513, -0.0112867686098609,
0.0101123419174794, -0.00470124454451681, 0.000956333395138531,
-0.013981828106854, 0.05549118128867, -0.0224186715153298, 0.0186897582435067,
0.0169772716816739, -0.0179058332425567, -0.00353064879469387,
0.0269073443173487, 0.011369485145996, 0.0753871802564881, 0.184940758413157,
0.0962788588369411, 0.12233382584365, 0.0542611920398092, 0.0470414915942155,
0.0303505098818263, 0.00573452177124659, 0.00764243118994532,
0.0586726631015619, 0.106336058543696, -0.0222151019972231, 0.00630121965465554,
0.0624011763113852, 0.0372172786669568, 0.0226016288240546, 0.0939930244398409,
0.0767415904317717, 0.0238800474741997, 0.0151188247761029, 0.134843980893931,
-0.0023819116674762, 0.00287890065299971, 0.0303739452657181,
-0.00394798169311598, -0.00886454847938266, 0.0530899335995167,
-0.00526457886952308, 0.100251697020821, 0.0651999010773084,
0.0618461082131486, 0.0746686396348302, 0.0225108225108226, -0.0228981338001482,
-0.00958321904192527, 0.0301744325038945, 0.0510329188455982,
-0.0184730200018259, 0.0702506964772114, 0.0167199245415362,
0.0790156649374397, -0.028524529551306, 0.0133581966776628, 0.025585253422798,
0.172427142313816, 0.0675994203009362, 0.00336311873127013, 0.0493948607018135,
0.218437612231768, 0.0214594460427004, 0.0907841657749202, 0.0333163617639509,
0.177206388698409, 0.126401976128862, 0.0399950893161636, 0.0501927169716304,
0.0893264027232592, 0.0157983828191964, 0.0615798877852506, -0.0197129750106304,
-0.0108884320377597, 0.0480698262099342, 0.00512957625928131,
0.0157721009527848, 0.00576798852432315, 0.132233222996491, 0.0427798891731844,
0.0213946620292803, 0.137026176169503, 0.106996875883653, -0.0054111837253851,
0.0251880361118362, 0.0253902053162975, 0.0270577225980805, 0.0124293785310734,
0.0743211614709804, 0.0322779541039503, -0.000515082314443601,
-0.15098468271335, 0.0705935165241118, 0.00396949242984507, 0.00570413156689748,
-0.028337357038222, 0.067618643584239, 0.215558122382694, -0.0902178675044036,
0.0116089174537522, 0.0163203173112244, 0.053199024052846, 0.190954107085158,
-0.00965520699607334, 0.0137508862783826, 0.0551559553375383,
0.0522566319201845, 0.0120000372271673, 0.0661660238047397, 0.0393034160850182,
0.0519061657312619, -0.0104570268884604, 0.0691173523364065,
0.0161805435668569, 0.017889600764444, 0.00112726390517828, 0.0127812045951454,
-0.0689255559600801, 0.0231502379228566, 0.0350293488462002,
0.0474816278264511, -0.00418170038125466, 0.0353685604292087,
0.0176274799482486, 0.0430171958669913, 0.0875537735181209, 0.070485815496694,
0.0135133879845454, 0.0128708660489427, 0.051611788565405, 0.0227212087169909,
0.0073359858475243, 0.00404515320751896, -0.00440846200360685,
0.0659739938613625, 0.0460166710841459, 0.0164323471803848, -0.00986110969267727,
-0.0393527714903421, 0.0187532041738799, 0.0120528540944261,
0.048649745777751, -0.016576662520347, 0.00722036925799145, 0.0532607499622092,
0.0287269581620362, 0.098387602988942, 0.0515373332109274, -0.00594807446048738,
-0.012996698372473, 0.186569861135154, 0.030123906032987, 0.0853131761769354,
0.0031929989708665, 0.0555338071316984, 0.0330892102352149, 0.0325300150784036,
-0.00129426068643155, 0.0964465282035648, 0.0748083984068019,
0.014810272368951, 0.0727362482346641, -0.022994161418619, 0.0176067452321714,
0.0316353702314177, 0.0366753960255026, 0.132973503081211, 0.00537653915699314,
0.0293447867014306, 0.00587487992108688, -0.0465459905172109,
-0.0208313288728689, 0.0461567115064294, -0.00992439500984909,
0.0478692174283319, 0.15870237135342, 0.0555146507925898, 0.0929129911374694,
0.136269426745194, 0.032515127489143, -0.0177415950997095, 0.0242936475495824,
0.0128679606829178, 0.00550255711518526, 0.00950135633958283,
-0.0171614203848253, -0.0480106149706695, 0.0436386144400421,
0.030020737993979, -0.0207207590715261, 0.118800465447344, 0.0287429378832495,
0.0277036046980229, -0.0326376675279461, 0.0111603851297697,
0.0168167717634637, 0.0543502952338948, 0.0268780423509474, 0.0244551896836229,
0.0330102007094411, 0.049560008022367, 0.0830113020712642, -0.0360281667532304,
0.0254731639150768, 0.019583911716062, 0.0311623362566958, 0.00306244869413074,
0.0687257755440167, 0.0205871906806436, -0.00980792960340933,
0.0430516082564022, 0.0402780110730929, 0.0705036986824154, 0.025854270339128,
0.108621667399996, 0.00718685874587846, 0.0571556864539545, 0.109159849895189,
-0.00840749532166959, 0.0463361727990188, 0.0651635196532618,
0.107210781390985, 0.134765229575272, 0.00173368529272275, -0.0123735133796356,
-0.0136090591818897, 0.0534703507636681, 0.0252682115120829,
0.0045029871851435, -0.032195767437093, 0.0467488481089099, 0.0538227606085845,
-0.0118386798369731, -0.0280299494295351, 0.0140538628567341,
0.0305966571267272, -0.00988880417457172, -0.00202667375935174,
-0.0227460178374512, 0.0117906927254463, 0.0508373391209442,
0.20565913566541, 0.079177499709108, 0.0422151179934303, 0.084419174697904,
0.0672922694952164, -0.00562397110021092, 0.0565550716302232,
0.0189608731051132, 0.0453269153630163, 0.0409743262622376, 0.0726855522439788,
0.0185422373323943, 0.0700699839671905, -0.00222797184477677,
-0.00250681151008622, 0.0255048645524291, 0.000885511372841601,
0.0788210113117177, 0.0116542476851704, 0.031007656424076, 0.011572121687957,
0.00197091513991245, 0.0332881463280234, 0.0115574465474555,
-0.0156436963370336, 0.224908551812914, 0.0160024082692785, 0.0108175914929918,
0.0850714918317297, -0.0209549958807167, 0.0649434433474199,
0.0572507308261867, 0.0108695652173913, 0.0149885395421411, 0.0504013409726928,
0.000985727295810551, 0.00206155161241882, -0.0273818619348245,
0.0117953575610268, 0.0160740457598691, 0.0439509870338097, 0.000374131432729126,
0.030965232935552, 0.0680925204656592, -0.040084138491466, 0.0579554457139488,
-0.0280709032744379, 0.0245619981154038, 0.0332658264722017,
0.0307493365246973, 0.0663529386077719, 0.0108866234143253, 0.0504739490524921,
0.0110051937760747, 0.114486710683534, 0.0885070446505914, 0.00713719270420306,
0.027840282585194, 0.0502624411934901, -0.0237152032254946, -0.0783599699259493,
0.00584404515799187, -0.031333005510123, 0.0191137823580835,
0.00399073109721866, -0.00489024968946272, -0.00926327094032102,
0.00380235683181465, 0.157304505375375, 0.0200440859609643, -0.0241686272566629,
-0.0800316610669412, 0.0239679002814968, 0.0538951680014091,
0.0941742251268998, 0.0521115186872178, -0.123540922378836, 0.0542314864434063,
0.0343854708502029, 0.0238689195785379, 0.123488443091325, 0.00983601617163751,
-0.00112278368930967, -0.0210112398529677, -0.0179204649311611,
-0.00187219424504028, -0.0094461471370453, 0.0616047615895669,
0.0582469526595368, 0.00304799229604126, 0.0674836370082559,
0.0853131761769354, 0.0358226068729712, 0.00994622612345991,
-0.0217167015464964, 0.0526997601984295, -0.0232294161006039,
0.100227237031891, 0.0294429036389532, 0.0172507494588479, 0.0034823068051,
0.0322430509250401, 0.0401401293054765, 0.0208112362723038, 0.00561429336771978,
0.037707481438362, 0.00862436025594276, 0.00476662316192879,
0.15812712691603, 0.0472541237269534, -0.00949623142375768, 0.00855607871651576,
-0.00281567087439197, 0.0397525174385432, -0.00711442815443551,
-0.00131577619047549, 0.106909062371648, 0.025164340439287, 0.0577629820111593,
0.0939246700745592, 0.00391223125066969, 0.0124049752133815,
-0.0275935528232096, 0.0762231355953777, 0.181010165393349, -0.0347198712523113,
0.0678716698457551, 0.0120798630452986, 0.0245162767779995, 0.0498518908504713,
0.0144272270551826, 0.0449534685914403, 0.000956463351216132,
0.0460805768375294, 0.0483340329448637, 0.0192578144013228, 0.0713319833154848,
0.0664774807491022, 0.0136479583643586, 0.047338990736079, -0.00984473290135361,
0.0791372581960931, 0.0256142818179614, 0.0598611692250246, 0.0265350466465605,
0.068670703501864, 0.00623102828923622, -0.00469207491878694,
0.157504094271494, 0.116984111424978, 0.0875878669508734, 0.0161726786019788,
0.175935713544914, 0.0637705286819113, 0.0197080817743348, 0.0452129305548087,
0.0233338613388849, 0.0936152099345934, 0.0199808837233982, 0.0274312796611153,
0.0721161218673364, 0.17082095580651, 0.034052131044711, 0.0512562246908417,
0.0487107440041036, 0.153481867586958, 0.0213458465510684, 0.0133431492518521,
0.119009291067923, 0.0253607374472143, 0.0202017481667936, 0.011516647246353,
0.0339052007553662, 0.0313036367875989, -0.00639827115782904,
0.0237304943165091, 0.00669172141737004, 0.0854219038259585,
0.0181012761967982, 0.0372169953336968, 0.219164421366707, 0.0387279347673063,
-0.00326722366281047, 0.0611109647647912, 0.0724982521544455,
0.0114916829563205, 0.0264328043306142, 0.100227237031891, 0.0694579153964423,
0.0256998227540187, 0.0937017943889341, 0.0137629525760519, -0.0046036224834336,
0.0270242177828994, 0.0650551686301645, 0.01402255124967)


very many thanks for your time and effort...
yours sincerely

AKSHAY M KULKARNI


________________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
Sent: Wednesday, March 20, 2019 1:32 PM
To: R help Mailing  list
Subject: [R] problem with nlsLM.....

dear members,
                             Yesterday, Duncan identified a silly mistake in an nls call...in the list of starting values, one of the names of the variable was getting repeated. But I am experiencing the same problem again, with a different formula:

> formulaDH3
HM1 ~ (a + (b * ((HM2 + 0.3)^(1/3)))) * (c * log(HM3 + 27))

here HM1 is the response variable, and HM2 and HM3 are predictors.....

> nonlin_modDH3 <- nls(formulaDH3, start = list(a = 0.43143, b = 0.68173,c = 0.02954))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates
> nonlin_modDH3 <- nlsLM(formulaDH3, start = list(a = 0.43143, b = 0.68173,c = 0.02954))
Error in nlsModel(formula, mf, start, wts) :
  singular gradient matrix at initial parameter estimates

I am using nlsLM function from the minpack.lm package which says that it will converge when nls fails with singular gradient matrix error...


Is there again a silly mistake(pardon me again if there is one!), or is the problem serious? If it is serious, any pointers towards a solution?

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From |r@nk|eboytje @end|ng |rom hotm@||@com  Wed Mar 20 10:15:44 2019
From: |r@nk|eboytje @end|ng |rom hotm@||@com (Frank van Berkum)
Date: Wed, 20 Mar 2019 09:15:44 +0000
Subject: [R] [mgcv] Memory issues with bam() on computer cluster
In-Reply-To: <AM0PR0302MB3458D10A11E1A8ED95EAF627BC410@AM0PR0302MB3458.eurprd03.prod.outlook.com>
References: <mailman.353984.1.1552734002.30403.r-help@r-project.org>,
 <AM0PR0302MB3458D10A11E1A8ED95EAF627BC410@AM0PR0302MB3458.eurprd03.prod.outlook.com>
Message-ID: <AM0PR0302MB345808E17FA547E951E52C36BC410@AM0PR0302MB3458.eurprd03.prod.outlook.com>

Dear Simon,

Thank you for your response! I was not able to provide you with the requested information at an earlier stage since I am not a full time academic / researcher.

An example of a bam call that may result in an error is:
bam(formula=Di ~ 1 + Gender + I(L_Dis==0) + s(DisPerc, by=as.numeric(L_Dis==2), bs='cr'), offset=log(Ei*Mi), family=poisson, data=dtPF, method="fREML", discrete=TRUE, gc.level=2);

Here, dtPF is a data.table object with 22m rows and 21 columns/variables, Gender is a factor variable, L_Dis is an integer variable which equals 0 if DisPerc is missing (manually set to 0.1), equals 1 if DisPerc==0, and equals 2 if DisPerc>0 (ranges from 0 to 0.25).

The sessionInfo() provides the following output:
R version 3.4.3 (2017-11-30)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux 9 (stretch)

Matrix products: default
BLAS/LAPACK: /sara/eb/Debian9/OpenBLAS/0.2.20-GCC-6.4.0-2.28/lib/libopenblas_sandybridgep-r0.2.20.so

locale:
 [1] LC_CTYPE=en_US       LC_NUMERIC=C         LC_TIME=en_US
 [4] LC_COLLATE=en_US     LC_MONETARY=en_US    LC_MESSAGES=en_US
 [7] LC_PAPER=en_US       LC_NAME=C            LC_ADDRESS=C
[10] LC_TELEPHONE=C       LC_MEASUREMENT=en_US LC_IDENTIFICATION=C

attached base packages:
[1] methods   stats     graphics  grDevices utils     datasets  base

other attached packages:
[1] mgcv_1.8-27       nlme_3.1-137      data.table_1.12.0

loaded via a namespace (and not attached):
[1] compiler_3.4.3  Matrix_1.2-16   tools_3.4.3     splines_3.4.3
[5] grid_3.4.3      lattice_0.20-38

Thank you for your help!

Frank

________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of r-help-request at r-project.org <r-help-request at r-project.org>
Sent: Saturday, March 16, 2019 11:00 AM
To: r-help at r-project.org
Subject: R-help Digest, Vol 193, Issue 16

Send R-help mailing list submissions to
        r-help at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
        https://stat.ethz.ch/mailman/listinfo/r-help
or, via email, send a message with subject or body 'help' to
        r-help-request at r-project.org

You can reach the person managing the list at
        r-help-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-help digest..."


Date: Fri, 15 Mar 2019 12:31:31 +0000
From: Simon Wood <simon.wood at bath.edu>
To: r-help at r-project.org
Subject: Re: [R] [mgcv] Memory issues with bam() on computer cluster
Message-ID: <d8e2643a-d960-0d86-4296-f0c7fcf149cb at bath.edu>
Content-Type: text/plain; charset="utf-8"

Can you supply the results of sessionInfo() please, and the full bam
call that causes this.

best,

Simon (mgcv maintainer)

On 15/03/2019 09:09, Frank van Berkum wrote:
> Dear Community,
>
> In our current research we are trying to fit Generalized Additive Models to a large dataset. We are using the package mgcv in R.
>
> Our dataset contains about 22 million records with less than 20 risk factors for each observation, so in our case n>>p. The dataset covers the period 2006 until 2011, and we analyse both the complete dataset and datasets in which we leave out a single year. The latter part is done to analyse robustness of the results. We understand k-fold cross validation may seem more appropriate, but out approach is closer to what is done in practice (how will one additional year of information affect your estimates?).
>
> We use the function bam as advocated in Wood et al. (2017), and we apply the following options: bam(?, discrete=TRUE, chunk.size=10000, gc.level=1). We run these analyses on a computer cluster (see https://userinfo.surfsara.nl/systems/lisa/description for details), and the job is allocated to a node within the computer cluster. A node has at least 16 cores and 64Gb memory.
>
> We had expected 64Gb of memory to be sufficient for these analyses, especially since the bam function is built specifically for large datasets. However, when applying this function to the different datasets described above with different regression specifications (different risk factors included in the linear predictor), we sometimes obtain errors of the following form.
>
> Error in XWyd(G$Xd, w, z, G$kd, G$ks, G$ts, G$dt, G$v, G$qc, G$drop, ar.stop,  :
>
>    'Calloc' could not allocate memory (22624897 of 8 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> XWyd
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> Error in Xbd(G$Xd, coef, G$kd, G$ks, G$ts, G$dt, G$v, G$qc, G$drop) :
>
>    'Calloc' could not allocate memory (18590685 of 8 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> Xbd
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> Error: cannot allocate vector of size 1.7 Gb
>
> Timing stopped at: 2 0.556 4.831
>
> Error in system.time(oo <- .C(C_XWXd0, XWX = as.double(rep(0, (pt + nt)^2)),  :
>
>    'Calloc' could not allocate memory (55315650 of 24 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> XWXd -> system.time -> .C
>
> Timing stopped at: 1.056 1.396 2.459
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> The errors seem to arise at different stages in the optimization process. We have analysed whether these errors disappear if different settings are used (different chunk.size, different gc.level), but this does not resolve our problem. Also, the errors occur on different datasets when using different settings, and even when using the same settings it is possible that an error that occurred on dataset X in one run it does not necessarily occur on dataset X in a different run. When using the discrete=TRUE option, optimization can be parallelized, but we have chosen to not employ this feature to ensure memory does not have to be shared between parallel processes.
>
> Naturally I cannot share our dataset with you which makes the problem difficult to analyse. However, based on your collective knowledge, could you pinpoint us to where the problem may occur? Is it something within the C-code used within the package (as the last error seems to indicate), or is it related to the computer cluster?
>
> Any help or insights is much appreciated.
>
> Kind regards,
>
> Frank
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Simon Wood, School of Mathematics, University of Bristol, BS8 1TW UK
https://people.maths.bris.ac.uk/~sw15190/

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Wed Mar 20 10:36:02 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 20 Mar 2019 12:36:02 +0300
Subject: [R] problem with nlsLM.....
In-Reply-To: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <20190320123602.6323c718@trisector>

On Wed, 20 Mar 2019 08:02:45 +0000
akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> formulaDH5 <- as.formula(HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) +
> (A*sin(w*HM3 + c) + C))

The problem with this formula is simple: the partial derivative with
respect to `a` is the same as the partial derivative with respect to
`C`. This makes the regression problem have an infinite number of
solutions, all of them satisfying equation \lambda_1 * a + \lambda_2 *
C + \lambda_3 = 0 for some values of \lambda_i. Gradient-based
optimizers (which both nls and nlsLM are) don't like problems with
non-unique solutions, especially when the model function has same
partial derivative with respect to different variables, making them
indistinguishable.

Solution: remove one of the variables.

> > formulaDH3  
> HM1 ~ (a + (b * ((HM2 + 0.3)^(1/3)))) * (c * log(HM3 + 27))

The problem with this formula is similar, albeit slightly different.

Suppose that (a, b, c) is a solution. Then (\lambda * a, \lambda * b,
c / \lambda) is also a solution for any real \lambda. Once again,
removing `c` should get rid of ambiguity.

> I've checked the Internet  for a method of getting the starting
> values, but they are not comprehensive....any resources for how to
> find the starting values?

Starting values depend on the particular function you are trying to
fit. The usual approach seems to be in transforming the formula and
getting rid of parts you can safely assume to be small until it looks
like linear regression, or applying domain specific knowledge (e.g.
when trying to it a peak function, look for the biggest local maximum
in the dataset).

If you cannot do that, there also are global optimization algorithms
(see `nloptr`), though they still perform better on some problems
and worse on others. It certainly helps to have upper and lower
bounds on all parameter values. I've heard about a scientific group
creating a pool of many initial Levenberg-Marquardt parameter estimates,
then improving them using a genetic algorithm. The whole thing
"converged overnight" on a powerful desktop computer.

-- 
Best regards,
Ivan


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Mar 20 10:43:11 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 20 Mar 2019 09:43:11 +0000
Subject: [R] problem with nlsLM.....
In-Reply-To: <20190320123602.6323c718@trisector>
References: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <20190320123602.6323c718@trisector>
Message-ID: <SL2P216MB009121AE0D6536279AFD656CC8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear Ivan,
                   THanks for the reply......
But doesn't removing some of the parameters reduce the precision of the relationship between the response variable and the predictors(inefficient estimates of the coefficients)?

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI
________________________________
From: Ivan Krylov <krylov.r00t at gmail.com>
Sent: Wednesday, March 20, 2019 3:06 PM
To: akshay kulkarni
Cc: R help Mailing list
Subject: Re: [R] problem with nlsLM.....

On Wed, 20 Mar 2019 08:02:45 +0000
akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> formulaDH5 <- as.formula(HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) +
> (A*sin(w*HM3 + c) + C))

The problem with this formula is simple: the partial derivative with
respect to `a` is the same as the partial derivative with respect to
`C`. This makes the regression problem have an infinite number of
solutions, all of them satisfying equation \lambda_1 * a + \lambda_2 *
C + \lambda_3 = 0 for some values of \lambda_i. Gradient-based
optimizers (which both nls and nlsLM are) don't like problems with
non-unique solutions, especially when the model function has same
partial derivative with respect to different variables, making them
indistinguishable.

Solution: remove one of the variables.

> > formulaDH3
> HM1 ~ (a + (b * ((HM2 + 0.3)^(1/3)))) * (c * log(HM3 + 27))

The problem with this formula is similar, albeit slightly different.

Suppose that (a, b, c) is a solution. Then (\lambda * a, \lambda * b,
c / \lambda) is also a solution for any real \lambda. Once again,
removing `c` should get rid of ambiguity.

> I've checked the Internet  for a method of getting the starting
> values, but they are not comprehensive....any resources for how to
> find the starting values?

Starting values depend on the particular function you are trying to
fit. The usual approach seems to be in transforming the formula and
getting rid of parts you can safely assume to be small until it looks
like linear regression, or applying domain specific knowledge (e.g.
when trying to it a peak function, look for the biggest local maximum
in the dataset).

If you cannot do that, there also are global optimization algorithms
(see `nloptr`), though they still perform better on some problems
and worse on others. It certainly helps to have upper and lower
bounds on all parameter values. I've heard about a scientific group
creating a pool of many initial Levenberg-Marquardt parameter estimates,
then improving them using a genetic algorithm. The whole thing
"converged overnight" on a powerful desktop computer.

--
Best regards,
Ivan

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Wed Mar 20 11:38:37 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 20 Mar 2019 13:38:37 +0300
Subject: [R] problem with nlsLM.....
In-Reply-To: <SL2P216MB009121AE0D6536279AFD656CC8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <20190320123602.6323c718@trisector>
 <SL2P216MB009121AE0D6536279AFD656CC8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <20190320133837.7fbcd615@trisector>

On Wed, 20 Mar 2019 09:43:11 +0000
akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> But doesn't removing some of the parameters reduce the precision of
> the relationship between the response variable and the
> predictors(inefficient estimates of the coefficients)?

No, it doesn't, since there is already more variables in the formula
than it has relationships between response and predictors.

Let me offer you an example. Suppose you have a function y(x) = a*b*x +
c. Let's try to simulate some data and then fit it:

# choose according to your taste
a <- ...
b <- ...
c <- ...

# simulate model data
abc <- data.frame(x = runif(100))
abc$y <- a*b*abc$x + c
# add some normally distributed noise
abc$y <- abc$y + rnorm(100, 0, 0.01)

Now try to fit formula y ~ a*b*x + c using data in data frame abc. Do
you get any results? Do they match the values you have originally
set?[*]

Then try a formula with the ambiguity removed: y ~ d*x + c. Do you get a
result? Does the obtained d match a*b you had originally set?

Note that for the d you obtained you can get an infinite amount of
(a,b) tuples equally satisfying the equation d = a*b and the original
regression problem, unless you constrain a or b.

-- 
Best regards,
Ivan

[*] Using R, I couldn't, but the nonlinear solver in gnuplot is
sometimes able to give *a* result for such a degenerate problem when
data is sufficiently noisy. Of course, such a result usually doesn't
match the originally set variable values and should not be trusted.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Mar 20 12:29:30 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 20 Mar 2019 11:29:30 +0000
Subject: [R] problem with nlsLM.....
In-Reply-To: <20190320133837.7fbcd615@trisector>
References: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <20190320123602.6323c718@trisector>
 <SL2P216MB009121AE0D6536279AFD656CC8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <20190320133837.7fbcd615@trisector>
Message-ID: <SL2P216MB00912C658B2DD5986A441999C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear Ivan,
                   Thank you very much...You have been very helpful....

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

________________________________
From: Ivan Krylov <krylov.r00t at gmail.com>
Sent: Wednesday, March 20, 2019 4:08 PM
To: akshay kulkarni
Cc: R help Mailing list
Subject: Re: [R] problem with nlsLM.....

On Wed, 20 Mar 2019 09:43:11 +0000
akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> But doesn't removing some of the parameters reduce the precision of
> the relationship between the response variable and the
> predictors(inefficient estimates of the coefficients)?

No, it doesn't, since there is already more variables in the formula
than it has relationships between response and predictors.

Let me offer you an example. Suppose you have a function y(x) = a*b*x +
c. Let's try to simulate some data and then fit it:

# choose according to your taste
a <- ...
b <- ...
c <- ...

# simulate model data
abc <- data.frame(x = runif(100))
abc$y <- a*b*abc$x + c
# add some normally distributed noise
abc$y <- abc$y + rnorm(100, 0, 0.01)

Now try to fit formula y ~ a*b*x + c using data in data frame abc. Do
you get any results? Do they match the values you have originally
set?[*]

Then try a formula with the ambiguity removed: y ~ d*x + c. Do you get a
result? Does the obtained d match a*b you had originally set?

Note that for the d you obtained you can get an infinite amount of
(a,b) tuples equally satisfying the equation d = a*b and the original
regression problem, unless you constrain a or b.

--
Best regards,
Ivan

[*] Using R, I couldn't, but the nonlinear solver in gnuplot is
sometimes able to give *a* result for such a degenerate problem when
data is sufficiently noisy. Of course, such a result usually doesn't
match the originally set variable values and should not be trusted.

	[[alternative HTML version deleted]]


From pro|jcn@@h @end|ng |rom gm@||@com  Wed Mar 20 13:45:14 2019
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 20 Mar 2019 08:45:14 -0400
Subject: [R] Fw: problem with nlsLM.....
In-Reply-To: <SL2P216MB0091E7E47082786F87FDA0D6C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <SL2P216MB0091E7E47082786F87FDA0D6C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <fe19b5fd-2021-42af-7f97-ffdca44bf281@gmail.com>

Of course, you might just try a more powerful approach. Duncan responded to the obvious issue earlier,
but the second problem seems to need the analytic derivatives of the nlsr package. Note that
nlsLM uses the SAME very simple forward difference derivative approximation for the Jacobian.
Optimization folk will generally say "singular Jacobian" and the singular values reported by
nlsr show how bad the case is here. Note that the nlsr output summary seems to imply that
the singular values are associated with specific parameters. My bad -- I wanted to keep
the summary tidy. The sv's apply to the problem in general, but there are as many of them as
parameters, so they fit on the page nicely where I've put them. The singularity leads to no
standard errors, of course.

JN

>> library(nlsr)
>> nonlin_modDH5 <- nlxb(formulaDH5, start = list(a = 0.43143, b = 0.68173,A = 0.09,w = 0.8,c = 0.01,C = 0.94))
> vn:[1] "HM1" "a"   "b"   "HM2" "A"   "w"   "HM3" "c"   "C"  
> no weights
>> nonlin_modDH5
> nlsr object: x 
> residual sumsquares =  0.66987  on  680 observations
>     after  15    Jacobian and  20 function evaluations
>   name            coeff          SE       tstat      pval      gradient    JSingval   
> a              0.0929101            NA         NA         NA  -5.817e-07       42.97  
> b               0.541246            NA         NA         NA  -3.359e-07       7.043  
> A             0.00921275            NA         NA         NA    9.42e-08       0.984  
> w               0.545654            NA         NA         NA  -8.693e-09       0.195  
> c              -0.770181            NA         NA         NA    -5.2e-09     0.07464  
> C                0.60148            NA         NA         NA  -5.817e-07   7.567e-14  
>> 



On 2019-03-20 4:20 a.m., akshay kulkarni wrote:
> dear members,
>                              I think Duncan's  solution is not working...I've checked the function again:
> 
>> formulaDH5 <- as.formula(HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) + (A*sin(w*HM3 + c) + C))
>> nonlin_modDH5 <- nls(formulaDH5, start = list(a = 0.43143, b = 0.68173,A = 0.09,w = 0.8,c = 0.01,C = 0.94))
> Error in nlsModel(formula, mf, start, wts) :
>   singular gradient matrix at initial parameter estimates
>> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 0.43143, b = 0.68173,A = 0.09,w = 0.8,c = 0.01,C = 0.94))
> Error in nlsModel(formula, mf, start, wts) :
>   singular gradient matrix at initial parameter estimates
> 
> 
> I've checked the Internet  for a method of getting the starting values, but they are not comprehensive....any resources for how to find the starting values?
> 
>> dput(HM1)
> c(1.01484280343579, 0.977004104656811, 1.02448653468064, 1.01562102015533,
> 0.968359321714289, 0.99105072887847, 0.99384582083167, 1.00579414727333,
> 0.967593120445675, 1.04694148058855, 1.03832347454026, 0.971040156128456,
> 0.976967848281943, 0.990386686964446, 1.02257020722784, 0.994529161072552,
> 1.05361176882747, 0.988021514764882, 0.998909692369571, 1.04933819895509,
> 0.988354009297047, 0.878584020586706, 1.01438895222572, 1.01522203155842,
> 0.999979601913003, 0.975862934766315, 1.03948445565434, 1.02493339899053,
> 0.972267168470219, 0.971001051421653, 1.03066167719129, 0.967898970631469,
> 0.976606662010519, 0.990061873979734, 1.03398666689317, 1.05491455733517,
> 0.995997711923663, 0.991213393857838, 0.9903216916016, 1.06850794640807,
> 1.01345738121581, 0.877847194198055, 0.992124544661245, 1.00927272338804,
> 0.988196514422814, 0.987504905313256, 0.990274516132497, 1.01484624802751,
> 0.9800176904083, 0.969321725342799, 1.07407427451262, 1.02023130729371,
> 0.975827723120998, 0.988260886231393, 1.05116598585815, 0.977996171738906,
> 0.985118207891832, 0.98243346941999, 0.991380612684958, 1.00960741365585,
> 1.03062446605116, 1.04176358759364, 1.02494798972349, 1.04645592406625,
> 0.999998705820515, 0.972161829098397, 0.991313521356105, 1.0211561076261,
> 0.971257832217016, 1.00935026461508, 0.997140402835885, 1.00719609467891,
> 0.97483865743955, 0.987491380127905, 1.0643888147255, 1.00804564512905,
> 0.989366232219398, 0.942042386899482, 1.0140328534339, 0.98812919092058,
> 1.04584882138055, 0.970586857498767, 1.0098232311784, 1.02237732533005,
> 0.99306513943649, 0.963808401790738, 1.03577231048959, 1.00540101137939,
> 0.970458918298484, 0.973460294926879, 0.992676651187501, 1.04079931972326,
> 0.988928168736541, 0.986376934855979, 1.04117941173535, 1.01252015337269,
> 0.920578394310677, 0.98933093765412, 0.98116698970429, 0.995498900493761,
> 0.96463760451942, 1.1059686605887, 1.03366353582116, 0.981256131441973,
> 1.0152599452654, 0.995362080418312, 0.997276311537458, 0.943438355295013,
> 1.04025117202718, 1.03102737097366, 1.02096902615856, 1.02506560388494,
> 0.959364103040633, 0.987217067188943, 1.00454456275356, 1.03517749036747,
> 1.0148342900634, 0.974518005568706, 1.02515903830848, 1.00435361604303,
> 0.973400954462685, 0.981702992961201, 1.0155969247203, 1.01064154947471,
> 0.957131570492054, 0.88504625471737, 1.01849907662046, 1.01503806358198,
> 0.987004149653273, 0.974485857983481, 0.987572979379099, 1.0037529762248,
> 0.976026658183349, 0.983176178747953, 1.03812943411666, 1.01828203593425,
> 0.984139742592733, 1.00377243020886, 1.02118652563454, 1.01115776483144,
> 0.98387406692053, 0.985919715423209, 0.989550590454508, 1.01845373071313,
> 1.00394660346757, 0.949076679285431, 1.06094051230739, 0.981940494629297,
> 0.982812578115724, 0.929673163266673, 1.05359335975355, 1.03498999963467,
> 0.989218608625026, 0.994412680504384, 1.03162650392014, 1.04359975620911,
> 0.96938669600183, 0.987150178669349, 0.990503131271205, 1.01120231626779,
> 0.977334529946535, 0.966013274319284, 1.04966722359666, 1.04745672849702,
> 0.960799643211444, 0.866626315091455, 1.01316731305781, 1.02919067036911,
> 0.988567285725018, 0.978702298855537, 1.0186604054056, 1.01398864226215,
> 0.985492431198792, 1.03982326960775, 1.02287049736119, 1.02357106122821,
> 0.977519287403578, 0.993818572820706, 1.01139405704906, 1.02683235508704,
> 0.943370973180458, 0.970153991236375, 0.970746163945049, 1.00996886938921,
> 0.970599365922215, 1.03230385881209, 1.02072718512322, 1.0161024503161,
> 0.987718207880278, 0.987512809150475, 1.04108232940019, 1.0153758143276,
> 1.03916475100072, 0.959776942350879, 1.01739288375256, 1.05438479662213,
> 0.970479916939806, 0.979826128477137, 0.982800613999074, 0.977050857426291,
> 0.977474025608337, 0.972905352371901, 1.02191626900472, 1.02737043327345,
> 0.987565251408109, 0.93131925605196, 0.991469304659888, 0.949708678221412,
> 0.948662234856045, 0.938158485016718, 1.02342896552764, 1.01869899424866,
> 1.01244484998263, 0.990637832491416, 1.04591108144934, 0.951244278685872,
> 0.977265525807748, 0.942206977147219, 1.09440128638859, 1.02935716108991,
> 0.978428723706514, 1.08589930790608, 1.0015813231405, 1.07512822987239,
> 0.976356469419301, 0.972775004620642, 1.00954673593815, 0.992551447194369,
> 0.985819189451289, 0.975310750212573, 1.05999783258221, 1.07815688799274,
> 0.959648862885262, 1.03418767541976, 1.07207529899183, 1.04949264142055,
> 0.966872387358102, 0.963564695272614, 1.01310836328613, 0.996622968927382,
> 1.00202486063801, 0.948215304098709, 1.02845448262427, 0.987124642586768,
> 0.999269328077687, 0.964674178364545, 0.991913103743918, 0.970556592778505,
> 0.979495816235886, 0.949160643807912, 0.999611005677085, 1.01697727168167,
> 0.966362681248743, 0.938703012004999, 1.01394761650376, 1.011369485146,
> 0.975929406359934, 0.97111480362557, 1.0581474202687, 1.03965938660391,
> 0.990447147628825, 0.967286377976687, 0.968162001740811, 1.00573452177125,
> 0.974628505381728, 0.983183640545729, 1.08185959707149, 0.976533333333333,
> 0.932582771965921, 1.02205682784386, 1.02162002635618, 1.00634335266967,
> 1.03045302554561, 1.05015537832235, 1.0238800474742, 1.0151188247761,
> 0.951176244898346, 0.937999477475998, 0.986664690881963, 1.00889938014614,
> 0.972546365957459, 0.937560562249233, 0.953227957137494, 0.964284336810156,
> 0.977909815090698, 0.988068102557503, 1.04429493287078, 1.01803069781624,
> 0.957632272450841, 0.971991500790857, 0.98209395086599, 1.02815448263624,
> 0.947227198465786, 0.941538843776026, 1.05745885548346, 1.01671992454154,
> 0.983992269183969, 0.951511089781983, 1.0110983382201, 1.00912039359684,
> 1.14741321154181, 0.985344569298617, 0.986725720078838, 1.03117619992574,
> 1.21391930217405, 0.997217690041687, 1.07387278335981, 1.02108776576674,
> 1.10313006343474, 1.10905015571117, 1.03999508931616, 1.03976033898847,
> 1.02524837903366, 0.984818521675578, 1.04179889608739, 0.975154632188378,
> 0.969607959579886, 1.02781726918172, 0.999963290601752, 1.01515310515696,
> 1.00520167771997, 0.984550628692601, 1.01902810624565, 1.00685219196689,
> 0.982173452231062, 0.999629285915004, 0.970558368481076, 0.995716235126211,
> 1.01314341676203, 1.02075675497478, 1.01128668171558, 1.05844449406008,
> 1.02510935720045, 0.983720171349822, 0.844396082698583, 1.05810846093782,
> 0.99048977314373, 0.971262209252963, 0.968530578900656, 1.04638759101296,
> 1.1799017507928, 0.898448146333193, 1.01160891745375, 1.01132611427775,
> 0.973840647644128, 1.04683923857471, 0.990344793003927, 1.0068683076688,
> 0.992798241837764, 0.99368375299494, 1.01200003722717, 1.0294016781563,
> 1.01547984207734, 1.04807175686468, 0.981737356425208, 1.06169292627851,
> 1.00518291863648, 0.976808854545251, 0.999321592083517, 0.97527078961014,
> 0.904376968942001, 0.987807226191329, 1.0350293488462, 1.02802944623052,
> 0.963692701708712, 0.998027399233401, 1.01762747994825, 1.0361812777154,
> 0.988298193013531, 1.04174794125517, 0.970839140069407, 1.0011387710754,
> 0.961691863428911, 1.02272120871699, 1.00519180537531, 0.999331326197155,
> 0.988055521308447, 1.00078630058504, 1.03915754537212, 0.971719181925134,
> 0.947846525645955, 0.955178087494908, 1.0019107391621, 1.01160524115586,
> 0.984958748948305, 0.967579294820259, 1.00552471207069, 1.04518978636097,
> 1.01810784117456, 0.993552343701494, 1.03535983577691, 0.982973964981781,
> 0.976175417305875, 0.971173421374951, 1.02622192154044, 1.06169292627851,
> 0.980316131595686, 0.98802873807095, 1.02522486330609, 1.01920704714191,
> 0.987730950969463, 0.972320506142784, 1.02950031603837, 1.01481027236895,
> 1.05066348592531, 0.941412086703545, 1.00426249434292, 0.997599618540051,
> 0.9727707483253, 0.987822018689403, 1.00496006171741, 1.00365488022554,
> 0.982021737672154, 0.946014836456916, 0.977232083157459, 1.04246004468132,
> 0.981228463109654, 0.971195860055527, 0.957569129533581, 0.987312165664453,
> 0.955583610052133, 0.991173211015633, 0.988434021882053, 0.94971029297998,
> 1.02260060019826, 0.986729432665294, 1.00044978044626, 1.00283798105021,
> 0.97876379280085, 0.948623031823408, 1.02317511219612, 1.02274213461243,
> 0.952138762776323, 0.974014522860041, 1.02874293788325, 1.02108205408977,
> 0.950582622331241, 0.986098109021331, 1.00098916530354, 1.05435029523389,
> 0.999921440746985, 0.964778188342829, 1.02087753056688, 1.0295810738595,
> 0.981181990342875, 0.94981916198993, 1.00828646284387, 1.01127872412662,
> 0.976192285243012, 0.95032633282931, 1.02884118341175, 1.01603776783957,
> 0.952947066257398, 0.988924105443658, 1.02727453593468, 1.02944328284254,
> 0.970561666318556, 0.982859453938812, 0.995886713501412, 1.04330648532137,
> 0.991844865771659, 0.99159250467833, 1.0332159700053, 0.993864541664301,
> 0.952716253755034, 0.960889912140351, 0.945669021854222, 0.985578531230794,
> 0.975053113912155, 0.999552576905685, 1.02526821151208, 1.00450298718514,
> 0.948193972002236, 0.99358065264941, 1.04855364680554, 0.982181523974446,
> 0.957570494265717, 0.989249667650167, 1.01928030167592, 0.977392376116208,
> 0.98989833978368, 0.950312758756191, 1.01179069272545, 1.03630963397181,
> 0.997439452068598, 0.981793158114732, 0.98560918506727, 1.0497573745934,
> 1.06729226949522, 0.987962794653476, 1.05655507163022, 1.01896087310511,
> 0.985627836611195, 1.03148215915346, 1.07268555224398, 1.00116324087534,
> 1.0353274520202, 0.965843323254256, 0.981731445105197, 1.01927876056569,
> 0.999117162412819, 0.984262994383997, 1.0058994935294, 1.03100765642408,
> 0.970000664632288, 0.974667000854514, 1.02683721041632, 1.00224180239507,
> 0.976488446884133, 0.989129579180466, 1.01111944121172, 1.01081759149299,
> 0.990822520380867, 0.951849309560414, 1.0533258785109, 1.02605855087945,
> 0.993203883495146, 1.01231349444546, 1.05040134097269, 0.988195090931879,
> 0.998324259112233, 0.955268105597824, 1.01179535756103, 1.00923640076821,
> 1.01018973127051, 0.97727388908476, 1.0163531272719, 1.03445181115965,
> 0.922709820364792, 1.05034425545701, 0.971929096725562, 1.01600521659268,
> 0.989307124797035, 0.966327502991904, 1.0148486097639, 1.00804453317389,
> 0.986688556832374, 1.01100519377607, 1.07022911567444, 1.01751745478207,
> 1.00395256916996, 1.02666291685486, 1.04306886282915, 0.976284796774505,
> 0.873912939480404, 0.985956634379203, 0.966005821428092, 1.01911378235808,
> 0.989971040708023, 0.948518684869746, 0.964578663605958, 1.00130118816861,
> 1.14681217078448, 0.989198271209915, 0.975831372743337, 0.87134009583841,
> 0.999320371256267, 1.04366317607907, 1.0941742251269, 1.04912747347822,
> 0.873169695550468, 0.9946703855144, 1.02687904508206, 1.01557590029223,
> 0.979039928979583, 0.978199700239161, 0.988038704848986, 0.944583404359117,
> 0.97831517569251, 0.9881631354812, 0.990553852862955, 1.05139702349736,
> 0.987315661692795, 0.973936995140175, 1.06038641236987, 1.0651525599012,
> 0.997891074790299, 0.979657304627723, 0.976638433723023, 0.981157058049022,
> 0.974773098042751, 1.04814547433216, 1.02327857487465, 1.01725074945885,
> 0.978364366421386, 1.02726007295919, 1.02997589351422, 1.0208112362723,
> 0.965389721633011, 1.01152741033802, 1.00352090319123, 0.981151102088714,
> 0.986010324656192, 0.97990659165448, 0.990503768576242, 0.985301104774446,
> 0.976384165205197, 0.983887706958686, 0.992885571845564, 0.985757622165226,
> 0.946874017209482, 0.990962963812142, 1.04319324396693, 1.07354098057006,
> 0.984597785175012, 0.97845443949194, 0.963960215777945, 1.06584156194012,
> 1.15623372835712, 0.94928101059165, 1.05512096334013, 1.00899261027951,
> 1.00760894032175, 0.989021593753401, 1.01442722705518, 1.04495346859144,
> 1.00095646335122, 0.958907195434402, 1.02045280866442, 1.00659622664478,
> 0.980590749755976, 1.01197688939839, 0.998468538022796, 1.040625279257,
> 0.934164344256759, 0.992160161968429, 0.998594942355232, 1.0532781805963,
> 0.974144103277039, 0.946253316413415, 1.00623102828924, 0.988787044999963,
> 0.987541467770869, 0.999140303075891, 1.05429436081972, 1.01617267860198,
> 1.17044068684611, 1.05663112916056, 1.01681282884482, 1.02872692218644,
> 1.00738468152863, 1.0222081131894, 1.01817103171172, 1.01222250929532,
> 0.97386289237006, 0.987701353725335, 1.00418624697519, 1.0183108729199,
> 0.956852868616883, 1.05440980534023, 1.01562402108019, 1.00083274000183,
> 0.945641654423597, 0.966367379950251, 1.02020174816679, 1.01151664724635,
> 0.940072052642951, 0.991027873457432, 0.984568985852697, 1.02373049431651,
> 0.985887377562884, 1.05498951399906, 0.98672427011774, 1.02570399134917,
> 1.06987898201568, 1.02648218241568, 0.971175525661877, 1.03211886190237,
> 0.992647697074835, 0.986388969656675, 1.02097305537141, 1.04518978636097,
> 0.987300416455667, 0.985182286987375, 1.02373433952904, 1.01376295257605,
> 0.992398195656577, 0.983765510339449, 1.06154013507033, 1.01312319643703
> )
> 
> 
>> dput(HM3)
> c(0.196622289085315, -0.0436031046458981, 0.000820037203846464,
> 0.392758326108918, 0.0445839547736504, 0.223969403187679, -0.070152338130932,
> 0.197471312884591, 0.0498910549108242, -0.620322188497616, 0.488037411730214,
> 0.0461937792271113, 0.0051066586622384, 0.413877974790452, -0.154637441642314,
> 0.229802002983503, 0.509904610124243, 0.126452664534026, 0.583037470359474,
> 0.470621010682895, -0.00578078533090776, -0.457074922218691,
> -0.0506356375273661, -0.767302691971105, -0.000822439375515026,
> -0.170080770417037, -0.0638533091192526, 0.218492017287635, -0.088210288825341,
> -0.389219867379597, 1.23355646106213, -1.78773586807154, 0.0117401974737664,
> -0.154882548871051, -2.11547741687211, -0.0638533091192526, -8.18190789706224,
> 0.282699360997887, 0.0383693451361519, -0.0638533091192526, 0.832793868600698,
> 0.894379977535782, -0.302329357989944, 0.573489523345283, -0.223540700260161,
> 0.229802002983503, 0.0239731624923989, 0.556201388601996, -0.112552578411774,
> -0.0438764575701239, 0.470621010682895, -4.92500180132548, 0.00309129784132211,
> 0.172407876390051, 0.229802002983503, 0.061799194890529, -0.000864150216291216,
> -0.0683834224257835, -1.42874041532869, 0.654009227783557, -0.620322188497616,
> 0.272034153890376, -0.00558557185457871, 0.229802002983503, -1.53118563423569e-05,
> 0.470621010682895, -0.736608540541868, -1.47747441666374, -0.00291475050109709,
> -1.56966101463791, 0.362334298169537, -1.41867286052432, -0.0062864890624406,
> -0.223090980170635, 0.229802002983503, 0, 0.0147973092972513,
> 0.757637727251206, 0.332227874554666, -0.620322188497616, -0.0638533091192526,
> -0.0638533091192526, 1.59477829009842, -0.247635059505396, -0.0638533091192526,
> 0.384123364351151, -0.909542286753924, -1.61643290186366, -0.00969493151693274,
> -0.0638533091192526, 0.335128597413319, 0.229802002983503, -3.36005853184083,
> -1.77772450660144, -0.223359097040026, -4.75082975711953, 0.545441153017038,
> -0.0181047188208818, 0.229802002983503, 0.718505786757723, -0.199075726922584,
> 0.0779145211225284, 0.0981591683724576, -0.0638533091192526,
> 0.771585236053205, -0.124873344480422, -0.143455357050914, -0.982192613814533,
> 0.840325570227771, -0.0639597669819456, 0.272309718636627, 0.470621010682895,
> 0.000149945344883234, 0.0179651303218593, 0.586470204231886,
> 1.12661379530659, 0.587538739887084, -0.389219867379597, 0.231816913894395,
> -2.32942853843292, 0.229802002983503, 0.470621010682895, 7.7137219916985,
> 3.14411367083572, 0.0371415305209129, 0.109258089720235, 0.0744990417275174,
> 0.978949955731298, 0.0046627432182197, 0.0778534035355464, -0.258075504911111,
> 0.428481177165365, 0.470621010682895, 0.0118295358531435, 0.601929630798962,
> 0.677233238299559, -0.620322188497616, 0.901650947775265, 0.229802002983503,
> 0.114385813284502, -0.287236365991277, 0.00362108510216535, 0.54836513813854,
> -0.360900820378516, 0.901102331983843, 0.303232126663717, -0.389219867379597,
> -0.389219867379597, -0.0638533091192526, 0.0172621246134547,
> -0.0638533091192526, 0.036790744537595, 0.0267711437261105, 0.229802002983503,
> 2.31402345437609, 0.229802002983503, -0.620322188497616, 0.470621010682895,
> -0.0638533091192526, 0, -0.0638533091192526, 0.683308812634821,
> 0.598730940691975, 0.229802002983503, 0.697360153425382, 0.00772365161682687,
> -4.74600966429007, 0.593589435095411, -0.152099577506429, 0.0064938252361837,
> -0.108940122157535, 0.0514396263381645, 1.10452888209261, 0.229802002983503,
> -0.0638533091192526, 0, 0.0362637715949993, -2.79167649513235,
> 0.0221979095188955, 0.122798837911884, -3.07802860403649, 0.887963749928486,
> -0.0638533091192526, -2.59080601794078, -0.0638533091192526,
> -0.0720032601992142, 0.188653367126112, -0.390900634446388, 0.00626262322890335,
> 0.10449726079375, 0.190234594305548, 0.229802002983503, 0.470621010682895,
> 0.441447072301397, 0.710575973379983, 0.424723018059659, 0.0242933124111495,
> -0.0638533091192526, 0.221839916416526, 0.470621010682895, -0.0108657434161119,
> 0.734169111179744, 0.465629851897761, 0.0252290916908343, -0.0261736586703599,
> 0.470621010682895, 0.00872967160561454, 0.820007851307147, -0.0638533091192526,
> 0.162601634884078, 0.15876421059534, 0.276805180633414, -0.386980840708979,
> 0.10228170638373, 0.0199458385530109, 0.470621010682895, 0.360613568422605,
> 0.0398196823378306, -0.389219867379597, -0.00269176292387149,
> 0.553663817786709, 0.00803266728255501, 0.470621010682895, -0.389219867379597,
> 0.0531985700905557, -0.00966390653254687, 0.948851286517476,
> 0, -0.0889303986002686, -0.289482205789089, 0.470621010682895,
> -0.620322188497616, -1.33981685878991, 0.0183827591576367, 0.011135182844728,
> -0.0638533091192526, 2.32607985155626, -0.138083520720171, 0.00471455153770216,
> 0.158058381916342, 0.00450504854498247, 0.0789824876681455, 0.0704699239866158,
> -2.19086590383429, 0.0809155162971262, -0.0638533091192526, 0.0329404929656065,
> 0.0281676116282727, -0.389219867379597, 0.012790919597435, -0.00704771101800125,
> 0.0403529797537856, 0.0558406758822688, 0.164832137315436, -0.165567865123191,
> -0.11103429903157, -0.0355024172803945, 9.48260587090123e-05,
> 0.470621010682895, -0.0495357136064147, 0.229802002983503, 0.0637860580674806,
> -0.620322188497616, -0.0226657756569641, -0.389219867379597,
> -0.316625748304681, 0.229802002983503, 0, 0.364124775102099,
> -0.0310987139252845, 0.492424057922772, -0.224535737231339, -0.133435578504748,
> 0.229802002983503, 0.184389526960601, -3.97960671764283, 0.223470481648489,
> -0.620322188497616, 1.57339312469411, -3.97007919079201, 0.028292556618311,
> -0.0208634007145496, 0.253055735744466, -0.0638533091192526,
> -1.93884051014153, 0.0301349400625612, -0.0638533091192526, 0.000295057056987376,
> 0, 0.286160612098488, 0.101207913446722, 0.374419900839882, -0.106217410937095,
> 0.229802002983503, 2.31067381143729, 2.09201022549168, 0.00385297198146129,
> 0.0512938203984638, -1.74049624153457, 0.426702921643898, -0.0638533091192526,
> 0.018935028523515, 0.23126745462646, 0.00767373530818363, 0.229802002983503,
> 3.84024898682852, -0.0638533091192526, 0.389584248100691, 0.0449003257302091,
> 1.02228506672615, 0.229802002983503, -0.0638533091192526, -0.0829134652872339,
> 3.50596545804498, -0.389219867379597, 0.9170368554337, -0.0638533091192526,
> 0.393117195150408, 0.228802985129773, 0.380899253549764, 0.232617830598094,
> -0.389219867379597, 0.0459161417216973, 1.31722638132631, -0.151301202089443,
> -0.116990991708309, 0.927131393095724, 0.287762794679405, 0.53782819173468,
> -0.389219867379597, 0, 0.778671412967675, -0.389219867379597,
> -0.0683403974328925, 0, 0.229802002983503, 0.473035927661744,
> -0.620322188497616, 0.994155024290692, 0.229802002983503, 0.470621010682895,
> -0.194201456791898, 0.735175033053973, 0, 8.3894395202509, -2.71421951365695,
> 1.63476338054133, 0, -0.358533993792374, -0.180900357181005,
> 0.464964929465083, 0.828110705634323, 0.290014059095831, 0.0265807736784206,
> 0.913325706925416, -0.620322188497616, 0.482569085367064, 0,
> 0.999426501553857, -0.0638533091192526, -0.894190901951266, 0.0608976332239153,
> -0.389219867379597, 0.000920175777920006, 0.0444241832790267,
> -0.389219867379597, 1.77775509259178, 0.00731201645980821, 0,
> -0.620322188497616, 0.229802002983503, 5.07464022484589, 1.48600912617683,
> 0.037030406316896, 5.19861898546793, 0.614580107047209, 0.140671508556198,
> -0.0638533091192526, 0.470621010682895, 1.44599607789379, 0.229802002983503,
> 0.0337669838054181, -0.339507807487096, -0.227604499761264, -0.0126140839785072,
> 0.329583322374931, -0.348480271857373, 0.229802002983503, 0.220851780815745,
> -0.00466286305694716, 0.470621010682895, 1.94004188027446, 0.00769634829635375,
> 0.229802002983503, 0.229802002983503, -0.0638533091192526, 0.00196933273638366,
> 0.362886861307995, -0.010895368171175, 0.00517992475169629, 0.0121875763733234,
> 0.229802002983503, -0.389219867379597, -0.000556322059116982,
> 0.229802002983503, 0.00260711445755056, -0.58609877956866, 0,
> 0.173845846564558, 0.050042474875399, -0.608356727049781, 1.29406201978428,
> -0.0207947874733201, 0.00589643228835565, 0.720668560955405,
> -0.0638533091192526, 0.0325689306223551, -0.620322188497616,
> -0.389219867379597, -0.0638533091192526, -0.620322188497616,
> 0.0384932081048732, -0.0349291605765668, 0.00798065241516178,
> -0.134723236994354, -0.0638533091192526, 0.281152548562933, 0.785134308633961,
> 0.00911501343278954, 0.0290625580516694, 0.229802002983503, 0.0364502381509222,
> 0.0200267754432485, 0.470621010682895, -2.30392713054924, 0.0133977174531081,
> 0.0313438687192373, -0.453629796591262, -0.444456614744397, 0.470621010682895,
> -0.00419309963021769, 0.229802002983503, 0.781666390558601, 0.263209713357162,
> -0.00288720581609739, 0.0157812058748433, 0.377266856994834,
> 0.211102246795362, -0.0244674117557579, 0.502934845080243, 0.470621010682895,
> -0.620322188497616, 0.119508324298399, 0.68156448564652, -0.389219867379597,
> 0.344388633359758, 0.541519544456095, 0.348203898915403, -0.617865773083779,
> -0.620322188497616, -0.389219867379597, -0.630469325697215, -0.389219867379597,
> -0.00185171176208817, 0.470621010682895, -0.0638533091192526,
> 0.0384110765023374, -0.476268477810135, 0.0275459003440332, -0.377613497242655,
> 0.480114279740138, 0.208177472628734, 0.0388621060580001, 0.314810488790625,
> -0.389219867379597, 0.622343433254446, 0.229802002983503, 0,
> 0.0116064532794852, 0.640880718848922, 0.0159256710348949, 0.0294218642201815,
> -0.11922554805783, 0.470621010682895, 0.0719964703043177, 0.176673405801693,
> -0.216160070123196, -0.620322188497616, 0.263287565296575, 0.334468720690456,
> -0.389219867379597, -0.389219867379597, 0, -0.0638533091192526,
> -0.0638533091192526, 0.328005934417359, 1.98345092754637, -0.0638533091192526,
> -0.322569885555602, 0.343583272797935, -0.0131035846556284, -0.0638533091192526,
> 0.332049938227108, 0.229802002983503, -0.184471568949188, 0.0738083137554319,
> 0.295094806935004, 2.00072915211246, 0.0524057350651284, -0.394994587233133,
> 0.11082820395266, 0.0470433275207449, -0.0244714107059427, -0.19392136026585,
> -0.389219867379597, 1.54447845601115, 0, 1.50461847711675, 0.470621010682895,
> -2.19230689436821, -2.15372326763204, 0.0602893383146939, -0.350076259362557,
> -0.389219867379597, -0.0491349223300808, -0.347427839125497,
> -0.125793370606986, -0.389219867379597, 0.218492017287635, -0.389219867379597,
> -2.19203559916863, 0.470621010682895, -0.0756276343860818, -0.620322188497616,
> 0.039618360679447, 0.229802002983503, 0.342229794086586, 1.22342842687211,
> -0.0638533091192526, -0.389219867379597, 0, -0.121360237566266,
> 0.470621010682895, 0.203522177713664, -2.66081323726908, 0.164981226545032,
> 0.208989371873146, 0.178977127763616, 0.00534834375159016, 0.0259183781780568,
> -0.0638533091192526, -0.620322188497616, 3.77342257919255, 0.077694414226668,
> -0.0638533091192526, -2.76001789665019, 0.93498899883197, 0.229802002983503,
> 0.229802002983503, -0.189889234701173, -0.878128024239223, 0.229802002983503,
> -0.971246200836784, 0.00238384747429768, -0.620322188497616,
> 0.440261892604354, 3.76354782684596, 0.470621010682895, 3.86172131986622,
> 0.575298756860043, 12.9023010859722, 0.470621010682895, 0.378503973417637,
> 0.474891582933229, 0, -0.389219867379597, 0.520087612011093,
> 0.00160600701629905, -1.70670322718237, 0.229802002983503, -0.0349116284595025,
> -0.389219867379597, 0.229802002983503, -0.0638533091192526, 0.0138641669555371,
> -1.76999545498462, 0.470621010682895, -0.0638533091192526, 0.470621010682895,
> 0.568835467760944, 0.196290116211879, 0.00657090150018764, 1.15108667563071,
> 0.229802002983503, 1.39616453144586, -0.0847185084207546, -0.063387618790934,
> 0.470621010682895, 0.111214507604595, -0.0859882232308096, 0.229802002983503,
> 0.00453833599472268, 0.334589717726727, 0.470621010682895, -0.390600326717378,
> 0.254343160688718, -0.951335732362531, -0.0638533091192526, -0.389219867379597,
> 0.185995171841162, -0.0638533091192526, 0.0161178324221534, -0.702070844948495,
> -0.466435025160672, 0.470621010682895, 0.470621010682895, -0.383876151221017,
> -0.0638533091192526, -0.0638533091192526, 0.470621010682895,
> -0.389219867379597, -0.0123155655016255, -0.386709341729674,
> 0.229802002983503, -0.620322188497616, 0.187872106293471, 1.0998198175547,
> 0.229802002983503, -0.0111456829865045, -0.620322188497616, -0.620322188497616,
> 0.0484181771965215, 0.29713373197861, 1.03416257683562, 0.470621010682895,
> -1.14286511426204, -0.620322188497616, -0.620322188497616, 3.57860778510592,
> 0.229802002983503, -0.62520946847561, 0.290707988341369, 0.0924839451700828,
> -0.292199030679553, 0.275232409230032, -0.0884019891746591, -0.613845322077624,
> 0.470621010682895, 0.27263770155515, 0.229802002983503, 0.470621010682895,
> -0.616577036861926, -0.0638533091192526, 0.229802002983503, 0.103227620279027,
> 0.000325827725882409, -0.389219867379597, -0.0213033086486025,
> 1.1517268134498, -0.0027549233829829, -5.20036700047062, -0.389219867379597,
> 1.38009140442044, 0.188112123387496, -0.389219867379597, -0.0715310034898503,
> 0.229802002983503, -1.42113754281828, 0.0671280483594289, -0.866540256758301,
> -0.389219867379597, -0.389219867379597, 0.112435457850701, -0.162158911168545,
> 0.470621010682895, 0.470621010682895, 0.0211915910476394, -0.00153721649859708,
> -0.0638533091192526, -0.102776844490052)
> 
>> dput(HM2)
> c(0.016497438441392, -0.0121005433105642, 0.0340103138875006,
> 0.0393128837416979, 0.144718671661076, 0.0775923152157973, 0.000894372752462123,
> 0.0129080340603581, -0.0022179741964203, 0.0540058630486591,
> 0.0383234745402589, -0.0142841740056228, -0.0113383036972718,
> 0.00764362896437113, 0.022570207227843, 0.0892462240318431, 0.060498120257716,
> 0.0432525311181986, 0.00222557516581898, 0.0493381989550945,
> -0.0116459907029527, -0.112480015005373, 0.0182691285457092,
> 0.0248597234825647, 0.00754701511666922, 0.0471476327719148,
> 0.0625841102244406, 0.0303698631973417, 0.00781124380799404,
> -0.0168950573556397, 0.0473901805181147, 0.00740505106540693,
> -0.0152639817574294, 0.00916841478852549, 0.0409730632910991,
> 0.0625866632066982, 0.00664005076885597, 0.0103946833001679,
> 0.0243728672440622, 0.0722309706115829, 0.0174699387206196, -0.088714507487996,
> -0.00504535647738195, 0.00927272338803572, -0.00707755664004029,
> 0.209001332673238, 0.0280975011236682, 0.0185107628508926, -0.0152895488479284,
> 0.0293166958720994, 0.0839281669393395, 0.0350355281775659, -0.0214691529922399,
> -0.00698847060167527, 0.069853381162298, 0.00880702674287574,
> 0.0440111442331915, 0.0576762465158807, 0.0180025302908308, 0.0104372279684426,
> 0.0399937793788931, 0.0484415593089806, 0.0359627022827372, 0.0610406756211853,
> 0.0400425377047152, 0.00783749255154968, 0.0033626359803225,
> 0.0253846624900874, 0.028956317299215, 0.0220327419434368, 0.0055491218235435,
> 0.021359789760335, 0.0053563472471669, 0.00770764128276149, 0.120606533813112,
> 0.0338481150043749, 0.00480652155775551, -0.0208260153687132,
> 0.0264346940874102, 0.0254170849175835, 0.0549828722223003, -0.0197153554154837,
> 0.0146319132316291, 0.0223773253300461, 0.046744336162787, 0.0439389840326429,
> 0.0730303072697907, 0.0157399337823011, 0.0189220332073482, 0.0332902036920697,
> -0.00629466834494764, 0.0674012192688983, 0.0200657055355507,
> 0.0101015050127517, 0.0425927774526406, 0.0282767872201141, 0.00648039037553307,
> 0.0515174537352363, 0.128342038159934, 0.0432170296083376, 0.01871577325763,
> 0.122157724187454, 0.0336635358211635, 0.0225239126708406, 0.0183439669703353,
> 0.134384074630845, 0.00984528984402488, -0.049573212443543, 0.0459003250172428,
> 0.0354638054442889, 0.0209690261585554, 0.0252976241664904, -0.0211582204487595,
> 0.00564472281613363, 0.0148258737510351, 0.0428454717776024,
> 0.0315492077820927, 0.0628515089974484, 0.034599813272583, 0.00983450218873532,
> 0.0790661896510682, 0.076097511515163, 0.015596924720301, 0.010641549474715,
> 0.00626447132858588, -0.0752885625254798, 0.0263105739632706,
> 0.0336726716607009, 0.0873459584037158, 0.0137508559242068, -0.00705208890843943,
> 0.0673463675870221, -0.000264192225152525, 0.0181803111819752,
> 0.0415821927667122, 0.0258248658300624, 0.0697171115138401, 0.0609029360829605,
> 0.0252388531172213, 0.0120346601757736, 0.0231724225476939, 0.00664147935356364,
> 0.00364229190085414, 0.0221357760157817, 0.00873744091784354,
> -0.0243757212939979, 0.0609405123073909, -0.0110205054092022,
> 0.0570746855530399, -0.0541885933947211, 0.0790835216830754,
> 0.0443624834606028, 0.010803474486455, 0.012658784733822, 0.0407444149396382,
> 0.0758344977136341, 0.0249648665726016, 0.0706224364244773, 0.0617623493482699,
> 0.0157667711676134, 0.159594212558186, 0.10429461374287, 0.0575594583605458,
> 0.0629746059562333, 0.187704355999386, -0.121475803700469, 0.0316944060343089,
> 0.0446672218032357, 0.0144595666502411, 0.0581713078328564, 0.0240501430003371,
> 0.0169003110123663, -0.00441905663049872, 0.0398232696077516,
> 0.0488599473848877, 0.0333193570494318, -0.0165383461076467,
> 0.0908431754234104, 0.0145316828198416, 0.0268323550870429, -0.00807316790584186,
> 0.0643886942067867, 0.0132605944827886, 0.0225125532638365, -0.029400634077785,
> 0.0470539042942522, 0.0266903807836804, 0.0216184350463828, -0.00196619725935374,
> -0.00896821424676157, 0.0471292741982513, 0.0153758143276018,
> 0.0391647510007153, 0.0638918758798112, 0.0350540963584708, 0.0621376260090577,
> -0.0235965026923011, 0.0722625556919611, -0.00136537610871872,
> 0.00876826451613888, 0.0314000163087439, 0.0327764509794023,
> 0.0416905332713813, 0.0287500712769455, 0.0363887020395213, -0.00500079481628151,
> 0.0284155664844784, -0.0252462616989099, 0.0448399729520494,
> 0.000185492290551008, 0.0334348196455053, 0.0875709173367935,
> 0.0351964196451633, 0.232571060368655, 0.1090264053299, 0.0617423514625135,
> 0.0768976100544436, -0.0546347871751927, 0.0996882491247678,
> 0.0381052389461968, 0.0652248201643501, 0.0858993079060812, 0.0666492513259647,
> 0.0751282298723881, -0.0192055466287929, 0.0869162270419438,
> 0.0162427091867147, 0.0351089781252477, 0.0226340139536196, 0.0652987163108841,
> 0.0779638975412273, 0.117722278377793, -0.0394478774616255, 0.0438529807975176,
> 0.0881967320593799, 0.0588214648998437, -0.0184036676567495,
> 0.0140741349441621, 0.0161750696830995, 0.0118833913020066, 0.00937605882553054,
> -0.0455051243509685, 0.0418377741899513, -0.0112867686098609,
> 0.0101123419174794, -0.00470124454451681, 0.000956333395138531,
> -0.013981828106854, 0.05549118128867, -0.0224186715153298, 0.0186897582435067,
> 0.0169772716816739, -0.0179058332425567, -0.00353064879469387,
> 0.0269073443173487, 0.011369485145996, 0.0753871802564881, 0.184940758413157,
> 0.0962788588369411, 0.12233382584365, 0.0542611920398092, 0.0470414915942155,
> 0.0303505098818263, 0.00573452177124659, 0.00764243118994532,
> 0.0586726631015619, 0.106336058543696, -0.0222151019972231, 0.00630121965465554,
> 0.0624011763113852, 0.0372172786669568, 0.0226016288240546, 0.0939930244398409,
> 0.0767415904317717, 0.0238800474741997, 0.0151188247761029, 0.134843980893931,
> -0.0023819116674762, 0.00287890065299971, 0.0303739452657181,
> -0.00394798169311598, -0.00886454847938266, 0.0530899335995167,
> -0.00526457886952308, 0.100251697020821, 0.0651999010773084,
> 0.0618461082131486, 0.0746686396348302, 0.0225108225108226, -0.0228981338001482,
> -0.00958321904192527, 0.0301744325038945, 0.0510329188455982,
> -0.0184730200018259, 0.0702506964772114, 0.0167199245415362,
> 0.0790156649374397, -0.028524529551306, 0.0133581966776628, 0.025585253422798,
> 0.172427142313816, 0.0675994203009362, 0.00336311873127013, 0.0493948607018135,
> 0.218437612231768, 0.0214594460427004, 0.0907841657749202, 0.0333163617639509,
> 0.177206388698409, 0.126401976128862, 0.0399950893161636, 0.0501927169716304,
> 0.0893264027232592, 0.0157983828191964, 0.0615798877852506, -0.0197129750106304,
> -0.0108884320377597, 0.0480698262099342, 0.00512957625928131,
> 0.0157721009527848, 0.00576798852432315, 0.132233222996491, 0.0427798891731844,
> 0.0213946620292803, 0.137026176169503, 0.106996875883653, -0.0054111837253851,
> 0.0251880361118362, 0.0253902053162975, 0.0270577225980805, 0.0124293785310734,
> 0.0743211614709804, 0.0322779541039503, -0.000515082314443601,
> -0.15098468271335, 0.0705935165241118, 0.00396949242984507, 0.00570413156689748,
> -0.028337357038222, 0.067618643584239, 0.215558122382694, -0.0902178675044036,
> 0.0116089174537522, 0.0163203173112244, 0.053199024052846, 0.190954107085158,
> -0.00965520699607334, 0.0137508862783826, 0.0551559553375383,
> 0.0522566319201845, 0.0120000372271673, 0.0661660238047397, 0.0393034160850182,
> 0.0519061657312619, -0.0104570268884604, 0.0691173523364065,
> 0.0161805435668569, 0.017889600764444, 0.00112726390517828, 0.0127812045951454,
> -0.0689255559600801, 0.0231502379228566, 0.0350293488462002,
> 0.0474816278264511, -0.00418170038125466, 0.0353685604292087,
> 0.0176274799482486, 0.0430171958669913, 0.0875537735181209, 0.070485815496694,
> 0.0135133879845454, 0.0128708660489427, 0.051611788565405, 0.0227212087169909,
> 0.0073359858475243, 0.00404515320751896, -0.00440846200360685,
> 0.0659739938613625, 0.0460166710841459, 0.0164323471803848, -0.00986110969267727,
> -0.0393527714903421, 0.0187532041738799, 0.0120528540944261,
> 0.048649745777751, -0.016576662520347, 0.00722036925799145, 0.0532607499622092,
> 0.0287269581620362, 0.098387602988942, 0.0515373332109274, -0.00594807446048738,
> -0.012996698372473, 0.186569861135154, 0.030123906032987, 0.0853131761769354,
> 0.0031929989708665, 0.0555338071316984, 0.0330892102352149, 0.0325300150784036,
> -0.00129426068643155, 0.0964465282035648, 0.0748083984068019,
> 0.014810272368951, 0.0727362482346641, -0.022994161418619, 0.0176067452321714,
> 0.0316353702314177, 0.0366753960255026, 0.132973503081211, 0.00537653915699314,
> 0.0293447867014306, 0.00587487992108688, -0.0465459905172109,
> -0.0208313288728689, 0.0461567115064294, -0.00992439500984909,
> 0.0478692174283319, 0.15870237135342, 0.0555146507925898, 0.0929129911374694,
> 0.136269426745194, 0.032515127489143, -0.0177415950997095, 0.0242936475495824,
> 0.0128679606829178, 0.00550255711518526, 0.00950135633958283,
> -0.0171614203848253, -0.0480106149706695, 0.0436386144400421,
> 0.030020737993979, -0.0207207590715261, 0.118800465447344, 0.0287429378832495,
> 0.0277036046980229, -0.0326376675279461, 0.0111603851297697,
> 0.0168167717634637, 0.0543502952338948, 0.0268780423509474, 0.0244551896836229,
> 0.0330102007094411, 0.049560008022367, 0.0830113020712642, -0.0360281667532304,
> 0.0254731639150768, 0.019583911716062, 0.0311623362566958, 0.00306244869413074,
> 0.0687257755440167, 0.0205871906806436, -0.00980792960340933,
> 0.0430516082564022, 0.0402780110730929, 0.0705036986824154, 0.025854270339128,
> 0.108621667399996, 0.00718685874587846, 0.0571556864539545, 0.109159849895189,
> -0.00840749532166959, 0.0463361727990188, 0.0651635196532618,
> 0.107210781390985, 0.134765229575272, 0.00173368529272275, -0.0123735133796356,
> -0.0136090591818897, 0.0534703507636681, 0.0252682115120829,
> 0.0045029871851435, -0.032195767437093, 0.0467488481089099, 0.0538227606085845,
> -0.0118386798369731, -0.0280299494295351, 0.0140538628567341,
> 0.0305966571267272, -0.00988880417457172, -0.00202667375935174,
> -0.0227460178374512, 0.0117906927254463, 0.0508373391209442,
> 0.20565913566541, 0.079177499709108, 0.0422151179934303, 0.084419174697904,
> 0.0672922694952164, -0.00562397110021092, 0.0565550716302232,
> 0.0189608731051132, 0.0453269153630163, 0.0409743262622376, 0.0726855522439788,
> 0.0185422373323943, 0.0700699839671905, -0.00222797184477677,
> -0.00250681151008622, 0.0255048645524291, 0.000885511372841601,
> 0.0788210113117177, 0.0116542476851704, 0.031007656424076, 0.011572121687957,
> 0.00197091513991245, 0.0332881463280234, 0.0115574465474555,
> -0.0156436963370336, 0.224908551812914, 0.0160024082692785, 0.0108175914929918,
> 0.0850714918317297, -0.0209549958807167, 0.0649434433474199,
> 0.0572507308261867, 0.0108695652173913, 0.0149885395421411, 0.0504013409726928,
> 0.000985727295810551, 0.00206155161241882, -0.0273818619348245,
> 0.0117953575610268, 0.0160740457598691, 0.0439509870338097, 0.000374131432729126,
> 0.030965232935552, 0.0680925204656592, -0.040084138491466, 0.0579554457139488,
> -0.0280709032744379, 0.0245619981154038, 0.0332658264722017,
> 0.0307493365246973, 0.0663529386077719, 0.0108866234143253, 0.0504739490524921,
> 0.0110051937760747, 0.114486710683534, 0.0885070446505914, 0.00713719270420306,
> 0.027840282585194, 0.0502624411934901, -0.0237152032254946, -0.0783599699259493,
> 0.00584404515799187, -0.031333005510123, 0.0191137823580835,
> 0.00399073109721866, -0.00489024968946272, -0.00926327094032102,
> 0.00380235683181465, 0.157304505375375, 0.0200440859609643, -0.0241686272566629,
> -0.0800316610669412, 0.0239679002814968, 0.0538951680014091,
> 0.0941742251268998, 0.0521115186872178, -0.123540922378836, 0.0542314864434063,
> 0.0343854708502029, 0.0238689195785379, 0.123488443091325, 0.00983601617163751,
> -0.00112278368930967, -0.0210112398529677, -0.0179204649311611,
> -0.00187219424504028, -0.0094461471370453, 0.0616047615895669,
> 0.0582469526595368, 0.00304799229604126, 0.0674836370082559,
> 0.0853131761769354, 0.0358226068729712, 0.00994622612345991,
> -0.0217167015464964, 0.0526997601984295, -0.0232294161006039,
> 0.100227237031891, 0.0294429036389532, 0.0172507494588479, 0.0034823068051,
> 0.0322430509250401, 0.0401401293054765, 0.0208112362723038, 0.00561429336771978,
> 0.037707481438362, 0.00862436025594276, 0.00476662316192879,
> 0.15812712691603, 0.0472541237269534, -0.00949623142375768, 0.00855607871651576,
> -0.00281567087439197, 0.0397525174385432, -0.00711442815443551,
> -0.00131577619047549, 0.106909062371648, 0.025164340439287, 0.0577629820111593,
> 0.0939246700745592, 0.00391223125066969, 0.0124049752133815,
> -0.0275935528232096, 0.0762231355953777, 0.181010165393349, -0.0347198712523113,
> 0.0678716698457551, 0.0120798630452986, 0.0245162767779995, 0.0498518908504713,
> 0.0144272270551826, 0.0449534685914403, 0.000956463351216132,
> 0.0460805768375294, 0.0483340329448637, 0.0192578144013228, 0.0713319833154848,
> 0.0664774807491022, 0.0136479583643586, 0.047338990736079, -0.00984473290135361,
> 0.0791372581960931, 0.0256142818179614, 0.0598611692250246, 0.0265350466465605,
> 0.068670703501864, 0.00623102828923622, -0.00469207491878694,
> 0.157504094271494, 0.116984111424978, 0.0875878669508734, 0.0161726786019788,
> 0.175935713544914, 0.0637705286819113, 0.0197080817743348, 0.0452129305548087,
> 0.0233338613388849, 0.0936152099345934, 0.0199808837233982, 0.0274312796611153,
> 0.0721161218673364, 0.17082095580651, 0.034052131044711, 0.0512562246908417,
> 0.0487107440041036, 0.153481867586958, 0.0213458465510684, 0.0133431492518521,
> 0.119009291067923, 0.0253607374472143, 0.0202017481667936, 0.011516647246353,
> 0.0339052007553662, 0.0313036367875989, -0.00639827115782904,
> 0.0237304943165091, 0.00669172141737004, 0.0854219038259585,
> 0.0181012761967982, 0.0372169953336968, 0.219164421366707, 0.0387279347673063,
> -0.00326722366281047, 0.0611109647647912, 0.0724982521544455,
> 0.0114916829563205, 0.0264328043306142, 0.100227237031891, 0.0694579153964423,
> 0.0256998227540187, 0.0937017943889341, 0.0137629525760519, -0.0046036224834336,
> 0.0270242177828994, 0.0650551686301645, 0.01402255124967)
> 
> 
> very many thanks for your time and effort...
> yours sincerely
> 
> AKSHAY M KULKARNI
> 
> 
> ________________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
> Sent: Wednesday, March 20, 2019 1:32 PM
> To: R help Mailing  list
> Subject: [R] problem with nlsLM.....
> 
> dear members,
>                              Yesterday, Duncan identified a silly mistake in an nls call...in the list of starting values, one of the names of the variable was getting repeated. But I am experiencing the same problem again, with a different formula:
> 
>> formulaDH3
> HM1 ~ (a + (b * ((HM2 + 0.3)^(1/3)))) * (c * log(HM3 + 27))
> 
> here HM1 is the response variable, and HM2 and HM3 are predictors.....
> 
>> nonlin_modDH3 <- nls(formulaDH3, start = list(a = 0.43143, b = 0.68173,c = 0.02954))
> Error in nlsModel(formula, mf, start, wts) :
>   singular gradient matrix at initial parameter estimates
>> nonlin_modDH3 <- nlsLM(formulaDH3, start = list(a = 0.43143, b = 0.68173,c = 0.02954))
> Error in nlsModel(formula, mf, start, wts) :
>   singular gradient matrix at initial parameter estimates
> 
> I am using nlsLM function from the minpack.lm package which says that it will converge when nls fails with singular gradient matrix error...
> 
> 
> Is there again a silly mistake(pardon me again if there is one!), or is the problem serious? If it is serious, any pointers towards a solution?
> 
> very many thanks for your time and effort....
> yours sincerely,
> AKSHAY M KULKARNI
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From k|m@j@cob@en @end|ng |rom gm@||@com  Tue Mar 19 23:29:11 2019
From: k|m@j@cob@en @end|ng |rom gm@||@com (Kim Jacobsen)
Date: Tue, 19 Mar 2019 22:29:11 +0000
Subject: [R] plot.xmean.ordinaly vs plot() in package "rms"
In-Reply-To: <7D856F8B-80C7-48BB-BF37-BF17D0204A5F@dcn.davis.ca.us>
References: <CAHn6EJ3LF286PCWaGPJ0tdk8wXSwejmJrsygzWtHeL2qyoUErw@mail.gmail.com>
 <031F9B89-F684-4735-A9B2-2AAD35942C91@dcn.davis.ca.us>
 <CAHn6EJ2=jLakgdMazEfwvUQ=X0JbjhjoEL4os3iR8wvFeMO2Kw@mail.gmail.com>
 <7D856F8B-80C7-48BB-BF37-BF17D0204A5F@dcn.davis.ca.us>
Message-ID: <CAHn6EJ17OO6KMFU9caPq8h1SkGOy_6v8LMrEbmPj-ymuD-steg@mail.gmail.com>

Mailing list now included (apologies, first time I post anything so not
quite sure how it works).

You are quite right, it was a typo. I meant to write that
plot.xmean.ordinaly(). So please let me correct my last statement: the
plot.xmean.ordinaly() command and plot() command are interchangeable as
long as x is an object x of class "xmean.ordinaly", and
plot.xmean.ordinaly() is best used if the object is not of class
"xmean.ordinaly" or if you are unsure what class it it. Is this a correct
encapsulation?


On Sun, 17 Mar 2019 at 14:38, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Please keep the mailing list included in the thread.
>
> I can't tell if you do understand and are just being sloppy, or if you are
> completely confused, because xmean.ordinaly() and plot.xmean.ordinaly() are
> two completely different symbols in R.
>
> As for being "safe"... you may choose to be specific or not, but plot and
> plot.xmean.ordinaly are both equally "safe" to call, and being too specific
> can cause problems sometimes as well.
>
> On March 17, 2019 6:40:10 AM PDT, Kim Jacobsen <kimsjacobsen at gmail.com>
> wrote:
> >Dear Jeff,
> >
> >Thank you so much! So if I understand the S3 object documents
> >correctly,
> >the xmean.ordinaly() command and plot() command are interchangeable as
> >long
> >as x is an object x of class "xmean.ordinaly"? So would I be right to
> >think
> >that I might as well just xmean.ordinaly() to be safe?
> >
> >Many thanks,
> >
> >
> >
> >On Sun, 17 Mar 2019 at 02:08, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> Read up on S3 object orientation[1]. If you have an object x of class
> >> "xmean.ordinaly" then writing
> >>
> >> plot(x)
> >>
> >> will end up invoking the plot.xmean.ordinaly function rather than the
> >> plot.default function in base graphics. This is broadly true
> >throughout R.
> >>
> >> [1] http://adv-r.had.co.nz/S3.html
> >>
> >> On March 16, 2019 11:03:06 AM PDT, Kim Jacobsen
> ><kimsjacobsen at gmail.com>
> >> wrote:
> >> >Would anyone be able to explain what the difference is between
> >> >plot.xmean.ordinaly and plot() in the "rms" package? (for the
> >purposes
> >> >of
> >> >testing the proportional odds assumption in ordinal models). In the
> >> >package
> >> >document (https://cran.r-project.org/web/packages/rms/rms.pdf) they
> >> >seem
> >> >both to be used interchangeably.
> >> >
> >> >Thank you!
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>


-- 
Kim Solve Jacobsen
PhD candidate, Wildlife Conservation Research Unit
University of Oxford
Recanati-Kaplan Centre
OX13 5QL Oxford, UK

[image: Image result for oxford logo]

	[[alternative HTML version deleted]]


From @@ndr@@ch@udron @end|ng |rom gm@||@com  Wed Mar 20 09:59:21 2019
From: @@ndr@@ch@udron @end|ng |rom gm@||@com (Sandra Elisabeth Chaudron)
Date: Wed, 20 Mar 2019 09:59:21 +0100
Subject: [R] System("source activate condaenv")
Message-ID: <CAOEdaTtffn2WLyhy015RW_LeSCHjLd=cEe-EVvroEM1WStUOcg@mail.gmail.com>

Hi,
I am using the server version of RStudio and I have a script where I want
to activate the conda environment that I set up for a bioinformatic tool
called MinVar.
For that I use in my script the command: system("source
/home/ubuntu/miniconda3/bin/activate minvar"). I provide the path to
activate because the tool is installed under my second user name in the
server I use.
And the error message is : sh: 1: source: not found error in running command
I tried to look for solution in any possible forum but did not find
anything.
I was wondering if a solution for such issue was already discussed? and an
answer found?
Thanks a lot.

	[[alternative HTML version deleted]]


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Wed Mar 20 14:40:18 2019
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Wed, 20 Mar 2019 13:40:18 +0000
Subject: [R] Installing littler package on MacOS
Message-ID: <BN8PR18MB269013BB66C1FA5AD66EECA5FA410@BN8PR18MB2690.namprd18.prod.outlook.com>

I just installed littler package on my Mac.  I got the message

 * new binary r installed in bin/ subdirectory                                        
 * consider adding a symbolic link from, e.g., /usr/local/bin                         
 * on OS X, you may have to name this 'lr' instead 

How can I create this link?  

Thanks,
Naresh

PS This is likely a trivial question for experienced Mac users.  I am a novice.

From kry|ov@r00t @end|ng |rom gm@||@com  Wed Mar 20 14:58:54 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Wed, 20 Mar 2019 16:58:54 +0300
Subject: [R] System("source activate condaenv")
In-Reply-To: <CAOEdaTtffn2WLyhy015RW_LeSCHjLd=cEe-EVvroEM1WStUOcg@mail.gmail.com>
References: <CAOEdaTtffn2WLyhy015RW_LeSCHjLd=cEe-EVvroEM1WStUOcg@mail.gmail.com>
Message-ID: <20190320165854.0cba3240@trisector>

On Wed, 20 Mar 2019 09:59:21 +0100
Sandra Elisabeth Chaudron <sandra.chaudron at gmail.com> wrote:

> I am using the server version of RStudio and I have a script where I
> want to activate the conda environment that I set up for a
> bioinformatic tool called MinVar.
> For that I use in my script the command: system("source
> /home/ubuntu/miniconda3/bin/activate minvar").

Unfortunately, this is probably not going to work as is.

In *nix-like systems, child processes cannot alter the environment
variables of their parents, so when one wants a shell script to alter
environment variables in the current session, they use "source".

The reason your command returns an error message is probably because
"source" is a command specific to /bin/bash, while R calls /bin/sh,
which might be symlinked to /bin/dash instead of /bin/bash on Ubuntu.
The POSIX-correct form would be ". /home/ubuntu/miniconda3/bin/activate
minvar", that is, just a dot instead of the word "source".

But the underlying issue would stay the same: R system()
function launches a subprocess /bin/sh; the "source" or "." command
causes the environment of the *child* shell process to be changed;
environment of the *parent* R process stays unchanged.

Your best bet would be to read the activate script, understand the
changes to the environment it causes and use Sys.getenv() /
Sys.setenv() to make equivalent changes in R environment variables.

-- 
Best regards,
Ivan


From p@u|bern@|07 @end|ng |rom gm@||@com  Wed Mar 20 15:13:28 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Wed, 20 Mar 2019 09:13:28 -0500
Subject: [R] How To Extracting Summary Stats for Sepwise Regression
Message-ID: <CAMOcQfOLOiUhzWrwqaaiiz_NeKKiCZpvsviRjSTGuogvQDCL4A@mail.gmail.com>

Hello dear friends,

What I want is to, somehow data.frame the summary of a stepwise regression
(the estimated coefficients and p-values) to show them as a report in table
format. However I have not been able to do that.

Any help and/or guidance will be greatly appreciated, below are all the
details of what I am doing:

I am working with R version 3.5.2 in Power BI.
The Power BI version is: 2.67.5404.581 64-bit (March 2019)
The error I am getting is the following: Error in step(basis_model,
direction = "both", scope = formula(FitAll)) :
  number of rows in use has changed: remove missing values?
In addition: Warning message:
In add1.lm(fit, scope$add, scale = scale, trace = trace, k = k,  :
  using the 106/107 rows from a combined fit
Execution halted


BasisModel <- lm(datos$Transits ~ 1)


FitAllModel_2 <- lm(datos$Transits ~ datos$CPI + datos$ConCharter +
datos$NewBuildingPrice + datos$HFO)

StepWiseModel_3 <- step(BasisModel, direction="both",
scope=formula(FitAllModel_2))

sum <- summary(StepWiseModel_3)
grid.table(sum)

I have also tried doing grid.table(sum$coef) and
grid.table(sum$coefficients) but none of that worked.

> dput(datos)
structure(list(Transits = c(23L, 86L, 29L, 85L, 36L, 80L, 36L,
68L, 26L, 97L, 24L, 85L, 27L, 87L, 26L, 66L, 28L, 76L, 23L, 68L,
67L, 38L, 30L, 68L, 65L, 37L, 31L, 85L, 27L, 85L, 26L, 87L, 96L,
30L, 24L, 85L, 22L, 82L, 23L, 80L, 81L, 26L, 25L, 81L, 29L, 94L,
23L, 83L, 90L, 33L, 27L, 94L, 25L, 90L, 17L, 86L, 40L, 89L, 31L,
81L, 87L, 26L, 18L, 76L, 85L, 17L, 23L, 87L, 88L, 23L, 37L, 82L,
38L, 102L, 39L, 90L, 40L, 93L, 33L, 84L, 41L, 84L, 27L, 71L,
37L, 63L, 26L, 76L, 28L, 63L, 34L, 74L, 37L, 68L, 37L, 76L, 42L,
79L, 41L, 81L, 40L, 77L, 34L, 74L, 39L, 58L, 35L, 55L, 36L, 51L,
34L, 57L, 39L, 61L, 35L, 68L, 43L, 70L, 40L, 59L, 46L, 60L, 34L,
63L, 42L, 67L, 43L, 58L, 44L, 59L, 53L, 26L, 32L, 58L, 34L, 41L,
42L, 39L, 30L, 42L, 30L, 48L, 37L, 42L, 36L, 60L, 44L, 65L, 43L,
55L, 22L, 53L, 31L, 8L, 9L, 25L, 12L, 33L, 25L, 45L, 22L, 47L,
25L, 43L, 28L, 37L, 28L, 48L, 19L, 35L, 11L, 28L, 9L, 22L, 12L,
25L, 9L, 14L, 9L, 13L, 13L, 12L, 13L, 25L, 10L, 24L, 13L, 24L,
10L, 22L, 10L, 19L, 11L, 15L, 8L, 16L, 12L, 9L, 8L, 17L, 9L,
4L, 8L, 11L, 8L, 12L, 6L, 11L, 8L, 11L, 21L, 8L, 14L, 11L, 12L,
11L, 10L, 7L, 13L, 6L, 8L), CargoTons = c(327994L, 1606616L,
392457L, 1712810L, 572909L, 1693921L, 696160L, 1633810L, 427511L,
1990180L, 434473L, 1838645L, 501521L, 1970717L, 548017L, 1588291L,
488718L, 1739769L, 457908L, 1623617L, 1677868L, 709159L, 467997L,
1491590L, 1329897L, 540330L, 454764L, 1696188L, 412585L, 1771492L,
514509L, 1843676L, 2033267L, 499482L, 393591L, 1610270L, 333572L,
1546048L, 417399L, 1901424L, 1686199L, 432638L, 428805L, 1861493L,
511944L, 2011693L, 421151L, 1837227L, 1912533L, 593826L, 453480L,
1921809L, 460072L, 1675982L, 341377L, 1774207L, 473299L, 1526080L,
499347L, 1464667L, 1684398L, 437215L, 309439L, 1739618L, 1764891L,
344394L, 393377L, 1671126L, 1663948L, 419590L, 480516L, 1593703L,
527984L, 1730980L, 505520L, 1537961L, 527088L, 1600723L, 484147L,
1541796L, 662549L, 1506180L, 433022L, 1424827L, 643535L, 1337935L,
424964L, 1622296L, 462553L, 1194666L, 514447L, 1840988L, 641739L,
1382457L, 563509L, 1378973L, 667025L, 1345523L, 649433L, 1338687L,
673892L, 1348285L, 584263L, 1528680L, 600019L, 944145L, 649869L,
912873L, 624110L, 850102L, 607789L, 1208901L, 615976L, 1182815L,
615512L, 1399101L, 697432L, 1325899L, 608629L, 872243L, 750331L,
848187L, 524618L, 872075L, 706920L, 986303L, 687968L, 896847L,
636234L, 724704L, 723088L, 350679L, 460177L, 722250L, 622978L,
723952L, 660514L, 689362L, 483923L, 698217L, 569948L, 813910L,
602531L, 671289L, 494437L, 778160L, 540130L, 717010L, 590081L,
664101L, 368966L, 633134L, 447660L, 159223L, 160920L, 413470L,
271818L, 488672L, 390754L, 640038L, 321733L, 577064L, 443330L,
639336L, 430580L, 451870L, 399601L, 621382L, 256492L, 526333L,
211949L, 467734L, 173952L, 343172L, 263817L, 447959L, 166116L,
257188L, 175630L, 241603L, 239032L, 214663L, 293355L, 421452L,
165754L, 394366L, 254257L, 438731L, 201630L, 340764L, 160402L,
343059L, 203165L, 184462L, 141414L, 247706L, 223107L, 153652L,
153972L, 261942L, 200656L, 59416L, 166912L, 234790L, 177974L,
228005L, 138093L, 219065L, 189138L, 176860L, 391862L, 161972L,
260940L, 152540L, 182772L, 218337L, 225203L, 109038L, 260027L,
118230L, 180304L), RcnstPCUMS = c(814089L, 4018838L, 1039070L,
3962734L, 1307110L, 3715584L, 1272525L, 3159759L, 918793L, 4480777L,
862597L, 3959057L, 982768L, 4027394L, 932933L, 3085639L, 1008364L,
3545084L, 829730L, 3151571L, 3106543L, 1378972L, 1049638L, 3143534L,
3004616L, 1314721L, 1099565L, 3926565L, 972551L, 3950212L, 937604L,
4037464L, 4454625L, 1099265L, 870325L, 3935381L, 796631L, 3803778L,
833356L, 3704866L, 3776594L, 928562L, 903368L, 3740199L, 1024314L,
4351353L, 837034L, 3848355L, 4151682L, 1177413L, 988868L, 4359297L,
914531L, 4162950L, 621160L, 3973881L, 1435287L, 4126620L, 1111529L,
3773231L, 4050280L, 941832L, 650553L, 3528985L, 3955725L, 616642L,
825445L, 4023044L, 4097211L, 834924L, 1311944L, 3796557L, 1364762L,
4737589L, 1375555L, 4176912L, 1448418L, 4329880L, 1186202L, 3896554L,
1473688L, 3918132L, 975482L, 3306258L, 1329400L, 2945480L, 929590L,
3522287L, 1020073L, 2917674L, 1225467L, 3434734L, 1337442L, 3163533L,
1312327L, 3546667L, 1519244L, 3672447L, 1464130L, 3779991L, 1443061L,
3585158L, 1222389L, 3455774L, 1412875L, 2727757L, 1270067L, 2566944L,
1310215L, 2416900L, 1227347L, 2663666L, 1401909L, 2846029L, 1271238L,
3177336L, 1538445L, 3274950L, 1436806L, 2761960L, 1667355L, 2829878L,
1242930L, 2959036L, 1522456L, 3163181L, 1573896L, 2718126L, 1603101L,
2758533L, 2462967L, 946629L, 1151827L, 2691777L, 1246668L, 1925651L,
1527348L, 1835005L, 1087464L, 1969332L, 1093925L, 2234340L, 1315999L,
1949677L, 1300176L, 2785853L, 1588889L, 3028544L, 1563391L, 2559034L,
796993L, 2483341L, 1426101L, 283420L, 320329L, 1154490L, 430564L,
1524589L, 908063L, 2103074L, 806928L, 2190153L, 903527L, 2011725L,
1018823L, 1711333L, 1004757L, 2225944L, 689023L, 1610759L, 388765L,
1295317L, 320579L, 1011523L, 446023L, 1164442L, 323091L, 653104L,
315790L, 594555L, 464906L, 548716L, 470282L, 1165447L, 373934L,
1115930L, 464005L, 1112178L, 355084L, 1016473L, 367861L, 885959L,
402602L, 691014L, 290094L, 746782L, 435042L, 417022L, 283322L,
785941L, 320143L, 178424L, 283567L, 397199L, 283370L, 549200L,
216474L, 499255L, 290743L, 515437L, 963622L, 290782L, 508405L,
514425L, 556650L, 403736L, 362223L, 313357L, 467175L, 278034L,
288877L), TotalToll = c(5658660L, 25045209L, 8409456L, 28334952L,
10684800L, 26256312L, 10232640L, 22154904L, 7385184L, 31794048L,
7105176L, 28139544L, 8081568L, 28620648L, 7618608L, 21787128L,
7995024L, 24984648L, 6678216L, 22065480L, 21767976L, 11140344L,
8342568L, 22126968L, 21198528L, 10703664L, 8950680L, 27573120L,
7887024L, 28178856L, 8583088L, 31479312L, 34815908L, 9713854L,
7785698L, 30481326L, 7141340L, 29531732L, 7506990L, 29051552L,
29541774L, 8399644L, 8077564L, 28938572L, 9193712L, 33842728L,
7619546L, 30069734L, 31965450L, 10622842L, 8939174L, 34113946L,
8367102L, 32325008L, 5509564L, 30988230L, 12738622L, 32011730L,
10079752L, 29351100L, 31599596L, 8405222L, 5867970L, 27722768L,
31299036L, 5551330L, 7411918L, 31425586L, 32109240L, 7644508L,
11586334L, 29576948L, 12308666L, 37031904L, 12060814L, 32598000L,
13147498L, 33810750L, 10585596L, 30252354L, 13395666L, 30832518L,
8939520L, 25772004L, 12196422L, 23161516L, 8370134L, 27664010L,
9265958L, 22574300L, 10874402L, 26518982L, 12227228L, 24745802L,
11917848L, 27524048L, 13781322L, 28628988L, 13258712L, 29704254L,
13062000L, 27882468L, 11171612L, 27395884L, 12708357L, 21812064L,
11455579L, 20552168L, 12002006L, 19359130L, 11233494L, 21034682L,
12731992L, 22272600L, 11642944L, 25170646L, 13997224L, 25843164L,
12775186L, 21979684L, 15149512L, 22494414L, 11309236L, 23744618L,
13475898L, 25170222L, 14266136L, 21695278L, 14599552L, 21839098L,
19512312L, 8684618L, 10421828L, 21498934L, 11384046L, 15646056L,
13728128L, 14582010L, 9858034L, 15779454L, 9967770L, 17877786L,
11890900L, 15472342L, 11782498L, 21993570L, 14439268L, 24119724L,
14170544L, 20208244L, 7208820L, 19774008L, 11551690L, 2555626L,
2774579L, 9384376L, 3853045L, 12234653L, 8380684L, 16982380L,
7210410L, 17477956L, 8221924L, 16287822L, 9037974L, 13420990L,
9030589L, 17503524L, 6018223L, 12638167L, 3503589L, 10144213L,
2776575L, 8044772L, 4032809L, 9474450L, 2732395L, 5228934L, 2765976L,
4672118L, 4066749L, 4249184L, 4253170L, 9525707L, 3201832L, 8718645L,
4056891L, 8829727L, 3089209L, 7862348L, 3264051L, 6812088L, 3466098L,
5082556L, 2458062L, 5979517L, 3894452L, 3116222L, 2486857L, 5945299L,
2809445L, 1390359L, 2503865L, 3582466L, 2505757L, 4432436L, 1930800L,
3794537L, 2623668L, 3959920L, 7499419L, 2472087L, 4411731L, 3827331L,
4234585L, 3521434L, 3207605L, 2375325L, 4079966L, 2137927L, 2540208L
), CPI = c(212.193, 212.193, 213.856, 213.856, 215.834, 215.834,
216.687, 216.687, 218.711, 218.711, 218.803, 218.803, 219.179,
219.179, 216.741, 216.741, 217.631, 217.631, 218.009, 218.009,
218.178, 218.178, 217.965, 217.965, 218.011, 218.011, 218.312,
218.312, 218.439, 218.439, 220.223, 220.223, 226.421, 226.421,
226.23, 226.23, 225.672, 225.672, 221.309, 221.309, 223.467,
223.467, 224.906, 224.906, 225.964, 225.964, 225.722, 225.722,
225.922, 225.922, 226.545, 226.545, 226.889, 226.889, 226.665,
226.665, 231.317, 231.317, 230.221, 230.221, 229.601, 229.601,
227.663, 227.663, 229.392, 229.392, 230.085, 230.085, 229.815,
229.815, 229.478, 229.478, 229.104, 229.104, 230.379, 230.379,
231.407, 231.407, 230.28, 230.28, 233.546, 233.546, 233.069,
233.069, 233.049, 233.049, 232.166, 232.166, 232.773, 232.773,
232.531, 232.531, 232.945, 232.945, 233.504, 233.504, 233.596,
233.596, 233.877, 233.877, 234.149, 234.149, 233.916, 233.916,
237.433, 237.433, 236.151, 236.151, 234.812, 234.812, 234.781,
234.781, 236.293, 236.293, 237.072, 237.072, 237.9, 237.9, 238.343,
238.343, 238.25, 238.25, 237.852, 237.852, 238.031, 238.031,
233.707, 233.707, 237.838, 237.838, 237.336, 237.336, 236.525,
236.525, 234.722, 234.722, 236.119, 236.119, 236.599, 236.599,
237.805, 237.805, 238.638, 238.638, 238.654, 238.654, 238.316,
238.316, 237.945, 237.945, 236.916, 236.916, 241.729, 241.729,
241.353, 241.353, 241.432, 241.432, 237.111, 237.111, 238.132,
238.132, 239.261, 239.261, 240.229, 240.229, 241.018, 241.018,
240.628, 240.628, 240.849, 240.849, 241.428, 241.428, 242.839,
242.839, 246.663, 246.663, 246.669, 246.669, 246.524, 246.524,
243.603, 243.603, 243.801, 243.801, 244.524, 244.524, 244.733,
244.733, 244.955, 244.955, 244.786, 244.786, 245.519, 245.519,
246.819, 246.819, 247.867, 247.867, 252.885, 252.885, 252.038,
251.233, 248.991, 248.991, 249.554, 249.554, 250.546, 250.546,
251.588, 251.588, 251.989, 251.989, 252.006, 252.006, 252.146,
252.146, 252.439, 252.439, 251.712), ConCharter = c(9185, 11832.34663,
5650, 7278.47125, 5650, 7278.47125, 4713, 6071, 18567, 23918,
14800, 19065, 13800, 17777, 5117, 6591, 6450, 8309, 7500, 9661,
14170, 11000, 14000, 18035, 23240, 18041, 16492, 21245, 16492,
21245, 16400, 21126, 10305, 8000, 7700, 9919, 7000, 9017, 16700,
21513, 24347, 18900, 20500, 26408, 19350, 24927, 19200, 24733,
21899, 17000, 15500, 19967, 12500, 16102, 7000, 9017, 7000, 9017,
6500, 8373, 8373, 6500, 6800, 8759, 9339, 7250, 7750, 9983, 10305,
8000, 7800, 10048, 7750, 9983, 7500, 9661, 7250, 9339, 6700,
8631, 7750, 9983, 7600, 9790, 7600, 9790, 6900, 8888, 7000, 9017,
7300, 9404, 7200, 9275, 7450, 9597, 7500, 9661, 7700, 9919, 7900,
10176, 7600, 9790, 8400, 10821, 8400, 10821, 8500, 10949, 7600,
9790, 7500, 9661, 7800, 10048, 8000, 10305, 8100, 10434, 8200,
10563, 8200, 10563, 8300, 10692, 8500, 10949, 10100, 13011, 12238,
9500, 8300, 10692, 8500, 10949, 8700, 11207, 9100, 11722, 9500,
12238, 10000, 12882, 10900, 14041, 10600, 13655, 10200, 13139,
7000, 9017, 6698, 5200, 5150, 6634, 5200, 6698, 6700, 8631, 6300,
8115, 6100, 7858, 5700, 7342, 5700, 7342, 5700, 7342, 5700, 7342,
5300, 6827, 5200, 6698, 8200, 10563, 8200, 10563, 8700, 11207,
5500, 7085, 5700, 7342, 7500, 9661, 9400, 12109, 9500, 12238,
9100, 11722, 8000, 10305, 8200, 10563, 8900, 11465, 10800, 13912,
9500, 9500, 9000, 11594, 9900, 12753, 10500, 13526, 14170, 11000,
11100, 14299, 15973, 12400, 12300, 15845, 12000, 15458, 8500),
    NewBuildingPrice = c(53700000, 68360100, 48000000, 61104000,
    41700000, 53084100, 35705882.35, 45453588.24, 40411764.71,
    51444176.47, 41764705.88, 53166470.59, 43117647.06, 54888764.71,
    35705882.35, 45453588.24, 35876470.59, 45670747.06, 36132352.94,
    45996485.29, 46539382.35, 36558823.53, 36558823.53, 46539382.35,
    47625176.47, 37411764.71, 38264705.88, 48710970.59, 39411764.71,
    50171176.47, 44000000, 56012000, 57285000, 45000000, 45000000,
    57285000, 45000000, 57285000, 44500000, 56648500, 59831000,
    47000000, 49000000, 62377000, 50000000, 63650000, 46000000,
    58558000, 58558000, 46000000, 46000000, 58558000, 45300000,
    57666900, 44500000, 56648500, 41500000, 52829500, 41000000,
    52193000, 52193000, 41000000, 44500000, 56648500, 56648500,
    44500000, 44000000, 56012000, 56012000, 44000000, 44000000,
    56012000, 43500000, 55375500, 42000000, 53466000, 42000000,
    53466000, 41000000, 52193000, 40032258.06, 50961064.52, 40000000,
    50920000, 40000000, 50920000, 38176470.59, 48598647.06, 38134328.36,
    48545000, 41000000, 52193000, 41000000, 52193000, 38093750,
    48493343.75, 40031250, 50959781.25, 38125000, 48533125, 38142857.14,
    48555857.14, 40000000, 50920000, 41000000, 52193000, 41000000,
    52193000, 41000000, 52193000, 40000000, 50920000, 40000000,
    50920000, 41000000, 52193000, 41000000, 52193000, 41000000,
    52193000, 41000000, 52193000, 41000000, 52193000, 41000000,
    52193000, 41000000, 52193000, 41000000, 52193000, 52193000,
    41000000, 41000000, 52193000, 41000000, 52193000, 41000000,
    52193000, 41000000, 52193000, 41000000, 52193000, 41000000,
    52193000, 41000000, 52193000, 41000000, 52193000, 41000000,
    52193000, 41000000, 52193000, 50920000, 40000000, 40000000,
    50920000, 40000000, 50920000, 41000000, 52193000, 41000000,
    52193000, 40000000, 50920000, 40000000, 50920000, 40000000,
    50920000, 40000000, 50920000, 40000000, 50920000, 40000000,
    50920000, 40000000, 50920000, 41000000, 52193000, 41000000,
    52193000, 42000000, 53466000, 40000000, 50920000, 40000000,
    50920000, 40000000, 50920000, 40000000, 50920000, 41000000,
    52193000, 41000000, 52193000, 41000000, 52193000, 41000000,
    52193000, 42000000, 53466000, 42000000, 53466000, 42000000,
    42000000, 42000000, 53466000, 42000000, 53466000, 43000000,
    54739000, 54739000, 43000000, 43000000, 54739000, 53466000,
    42000000, 42000000, 53466000, 42000000, 53466000, 42000000
    ), HFO = c(265.5382699, 265.5382699, 345.6949275, 345.6949275,
    445.9030797, 445.9030797, 481.5938406, 481.5938406, 474.6855046,
    474.6855046, 494.0818807, 494.0818807, 508.073211, 508.073211,
    466.3808877, 466.3808877, 469.3727355, 469.3727355, 485.209058,
    485.209058, 462.8575634, 462.8575634, 447.1970109, 447.1970109,
    446.4384058, 446.4384058, 454.4567482, 454.4567482, 447.6084692,
    447.6084692, 535.3327982, 535.3327982, 661.2970183, 661.2970183,
    681.5729358, 681.5729358, 663.2363303, 663.2363303, 600.8077982,
    600.8077982, 642.783945, 642.783945, 673.2849541, 673.2849541,
    653.8043578, 653.8043578, 662.915367, 662.915367, 671.2282569,
    671.2282569, 660.6488532, 660.6488532, 660.7256881, 660.7256881,
    708.3672018, 708.3672018, 648.4905963, 648.4905963, 624.0271186,
    624.0271186, 616.5951036, 616.5951036, 725.7183486, 725.7183486,
    739.9016514, 739.9016514, 726.0190367, 726.0190367, 682.0949541,
    682.0949541, 600.1110092, 600.1110092, 611.887844, 611.887844,
    661.6696798, 661.6696798, 667.6007533, 667.6007533, 634.8754708,
    634.8754708, 615.2807895, 615.2807895, 608.9688034, 608.9688034,
    615.4396368, 615.4396368, 658.9545669, 658.9545669, 634.4610169,
    634.4610169, 614.9274809, 614.9274809, 606.5840954, 606.5840954,
    606.4353877, 606.4353877, 609.7552923, 609.7552923, 616.1560484,
    616.1560484, 618.7678942, 618.7678942, 614.3444685, 614.3444685,
    511.3463592, 511.3463592, 454.7987654, 454.7987654, 365.7330402,
    365.7330402, 616.6238987, 616.6238987, 610.7062225, 610.7062225,
    603.4524609, 603.4524609, 604.0654362, 604.0654362, 609.8914773,
    609.8914773, 600.3314088, 600.3314088, 592.171831, 592.171831,
    576.8988663, 576.8988663, 292.1725441, 292.1725441, 246.4201183,
    246.4201183, 229.0865385, 229.0865385, 184.9615385, 184.9615385,
    342.7136076, 342.7136076, 333.5895619, 333.5895619, 336.7940292,
    336.7940292, 370.07888, 370.07888, 360.3186667, 360.3186667,
    316.5268617, 316.5268617, 255.4885216, 255.4885216, 249.9428571,
    249.9428571, 155.6958042, 155.6958042, 285.6822917, 285.6822917,
    276.8942308, 276.8942308, 323.1230769, 323.1230769, 161.2596154,
    161.2596154, 176.7019231, 176.7019231, 191.8435897, 191.8435897,
    228.8461538, 228.8461538, 245.875, 245.875, 249.4307692,
    249.4307692, 241.2403846, 241.2403846, 258.3384615, 258.3384615,
    337.5432692, 337.5432692, 340.9807692, 340.9807692, 373.3173077,
    373.3173077, 371.4692308, 371.4692308, 326.7980769, 326.7980769,
    309.1347633, 309.1347633, 314.2120726, 314.2120726, 309.9615385,
    309.9615385, 299.1931818, 299.1931818, 308.4807692, 308.4807692,
    316.7596154, 316.7596154, 333.7692308, 333.7692308, 395.0576923,
    395.0576923, 495.5480769, 495.5480769, 469.5985183, 388.7115385,
    379.6442308, 379.6442308, 374.0428571, 374.0428571, 393.9711538,
    393.9711538, 440.5865385, 440.5865385, 449.6538462, 449.6538462,
    455.9711538, 455.9711538, 453.4538462, 453.4538462, 465.5096154,
    465.5096154, 386.3028846), IFO = c(286.9981846, 286.9981846,
    361.859751, 361.859751, 460.747666, 460.747666, 496.6584232,
    496.6584232, 489.6275789, 489.6275789, 507.4778947, 507.4778947,
    523.4875789, 523.4875789, 483.5749481, 483.5749481, 490.6869813,
    490.6869813, 502.8487552, 502.8487552, 480.3807054, 480.3807054,
    469.1151452, 469.1151452, 465.9790456, 465.9790456, 474.4701763,
    474.4701763, 465.2258817, 465.2258817, 556.2789474, 556.2789474,
    684.9715789, 684.9715789, 705.7531579, 705.7531579, 687.4496421,
    687.4496421, 620.2205263, 620.2205263, 667.03, 667.03, 697.0783158,
    697.0783158, 677.82, 677.82, 688.0405263, 688.0405263, 696.2004211,
    696.2004211, 685.1592105, 685.1592105, 683.9185263, 683.9185263,
    729.9213158, 729.9213158, 678.1105263, 678.1105263, 660.056833,
    660.056833, 655.233731, 655.233731, 748.6018421, 748.6018421,
    763.4444211, 763.4444211, 747.6252632, 747.6252632, 706.8092105,
    706.8092105, 626.1515789, 626.1515789, 635.915, 635.915,
    683.818872, 683.818872, 704.5911063, 704.5911063, 670.9761388,
    670.9761388, 650.054321, 650.054321, 643.8965432, 643.8965432,
    652.5866834, 652.5866834, 693.9902386, 693.9902386, 673.625705,
    673.625705, 649.965859, 649.965859, 644.3718822, 644.3718822,
    644.1045035, 644.1045035, 645.8142606, 645.8142606, 654.8107981,
    654.8107981, 657.1025485, 657.1025485, 651.8107417, 651.8107417,
    563.7994152, 563.7994152, 506.9328358, 506.9328358, 430.4456301,
    430.4456301, 649.5345052, 649.5345052, 645.6389974, 645.6389974,
    640.6578249, 640.6578249, 646.264191, 646.264191, 646.4189189,
    646.4189189, 638.6136364, 638.6136364, 635.7407303, 635.7407303,
    624.032235, 624.032235, 348.2040248, 348.2040248, 284.7455808,
    284.7455808, 262.2386364, 262.2386364, 219.9318182, 219.9318182,
    377.4450464, 377.4450464, 373.4734968, 373.4734968, 374.8380802,
    374.8380802, 408.3593443, 408.3593443, 403.8184426, 403.8184426,
    364.935339, 364.935339, 304.8829225, 304.8829225, 289.5515511,
    289.5515511, 181.3295455, 181.3295455, 317.329072, 317.329072,
    309.0113636, 309.0113636, 351.1181818, 351.1181818, 186.4318182,
    186.4318182, 205.5681818, 205.5681818, 221.92625, 221.92625,
    256.3977273, 256.3977273, 279, 279, 286.2818182, 286.2818182,
    278.2045455, 278.2045455, 293.3545455, 293.3545455, 362.599026,
    362.599026, 368.8636364, 368.8636364, 406.1136364, 406.1136364,
    407.1727273, 407.1727273, 355.2727273, 355.2727273, 339.004021,
    339.004021, 343.7635732, 343.7635732, 340.4659091, 340.4659091,
    335.6105372, 335.6105372, 345.3181818, 345.3181818, 356.8863636,
    356.8863636, 367.1727273, 367.1727273, 430.4431818, 430.4431818,
    521.25, 521.25, 492.6477273, 419.2954545, 411.1704545, 411.1704545,
    410.2459416, 410.2459416, 422.9318182, 422.9318182, 465.5340909,
    465.5340909, 478.2545455, 478.2545455, 486.5909091, 486.5909091,
    483.9090909, 483.9090909, 496.2045455, 496.2045455, 407.7159091
    ), MGO = c(453.2946429, 453.2946429, 477.9595238, 477.9595238,
    591.0833333, 591.0833333, 623.2161905, 623.2161905, 692.458624,
    692.458624, 711.9907946, 711.9907946, 742.2348837, 742.2348837,
    614.5327381, 614.5327381, 643.2946429, 643.2946429, 686.0642857,
    686.0642857, 657.7142857, 657.7142857, 651.0505952, 651.0505952,
    643.5, 643.5, 656.4315476, 656.4315476, 657.985119, 657.985119,
    786.6656977, 786.6656977, 922.6119186, 922.6119186, 956.1061047,
    956.1061047, 930.755814, 930.755814, 866.1065891, 866.1065891,
    938.9481589, 938.9481589, 971.4492248, 971.4492248, 937.7955426,
    937.7955426, 941.8498062, 941.8498062, 948.1639535, 948.1639535,
    937.3231589, 937.3231589, 933.9395349, 933.9395349, 959.095057,
    959.095057, 1019.252193, 1019.252193, 991.6284211, 991.6284211,
    985.2127193, 985.2127193, 979.1858365, 979.1858365, 1000.876426,
    1000.876426, 985.2276616, 985.2276616, 948.5893536, 948.5893536,
    872.8977186, 872.8977186, 896.7889734, 896.7889734, 998.4410526,
    998.4410526, 1019.827632, 1019.827632, 994.0855263, 994.0855263,
    979.0090201, 979.0090201, 967.2960674, 967.2960674, 983.7078652,
    983.7078652, 1027.866228, 1027.866228, 977.4957895, 977.4957895,
    942.8925439, 942.8925439, 930.0765464, 930.0765464, 937.4226804,
    937.4226804, 963.5524055, 963.5524055, 980.5042955, 980.5042955,
    985.0053763, 985.0053763, 968.5701149, 968.5701149, 837.2136986,
    837.2136986, 788.818662, 788.818662, 685.173913, 685.173913,
    972.9114379, 972.9114379, 962.829902, 962.829902, 964.2464859,
    964.2464859, 961.3871486, 961.3871486, 957.4074074, 957.4074074,
    942.0369198, 942.0369198, 931.2835498, 931.2835498, 908.5083333,
    908.5083333, 585.1183168, 585.1183168, 491.1208333, 491.1208333,
    476.0892857, 476.0892857, 408.6428571, 408.6428571, 621.1639851,
    621.1639851, 605.6785714, 605.6785714, 598.0431548, 598.0431548,
    642.6857143, 642.6857143, 630.0462963, 630.0462963, 571.7781746,
    571.7781746, 507.6554487, 507.6554487, 505.9642857, 505.9642857,
    351.8231293, 351.8231293, 497.1383929, 497.1383929, 469.3392857,
    469.3392857, 508.4142857, 508.4142857, 340.9642857, 340.9642857,
    371.6964286, 371.6964286, 386.1060714, 386.1060714, 449.6785714,
    449.6785714, 475.9464286, 475.9464286, 453.5428571, 453.5428571,
    441.3214286, 441.3214286, 452.0428571, 452.0428571, 517.7104592,
    517.7104592, 547.625, 547.625, 577.0892857, 577.0892857,
    586.4142857, 586.4142857, 520.0535714, 520.0535714, 500.5695055,
    500.5695055, 510.5292659, 510.5292659, 491.9642857, 491.9642857,
    470.5665584, 470.5665584, 480.5892857, 480.5892857, 501,
    501, 541, 541, 627.3214286, 627.3214286, 746.9642857, 746.9642857,
    699.3928571, 600.1607143, 613.4285714, 613.4285714, 611.1628401,
    611.1628401, 648.0535714, 648.0535714, 694.8214286, 694.8214286,
    686.0571429, 686.0571429, 678.6428571, 678.6428571, 683.2142857,
    683.2142857, 720.8214286, 720.8214286, 597.1875)), class =
"data.frame", row.names = c(NA,
-221L))

	[[alternative HTML version deleted]]


From tg@77m @end|ng |rom y@hoo@com  Wed Mar 20 15:39:10 2019
From: tg@77m @end|ng |rom y@hoo@com (Thomas Subia)
Date: Wed, 20 Mar 2019 14:39:10 +0000 (UTC)
Subject: [R] High p-value
References: <2026916189.8548620.1553092750815.ref@mail.yahoo.com>
Message-ID: <2026916189.8548620.1553092750815@mail.yahoo.com>

>From previous posting:

"This is my function:

wilcox.test(A,B, data = data, paired = FALSE)

It gives me high p value, though the median of A column is 6900 and B
column is 3500.

Why it gives p value high if there is a difference in the median?"

Let's examine your choice to use the Wilcoxon test with paired = FALSE.


If both A and B are given and paired is FALSE, a Wilcoxon rank sum test (equivalent to the Mann-Whitney test) is carried out. In this case, the null hypothesis is that the distributions of x and y differ by a location shift of mu and the alternative is that they differ by some other location shift (and the one-sided alternative "greater" is that A is shifted to the right of B). 

You observed that the medians are different. While the medians may be different, the test is dependent on ranks not medians.

HTH,

Thomas Subia


From bgunter@4567 @end|ng |rom gm@||@com  Wed Mar 20 15:53:56 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 20 Mar 2019 07:53:56 -0700
Subject: [R] plot.xmean.ordinaly vs plot() in package "rms"
In-Reply-To: <CAHn6EJ17OO6KMFU9caPq8h1SkGOy_6v8LMrEbmPj-ymuD-steg@mail.gmail.com>
References: <CAHn6EJ3LF286PCWaGPJ0tdk8wXSwejmJrsygzWtHeL2qyoUErw@mail.gmail.com>
 <031F9B89-F684-4735-A9B2-2AAD35942C91@dcn.davis.ca.us>
 <CAHn6EJ2=jLakgdMazEfwvUQ=X0JbjhjoEL4os3iR8wvFeMO2Kw@mail.gmail.com>
 <7D856F8B-80C7-48BB-BF37-BF17D0204A5F@dcn.davis.ca.us>
 <CAHn6EJ17OO6KMFU9caPq8h1SkGOy_6v8LMrEbmPj-ymuD-steg@mail.gmail.com>
Message-ID: <CAGxFJbQjO-ta=-0bqAPiO6dbBvTFU-6-c=MSKERETOvnxXvLZg@mail.gmail.com>

Please study the documentation to which you were referred. This list is not
appropriate for comprehensive tutorials, which is what you need, although
all help is of course in some sense a tutorial.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Mar 20, 2019 at 6:30 AM Kim Jacobsen <kimsjacobsen at gmail.com> wrote:

> Mailing list now included (apologies, first time I post anything so not
> quite sure how it works).
>
> You are quite right, it was a typo. I meant to write that
> plot.xmean.ordinaly(). So please let me correct my last statement: the
> plot.xmean.ordinaly() command and plot() command are interchangeable as
> long as x is an object x of class "xmean.ordinaly", and
> plot.xmean.ordinaly() is best used if the object is not of class
> "xmean.ordinaly" or if you are unsure what class it it. Is this a correct
> encapsulation?
>
>
> On Sun, 17 Mar 2019 at 14:38, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
> > Please keep the mailing list included in the thread.
> >
> > I can't tell if you do understand and are just being sloppy, or if you
> are
> > completely confused, because xmean.ordinaly() and plot.xmean.ordinaly()
> are
> > two completely different symbols in R.
> >
> > As for being "safe"... you may choose to be specific or not, but plot and
> > plot.xmean.ordinaly are both equally "safe" to call, and being too
> specific
> > can cause problems sometimes as well.
> >
> > On March 17, 2019 6:40:10 AM PDT, Kim Jacobsen <kimsjacobsen at gmail.com>
> > wrote:
> > >Dear Jeff,
> > >
> > >Thank you so much! So if I understand the S3 object documents
> > >correctly,
> > >the xmean.ordinaly() command and plot() command are interchangeable as
> > >long
> > >as x is an object x of class "xmean.ordinaly"? So would I be right to
> > >think
> > >that I might as well just xmean.ordinaly() to be safe?
> > >
> > >Many thanks,
> > >
> > >
> > >
> > >On Sun, 17 Mar 2019 at 02:08, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> > >wrote:
> > >
> > >> Read up on S3 object orientation[1]. If you have an object x of class
> > >> "xmean.ordinaly" then writing
> > >>
> > >> plot(x)
> > >>
> > >> will end up invoking the plot.xmean.ordinaly function rather than the
> > >> plot.default function in base graphics. This is broadly true
> > >throughout R.
> > >>
> > >> [1] http://adv-r.had.co.nz/S3.html
> > >>
> > >> On March 16, 2019 11:03:06 AM PDT, Kim Jacobsen
> > ><kimsjacobsen at gmail.com>
> > >> wrote:
> > >> >Would anyone be able to explain what the difference is between
> > >> >plot.xmean.ordinaly and plot() in the "rms" package? (for the
> > >purposes
> > >> >of
> > >> >testing the proportional odds assumption in ordinal models). In the
> > >> >package
> > >> >document (https://cran.r-project.org/web/packages/rms/rms.pdf) they
> > >> >seem
> > >> >both to be used interchangeably.
> > >> >
> > >> >Thank you!
> > >>
> > >> --
> > >> Sent from my phone. Please excuse my brevity.
> > >>
> >
> > --
> > Sent from my phone. Please excuse my brevity.
> >
>
>
> --
> Kim Solve Jacobsen
> PhD candidate, Wildlife Conservation Research Unit
> University of Oxford
> Recanati-Kaplan Centre
> OX13 5QL Oxford, UK
>
> [image: Image result for oxford logo]
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Mar 20 15:55:53 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 20 Mar 2019 07:55:53 -0700
Subject: [R] System("source activate condaenv")
In-Reply-To: <CAOEdaTtffn2WLyhy015RW_LeSCHjLd=cEe-EVvroEM1WStUOcg@mail.gmail.com>
References: <CAOEdaTtffn2WLyhy015RW_LeSCHjLd=cEe-EVvroEM1WStUOcg@mail.gmail.com>
Message-ID: <CAGxFJbSOW0b3dc0hLUyEP58TrKC=MKMs7tVA1-GSiLtBFzqL+A@mail.gmail.com>

R is *not* RStudio. Please go to the RStudio site, not here,  for help with
that software.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Mar 20, 2019 at 6:30 AM Sandra Elisabeth Chaudron <
sandra.chaudron at gmail.com> wrote:

> Hi,
> I am using the server version of RStudio and I have a script where I want
> to activate the conda environment that I set up for a bioinformatic tool
> called MinVar.
> For that I use in my script the command: system("source
> /home/ubuntu/miniconda3/bin/activate minvar"). I provide the path to
> activate because the tool is installed under my second user name in the
> server I use.
> And the error message is : sh: 1: source: not found error in running
> command
> I tried to look for solution in any possible forum but did not find
> anything.
> I was wondering if a solution for such issue was already discussed? and an
> answer found?
> Thanks a lot.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From putr@_@utumn86 @end|ng |rom y@hoo@com  Wed Mar 20 16:43:12 2019
From: putr@_@utumn86 @end|ng |rom y@hoo@com (smart hendsome)
Date: Wed, 20 Mar 2019 15:43:12 +0000 (UTC)
Subject: [R] Solving Function using Conjugate Gradient
References: <82679891.7387935.1553096592702.ref@mail.yahoo.com>
Message-ID: <82679891.7387935.1553096592702@mail.yahoo.com>

Hi R-user,
I have a problem regarding to estimate lambda[r] by minimizing the convex function using conjugate gradient in R-programming.? The function as below:


Hopefully, anybody in this forum can help me. Thanks so much.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: lag.png
Type: image/png
Size: 6204 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190320/05b46132/attachment.png>

From ||@t@ @end|ng |rom dewey@myzen@co@uk  Wed Mar 20 16:53:12 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Wed, 20 Mar 2019 15:53:12 +0000
Subject: [R] plot.xmean.ordinaly vs plot() in package "rms"
In-Reply-To: <CAHn6EJ17OO6KMFU9caPq8h1SkGOy_6v8LMrEbmPj-ymuD-steg@mail.gmail.com>
References: <CAHn6EJ3LF286PCWaGPJ0tdk8wXSwejmJrsygzWtHeL2qyoUErw@mail.gmail.com>
 <031F9B89-F684-4735-A9B2-2AAD35942C91@dcn.davis.ca.us>
 <CAHn6EJ2=jLakgdMazEfwvUQ=X0JbjhjoEL4os3iR8wvFeMO2Kw@mail.gmail.com>
 <7D856F8B-80C7-48BB-BF37-BF17D0204A5F@dcn.davis.ca.us>
 <CAHn6EJ17OO6KMFU9caPq8h1SkGOy_6v8LMrEbmPj-ymuD-steg@mail.gmail.com>
Message-ID: <c19f5677-f935-2a7b-b650-13e8e67c8231@dewey.myzen.co.uk>

Dear Kim

See inline

On 19/03/2019 22:29, Kim Jacobsen wrote:
> Mailing list now included (apologies, first time I post anything so not
> quite sure how it works).
> 
> You are quite right, it was a typo. I meant to write that
> plot.xmean.ordinaly(). So please let me correct my last statement: the
> plot.xmean.ordinaly() command and plot() command are interchangeable as
> long as x is an object x of class "xmean.ordinaly", and
> plot.xmean.ordinaly() is best used if the object is not of class
> "xmean.ordinaly" or if you are unsure what class it it. Is this a correct
> encapsulation?
> 

I think the best way to think about it is that plot() looks at whatever 
you gave it to plot and then looks at all the plot methods it knows 
about to see if it has one which matches the class of whatever you gave 
it. It then uses that one. In general unless you know you need to 
override that behaviour you should never need to explicitly use any of 
the other plot methods. The same applies to all the other methods like 
print() summary() and so on.

Michael

> 
> On Sun, 17 Mar 2019 at 14:38, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> 
>> Please keep the mailing list included in the thread.
>>
>> I can't tell if you do understand and are just being sloppy, or if you are
>> completely confused, because xmean.ordinaly() and plot.xmean.ordinaly() are
>> two completely different symbols in R.
>>
>> As for being "safe"... you may choose to be specific or not, but plot and
>> plot.xmean.ordinaly are both equally "safe" to call, and being too specific
>> can cause problems sometimes as well.
>>
>> On March 17, 2019 6:40:10 AM PDT, Kim Jacobsen <kimsjacobsen at gmail.com>
>> wrote:
>>> Dear Jeff,
>>>
>>> Thank you so much! So if I understand the S3 object documents
>>> correctly,
>>> the xmean.ordinaly() command and plot() command are interchangeable as
>>> long
>>> as x is an object x of class "xmean.ordinaly"? So would I be right to
>>> think
>>> that I might as well just xmean.ordinaly() to be safe?
>>>
>>> Many thanks,
>>>
>>>
>>>
>>> On Sun, 17 Mar 2019 at 02:08, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>>> wrote:
>>>
>>>> Read up on S3 object orientation[1]. If you have an object x of class
>>>> "xmean.ordinaly" then writing
>>>>
>>>> plot(x)
>>>>
>>>> will end up invoking the plot.xmean.ordinaly function rather than the
>>>> plot.default function in base graphics. This is broadly true
>>> throughout R.
>>>>
>>>> [1] http://adv-r.had.co.nz/S3.html
>>>>
>>>> On March 16, 2019 11:03:06 AM PDT, Kim Jacobsen
>>> <kimsjacobsen at gmail.com>
>>>> wrote:
>>>>> Would anyone be able to explain what the difference is between
>>>>> plot.xmean.ordinaly and plot() in the "rms" package? (for the
>>> purposes
>>>>> of
>>>>> testing the proportional odds assumption in ordinal models). In the
>>>>> package
>>>>> document (https://cran.r-project.org/web/packages/rms/rms.pdf) they
>>>>> seem
>>>>> both to be used interchangeably.
>>>>>
>>>>> Thank you!
>>>>
>>>> --
>>>> Sent from my phone. Please excuse my brevity.
>>>>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>
> 
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From |zm|r||g @end|ng |rom m@||@n|h@gov  Wed Mar 20 17:01:26 2019
From: |zm|r||g @end|ng |rom m@||@n|h@gov (Izmirlian, Grant (NIH/NCI) [E])
Date: Wed, 20 Mar 2019 16:01:26 +0000
Subject: [R] data frame solution
In-Reply-To: <1314762859.7288924.1552993582101@mail.yahoo.com>
References: <1314762859.7288924.1552993582101.ref@mail.yahoo.com>,
 <1314762859.7288924.1552993582101@mail.yahoo.com>
Message-ID: <BL0PR0901MB240216E22BC214134FB39139E5410@BL0PR0901MB2402.namprd09.prod.outlook.com>

Statements like c(rbind(x, xx+yy), max(t)) and rep(0,length(df$b[1]))  don't make any sense. You're example will be easier to understand if you show us the nrow(df) ==3
case. Thanks


Grant Izmirlian, Ph.D.
Mathematical Statistician
izmirlig at mail.nih.gov

Delivery Address:
9609 Medical Center Dr, RM 5E130
Rockville MD 20850

Postal Address:
BG 9609 RM 5E130 MSC 9789
9609 Medical Center Dr
Bethesda, MD 20892-9789

 ofc:  240-276-7025
 cell: 240-888-7367
  fax: 240-276-7845


________________________________
From: Andras Farkas <motyocska at yahoo.com>
Sent: Tuesday, March 19, 2019 7:06 AM
To: R-help Mailing List
Subject: [R] data frame solution

Hello All,

wonder if you have thoughts on a clever solution for this code:



df       <- data.frame(a = c(6,1), b = c(1000,1200), c =c(-1,3))

#the caveat here is that the number of rows for df can be anything from 1 row to in the hundreds. I kept it to 2 to have minimal reproducible

t<-seq(-5,24,0.1) #min(t) will always be <=df$c[1], which is the value that is always going to equal to min(df$c)

times1 <- c(rbind(df$c[1],df$c[1]+df$a[1]),max(t)) #length of times1 will always be 3, see times2 is of length 4

input1   <- c(rbind(df$b[1]/df$a[1],rep(0,length(df$b[1]))),0) #length of input1 will always be 3, see input2 is of length 4

out1 <-data.frame(t,ifelse(t>=times1[1]&t<times1[2],input1[1],ifelse(t>=times1[2]&t<times1[3],input1[2],input1[3])))

times2 <- c(times1[1],rbind(df$c[2],df$c[2]+df$a[2]),max(t)) #note 1st value of times2, (or for times3, times4,.. if more rows in df) is times1[1], which will always be < times2[1] (or times3[1], times4[1],.. if more rows in df)

input2   <- c(0,rbind(df$b[2]/df$a[2],rep(0,length(df$b[2]))),0) #note 1st value of input2  (or for input3, input4,.. if more rows in df)  is 0, which will always be 0 (or for all input2-n other then for input1 as that will be as above ,.. if more rows in df)

out2 <-data.frame(t,ifelse(t>=times2[1]&t<times2[2],input2[1],ifelse(t>=times2[2]&t<times2[3],input2[2],ifelse(t>=times2[3]&t<times2[4],input2[3],input2[3]))))

result<-data.frame(t,out1[,2]+out2[,2])

so if I did it all manually then for row 3 in df I would calculate out3 and so on... Would like to be able to do this with a clean function solution that allows for different row numbers in df...

as always your help is appreciated

thanks

Andras



	[[alternative HTML version deleted]]


From |r@nk|eboytje @end|ng |rom hotm@||@com  Wed Mar 20 10:14:39 2019
From: |r@nk|eboytje @end|ng |rom hotm@||@com (Frank van Berkum)
Date: Wed, 20 Mar 2019 09:14:39 +0000
Subject: [R] R-help Digest, Vol 193, Issue 16
In-Reply-To: <mailman.353984.1.1552734002.30403.r-help@r-project.org>
References: <mailman.353984.1.1552734002.30403.r-help@r-project.org>
Message-ID: <AM0PR0302MB3458D10A11E1A8ED95EAF627BC410@AM0PR0302MB3458.eurprd03.prod.outlook.com>

Dear Simon,

Thank you for your response! I was not able to provide you with the requested information at an earlier stage since I am not a full time academic / researcher.

An example of a bam call that may result in an error is:
bam(formula=Di ~ 1 + Gender + I(L_Dis==0) + s(DisPerc, by=as.numeric(L_Dis==2), bs='cr'), offset=log(Ei*Mi), family=poisson, data=dtPF, method="fREML", discrete=TRUE, gc.level=2);

Here, dtPF is a data.table object with 22m rows and 21 columns/variables, Gender is a factor variable, L_Dis is an integer variable which equals 0 if DisPerc is missing (manually set to 0.1), equals 1 if DisPerc==0, and equals 2 if DisPerc>0 (ranges from 0 to 0.25).

The sessionInfo() provides the following output:
R version 3.4.3 (2017-11-30)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux 9 (stretch)

Matrix products: default
BLAS/LAPACK: /sara/eb/Debian9/OpenBLAS/0.2.20-GCC-6.4.0-2.28/lib/libopenblas_sandybridgep-r0.2.20.so

locale:
 [1] LC_CTYPE=en_US       LC_NUMERIC=C         LC_TIME=en_US
 [4] LC_COLLATE=en_US     LC_MONETARY=en_US    LC_MESSAGES=en_US
 [7] LC_PAPER=en_US       LC_NAME=C            LC_ADDRESS=C
[10] LC_TELEPHONE=C       LC_MEASUREMENT=en_US LC_IDENTIFICATION=C

attached base packages:
[1] methods   stats     graphics  grDevices utils     datasets  base

other attached packages:
[1] mgcv_1.8-27       nlme_3.1-137      data.table_1.12.0

loaded via a namespace (and not attached):
[1] compiler_3.4.3  Matrix_1.2-16   tools_3.4.3     splines_3.4.3
[5] grid_3.4.3      lattice_0.20-38

Thank you for your help!

Frank

________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of r-help-request at r-project.org <r-help-request at r-project.org>
Sent: Saturday, March 16, 2019 11:00 AM
To: r-help at r-project.org
Subject: R-help Digest, Vol 193, Issue 16

Send R-help mailing list submissions to
        r-help at r-project.org

To subscribe or unsubscribe via the World Wide Web, visit
        https://stat.ethz.ch/mailman/listinfo/r-help
or, via email, send a message with subject or body 'help' to
        r-help-request at r-project.org

You can reach the person managing the list at
        r-help-owner at r-project.org

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-help digest..."


Date: Fri, 15 Mar 2019 12:31:31 +0000
From: Simon Wood <simon.wood at bath.edu>
To: r-help at r-project.org
Subject: Re: [R] [mgcv] Memory issues with bam() on computer cluster
Message-ID: <d8e2643a-d960-0d86-4296-f0c7fcf149cb at bath.edu>
Content-Type: text/plain; charset="utf-8"

Can you supply the results of sessionInfo() please, and the full bam
call that causes this.

best,

Simon (mgcv maintainer)

On 15/03/2019 09:09, Frank van Berkum wrote:
> Dear Community,
>
> In our current research we are trying to fit Generalized Additive Models to a large dataset. We are using the package mgcv in R.
>
> Our dataset contains about 22 million records with less than 20 risk factors for each observation, so in our case n>>p. The dataset covers the period 2006 until 2011, and we analyse both the complete dataset and datasets in which we leave out a single year. The latter part is done to analyse robustness of the results. We understand k-fold cross validation may seem more appropriate, but out approach is closer to what is done in practice (how will one additional year of information affect your estimates?).
>
> We use the function bam as advocated in Wood et al. (2017), and we apply the following options: bam(?, discrete=TRUE, chunk.size=10000, gc.level=1). We run these analyses on a computer cluster (see https://userinfo.surfsara.nl/systems/lisa/description for details), and the job is allocated to a node within the computer cluster. A node has at least 16 cores and 64Gb memory.
>
> We had expected 64Gb of memory to be sufficient for these analyses, especially since the bam function is built specifically for large datasets. However, when applying this function to the different datasets described above with different regression specifications (different risk factors included in the linear predictor), we sometimes obtain errors of the following form.
>
> Error in XWyd(G$Xd, w, z, G$kd, G$ks, G$ts, G$dt, G$v, G$qc, G$drop, ar.stop,  :
>
>    'Calloc' could not allocate memory (22624897 of 8 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> XWyd
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> Error in Xbd(G$Xd, coef, G$kd, G$ks, G$ts, G$dt, G$v, G$qc, G$drop) :
>
>    'Calloc' could not allocate memory (18590685 of 8 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> Xbd
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> Error: cannot allocate vector of size 1.7 Gb
>
> Timing stopped at: 2 0.556 4.831
>
> Error in system.time(oo <- .C(C_XWXd0, XWX = as.double(rep(0, (pt + nt)^2)),  :
>
>    'Calloc' could not allocate memory (55315650 of 24 bytes)
>
> Calls: fnEstimateModel_bam -> bam -> bgam.fitd -> XWXd -> system.time -> .C
>
> Timing stopped at: 1.056 1.396 2.459
>
> Execution halted
>
> Warning message:
>
> system call failed: Cannot allocate memory
>
> The errors seem to arise at different stages in the optimization process. We have analysed whether these errors disappear if different settings are used (different chunk.size, different gc.level), but this does not resolve our problem. Also, the errors occur on different datasets when using different settings, and even when using the same settings it is possible that an error that occurred on dataset X in one run it does not necessarily occur on dataset X in a different run. When using the discrete=TRUE option, optimization can be parallelized, but we have chosen to not employ this feature to ensure memory does not have to be shared between parallel processes.
>
> Naturally I cannot share our dataset with you which makes the problem difficult to analyse. However, based on your collective knowledge, could you pinpoint us to where the problem may occur? Is it something within the C-code used within the package (as the last error seems to indicate), or is it related to the computer cluster?
>
> Any help or insights is much appreciated.
>
> Kind regards,
>
> Frank
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Simon Wood, School of Mathematics, University of Bristol, BS8 1TW UK
https://people.maths.bris.ac.uk/~sw15190/

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Mar 20 18:34:16 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 20 Mar 2019 10:34:16 -0700
Subject: [R] Solving Function using Conjugate Gradient
In-Reply-To: <82679891.7387935.1553096592702@mail.yahoo.com>
References: <82679891.7387935.1553096592702.ref@mail.yahoo.com>
 <82679891.7387935.1553096592702@mail.yahoo.com>
Message-ID: <CAGxFJbTqkS5CCoO8sqAd9hK+CAT2PHMY09bDG1etg7wTFVcJxQ@mail.gmail.com>

This list has a no homework policy.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Mar 20, 2019 at 8:54 AM smart hendsome via R-help <
r-help at r-project.org> wrote:

> Hi R-user,
> I have a problem regarding to estimate lambda[r] by minimizing the convex
> function using conjugate gradient in R-programming.  The function as below:
>
>
> Hopefully, anybody in this forum can help me. Thanks so much.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Mar 20 19:11:52 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 20 Mar 2019 11:11:52 -0700
Subject: [R] Solving Function using Conjugate Gradient
In-Reply-To: <1341899265.7446454.1553103723351@mail.yahoo.com>
References: <82679891.7387935.1553096592702.ref@mail.yahoo.com>
 <82679891.7387935.1553096592702@mail.yahoo.com>
 <CAGxFJbTqkS5CCoO8sqAd9hK+CAT2PHMY09bDG1etg7wTFVcJxQ@mail.gmail.com>
 <1341899265.7446454.1553103723351@mail.yahoo.com>
Message-ID: <CAGxFJbRgoE19zseA=K=oYu2jpPr5cZeUd637Hn8OBR6Z1Pvt2w@mail.gmail.com>

You should (almost) always also reply to the list, especially in this case,
where others my also fail to help because they perceive your query as
homework.

However, I should warn you that such comprehensive queries are often
dismissed as "do my work for me" requests. See the Posting Guide linked
below for a comprehension description of what and how to post here.
Following the recommended protocols will certainly increase your chance of
receiving a helpful reply.

Cheers,
Bert




On Wed, Mar 20, 2019 at 10:42 AM smart hendsome <putra_autumn86 at yahoo.com>
wrote:

> Hi Gunter,
>
> Actually, thats not my homework, I just curios regarding how the paper of
> entropy optimized that function using either raphson method or conjugate
> gradient, if you dont want to help thats ok.
>
>
> Zuhri
>
> Sent from Yahoo Mail on Android
> <https://go.onelink.me/107872968?pid=InProduct&c=Global_Internal_YGrowth_AndroidEmailSig__AndroidUsers&af_wl=ym&af_sub1=Internal&af_sub2=Global_YGrowth&af_sub3=EmailSignature>
>
> On Thu, 21 Mar 2019 at 1:34 a.m., Bert Gunter
> <bgunter.4567 at gmail.com> wrote:
> This list has a no homework policy.
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Mar 20, 2019 at 8:54 AM smart hendsome via R-help <
> r-help at r-project.org> wrote:
>
> Hi R-user,
> I have a problem regarding to estimate lambda[r] by minimizing the convex
> function using conjugate gradient in R-programming.  The function as below:
>
>
> Hopefully, anybody in this forum can help me. Thanks so much.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From j@ne@wong083 @end|ng |rom gm@||@com  Thu Mar 21 03:52:37 2019
From: j@ne@wong083 @end|ng |rom gm@||@com (wong jane)
Date: Thu, 21 Mar 2019 10:52:37 +0800
Subject: [R] Problem of data format using function of tmerger()
Message-ID: <CACnBwQsuYMsEd1UJwcOQSwz_pPbJSU_S6XNcQwELcyDVcy8vDQ@mail.gmail.com>

We want to perform a survival analysis using time-dependent covariates in
the Cox
regression. In this analysis, ESRD1_END and ESRD1_TIME are the outcome and
follow-up time,respectively. AKI_END is a binary time-dependent variable,
while
AKI_TIME is the time point for AKI_END measurement. The Cox model is some-
thing like: Surv(tstart, tstop, ESRD1_END) ? AKI_END.
However, it seems the formatted data were incorrect using the functions of
event()
and tdc(), mainly including two problems:
1, The output of event(y, x) is inconsistent with the input data x.
2, Missing value NA was generated from tdc(y, x).


> library(survival)
> ori_data <- read.table("test_data.csv", sep=",", header=TRUE,
stringsAsFactors=F)
> str(ori_data)
' data.frame ' : 7 obs. of 5 variables:
$ data95_Obs: int 10112 15 11 12 13 14 16
$ ESRD1_TIME: num 6.27 9.35 16.77 10.84 18.33 ...
$ ESRD1_END : int 1 0 0 0 0 1 1
$ AKI_END : int 1 1 0 0 0 0 1
$ AKI_TIME : num 3.23 7.65 16.77 10.84 18.33 ...
> dim(ori_data)
[1] 7 5
> head(ori_data)
data95_Obs ESRD1_TIME ESRD1_END AKI_END AKI_TIME
 10112 6.267 1 1 3.233
 15 9.347 0 1 7.654
 11 16.775 0 0 16.775
 12 10.836 0 0 10.836
 13 18.330 0 0 18.330
 14 6.971 1 0 6.971

> #generate the variables "tstart", "tstop" and "status". However, the
"status" is different
> #from "ESRD1_END"
> ori_data <- tmerge(ori_data, ori_data, id=data95_Obs,
status=event(ESRD1_TIME, ESRD1_END))
> dim(ori_data)
[1] 7 9
> head(ori_data)
data95_Obs ESRD1_TIME ESRD1_END AKI_END AKI_TIME id tstart tstop status
 10112 6.267 1 1 3.233 10112 0 6.267 1
 15 9.347 0 1 7.654 15 0 9.347 0
 11 16.775 0 0 16.775 11 0 16.775 1
 12 10.836 0 0 10.836 12 0 10.836 0
 13 18.330 0 0 18.330 13 0 18.330 0
 14 6.971 1 0 6.971 14 0 6.971 0

> #generate the time-dependent variable "AKI_TC". However, why there are
some NAs.
> ori_data <- tmerge(ori_data, ori_data, id=data95_Obs,
AKI_TC=tdc(AKI_TIME, AKI_END))
> dim(ori_data)
[1] 9 10
> head(ori_data)
data95_Obs ESRD1_TIME ESRD1_END AKI_END AKI_TIME id tstart tstop status
 10112 6.267 1 1 3.233 10112 0.000 3.233 0
 10112 6.267 1 1 3.233 10112 3.233 6.267 1
 15 9.347 0 1 7.654 15 0.000 7.654 0
 15 9.347 0 1 7.654 15 7.654 9.347 0
 11 16.775 0 0 16.775 11 0.000 16.775 1
 12 10.836 0 0 10.836 12 0.000 10.836 0
AKI_TC
 NA
 1
 NA
 0
 NA
 NA

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Mar 21 12:56:19 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 21 Mar 2019 11:56:19 +0000
Subject: [R] problem with nls....
Message-ID: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members,
                            I have the following nls call:

> HF53nl <- nls(HF1 ~ ((m/HF6) + 1),data = data.frame(HF6,HF1),start = list(m = 0.1))
> overview(HF53nl)

------
Formula: HF1 ~ ((m/HF6) + 1)

Parameters:
   Estimate Std. Error t value Pr(>|t|)
m 2.147e-07  1.852e-06   0.116    0.908

Residual standard error: 0.03596 on 799 degrees of freedom

Number of iterations to convergence: 1
Achieved convergence tolerance: 1.246e-06

------
Residual sum of squares: 1.03

------
t-based confidence interval:
           2.5%        97.5%
1 -3.420983e-06 3.850292e-06

------
Correlation matrix:
  m
m 1

The scatter plot of HF6 and HF1 and the corresponding fitted line according to the above output of nls is attached(HF53nl). The fitted line is almost a straight line. But it should be a curve something of: y ~ 1/x.  I think the very small value of m is making the curve a straight line.

But the fitted curve of the following call makes sense(attached: HF43nl):

> HF43nl <- nls(HF1 ~ ((k/HF5) + 1),data = data.frame(HF5,HF1),start = list(k = 0.1))
> overview(HF43nl)

------
Formula: HF1 ~ ((k/HF5) + 1)

Parameters:
    Estimate Std. Error t value Pr(>|t|)
k -5.367e-04  5.076e-05  -10.57   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 0.03368 on 799 degrees of freedom

Number of iterations to convergence: 1
Achieved convergence tolerance: 3.076e-07

------
Residual sum of squares: 0.906

------
t-based confidence interval:
           2.5%         97.5%
1 -0.0006363717 -0.0004370954

------
Correlation matrix:
  k
k 1

The queer thing is that the RSS for HF53nl and HF43nl is almost the same, which points to the purported validity of HF53nl.  How is this possible? Can I go with the above estimates for the coefficient m of HF6 being equal to 2.147 * 10^(-7)? How do I make an nls call so that there is a better fit to HF1 and HF6.

NB: If you can't access the attached graphs, how do I send it to you otherwise? I can also give you HF1,HF6,HF5 if needed....

very many thanks for your time and effort....
yours sincerely,

AKSHAY M KULKARNI

-------------- next part --------------
A non-text attachment was scrubbed...
Name: HF53nl.png
Type: image/png
Size: 11024 bytes
Desc: HF53nl.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190321/8f8fbf70/attachment.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: HF43nl.png
Type: image/png
Size: 11402 bytes
Desc: HF43nl.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190321/8f8fbf70/attachment-0001.png>

From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Mar 21 13:25:37 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 21 Mar 2019 12:25:37 +0000
Subject: [R] Fw: problem with nls....
In-Reply-To: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <SL2P216MB0091616E2AB8AC8D0CEADA5FC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear members,
                            On a closer inspection, I can see that the scatterplot of HF1 and HF5 is of the form y ~ -(1/x), while that of HF1 and HF6 is of the form y ~ (1/x). Is it possible that HF43nl is converging almost due to chance? I mean, for HF53nl, a straight line minimizes the RSS rather than for a curve like y ~ (1/x). Is it possible? If that is the case, should I model it linearly rather than nonlinearly? It is unsettling(this would always gives the wrong prediction given a predictor!). Or rather picewise nonlinear regression(for HF6 < 0 and HF6 > 0)?

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI


________________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
Sent: Thursday, March 21, 2019 5:26 PM
To: R help Mailing  list
Subject: [R] problem with nls....

dear members,
                            I have the following nls call:

> HF53nl <- nls(HF1 ~ ((m/HF6) + 1),data = data.frame(HF6,HF1),start = list(m = 0.1))
> overview(HF53nl)

------
Formula: HF1 ~ ((m/HF6) + 1)

Parameters:
   Estimate Std. Error t value Pr(>|t|)
m 2.147e-07  1.852e-06   0.116    0.908

Residual standard error: 0.03596 on 799 degrees of freedom

Number of iterations to convergence: 1
Achieved convergence tolerance: 1.246e-06

------
Residual sum of squares: 1.03

------
t-based confidence interval:
           2.5%        97.5%
1 -3.420983e-06 3.850292e-06

------
Correlation matrix:
  m
m 1

The scatter plot of HF6 and HF1 and the corresponding fitted line according to the above output of nls is attached(HF53nl). The fitted line is almost a straight line. But it should be a curve something of: y ~ 1/x.  I think the very small value of m is making the curve a straight line.

But the fitted curve of the following call makes sense(attached: HF43nl):

> HF43nl <- nls(HF1 ~ ((k/HF5) + 1),data = data.frame(HF5,HF1),start = list(k = 0.1))
> overview(HF43nl)

------
Formula: HF1 ~ ((k/HF5) + 1)

Parameters:
    Estimate Std. Error t value Pr(>|t|)
k -5.367e-04  5.076e-05  -10.57   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 0.03368 on 799 degrees of freedom

Number of iterations to convergence: 1
Achieved convergence tolerance: 3.076e-07

------
Residual sum of squares: 0.906

------
t-based confidence interval:
           2.5%         97.5%
1 -0.0006363717 -0.0004370954

------
Correlation matrix:
  k
k 1

The queer thing is that the RSS for HF53nl and HF43nl is almost the same, which points to the purported validity of HF53nl.  How is this possible? Can I go with the above estimates for the coefficient m of HF6 being equal to 2.147 * 10^(-7)? How do I make an nls call so that there is a better fit to HF1 and HF6.

NB: If you can't access the attached graphs, how do I send it to you otherwise? I can also give you HF1,HF6,HF5 if needed....

very many thanks for your time and effort....
yours sincerely,

AKSHAY M KULKARNI

-------------- next part --------------
A non-text attachment was scrubbed...
Name: HF53nl.png
Type: image/png
Size: 11024 bytes
Desc: HF53nl.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190321/e0d26bbf/attachment.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: HF43nl.png
Type: image/png
Size: 11402 bytes
Desc: HF43nl.png
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190321/e0d26bbf/attachment-0001.png>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ATT00001.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190321/e0d26bbf/attachment.txt>

From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Mar 21 13:34:35 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 21 Mar 2019 12:34:35 +0000
Subject: [R] Fw: problem with nlsLM.....
In-Reply-To: <fe19b5fd-2021-42af-7f97-ffdca44bf281@gmail.com>
References: <SL2P216MB0091F20CC45BDE987CCD8531C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <SL2P216MB0091E7E47082786F87FDA0D6C8410@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <fe19b5fd-2021-42af-7f97-ffdca44bf281@gmail.com>
Message-ID: <SL2P216MB0091696EBEF5726AA5DAA2C3C8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear JN,
                nlsr seems to impress me....But I have posted a different but related problem I am facing with nls(please check the mail list of today(21/03/2019 - 5:45 Indian standard time)). Can nlsr help me in this case too?

very many thanks for your time and effort...
yours sincerely,
AKSHAY M KULKARNI

________________________________
From: J C Nash <profjcnash at gmail.com>
Sent: Wednesday, March 20, 2019 6:15 PM
To: akshay kulkarni; R help Mailing list
Subject: Re: [R] Fw: problem with nlsLM.....

Of course, you might just try a more powerful approach. Duncan responded to the obvious issue earlier,
but the second problem seems to need the analytic derivatives of the nlsr package. Note that
nlsLM uses the SAME very simple forward difference derivative approximation for the Jacobian.
Optimization folk will generally say "singular Jacobian" and the singular values reported by
nlsr show how bad the case is here. Note that the nlsr output summary seems to imply that
the singular values are associated with specific parameters. My bad -- I wanted to keep
the summary tidy. The sv's apply to the problem in general, but there are as many of them as
parameters, so they fit on the page nicely where I've put them. The singularity leads to no
standard errors, of course.

JN

>> library(nlsr)
>> nonlin_modDH5 <- nlxb(formulaDH5, start = list(a = 0.43143, b = 0.68173,A = 0.09,w = 0.8,c = 0.01,C = 0.94))
> vn:[1] "HM1" "a"   "b"   "HM2" "A"   "w"   "HM3" "c"   "C"
> no weights
>> nonlin_modDH5
> nlsr object: x
> residual sumsquares =  0.66987  on  680 observations
>     after  15    Jacobian and  20 function evaluations
>   name            coeff          SE       tstat      pval      gradient    JSingval
> a              0.0929101            NA         NA         NA  -5.817e-07       42.97
> b               0.541246            NA         NA         NA  -3.359e-07       7.043
> A             0.00921275            NA         NA         NA    9.42e-08       0.984
> w               0.545654            NA         NA         NA  -8.693e-09       0.195
> c              -0.770181            NA         NA         NA    -5.2e-09     0.07464
> C                0.60148            NA         NA         NA  -5.817e-07   7.567e-14
>>



On 2019-03-20 4:20 a.m., akshay kulkarni wrote:
> dear members,
>                              I think Duncan's  solution is not working...I've checked the function again:
>
>> formulaDH5 <- as.formula(HM1 ~ (a + (b * ((HM2 + 0.3)^(1/2)))) + (A*sin(w*HM3 + c) + C))
>> nonlin_modDH5 <- nls(formulaDH5, start = list(a = 0.43143, b = 0.68173,A = 0.09,w = 0.8,c = 0.01,C = 0.94))
> Error in nlsModel(formula, mf, start, wts) :
>   singular gradient matrix at initial parameter estimates
>> nonlin_modDH5 <- nlsLM(formulaDH5, start = list(a = 0.43143, b = 0.68173,A = 0.09,w = 0.8,c = 0.01,C = 0.94))
> Error in nlsModel(formula, mf, start, wts) :
>   singular gradient matrix at initial parameter estimates
>
>
> I've checked the Internet  for a method of getting the starting values, but they are not comprehensive....any resources for how to find the starting values?
>
>> dput(HM1)
> c(1.01484280343579, 0.977004104656811, 1.02448653468064, 1.01562102015533,
> 0.968359321714289, 0.99105072887847, 0.99384582083167, 1.00579414727333,
> 0.967593120445675, 1.04694148058855, 1.03832347454026, 0.971040156128456,
> 0.976967848281943, 0.990386686964446, 1.02257020722784, 0.994529161072552,
> 1.05361176882747, 0.988021514764882, 0.998909692369571, 1.04933819895509,
> 0.988354009297047, 0.878584020586706, 1.01438895222572, 1.01522203155842,
> 0.999979601913003, 0.975862934766315, 1.03948445565434, 1.02493339899053,
> 0.972267168470219, 0.971001051421653, 1.03066167719129, 0.967898970631469,
> 0.976606662010519, 0.990061873979734, 1.03398666689317, 1.05491455733517,
> 0.995997711923663, 0.991213393857838, 0.9903216916016, 1.06850794640807,
> 1.01345738121581, 0.877847194198055, 0.992124544661245, 1.00927272338804,
> 0.988196514422814, 0.987504905313256, 0.990274516132497, 1.01484624802751,
> 0.9800176904083, 0.969321725342799, 1.07407427451262, 1.02023130729371,
> 0.975827723120998, 0.988260886231393, 1.05116598585815, 0.977996171738906,
> 0.985118207891832, 0.98243346941999, 0.991380612684958, 1.00960741365585,
> 1.03062446605116, 1.04176358759364, 1.02494798972349, 1.04645592406625,
> 0.999998705820515, 0.972161829098397, 0.991313521356105, 1.0211561076261,
> 0.971257832217016, 1.00935026461508, 0.997140402835885, 1.00719609467891,
> 0.97483865743955, 0.987491380127905, 1.0643888147255, 1.00804564512905,
> 0.989366232219398, 0.942042386899482, 1.0140328534339, 0.98812919092058,
> 1.04584882138055, 0.970586857498767, 1.0098232311784, 1.02237732533005,
> 0.99306513943649, 0.963808401790738, 1.03577231048959, 1.00540101137939,
> 0.970458918298484, 0.973460294926879, 0.992676651187501, 1.04079931972326,
> 0.988928168736541, 0.986376934855979, 1.04117941173535, 1.01252015337269,
> 0.920578394310677, 0.98933093765412, 0.98116698970429, 0.995498900493761,
> 0.96463760451942, 1.1059686605887, 1.03366353582116, 0.981256131441973,
> 1.0152599452654, 0.995362080418312, 0.997276311537458, 0.943438355295013,
> 1.04025117202718, 1.03102737097366, 1.02096902615856, 1.02506560388494,
> 0.959364103040633, 0.987217067188943, 1.00454456275356, 1.03517749036747,
> 1.0148342900634, 0.974518005568706, 1.02515903830848, 1.00435361604303,
> 0.973400954462685, 0.981702992961201, 1.0155969247203, 1.01064154947471,
> 0.957131570492054, 0.88504625471737, 1.01849907662046, 1.01503806358198,
> 0.987004149653273, 0.974485857983481, 0.987572979379099, 1.0037529762248,
> 0.976026658183349, 0.983176178747953, 1.03812943411666, 1.01828203593425,
> 0.984139742592733, 1.00377243020886, 1.02118652563454, 1.01115776483144,
> 0.98387406692053, 0.985919715423209, 0.989550590454508, 1.01845373071313,
> 1.00394660346757, 0.949076679285431, 1.06094051230739, 0.981940494629297,
> 0.982812578115724, 0.929673163266673, 1.05359335975355, 1.03498999963467,
> 0.989218608625026, 0.994412680504384, 1.03162650392014, 1.04359975620911,
> 0.96938669600183, 0.987150178669349, 0.990503131271205, 1.01120231626779,
> 0.977334529946535, 0.966013274319284, 1.04966722359666, 1.04745672849702,
> 0.960799643211444, 0.866626315091455, 1.01316731305781, 1.02919067036911,
> 0.988567285725018, 0.978702298855537, 1.0186604054056, 1.01398864226215,
> 0.985492431198792, 1.03982326960775, 1.02287049736119, 1.02357106122821,
> 0.977519287403578, 0.993818572820706, 1.01139405704906, 1.02683235508704,
> 0.943370973180458, 0.970153991236375, 0.970746163945049, 1.00996886938921,
> 0.970599365922215, 1.03230385881209, 1.02072718512322, 1.0161024503161,
> 0.987718207880278, 0.987512809150475, 1.04108232940019, 1.0153758143276,
> 1.03916475100072, 0.959776942350879, 1.01739288375256, 1.05438479662213,
> 0.970479916939806, 0.979826128477137, 0.982800613999074, 0.977050857426291,
> 0.977474025608337, 0.972905352371901, 1.02191626900472, 1.02737043327345,
> 0.987565251408109, 0.93131925605196, 0.991469304659888, 0.949708678221412,
> 0.948662234856045, 0.938158485016718, 1.02342896552764, 1.01869899424866,
> 1.01244484998263, 0.990637832491416, 1.04591108144934, 0.951244278685872,
> 0.977265525807748, 0.942206977147219, 1.09440128638859, 1.02935716108991,
> 0.978428723706514, 1.08589930790608, 1.0015813231405, 1.07512822987239,
> 0.976356469419301, 0.972775004620642, 1.00954673593815, 0.992551447194369,
> 0.985819189451289, 0.975310750212573, 1.05999783258221, 1.07815688799274,
> 0.959648862885262, 1.03418767541976, 1.07207529899183, 1.04949264142055,
> 0.966872387358102, 0.963564695272614, 1.01310836328613, 0.996622968927382,
> 1.00202486063801, 0.948215304098709, 1.02845448262427, 0.987124642586768,
> 0.999269328077687, 0.964674178364545, 0.991913103743918, 0.970556592778505,
> 0.979495816235886, 0.949160643807912, 0.999611005677085, 1.01697727168167,
> 0.966362681248743, 0.938703012004999, 1.01394761650376, 1.011369485146,
> 0.975929406359934, 0.97111480362557, 1.0581474202687, 1.03965938660391,
> 0.990447147628825, 0.967286377976687, 0.968162001740811, 1.00573452177125,
> 0.974628505381728, 0.983183640545729, 1.08185959707149, 0.976533333333333,
> 0.932582771965921, 1.02205682784386, 1.02162002635618, 1.00634335266967,
> 1.03045302554561, 1.05015537832235, 1.0238800474742, 1.0151188247761,
> 0.951176244898346, 0.937999477475998, 0.986664690881963, 1.00889938014614,
> 0.972546365957459, 0.937560562249233, 0.953227957137494, 0.964284336810156,
> 0.977909815090698, 0.988068102557503, 1.04429493287078, 1.01803069781624,
> 0.957632272450841, 0.971991500790857, 0.98209395086599, 1.02815448263624,
> 0.947227198465786, 0.941538843776026, 1.05745885548346, 1.01671992454154,
> 0.983992269183969, 0.951511089781983, 1.0110983382201, 1.00912039359684,
> 1.14741321154181, 0.985344569298617, 0.986725720078838, 1.03117619992574,
> 1.21391930217405, 0.997217690041687, 1.07387278335981, 1.02108776576674,
> 1.10313006343474, 1.10905015571117, 1.03999508931616, 1.03976033898847,
> 1.02524837903366, 0.984818521675578, 1.04179889608739, 0.975154632188378,
> 0.969607959579886, 1.02781726918172, 0.999963290601752, 1.01515310515696,
> 1.00520167771997, 0.984550628692601, 1.01902810624565, 1.00685219196689,
> 0.982173452231062, 0.999629285915004, 0.970558368481076, 0.995716235126211,
> 1.01314341676203, 1.02075675497478, 1.01128668171558, 1.05844449406008,
> 1.02510935720045, 0.983720171349822, 0.844396082698583, 1.05810846093782,
> 0.99048977314373, 0.971262209252963, 0.968530578900656, 1.04638759101296,
> 1.1799017507928, 0.898448146333193, 1.01160891745375, 1.01132611427775,
> 0.973840647644128, 1.04683923857471, 0.990344793003927, 1.0068683076688,
> 0.992798241837764, 0.99368375299494, 1.01200003722717, 1.0294016781563,
> 1.01547984207734, 1.04807175686468, 0.981737356425208, 1.06169292627851,
> 1.00518291863648, 0.976808854545251, 0.999321592083517, 0.97527078961014,
> 0.904376968942001, 0.987807226191329, 1.0350293488462, 1.02802944623052,
> 0.963692701708712, 0.998027399233401, 1.01762747994825, 1.0361812777154,
> 0.988298193013531, 1.04174794125517, 0.970839140069407, 1.0011387710754,
> 0.961691863428911, 1.02272120871699, 1.00519180537531, 0.999331326197155,
> 0.988055521308447, 1.00078630058504, 1.03915754537212, 0.971719181925134,
> 0.947846525645955, 0.955178087494908, 1.0019107391621, 1.01160524115586,
> 0.984958748948305, 0.967579294820259, 1.00552471207069, 1.04518978636097,
> 1.01810784117456, 0.993552343701494, 1.03535983577691, 0.982973964981781,
> 0.976175417305875, 0.971173421374951, 1.02622192154044, 1.06169292627851,
> 0.980316131595686, 0.98802873807095, 1.02522486330609, 1.01920704714191,
> 0.987730950969463, 0.972320506142784, 1.02950031603837, 1.01481027236895,
> 1.05066348592531, 0.941412086703545, 1.00426249434292, 0.997599618540051,
> 0.9727707483253, 0.987822018689403, 1.00496006171741, 1.00365488022554,
> 0.982021737672154, 0.946014836456916, 0.977232083157459, 1.04246004468132,
> 0.981228463109654, 0.971195860055527, 0.957569129533581, 0.987312165664453,
> 0.955583610052133, 0.991173211015633, 0.988434021882053, 0.94971029297998,
> 1.02260060019826, 0.986729432665294, 1.00044978044626, 1.00283798105021,
> 0.97876379280085, 0.948623031823408, 1.02317511219612, 1.02274213461243,
> 0.952138762776323, 0.974014522860041, 1.02874293788325, 1.02108205408977,
> 0.950582622331241, 0.986098109021331, 1.00098916530354, 1.05435029523389,
> 0.999921440746985, 0.964778188342829, 1.02087753056688, 1.0295810738595,
> 0.981181990342875, 0.94981916198993, 1.00828646284387, 1.01127872412662,
> 0.976192285243012, 0.95032633282931, 1.02884118341175, 1.01603776783957,
> 0.952947066257398, 0.988924105443658, 1.02727453593468, 1.02944328284254,
> 0.970561666318556, 0.982859453938812, 0.995886713501412, 1.04330648532137,
> 0.991844865771659, 0.99159250467833, 1.0332159700053, 0.993864541664301,
> 0.952716253755034, 0.960889912140351, 0.945669021854222, 0.985578531230794,
> 0.975053113912155, 0.999552576905685, 1.02526821151208, 1.00450298718514,
> 0.948193972002236, 0.99358065264941, 1.04855364680554, 0.982181523974446,
> 0.957570494265717, 0.989249667650167, 1.01928030167592, 0.977392376116208,
> 0.98989833978368, 0.950312758756191, 1.01179069272545, 1.03630963397181,
> 0.997439452068598, 0.981793158114732, 0.98560918506727, 1.0497573745934,
> 1.06729226949522, 0.987962794653476, 1.05655507163022, 1.01896087310511,
> 0.985627836611195, 1.03148215915346, 1.07268555224398, 1.00116324087534,
> 1.0353274520202, 0.965843323254256, 0.981731445105197, 1.01927876056569,
> 0.999117162412819, 0.984262994383997, 1.0058994935294, 1.03100765642408,
> 0.970000664632288, 0.974667000854514, 1.02683721041632, 1.00224180239507,
> 0.976488446884133, 0.989129579180466, 1.01111944121172, 1.01081759149299,
> 0.990822520380867, 0.951849309560414, 1.0533258785109, 1.02605855087945,
> 0.993203883495146, 1.01231349444546, 1.05040134097269, 0.988195090931879,
> 0.998324259112233, 0.955268105597824, 1.01179535756103, 1.00923640076821,
> 1.01018973127051, 0.97727388908476, 1.0163531272719, 1.03445181115965,
> 0.922709820364792, 1.05034425545701, 0.971929096725562, 1.01600521659268,
> 0.989307124797035, 0.966327502991904, 1.0148486097639, 1.00804453317389,
> 0.986688556832374, 1.01100519377607, 1.07022911567444, 1.01751745478207,
> 1.00395256916996, 1.02666291685486, 1.04306886282915, 0.976284796774505,
> 0.873912939480404, 0.985956634379203, 0.966005821428092, 1.01911378235808,
> 0.989971040708023, 0.948518684869746, 0.964578663605958, 1.00130118816861,
> 1.14681217078448, 0.989198271209915, 0.975831372743337, 0.87134009583841,
> 0.999320371256267, 1.04366317607907, 1.0941742251269, 1.04912747347822,
> 0.873169695550468, 0.9946703855144, 1.02687904508206, 1.01557590029223,
> 0.979039928979583, 0.978199700239161, 0.988038704848986, 0.944583404359117,
> 0.97831517569251, 0.9881631354812, 0.990553852862955, 1.05139702349736,
> 0.987315661692795, 0.973936995140175, 1.06038641236987, 1.0651525599012,
> 0.997891074790299, 0.979657304627723, 0.976638433723023, 0.981157058049022,
> 0.974773098042751, 1.04814547433216, 1.02327857487465, 1.01725074945885,
> 0.978364366421386, 1.02726007295919, 1.02997589351422, 1.0208112362723,
> 0.965389721633011, 1.01152741033802, 1.00352090319123, 0.981151102088714,
> 0.986010324656192, 0.97990659165448, 0.990503768576242, 0.985301104774446,
> 0.976384165205197, 0.983887706958686, 0.992885571845564, 0.985757622165226,
> 0.946874017209482, 0.990962963812142, 1.04319324396693, 1.07354098057006,
> 0.984597785175012, 0.97845443949194, 0.963960215777945, 1.06584156194012,
> 1.15623372835712, 0.94928101059165, 1.05512096334013, 1.00899261027951,
> 1.00760894032175, 0.989021593753401, 1.01442722705518, 1.04495346859144,
> 1.00095646335122, 0.958907195434402, 1.02045280866442, 1.00659622664478,
> 0.980590749755976, 1.01197688939839, 0.998468538022796, 1.040625279257,
> 0.934164344256759, 0.992160161968429, 0.998594942355232, 1.0532781805963,
> 0.974144103277039, 0.946253316413415, 1.00623102828924, 0.988787044999963,
> 0.987541467770869, 0.999140303075891, 1.05429436081972, 1.01617267860198,
> 1.17044068684611, 1.05663112916056, 1.01681282884482, 1.02872692218644,
> 1.00738468152863, 1.0222081131894, 1.01817103171172, 1.01222250929532,
> 0.97386289237006, 0.987701353725335, 1.00418624697519, 1.0183108729199,
> 0.956852868616883, 1.05440980534023, 1.01562402108019, 1.00083274000183,
> 0.945641654423597, 0.966367379950251, 1.02020174816679, 1.01151664724635,
> 0.940072052642951, 0.991027873457432, 0.984568985852697, 1.02373049431651,
> 0.985887377562884, 1.05498951399906, 0.98672427011774, 1.02570399134917,
> 1.06987898201568, 1.02648218241568, 0.971175525661877, 1.03211886190237,
> 0.992647697074835, 0.986388969656675, 1.02097305537141, 1.04518978636097,
> 0.987300416455667, 0.985182286987375, 1.02373433952904, 1.01376295257605,
> 0.992398195656577, 0.983765510339449, 1.06154013507033, 1.01312319643703
> )
>
>
>> dput(HM3)
> c(0.196622289085315, -0.0436031046458981, 0.000820037203846464,
> 0.392758326108918, 0.0445839547736504, 0.223969403187679, -0.070152338130932,
> 0.197471312884591, 0.0498910549108242, -0.620322188497616, 0.488037411730214,
> 0.0461937792271113, 0.0051066586622384, 0.413877974790452, -0.154637441642314,
> 0.229802002983503, 0.509904610124243, 0.126452664534026, 0.583037470359474,
> 0.470621010682895, -0.00578078533090776, -0.457074922218691,
> -0.0506356375273661, -0.767302691971105, -0.000822439375515026,
> -0.170080770417037, -0.0638533091192526, 0.218492017287635, -0.088210288825341,
> -0.389219867379597, 1.23355646106213, -1.78773586807154, 0.0117401974737664,
> -0.154882548871051, -2.11547741687211, -0.0638533091192526, -8.18190789706224,
> 0.282699360997887, 0.0383693451361519, -0.0638533091192526, 0.832793868600698,
> 0.894379977535782, -0.302329357989944, 0.573489523345283, -0.223540700260161,
> 0.229802002983503, 0.0239731624923989, 0.556201388601996, -0.112552578411774,
> -0.0438764575701239, 0.470621010682895, -4.92500180132548, 0.00309129784132211,
> 0.172407876390051, 0.229802002983503, 0.061799194890529, -0.000864150216291216,
> -0.0683834224257835, -1.42874041532869, 0.654009227783557, -0.620322188497616,
> 0.272034153890376, -0.00558557185457871, 0.229802002983503, -1.53118563423569e-05,
> 0.470621010682895, -0.736608540541868, -1.47747441666374, -0.00291475050109709,
> -1.56966101463791, 0.362334298169537, -1.41867286052432, -0.0062864890624406,
> -0.223090980170635, 0.229802002983503, 0, 0.0147973092972513,
> 0.757637727251206, 0.332227874554666, -0.620322188497616, -0.0638533091192526,
> -0.0638533091192526, 1.59477829009842, -0.247635059505396, -0.0638533091192526,
> 0.384123364351151, -0.909542286753924, -1.61643290186366, -0.00969493151693274,
> -0.0638533091192526, 0.335128597413319, 0.229802002983503, -3.36005853184083,
> -1.77772450660144, -0.223359097040026, -4.75082975711953, 0.545441153017038,
> -0.0181047188208818, 0.229802002983503, 0.718505786757723, -0.199075726922584,
> 0.0779145211225284, 0.0981591683724576, -0.0638533091192526,
> 0.771585236053205, -0.124873344480422, -0.143455357050914, -0.982192613814533,
> 0.840325570227771, -0.0639597669819456, 0.272309718636627, 0.470621010682895,
> 0.000149945344883234, 0.0179651303218593, 0.586470204231886,
> 1.12661379530659, 0.587538739887084, -0.389219867379597, 0.231816913894395,
> -2.32942853843292, 0.229802002983503, 0.470621010682895, 7.7137219916985,
> 3.14411367083572, 0.0371415305209129, 0.109258089720235, 0.0744990417275174,
> 0.978949955731298, 0.0046627432182197, 0.0778534035355464, -0.258075504911111,
> 0.428481177165365, 0.470621010682895, 0.0118295358531435, 0.601929630798962,
> 0.677233238299559, -0.620322188497616, 0.901650947775265, 0.229802002983503,
> 0.114385813284502, -0.287236365991277, 0.00362108510216535, 0.54836513813854,
> -0.360900820378516, 0.901102331983843, 0.303232126663717, -0.389219867379597,
> -0.389219867379597, -0.0638533091192526, 0.0172621246134547,
> -0.0638533091192526, 0.036790744537595, 0.0267711437261105, 0.229802002983503,
> 2.31402345437609, 0.229802002983503, -0.620322188497616, 0.470621010682895,
> -0.0638533091192526, 0, -0.0638533091192526, 0.683308812634821,
> 0.598730940691975, 0.229802002983503, 0.697360153425382, 0.00772365161682687,
> -4.74600966429007, 0.593589435095411, -0.152099577506429, 0.0064938252361837,
> -0.108940122157535, 0.0514396263381645, 1.10452888209261, 0.229802002983503,
> -0.0638533091192526, 0, 0.0362637715949993, -2.79167649513235,
> 0.0221979095188955, 0.122798837911884, -3.07802860403649, 0.887963749928486,
> -0.0638533091192526, -2.59080601794078, -0.0638533091192526,
> -0.0720032601992142, 0.188653367126112, -0.390900634446388, 0.00626262322890335,
> 0.10449726079375, 0.190234594305548, 0.229802002983503, 0.470621010682895,
> 0.441447072301397, 0.710575973379983, 0.424723018059659, 0.0242933124111495,
> -0.0638533091192526, 0.221839916416526, 0.470621010682895, -0.0108657434161119,
> 0.734169111179744, 0.465629851897761, 0.0252290916908343, -0.0261736586703599,
> 0.470621010682895, 0.00872967160561454, 0.820007851307147, -0.0638533091192526,
> 0.162601634884078, 0.15876421059534, 0.276805180633414, -0.386980840708979,
> 0.10228170638373, 0.0199458385530109, 0.470621010682895, 0.360613568422605,
> 0.0398196823378306, -0.389219867379597, -0.00269176292387149,
> 0.553663817786709, 0.00803266728255501, 0.470621010682895, -0.389219867379597,
> 0.0531985700905557, -0.00966390653254687, 0.948851286517476,
> 0, -0.0889303986002686, -0.289482205789089, 0.470621010682895,
> -0.620322188497616, -1.33981685878991, 0.0183827591576367, 0.011135182844728,
> -0.0638533091192526, 2.32607985155626, -0.138083520720171, 0.00471455153770216,
> 0.158058381916342, 0.00450504854498247, 0.0789824876681455, 0.0704699239866158,
> -2.19086590383429, 0.0809155162971262, -0.0638533091192526, 0.0329404929656065,
> 0.0281676116282727, -0.389219867379597, 0.012790919597435, -0.00704771101800125,
> 0.0403529797537856, 0.0558406758822688, 0.164832137315436, -0.165567865123191,
> -0.11103429903157, -0.0355024172803945, 9.48260587090123e-05,
> 0.470621010682895, -0.0495357136064147, 0.229802002983503, 0.0637860580674806,
> -0.620322188497616, -0.0226657756569641, -0.389219867379597,
> -0.316625748304681, 0.229802002983503, 0, 0.364124775102099,
> -0.0310987139252845, 0.492424057922772, -0.224535737231339, -0.133435578504748,
> 0.229802002983503, 0.184389526960601, -3.97960671764283, 0.223470481648489,
> -0.620322188497616, 1.57339312469411, -3.97007919079201, 0.028292556618311,
> -0.0208634007145496, 0.253055735744466, -0.0638533091192526,
> -1.93884051014153, 0.0301349400625612, -0.0638533091192526, 0.000295057056987376,
> 0, 0.286160612098488, 0.101207913446722, 0.374419900839882, -0.106217410937095,
> 0.229802002983503, 2.31067381143729, 2.09201022549168, 0.00385297198146129,
> 0.0512938203984638, -1.74049624153457, 0.426702921643898, -0.0638533091192526,
> 0.018935028523515, 0.23126745462646, 0.00767373530818363, 0.229802002983503,
> 3.84024898682852, -0.0638533091192526, 0.389584248100691, 0.0449003257302091,
> 1.02228506672615, 0.229802002983503, -0.0638533091192526, -0.0829134652872339,
> 3.50596545804498, -0.389219867379597, 0.9170368554337, -0.0638533091192526,
> 0.393117195150408, 0.228802985129773, 0.380899253549764, 0.232617830598094,
> -0.389219867379597, 0.0459161417216973, 1.31722638132631, -0.151301202089443,
> -0.116990991708309, 0.927131393095724, 0.287762794679405, 0.53782819173468,
> -0.389219867379597, 0, 0.778671412967675, -0.389219867379597,
> -0.0683403974328925, 0, 0.229802002983503, 0.473035927661744,
> -0.620322188497616, 0.994155024290692, 0.229802002983503, 0.470621010682895,
> -0.194201456791898, 0.735175033053973, 0, 8.3894395202509, -2.71421951365695,
> 1.63476338054133, 0, -0.358533993792374, -0.180900357181005,
> 0.464964929465083, 0.828110705634323, 0.290014059095831, 0.0265807736784206,
> 0.913325706925416, -0.620322188497616, 0.482569085367064, 0,
> 0.999426501553857, -0.0638533091192526, -0.894190901951266, 0.0608976332239153,
> -0.389219867379597, 0.000920175777920006, 0.0444241832790267,
> -0.389219867379597, 1.77775509259178, 0.00731201645980821, 0,
> -0.620322188497616, 0.229802002983503, 5.07464022484589, 1.48600912617683,
> 0.037030406316896, 5.19861898546793, 0.614580107047209, 0.140671508556198,
> -0.0638533091192526, 0.470621010682895, 1.44599607789379, 0.229802002983503,
> 0.0337669838054181, -0.339507807487096, -0.227604499761264, -0.0126140839785072,
> 0.329583322374931, -0.348480271857373, 0.229802002983503, 0.220851780815745,
> -0.00466286305694716, 0.470621010682895, 1.94004188027446, 0.00769634829635375,
> 0.229802002983503, 0.229802002983503, -0.0638533091192526, 0.00196933273638366,
> 0.362886861307995, -0.010895368171175, 0.00517992475169629, 0.0121875763733234,
> 0.229802002983503, -0.389219867379597, -0.000556322059116982,
> 0.229802002983503, 0.00260711445755056, -0.58609877956866, 0,
> 0.173845846564558, 0.050042474875399, -0.608356727049781, 1.29406201978428,
> -0.0207947874733201, 0.00589643228835565, 0.720668560955405,
> -0.0638533091192526, 0.0325689306223551, -0.620322188497616,
> -0.389219867379597, -0.0638533091192526, -0.620322188497616,
> 0.0384932081048732, -0.0349291605765668, 0.00798065241516178,
> -0.134723236994354, -0.0638533091192526, 0.281152548562933, 0.785134308633961,
> 0.00911501343278954, 0.0290625580516694, 0.229802002983503, 0.0364502381509222,
> 0.0200267754432485, 0.470621010682895, -2.30392713054924, 0.0133977174531081,
> 0.0313438687192373, -0.453629796591262, -0.444456614744397, 0.470621010682895,
> -0.00419309963021769, 0.229802002983503, 0.781666390558601, 0.263209713357162,
> -0.00288720581609739, 0.0157812058748433, 0.377266856994834,
> 0.211102246795362, -0.0244674117557579, 0.502934845080243, 0.470621010682895,
> -0.620322188497616, 0.119508324298399, 0.68156448564652, -0.389219867379597,
> 0.344388633359758, 0.541519544456095, 0.348203898915403, -0.617865773083779,
> -0.620322188497616, -0.389219867379597, -0.630469325697215, -0.389219867379597,
> -0.00185171176208817, 0.470621010682895, -0.0638533091192526,
> 0.0384110765023374, -0.476268477810135, 0.0275459003440332, -0.377613497242655,
> 0.480114279740138, 0.208177472628734, 0.0388621060580001, 0.314810488790625,
> -0.389219867379597, 0.622343433254446, 0.229802002983503, 0,
> 0.0116064532794852, 0.640880718848922, 0.0159256710348949, 0.0294218642201815,
> -0.11922554805783, 0.470621010682895, 0.0719964703043177, 0.176673405801693,
> -0.216160070123196, -0.620322188497616, 0.263287565296575, 0.334468720690456,
> -0.389219867379597, -0.389219867379597, 0, -0.0638533091192526,
> -0.0638533091192526, 0.328005934417359, 1.98345092754637, -0.0638533091192526,
> -0.322569885555602, 0.343583272797935, -0.0131035846556284, -0.0638533091192526,
> 0.332049938227108, 0.229802002983503, -0.184471568949188, 0.0738083137554319,
> 0.295094806935004, 2.00072915211246, 0.0524057350651284, -0.394994587233133,
> 0.11082820395266, 0.0470433275207449, -0.0244714107059427, -0.19392136026585,
> -0.389219867379597, 1.54447845601115, 0, 1.50461847711675, 0.470621010682895,
> -2.19230689436821, -2.15372326763204, 0.0602893383146939, -0.350076259362557,
> -0.389219867379597, -0.0491349223300808, -0.347427839125497,
> -0.125793370606986, -0.389219867379597, 0.218492017287635, -0.389219867379597,
> -2.19203559916863, 0.470621010682895, -0.0756276343860818, -0.620322188497616,
> 0.039618360679447, 0.229802002983503, 0.342229794086586, 1.22342842687211,
> -0.0638533091192526, -0.389219867379597, 0, -0.121360237566266,
> 0.470621010682895, 0.203522177713664, -2.66081323726908, 0.164981226545032,
> 0.208989371873146, 0.178977127763616, 0.00534834375159016, 0.0259183781780568,
> -0.0638533091192526, -0.620322188497616, 3.77342257919255, 0.077694414226668,
> -0.0638533091192526, -2.76001789665019, 0.93498899883197, 0.229802002983503,
> 0.229802002983503, -0.189889234701173, -0.878128024239223, 0.229802002983503,
> -0.971246200836784, 0.00238384747429768, -0.620322188497616,
> 0.440261892604354, 3.76354782684596, 0.470621010682895, 3.86172131986622,
> 0.575298756860043, 12.9023010859722, 0.470621010682895, 0.378503973417637,
> 0.474891582933229, 0, -0.389219867379597, 0.520087612011093,
> 0.00160600701629905, -1.70670322718237, 0.229802002983503, -0.0349116284595025,
> -0.389219867379597, 0.229802002983503, -0.0638533091192526, 0.0138641669555371,
> -1.76999545498462, 0.470621010682895, -0.0638533091192526, 0.470621010682895,
> 0.568835467760944, 0.196290116211879, 0.00657090150018764, 1.15108667563071,
> 0.229802002983503, 1.39616453144586, -0.0847185084207546, -0.063387618790934,
> 0.470621010682895, 0.111214507604595, -0.0859882232308096, 0.229802002983503,
> 0.00453833599472268, 0.334589717726727, 0.470621010682895, -0.390600326717378,
> 0.254343160688718, -0.951335732362531, -0.0638533091192526, -0.389219867379597,
> 0.185995171841162, -0.0638533091192526, 0.0161178324221534, -0.702070844948495,
> -0.466435025160672, 0.470621010682895, 0.470621010682895, -0.383876151221017,
> -0.0638533091192526, -0.0638533091192526, 0.470621010682895,
> -0.389219867379597, -0.0123155655016255, -0.386709341729674,
> 0.229802002983503, -0.620322188497616, 0.187872106293471, 1.0998198175547,
> 0.229802002983503, -0.0111456829865045, -0.620322188497616, -0.620322188497616,
> 0.0484181771965215, 0.29713373197861, 1.03416257683562, 0.470621010682895,
> -1.14286511426204, -0.620322188497616, -0.620322188497616, 3.57860778510592,
> 0.229802002983503, -0.62520946847561, 0.290707988341369, 0.0924839451700828,
> -0.292199030679553, 0.275232409230032, -0.0884019891746591, -0.613845322077624,
> 0.470621010682895, 0.27263770155515, 0.229802002983503, 0.470621010682895,
> -0.616577036861926, -0.0638533091192526, 0.229802002983503, 0.103227620279027,
> 0.000325827725882409, -0.389219867379597, -0.0213033086486025,
> 1.1517268134498, -0.0027549233829829, -5.20036700047062, -0.389219867379597,
> 1.38009140442044, 0.188112123387496, -0.389219867379597, -0.0715310034898503,
> 0.229802002983503, -1.42113754281828, 0.0671280483594289, -0.866540256758301,
> -0.389219867379597, -0.389219867379597, 0.112435457850701, -0.162158911168545,
> 0.470621010682895, 0.470621010682895, 0.0211915910476394, -0.00153721649859708,
> -0.0638533091192526, -0.102776844490052)
>
>> dput(HM2)
> c(0.016497438441392, -0.0121005433105642, 0.0340103138875006,
> 0.0393128837416979, 0.144718671661076, 0.0775923152157973, 0.000894372752462123,
> 0.0129080340603581, -0.0022179741964203, 0.0540058630486591,
> 0.0383234745402589, -0.0142841740056228, -0.0113383036972718,
> 0.00764362896437113, 0.022570207227843, 0.0892462240318431, 0.060498120257716,
> 0.0432525311181986, 0.00222557516581898, 0.0493381989550945,
> -0.0116459907029527, -0.112480015005373, 0.0182691285457092,
> 0.0248597234825647, 0.00754701511666922, 0.0471476327719148,
> 0.0625841102244406, 0.0303698631973417, 0.00781124380799404,
> -0.0168950573556397, 0.0473901805181147, 0.00740505106540693,
> -0.0152639817574294, 0.00916841478852549, 0.0409730632910991,
> 0.0625866632066982, 0.00664005076885597, 0.0103946833001679,
> 0.0243728672440622, 0.0722309706115829, 0.0174699387206196, -0.088714507487996,
> -0.00504535647738195, 0.00927272338803572, -0.00707755664004029,
> 0.209001332673238, 0.0280975011236682, 0.0185107628508926, -0.0152895488479284,
> 0.0293166958720994, 0.0839281669393395, 0.0350355281775659, -0.0214691529922399,
> -0.00698847060167527, 0.069853381162298, 0.00880702674287574,
> 0.0440111442331915, 0.0576762465158807, 0.0180025302908308, 0.0104372279684426,
> 0.0399937793788931, 0.0484415593089806, 0.0359627022827372, 0.0610406756211853,
> 0.0400425377047152, 0.00783749255154968, 0.0033626359803225,
> 0.0253846624900874, 0.028956317299215, 0.0220327419434368, 0.0055491218235435,
> 0.021359789760335, 0.0053563472471669, 0.00770764128276149, 0.120606533813112,
> 0.0338481150043749, 0.00480652155775551, -0.0208260153687132,
> 0.0264346940874102, 0.0254170849175835, 0.0549828722223003, -0.0197153554154837,
> 0.0146319132316291, 0.0223773253300461, 0.046744336162787, 0.0439389840326429,
> 0.0730303072697907, 0.0157399337823011, 0.0189220332073482, 0.0332902036920697,
> -0.00629466834494764, 0.0674012192688983, 0.0200657055355507,
> 0.0101015050127517, 0.0425927774526406, 0.0282767872201141, 0.00648039037553307,
> 0.0515174537352363, 0.128342038159934, 0.0432170296083376, 0.01871577325763,
> 0.122157724187454, 0.0336635358211635, 0.0225239126708406, 0.0183439669703353,
> 0.134384074630845, 0.00984528984402488, -0.049573212443543, 0.0459003250172428,
> 0.0354638054442889, 0.0209690261585554, 0.0252976241664904, -0.0211582204487595,
> 0.00564472281613363, 0.0148258737510351, 0.0428454717776024,
> 0.0315492077820927, 0.0628515089974484, 0.034599813272583, 0.00983450218873532,
> 0.0790661896510682, 0.076097511515163, 0.015596924720301, 0.010641549474715,
> 0.00626447132858588, -0.0752885625254798, 0.0263105739632706,
> 0.0336726716607009, 0.0873459584037158, 0.0137508559242068, -0.00705208890843943,
> 0.0673463675870221, -0.000264192225152525, 0.0181803111819752,
> 0.0415821927667122, 0.0258248658300624, 0.0697171115138401, 0.0609029360829605,
> 0.0252388531172213, 0.0120346601757736, 0.0231724225476939, 0.00664147935356364,
> 0.00364229190085414, 0.0221357760157817, 0.00873744091784354,
> -0.0243757212939979, 0.0609405123073909, -0.0110205054092022,
> 0.0570746855530399, -0.0541885933947211, 0.0790835216830754,
> 0.0443624834606028, 0.010803474486455, 0.012658784733822, 0.0407444149396382,
> 0.0758344977136341, 0.0249648665726016, 0.0706224364244773, 0.0617623493482699,
> 0.0157667711676134, 0.159594212558186, 0.10429461374287, 0.0575594583605458,
> 0.0629746059562333, 0.187704355999386, -0.121475803700469, 0.0316944060343089,
> 0.0446672218032357, 0.0144595666502411, 0.0581713078328564, 0.0240501430003371,
> 0.0169003110123663, -0.00441905663049872, 0.0398232696077516,
> 0.0488599473848877, 0.0333193570494318, -0.0165383461076467,
> 0.0908431754234104, 0.0145316828198416, 0.0268323550870429, -0.00807316790584186,
> 0.0643886942067867, 0.0132605944827886, 0.0225125532638365, -0.029400634077785,
> 0.0470539042942522, 0.0266903807836804, 0.0216184350463828, -0.00196619725935374,
> -0.00896821424676157, 0.0471292741982513, 0.0153758143276018,
> 0.0391647510007153, 0.0638918758798112, 0.0350540963584708, 0.0621376260090577,
> -0.0235965026923011, 0.0722625556919611, -0.00136537610871872,
> 0.00876826451613888, 0.0314000163087439, 0.0327764509794023,
> 0.0416905332713813, 0.0287500712769455, 0.0363887020395213, -0.00500079481628151,
> 0.0284155664844784, -0.0252462616989099, 0.0448399729520494,
> 0.000185492290551008, 0.0334348196455053, 0.0875709173367935,
> 0.0351964196451633, 0.232571060368655, 0.1090264053299, 0.0617423514625135,
> 0.0768976100544436, -0.0546347871751927, 0.0996882491247678,
> 0.0381052389461968, 0.0652248201643501, 0.0858993079060812, 0.0666492513259647,
> 0.0751282298723881, -0.0192055466287929, 0.0869162270419438,
> 0.0162427091867147, 0.0351089781252477, 0.0226340139536196, 0.0652987163108841,
> 0.0779638975412273, 0.117722278377793, -0.0394478774616255, 0.0438529807975176,
> 0.0881967320593799, 0.0588214648998437, -0.0184036676567495,
> 0.0140741349441621, 0.0161750696830995, 0.0118833913020066, 0.00937605882553054,
> -0.0455051243509685, 0.0418377741899513, -0.0112867686098609,
> 0.0101123419174794, -0.00470124454451681, 0.000956333395138531,
> -0.013981828106854, 0.05549118128867, -0.0224186715153298, 0.0186897582435067,
> 0.0169772716816739, -0.0179058332425567, -0.00353064879469387,
> 0.0269073443173487, 0.011369485145996, 0.0753871802564881, 0.184940758413157,
> 0.0962788588369411, 0.12233382584365, 0.0542611920398092, 0.0470414915942155,
> 0.0303505098818263, 0.00573452177124659, 0.00764243118994532,
> 0.0586726631015619, 0.106336058543696, -0.0222151019972231, 0.00630121965465554,
> 0.0624011763113852, 0.0372172786669568, 0.0226016288240546, 0.0939930244398409,
> 0.0767415904317717, 0.0238800474741997, 0.0151188247761029, 0.134843980893931,
> -0.0023819116674762, 0.00287890065299971, 0.0303739452657181,
> -0.00394798169311598, -0.00886454847938266, 0.0530899335995167,
> -0.00526457886952308, 0.100251697020821, 0.0651999010773084,
> 0.0618461082131486, 0.0746686396348302, 0.0225108225108226, -0.0228981338001482,
> -0.00958321904192527, 0.0301744325038945, 0.0510329188455982,
> -0.0184730200018259, 0.0702506964772114, 0.0167199245415362,
> 0.0790156649374397, -0.028524529551306, 0.0133581966776628, 0.025585253422798,
> 0.172427142313816, 0.0675994203009362, 0.00336311873127013, 0.0493948607018135,
> 0.218437612231768, 0.0214594460427004, 0.0907841657749202, 0.0333163617639509,
> 0.177206388698409, 0.126401976128862, 0.0399950893161636, 0.0501927169716304,
> 0.0893264027232592, 0.0157983828191964, 0.0615798877852506, -0.0197129750106304,
> -0.0108884320377597, 0.0480698262099342, 0.00512957625928131,
> 0.0157721009527848, 0.00576798852432315, 0.132233222996491, 0.0427798891731844,
> 0.0213946620292803, 0.137026176169503, 0.106996875883653, -0.0054111837253851,
> 0.0251880361118362, 0.0253902053162975, 0.0270577225980805, 0.0124293785310734,
> 0.0743211614709804, 0.0322779541039503, -0.000515082314443601,
> -0.15098468271335, 0.0705935165241118, 0.00396949242984507, 0.00570413156689748,
> -0.028337357038222, 0.067618643584239, 0.215558122382694, -0.0902178675044036,
> 0.0116089174537522, 0.0163203173112244, 0.053199024052846, 0.190954107085158,
> -0.00965520699607334, 0.0137508862783826, 0.0551559553375383,
> 0.0522566319201845, 0.0120000372271673, 0.0661660238047397, 0.0393034160850182,
> 0.0519061657312619, -0.0104570268884604, 0.0691173523364065,
> 0.0161805435668569, 0.017889600764444, 0.00112726390517828, 0.0127812045951454,
> -0.0689255559600801, 0.0231502379228566, 0.0350293488462002,
> 0.0474816278264511, -0.00418170038125466, 0.0353685604292087,
> 0.0176274799482486, 0.0430171958669913, 0.0875537735181209, 0.070485815496694,
> 0.0135133879845454, 0.0128708660489427, 0.051611788565405, 0.0227212087169909,
> 0.0073359858475243, 0.00404515320751896, -0.00440846200360685,
> 0.0659739938613625, 0.0460166710841459, 0.0164323471803848, -0.00986110969267727,
> -0.0393527714903421, 0.0187532041738799, 0.0120528540944261,
> 0.048649745777751, -0.016576662520347, 0.00722036925799145, 0.0532607499622092,
> 0.0287269581620362, 0.098387602988942, 0.0515373332109274, -0.00594807446048738,
> -0.012996698372473, 0.186569861135154, 0.030123906032987, 0.0853131761769354,
> 0.0031929989708665, 0.0555338071316984, 0.0330892102352149, 0.0325300150784036,
> -0.00129426068643155, 0.0964465282035648, 0.0748083984068019,
> 0.014810272368951, 0.0727362482346641, -0.022994161418619, 0.0176067452321714,
> 0.0316353702314177, 0.0366753960255026, 0.132973503081211, 0.00537653915699314,
> 0.0293447867014306, 0.00587487992108688, -0.0465459905172109,
> -0.0208313288728689, 0.0461567115064294, -0.00992439500984909,
> 0.0478692174283319, 0.15870237135342, 0.0555146507925898, 0.0929129911374694,
> 0.136269426745194, 0.032515127489143, -0.0177415950997095, 0.0242936475495824,
> 0.0128679606829178, 0.00550255711518526, 0.00950135633958283,
> -0.0171614203848253, -0.0480106149706695, 0.0436386144400421,
> 0.030020737993979, -0.0207207590715261, 0.118800465447344, 0.0287429378832495,
> 0.0277036046980229, -0.0326376675279461, 0.0111603851297697,
> 0.0168167717634637, 0.0543502952338948, 0.0268780423509474, 0.0244551896836229,
> 0.0330102007094411, 0.049560008022367, 0.0830113020712642, -0.0360281667532304,
> 0.0254731639150768, 0.019583911716062, 0.0311623362566958, 0.00306244869413074,
> 0.0687257755440167, 0.0205871906806436, -0.00980792960340933,
> 0.0430516082564022, 0.0402780110730929, 0.0705036986824154, 0.025854270339128,
> 0.108621667399996, 0.00718685874587846, 0.0571556864539545, 0.109159849895189,
> -0.00840749532166959, 0.0463361727990188, 0.0651635196532618,
> 0.107210781390985, 0.134765229575272, 0.00173368529272275, -0.0123735133796356,
> -0.0136090591818897, 0.0534703507636681, 0.0252682115120829,
> 0.0045029871851435, -0.032195767437093, 0.0467488481089099, 0.0538227606085845,
> -0.0118386798369731, -0.0280299494295351, 0.0140538628567341,
> 0.0305966571267272, -0.00988880417457172, -0.00202667375935174,
> -0.0227460178374512, 0.0117906927254463, 0.0508373391209442,
> 0.20565913566541, 0.079177499709108, 0.0422151179934303, 0.084419174697904,
> 0.0672922694952164, -0.00562397110021092, 0.0565550716302232,
> 0.0189608731051132, 0.0453269153630163, 0.0409743262622376, 0.0726855522439788,
> 0.0185422373323943, 0.0700699839671905, -0.00222797184477677,
> -0.00250681151008622, 0.0255048645524291, 0.000885511372841601,
> 0.0788210113117177, 0.0116542476851704, 0.031007656424076, 0.011572121687957,
> 0.00197091513991245, 0.0332881463280234, 0.0115574465474555,
> -0.0156436963370336, 0.224908551812914, 0.0160024082692785, 0.0108175914929918,
> 0.0850714918317297, -0.0209549958807167, 0.0649434433474199,
> 0.0572507308261867, 0.0108695652173913, 0.0149885395421411, 0.0504013409726928,
> 0.000985727295810551, 0.00206155161241882, -0.0273818619348245,
> 0.0117953575610268, 0.0160740457598691, 0.0439509870338097, 0.000374131432729126,
> 0.030965232935552, 0.0680925204656592, -0.040084138491466, 0.0579554457139488,
> -0.0280709032744379, 0.0245619981154038, 0.0332658264722017,
> 0.0307493365246973, 0.0663529386077719, 0.0108866234143253, 0.0504739490524921,
> 0.0110051937760747, 0.114486710683534, 0.0885070446505914, 0.00713719270420306,
> 0.027840282585194, 0.0502624411934901, -0.0237152032254946, -0.0783599699259493,
> 0.00584404515799187, -0.031333005510123, 0.0191137823580835,
> 0.00399073109721866, -0.00489024968946272, -0.00926327094032102,
> 0.00380235683181465, 0.157304505375375, 0.0200440859609643, -0.0241686272566629,
> -0.0800316610669412, 0.0239679002814968, 0.0538951680014091,
> 0.0941742251268998, 0.0521115186872178, -0.123540922378836, 0.0542314864434063,
> 0.0343854708502029, 0.0238689195785379, 0.123488443091325, 0.00983601617163751,
> -0.00112278368930967, -0.0210112398529677, -0.0179204649311611,
> -0.00187219424504028, -0.0094461471370453, 0.0616047615895669,
> 0.0582469526595368, 0.00304799229604126, 0.0674836370082559,
> 0.0853131761769354, 0.0358226068729712, 0.00994622612345991,
> -0.0217167015464964, 0.0526997601984295, -0.0232294161006039,
> 0.100227237031891, 0.0294429036389532, 0.0172507494588479, 0.0034823068051,
> 0.0322430509250401, 0.0401401293054765, 0.0208112362723038, 0.00561429336771978,
> 0.037707481438362, 0.00862436025594276, 0.00476662316192879,
> 0.15812712691603, 0.0472541237269534, -0.00949623142375768, 0.00855607871651576,
> -0.00281567087439197, 0.0397525174385432, -0.00711442815443551,
> -0.00131577619047549, 0.106909062371648, 0.025164340439287, 0.0577629820111593,
> 0.0939246700745592, 0.00391223125066969, 0.0124049752133815,
> -0.0275935528232096, 0.0762231355953777, 0.181010165393349, -0.0347198712523113,
> 0.0678716698457551, 0.0120798630452986, 0.0245162767779995, 0.0498518908504713,
> 0.0144272270551826, 0.0449534685914403, 0.000956463351216132,
> 0.0460805768375294, 0.0483340329448637, 0.0192578144013228, 0.0713319833154848,
> 0.0664774807491022, 0.0136479583643586, 0.047338990736079, -0.00984473290135361,
> 0.0791372581960931, 0.0256142818179614, 0.0598611692250246, 0.0265350466465605,
> 0.068670703501864, 0.00623102828923622, -0.00469207491878694,
> 0.157504094271494, 0.116984111424978, 0.0875878669508734, 0.0161726786019788,
> 0.175935713544914, 0.0637705286819113, 0.0197080817743348, 0.0452129305548087,
> 0.0233338613388849, 0.0936152099345934, 0.0199808837233982, 0.0274312796611153,
> 0.0721161218673364, 0.17082095580651, 0.034052131044711, 0.0512562246908417,
> 0.0487107440041036, 0.153481867586958, 0.0213458465510684, 0.0133431492518521,
> 0.119009291067923, 0.0253607374472143, 0.0202017481667936, 0.011516647246353,
> 0.0339052007553662, 0.0313036367875989, -0.00639827115782904,
> 0.0237304943165091, 0.00669172141737004, 0.0854219038259585,
> 0.0181012761967982, 0.0372169953336968, 0.219164421366707, 0.0387279347673063,
> -0.00326722366281047, 0.0611109647647912, 0.0724982521544455,
> 0.0114916829563205, 0.0264328043306142, 0.100227237031891, 0.0694579153964423,
> 0.0256998227540187, 0.0937017943889341, 0.0137629525760519, -0.0046036224834336,
> 0.0270242177828994, 0.0650551686301645, 0.01402255124967)
>
>
> very many thanks for your time and effort...
> yours sincerely
>
> AKSHAY M KULKARNI
>
>
> ________________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
> Sent: Wednesday, March 20, 2019 1:32 PM
> To: R help Mailing  list
> Subject: [R] problem with nlsLM.....
>
> dear members,
>                              Yesterday, Duncan identified a silly mistake in an nls call...in the list of starting values, one of the names of the variable was getting repeated. But I am experiencing the same problem again, with a different formula:
>
>> formulaDH3
> HM1 ~ (a + (b * ((HM2 + 0.3)^(1/3)))) * (c * log(HM3 + 27))
>
> here HM1 is the response variable, and HM2 and HM3 are predictors.....
>
>> nonlin_modDH3 <- nls(formulaDH3, start = list(a = 0.43143, b = 0.68173,c = 0.02954))
> Error in nlsModel(formula, mf, start, wts) :
>   singular gradient matrix at initial parameter estimates
>> nonlin_modDH3 <- nlsLM(formulaDH3, start = list(a = 0.43143, b = 0.68173,c = 0.02954))
> Error in nlsModel(formula, mf, start, wts) :
>   singular gradient matrix at initial parameter estimates
>
> I am using nlsLM function from the minpack.lm package which says that it will converge when nls fails with singular gradient matrix error...
>
>
> Is there again a silly mistake(pardon me again if there is one!), or is the problem serious? If it is serious, any pointers towards a solution?
>
> very many thanks for your time and effort....
> yours sincerely,
> AKSHAY M KULKARNI
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@r|ejoe@k|oury @end|ng |rom hotm@||@com  Thu Mar 21 09:28:06 2019
From: m@r|ejoe@k|oury @end|ng |rom hotm@||@com (Marie-joe Kfoury)
Date: Thu, 21 Mar 2019 08:28:06 +0000
Subject: [R] Question regarding the dcast function
Message-ID: <AM5PR0501MB23701AD03278D8F91F4171B2EF420@AM5PR0501MB2370.eurprd05.prod.outlook.com>

Hi,


I am having a problem with installing the dcast function in R 3.5.3

Here is the message that I am getting when trying to install it:

> install.packages("dcast")
Installing package into ?C:/Users/marie/Documents/R/win-library/3.5?
(as ?lib? is unspecified)
Warning message:
package ?dcast? is not available (for R version 3.5.3)



Marie Joe Kfoury

	[[alternative HTML version deleted]]


From @c|@nc|tto @end|ng |rom y@hoo@com  Thu Mar 21 11:52:42 2019
From: @c|@nc|tto @end|ng |rom y@hoo@com (Sara Ciancitto)
Date: Thu, 21 Mar 2019 05:52:42 -0500
Subject: [R] Error Message During ANOVA
Message-ID: <BB0A0422-B757-4E8E-B6B7-D33053577B7A@yahoo.com>

I know this is several years later but...

I ran into the same issue and was able to solve it by verifying the spacing between variables within original text file. For example, Item1, Item2, and Item3 had a single space between them while the numbers in each item had 5 spaces between them, resulting in a mismatch between r and tm. 

Hope this helps somebody. 

Chris

From b@mrut| @end|ng |rom hotm@||@com  Thu Mar 21 12:07:06 2019
From: b@mrut| @end|ng |rom hotm@||@com (SMRUTI BULSARI)
Date: Thu, 21 Mar 2019 11:07:06 +0000
Subject: [R] Issue with t.test
In-Reply-To: <CA+8X3fWZDrxr4a0DbyVV-LfsSM4GZdaw_mEFSFYtc2tkX_CCSQ@mail.gmail.com>
References: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>
 <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>
 <PS2PR02MB2839F67B6327A80D90A9A243A94A0@PS2PR02MB2839.apcprd02.prod.outlook.com>,
 <CA+8X3fWZDrxr4a0DbyVV-LfsSM4GZdaw_mEFSFYtc2tkX_CCSQ@mail.gmail.com>
Message-ID: <PS2PR02MB2839DC686160E0A9A7DA8505A9420@PS2PR02MB2839.apcprd02.prod.outlook.com>

Dear Jim Lemon:
First of all, very sorry for such a long delay in replying to your e-mail.
I am quite surprised that you did not receive the .csv and .R files attachment with my previous e-mail. I am re-sending those files.  However, by any chance, you do not receive them even this time, I must say that you have simulated the values that are almost similar in the example on which I undertook the one sample t-test using t.test. I have got a clear idea of how the t.test algorithm works and generates values through the communication with the R-team so far. Thank you very much for your detailed and prompt reply.
Kindest regards,
Smruti Bulsari

________________________________
From: Jim Lemon <drjimlemon at gmail.com>
Sent: 14 March 2019 02:25
To: SMRUTI BULSARI; r-help mailing list
Subject: Re: [R] Issue with t.test

Hi Smruti,
In the example in question, you are probably doing something like this:

# I didn't see any attachment for the data
X<-rnorm(99,mean=59.96753)
t.test(X,mu=50,alternative="less")

       One Sample t-test

data:  X
t = 101.29, df = 98, p-value = 1
alternative hypothesis: true mean is less than 50
95 percent confidence interval:
    -Inf 60.07022
sample estimates:
mean of x
59.90779

What this probably means is since the lower confidence interval for
the test is undefined, the program returns a p-value of 1 as an
estimate, much like the p-value<2.2e-16 for the other tests.

> 1-2e-16
[1] 1

Neither of these values are exact, but indicate that the null
hypothesis should be accepted or rejected respectively.

Jim

From @n@@nth@np|||@| @end|ng |rom gm@||@com  Thu Mar 21 13:01:27 2019
From: @n@@nth@np|||@| @end|ng |rom gm@||@com (Anaanthan Pillai)
Date: Thu, 21 Mar 2019 20:01:27 +0800
Subject: [R] Coding help for data frame.
Message-ID: <E9DF8DB7-E840-44F6-8CED-EE9B8ECFB3FD@gmail.com>

Good day,

#I?ve created hypothetical data for drug X and drug Y whereby both drug have the ability to have HbA1c reduction.

set.seed(10)
drugx= rnorm(50, mean = 0.1, sd=0.02)

set.seed(11)
drugy= rnorm(50, mean=0.15, sd=0.03)

#And created a data frame, compare drugs (comdrug) of 50 patients for each drug.

comdrug= data.frame("ID"=c(1:50), "DrugX"=drugx, "DrugY"=drugy)

# whereby the data would look like this

head(comdrug)

> head(comdrug)
  ID      DrugX     DrugY
1  1 0.10037492 0.1322691
2  2 0.09631495 0.1507978
3  3 0.07257339 0.1045034
4  4 0.08801665 0.1091204
5  5 0.10589090 0.1853547
6  6 0.10779589 0.1219755

Is there anyway of coding if I could arrange like this?




From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu Mar 21 14:56:43 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 21 Mar 2019 09:56:43 -0400
Subject: [R] Coding help for data frame.
In-Reply-To: <E9DF8DB7-E840-44F6-8CED-EE9B8ECFB3FD@gmail.com>
References: <E9DF8DB7-E840-44F6-8CED-EE9B8ECFB3FD@gmail.com>
Message-ID: <CAM_vju=PBOJGtYLj5btu79e-UTM3a9n-bg9qVyLt_JdHu9kCNw@mail.gmail.com>

I'm sorry, I don't understand what you're trying to do with your
hypothetical data.

Can you expand on what your question is?

Sarah

On Thu, Mar 21, 2019 at 9:54 AM Anaanthan Pillai
<anaanthanpillai at gmail.com> wrote:
>
> Good day,
>
> #I?ve created hypothetical data for drug X and drug Y whereby both drug have the ability to have HbA1c reduction.
>
> set.seed(10)
> drugx= rnorm(50, mean = 0.1, sd=0.02)
>
> set.seed(11)
> drugy= rnorm(50, mean=0.15, sd=0.03)
>
> #And created a data frame, compare drugs (comdrug) of 50 patients for each drug.
>
> comdrug= data.frame("ID"=c(1:50), "DrugX"=drugx, "DrugY"=drugy)
>
> # whereby the data would look like this
>
> head(comdrug)
>
> > head(comdrug)
>   ID      DrugX     DrugY
> 1  1 0.10037492 0.1322691
> 2  2 0.09631495 0.1507978
> 3  3 0.07257339 0.1045034
> 4  4 0.08801665 0.1091204
> 5  5 0.10589090 0.1853547
> 6  6 0.10779589 0.1219755
>
> Is there anyway of coding if I could arrange like this?
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu Mar 21 14:59:49 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 21 Mar 2019 09:59:49 -0400
Subject: [R] Question regarding the dcast function
In-Reply-To: <AM5PR0501MB23701AD03278D8F91F4171B2EF420@AM5PR0501MB2370.eurprd05.prod.outlook.com>
References: <AM5PR0501MB23701AD03278D8F91F4171B2EF420@AM5PR0501MB2370.eurprd05.prod.outlook.com>
Message-ID: <CAM_vjukoOOBcky2qX4N1czoPR+5PbHZQj68NVMo2TDdTRPN3Zg@mail.gmail.com>

Hi,

dcast() is a function, not a package, so R can't find it in the package archive.

You need to install the package that contains it. Probably you want reshape2

install.packages("reshape2")

Sarah

On Thu, Mar 21, 2019 at 9:54 AM Marie-joe Kfoury
<mariejoe.kfoury at hotmail.com> wrote:
>
> Hi,
>
>
> I am having a problem with installing the dcast function in R 3.5.3
>
> Here is the message that I am getting when trying to install it:
>
> > install.packages("dcast")
> Installing package into ?C:/Users/marie/Documents/R/win-library/3.5?
> (as ?lib? is unspecified)
> Warning message:
> package ?dcast? is not available (for R version 3.5.3)
>
>
>
> Marie Joe Kfoury
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Mar 21 15:51:08 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 21 Mar 2019 14:51:08 +0000
Subject: [R] Question regarding the dcast function
In-Reply-To: <AM5PR0501MB23701AD03278D8F91F4171B2EF420@AM5PR0501MB2370.eurprd05.prod.outlook.com>
References: <AM5PR0501MB23701AD03278D8F91F4171B2EF420@AM5PR0501MB2370.eurprd05.prod.outlook.com>
Message-ID: <6c4352c21acd413681b82cea1e932cf5@SRVEXCHCM1302.precheza.cz>

Hi

You shouldn't use HTML formating unless you want to surprise us with weird and messy email.

function dcast is in data.table and/or in reshape2 packages. For both

install.packages("data.table")
install.packages("reshape2")

followed by
library(data.table)
library(reshape2)

should be enough to make function dcast available.

Maybe you could spent some time reading R-intro docs.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Marie-joe Kfoury
> Sent: Thursday, March 21, 2019 9:28 AM
> To: r-help at r-project.org
> Subject: [R] Question regarding the dcast function
>
> Hi,
>
>
> I am having a problem with installing the dcast function in R 3.5.3
>
> Here is the message that I am getting when trying to install it:
>
> > install.packages("dcast")
> Installing package into ?C:/Users/marie/Documents/R/win-library/3.5?
> (as ?lib? is unspecified)
> Warning message:
> package ?dcast? is not available (for R version 3.5.3)
>
>
>
> Marie Joe Kfoury
>
> [[alternative HTML version deleted]]

Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Thu Mar 21 15:55:27 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 21 Mar 2019 15:55:27 +0100
Subject: [R] [FORGED] Re:  Issue with t.test
In-Reply-To: <a43c2498-40f5-090e-b8fb-22724da41383@auckland.ac.nz>
References: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>
 <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>
 <a43c2498-40f5-090e-b8fb-22724da41383@auckland.ac.nz>
Message-ID: <23699.42463.361117.519760@stat.math.ethz.ch>

>>>>> Rolf Turner 
>>>>>     on Wed, 13 Mar 2019 09:38:24 +1300 writes:

    > On 13/03/19 9:06 AM, Greg Snow wrote:
    > <SNIP>

    >> The only time I have seen t.test give a p-value of 1 is when the
    >> data mean exactly equals the null hypothesis mean and the alternative
    >> is the default of two.sided.

    > <SNIP>

    > Doesn't have to be *exact* equality.  Just close!

    > E.g.:

    > set.seed(42)
    > x <- runif(10)
    > mew <- 0.63626
    > mew==mean(x) # FALSE
    > t.test(x,mu=mew)

Well, it *prints* as 1.  But we've talked about *exact* above:

  > t.test(x,mu=mew)$p.value
  [1] 0.9999792

  > getOption("digits")
  [1] 7
  > mean(x)
  [1] 0.6362622
  > print(mean(x), digits=14)
  [1] 0.63626220719889
  > t.test(x,mu=0.6362622072)$p.value
  [1] 1
  > 1 - t.test(x,mu=0.6362622072)$p.value
  [1] 1.049139e-11
  > 

So even we get much closer to the mean, there's still a
difference to one, even though even less visibly.

And then if you go really really close to the mean, small
accuracy loss in the  pt() function will kick in, and you'll occasionally
will be correct, Rolf ... but only if you move **much** much
closer .... and in this case, it even seems not to happen at all:

  > mx <- mean(x)
  > mx - mx*(1 + 2^(-53+7e-15))
  [1] -1.110223e-16
  > 1 - t.test(x,mu=mx*(1 + 2^(-53+7e-15)))$p.value
  [1] 1.110223e-15
  > 

Cheers, Martin


    > cheers,
    > Rolf

    > -- 
    > Honorary Research Fellow
    > Department of Statistics
    > University of Auckland


From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu Mar 21 16:20:59 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 21 Mar 2019 11:20:59 -0400
Subject: [R] Coding help for data frame.
In-Reply-To: <D5A54BE9-8587-465A-B475-64A62B2A5545@gmail.com>
References: <CAM_vju=PBOJGtYLj5btu79e-UTM3a9n-bg9qVyLt_JdHu9kCNw@mail.gmail.com>
 <D5A54BE9-8587-465A-B475-64A62B2A5545@gmail.com>
Message-ID: <CAM_vjun1Ltx7jEpNea2v9r0gZkZFposO9yjMr1qVfDOt5LdNpA@mail.gmail.com>

Please also copy the R-help email list when you reply.

On Thu, Mar 21, 2019 at 10:05 AM Anaanthan Pillai
<anaanthanpillai at gmail.com> wrote:
>
> The hypothetical data is about drug x and drug y both we assume as diabetic drug which can reduce hbaic level (sugar level)
>
> Currently I have a data frame of 3X50, whereby the columns is (ID, drug x?s hba1c reduction, drug y?s hba1c reduction)
>
> I want to reaarrange the data into ID, type of drugs (x or y) and their  corresponding hba1c level.

Probably you need the reshape2 package. Can you provide an example of
what you expect the resulting data frame to look like? I'm assuming
that your sample data is the starting point.

Or do you simply need to rbind the two columns together?

Sarah


> Anand
>
> Begin forwarded message:
>
> From: Sarah Goslee <sarah.goslee at gmail.com>
> Subject: Re: [R] Coding help for data frame.
> Date: 21 March 2019 at 9:56:43 PM MYT
> To: Anaanthan Pillai <anaanthanpillai at gmail.com>
> Cc: r-help <r-help at r-project.org>
>
> I'm sorry, I don't understand what you're trying to do with your
> hypothetical data.
>
> Can you expand on what your question is?
>
> Sarah
>
> On Thu, Mar 21, 2019 at 9:54 AM Anaanthan Pillai
> <anaanthanpillai at gmail.com> wrote:
>
>
> Good day,
>
> #I?ve created hypothetical data for drug X and drug Y whereby both drug have the ability to have HbA1c reduction.
>
> set.seed(10)
> drugx= rnorm(50, mean = 0.1, sd=0.02)
>
> set.seed(11)
> drugy= rnorm(50, mean=0.15, sd=0.03)
>
> #And created a data frame, compare drugs (comdrug) of 50 patients for each drug.
>
> comdrug= data.frame("ID"=c(1:50), "DrugX"=drugx, "DrugY"=drugy)
>
> # whereby the data would look like this
>
> head(comdrug)
>
> head(comdrug)
>
>  ID      DrugX     DrugY
> 1  1 0.10037492 0.1322691
> 2  2 0.09631495 0.1507978
> 3  3 0.07257339 0.1045034
> 4  4 0.08801665 0.1091204
> 5  5 0.10589090 0.1853547
> 6  6 0.10779589 0.1219755
>
> Is there anyway of coding if I could arrange like this?
>
>

-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Mar 21 16:36:20 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 21 Mar 2019 18:36:20 +0300
Subject: [R] problem with nls....
In-Reply-To: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <20190321183620.26d27967@trisector>

One of the assumptions made by least squares method is that the
residuals are independent and normally distributed with same parameters
(or, in case of weighted regression, the standard deviation of the
residual is known for every point). If this is the case, the parameters
that minimize the sum of squared residuals are the maximum likelihood
estimation of the true parameter values.

The problem is, your data doesn't seem to adhere well to your formula.
Have you tried plotting your HF1 - ((m/HF6) + 1) against HF6 (i.e. the
residuals themselves)? With large residual values (outliers?), the loss
function (i.e. sum of squared residuals) is disturbed and doesn't
reflect the values you would expect to get otherwise. Try computing
sum((HF1 - ((m/HF6) + 1))^2) for different values of m and see if
changing m makes any difference.

Try looking up "robust regression" (e.g. minimize sum of absolute
residuals instead of squared residuals; a unique solution is not
guaranteed, but it's be less disturbed by outliers).

-- 
Best regards,
Ivan


From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu Mar 21 16:48:20 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 21 Mar 2019 11:48:20 -0400
Subject: [R] Coding help for data frame.
In-Reply-To: <B4B3F923-E368-45A6-BBA5-0935479C7568@gmail.com>
References: <E9DF8DB7-E840-44F6-8CED-EE9B8ECFB3FD@gmail.com>
 <B4B3F923-E368-45A6-BBA5-0935479C7568@gmail.com>
Message-ID: <CAM_vjuk1ASBaZgCAyepkda=OsPOSmeMuQ2NmdH47WqMt9c5GpQ@mail.gmail.com>

I'm glad you figured it out.

You do still need to reply to the R-help list, not me, but because you
did reply just to me I discovered that your orignal question had an
image in it of your desired result, which was stripped by the
pliant-text R-help server.

So your original email ended with:

"Is there anyway of coding if I could arrange like this?"

with absolutely no way to know what that was referring to.

Please take a look at the posting guide for this list to better
understand how to frame good questions, so people can help you.

Sarah

On Thu, Mar 21, 2019 at 11:42 AM Anaanthan Pillai
<anaanthanpillai at gmail.com> wrote:
>
>
> Hi,
>
> I?ve managed to sort out the problem. Yes you are correct, one of the coding method is using reshape2
>
> The coding is as below:
>
> > library(tidyr)
> > comdrug2 <- gather(comdrug, key = DrugType, value = Reduction, DrugX, DrugY)
> > head(comdrug2)
>   ID DrugType  Reduction
> 1  1    DrugX 0.10037492
> 2  2    DrugX 0.09631495
> 3  3    DrugX 0.07257339
> 4  4    DrugX 0.08801665
> 5  5    DrugX 0.10589090
> 6  6    DrugX 0.10779589
> > tail(comdrug2)
>     ID DrugType  Reduction
> 95  45    DrugY 0.17670152
> 96  46    DrugY 0.13968377
> 97  47    DrugY 0.08439656
> 98  48    DrugY 0.17640175
> 99  49    DrugY 0.17171570
> 100 50    DrugY 0.15659558
>
>
> Alternatively could be done as:
>
> ## option 2 to combine drug effects into single columnn (Drugtype)
> ## (will provide same result)
>
> comdrug_long <- reshape2::melt(comdrug, id.vars = "ID",
>                                value.name = "HbA1c Reduction",
>                                variable.name = "Types of Drug?)
>
>
> Regards,
>
> Anaanthan
>
>
> > Begin forwarded message:
>
> From: Anaanthan Pillai <anaanthanpillai at gmail.com>
> Subject: Coding help for data frame.
> Date: 21 March 2019 at 8:01:27 PM MYT
> To: r-help at r-project.org
>
> Good day,
>
> #I?ve created hypothetical data for drug X and drug Y whereby both drug have the ability to have HbA1c reduction.
>
> set.seed(10)
> drugx= rnorm(50, mean = 0.1, sd=0.02)
>
> set.seed(11)
> drugy= rnorm(50, mean=0.15, sd=0.03)
>
> #And created a data frame, compare drugs (comdrug) of 50 patients for each drug.
>
> comdrug= data.frame("ID"=c(1:50), "DrugX"=drugx, "DrugY"=drugy)
>
> # whereby the data would look like this
>
> head(comdrug)
>
> > head(comdrug)
>   ID      DrugX     DrugY
> 1  1 0.10037492 0.1322691
> 2  2 0.09631495 0.1507978
> 3  3 0.07257339 0.1045034
> 4  4 0.08801665 0.1091204
> 5  5 0.10589090 0.1853547
> 6  6 0.10779589 0.1219755
>
> Is there anyway of coding if I could arrange like this?
>
>
>


-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Mar 21 16:55:55 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 21 Mar 2019 15:55:55 +0000
Subject: [R] problem with nls....
In-Reply-To: <20190321183620.26d27967@trisector>
References: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <20190321183620.26d27967@trisector>
Message-ID: <SL2P216MB0091A43D999B295F7C7C5CE4C8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear Ivan,
                   I've not gone into residual analysis; but my observation is simple: I've checked the hist of both HF5 and HF6. There is not much difference. Also I've replaced all outliers.
HF1 ~ (m/HF5 )+ 1 is getting fitted properly, but not HF1 ~ (m/HF6) + 1.
                    The following are the actual values:

> HF1
Time Series:
Start = 1
End = 800
Frequency = 1
  [1] 1.0319256 0.9842066 1.0098243 1.0446384 0.9177308 1.0060822 0.9609599 1.0374124 1.0139675 0.9973329 0.9559346 0.9848896
 [13] 0.9749513 1.0511627 0.9789968 1.0964832 0.9879833 0.9549759 0.9787043 1.0203225 0.9947078 0.9813439 1.0138056 0.9670097
 [25] 0.9711946 0.9873085 1.0858024 1.0394149 0.9766102 0.9689002 1.0097453 1.0235376 0.9873976 0.9705998 1.0356838 1.0165155
 [37] 0.9855907 1.0757638 1.0072182 1.0280799 0.9281543 0.9587241 1.1086856 1.0446199 1.0158398 0.9529567 1.0610853 0.9976204
 [49] 0.9575143 0.9803208 1.1238821 1.0118991 1.0112989 0.9415333 1.0424331 0.9912462 1.0106361 0.9802978 1.0108935 1.0159902
 [61] 0.9892313 0.9438749 1.0118004 0.9953912 0.9175923 0.9479009 1.0235502 1.0060517 0.9890903 0.9885812 0.9900430 1.0350717
 [73] 1.0108698 1.0468498 1.0656555 1.0436655 0.9908752 0.9751098 1.0163194 0.9851445 0.9710072 0.9885114 1.0109649 1.0490736
 [85] 0.9795251 1.0108749 1.0029784 1.0149087 0.9965277 0.9893746 0.9917926 1.0115123 1.0472170 1.0437206 1.0139089 1.0372349
 [97] 1.0038352 0.9586151 1.0085806 1.0119048 1.0118624 0.9896469 1.0272961 1.0172400 1.0134005 0.9757968 0.9717420 1.0269058
[109] 1.0114416 0.9512890 1.0181753 1.0565599 1.0376291 0.9865798 1.0212159 1.0701965 1.0324734 0.9899814 0.9973403 1.0172419
[121] 1.0020050 0.9889063 1.0129236 1.0277797 0.9826509 0.9922282 1.0988522 1.0275115 1.0183555 0.9774303 1.0172997 1.0150803
[133] 0.9685015 0.9924186 0.9937192 1.0072210 0.9673327 1.0473338 1.0562761 0.9707440 0.9771936 0.9883559 1.0208805 0.9894798
[145] 1.0694593 0.9754638 1.0383527 1.0013232 0.9863309 0.8778824 1.0157532 1.0438316 1.0000022 0.9740199 1.0305441 1.0275372
[157] 0.9723386 0.9954525 1.0046082 0.9531964 0.9768512 0.9899314 1.0496263 1.0546074 0.9616430 1.0210772 0.9901334 1.0689765
[169] 1.0154938 0.8765444 0.9919604 1.0082690 0.9860675 0.9823378 0.9897682 1.0363582 0.9805102 0.9723787 1.0741545 1.0290322
[181] 0.9760903 0.9850951 1.0500385 0.9774908 0.9861186 0.9898369 0.9941887 1.0097938 1.0187774 1.0591694 1.0270933 1.0466363
[193] 1.0000043 0.9815685 1.0238718 0.9740055 0.9717232 1.0251001 0.9946316 1.0075567 0.9751129 0.9871612 1.0643235 1.0075491
[205] 0.9888058 0.9396797 1.0068366 0.9962325 1.0455487 1.0442334 1.0103938 1.0236919 0.9852552 0.9767037 1.0063593 1.0518584
[217] 0.9705860 0.9718808 1.0178662 1.0414515 0.9883699 0.9860597 1.0394941 1.0103630 0.9082023 0.9889798 0.9646139 1.0052705
[229] 0.9688456 1.0559528 1.0401153 0.9785603 1.0169463 0.9929363 0.9812825 0.9302532 1.0272447 1.0644704 1.0201468 1.0248872
[241] 0.9587034 0.9884793 1.0065787 1.0568458 1.0167972 0.9702934 1.0233577 1.0052691 0.9690838 0.9900543 1.0171212 1.0093782
[253] 0.9518359 0.8953816 1.1180924 1.0126421 0.9847542 0.9731075 0.9906067 1.0191311 0.9757062 0.9819144 1.0392988 1.0358210
[265] 0.9842700 1.0057314 1.0206313 1.0088607 0.9779384 0.9860996 0.9894232 1.0180867 1.0060215 0.9419578 1.0604701 1.0186874
[277] 0.9824626 0.9303484 1.0491317 1.0204767 0.9892820 0.9971268 1.0322837 1.0435960 1.0123649 0.9791956 0.9880841 1.0203823
[289] 0.9696436 0.9769832 1.0704628 1.0230000 0.9665417 0.8624573 1.0152342 1.0538081 0.9885551 0.9605257 1.0196322 1.0135050
[301] 1.0420189 0.9875982 1.0228686 1.0224319 0.9778704 0.9912653 1.0116106 1.0226598 0.9387455 0.9717815 1.0122788 0.9889690
[313] 1.0232488 1.0276606 1.0173681 1.0159885 0.9877074 0.9838069 1.0374707 1.0152624 0.9789677 0.9612178 1.0192874 1.0644549
[325] 0.9715407 0.9787567 0.9925342 0.9790322 0.9777879 0.9680505 1.0224064 1.0348370 0.9875051 0.9457753 0.9914921 0.9591109
[337] 0.9629202 0.9995519 1.0136481 1.0221348 1.0148608 0.9912785 1.0439862 1.0330749 0.9762325 0.9983923 0.9348918 1.0227065
[349] 0.9794121 0.9733227 1.0082373 1.0421889 0.9767361 0.9726911 1.0100370 0.9921361 0.9861159 0.9749961 1.0594331 1.0806732
[361] 1.0276992 1.0329190 1.0686383 1.0466639 0.9740776 0.9672371 1.0128714 0.9934691 0.9582222 0.9332858 1.0029784 1.0250300
[373] 1.0059249 0.9999445 1.0082015 1.0252359 0.9760324 0.9493543 0.9996351 1.0116540 0.9675301 0.9470141 1.0127507 1.0112527
[385] 0.9766712 0.9703953 1.0592567 1.0360448 0.9790881 0.9680051 0.9711350 1.0049626 0.9738689 0.9819661 1.0835125 0.9765333
[397] 0.9138484 1.0220322 1.0465788 1.0065803 1.0273082 0.9838126 1.0151329 1.0146824 0.9452442 0.9489901 0.9921946 1.0101152
[409] 0.9730738 0.9354592 0.9542558 0.9681532 0.9792620 1.0352246 1.0426173 1.0180344 0.9576323 0.9533448 0.9846387 1.0261479
[421] 0.9453757 0.9455791 1.0691109 1.0084141 0.9844405 0.9537970 1.0118840 1.0094733 1.1493009 0.9922558 0.9941628 1.0290179
[433] 1.0020050 0.9971342 1.0436267 1.0726863 1.0925811 1.1072580 1.0390200 1.0376942 1.0302470 0.9838505 1.0420336 0.9793092
[445] 0.9850191 1.0196805 1.0065491 1.0158645 1.0117730 0.9406381 1.0097070 0.9870108 0.9818856 1.0040046 0.9712323 0.9951345
[457] 1.0199816 1.0551752 1.0112867 1.0763534 1.0253155 1.0029784 1.0251464 1.0814414 0.9987183 0.9771628 0.9726044 1.0482059
[469] 1.0020050 0.8931139 1.0367775 1.0260033 0.9728766 1.0225689 0.9908196 1.0068729 0.9912127 0.9931128 1.0158280 1.0433496
[481] 1.0203120 1.0085496 0.9812741 1.0615742 1.0119223 0.9849236 0.9992032 0.9879929 0.9000571 0.9891419 1.0345521 1.0381184
[493] 0.9886766 0.9574869 1.0149106 1.0294410 0.9882982 1.0244778 0.9812230 1.0082813 0.9664091 1.0283733 1.0124268 0.9992115
[505] 0.9872004 0.9884649 1.0386713 0.9763343 0.9597727 0.9567414 1.0086152 1.0165768 0.9848861 0.9620526 1.0123326 1.0447678
[517] 0.9934084 0.9669690 1.0360421 0.9829837 0.9761610 0.9708850 1.0014170 1.0195497 0.9806560 0.9757284 1.0251931 1.0116233
[529] 0.9868054 0.9756085 1.0303624 1.0077517 1.0505017 0.9414114 1.0124536 1.0131595 0.9638660 0.9887363 1.0132553 1.0052792
[541] 0.9820370 0.9460134 1.0125483 1.0426700 0.9818528 0.9762532 0.9582658 0.9814603 0.9618717 0.9615659 0.9496436 0.9877108
[553] 0.9999971 1.0284677 1.0106125 1.0031898 0.9793703 0.9486161 1.0226473 1.0236002 0.9538295 0.9689285 1.0313897 1.0212912
[565] 0.9505638 0.9921170 1.0130086 1.0419494 1.0000323 0.9607922 1.0211809 1.0424671 0.9795343 0.9497697 1.0231071 1.0142700
[577] 0.9765539 0.9492815 1.0267628 1.0135138 0.9885966 0.9529603 1.0264062 1.0249176 0.9872525 0.9849608 0.9986306 1.0437033
[589] 1.0041780 0.9931204 1.0329029 0.9939742 0.9459785 0.9629758 0.9456565 0.9836949 0.9754926 0.9976241 1.0232742 1.0050830
[601] 0.9481952 0.9854969 1.0352188 1.0337062 0.9892019 0.9554122 1.0189333 0.9793607 0.9899167 0.9503345 1.0117583 1.0371750
[613] 1.0070349 0.9804208 1.0500940 1.0107281 1.0698735 0.9881469 1.0565684 1.0179031 0.9856278 1.0314952 1.0720689 1.0011222
[625] 0.9743944 1.0034468 0.9824861 1.0192735 0.9991494 0.9842630 1.0060971 1.0294506 0.9695057 0.9725408 1.0227924 1.0088150
[637] 0.9765886 0.9889828 1.0108903 1.0068109 0.9905286 0.9517037 1.0527706 1.0257783 0.9932039 1.0121870 1.0506565 0.9816386
[649] 0.9843450 0.9552800 1.0124886 1.0332463 1.0021401 0.9885442 1.0136001 1.0381933 0.9594773 1.0679251 0.9653448 0.9997715
[661] 0.9890589 0.9658054 1.0079124 1.1292276 0.9873225 0.9730770 1.0699042 1.0174021 1.0041981 1.0232245 1.0389181 0.9720513
[673] 0.8686271 0.9915428 0.9606290 1.0482094 0.9898013 0.9510998 0.9602020 0.9976802 1.1427011 0.9917742 0.9770992 0.8638270
[685] 0.9991782 1.0455336 1.1043633 1.0489159 1.0029784 0.9906192 1.0307161 1.0182152 0.9677313 1.0090984 0.9851279 0.9596324
[697] 0.9743092 0.9748568 1.0206321 1.0517142 0.9876535 0.9732838 1.0656093 1.0603864 0.9980164 0.9795437 0.9746766 0.9784871
[709] 0.9746066 1.0484975 1.0228157 1.0165735 0.9785301 1.0322862 1.0303562 1.0203352 0.9606113 1.0674109 1.0051598 1.0095761
[721] 1.0138837 0.9862772 1.0173451 0.9879873 0.9761662 0.9828150 0.9839169 0.9887962 0.9474475 0.9786754 1.0405266 1.0246702
[733] 0.9764242 0.9782060 1.0004626 1.0653315 1.1480925 0.9567859 1.0410088 1.0246378 1.0025964 0.9894414 1.0146759 1.0449204
[745] 0.9917509 0.9706269 1.0199806 1.0044524 0.9942750 1.0145927 0.9917488 1.0314604 0.9495737 1.0005564 0.9972033 0.9849848
[757] 0.9741118 0.9693319 1.0061280 0.9892915 0.9944768 1.0101943 1.0545997 1.0044063 1.0020050 1.0127975 1.0164313 1.0285558
[769] 1.0043574 0.9854983 1.0122655 1.0123857 0.9879603 0.9734764 0.9995228 1.0315182 0.9564373 1.0543879 1.0099970 0.9987432
[781] 0.9580883 0.9724853 1.0167722 1.0102822 0.9629902 0.9908875 0.9838395 0.9733901 1.0207349 0.9848377 1.0633785 1.0312998
[793] 1.0316422 1.0335433 0.9890110 1.0334082 0.9915590 0.9909167 1.0208474 0.9899497

> HF6
Time Series:
Start = 1
End = 800
Frequency = 1
  [1]  9.703261e-02 -3.302060e-01  5.100922e+00  1.932550e+00 -1.386912e-01  1.482268e-02 -1.137384e+00  3.732522e-01  2.506729e-01
 [10] -2.919045e-01 -6.675508e-02 -1.267444e+00 -4.271286e-01  1.539651e-01 -1.424168e-01  2.632788e-01 -6.013491e-02 -5.743224e-02
 [19] -1.955379e-01  7.423308e-01 -3.041726e-03 -2.667225e-02  2.409421e-01 -4.339732e-02 -2.372542e-01 -2.194143e-01  2.712374e-01
 [28]  1.764577e+00 -1.583502e-01 -1.558412e-01  4.859185e+00  6.595212e-02 -6.227563e-02 -3.663468e-02  9.338089e-01  1.165410e+00
 [37] -3.776054e-02  1.015936e+01  4.269841e+00  8.659153e-01 -1.045996e+00 -8.061952e-01  2.627137e-01  1.023131e-01  2.757644e-01
 [46] -6.199723e-02  1.466399e-01 -3.353696e-01 -2.881873e-01 -1.560865e-01  2.946743e-01  1.825263e-01  6.075510e-01 -7.659018e-02
 [55]  9.332004e-02 -7.924914e-01 -2.995696e+00 -2.625424e-01  6.959834e+00  2.882190e-01 -5.555718e-02 -3.191530e+00 -2.894247e+00
 [64] -7.495410e-01 -5.698178e-01 -2.920025e-01  7.262345e-02  6.955618e-01 -7.509777e-01 -3.111461e-02 -1.757717e+00  9.583333e-02
 [73]  2.022944e-01  1.481875e-01  3.709509e-01  3.297667e+00 -1.679928e-02 -6.633111e-01  3.081464e-01 -1.522342e-01 -2.697393e-01
 [82] -2.474069e-01  1.267182e+00  2.990766e-01 -1.483910e-01  1.851073e-02 -3.320246e+00  5.365467e-01  3.685251e-02 -5.869044e-02
 [91] -5.304953e-01  8.510204e-02 -1.943394e+00  6.796528e-01  8.707915e+00  5.339946e-01  3.334323e-01 -5.567989e-01  1.741750e-01
[100]  3.974109e-01 -1.180250e-01 -3.248193e-01  2.839601e-01  8.396776e-01  1.587400e+00 -1.052848e-01 -5.427561e-02  1.308345e+00
[109] -4.321102e+00 -2.114642e+00  2.545551e-01  2.608206e-01  2.468002e-01 -3.503397e-01  8.657229e-02  4.993098e-01  2.432785e+00
[118] -1.896142e-02 -2.014234e+00  2.029458e+00  6.079714e+00 -2.764164e-01  2.669853e-01  3.423891e+00 -5.324067e-01 -1.615363e-02
[127]  2.728479e+00  2.063365e+00  3.873700e-01 -9.717373e-01  2.802471e-01  3.221953e+00 -1.380415e+00 -2.251014e-01 -9.367013e-01
[136]  1.453974e-01 -9.212878e-01  6.660146e-01  2.698844e-01 -2.378487e-01 -1.841615e-01 -7.505472e-01  2.545551e-01 -1.904946e-02
[145]  2.825536e-01 -1.849939e-01  3.591260e-01  3.743418e-01 -2.778478e+00 -1.329060e+00  3.160122e-01  4.643313e-01  5.750524e-05
[154] -6.072878e-01  2.644429e-01  1.874244e+00 -2.695451e-01 -9.715273e-03  3.494761e-01 -9.281908e-02 -1.818026e-01 -3.065760e+00
[163]  1.745485e-01  4.058502e-01 -7.937648e-02  4.082885e-01 -5.328007e-02  5.173842e-01  3.014029e-01 -1.332769e+00 -1.525841e+00
[172]  1.278083e+00 -2.592115e+00 -5.447981e-02 -5.511966e-02  1.499697e-01 -7.537936e-01 -6.736513e-01  2.502264e-01  1.421474e+00
[181] -1.908278e-01 -2.629398e+00  3.030101e-01 -5.162059e-01 -2.154668e-01  1.774540e-03 -8.088480e-01  5.501430e-01  5.268684e-02
[190]  2.180616e-01  5.120812e-01  2.823400e-01  1.174173e-04 -3.871419e-02  3.158028e+00 -1.044852e+00 -2.686278e-01  7.454716e-01
[199]  7.658868e+00  1.125989e+00 -1.923856e-01 -2.441550e+00  5.024290e-01  2.290590e+00 -5.988608e-02 -7.947542e-01  4.383090e-01
[208] -6.176262e-03  2.480572e-01  6.266179e-01  3.552698e+00  7.503722e-01 -2.675535e-02 -5.389840e-01  8.622592e-01  1.991035e-01
[217] -2.189162e-01 -1.161234e+00  6.972145e-01  2.780796e-01 -6.312992e-02 -2.608414e+00  2.618422e+00  5.462640e+00 -1.142624e+00
[226] -8.490826e-01 -1.288512e-01  4.011994e-02 -2.048321e-01  3.272460e+00  6.866489e-01 -5.466328e-02  3.070392e-01 -9.886495e-01
[235] -1.569525e+00 -1.306696e-01  3.623149e+00  1.978161e-01  2.545551e-01  2.002372e+00 -5.400229e-01 -1.656640e-02  1.206290e-01
[244]  2.054049e-01  3.155210e-01 -1.744452e-01  2.545551e-01  7.412855e-01 -1.234508e-01 -1.718832e-02  7.214410e-01  6.076084e-01
[253] -1.024878e+01 -1.287062e+00  3.080408e+00  5.876054e-01 -2.777261e-01 -1.009898e+00 -5.856174e-01  6.067372e-02 -8.840570e-02
[262] -3.594392e-01  1.813739e+00  1.405564e-01 -3.372484e-02  8.371325e-01  3.424895e-01  1.493120e+00 -1.433819e+00 -4.222862e-01
[271] -6.719543e-02  3.063784e+00  1.282949e-01 -1.294272e-01  3.830982e-01  2.144883e+00 -8.491788e-02 -2.231544e+00  3.286810e-01
[280]  7.426660e-01 -6.825884e-02 -3.435546e-02  1.707858e+00  2.714363e-01  2.583534e-01 -1.856543e-01 -2.983154e-02  1.084806e+00
[289] -2.070454e+00 -1.513304e-01  2.514891e-01  1.531216e+00 -1.656472e+00 -1.337082e+00  3.243848e-01  1.715378e-01 -7.437379e-02
[298] -6.363805e-01  8.216420e-01  2.398677e+00  2.481079e-01 -1.493501e+00  1.788803e+00  4.053156e-01 -1.932145e-01 -1.698920e-01
[307]  3.829476e+00  1.608798e+00 -1.129716e-01 -1.141733e+00  1.621158e+00 -3.343241e-02  4.700237e+00  2.001713e+00  4.618576e+00
[316]  2.501457e+00 -7.836369e-02 -1.908722e-01  1.028320e+01  5.647405e-02 -1.710083e-01 -1.065351e+00  4.019061e-01  2.086564e-01
[325] -3.025440e-01 -9.047322e-02 -6.460067e-01 -6.844164e-01 -1.881830e-01 -8.318599e-02  3.826786e+00  9.766119e-01 -7.635718e-02
[334] -2.220902e-01 -3.856785e+00 -1.094635e+00 -7.540428e-02  3.058904e-02  9.137646e-01  3.775832e+00  2.989209e-01 -1.054544e-01
[343]  1.553016e-01  2.089815e+00 -1.232086e+00 -6.334727e-03 -7.148192e-01  1.619745e+00 -1.637780e-01 -2.134504e-01  1.515387e-01
[352]  2.434295e-01 -1.831071e-01 -9.457498e-01  2.459490e+00 -4.414135e-02 -8.114919e-02 -4.016494e-01  3.734457e-01  4.896307e-01
[361]  4.049245e+00  2.017318e+00  2.488363e-01  2.780003e-01 -1.550511e+00 -3.131615e+00  3.587101e+00 -8.323622e-02 -9.310946e-01
[370] -1.594693e-01 -6.449885e+00  8.850238e-01  2.298414e-01  2.548747e-02  3.259190e-01  3.200593e+00 -4.624236e-02 -3.353935e+00
[379] -9.192395e-03  1.659781e+00 -3.100714e-01 -2.432555e+00  2.493722e+00  6.676577e+00 -1.780416e-01 -8.183622e-01  2.873045e-01
[388]  1.873401e+00 -3.159656e-01 -1.063461e+00 -2.275333e-01  2.545551e-01 -1.147574e-01 -2.423924e-01  5.087483e-01 -4.946087e+00
[397] -1.404096e+00  5.942892e-01  1.850712e-01  2.305337e+00  1.519311e+00 -1.659701e-01  1.099995e+00  8.581784e-01 -1.180922e+00
[406] -1.422570e-01 -1.017652e-01  6.907890e-01 -2.043544e-01 -1.268577e-01 -7.520662e-01 -3.144183e-01 -3.326276e-01  2.571825e+00
[415]  2.758053e-01  5.519530e-01 -4.779812e+00 -8.598340e-01 -2.733269e-01  1.746146e+00 -1.152102e-01 -1.696746e-01  8.731155e+00
[424]  4.202525e-01 -8.863015e-02 -1.811685e+00  1.030848e+00  6.446866e+00  4.923567e+00 -2.394358e-02 -1.384412e-01  1.970634e-01
[433]  5.853398e+00 -5.313277e-01  2.451911e-01  5.068814e-01  1.001283e+01  2.545551e-01  2.991683e-01  2.978852e-01  1.920352e+00
[442] -6.436663e-01  2.865144e-01 -2.985307e+00 -7.993755e-02  6.913159e+00  1.263291e-01  4.946523e-01  2.384704e-01 -2.053579e-01
[451]  1.041425e+00 -3.840293e-02 -2.728177e-01  1.088364e-01 -2.959904e+00 -3.448277e-01  5.773138e-01  8.970300e-01  4.063197e-01
[460]  5.289729e-01  9.353198e-02 -5.891475e+00  2.352159e-01  1.773673e+00 -3.609789e-02 -5.292783e-02 -3.005821e+00  5.409213e-01
[469]  5.832009e+00 -1.372102e+00  3.681388e-01  2.022951e-01 -1.088937e+00  1.301919e+00 -1.351298e-01  1.666197e+00 -4.925816e-01
[478] -2.756112e-01  4.107335e-01  2.707616e-01  2.778384e-01  2.869210e-01 -2.518847e+00  2.731433e-01  3.057766e-01 -6.508426e-02
[487] -6.508053e-02 -5.386767e-02 -1.319746e+00 -5.511262e-01  2.439812e-01  3.581400e-01 -1.816593e-02 -2.177485e+00  1.568457e+00
[496]  2.416807e+00 -6.700124e-01  1.588973e-01 -8.862605e-02  2.121985e-01 -1.143138e+00  1.076068e+00  3.220819e-01 -6.488642e-02
[505] -9.119583e-02 -1.900989e-01  3.154147e-01 -5.464995e-01 -2.187182e-01 -2.180317e+00  2.234639e-01  2.134121e+00 -9.894065e-01
[514] -6.472309e-01  3.239172e-01  2.761212e-01 -1.826730e-01 -1.782663e-01  3.954665e-01 -1.855077e+00 -2.951616e-01 -1.235170e-01
[523]  2.251821e-01  5.553531e-01 -2.023192e-01 -9.014496e-01  6.563737e-01  4.016597e-01 -9.749756e-02 -6.402956e-02  2.750496e-01
[532]  2.754876e-01  3.293551e-01 -5.005642e+00  3.231129e-01  2.545811e-01 -1.022154e+00 -8.060808e-01  3.521192e-01  2.119989e-01
[541] -2.381150e-01 -3.288550e+00  1.421280e+00  2.847853e-01 -2.001002e-01 -1.646902e-01 -1.850079e-01 -7.654026e-02 -1.188583e-01
[550] -1.742599e-01 -1.984759e+00 -8.682619e-02  2.071753e-02  1.087379e+00  2.844600e-01  7.515339e-01 -2.224236e-01 -3.075256e+00
[559]  8.950929e-02  2.394501e+00 -2.165059e+00 -1.233829e-01  2.701741e+00  2.282062e+00 -1.924431e+00 -2.082254e-01  3.412529e-01
[568]  2.607147e-01  1.124918e-03 -1.203852e-01  2.657898e+00  1.957524e-01 -2.931182e-01 -1.971033e+00  1.807135e+00  8.706652e-01
[577] -2.326451e-01 -8.473731e-01  6.189609e-01  5.287915e-01 -7.740822e-01 -1.821481e+00  1.237988e-01  2.233247e+00 -3.362100e-01
[586] -2.170038e-01 -2.635905e-02  2.472973e-01  3.868325e-02 -1.159021e-01  2.500380e-01 -6.085198e-01 -1.899817e-01 -1.431927e-01
[595] -3.052454e+00 -3.469223e-01 -2.444967e-01  1.652156e-02  2.119758e-01  4.240047e-01 -1.911114e+00 -9.038186e-02  2.528617e-01
[604]  2.756286e+00 -3.906295e-01 -1.484883e-01  6.719161e-01 -3.866083e-01 -6.435435e-02 -1.902576e+00  5.826201e-01  2.644784e-01
[613]  9.538629e-02 -2.011244e-01  2.984296e-01  2.771165e+00  8.282686e+00 -1.932610e+00  4.163817e-01  1.595534e-01 -1.118635e+00
[622]  2.470832e-01  5.270764e-01  3.958194e-01 -6.165985e-02  5.335154e-01 -3.571421e-01  2.505762e+00 -2.753137e-02 -5.469856e-01
[631]  9.252352e-01  6.461865e-01 -4.461937e-01 -7.364934e-01  2.285105e+00  7.138954e-01 -2.720298e-01 -8.693912e-01  2.993691e+00
[640]  8.028559e-01 -6.743009e-02 -5.307888e-01  3.119620e-01  3.434619e+00 -1.452196e+00  1.684125e+00  3.311307e-01 -5.669708e-01
[649] -3.553324e-01 -1.896185e+00  3.446842e+00  7.830310e-01 -1.711669e+00 -1.277983e+00  4.621170e-01  2.421051e-01 -1.701033e-01
[658]  8.163954e-01 -7.261633e-01 -1.299687e-02 -8.016282e-02 -1.140658e-01  1.323860e+00  2.545551e-01 -2.675428e-01 -8.433672e-01
[667]  5.228999e-01  1.632592e-01 -2.957998e-01 -5.553013e-01  3.011379e-01 -6.305225e-01 -8.612630e+00 -3.065235e-01 -8.305565e-01
[676]  3.059103e-01 -7.797204e-02 -1.786650e+00 -1.488405e-01 -5.183906e-02  5.218233e+00  1.728557e-01 -4.481405e-01 -5.693220e+00
[685]  2.999499e-01  3.040171e-01  7.464381e+00  6.972166e+00 -8.403415e+00 -6.402848e-02  6.964933e-01  4.432842e-01 -1.786165e-01
[694]  5.528044e-01 -3.616451e-01 -1.591170e-01 -5.733156e-01 -3.732071e-01  1.612954e+00  4.521562e-01 -2.379350e-01 -7.817489e-01
[703]  5.615589e-01  7.984968e+00 -1.419967e-03 -3.413697e-01 -5.719286e-01 -8.776290e-02 -3.640012e-01  3.454289e-01  1.116740e-01
[712]  1.435803e-01 -2.474860e-01  1.093524e+00  2.347177e-01  1.730654e-01 -1.228630e-01  5.803183e-01  1.384456e+00  5.756865e-01
[721]  4.993324e-01 -3.569028e-01  1.399057e+00 -1.404911e+00 -2.982227e-01 -1.440777e-01 -3.008329e-01 -1.480333e-01 -2.034551e-01
[730] -6.386024e-02  5.534418e+00  1.260242e+00 -8.166069e-01 -2.222623e-01  7.545084e-02  5.541968e-01  5.556761e+00 -8.453119e-01
[739]  3.526444e-01  2.922084e+00  2.172985e-01 -1.469902e+00  1.632111e-01  3.493874e-01 -1.966893e-01 -2.924236e-01  1.102409e-01
[748]  6.462530e-02 -1.704171e-02  4.125223e-01 -2.080039e-01  2.607025e-01 -2.169992e-01  8.481024e-02  1.636632e-01 -5.962679e-01
[757] -3.093319e-01 -1.073459e-01  1.363464e+00 -1.401023e+00 -2.107503e-02  5.597599e-01  5.385795e-01  1.863389e+00  6.397418e+00
[766]  1.861240e+00  1.194237e+00  2.997435e-01  1.611484e-01 -5.484613e-01  1.189842e+00  3.837644e+00 -2.338096e-01 -5.737762e-01
[775]  2.413753e-03  3.455508e+00 -6.577547e-01  3.693522e-01  1.219843e-01 -2.947803e-02 -2.569075e-01 -1.664628e-01  2.123460e+00
[784]  4.798212e+00 -1.675385e-01 -7.847523e-02 -3.216411e-01 -2.386470e-01  2.017492e+00 -2.721515e-01  2.467115e+00  5.415200e-01
[793]  1.157133e+00  4.056428e-01 -2.823108e-01  4.366070e-01 -9.937483e-01 -6.383019e-02  2.599436e-01 -2.154435e-01


> HF5
Time Series:
Start = 1
End = 800
Frequency = 1
  [1]  -0.053649858   0.473045129  -1.855791134  -0.218807875   0.014571536   0.159596481   0.081564240   0.155658287  -0.106533556
 [10]   1.349296039   0.043722595   1.184981768   2.936701948  -0.020890782   0.287209659  -0.343506862   0.182125744   0.321655319
 [19]   0.152265596  -0.130765767   0.301479728   0.237639510  -0.113640154   0.116768441   0.245115225   2.653399743  -0.014934997
 [28]  -0.376484031   0.098036596   0.078589972 -10.566796506  -0.033271764   0.007003626   0.336061740  -0.290931130  -1.003161849
 [37]   2.126410832  -0.339519118  -1.984023750  -0.188090224   8.562631062   1.515090731  -0.089723677  -0.035854499  -0.156259607
 [46]   0.087586561  -0.099550560  -0.386082232   1.231825926   0.665419202  -0.018845938  -0.273064427  -0.441205599   0.058367119
 [55]  -0.035881404   0.231425905  -1.731548372   0.511813720  -2.652292762  -0.163362295   0.112082704   2.127449901  -1.984516819
 [64]  -1.681618131  -0.291551146   1.301356201  -0.231139629  -1.761747111   2.198414002   0.031431206   4.817803230  -0.053835827
 [73]  -0.241400896  -0.132737682  -0.494221853  -3.427015165   0.158200929   6.618433733  -0.255699644   0.399170676   0.469394277
 [82]   1.403215236  -0.539113590  -0.074840353   0.315268293   0.017757466  -0.069162940  -0.591611166   0.952482170   0.040122193
 [91]  -0.224477562  -0.035925567  -0.213900914  -2.517367299  -6.371690768  -0.379127361   1.742901426   0.115588677  -0.070774432
[100]  -0.042019349  -0.203094162   6.187633937  -0.030255588 -14.135776862  -0.292919023   0.210526422   0.015809975  -0.191603291
[109]  -0.028013930   1.196397848  -3.136609657  -0.016623343  -0.047866621   0.235623521  -0.033250628  -0.029950585  -0.152733306
[118]   0.283585876   0.725438532  -5.321553815  -0.262818166   0.333358093  -0.127887678  -0.825616353   0.729752381   0.161256587
[127]  -0.127870499  -3.504392573  -0.056818942   1.051455503  -0.610581967  -1.772628876   7.763088022   6.377255877   0.982331777
[136]  -0.049720167   1.668057546  -0.081526679  -0.016646304   0.027828635   0.021019397   0.638606658  -2.639610493   0.252214128
[145]  -0.021177179   0.309961739  -0.159709264  -0.791889902   0.221659320   0.071520603  -0.191792830  -0.095849781   0.147118019
[154]   0.423879172  -0.015966532  -0.814997357   0.182181232   0.740886403  -0.425878632   0.063399111   0.084086002   6.242629054
[163]  -0.166059640  -0.164944798   0.047540050   0.077867048   0.210206725  -0.059959501  -0.134972375   0.724032252   1.208053371
[172]  -1.450416248   0.767002941   0.567248602   0.203201419  -0.017947959   0.723492119   2.315582396  -0.003250712  -0.214061467
[181]   0.140103172   1.485875229  -0.063942314   0.504794752   1.203681332   2.916254637  -0.599141209  -0.504639390  -0.035917702
[190]  -0.066508032  -0.774334925  -0.044962400   0.539379226   0.035552430  -0.852486162   0.582649427   0.511438446  -0.054846737
[199]  -1.363899566  -0.925264908   0.378290631   8.418752812  -0.329694296  -0.646217899   0.014012816   0.229314359  -0.822570550
[208]   0.035566960  -0.048042626  -0.230902834  -0.983361624  -0.524604472   0.142112334   3.178630141  -0.679181149  -0.021673949
[217]   0.455549743   2.884246546   0.995534936  -0.074915836   0.294327209   8.707540120  -0.891039780  -5.116943345   1.807300268
[226]   4.653522444   0.176254186   0.096643906   0.097676112  -0.442654494  -0.176779103   0.047338893  -0.206053506  24.897248697
[235]  -0.558294814   0.015836695  -0.754272940  -0.071928292  -4.065491270  -1.044690836   0.376283222   0.588343352  -0.355257156
[244]  -0.083364082  -0.007105364   0.529359954  -8.021221450  -0.546179595   0.646916952   0.178089665  -0.051864657   0.253082405
[253]  20.713299311   0.759143878  -0.539982015  -0.368425364   3.538669564   0.567510757   0.191253562  -0.053957999   0.047407002
[262]   1.072823567  -2.268748418  -0.033168432   0.252976417   4.010509758  -0.085581899  -1.500184616   2.623411867   0.129878721
[271]   0.196248634  -0.481771250  -0.056828978   0.031878859  -0.144508992  -1.702092094   0.450120295   2.201565243  -0.208548110
[280]  -0.607892432   0.210270224  -0.053396495  -1.098250818  -0.497645770  -0.019981605   0.026436961   0.269281987  -0.203943129
[289]   5.803557957   0.898143581  -0.016638145  -2.870511105  12.535882950   0.089523798  -0.071056442  -0.083304296   0.371506360
[298]   2.549915204  -1.388314210  -5.815539352  -0.210410307  -0.046665442  -1.398770615  -0.420604388   0.007011299   7.295412551
[307]  -3.567433713  -2.259342153   0.111306625   9.424201044  -0.099990007   0.058851186  -0.319394368  -1.188994369 -13.473532466
[316]  -4.123907909   0.007008249   0.026816923 -11.866142680  -0.017989270   0.277994379   3.218420951  -0.312668928  -0.149944044
[325]   0.056081944   0.507135605   0.016331503   1.002122867   0.715103715   0.127326780  -5.870328468  -0.530081308   0.336482105
[334]   0.162019469   0.136228079   0.109176188   0.265903072   1.954660692  -1.262932946   0.190943304  -0.114749082   3.999535346
[343]  -0.022649183  -2.320142948   4.864378799   0.107269719  -0.147020500  -0.880102075   0.896221148   0.383806031  -0.014315332
[352]  -0.064334680   0.126186237   1.836083215  -1.195093675   0.247399700   0.210305271   1.205910512  -0.128797914  -0.015077554
[361]  -0.076260331  -1.984423551  -0.050001641  -0.060265425   2.076702500  -1.560055417 -13.194087713  -0.433552321   0.259611369
[370]   0.015964183   0.105154985  -0.359399469   0.056095783   0.468687701   0.237191558  -0.180564873   0.071450142   1.633298516
[379]   0.098140453  -0.865187577   0.126199486   3.933684419  -2.439808356  -5.299750403   0.659122872   5.485314982  -0.060356968
[388]  -0.273469732   0.881925411   1.401311316   0.357596451   0.190943304   0.225427150   1.918599734  -0.135809594  -1.539832826
[397]   1.422332786  -0.216520004  -0.116653447  -0.945873057  -1.076258982   0.530886207 -14.858856834 -10.332358918   3.337007186
[406]   0.029548722  -0.177707477  -0.465327158   0.126175184   0.079632842   1.568379101   0.039669024   3.115629690  -0.035586517
[415]  -0.195985895  -0.577867640  -0.677797610   1.094892700  -0.376687825  -4.102964091   0.143566503   0.088692628  -1.453786558
[424]  -0.299393058   1.107887381   0.674420993  -2.646522732  -1.389318035  -0.211813487   0.390659815  -0.355424658  -0.228305953
[433]  -0.390932354   0.797608035  -0.400864878  -0.664606830  -2.128206234  -1.332743740  -0.048330806  -0.302019469  -1.364677631
[442]   0.850007608  -0.498655463  -1.120737861   0.044377074  -4.067918353  -0.177685144  -0.238197352  -0.099527123   0.063876860
[451]  -0.109607175   0.286745627   3.228227858   1.155269654  -1.714722099   2.044204651  -0.376951275  -0.256646314  -0.462593438
[460]  -0.150378251  -0.036127900  -0.168221859  -0.250161833  -0.132990944  -0.092393191   0.017930849  -0.821142000  -0.045328381
[469]  -0.156346755   0.251034593  -0.015115175  -0.049923585   9.808254454 -10.923406279   0.218855389  -3.481913209   2.269242277
[478]   1.618053635  -0.568450760  -0.100185900  -0.218372487  -0.533170368  -3.035492341  -0.018080448  -0.270147632   0.064100947
[487]  -1.656390523   0.044406230   0.403815427   4.463785965  -0.242225219  -0.060562600   0.111633931   1.952214234  -1.702724573
[496]  -2.321447242   7.091924992  -0.100624956   0.014795836  -0.127924982   2.603975673  -0.238145700  -0.206169571  -0.280573145
[505]   0.133267731   1.334596328  -0.080741903   1.136959798   0.103590046   0.382617191  -0.120816198 -10.255711630   4.959744726
[514]   0.203784749  -0.241718413  -0.418653153   7.263519760   0.175385478  -0.121027999  -0.921554800   0.014025795   0.812840365
[523]  -0.033736823  -0.881201126   0.014024605   1.413383175  -0.084845152  -1.501412214   0.014026681   0.351134701  -0.105825219
[532]  -0.190871159  -0.016198856   1.582199545  -0.355298930  -0.119154942   1.119952829  15.442702714  -0.526105476   0.254349129
[541]   0.182400471   0.290090107  -0.544720386  -0.704947881   0.182413507   0.415254298   0.478675043   0.337429554   0.015951895
[550]   0.579123775   0.276916517   0.336730820   0.166459998  -0.165019677  -0.170639297  -2.156172309   0.007015360   0.679413452
[559]  -0.180776170  -0.412514852   1.919485711   0.564221261  -1.814078186  -0.523469370   0.850946856   0.654857991  -0.433715765
[568]  -0.064963825   0.315600383   0.208368575  -1.362624306  -0.036163560   1.522171004   0.501262098  -2.957851936   0.041325375
[577]   0.792613097   0.461937482  -0.549182039  -0.201497868   0.559521219   2.330550367  -0.072344287  -0.696108042   0.255478720
[586]   1.414073413  -0.568775940  -0.324689599   0.489813834  -0.149345898  -0.083926323  12.669192340   0.395227403   0.575646109
[595]   0.969961408   0.139445941   0.028068230   0.961621369  -0.747248768  -1.600810860   1.075547238   0.420965386  -0.113631655
[604]  -0.903460198   0.524895605   0.044527148  -0.461720956  -0.547639601   0.084186467   1.174865283  -1.097194604  -0.032498414
[613]   2.392118505   0.986643396  -0.162424173  -3.164451200  -0.769422900  -0.184600192  -0.197917436  -0.352329082   1.290558257
[622]  -0.184791050  -0.076097183  -1.798061452   0.080099125   2.062484105  -0.348501142  -2.106705631   0.007017747   2.305538391
[631]  -0.915838960  -0.220861746   0.077200047   0.610466280  -0.688401758  -0.207153770   0.049139553  22.362186197  -5.441551857
[640]  -1.708605711   0.926299855   0.207035751  -0.106446657  -2.675294607  -0.404511023  -0.788943233  -0.048807464  -0.227614326
[649]  -0.085333035   0.977385829 -16.826537503  -0.200423157  -0.051340705   6.499090143  -0.304213082  -0.065082852   0.048070630
[658]   0.666539778  -0.064016381  -0.109602571   0.533325153   0.528565621  -0.248317213  -3.473373955   1.272400022   1.711836935
[667]  -0.228344960  -0.252753461  -0.488373752  -0.401594723  -0.030427542  -0.455079097   3.252051577   0.960391227  -0.256075733
[676]  -0.136915862   0.098237444   1.674612416   0.044609980  -0.248469202  -0.298662830   1.173803660  -0.208363252   1.850645023
[685]   1.036270876  -0.152137097  -0.048105658  -1.277109207   0.059431246   0.064698690  -0.246801765  -0.196775000   0.533961473
[694]   0.220699810  -0.312873635   0.014867680  -0.241851486   0.032156787  -1.816522484  -0.167733410   4.158794025   2.099466739
[703]  -0.030488507   0.112566051   0.075675048   0.302820106  -0.469476310   0.210115120   0.056139143  -0.030484607  -0.072570524
[712]  -0.033886465   0.091219120  -0.248023454  -0.081455556  -0.203068185   0.054095543  -0.015255905  -3.701371648  -0.623879061
[721]   0.853410776   6.103753013  -1.341198580   1.247921308   0.751060465   0.781642884   0.379135477   1.145320110   0.313305428
[730]   0.112618403  -1.412256823  -0.103142715  -0.007113346   0.659901598  -2.519498558  -0.365995410  -0.184937991   0.897416670
[739]  -0.518805259  -3.353209940   1.867572217   9.205127781  -0.187969046  -0.778383177  -0.042669664   0.806807477  -0.090799820
[748]  -0.021826161   0.448223805  -0.164371146  -0.618774302  -0.244839681   0.194235170   1.570125546   1.754972837   0.500679719
[757]   0.870366653   0.433784961  -1.002863246   2.101960944   0.697030522   7.950881827  -0.061270167  -2.371332122  -0.142291873
[766]  -1.729969712  -1.941166110  -0.245036824  -0.106730528   5.057757700  -1.038846526  -0.858866602   3.386084663   1.395573786
[775]  -0.291650577  -2.212035645   0.856991031  -1.532383568  -0.185747818  -0.711396025   1.062315644   0.241829929  -1.838103065
[784] -12.577074634   1.735801542   0.484405184   0.013854970   0.416285923  -1.975226723   0.938110382  -0.647308291  -0.706063547
[793]  -0.082810695  -0.054601369  -0.014973073   0.127614348  18.906618087   0.502810107  -0.152371107  -0.036187828


very many thanks for your time and effort....
Yours sincerely,
AKSHAY M KULKARNI

________________________________
From: Ivan Krylov <krylov.r00t at gmail.com>
Sent: Thursday, March 21, 2019 9:06 PM
To: r-help at r-project.org
Cc: akshay kulkarni
Subject: Re: [R] problem with nls....

One of the assumptions made by least squares method is that the
residuals are independent and normally distributed with same parameters
(or, in case of weighted regression, the standard deviation of the
residual is known for every point). If this is the case, the parameters
that minimize the sum of squared residuals are the maximum likelihood
estimation of the true parameter values.

The problem is, your data doesn't seem to adhere well to your formula.
Have you tried plotting your HF1 - ((m/HF6) + 1) against HF6 (i.e. the
residuals themselves)? With large residual values (outliers?), the loss
function (i.e. sum of squared residuals) is disturbed and doesn't
reflect the values you would expect to get otherwise. Try computing
sum((HF1 - ((m/HF6) + 1))^2) for different values of m and see if
changing m makes any difference.

Try looking up "robust regression" (e.g. minimize sum of absolute
residuals instead of squared residuals; a unique solution is not
guaranteed, but it's be less disturbed by outliers).

--
Best regards,
Ivan

	[[alternative HTML version deleted]]


From @n@@nth@np|||@| @end|ng |rom gm@||@com  Thu Mar 21 17:00:01 2019
From: @n@@nth@np|||@| @end|ng |rom gm@||@com (Anaanthan Pillai)
Date: Fri, 22 Mar 2019 00:00:01 +0800
Subject: [R] Fwd:  Coding help for data frame.
References: <CAM_vjun1Ltx7jEpNea2v9r0gZkZFposO9yjMr1qVfDOt5LdNpA@mail.gmail.com>
Message-ID: <8CB781CE-E1FE-42AE-8023-4CFDC6986DF7@gmail.com>

> Hi,
> 
> I?ve managed to sort out the problem. Yes you are correct, one of the coding method is using reshape2
> 
> The coding is as below:
> 
>> library(tidyr)
>> comdrug2 <- gather(comdrug, key = DrugType, value = Reduction, DrugX, DrugY)
>> head(comdrug2)
>  ID DrugType  Reduction
> 1  1    DrugX 0.10037492
> 2  2    DrugX 0.09631495
> 3  3    DrugX 0.07257339
> 4  4    DrugX 0.08801665
> 5  5    DrugX 0.10589090
> 6  6    DrugX 0.10779589
>> tail(comdrug2)
>    ID DrugType  Reduction
> 95  45    DrugY 0.17670152
> 96  46    DrugY 0.13968377
> 97  47    DrugY 0.08439656
> 98  48    DrugY 0.17640175
> 99  49    DrugY 0.17171570
> 100 50    DrugY 0.15659558
> 
> 
> Alternatively could be done as:
> 
> ## option 2 to combine drug effects into single columnn (Drugtype)
> ## (will provide same result)
> 
> comdrug_long <- reshape2::melt(comdrug, id.vars = "ID",
>                               value.name = "HbA1c Reduction",
>                               variable.name = "Types of Drug?)
> 
> 
> Regards,
> 
> Anaanthan
> Begin forwarded message:
> 
> From: Sarah Goslee <sarah.goslee at gmail.com>
> Subject: Re: [R] Coding help for data frame.
> Date: 21 March 2019 at 11:20:59 PM MYT
> To: Anaanthan Pillai <anaanthanpillai at gmail.com>, r-help <r-help at r-project.org>
> 
> Please also copy the R-help email list when you reply.
> 
> On Thu, Mar 21, 2019 at 10:05 AM Anaanthan Pillai
> <anaanthanpillai at gmail.com> wrote:
>> 
>> The hypothetical data is about drug x and drug y both we assume as diabetic drug which can reduce hbaic level (sugar level)
>> 
>> Currently I have a data frame of 3X50, whereby the columns is (ID, drug x?s hba1c reduction, drug y?s hba1c reduction)
>> 
>> I want to reaarrange the data into ID, type of drugs (x or y) and their  corresponding hba1c level.
> 
> Probably you need the reshape2 package. Can you provide an example of
> what you expect the resulting data frame to look like? I'm assuming
> that your sample data is the starting point.
> 
> Or do you simply need to rbind the two columns together?
> 
> Sarah
> 
> 
>> Anand
>> 
>> Begin forwarded message:
>> 
>> From: Sarah Goslee <sarah.goslee at gmail.com>
>> Subject: Re: [R] Coding help for data frame.
>> Date: 21 March 2019 at 9:56:43 PM MYT
>> To: Anaanthan Pillai <anaanthanpillai at gmail.com>
>> Cc: r-help <r-help at r-project.org>
>> 
>> I'm sorry, I don't understand what you're trying to do with your
>> hypothetical data.
>> 
>> Can you expand on what your question is?
>> 
>> Sarah
>> 
>> On Thu, Mar 21, 2019 at 9:54 AM Anaanthan Pillai
>> <anaanthanpillai at gmail.com> wrote:
>> 
>> 
>> Good day,
>> 
>> #I?ve created hypothetical data for drug X and drug Y whereby both drug have the ability to have HbA1c reduction.
>> 
>> set.seed(10)
>> drugx= rnorm(50, mean = 0.1, sd=0.02)
>> 
>> set.seed(11)
>> drugy= rnorm(50, mean=0.15, sd=0.03)
>> 
>> #And created a data frame, compare drugs (comdrug) of 50 patients for each drug.
>> 
>> comdrug= data.frame("ID"=c(1:50), "DrugX"=drugx, "DrugY"=drugy)
>> 
>> # whereby the data would look like this
>> 
>> head(comdrug)
>> 
>> head(comdrug)
>> 
>> ID      DrugX     DrugY
>> 1  1 0.10037492 0.1322691
>> 2  2 0.09631495 0.1507978
>> 3  3 0.07257339 0.1045034
>> 4  4 0.08801665 0.1091204
>> 5  5 0.10589090 0.1853547
>> 6  6 0.10779589 0.1219755
>> 
>> Is there anyway of coding if I could arrange like this?
>> 
>> 
> 
> -- 
> Sarah Goslee (she/her)
> http://www.numberwright.com


	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Mar 21 21:07:58 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 21 Mar 2019 13:07:58 -0700
Subject: [R] Rank ANCOVA
In-Reply-To: <VI1PR04MB5488035B17D6AB450F6349B9B34A0@VI1PR04MB5488.eurprd04.prod.outlook.com>
References: <VI1PR04MB5488035B17D6AB450F6349B9B34A0@VI1PR04MB5488.eurprd04.prod.outlook.com>
Message-ID: <4e196dc4-ce87-ce27-27e6-af4c02f4a977@comcast.net>

If you are looking for robust multivariate or multivariable methods, 
then review the Robust Methods Task View:


https://cran.r-project.org/view=Robust


-- 

David.

On 3/13/19 4:06 AM, Jackson, Daniel wrote:
> Hi Frank and Dennis
>
> I am in a similar situation but I would prefer to use a proportional odds model. 2 questions.
>
>
>    1.  Has rank ancova been implemented in R now, despite its short comings?
>    2.  Where has it been shown to yield unreliable analyses? I would like this evidence (which I believe but I would like to convince others!).
>
> Thanks, Dan
> ________________________________
>
> AstraZeneca UK Limited is a company incorporated in Engl...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu  Thu Mar 21 21:58:04 2019
From: mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu (Matthew)
Date: Thu, 21 Mar 2019 16:58:04 -0400
Subject: [R] creating a dataframe with full_join and looping over a list of
 lists
Message-ID: <ca9d6b37-5369-0554-f7b3-37665dbff802@molbio.mgh.harvard.edu>

I have been trying create a dataframe by looping through a list of lists,

and using dplyr's full_join so as to keep common elements on the same row.

But, I have a couple of problems.

1) The lists have different numbers of elements.

2) In the final dataframe, I would like the column names to be the names 
of the lists.

Is it possible ?


 ?for(j in avector){

 ??? mydf3 <- data.frame(myenter)?????????? # Start out with a list, 
myenter, to dataframe. mydf3 now has 1 column.
 ??? ????????????????????????????????????????????????????????? # This 
first column will be the longest column in the final mydf3.
 ??? atglsts <- as.data.frame(comatgs[j])? # Loop through a list of 
lists, comatgs, and with each loop a particular list
 ??? ????????????????????????????????????????????????????????? # is made 
into a dataframe of one column, atglsts.
 ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ? ? ? ? ? ? ? ? ? ? ?? # The 
name of the column is the name of the list.
 ??? ????????????????????????????????????????????????????????? # Each 
atglsts dataframe has a different number of elements.
 ??? mydf3 <- full_join(mydf3, atglsts)???? # What I want to do, is to 
add the newly made dataframe, atglsts, as a
 ??? ?????????????? }???????????????????????????????????????? # new 
column of the data frame, mydf3 using full_join
 ??? ??? ??? ??? ???????????????????????????????????????????? # in order 
to keep common elements on the same row.
 ???????????????????????????????????????????????????????????? # I could 
rename the colname to 'AGI' so that I can join by 'AGI',
 ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ???????????????????? # but then 
I would lose the name of the list.
 ??? ??? ??? ??? ??? ??? ??? ??? ??? ??? ??????????????????? # In the 
final dataframe, I want to know the name of the original list

 ??????????????????? # the column was made from.

Matthew



	[[alternative HTML version deleted]]


From mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu  Thu Mar 21 22:08:45 2019
From: mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu (Matthew)
Date: Thu, 21 Mar 2019 17:08:45 -0400
Subject: [R] creating a dataframe with full_join and looping over a list of
 lists.
Message-ID: <9d5bf04e-7845-f6b2-95c9-5560d1c0785f@molbio.mgh.harvard.edu>

   My apologies, my first e-mail formatted very poorly when sent, so I am trying again with something I hope will be less confusing.

I have been trying create a dataframe by looping through a list of lists,
and using dplyr's full_join so as to keep common elements on the same row.
But, I have a couple of problems.

1) The lists have different numbers of elements.

2) In the final dataframe, I would like the column names to be the names
of the lists.

Is it possible ?

Code: *for(j in avector){****mydf3 <- data.frame(myenter) ****atglsts <- 
as.data.frame(comatgs[j]) ****mydf3 <- full_join(mydf3, atglsts) ****}* 
Explanation: # Start out with a list, myenter, to dataframe. mydf3 now 
has 1 column. # This first column will be the longest column in the 
final mydf3. # Loop through a list of lists, comatgs, and with each loop 
a particular list # is made into a dataframe of one column, atglsts. # 
The name of the column is the name of the list. # Each atglsts dataframe 
has a different number of elements. # What I want to do, is to add the 
newly made dataframe, atglsts, as a # new column of the data frame, 
mydf3 using full_join # in order to keep common elements on the same 
row. # I could rename the colname to 'AGI' so that I can join by 'AGI', 
# but then I would lose the name of the list. # In the final dataframe, 
I want to know the name of the original list # the column was made from. Matthew




	[[alternative HTML version deleted]]


From @ntov|r@| @end|ng |rom gm@||@com  Thu Mar 21 23:04:32 2019
From: @ntov|r@| @end|ng |rom gm@||@com (Antonello Preti)
Date: Thu, 21 Mar 2019 23:04:32 +0100
Subject: [R] Problem with forest plot in 'meta' after update and byvar
 command
Message-ID: <CAPmpGDvwefTHfHF=oYMsDNTv9Akxx06yhq1-4tmxt6XkScfbpg@mail.gmail.com>

Dear all, I have a problem with the package ?meta?.
I have 15 studies in a meta-analysis, belonging to two groups (a different
control was used depending on the study).
After the general estimation of the effect of the experimental treatment, I
want to evaluate the effect by type of control. So I used the ?update?
function with the ?byvar? command.

I want the sample size of the experimental group and the control group to
be printed in the forest plot. So I added the sample size to the result of
the subgroup analysis.

Then I have arranged the forest plot to have the sample size to be printed
before the effect size of each study, by grouping the studies according to
their type of control (two groups).

In the forest plot, I can see the total sample size of the whole sample of
studies, but the forest plot does not print the total sample size by the
group. Instead, it prints a point.

I do not know why.

Usually the forest plot after the ?update byvar? print the total sample
size by the group, as in the fig. 2.8 of Schwarzer, Carpenter, Rucker,
Meta-analysis with R, Springer, 2015.

What can I do?

Thank you in advance,
Antonello Preti

### Here the data and the codes I have used.

### The data

datAgg <- structure(list(Study = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 13L, 14L, 15L, 16L), .Label = c("Amber et al. 2015
Other         ",
"Beta et al. 1994 TAU           ", "Gemma?et al. 2012 TAU     ",
"Delta et al. 2015 Other         ", "Delta et al. 2015 TAU           ",
"Heta 1989 Other ", "Heta 1989 TAU   ",
"Lemme et al. 2008 Other   ", "Roda et al. 2011 Other      ",
"Saint et al. 2013 TAU         ", "Stabat et al. 2005 TAU          ",
"Spera et al. 1999 Other     ", "Tania et al. 2016 TAU             ",
"Vanda et al. 2002 TAU    ", "Vasto et al. 2005 Other         ",
"Vasto et al. 2005 TAU           ", "Wolf et al. 2005 Other        ",
"Wolf et al. 2005 TAU          "), class = "factor"), yi =
c(0.413781967817236,
0.897867818781807, -0.629691420114331, -0.0570629170645163,
0.100199348083711,
0.166720438987659, -0.0791416968479593, -0.516848713049707,
-0.588171436440896,
1.68578725963673, 0.549424389382073, -0.528085519326364,
0.0552725558889493,
0.450384415544429, 0.455766346836709), vi = c(0.0526848369519856,
0.0653245457221679, 0.0403678427142577, 0.0460018003388296,
0.0465699994730023,
0.200694892619406, 0.200156585204499, 0.0574699191913509,
0.0542032153656511,
0.060279300844124, 0.0988317519005698, 0.039802280364049,
0.0952744649456488,
0.0446012425417145, 0.0441325163026295), tr = c(42, 40, 52, 43,
43, 10, 10, 41, 38, 38, 21, 52, 21, 47, 47), ct = c(36, 29, 52,
44, 43, 10, 10, 32, 39, 54, 21, 52, 21, 45, 46), group = c("Vale",
"ADE", "ADE", "Vale", "ADE", "Vale", "ADE", "Vale", "Vale",
"ADE", "ADE", "ADE", "ADE", "Vale", "ADE"), sem = c(0.229531777651779,
0.255586669687932, 0.200917502259653, 0.214480302915745, 0.215800832883014,
0.447989835397418, 0.44738862882789, 0.239728845138316, 0.232815840023077,
0.245518432799095, 0.314375176978988, 0.199505088566806, 0.308665619960579,
0.211190062601711, 0.210077405502423)), row.names = c(NA, -15L
), class = "data.frame")


###########################################################
#
# Analysis with 'meta'
#
###########################################################

### aggregated data, check

head(datAgg)


### call the library

library(meta)

### analysis

meta1 <- metagen(TE=yi, seTE=sem, data=datAgg, studlab=datAgg$Study,
    sm="SMD", hakn=TRUE, method.tau="EB")

### add sample size to the results of the meta-analysis

meta1$n.e=datAgg$tr
meta1$n.c=datAgg$ct

### summary

summary(meta1)

### forest plot
### digits.se=2 is the minimum number of significant digits for standard
error (in this case = 2)

forest(meta1, digits.se=2, leftcols=c("studlab","n.e", "n.c", "TE",
"seTE"),
    leftlabs=c("Study", "n? \n Treated", "n? \n Controls", "Estimated \n
Effect Size", "Standard \n error"))


###########################################################
### subgroup analysis
###########################################################

modelsub <- update(meta1, byvar=group)

summary(modelsub)


### add sample size to the results of the meta-analysis

modelsub$n.e=datAgg$tr
modelsub$n.c=datAgg$ct

###########################################################
### forest plot
###########################################################

### save as .tiff

tiff("Figure.tiff", width = 12, height = 8, units = 'in', res = 300,
compression = 'lzw')

### forest plot
### digits=2 is the minimum number of significant digits for data

forest(modelsub,studlab=paste(datAgg$Study), print.byvar=FALSE,
fontsize=10,fs.heading=10,digits=2,
leftcols=c("studlab","n.e", "n.c"),comb.fixed=T, overall=T, col.by="black",

leftlabs=c("Study", "n? \n Treated", "n? \n Controls"))

dev.off()
### done

	[[alternative HTML version deleted]]


From reichm@@j m@iii@g oii sbcgiob@i@@et  Thu Mar 21 23:31:06 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Thu, 21 Mar 2019 17:31:06 -0500
Subject: [R] counting unique values (summary stats)
Message-ID: <002b01d4e035$c70acf90$55206eb0$@sbcglobal.net>

r-help

I have the following little scrip to create a df of summary stats.  I'm
having problems obtaining the # of unique values 

           unique=sapply(myData, function (x)
             length(unique(x), replace = TRUE))

Can I do that, or am I using the wrong R function?

summary.stats <- data.frame(mean=sapply(myData, mean, na.rm=TRUE), 
           sd=sapply(myData, sd, na.rm=TRUE), 
           min=sapply(myData, min, na.rm=TRUE), 
           max=sapply(myData, max, na.rm=TRUE), 
           median=sapply(myData, median, na.rm=TRUE), 
           length=sapply(myData, length),
           unique=sapply(myData, function (x)
             length(unique(x), replace = TRUE))
           miss.val=sapply(myData, function(y) 
             sum(length(which(is.na(y))))))

Jeff Reichman


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Mar 21 23:54:32 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 21 Mar 2019 15:54:32 -0700
Subject: [R] counting unique values (summary stats)
In-Reply-To: <002b01d4e035$c70acf90$55206eb0$@sbcglobal.net>
References: <002b01d4e035$c70acf90$55206eb0$@sbcglobal.net>
Message-ID: <d029c618-01de-74a7-6321-b2bdf209b838@comcast.net>


On 3/21/19 3:31 PM, reichmanj at sbcglobal.net wrote:
> r-help
>
> I have the following little scrip to create a df of summary stats.  I'm
> having problems obtaining the # of unique values
>
>             unique=sapply(myData, function (x)
>               length(unique(x), replace = TRUE))

I just looked up the usage on `length` and do not see any possibility of 
using a "replace" parameter. It's also unclear what sort of data object 
`myData` might be. (And you might consider using column names other than 
the names of R functions.)


-- 

David.

>
> Can I do that, or am I using the wrong R function?
>
> summary.stats <- data.frame(mean=sapply(myData, mean, na.rm=TRUE),
>             sd=sapply(myData, sd, na.rm=TRUE),
>             min=sapply(myData, min, na.rm=TRUE),
>             max=sapply(myData, max, na.rm=TRUE),
>             median=sapply(myData, median, na.rm=TRUE),
>             length=sapply(myData, length),
>             unique=sapply(myData, function (x)
>               length(unique(x), replace = TRUE))
>             miss.val=sapply(myData, function(y)
>               sum(length(which(is.na(y))))))
>
> Jeff Reichman
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Fri Mar 22 00:12:40 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 22 Mar 2019 10:12:40 +1100
Subject: [R] 
 creating a dataframe with full_join and looping over a list of
 lists.
In-Reply-To: <9d5bf04e-7845-f6b2-95c9-5560d1c0785f@molbio.mgh.harvard.edu>
References: <9d5bf04e-7845-f6b2-95c9-5560d1c0785f@molbio.mgh.harvard.edu>
Message-ID: <CA+8X3fWMV93TkNLN_izMzsmVd2LXGt=32MJVjmyeoRPBgh_5HA@mail.gmail.com>

Hi Matthew,
First thing, don't put:

mydf3 <- data.frame(myenter)

inside your loop, otherwise you will reset the value of mydf3 each
time and end up with only "myenter" and the final list. Without some
idea of the contents of comatgs, it is difficult to suggest a way to
get what you want.

Jim

On Fri, Mar 22, 2019 at 8:16 AM Matthew
<mccormack at molbio.mgh.harvard.edu> wrote:
>
>    My apologies, my first e-mail formatted very poorly when sent, so I am trying again with something I hope will be less confusing.
>
> I have been trying create a dataframe by looping through a list of lists,
> and using dplyr's full_join so as to keep common elements on the same row.
> But, I have a couple of problems.
>
> 1) The lists have different numbers of elements.
>
> 2) In the final dataframe, I would like the column names to be the names
> of the lists.
>
> Is it possible ?
>
> Code: *for(j in avector){****mydf3 <- data.frame(myenter) ****atglsts <-
> as.data.frame(comatgs[j]) ****mydf3 <- full_join(mydf3, atglsts) ****}*
> Explanation: # Start out with a list, myenter, to dataframe. mydf3 now
> has 1 column. # This first column will be the longest column in the
> final mydf3. # Loop through a list of lists, comatgs, and with each loop
> a particular list # is made into a dataframe of one column, atglsts. #
> The name of the column is the name of the list. # Each atglsts dataframe
> has a different number of elements. # What I want to do, is to add the
> newly made dataframe, atglsts, as a # new column of the data frame,
> mydf3 using full_join # in order to keep common elements on the same
> row. # I could rename the colname to 'AGI' so that I can join by 'AGI',
> # but then I would lose the name of the list. # In the final dataframe,
> I want to know the name of the original list # the column was made from. Matthew
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Mar 22 01:36:47 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 21 Mar 2019 17:36:47 -0700
Subject: [R] 
 creating a dataframe with full_join and looping over a list of lists
In-Reply-To: <ca9d6b37-5369-0554-f7b3-37665dbff802@molbio.mgh.harvard.edu>
References: <ca9d6b37-5369-0554-f7b3-37665dbff802@molbio.mgh.harvard.edu>
Message-ID: <CAGxFJbRGXPYhfCDzCDwzomeom5RxouFr62Qdg7Ujv9NapKeTeA@mail.gmail.com>

1. This is a plain text list. Do not post in HTML.

2. Read the posting guide. You will enhance your chnce of getting a useful
reply if you do what it says, especially prividing a reproducible example
that shows what you have, what you want, and the code you used, along with
any error messages you received.

Please read the ?data.frame or a tutorial on data frames. All columns
*must* have the same length. So what it comes down to is probably how you
want to fill with NA's.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Mar 21, 2019 at 1:58 PM Matthew <mccormack at molbio.mgh.harvard.edu>
wrote:

> I have been trying create a dataframe by looping through a list of lists,
>
> and using dplyr's full_join so as to keep common elements on the same row.
>
> But, I have a couple of problems.
>
> 1) The lists have different numbers of elements.
>
> 2) In the final dataframe, I would like the column names to be the names
> of the lists.
>
> Is it possible ?
>
>
>   for(j in avector){
>
>      mydf3 <- data.frame(myenter)           # Start out with a list,
> myenter, to dataframe. mydf3 now has 1 column.
>                                                                # This
> first column will be the longest column in the final mydf3.
>      atglsts <- as.data.frame(comatgs[j])  # Loop through a list of
> lists, comatgs, and with each loop a particular list
>                                                                # is made
> into a dataframe of one column, atglsts.
>                                                                 # The
> name of the column is the name of the list.
>                                                                # Each
> atglsts dataframe has a different number of elements.
>      mydf3 <- full_join(mydf3, atglsts)     # What I want to do, is to
> add the newly made dataframe, atglsts, as a
>                     }                                         # new
> column of the data frame, mydf3 using full_join
>                                                               # in order
> to keep common elements on the same row.
>                                                               # I could
> rename the colname to 'AGI' so that I can join by 'AGI',
>                                                               # but then
> I would lose the name of the list.
>                                                              # In the
> final dataframe, I want to know the name of the original list
>
>                      # the column was made from.
>
> Matthew
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri Mar 22 04:01:17 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 22 Mar 2019 14:01:17 +1100
Subject: [R] 
 creating a dataframe with full_join and looping over a list of
 lists.
In-Reply-To: <804f0547-28b2-2f7a-7f71-9c94305f0c88@molbio.mgh.harvard.edu>
References: <9d5bf04e-7845-f6b2-95c9-5560d1c0785f@molbio.mgh.harvard.edu>
 <CA+8X3fWMV93TkNLN_izMzsmVd2LXGt=32MJVjmyeoRPBgh_5HA@mail.gmail.com>
 <804f0547-28b2-2f7a-7f71-9c94305f0c88@molbio.mgh.harvard.edu>
Message-ID: <CA+8X3fWrSzGHRxCDk2LJ2SH5=ysaWpeAYZUP7LHdJAjTxCGvTw@mail.gmail.com>

Hi Matthew,
Remember, keep it on the list so that people know the status of the request.
I couldn't get this to work with the "_source_info_" variable. It
seems to be unreadable as a variable name. So, this _may_ be what you
want. I don't know if it can be done with "merge" and I don't know the
function "full_join".

WRKY8_colamp_a<-as.character(
 c("AT1G02920","AT1G06135","AT1G07160","AT1G11925","AT1G14540","AT1G16150",
 "AT1G21120","AT1G26380","AT1G26410","AT1G35210","AT1G49000","AT1G51920",
 "AT1G56250","AT1G66090","AT1G72520","AT1G80840","AT2G02010","AT2G18690",
 "AT2G30750","AT2G39200","AT2G43620","AT3G01830","AT3G54150","AT3G55840",
 "AT4G03460","AT4G11470","AT4G11890","AT4G14370","AT4G15417","AT4G15975",
 "AT4G31940","AT4G35180","AT5G01540","AT5G05300","AT5G11140","AT5G24110",
 "AT5G25250","AT5G36925","AT5G46295","AT5G64750","AT5G64905","AT5G66020"))

bHLH10_col_a<-as.character(c("AT1G72520","AT3G55840","AT5G20230","AT5G64750"))

bHLH10_colamp_a<-as.character(
 c("AT1G01560","AT1G02920","AT1G16420","AT1G17147","AT1G35210","AT1G51620",
 "AT1G57630","AT1G72520","AT2G18690","AT2G19190","AT2G40180","AT2G44370",
 "AT3G23250","AT3G55840","AT4G03460","AT4G04480","AT4G04540","AT4G08555",
 "AT4G11470","AT4G11890","AT4G16820","AT4G23280","AT4G35180","AT5G01540",
 "AT5G05300","AT5G20230","AT5G22530","AT5G24110","AT5G56960","AT5G57010",
 "AT5G57220","AT5G64750","AT5G66020"))

# let myenter be the sorted superset
myenter<-
 sort(unique(c(WRKY8_colamp_a,bHLH10_col_a,bHLH10_colamp_a)))

splice<-function(x,y) {
 nx<-length(x)
 ny<-length(y)
 newy<-rep(NA,nx)
 if(ny) {
  yi<-1
  for(xi in 1:nx) {
   if(x[xi] == y[yi]) {
    newy[xi]<-y[yi]
    yi<-yi+1
   }
   if(yi>ny) break()
  }
 }
 return(newy)
}

comatgs<-list(WRKY8_colamp_a=WRKY8_colamp_a,
 bHLH10_col_a=bHLH10_col_a,bHLH10_colamp_a=bHLH10_colamp_a)
mydf3<-data.frame(myenter,stringsAsFactors=FALSE)
for(j in 1:length(comatgs)) {
 tmp<-data.frame(splice(myenter,sort(comatgs[[j]])))
 names(tmp)<-names(comatgs)[j]
 mydf3<-cbind(mydf3,tmp)
}

Jim

On Fri, Mar 22, 2019 at 10:29 AM Matthew
<mccormack at molbio.mgh.harvard.edu> wrote:
>
> Hi Jim,
>
>     Thanks for the reply.  That was pretty dumb of me.  I took that out of the loop.
>
> comatgs is longer than this but here is a sample of 4 of 569 elements:
>
> $WRKY8_colamp_a
>  [1] "AT1G02920" "AT1G06135" "AT1G07160" "AT1G11925" "AT1G14540" "AT1G16150" "AT1G21120"
>  [8] "AT1G26380" "AT1G26410" "AT1G35210" "AT1G49000" "AT1G51920" "AT1G56250" "AT1G66090"
> [15] "AT1G72520" "AT1G80840" "AT2G02010" "AT2G18690" "AT2G30750" "AT2G39200" "AT2G43620"
> [22] "AT3G01830" "AT3G54150" "AT3G55840" "AT4G03460" "AT4G11470" "AT4G11890" "AT4G14370"
> [29] "AT4G15417" "AT4G15975" "AT4G31940" "AT4G35180" "AT5G01540" "AT5G05300" "AT5G11140"
> [36] "AT5G24110" "AT5G25250" "AT5G36925" "AT5G46295" "AT5G64750" "AT5G64905" "AT5G66020"
>
> $`_source_info_`
> character(0)
>
> $bHLH10_col_a
> [1] "AT1G72520" "AT3G55840" "AT5G20230" "AT5G64750"
>
> $bHLH10_colamp_a
>  [1] "AT1G01560" "AT1G02920" "AT1G16420" "AT1G17147" "AT1G35210" "AT1G51620" "AT1G57630"
>  [8] "AT1G72520" "AT2G18690" "AT2G19190" "AT2G40180" "AT2G44370" "AT3G23250" "AT3G55840"
> [15] "AT4G03460" "AT4G04480" "AT4G04540" "AT4G08555" "AT4G11470" "AT4G11890" "AT4G16820"
> [22] "AT4G23280" "AT4G35180" "AT5G01540" "AT5G05300" "AT5G20230" "AT5G22530" "AT5G24110"
> [29] "AT5G56960" "AT5G57010" "AT5G57220" "AT5G64750" "AT5G66020"
>
>
>       I have been thinking of something like this:
>
> lenmyen <- length(myenter)                        # get length of longest list
> length(comatgs[[j]) <- lenmyen                   # make each list length of myenter
> atglsts <- as.data.frame(comatgs[j])           # create dataframe
> colnames(atglsts) <- "AGI"                         # rename column to 'AGI'
>
> mydf3 <- full_join(mydf3, atglsts, by = "AGI"    # full_join
>
> Matthew
>
> On 3/21/2019 7:12 PM, Jim Lemon wrote:
>
>         External Email - Use Caution
>
> Hi Matthew,
> First thing, don't put:
>
> mydf3 <- data.frame(myenter)
>
> inside your loop, otherwise you will reset the value of mydf3 each
> time and end up with only "myenter" and the final list. Without some
> idea of the contents of comatgs, it is difficult to suggest a way to
> get what you want.
>
> Jim
>
> On Fri, Mar 22, 2019 at 8:16 AM Matthew
> <mccormack at molbio.mgh.harvard.edu> wrote:
>
>    My apologies, my first e-mail formatted very poorly when sent, so I am trying again with something I hope will be less confusing.
>
> I have been trying create a dataframe by looping through a list of lists,
> and using dplyr's full_join so as to keep common elements on the same row.
> But, I have a couple of problems.
>
> 1) The lists have different numbers of elements.
>
> 2) In the final dataframe, I would like the column names to be the names
> of the lists.
>
> Is it possible ?
>
> Code: *for(j in avector){****mydf3 <- data.frame(myenter) ****atglsts <-
> as.data.frame(comatgs[j]) ****mydf3 <- full_join(mydf3, atglsts) ****}*
> Explanation: # Start out with a list, myenter, to dataframe. mydf3 now
> has 1 column. # This first column will be the longest column in the
> final mydf3. # Loop through a list of lists, comatgs, and with each loop
> a particular list # is made into a dataframe of one column, atglsts. #
> The name of the column is the name of the list. # Each atglsts dataframe
> has a different number of elements. # What I want to do, is to add the
> newly made dataframe, atglsts, as a # new column of the data frame,
> mydf3 using full_join # in order to keep common elements on the same
> row. # I could rename the colname to 'AGI' so that I can join by 'AGI',
> # but then I would lose the name of the list. # In the final dataframe,
> I want to know the name of the original list # the column was made from. Matthew
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Mar 22 10:19:13 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 22 Mar 2019 10:19:13 +0100
Subject: [R] problem with nls....
In-Reply-To: <20190321183620.26d27967@trisector>
References: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <20190321183620.26d27967@trisector>
Message-ID: <23700.43153.867477.314835@stat.math.ethz.ch>

>>>>> Ivan Krylov 
>>>>>     on Thu, 21 Mar 2019 18:36:20 +0300 writes:

    > One of the assumptions made by least squares method is that the
    > residuals are independent and normally distributed with same parameters
    > (or, in case of weighted regression, the standard deviation of the
    > residual is known for every point). If this is the case, the parameters
    > that minimize the sum of squared residuals are the maximum likelihood
    > estimation of the true parameter values.

    > The problem is, your data doesn't seem to adhere well to your formula.
    > Have you tried plotting your HF1 - ((m/HF6) + 1) against HF6 (i.e. the
    > residuals themselves)? With large residual values (outliers?), the loss
    > function (i.e. sum of squared residuals) is disturbed and doesn't
    > reflect the values you would expect to get otherwise. Try computing
    > sum((HF1 - ((m/HF6) + 1))^2) for different values of m and see if
    > changing m makes any difference.

    > Try looking up "robust regression" (e.g. minimize sum of absolute
    > residuals instead of squared residuals; a unique solution is not
    > guaranteed, but it's be less disturbed by outliers).

Very good point, Ivan (as your previous ones on this thread -
thank you! it's great to have a couple of smart and patient
R-helpers such as you !!).

CRAN package robustbase
     https://cran.r-project.org/package=robustbase

has function nlrob()  to do non-linear
regression *robustly*, even using several methods, the default
one using robustly re-weighted nls().  I'm the maintainer of the
package and have been the "moderator" of the current nlrob()
function, but as you can read on it's help page, I'm not the
author of that function and its submethods:
https://www.rdocumentation.org/packages/robustbase/versions/0.93-4/topics/nlrob

Martin Maechler
ETH Zurich

    > -- 
    > Best regards,
    > Ivan

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From pd@me@ @end|ng |rom cb@@dk  Fri Mar 22 11:10:18 2019
From: pd@me@ @end|ng |rom cb@@dk (Peter Dalgaard)
Date: Fri, 22 Mar 2019 10:10:18 +0000
Subject: [R] R 3.6.0 scheduled for April 26
Message-ID: <E4634839-B3B1-4BDB-BB20-73C442364285@cbs.dk>

Full schedule is available on developer.r-project.org.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From @k@h@y_e4 @end|ng |rom hotm@||@com  Fri Mar 22 13:29:14 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Fri, 22 Mar 2019 12:29:14 +0000
Subject: [R] problem with nls....
In-Reply-To: <20190321183620.26d27967@trisector>
References: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <20190321183620.26d27967@trisector>
Message-ID: <SL2P216MB0091EA0A0C6091325304BA59C8430@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear Ivan,
                    I think my nls call is not converging to the proper value. I've gone through the Gauss Newton algorithm implemented by nls. How do I get the gradient, Hessian, and the jacobian of the objective function created by call to the nls? Perhaps I can compare all of them between my succesful nls call and the one that didn't. I've gone through debug(nls) but of no avail.

Also, I've checked the residuals...they are approximately normally distributed....I am still wondering why the nls call is not getting converged....!

Also, is it possible that if I give the vectors HF1,Hf5,HF6 it will help members in the mailing list to get to the bottom of the problem( I am sorry to have given the printed values of the vectors in my previous response to your mail...The dput values were very large. However, I will give the dput values this time around )?

very many thanks for your time and effort....
yours sincerely,
AKSHAY M KULKARNI

________________________________
From: Ivan Krylov <krylov.r00t at gmail.com>
Sent: Thursday, March 21, 2019 9:06 PM
To: r-help at r-project.org
Cc: akshay kulkarni
Subject: Re: [R] problem with nls....

One of the assumptions made by least squares method is that the
residuals are independent and normally distributed with same parameters
(or, in case of weighted regression, the standard deviation of the
residual is known for every point). If this is the case, the parameters
that minimize the sum of squared residuals are the maximum likelihood
estimation of the true parameter values.

The problem is, your data doesn't seem to adhere well to your formula.
Have you tried plotting your HF1 - ((m/HF6) + 1) against HF6 (i.e. the
residuals themselves)? With large residual values (outliers?), the loss
function (i.e. sum of squared residuals) is disturbed and doesn't
reflect the values you would expect to get otherwise. Try computing
sum((HF1 - ((m/HF6) + 1))^2) for different values of m and see if
changing m makes any difference.

Try looking up "robust regression" (e.g. minimize sum of absolute
residuals instead of squared residuals; a unique solution is not
guaranteed, but it's be less disturbed by outliers).

--
Best regards,
Ivan

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Fri Mar 22 13:40:15 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Fri, 22 Mar 2019 12:40:15 +0000
Subject: [R] problem with nls....
In-Reply-To: <23700.43153.867477.314835@stat.math.ethz.ch>
References: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <20190321183620.26d27967@trisector>,
 <23700.43153.867477.314835@stat.math.ethz.ch>
Message-ID: <SL2P216MB009128C0F33810A50EBF6E46C8430@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear Martin,
                         I've not yet tried robust regression; I want to get to the root of the problem. I've replied to Ivan requesting how to get the Gradient,Hessian and the Jacobian of the objective function in an nls call. My question to you is, does robust regression takes an entirely different algorithm other than Gauss Newton? Will that help in my case? My point is ,I've two calls to nls, the variables only slightly different, but one converging but the other one not...If GN fails, what is the guarantee that the other succeeds(also, there is no problem with the residuals, they are approximately, normally distributed..so whether its robust regression or not, I am beset with same problem..!)

very many thanks for your time and effort...
yours sincerely,
AKSHAY M KULKARNI

________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of Martin Maechler <maechler at stat.math.ethz.ch>
Sent: Friday, March 22, 2019 2:49 PM
To: Ivan Krylov
Cc: r-help at r-project.org
Subject: Re: [R] problem with nls....

>>>>> Ivan Krylov
>>>>>     on Thu, 21 Mar 2019 18:36:20 +0300 writes:

    > One of the assumptions made by least squares method is that the
    > residuals are independent and normally distributed with same parameters
    > (or, in case of weighted regression, the standard deviation of the
    > residual is known for every point). If this is the case, the parameters
    > that minimize the sum of squared residuals are the maximum likelihood
    > estimation of the true parameter values.

    > The problem is, your data doesn't seem to adhere well to your formula.
    > Have you tried plotting your HF1 - ((m/HF6) + 1) against HF6 (i.e. the
    > residuals themselves)? With large residual values (outliers?), the loss
    > function (i.e. sum of squared residuals) is disturbed and doesn't
    > reflect the values you would expect to get otherwise. Try computing
    > sum((HF1 - ((m/HF6) + 1))^2) for different values of m and see if
    > changing m makes any difference.

    > Try looking up "robust regression" (e.g. minimize sum of absolute
    > residuals instead of squared residuals; a unique solution is not
    > guaranteed, but it's be less disturbed by outliers).

Very good point, Ivan (as your previous ones on this thread -
thank you! it's great to have a couple of smart and patient
R-helpers such as you !!).

CRAN package robustbase
     https://cran.r-project.org/package=robustbase

has function nlrob()  to do non-linear
regression *robustly*, even using several methods, the default
one using robustly re-weighted nls().  I'm the maintainer of the
package and have been the "moderator" of the current nlrob()
function, but as you can read on it's help page, I'm not the
author of that function and its submethods:
https://www.rdocumentation.org/packages/robustbase/versions/0.93-4/topics/nlrob

Martin Maechler
ETH Zurich

    > --
    > Best regards,
    > Ivan

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From dc@r|@on @end|ng |rom t@mu@edu  Fri Mar 22 15:33:24 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Fri, 22 Mar 2019 14:33:24 +0000
Subject: [R] counting unique values (summary stats)
In-Reply-To: <d029c618-01de-74a7-6321-b2bdf209b838@comcast.net>
References: <002b01d4e035$c70acf90$55206eb0$@sbcglobal.net>
 <d029c618-01de-74a7-6321-b2bdf209b838@comcast.net>
Message-ID: <7ba9c5e130b54a97a525762c9bde5d12@tamu.edu>

You have several problems. As David W pointed out, there is no replace= argument in the unique() function. The first step in debugging your code should be to read the manual page for any function returning an error. Also you did not include a comma at the end of the line containing replace=TRUE. Finally the code for counting the missing values is more complicated than it needs to be. 

This code will only work if myData is a data frame that contains only columns with numeric data.

options(digits=4)
myData <- USArrests
summary.stats <- data.frame(mean=sapply(myData, mean, na.rm=TRUE), 
     sd=sapply(myData, sd, na.rm=TRUE), 
     min=sapply(myData, min, na.rm=TRUE), 
     max=sapply(myData, max, na.rm=TRUE), 
     median=sapply(myData, median, na.rm=TRUE), 
     length=sapply(myData, length),
     unique=sapply(myData, function (x) length(unique(x))),
     miss.val=sapply(myData, function(y) sum(is.na(y))))
summary.stats

            mean     sd  min   max median length unique miss.val
# Murder     7.788  4.356  0.8  17.4   7.25     50     43        0
# Assault  170.760 83.338 45.0 337.0 159.00     50     45        0
# UrbanPop  65.540 14.475 32.0  91.0  66.00     50     36        0
# Rape      21.232  9.366  7.3  46.0  20.10     50     48        0

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of David Winsemius
Sent: Thursday, March 21, 2019 5:55 PM
To: reichmanj at sbcglobal.net; 'r-help mailing list' <r-help at r-project.org>
Subject: Re: [R] counting unique values (summary stats)


On 3/21/19 3:31 PM, reichmanj at sbcglobal.net wrote:
> r-help
>
> I have the following little scrip to create a df of summary stats.  I'm
> having problems obtaining the # of unique values
>
>             unique=sapply(myData, function (x)
>               length(unique(x), replace = TRUE))

I just looked up the usage on `length` and do not see any possibility of 
using a "replace" parameter. It's also unclear what sort of data object 
`myData` might be. (And you might consider using column names other than 
the names of R functions.)


-- 

David.

>
> Can I do that, or am I using the wrong R function?
>
> summary.stats <- data.frame(mean=sapply(myData, mean, na.rm=TRUE),
>             sd=sapply(myData, sd, na.rm=TRUE),
>             min=sapply(myData, min, na.rm=TRUE),
>             max=sapply(myData, max, na.rm=TRUE),
>             median=sapply(myData, median, na.rm=TRUE),
>             length=sapply(myData, length),
>             unique=sapply(myData, function (x)
>               length(unique(x), replace = TRUE))
>             miss.val=sapply(myData, function(y)
>               sum(length(which(is.na(y))))))
>
> Jeff Reichman
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Fri Mar 22 15:35:19 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Fri, 22 Mar 2019 14:35:19 +0000
Subject: [R] Problem with forest plot in 'meta' after update and byvar
 command
In-Reply-To: <CAPmpGDvwefTHfHF=oYMsDNTv9Akxx06yhq1-4tmxt6XkScfbpg@mail.gmail.com>
References: <CAPmpGDvwefTHfHF=oYMsDNTv9Akxx06yhq1-4tmxt6XkScfbpg@mail.gmail.com>
Message-ID: <b0dde03a-55a3-915b-bdec-96b7e60c6fd4@dewey.myzen.co.uk>

Dear Antonello

There is a mailing list dedicated to meta-analysis and R. People from 
the team behind the package you mention do post there.

https://stat.ethz.ch/mailman/listinfo/r-sig-meta-analysis//

Remember to register first please.

Michael

On 21/03/2019 22:04, Antonello Preti wrote:
> Dear all, I have a problem with the package ?meta?.
> I have 15 studies in a meta-analysis, belonging to two groups (a different
> control was used depending on the study).
> After the general estimation of the effect of the experimental treatment, I
> want to evaluate the effect by type of control. So I used the ?update?
> function with the ?byvar? command.
> 
> I want the sample size of the experimental group and the control group to
> be printed in the forest plot. So I added the sample size to the result of
> the subgroup analysis.
> 
> Then I have arranged the forest plot to have the sample size to be printed
> before the effect size of each study, by grouping the studies according to
> their type of control (two groups).
> 
> In the forest plot, I can see the total sample size of the whole sample of
> studies, but the forest plot does not print the total sample size by the
> group. Instead, it prints a point.
> 
> I do not know why.
> 
> Usually the forest plot after the ?update byvar? print the total sample
> size by the group, as in the fig. 2.8 of Schwarzer, Carpenter, Rucker,
> Meta-analysis with R, Springer, 2015.
> 
> What can I do?
> 
> Thank you in advance,
> Antonello Preti
> 
> ### Here the data and the codes I have used.
> 
> ### The data
> 
> datAgg <- structure(list(Study = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
> 8L, 9L, 10L, 11L, 13L, 14L, 15L, 16L), .Label = c("Amber et al. 2015
> Other         ",
> "Beta et al. 1994 TAU           ", "Gemma?et al. 2012 TAU     ",
> "Delta et al. 2015 Other         ", "Delta et al. 2015 TAU           ",
> "Heta 1989 Other ", "Heta 1989 TAU   ",
> "Lemme et al. 2008 Other   ", "Roda et al. 2011 Other      ",
> "Saint et al. 2013 TAU         ", "Stabat et al. 2005 TAU          ",
> "Spera et al. 1999 Other     ", "Tania et al. 2016 TAU             ",
> "Vanda et al. 2002 TAU    ", "Vasto et al. 2005 Other         ",
> "Vasto et al. 2005 TAU           ", "Wolf et al. 2005 Other        ",
> "Wolf et al. 2005 TAU          "), class = "factor"), yi =
> c(0.413781967817236,
> 0.897867818781807, -0.629691420114331, -0.0570629170645163,
> 0.100199348083711,
> 0.166720438987659, -0.0791416968479593, -0.516848713049707,
> -0.588171436440896,
> 1.68578725963673, 0.549424389382073, -0.528085519326364,
> 0.0552725558889493,
> 0.450384415544429, 0.455766346836709), vi = c(0.0526848369519856,
> 0.0653245457221679, 0.0403678427142577, 0.0460018003388296,
> 0.0465699994730023,
> 0.200694892619406, 0.200156585204499, 0.0574699191913509,
> 0.0542032153656511,
> 0.060279300844124, 0.0988317519005698, 0.039802280364049,
> 0.0952744649456488,
> 0.0446012425417145, 0.0441325163026295), tr = c(42, 40, 52, 43,
> 43, 10, 10, 41, 38, 38, 21, 52, 21, 47, 47), ct = c(36, 29, 52,
> 44, 43, 10, 10, 32, 39, 54, 21, 52, 21, 45, 46), group = c("Vale",
> "ADE", "ADE", "Vale", "ADE", "Vale", "ADE", "Vale", "Vale",
> "ADE", "ADE", "ADE", "ADE", "Vale", "ADE"), sem = c(0.229531777651779,
> 0.255586669687932, 0.200917502259653, 0.214480302915745, 0.215800832883014,
> 0.447989835397418, 0.44738862882789, 0.239728845138316, 0.232815840023077,
> 0.245518432799095, 0.314375176978988, 0.199505088566806, 0.308665619960579,
> 0.211190062601711, 0.210077405502423)), row.names = c(NA, -15L
> ), class = "data.frame")
> 
> 
> ###########################################################
> #
> # Analysis with 'meta'
> #
> ###########################################################
> 
> ### aggregated data, check
> 
> head(datAgg)
> 
> 
> ### call the library
> 
> library(meta)
> 
> ### analysis
> 
> meta1 <- metagen(TE=yi, seTE=sem, data=datAgg, studlab=datAgg$Study,
>      sm="SMD", hakn=TRUE, method.tau="EB")
> 
> ### add sample size to the results of the meta-analysis
> 
> meta1$n.e=datAgg$tr
> meta1$n.c=datAgg$ct
> 
> ### summary
> 
> summary(meta1)
> 
> ### forest plot
> ### digits.se=2 is the minimum number of significant digits for standard
> error (in this case = 2)
> 
> forest(meta1, digits.se=2, leftcols=c("studlab","n.e", "n.c", "TE",
> "seTE"),
>      leftlabs=c("Study", "n? \n Treated", "n? \n Controls", "Estimated \n
> Effect Size", "Standard \n error"))
> 
> 
> ###########################################################
> ### subgroup analysis
> ###########################################################
> 
> modelsub <- update(meta1, byvar=group)
> 
> summary(modelsub)
> 
> 
> ### add sample size to the results of the meta-analysis
> 
> modelsub$n.e=datAgg$tr
> modelsub$n.c=datAgg$ct
> 
> ###########################################################
> ### forest plot
> ###########################################################
> 
> ### save as .tiff
> 
> tiff("Figure.tiff", width = 12, height = 8, units = 'in', res = 300,
> compression = 'lzw')
> 
> ### forest plot
> ### digits=2 is the minimum number of significant digits for data
> 
> forest(modelsub,studlab=paste(datAgg$Study), print.byvar=FALSE,
> fontsize=10,fs.heading=10,digits=2,
> leftcols=c("studlab","n.e", "n.c"),comb.fixed=T, overall=T, col.by="black",
> 
> leftlabs=c("Study", "n? \n Treated", "n? \n Controls"))
> 
> dev.off()
> ### done
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From j|en@gu90 @end|ng |rom gm@||@com  Thu Mar 21 22:09:51 2019
From: j|en@gu90 @end|ng |rom gm@||@com (Jiena McLellan)
Date: Thu, 21 Mar 2019 16:09:51 -0500
Subject: [R] [R-pkgs] noteMD v0.1.0 on CRAN
Message-ID: <CAB+AFnLrRb4ywqTw+oMsu3mNz1kMR-WRoS-r5FK472vS8amkRQ@mail.gmail.com>

 R and Shiny Developers,

Package noteMD v0.1.0 is released on CRAN now.
When building a 'shiny' app to generate reports (pdf or 'word'), we can
insert a comment box in front-end side for user to write down notes and
user this package to document those notes in reports.


CRAN:
https://cran.r-project.org/web/packages/noteMD/index.html
Github:
https://github.com/jienagu/noteMD

Welcome to give me any feedback!

Best Regards,
Jiena

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From b@mrut| @end|ng |rom hotm@||@com  Fri Mar 22 18:19:16 2019
From: b@mrut| @end|ng |rom hotm@||@com (SMRUTI BULSARI)
Date: Fri, 22 Mar 2019 17:19:16 +0000
Subject: [R] [FORGED] Re:  Issue with t.test
In-Reply-To: <23699.42463.361117.519760@stat.math.ethz.ch>
References: <PS2PR02MB28397FBE479CFFFBB677B46DA9490@PS2PR02MB2839.apcprd02.prod.outlook.com>
 <CAFEqCdyNLzuSyTHK_NWLbb6Ee0kCko_5NwWFrx4yJxcjqXJ96Q@mail.gmail.com>
 <a43c2498-40f5-090e-b8fb-22724da41383@auckland.ac.nz>,
 <23699.42463.361117.519760@stat.math.ethz.ch>
Message-ID: <PS2PR02MB2839DA49CD5AA87D6CB5C290A9430@PS2PR02MB2839.apcprd02.prod.outlook.com>

Thank you very much, Rolf Turner, Greg Snow, Martin Maechler and the entire team for the discussion and responses to my query. It has helped getting and enhancing my clarity on the issue.
Kindest regards,
Smruti

Get Outlook for Android<https://aka.ms/ghei36>



From: Martin Maechler
Sent: Thursday 21 March, 20:25
Subject: Re: [R] [FORGED] Re:  Issue with t.test
To: Rolf Turner
Cc: Greg Snow, r-help at R-project.org, SMRUTI BULSARI


>>>>> Rolf Turner >>>>> on Wed, 13 Mar 2019 09:38:24 +1300 writes: > On 13/03/19 9:06 AM, Greg Snow wrote: > >> The only time I have seen t.test give a p-value of 1 is when the >> data mean exactly equals the null hypothesis mean and the alternative >> is the default of two.sided. > > Doesn't have to be *exact* equality. Just close! > E.g.: > set.seed(42) > x mew mew==mean(x) # FALSE > t.test(x,mu=mew) Well, it *prints* as 1. But we've talked about *exact* above: > t.test(x,mu=mew)$p.value [1] 0.9999792 > getOption("digits") [1] 7 > mean(x) [1] 0.6362622 > print(mean(x), digits=14) [1] 0.63626220719889 > t.test(x,mu=0.6362622072)$p.value [1] 1 > 1 - t.test(x,mu=0.6362622072)$p.value [1] 1.049139e-11 > So even we get much closer to the mean, there's still a difference to one, even though even less visibly. And then if you go really really close to the mean, small accuracy loss in the pt() function will kick in, and you'll occasionally will be correct, Rolf ... but only if you move **much** much closer .... and in this case, it even seems not to happen at all: > mx mx - mx*(1 + 2^(-53+7e-15)) [1] -1.110223e-16 > 1 - t.test(x,mu=mx*(1 + 2^(-53+7e-15)))$p.value [1] 1.110223e-15 > Cheers, Martin > cheers, > Rolf > -- > Honorary Research Fellow > Department of Statistics > University of Auckland


	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Sat Mar 23 09:04:43 2019
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Sat, 23 Mar 2019 08:04:43 +0000
Subject: [R] problem with nls....
In-Reply-To: <20190321183620.26d27967@trisector>
References: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>,
 <20190321183620.26d27967@trisector>
Message-ID: <SL2P216MB0091A3A393F79C84CE166647C85C0@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>

dear Ivan and members,
                                              I was able to solve my problem. After going through Gauss Newton method, I tried to extract the Hessian,Gradient and the Jacobian from the nls call. But I could not succeed. However I observed that my formula contained only one parameter. Then the objective function is just a quadratic in that parameter. I applied directly Newton Raphson method and got the value of the parameter. To my surprise, it was the same as the output of the nls call!

I think I have to accept the value of the parameter, even though it is not a good fit. The world is very harsh(sometimes only?)!

I should thank Ivan for initiating me in the right direction...

very many thanks for your time and effort...
Yours sincerely,
AKSHAY M KULKARNI

________________________________
From: Ivan Krylov <krylov.r00t at gmail.com>
Sent: Thursday, March 21, 2019 9:06 PM
To: r-help at r-project.org
Cc: akshay kulkarni
Subject: Re: [R] problem with nls....

One of the assumptions made by least squares method is that the
residuals are independent and normally distributed with same parameters
(or, in case of weighted regression, the standard deviation of the
residual is known for every point). If this is the case, the parameters
that minimize the sum of squared residuals are the maximum likelihood
estimation of the true parameter values.

The problem is, your data doesn't seem to adhere well to your formula.
Have you tried plotting your HF1 - ((m/HF6) + 1) against HF6 (i.e. the
residuals themselves)? With large residual values (outliers?), the loss
function (i.e. sum of squared residuals) is disturbed and doesn't
reflect the values you would expect to get otherwise. Try computing
sum((HF1 - ((m/HF6) + 1))^2) for different values of m and see if
changing m makes any difference.

Try looking up "robust regression" (e.g. minimize sum of absolute
residuals instead of squared residuals; a unique solution is not
guaranteed, but it's be less disturbed by outliers).

--
Best regards,
Ivan

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Sat Mar 23 16:54:14 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sat, 23 Mar 2019 18:54:14 +0300
Subject: [R] problem with nls....
In-Reply-To: <SL2P216MB0091EA0A0C6091325304BA59C8430@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB0091FB4096F69DCBC53F9D0CC8420@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
 <20190321183620.26d27967@trisector>
 <SL2P216MB0091EA0A0C6091325304BA59C8430@SL2P216MB0091.KORP216.PROD.OUTLOOK.COM>
Message-ID: <20190323185414.3b9ec656@Tarkus>

On Fri, 22 Mar 2019 12:29:14 +0000
akshay kulkarni <akshay_e4 at hotmail.com> wrote:

> How do I get the gradient, Hessian, and the jacobian of the
> objective function created by call to the nls?

nls() return value is a list containing an entry named `m`, which is an
object of type "nlsModel". It doesn't seem to be documented in modern
versions of R[*], so what I am describing might be an implementation
detail subject to change. Still, model$m$gradient() should return the
jacobian; Hessian is usually estimated as crossprod() of jacobian; and
the gradient of the objective function is computed as
-2*colSums(model$m$resid() * model$m$gradient()).

> Also, I've checked the residuals...they are approximately normally
> distributed....I am still wondering why the nls call is not getting
> converged....!

The more important question is, how does the objective function (sum of
squared residuals) depend on the parameter `m` you are trying to find?
Try computing it for various values of `m` and looking at the result:

plot(
	Vectorize(
		function(m) {
			model$m$setPars(m);
			model$m$deviance()
		}
	),
	from = ..., to = ... # fill as needed
)

-- 
Best regards,
Ivan

[*] But used to be:
http://unixlab.stat.ubc.ca/R/library/stats/html/nlsModel.html


From |@h@rre|| @end|ng |rom vumc@org  Sat Mar 23 15:41:28 2019
From: |@h@rre|| @end|ng |rom vumc@org (Harrell, Frank E)
Date: Sat, 23 Mar 2019 14:41:28 +0000
Subject: [R] Regression Modeling Strategies and the R rms Package Short
 Course 2019
Message-ID: <BN6PR1201MB0003E9A451FFC5C4E9B0257E875C0@BN6PR1201MB0003.namprd12.prod.outlook.com>

*Regression Modeling Strategies Short Course 2019*

Frank E. Harrell, Jr., Ph.D., Professor

Department of Biostatistics, Vanderbilt University School of Medicine

fharrell.com ? ? @f2harrell



*May 14-17, 2019* With Optional R Workshop May 13

9:00am - 4:00pm

Alumni Hall

Vanderbilt University

Nashville Tennessee USA



See http://biostat.mc.vanderbilt.edu/RMSShortCourse2019 for details.



The course includes statistical methodology, case studies, and use of

the R rms package. ?Emphasis is on developing predictive models, model validation, and quantifying predictive accuracy, plus many more topics including navigating the choice of statistical models vs. machine learning.







Frank E Harrell Jr?????	Professor? ? ?	School of Medicine

	Department of Biostatistics?????	Vanderbilt University



From |@h@rre|| @end|ng |rom vumc@org  Sun Mar 24 15:16:17 2019
From: |@h@rre|| @end|ng |rom vumc@org (Harrell, Frank E)
Date: Sun, 24 Mar 2019 14:16:17 +0000
Subject: [R] Regression Modeling Strategies and the R rms Package Short
 Course 2019
In-Reply-To: <4FEA4C1C-F196-4773-8012-1DDBEB462DF5@gmail.com>
References: <BN6PR1201MB0003E9A451FFC5C4E9B0257E875C0@BN6PR1201MB0003.namprd12.prod.outlook.com>,
 <4FEA4C1C-F196-4773-8012-1DDBEB462DF5@gmail.com>
Message-ID: <BN6PR1201MB00039922BA4E901FF78E2E35875D0@BN6PR1201MB0003.namprd12.prod.outlook.com>

You'll need to see hundreds of past course evaluations to see why, but a main reason is the learning environment we provide, with hours of discussion among participants occuring during the 4-day course.  This includes topics such as how to collaborate with non-statisticians especially when explaining the results of complex statistical models.  We also have discussions during the lunches in the dining hall next to the lecture room.  Many of the topics I cover are set by the participants during the week.  And all of the course handouts are freely available for everyone.

Frank


________________________________
Frank E Harrell Jr      Professor       School of Medicine

        Department of Biostatistics             Vanderbilt University


________________________________
From: Graeme Davidson <graeme.r.davidson at gmail.com>
Sent: Sunday, March 24, 2019 04:42
To: Harrell, Frank E
Cc: r-help at r-project.org
Subject: Re: [R] Regression Modeling Strategies and the R rms Package Short Course 2019

Hi Frank,

As part of the R community, you will be aware that the vast majority of knowledge regarding statistics such as linear modelling is online for free. What makes this course worthy of payment compared to freely available information and/or well structured fee paying courses such as DataCamp?

All the best

Graeme R Davidson PhD

Data and Insight Analyst

> On 23 Mar 2019, at 14:41, Harrell, Frank E <f.harrell at vumc.org> wrote:
>
> *Regression Modeling Strategies Short Course 2019*
>
> Frank E. Harrell, Jr., Ph.D., Professor
>
> Department of Biostatistics, Vanderbilt University School of Medicine
>
> fharrell.com     @f2harrell
>
>
>
> *May 14-17, 2019* With Optional R Workshop May 13
>
> 9:00am - 4:00pm
>
> Alumni Hall
>
> Vanderbilt University
>
> Nashville Tennessee USA
>
>
>
> See https://nam05.safelinks.protection.outlook.com/?url=http%3A%2F%2Fbiostat.mc.vanderbilt.edu%2FRMSShortCourse2019&amp;data=02%7C01%7Cf.harrell%40vumc.org%7C933ae76953c84add589c08d6b03cfa6c%7Cef57503014244ed8b83c12c533d879ab%7C0%7C0%7C636890173287469754&amp;sdata=8GcqD8no7%2FNJ2Ytw%2B0U7DwwKvs6flF2buPvuHc3ra%2Bc%3D&amp;reserved=0 for details.
>
>
>
> The course includes statistical methodology, case studies, and use of
>
> the R rms package.  Emphasis is on developing predictive models, model validation, and quantifying predictive accuracy, plus many more topics including navigating the choice of statistical models vs. machine learning.
>
>
>
>
>
>
>
> Frank E Harrell Jr         Professor         School of Medicine
>
>    Department of Biostatistics         Vanderbilt University
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam05.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7Cf.harrell%40vumc.org%7C933ae76953c84add589c08d6b03cfa6c%7Cef57503014244ed8b83c12c533d879ab%7C0%7C0%7C636890173287469754&amp;sdata=UTdWE%2BH9AAlL8XytaifKp7BdaLKYwK4zDzb%2B2TaCnRY%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam05.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html&amp;data=02%7C01%7Cf.harrell%40vumc.org%7C933ae76953c84add589c08d6b03cfa6c%7Cef57503014244ed8b83c12c533d879ab%7C0%7C0%7C636890173287469754&amp;sdata=CyHkkgXrAkPTxSh87JUEbvg%2BwmV8LhqZYgYoMWgyzak%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From gr@eme@r@d@v|d@on @end|ng |rom gm@||@com  Sun Mar 24 10:42:03 2019
From: gr@eme@r@d@v|d@on @end|ng |rom gm@||@com (Graeme Davidson)
Date: Sun, 24 Mar 2019 09:42:03 +0000
Subject: [R] Regression Modeling Strategies and the R rms Package Short
 Course 2019
In-Reply-To: <BN6PR1201MB0003E9A451FFC5C4E9B0257E875C0@BN6PR1201MB0003.namprd12.prod.outlook.com>
References: <BN6PR1201MB0003E9A451FFC5C4E9B0257E875C0@BN6PR1201MB0003.namprd12.prod.outlook.com>
Message-ID: <4FEA4C1C-F196-4773-8012-1DDBEB462DF5@gmail.com>

Hi Frank, 

As part of the R community, you will be aware that the vast majority of knowledge regarding statistics such as linear modelling is online for free. What makes this course worthy of payment compared to freely available information and/or well structured fee paying courses such as DataCamp? 

All the best

Graeme R Davidson PhD

Data and Insight Analyst 

> On 23 Mar 2019, at 14:41, Harrell, Frank E <f.harrell at vumc.org> wrote:
> 
> *Regression Modeling Strategies Short Course 2019*
> 
> Frank E. Harrell, Jr., Ph.D., Professor
> 
> Department of Biostatistics, Vanderbilt University School of Medicine
> 
> fharrell.com     @f2harrell
> 
> 
> 
> *May 14-17, 2019* With Optional R Workshop May 13
> 
> 9:00am - 4:00pm
> 
> Alumni Hall
> 
> Vanderbilt University
> 
> Nashville Tennessee USA
> 
> 
> 
> See http://biostat.mc.vanderbilt.edu/RMSShortCourse2019 for details.
> 
> 
> 
> The course includes statistical methodology, case studies, and use of
> 
> the R rms package.  Emphasis is on developing predictive models, model validation, and quantifying predictive accuracy, plus many more topics including navigating the choice of statistical models vs. machine learning.
> 
> 
> 
> 
> 
> 
> 
> Frank E Harrell Jr         Professor         School of Medicine
> 
>    Department of Biostatistics         Vanderbilt University
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Mar 24 17:06:26 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 24 Mar 2019 09:06:26 -0700
Subject: [R] Regression Modeling Strategies and the R rms Package Short
 Course 2019
In-Reply-To: <4FEA4C1C-F196-4773-8012-1DDBEB462DF5@gmail.com>
References: <BN6PR1201MB0003E9A451FFC5C4E9B0257E875C0@BN6PR1201MB0003.namprd12.prod.outlook.com>
 <4FEA4C1C-F196-4773-8012-1DDBEB462DF5@gmail.com>
Message-ID: <c9f0e338-37ae-fbb8-ff9d-eaa93e3631d0@comcast.net>


Hi Graeme;


I took the course about ten years ago. I did so after getting a Masters 
in Epidemiology from the University of Washington and doing very well in 
all my stats courses and submitting my thesis work on solving regression 
problems with stratified sampling using bootstrap methods. So I think I 
probably had a much more solid grounding in regression methods than the 
average Datacamp customer. I found the course work and discussion very 
useful. I already had a copy of Harrell's RMS text and had read much of 
it before that class as well as applying several of the methods he 
illustrated. It covered topics of validity in inference and modeling of 
covariate functional relationships in much greater depth than I have 
ever seen in any of the online material I have reviewed in the last ten 
years. It was 4 days well spent and far cheaper than I could have gotten 
from a typical consultation.

The typical online course work demonstrates the regression machinery but 
very rarely gets into the issues of modeling splines or penalized 
methods. Model comparison and assessment of validity is often given 
cursory treatment. I continue to see questions on CrossValidated.com and 
StackOverflow that demonstrate that the bulk of the self-learners or 
distance learners have so far failed to acquire the knowledge base that 
vould be acquired during Frank's course. I would advise someone who has 
taken a Datacamp course in regression methods to take Frank's course as 
the next step to being "statisitcally educated".

-- 

David.

On 3/24/19 2:42 AM, Graeme Davidson wrote:
> Hi Frank,
>
> As part of the R community, you will be aware that the vast majority of knowledge regarding statistics such as linear modelling is online for free. What makes this course worthy of payment compared to freely available information and/or well structured fee paying courses such as DataCamp?
>
> All the best
>
> Graeme R Davidson PhD
>
> Data and Insight Analyst
>
>> On 23 Mar 2019, at 14:41, Harrell, Frank E <f.harrell at vumc.org> wrote:
>>
>> *Regression Modeling Strategies Short Course 2019*
>>
>> Frank E. Harrell, Jr., Ph.D., Professor
>>
>> Department of Biostatistics, Vanderbilt University School of Medicine
>>
>> fharrell.com     @f2harrell
>>
>>
>>
>> *May 14-17, 2019* With Optional R Workshop May 13
>>
>> 9:00am - 4:00pm
>>
>> Alumni Hall
>>
>> Vanderbilt University
>>
>> Nashville Tennessee USA
>>
>>
>>
>> See http://biostat.mc.vanderbilt.edu/RMSShortCourse2019 for details.
>>
>>
>>
>> The course includes statistical methodology, case studies, and use of
>>
>> the R rms package.  Emphasis is on developing predictive models, model validation, and quantifying predictive accuracy, plus many more topics including navigating the choice of statistical models vs. machine learning.
>>
>>
>>
>>
>>
>>
>>
>> Frank E Harrell Jr         Professor         School of Medicine
>>
>>     Department of Biostatistics         Vanderbilt University
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @yen @end|ng |rom hqu@edu@cn  Mon Mar 25 02:30:07 2019
From: @yen @end|ng |rom hqu@edu@cn (Steven Yen)
Date: Mon, 25 Mar 2019 09:30:07 +0800
Subject: [R] Printing vectrix
Message-ID: <2562d8fb-f109-35ee-cf1e-18934ffa993b@hqu.edu.cn>

I like to print a vector, wrapped by rows of 10.
Below the first command below works for 20 numbers.

The second command is ugly. How can I print the 25 numbers into 2 rows 
of ten plus a helf row of 5? Thanks.

 > x<-1:20; matrix(x,nrow=2,byrow=T)
 ???? [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]??? 1??? 2??? 3??? 4??? 5??? 6??? 7??? 8??? 9??? 10
[2,]?? 11?? 12?? 13?? 14?? 15?? 16?? 17?? 18?? 19??? 20
 > x<-1:25; matrix(x,nrow=2,byrow=T)
 ???? [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
[1,]??? 1??? 2??? 3??? 4??? 5??? 6??? 7??? 8??? 9??? 10??? 11 12??? 13
[2,]?? 14?? 15?? 16?? 17?? 18?? 19?? 20?? 21?? 22??? 23??? 24 25???? 1
Warning message:
In matrix(x, nrow = 2, byrow = T) :
 ? data length [25] is not a sub-multiple or multiple of the number of 
rows [2]
 >

-- 
syen at hqu.edu.cn


	[[alternative HTML version deleted]]


From choco|d12 @end|ng |rom gm@||@com  Mon Mar 25 07:17:47 2019
From: choco|d12 @end|ng |rom gm@||@com (lily li)
Date: Mon, 25 Mar 2019 14:17:47 +0800
Subject: [R] How to create gridded data
In-Reply-To: <ca572c18ecad45eba5e74225c33f29f3@tamu.edu>
References: <CAN5afy9jzOCZDzvTuzXBzeTzi_aZLsnVnYM1VuBWe0e5ziUWsQ@mail.gmail.com>
 <CAM_vjumM-nNbQBp-6vRbFKqWHpap9SQxwPBqsfkAs0+-XNj_tA@mail.gmail.com>
 <966004e209f949dba2ccf205bcef9983@tamu.edu>
 <CAN5afy-Ts_3fUmTptkCXxTpxRVe5z-Jk07RmxtfO=JaV6phkNg@mail.gmail.com>
 <ca572c18ecad45eba5e74225c33f29f3@tamu.edu>
Message-ID: <CAN5afy9bd1w9_Adxs3eqWsExaitAFtJGKhGsHC8zyGh0sG-6Rw@mail.gmail.com>

Now I have new question about this post. If the grid coordinates in DF1 are
not complete, i.e. there are missing coordinates, how to fill these with
-99 in the exported DF2? Thanks.

On Thu, Nov 15, 2018 at 10:57 PM David L Carlson <dcarlson at tamu.edu> wrote:

> It would depend on the format of the gridded data. Assuming it is a data
> frame like DF2 in my earlier answer, you just reverse the steps:
>
> > DF2
>      110.5 111 111.5 112
> 46     6.1 4.5   7.8 5.5
> 45.5   3.2 5.0   1.8 2.0
>
> > DF3 <- data.frame(as.table(as.matrix(DF2)))
>   Var1  Var2 Freq
> 1   46 110.5  6.1
> 2 45.5 110.5  3.2
> 3   46   111  4.5
> 4 45.5   111  5.0
> 5   46 111.5  7.8
> 6 45.5 111.5  1.8
> 7   46   112  5.5
> 8 45.5   112  2.0
>
> But the latitude and longitude get converted to factors and we lose the
> column names:
>
> > DF3 <- data.frame(as.table(as.matrix(DF2)))
> > colnames(DF3) <- c("latitude", "longitude", "Precip")
> > DF3$latitude <- as.numeric(as.character(DF3$latitude))
> > DF3$longitude <- as.numeric(as.character(DF3$longitude))
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
> From: lily li <chocold12 at gmail.com>
> Sent: Tuesday, November 13, 2018 10:50 PM
> To: David L Carlson <dcarlson at tamu.edu>
> Cc: Sarah Goslee <sarah.goslee at gmail.com>; R mailing list <
> r-help at r-project.org>
> Subject: Re: [R] How to create gridded data
>
> Thanks, Sarah's answer helps the question. Now how to change the gridded
> data back to DF1 format? I don't know how to name the format, thanks.
>
> On Tue, Nov 13, 2018 at 10:56 PM David L Carlson <mailto:dcarlson at tamu.edu>
> wrote:
> Sarah's answer is probably better depending on what you want to do with
> the resulting data, but here's a way to go from your original DF1 to DF2:
>
> > DF1 <- structure(list(latitude = c(45.5, 45.5, 45.5, 45.5, 46, 46, 46,
> +         46), longitude = c(110.5, 111, 111.5, 112, 110.5, 111, 111.5,
> +         112), Precip = c(3.2, 5, 1.8, 2, 6.1, 4.5, 7.8, 5.5)),
> +         class = "data.frame", row.names = c(NA, -8L))
> >
> # Convert to table with xtabs()
> > DF2 <- xtabs(Precip~latitude+longitude, DF1)
> >
>
> # Reverse the order of the latitudes
> > DF2 <- DF2[rev(rownames(DF2)), ]
> > DF2
>         longitude
> latitude 110.5 111 111.5 112
>     46     6.1 4.5   7.8 5.5
>     45.5   3.2 5.0   1.8 2.0
>
> # Convert to a data frame
> > DF2 <- as.data.frame.matrix(DF2)
> > DF2
>      110.5 111 111.5 112
> 46     6.1 4.5   7.8 5.5
> 45.5   3.2 5.0   1.8 2.0
>
> ----------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77843-4352
>
>
> -----Original Message-----
> From: R-help <mailto:r-help-bounces at r-project.org> On Behalf Of Sarah
> Goslee
> Sent: Tuesday, November 13, 2018 8:16 AM
> To: lily li <mailto:chocold12 at gmail.com>
> Cc: r-help <mailto:r-help at r-project.org>
> Subject: Re: [R] How to create gridded data
>
> If you want an actual spatial dataset, the best place to ask is R-sig-geo
>
> R has substantial capabilities for dealing with gridded spatial data,
> including in the sp, raster, and sf packages.
>
> Here's one approach, creating a SpatialGridDataFrame, which can be
> exported in any standard raster format using the rgdal package.
>
> DF2 <- DF1
> coordinates(DF2) <- ~longitude + latitude
> gridded(DF2) <- TRUE
> fullgrid(DF2) <- TRUE
>
> I recommend Roger Bivand's excellent book:
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.springer.com_us_book_9781461476177&d=DwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=vZqNKoDe8N1TzBzeK12g2oa0cBS8VD6NDCs-hUhvt5o&s=B73PwZQrdKUmM1ML2Y5zjaEz7xqkHlzBDCrhluogK2U&e=
>
> and there are abundant web tutorials.
>
> Sarah
> On Tue, Nov 13, 2018 at 2:22 AM lily li <mailto:chocold12 at gmail.com>
> wrote:
> >
> > Hi R users,
> >
> > I have a question about manipulating data. For example, I have DF1 as the
> > following, how to transform it to a gridded dataset DF2? In DF2, each
> value
> > Precip is an attribute of the corresponding grid cell. So DF2 is like a
> > spatial surface, and can be imported to ArcGIS. Thanks for your help.
> >
> > DF1
> > latitude   longitude   Precip
> > 45.5           110.5         3.2
> > 45.5           111            5.0
> > 45.5           111.5         1.8
> > 45.5           112            2.0
> > 46              110.5         6.1
> > 46              111            4.5
> > 46              111.5         7.8
> > 46              112            5.5
> > ...
> >
> >
> > DF2
> > 6.1   4.5   7.8   5.5
> > 3.2   5.0   1.8   2.0
> > ...
> >
>
>
> --
> Sarah Goslee (she/her)
>
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.numberwright.com&d=DwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=vZqNKoDe8N1TzBzeK12g2oa0cBS8VD6NDCs-hUhvt5o&s=qSosThG59aeSFYzVFf1e-YQGbuBKVbvgVi1z9nFm884&e=
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=vZqNKoDe8N1TzBzeK12g2oa0cBS8VD6NDCs-hUhvt5o&s=2pS9yFu5bpcRyCi1vX_OEDD2Ie8ZihvOQrkDQSNu8RM&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=vZqNKoDe8N1TzBzeK12g2oa0cBS8VD6NDCs-hUhvt5o&s=1brzEOjZ4EUr_llqwyO274DfJfsOpRBI-pmd-hp0WAQ&e=
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@||||@t@ @end|ng |rom pp@|net@||  Mon Mar 25 08:25:31 2019
From: m@||||@t@ @end|ng |rom pp@|net@|| (K. Elo)
Date: Mon, 25 Mar 2019 09:25:31 +0200
Subject: [R] Printing vectrix
In-Reply-To: <2562d8fb-f109-35ee-cf1e-18934ffa993b@hqu.edu.cn>
References: <2562d8fb-f109-35ee-cf1e-18934ffa993b@hqu.edu.cn>
Message-ID: <dbf7f2094561e79253b77a30710100efca010ecc.camel@pp.inet.fi>

Hi!

2019-03-25 kello 09:30 +0800, Steven Yen wrote:
> The second command is ugly. How can I print the 25 numbers into 2
> rows 
> of ten plus a helf row of 5? Thanks.

Something like this?

x<-1:25; for (i in seq(1,length(x),10))
print(x[i:ifelse((i+9)>length(x),length(x),i+9)])

HTH,
Kimmo


From m|@ojpm @end|ng |rom gm@||@com  Mon Mar 25 08:44:34 2019
From: m|@ojpm @end|ng |rom gm@||@com (John)
Date: Mon, 25 Mar 2019 15:44:34 +0800
Subject: [R] lag function row name with
Message-ID: <CABcx46D1rak0wd3vwEugSQuQJXfTf69ynwAzOWQukvL1TOxauQ@mail.gmail.com>

Hi,

   I have a dataset whose row names corresponds to months. When I apply lag
function (dplyr package) on this dataset, I get NAs with warning messages.
Is there any lag function that carries out the lag but keep the row names?
   I will have two datasets. The dates of the datasets are not exactly the
same, and I want to find out the correlation for the overlapping period.

   Thanks,


> temp
         oil95
1981M01 103.27
1981M02 107.92
1981M03 110.26
1981M04 110.26
1981M05 110.11
1981M06 109.93
1981M07 109.93
1981M08 109.93
1981M09 109.93
1981M10 109.93

> dplyr::lag(temp, 2)
        oil95
1981M01    NA
1981M02  <NA>
1981M03  <NA>
1981M04  <NA>
1981M05  <NA>
1981M06  <NA>
1981M07  <NA>
1981M08  <NA>
1981M09  <NA>
1981M10  <NA>
Warning message:
In format.data.frame(x, digits = digits, na.encode = FALSE) :
  corrupt data frame: columns will be truncated or padded with NAs
>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Mon Mar 25 09:16:10 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Mon, 25 Mar 2019 10:16:10 +0200
Subject: [R] lag function row name with
In-Reply-To: <CABcx46D1rak0wd3vwEugSQuQJXfTf69ynwAzOWQukvL1TOxauQ@mail.gmail.com>
References: <CABcx46D1rak0wd3vwEugSQuQJXfTf69ynwAzOWQukvL1TOxauQ@mail.gmail.com>
Message-ID: <CAGgJW74d85GfZLB7g4S+Wvh1u-pqYfihOv+o-VywZgQSGBxmkQ@mail.gmail.com>

Hi John,
dplyr::lag expects a vector. The following should work

dplyr::lag(temp[,1],2)

HTH,
Eric


On Mon, Mar 25, 2019 at 9:45 AM John <miaojpm at gmail.com> wrote:

> Hi,
>
>    I have a dataset whose row names corresponds to months. When I apply lag
> function (dplyr package) on this dataset, I get NAs with warning messages.
> Is there any lag function that carries out the lag but keep the row names?
>    I will have two datasets. The dates of the datasets are not exactly the
> same, and I want to find out the correlation for the overlapping period.
>
>    Thanks,
>
>
> > temp
>          oil95
> 1981M01 103.27
> 1981M02 107.92
> 1981M03 110.26
> 1981M04 110.26
> 1981M05 110.11
> 1981M06 109.93
> 1981M07 109.93
> 1981M08 109.93
> 1981M09 109.93
> 1981M10 109.93
>
> > dplyr::lag(temp, 2)
>         oil95
> 1981M01    NA
> 1981M02  <NA>
> 1981M03  <NA>
> 1981M04  <NA>
> 1981M05  <NA>
> 1981M06  <NA>
> 1981M07  <NA>
> 1981M08  <NA>
> 1981M09  <NA>
> 1981M10  <NA>
> Warning message:
> In format.data.frame(x, digits = digits, na.encode = FALSE) :
>   corrupt data frame: columns will be truncated or padded with NAs
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From choco|d12 @end|ng |rom gm@||@com  Mon Mar 25 09:30:11 2019
From: choco|d12 @end|ng |rom gm@||@com (lily li)
Date: Mon, 25 Mar 2019 16:30:11 +0800
Subject: [R] How to aggregate values in corresponding dataframes
Message-ID: <CAN5afy8LXCAM0RnvZQJS6KLYS1yBd-kW4H=YZzxsOqvB6y48MQ@mail.gmail.com>

Hi R users,

I have multiple dataframes in a directory, and with the same postfix
".txt". Each dataframe is a gridded file with just values, but each value
represents one grid cell. There are many years, and each year has 12
months, so there are many such files. For each year, I want to read the
corresponding year's files and add the values of the dataframes, but I
don't know how to do that. For example, the file names are
1990_file01.txt
1990_file02.txt
...
1990_file12.txt
1991_file01.txt
...
1991_file12.txt
...

And my code is like this:
f1990 = list.files("~/directory", pattern = "^1990(.*).txt$")
for(i in 1:length(f1990)){
r1990 = read.table(paste('~/directory',f1990[i],sep='/'),head=F)
}

Could you provide some help on this? Thanks very much.

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Mar 25 11:20:16 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 25 Mar 2019 10:20:16 +0000
Subject: [R] How to aggregate values in corresponding dataframes
In-Reply-To: <CAN5afy8LXCAM0RnvZQJS6KLYS1yBd-kW4H=YZzxsOqvB6y48MQ@mail.gmail.com>
References: <CAN5afy8LXCAM0RnvZQJS6KLYS1yBd-kW4H=YZzxsOqvB6y48MQ@mail.gmail.com>
Message-ID: <7a6ce5af-4aa4-6c14-1a9b-8fe07724e79e@sapo.pt>

Hello,

Maybe something like this?
Note that you *never* need to set header = FALSE, it already is the 
default of read.table. You would have to with read.csv.


old_dir <- setwd("~/directory")
f1990 <- list.files(pattern = "^199.*\\.txt$")
r1990 <- lapply(f1990, read.table)
setwd(old_dir)


Hope this helps,

Rui Barradas


?s 08:30 de 25/03/2019, lily li escreveu:
> Hi R users,
> 
> I have multiple dataframes in a directory, and with the same postfix
> ".txt". Each dataframe is a gridded file with just values, but each value
> represents one grid cell. There are many years, and each year has 12
> months, so there are many such files. For each year, I want to read the
> corresponding year's files and add the values of the dataframes, but I
> don't know how to do that. For example, the file names are
> 1990_file01.txt
> 1990_file02.txt
> ...
> 1990_file12.txt
> 1991_file01.txt
> ...
> 1991_file12.txt
> ...
> 
> And my code is like this:
> f1990 = list.files("~/directory", pattern = "^1990(.*).txt$")
> for(i in 1:length(f1990)){
> r1990 = read.table(paste('~/directory',f1990[i],sep='/'),head=F)
> }
> 
> Could you provide some help on this? Thanks very much.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Mar 25 11:22:32 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 25 Mar 2019 10:22:32 +0000
Subject: [R] How to aggregate values in corresponding dataframes
In-Reply-To: <7a6ce5af-4aa4-6c14-1a9b-8fe07724e79e@sapo.pt>
References: <CAN5afy8LXCAM0RnvZQJS6KLYS1yBd-kW4H=YZzxsOqvB6y48MQ@mail.gmail.com>
 <7a6ce5af-4aa4-6c14-1a9b-8fe07724e79e@sapo.pt>
Message-ID: <f3438d58-f91c-b0b8-d716-0bd319dad081@sapo.pt>

Sorry, I forgot to ask something.

When you say you want to add the df's values, what exactly do you mean? 
All of the values, by row, by column, what?

Rui Barradas

?s 10:20 de 25/03/2019, Rui Barradas escreveu:
> Hello,
> 
> Maybe something like this?
> Note that you *never* need to set header = FALSE, it already is the 
> default of read.table. You would have to with read.csv.
> 
> 
> old_dir <- setwd("~/directory")
> f1990 <- list.files(pattern = "^199.*\\.txt$")
> r1990 <- lapply(f1990, read.table)
> setwd(old_dir)
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> 
> ?s 08:30 de 25/03/2019, lily li escreveu:
>> Hi R users,
>>
>> I have multiple dataframes in a directory, and with the same postfix
>> ".txt". Each dataframe is a gridded file with just values, but each value
>> represents one grid cell. There are many years, and each year has 12
>> months, so there are many such files. For each year, I want to read the
>> corresponding year's files and add the values of the dataframes, but I
>> don't know how to do that. For example, the file names are
>> 1990_file01.txt
>> 1990_file02.txt
>> ...
>> 1990_file12.txt
>> 1991_file01.txt
>> ...
>> 1991_file12.txt
>> ...
>>
>> And my code is like this:
>> f1990 = list.files("~/directory", pattern = "^1990(.*).txt$")
>> for(i in 1:length(f1990)){
>> r1990 = read.table(paste('~/directory',f1990[i],sep='/'),head=F)
>> }
>>
>> Could you provide some help on this? Thanks very much.
>>
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From eu|erg@u@@r|em@nn @end|ng |rom gm@||@com  Mon Mar 25 07:35:30 2019
From: eu|erg@u@@r|em@nn @end|ng |rom gm@||@com (David Goldsmith)
Date: Sun, 24 Mar 2019 23:35:30 -0700
Subject: [R] Just confirming (?): No way to "export" a (Quartz) plot...
Message-ID: <CA+_2Pn0qRDnHVksmqOh3z_UEbg=7+8hm3w14JnbYFsyEFz4oNQ@mail.gmail.com>

...once I've got it the way I want it, (e.g., to a PNG)?  I.e., I either
have to create a PNG device and then plot to that, or use Mac's (screen)
"Grab" app?  Thanks.

DLG

> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS:
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK:
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] lattice_0.20-38 ISwR_2.0-7

loaded via a namespace (and not attached):
[1] compiler_3.5.2 tools_3.5.2    grid_3.5.2

	[[alternative HTML version deleted]]


From @monk|regu @end|ng |rom gm@||@com  Mon Mar 25 07:44:57 2019
From: @monk|regu @end|ng |rom gm@||@com (Amon kiregu)
Date: Mon, 25 Mar 2019 09:44:57 +0300
Subject: [R] simulation of PowerGARCH,Threshold GARCH,and GJR GARCH
Message-ID: <CAGa=WURoCyYh_R+U1+PGfeJqqcoEqXiwk0xZP9bRT5mhwA75iQ@mail.gmail.com>

what is the r code for  simulating PowerGARCH,Threshold GARCH,and GJR GARCH
in order to capture heteroscedasticity,volatility clustering,etc,,so that i
can have simulation of mean part and simulation on innovation part.
thanks

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Mon Mar 25 14:28:20 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Mon, 25 Mar 2019 15:28:20 +0200
Subject: [R] simulation of PowerGARCH,Threshold GARCH,and GJR GARCH
In-Reply-To: <CAGa=WURoCyYh_R+U1+PGfeJqqcoEqXiwk0xZP9bRT5mhwA75iQ@mail.gmail.com>
References: <CAGa=WURoCyYh_R+U1+PGfeJqqcoEqXiwk0xZP9bRT5mhwA75iQ@mail.gmail.com>
Message-ID: <CAGgJW75xkt331DNhQ56H+=1P7m3C0QfSu5JZLDmNY+qEirJDug@mail.gmail.com>

Doing a web search on
R CRAN GJR GARCH
brought up the rugarch package. The models you mentioned are discussed in
the documentation to that package

https://cran.r-project.org/web/packages/rugarch/vignettes/Introduction_to_the_rugarch_package.pdf




On Mon, Mar 25, 2019 at 2:06 PM Amon kiregu <amonkiregu at gmail.com> wrote:

> what is the r code for  simulating PowerGARCH,Threshold GARCH,and GJR GARCH
> in order to capture heteroscedasticity,volatility clustering,etc,,so that i
> can have simulation of mean part and simulation on innovation part.
> thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Mar 25 16:18:26 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 25 Mar 2019 18:18:26 +0300
Subject: [R] Just confirming (?): No way to "export" a (Quartz) plot...
In-Reply-To: <CA+_2Pn0qRDnHVksmqOh3z_UEbg=7+8hm3w14JnbYFsyEFz4oNQ@mail.gmail.com>
References: <CA+_2Pn0qRDnHVksmqOh3z_UEbg=7+8hm3w14JnbYFsyEFz4oNQ@mail.gmail.com>
Message-ID: <20190325181826.73633086@trisector>

On Sun, 24 Mar 2019 23:35:30 -0700
David Goldsmith <eulergaussriemann at gmail.com> wrote:

> No way to "export" a (Quartz) plot...
> ...once I've got it the way I want it, (e.g., to a PNG)

Well, there seems to be a quartz.save() function, but I cannot attest
to its well-behavedness, not having a Mac to test it on.

-- 
Best regards,
Ivan


From dc@r|@on @end|ng |rom t@mu@edu  Mon Mar 25 16:48:22 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Mon, 25 Mar 2019 15:48:22 +0000
Subject: [R] Printing vectrix
In-Reply-To: <dbf7f2094561e79253b77a30710100efca010ecc.camel@pp.inet.fi>
References: <2562d8fb-f109-35ee-cf1e-18934ffa993b@hqu.edu.cn>
 <dbf7f2094561e79253b77a30710100efca010ecc.camel@pp.inet.fi>
Message-ID: <98d878edbdfe4093a5e2d93bd9de088d@tamu.edu>

Here are a couple of other options. One changes the console width and the other pads the matrix before printing:

> # Print rows of 10 values
> x <- 1:25
> # Basically cheating and you don't get column numbers
> # wd depends on the size of the values and the number
> # of values
> wd <- (floor(log10(max(x)) + 2) * 10 + floor(log10(length(x))) + 3)
> options(width=wd)
> x
 [1]  1  2  3  4  5  6  7  8  9 10
[11] 11 12 13 14 15 16 17 18 19 20
[21] 21 22 23 24 25
> options(width=80)
> # Or print as a matrix
> rows <- ceiling(length(x) / 10)
> pad <- rows * 10 - length(x)
> x2 <- matrix(c(x, rep(NA, pad)), rows, 10, byrow=TRUE)
> print(x2, na.print="")
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    1    2    3    4    5    6    7    8    9    10
[2,]   11   12   13   14   15   16   17   18   19    20
[3,]   21   22   23   24   25                          

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of K. Elo
Sent: Monday, March 25, 2019 2:26 AM
To: r-help at r-project.org
Subject: Re: [R] Printing vectrix

Hi!

2019-03-25 kello 09:30 +0800, Steven Yen wrote:
> The second command is ugly. How can I print the 25 numbers into 2
> rows 
> of ten plus a helf row of 5? Thanks.

Something like this?

x<-1:25; for (i in seq(1,length(x),10))
print(x[i:ifelse((i+9)>length(x),length(x),i+9)])

HTH,
Kimmo

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dc@r|@on @end|ng |rom t@mu@edu  Mon Mar 25 17:05:38 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Mon, 25 Mar 2019 16:05:38 +0000
Subject: [R] How to create gridded data
In-Reply-To: <CAN5afy9bd1w9_Adxs3eqWsExaitAFtJGKhGsHC8zyGh0sG-6Rw@mail.gmail.com>
References: <CAN5afy9jzOCZDzvTuzXBzeTzi_aZLsnVnYM1VuBWe0e5ziUWsQ@mail.gmail.com>
 <CAM_vjumM-nNbQBp-6vRbFKqWHpap9SQxwPBqsfkAs0+-XNj_tA@mail.gmail.com>
 <966004e209f949dba2ccf205bcef9983@tamu.edu>
 <CAN5afy-Ts_3fUmTptkCXxTpxRVe5z-Jk07RmxtfO=JaV6phkNg@mail.gmail.com>
 <ca572c18ecad45eba5e74225c33f29f3@tamu.edu>
 <CAN5afy9bd1w9_Adxs3eqWsExaitAFtJGKhGsHC8zyGh0sG-6Rw@mail.gmail.com>
Message-ID: <2f5280cde77a48d4872ffd194c1a9dc7@tamu.edu>

If the grid is not full you need to identify the missing cells. As an example we can remove rows 3 and 6 from DF1:

> DF1miss <- DF1[-c(3, 6), ]
> DF2miss <- xtabs(Precip~latitude+longitude, DF1miss)
> DF2miss
        longitude
latitude 110.5 111 111.5 112
    45.5   3.2 5.0   0.0 2.0
    46     6.1 0.0   7.8 5.5

The table command inserts 0 for empty cells, but for your data zero is a valid value so we need to identify the missing values and replace them with NA:

> DF2mod <- xtabs(~latitude+longitude, DF1miss) < 1
> DF2mod
        longitude
latitude 110.5   111 111.5   112
    45.5 FALSE FALSE  TRUE FALSE
    46   FALSE  TRUE FALSE FALSE

> DF2miss[DF2mod] <- NA
> DF2miss   # Print table with blanks for missing values:
        longitude
latitude 110.5 111 111.5 112
    45.5   3.2 5.0       2.0
    46     6.1       7.8 5.5
> print(DF2miss, na.print=NA)  # Print table with <NA> for missing values:
        longitude
latitude 110.5  111 111.5 112
    45.5   3.2  5.0  <NA> 2.0
    46     6.1 <NA>   7.8 5.5

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352





From: lily li <chocold12 at gmail.com> 
Sent: Monday, March 25, 2019 1:18 AM
To: David L Carlson <dcarlson at tamu.edu>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] How to create gridded data

Now I have new question about this post. If the grid coordinates in DF1 are not complete, i.e. there are missing coordinates, how to fill these with -99 in the exported DF2? Thanks.

On Thu, Nov 15, 2018 at 10:57 PM David L Carlson <mailto:dcarlson at tamu.edu> wrote:
It would depend on the format of the gridded data. Assuming it is a data frame like DF2 in my earlier answer, you just reverse the steps:

> DF2
? ? ?110.5 111 111.5 112
46? ? ?6.1 4.5? ?7.8 5.5
45.5? ?3.2 5.0? ?1.8 2.0

> DF3 <- data.frame(as.table(as.matrix(DF2)))
? Var1? Var2 Freq
1? ?46 110.5? 6.1
2 45.5 110.5? 3.2
3? ?46? ?111? 4.5
4 45.5? ?111? 5.0
5? ?46 111.5? 7.8
6 45.5 111.5? 1.8
7? ?46? ?112? 5.5
8 45.5? ?112? 2.0

But the latitude and longitude get converted to factors and we lose the column names:

> DF3 <- data.frame(as.table(as.matrix(DF2)))
> colnames(DF3) <- c("latitude", "longitude", "Precip")
> DF3$latitude <- as.numeric(as.character(DF3$latitude))
> DF3$longitude <- as.numeric(as.character(DF3$longitude))

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352

From: lily li <mailto:chocold12 at gmail.com> 
Sent: Tuesday, November 13, 2018 10:50 PM
To: David L Carlson <mailto:dcarlson at tamu.edu>
Cc: Sarah Goslee <mailto:sarah.goslee at gmail.com>; R mailing list <mailto:r-help at r-project.org>
Subject: Re: [R] How to create gridded data

Thanks, Sarah's answer helps the question. Now how to change the gridded data back to DF1 format? I don't know how to name the format, thanks.

On Tue, Nov 13, 2018 at 10:56 PM David L Carlson <mailto:mailto:dcarlson at tamu.edu> wrote:
Sarah's answer is probably better depending on what you want to do with the resulting data, but here's a way to go from your original DF1 to DF2:

> DF1 <- structure(list(latitude = c(45.5, 45.5, 45.5, 45.5, 46, 46, 46, 
+? ? ? ? ?46), longitude = c(110.5, 111, 111.5, 112, 110.5, 111, 111.5, 
+? ? ? ? ?112), Precip = c(3.2, 5, 1.8, 2, 6.1, 4.5, 7.8, 5.5)),
+? ? ? ? ?class = "data.frame", row.names = c(NA, -8L))
> 
# Convert to table with xtabs()
> DF2 <- xtabs(Precip~latitude+longitude, DF1)
> 

# Reverse the order of the latitudes
> DF2 <- DF2[rev(rownames(DF2)), ]
> DF2
? ? ? ? longitude
latitude 110.5 111 111.5 112
? ? 46? ? ?6.1 4.5? ?7.8 5.5
? ? 45.5? ?3.2 5.0? ?1.8 2.0

# Convert to a data frame
> DF2 <- as.data.frame.matrix(DF2)
> DF2
? ? ?110.5 111 111.5 112
46? ? ?6.1 4.5? ?7.8 5.5
45.5? ?3.2 5.0? ?1.8 2.0

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352


-----Original Message-----
From: R-help <mailto:mailto:r-help-bounces at r-project.org> On Behalf Of Sarah Goslee
Sent: Tuesday, November 13, 2018 8:16 AM
To: lily li <mailto:mailto:chocold12 at gmail.com>
Cc: r-help <mailto:mailto:r-help at r-project.org>
Subject: Re: [R] How to create gridded data

If you want an actual spatial dataset, the best place to ask is R-sig-geo

R has substantial capabilities for dealing with gridded spatial data,
including in the sp, raster, and sf packages.

Here's one approach, creating a SpatialGridDataFrame, which can be
exported in any standard raster format using the rgdal package.

DF2 <- DF1
coordinates(DF2) <- ~longitude + latitude
gridded(DF2) <- TRUE
fullgrid(DF2) <- TRUE

I recommend Roger Bivand's excellent book:
https://urldefense.proofpoint.com/v2/url?u=https-3A__www.springer.com_us_book_9781461476177&d=DwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=vZqNKoDe8N1TzBzeK12g2oa0cBS8VD6NDCs-hUhvt5o&s=B73PwZQrdKUmM1ML2Y5zjaEz7xqkHlzBDCrhluogK2U&e=

and there are abundant web tutorials.

Sarah
On Tue, Nov 13, 2018 at 2:22 AM lily li <mailto:mailto:chocold12 at gmail.com> wrote:
>
> Hi R users,
>
> I have a question about manipulating data. For example, I have DF1 as the
> following, how to transform it to a gridded dataset DF2? In DF2, each value
> Precip is an attribute of the corresponding grid cell. So DF2 is like a
> spatial surface, and can be imported to ArcGIS. Thanks for your help.
>
> DF1
> latitude? ?longitude? ?Precip
> 45.5? ? ? ? ? ?110.5? ? ? ? ?3.2
> 45.5? ? ? ? ? ?111? ? ? ? ? ? 5.0
> 45.5? ? ? ? ? ?111.5? ? ? ? ?1.8
> 45.5? ? ? ? ? ?112? ? ? ? ? ? 2.0
> 46? ? ? ? ? ? ? 110.5? ? ? ? ?6.1
> 46? ? ? ? ? ? ? 111? ? ? ? ? ? 4.5
> 46? ? ? ? ? ? ? 111.5? ? ? ? ?7.8
> 46? ? ? ? ? ? ? 112? ? ? ? ? ? 5.5
> ...
>
>
> DF2
> 6.1? ?4.5? ?7.8? ?5.5
> 3.2? ?5.0? ?1.8? ?2.0
> ...
>


-- 
Sarah Goslee (she/her)
https://urldefense.proofpoint.com/v2/url?u=http-3A__www.numberwright.com&d=DwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=vZqNKoDe8N1TzBzeK12g2oa0cBS8VD6NDCs-hUhvt5o&s=qSosThG59aeSFYzVFf1e-YQGbuBKVbvgVi1z9nFm884&e=

______________________________________________
mailto:mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=vZqNKoDe8N1TzBzeK12g2oa0cBS8VD6NDCs-hUhvt5o&s=2pS9yFu5bpcRyCi1vX_OEDD2Ie8ZihvOQrkDQSNu8RM&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=ODFT-G5SujMiGrKuoJJjVg&r=veMGHMCNZShld-KX-bIj4jRE_tP9ojUvB_Lqp0ieSdk&m=vZqNKoDe8N1TzBzeK12g2oa0cBS8VD6NDCs-hUhvt5o&s=1brzEOjZ4EUr_llqwyO274DfJfsOpRBI-pmd-hp0WAQ&e=
and provide commented, minimal, self-contained, reproducible code.

From ruetten@uer @end|ng |rom @ow|@un|-k|@de  Fri Mar 22 12:41:30 2019
From: ruetten@uer @end|ng |rom @ow|@un|-k|@de (=?iso-8859-1?Q?Tobias_R=FCttenauer?=)
Date: Fri, 22 Mar 2019 12:41:30 +0100
Subject: [R] [R-pkgs] New package feisr: Fixed effects individual slope
 models
Message-ID: <001d01d4e0a4$31a9d8a0$94fd89e0$@sowi.uni-kl.de>

Dear R users,

Are you worried about the parallel trends assumption in your panel
regression? Use fixed effects individual slope models, controlling for
heterogeneous trends!

I am very pleased to announce that the package feisr is now available on
CRAN!

CRAN: https://cran.r-project.org/package=feisr
GitHub: https://github.com/ruettenauer/feisr

The packages feisr provides a function to estimate fixed effects individual
slope (FEIS) models. FEIS models constitute a more general version of the
often used conventional fixed effects (FE) panel models. In contrast to
conventional fixed effects models, data are not person ?demeaned?, but
'detrended? by the predicted individual slope of each person or group, which
relaxes the assumptions of parallel trends between treated and untreated
groups.

We are happy about any comments or feedback!

Best regards,
Tobias


Tobias R?ttenauer
Department of Social Sciences
TU Kaiserslautern
Erwin-Schroedinger-Str. 57
67663 Kaiserslautern

ruettenauer at sowi.uni-kl.de
Tel.: +49 631 205 5785

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From eu|erg@u@@r|em@nn @end|ng |rom gm@||@com  Mon Mar 25 16:33:26 2019
From: eu|erg@u@@r|em@nn @end|ng |rom gm@||@com (David Goldsmith)
Date: Mon, 25 Mar 2019 08:33:26 -0700
Subject: [R] Just confirming (?): No way to "export" a (Quartz) plot...
In-Reply-To: <20190325181826.73633086@trisector>
References: <CA+_2Pn0qRDnHVksmqOh3z_UEbg=7+8hm3w14JnbYFsyEFz4oNQ@mail.gmail.com>
 <20190325181826.73633086@trisector>
Message-ID: <CA+_2Pn3kpLR8H6Bk1Rbagu10KJjisPu_H9=HyRns4-mFtTin7Q@mail.gmail.com>

Excellent, thanks!  (It just goes to show how hard it can be to find
something when you don't know precisely what to look for.)

DLG

On Mon, Mar 25, 2019 at 8:18 AM Ivan Krylov <krylov.r00t at gmail.com> wrote:

> On Sun, 24 Mar 2019 23:35:30 -0700
> David Goldsmith <eulergaussriemann at gmail.com> wrote:
>
> > No way to "export" a (Quartz) plot...
> > ...once I've got it the way I want it, (e.g., to a PNG)
>
> Well, there seems to be a quartz.save() function, but I cannot attest
> to its well-behavedness, not having a Mac to test it on.
>
> --
> Best regards,
> Ivan
>

	[[alternative HTML version deleted]]


From vrm4 @end|ng |rom cdc@gov  Mon Mar 25 17:10:18 2019
From: vrm4 @end|ng |rom cdc@gov (Yuan, Keming (CDC/DDNID/NCIPC/DVP))
Date: Mon, 25 Mar 2019 16:10:18 +0000
Subject: [R] loop through columns in a data frame
Message-ID: <fda0010406494d6481b38a65ec4cfbfb@cdc.gov>

Hi All,

I have a data frame with variable names like A_le, A_me, B_le, B_me, C_le, C_me....
if A_le=1 or A_me=1 then  I need to create a new column A_new=1. Same operation to create columns B_new, C_new...
Does anyone know how to use loop (or other methods) to create new columns? In SAS, I can use array to get it done. But I don't know how to do it in R.

Thanks,

Keming Yuan
CDC


	[[alternative HTML version deleted]]


From jho|tm@n @end|ng |rom gm@||@com  Mon Mar 25 19:06:42 2019
From: jho|tm@n @end|ng |rom gm@||@com (jim holtman)
Date: Mon, 25 Mar 2019 11:06:42 -0700
Subject: [R] loop through columns in a data frame
In-Reply-To: <fda0010406494d6481b38a65ec4cfbfb@cdc.gov>
References: <fda0010406494d6481b38a65ec4cfbfb@cdc.gov>
Message-ID: <CAAxdm-7dfjJdty1CE798shhZzzH9DRFH96gpub85i3Xbkn18bQ@mail.gmail.com>

R Notebook

You forgot to provide what your test data looks like. For example, are all
the columns a single letter followed by ?_" as the name, or are there
longer names? Are there always matched pairs (?le? and ?me?) or can singles
occur?
Hide

library(tidyverse)# create some data
test <- tibble(a_le = sample(3, 10, TRUE),
               a_me = sample(3, 10, TRUE),
               b_le = sample(3, 10, TRUE),
               b_me = sample(3, 10, TRUE),
               long_le = sample(3, 10, TRUE),
               long_me = sample(3, 10, TRUE),
               short_le = sample(3, 10, TRUE)
)

So get the names of the columns that contain ?le? or ?me? and group them
together for processing
Hide

col_names <- grep("_(le|me)$", names(test), value = TRUE)
group <- tibble(id = str_remove(col_names, "_.*"), col = col_names)
result <- group %>%
  group_by(id) %>%
  do(tibble(x = rowSums(test[, .$col] == 1)))# add new columns backfor
(i in split(result, result$id)){
  test[, paste0(i$id[1], "_new")] <- as.integer(i$x > 0)
}
test

a_le
<int>
a_me
<int>
b_le
<int>
b_me
<int>
long_le
<int>
long_me
<int>
short_le
<int>
a_new
<int>
b_new
<int>
long_new
<int>
3 1 2 3 1 2 2 1 0 1
2 3 3 2 1 1 1 0 0 1
3 2 3 2 1 3 3 0 0 1
2 3 1 3 3 1 2 0 1 1
1 1 2 1 1 2 2 1 1 1
3 3 3 1 1 1 1 0 1 1
1 2 1 2 2 2 2 1 1 0
1 3 2 3 1 1 3 1 0 1
3 1 1 1 3 3 2 1 1 0
1 1 1 2 3 3 3 1 1 0
1-10 of 10 rows | 1-10 of 11 columns

Jim Holtman
*Data Munger Guru*


*What is the problem that you are trying to solve?Tell me what you want to
do, not how you want to do it.*


On Mon, Mar 25, 2019 at 10:08 AM Yuan, Keming (CDC/DDNID/NCIPC/DVP) via
R-help <r-help at r-project.org> wrote:

> Hi All,
>
> I have a data frame with variable names like A_le, A_me, B_le, B_me, C_le,
> C_me....
> if A_le=1 or A_me=1 then  I need to create a new column A_new=1. Same
> operation to create columns B_new, C_new...
> Does anyone know how to use loop (or other methods) to create new columns?
> In SAS, I can use array to get it done. But I don't know how to do it in R.
>
> Thanks,
>
> Keming Yuan
> CDC
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Mon Mar 25 19:15:28 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 25 Mar 2019 11:15:28 -0700
Subject: [R] loop through columns in a data frame
In-Reply-To: <fda0010406494d6481b38a65ec4cfbfb@cdc.gov>
References: <fda0010406494d6481b38a65ec4cfbfb@cdc.gov>
Message-ID: <CAGxFJbTEk1dpzq-cYD2SWoXeOk5Sc4V3DubrfG4+-KVyvr0n-w@mail.gmail.com>

"Does anyone know how to use loop (or other methods) to create new columns?
In SAS, I can use array to get it done. But I don't know how to do it in R."

Yup. Practically all users of R know how, as this is entirely elementary.
You will too if you make the effort to go through a basic R tutorial, of
which there are many on the web (and one shipped with R).

Cheers,
Bert

On Mon, Mar 25, 2019 at 10:08 AM Yuan, Keming (CDC/DDNID/NCIPC/DVP) via
R-help <r-help at r-project.org> wrote:

> Hi All,
>
> I have a data frame with variable names like A_le, A_me, B_le, B_me, C_le,
> C_me....
> if A_le=1 or A_me=1 then  I need to create a new column A_new=1. Same
> operation to create columns B_new, C_new...
> Does anyone know how to use loop (or other methods) to create new columns?
> In SAS, I can use array to get it done. But I don't know how to do it in R.
>
> Thanks,
>
> Keming Yuan
> CDC
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rn@@w@g|ey @end|ng |rom gm@||@com  Tue Mar 26 00:30:57 2019
From: m@rn@@w@g|ey @end|ng |rom gm@||@com (Marna Wagley)
Date: Mon, 25 Mar 2019 16:30:57 -0700
Subject: [R] Monte Carlo simulation for ratio and its CI
Message-ID: <CAMwU6B1J8RrQK0nWjm5PgX8_-2pv2VM6JAkNfZg=VMWnFRcnLg@mail.gmail.com>

Hi R User,
I was trying to calculate ratios with confidence interval using Monte Carlo
simulation but I could not figure it out.
Here is the example of my data (see below), I want to calculate ratios
(dat$v1/dat$v3 & dat$v2/dat$v3) and its confidence intervals using a 100
randomly selected data sets.
Could you please give me your suggestions how I can estimate ratios with
CI?
I will be very grateful to you.
Sincerely,

MW
---
dat<-structure(list(v1 = c(NA, TRUE, TRUE, TRUE, TRUE, TRUE, NA, TRUE,

NA, NA, TRUE, TRUE, TRUE, TRUE, NA, NA, TRUE, TRUE), v2 = c(TRUE,

NA, NA, NA, NA, TRUE, NA, NA, TRUE, TRUE, NA, TRUE, TRUE, NA,

NA, TRUE, TRUE, NA), v3 = c(TRUE, TRUE, NA, TRUE, TRUE, NA, NA,

TRUE, TRUE, NA, NA, TRUE, TRUE, TRUE, NA, NA, TRUE, NA)), .Names = c("v1",

"v2", "v3"), class = "data.frame", row.names = c(NA, -18L))


ratio1<-length(which(dat$v1 == "TRUE"))/length(which(dat$v3 == "TRUE"))

ratio2<-length(which(dat$v2 == "TRUE"))/length(which(dat$v3 == "TRUE"))


Thanks

	[[alternative HTML version deleted]]


From mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu  Tue Mar 26 00:48:55 2019
From: mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu (Matthew)
Date: Mon, 25 Mar 2019 19:48:55 -0400
Subject: [R] 
 creating a dataframe with full_join and looping over a list of
 lists.
In-Reply-To: <CA+8X3fWrSzGHRxCDk2LJ2SH5=ysaWpeAYZUP7LHdJAjTxCGvTw@mail.gmail.com>
References: <9d5bf04e-7845-f6b2-95c9-5560d1c0785f@molbio.mgh.harvard.edu>
 <CA+8X3fWMV93TkNLN_izMzsmVd2LXGt=32MJVjmyeoRPBgh_5HA@mail.gmail.com>
 <804f0547-28b2-2f7a-7f71-9c94305f0c88@molbio.mgh.harvard.edu>
 <CA+8X3fWrSzGHRxCDk2LJ2SH5=ysaWpeAYZUP7LHdJAjTxCGvTw@mail.gmail.com>
Message-ID: <5ac918c4-2c5f-be00-533b-0d2a98b0c358@molbio.mgh.harvard.edu>

This is fantastic !? It was exactly what I was looking for. It is part 
of a larger Shiny app, so difficult to provide a working example as part 
of the post, and after figuring out how your code works ( I am an R 
novice), I made a couple of small tweaks and it works great !? Thank you 
very much, Jim, for the work you put into this.

Matthew

On 3/21/2019 11:01 PM, Jim Lemon wrote:
>          External Email - Use Caution
>
> Hi Matthew,
> Remember, keep it on the list so that people know the status of the request.
> I couldn't get this to work with the "_source_info_" variable. It
> seems to be unreadable as a variable name. So, this _may_ be what you
> want. I don't know if it can be done with "merge" and I don't know the
> function "full_join".
>
> WRKY8_colamp_a<-as.character(
>   c("AT1G02920","AT1G06135","AT1G07160","AT1G11925","AT1G14540","AT1G16150",
>   "AT1G21120","AT1G26380","AT1G26410","AT1G35210","AT1G49000","AT1G51920",
>   "AT1G56250","AT1G66090","AT1G72520","AT1G80840","AT2G02010","AT2G18690",
>   "AT2G30750","AT2G39200","AT2G43620","AT3G01830","AT3G54150","AT3G55840",
>   "AT4G03460","AT4G11470","AT4G11890","AT4G14370","AT4G15417","AT4G15975",
>   "AT4G31940","AT4G35180","AT5G01540","AT5G05300","AT5G11140","AT5G24110",
>   "AT5G25250","AT5G36925","AT5G46295","AT5G64750","AT5G64905","AT5G66020"))
>
> bHLH10_col_a<-as.character(c("AT1G72520","AT3G55840","AT5G20230","AT5G64750"))
>
> bHLH10_colamp_a<-as.character(
>   c("AT1G01560","AT1G02920","AT1G16420","AT1G17147","AT1G35210","AT1G51620",
>   "AT1G57630","AT1G72520","AT2G18690","AT2G19190","AT2G40180","AT2G44370",
>   "AT3G23250","AT3G55840","AT4G03460","AT4G04480","AT4G04540","AT4G08555",
>   "AT4G11470","AT4G11890","AT4G16820","AT4G23280","AT4G35180","AT5G01540",
>   "AT5G05300","AT5G20230","AT5G22530","AT5G24110","AT5G56960","AT5G57010",
>   "AT5G57220","AT5G64750","AT5G66020"))
>
> # let myenter be the sorted superset
> myenter<-
>   sort(unique(c(WRKY8_colamp_a,bHLH10_col_a,bHLH10_colamp_a)))
>
> splice<-function(x,y) {
>   nx<-length(x)
>   ny<-length(y)
>   newy<-rep(NA,nx)
>   if(ny) {
>    yi<-1
>    for(xi in 1:nx) {
>     if(x[xi] == y[yi]) {
>      newy[xi]<-y[yi]
>      yi<-yi+1
>     }
>     if(yi>ny) break()
>    }
>   }
>   return(newy)
> }
>
> comatgs<-list(WRKY8_colamp_a=WRKY8_colamp_a,
>   bHLH10_col_a=bHLH10_col_a,bHLH10_colamp_a=bHLH10_colamp_a)
> mydf3<-data.frame(myenter,stringsAsFactors=FALSE)
> for(j in 1:length(comatgs)) {
>   tmp<-data.frame(splice(myenter,sort(comatgs[[j]])))
>   names(tmp)<-names(comatgs)[j]
>   mydf3<-cbind(mydf3,tmp)
> }
>
> Jim
>
> On Fri, Mar 22, 2019 at 10:29 AM Matthew
> <mccormack at molbio.mgh.harvard.edu> wrote:
>> Hi Jim,
>>
>>      Thanks for the reply.  That was pretty dumb of me.  I took that out of the loop.
>>
>> comatgs is longer than this but here is a sample of 4 of 569 elements:
>>
>> $WRKY8_colamp_a
>>   [1] "AT1G02920" "AT1G06135" "AT1G07160" "AT1G11925" "AT1G14540" "AT1G16150" "AT1G21120"
>>   [8] "AT1G26380" "AT1G26410" "AT1G35210" "AT1G49000" "AT1G51920" "AT1G56250" "AT1G66090"
>> [15] "AT1G72520" "AT1G80840" "AT2G02010" "AT2G18690" "AT2G30750" "AT2G39200" "AT2G43620"
>> [22] "AT3G01830" "AT3G54150" "AT3G55840" "AT4G03460" "AT4G11470" "AT4G11890" "AT4G14370"
>> [29] "AT4G15417" "AT4G15975" "AT4G31940" "AT4G35180" "AT5G01540" "AT5G05300" "AT5G11140"
>> [36] "AT5G24110" "AT5G25250" "AT5G36925" "AT5G46295" "AT5G64750" "AT5G64905" "AT5G66020"
>>
>> $`_source_info_`
>> character(0)
>>
>> $bHLH10_col_a
>> [1] "AT1G72520" "AT3G55840" "AT5G20230" "AT5G64750"
>>
>> $bHLH10_colamp_a
>>   [1] "AT1G01560" "AT1G02920" "AT1G16420" "AT1G17147" "AT1G35210" "AT1G51620" "AT1G57630"
>>   [8] "AT1G72520" "AT2G18690" "AT2G19190" "AT2G40180" "AT2G44370" "AT3G23250" "AT3G55840"
>> [15] "AT4G03460" "AT4G04480" "AT4G04540" "AT4G08555" "AT4G11470" "AT4G11890" "AT4G16820"
>> [22] "AT4G23280" "AT4G35180" "AT5G01540" "AT5G05300" "AT5G20230" "AT5G22530" "AT5G24110"
>> [29] "AT5G56960" "AT5G57010" "AT5G57220" "AT5G64750" "AT5G66020"
>>
>>
>>        I have been thinking of something like this:
>>
>> lenmyen <- length(myenter)                        # get length of longest list
>> length(comatgs[[j]) <- lenmyen                   # make each list length of myenter
>> atglsts <- as.data.frame(comatgs[j])           # create dataframe
>> colnames(atglsts) <- "AGI"                         # rename column to 'AGI'
>>
>> mydf3 <- full_join(mydf3, atglsts, by = "AGI"    # full_join
>>
>> Matthew
>>
>> On 3/21/2019 7:12 PM, Jim Lemon wrote:
>>
>>          External Email - Use Caution
>>
>> Hi Matthew,
>> First thing, don't put:
>>
>> mydf3 <- data.frame(myenter)
>>
>> inside your loop, otherwise you will reset the value of mydf3 each
>> time and end up with only "myenter" and the final list. Without some
>> idea of the contents of comatgs, it is difficult to suggest a way to
>> get what you want.
>>
>> Jim
>>
>> On Fri, Mar 22, 2019 at 8:16 AM Matthew
>> <mccormack at molbio.mgh.harvard.edu> wrote:
>>
>>     My apologies, my first e-mail formatted very poorly when sent, so I am trying again with something I hope will be less confusing.
>>
>> I have been trying create a dataframe by looping through a list of lists,
>> and using dplyr's full_join so as to keep common elements on the same row.
>> But, I have a couple of problems.
>>
>> 1) The lists have different numbers of elements.
>>
>> 2) In the final dataframe, I would like the column names to be the names
>> of the lists.
>>
>> Is it possible ?
>>
>> Code: *for(j in avector){****mydf3 <- data.frame(myenter) ****atglsts <-
>> as.data.frame(comatgs[j]) ****mydf3 <- full_join(mydf3, atglsts) ****}*
>> Explanation: # Start out with a list, myenter, to dataframe. mydf3 now
>> has 1 column. # This first column will be the longest column in the
>> final mydf3. # Loop through a list of lists, comatgs, and with each loop
>> a particular list # is made into a dataframe of one column, atglsts. #
>> The name of the column is the name of the list. # Each atglsts dataframe
>> has a different number of elements. # What I want to do, is to add the
>> newly made dataframe, atglsts, as a # new column of the data frame,
>> mydf3 using full_join # in order to keep common elements on the same
>> row. # I could rename the colname to 'AGI' so that I can join by 'AGI',
>> # but then I would lose the name of the list. # In the final dataframe,
>> I want to know the name of the original list # the column was made from. Matthew
>>
>>
>>
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Tue Mar 26 00:50:45 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 25 Mar 2019 16:50:45 -0700
Subject: [R] Monte Carlo simulation for ratio and its CI
In-Reply-To: <CAMwU6B1J8RrQK0nWjm5PgX8_-2pv2VM6JAkNfZg=VMWnFRcnLg@mail.gmail.com>
References: <CAMwU6B1J8RrQK0nWjm5PgX8_-2pv2VM6JAkNfZg=VMWnFRcnLg@mail.gmail.com>
Message-ID: <CAGxFJbSvPrh==QGVY-EB46SkpP0j7fq8QWWnrR2xWVvN=pcWSg@mail.gmail.com>

> ratio1 <- with(dat, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
> ratio1
[1] 1.2

It looks like you should spend some more time with an R tutorial or two.
This is basic stuff (if I understand what you wanted correctly).

Also, this is not how a "confidence interval" should be calculated, but
that is another off topic discussion for which stats.stackexchange.com is a
more appropriate venue.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Mar 25, 2019 at 4:31 PM Marna Wagley <marna.wagley at gmail.com> wrote:

> Hi R User,
> I was trying to calculate ratios with confidence interval using Monte Carlo
> simulation but I could not figure it out.
> Here is the example of my data (see below), I want to calculate ratios
> (dat$v1/dat$v3 & dat$v2/dat$v3) and its confidence intervals using a 100
> randomly selected data sets.
> Could you please give me your suggestions how I can estimate ratios with
> CI?
> I will be very grateful to you.
> Sincerely,
>
> MW
> ---
> dat<-structure(list(v1 = c(NA, TRUE, TRUE, TRUE, TRUE, TRUE, NA, TRUE,
>
> NA, NA, TRUE, TRUE, TRUE, TRUE, NA, NA, TRUE, TRUE), v2 = c(TRUE,
>
> NA, NA, NA, NA, TRUE, NA, NA, TRUE, TRUE, NA, TRUE, TRUE, NA,
>
> NA, TRUE, TRUE, NA), v3 = c(TRUE, TRUE, NA, TRUE, TRUE, NA, NA,
>
> TRUE, TRUE, NA, NA, TRUE, TRUE, TRUE, NA, NA, TRUE, NA)), .Names = c("v1",
>
> "v2", "v3"), class = "data.frame", row.names = c(NA, -18L))
>
>
> ratio1<-length(which(dat$v1 == "TRUE"))/length(which(dat$v3 == "TRUE"))
>
> ratio2<-length(which(dat$v2 == "TRUE"))/length(which(dat$v3 == "TRUE"))
>
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From choco|d12 @end|ng |rom gm@||@com  Tue Mar 26 03:40:26 2019
From: choco|d12 @end|ng |rom gm@||@com (lily li)
Date: Tue, 26 Mar 2019 10:40:26 +0800
Subject: [R] How to aggregate values in corresponding dataframes
In-Reply-To: <f3438d58-f91c-b0b8-d716-0bd319dad081@sapo.pt>
References: <CAN5afy8LXCAM0RnvZQJS6KLYS1yBd-kW4H=YZzxsOqvB6y48MQ@mail.gmail.com>
 <7a6ce5af-4aa4-6c14-1a9b-8fe07724e79e@sapo.pt>
 <f3438d58-f91c-b0b8-d716-0bd319dad081@sapo.pt>
Message-ID: <CAN5afy-D6f5cYDXE=PRZnwX6adoWvscjdxOCd4cgqFk8ZrYKCQ@mail.gmail.com>

Each dataframe is a gridded file, so I want to add the values at
corresponding grid cells from all the dataframes.

On Mon, Mar 25, 2019 at 6:22 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Sorry, I forgot to ask something.
>
> When you say you want to add the df's values, what exactly do you mean?
> All of the values, by row, by column, what?
>
> Rui Barradas
>
> ?s 10:20 de 25/03/2019, Rui Barradas escreveu:
> > Hello,
> >
> > Maybe something like this?
> > Note that you *never* need to set header = FALSE, it already is the
> > default of read.table. You would have to with read.csv.
> >
> >
> > old_dir <- setwd("~/directory")
> > f1990 <- list.files(pattern = "^199.*\\.txt$")
> > r1990 <- lapply(f1990, read.table)
> > setwd(old_dir)
> >
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> >
> > ?s 08:30 de 25/03/2019, lily li escreveu:
> >> Hi R users,
> >>
> >> I have multiple dataframes in a directory, and with the same postfix
> >> ".txt". Each dataframe is a gridded file with just values, but each
> value
> >> represents one grid cell. There are many years, and each year has 12
> >> months, so there are many such files. For each year, I want to read the
> >> corresponding year's files and add the values of the dataframes, but I
> >> don't know how to do that. For example, the file names are
> >> 1990_file01.txt
> >> 1990_file02.txt
> >> ...
> >> 1990_file12.txt
> >> 1991_file01.txt
> >> ...
> >> 1991_file12.txt
> >> ...
> >>
> >> And my code is like this:
> >> f1990 = list.files("~/directory", pattern = "^1990(.*).txt$")
> >> for(i in 1:length(f1990)){
> >> r1990 = read.table(paste('~/directory',f1990[i],sep='/'),head=F)
> >> }
> >>
> >> Could you provide some help on this? Thanks very much.
> >>
> >>     [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rn@@w@g|ey @end|ng |rom gm@||@com  Tue Mar 26 13:57:17 2019
From: m@rn@@w@g|ey @end|ng |rom gm@||@com (Marna Wagley)
Date: Tue, 26 Mar 2019 05:57:17 -0700
Subject: [R] Monte Carlo simulation for ratio and its CI
In-Reply-To: <CAGxFJbSvPrh==QGVY-EB46SkpP0j7fq8QWWnrR2xWVvN=pcWSg@mail.gmail.com>
References: <CAMwU6B1J8RrQK0nWjm5PgX8_-2pv2VM6JAkNfZg=VMWnFRcnLg@mail.gmail.com>
 <CAGxFJbSvPrh==QGVY-EB46SkpP0j7fq8QWWnrR2xWVvN=pcWSg@mail.gmail.com>
Message-ID: <CAMwU6B35BbPi6E6bD0XAhRMqba-27dC_3WU7Qu2VZdmQ=DDF5Q@mail.gmail.com>

Dear Bert,
Thank you very much for the response.
I did it manually but I could not put them in a loop so that I created the
table manually with selecting the rows randomly several times. Here what I
have done so far, please find it. I want to create the table 100 times and
calculate its mean and CI from those 100 values. If anyone can give me some
hint to make a loop, that would be great. I am very grateful with your help.
Thanks,

library(dplyr)

library(plyr)

dat<-structure(list(v1 = c(NA, TRUE, TRUE, TRUE, TRUE, TRUE, NA, TRUE,

NA, NA, TRUE, TRUE, TRUE, TRUE, NA, NA, TRUE, TRUE), v2 = c(TRUE,

NA, NA, NA, NA, TRUE, NA, NA, TRUE, TRUE, NA, TRUE, TRUE, NA,

NA, TRUE, TRUE, NA), v3 = c(TRUE, TRUE, NA, TRUE, TRUE, NA, NA,

TRUE, TRUE, NA, NA, TRUE, TRUE, TRUE, NA, NA, TRUE, NA)), .Names = c("v1",

"v2", "v3"), class = "data.frame", row.names = c(NA, -18L))


ratio1 <- with(dat, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))

ratio2 <- with(dat, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))

#

A1<-sample_n(dat1, 16)# created a table with selecting a 16 sample size
(rows)

A1.ratio1<-with(A1, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))

A1.ratio2 <- with(A1, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))

A1.Table<-data.frame(Ratio1=A1.ratio1, Ratio2=A1.ratio2)

#

A2<-sample_n(dat1, 16)

A2.ratio1<-with(A2, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))

A2.ratio2 <- with(A2, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))

A2.Table<-data.frame(Ratio1=A2.ratio1, Ratio2=A2.ratio2)

#

A3<-sample_n(dat1, 16)

A3.ratio1<-with(A3, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))

A3.ratio2 <- with(A3, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))

A3.Table<-data.frame(Ratio1=A3.ratio1, Ratio2=A3.ratio2)

#

##..............

# I was thinking to repeat this procedure 100 times and calculate the ratio

A100<-sample_n(dat1, 16)

A100.ratio1<-with(A100, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))

A100.ratio2 <- with(A100, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))

A100.Table<-data.frame(Ratio1=A100.ratio1, Ratio2=A100.ratio2)

#

Tab<-rbind(A1.Table, A2.Table, A3.Table, A100.Table)


#Compute the mean for each ratio

Ratio1<-mean(Table1[,1])

Ratio2<-mean(Table1[,2])


summary <- ddply(subset(Tab), c(""),summarise,

               N    = length(Tab),

               mean.R1 = mean(Ratio1, na.rm=T),

               median.R1=median(Ratio1, na.rm=T),

               sd.R1   = sd(Ratio1, na.rm=T),

               se.R1   = sd / sqrt(N),

               LCI.95.R1=mean.R1-1.95*se.R1,

               UCI.95.R1=mean.R1+1.95*se.R1,



               mean.R2 = mean(Ratio2, na.rm=T),

               median.R2=median(Ratio2, na.rm=T),

               sd.R2   = sd(Ratio2, na.rm=T),

               se.R2   = sd / sqrt(N),

               LCI.95.R2=mean.R2-1.95*se.R2,

               UCI.95.R2=mean.R2+1.95*se.R2

               )

 summary



On Mon, Mar 25, 2019 at 4:50 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

>
> > ratio1 <- with(dat, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
> > ratio1
> [1] 1.2
>
> It looks like you should spend some more time with an R tutorial or two.
> This is basic stuff (if I understand what you wanted correctly).
>
> Also, this is not how a "confidence interval" should be calculated, but
> that is another off topic discussion for which stats.stackexchange.com is
> a more appropriate venue.
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Mon, Mar 25, 2019 at 4:31 PM Marna Wagley <marna.wagley at gmail.com>
> wrote:
>
>> Hi R User,
>> I was trying to calculate ratios with confidence interval using Monte
>> Carlo
>> simulation but I could not figure it out.
>> Here is the example of my data (see below), I want to calculate ratios
>> (dat$v1/dat$v3 & dat$v2/dat$v3) and its confidence intervals using a 100
>> randomly selected data sets.
>> Could you please give me your suggestions how I can estimate ratios with
>> CI?
>> I will be very grateful to you.
>> Sincerely,
>>
>> MW
>> ---
>> dat<-structure(list(v1 = c(NA, TRUE, TRUE, TRUE, TRUE, TRUE, NA, TRUE,
>>
>> NA, NA, TRUE, TRUE, TRUE, TRUE, NA, NA, TRUE, TRUE), v2 = c(TRUE,
>>
>> NA, NA, NA, NA, TRUE, NA, NA, TRUE, TRUE, NA, TRUE, TRUE, NA,
>>
>> NA, TRUE, TRUE, NA), v3 = c(TRUE, TRUE, NA, TRUE, TRUE, NA, NA,
>>
>> TRUE, TRUE, NA, NA, TRUE, TRUE, TRUE, NA, NA, TRUE, NA)), .Names = c("v1",
>>
>> "v2", "v3"), class = "data.frame", row.names = c(NA, -18L))
>>
>>
>> ratio1<-length(which(dat$v1 == "TRUE"))/length(which(dat$v3 == "TRUE"))
>>
>> ratio2<-length(which(dat$v2 == "TRUE"))/length(which(dat$v3 == "TRUE"))
>>
>>
>> Thanks
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Tue Mar 26 14:27:48 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Tue, 26 Mar 2019 13:27:48 +0000
Subject: [R] Substitution in expressions
Message-ID: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>

Hi All,

I am trying to create a vector of expressions, where the elements in the expressions are contained in other vectors (i.e., they should be substituted). I made some attempts with substitute() and bquote(), but couldn't get this to work. My solution so far is:

base <- 1:5
expo <- c(2,2,3,3,4)
exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)), base, expo)))

plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
text(1:5, 1, exvec)

Any ideas how I could get this to work with substitute() and/or bquote()?

Best,
Wolfgang


From pd@|gd @end|ng |rom gm@||@com  Tue Mar 26 14:41:51 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Tue, 26 Mar 2019 14:41:51 +0100
Subject: [R] Substitution in expressions
In-Reply-To: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
References: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
Message-ID: <E448AC73-CCC7-4B63-983E-CF896DB49689@gmail.com>

Er, I'm confused.

You post some code, the code does something. In which sense is this not what you want?

This is slightly more direct:

> mapply(function(x,y) as.expression(bquote(.(x)^.(y))), base, expo)
expression(1L^2, 2L^2, 3L^3, 4L^3, 5L^4)

but I sense that you are looking for something else?

-pd

> On 26 Mar 2019, at 14:27 , Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> 
> Hi All,
> 
> I am trying to create a vector of expressions, where the elements in the expressions are contained in other vectors (i.e., they should be substituted). I made some attempts with substitute() and bquote(), but couldn't get this to work. My solution so far is:
> 
> base <- 1:5
> expo <- c(2,2,3,3,4)
> exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)), base, expo)))
> 
> plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
> text(1:5, 1, exvec)
> 
> Any ideas how I could get this to work with substitute() and/or bquote()?
> 
> Best,
> Wolfgang
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Tue Mar 26 15:04:58 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Tue, 26 Mar 2019 14:04:58 +0000
Subject: [R] Substitution in expressions
In-Reply-To: <E448AC73-CCC7-4B63-983E-CF896DB49689@gmail.com>
References: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
 <E448AC73-CCC7-4B63-983E-CF896DB49689@gmail.com>
Message-ID: <6055054266b34a84ad6157f755f77511@UM-MAIL3214.unimaas.nl>

Apologies for not being clearer. The code does what I want, but I was wondering if there is a simpler way of doing this, using substitute()/bquote() directly without the mapply().

Best,
Wolfgang

-----Original Message-----
From: peter dalgaard [mailto:pdalgd at gmail.com] 
Sent: Tuesday, 26 March, 2019 14:42
To: Viechtbauer, Wolfgang (SP)
Cc: r-help mailing list
Subject: Re: [R] Substitution in expressions

Er, I'm confused.

You post some code, the code does something. In which sense is this not what you want?

This is slightly more direct:

> mapply(function(x,y) as.expression(bquote(.(x)^.(y))), base, expo)
expression(1L^2, 2L^2, 3L^3, 4L^3, 5L^4)

but I sense that you are looking for something else?

-pd

> On 26 Mar 2019, at 14:27 , Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> 
> Hi All,
> 
> I am trying to create a vector of expressions, where the elements in the expressions are contained in other vectors (i.e., they should be substituted). I made some attempts with substitute() and bquote(), but couldn't get this to work. My solution so far is:
> 
> base <- 1:5
> expo <- c(2,2,3,3,4)
> exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)), base, expo)))
> 
> plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
> text(1:5, 1, exvec)
> 
> Any ideas how I could get this to work with substitute() and/or bquote()?
> 
> Best,
> Wolfgang


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Mar 26 15:26:48 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 26 Mar 2019 07:26:48 -0700
Subject: [R] Monte Carlo simulation for ratio and its CI
In-Reply-To: <CAMwU6B35BbPi6E6bD0XAhRMqba-27dC_3WU7Qu2VZdmQ=DDF5Q@mail.gmail.com>
References: <CAMwU6B1J8RrQK0nWjm5PgX8_-2pv2VM6JAkNfZg=VMWnFRcnLg@mail.gmail.com>
 <CAGxFJbSvPrh==QGVY-EB46SkpP0j7fq8QWWnrR2xWVvN=pcWSg@mail.gmail.com>
 <CAMwU6B35BbPi6E6bD0XAhRMqba-27dC_3WU7Qu2VZdmQ=DDF5Q@mail.gmail.com>
Message-ID: <5390FED8-801D-4167-ABEE-ECF00844F168@dcn.davis.ca.us>

Do you really not know how to use a for loop? The tutorial recommendation seems apropos...

On March 26, 2019 5:57:17 AM PDT, Marna Wagley <marna.wagley at gmail.com> wrote:
>Dear Bert,
>Thank you very much for the response.
>I did it manually but I could not put them in a loop so that I created
>the
>table manually with selecting the rows randomly several times. Here
>what I
>have done so far, please find it. I want to create the table 100 times
>and
>calculate its mean and CI from those 100 values. If anyone can give me
>some
>hint to make a loop, that would be great. I am very grateful with your
>help.
>Thanks,
>
>library(dplyr)
>
>library(plyr)
>
>dat<-structure(list(v1 = c(NA, TRUE, TRUE, TRUE, TRUE, TRUE, NA, TRUE,
>
>NA, NA, TRUE, TRUE, TRUE, TRUE, NA, NA, TRUE, TRUE), v2 = c(TRUE,
>
>NA, NA, NA, NA, TRUE, NA, NA, TRUE, TRUE, NA, TRUE, TRUE, NA,
>
>NA, TRUE, TRUE, NA), v3 = c(TRUE, TRUE, NA, TRUE, TRUE, NA, NA,
>
>TRUE, TRUE, NA, NA, TRUE, TRUE, TRUE, NA, NA, TRUE, NA)), .Names =
>c("v1",
>
>"v2", "v3"), class = "data.frame", row.names = c(NA, -18L))
>
>
>ratio1 <- with(dat, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>ratio2 <- with(dat, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>#
>
>A1<-sample_n(dat1, 16)# created a table with selecting a 16 sample size
>(rows)
>
>A1.ratio1<-with(A1, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A1.ratio2 <- with(A1, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A1.Table<-data.frame(Ratio1=A1.ratio1, Ratio2=A1.ratio2)
>
>#
>
>A2<-sample_n(dat1, 16)
>
>A2.ratio1<-with(A2, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A2.ratio2 <- with(A2, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A2.Table<-data.frame(Ratio1=A2.ratio1, Ratio2=A2.ratio2)
>
>#
>
>A3<-sample_n(dat1, 16)
>
>A3.ratio1<-with(A3, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A3.ratio2 <- with(A3, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A3.Table<-data.frame(Ratio1=A3.ratio1, Ratio2=A3.ratio2)
>
>#
>
>##..............
>
># I was thinking to repeat this procedure 100 times and calculate the
>ratio
>
>A100<-sample_n(dat1, 16)
>
>A100.ratio1<-with(A100, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A100.ratio2 <- with(A100, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A100.Table<-data.frame(Ratio1=A100.ratio1, Ratio2=A100.ratio2)
>
>#
>
>Tab<-rbind(A1.Table, A2.Table, A3.Table, A100.Table)
>
>
>#Compute the mean for each ratio
>
>Ratio1<-mean(Table1[,1])
>
>Ratio2<-mean(Table1[,2])
>
>
>summary <- ddply(subset(Tab), c(""),summarise,
>
>               N    = length(Tab),
>
>               mean.R1 = mean(Ratio1, na.rm=T),
>
>               median.R1=median(Ratio1, na.rm=T),
>
>               sd.R1   = sd(Ratio1, na.rm=T),
>
>               se.R1   = sd / sqrt(N),
>
>               LCI.95.R1=mean.R1-1.95*se.R1,
>
>               UCI.95.R1=mean.R1+1.95*se.R1,
>
>
>
>               mean.R2 = mean(Ratio2, na.rm=T),
>
>               median.R2=median(Ratio2, na.rm=T),
>
>               sd.R2   = sd(Ratio2, na.rm=T),
>
>               se.R2   = sd / sqrt(N),
>
>               LCI.95.R2=mean.R2-1.95*se.R2,
>
>               UCI.95.R2=mean.R2+1.95*se.R2
>
>               )
>
> summary
>
>
>
>On Mon, Mar 25, 2019 at 4:50 PM Bert Gunter <bgunter.4567 at gmail.com>
>wrote:
>
>>
>> > ratio1 <- with(dat, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>> > ratio1
>> [1] 1.2
>>
>> It looks like you should spend some more time with an R tutorial or
>two.
>> This is basic stuff (if I understand what you wanted correctly).
>>
>> Also, this is not how a "confidence interval" should be calculated,
>but
>> that is another off topic discussion for which
>stats.stackexchange.com is
>> a more appropriate venue.
>>
>> Cheers,
>> Bert
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming
>along and
>> sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Mon, Mar 25, 2019 at 4:31 PM Marna Wagley <marna.wagley at gmail.com>
>> wrote:
>>
>>> Hi R User,
>>> I was trying to calculate ratios with confidence interval using
>Monte
>>> Carlo
>>> simulation but I could not figure it out.
>>> Here is the example of my data (see below), I want to calculate
>ratios
>>> (dat$v1/dat$v3 & dat$v2/dat$v3) and its confidence intervals using a
>100
>>> randomly selected data sets.
>>> Could you please give me your suggestions how I can estimate ratios
>with
>>> CI?
>>> I will be very grateful to you.
>>> Sincerely,
>>>
>>> MW
>>> ---
>>> dat<-structure(list(v1 = c(NA, TRUE, TRUE, TRUE, TRUE, TRUE, NA,
>TRUE,
>>>
>>> NA, NA, TRUE, TRUE, TRUE, TRUE, NA, NA, TRUE, TRUE), v2 = c(TRUE,
>>>
>>> NA, NA, NA, NA, TRUE, NA, NA, TRUE, TRUE, NA, TRUE, TRUE, NA,
>>>
>>> NA, TRUE, TRUE, NA), v3 = c(TRUE, TRUE, NA, TRUE, TRUE, NA, NA,
>>>
>>> TRUE, TRUE, NA, NA, TRUE, TRUE, TRUE, NA, NA, TRUE, NA)), .Names =
>c("v1",
>>>
>>> "v2", "v3"), class = "data.frame", row.names = c(NA, -18L))
>>>
>>>
>>> ratio1<-length(which(dat$v1 == "TRUE"))/length(which(dat$v3 ==
>"TRUE"))
>>>
>>> ratio2<-length(which(dat$v2 == "TRUE"))/length(which(dat$v3 ==
>"TRUE"))
>>>
>>>
>>> Thanks
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From pd@|gd @end|ng |rom gm@||@com  Tue Mar 26 15:40:12 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Tue, 26 Mar 2019 15:40:12 +0100
Subject: [R] Substitution in expressions
In-Reply-To: <6055054266b34a84ad6157f755f77511@UM-MAIL3214.unimaas.nl>
References: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
 <E448AC73-CCC7-4B63-983E-CF896DB49689@gmail.com>
 <6055054266b34a84ad6157f755f77511@UM-MAIL3214.unimaas.nl>
Message-ID: <A7FFB5A7-AA60-4B7D-BB11-A940D1DF45B4@gmail.com>

I think mapply() is fine. You could do 

> Vectorize(function(x,y) as.expression(bquote(.(x)^.(y))))(base,expo)
expression(1L^2, 2L^2, 3L^3, 4L^3, 5L^4)

but it is really not that much clearer and does mapply() internally anyway. There's no automatic vectorization in substitute/bquote, so you do need to do it manually. 

(You might have hoped that Vectorize(function(x,y) substitute(x^y)) would work, but it doesn't)

-pd


> On 26 Mar 2019, at 15:04 , Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> 
> Apologies for not being clearer. The code does what I want, but I was wondering if there is a simpler way of doing this, using substitute()/bquote() directly without the mapply().
> 
> Best,
> Wolfgang
> 
> -----Original Message-----
> From: peter dalgaard [mailto:pdalgd at gmail.com] 
> Sent: Tuesday, 26 March, 2019 14:42
> To: Viechtbauer, Wolfgang (SP)
> Cc: r-help mailing list
> Subject: Re: [R] Substitution in expressions
> 
> Er, I'm confused.
> 
> You post some code, the code does something. In which sense is this not what you want?
> 
> This is slightly more direct:
> 
>> mapply(function(x,y) as.expression(bquote(.(x)^.(y))), base, expo)
> expression(1L^2, 2L^2, 3L^3, 4L^3, 5L^4)
> 
> but I sense that you are looking for something else?
> 
> -pd
> 
>> On 26 Mar 2019, at 14:27 , Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
>> 
>> Hi All,
>> 
>> I am trying to create a vector of expressions, where the elements in the expressions are contained in other vectors (i.e., they should be substituted). I made some attempts with substitute() and bquote(), but couldn't get this to work. My solution so far is:
>> 
>> base <- 1:5
>> expo <- c(2,2,3,3,4)
>> exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)), base, expo)))
>> 
>> plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
>> text(1:5, 1, exvec)
>> 
>> Any ideas how I could get this to work with substitute() and/or bquote()?
>> 
>> Best,
>> Wolfgang

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From bgunter@4567 @end|ng |rom gm@||@com  Tue Mar 26 15:51:46 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 26 Mar 2019 07:51:46 -0700
Subject: [R] Substitution in expressions
In-Reply-To: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
References: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
Message-ID: <CAGxFJbTbZ1m9Ps1QJ-d=Gi40b8SWc2BVNmoTw6-xdJyvxYgTrQ@mail.gmail.com>

I believe you're going about this the wrong way. You seem to want
mathematical expressions. Fot this, see ?plotmath.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Mar 26, 2019 at 6:28 AM Viechtbauer, Wolfgang (SP) <
wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:

> Hi All,
>
> I am trying to create a vector of expressions, where the elements in the
> expressions are contained in other vectors (i.e., they should be
> substituted). I made some attempts with substitute() and bquote(), but
> couldn't get this to work. My solution so far is:
>
> base <- 1:5
> expo <- c(2,2,3,3,4)
> exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)),
> base, expo)))
>
> plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
> text(1:5, 1, exvec)
>
> Any ideas how I could get this to work with substitute() and/or bquote()?
>
> Best,
> Wolfgang
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Tue Mar 26 16:23:47 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Tue, 26 Mar 2019 15:23:47 +0000
Subject: [R] Substitution in expressions
In-Reply-To: <A7FFB5A7-AA60-4B7D-BB11-A940D1DF45B4@gmail.com>
References: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
 <E448AC73-CCC7-4B63-983E-CF896DB49689@gmail.com>
 <6055054266b34a84ad6157f755f77511@UM-MAIL3214.unimaas.nl>
 <A7FFB5A7-AA60-4B7D-BB11-A940D1DF45B4@gmail.com>
Message-ID: <6b3ee9492edd432293ad031eb6283b80@UM-MAIL3214.unimaas.nl>

Ok, thanks. Happy to stick with mapply() then.

Best,
Wolfgang 

-----Original Message-----
From: peter dalgaard [mailto:pdalgd at gmail.com] 
Sent: Tuesday, 26 March, 2019 15:40
To: Viechtbauer, Wolfgang (SP)
Cc: r-help mailing list
Subject: Re: [R] Substitution in expressions

I think mapply() is fine. You could do 

> Vectorize(function(x,y) as.expression(bquote(.(x)^.(y))))(base,expo)
expression(1L^2, 2L^2, 3L^3, 4L^3, 5L^4)

but it is really not that much clearer and does mapply() internally anyway. There's no automatic vectorization in substitute/bquote, so you do need to do it manually. 

(You might have hoped that Vectorize(function(x,y) substitute(x^y)) would work, but it doesn't)

-pd

> On 26 Mar 2019, at 15:04 , Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> 
> Apologies for not being clearer. The code does what I want, but I was wondering if there is a simpler way of doing this, using substitute()/bquote() directly without the mapply().
> 
> Best,
> Wolfgang
> 
> -----Original Message-----
> From: peter dalgaard [mailto:pdalgd at gmail.com] 
> Sent: Tuesday, 26 March, 2019 14:42
> To: Viechtbauer, Wolfgang (SP)
> Cc: r-help mailing list
> Subject: Re: [R] Substitution in expressions
> 
> Er, I'm confused.
> 
> You post some code, the code does something. In which sense is this not what you want?
> 
> This is slightly more direct:
> 
>> mapply(function(x,y) as.expression(bquote(.(x)^.(y))), base, expo)
> expression(1L^2, 2L^2, 3L^3, 4L^3, 5L^4)
> 
> but I sense that you are looking for something else?
> 
> -pd
> 
>> On 26 Mar 2019, at 14:27 , Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
>> 
>> Hi All,
>> 
>> I am trying to create a vector of expressions, where the elements in the expressions are contained in other vectors (i.e., they should be substituted). I made some attempts with substitute() and bquote(), but couldn't get this to work. My solution so far is:
>> 
>> base <- 1:5
>> expo <- c(2,2,3,3,4)
>> exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)), base, expo)))
>> 
>> plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
>> text(1:5, 1, exvec)
>> 
>> Any ideas how I could get this to work with substitute() and/or bquote()?
>> 
>> Best,
>> Wolfgang


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Tue Mar 26 16:27:41 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Tue, 26 Mar 2019 15:27:41 +0000
Subject: [R] Substitution in expressions
In-Reply-To: <CAGxFJbTbZ1m9Ps1QJ-d=Gi40b8SWc2BVNmoTw6-xdJyvxYgTrQ@mail.gmail.com>
References: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
 <CAGxFJbTbZ1m9Ps1QJ-d=Gi40b8SWc2BVNmoTw6-xdJyvxYgTrQ@mail.gmail.com>
Message-ID: <3b89ae6bb7d846e78b3c8cbf56330f0f@UM-MAIL3214.unimaas.nl>

Hi Bert,

I am indeed creating a mathematical expression, but ?plotmath doesn't cover how to do such a vectorized substitution.

Best,
Wolfgang

-----Original Message-----
From: Bert Gunter [mailto:bgunter.4567 at gmail.com] 
Sent: Tuesday, 26 March, 2019 15:52
To: Viechtbauer, Wolfgang (SP)
Cc: r-help mailing list
Subject: Re: [R] Substitution in expressions

I believe you're going about this the wrong way. You seem to want mathematical expressions. Fot this, see ?plotmath.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Mar 26, 2019 at 6:28 AM Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
Hi All,

I am trying to create a vector of expressions, where the elements in the expressions are contained in other vectors (i.e., they should be substituted). I made some attempts with substitute() and bquote(), but couldn't get this to work. My solution so far is:

base <- 1:5
expo <- c(2,2,3,3,4)
exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)), base, expo)))

plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
text(1:5, 1, exvec)

Any ideas how I could get this to work with substitute() and/or bquote()?

Best,
Wolfgang

From r-p@ck@ge@ @end|ng |rom r-project@org  Tue Mar 26 15:17:04 2019
From: r-p@ck@ge@ @end|ng |rom r-project@org (Heinzen, Ethan P. via R-packages)
Date: Tue, 26 Mar 2019 14:17:04 +0000
Subject: [R] [R-pkgs] arsenal v3.0.0: An Arsenal of 'R' Functions for
 Large-Scale Statistical Summaries
Message-ID: <b23057$bcgv6f@ironport10.mayo.edu>

I'm pleased to announce the release of the "arsenal" package v3.0.0 on CRAN (https://cran.r-project.org/package=arsenal)!

"arsenal" has experienced some major growth in the last few months, starting with the release of v2.0.0 in January.  We tried to maintain backwards compatibility wherever possible, but had to eliminate it in some places.  With that said, here's the latest and greatest of "arsenal":


1.      The compare() function was renamed to comparedf(), to avoid conflicts with the "testthat" package.  comparedf() is used to compare two data.frames, and now includes additional functionality to specify which variables should be compared if they have different column names.

2.      tableby() is our Table-1-like function.  It now accepts a strata term and multiple left-hand-side terms.

3.      paired() piggybacks off of tableby() for comparisons across two time points.

4.      modelsum() runs [lm, coxph, glm, etc.] models on multiple variables, adjusted for the same covariates.  It also accepts a strata term and multiple left-hand-side terms.

5.      freqlist() generates frequency tables, much like SAS's "proc freq" with the "list" option.  It, too, accepts a strata term and multiple left-hand-side terms.

6.      write2html(), write2pdf(), and write2word() are capable of outputting tables to documents without needing to open a whole new .Rmd script.

7.      formulize() is a useful function for creating formulas from character vectors (or columns of data.frames).

As always, contributions and bug reports are welcome on arsenal's GitHub page: https://github.com/eheinzen/arsenal

Cheers!
Ethan Heinzen
heinzen.ethan at mayo.edu<mailto:heinzen.ethan at mayo.edu>
https://github.com/eheinzen


	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From bgunter@4567 @end|ng |rom gm@||@com  Tue Mar 26 17:43:15 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 26 Mar 2019 09:43:15 -0700
Subject: [R] Substitution in expressions
In-Reply-To: <3b89ae6bb7d846e78b3c8cbf56330f0f@UM-MAIL3214.unimaas.nl>
References: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
 <CAGxFJbTbZ1m9Ps1QJ-d=Gi40b8SWc2BVNmoTw6-xdJyvxYgTrQ@mail.gmail.com>
 <3b89ae6bb7d846e78b3c8cbf56330f0f@UM-MAIL3214.unimaas.nl>
Message-ID: <CAGxFJbR8Ev14xqOvBneV3n+8bKwTFfR5_TihFbSiDaYM1cpJjw@mail.gmail.com>

Perhaps something like this (apologies if beating a dead horse):

plot(NA,NA, xlim = c(-1,5),ylim = c(-1,5), xlab = "", ylab = "")
for(i in 1:3)  text(i,i,labels =bquote(2^.(i)))

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Mar 26, 2019 at 8:27 AM Viechtbauer, Wolfgang (SP) <
wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:

> Hi Bert,
>
> I am indeed creating a mathematical expression, but ?plotmath doesn't
> cover how to do such a vectorized substitution.
>
> Best,
> Wolfgang
>
> -----Original Message-----
> From: Bert Gunter [mailto:bgunter.4567 at gmail.com]
> Sent: Tuesday, 26 March, 2019 15:52
> To: Viechtbauer, Wolfgang (SP)
> Cc: r-help mailing list
> Subject: Re: [R] Substitution in expressions
>
> I believe you're going about this the wrong way. You seem to want
> mathematical expressions. Fot this, see ?plotmath.
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Tue, Mar 26, 2019 at 6:28 AM Viechtbauer, Wolfgang (SP) <
> wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> Hi All,
>
> I am trying to create a vector of expressions, where the elements in the
> expressions are contained in other vectors (i.e., they should be
> substituted). I made some attempts with substitute() and bquote(), but
> couldn't get this to work. My solution so far is:
>
> base <- 1:5
> expo <- c(2,2,3,3,4)
> exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)),
> base, expo)))
>
> plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
> text(1:5, 1, exvec)
>
> Any ideas how I could get this to work with substitute() and/or bquote()?
>
> Best,
> Wolfgang
>

	[[alternative HTML version deleted]]


From vrm4 @end|ng |rom cdc@gov  Mon Mar 25 19:26:41 2019
From: vrm4 @end|ng |rom cdc@gov (Yuan, Keming (CDC/DDNID/NCIPC/DVP))
Date: Mon, 25 Mar 2019 18:26:41 +0000
Subject: [R] loop through columns in a data frame
In-Reply-To: <CAAxdm-7dfjJdty1CE798shhZzzH9DRFH96gpub85i3Xbkn18bQ@mail.gmail.com>
References: <fda0010406494d6481b38a65ec4cfbfb@cdc.gov>
 <CAAxdm-7dfjJdty1CE798shhZzzH9DRFH96gpub85i3Xbkn18bQ@mail.gmail.com>
Message-ID: <3f97b5ea87944d3ea627d97beafcd50a@cdc.gov>

Thank you so much, Jim. That?s exactly what I need. Sorry for not providing the data frame. But you created the correct data structure. Thanks again!

From: jim holtman <jholtman at gmail.com>
Sent: Monday, March 25, 2019 2:07 PM
To: Yuan, Keming (CDC/DDNID/NCIPC/DVP) <vrm4 at cdc.gov>
Cc: R-help at r-project.org
Subject: Re: [R] loop through columns in a data frame

R Notebook

You forgot to provide what your test data looks like. For example, are all the columns a single letter followed by ?_" as the name, or are there longer names? Are there always matched pairs (?le? and ?me?) or can singles occur?
Hide

library(tidyverse)

# create some data

test <- tibble(a_le = sample(3, 10, TRUE),

               a_me = sample(3, 10, TRUE),

               b_le = sample(3, 10, TRUE),

               b_me = sample(3, 10, TRUE),

               long_le = sample(3, 10, TRUE),

               long_me = sample(3, 10, TRUE),

               short_le = sample(3, 10, TRUE)

)

So get the names of the columns that contain ?le? or ?me? and group them together for processing
Hide

col_names <- grep("_(le|me)$", names(test), value = TRUE)

group <- tibble(id = str_remove(col_names, "_.*"), col = col_names)

result <- group %>%

  group_by(id) %>%

  do(tibble(x = rowSums(test[, .$col] == 1)))

# add new columns back

for (i in split(result, result$id)){

  test[, paste0(i$id[1], "_new")] <- as.integer(i$x > 0)

}

test


a_le
<int>

a_me
<int>

b_le
<int>

b_me
<int>

long_le
<int>

long_me
<int>

short_le
<int>

a_new
<int>

b_new
<int>

long_new
<int>

3

1

2

3

1

2

2

1

0

1

2

3

3

2

1

1

1

0

0

1

3

2

3

2

1

3

3

0

0

1

2

3

1

3

3

1

2

0

1

1

1

1

2

1

1

2

2

1

1

1

3

3

3

1

1

1

1

0

1

1

1

2

1

2

2

2

2

1

1

0

1

3

2

3

1

1

3

1

0

1

3

1

1

1

3

3

2

1

1

0

1

1

1

2

3

3

3

1

1

0

1-10 of 10 rows | 1-10 of 11 columns

Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.


On Mon, Mar 25, 2019 at 10:08 AM Yuan, Keming (CDC/DDNID/NCIPC/DVP) via R-help <r-help at r-project.org<mailto:r-help at r-project.org>> wrote:
Hi All,

I have a data frame with variable names like A_le, A_me, B_le, B_me, C_le, C_me....
if A_le=1 or A_me=1 then  I need to create a new column A_new=1. Same operation to create columns B_new, C_new...
Does anyone know how to use loop (or other methods) to create new columns? In SAS, I can use array to get it done. But I don't know how to do it in R.

Thanks,

Keming Yuan
CDC


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Mon Mar 25 19:53:13 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Mon, 25 Mar 2019 14:53:13 -0400 (EDT)
Subject: [R] Quantile Density Contours
Message-ID: <2047358266.494500.1553539993171@connect.xfinity.com>

I want to see if I can reproduce the plot below in R. If I understand it correctly, i takes my bivariate data and creates quantile density contours. My interpretation of these contours is that they enclose a certain % of the total data. I am using the bkde2D function in library KernSmooth which gives density values that can be plotted on a contour plot but I would like the curves that enclose a given % of the data, if that is possible


Thanks





Bernard McGarvey

Director, Fort Myers Beach Lions Foundation, Inc.

Retired (Lilly Engineering Fellow).



From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Tue Mar 26 17:56:33 2019
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Tue, 26 Mar 2019 16:56:33 +0000
Subject: [R] Substitution in expressions
In-Reply-To: <CAGxFJbR8Ev14xqOvBneV3n+8bKwTFfR5_TihFbSiDaYM1cpJjw@mail.gmail.com>
References: <633c6e4b2b584a14a35a3a7433e84162@UM-MAIL3214.unimaas.nl>
 <CAGxFJbTbZ1m9Ps1QJ-d=Gi40b8SWc2BVNmoTw6-xdJyvxYgTrQ@mail.gmail.com>
 <3b89ae6bb7d846e78b3c8cbf56330f0f@UM-MAIL3214.unimaas.nl>
 <CAGxFJbR8Ev14xqOvBneV3n+8bKwTFfR5_TihFbSiDaYM1cpJjw@mail.gmail.com>
Message-ID: <a983ec33cedd47e38344a410a81dcb9a@UM-MAIL3214.unimaas.nl>

I initially had a loop, but wanted something vectorized (the vector of expressions is being used as an argument in a function call). But I am happy with sticking to mapply() (plus the slight simplification suggested by Peter Dalgaard).

Best,
Wolfgang

-----Original Message-----
From: Bert Gunter [mailto:bgunter.4567 at gmail.com] 
Sent: Tuesday, 26 March, 2019 17:43
To: Viechtbauer, Wolfgang (SP)
Cc: r-help mailing list
Subject: Re: [R] Substitution in expressions

Perhaps something like this (apologies if beating a dead horse):

plot(NA,NA, xlim = c(-1,5),ylim = c(-1,5), xlab = "", ylab = "")
for(i in 1:3)? text(i,i,labels =bquote(2^.(i)))

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Mar 26, 2019 at 8:27 AM Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
Hi Bert,

I am indeed creating a mathematical expression, but ?plotmath doesn't cover how to do such a vectorized substitution.

Best,
Wolfgang

-----Original Message-----
From: Bert Gunter [mailto:bgunter.4567 at gmail.com] 
Sent: Tuesday, 26 March, 2019 15:52
To: Viechtbauer, Wolfgang (SP)
Cc: r-help mailing list
Subject: Re: [R] Substitution in expressions

I believe you're going about this the wrong way. You seem to want mathematical expressions. Fot this, see ?plotmath.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, Mar 26, 2019 at 6:28 AM Viechtbauer, Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
Hi All,

I am trying to create a vector of expressions, where the elements in the expressions are contained in other vectors (i.e., they should be substituted). I made some attempts with substitute() and bquote(), but couldn't get this to work. My solution so far is:

base <- 1:5
expo <- c(2,2,3,3,4)
exvec <- as.expression(unname(mapply(function(x,y) bquote(.(x)^.(y)), base, expo)))

plot(NA, NA, xlim=c(0,6), ylim=c(0,2))
text(1:5, 1, exvec)

Any ideas how I could get this to work with substitute() and/or bquote()?

Best,
Wolfgang

From dc@r|@on @end|ng |rom t@mu@edu  Tue Mar 26 20:48:12 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Tue, 26 Mar 2019 19:48:12 +0000
Subject: [R] Monte Carlo simulation for ratio and its CI
In-Reply-To: <5390FED8-801D-4167-ABEE-ECF00844F168@dcn.davis.ca.us>
References: <CAMwU6B1J8RrQK0nWjm5PgX8_-2pv2VM6JAkNfZg=VMWnFRcnLg@mail.gmail.com>
 <CAGxFJbSvPrh==QGVY-EB46SkpP0j7fq8QWWnrR2xWVvN=pcWSg@mail.gmail.com>
 <CAMwU6B35BbPi6E6bD0XAhRMqba-27dC_3WU7Qu2VZdmQ=DDF5Q@mail.gmail.com>
 <5390FED8-801D-4167-ABEE-ECF00844F168@dcn.davis.ca.us>
Message-ID: <d12d64b6dd144d76a3a548b85ed61a82@tamu.edu>

I second the vote on needing a tutorial. You need to learn about how R does things and get familiar with vectorization and the apply() family of functions. You defined dat but not dat1 in your code so I'll just use dat. First, to get the ratios:

(ratios <- colSums(dat[-3], na.rm=TRUE)/colSums(dat[3], na.rm=TRUE))
#  v1  v2 
# 1.2 0.8

Then create a function for the Monte Carlo simulation that generates a sample and computes the ratios. Finally, use the function with replicate() to generate the 100 samples:

nratios <- function(x) {
     sdat <- x[sample.int(18,16), ]
     colSums(sdat[-3], na.rm=TRUE)/colSums(sdat[3], na.rm=TRUE)
}

mcrat <- replicate(100, nratios(dat))
str(mcrat)
#  num [1:2, 1:100] 1 0.8 1.222 0.778 1.111 ...
#  - attr(*, "dimnames")=List of 2
#   ..$ : chr [1:2] "v1" "v2"
#  ..$ : NULL

100 values of ratio1 are stored as mcrat["v1", ] and 100 values of ratio2 are stored as mcrat["v2", ].

Now you can generate your summary statistics. 

----------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77843-4352



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jeff Newmiller
Sent: Tuesday, March 26, 2019 9:27 AM
To: r-help at r-project.org; Marna Wagley <marna.wagley at gmail.com>; Bert Gunter <bgunter.4567 at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Monte Carlo simulation for ratio and its CI

Do you really not know how to use a for loop? The tutorial recommendation seems apropos...

On March 26, 2019 5:57:17 AM PDT, Marna Wagley <marna.wagley at gmail.com> wrote:
>Dear Bert,
>Thank you very much for the response.
>I did it manually but I could not put them in a loop so that I created
>the
>table manually with selecting the rows randomly several times. Here
>what I
>have done so far, please find it. I want to create the table 100 times
>and
>calculate its mean and CI from those 100 values. If anyone can give me
>some
>hint to make a loop, that would be great. I am very grateful with your
>help.
>Thanks,
>
>library(dplyr)
>
>library(plyr)
>
>dat<-structure(list(v1 = c(NA, TRUE, TRUE, TRUE, TRUE, TRUE, NA, TRUE,
>
>NA, NA, TRUE, TRUE, TRUE, TRUE, NA, NA, TRUE, TRUE), v2 = c(TRUE,
>
>NA, NA, NA, NA, TRUE, NA, NA, TRUE, TRUE, NA, TRUE, TRUE, NA,
>
>NA, TRUE, TRUE, NA), v3 = c(TRUE, TRUE, NA, TRUE, TRUE, NA, NA,
>
>TRUE, TRUE, NA, NA, TRUE, TRUE, TRUE, NA, NA, TRUE, NA)), .Names =
>c("v1",
>
>"v2", "v3"), class = "data.frame", row.names = c(NA, -18L))
>
>
>ratio1 <- with(dat, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>ratio2 <- with(dat, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>#
>
>A1<-sample_n(dat1, 16)# created a table with selecting a 16 sample size
>(rows)
>
>A1.ratio1<-with(A1, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A1.ratio2 <- with(A1, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A1.Table<-data.frame(Ratio1=A1.ratio1, Ratio2=A1.ratio2)
>
>#
>
>A2<-sample_n(dat1, 16)
>
>A2.ratio1<-with(A2, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A2.ratio2 <- with(A2, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A2.Table<-data.frame(Ratio1=A2.ratio1, Ratio2=A2.ratio2)
>
>#
>
>A3<-sample_n(dat1, 16)
>
>A3.ratio1<-with(A3, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A3.ratio2 <- with(A3, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A3.Table<-data.frame(Ratio1=A3.ratio1, Ratio2=A3.ratio2)
>
>#
>
>##..............
>
># I was thinking to repeat this procedure 100 times and calculate the
>ratio
>
>A100<-sample_n(dat1, 16)
>
>A100.ratio1<-with(A100, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A100.ratio2 <- with(A100, sum(v2,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>
>A100.Table<-data.frame(Ratio1=A100.ratio1, Ratio2=A100.ratio2)
>
>#
>
>Tab<-rbind(A1.Table, A2.Table, A3.Table, A100.Table)
>
>
>#Compute the mean for each ratio
>
>Ratio1<-mean(Table1[,1])
>
>Ratio2<-mean(Table1[,2])
>
>
>summary <- ddply(subset(Tab), c(""),summarise,
>
>               N    = length(Tab),
>
>               mean.R1 = mean(Ratio1, na.rm=T),
>
>               median.R1=median(Ratio1, na.rm=T),
>
>               sd.R1   = sd(Ratio1, na.rm=T),
>
>               se.R1   = sd / sqrt(N),
>
>               LCI.95.R1=mean.R1-1.95*se.R1,
>
>               UCI.95.R1=mean.R1+1.95*se.R1,
>
>
>
>               mean.R2 = mean(Ratio2, na.rm=T),
>
>               median.R2=median(Ratio2, na.rm=T),
>
>               sd.R2   = sd(Ratio2, na.rm=T),
>
>               se.R2   = sd / sqrt(N),
>
>               LCI.95.R2=mean.R2-1.95*se.R2,
>
>               UCI.95.R2=mean.R2+1.95*se.R2
>
>               )
>
> summary
>
>
>
>On Mon, Mar 25, 2019 at 4:50 PM Bert Gunter <bgunter.4567 at gmail.com>
>wrote:
>
>>
>> > ratio1 <- with(dat, sum(v1,na.rm = TRUE)/sum(v3,na.rm=TRUE))
>> > ratio1
>> [1] 1.2
>>
>> It looks like you should spend some more time with an R tutorial or
>two.
>> This is basic stuff (if I understand what you wanted correctly).
>>
>> Also, this is not how a "confidence interval" should be calculated,
>but
>> that is another off topic discussion for which
>stats.stackexchange.com is
>> a more appropriate venue.
>>
>> Cheers,
>> Bert
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming
>along and
>> sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Mon, Mar 25, 2019 at 4:31 PM Marna Wagley <marna.wagley at gmail.com>
>> wrote:
>>
>>> Hi R User,
>>> I was trying to calculate ratios with confidence interval using
>Monte
>>> Carlo
>>> simulation but I could not figure it out.
>>> Here is the example of my data (see below), I want to calculate
>ratios
>>> (dat$v1/dat$v3 & dat$v2/dat$v3) and its confidence intervals using a
>100
>>> randomly selected data sets.
>>> Could you please give me your suggestions how I can estimate ratios
>with
>>> CI?
>>> I will be very grateful to you.
>>> Sincerely,
>>>
>>> MW
>>> ---
>>> dat<-structure(list(v1 = c(NA, TRUE, TRUE, TRUE, TRUE, TRUE, NA,
>TRUE,
>>>
>>> NA, NA, TRUE, TRUE, TRUE, TRUE, NA, NA, TRUE, TRUE), v2 = c(TRUE,
>>>
>>> NA, NA, NA, NA, TRUE, NA, NA, TRUE, TRUE, NA, TRUE, TRUE, NA,
>>>
>>> NA, TRUE, TRUE, NA), v3 = c(TRUE, TRUE, NA, TRUE, TRUE, NA, NA,
>>>
>>> TRUE, TRUE, NA, NA, TRUE, TRUE, TRUE, NA, NA, TRUE, NA)), .Names =
>c("v1",
>>>
>>> "v2", "v3"), class = "data.frame", row.names = c(NA, -18L))
>>>
>>>
>>> ratio1<-length(which(dat$v1 == "TRUE"))/length(which(dat$v3 ==
>"TRUE"))
>>>
>>> ratio2<-length(which(dat$v2 == "TRUE"))/length(which(dat$v3 ==
>"TRUE"))
>>>
>>>
>>> Thanks
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Wed Mar 27 12:44:54 2019
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Wed, 27 Mar 2019 07:44:54 -0400
Subject: [R] Quantile Density Contours
In-Reply-To: <2047358266.494500.1553539993171@connect.xfinity.com>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
Message-ID: <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>

The figure did not get  through. Perhaps try a pdf?

On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
<mcgarvey.bernard at comcast.net> wrote:
>
> I want to see if I can reproduce the plot below in R. If I understand it correctly, i takes my bivariate data and creates quantile density contours. My interpretation of these contours is that they enclose a certain % of the total data. I am using the bkde2D function in library KernSmooth which gives density values that can be plotted on a contour plot but I would like the curves that enclose a given % of the data, if that is possible
>
>
> Thanks
>
>
>
>
>
> Bernard McGarvey
>
> Director, Fort Myers Beach Lions Foundation, Inc.
>
> Retired (Lilly Engineering Fellow).
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From @u@@n@e||@@ @end|ng |rom m@|ne@edu  Wed Mar 27 14:58:35 2019
From: @u@@n@e||@@ @end|ng |rom m@|ne@edu (Susan Elias)
Date: Wed, 27 Mar 2019 09:58:35 -0400
Subject: [R] ref.df in mgcv gam
Message-ID: <CAKXsTQOLz-Nivxa4O7LTO9XHXEm_4xTmH9q+6nu0tv3Rw7JJHw@mail.gmail.com>

Hello Group.

Regarding degrees of freedom in GAM models fit by mgcv, I do understand the
estimated df, but what does "ref.df" in the summary output mean?

I didn't see an explanation in Wood (2017) or find it in the function
code.  The answer may be on page 222 of

Wood SN. On p-values for smooth components of an extended generalized
additive model. Biometrika 2013;100(1):221-228

or thereafter, but I am not a statistician.  I could use help understanding
what the reference degrees of freedom are.

Thank you,

Susan




-- 
Susan Elias
Vector-borne Disease Lab, Maine Medical Center Research Institute

	[[alternative HTML version deleted]]


From @|mon@wood @end|ng |rom b@th@edu  Wed Mar 27 17:10:06 2019
From: @|mon@wood @end|ng |rom b@th@edu (Simon Wood)
Date: Wed, 27 Mar 2019 16:10:06 +0000
Subject: [R] ref.df in mgcv gam
In-Reply-To: <CAKXsTQOLz-Nivxa4O7LTO9XHXEm_4xTmH9q+6nu0tv3Rw7JJHw@mail.gmail.com>
References: <CAKXsTQOLz-Nivxa4O7LTO9XHXEm_4xTmH9q+6nu0tv3Rw7JJHw@mail.gmail.com>
Message-ID: <7987dd38-be1d-0949-3a4a-7f1af3d4a43b@bath.edu>

These are a bit of a throwback really, and are not very useful - they 
are reference degrees of freedom used in computing test statistic and 
the p-values, but since the null distributions are non-standard the 
reference DoF is not very interpretable.

best,

Simon

On 27/03/2019 13:58, Susan Elias wrote:
> Hello Group.
>
> Regarding degrees of freedom in GAM models fit by mgcv, I do understand the
> estimated df, but what does "ref.df" in the summary output mean?
>
> I didn't see an explanation in Wood (2017) or find it in the function
> code.  The answer may be on page 222 of
>
> Wood SN. On p-values for smooth components of an extended generalized
> additive model. Biometrika 2013;100(1):221-228
>
> or thereafter, but I am not a statistician.  I could use help understanding
> what the reference degrees of freedom are.
>
> Thank you,
>
> Susan
>
>
>
>
-- 
Simon Wood, School of Mathematics, University of Bristol, BS8 1TW UK
https://people.maths.bris.ac.uk/~sw15190/


From @u@@n@e||@@ @end|ng |rom m@|ne@edu  Wed Mar 27 17:27:23 2019
From: @u@@n@e||@@ @end|ng |rom m@|ne@edu (Susan Elias)
Date: Wed, 27 Mar 2019 12:27:23 -0400
Subject: [R] ref.df in mgcv gam
In-Reply-To: <7987dd38-be1d-0949-3a4a-7f1af3d4a43b@bath.edu>
References: <CAKXsTQOLz-Nivxa4O7LTO9XHXEm_4xTmH9q+6nu0tv3Rw7JJHw@mail.gmail.com>
 <7987dd38-be1d-0949-3a4a-7f1af3d4a43b@bath.edu>
Message-ID: <CAKXsTQNePMUZa_jpvaNFevBVoNSGCLOYLikhdJ2i=bZFB5GT7A@mail.gmail.com>

Thank you, Dr. Wood, that is very helpful.

Best,

Susan


On Wed, Mar 27, 2019 at 12:10 PM Simon Wood <simon.wood at bath.edu> wrote:

> These are a bit of a throwback really, and are not very useful - they
> are reference degrees of freedom used in computing test statistic and
> the p-values, but since the null distributions are non-standard the
> reference DoF is not very interpretable.
>
> best,
>
> Simon
>
> On 27/03/2019 13:58, Susan Elias wrote:
> > Hello Group.
> >
> > Regarding degrees of freedom in GAM models fit by mgcv, I do understand
> the
> > estimated df, but what does "ref.df" in the summary output mean?
> >
> > I didn't see an explanation in Wood (2017) or find it in the function
> > code.  The answer may be on page 222 of
> >
> > Wood SN. On p-values for smooth components of an extended generalized
> > additive model. Biometrika 2013;100(1):221-228
> >
> > or thereafter, but I am not a statistician.  I could use help
> understanding
> > what the reference degrees of freedom are.
> >
> > Thank you,
> >
> > Susan
> >
> >
> >
> >
> --
> Simon Wood, School of Mathematics, University of Bristol, BS8 1TW UK
> https://people.maths.bris.ac.uk/~sw15190/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Susan Elias
PhD Candidate, School of Earth and Climate Sciences, University of Maine
Research Associate, Vector-borne Disease Lab, Maine Medical Center Research
Institute
207-756-2761

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed Mar 27 20:37:06 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Wed, 27 Mar 2019 15:37:06 -0400 (EDT)
Subject: [R] Quantile Density Contours
In-Reply-To: <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
Message-ID: <2045542728.525463.1553715426554@connect.xfinity.com>

John, I have attached a pdf of the plot. Hopefully you can read this.

If I understand correctly, this plot is basically the 2-D version of the 1-D quantile plot.

Thanks

Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).


> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com> wrote:
> 
> 
> The figure did not get  through. Perhaps try a pdf?
> 
> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
> <mcgarvey.bernard at comcast.net> wrote:
> >
> > I want to see if I can reproduce the plot below in R. If I understand it correctly, i takes my bivariate data and creates quantile density contours. My interpretation of these contours is that they enclose a certain % of the total data. I am using the bkde2D function in library KernSmooth which gives density values that can be plotted on a contour plot but I would like the curves that enclose a given % of the data, if that is possible
> >
> >
> > Thanks
> >
> >
> >
> >
> >
> > Bernard McGarvey
> >
> > Director, Fort Myers Beach Lions Foundation, Inc.
> >
> > Retired (Lilly Engineering Fellow).
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> -- 
> John Kane
> Kingston ON Canada
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Qunatile Density Contours Plot.pdf
Type: application/pdf
Size: 52676 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190327/45a2e02a/attachment.pdf>

From p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz  Wed Mar 27 20:57:11 2019
From: p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz (Paul Murrell)
Date: Thu, 28 Mar 2019 08:57:11 +1300
Subject: [R] [FORGED] Re:  Quantile Density Contours
In-Reply-To: <2045542728.525463.1553715426554@connect.xfinity.com>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
Message-ID: <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>


Are you looking for the contourLines() function ?

Paul

On 28/03/19 8:37 AM, Bernard McGarvey wrote:
> John, I have attached a pdf of the plot. Hopefully you can read this.
> 
> If I understand correctly, this plot is basically the 2-D version of the 1-D quantile plot.
> 
> Thanks
> 
> Bernard McGarvey
> 
> 
> Director, Fort Myers Beach Lions Foundation, Inc.
> 
> 
> Retired (Lilly Engineering Fellow).
> 
> 
>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com> wrote:
>>
>>
>> The figure did not get  through. Perhaps try a pdf?
>>
>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>> <mcgarvey.bernard at comcast.net> wrote:
>>>
>>> I want to see if I can reproduce the plot below in R. If I understand it correctly, i takes my bivariate data and creates quantile density contours. My interpretation of these contours is that they enclose a certain % of the total data. I am using the bkde2D function in library KernSmooth which gives density values that can be plotted on a contour plot but I would like the curves that enclose a given % of the data, if that is possible
>>>
>>>
>>> Thanks
>>>
>>>
>>>
>>>
>>>
>>> Bernard McGarvey
>>>
>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>
>>> Retired (Lilly Engineering Fellow).
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> -- 
>> John Kane
>> Kingston ON Canada
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed Mar 27 21:55:18 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Wed, 27 Mar 2019 16:55:18 -0400 (EDT)
Subject: [R] [FORGED] Re:  Quantile Density Contours
In-Reply-To: <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
Message-ID: <654807954.527013.1553720118524@connect.xfinity.com>

If I understand correctly the ContourLines function gives you the contour lines when you put in the data. But before this I need to data to put into that function. I think this is something like a 2D CDF of the data that then leads to the 2D quantiles but I am not 100% sure. What I am basically looking for is the 2D curve that encloses say 95% of the data in a similar fashion to a 1D quantile where the quantile represents the value that x% of the data is below. I think what I am looking for is the 2D bivariate version of the 1D quantile plot (where the quantile value is plotted vs the % value).

I hope this makes some sense.

Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).


> On March 27, 2019 at 3:57 PM Paul Murrell <paul at stat.auckland.ac.nz> wrote:
> 
> 
> 
> Are you looking for the contourLines() function ?
> 
> Paul
> 
> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
> > John, I have attached a pdf of the plot. Hopefully you can read this.
> > 
> > If I understand correctly, this plot is basically the 2-D version of the 1-D quantile plot.
> > 
> > Thanks
> > 
> > Bernard McGarvey
> > 
> > 
> > Director, Fort Myers Beach Lions Foundation, Inc.
> > 
> > 
> > Retired (Lilly Engineering Fellow).
> > 
> > 
> >> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com> wrote:
> >>
> >>
> >> The figure did not get  through. Perhaps try a pdf?
> >>
> >> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
> >> <mcgarvey.bernard at comcast.net> wrote:
> >>>
> >>> I want to see if I can reproduce the plot below in R. If I understand it correctly, i takes my bivariate data and creates quantile density contours. My interpretation of these contours is that they enclose a certain % of the total data. I am using the bkde2D function in library KernSmooth which gives density values that can be plotted on a contour plot but I would like the curves that enclose a given % of the data, if that is possible
> >>>
> >>>
> >>> Thanks
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> Bernard McGarvey
> >>>
> >>> Director, Fort Myers Beach Lions Foundation, Inc.
> >>>
> >>> Retired (Lilly Engineering Fellow).
> >>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >> -- 
> >> John Kane
> >> Kingston ON Canada
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/


From bgunter@4567 @end|ng |rom gm@||@com  Wed Mar 27 22:18:18 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 Mar 2019 17:18:18 -0400
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <654807954.527013.1553720118524@connect.xfinity.com>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
Message-ID: <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>

You are missing a crucial point. The reals are well ordered; higher
dimensions are not. Therefore 2d quantile contours are not unique.

Of course assuming I understand your query correctly.


Bert

On Wed, Mar 27, 2019, 13:55 Bernard McGarvey <mcgarvey.bernard at comcast.net>
wrote:

> If I understand correctly the ContourLines function gives you the contour
> lines when you put in the data. But before this I need to data to put into
> that function. I think this is something like a 2D CDF of the data that
> then leads to the 2D quantiles but I am not 100% sure. What I am basically
> looking for is the 2D curve that encloses say 95% of the data in a similar
> fashion to a 1D quantile where the quantile represents the value that x% of
> the data is below. I think what I am looking for is the 2D bivariate
> version of the 1D quantile plot (where the quantile value is plotted vs the
> % value).
>
> I hope this makes some sense.
>
> Bernard McGarvey
>
>
> Director, Fort Myers Beach Lions Foundation, Inc.
>
>
> Retired (Lilly Engineering Fellow).
>
>
> > On March 27, 2019 at 3:57 PM Paul Murrell <paul at stat.auckland.ac.nz>
> wrote:
> >
> >
> >
> > Are you looking for the contourLines() function ?
> >
> > Paul
> >
> > On 28/03/19 8:37 AM, Bernard McGarvey wrote:
> > > John, I have attached a pdf of the plot. Hopefully you can read this.
> > >
> > > If I understand correctly, this plot is basically the 2-D version of
> the 1-D quantile plot.
> > >
> > > Thanks
> > >
> > > Bernard McGarvey
> > >
> > >
> > > Director, Fort Myers Beach Lions Foundation, Inc.
> > >
> > >
> > > Retired (Lilly Engineering Fellow).
> > >
> > >
> > >> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com> wrote:
> > >>
> > >>
> > >> The figure did not get  through. Perhaps try a pdf?
> > >>
> > >> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
> > >> <mcgarvey.bernard at comcast.net> wrote:
> > >>>
> > >>> I want to see if I can reproduce the plot below in R. If I
> understand it correctly, i takes my bivariate data and creates quantile
> density contours. My interpretation of these contours is that they enclose
> a certain % of the total data. I am using the bkde2D function in library
> KernSmooth which gives density values that can be plotted on a contour plot
> but I would like the curves that enclose a given % of the data, if that is
> possible
> > >>>
> > >>>
> > >>> Thanks
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>> Bernard McGarvey
> > >>>
> > >>> Director, Fort Myers Beach Lions Foundation, Inc.
> > >>>
> > >>> Retired (Lilly Engineering Fellow).
> > >>>
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >>
> > >>
> > >> --
> > >> John Kane
> > >> Kingston ON Canada
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Dr Paul Murrell
> > Department of Statistics
> > The University of Auckland
> > Private Bag 92019
> > Auckland
> > New Zealand
> > 64 9 3737599 x85392
> > paul at stat.auckland.ac.nz
> > http://www.stat.auckland.ac.nz/~paul/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed Mar 27 22:20:56 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 27 Mar 2019 17:20:56 -0400
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
Message-ID: <80C9418D-1231-4754-A535-740B9CEE5061@comcast.net>

That thought had crossed my mind so thanks for that clarification Bert. i think you are correct and so the plot I am looking at must be doing something different than I was thinking.

Thanks

Bernard
Sent from my iPhone so please excuse the spelling!"

> On Mar 27, 2019, at 5:18 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> You are missing a crucial point. The reals are well ordered; higher dimensions are not. Therefore 2d quantile contours are not unique. 
> 
> Of course assuming I understand your query correctly.
> 
> 
> Bert
> 
>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>> If I understand correctly the ContourLines function gives you the contour lines when you put in the data. But before this I need to data to put into that function. I think this is something like a 2D CDF of the data that then leads to the 2D quantiles but I am not 100% sure. What I am basically looking for is the 2D curve that encloses say 95% of the data in a similar fashion to a 1D quantile where the quantile represents the value that x% of the data is below. I think what I am looking for is the 2D bivariate version of the 1D quantile plot (where the quantile value is plotted vs the % value).
>> 
>> I hope this makes some sense.
>> 
>> Bernard McGarvey
>> 
>> 
>> Director, Fort Myers Beach Lions Foundation, Inc.
>> 
>> 
>> Retired (Lilly Engineering Fellow).
>> 
>> 
>> > On March 27, 2019 at 3:57 PM Paul Murrell <paul at stat.auckland.ac.nz> wrote:
>> > 
>> > 
>> > 
>> > Are you looking for the contourLines() function ?
>> > 
>> > Paul
>> > 
>> > On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>> > > John, I have attached a pdf of the plot. Hopefully you can read this.
>> > > 
>> > > If I understand correctly, this plot is basically the 2-D version of the 1-D quantile plot.
>> > > 
>> > > Thanks
>> > > 
>> > > Bernard McGarvey
>> > > 
>> > > 
>> > > Director, Fort Myers Beach Lions Foundation, Inc.
>> > > 
>> > > 
>> > > Retired (Lilly Engineering Fellow).
>> > > 
>> > > 
>> > >> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com> wrote:
>> > >>
>> > >>
>> > >> The figure did not get  through. Perhaps try a pdf?
>> > >>
>> > >> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>> > >> <mcgarvey.bernard at comcast.net> wrote:
>> > >>>
>> > >>> I want to see if I can reproduce the plot below in R. If I understand it correctly, i takes my bivariate data and creates quantile density contours. My interpretation of these contours is that they enclose a certain % of the total data. I am using the bkde2D function in library KernSmooth which gives density values that can be plotted on a contour plot but I would like the curves that enclose a given % of the data, if that is possible
>> > >>>
>> > >>>
>> > >>> Thanks
>> > >>>
>> > >>>
>> > >>>
>> > >>>
>> > >>>
>> > >>> Bernard McGarvey
>> > >>>
>> > >>> Director, Fort Myers Beach Lions Foundation, Inc.
>> > >>>
>> > >>> Retired (Lilly Engineering Fellow).
>> > >>>
>> > >>>
>> > >>> ______________________________________________
>> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > >>> and provide commented, minimal, self-contained, reproducible code.
>> > >>
>> > >>
>> > >>
>> > >> -- 
>> > >> John Kane
>> > >> Kingston ON Canada
>> > >>
>> > >> ______________________________________________
>> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > >> and provide commented, minimal, self-contained, reproducible code.
>> > 
>> > -- 
>> > Dr Paul Murrell
>> > Department of Statistics
>> > The University of Auckland
>> > Private Bag 92019
>> > Auckland
>> > New Zealand
>> > 64 9 3737599 x85392
>> > paul at stat.auckland.ac.nz
>> > http://www.stat.auckland.ac.nz/~paul/
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Mar 27 23:38:15 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 Mar 2019 15:38:15 -0700
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
Message-ID: <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>

Regardless of how many dimensions you have for independent variables, the density is one-dimensional, and if you assume the density function has been determined (e.g. by kernel estimation or by a Gaussian copula) then if you integrate the density function along that dimension there will be unique slices of the multivariate input domain determined by those slices. They might in general be disjoint regions of the independent variable space, but that is what the contour function does.

I am not seeing your point, Bert, unless you are unwilling to assume a density function model?

On March 27, 2019 2:18:18 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>You are missing a crucial point. The reals are well ordered; higher
>dimensions are not. Therefore 2d quantile contours are not unique.
>
>Of course assuming I understand your query correctly.
>
>
>Bert
>
>On Wed, Mar 27, 2019, 13:55 Bernard McGarvey
><mcgarvey.bernard at comcast.net>
>wrote:
>
>> If I understand correctly the ContourLines function gives you the
>contour
>> lines when you put in the data. But before this I need to data to put
>into
>> that function. I think this is something like a 2D CDF of the data
>that
>> then leads to the 2D quantiles but I am not 100% sure. What I am
>basically
>> looking for is the 2D curve that encloses say 95% of the data in a
>similar
>> fashion to a 1D quantile where the quantile represents the value that
>x% of
>> the data is below. I think what I am looking for is the 2D bivariate
>> version of the 1D quantile plot (where the quantile value is plotted
>vs the
>> % value).
>>
>> I hope this makes some sense.
>>
>> Bernard McGarvey
>>
>>
>> Director, Fort Myers Beach Lions Foundation, Inc.
>>
>>
>> Retired (Lilly Engineering Fellow).
>>
>>
>> > On March 27, 2019 at 3:57 PM Paul Murrell
><paul at stat.auckland.ac.nz>
>> wrote:
>> >
>> >
>> >
>> > Are you looking for the contourLines() function ?
>> >
>> > Paul
>> >
>> > On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>> > > John, I have attached a pdf of the plot. Hopefully you can read
>this.
>> > >
>> > > If I understand correctly, this plot is basically the 2-D version
>of
>> the 1-D quantile plot.
>> > >
>> > > Thanks
>> > >
>> > > Bernard McGarvey
>> > >
>> > >
>> > > Director, Fort Myers Beach Lions Foundation, Inc.
>> > >
>> > >
>> > > Retired (Lilly Engineering Fellow).
>> > >
>> > >
>> > >> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com>
>wrote:
>> > >>
>> > >>
>> > >> The figure did not get  through. Perhaps try a pdf?
>> > >>
>> > >> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>> > >> <mcgarvey.bernard at comcast.net> wrote:
>> > >>>
>> > >>> I want to see if I can reproduce the plot below in R. If I
>> understand it correctly, i takes my bivariate data and creates
>quantile
>> density contours. My interpretation of these contours is that they
>enclose
>> a certain % of the total data. I am using the bkde2D function in
>library
>> KernSmooth which gives density values that can be plotted on a
>contour plot
>> but I would like the curves that enclose a given % of the data, if
>that is
>> possible
>> > >>>
>> > >>>
>> > >>> Thanks
>> > >>>
>> > >>>
>> > >>>
>> > >>>
>> > >>>
>> > >>> Bernard McGarvey
>> > >>>
>> > >>> Director, Fort Myers Beach Lions Foundation, Inc.
>> > >>>
>> > >>> Retired (Lilly Engineering Fellow).
>> > >>>
>> > >>>
>> > >>> ______________________________________________
>> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > >>> and provide commented, minimal, self-contained, reproducible
>code.
>> > >>
>> > >>
>> > >>
>> > >> --
>> > >> John Kane
>> > >> Kingston ON Canada
>> > >>
>> > >> ______________________________________________
>> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> > >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > >> and provide commented, minimal, self-contained, reproducible
>code.
>> >
>> > --
>> > Dr Paul Murrell
>> > Department of Statistics
>> > The University of Auckland
>> > Private Bag 92019
>> > Auckland
>> > New Zealand
>> > 64 9 3737599 x85392
>> > paul at stat.auckland.ac.nz
>> > http://www.stat.auckland.ac.nz/~paul/
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed Mar 27 23:43:52 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 27 Mar 2019 18:43:52 -0400
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
Message-ID: <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>

To follow on Jeff, is there a function to do 2-D (double) numerical integration in R?

Bernard
Sent from my iPhone so please excuse the spelling!"

> On Mar 27, 2019, at 6:38 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> Regardless of how many dimensions you have for independent variables, the density is one-dimensional, and if you assume the density function has been determined (e.g. by kernel estimation or by a Gaussian copula) then if you integrate the density function along that dimension there will be unique slices of the multivariate input domain determined by those slices. They might in general be disjoint regions of the independent variable space, but that is what the contour function does.
> 
> I am not seeing your point, Bert, unless you are unwilling to assume a density function model?
> 
>> On March 27, 2019 2:18:18 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> You are missing a crucial point. The reals are well ordered; higher
>> dimensions are not. Therefore 2d quantile contours are not unique.
>> 
>> Of course assuming I understand your query correctly.
>> 
>> 
>> Bert
>> 
>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey
>> <mcgarvey.bernard at comcast.net>
>> wrote:
>> 
>>> If I understand correctly the ContourLines function gives you the
>> contour
>>> lines when you put in the data. But before this I need to data to put
>> into
>>> that function. I think this is something like a 2D CDF of the data
>> that
>>> then leads to the 2D quantiles but I am not 100% sure. What I am
>> basically
>>> looking for is the 2D curve that encloses say 95% of the data in a
>> similar
>>> fashion to a 1D quantile where the quantile represents the value that
>> x% of
>>> the data is below. I think what I am looking for is the 2D bivariate
>>> version of the 1D quantile plot (where the quantile value is plotted
>> vs the
>>> % value).
>>> 
>>> I hope this makes some sense.
>>> 
>>> Bernard McGarvey
>>> 
>>> 
>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>> 
>>> 
>>> Retired (Lilly Engineering Fellow).
>>> 
>>> 
>>>> On March 27, 2019 at 3:57 PM Paul Murrell
>> <paul at stat.auckland.ac.nz>
>>> wrote:
>>>> 
>>>> 
>>>> 
>>>> Are you looking for the contourLines() function ?
>>>> 
>>>> Paul
>>>> 
>>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>>>>> John, I have attached a pdf of the plot. Hopefully you can read
>> this.
>>>>> 
>>>>> If I understand correctly, this plot is basically the 2-D version
>> of
>>> the 1-D quantile plot.
>>>>> 
>>>>> Thanks
>>>>> 
>>>>> Bernard McGarvey
>>>>> 
>>>>> 
>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>> 
>>>>> 
>>>>> Retired (Lilly Engineering Fellow).
>>>>> 
>>>>> 
>>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com>
>> wrote:
>>>>>> 
>>>>>> 
>>>>>> The figure did not get  through. Perhaps try a pdf?
>>>>>> 
>>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>>>>>> <mcgarvey.bernard at comcast.net> wrote:
>>>>>>> 
>>>>>>> I want to see if I can reproduce the plot below in R. If I
>>> understand it correctly, i takes my bivariate data and creates
>> quantile
>>> density contours. My interpretation of these contours is that they
>> enclose
>>> a certain % of the total data. I am using the bkde2D function in
>> library
>>> KernSmooth which gives density values that can be plotted on a
>> contour plot
>>> but I would like the curves that enclose a given % of the data, if
>> that is
>>> possible
>>>>>>> 
>>>>>>> 
>>>>>>> Thanks
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> Bernard McGarvey
>>>>>>> 
>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>> 
>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>> 
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible
>> code.
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> --
>>>>>> John Kane
>>>>>> Kingston ON Canada
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible
>> code.
>>>> 
>>>> --
>>>> Dr Paul Murrell
>>>> Department of Statistics
>>>> The University of Auckland
>>>> Private Bag 92019
>>>> Auckland
>>>> New Zealand
>>>> 64 9 3737599 x85392
>>>> paul at stat.auckland.ac.nz
>>>> http://www.stat.auckland.ac.nz/~paul/
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>>    [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Mar 27 23:57:50 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 Mar 2019 15:57:50 -0700
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
 <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
Message-ID: <3AD99747-246D-4AFE-8C2E-EEF29E7FAD33@dcn.davis.ca.us>

I don't know. Have you looked at the Multivariate Task View?

On March 27, 2019 3:43:52 PM PDT, Bernard Comcast <mcgarvey.bernard at comcast.net> wrote:
>To follow on Jeff, is there a function to do 2-D (double) numerical
>integration in R?
>
>Bernard
>Sent from my iPhone so please excuse the spelling!"
>
>> On Mar 27, 2019, at 6:38 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>> 
>> Regardless of how many dimensions you have for independent variables,
>the density is one-dimensional, and if you assume the density function
>has been determined (e.g. by kernel estimation or by a Gaussian copula)
>then if you integrate the density function along that dimension there
>will be unique slices of the multivariate input domain determined by
>those slices. They might in general be disjoint regions of the
>independent variable space, but that is what the contour function does.
>> 
>> I am not seeing your point, Bert, unless you are unwilling to assume
>a density function model?
>> 
>>> On March 27, 2019 2:18:18 PM PDT, Bert Gunter
><bgunter.4567 at gmail.com> wrote:
>>> You are missing a crucial point. The reals are well ordered; higher
>>> dimensions are not. Therefore 2d quantile contours are not unique.
>>> 
>>> Of course assuming I understand your query correctly.
>>> 
>>> 
>>> Bert
>>> 
>>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey
>>> <mcgarvey.bernard at comcast.net>
>>> wrote:
>>> 
>>>> If I understand correctly the ContourLines function gives you the
>>> contour
>>>> lines when you put in the data. But before this I need to data to
>put
>>> into
>>>> that function. I think this is something like a 2D CDF of the data
>>> that
>>>> then leads to the 2D quantiles but I am not 100% sure. What I am
>>> basically
>>>> looking for is the 2D curve that encloses say 95% of the data in a
>>> similar
>>>> fashion to a 1D quantile where the quantile represents the value
>that
>>> x% of
>>>> the data is below. I think what I am looking for is the 2D
>bivariate
>>>> version of the 1D quantile plot (where the quantile value is
>plotted
>>> vs the
>>>> % value).
>>>> 
>>>> I hope this makes some sense.
>>>> 
>>>> Bernard McGarvey
>>>> 
>>>> 
>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>> 
>>>> 
>>>> Retired (Lilly Engineering Fellow).
>>>> 
>>>> 
>>>>> On March 27, 2019 at 3:57 PM Paul Murrell
>>> <paul at stat.auckland.ac.nz>
>>>> wrote:
>>>>> 
>>>>> 
>>>>> 
>>>>> Are you looking for the contourLines() function ?
>>>>> 
>>>>> Paul
>>>>> 
>>>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>>>>>> John, I have attached a pdf of the plot. Hopefully you can read
>>> this.
>>>>>> 
>>>>>> If I understand correctly, this plot is basically the 2-D version
>>> of
>>>> the 1-D quantile plot.
>>>>>> 
>>>>>> Thanks
>>>>>> 
>>>>>> Bernard McGarvey
>>>>>> 
>>>>>> 
>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>> 
>>>>>> 
>>>>>> Retired (Lilly Engineering Fellow).
>>>>>> 
>>>>>> 
>>>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com>
>>> wrote:
>>>>>>> 
>>>>>>> 
>>>>>>> The figure did not get  through. Perhaps try a pdf?
>>>>>>> 
>>>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>>>>>>> <mcgarvey.bernard at comcast.net> wrote:
>>>>>>>> 
>>>>>>>> I want to see if I can reproduce the plot below in R. If I
>>>> understand it correctly, i takes my bivariate data and creates
>>> quantile
>>>> density contours. My interpretation of these contours is that they
>>> enclose
>>>> a certain % of the total data. I am using the bkde2D function in
>>> library
>>>> KernSmooth which gives density values that can be plotted on a
>>> contour plot
>>>> but I would like the curves that enclose a given % of the data, if
>>> that is
>>>> possible
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Thanks
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Bernard McGarvey
>>>>>>>> 
>>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>>> 
>>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>>> 
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> --
>>>>>>> John Kane
>>>>>>> Kingston ON Canada
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>> 
>>>>> --
>>>>> Dr Paul Murrell
>>>>> Department of Statistics
>>>>> The University of Auckland
>>>>> Private Bag 92019
>>>>> Auckland
>>>>> New Zealand
>>>>> 64 9 3737599 x85392
>>>>> paul at stat.auckland.ac.nz
>>>>> http://www.stat.auckland.ac.nz/~paul/
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>>    [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> -- 
>> Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu Mar 28 00:05:31 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 27 Mar 2019 19:05:31 -0400
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <3AD99747-246D-4AFE-8C2E-EEF29E7FAD33@dcn.davis.ca.us>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
 <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
 <3AD99747-246D-4AFE-8C2E-EEF29E7FAD33@dcn.davis.ca.us>
Message-ID: <BB9D26F3-7142-4478-BAD9-BC8634070CDD@comcast.net>

No - how do I access that?

Bernard
Sent from my iPhone so please excuse the spelling!"

> On Mar 27, 2019, at 6:57 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> I don't know. Have you looked at the Multivariate Task View?
> 
>> On March 27, 2019 3:43:52 PM PDT, Bernard Comcast <mcgarvey.bernard at comcast.net> wrote:
>> To follow on Jeff, is there a function to do 2-D (double) numerical
>> integration in R?
>> 
>> Bernard
>> Sent from my iPhone so please excuse the spelling!"
>> 
>>> On Mar 27, 2019, at 6:38 PM, Jeff Newmiller
>> <jdnewmil at dcn.davis.ca.us> wrote:
>>> 
>>> Regardless of how many dimensions you have for independent variables,
>> the density is one-dimensional, and if you assume the density function
>> has been determined (e.g. by kernel estimation or by a Gaussian copula)
>> then if you integrate the density function along that dimension there
>> will be unique slices of the multivariate input domain determined by
>> those slices. They might in general be disjoint regions of the
>> independent variable space, but that is what the contour function does.
>>> 
>>> I am not seeing your point, Bert, unless you are unwilling to assume
>> a density function model?
>>> 
>>>> On March 27, 2019 2:18:18 PM PDT, Bert Gunter
>> <bgunter.4567 at gmail.com> wrote:
>>>> You are missing a crucial point. The reals are well ordered; higher
>>>> dimensions are not. Therefore 2d quantile contours are not unique.
>>>> 
>>>> Of course assuming I understand your query correctly.
>>>> 
>>>> 
>>>> Bert
>>>> 
>>>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey
>>>> <mcgarvey.bernard at comcast.net>
>>>> wrote:
>>>> 
>>>>> If I understand correctly the ContourLines function gives you the
>>>> contour
>>>>> lines when you put in the data. But before this I need to data to
>> put
>>>> into
>>>>> that function. I think this is something like a 2D CDF of the data
>>>> that
>>>>> then leads to the 2D quantiles but I am not 100% sure. What I am
>>>> basically
>>>>> looking for is the 2D curve that encloses say 95% of the data in a
>>>> similar
>>>>> fashion to a 1D quantile where the quantile represents the value
>> that
>>>> x% of
>>>>> the data is below. I think what I am looking for is the 2D
>> bivariate
>>>>> version of the 1D quantile plot (where the quantile value is
>> plotted
>>>> vs the
>>>>> % value).
>>>>> 
>>>>> I hope this makes some sense.
>>>>> 
>>>>> Bernard McGarvey
>>>>> 
>>>>> 
>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>> 
>>>>> 
>>>>> Retired (Lilly Engineering Fellow).
>>>>> 
>>>>> 
>>>>>> On March 27, 2019 at 3:57 PM Paul Murrell
>>>> <paul at stat.auckland.ac.nz>
>>>>> wrote:
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> Are you looking for the contourLines() function ?
>>>>>> 
>>>>>> Paul
>>>>>> 
>>>>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>>>>>>> John, I have attached a pdf of the plot. Hopefully you can read
>>>> this.
>>>>>>> 
>>>>>>> If I understand correctly, this plot is basically the 2-D version
>>>> of
>>>>> the 1-D quantile plot.
>>>>>>> 
>>>>>>> Thanks
>>>>>>> 
>>>>>>> Bernard McGarvey
>>>>>>> 
>>>>>>> 
>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>> 
>>>>>>> 
>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>> 
>>>>>>> 
>>>>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com>
>>>> wrote:
>>>>>>>> 
>>>>>>>> 
>>>>>>>> The figure did not get  through. Perhaps try a pdf?
>>>>>>>> 
>>>>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>>>>>>>> <mcgarvey.bernard at comcast.net> wrote:
>>>>>>>>> 
>>>>>>>>> I want to see if I can reproduce the plot below in R. If I
>>>>> understand it correctly, i takes my bivariate data and creates
>>>> quantile
>>>>> density contours. My interpretation of these contours is that they
>>>> enclose
>>>>> a certain % of the total data. I am using the bkde2D function in
>>>> library
>>>>> KernSmooth which gives density values that can be plotted on a
>>>> contour plot
>>>>> but I would like the curves that enclose a given % of the data, if
>>>> that is
>>>>> possible
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> Thanks
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> Bernard McGarvey
>>>>>>>>> 
>>>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>>>> 
>>>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>> see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>> code.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> --
>>>>>>>> John Kane
>>>>>>>> Kingston ON Canada
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>> see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>> code.
>>>>>> 
>>>>>> --
>>>>>> Dr Paul Murrell
>>>>>> Department of Statistics
>>>>>> The University of Auckland
>>>>>> Private Bag 92019
>>>>>> Auckland
>>>>>> New Zealand
>>>>>> 64 9 3737599 x85392
>>>>>> paul at stat.auckland.ac.nz
>>>>>> http://www.stat.auckland.ac.nz/~paul/
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>> 
>>>>   [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> -- 
>>> Sent from my phone. Please excuse my brevity.
> 
> -- 
> Sent from my phone. Please excuse my brevity.


From pd@|gd @end|ng |rom gm@||@com  Thu Mar 28 00:46:17 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 28 Mar 2019 00:46:17 +0100
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <80C9418D-1231-4754-A535-740B9CEE5061@comcast.net>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <80C9418D-1231-4754-A535-740B9CEE5061@comcast.net>
Message-ID: <667DEEFE-5B79-4028-903F-704BE551D18B@gmail.com>

You might be wishing for a contour plot of the density, labeled by the probability mass outside of each contour, but there is no general simple connection between density contours and the mass inside of them. You can work it out (I think) for elliptically contoured distributions, but I suspect that is about all.

You can do contours of the multivariate CDF, but that comes with its own set of issues, e.g. that F(x,y) = 0.1 means P(X <= x, Y <= y) = 0.1, so contour lines consist of points with the same probability mass to the "south-west" of them.

-pd

> On 27 Mar 2019, at 22:20 , Bernard Comcast <mcgarvey.bernard at comcast.net> wrote:
> 
> That thought had crossed my mind so thanks for that clarification Bert. i think you are correct and so the plot I am looking at must be doing something different than I was thinking.
> 
> Thanks
> 
> Bernard
> Sent from my iPhone so please excuse the spelling!"
> 
>> On Mar 27, 2019, at 5:18 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> 
>> You are missing a crucial point. The reals are well ordered; higher dimensions are not. Therefore 2d quantile contours are not unique. 
>> 
>> Of course assuming I understand your query correctly.
>> 
>> 
>> Bert
>> 
>>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>>> If I understand correctly the ContourLines function gives you the contour lines when you put in the data. But before this I need to data to put into that function. I think this is something like a 2D CDF of the data that then leads to the 2D quantiles but I am not 100% sure. What I am basically looking for is the 2D curve that encloses say 95% of the data in a similar fashion to a 1D quantile where the quantile represents the value that x% of the data is below. I think what I am looking for is the 2D bivariate version of the 1D quantile plot (where the quantile value is plotted vs the % value).
>>> 
>>> I hope this makes some sense.
>>> 
>>> Bernard McGarvey
>>> 
>>> 
>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>> 
>>> 
>>> Retired (Lilly Engineering Fellow).
>>> 
>>> 
>>>> On March 27, 2019 at 3:57 PM Paul Murrell <paul at stat.auckland.ac.nz> wrote:
>>>> 
>>>> 
>>>> 
>>>> Are you looking for the contourLines() function ?
>>>> 
>>>> Paul
>>>> 
>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>>>>> John, I have attached a pdf of the plot. Hopefully you can read this.
>>>>> 
>>>>> If I understand correctly, this plot is basically the 2-D version of the 1-D quantile plot.
>>>>> 
>>>>> Thanks
>>>>> 
>>>>> Bernard McGarvey
>>>>> 
>>>>> 
>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>> 
>>>>> 
>>>>> Retired (Lilly Engineering Fellow).
>>>>> 
>>>>> 
>>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com> wrote:
>>>>>> 
>>>>>> 
>>>>>> The figure did not get  through. Perhaps try a pdf?
>>>>>> 
>>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>>>>>> <mcgarvey.bernard at comcast.net> wrote:
>>>>>>> 
>>>>>>> I want to see if I can reproduce the plot below in R. If I understand it correctly, i takes my bivariate data and creates quantile density contours. My interpretation of these contours is that they enclose a certain % of the total data. I am using the bkde2D function in library KernSmooth which gives density values that can be plotted on a contour plot but I would like the curves that enclose a given % of the data, if that is possible
>>>>>>> 
>>>>>>> 
>>>>>>> Thanks
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> Bernard McGarvey
>>>>>>> 
>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>> 
>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>> 
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> -- 
>>>>>> John Kane
>>>>>> Kingston ON Canada
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> -- 
>>>> Dr Paul Murrell
>>>> Department of Statistics
>>>> The University of Auckland
>>>> Private Bag 92019
>>>> Auckland
>>>> New Zealand
>>>> 64 9 3737599 x85392
>>>> paul at stat.auckland.ac.nz
>>>> http://www.stat.auckland.ac.nz/~paul/
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Mar 28 00:46:56 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 Mar 2019 16:46:56 -0700
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <BB9D26F3-7142-4478-BAD9-BC8634070CDD@comcast.net>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
 <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
 <3AD99747-246D-4AFE-8C2E-EEF29E7FAD33@dcn.davis.ca.us>
 <BB9D26F3-7142-4478-BAD9-BC8634070CDD@comcast.net>
Message-ID: <AE086216-8544-4C7B-9073-C65376CEB3C3@dcn.davis.ca.us>

https://cran.r-project.org/web/views/Multivariate.html

https://cran.r-project.org/web/views/

On March 27, 2019 4:05:31 PM PDT, Bernard Comcast <mcgarvey.bernard at comcast.net> wrote:
>No - how do I access that?
>
>Bernard
>Sent from my iPhone so please excuse the spelling!"
>
>> On Mar 27, 2019, at 6:57 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>> 
>> I don't know. Have you looked at the Multivariate Task View?
>> 
>>> On March 27, 2019 3:43:52 PM PDT, Bernard Comcast
><mcgarvey.bernard at comcast.net> wrote:
>>> To follow on Jeff, is there a function to do 2-D (double) numerical
>>> integration in R?
>>> 
>>> Bernard
>>> Sent from my iPhone so please excuse the spelling!"
>>> 
>>>> On Mar 27, 2019, at 6:38 PM, Jeff Newmiller
>>> <jdnewmil at dcn.davis.ca.us> wrote:
>>>> 
>>>> Regardless of how many dimensions you have for independent
>variables,
>>> the density is one-dimensional, and if you assume the density
>function
>>> has been determined (e.g. by kernel estimation or by a Gaussian
>copula)
>>> then if you integrate the density function along that dimension
>there
>>> will be unique slices of the multivariate input domain determined by
>>> those slices. They might in general be disjoint regions of the
>>> independent variable space, but that is what the contour function
>does.
>>>> 
>>>> I am not seeing your point, Bert, unless you are unwilling to
>assume
>>> a density function model?
>>>> 
>>>>> On March 27, 2019 2:18:18 PM PDT, Bert Gunter
>>> <bgunter.4567 at gmail.com> wrote:
>>>>> You are missing a crucial point. The reals are well ordered;
>higher
>>>>> dimensions are not. Therefore 2d quantile contours are not unique.
>>>>> 
>>>>> Of course assuming I understand your query correctly.
>>>>> 
>>>>> 
>>>>> Bert
>>>>> 
>>>>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey
>>>>> <mcgarvey.bernard at comcast.net>
>>>>> wrote:
>>>>> 
>>>>>> If I understand correctly the ContourLines function gives you the
>>>>> contour
>>>>>> lines when you put in the data. But before this I need to data to
>>> put
>>>>> into
>>>>>> that function. I think this is something like a 2D CDF of the
>data
>>>>> that
>>>>>> then leads to the 2D quantiles but I am not 100% sure. What I am
>>>>> basically
>>>>>> looking for is the 2D curve that encloses say 95% of the data in
>a
>>>>> similar
>>>>>> fashion to a 1D quantile where the quantile represents the value
>>> that
>>>>> x% of
>>>>>> the data is below. I think what I am looking for is the 2D
>>> bivariate
>>>>>> version of the 1D quantile plot (where the quantile value is
>>> plotted
>>>>> vs the
>>>>>> % value).
>>>>>> 
>>>>>> I hope this makes some sense.
>>>>>> 
>>>>>> Bernard McGarvey
>>>>>> 
>>>>>> 
>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>> 
>>>>>> 
>>>>>> Retired (Lilly Engineering Fellow).
>>>>>> 
>>>>>> 
>>>>>>> On March 27, 2019 at 3:57 PM Paul Murrell
>>>>> <paul at stat.auckland.ac.nz>
>>>>>> wrote:
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> Are you looking for the contourLines() function ?
>>>>>>> 
>>>>>>> Paul
>>>>>>> 
>>>>>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>>>>>>>> John, I have attached a pdf of the plot. Hopefully you can read
>>>>> this.
>>>>>>>> 
>>>>>>>> If I understand correctly, this plot is basically the 2-D
>version
>>>>> of
>>>>>> the 1-D quantile plot.
>>>>>>>> 
>>>>>>>> Thanks
>>>>>>>> 
>>>>>>>> Bernard McGarvey
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com>
>>>>> wrote:
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> The figure did not get  through. Perhaps try a pdf?
>>>>>>>>> 
>>>>>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>>>>>>>>> <mcgarvey.bernard at comcast.net> wrote:
>>>>>>>>>> 
>>>>>>>>>> I want to see if I can reproduce the plot below in R. If I
>>>>>> understand it correctly, i takes my bivariate data and creates
>>>>> quantile
>>>>>> density contours. My interpretation of these contours is that
>they
>>>>> enclose
>>>>>> a certain % of the total data. I am using the bkde2D function in
>>>>> library
>>>>>> KernSmooth which gives density values that can be plotted on a
>>>>> contour plot
>>>>>> but I would like the curves that enclose a given % of the data,
>if
>>>>> that is
>>>>>> possible
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> Thanks
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> Bernard McGarvey
>>>>>>>>>> 
>>>>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>>>>> 
>>>>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> ______________________________________________
>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>> see
>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> --
>>>>>>>>> John Kane
>>>>>>>>> Kingston ON Canada
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>> see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>> 
>>>>>>> --
>>>>>>> Dr Paul Murrell
>>>>>>> Department of Statistics
>>>>>>> The University of Auckland
>>>>>>> Private Bag 92019
>>>>>>> Auckland
>>>>>>> New Zealand
>>>>>>> 64 9 3737599 x85392
>>>>>>> paul at stat.auckland.ac.nz
>>>>>>> http://www.stat.auckland.ac.nz/~paul/
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible
>code.
>>>>>> 
>>>>> 
>>>>>   [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> -- 
>>>> Sent from my phone. Please excuse my brevity.
>> 
>> -- 
>> Sent from my phone. Please excuse my brevity.

-- 
Sent from my phone. Please excuse my brevity.


From c|rku@01 @end|ng |rom y@hoo@com  Wed Mar 27 21:40:07 2019
From: c|rku@01 @end|ng |rom y@hoo@com (cir p)
Date: Wed, 27 Mar 2019 20:40:07 +0000 (UTC)
Subject: [R] aggregate output to data frame
References: <1277208145.12198189.1553719207167.ref@mail.yahoo.com>
Message-ID: <1277208145.12198189.1553719207167@mail.yahoo.com>

Dear users,
i am trying to summarize data using "aggregate" with the following command:

aggregate(pcr$Ct,c(pcr["Gene"],pcr["Type"],pcr["Rep"]),FUN=function(x){c(mean(x), sd(x), sd(x)/sqrt(sd(x)))})

and the structure of the resulting data frame is 

'data.frame':????66 obs. of??4 variables:
$ Gene: Factor w/ 22 levels "14-3-3e","Act5C",..: 1 2 3 4 5 6 7 8 9 10 ...
$ Type: Factor w/ 2 levels "Std","Unkn": 2 2 2 2 2 2 2 2 2 2 ...
$ Rep : int??1 1 1 1 1 1 1 1 1 1 ...
 $ x?? : num [1:66, 1:3] 16.3 16.7 18.2 17.1 18.6 ...

The actual data is "bundled" in a matrix $x of the data frame. I would like to have the columns of this matrix as individual numeric columns in the same data frame instead of a matrix, but cant really figure it out how to do this in an efficient way. Could someone help me with the construction of this?

Thanks a lot,

Cyrus


From drj|m|emon @end|ng |rom gm@||@com  Thu Mar 28 03:39:38 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 28 Mar 2019 13:39:38 +1100
Subject: [R] aggregate output to data frame
In-Reply-To: <1277208145.12198189.1553719207167@mail.yahoo.com>
References: <1277208145.12198189.1553719207167.ref@mail.yahoo.com>
 <1277208145.12198189.1553719207167@mail.yahoo.com>
Message-ID: <CA+8X3fXotyay_+hUDD6LGo4G5OQ_R0+ZytXSdLVvrm962DT5WQ@mail.gmail.com>

Hi Cyrus,
Try this:

pcr<-data.frame(Ct=runif(66,10,20),Gene=rep(LETTERS[1:22],3),
 Type=rep(c("Std","Unkn"),33),Rep=rep(1:3,each=22))
testagg<-aggregate(pcr$Ct,c(pcr["Gene"],pcr["Type"],pcr["Rep"]),
 FUN=function(x){c(mean(x), sd(x), sd(x)/sqrt(sd(x)))})
nxcol<-dim(testagg$x)[2]
newxs<-paste("x",1:nxcol,sep="")
for(col in 1:nxcol)
 testagg[[newxs[col]]]<-testagg$x[,col]
testagg$x<-NULL

Jim

On Thu, Mar 28, 2019 at 12:39 PM cir p via R-help <r-help at r-project.org> wrote:
>
> Dear users,
> i am trying to summarize data using "aggregate" with the following command:
>
> aggregate(pcr$Ct,c(pcr["Gene"],pcr["Type"],pcr["Rep"]),FUN=function(x){c(mean(x), sd(x), sd(x)/sqrt(sd(x)))})
>
> and the structure of the resulting data frame is
>
> 'data.frame':    66 obs. of  4 variables:
> $ Gene: Factor w/ 22 levels "14-3-3e","Act5C",..: 1 2 3 4 5 6 7 8 9 10 ...
> $ Type: Factor w/ 2 levels "Std","Unkn": 2 2 2 2 2 2 2 2 2 2 ...
> $ Rep : int  1 1 1 1 1 1 1 1 1 1 ...
>  $ x   : num [1:66, 1:3] 16.3 16.7 18.2 17.1 18.6 ...
>
> The actual data is "bundled" in a matrix $x of the data frame. I would like to have the columns of this matrix as individual numeric columns in the same data frame instead of a matrix, but cant really figure it out how to do this in an efficient way. Could someone help me with the construction of this?
>
> Thanks a lot,
>
> Cyrus
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Mar 28 07:26:29 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 28 Mar 2019 06:26:29 +0000
Subject: [R] [FORGED] Re:  Quantile Density Contours
In-Reply-To: <654807954.527013.1553720118524@connect.xfinity.com>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
Message-ID: <f305d2d245174e6c91466963170d5e90@SRVEXCHCM1301.precheza.cz>

Hallo Bernard

I did not follow all emails in this thread but it seems to me that your request is similar to Bioconductor packages dealing with Flow Cytometry data.

Especially flowViz package is designed to visualise such data.

Cheers
Petr


> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Bernard McGarvey
> Sent: Wednesday, March 27, 2019 9:55 PM
> To: Paul Murrell <paul at stat.auckland.ac.nz>; John Kane
> <jrkrideau at gmail.com>
> Cc: R. Help Mailing List <r-help at r-project.org>
> Subject: Re: [R] [FORGED] Re: Quantile Density Contours
>
> If I understand correctly the ContourLines function gives you the contour lines
> when you put in the data. But before this I need to data to put into that
> function. I think this is something like a 2D CDF of the data that then leads to
> the 2D quantiles but I am not 100% sure. What I am basically looking for is the
> 2D curve that encloses say 95% of the data in a similar fashion to a 1D quantile
> where the quantile represents the value that x% of the data is below. I think
> what I am looking for is the 2D bivariate version of the 1D quantile plot (where
> the quantile value is plotted vs the % value).
>
> I hope this makes some sense.
>
> Bernard McGarvey
>
>
> Director, Fort Myers Beach Lions Foundation, Inc.
>
>
> Retired (Lilly Engineering Fellow).
>
>
> > On March 27, 2019 at 3:57 PM Paul Murrell <paul at stat.auckland.ac.nz>
> wrote:
> >
> >
> >
> > Are you looking for the contourLines() function ?
> >
> > Paul
> >
> > On 28/03/19 8:37 AM, Bernard McGarvey wrote:
> > > John, I have attached a pdf of the plot. Hopefully you can read this.
> > >
> > > If I understand correctly, this plot is basically the 2-D version of the 1-D
> quantile plot.
> > >
> > > Thanks
> > >
> > > Bernard McGarvey
> > >
> > >
> > > Director, Fort Myers Beach Lions Foundation, Inc.
> > >
> > >
> > > Retired (Lilly Engineering Fellow).
> > >
> > >
> > >> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com> wrote:
> > >>
> > >>
> > >> The figure did not get  through. Perhaps try a pdf?
> > >>
> > >> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
> > >> <mcgarvey.bernard at comcast.net> wrote:
> > >>>
> > >>> I want to see if I can reproduce the plot below in R. If I
> > >>> understand it correctly, i takes my bivariate data and creates
> > >>> quantile density contours. My interpretation of these contours is
> > >>> that they enclose a certain % of the total data. I am using the
> > >>> bkde2D function in library KernSmooth which gives density values
> > >>> that can be plotted on a contour plot but I would like the curves
> > >>> that enclose a given % of the data, if that is possible
> > >>>
> > >>>
> > >>> Thanks
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>> Bernard McGarvey
> > >>>
> > >>> Director, Fort Myers Beach Lions Foundation, Inc.
> > >>>
> > >>> Retired (Lilly Engineering Fellow).
> > >>>
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide
> > >>> http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >>
> > >>
> > >> --
> > >> John Kane
> > >> Kingston ON Canada
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > >> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Dr Paul Murrell
> > Department of Statistics
> > The University of Auckland
> > Private Bag 92019
> > Auckland
> > New Zealand
> > 64 9 3737599 x85392
> > paul at stat.auckland.ac.nz
> > http://www.stat.auckland.ac.nz/~paul/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu Mar 28 12:56:27 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Thu, 28 Mar 2019 07:56:27 -0400
Subject: [R] [FORGED] Re:  Quantile Density Contours
In-Reply-To: <f305d2d245174e6c91466963170d5e90@SRVEXCHCM1301.precheza.cz>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <f305d2d245174e6c91466963170d5e90@SRVEXCHCM1301.precheza.cz>
Message-ID: <2F8B0F6D-88B9-43E5-9052-E8D7D4C42E91@comcast.net>

Thanks Petr

Bernard
Sent from my iPhone so please excuse the spelling!"

> On Mar 28, 2019, at 2:26 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> 
> Hallo Bernard
> 
> I did not follow all emails in this thread but it seems to me that your request is similar to Bioconductor packages dealing with Flow Cytometry data.
> 
> Especially flowViz package is designed to visualise such data.
> 
> Cheers
> Petr
> 
> 
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Bernard McGarvey
>> Sent: Wednesday, March 27, 2019 9:55 PM
>> To: Paul Murrell <paul at stat.auckland.ac.nz>; John Kane
>> <jrkrideau at gmail.com>
>> Cc: R. Help Mailing List <r-help at r-project.org>
>> Subject: Re: [R] [FORGED] Re: Quantile Density Contours
>> 
>> If I understand correctly the ContourLines function gives you the contour lines
>> when you put in the data. But before this I need to data to put into that
>> function. I think this is something like a 2D CDF of the data that then leads to
>> the 2D quantiles but I am not 100% sure. What I am basically looking for is the
>> 2D curve that encloses say 95% of the data in a similar fashion to a 1D quantile
>> where the quantile represents the value that x% of the data is below. I think
>> what I am looking for is the 2D bivariate version of the 1D quantile plot (where
>> the quantile value is plotted vs the % value).
>> 
>> I hope this makes some sense.
>> 
>> Bernard McGarvey
>> 
>> 
>> Director, Fort Myers Beach Lions Foundation, Inc.
>> 
>> 
>> Retired (Lilly Engineering Fellow).
>> 
>> 
>>> On March 27, 2019 at 3:57 PM Paul Murrell <paul at stat.auckland.ac.nz>
>> wrote:
>>> 
>>> 
>>> 
>>> Are you looking for the contourLines() function ?
>>> 
>>> Paul
>>> 
>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>>>> John, I have attached a pdf of the plot. Hopefully you can read this.
>>>> 
>>>> If I understand correctly, this plot is basically the 2-D version of the 1-D
>> quantile plot.
>>>> 
>>>> Thanks
>>>> 
>>>> Bernard McGarvey
>>>> 
>>>> 
>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>> 
>>>> 
>>>> Retired (Lilly Engineering Fellow).
>>>> 
>>>> 
>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com> wrote:
>>>>> 
>>>>> 
>>>>> The figure did not get  through. Perhaps try a pdf?
>>>>> 
>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>>>>> <mcgarvey.bernard at comcast.net> wrote:
>>>>>> 
>>>>>> I want to see if I can reproduce the plot below in R. If I
>>>>>> understand it correctly, i takes my bivariate data and creates
>>>>>> quantile density contours. My interpretation of these contours is
>>>>>> that they enclose a certain % of the total data. I am using the
>>>>>> bkde2D function in library KernSmooth which gives density values
>>>>>> that can be plotted on a contour plot but I would like the curves
>>>>>> that enclose a given % of the data, if that is possible
>>>>>> 
>>>>>> 
>>>>>> Thanks
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> Bernard McGarvey
>>>>>> 
>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>> 
>>>>>> Retired (Lilly Engineering Fellow).
>>>>>> 
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> 
>>>>> 
>>>>> --
>>>>> John Kane
>>>>> Kingston ON Canada
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> --
>>> Dr Paul Murrell
>>> Department of Statistics
>>> The University of Auckland
>>> Private Bag 92019
>>> Auckland
>>> New Zealand
>>> 64 9 3737599 x85392
>>> paul at stat.auckland.ac.nz
>>> http://www.stat.auckland.ac.nz/~paul/
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> Osobn? ?daje: Informace o zpracov?n? a ochran? osobn?ch ?daj? obchodn?ch partner? PRECHEZA a.s. jsou zve?ejn?ny na: https://www.precheza.cz/zasady-ochrany-osobnich-udaju/ | Information about processing and protection of business partner?s personal data are available on website: https://www.precheza.cz/en/personal-data-protection-principles/
> D?v?rnost: Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a podl?haj? tomuto pr?vn? z?vazn?mu prohl??en? o vylou?en? odpov?dnosti: https://www.precheza.cz/01-dovetek/ | This email and any documents attached to it may be confidential and are subject to the legally binding disclaimer: https://www.precheza.cz/en/01-disclaimer/
> 


From |gegbem|@o|@ @end|ng |rom y@hoo@com  Thu Mar 28 10:11:02 2019
From: |gegbem|@o|@ @end|ng |rom y@hoo@com (oluwayemisi ige)
Date: Thu, 28 Mar 2019 07:11:02 -0200
Subject: [R] Analysing diallel and line x tester mating design with multiple
 location in R
Message-ID: <mailman.354149.18.1553787387.8483.r-help@r-project.org>

Good morning.
My name is Gbemisola, am a PhD student in Plant Breeding. Please am having difficulties in analyzing diallel and line x tester mating design with multiple location in R, can you be of help?

Thanks for your response.
Gbemisola

Sent from Mail for Windows 10


	[[alternative HTML version deleted]]


From jrkr|de@u @end|ng |rom gm@||@com  Thu Mar 28 16:45:03 2019
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Thu, 28 Mar 2019 11:45:03 -0400
Subject: [R] 
 Analysing diallel and line x tester mating design with multiple
 location in R
In-Reply-To: <20190328151734.52CAF225F@hypatia.math.ethz.ch>
References: <20190328151734.52CAF225F@hypatia.math.ethz.ch>
Message-ID: <CAKZQJMBLp=LycmLLUBnswFa_fJ6N=UK4-J_5s5wB6gNRNkwLsQ@mail.gmail.com>

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

 http://adv-r.had.co.nz/Reproducibility.html

On Thu, 28 Mar 2019 at 11:17, oluwayemisi ige via R-help
<r-help at r-project.org> wrote:
>
> Good morning.
> My name is Gbemisola, am a PhD student in Plant Breeding. Please am having difficulties in analyzing diallel and line x tester mating design with multiple location in R, can you be of help?
>
> Thanks for your response.
> Gbemisola
>
> Sent from Mail for Windows 10
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Mar 28 18:40:24 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 28 Mar 2019 10:40:24 -0700
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
 <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
Message-ID: <c67326f6-1964-5f92-47cc-6a47334c790b@comcast.net>


On 3/27/19 3:43 PM, Bernard Comcast wrote:
> To follow on Jeff, is there a function to do 2-D (double) numerical integration in R?

Packages pracma and cubature offer a variety of solutions to that task.


-- 

David.


>
> Bernard
> Sent from my iPhone so please excuse the spelling!"
>
>> On Mar 27, 2019, at 6:38 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>
>> Regardless of how many dimensions you have for independent variables, the density is one-dimensional, and if you assume the density function has been determined (e.g. by kernel estimation or by a Gaussian copula) then if you integrate the density function along that dimension there will be unique slices of the multivariate input domain determined by those slices. They might in general be disjoint regions of the independent variable space, but that is what the contour function does.
>>
>> I am not seeing your point, Bert, unless you are unwilling to assume a density function model?
>>
>>> On March 27, 2019 2:18:18 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>> You are missing a crucial point. The reals are well ordered; higher
>>> dimensions are not. Therefore 2d quantile contours are not unique.
>>>
>>> Of course assuming I understand your query correctly.
>>>
>>>
>>> Bert
>>>
>>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey
>>> <mcgarvey.bernard at comcast.net>
>>> wrote:
>>>
>>>> If I understand correctly the ContourLines function gives you the
>>> contour
>>>> lines when you put in the data. But before this I need to data to put
>>> into
>>>> that function. I think this is something like a 2D CDF of the data
>>> that
>>>> then leads to the 2D quantiles but I am not 100% sure. What I am
>>> basically
>>>> looking for is the 2D curve that encloses say 95% of the data in a
>>> similar
>>>> fashion to a 1D quantile where the quantile represents the value that
>>> x% of
>>>> the data is below. I think what I am looking for is the 2D bivariate
>>>> version of the 1D quantile plot (where the quantile value is plotted
>>> vs the
>>>> % value).
>>>>
>>>> I hope this makes some sense.
>>>>
>>>> Bernard McGarvey
>>>>
>>>>
>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>
>>>>
>>>> Retired (Lilly Engineering Fellow).
>>>>
>>>>
>>>>> On March 27, 2019 at 3:57 PM Paul Murrell
>>> <paul at stat.auckland.ac.nz>
>>>> wrote:
>>>>>
>>>>>
>>>>> Are you looking for the contourLines() function ?
>>>>>
>>>>> Paul
>>>>>
>>>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>>>>>> John, I have attached a pdf of the plot. Hopefully you can read
>>> this.
>>>>>> If I understand correctly, this plot is basically the 2-D version
>>> of
>>>> the 1-D quantile plot.
>>>>>> Thanks
>>>>>>
>>>>>> Bernard McGarvey
>>>>>>
>>>>>>
>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>
>>>>>>
>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>
>>>>>>
>>>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com>
>>> wrote:
>>>>>>>
>>>>>>> The figure did not get  through. Perhaps try a pdf?
>>>>>>>
>>>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>>>>>>> <mcgarvey.bernard at comcast.net> wrote:
>>>>>>>> I want to see if I can reproduce the plot below in R. If I
>>>> understand it correctly, i takes my bivariate data and creates
>>> quantile
>>>> density contours. My interpretation of these contours is that they
>>> enclose
>>>> a certain % of the total data. I am using the bkde2D function in
>>> library
>>>> KernSmooth which gives density values that can be plotted on a
>>> contour plot
>>>> but I would like the curves that enclose a given % of the data, if
>>> that is
>>>> possible
>>>>>>>>
>>>>>>>> Thanks
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Bernard McGarvey
>>>>>>>>
>>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>>>
>>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>>>
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> John Kane
>>>>>>> Kingston ON Canada
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>> --
>>>>> Dr Paul Murrell
>>>>> Department of Statistics
>>>>> The University of Auckland
>>>>> Private Bag 92019
>>>>> Auckland
>>>>> New Zealand
>>>>> 64 9 3737599 x85392
>>>>> paul at stat.auckland.ac.nz
>>>>> http://www.stat.auckland.ac.nz/~paul/
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>     [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> -- 
>> Sent from my phone. Please excuse my brevity.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu Mar 28 19:33:29 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Thu, 28 Mar 2019 14:33:29 -0400 (EDT)
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <c67326f6-1964-5f92-47cc-6a47334c790b@comcast.net>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
 <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
 <c67326f6-1964-5f92-47cc-6a47334c790b@comcast.net>
Message-ID: <265298134.538255.1553798009655@connect.xfinity.com>

pracma meets my needs - thanks - this is an amazing package with lots of very very useful functions. Thanks for bringing it to my attention.

Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).


> On March 28, 2019 at 1:40 PM David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
> 
> On 3/27/19 3:43 PM, Bernard Comcast wrote:
> > To follow on Jeff, is there a function to do 2-D (double) numerical integration in R?
> 
> Packages pracma and cubature offer a variety of solutions to that task.
> 
> 
> -- 
> 
> David.
> 
> 
> >
> > Bernard
> > Sent from my iPhone so please excuse the spelling!"
> >
> >> On Mar 27, 2019, at 6:38 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> >>
> >> Regardless of how many dimensions you have for independent variables, the density is one-dimensional, and if you assume the density function has been determined (e.g. by kernel estimation or by a Gaussian copula) then if you integrate the density function along that dimension there will be unique slices of the multivariate input domain determined by those slices. They might in general be disjoint regions of the independent variable space, but that is what the contour function does.
> >>
> >> I am not seeing your point, Bert, unless you are unwilling to assume a density function model?
> >>
> >>> On March 27, 2019 2:18:18 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >>> You are missing a crucial point. The reals are well ordered; higher
> >>> dimensions are not. Therefore 2d quantile contours are not unique.
> >>>
> >>> Of course assuming I understand your query correctly.
> >>>
> >>>
> >>> Bert
> >>>
> >>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey
> >>> <mcgarvey.bernard at comcast.net>
> >>> wrote:
> >>>
> >>>> If I understand correctly the ContourLines function gives you the
> >>> contour
> >>>> lines when you put in the data. But before this I need to data to put
> >>> into
> >>>> that function. I think this is something like a 2D CDF of the data
> >>> that
> >>>> then leads to the 2D quantiles but I am not 100% sure. What I am
> >>> basically
> >>>> looking for is the 2D curve that encloses say 95% of the data in a
> >>> similar
> >>>> fashion to a 1D quantile where the quantile represents the value that
> >>> x% of
> >>>> the data is below. I think what I am looking for is the 2D bivariate
> >>>> version of the 1D quantile plot (where the quantile value is plotted
> >>> vs the
> >>>> % value).
> >>>>
> >>>> I hope this makes some sense.
> >>>>
> >>>> Bernard McGarvey
> >>>>
> >>>>
> >>>> Director, Fort Myers Beach Lions Foundation, Inc.
> >>>>
> >>>>
> >>>> Retired (Lilly Engineering Fellow).
> >>>>
> >>>>
> >>>>> On March 27, 2019 at 3:57 PM Paul Murrell
> >>> <paul at stat.auckland.ac.nz>
> >>>> wrote:
> >>>>>
> >>>>>
> >>>>> Are you looking for the contourLines() function ?
> >>>>>
> >>>>> Paul
> >>>>>
> >>>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
> >>>>>> John, I have attached a pdf of the plot. Hopefully you can read
> >>> this.
> >>>>>> If I understand correctly, this plot is basically the 2-D version
> >>> of
> >>>> the 1-D quantile plot.
> >>>>>> Thanks
> >>>>>>
> >>>>>> Bernard McGarvey
> >>>>>>
> >>>>>>
> >>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
> >>>>>>
> >>>>>>
> >>>>>> Retired (Lilly Engineering Fellow).
> >>>>>>
> >>>>>>
> >>>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com>
> >>> wrote:
> >>>>>>>
> >>>>>>> The figure did not get  through. Perhaps try a pdf?
> >>>>>>>
> >>>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
> >>>>>>> <mcgarvey.bernard at comcast.net> wrote:
> >>>>>>>> I want to see if I can reproduce the plot below in R. If I
> >>>> understand it correctly, i takes my bivariate data and creates
> >>> quantile
> >>>> density contours. My interpretation of these contours is that they
> >>> enclose
> >>>> a certain % of the total data. I am using the bkde2D function in
> >>> library
> >>>> KernSmooth which gives density values that can be plotted on a
> >>> contour plot
> >>>> but I would like the curves that enclose a given % of the data, if
> >>> that is
> >>>> possible
> >>>>>>>>
> >>>>>>>> Thanks
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> Bernard McGarvey
> >>>>>>>>
> >>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
> >>>>>>>>
> >>>>>>>> Retired (Lilly Engineering Fellow).
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> ______________________________________________
> >>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>> see
> >>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>>>>>> and provide commented, minimal, self-contained, reproducible
> >>> code.
> >>>>>>>
> >>>>>>>
> >>>>>>> --
> >>>>>>> John Kane
> >>>>>>> Kingston ON Canada
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >>> see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible
> >>> code.
> >>>>> --
> >>>>> Dr Paul Murrell
> >>>>> Department of Statistics
> >>>>> The University of Auckland
> >>>>> Private Bag 92019
> >>>>> Auckland
> >>>>> New Zealand
> >>>>> 64 9 3737599 x85392
> >>>>> paul at stat.auckland.ac.nz
> >>>>> http://www.stat.auckland.ac.nz/~paul/
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>     [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >> -- 
> >> Sent from my phone. Please excuse my brevity.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Mar 28 20:16:12 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Thu, 28 Mar 2019 12:16:12 -0700
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <265298134.538255.1553798009655@connect.xfinity.com>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
 <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
 <c67326f6-1964-5f92-47cc-6a47334c790b@comcast.net>
 <265298134.538255.1553798009655@connect.xfinity.com>
Message-ID: <e4f1de36-f397-0054-8451-3ae9f3057901@comcast.net>


On 3/28/19 11:33 AM, Bernard McGarvey wrote:
> pracma meets my needs - thanks - this is an amazing package with lots of very very useful functions. Thanks for bringing it to my attention.


Excellent. I'll mention also the hdrcde package which addresses some of 
your questions as well. It also has a "sliced" approach to disply of 2d 
densities that I found informative and attractive.


-- 

David.

>
> Bernard McGarvey
>
>
> Director, Fort Myers Beach Lions Foundation, Inc.
>
>
> Retired (Lilly Engineering Fellow).
>
>
>> On March 28, 2019 at 1:40 PM David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>
>>
>> On 3/27/19 3:43 PM, Bernard Comcast wrote:
>>> To follow on Jeff, is there a function to do 2-D (double) numerical integration in R?
>> Packages pracma and cubature offer a variety of solutions to that task.
>>
>>
>> -- 
>>
>> David.
>>
>>
>>> Bernard
>>> Sent from my iPhone so please excuse the spelling!"
>>>
>>>> On Mar 27, 2019, at 6:38 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>>>>
>>>> Regardless of how many dimensions you have for independent variables, the density is one-dimensional, and if you assume the density function has been determined (e.g. by kernel estimation or by a Gaussian copula) then if you integrate the density function along that dimension there will be unique slices of the multivariate input domain determined by those slices. They might in general be disjoint regions of the independent variable space, but that is what the contour function does.
>>>>
>>>> I am not seeing your point, Bert, unless you are unwilling to assume a density function model?
>>>>
>>>>> On March 27, 2019 2:18:18 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>>> You are missing a crucial point. The reals are well ordered; higher
>>>>> dimensions are not. Therefore 2d quantile contours are not unique.
>>>>>
>>>>> Of course assuming I understand your query correctly.
>>>>>
>>>>>
>>>>> Bert
>>>>>
>>>>> On Wed, Mar 27, 2019, 13:55 Bernard McGarvey
>>>>> <mcgarvey.bernard at comcast.net>
>>>>> wrote:
>>>>>
>>>>>> If I understand correctly the ContourLines function gives you the
>>>>> contour
>>>>>> lines when you put in the data. But before this I need to data to put
>>>>> into
>>>>>> that function. I think this is something like a 2D CDF of the data
>>>>> that
>>>>>> then leads to the 2D quantiles but I am not 100% sure. What I am
>>>>> basically
>>>>>> looking for is the 2D curve that encloses say 95% of the data in a
>>>>> similar
>>>>>> fashion to a 1D quantile where the quantile represents the value that
>>>>> x% of
>>>>>> the data is below. I think what I am looking for is the 2D bivariate
>>>>>> version of the 1D quantile plot (where the quantile value is plotted
>>>>> vs the
>>>>>> % value).
>>>>>>
>>>>>> I hope this makes some sense.
>>>>>>
>>>>>> Bernard McGarvey
>>>>>>
>>>>>>
>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>
>>>>>>
>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>
>>>>>>
>>>>>>> On March 27, 2019 at 3:57 PM Paul Murrell
>>>>> <paul at stat.auckland.ac.nz>
>>>>>> wrote:
>>>>>>>
>>>>>>> Are you looking for the contourLines() function ?
>>>>>>>
>>>>>>> Paul
>>>>>>>
>>>>>>>> On 28/03/19 8:37 AM, Bernard McGarvey wrote:
>>>>>>>> John, I have attached a pdf of the plot. Hopefully you can read
>>>>> this.
>>>>>>>> If I understand correctly, this plot is basically the 2-D version
>>>>> of
>>>>>> the 1-D quantile plot.
>>>>>>>> Thanks
>>>>>>>>
>>>>>>>> Bernard McGarvey
>>>>>>>>
>>>>>>>>
>>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>>>
>>>>>>>>
>>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>>>
>>>>>>>>
>>>>>>>>> On March 27, 2019 at 7:44 AM John Kane <jrkrideau at gmail.com>
>>>>> wrote:
>>>>>>>>> The figure did not get  through. Perhaps try a pdf?
>>>>>>>>>
>>>>>>>>> On Tue, 26 Mar 2019 at 13:41, Bernard McGarvey
>>>>>>>>> <mcgarvey.bernard at comcast.net> wrote:
>>>>>>>>>> I want to see if I can reproduce the plot below in R. If I
>>>>>> understand it correctly, i takes my bivariate data and creates
>>>>> quantile
>>>>>> density contours. My interpretation of these contours is that they
>>>>> enclose
>>>>>> a certain % of the total data. I am using the bkde2D function in
>>>>> library
>>>>>> KernSmooth which gives density values that can be plotted on a
>>>>> contour plot
>>>>>> but I would like the curves that enclose a given % of the data, if
>>>>> that is
>>>>>> possible
>>>>>>>>>> Thanks
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Bernard McGarvey
>>>>>>>>>>
>>>>>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>>>>>
>>>>>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> ______________________________________________
>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>> see
>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> John Kane
>>>>>>>>> Kingston ON Canada
>>>>>>>>>
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>> see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>> --
>>>>>>> Dr Paul Murrell
>>>>>>> Department of Statistics
>>>>>>> The University of Auckland
>>>>>>> Private Bag 92019
>>>>>>> Auckland
>>>>>>> New Zealand
>>>>>>> 64 9 3737599 x85392
>>>>>>> paul at stat.auckland.ac.nz
>>>>>>> http://www.stat.auckland.ac.nz/~paul/
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>      [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> -- 
>>>> Sent from my phone. Please excuse my brevity.
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Mar 29 03:16:51 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 28 Mar 2019 19:16:51 -0700
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <e4f1de36-f397-0054-8451-3ae9f3057901@comcast.net>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
 <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
 <c67326f6-1964-5f92-47cc-6a47334c790b@comcast.net>
 <265298134.538255.1553798009655@connect.xfinity.com>
 <e4f1de36-f397-0054-8451-3ae9f3057901@comcast.net>
Message-ID: <16E8277F-5C7A-4A6D-BB45-C2C3B41725B4@dcn.davis.ca.us>

I just found the "ks" package which looks promising... [1]

https://cran.r-project.org/web/packages/ks/vignettes/kde.pdf

On March 28, 2019 12:16:12 PM PDT, David Winsemius <dwinsemius at comcast.net> wrote:
>
>On 3/28/19 11:33 AM, Bernard McGarvey wrote:
>> pracma meets my needs - thanks - this is an amazing package with lots
>of very very useful functions. Thanks for bringing it to my attention.
>
>
>Excellent. I'll mention also the hdrcde package which addresses some of
>
>your questions as well. It also has a "sliced" approach to disply of 2d
>
>densities that I found informative and attractive.

-- 
Sent from my phone. Please excuse my brevity.


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Fri Mar 29 03:19:30 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Thu, 28 Mar 2019 22:19:30 -0400
Subject: [R] [FORGED] Re: Quantile Density Contours
In-Reply-To: <16E8277F-5C7A-4A6D-BB45-C2C3B41725B4@dcn.davis.ca.us>
References: <2047358266.494500.1553539993171@connect.xfinity.com>
 <CAKZQJMBbEd-DGVZDmyUiJY+n60bE9edaKcEmnCGmR-xq-hHrgA@mail.gmail.com>
 <2045542728.525463.1553715426554@connect.xfinity.com>
 <23e91a67-0e54-1150-ce19-ec5e0fa55a8c@stat.auckland.ac.nz>
 <654807954.527013.1553720118524@connect.xfinity.com>
 <CAGxFJbRWkGTxnMBwF+y_G-ikZMfFCiVJsacPtOfYZeABArOfLA@mail.gmail.com>
 <DC975F45-1E0C-4AA4-90DB-88CFCE73FF96@dcn.davis.ca.us>
 <85093750-2B17-45D8-98AD-62BBD7207A03@comcast.net>
 <c67326f6-1964-5f92-47cc-6a47334c790b@comcast.net>
 <265298134.538255.1553798009655@connect.xfinity.com>
 <e4f1de36-f397-0054-8451-3ae9f3057901@comcast.net>
 <16E8277F-5C7A-4A6D-BB45-C2C3B41725B4@dcn.davis.ca.us>
Message-ID: <9CD0F113-40AF-4174-A2F7-FA77E58DB72F@comcast.net>

I will take a look but the hdrcde package appears to give me the plots I was looking for.

Thanks

Bernard
Sent from my iPhone so please excuse the spelling!"

> On Mar 28, 2019, at 10:16 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> I just found the "ks" package which looks promising... [1]
> 
> https://cran.r-project.org/web/packages/ks/vignettes/kde.pdf
> 
>> On March 28, 2019 12:16:12 PM PDT, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>>> On 3/28/19 11:33 AM, Bernard McGarvey wrote:
>>> pracma meets my needs - thanks - this is an amazing package with lots
>> of very very useful functions. Thanks for bringing it to my attention.
>> 
>> 
>> Excellent. I'll mention also the hdrcde package which addresses some of
>> 
>> your questions as well. It also has a "sliced" approach to disply of 2d
>> 
>> densities that I found informative and attractive.
> 
> -- 
> Sent from my phone. Please excuse my brevity.


From @||redo@rocc@to @end|ng |rom |@@twebnet@|t  Fri Mar 29 14:35:17 2019
From: @||redo@rocc@to @end|ng |rom |@@twebnet@|t (Alfredo)
Date: Fri, 29 Mar 2019 14:35:17 +0100
Subject: [R] Structuring data for Correspondence Analysis
Message-ID: <005201d4e634$3f537930$bdfa6b90$@fastwebnet.it>

Hi, I am very new to r and need help from you to do a correspondence
analysis because I don't know how to structure the following data:

Thank you.

Alfredo

 

library(ca,lib.loc=folder)

table <- read.csv(file="C:\\Temp\\Survey_Data.csv", header=TRUE, sep=",")

head (table, n=20)

                Preference   Sex        Age   Time

1           News/Info/Talk     M      25-30  06-09

2                Classical     F      >35    09-12

3          Rock and Top 40     F      21-25  12-13

4                     Jazz     M      >35    13-16  

5           News/Info/Talk     F      25-30  16-18

6             Don't listen     F      30-35  18-20

...

19         Rock and Top 40     M      25-30  16-18

20          Easy Listening     F      >35    18-20

 

In SAS I would simply do this:

proc corresp data=table dim=2 outc=_coord;

   table Preference, Sex Age Time;

run;

 

I don't know how convert in R a data frame to a frequency table to execute
properly this function:

ca <- ca(<frequency table>, graph=FALSE)


	[[alternative HTML version deleted]]


From |rymor @end|ng |rom gm@||@com  Fri Mar 29 14:07:16 2019
From: |rymor @end|ng |rom gm@||@com (Assa Yeroslaviz)
Date: Fri, 29 Mar 2019 14:07:16 +0100
Subject: [R] converting a character string to an object name
Message-ID: <CA+8Xemyj_GmqMr4tuV=R1KfzZ3DCVh7WiQmNV7uRK0Jk11_xxA@mail.gmail.com>

I am trying to automate the way i read my tables. I have an Excel sheet I'm
reading using openxlsx package. The sheet contains over 30 sheets, i would
like to save each of them as separate objects.

my workflow for now is as such:

wb <- loadWorkbook(xlsxFile = "Output/Up_Down_Regulated_Gene_Lists.xlsx")
NAMES <- gsub(pattern = " ", replacement = "_", x = names(wb))
[1] "KO1_vs._WT_up"    "KO2_vs._WT_down"  "KO3_vs._WT_up"
"KO1_vs._WT_down"  "KO2_vs._WT_up" ...
for (i in 2:length(names(wb)) ){
  tmp <- read.xlsx(wb, sheet = i)

... Here I would like to have each sheet read in and sacved as a separate
data.frame.

}

Is there a way to read for each sheet the name from names(wb) and convert
it to a name for the object?

something like this
The object  KO1_vs._WT_up will save the first sheet with the same name
The object  KO2_vs._WT_down will save the second sheet.
etc.

Any ideas?

thanks
Assa

	[[alternative HTML version deleted]]


From b@k|un@| @end|ng |rom y@hoo@com  Fri Mar 29 18:13:14 2019
From: b@k|un@| @end|ng |rom y@hoo@com (Baki UNAL)
Date: Fri, 29 Mar 2019 17:13:14 +0000 (UTC)
Subject: [R] Error while using keras package
References: <167524176.12080371.1553879595537.ref@mail.yahoo.com>
Message-ID: <167524176.12080371.1553879595537@mail.yahoo.com>

Hello
I successfully installed keras and TensorFlow backend with install_keras(). I attached the installation log as keras_install_log.txt. After that I tried to download mnist data with dataset_mnist() function. However I got the following error message:
#----------------------------------------------------------------------------
> library(keras)
> mnist <- dataset_mnist()
Using TensorFlow backend.
Error: ImportError: Traceback (most recent call last):
? File "C:\Users\user\MINICO~1\envs\R-TENS~1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py", line 14, in swig_import_helper? ? return importlib.import_module(mname)
? File "C:\Users\user\MINICO~1\envs\R-TENS~1\lib\importlib\__init__.py", line 126, in import_module? ? return _bootstrap._gcd_import(name[level:], package, level)
? File "<frozen importlib._bootstrap>", line 994, in _gcd_import
? File "<frozen importlib._bootstrap>", line 971, in _find_and_load
? File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
? File "<frozen importlib._bootstrap>", line 658, in _load_unlocked
? File "<frozen importlib._bootstrap>", line 571, in module_from_spec
? File "<frozen importlib._bootstrap_external>", line 922, in create_module
? File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
ImportError: DLL load failed with error code -1073741795

During handling of the above exce
#---------------------------------------------------------------------------------------

I also attached the error message as error_message.txt
How can I fix this problem.
Thanks


-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: keras_install_log.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190329/319f082b/attachment.txt>

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: error_message.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20190329/319f082b/attachment-0001.txt>

From kry|ov@r00t @end|ng |rom gm@||@com  Fri Mar 29 18:54:36 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 29 Mar 2019 20:54:36 +0300
Subject: [R] converting a character string to an object name
In-Reply-To: <CA+8Xemyj_GmqMr4tuV=R1KfzZ3DCVh7WiQmNV7uRK0Jk11_xxA@mail.gmail.com>
References: <CA+8Xemyj_GmqMr4tuV=R1KfzZ3DCVh7WiQmNV7uRK0Jk11_xxA@mail.gmail.com>
Message-ID: <20190329205436.675e578e@trisector>

On Fri, 29 Mar 2019 14:07:16 +0100
Assa Yeroslaviz <frymor at gmail.com> wrote:

> Is there a way to read for each sheet the name from names(wb) and
> convert it to a name for the object?

See `get` and `assign` functions for a way to use strings as object
names. Generally, it might not be a good idea to do that (what if your
spreadsheet contains a sheet named `c` or `q`, or `data.frame`?).

If you want to save yourself some typing, consider using `with` or
`within`, though they suffer from similar problems.

-- 
Best regards,
Ivan


From bgunter@4567 @end|ng |rom gm@||@com  Fri Mar 29 18:55:19 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 29 Mar 2019 10:55:19 -0700
Subject: [R] converting a character string to an object name
In-Reply-To: <CA+8Xemyj_GmqMr4tuV=R1KfzZ3DCVh7WiQmNV7uRK0Jk11_xxA@mail.gmail.com>
References: <CA+8Xemyj_GmqMr4tuV=R1KfzZ3DCVh7WiQmNV7uRK0Jk11_xxA@mail.gmail.com>
Message-ID: <CAGxFJbSA+zpZmgWQ0=PgfZasZx+dvOpvsn__y6ifMMMTwaGWRQ@mail.gmail.com>

I think you want ?assign


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Mar 29, 2019 at 10:39 AM Assa Yeroslaviz <frymor at gmail.com> wrote:

> I am trying to automate the way i read my tables. I have an Excel sheet I'm
> reading using openxlsx package. The sheet contains over 30 sheets, i would
> like to save each of them as separate objects.
>
> my workflow for now is as such:
>
> wb <- loadWorkbook(xlsxFile = "Output/Up_Down_Regulated_Gene_Lists.xlsx")
> NAMES <- gsub(pattern = " ", replacement = "_", x = names(wb))
> [1] "KO1_vs._WT_up"    "KO2_vs._WT_down"  "KO3_vs._WT_up"
> "KO1_vs._WT_down"  "KO2_vs._WT_up" ...
> for (i in 2:length(names(wb)) ){
>   tmp <- read.xlsx(wb, sheet = i)
>
> ... Here I would like to have each sheet read in and sacved as a separate
> data.frame.
>
> }
>
> Is there a way to read for each sheet the name from names(wb) and convert
> it to a name for the object?
>
> something like this
> The object  KO1_vs._WT_up will save the first sheet with the same name
> The object  KO2_vs._WT_down will save the second sheet.
> etc.
>
> Any ideas?
>
> thanks
> Assa
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Mar 29 19:33:01 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 29 Mar 2019 18:33:01 +0000
Subject: [R] converting a character string to an object name
In-Reply-To: <CA+8Xemyj_GmqMr4tuV=R1KfzZ3DCVh7WiQmNV7uRK0Jk11_xxA@mail.gmail.com>
References: <CA+8Xemyj_GmqMr4tuV=R1KfzZ3DCVh7WiQmNV7uRK0Jk11_xxA@mail.gmail.com>
Message-ID: <5892cc07-7e5a-ece5-abe9-f29c2d6fb85e@sapo.pt>

Hello,

I recommend you save them all in a list. It's a much better idea than to 
have length(names(wb)) objects in the globalenv.
Something like the following would read them all in one go.

xls_list <- lapply(seq_along(names(wb))[-1], function(i){
     read.xlsx(wb, sheet = i)
})

names(xls_list) <- gsub(" ", "_", names(wb)[-1])


Then, you can process the list members with the *apply() functions, as in

result_list <- lapply(xls_list, some_function, arg2, arg3)

The first function argument is the sheet, arg2, etc, would be other 
function arguments.

Hope this helps,

Rui Barradas

?s 13:07 de 29/03/2019, Assa Yeroslaviz escreveu:
> I am trying to automate the way i read my tables. I have an Excel sheet I'm
> reading using openxlsx package. The sheet contains over 30 sheets, i would
> like to save each of them as separate objects.
> 
> my workflow for now is as such:
> 
> wb <- loadWorkbook(xlsxFile = "Output/Up_Down_Regulated_Gene_Lists.xlsx")
> NAMES <- gsub(pattern = " ", replacement = "_", x = names(wb))
> [1] "KO1_vs._WT_up"    "KO2_vs._WT_down"  "KO3_vs._WT_up"
> "KO1_vs._WT_down"  "KO2_vs._WT_up" ...
> for (i in 2:length(names(wb)) ){
>    tmp <- read.xlsx(wb, sheet = i)
> 
> ... Here I would like to have each sheet read in and sacved as a separate
> data.frame.
> 
> }
> 
> Is there a way to read for each sheet the name from names(wb) and convert
> it to a name for the object?
> 
> something like this
> The object  KO1_vs._WT_up will save the first sheet with the same name
> The object  KO2_vs._WT_down will save the second sheet.
> etc.
> 
> Any ideas?
> 
> thanks
> Assa
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Mar 29 20:16:57 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 29 Mar 2019 12:16:57 -0700
Subject: [R] converting a character string to an object name
In-Reply-To: <5892cc07-7e5a-ece5-abe9-f29c2d6fb85e@sapo.pt>
References: <CA+8Xemyj_GmqMr4tuV=R1KfzZ3DCVh7WiQmNV7uRK0Jk11_xxA@mail.gmail.com>
 <5892cc07-7e5a-ece5-abe9-f29c2d6fb85e@sapo.pt>
Message-ID: <E9FD1E5A-67B5-4E93-9390-19D9B21881A6@dcn.davis.ca.us>

Seconded!

On March 29, 2019 11:33:01 AM PDT, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>Hello,
>
>I recommend you save them all in a list. It's a much better idea than
>to 
>have length(names(wb)) objects in the globalenv.
>Something like the following would read them all in one go.
>
>xls_list <- lapply(seq_along(names(wb))[-1], function(i){
>     read.xlsx(wb, sheet = i)
>})
>
>names(xls_list) <- gsub(" ", "_", names(wb)[-1])
>
>
>Then, you can process the list members with the *apply() functions, as
>in
>
>result_list <- lapply(xls_list, some_function, arg2, arg3)
>
>The first function argument is the sheet, arg2, etc, would be other 
>function arguments.
>
>Hope this helps,
>
>Rui Barradas
>
>?s 13:07 de 29/03/2019, Assa Yeroslaviz escreveu:
>> I am trying to automate the way i read my tables. I have an Excel
>sheet I'm
>> reading using openxlsx package. The sheet contains over 30 sheets, i
>would
>> like to save each of them as separate objects.
>> 
>> my workflow for now is as such:
>> 
>> wb <- loadWorkbook(xlsxFile =
>"Output/Up_Down_Regulated_Gene_Lists.xlsx")
>> NAMES <- gsub(pattern = " ", replacement = "_", x = names(wb))
>> [1] "KO1_vs._WT_up"    "KO2_vs._WT_down"  "KO3_vs._WT_up"
>> "KO1_vs._WT_down"  "KO2_vs._WT_up" ...
>> for (i in 2:length(names(wb)) ){
>>    tmp <- read.xlsx(wb, sheet = i)
>> 
>> ... Here I would like to have each sheet read in and sacved as a
>separate
>> data.frame.
>> 
>> }
>> 
>> Is there a way to read for each sheet the name from names(wb) and
>convert
>> it to a name for the object?
>> 
>> something like this
>> The object  KO1_vs._WT_up will save the first sheet with the same
>name
>> The object  KO2_vs._WT_down will save the second sheet.
>> etc.
>> 
>> Any ideas?
>> 
>> thanks
>> Assa
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jho|tm@n @end|ng |rom gm@||@com  Fri Mar 29 22:24:31 2019
From: jho|tm@n @end|ng |rom gm@||@com (jim holtman)
Date: Fri, 29 Mar 2019 14:24:31 -0700
Subject: [R] aggregate output to data frame
In-Reply-To: <CA+8X3fXotyay_+hUDD6LGo4G5OQ_R0+ZytXSdLVvrm962DT5WQ@mail.gmail.com>
References: <1277208145.12198189.1553719207167.ref@mail.yahoo.com>
 <1277208145.12198189.1553719207167@mail.yahoo.com>
 <CA+8X3fXotyay_+hUDD6LGo4G5OQ_R0+ZytXSdLVvrm962DT5WQ@mail.gmail.com>
Message-ID: <CAAxdm-6nj64Z3voLKO0K2efY9JGfYR3JVRNrJsLKD1XPv8AiGA@mail.gmail.com>

You can also use 'dplyr'

library(tidyverse)
result <- pcr %>%
  group_by(Gene, Type, Rep) %>%
  summarise(mean = mean(Ct),
                   sd = sd(Ct),
                   oth = sd(Ct) / sqrt(sd(Ct))
  )

Jim Holtman
*Data Munger Guru*


*What is the problem that you are trying to solve?Tell me what you want to
do, not how you want to do it.*


On Wed, Mar 27, 2019 at 7:40 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Cyrus,
> Try this:
>
> pcr<-data.frame(Ct=runif(66,10,20),Gene=rep(LETTERS[1:22],3),
>  Type=rep(c("Std","Unkn"),33),Rep=rep(1:3,each=22))
> testagg<-aggregate(pcr$Ct,c(pcr["Gene"],pcr["Type"],pcr["Rep"]),
>  FUN=function(x){c(mean(x), sd(x), sd(x)/sqrt(sd(x)))})
> nxcol<-dim(testagg$x)[2]
> newxs<-paste("x",1:nxcol,sep="")
> for(col in 1:nxcol)
>  testagg[[newxs[col]]]<-testagg$x[,col]
> testagg$x<-NULL
>
> Jim
>
> On Thu, Mar 28, 2019 at 12:39 PM cir p via R-help <r-help at r-project.org>
> wrote:
> >
> > Dear users,
> > i am trying to summarize data using "aggregate" with the following
> command:
> >
> >
> aggregate(pcr$Ct,c(pcr["Gene"],pcr["Type"],pcr["Rep"]),FUN=function(x){c(mean(x),
> sd(x), sd(x)/sqrt(sd(x)))})
> >
> > and the structure of the resulting data frame is
> >
> > 'data.frame':    66 obs. of  4 variables:
> > $ Gene: Factor w/ 22 levels "14-3-3e","Act5C",..: 1 2 3 4 5 6 7 8 9 10
> ...
> > $ Type: Factor w/ 2 levels "Std","Unkn": 2 2 2 2 2 2 2 2 2 2 ...
> > $ Rep : int  1 1 1 1 1 1 1 1 1 1 ...
> >  $ x   : num [1:66, 1:3] 16.3 16.7 18.2 17.1 18.6 ...
> >
> > The actual data is "bundled" in a matrix $x of the data frame. I would
> like to have the columns of this matrix as individual numeric columns in
> the same data frame instead of a matrix, but cant really figure it out how
> to do this in an efficient way. Could someone help me with the construction
> of this?
> >
> > Thanks a lot,
> >
> > Cyrus
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jho|tm@n @end|ng |rom gm@||@com  Fri Mar 29 22:29:29 2019
From: jho|tm@n @end|ng |rom gm@||@com (jim holtman)
Date: Fri, 29 Mar 2019 14:29:29 -0700
Subject: [R] Structuring data for Correspondence Analysis
In-Reply-To: <005201d4e634$3f537930$bdfa6b90$@fastwebnet.it>
References: <005201d4e634$3f537930$bdfa6b90$@fastwebnet.it>
Message-ID: <CAAxdm-4+mtgznadVne7q0dc8x0F7RwFDyuHSWokyH7WeXjCUbg@mail.gmail.com>

I am not familiar with SAS, so what did you want your output to look like.
There is the 'table' function that might do the job and then there is
always 'dplyr' which can do the hard stuff.  So we need more information on
what you want.

Jim Holtman
*Data Munger Guru*


*What is the problem that you are trying to solve?Tell me what you want to
do, not how you want to do it.*


On Fri, Mar 29, 2019 at 6:35 AM Alfredo <alfredo.roccato at fastwebnet.it>
wrote:

> Hi, I am very new to r and need help from you to do a correspondence
> analysis because I don't know how to structure the following data:
>
> Thank you.
>
> Alfredo
>
>
>
> library(ca,lib.loc=folder)
>
> table <- read.csv(file="C:\\Temp\\Survey_Data.csv", header=TRUE, sep=",")
>
> head (table, n=20)
>
>                 Preference   Sex        Age   Time
>
> 1           News/Info/Talk     M      25-30  06-09
>
> 2                Classical     F      >35    09-12
>
> 3          Rock and Top 40     F      21-25  12-13
>
> 4                     Jazz     M      >35    13-16
>
> 5           News/Info/Talk     F      25-30  16-18
>
> 6             Don't listen     F      30-35  18-20
>
> ...
>
> 19         Rock and Top 40     M      25-30  16-18
>
> 20          Easy Listening     F      >35    18-20
>
>
>
> In SAS I would simply do this:
>
> proc corresp data=table dim=2 outc=_coord;
>
>    table Preference, Sex Age Time;
>
> run;
>
>
>
> I don't know how convert in R a data frame to a frequency table to execute
> properly this function:
>
> ca <- ca(<frequency table>, graph=FALSE)
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Sat Mar 30 01:55:14 2019
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Fri, 29 Mar 2019 20:55:14 -0400
Subject: [R] Quantile Density Contours
In-Reply-To: <CAB8pepw7eqS+qLc76mDZjWPODY3sWJoyQDwnQD3nt7m0DRah1Q@mail.gmail.com>
References: <CAB8pepw7eqS+qLc76mDZjWPODY3sWJoyQDwnQD3nt7m0DRah1Q@mail.gmail.com>
Message-ID: <0B074B73-3F90-4D45-BA21-6EAB4B363892@comcast.net>

Thanks Abs - I was able to get the plot I needed with the hdrcde package but I will check out your package as well.

I continue to be impressed with the power Of R and the various packages available.

Thanks again


Bernard
Sent from my iPhone so please excuse the spelling!"

> On Mar 29, 2019, at 8:51 PM, Abs Spurdle <spurdle.a at gmail.com> wrote:
> 
> My R package, "probhat", provides plots of bivariate PDFs and bivariate CDFs, using kernel smoothing.
> Note that there is no bivariate quantile function, as such.
> 
> Here's the vignette:
> https://cran.r-project.org/web/packages/probhat/vignettes/probhat.pdf
> 
> This contains examples.
> 
> Note that I'm not subscribed to this mailing list.
> (Maybe I will subscribe).
> And I had difficulty reading the email addresses from the archive page.
> 

	[[alternative HTML version deleted]]


From dj|uckett @end|ng |rom gm@||@com  Sat Mar 30 03:49:30 2019
From: dj|uckett @end|ng |rom gm@||@com (David Luckett)
Date: Sat, 30 Mar 2019 13:49:30 +1100
Subject: [R] 
 Analysing diallel and line x tester mating design with multiple
 location in R
Message-ID: <CAM4LFTJbf8NSdvjyTXoh7fPNu=KOZzGko=oML+dgyTVkd1LoxA@mail.gmail.com>

Hi Gbemisola,
You can find functions for diallel analysis (Griffing and Hayman) and
LxT analysis in several R packages:
agricolae (on CRAN),
DiallelAnalysisR (on CRAN),
plantbreeding (on R-forge), and
AGD-R (on the CIMMYT website; a front-end built on R).

If these are not enough (!) you could try other stand-alone software,
such as: PBTOOLS, GENES, QGAStation, TNAUSTAT, ICRISAT Genstat
scripts, GSCA (not free), or Dial98.

HTH, Cheers
David Luckett
djluckett at gmail.com
0408 750 703


From jrkr|de@u @end|ng |rom gm@||@com  Sat Mar 30 14:15:48 2019
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Sat, 30 Mar 2019 09:15:48 -0400
Subject: [R] Structuring data for Correspondence Analysis
In-Reply-To: <CAAxdm-4+mtgznadVne7q0dc8x0F7RwFDyuHSWokyH7WeXjCUbg@mail.gmail.com>
References: <005201d4e634$3f537930$bdfa6b90$@fastwebnet.it>
 <CAAxdm-4+mtgznadVne7q0dc8x0F7RwFDyuHSWokyH7WeXjCUbg@mail.gmail.com>
Message-ID: <CAKZQJMBQhY8pEeGbJfDomGp8iw=o_8WYYCwZe5yz=VvJMrvHTA@mail.gmail.com>

Hi Alfredo,
I have not used SAS nor done a correspondence analysis in many years
but to give R-help readers an idea of what you are doing, we probably
need a short statement of the substantive  problem  that would lead to
the SAS program:
proc corresp data=table dim=2 outc=_coord;
   table Preference, Sex Age Time;

I believe that there are several packages in R that will do a
correspondence analysis (For one see
https://www.statmethods.net/advstats/ca.html). Have you checked out
any of the packages? If so which one are you thinking of using?

Next, we need to see some sample data. Have a look at these two links
that may help you give us more information on the problem and what you
are looking for.  It is important to supply some sample data. It does
not have to be much.  The very best way to supply the sample data is
to use the dput() function that you will find described in the links.

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

 http://adv-r.had.co.nz/Reproducibility.html

On Fri, 29 Mar 2019 at 17:35, jim holtman <jholtman at gmail.com> wrote:
>
> I am not familiar with SAS, so what did you want your output to look like.
> There is the 'table' function that might do the job and then there is
> always 'dplyr' which can do the hard stuff.  So we need more information on
> what you want.
>
> Jim Holtman
> *Data Munger Guru*
>
>
> *What is the problem that you are trying to solve?Tell me what you want to
> do, not how you want to do it.*
>
>
> On Fri, Mar 29, 2019 at 6:35 AM Alfredo <alfredo.roccato at fastwebnet.it>
> wrote:
>
> > Hi, I am very new to r and need help from you to do a correspondence
> > analysis because I don't know how to structure the following data:
> >
> > Thank you.
> >
> > Alfredo
> >
> >
> >
> > library(ca,lib.loc=folder)
> >
> > table <- read.csv(file="C:\\Temp\\Survey_Data.csv", header=TRUE, sep=",")
> >
> > head (table, n=20)
> >
> >                 Preference   Sex        Age   Time
> >
> > 1           News/Info/Talk     M      25-30  06-09
> >
> > 2                Classical     F      >35    09-12
> >
> > 3          Rock and Top 40     F      21-25  12-13
> >
> > 4                     Jazz     M      >35    13-16
> >
> > 5           News/Info/Talk     F      25-30  16-18
> >
> > 6             Don't listen     F      30-35  18-20
> >
> > ...
> >
> > 19         Rock and Top 40     M      25-30  16-18
> >
> > 20          Easy Listening     F      >35    18-20
> >
> >
> >
> > In SAS I would simply do this:
> >
> > proc corresp data=table dim=2 outc=_coord;
> >
> >    table Preference, Sex Age Time;
> >
> > run;
> >
> >
> >
> > I don't know how convert in R a data frame to a frequency table to execute
> > properly this function:
> >
> > ca <- ca(<frequency table>, graph=FALSE)
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
John Kane
Kingston ON Canada


From |r|end|y @end|ng |rom yorku@c@  Sat Mar 30 16:51:40 2019
From: |r|end|y @end|ng |rom yorku@c@ (Michael Friendly)
Date: Sat, 30 Mar 2019 11:51:40 -0400
Subject: [R] Structuring data for Correspondence Analysis
In-Reply-To: <005201d4e634$3f537930$bdfa6b90$@fastwebnet.it>
References: <005201d4e634$3f537930$bdfa6b90$@fastwebnet.it>
Message-ID: <e7fce186-379d-620d-291c-f3f989972db8@yorku.ca>

I think something like table(Preference, Sex, data=table) will get you 
started. With 3+ variables, you are probably looking for a MCA analysis
or simple CA using the stacked approach.

Your SAS table statement,

table Preference, Sex Age Time;

treats Preference vs. all combinations of Sex, Age & Time.  This 
corresponds to a loglinear model asserting Preference is jointly
independent of the other three.

See the vignette for the vcdExtra package for this kind of thing more 
generally.

install.packages("vcdExtra")
browseVignettes("vcdExtra")

See my book, Discrete Data Analysis with R, http://ddar.datavis.ca/

best,
-Michael

On 3/29/2019 9:35 AM, Alfredo wrote:
> Hi, I am very new to r and need help from you to do a correspondence
> analysis because I don't know how to structure the following data:
> 
> Thank you.
> 
> Alfredo
> 
>   
> 
> library(ca,lib.loc=folder)
> 
> table <- read.csv(file="C:\\Temp\\Survey_Data.csv", header=TRUE, sep=",")
> 
> head (table, n=20)
> 
>                  Preference   Sex        Age   Time
> 
> 1           News/Info/Talk     M      25-30  06-09
> 
> 2                Classical     F      >35    09-12
> 
> 3          Rock and Top 40     F      21-25  12-13
> 
> 4                     Jazz     M      >35    13-16
> 
> 5           News/Info/Talk     F      25-30  16-18
> 
> 6             Don't listen     F      30-35  18-20
> 
> ...
> 
> 19         Rock and Top 40     M      25-30  16-18
> 
> 20          Easy Listening     F      >35    18-20
> 
>   
> 
> In SAS I would simply do this:
> 
> proc corresp data=table dim=2 outc=_coord;
> 
>     table Preference, Sex Age Time;
> 
> run;
> 
>   
> 
> I don't know how convert in R a data frame to a frequency table to execute
> properly this function:
> 
> ca <- ca(<frequency table>, graph=FALSE)
> 
> 
> 	[[alternative HTML version deleted]]
> 


-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept. & Chair, ASA Statistical Graphics Section
York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
4700 Keele Street    Web:   http://www.datavis.ca  |  @datavisFriendly
Toronto, ONT  M3J 1P3 CANADA


From @purd|e@@ @end|ng |rom gm@||@com  Sat Mar 30 01:51:54 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abs Spurdle)
Date: Sat, 30 Mar 2019 13:51:54 +1300
Subject: [R] Quantile Density Contours
Message-ID: <CAB8pepw7eqS+qLc76mDZjWPODY3sWJoyQDwnQD3nt7m0DRah1Q@mail.gmail.com>

My R package, "probhat", provides plots of bivariate PDFs and bivariate
CDFs, using kernel smoothing.
Note that there is no bivariate quantile function, as such.

Here's the vignette:
https://cran.r-project.org/web/packages/probhat/vignettes/probhat.pdf

This contains examples.

Note that I'm not subscribed to this mailing list.
(Maybe I will subscribe).
And I had difficulty reading the email addresses from the archive page.

	[[alternative HTML version deleted]]


From bie@ve@idozom@ m@iii@g oii gm@ii@com  Sun Mar 31 09:57:45 2019
From: bie@ve@idozom@ m@iii@g oii gm@ii@com (bie@ve@idozom@ m@iii@g oii gm@ii@com)
Date: Sun, 31 Mar 2019 07:57:45 +0000 (UTC)
Subject: [R] Cluster analysis
References: <797345254.22627454.1554019065212.ref@mail.yahoo.com>
Message-ID: <797345254.22627454.1554019065212@mail.yahoo.com>

Hi,
I have data from farmers with different variables. I would like to classify them according to some variables. Can you help me with "R" to find the best variables to classify them and how to classify them with "R". Some variables are numerical others are ordinal.

Best regards,
Bienvenue
	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Sun Mar 31 23:48:37 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Sun, 31 Mar 2019 17:48:37 -0400
Subject: [R] Cluster analysis
In-Reply-To: <797345254.22627454.1554019065212@mail.yahoo.com>
References: <797345254.22627454.1554019065212.ref@mail.yahoo.com>
 <797345254.22627454.1554019065212@mail.yahoo.com>
Message-ID: <CAM_vjukjwH-t8pLauC21kufCax_49eDjnpg0FnuTZmDnQcYy+Q@mail.gmail.com>

Hi,

R has a vast array of tools for cluster analysis. There's even a task
view: https://cran.r-project.org/web/views/Cluster.html

Which method is best for your needs is going to require you spending
some time working to understand the pros and cons, and possibly
consulting with a local statistician.

Sarah

On Sun, Mar 31, 2019 at 4:20 PM bienvenidozoma at gmail.com
<bienvenidozoma at gmail.com> wrote:
>
> Hi,
> I have data from farmers with different variables. I would like to classify them according to some variables. Can you help me with "R" to find the best variables to classify them and how to classify them with "R". Some variables are numerical others are ordinal.
>
> Best regards,
> Bienvenue
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


