From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Aug  2 09:10:43 2022
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 2 Aug 2022 09:10:43 +0200
Subject: [R] Predicted values from glm() when linear predictor is NA.
In-Reply-To: <20220728141949.53afcb46@rolf-Latitude-E7470>
References: <20220728122628.4381daa4@rolf-Latitude-E7470>
 <5371d080-d666-f059-c6f8-e3051a0faea2@comcast.net>
 <20220728141949.53afcb46@rolf-Latitude-E7470>
Message-ID: <25320.52723.249947.577244@stat.math.ethz.ch>

>>>>> Rolf Turner 
>>>>>     on Thu, 28 Jul 2022 14:19:49 +1200 writes:

    > On Wed, 27 Jul 2022 18:25:23 -0700 David Winsemius
    > <dwinsemius at comcast.net> wrote:

    > <SNIP>

    >> The NA is most likely caused by aliasing, so some other
    >> combination of factors a perfect surrogate for every case
    >> with that level of the interaction.

    > <SNIP>

    > No, I think it's much simpler than that.  Essentially it
    > boils down to the fact that if y is 1:10 and x is
    > rep(1,10) then

    >     lm(y ~ x)

    > gives a slope estimate of NA.  As it clearly should.

*and*  predict() should surely *not* be NA, either, right ??

You can easily confirm with

 x <- rep(1,7); y <- 1:7; summary(fm <- lm(y~x))
 predict(fm)

which gives all '4'  (= mean(y))

Note that this *is* aliasing:  x is aliased with the intercept,
and it fits entirely your situation:

If a predictor variable is superfluous, i.e., more precisely, a
column of the X design matrix is an exact linear combination of
previous columns,
prediction is no problem at all and gives exactly what you'd get
if you omit that superfluous column/variable.

Are you sure this is not simply your situation?
Martin


    > cheers,
    > Rolf

    > -- 
    > Honorary Research Fellow Department of Statistics
    > University of Auckland Phone: +64-9-373-7599 ext. 88276


From |u||@dmtru @end|ng |rom gm@||@com  Tue Aug  2 11:18:06 2022
From: |u||@dmtru @end|ng |rom gm@||@com (Iulia Dumitru)
Date: Tue, 2 Aug 2022 12:18:06 +0300
Subject: [R] I don't understand the result of `svyboxplot` function from the
 Survey package
Message-ID: <06F1D8CB-79C6-4FAF-B116-709508A3E211@gmail.com>

After following the example given here: https://www.rdocumentation.org/packages/survey/versions/4.1-1/topics/svyhist
for `svyboxplot` I get the result in the attached image. This is a box plot of the `enroll` variable from the stratified dataset `apistrat`, grouped by `stype`: E (elementary school), M (middle school) and H (high school). If I use the `svyby` function to group the data by `stype` and find the mean for each group, I get this result:

> svyby(~enroll, ~stype, dstrat, svymean)
  stype  enroll       se
E     E  416.78 16.41740
H     H 1320.70 91.70781
M     M  832.48 54.52157

Clearly the means are very different from each other. Then why don?t the box plots show this? I don?t know how to interpret the plot. Could someone please offer some insight on this? Thank you!



From @jd@m|co @end|ng |rom gm@||@com  Tue Aug  2 13:49:48 2022
From: @jd@m|co @end|ng |rom gm@||@com (Anthony Damico)
Date: Tue, 2 Aug 2022 07:49:48 -0400
Subject: [R] 
 I don't understand the result of `svyboxplot` function from the
 Survey package
In-Reply-To: <06F1D8CB-79C6-4FAF-B116-709508A3E211@gmail.com>
References: <06F1D8CB-79C6-4FAF-B116-709508A3E211@gmail.com>
Message-ID: <CAOwvMDy4u-rSxsRogzCzQqLq2gpgt4dKdY+cit6MtWHLNCt1iw@mail.gmail.com>

hi, nice catch!  i'm ccing the author of the survey package because this
might be a issue.  when i run ?svyboxplot, i also see all three boxes in
the exact same place..  seems like the svyby() call inside of svyboxplot
does something unexpected when svyquantile gets passed using keep.var=FALSE
and ci=FALSE


library(survey)

data(api)

dstrat <- svydesign(id = ~1, strata = ~stype, weights = ~pw, data =
apistrat, fpc = ~fpc)

# looks OK
svyby(~enroll, ~stype, dstrat, svyquantile, quantiles = c(0, 0.25, 0.5,
0.75, 1), na.rm = TRUE)

# returns each result three times in an unexpected configuration..
svyboxplot then grabs the repeated information from the first six columns
svyby(~enroll, ~stype, dstrat, svyquantile, ci = FALSE, keep.var = FALSE,
quantiles = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)





On Tue, Aug 2, 2022 at 7:30 AM Iulia Dumitru <iuliadmtru at gmail.com> wrote:

> After following the example given here:
> https://www.rdocumentation.org/packages/survey/versions/4.1-1/topics/svyhist
> for `svyboxplot` I get the result in the attached image. This is a box
> plot of the `enroll` variable from the stratified dataset `apistrat`,
> grouped by `stype`: E (elementary school), M (middle school) and H (high
> school). If I use the `svyby` function to group the data by `stype` and
> find the mean for each group, I get this result:
>
> > svyby(~enroll, ~stype, dstrat, svymean)
>   stype  enroll       se
> E     E  416.78 16.41740
> H     H 1320.70 91.70781
> M     M  832.48 54.52157
>
> Clearly the means are very different from each other. Then why don?t the
> box plots show this? I don?t know how to interpret the plot. Could someone
> please offer some insight on this? Thank you!
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From t@|um|ey @end|ng |rom @uck|@nd@@c@nz  Tue Aug  2 23:47:02 2022
From: t@|um|ey @end|ng |rom @uck|@nd@@c@nz (Thomas Lumley)
Date: Tue, 2 Aug 2022 21:47:02 +0000
Subject: [R] 
 I don't understand the result of `svyboxplot` function from the
 Survey package
In-Reply-To: <CAOwvMDy4u-rSxsRogzCzQqLq2gpgt4dKdY+cit6MtWHLNCt1iw@mail.gmail.com>
References: <06F1D8CB-79C6-4FAF-B116-709508A3E211@gmail.com>
 <CAOwvMDy4u-rSxsRogzCzQqLq2gpgt4dKdY+cit6MtWHLNCt1iw@mail.gmail.com>
Message-ID: <SY4PR01MB6735DB272BA7F214D28DB29EA99D9@SY4PR01MB6735.ausprd01.prod.outlook.com>

It's a bug resulting from the new svyquantile() implementation.  It's fixed in the development version, which you can get from r-forge here: https://r-forge.r-project.org/R/?group_id=1788

  -thomas

Thomas Lumley
Professor of Biostatistics

________________________________________
From: Anthony Damico <ajdamico at gmail.com>
Sent: Tuesday, August 2, 2022 11:49 PM
To: Iulia Dumitru
Cc: r-help at r-project.org; Thomas Lumley
Subject: Re: [R] I don't understand the result of `svyboxplot` function from the Survey package

hi, nice catch!  i'm ccing the author of the survey package because this might be a issue.  when i run ?svyboxplot, i also see all three boxes in the exact same place..  seems like the svyby() call inside of svyboxplot does something unexpected when svyquantile gets passed using keep.var=FALSE and ci=FALSE


library(survey)

data(api)

dstrat <- svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat, fpc = ~fpc)

# looks OK
svyby(~enroll, ~stype, dstrat, svyquantile, quantiles = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)

# returns each result three times in an unexpected configuration..  svyboxplot then grabs the repeated information from the first six columns
svyby(~enroll, ~stype, dstrat, svyquantile, ci = FALSE, keep.var = FALSE, quantiles = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)





On Tue, Aug 2, 2022 at 7:30 AM Iulia Dumitru <iuliadmtru at gmail.com<mailto:iuliadmtru at gmail.com>> wrote:
After following the example given here: https://www.rdocumentation.org/packages/survey/versions/4.1-1/topics/svyhist<https://www.rdocumentation.org/packages/survey/versions/4.1-1/topics/svyhist>
for `svyboxplot` I get the result in the attached image. This is a box plot of the `enroll` variable from the stratified dataset `apistrat`, grouped by `stype`: E (elementary school), M (middle school) and H (high school). If I use the `svyby` function to group the data by `stype` and find the mean for each group, I get this result:

> svyby(~enroll, ~stype, dstrat, svymean)
  stype  enroll       se
E     E  416.78 16.41740
H     H 1320.70 91.70781
M     M  832.48 54.52157

Clearly the means are very different from each other. Then why don?t the box plots show this? I don?t know how to interpret the plot. Could someone please offer some insight on this? Thank you!


______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help<https://stat.ethz.ch/mailman/listinfo/r-help>
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html<http://www.R-project.org/posting-guide.html>
and provide commented, minimal, self-contained, reproducible code.


From s@ski@@grob m@iii@g oii quickii@e@ch  Wed Aug  3 08:37:28 2022
From: s@ski@@grob m@iii@g oii quickii@e@ch (s@ski@@grob m@iii@g oii quickii@e@ch)
Date: Wed, 3 Aug 2022 06:37:28 +0000
Subject: [R] cumulative survival/number of deaths after weighting using
 survey::svykm()
Message-ID: <VI1PR06MB563155E467B4410AB9E2F962FC9C9@VI1PR06MB5631.eurprd06.prod.outlook.com>

Hey guys.
I have a question.
I drew a weighted Kaplan-Meier survival plot using the survey::svykm() function, like this:
```
library(survey)
data(pbc, package="survival")
pbc$randomized <- with(pbc, !is.na(trt) & trt>0)
biasmodel <- glm(randomized~age*edema,data=pbc)
pbc$randprob <- fitted(biasmodel)

dpbc<-svydesign(id=~1, prob=~randprob, strata=~edema, data=subset(pbc,randomized))
s2 <-svykm(Surv(time,status==1) ~ sex, design = dpbc)
```

Now I would like to calculate the survival rates and number of deaths after x months similar to what I would get when using summary(s2, times = 320) on a survfit object. Does anybody know how I can extract these values?
Many thanks in advance!
Saskia

	[[alternative HTML version deleted]]


From ho@@@|nmm @end|ng |rom jun|v@edu  Thu Aug  4 15:30:35 2022
From: ho@@@|nmm @end|ng |rom jun|v@edu (Md. Moyazzem Hossain)
Date: Thu, 4 Aug 2022 14:30:35 +0100
Subject: [R] Need help
Message-ID: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>

Dear R Experts,

I hope that you are doing well.

I am facing a problem to find out the value of the following function. I
need help in this regard.

#####
a=rnorm(1000, 110, 5)
b = rnorm(1000, -0.3, 0.4)
s = length(a)
lam=0.15
thr=70
r= 10

ff = function(zz){
  inner = vector("numeric", length = s)
     for(k in 1:s){
      inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
          }
  answer = mean(inner)- (1- (1/r))
  return(answer)
  }
########
out=uniroot(ff, lower = 0, upper = 10000 )$root
out

########### Error ########
Error in uniroot(ff, lower = 0, upper = 10000) :
  f.upper = f(upper) is NA

Please help me. Thanks in advance.

Take care.

Hossain

-- 
Best Regards,
Md. Moyazzem Hossain
Associate Professor
Department of Statistics
Jahangirnagar University
Savar, Dhaka-1342, Bangladesh

	[[alternative HTML version deleted]]


From pro|jcn@@h @end|ng |rom gm@||@com  Thu Aug  4 15:40:28 2022
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Thu, 4 Aug 2022 09:40:28 -0400
Subject: [R] Need help
In-Reply-To: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
Message-ID: <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>

Have you checked that your function actually crosses zero?

You should also set a seed if you want a reproducible result.

JN

On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
> Dear R Experts,
> 
> I hope that you are doing well.
> 
> I am facing a problem to find out the value of the following function. I
> need help in this regard.
> 
> #####
> a=rnorm(1000, 110, 5)
> b = rnorm(1000, -0.3, 0.4)
> s = length(a)
> lam=0.15
> thr=70
> r= 10
> 
> ff = function(zz){
>    inner = vector("numeric", length = s)
>       for(k in 1:s){
>        inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>            }
>    answer = mean(inner)- (1- (1/r))
>    return(answer)
>    }
> ########
> out=uniroot(ff, lower = 0, upper = 10000 )$root
> out
> 
> ########### Error ########
> Error in uniroot(ff, lower = 0, upper = 10000) :
>    f.upper = f(upper) is NA
> 
> Please help me. Thanks in advance.
> 
> Take care.
> 
> Hossain
>


From ho@@@|nmm @end|ng |rom jun|v@edu  Thu Aug  4 15:49:46 2022
From: ho@@@|nmm @end|ng |rom jun|v@edu (Md. Moyazzem Hossain)
Date: Thu, 4 Aug 2022 14:49:46 +0100
Subject: [R] Need help
In-Reply-To: <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
Message-ID: <CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>

Dear JN,

Thanks.

I do not check whether the function actually crosses zero or not. However,
by assumption, the value would be greater than zero.

Hossain

On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:

> Have you checked that your function actually crosses zero?
>
> You should also set a seed if you want a reproducible result.
>
> JN
>
> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
> > Dear R Experts,
> >
> > I hope that you are doing well.
> >
> > I am facing a problem to find out the value of the following function. I
> > need help in this regard.
> >
> > #####
> > a=rnorm(1000, 110, 5)
> > b = rnorm(1000, -0.3, 0.4)
> > s = length(a)
> > lam=0.15
> > thr=70
> > r= 10
> >
> > ff = function(zz){
> >    inner = vector("numeric", length = s)
> >       for(k in 1:s){
> >        inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
> >            }
> >    answer = mean(inner)- (1- (1/r))
> >    return(answer)
> >    }
> > ########
> > out=uniroot(ff, lower = 0, upper = 10000 )$root
> > out
> >
> > ########### Error ########
> > Error in uniroot(ff, lower = 0, upper = 10000) :
> >    f.upper = f(upper) is NA
> >
> > Please help me. Thanks in advance.
> >
> > Take care.
> >
> > Hossain
> >
>


-- 
Best Regards,
Md. Moyazzem Hossain
Associate Professor
Department of Statistics
Jahangirnagar University
Savar, Dhaka-1342, Bangladesh
Website: http://www.juniv.edu/teachers/hossainmm
Research: *[image: Google Scholar]
<https://scholar.google.com/citations?hl=en&user=-U03XCgAAAAJ>* | *ResearchGate
<https://www.researchgate.net/profile/Md_Hossain107>* | *ORCID iD
<https://orcid.org/0000-0003-3593-6936>*

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Thu Aug  4 16:00:58 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Thu, 4 Aug 2022 10:00:58 -0400
Subject: [R] Need help
In-Reply-To: <29425_1659621083_274DpMYL008226_CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
 <29425_1659621083_274DpMYL008226_CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
Message-ID: <3489b6ec-4ad3-d510-8d75-26b85bc8245a@mcmaster.ca>

Dear Hossain,

Did you look at the values that your function returns? As John Nash 
pointed out, these values presumably depend on the random values of a and b.

I tried the following:

set.seed(123) # for reproducibility
a=rnorm(1000, 110, 5)
b = rnorm(1000, -0.3, 0.4)
s = length(a)
lam=0.15
thr=70
r= 10

ff = function(zz){
   inner = vector("numeric", length = s)
   for(k in 1:s){
     inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
   }
   answer = mean(inner)- (1- (1/r))
   return(answer)
}

res <- sapply(0:10000, ff)
res[1:500]
plot(0:150, res[1:151], type="l")


It was quickly clear that ff() produces NaNs for all but the smallest 
values of zz.

Beyond this fact, it's unclear to me what the purpose of the computation 
is. In the short domain where it produces numbers, the function returns 
both negative and positive values (for my values of a and b) -- that is, 
does cross 0 -- and if it did not, it would be nonsense to look for roots.

I hope this helps,
  John


On 2022-08-04 9:49 a.m., Md. Moyazzem Hossain wrote:
> Dear JN,
> 
> Thanks.
> 
> I do not check whether the function actually crosses zero or not. However,
> by assumption, the value would be greater than zero.
> 
> Hossain
> 
> On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:
> 
>> Have you checked that your function actually crosses zero?
>>
>> You should also set a seed if you want a reproducible result.
>>
>> JN
>>
>> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
>>> Dear R Experts,
>>>
>>> I hope that you are doing well.
>>>
>>> I am facing a problem to find out the value of the following function. I
>>> need help in this regard.
>>>
>>> #####
>>> a=rnorm(1000, 110, 5)
>>> b = rnorm(1000, -0.3, 0.4)
>>> s = length(a)
>>> lam=0.15
>>> thr=70
>>> r= 10
>>>
>>> ff = function(zz){
>>>     inner = vector("numeric", length = s)
>>>        for(k in 1:s){
>>>         inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>>>             }
>>>     answer = mean(inner)- (1- (1/r))
>>>     return(answer)
>>>     }
>>> ########
>>> out=uniroot(ff, lower = 0, upper = 10000 )$root
>>> out
>>>
>>> ########### Error ########
>>> Error in uniroot(ff, lower = 0, upper = 10000) :
>>>     f.upper = f(upper) is NA
>>>
>>> Please help me. Thanks in advance.
>>>
>>> Take care.
>>>
>>> Hossain
>>>
>>
> 
> 
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From tebert @end|ng |rom u||@edu  Thu Aug  4 16:08:49 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 4 Aug 2022 14:08:49 +0000
Subject: [R] Need help
In-Reply-To: <CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
 <CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
Message-ID: <BN6PR2201MB1553335262E763DEB4E74B87CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>

A few issues
1) What does this function do? If you describe the problem and goal there might be a better answer. We appreciate seeing that you have attempted a solution, but you have trapped us all in blindly following your attempt. 

2) This is an error checking phase of programming. Setting the seed is a good idea. Just remember to unset the seed after finishing the debugging phase.

3) In uniroot you call the function, but the function expects an argument (zz) that is not provided, and there is no default.

4) I set.seed(42), and entered ff(12) getting an answer of -0.1468287. Is this the expected result?

Regards,
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Md. Moyazzem Hossain
Sent: Thursday, August 4, 2022 9:50 AM
To: J C Nash <profjcnash at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Need help

[External Email]

Dear JN,

Thanks.

I do not check whether the function actually crosses zero or not. However, by assumption, the value would be greater than zero.

Hossain

On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:

> Have you checked that your function actually crosses zero?
>
> You should also set a seed if you want a reproducible result.
>
> JN
>
> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
> > Dear R Experts,
> >
> > I hope that you are doing well.
> >
> > I am facing a problem to find out the value of the following 
> > function. I need help in this regard.
> >
> > #####
> > a=rnorm(1000, 110, 5)
> > b = rnorm(1000, -0.3, 0.4)
> > s = length(a)
> > lam=0.15
> > thr=70
> > r= 10
> >
> > ff = function(zz){
> >    inner = vector("numeric", length = s)
> >       for(k in 1:s){
> >        inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
> >            }
> >    answer = mean(inner)- (1- (1/r))
> >    return(answer)
> >    }
> > ########
> > out=uniroot(ff, lower = 0, upper = 10000 )$root out
> >
> > ########### Error ########
> > Error in uniroot(ff, lower = 0, upper = 10000) :
> >    f.upper = f(upper) is NA
> >
> > Please help me. Thanks in advance.
> >
> > Take care.
> >
> > Hossain
> >
>


--
Best Regards,
Md. Moyazzem Hossain
Associate Professor
Department of Statistics
Jahangirnagar University
Savar, Dhaka-1342, Bangladesh
Website: https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.juniv.edu%2Fteachers%2Fhossainmm&amp;data=05%7C01%7Ctebert%40ufl.edu%7C5f71ee421f734ed8758b08da762083f7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952179362158744%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=z%2Bq179lF8MAawyc7KPXgNgGfwC9Sf0dnu34XRJxnpGY%3D&amp;reserved=0
Research: *[image: Google Scholar]
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fhl%3Den%26user%3D-U03XCgAAAAJ&amp;data=05%7C01%7Ctebert%40ufl.edu%7C5f71ee421f734ed8758b08da762083f7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952179362158744%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N6t1WrH0iRPVvd4L%2FmwqJLNXzFaK1ViPo6v1PrD8gDk%3D&amp;reserved=0>* | *ResearchGate
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMd_Hossain107&amp;data=05%7C01%7Ctebert%40ufl.edu%7C5f71ee421f734ed8758b08da762083f7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952179362158744%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=qxDL1f3bKTdxyMYH8cQaxOwCmgclU1PUSB8iS1FdPRQ%3D&amp;reserved=0>* | *ORCID iD
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Forcid.org%2F0000-0003-3593-6936&amp;data=05%7C01%7Ctebert%40ufl.edu%7C5f71ee421f734ed8758b08da762083f7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952179362158744%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=XEb%2BO2pnwNwDDOt0bDgRcIS6MzBd3q%2Bz8xnByUl6Y8w%3D&amp;reserved=0>*

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C5f71ee421f734ed8758b08da762083f7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952179362158744%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=g8ytuAGlQp5gAkXSiOkMzs0aurSim4hjX7MGg4dnjd4%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C5f71ee421f734ed8758b08da762083f7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952179362158744%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GhE0lfrdqQ8fe4YXl3PwxtTAG2SPgG6BYzv5eUXe2UQ%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Thu Aug  4 16:27:56 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 4 Aug 2022 14:27:56 +0000
Subject: [R] Need help
In-Reply-To: <BN6PR2201MB1553335262E763DEB4E74B87CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
 <CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
 <BN6PR2201MB1553335262E763DEB4E74B87CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <BN6PR2201MB155371C158B850FA215EC97ACF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>

I wish I could edit my earlier reply. 
ff(144.43) returns 0.03142121, but ff(144.44) returns NaN.
ff(12) returns -0.1468287

out=uniroot(ff, lower=12, upper =144) does not have errors.
out is a list of 5.
$root is 112
$f.root is 1.09e-08
$iter is 5
$Init.it is NA
$estim.prec is 6.1e-05

Is this working?

Regards,
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, August 4, 2022 10:09 AM
To: Md. Moyazzem Hossain <hossainmm at juniv.edu>; J C Nash <profjcnash at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Need help

[External Email]

A few issues
1) What does this function do? If you describe the problem and goal there might be a better answer. We appreciate seeing that you have attempted a solution, but you have trapped us all in blindly following your attempt.

2) This is an error checking phase of programming. Setting the seed is a good idea. Just remember to unset the seed after finishing the debugging phase.

3) In uniroot you call the function, but the function expects an argument (zz) that is not provided, and there is no default.

4) I set.seed(42), and entered ff(12) getting an answer of -0.1468287. Is this the expected result?

Regards,
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Md. Moyazzem Hossain
Sent: Thursday, August 4, 2022 9:50 AM
To: J C Nash <profjcnash at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Need help

[External Email]

Dear JN,

Thanks.

I do not check whether the function actually crosses zero or not. However, by assumption, the value would be greater than zero.

Hossain

On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:

> Have you checked that your function actually crosses zero?
>
> You should also set a seed if you want a reproducible result.
>
> JN
>
> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
> > Dear R Experts,
> >
> > I hope that you are doing well.
> >
> > I am facing a problem to find out the value of the following 
> > function. I need help in this regard.
> >
> > #####
> > a=rnorm(1000, 110, 5)
> > b = rnorm(1000, -0.3, 0.4)
> > s = length(a)
> > lam=0.15
> > thr=70
> > r= 10
> >
> > ff = function(zz){
> >    inner = vector("numeric", length = s)
> >       for(k in 1:s){
> >        inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
> >            }
> >    answer = mean(inner)- (1- (1/r))
> >    return(answer)
> >    }
> > ########
> > out=uniroot(ff, lower = 0, upper = 10000 )$root out
> >
> > ########### Error ########
> > Error in uniroot(ff, lower = 0, upper = 10000) :
> >    f.upper = f(upper) is NA
> >
> > Please help me. Thanks in advance.
> >
> > Take care.
> >
> > Hossain
> >
>


--
Best Regards,
Md. Moyazzem Hossain
Associate Professor
Department of Statistics
Jahangirnagar University
Savar, Dhaka-1342, Bangladesh
Website: https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.juniv.edu%2Fteachers%2Fhossainmm&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ca36bab5860dc491744ea08da762360d7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952191569778102%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=Vh2lFXXqsLRyKwQES0mlh0LP%2FEtn%2FMfZ56Y9Q1vGRAw%3D&amp;reserved=0
Research: *[image: Google Scholar]
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fhl%3Den%26user%3D-U03XCgAAAAJ&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ca36bab5860dc491744ea08da762360d7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952191569778102%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=st16paDG0HelgvvZZcIlstIEKmayjZFSNdujRJQtFsY%3D&amp;reserved=0>* | *ResearchGate
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMd_Hossain107&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ca36bab5860dc491744ea08da762360d7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952191569778102%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=aB9z5UrHElxeTfUVTJB6L8Sj2enqLGWjBuEXXud%2BRyI%3D&amp;reserved=0>* | *ORCID iD
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Forcid.org%2F0000-0003-3593-6936&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ca36bab5860dc491744ea08da762360d7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952191569778102%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=OfjVBjaUn8LaViyL9auEJRxdyx7%2Fs7Du1UPauifNDPs%3D&amp;reserved=0>*

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ca36bab5860dc491744ea08da762360d7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952191569778102%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=YUut6AHh2ON5dCwKO9c9T1rPvRD1rfQirZCEASNWOfg%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ca36bab5860dc491744ea08da762360d7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952191569778102%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=MKsMalwzucmuzoHVv1BWSPE88zD9bQbsAoHLEESlwSo%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ca36bab5860dc491744ea08da762360d7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952191569778102%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=YUut6AHh2ON5dCwKO9c9T1rPvRD1rfQirZCEASNWOfg%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ca36bab5860dc491744ea08da762360d7%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952191569778102%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=MKsMalwzucmuzoHVv1BWSPE88zD9bQbsAoHLEESlwSo%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Thu Aug  4 16:37:56 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 4 Aug 2022 14:37:56 +0000
Subject: [R] Need help
In-Reply-To: <BN6PR2201MB155371C158B850FA215EC97ACF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
 <CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
 <BN6PR2201MB1553335262E763DEB4E74B87CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB155371C158B850FA215EC97ACF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <BN6PR2201MB1553760A52B0CA15CA317BA9CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>

And one last edit.
Rstudio Environment shows out$root as 112. However, if I then print out$root I get 111.5303 which is much closer to the right answer.  Entering values for ff, I can work out the answer to slightly more than 111.53030196.

Regards,
Tim

-----Original Message-----
From: Ebert,Timothy Aaron <tebert at ufl.edu> 
Sent: Thursday, August 4, 2022 10:28 AM
To: Ebert,Timothy Aaron <tebert at ufl.edu>; Md. Moyazzem Hossain <hossainmm at juniv.edu>; J C Nash <profjcnash at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: RE: [R] Need help

I wish I could edit my earlier reply. 
ff(144.43) returns 0.03142121, but ff(144.44) returns NaN.
ff(12) returns -0.1468287

out=uniroot(ff, lower=12, upper =144) does not have errors.
out is a list of 5.
$root is 112
$f.root is 1.09e-08
$iter is 5
$Init.it is NA
$estim.prec is 6.1e-05

Is this working?

Regards,
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, August 4, 2022 10:09 AM
To: Md. Moyazzem Hossain <hossainmm at juniv.edu>; J C Nash <profjcnash at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Need help

[External Email]

A few issues
1) What does this function do? If you describe the problem and goal there might be a better answer. We appreciate seeing that you have attempted a solution, but you have trapped us all in blindly following your attempt.

2) This is an error checking phase of programming. Setting the seed is a good idea. Just remember to unset the seed after finishing the debugging phase.

3) In uniroot you call the function, but the function expects an argument (zz) that is not provided, and there is no default.

4) I set.seed(42), and entered ff(12) getting an answer of -0.1468287. Is this the expected result?

Regards,
Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Md. Moyazzem Hossain
Sent: Thursday, August 4, 2022 9:50 AM
To: J C Nash <profjcnash at gmail.com>
Cc: r-help mailing list <r-help at r-project.org>
Subject: Re: [R] Need help

[External Email]

Dear JN,

Thanks.

I do not check whether the function actually crosses zero or not. However, by assumption, the value would be greater than zero.

Hossain

On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:

> Have you checked that your function actually crosses zero?
>
> You should also set a seed if you want a reproducible result.
>
> JN
>
> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
> > Dear R Experts,
> >
> > I hope that you are doing well.
> >
> > I am facing a problem to find out the value of the following 
> > function. I need help in this regard.
> >
> > #####
> > a=rnorm(1000, 110, 5)
> > b = rnorm(1000, -0.3, 0.4)
> > s = length(a)
> > lam=0.15
> > thr=70
> > r= 10
> >
> > ff = function(zz){
> >    inner = vector("numeric", length = s)
> >       for(k in 1:s){
> >        inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
> >            }
> >    answer = mean(inner)- (1- (1/r))
> >    return(answer)
> >    }
> > ########
> > out=uniroot(ff, lower = 0, upper = 10000 )$root out
> >
> > ########### Error ########
> > Error in uniroot(ff, lower = 0, upper = 10000) :
> >    f.upper = f(upper) is NA
> >
> > Please help me. Thanks in advance.
> >
> > Take care.
> >
> > Hossain
> >
>


--
Best Regards,
Md. Moyazzem Hossain
Associate Professor
Department of Statistics
Jahangirnagar University
Savar, Dhaka-1342, Bangladesh
Website: https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.juniv.edu%2Fteachers%2Fhossainmm&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=3GVM6FLCtmP9oKjjtVzE0LZpI6LL8ldXknodpFQJUbM%3D&amp;reserved=0
Research: *[image: Google Scholar]
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fhl%3Den%26user%3D-U03XCgAAAAJ&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=8EfuEse2%2FnvwuZqvKskGXN4tLnM%2FDTA8XsU7nMma04c%3D&amp;reserved=0>* | *ResearchGate
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMd_Hossain107&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=IRK69rC%2F251zGT0Kpdo8jfHI1gkQhwoHxzYmozwiYKM%3D&amp;reserved=0>* | *ORCID iD
<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Forcid.org%2F0000-0003-3593-6936&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=XzQ51GyGR1tEB2iOG90cX9Np8pyeXSDBeeUpC0wDfEc%3D&amp;reserved=0>*

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=VD0EWkp1hKz%2FEKuJsGAsegL9K33y0S1L2O2jDzM6EsM%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9wjGhM9eZTIbW6ANRoxzvEI%2BH6tHeSLbGwi7fuACKmE%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=VD0EWkp1hKz%2FEKuJsGAsegL9K33y0S1L2O2jDzM6EsM%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9wjGhM9eZTIbW6ANRoxzvEI%2BH6tHeSLbGwi7fuACKmE%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Aug  4 17:24:49 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 4 Aug 2022 16:24:49 +0100
Subject: [R] Need help
In-Reply-To: <BN6PR2201MB1553760A52B0CA15CA317BA9CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
 <CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
 <BN6PR2201MB1553335262E763DEB4E74B87CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB155371C158B850FA215EC97ACF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553760A52B0CA15CA317BA9CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <11045d05-ca3e-44b2-c5db-a23406423d60@sapo.pt>

Hello,

Like others have said, the root does depend on the seed but not by much.
Here is a loop finding the roots for seeds from 1 to 10,000.

I have defined a variable n == 1000 instead of having the calls to rnorm 
depend on a hard-coded constant.



ff <- function(zz){
   inner = vector("numeric", length = s)
   for(k in 1:s){
     inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
   }
   answer = mean(inner) - (1 - (1/r))
   return(answer)
}

n <- 1000
s <- n
lam <- 0.15
thr <- 70
r <- 10

up_lims <- 112.5   # found by trial and error to be nice most
                    # of the time, see mean(ok) below
out_list <- vector("list", length = 1e4)

for(i in seq_along(out_list)) {
   # give the user feedback
   if(i %% 100 == 0) print(i)

   set.seed(i)
   a <- rnorm(n, 110, 5)
   b <- rnorm(n, -0.3, 0.4)

   out_list[[i]] <- tryCatch(
     uniroot(ff, lower = 0, upper = up_lims),
     error = function(e) e
   )
}

ok <- !sapply(out_list, inherits, 'error')
mean(ok)
#[1] 0.9978

root <- sapply(out_list[ok], '[[', 'root')
hist(root)



Hope this helps,

Rui Barradas

?s 15:37 de 04/08/2022, Ebert,Timothy Aaron escreveu:
> And one last edit.
> Rstudio Environment shows out$root as 112. However, if I then print out$root I get 111.5303 which is much closer to the right answer.  Entering values for ff, I can work out the answer to slightly more than 111.53030196.
> 
> Regards,
> Tim
> 
> -----Original Message-----
> From: Ebert,Timothy Aaron <tebert at ufl.edu>
> Sent: Thursday, August 4, 2022 10:28 AM
> To: Ebert,Timothy Aaron <tebert at ufl.edu>; Md. Moyazzem Hossain <hossainmm at juniv.edu>; J C Nash <profjcnash at gmail.com>
> Cc: r-help mailing list <r-help at r-project.org>
> Subject: RE: [R] Need help
> 
> I wish I could edit my earlier reply.
> ff(144.43) returns 0.03142121, but ff(144.44) returns NaN.
> ff(12) returns -0.1468287
> 
> out=uniroot(ff, lower=12, upper =144) does not have errors.
> out is a list of 5.
> $root is 112
> $f.root is 1.09e-08
> $iter is 5
> $Init.it is NA
> $estim.prec is 6.1e-05
> 
> Is this working?
> 
> Regards,
> Tim
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
> Sent: Thursday, August 4, 2022 10:09 AM
> To: Md. Moyazzem Hossain <hossainmm at juniv.edu>; J C Nash <profjcnash at gmail.com>
> Cc: r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] Need help
> 
> [External Email]
> 
> A few issues
> 1) What does this function do? If you describe the problem and goal there might be a better answer. We appreciate seeing that you have attempted a solution, but you have trapped us all in blindly following your attempt.
> 
> 2) This is an error checking phase of programming. Setting the seed is a good idea. Just remember to unset the seed after finishing the debugging phase.
> 
> 3) In uniroot you call the function, but the function expects an argument (zz) that is not provided, and there is no default.
> 
> 4) I set.seed(42), and entered ff(12) getting an answer of -0.1468287. Is this the expected result?
> 
> Regards,
> Tim
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Md. Moyazzem Hossain
> Sent: Thursday, August 4, 2022 9:50 AM
> To: J C Nash <profjcnash at gmail.com>
> Cc: r-help mailing list <r-help at r-project.org>
> Subject: Re: [R] Need help
> 
> [External Email]
> 
> Dear JN,
> 
> Thanks.
> 
> I do not check whether the function actually crosses zero or not. However, by assumption, the value would be greater than zero.
> 
> Hossain
> 
> On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:
> 
>> Have you checked that your function actually crosses zero?
>>
>> You should also set a seed if you want a reproducible result.
>>
>> JN
>>
>> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
>>> Dear R Experts,
>>>
>>> I hope that you are doing well.
>>>
>>> I am facing a problem to find out the value of the following
>>> function. I need help in this regard.
>>>
>>> #####
>>> a=rnorm(1000, 110, 5)
>>> b = rnorm(1000, -0.3, 0.4)
>>> s = length(a)
>>> lam=0.15
>>> thr=70
>>> r= 10
>>>
>>> ff = function(zz){
>>>     inner = vector("numeric", length = s)
>>>        for(k in 1:s){
>>>         inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>>>             }
>>>     answer = mean(inner)- (1- (1/r))
>>>     return(answer)
>>>     }
>>> ########
>>> out=uniroot(ff, lower = 0, upper = 10000 )$root out
>>>
>>> ########### Error ########
>>> Error in uniroot(ff, lower = 0, upper = 10000) :
>>>     f.upper = f(upper) is NA
>>>
>>> Please help me. Thanks in advance.
>>>
>>> Take care.
>>>
>>> Hossain
>>>
>>
> 
> 
> --
> Best Regards,
> Md. Moyazzem Hossain
> Associate Professor
> Department of Statistics
> Jahangirnagar University
> Savar, Dhaka-1342, Bangladesh
> Website: https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.juniv.edu%2Fteachers%2Fhossainmm&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=3GVM6FLCtmP9oKjjtVzE0LZpI6LL8ldXknodpFQJUbM%3D&amp;reserved=0
> Research: *[image: Google Scholar]
> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fhl%3Den%26user%3D-U03XCgAAAAJ&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=8EfuEse2%2FnvwuZqvKskGXN4tLnM%2FDTA8XsU7nMma04c%3D&amp;reserved=0>* | *ResearchGate
> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMd_Hossain107&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=IRK69rC%2F251zGT0Kpdo8jfHI1gkQhwoHxzYmozwiYKM%3D&amp;reserved=0>* | *ORCID iD
> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Forcid.org%2F0000-0003-3593-6936&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=XzQ51GyGR1tEB2iOG90cX9Np8pyeXSDBeeUpC0wDfEc%3D&amp;reserved=0>*
> 
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=VD0EWkp1hKz%2FEKuJsGAsegL9K33y0S1L2O2jDzM6EsM%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9wjGhM9eZTIbW6ANRoxzvEI%2BH6tHeSLbGwi7fuACKmE%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=VD0EWkp1hKz%2FEKuJsGAsegL9K33y0S1L2O2jDzM6EsM%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9wjGhM9eZTIbW6ANRoxzvEI%2BH6tHeSLbGwi7fuACKmE%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Aug  4 17:29:28 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 4 Aug 2022 16:29:28 +0100
Subject: [R] Need help
In-Reply-To: <11045d05-ca3e-44b2-c5db-a23406423d60@sapo.pt>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
 <CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
 <BN6PR2201MB1553335262E763DEB4E74B87CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB155371C158B850FA215EC97ACF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553760A52B0CA15CA317BA9CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <11045d05-ca3e-44b2-c5db-a23406423d60@sapo.pt>
Message-ID: <ba7e23c1-35d8-c45a-6034-248428814d4b@sapo.pt>

And for the sake of completeness


which(!ok)
sapply(out_list[!ok], conditionMessage)

Hope this helps,

Rui Barradas

?s 16:24 de 04/08/2022, Rui Barradas escreveu:
> Hello,
> 
> Like others have said, the root does depend on the seed but not by much.
> Here is a loop finding the roots for seeds from 1 to 10,000.
> 
> I have defined a variable n == 1000 instead of having the calls to rnorm 
> depend on a hard-coded constant.
> 
> 
> 
> ff <- function(zz){
>  ? inner = vector("numeric", length = s)
>  ? for(k in 1:s){
>  ??? inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>  ? }
>  ? answer = mean(inner) - (1 - (1/r))
>  ? return(answer)
> }
> 
> n <- 1000
> s <- n
> lam <- 0.15
> thr <- 70
> r <- 10
> 
> up_lims <- 112.5?? # found by trial and error to be nice most
>  ?????????????????? # of the time, see mean(ok) below
> out_list <- vector("list", length = 1e4)
> 
> for(i in seq_along(out_list)) {
>  ? # give the user feedback
>  ? if(i %% 100 == 0) print(i)
> 
>  ? set.seed(i)
>  ? a <- rnorm(n, 110, 5)
>  ? b <- rnorm(n, -0.3, 0.4)
> 
>  ? out_list[[i]] <- tryCatch(
>  ??? uniroot(ff, lower = 0, upper = up_lims),
>  ??? error = function(e) e
>  ? )
> }
> 
> ok <- !sapply(out_list, inherits, 'error')
> mean(ok)
> #[1] 0.9978
> 
> root <- sapply(out_list[ok], '[[', 'root')
> hist(root)
> 
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 15:37 de 04/08/2022, Ebert,Timothy Aaron escreveu:
>> And one last edit.
>> Rstudio Environment shows out$root as 112. However, if I then print 
>> out$root I get 111.5303 which is much closer to the right answer.  
>> Entering values for ff, I can work out the answer to slightly more 
>> than 111.53030196.
>>
>> Regards,
>> Tim
>>
>> -----Original Message-----
>> From: Ebert,Timothy Aaron <tebert at ufl.edu>
>> Sent: Thursday, August 4, 2022 10:28 AM
>> To: Ebert,Timothy Aaron <tebert at ufl.edu>; Md. Moyazzem Hossain 
>> <hossainmm at juniv.edu>; J C Nash <profjcnash at gmail.com>
>> Cc: r-help mailing list <r-help at r-project.org>
>> Subject: RE: [R] Need help
>>
>> I wish I could edit my earlier reply.
>> ff(144.43) returns 0.03142121, but ff(144.44) returns NaN.
>> ff(12) returns -0.1468287
>>
>> out=uniroot(ff, lower=12, upper =144) does not have errors.
>> out is a list of 5.
>> $root is 112
>> $f.root is 1.09e-08
>> $iter is 5
>> $Init.it is NA
>> $estim.prec is 6.1e-05
>>
>> Is this working?
>>
>> Regards,
>> Tim
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy 
>> Aaron
>> Sent: Thursday, August 4, 2022 10:09 AM
>> To: Md. Moyazzem Hossain <hossainmm at juniv.edu>; J C Nash 
>> <profjcnash at gmail.com>
>> Cc: r-help mailing list <r-help at r-project.org>
>> Subject: Re: [R] Need help
>>
>> [External Email]
>>
>> A few issues
>> 1) What does this function do? If you describe the problem and goal 
>> there might be a better answer. We appreciate seeing that you have 
>> attempted a solution, but you have trapped us all in blindly following 
>> your attempt.
>>
>> 2) This is an error checking phase of programming. Setting the seed is 
>> a good idea. Just remember to unset the seed after finishing the 
>> debugging phase.
>>
>> 3) In uniroot you call the function, but the function expects an 
>> argument (zz) that is not provided, and there is no default.
>>
>> 4) I set.seed(42), and entered ff(12) getting an answer of -0.1468287. 
>> Is this the expected result?
>>
>> Regards,
>> Tim
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Md. Moyazzem 
>> Hossain
>> Sent: Thursday, August 4, 2022 9:50 AM
>> To: J C Nash <profjcnash at gmail.com>
>> Cc: r-help mailing list <r-help at r-project.org>
>> Subject: Re: [R] Need help
>>
>> [External Email]
>>
>> Dear JN,
>>
>> Thanks.
>>
>> I do not check whether the function actually crosses zero or not. 
>> However, by assumption, the value would be greater than zero.
>>
>> Hossain
>>
>> On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:
>>
>>> Have you checked that your function actually crosses zero?
>>>
>>> You should also set a seed if you want a reproducible result.
>>>
>>> JN
>>>
>>> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
>>>> Dear R Experts,
>>>>
>>>> I hope that you are doing well.
>>>>
>>>> I am facing a problem to find out the value of the following
>>>> function. I need help in this regard.
>>>>
>>>> #####
>>>> a=rnorm(1000, 110, 5)
>>>> b = rnorm(1000, -0.3, 0.4)
>>>> s = length(a)
>>>> lam=0.15
>>>> thr=70
>>>> r= 10
>>>>
>>>> ff = function(zz){
>>>> ??? inner = vector("numeric", length = s)
>>>> ?????? for(k in 1:s){
>>>> ??????? inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>>>> ??????????? }
>>>> ??? answer = mean(inner)- (1- (1/r))
>>>> ??? return(answer)
>>>> ??? }
>>>> ########
>>>> out=uniroot(ff, lower = 0, upper = 10000 )$root out
>>>>
>>>> ########### Error ########
>>>> Error in uniroot(ff, lower = 0, upper = 10000) :
>>>> ??? f.upper = f(upper) is NA
>>>>
>>>> Please help me. Thanks in advance.
>>>>
>>>> Take care.
>>>>
>>>> Hossain
>>>>
>>>
>>
>>
>> -- 
>> Best Regards,
>> Md. Moyazzem Hossain
>> Associate Professor
>> Department of Statistics
>> Jahangirnagar University
>> Savar, Dhaka-1342, Bangladesh
>> Website: 
>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.juniv.edu%2Fteachers%2Fhossainmm&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=3GVM6FLCtmP9oKjjtVzE0LZpI6LL8ldXknodpFQJUbM%3D&amp;reserved=0 
>>
>> Research: *[image: Google Scholar]
>> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fhl%3Den%26user%3D-U03XCgAAAAJ&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=8EfuEse2%2FnvwuZqvKskGXN4tLnM%2FDTA8XsU7nMma04c%3D&amp;reserved=0>* 
>> | *ResearchGate
>> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMd_Hossain107&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=IRK69rC%2F251zGT0Kpdo8jfHI1gkQhwoHxzYmozwiYKM%3D&amp;reserved=0>* 
>> | *ORCID iD
>> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Forcid.org%2F0000-0003-3593-6936&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=XzQ51GyGR1tEB2iOG90cX9Np8pyeXSDBeeUpC0wDfEc%3D&amp;reserved=0>* 
>>
>>
>> ???????? [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=VD0EWkp1hKz%2FEKuJsGAsegL9K33y0S1L2O2jDzM6EsM%3D&amp;reserved=0 
>>
>> PLEASE do read the posting guide 
>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9wjGhM9eZTIbW6ANRoxzvEI%2BH6tHeSLbGwi7fuACKmE%3D&amp;reserved=0 
>>
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=VD0EWkp1hKz%2FEKuJsGAsegL9K33y0S1L2O2jDzM6EsM%3D&amp;reserved=0 
>>
>> PLEASE do read the posting guide 
>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9wjGhM9eZTIbW6ANRoxzvEI%2BH6tHeSLbGwi7fuACKmE%3D&amp;reserved=0 
>>
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Thu Aug  4 17:32:07 2022
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Thu, 4 Aug 2022 11:32:07 -0400
Subject: [R] Need help
In-Reply-To: <11045d05-ca3e-44b2-c5db-a23406423d60@sapo.pt>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
 <CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
 <BN6PR2201MB1553335262E763DEB4E74B87CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB155371C158B850FA215EC97ACF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553760A52B0CA15CA317BA9CF9F9@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <11045d05-ca3e-44b2-c5db-a23406423d60@sapo.pt>
Message-ID: <2634d844-8895-1800-5b0f-d8e4c60d72b7@gmail.com>

FWIW, package Rmpfr has unirootR which Martin Maechler included after I coded a version of zeroin
in pure R at UseR2011. (There was a long session in biostats that was a bit outside my area, and ...)

Sometimes useful when high precision necessary.

JN


On 2022-08-04 11:24, Rui Barradas wrote:
> Hello,
> 
> Like others have said, the root does depend on the seed but not by much.
> Here is a loop finding the roots for seeds from 1 to 10,000.
> 
> I have defined a variable n == 1000 instead of having the calls to rnorm depend on a hard-coded constant.
> 
> 
> 
> ff <- function(zz){
>  ? inner = vector("numeric", length = s)
>  ? for(k in 1:s){
>  ??? inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>  ? }
>  ? answer = mean(inner) - (1 - (1/r))
>  ? return(answer)
> }
> 
> n <- 1000
> s <- n
> lam <- 0.15
> thr <- 70
> r <- 10
> 
> up_lims <- 112.5?? # found by trial and error to be nice most
>  ?????????????????? # of the time, see mean(ok) below
> out_list <- vector("list", length = 1e4)
> 
> for(i in seq_along(out_list)) {
>  ? # give the user feedback
>  ? if(i %% 100 == 0) print(i)
> 
>  ? set.seed(i)
>  ? a <- rnorm(n, 110, 5)
>  ? b <- rnorm(n, -0.3, 0.4)
> 
>  ? out_list[[i]] <- tryCatch(
>  ??? uniroot(ff, lower = 0, upper = up_lims),
>  ??? error = function(e) e
>  ? )
> }
> 
> ok <- !sapply(out_list, inherits, 'error')
> mean(ok)
> #[1] 0.9978
> 
> root <- sapply(out_list[ok], '[[', 'root')
> hist(root)
> 
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 15:37 de 04/08/2022, Ebert,Timothy Aaron escreveu:
>> And one last edit.
>> Rstudio Environment shows out$root as 112. However, if I then print out$root I get 111.5303 which is much closer to 
>> the right answer.? Entering values for ff, I can work out the answer to slightly more than 111.53030196.
>>
>> Regards,
>> Tim
>>
>> -----Original Message-----
>> From: Ebert,Timothy Aaron <tebert at ufl.edu>
>> Sent: Thursday, August 4, 2022 10:28 AM
>> To: Ebert,Timothy Aaron <tebert at ufl.edu>; Md. Moyazzem Hossain <hossainmm at juniv.edu>; J C Nash <profjcnash at gmail.com>
>> Cc: r-help mailing list <r-help at r-project.org>
>> Subject: RE: [R] Need help
>>
>> I wish I could edit my earlier reply.
>> ff(144.43) returns 0.03142121, but ff(144.44) returns NaN.
>> ff(12) returns -0.1468287
>>
>> out=uniroot(ff, lower=12, upper =144) does not have errors.
>> out is a list of 5.
>> $root is 112
>> $f.root is 1.09e-08
>> $iter is 5
>> $Init.it is NA
>> $estim.prec is 6.1e-05
>>
>> Is this working?
>>
>> Regards,
>> Tim
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
>> Sent: Thursday, August 4, 2022 10:09 AM
>> To: Md. Moyazzem Hossain <hossainmm at juniv.edu>; J C Nash <profjcnash at gmail.com>
>> Cc: r-help mailing list <r-help at r-project.org>
>> Subject: Re: [R] Need help
>>
>> [External Email]
>>
>> A few issues
>> 1) What does this function do? If you describe the problem and goal there might be a better answer. We appreciate 
>> seeing that you have attempted a solution, but you have trapped us all in blindly following your attempt.
>>
>> 2) This is an error checking phase of programming. Setting the seed is a good idea. Just remember to unset the seed 
>> after finishing the debugging phase.
>>
>> 3) In uniroot you call the function, but the function expects an argument (zz) that is not provided, and there is no 
>> default.
>>
>> 4) I set.seed(42), and entered ff(12) getting an answer of -0.1468287. Is this the expected result?
>>
>> Regards,
>> Tim
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Md. Moyazzem Hossain
>> Sent: Thursday, August 4, 2022 9:50 AM
>> To: J C Nash <profjcnash at gmail.com>
>> Cc: r-help mailing list <r-help at r-project.org>
>> Subject: Re: [R] Need help
>>
>> [External Email]
>>
>> Dear JN,
>>
>> Thanks.
>>
>> I do not check whether the function actually crosses zero or not. However, by assumption, the value would be greater 
>> than zero.
>>
>> Hossain
>>
>> On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:
>>
>>> Have you checked that your function actually crosses zero?
>>>
>>> You should also set a seed if you want a reproducible result.
>>>
>>> JN
>>>
>>> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
>>>> Dear R Experts,
>>>>
>>>> I hope that you are doing well.
>>>>
>>>> I am facing a problem to find out the value of the following
>>>> function. I need help in this regard.
>>>>
>>>> #####
>>>> a=rnorm(1000, 110, 5)
>>>> b = rnorm(1000, -0.3, 0.4)
>>>> s = length(a)
>>>> lam=0.15
>>>> thr=70
>>>> r= 10
>>>>
>>>> ff = function(zz){
>>>> ??? inner = vector("numeric", length = s)
>>>> ?????? for(k in 1:s){
>>>> ??????? inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>>>> ??????????? }
>>>> ??? answer = mean(inner)- (1- (1/r))
>>>> ??? return(answer)
>>>> ??? }
>>>> ########
>>>> out=uniroot(ff, lower = 0, upper = 10000 )$root out
>>>>
>>>> ########### Error ########
>>>> Error in uniroot(ff, lower = 0, upper = 10000) :
>>>> ??? f.upper = f(upper) is NA
>>>>
>>>> Please help me. Thanks in advance.
>>>>
>>>> Take care.
>>>>
>>>> Hossain
>>>>
>>>
>>
>>
>> -- 
>> Best Regards,
>> Md. Moyazzem Hossain
>> Associate Professor
>> Department of Statistics
>> Jahangirnagar University
>> Savar, Dhaka-1342, Bangladesh
>> Website: 
>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.juniv.edu%2Fteachers%2Fhossainmm&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=3GVM6FLCtmP9oKjjtVzE0LZpI6LL8ldXknodpFQJUbM%3D&amp;reserved=0 
>>
>> Research: *[image: Google Scholar]
>> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fhl%3Den%26user%3D-U03XCgAAAAJ&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=8EfuEse2%2FnvwuZqvKskGXN4tLnM%2FDTA8XsU7nMma04c%3D&amp;reserved=0>* 
>> | *ResearchGate
>> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMd_Hossain107&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=IRK69rC%2F251zGT0Kpdo8jfHI1gkQhwoHxzYmozwiYKM%3D&amp;reserved=0>* 
>> | *ORCID iD
>> <https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Forcid.org%2F0000-0003-3593-6936&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=XzQ51GyGR1tEB2iOG90cX9Np8pyeXSDBeeUpC0wDfEc%3D&amp;reserved=0>* 
>>
>>
>> ???????? [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=VD0EWkp1hKz%2FEKuJsGAsegL9K33y0S1L2O2jDzM6EsM%3D&amp;reserved=0 
>>
>> PLEASE do read the posting guide 
>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9wjGhM9eZTIbW6ANRoxzvEI%2BH6tHeSLbGwi7fuACKmE%3D&amp;reserved=0 
>>
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=VD0EWkp1hKz%2FEKuJsGAsegL9K33y0S1L2O2jDzM6EsM%3D&amp;reserved=0 
>>
>> PLEASE do read the posting guide 
>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C92190c03aa824920e8a108da76258618%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637952200791209718%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9wjGhM9eZTIbW6ANRoxzvEI%2BH6tHeSLbGwi7fuACKmE%3D&amp;reserved=0 
>>
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Aug  4 18:08:12 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 4 Aug 2022 12:08:12 -0400
Subject: [R] Need help
In-Reply-To: <00af01d8a81c$37251f40$a56f5dc0$@gmail.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <00af01d8a81c$37251f40$a56f5dc0$@gmail.com>
Message-ID: <00b101d8a81c$66167c90$324375b0$@gmail.com>


Another exceedingly polite questioner. Cultural differences!

I think we can skip discussing if we are doing well, and get to the point.

To start with, I got thrown by these two lines:

a=rnorm(1000, 110, 5)
s = length(a)

This does not relate to the difficulty, but is a sort of sloppy use as a was
created with a request to be 1,000 random numbers. No need to calculate the
length. You could just say 

s <- 1000

We have no idea what the purpose of your function ff is meant to be used for
and what it is normally given as an argument as you seem to just pass the
function as an argument to a stats package function called uniroot. As such,
the function should return values for a given argument that can be evaluated
between lower and upper bounds. OK, it took some time to look things up that
would have been easier had you told us a bit more.
 
But it gets worse as your function references "s" in this line within:

inner = vector("numeric", length = s)

You are not passing s to the function but using it from outside to make an
empty variable of the same length? Similarly, you are putting other
variables like r, lam, thr  in a global environment and using it within the
function. Is this a good design?

As other have pointed out, you should evaluate the function ff to see what
it returns on it's own, not wait for uniroot to fail in calling it.

Your function ff seems to make a vector of 1,000 complicated values based on
the global values including the values in "a" and 'b" that are random then
take the mean of the results with an adjustment.  I have no idea if your
code is modeled after anything in particular or is correct. But I exercised
it in the lower range and it produces reasonable numbers:

sapply(0:10,ff)
 [1] -0.1715207 -0.1693173 -0.1671317 -0.1649635 -0.1628125 -0.1606784
-0.1585609 -0.1564598
 [9] -0.1543749 -0.1523059 -0.1502526

But in the upper range your function returns a NaN or NOT A NUMBER

sapply(9995:10000,ff)
[1] NaN NaN NaN NaN NaN NaN

That would explain the error message.

So you need to look at ff and see how the NaN is introduced perhaps just for
larger numbers. You may have an overflow problem for example where you are
making number too large to fit. Who knows?

What I know is when I make your 10,001 calculations only the first 140 or so
are not Nan. ff(142) and beyond all fail.

> sapply(0:10000,ff) -> temp
> sum(is.nan(temp))
[1] 9859

So it may be obvious to you why zz-thr has such a threshold, but not to me.

I would try to make sure you implemented the algorithm required correctly in
the code and that the range of random numbers you select fits the needs. If
you can get ff() to return only valid numbers, it may be a step. And note
that when I change the mean() function you call to add na.rm=TRUE, it
removes the NaN from the calculation (albeit that should not mean it is now
correct) and a new error shows up:

> ff = function(zz){
+     inner = vector("numeric", length = s)
+     for(k in 1:s){
+         inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
+     }
+     answer = mean(inner, na.rm=TRUE)- (1- (1/r))
+     return(answer)
+ }

> out=uniroot(ff, lower = 0, upper = 10000 )$root out
Error: unexpected symbol in "out=uniroot(ff, lower = 0, upper = 10000 )$root
out"

You left a dangling " out" at the end. You may mean:

out=uniroot(ff, lower = 0, upper = 10000 )$root

My current result is 111.6597 but as noted is meaningless as I threw out
lots of NaN.



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Md. Moyazzem
Hossain
Sent: Thursday, August 4, 2022 9:31 AM
To: r-help mailing list <r-help at r-project.org>
Subject: [R] Need help

Dear R Experts,

I hope that you are doing well.

I am facing a problem to find out the value of the following function. I
need help in this regard.

#####
a=rnorm(1000, 110, 5)
b = rnorm(1000, -0.3, 0.4)
s = length(a)
lam=0.15
thr=70
r= 10

ff = function(zz){
  inner = vector("numeric", length = s)
     for(k in 1:s){
      inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
          }
  answer = mean(inner)- (1- (1/r))
  return(answer)
  }
########
out=uniroot(ff, lower = 0, upper = 10000 )$root out

########### Error ########
Error in uniroot(ff, lower = 0, upper = 10000) :
  f.upper = f(upper) is NA

Please help me. Thanks in advance.

Take care.

Hossain

--
Best Regards,
Md. Moyazzem Hossain
Associate Professor
Department of Statistics
Jahangirnagar University
Savar, Dhaka-1342, Bangladesh

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ho@@@|nmm @end|ng |rom jun|v@edu  Thu Aug  4 21:05:34 2022
From: ho@@@|nmm @end|ng |rom jun|v@edu (Md. Moyazzem Hossain)
Date: Thu, 4 Aug 2022 20:05:34 +0100
Subject: [R] Need help
In-Reply-To: <3489b6ec-4ad3-d510-8d75-26b85bc8245a@mcmaster.ca>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <c6093753-6961-c5f2-4184-ad4f90db2a04@gmail.com>
 <29425_1659621083_274DpMYL008226_CAO29qn6kLevZfJt1BrNN4KmcUR2sbFZdz5hZfFsQ-6041f9WCg@mail.gmail.com>
 <3489b6ec-4ad3-d510-8d75-26b85bc8245a@mcmaster.ca>
Message-ID: <CAO29qn4-hPagCaw05qXt7A8hF+5ZOH7xPDxLkue2z_0=5pK9=g@mail.gmail.com>

Dear John

Thank you very much for your feedback.

Take care.

Hossain

On Thu, Aug 4, 2022 at 3:01 PM John Fox <jfox at mcmaster.ca> wrote:

> Dear Hossain,
>
> Did you look at the values that your function returns? As John Nash
> pointed out, these values presumably depend on the random values of a and
> b.
>
> I tried the following:
>
> set.seed(123) # for reproducibility
> a=rnorm(1000, 110, 5)
> b = rnorm(1000, -0.3, 0.4)
> s = length(a)
> lam=0.15
> thr=70
> r= 10
>
> ff = function(zz){
>    inner = vector("numeric", length = s)
>    for(k in 1:s){
>      inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>    }
>    answer = mean(inner)- (1- (1/r))
>    return(answer)
> }
>
> res <- sapply(0:10000, ff)
> res[1:500]
> plot(0:150, res[1:151], type="l")
>
>
> It was quickly clear that ff() produces NaNs for all but the smallest
> values of zz.
>
> Beyond this fact, it's unclear to me what the purpose of the computation
> is. In the short domain where it produces numbers, the function returns
> both negative and positive values (for my values of a and b) -- that is,
> does cross 0 -- and if it did not, it would be nonsense to look for roots.
>
> I hope this helps,
>   John
>
>
> On 2022-08-04 9:49 a.m., Md. Moyazzem Hossain wrote:
> > Dear JN,
> >
> > Thanks.
> >
> > I do not check whether the function actually crosses zero or not.
> However,
> > by assumption, the value would be greater than zero.
> >
> > Hossain
> >
> > On Thu, Aug 4, 2022 at 2:40 PM J C Nash <profjcnash at gmail.com> wrote:
> >
> >> Have you checked that your function actually crosses zero?
> >>
> >> You should also set a seed if you want a reproducible result.
> >>
> >> JN
> >>
> >> On 2022-08-04 09:30, Md. Moyazzem Hossain wrote:
> >>> Dear R Experts,
> >>>
> >>> I hope that you are doing well.
> >>>
> >>> I am facing a problem to find out the value of the following function.
> I
> >>> need help in this regard.
> >>>
> >>> #####
> >>> a=rnorm(1000, 110, 5)
> >>> b = rnorm(1000, -0.3, 0.4)
> >>> s = length(a)
> >>> lam=0.15
> >>> thr=70
> >>> r= 10
> >>>
> >>> ff = function(zz){
> >>>     inner = vector("numeric", length = s)
> >>>        for(k in 1:s){
> >>>         inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
> >>>             }
> >>>     answer = mean(inner)- (1- (1/r))
> >>>     return(answer)
> >>>     }
> >>> ########
> >>> out=uniroot(ff, lower = 0, upper = 10000 )$root
> >>> out
> >>>
> >>> ########### Error ########
> >>> Error in uniroot(ff, lower = 0, upper = 10000) :
> >>>     f.upper = f(upper) is NA
> >>>
> >>> Please help me. Thanks in advance.
> >>>
> >>> Take care.
> >>>
> >>> Hossain
> >>>
> >>
> >
> >
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://socialsciences.mcmaster.ca/jfox/
>
>

-- 
Best Regards,
Md. Moyazzem Hossain
Associate Professor
Department of Statistics
Jahangirnagar University
Savar, Dhaka-1342, Bangladesh
Website: http://www.juniv.edu/teachers/hossainmm
Research: *[image: Google Scholar]
<https://scholar.google.com/citations?hl=en&user=-U03XCgAAAAJ>* | *ResearchGate
<https://www.researchgate.net/profile/Md_Hossain107>* | *ORCID iD
<https://orcid.org/0000-0003-3593-6936>*

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Aug  5 03:34:13 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 5 Aug 2022 13:34:13 +1200
Subject: [R] Need help
In-Reply-To: <00b101d8a81c$66167c90$324375b0$@gmail.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <00af01d8a81c$37251f40$a56f5dc0$@gmail.com>
 <00b101d8a81c$66167c90$324375b0$@gmail.com>
Message-ID: <20220805133413.719cda3e@rolf-Latitude-E7470>


On Thu, 4 Aug 2022 12:08:12 -0400
<avi.e.gross at gmail.com> wrote:

> 
> Another exceedingly polite questioner. Cultural differences!
> 
> I think we can skip discussing if we are doing well, and get to the
> point.

<SNIP>

Moreover, it would be considerate to provide a more informative subject
than "Need help".


See fortunes::fortune(285).

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ho@@@|nmm @end|ng |rom jun|v@edu  Fri Aug  5 21:40:13 2022
From: ho@@@|nmm @end|ng |rom jun|v@edu (Md. Moyazzem Hossain)
Date: Fri, 5 Aug 2022 20:40:13 +0100
Subject: [R] Need help
In-Reply-To: <CAPcHnpRA0YhCG5vg3qFsSb--bGVgca344wAbe0Eh2uY6euGmDw@mail.gmail.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <CAPcHnpTN7te34OZbFRTHqdChjcyfecrVOvah0xnUk0wcvSKzTw@mail.gmail.com>
 <CAO29qn6G4A2SGihVCyyowQ3uPwC=vur6guNk3YPkZXGs=iWJYA@mail.gmail.com>
 <CAPcHnpRA0YhCG5vg3qFsSb--bGVgca344wAbe0Eh2uY6euGmDw@mail.gmail.com>
Message-ID: <CAO29qn7GqP0By8zEQpTH_8rZb_4CPUfby2-x0KVYeDm6GFgd-A@mail.gmail.com>

Dear All,

Thank you very much for all of your help. If anyone feels unhappy with my
words then I apologize.

Actually, I am not an expert user as a result my coding seems to be wrong
on some points. Now, it is working.

I am thankful to all of you for your kind support. Take care.

Hossain



On Fri, Aug 5, 2022 at 4:27 AM Andrew Simmons <akwsimmo at gmail.com> wrote:

> Hello again,
>
>
> Since the GEV distribution and the GPD distribution are similar, I was
> able to make dgpd, pgpd, qgpd, and rgpd with no trouble. You should be able
> to use the same piece of code I gave earlier to install the package, here
> it is again:
>
> install.packages(c("this.path", "essentials"), repos = "
> https://raw.githubusercontent.com/ArcadeAntics/PACKAGES")
>
> I tried to make your code a little nicer and also use 'pgpd', let me know
> if this behaves as expected.
>
> #####
> library(essentials)
> a <- rnorm(1000, 110, 5)
> b <- rnorm(1000, -0.3, 0.4)
> s <- length(a)
> lam <- 0.15
> thr <- 70
> r <- 10
>
> ff <- function(zz) {
>     inner <- numeric(s)
>     for (k in 1:s) {
>         inner[k] <- 1 - lam * pgpd(zz, location = thr, scale = a[k], shape
> = b[k], lower.tail = FALSE)
>     }
>
>
>     # if zz is length 1 (it is for uniroot), it would be much faster to do
> this:
>     # inner <- 1 - lam * pgpd(zz, location = thr, scale = a, shape = b,
> lower.tail = FALSE)
>     #
>     # use whichever you please
>
>     mean(inner) - (1 - 1/r)
> }
> ########
> out <- uniroot(ff, lower = 0, upper = 10000)$root
> out
>
> On Thu, Aug 4, 2022 at 3:17 PM Md. Moyazzem Hossain <hossainmm at juniv.edu>
> wrote:
>
>> Dear Andrew Simmons,
>>
>> Thank you very much.
>>
>> Actually, I want to calculate the predictive return level of GPD and
>> compare it with the return level to see the difference. I have used the
>> following function before but it does not provide the results as expected.
>> It is OK for GEV but not GPD.
>>
>>         inner[k] = pgpd(zz, scale = a[k],  shape = b[k] )
>>
>> Other lines of the code were the same.
>>
>> Please help me to calculate the predictive return level of GPD.
>>
>> Thanks in advance.
>>
>> Hossain
>>
>> On Thu, Aug 4, 2022 at 5:11 PM Andrew Simmons <akwsimmo at gmail.com> wrote:
>>
>>> Is this a GEV distribution by chance? If so, I made functions dgev,
>>> pgev, qgev, and rgev that you can find install with:
>>>
>>> install.packages(c("this.path", "essentials"), repos = "
>>> https://raw.githubusercontent.com/ArcadeAntics/PACKAGES")
>>>
>>> and then in your R script
>>>
>>> library(essentials)
>>> dgev(...) or pgev or qgev as needed
>>>
>>> On Thu, Aug 4, 2022, 09:31 Md. Moyazzem Hossain <hossainmm at juniv.edu>
>>> wrote:
>>>
>>>> Dear R Experts,
>>>>
>>>> I hope that you are doing well.
>>>>
>>>> I am facing a problem to find out the value of the following function. I
>>>> need help in this regard.
>>>>
>>>> #####
>>>> a=rnorm(1000, 110, 5)
>>>> b = rnorm(1000, -0.3, 0.4)
>>>> s = length(a)
>>>> lam=0.15
>>>> thr=70
>>>> r= 10
>>>>
>>>> ff = function(zz){
>>>>   inner = vector("numeric", length = s)
>>>>      for(k in 1:s){
>>>>       inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>>>>           }
>>>>   answer = mean(inner)- (1- (1/r))
>>>>   return(answer)
>>>>   }
>>>> ########
>>>> out=uniroot(ff, lower = 0, upper = 10000 )$root
>>>> out
>>>>
>>>> ########### Error ########
>>>> Error in uniroot(ff, lower = 0, upper = 10000) :
>>>>   f.upper = f(upper) is NA
>>>>
>>>> Please help me. Thanks in advance.
>>>>
>>>> Take care.
>>>>
>>>> Hossain
>>>>
>>>> --
>>>> Best Regards,
>>>> Md. Moyazzem Hossain
>>>> Associate Professor
>>>> Department of Statistics
>>>> Jahangirnagar University
>>>> Savar, Dhaka-1342, Bangladesh
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>
>> --
>> Best Regards,
>> Md. Moyazzem Hossain
>> Associate Professor
>> Department of Statistics
>> Jahangirnagar University
>> Savar, Dhaka-1342, Bangladesh
>> Website: http://www.juniv.edu/teachers/hossainmm
>> Research: *[image: Google Scholar]
>> <https://scholar.google.com/citations?hl=en&user=-U03XCgAAAAJ>* | *ResearchGate
>> <https://www.researchgate.net/profile/Md_Hossain107>* | *ORCID iD
>> <https://orcid.org/0000-0003-3593-6936>*
>>
>

-- 
Best Regards,
Md. Moyazzem Hossain
Associate Professor
Department of Statistics
Jahangirnagar University
Savar, Dhaka-1342, Bangladesh
Website: http://www.juniv.edu/teachers/hossainmm
Research: *[image: Google Scholar]
<https://scholar.google.com/citations?hl=en&user=-U03XCgAAAAJ>* | *ResearchGate
<https://www.researchgate.net/profile/Md_Hossain107>* | *ORCID iD
<https://orcid.org/0000-0003-3593-6936>*

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Aug  5 21:58:52 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 5 Aug 2022 20:58:52 +0100
Subject: [R] Need help
In-Reply-To: <CAO29qn7GqP0By8zEQpTH_8rZb_4CPUfby2-x0KVYeDm6GFgd-A@mail.gmail.com>
References: <CAO29qn5UwzkmK1LdAfV9B5YsOiOKmVqYbq9-vVSk-26gz8F7yw@mail.gmail.com>
 <CAPcHnpTN7te34OZbFRTHqdChjcyfecrVOvah0xnUk0wcvSKzTw@mail.gmail.com>
 <CAO29qn6G4A2SGihVCyyowQ3uPwC=vur6guNk3YPkZXGs=iWJYA@mail.gmail.com>
 <CAPcHnpRA0YhCG5vg3qFsSb--bGVgca344wAbe0Eh2uY6euGmDw@mail.gmail.com>
 <CAO29qn7GqP0By8zEQpTH_8rZb_4CPUfby2-x0KVYeDm6GFgd-A@mail.gmail.com>
Message-ID: <cf052c3a-ebe2-2c02-12c9-b2b9a0b0ae08@sapo.pt>

Hello,

Inline.

?s 20:40 de 05/08/2022, Md. Moyazzem Hossain escreveu:
> Dear All,
> 
> Thank you very much for all of your help. If anyone feels unhappy with my
> words then I apologize.

You're welcome. Anytime. Your question shows a real use case problem and 
those are the good questions.

> 
> Actually, I am not an expert user as a result my coding seems to be wrong
> on some points. Now, it is working.

Glad it helped.

Rui Barradas

> 
> I am thankful to all of you for your kind support. Take care.
> 
> Hossain
> 
> 
> 
> On Fri, Aug 5, 2022 at 4:27 AM Andrew Simmons <akwsimmo at gmail.com> wrote:
> 
>> Hello again,
>>
>>
>> Since the GEV distribution and the GPD distribution are similar, I was
>> able to make dgpd, pgpd, qgpd, and rgpd with no trouble. You should be able
>> to use the same piece of code I gave earlier to install the package, here
>> it is again:
>>
>> install.packages(c("this.path", "essentials"), repos = "
>> https://raw.githubusercontent.com/ArcadeAntics/PACKAGES")
>>
>> I tried to make your code a little nicer and also use 'pgpd', let me know
>> if this behaves as expected.
>>
>> #####
>> library(essentials)
>> a <- rnorm(1000, 110, 5)
>> b <- rnorm(1000, -0.3, 0.4)
>> s <- length(a)
>> lam <- 0.15
>> thr <- 70
>> r <- 10
>>
>> ff <- function(zz) {
>>      inner <- numeric(s)
>>      for (k in 1:s) {
>>          inner[k] <- 1 - lam * pgpd(zz, location = thr, scale = a[k], shape
>> = b[k], lower.tail = FALSE)
>>      }
>>
>>
>>      # if zz is length 1 (it is for uniroot), it would be much faster to do
>> this:
>>      # inner <- 1 - lam * pgpd(zz, location = thr, scale = a, shape = b,
>> lower.tail = FALSE)
>>      #
>>      # use whichever you please
>>
>>      mean(inner) - (1 - 1/r)
>> }
>> ########
>> out <- uniroot(ff, lower = 0, upper = 10000)$root
>> out
>>
>> On Thu, Aug 4, 2022 at 3:17 PM Md. Moyazzem Hossain <hossainmm at juniv.edu>
>> wrote:
>>
>>> Dear Andrew Simmons,
>>>
>>> Thank you very much.
>>>
>>> Actually, I want to calculate the predictive return level of GPD and
>>> compare it with the return level to see the difference. I have used the
>>> following function before but it does not provide the results as expected.
>>> It is OK for GEV but not GPD.
>>>
>>>          inner[k] = pgpd(zz, scale = a[k],  shape = b[k] )
>>>
>>> Other lines of the code were the same.
>>>
>>> Please help me to calculate the predictive return level of GPD.
>>>
>>> Thanks in advance.
>>>
>>> Hossain
>>>
>>> On Thu, Aug 4, 2022 at 5:11 PM Andrew Simmons <akwsimmo at gmail.com> wrote:
>>>
>>>> Is this a GEV distribution by chance? If so, I made functions dgev,
>>>> pgev, qgev, and rgev that you can find install with:
>>>>
>>>> install.packages(c("this.path", "essentials"), repos = "
>>>> https://raw.githubusercontent.com/ArcadeAntics/PACKAGES")
>>>>
>>>> and then in your R script
>>>>
>>>> library(essentials)
>>>> dgev(...) or pgev or qgev as needed
>>>>
>>>> On Thu, Aug 4, 2022, 09:31 Md. Moyazzem Hossain <hossainmm at juniv.edu>
>>>> wrote:
>>>>
>>>>> Dear R Experts,
>>>>>
>>>>> I hope that you are doing well.
>>>>>
>>>>> I am facing a problem to find out the value of the following function. I
>>>>> need help in this regard.
>>>>>
>>>>> #####
>>>>> a=rnorm(1000, 110, 5)
>>>>> b = rnorm(1000, -0.3, 0.4)
>>>>> s = length(a)
>>>>> lam=0.15
>>>>> thr=70
>>>>> r= 10
>>>>>
>>>>> ff = function(zz){
>>>>>    inner = vector("numeric", length = s)
>>>>>       for(k in 1:s){
>>>>>        inner[k]=(1- lam*((1+b[k]*((zz-thr)/a[k]))^(-1/b[k])))
>>>>>            }
>>>>>    answer = mean(inner)- (1- (1/r))
>>>>>    return(answer)
>>>>>    }
>>>>> ########
>>>>> out=uniroot(ff, lower = 0, upper = 10000 )$root
>>>>> out
>>>>>
>>>>> ########### Error ########
>>>>> Error in uniroot(ff, lower = 0, upper = 10000) :
>>>>>    f.upper = f(upper) is NA
>>>>>
>>>>> Please help me. Thanks in advance.
>>>>>
>>>>> Take care.
>>>>>
>>>>> Hossain
>>>>>
>>>>> --
>>>>> Best Regards,
>>>>> Md. Moyazzem Hossain
>>>>> Associate Professor
>>>>> Department of Statistics
>>>>> Jahangirnagar University
>>>>> Savar, Dhaka-1342, Bangladesh
>>>>>
>>>>>          [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>
>>> --
>>> Best Regards,
>>> Md. Moyazzem Hossain
>>> Associate Professor
>>> Department of Statistics
>>> Jahangirnagar University
>>> Savar, Dhaka-1342, Bangladesh
>>> Website: http://www.juniv.edu/teachers/hossainmm
>>> Research: *[image: Google Scholar]
>>> <https://scholar.google.com/citations?hl=en&user=-U03XCgAAAAJ>* | *ResearchGate
>>> <https://www.researchgate.net/profile/Md_Hossain107>* | *ORCID iD
>>> <https://orcid.org/0000-0003-3593-6936>*
>>>
>>
>


From er|nm@hodge@@ @end|ng |rom gm@||@com  Sun Aug  7 02:15:43 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Sat, 6 Aug 2022 20:15:43 -0400
Subject: [R] Reading a CSV file
Message-ID: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>

Hello!

Is there a way to read the first line of a CSV file, then skip 4 lines,
then continue reading, please?

I know you can skip from the top, but I don't know if you can read and then
skip.

Thanks,
Erin


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de  Sun Aug  7 02:51:12 2022
From: ||gge@ @end|ng |rom @t@t|@t|k@tu-dortmund@de (Uwe Ligges)
Date: Sun, 7 Aug 2022 02:51:12 +0200
Subject: [R] Reading a CSV file
In-Reply-To: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
Message-ID: <88b129f8-3d41-ecb6-0c53-3c111f6c236b@statistik.tu-dortmund.de>

csv <- readLines(filename)
read.csv(text = csv[-(2:5)]

Best,
Uwe Ligges


On 07.08.2022 02:15, Erin Hodgess wrote:
> Hello!
> 
> Is there a way to read the first line of a CSV file, then skip 4 lines,
> then continue reading, please?
> 
> I know you can skip from the top, but I don't know if you can read and then
> skip.
> 
> Thanks,
> Erin
> 
> 
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From er|nm@hodge@@ @end|ng |rom gm@||@com  Sun Aug  7 02:51:34 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Sat, 6 Aug 2022 20:51:34 -0400
Subject: [R] Reading a CSV file
In-Reply-To: <88b129f8-3d41-ecb6-0c53-3c111f6c236b@statistik.tu-dortmund.de>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
 <88b129f8-3d41-ecb6-0c53-3c111f6c236b@statistik.tu-dortmund.de>
Message-ID: <CACxE24k+RNF_peQXuevrWTWQrW0_iEd-yVPpJEVvg1uyHocGXA@mail.gmail.com>

Awesome!  Thanks so much!

On Sat, Aug 6, 2022 at 8:51 PM Uwe Ligges <ligges at statistik.tu-dortmund.de>
wrote:

> csv <- readLines(filename)
> read.csv(text = csv[-(2:5)]
>
> Best,
> Uwe Ligges
>
>
> On 07.08.2022 02:15, Erin Hodgess wrote:
> > Hello!
> >
> > Is there a way to read the first line of a CSV file, then skip 4 lines,
> > then continue reading, please?
> >
> > I know you can skip from the top, but I don't know if you can read and
> then
> > skip.
> >
> > Thanks,
> > Erin
> >
> >
> > Erin Hodgess, PhD
> > mailto: erinm.hodgess at gmail.com
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sun Aug  7 02:53:33 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 7 Aug 2022 10:53:33 +1000
Subject: [R] Reading a CSV file
In-Reply-To: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
Message-ID: <CA+8X3fUoz_UGFE9Lee5ooJWed3FL0urpEy+01egQhEYjmCthTw@mail.gmail.com>

HI Erin,
Just index out the lines in the result vector.

Jim

On Sun, Aug 7, 2022 at 10:16 AM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> Hello!
>
> Is there a way to read the first line of a CSV file, then skip 4 lines,
> then continue reading, please?
>
> I know you can skip from the top, but I don't know if you can read and then
> skip.
>
> Thanks,
> Erin
>
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Aug  7 02:59:38 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 06 Aug 2022 17:59:38 -0700
Subject: [R] Reading a CSV file
In-Reply-To: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
Message-ID: <A23F625C-A8EB-4DF5-8193-818248309A4B@dcn.davis.ca.us>

There is an example here [1] that illustrates one way to accomplish this.

[1] https://jdnewmil.github.io/blog/post/functions-talk/

On August 6, 2022 5:15:43 PM PDT, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>Hello!
>
>Is there a way to read the first line of a CSV file, then skip 4 lines,
>then continue reading, please?
>
>I know you can skip from the top, but I don't know if you can read and then
>skip.
>
>Thanks,
>Erin
>
>
>Erin Hodgess, PhD
>mailto: erinm.hodgess at gmail.com
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Aug  7 03:02:47 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 06 Aug 2022 18:02:47 -0700
Subject: [R] Reading a CSV file
In-Reply-To: <88b129f8-3d41-ecb6-0c53-3c111f6c236b@statistik.tu-dortmund.de>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
 <88b129f8-3d41-ecb6-0c53-3c111f6c236b@statistik.tu-dortmund.de>
Message-ID: <B29D865A-4CD9-4C51-823B-F0ACC9AFB43A@dcn.davis.ca.us>

Nice! (Needs just one more closing paren...)

On August 6, 2022 5:51:12 PM PDT, Uwe Ligges <ligges at statistik.tu-dortmund.de> wrote:
>csv <- readLines(filename)
>read.csv(text = csv[-(2:5)]
>
>Best,
>Uwe Ligges
>
>
>On 07.08.2022 02:15, Erin Hodgess wrote:
>> Hello!
>> 
>> Is there a way to read the first line of a CSV file, then skip 4 lines,
>> then continue reading, please?
>> 
>> I know you can skip from the top, but I don't know if you can read and then
>> skip.
>> 
>> Thanks,
>> Erin
>> 
>> 
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From er|nm@hodge@@ @end|ng |rom gm@||@com  Sun Aug  7 03:15:16 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Sat, 6 Aug 2022 21:15:16 -0400
Subject: [R] Reading a CSV file
In-Reply-To: <B29D865A-4CD9-4C51-823B-F0ACC9AFB43A@dcn.davis.ca.us>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
 <88b129f8-3d41-ecb6-0c53-3c111f6c236b@statistik.tu-dortmund.de>
 <B29D865A-4CD9-4C51-823B-F0ACC9AFB43A@dcn.davis.ca.us>
Message-ID: <CACxE24mnYNqswQPTjqoEDVG-yEQG9v+toOGTV5Y0q5E+3E6wig@mail.gmail.com>

All of these are so great!  Thanks so much, particularly on a Saturday
night!

Sincerely,
Erin

On Sat, Aug 6, 2022 at 9:02 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Nice! (Needs just one more closing paren...)
>
> On August 6, 2022 5:51:12 PM PDT, Uwe Ligges <
> ligges at statistik.tu-dortmund.de> wrote:
> >csv <- readLines(filename)
> >read.csv(text = csv[-(2:5)]
> >
> >Best,
> >Uwe Ligges
> >
> >
> >On 07.08.2022 02:15, Erin Hodgess wrote:
> >> Hello!
> >>
> >> Is there a way to read the first line of a CSV file, then skip 4 lines,
> >> then continue reading, please?
> >>
> >> I know you can skip from the top, but I don't know if you can read and
> then
> >> skip.
> >>
> >> Thanks,
> >> Erin
> >>
> >>
> >> Erin Hodgess, PhD
> >> mailto: erinm.hodgess at gmail.com
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Aug  7 03:24:59 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 06 Aug 2022 18:24:59 -0700
Subject: [R] Reading a CSV file
In-Reply-To: <CA+8X3fUoz_UGFE9Lee5ooJWed3FL0urpEy+01egQhEYjmCthTw@mail.gmail.com>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
 <CA+8X3fUoz_UGFE9Lee5ooJWed3FL0urpEy+01egQhEYjmCthTw@mail.gmail.com>
Message-ID: <918124C6-F34A-4D01-95D7-201A804FF18F@dcn.davis.ca.us>

Unfortunately, this can mess with your data types. Uwe's (and my more cluttered) approach doesn't.

On August 6, 2022 5:53:33 PM PDT, Jim Lemon <drjimlemon at gmail.com> wrote:
>HI Erin,
>Just index out the lines in the result vector.
>
>Jim
>
>On Sun, Aug 7, 2022 at 10:16 AM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>>
>> Hello!
>>
>> Is there a way to read the first line of a CSV file, then skip 4 lines,
>> then continue reading, please?
>>
>> I know you can skip from the top, but I don't know if you can read and then
>> skip.
>>
>> Thanks,
>> Erin
>>
>>
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @vi@e@gross m@iii@g oii gm@ii@com  Sun Aug  7 06:04:16 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Sun, 7 Aug 2022 00:04:16 -0400
Subject: [R] Reading a CSV file
In-Reply-To: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
Message-ID: <00a901d8aa12$c337fff0$49a7ffd0$@gmail.com>

Erin,

No explanation of why you want to do that. Are there comments on those
lines, for example?

I see others have replied with things that boils down to reading the entire
file into a data structure with lines, then using indexing of some sort to
eliminate the lines you want to skip and make data from the data structure
rather than the file. . That works, albeit for large files, ...

But there are, as usual, many ways to do things. Some people read in files
on their own and do the comma separation, type checking of columns and so on
and are free to make their own structure, perhaps the hard way. Clearly
skipping lines becomes trivial.

Then  there is the concept of reading it twice with the first pass trivially
picking up just a comma-separated line you can make into a series of headers
and then call something like read.csv and tell it to skip your N lines and
use these names for the columns.

Let me suggest an alternate solution IF you can arrange for the lines you do
not want (and only those) to begin with some comment character like "#" in
my example below:

text <- 'head1, head2
# ignore
# ignore 2
# ignore 3
# ignore 4
1,2
3,2'

hi <- read.csv(text=text, comment.char="#")

The above returns:

head1 head2
1     1     2
2     3     2

It will ignore any number of lines. If the data has anything special like
that, this might be a way to get what you want.

There are other packages like dplyr in the tidyverse with related but
sometimes different functionality and the same effect can be had with a
variation of my technique using read_csv() [note underscore not period]

hi <- text %>%  read_csv(comment="#")

Or use the new pipe symbol if you prefer:

hi <- text |> read_csv(comment="#")

What this gives you perhaps is more options such as a skip_empty_rows=TRUE
option that would remove the lines if blank.

And I tried some rather weird ideas like this:

text <- 'head1, head2
# ignore
# ignore 2 more stuff
# ignore 3, more stuff
# ignore 4
1,2
3,2'

hi <- text |> read_csv(col_types=c(col_integer(), col_integer()))

The idea was to TELL it what type to expect and hope the bad lines become
NA. Well, not quite. It made everything character given the above data that
was no longer suppressing comments:

> hi
# A tibble: 6 ? 2
head1                 head2     
<chr>                 <chr>     
  1 # ignore              NA        
2 # ignore 2 more stuff NA        
3 # ignore 3            more stuff
4 # ignore 4            NA        
5 1                     2         
6 3                     2         
> typeof(hi$head1)
[1] "character"
> typeof(hi$head2)
[1] "character"

But although this seems bad, it opens a door to consider. As long as
whatever is on those 4 lines does not mess things up by say making
additional columns, you probably can read the darn thing in to a tibble or
data.frame and then remove the rows you do not want and convert the columns
from character to whatever you want such as integer or numeric. 

Finally, if you have any control over the file contents, guess what happens
if you place the header line AFTER the four skipped lines like this?

text <- '# ignore
# ignore 2 more stuff
# ignore 3, more stuff
# ignore 4
head1, head2
1,2
3,2'

You now tell it to skip 4 lines AND use a header and it works for me!

read.csv(text=text, header=TRUE, skip=4)

There seems to be many ways to consider and I would not be shocked if some
program that does this data import even allowed you to specify what rows to
ignore more dynamically. 

But perhaps the first solution you got is more dynamic as it allows you to
process the text as a series of lines in all kinds of ways, such as removing
any rows that contain the number 666 or even editing it in some way,
combining data from multiple files, and so on.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
Sent: Saturday, August 6, 2022 8:16 PM
To: r-help at r-project.org
Subject: [R] Reading a CSV file

Hello!

Is there a way to read the first line of a CSV file, then skip 4 lines, then
continue reading, please?

I know you can skip from the top, but I don't know if you can read and then
skip.

Thanks,
Erin


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Sun Aug  7 06:06:50 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Sun, 7 Aug 2022 00:06:50 -0400
Subject: [R] Reading a CSV file
In-Reply-To: <CA+8X3fUoz_UGFE9Lee5ooJWed3FL0urpEy+01egQhEYjmCthTw@mail.gmail.com>
References: <CACxE24na6zZ=S5U0jD1VnTvbp38gtGTi-QWejPb_YyYFNO-Obg@mail.gmail.com>
 <CA+8X3fUoz_UGFE9Lee5ooJWed3FL0urpEy+01egQhEYjmCthTw@mail.gmail.com>
Message-ID: <00c201d8aa13$1f1aa4d0$5d4fee70$@gmail.com>

Jim,

The resulting column vectors may not come out right as they will not be read
into the same number of columns unless you have the right number of
commas/separators and worse, the result may likely be columns changed to the
same type such as text or float even if you intended integer.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Jim Lemon
Sent: Saturday, August 6, 2022 8:54 PM
To: Erin Hodgess <erinm.hodgess at gmail.com>
Cc: r-help at r-project.org
Subject: Re: [R] Reading a CSV file

HI Erin,
Just index out the lines in the result vector.

Jim

On Sun, Aug 7, 2022 at 10:16 AM Erin Hodgess <erinm.hodgess at gmail.com>
wrote:
>
> Hello!
>
> Is there a way to read the first line of a CSV file, then skip 4 
> lines, then continue reading, please?
>
> I know you can skip from the top, but I don't know if you can read and 
> then skip.
>
> Thanks,
> Erin
>
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From er|nm@hodge@@ @end|ng |rom gm@||@com  Mon Aug  8 19:22:51 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Mon, 8 Aug 2022 13:22:51 -0400
Subject: [R] Odd behavior of a function within apply
Message-ID: <CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>

Hello!

I have the following data.frame
 dput(test1.df[1:10,8:10])
structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
"June", "August", "", "", "August", "April", "February", "",
"December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")

And the following function:
> dput(count1a)
function (x)
{
    if (typeof(x) == "integer")
        y <- sum(is.na(x))
    if (typeof(x) == "character")
        y <- sum(x == "")
    return(y)
}
When I use the apply function with count1a, I get the following:
 apply(test1.df[1:10,8:10],2,count1a)
    X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
           NA             3            NA
However, when I do use columns 8 and 10, I get the correct response:
 apply(test1.df[1:10,c(8,10)],2,count1a)
   X1_1_HZP1 X1_1_HBM1_yr
           2            3
>
I am really baffled.  If I use count1a on a single column, it works fine.

Any suggestions much appreciated.
Thanks,
Sincerely,
Erin


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From @kw@|mmo @end|ng |rom gm@||@com  Mon Aug  8 19:29:19 2022
From: @kw@|mmo @end|ng |rom gm@||@com (Andrew Simmons)
Date: Mon, 8 Aug 2022 13:29:19 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
References: <CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
Message-ID: <CAPcHnpTL_s+YyNGcQZ18+SwOPgOSCTsYY+aBzZr7BrG8VbVyVw@mail.gmail.com>

When you use apply, it converts the input to an array, so all of the
integer columns in your data frame are converted to character class. I
would use lapply, sapply, or vapply instead to do this, I think you just
need to remove the "MARGIN" argument.

On Mon, Aug 8, 2022, 13:25 Erin Hodgess <erinm.hodgess at gmail.com> wrote:

> Hello!
>
> I have the following data.frame
>  dput(test1.df[1:10,8:10])
> structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
> NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
> "June", "August", "", "", "August", "April", "February", "",
> "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
> 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")
>
> And the following function:
> > dput(count1a)
> function (x)
> {
>     if (typeof(x) == "integer")
>         y <- sum(is.na(x))
>     if (typeof(x) == "character")
>         y <- sum(x == "")
>     return(y)
> }
> When I use the apply function with count1a, I get the following:
>  apply(test1.df[1:10,8:10],2,count1a)
>     X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>            NA             3            NA
> However, when I do use columns 8 and 10, I get the correct response:
>  apply(test1.df[1:10,c(8,10)],2,count1a)
>    X1_1_HZP1 X1_1_HBM1_yr
>            2            3
> >
> I am really baffled.  If I use count1a on a single column, it works fine.
>
> Any suggestions much appreciated.
> Thanks,
> Sincerely,
> Erin
>
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Mon Aug  8 19:37:58 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Mon, 8 Aug 2022 13:37:58 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
References: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
Message-ID: <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>

Dear Erin,

The problem is that the data frame gets coerced to a character matrix, 
and the only column with "" entries is the 9th (the second one you 
supplied):

as.matrix(test1.df)
    X1_1_HZP1 X1_1_HBM1_mon X1_1_HBM1_yr
1  "48160"   "December"    "2014"
2  "48198"   "June"        "2018"
3  "80027"   "August"      "2016"
4  "48161"   ""            NA
5  NA        ""            NA
6  "48911"   "August"      "1985"
7  NA        "April"       "2019"
8  "48197"   "February"    "1993"
9  "48021"   ""            NA
10 "11355"   "December"    "1990"

(Here, test1.df only contains the three columns you provided.)

A solution is to use sapply:

 > sapply(test1.df, count1a)
     X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
             2             3             3


I hope this helps,
  John


On 2022-08-08 1:22 p.m., Erin Hodgess wrote:
> Hello!
> 
> I have the following data.frame
>   dput(test1.df[1:10,8:10])
> structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
> NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
> "June", "August", "", "", "August", "April", "February", "",
> "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
> 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")
> 
> And the following function:
>> dput(count1a)
> function (x)
> {
>      if (typeof(x) == "integer")
>          y <- sum(is.na(x))
>      if (typeof(x) == "character")
>          y <- sum(x == "")
>      return(y)
> }
> When I use the apply function with count1a, I get the following:
>   apply(test1.df[1:10,8:10],2,count1a)
>      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>             NA             3            NA
> However, when I do use columns 8 and 10, I get the correct response:
>   apply(test1.df[1:10,c(8,10)],2,count1a)
>     X1_1_HZP1 X1_1_HBM1_yr
>             2            3
>>
> I am really baffled.  If I use count1a on a single column, it works fine.
> 
> Any suggestions much appreciated.
> Thanks,
> Sincerely,
> Erin
> 
> 
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From er|nm@hodge@@ @end|ng |rom gm@||@com  Mon Aug  8 19:38:44 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Mon, 8 Aug 2022 13:38:44 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <CAPcHnpTL_s+YyNGcQZ18+SwOPgOSCTsYY+aBzZr7BrG8VbVyVw@mail.gmail.com>
References: <CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <CAPcHnpTL_s+YyNGcQZ18+SwOPgOSCTsYY+aBzZr7BrG8VbVyVw@mail.gmail.com>
Message-ID: <CACxE24kFa=rVDp1txg-XwOvF5hyn7DMmG8TSp8LduP6fiWg8bw@mail.gmail.com>

Great, thank you so much!


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Mon, Aug 8, 2022 at 1:29 PM Andrew Simmons <akwsimmo at gmail.com> wrote:

> When you use apply, it converts the input to an array, so all of the
> integer columns in your data frame are converted to character class. I
> would use lapply, sapply, or vapply instead to do this, I think you just
> need to remove the "MARGIN" argument.
>
> On Mon, Aug 8, 2022, 13:25 Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
>> Hello!
>>
>> I have the following data.frame
>>  dput(test1.df[1:10,8:10])
>> structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
>> NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
>> "June", "August", "", "", "August", "April", "February", "",
>> "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
>> 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")
>>
>> And the following function:
>> > dput(count1a)
>> function (x)
>> {
>>     if (typeof(x) == "integer")
>>         y <- sum(is.na(x))
>>     if (typeof(x) == "character")
>>         y <- sum(x == "")
>>     return(y)
>> }
>> When I use the apply function with count1a, I get the following:
>>  apply(test1.df[1:10,8:10],2,count1a)
>>     X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>>            NA             3            NA
>> However, when I do use columns 8 and 10, I get the correct response:
>>  apply(test1.df[1:10,c(8,10)],2,count1a)
>>    X1_1_HZP1 X1_1_HBM1_yr
>>            2            3
>> >
>> I am really baffled.  If I use count1a on a single column, it works fine.
>>
>> Any suggestions much appreciated.
>> Thanks,
>> Sincerely,
>> Erin
>>
>>
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Mon Aug  8 19:41:23 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Mon, 8 Aug 2022 13:41:23 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <CAPcHnpTL_s+YyNGcQZ18+SwOPgOSCTsYY+aBzZr7BrG8VbVyVw@mail.gmail.com>
References: <CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <CAPcHnpTL_s+YyNGcQZ18+SwOPgOSCTsYY+aBzZr7BrG8VbVyVw@mail.gmail.com>
Message-ID: <CACxE24nNV2oTKN_MLX_+nqJsVD1SGniS5om62seMZ7TkqiYNrg@mail.gmail.com>

lapply worked perfectly!

Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Mon, Aug 8, 2022 at 1:29 PM Andrew Simmons <akwsimmo at gmail.com> wrote:

> When you use apply, it converts the input to an array, so all of the
> integer columns in your data frame are converted to character class. I
> would use lapply, sapply, or vapply instead to do this, I think you just
> need to remove the "MARGIN" argument.
>
> On Mon, Aug 8, 2022, 13:25 Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
>> Hello!
>>
>> I have the following data.frame
>>  dput(test1.df[1:10,8:10])
>> structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
>> NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
>> "June", "August", "", "", "August", "April", "February", "",
>> "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
>> 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")
>>
>> And the following function:
>> > dput(count1a)
>> function (x)
>> {
>>     if (typeof(x) == "integer")
>>         y <- sum(is.na(x))
>>     if (typeof(x) == "character")
>>         y <- sum(x == "")
>>     return(y)
>> }
>> When I use the apply function with count1a, I get the following:
>>  apply(test1.df[1:10,8:10],2,count1a)
>>     X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>>            NA             3            NA
>> However, when I do use columns 8 and 10, I get the correct response:
>>  apply(test1.df[1:10,c(8,10)],2,count1a)
>>    X1_1_HZP1 X1_1_HBM1_yr
>>            2            3
>> >
>> I am really baffled.  If I use count1a on a single column, it works fine.
>>
>> Any suggestions much appreciated.
>> Thanks,
>> Sincerely,
>> Erin
>>
>>
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Mon Aug  8 19:41:56 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Mon, 8 Aug 2022 13:41:56 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>
References: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>
Message-ID: <CACxE24nUyFMb2sUneC4efnh11BiCvAdxGt50ZVe_GqyP1o20SQ@mail.gmail.com>

Awesome, thanks so much!!

Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Mon, Aug 8, 2022 at 1:38 PM John Fox <jfox at mcmaster.ca> wrote:

> Dear Erin,
>
> The problem is that the data frame gets coerced to a character matrix,
> and the only column with "" entries is the 9th (the second one you
> supplied):
>
> as.matrix(test1.df)
>     X1_1_HZP1 X1_1_HBM1_mon X1_1_HBM1_yr
> 1  "48160"   "December"    "2014"
> 2  "48198"   "June"        "2018"
> 3  "80027"   "August"      "2016"
> 4  "48161"   ""            NA
> 5  NA        ""            NA
> 6  "48911"   "August"      "1985"
> 7  NA        "April"       "2019"
> 8  "48197"   "February"    "1993"
> 9  "48021"   ""            NA
> 10 "11355"   "December"    "1990"
>
> (Here, test1.df only contains the three columns you provided.)
>
> A solution is to use sapply:
>
>  > sapply(test1.df, count1a)
>      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>              2             3             3
>
>
> I hope this helps,
>   John
>
>
> On 2022-08-08 1:22 p.m., Erin Hodgess wrote:
> > Hello!
> >
> > I have the following data.frame
> >   dput(test1.df[1:10,8:10])
> > structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
> > NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
> > "June", "August", "", "", "August", "April", "February", "",
> > "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
> > 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")
> >
> > And the following function:
> >> dput(count1a)
> > function (x)
> > {
> >      if (typeof(x) == "integer")
> >          y <- sum(is.na(x))
> >      if (typeof(x) == "character")
> >          y <- sum(x == "")
> >      return(y)
> > }
> > When I use the apply function with count1a, I get the following:
> >   apply(test1.df[1:10,8:10],2,count1a)
> >      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
> >             NA             3            NA
> > However, when I do use columns 8 and 10, I get the correct response:
> >   apply(test1.df[1:10,c(8,10)],2,count1a)
> >     X1_1_HZP1 X1_1_HBM1_yr
> >             2            3
> >>
> > I am really baffled.  If I use count1a on a single column, it works fine.
> >
> > Any suggestions much appreciated.
> > Thanks,
> > Sincerely,
> > Erin
> >
> >
> > Erin Hodgess, PhD
> > mailto: erinm.hodgess at gmail.com
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://socialsciences.mcmaster.ca/jfox/
>
>

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Mon Aug  8 20:05:23 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Mon, 8 Aug 2022 14:05:23 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <CACxE24mLEnwUvWACXEt8+EWNnGbgNWEMwndqTrxY=yCfp1oxfA@mail.gmail.com>
References: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>
 <CACxE24nUyFMb2sUneC4efnh11BiCvAdxGt50ZVe_GqyP1o20SQ@mail.gmail.com>
 <CACxE24mLEnwUvWACXEt8+EWNnGbgNWEMwndqTrxY=yCfp1oxfA@mail.gmail.com>
Message-ID: <CACxE24kdpGsVX0g6069Y_Lhkx2tN94iBA-Y1WiXTPBy4uDWJAg@mail.gmail.com>

Nailed it!

There were a few "logical" columns in my data.frame.

Thanks for all of the help!
Sincerely,
Erin

Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Mon, Aug 8, 2022 at 1:51 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:

> OK.  I'm back again.
>
> So my test1.df is 236x390
>
> If I put in the following:
>  lapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> > lapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> > sapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> >
> What am I doing wrong, please?
> Thanks,
> Erin
>
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>
> On Mon, Aug 8, 2022 at 1:41 PM Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
>
>> Awesome, thanks so much!!
>>
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>
>> On Mon, Aug 8, 2022 at 1:38 PM John Fox <jfox at mcmaster.ca> wrote:
>>
>>> Dear Erin,
>>>
>>> The problem is that the data frame gets coerced to a character matrix,
>>> and the only column with "" entries is the 9th (the second one you
>>> supplied):
>>>
>>> as.matrix(test1.df)
>>>     X1_1_HZP1 X1_1_HBM1_mon X1_1_HBM1_yr
>>> 1  "48160"   "December"    "2014"
>>> 2  "48198"   "June"        "2018"
>>> 3  "80027"   "August"      "2016"
>>> 4  "48161"   ""            NA
>>> 5  NA        ""            NA
>>> 6  "48911"   "August"      "1985"
>>> 7  NA        "April"       "2019"
>>> 8  "48197"   "February"    "1993"
>>> 9  "48021"   ""            NA
>>> 10 "11355"   "December"    "1990"
>>>
>>> (Here, test1.df only contains the three columns you provided.)
>>>
>>> A solution is to use sapply:
>>>
>>>  > sapply(test1.df, count1a)
>>>      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>>>              2             3             3
>>>
>>>
>>> I hope this helps,
>>>   John
>>>
>>>
>>> On 2022-08-08 1:22 p.m., Erin Hodgess wrote:
>>> > Hello!
>>> >
>>> > I have the following data.frame
>>> >   dput(test1.df[1:10,8:10])
>>> > structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
>>> > NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
>>> > "June", "August", "", "", "August", "April", "February", "",
>>> > "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
>>> > 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class =
>>> "data.frame")
>>> >
>>> > And the following function:
>>> >> dput(count1a)
>>> > function (x)
>>> > {
>>> >      if (typeof(x) == "integer")
>>> >          y <- sum(is.na(x))
>>> >      if (typeof(x) == "character")
>>> >          y <- sum(x == "")
>>> >      return(y)
>>> > }
>>> > When I use the apply function with count1a, I get the following:
>>> >   apply(test1.df[1:10,8:10],2,count1a)
>>> >      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>>> >             NA             3            NA
>>> > However, when I do use columns 8 and 10, I get the correct response:
>>> >   apply(test1.df[1:10,c(8,10)],2,count1a)
>>> >     X1_1_HZP1 X1_1_HBM1_yr
>>> >             2            3
>>> >>
>>> > I am really baffled.  If I use count1a on a single column, it works
>>> fine.
>>> >
>>> > Any suggestions much appreciated.
>>> > Thanks,
>>> > Sincerely,
>>> > Erin
>>> >
>>> >
>>> > Erin Hodgess, PhD
>>> > mailto: erinm.hodgess at gmail.com
>>> >
>>> >       [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>> --
>>> John Fox, Professor Emeritus
>>> McMaster University
>>> Hamilton, Ontario, Canada
>>> web: https://socialsciences.mcmaster.ca/jfox/
>>>
>>>

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Mon Aug  8 19:51:01 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Mon, 8 Aug 2022 13:51:01 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <CACxE24nUyFMb2sUneC4efnh11BiCvAdxGt50ZVe_GqyP1o20SQ@mail.gmail.com>
References: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>
 <CACxE24nUyFMb2sUneC4efnh11BiCvAdxGt50ZVe_GqyP1o20SQ@mail.gmail.com>
Message-ID: <CACxE24mLEnwUvWACXEt8+EWNnGbgNWEMwndqTrxY=yCfp1oxfA@mail.gmail.com>

OK.  I'm back again.

So my test1.df is 236x390

If I put in the following:
 lapply(test1.df,count1a)
Error in FUN(X[[i]], ...) : object 'y' not found
> lapply(test1.df,count1a)
Error in FUN(X[[i]], ...) : object 'y' not found
> sapply(test1.df,count1a)
Error in FUN(X[[i]], ...) : object 'y' not found
>
What am I doing wrong, please?
Thanks,
Erin


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Mon, Aug 8, 2022 at 1:41 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:

> Awesome, thanks so much!!
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>
> On Mon, Aug 8, 2022 at 1:38 PM John Fox <jfox at mcmaster.ca> wrote:
>
>> Dear Erin,
>>
>> The problem is that the data frame gets coerced to a character matrix,
>> and the only column with "" entries is the 9th (the second one you
>> supplied):
>>
>> as.matrix(test1.df)
>>     X1_1_HZP1 X1_1_HBM1_mon X1_1_HBM1_yr
>> 1  "48160"   "December"    "2014"
>> 2  "48198"   "June"        "2018"
>> 3  "80027"   "August"      "2016"
>> 4  "48161"   ""            NA
>> 5  NA        ""            NA
>> 6  "48911"   "August"      "1985"
>> 7  NA        "April"       "2019"
>> 8  "48197"   "February"    "1993"
>> 9  "48021"   ""            NA
>> 10 "11355"   "December"    "1990"
>>
>> (Here, test1.df only contains the three columns you provided.)
>>
>> A solution is to use sapply:
>>
>>  > sapply(test1.df, count1a)
>>      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>>              2             3             3
>>
>>
>> I hope this helps,
>>   John
>>
>>
>> On 2022-08-08 1:22 p.m., Erin Hodgess wrote:
>> > Hello!
>> >
>> > I have the following data.frame
>> >   dput(test1.df[1:10,8:10])
>> > structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
>> > NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
>> > "June", "August", "", "", "August", "April", "February", "",
>> > "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
>> > 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")
>> >
>> > And the following function:
>> >> dput(count1a)
>> > function (x)
>> > {
>> >      if (typeof(x) == "integer")
>> >          y <- sum(is.na(x))
>> >      if (typeof(x) == "character")
>> >          y <- sum(x == "")
>> >      return(y)
>> > }
>> > When I use the apply function with count1a, I get the following:
>> >   apply(test1.df[1:10,8:10],2,count1a)
>> >      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>> >             NA             3            NA
>> > However, when I do use columns 8 and 10, I get the correct response:
>> >   apply(test1.df[1:10,c(8,10)],2,count1a)
>> >     X1_1_HZP1 X1_1_HBM1_yr
>> >             2            3
>> >>
>> > I am really baffled.  If I use count1a on a single column, it works
>> fine.
>> >
>> > Any suggestions much appreciated.
>> > Thanks,
>> > Sincerely,
>> > Erin
>> >
>> >
>> > Erin Hodgess, PhD
>> > mailto: erinm.hodgess at gmail.com
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> --
>> John Fox, Professor Emeritus
>> McMaster University
>> Hamilton, Ontario, Canada
>> web: https://socialsciences.mcmaster.ca/jfox/
>>
>>

	[[alternative HTML version deleted]]


From dc@r|@on @end|ng |rom t@mu@edu  Tue Aug  9 17:33:11 2022
From: dc@r|@on @end|ng |rom t@mu@edu (David Carlson)
Date: Tue, 9 Aug 2022 10:33:11 -0500
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <CACxE24mLEnwUvWACXEt8+EWNnGbgNWEMwndqTrxY=yCfp1oxfA@mail.gmail.com>
References: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>
 <CACxE24nUyFMb2sUneC4efnh11BiCvAdxGt50ZVe_GqyP1o20SQ@mail.gmail.com>
 <CACxE24mLEnwUvWACXEt8+EWNnGbgNWEMwndqTrxY=yCfp1oxfA@mail.gmail.com>
Message-ID: <CAE-dL2r2m+RPkv2etCv3Nt54_OH_OwN8ho=UCb00iS57pkcooA@mail.gmail.com>

Could you have columns that are not character or integer so that y is never
defined in the function?

count1a(1:5/3)
Error in count1a(1:5/3) : object 'y' not found

David Carlson


On Mon, Aug 8, 2022 at 1:35 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:

> OK.?? I'm back again.?? So my test1.??df is 236x390 If I put in the
> following:?? lapply(test1.??df,count1a) Error in FUN(X[[i]], .??.??.??) :??
> object 'y' not found > lapply(test1.??df,count1a) Error in FUN(X[[i]],
> .??.??.??) :?? object 'y' not found > sapply(test1.??df,count1a)
> ZjQcmQRYFpfptBannerStart
> This Message Is From an External Sender
> This message came from outside your organization.
>
> ZjQcmQRYFpfptBannerEnd
>
> OK.  I'm back again.
>
> So my test1.df is 236x390
>
> If I put in the following:
>  lapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> > lapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> > sapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> >
> What am I doing wrong, please?
> Thanks,
> Erin
>
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>
> On Mon, Aug 8, 2022 at 1:41 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> > Awesome, thanks so much!!
> >
> > Erin Hodgess, PhD
> > mailto: erinm.hodgess at gmail.com
> >
> >
> > On Mon, Aug 8, 2022 at 1:38 PM John Fox <jfox at mcmaster.ca> wrote:
> >
> >> Dear Erin,
> >>
> >> The problem is that the data frame gets coerced to a character matrix,
> >> and the only column with "" entries is the 9th (the second one you
> >> supplied):
> >>
> >> as.matrix(test1.df)
> >>     X1_1_HZP1 X1_1_HBM1_mon X1_1_HBM1_yr
> >> 1  "48160"   "December"    "2014"
> >> 2  "48198"   "June"        "2018"
> >> 3  "80027"   "August"      "2016"
> >> 4  "48161"   ""            NA
> >> 5  NA        ""            NA
> >> 6  "48911"   "August"      "1985"
> >> 7  NA        "April"       "2019"
> >> 8  "48197"   "February"    "1993"
> >> 9  "48021"   ""            NA
> >> 10 "11355"   "December"    "1990"
> >>
> >> (Here, test1.df only contains the three columns you provided.)
> >>
> >> A solution is to use sapply:
> >>
> >>  > sapply(test1.df, count1a)
> >>      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
> >>              2             3             3
> >>
> >>
> >> I hope this helps,
> >>   John
> >>
> >>
> >> On 2022-08-08 1:22 p.m., Erin Hodgess wrote:
> >> > Hello!
> >> >
> >> > I have the following data.frame
> >> >   dput(test1.df[1:10,8:10])
> >> > structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
> >> > NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
> >> > "June", "August", "", "", "August", "April", "February", "",
> >> > "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
> >> > 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")
> >> >
> >> > And the following function:
> >> >> dput(count1a)
> >> > function (x)
> >> > {
> >> >      if (typeof(x) == "integer")
> >> >          y <- sum(is.na(x))
> >> >      if (typeof(x) == "character")
> >> >          y <- sum(x == "")
> >> >      return(y)
> >> > }
> >> > When I use the apply function with count1a, I get the following:
> >> >   apply(test1.df[1:10,8:10],2,count1a)
> >> >      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
> >> >             NA             3            NA
> >> > However, when I do use columns 8 and 10, I get the correct response:
> >> >   apply(test1.df[1:10,c(8,10)],2,count1a)
> >> >     X1_1_HZP1 X1_1_HBM1_yr
> >> >             2            3
> >> >>
> >> > I am really baffled.  If I use count1a on a single column, it works
> >> fine.
> >> >
> >> > Any suggestions much appreciated.
> >> > Thanks,
> >> > Sincerely,
> >> > Erin
> >> >
> >> >
> >> > Erin Hodgess, PhD
> >> > mailto: erinm.hodgess at gmail.com
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-help__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MBvSbrfE$
> >> > PLEASE do read the posting guide
> >> https://urldefense.com/v3/__http://www.R-project.org/posting-guide.html__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MdYOqruE$
> >> > and provide commented, minimal, self-contained, reproducible code.
> >> --
> >> John Fox, Professor Emeritus
> >> McMaster University
> >> Hamilton, Ontario, Canada
> >> web: https://urldefense.com/v3/__https://socialsciences.mcmaster.ca/jfox/__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MRU4wu3o$
> >>
> >>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, seehttps://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-help__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MBvSbrfE$
> PLEASE do read the posting guide https://urldefense.com/v3/__http://www.R-project.org/posting-guide.html__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MdYOqruE$
> and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Tue Aug  9 17:35:29 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 9 Aug 2022 11:35:29 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <CAE-dL2r2m+RPkv2etCv3Nt54_OH_OwN8ho=UCb00iS57pkcooA@mail.gmail.com>
References: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>
 <CACxE24nUyFMb2sUneC4efnh11BiCvAdxGt50ZVe_GqyP1o20SQ@mail.gmail.com>
 <CACxE24mLEnwUvWACXEt8+EWNnGbgNWEMwndqTrxY=yCfp1oxfA@mail.gmail.com>
 <CAE-dL2r2m+RPkv2etCv3Nt54_OH_OwN8ho=UCb00iS57pkcooA@mail.gmail.com>
Message-ID: <CACxE24k7Q2qV1qO9Jd4BfFpsyoRnOkPJ5JK+6LN26goq7bCnWQ@mail.gmail.com>

My ?count1a? function was only looking for types of integers and
characters.  There were a few logical types, which were the source of the
error.

Thanks,
Erin

On Tue, Aug 9, 2022 at 11:33 AM David Carlson <dcarlson at tamu.edu> wrote:

> Could you have columns that are not character or integer so that y is
> never defined in the function?
>
> count1a(1:5/3)
> Error in count1a(1:5/3) : object 'y' not found
>
> David Carlson
>
>
> On Mon, Aug 8, 2022 at 1:35 PM Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
>
>> OK.?? I'm back again.?? So my test1.??df is 236x390 If I put in the
>> following:?? lapply(test1.??df,count1a) Error in FUN(X[[i]], .??.??.??) :??
>> object 'y' not found > lapply(test1.??df,count1a) Error in FUN(X[[i]],
>> .??.??.??) :?? object 'y' not found > sapply(test1.??df,count1a)
>> ZjQcmQRYFpfptBannerStart
>> This Message Is From an External Sender
>> This message came from outside your organization.
>>
>> ZjQcmQRYFpfptBannerEnd
>>
>> OK.  I'm back again.
>>
>> So my test1.df is 236x390
>>
>> If I put in the following:
>>  lapply(test1.df,count1a)
>> Error in FUN(X[[i]], ...) : object 'y' not found
>> > lapply(test1.df,count1a)
>> Error in FUN(X[[i]], ...) : object 'y' not found
>> > sapply(test1.df,count1a)
>> Error in FUN(X[[i]], ...) : object 'y' not found
>> >
>> What am I doing wrong, please?
>> Thanks,
>> Erin
>>
>>
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>
>> On Mon, Aug 8, 2022 at 1:41 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>>
>> > Awesome, thanks so much!!
>> >
>> > Erin Hodgess, PhD
>> > mailto: erinm.hodgess at gmail.com
>> >
>> >
>> > On Mon, Aug 8, 2022 at 1:38 PM John Fox <jfox at mcmaster.ca> wrote:
>> >
>> >> Dear Erin,
>> >>
>> >> The problem is that the data frame gets coerced to a character matrix,
>> >> and the only column with "" entries is the 9th (the second one you
>> >> supplied):
>> >>
>> >> as.matrix(test1.df)
>> >>     X1_1_HZP1 X1_1_HBM1_mon X1_1_HBM1_yr
>> >> 1  "48160"   "December"    "2014"
>> >> 2  "48198"   "June"        "2018"
>> >> 3  "80027"   "August"      "2016"
>> >> 4  "48161"   ""            NA
>> >> 5  NA        ""            NA
>> >> 6  "48911"   "August"      "1985"
>> >> 7  NA        "April"       "2019"
>> >> 8  "48197"   "February"    "1993"
>> >> 9  "48021"   ""            NA
>> >> 10 "11355"   "December"    "1990"
>> >>
>> >> (Here, test1.df only contains the three columns you provided.)
>> >>
>> >> A solution is to use sapply:
>> >>
>> >>  > sapply(test1.df, count1a)
>> >>      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>> >>              2             3             3
>> >>
>> >>
>> >> I hope this helps,
>> >>   John
>> >>
>> >>
>> >> On 2022-08-08 1:22 p.m., Erin Hodgess wrote:
>> >> > Hello!
>> >> >
>> >> > I have the following data.frame
>> >> >   dput(test1.df[1:10,8:10])
>> >> > structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L,
>> >> > NA, 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = c("December",
>> >> > "June", "August", "", "", "August", "April", "February", "",
>> >> > "December"), X1_1_HBM1_yr = c(2014L, 2018L, 2016L, NA, NA, 1985L,
>> >> > 2019L, 1993L, NA, 1990L)), row.names = c(NA, 10L), class = "data.frame")
>> >> >
>> >> > And the following function:
>> >> >> dput(count1a)
>> >> > function (x)
>> >> > {
>> >> >      if (typeof(x) == "integer")
>> >> >          y <- sum(is.na(x))
>> >> >      if (typeof(x) == "character")
>> >> >          y <- sum(x == "")
>> >> >      return(y)
>> >> > }
>> >> > When I use the apply function with count1a, I get the following:
>> >> >   apply(test1.df[1:10,8:10],2,count1a)
>> >> >      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
>> >> >             NA             3            NA
>> >> > However, when I do use columns 8 and 10, I get the correct response:
>> >> >   apply(test1.df[1:10,c(8,10)],2,count1a)
>> >> >     X1_1_HZP1 X1_1_HBM1_yr
>> >> >             2            3
>> >> >>
>> >> > I am really baffled.  If I use count1a on a single column, it works
>> >> fine.
>> >> >
>> >> > Any suggestions much appreciated.
>> >> > Thanks,
>> >> > Sincerely,
>> >> > Erin
>> >> >
>> >> >
>> >> > Erin Hodgess, PhD
>> >> > mailto: erinm.hodgess at gmail.com
>> >> >
>> >> >       [[alternative HTML version deleted]]
>> >> >
>> >> > ______________________________________________
>> >> > R-help at r-project.org
>>
>>  mailing list -- To UNSUBSCRIBE and more, see
>> >> > https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-help__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MBvSbrfE$
>> >> > PLEASE do read the posting guide
>> >> https://urldefense.com/v3/__http://www.R-project.org/posting-guide.html__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MdYOqruE$
>> >> > and provide commented, minimal, self-contained, reproducible code.
>> >> --
>> >> John Fox, Professor Emeritus
>> >> McMaster University
>> >> Hamilton, Ontario, Canada
>> >> web: https://urldefense.com/v3/__https://socialsciences.mcmaster.ca/jfox/__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MRU4wu3o$
>> >>
>> >>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, seehttps://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-help__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MBvSbrfE$
>> PLEASE do read the posting guide https://urldefense.com/v3/__http://www.R-project.org/posting-guide.html__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MdYOqruE$
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Tue Aug  9 18:55:22 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Tue, 9 Aug 2022 12:55:22 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <CAE-dL2r2m+RPkv2etCv3Nt54_OH_OwN8ho=UCb00iS57pkcooA@mail.gmail.com>
References: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>
 <CACxE24nUyFMb2sUneC4efnh11BiCvAdxGt50ZVe_GqyP1o20SQ@mail.gmail.com>
 <CACxE24mLEnwUvWACXEt8+EWNnGbgNWEMwndqTrxY=yCfp1oxfA@mail.gmail.com>
 <CAE-dL2r2m+RPkv2etCv3Nt54_OH_OwN8ho=UCb00iS57pkcooA@mail.gmail.com>
Message-ID: <00d001d8ac10$d0972450$71c56cf0$@gmail.com>

Yes, David, the function described seems to insist it be of type integer or type character and if the type was double or others might well fail as y would never be initialized.

The goal seems to be to count how many "missing" values are found as in NA if a numeric type or an empty string if character.

But you can have some form of NA in all kinds of object types including character as in this construct:

> x <- c("a", NA, "", "b", "NA)")
> x
[1] "a"   NA    ""    "b"   "NA)"

The above has three useless elements if both NA and "" are considered empty. So logically the condition could be to count NA and IF it is of type character, also count "". 

So rather than play games testing not just is.integer, is.double (or just is.numeric) as well as is.logical and is.raw, all the above can be tested with is.na() first to add up how many Na they contain. If then it is of type character, you can add any blank strings. 

So the algorithm would initialize y to sum(is.na(vec)) and then if the vec is character, add the sum of how many empty strings.

Alternately, the function should deal with what it wants to do if any other type is encountered. You can internally converts many things to integer or character and then operate on them. Or you can return a zero or raise an alarm when given something else.

In this case, simply setting y to zero before using it would make it defined and avoid the error, albeit report nothing found if it was a double or Boolean vector even if it did contain NA.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of David Carlson via R-help
Sent: Tuesday, August 9, 2022 11:33 AM
To: Erin Hodgess <erinm.hodgess at gmail.com>
Cc: r-help at r-project.org
Subject: Re: [R] Odd behavior of a function within apply

Could you have columns that are not character or integer so that y is never defined in the function?

count1a(1:5/3)
Error in count1a(1:5/3) : object 'y' not found

David Carlson


On Mon, Aug 8, 2022 at 1:35 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:

> OK.?? I'm back again.?? So my test1.??df is 236x390 If I put in the 
> following:?? lapply(test1.??df,count1a) Error in FUN(X[[i]], 
> .??.??.??) :?? object 'y' not found > lapply(test1.??df,count1a) Error 
> in FUN(X[[i]],
> .??.??.??) :?? object 'y' not found > sapply(test1.??df,count1a) 
> ZjQcmQRYFpfptBannerStart This Message Is From an External Sender This 
> message came from outside your organization.
>
> ZjQcmQRYFpfptBannerEnd
>
> OK.  I'm back again.
>
> So my test1.df is 236x390
>
> If I put in the following:
>  lapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> > lapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> > sapply(test1.df,count1a)
> Error in FUN(X[[i]], ...) : object 'y' not found
> >
> What am I doing wrong, please?
> Thanks,
> Erin
>
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>
> On Mon, Aug 8, 2022 at 1:41 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> > Awesome, thanks so much!!
> >
> > Erin Hodgess, PhD
> > mailto: erinm.hodgess at gmail.com
> >
> >
> > On Mon, Aug 8, 2022 at 1:38 PM John Fox <jfox at mcmaster.ca> wrote:
> >
> >> Dear Erin,
> >>
> >> The problem is that the data frame gets coerced to a character 
> >> matrix, and the only column with "" entries is the 9th (the second 
> >> one you
> >> supplied):
> >>
> >> as.matrix(test1.df)
> >>     X1_1_HZP1 X1_1_HBM1_mon X1_1_HBM1_yr
> >> 1  "48160"   "December"    "2014"
> >> 2  "48198"   "June"        "2018"
> >> 3  "80027"   "August"      "2016"
> >> 4  "48161"   ""            NA
> >> 5  NA        ""            NA
> >> 6  "48911"   "August"      "1985"
> >> 7  NA        "April"       "2019"
> >> 8  "48197"   "February"    "1993"
> >> 9  "48021"   ""            NA
> >> 10 "11355"   "December"    "1990"
> >>
> >> (Here, test1.df only contains the three columns you provided.)
> >>
> >> A solution is to use sapply:
> >>
> >>  > sapply(test1.df, count1a)
> >>      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
> >>              2             3             3
> >>
> >>
> >> I hope this helps,
> >>   John
> >>
> >>
> >> On 2022-08-08 1:22 p.m., Erin Hodgess wrote:
> >> > Hello!
> >> >
> >> > I have the following data.frame
> >> >   dput(test1.df[1:10,8:10])
> >> > structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L, NA, 
> >> > 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon = 
> >> > c("December", "June", "August", "", "", "August", "April", 
> >> > "February", "", "December"), X1_1_HBM1_yr = c(2014L, 2018L, 
> >> > 2016L, NA, NA, 1985L, 2019L, 1993L, NA, 1990L)), row.names = 
> >> > c(NA, 10L), class = "data.frame")
> >> >
> >> > And the following function:
> >> >> dput(count1a)
> >> > function (x)
> >> > {
> >> >      if (typeof(x) == "integer")
> >> >          y <- sum(is.na(x))
> >> >      if (typeof(x) == "character")
> >> >          y <- sum(x == "")
> >> >      return(y)
> >> > }
> >> > When I use the apply function with count1a, I get the following:
> >> >   apply(test1.df[1:10,8:10],2,count1a)
> >> >      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
> >> >             NA             3            NA
> >> > However, when I do use columns 8 and 10, I get the correct response:
> >> >   apply(test1.df[1:10,c(8,10)],2,count1a)
> >> >     X1_1_HZP1 X1_1_HBM1_yr
> >> >             2            3
> >> >>
> >> > I am really baffled.  If I use count1a on a single column, it 
> >> > works
> >> fine.
> >> >
> >> > Any suggestions much appreciated.
> >> > Thanks,
> >> > Sincerely,
> >> > Erin
> >> >
> >> >
> >> > Erin Hodgess, PhD
> >> > mailto: erinm.hodgess at gmail.com
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> >> > https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo
> >> > /r-help__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kd
> >> > RHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MBvSbrfE$
> >> > PLEASE do read the posting guide
> >> https://urldefense.com/v3/__http://www.R-project.org/posting-guide.
> >> html__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8
> >> SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MdYOqruE$
> >> > and provide commented, minimal, self-contained, reproducible code.
> >> --
> >> John Fox, Professor Emeritus
> >> McMaster University
> >> Hamilton, Ontario, Canada
> >> web: 
> >> https://urldefense.com/v3/__https://socialsciences.mcmaster.ca/jfox
> >> /__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJ
> >> dx6Uq0p4rpBa4E3DkmQ65UImH48MRU4wu3o$
> >>
> >>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________R-help at r-project.org 
> mailing list -- To UNSUBSCRIBE and more, 
> seehttps://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r
> -help__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJ
> Jdx6Uq0p4rpBa4E3DkmQ65UImH48MBvSbrfE$
> PLEASE do read the posting guide 
> https://urldefense.com/v3/__http://www.R-project.org/posting-guide.htm
> l__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6
> Uq0p4rpBa4E3DkmQ65UImH48MdYOqruE$ and provide commented, minimal, 
> self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From er|nm@hodge@@ @end|ng |rom gm@||@com  Tue Aug  9 19:04:11 2022
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 9 Aug 2022 13:04:11 -0400
Subject: [R] Odd behavior of a function within apply
In-Reply-To: <00d001d8ac10$d0972450$71c56cf0$@gmail.com>
References: <9435_1659979524_278HPNvu030617_CACxE24m2C3vzwGpYYoWVrLpLcx7jAK1FnnRHorwm-YFwDDzqPg@mail.gmail.com>
 <565e3f72-a449-753f-ebee-b44d0f62eb2b@mcmaster.ca>
 <CACxE24nUyFMb2sUneC4efnh11BiCvAdxGt50ZVe_GqyP1o20SQ@mail.gmail.com>
 <CACxE24mLEnwUvWACXEt8+EWNnGbgNWEMwndqTrxY=yCfp1oxfA@mail.gmail.com>
 <CAE-dL2r2m+RPkv2etCv3Nt54_OH_OwN8ho=UCb00iS57pkcooA@mail.gmail.com>
 <00d001d8ac10$d0972450$71c56cf0$@gmail.com>
Message-ID: <CACxE24nczvwmpB58+655guHmDE476MFvGUwGRJr78hP48Ei3Fg@mail.gmail.com>

Avi, that?s great!

Thanks

On Tue, Aug 9, 2022 at 12:56 PM <avi.e.gross at gmail.com> wrote:

> Yes, David, the function described seems to insist it be of type integer
> or type character and if the type was double or others might well fail as y
> would never be initialized.
>
> The goal seems to be to count how many "missing" values are found as in NA
> if a numeric type or an empty string if character.
>
> But you can have some form of NA in all kinds of object types including
> character as in this construct:
>
> > x <- c("a", NA, "", "b", "NA)")
> > x
> [1] "a"   NA    ""    "b"   "NA)"
>
> The above has three useless elements if both NA and "" are considered
> empty. So logically the condition could be to count NA and IF it is of type
> character, also count "".
>
> So rather than play games testing not just is.integer, is.double (or just
> is.numeric) as well as is.logical and is.raw, all the above can be tested
> with is.na() first to add up how many Na they contain. If then it is of
> type character, you can add any blank strings.
>
> So the algorithm would initialize y to sum(is.na(vec)) and then if the
> vec is character, add the sum of how many empty strings.
>
> Alternately, the function should deal with what it wants to do if any
> other type is encountered. You can internally converts many things to
> integer or character and then operate on them. Or you can return a zero or
> raise an alarm when given something else.
>
> In this case, simply setting y to zero before using it would make it
> defined and avoid the error, albeit report nothing found if it was a double
> or Boolean vector even if it did contain NA.
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of David Carlson
> via R-help
> Sent: Tuesday, August 9, 2022 11:33 AM
> To: Erin Hodgess <erinm.hodgess at gmail.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] Odd behavior of a function within apply
>
> Could you have columns that are not character or integer so that y is
> never defined in the function?
>
> count1a(1:5/3)
> Error in count1a(1:5/3) : object 'y' not found
>
> David Carlson
>
>
> On Mon, Aug 8, 2022 at 1:35 PM Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
>
> > OK.?? I'm back again.?? So my test1.??df is 236x390 If I put in the
> > following:?? lapply(test1.??df,count1a) Error in FUN(X[[i]],
> > .??.??.??) :?? object 'y' not found > lapply(test1.??df,count1a) Error
> > in FUN(X[[i]],
> > .??.??.??) :?? object 'y' not found > sapply(test1.??df,count1a)
> > ZjQcmQRYFpfptBannerStart This Message Is From an External Sender This
> > message came from outside your organization.
> >
> > ZjQcmQRYFpfptBannerEnd
> >
> > OK.  I'm back again.
> >
> > So my test1.df is 236x390
> >
> > If I put in the following:
> >  lapply(test1.df,count1a)
> > Error in FUN(X[[i]], ...) : object 'y' not found
> > > lapply(test1.df,count1a)
> > Error in FUN(X[[i]], ...) : object 'y' not found
> > > sapply(test1.df,count1a)
> > Error in FUN(X[[i]], ...) : object 'y' not found
> > >
> > What am I doing wrong, please?
> > Thanks,
> > Erin
> >
> >
> > Erin Hodgess, PhD
> > mailto: erinm.hodgess at gmail.com
> >
> >
> > On Mon, Aug 8, 2022 at 1:41 PM Erin Hodgess <erinm.hodgess at gmail.com>
> wrote:
> >
> > > Awesome, thanks so much!!
> > >
> > > Erin Hodgess, PhD
> > > mailto: erinm.hodgess at gmail.com
> > >
> > >
> > > On Mon, Aug 8, 2022 at 1:38 PM John Fox <jfox at mcmaster.ca> wrote:
> > >
> > >> Dear Erin,
> > >>
> > >> The problem is that the data frame gets coerced to a character
> > >> matrix, and the only column with "" entries is the 9th (the second
> > >> one you
> > >> supplied):
> > >>
> > >> as.matrix(test1.df)
> > >>     X1_1_HZP1 X1_1_HBM1_mon X1_1_HBM1_yr
> > >> 1  "48160"   "December"    "2014"
> > >> 2  "48198"   "June"        "2018"
> > >> 3  "80027"   "August"      "2016"
> > >> 4  "48161"   ""            NA
> > >> 5  NA        ""            NA
> > >> 6  "48911"   "August"      "1985"
> > >> 7  NA        "April"       "2019"
> > >> 8  "48197"   "February"    "1993"
> > >> 9  "48021"   ""            NA
> > >> 10 "11355"   "December"    "1990"
> > >>
> > >> (Here, test1.df only contains the three columns you provided.)
> > >>
> > >> A solution is to use sapply:
> > >>
> > >>  > sapply(test1.df, count1a)
> > >>      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
> > >>              2             3             3
> > >>
> > >>
> > >> I hope this helps,
> > >>   John
> > >>
> > >>
> > >> On 2022-08-08 1:22 p.m., Erin Hodgess wrote:
> > >> > Hello!
> > >> >
> > >> > I have the following data.frame
> > >> >   dput(test1.df[1:10,8:10])
> > >> > structure(list(X1_1_HZP1 = c(48160L, 48198L, 80027L, 48161L, NA,
> > >> > 48911L, NA, 48197L, 48021L, 11355L), X1_1_HBM1_mon =
> > >> > c("December", "June", "August", "", "", "August", "April",
> > >> > "February", "", "December"), X1_1_HBM1_yr = c(2014L, 2018L,
> > >> > 2016L, NA, NA, 1985L, 2019L, 1993L, NA, 1990L)), row.names =
> > >> > c(NA, 10L), class = "data.frame")
> > >> >
> > >> > And the following function:
> > >> >> dput(count1a)
> > >> > function (x)
> > >> > {
> > >> >      if (typeof(x) == "integer")
> > >> >          y <- sum(is.na(x))
> > >> >      if (typeof(x) == "character")
> > >> >          y <- sum(x == "")
> > >> >      return(y)
> > >> > }
> > >> > When I use the apply function with count1a, I get the following:
> > >> >   apply(test1.df[1:10,8:10],2,count1a)
> > >> >      X1_1_HZP1 X1_1_HBM1_mon  X1_1_HBM1_yr
> > >> >             NA             3            NA
> > >> > However, when I do use columns 8 and 10, I get the correct response:
> > >> >   apply(test1.df[1:10,c(8,10)],2,count1a)
> > >> >     X1_1_HZP1 X1_1_HBM1_yr
> > >> >             2            3
> > >> >>
> > >> > I am really baffled.  If I use count1a on a single column, it
> > >> > works
> > >> fine.
> > >> >
> > >> > Any suggestions much appreciated.
> > >> > Thanks,
> > >> > Sincerely,
> > >> > Erin
> > >> >
> > >> >
> > >> > Erin Hodgess, PhD
> > >> > mailto: erinm.hodgess at gmail.com
> > >> >
> > >> >       [[alternative HTML version deleted]]
> > >> >
> > >> > ______________________________________________
> > >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> > https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo
> > >> > /r-help__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kd
> > >> > RHAy8SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MBvSbrfE$
> > >> > PLEASE do read the posting guide
> > >> https://urldefense.com/v3/__http://www.R-project.org/posting-guide.
> > >> html__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8
> > >> SJJdx6Uq0p4rpBa4E3DkmQ65UImH48MdYOqruE$
> > >> > and provide commented, minimal, self-contained, reproducible code.
> > >> --
> > >> John Fox, Professor Emeritus
> > >> McMaster University
> > >> Hamilton, Ontario, Canada
> > >> web:
> > >> https://urldefense.com/v3/__https://socialsciences.mcmaster.ca/jfox
> > >> /__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJ
> > >> dx6Uq0p4rpBa4E3DkmQ65UImH48MRU4wu3o$
> > >>
> > >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________R-help at r-project.org
> > mailing list -- To UNSUBSCRIBE and more,
> > seehttps://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r
> > -help__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJ
> > Jdx6Uq0p4rpBa4E3DkmQ65UImH48MBvSbrfE$
> > PLEASE do read the posting guide
> > https://urldefense.com/v3/__http://www.R-project.org/posting-guide.htm
> > l__;!!KwNVnqRv!CHx9JKnbOObpAt0LltEogLSxDUEl9qJDI6FgqMJBG_kdRHAy8SJJdx6
> > Uq0p4rpBa4E3DkmQ65UImH48MdYOqruE$ and provide commented, minimal,
> > self-contained, reproducible code.
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Wed Aug 10 13:18:56 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Wed, 10 Aug 2022 07:18:56 -0400
Subject: [R] lattice question
Message-ID: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>


I want to merge two panels into one.  Is it possible to do this?

Thanks,
Naresh

library(lattice)
mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
long = runif(20, 2, 10), short = runif(20, -10, 0))

# This plots data in two panels.  I want all four lines in one panel. 
xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
"g"))

# This does not work
# No error in R session
# Graph window says: "Error using packet 1
# argument 'subscripts' is missing, with no default"
xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
panel = function(x, y, ..., subscripts) {
  panel.xyplot(x, y, ...)
  panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Wed Aug 10 15:37:03 2022
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Wed, 10 Aug 2022 19:07:03 +0530
Subject: [R] lattice question
In-Reply-To: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
Message-ID: <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>

On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
<naresh_gurbuxani at hotmail.com> wrote:
>
>
> I want to merge two panels into one.  Is it possible to do this?
>
> Thanks,
> Naresh
>
> library(lattice)
> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
> long = runif(20, 2, 10), short = runif(20, -10, 0))
>
> # This plots data in two panels.  I want all four lines in one panel.
> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
> "g"))

The "extended" formula API (with +) is really only meant as an
alternative to reshape() for simple cases. In your case, you probably
want something like

mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
"short")), v.names = "X", timevar = "G", times = c("long", "short"))

xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
= c("l", "g"))

-Deepayan

> # This does not work
> # No error in R session
> # Graph window says: "Error using packet 1
> # argument 'subscripts' is missing, with no default"
> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
> panel = function(x, y, ..., subscripts) {
>   panel.xyplot(x, y, ...)
>   panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Wed Aug 10 23:12:14 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Wed, 10 Aug 2022 21:12:14 +0000
Subject: [R] lattice question
In-Reply-To: <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
Message-ID: <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>

Deepayan,

Thanks for providing a solution.  While this is close to my goal, I want one more change.  The line type (lty) should be the same for long and short.  The line type should only change according to ?name? group.  So the the graph will have two line types (not four as in your solution).  

Is it possible?

Thanks,
Narrsh

Sent from my iPhone

> On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> 
> ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
> <naresh_gurbuxani at hotmail.com> wrote:
>> 
>> 
>> I want to merge two panels into one.  Is it possible to do this?
>> 
>> Thanks,
>> Naresh
>> 
>> library(lattice)
>> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
>> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
>> long = runif(20, 2, 10), short = runif(20, -10, 0))
>> 
>> # This plots data in two panels.  I want all four lines in one panel.
>> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
>> "g"))
> 
> The "extended" formula API (with +) is really only meant as an
> alternative to reshape() for simple cases. In your case, you probably
> want something like
> 
> mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
> 
> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
> = c("l", "g"))
> 
> -Deepayan
> 
>> # This does not work
>> # No error in R session
>> # Graph window says: "Error using packet 1
>> # argument 'subscripts' is missing, with no default"
>> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
>> panel = function(x, y, ..., subscripts) {
>>  panel.xyplot(x, y, ...)
>>  panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

From bgunter@4567 @end|ng |rom gm@||@com  Wed Aug 10 23:40:23 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 10 Aug 2022 14:40:23 -0700
Subject: [R] lattice question
In-Reply-To: <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
 <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
Message-ID: <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>

I assume you mean two line colors, not types.
Like this?

xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
       type = c("l", "g"), col.line = c("blue","red"))


Cheers,
Bert

On Wed, Aug 10, 2022 at 2:12 PM Naresh Gurbuxani
<naresh_gurbuxani at hotmail.com> wrote:
>
> Deepayan,
>
> Thanks for providing a solution.  While this is close to my goal, I want one more change.  The line type (lty) should be the same for long and short.  The line type should only change according to ?name? group.  So the the graph will have two line types (not four as in your solution).
>
> Is it possible?
>
> Thanks,
> Narrsh
>
> Sent from my iPhone
>
> > On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> >
> > ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
> > <naresh_gurbuxani at hotmail.com> wrote:
> >>
> >>
> >> I want to merge two panels into one.  Is it possible to do this?
> >>
> >> Thanks,
> >> Naresh
> >>
> >> library(lattice)
> >> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
> >> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
> >> long = runif(20, 2, 10), short = runif(20, -10, 0))
> >>
> >> # This plots data in two panels.  I want all four lines in one panel.
> >> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
> >> "g"))
> >
> > The "extended" formula API (with +) is really only meant as an
> > alternative to reshape() for simple cases. In your case, you probably
> > want something like
> >
> > mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
> > "short")), v.names = "X", timevar = "G", times = c("long", "short"))
> >
> > xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
> > = c("l", "g"))
> >
> > -Deepayan
> >
> >> # This does not work
> >> # No error in R session
> >> # Graph window says: "Error using packet 1
> >> # argument 'subscripts' is missing, with no default"
> >> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
> >> panel = function(x, y, ..., subscripts) {
> >>  panel.xyplot(x, y, ...)
> >>  panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Thu Aug 11 01:57:11 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Wed, 10 Aug 2022 23:57:11 +0000
Subject: [R] lattice question
In-Reply-To: <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
 <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>
Message-ID: <BL0PR01MB4036724D69DF755E571D1DD7FA659@BL0PR01MB4036.prod.exchangelabs.com>

Actually I meant types (e.g., lty = 1:2).  But colors would also work.  But I do not want to specify these in call to xyplot().

In my actual problem, there are more than two groups.  Moreover, the number of groups changes from case to case.  In my set up, I use trellis.par.set() to set line styles, colors, pch, etc.  

I would the call to xyplot() to automatically use the set options.

Thanks,
Naresh

Sent from my iPhone

> On Aug 10, 2022, at 5:40 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> ?I assume you mean two line colors, not types.
> Like this?
> 
> xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
>       type = c("l", "g"), col.line = c("blue","red"))
> 
> 
> Cheers,
> Bert
> 
>> On Wed, Aug 10, 2022 at 2:12 PM Naresh Gurbuxani
>> <naresh_gurbuxani at hotmail.com> wrote:
>> 
>> Deepayan,
>> 
>> Thanks for providing a solution.  While this is close to my goal, I want one more change.  The line type (lty) should be the same for long and short.  The line type should only change according to ?name? group.  So the the graph will have two line types (not four as in your solution).
>> 
>> Is it possible?
>> 
>> Thanks,
>> Narrsh
>> 
>> Sent from my iPhone
>> 
>>>> On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
>>> 
>>> ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
>>> <naresh_gurbuxani at hotmail.com> wrote:
>>>> 
>>>> 
>>>> I want to merge two panels into one.  Is it possible to do this?
>>>> 
>>>> Thanks,
>>>> Naresh
>>>> 
>>>> library(lattice)
>>>> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
>>>> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
>>>> long = runif(20, 2, 10), short = runif(20, -10, 0))
>>>> 
>>>> # This plots data in two panels.  I want all four lines in one panel.
>>>> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
>>>> "g"))
>>> 
>>> The "extended" formula API (with +) is really only meant as an
>>> alternative to reshape() for simple cases. In your case, you probably
>>> want something like
>>> 
>>> mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
>>> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
>>> 
>>> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
>>> = c("l", "g"))
>>> 
>>> -Deepayan
>>> 
>>>> # This does not work
>>>> # No error in R session
>>>> # Graph window says: "Error using packet 1
>>>> # argument 'subscripts' is missing, with no default"
>>>> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
>>>> panel = function(x, y, ..., subscripts) {
>>>> panel.xyplot(x, y, ...)
>>>> panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.

From bgunter@4567 @end|ng |rom gm@||@com  Thu Aug 11 04:02:16 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 10 Aug 2022 19:02:16 -0700
Subject: [R] lattice question
In-Reply-To: <BL0PR01MB4036724D69DF755E571D1DD7FA659@BL0PR01MB4036.prod.exchangelabs.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
 <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>
 <BL0PR01MB4036724D69DF755E571D1DD7FA659@BL0PR01MB4036.prod.exchangelabs.com>
Message-ID: <CAGxFJbSkuspyGR1-f=aED+WnR0bEVfxLV_C-pfkA8Xj4yfM9_w@mail.gmail.com>

It is unclear to me how you wish to define and distinguish groups.
Assuming you wish to have separate lines for the interaction as
Deepayan showed, but want the colors (or line types or both) to differ
only by by the "name" factor, then is this what you want?

trellis.par.set(superpose.line = list(lty = 1:6)) ## or other line
types you create
u.names <- unique(mydf.long$name)
xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
       type = c("l","g"),
       col.line = trellis.par.get("superpose.line")$col[
          seq_along(u.names)],
       lty =  trellis.par.get("superpose.line")$lty[
          seq_along(u.names)]
)

Notes:
1. If this is not what you want, I give up. Others may have better insight.
2. If this is what you want, Deepayan may be able to provide you a
nicer way to do it.
3. If you have more different names than 6 or 7, then you may have to
add more line types or colors to the superpose.line settings. Though I
would think the plot would be pretty much a mess, if so.

Bert


On Wed, Aug 10, 2022 at 4:57 PM Naresh Gurbuxani
<naresh_gurbuxani at hotmail.com> wrote:
>
> Actually I meant types (e.g., lty = 1:2).  But colors would also work.  But I do not want to specify these in call to xyplot().
>
> In my actual problem, there are more than two groups.  Moreover, the number of groups changes from case to case.  In my set up, I use trellis.par.set() to set line styles, colors, pch, etc.
>
> I would the call to xyplot() to automatically use the set options.
>
> Thanks,
> Naresh
>
> Sent from my iPhone
>
> > On Aug 10, 2022, at 5:40 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >
> > ?I assume you mean two line colors, not types.
> > Like this?
> >
> > xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
> >       type = c("l", "g"), col.line = c("blue","red"))
> >
> >
> > Cheers,
> > Bert
> >
> >> On Wed, Aug 10, 2022 at 2:12 PM Naresh Gurbuxani
> >> <naresh_gurbuxani at hotmail.com> wrote:
> >>
> >> Deepayan,
> >>
> >> Thanks for providing a solution.  While this is close to my goal, I want one more change.  The line type (lty) should be the same for long and short.  The line type should only change according to ?name? group.  So the the graph will have two line types (not four as in your solution).
> >>
> >> Is it possible?
> >>
> >> Thanks,
> >> Narrsh
> >>
> >> Sent from my iPhone
> >>
> >>>> On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> >>>
> >>> ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
> >>> <naresh_gurbuxani at hotmail.com> wrote:
> >>>>
> >>>>
> >>>> I want to merge two panels into one.  Is it possible to do this?
> >>>>
> >>>> Thanks,
> >>>> Naresh
> >>>>
> >>>> library(lattice)
> >>>> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
> >>>> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
> >>>> long = runif(20, 2, 10), short = runif(20, -10, 0))
> >>>>
> >>>> # This plots data in two panels.  I want all four lines in one panel.
> >>>> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
> >>>> "g"))
> >>>
> >>> The "extended" formula API (with +) is really only meant as an
> >>> alternative to reshape() for simple cases. In your case, you probably
> >>> want something like
> >>>
> >>> mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
> >>> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
> >>>
> >>> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
> >>> = c("l", "g"))
> >>>
> >>> -Deepayan
> >>>
> >>>> # This does not work
> >>>> # No error in R session
> >>>> # Graph window says: "Error using packet 1
> >>>> # argument 'subscripts' is missing, with no default"
> >>>> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
> >>>> panel = function(x, y, ..., subscripts) {
> >>>> panel.xyplot(x, y, ...)
> >>>> panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.


From o1b|gtenor @end|ng |rom gm@||@com  Wed Aug 10 18:26:59 2022
From: o1b|gtenor @end|ng |rom gm@||@com (o1bigtenor)
Date: Wed, 10 Aug 2022 11:26:59 -0500
Subject: [R] lattice question
In-Reply-To: <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
Message-ID: <CAPpdf59NnXcLKsFRT+bi5vE0bfU=ntc62rUJJqmJETWo2n8M9Q@mail.gmail.com>

On Wed, Aug 10, 2022 at 8:38 AM Deepayan Sarkar
<deepayan.sarkar at gmail.com> wrote:
>
> On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
> <naresh_gurbuxani at hotmail.com> wrote:
> >
> >
> > I want to merge two panels into one.  Is it possible to do this?
> >
> > Thanks,
> > Naresh
> >
> > library(lattice)
> > mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
> > length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
> > long = runif(20, 2, 10), short = runif(20, -10, 0))
> >
> > # This plots data in two panels.  I want all four lines in one panel.
> > xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
> > "g"))
>
> The "extended" formula API (with +) is really only meant as an
> alternative to reshape() for simple cases. In your case, you probably
> want something like
>
> mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
>
> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
> = c("l", "g"))
>

(In the interest of learning- - ) Why?

Regards


From n|ckmwr@y @end|ng |rom gm@||@com  Thu Aug 11 13:42:26 2022
From: n|ckmwr@y @end|ng |rom gm@||@com (Nick Wray)
Date: Thu, 11 Aug 2022 12:42:26 +0100
Subject: [R] lfstat package
Message-ID: <CABxY9BOruMUfw4+5Gc5saNhUOZRFqBEAWZXTY220Dwt6nqASfA@mail.gmail.com>

Hello - does anyone know whether the lfstat package is still running and if
so, whence it can be downloaded and installed, as it doesn't seem to be
available from CRAN.  I'm trying to get the function dmcurve() for
hydrological double mass curves - perhaps that's now elsewhere although i
can't find it anywhere but in lfstat

thanks Nick Wray

	[[alternative HTML version deleted]]


From co|or|e @end|ng |rom gm@||@com  Thu Aug 11 14:03:06 2022
From: co|or|e @end|ng |rom gm@||@com (Carlos Ortega)
Date: Thu, 11 Aug 2022 14:03:06 +0200
Subject: [R] lfstat package
In-Reply-To: <CABxY9BOruMUfw4+5Gc5saNhUOZRFqBEAWZXTY220Dwt6nqASfA@mail.gmail.com>
References: <CABxY9BOruMUfw4+5Gc5saNhUOZRFqBEAWZXTY220Dwt6nqASfA@mail.gmail.com>
Message-ID: <CAFKNbk+zVS3jQ4JejBwzh1L=9Wr02pwsQkq0LQnXH38HiYEC7Q@mail.gmail.com>

Hi,

Yes, it is not available in CRAN.
You can find packages that have been removed from CRAN here:

   - https://cran.r-project.org/src/contrib/Archive

And in particular "lfstat" is here, with all its historical releases (in
tar.gz format).
You can download from there and install, the version you need.


   - https://cran.r-project.org/src/contrib/Archive/lfstat/


Thanks,
Carlos Ortega.


On Thu, Aug 11, 2022 at 1:42 PM Nick Wray <nickmwray at gmail.com> wrote:

> Hello - does anyone know whether the lfstat package is still running and if
> so, whence it can be downloaded and installed, as it doesn't seem to be
> available from CRAN.  I'm trying to get the function dmcurve() for
> hydrological double mass curves - perhaps that's now elsewhere although i
> can't find it anywhere but in lfstat
>
> thanks Nick Wray
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Thu Aug 11 17:33:38 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Thu, 11 Aug 2022 15:33:38 +0000
Subject: [R] lattice question
In-Reply-To: <CAGxFJbSkuspyGR1-f=aED+WnR0bEVfxLV_C-pfkA8Xj4yfM9_w@mail.gmail.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
 <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>
 <BL0PR01MB4036724D69DF755E571D1DD7FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbSkuspyGR1-f=aED+WnR0bEVfxLV_C-pfkA8Xj4yfM9_w@mail.gmail.com>
Message-ID: <BL0PR01MB403675492C3C426B8607BF8BFA649@BL0PR01MB4036.prod.exchangelabs.com>

Bert,

Thanks for providing this solution.  It produces the desired graph.

To see how I want to distinguish groups, you should look at original data (mydf).  There are two groups (Aa and Bb), each with two time series (long and short).  Long is always positive.  Short is always negative.  Therefore, there is no need to distinguish between long and short.  I only need to distinguish between Aa and Bb.  

I agree that more than six lines in the graph will make it cluttered.  In fact the above exercise is to avoid clutter in the key.  No need to show Aa.long and Aa.short, because long and short are obvious.

Thanks,
Naresh

Sent from my iPhone

> On Aug 10, 2022, at 10:02 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> ?It is unclear to me how you wish to define and distinguish groups.
> Assuming you wish to have separate lines for the interaction as
> Deepayan showed, but want the colors (or line types or both) to differ
> only by by the "name" factor, then is this what you want?
> 
> trellis.par.set(superpose.line = list(lty = 1:6)) ## or other line
> types you create
> u.names <- unique(mydf.long$name)
> xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
>       type = c("l","g"),
>       col.line = trellis.par.get("superpose.line")$col[
>          seq_along(u.names)],
>       lty =  trellis.par.get("superpose.line")$lty[
>          seq_along(u.names)]
> )
> 
> Notes:
> 1. If this is not what you want, I give up. Others may have better insight.
> 2. If this is what you want, Deepayan may be able to provide you a
> nicer way to do it.
> 3. If you have more different names than 6 or 7, then you may have to
> add more line types or colors to the superpose.line settings. Though I
> would think the plot would be pretty much a mess, if so.
> 
> Bert
> 
> 
>> On Wed, Aug 10, 2022 at 4:57 PM Naresh Gurbuxani
>> <naresh_gurbuxani at hotmail.com> wrote:
>> 
>> Actually I meant types (e.g., lty = 1:2).  But colors would also work.  But I do not want to specify these in call to xyplot().
>> 
>> In my actual problem, there are more than two groups.  Moreover, the number of groups changes from case to case.  In my set up, I use trellis.par.set() to set line styles, colors, pch, etc.
>> 
>> I would the call to xyplot() to automatically use the set options.
>> 
>> Thanks,
>> Naresh
>> 
>> Sent from my iPhone
>> 
>>>> On Aug 10, 2022, at 5:40 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>> 
>>> ?I assume you mean two line colors, not types.
>>> Like this?
>>> 
>>> xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
>>>      type = c("l", "g"), col.line = c("blue","red"))
>>> 
>>> 
>>> Cheers,
>>> Bert
>>> 
>>>> On Wed, Aug 10, 2022 at 2:12 PM Naresh Gurbuxani
>>>> <naresh_gurbuxani at hotmail.com> wrote:
>>>> 
>>>> Deepayan,
>>>> 
>>>> Thanks for providing a solution.  While this is close to my goal, I want one more change.  The line type (lty) should be the same for long and short.  The line type should only change according to ?name? group.  So the the graph will have two line types (not four as in your solution).
>>>> 
>>>> Is it possible?
>>>> 
>>>> Thanks,
>>>> Narrsh
>>>> 
>>>> Sent from my iPhone
>>>> 
>>>>>> On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
>>>>> 
>>>>> ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
>>>>> <naresh_gurbuxani at hotmail.com> wrote:
>>>>>> 
>>>>>> 
>>>>>> I want to merge two panels into one.  Is it possible to do this?
>>>>>> 
>>>>>> Thanks,
>>>>>> Naresh
>>>>>> 
>>>>>> library(lattice)
>>>>>> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
>>>>>> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
>>>>>> long = runif(20, 2, 10), short = runif(20, -10, 0))
>>>>>> 
>>>>>> # This plots data in two panels.  I want all four lines in one panel.
>>>>>> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
>>>>>> "g"))
>>>>> 
>>>>> The "extended" formula API (with +) is really only meant as an
>>>>> alternative to reshape() for simple cases. In your case, you probably
>>>>> want something like
>>>>> 
>>>>> mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
>>>>> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
>>>>> 
>>>>> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
>>>>> = c("l", "g"))
>>>>> 
>>>>> -Deepayan
>>>>> 
>>>>>> # This does not work
>>>>>> # No error in R session
>>>>>> # Graph window says: "Error using packet 1
>>>>>> # argument 'subscripts' is missing, with no default"
>>>>>> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
>>>>>> panel = function(x, y, ..., subscripts) {
>>>>>> panel.xyplot(x, y, ...)
>>>>>> panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.

From bogu@@chr|@to|er @end|ng |rom gm@||@com  Thu Aug 11 16:50:09 2022
From: bogu@@chr|@to|er @end|ng |rom gm@||@com (bogus christofer)
Date: Thu, 11 Aug 2022 20:20:09 +0530
Subject: [R] Fitted values from AR model
Message-ID: <CAJKTqP7tOuF7TUiiuMpbRQOVwRCXsZVPXe9R7bQhG2O-Wj6HVg@mail.gmail.com>

Hi,

I have below AR model and fitted values from the forecast package

library(forecast)
dta = c(5.0, 11, 16, 23, 36, 58, 29, 20, 10, 8, 3, 0, 0, 2, 11, 27, 47, 63,
60, 39)
fit <- arima(dta, order=c(2,0,0))
fitted(fit)

This gives fitted values as

Time Series:
Start = 1
End = 20
Frequency = 1
 [1] 13.461017  9.073427 18.022166 20.689420 26.352282 38.165635 57.502926
9.812106 15.335303  8.298995 11.543320  6.606999  5.800820  7.502621
9.930962 19.723966 34.045298 49.252447 57.333846 44.615067


However when I compare this result with Python, I see significant
difference particularly in the first few values as below

from statsmodels.tsa.arima.model import ARIMA
dta = [5.0, 11, 16, 23, 36, 58, 29, 20, 10, 8, 3, 0, 0, 2, 11, 27, 47, 63,
60, 39]
fit = ARIMA(dta, order=(2, 0, 0)).fit()
fit.predict()

array([21.24816788, 8.66048306, 18.02197059, 20.68931006,
26.35225759,38.16574655, 57.503278 , 9.81253693, 15.33539514,
8.29894655,11.54316056, 6.60679489, 5.80055038, 7.50232004,
9.93067155,19.72374025, 34.04524337, 49.25265365, 57.3343347 , 44.6157026 ])

Any idea why there are such difference between R and Python results will be
very helpful.

Thanks,

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Fri Aug 12 10:13:40 2022
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Fri, 12 Aug 2022 11:13:40 +0300
Subject: [R] Fitted values from AR model
In-Reply-To: <CAJKTqP7tOuF7TUiiuMpbRQOVwRCXsZVPXe9R7bQhG2O-Wj6HVg@mail.gmail.com>
References: <CAJKTqP7tOuF7TUiiuMpbRQOVwRCXsZVPXe9R7bQhG2O-Wj6HVg@mail.gmail.com>
Message-ID: <CAGgJW74rxrDzwL2CqKy7c-YmdakRqkn8_1Jm=88_Skd_vZX+JA@mail.gmail.com>

The model that you are fitting to the data is an AR(2) model, which means

y(t) = a0 + a1 * y(t-1) + a2 * y(t-2) + eps(t)                 (1)

The fitting procedure estimates the coefficients a0, a1, a2 (and the
variance of eps).

After the coefficients have been estimated, the fitted values can be
calculated using equation (1) (setting eps(t) = 0)
using
fitted(t) = a0 + a1 * y(t-1) + a2 * y(t-2)

Assuming the series is given for t=1,2,...,20, there is no problem to
apply equation (1) to get the
fitted values for t=3,4,...,20. But there is a problem for t=1 and
t=2. For example, for t=1, equation (1)
says

fitted(1) = a0 + a1 * y(0) + a2 * y(-1)

But there are no values given for y(0) and y(-1). So, either no fitted
values should be given for t=1,2, or some
other method is being used. Apparently, the arima functions in R and
python use different methodology
to generate these two fitted points. (For all the other values, the
fits are extremely close.)

HTH,
Eric

On Thu, Aug 11, 2022 at 9:53 PM bogus christofer
<bogus.christofer at gmail.com> wrote:
>
> Hi,
>
> I have below AR model and fitted values from the forecast package
>
> library(forecast)
> dta = c(5.0, 11, 16, 23, 36, 58, 29, 20, 10, 8, 3, 0, 0, 2, 11, 27, 47, 63,
> 60, 39)
> fit <- arima(dta, order=c(2,0,0))
> fitted(fit)
>
> This gives fitted values as
>
> Time Series:
> Start = 1
> End = 20
> Frequency = 1
>  [1] 13.461017  9.073427 18.022166 20.689420 26.352282 38.165635 57.502926
> 9.812106 15.335303  8.298995 11.543320  6.606999  5.800820  7.502621
> 9.930962 19.723966 34.045298 49.252447 57.333846 44.615067
>
>
> However when I compare this result with Python, I see significant
> difference particularly in the first few values as below
>
> from statsmodels.tsa.arima.model import ARIMA
> dta = [5.0, 11, 16, 23, 36, 58, 29, 20, 10, 8, 3, 0, 0, 2, 11, 27, 47, 63,
> 60, 39]
> fit = ARIMA(dta, order=(2, 0, 0)).fit()
> fit.predict()
>
> array([21.24816788, 8.66048306, 18.02197059, 20.68931006,
> 26.35225759,38.16574655, 57.503278 , 9.81253693, 15.33539514,
> 8.29894655,11.54316056, 6.60679489, 5.80055038, 7.50232004,
> 9.93067155,19.72374025, 34.04524337, 49.25265365, 57.3343347 , 44.6157026 ])
>
> Any idea why there are such difference between R and Python results will be
> very helpful.
>
> Thanks,
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Fri Aug 12 14:01:51 2022
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Fri, 12 Aug 2022 17:31:51 +0530
Subject: [R] lattice question
In-Reply-To: <BL0PR01MB403675492C3C426B8607BF8BFA649@BL0PR01MB4036.prod.exchangelabs.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
 <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>
 <BL0PR01MB4036724D69DF755E571D1DD7FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbSkuspyGR1-f=aED+WnR0bEVfxLV_C-pfkA8Xj4yfM9_w@mail.gmail.com>
 <BL0PR01MB403675492C3C426B8607BF8BFA649@BL0PR01MB4036.prod.exchangelabs.com>
Message-ID: <CADfFDC5qH_e1dh96pfRAViwt5GEu_shgUh=Fj-2eLV6hZsySDg@mail.gmail.com>

On Thu, Aug 11, 2022 at 9:03 PM Naresh Gurbuxani <
naresh_gurbuxani at hotmail.com> wrote:

> Bert,
>
> Thanks for providing this solution.  It produces the desired graph.
>
> To see how I want to distinguish groups, you should look at original data
> (mydf).  There are two groups (Aa and Bb), each with two time series (long
> and short).  Long is always positive.  Short is always negative.
> Therefore, there is no need to distinguish between long and short.  I only
> need to distinguish between Aa and Bb.
>
> I agree that more than six lines in the graph will make it cluttered.  In
> fact the above exercise is to avoid clutter in the key.  No need to show
> Aa.long and Aa.short, because long and short are obvious.
>

In that case, this alternative approach may be conceptually simpler:

library(latticeExtra)

r <- with(mydf, extendrange(range(long, short)))

plong <- xyplot(long ~ date, groups = name, data = mydf,
                type = "l", grid = TRUE,
                ylim = r)

pshort <- xyplot(short ~ date, groups = name, data = mydf, type = "l")

plong + pshort

The first plot here is the "main" one, so you need to make sure that its
'ylim' is big enough for all the data.

-Deepayan



>
> Thanks,
> Naresh
>
> Sent from my iPhone
>
> > On Aug 10, 2022, at 10:02 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >
> > ?It is unclear to me how you wish to define and distinguish groups.
> > Assuming you wish to have separate lines for the interaction as
> > Deepayan showed, but want the colors (or line types or both) to differ
> > only by by the "name" factor, then is this what you want?
> >
> > trellis.par.set(superpose.line = list(lty = 1:6)) ## or other line
> > types you create
> > u.names <- unique(mydf.long$name)
> > xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
> >       type = c("l","g"),
> >       col.line = trellis.par.get("superpose.line")$col[
> >          seq_along(u.names)],
> >       lty =  trellis.par.get("superpose.line")$lty[
> >          seq_along(u.names)]
> > )
> >
> > Notes:
> > 1. If this is not what you want, I give up. Others may have better
> insight.
> > 2. If this is what you want, Deepayan may be able to provide you a
> > nicer way to do it.
> > 3. If you have more different names than 6 or 7, then you may have to
> > add more line types or colors to the superpose.line settings. Though I
> > would think the plot would be pretty much a mess, if so.
> >
> > Bert
> >
> >
> >> On Wed, Aug 10, 2022 at 4:57 PM Naresh Gurbuxani
> >> <naresh_gurbuxani at hotmail.com> wrote:
> >>
> >> Actually I meant types (e.g., lty = 1:2).  But colors would also work.
> But I do not want to specify these in call to xyplot().
> >>
> >> In my actual problem, there are more than two groups.  Moreover, the
> number of groups changes from case to case.  In my set up, I use
> trellis.par.set() to set line styles, colors, pch, etc.
> >>
> >> I would the call to xyplot() to automatically use the set options.
> >>
> >> Thanks,
> >> Naresh
> >>
> >> Sent from my iPhone
> >>
> >>>> On Aug 10, 2022, at 5:40 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >>>
> >>> ?I assume you mean two line colors, not types.
> >>> Like this?
> >>>
> >>> xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
> >>>      type = c("l", "g"), col.line = c("blue","red"))
> >>>
> >>>
> >>> Cheers,
> >>> Bert
> >>>
> >>>> On Wed, Aug 10, 2022 at 2:12 PM Naresh Gurbuxani
> >>>> <naresh_gurbuxani at hotmail.com> wrote:
> >>>>
> >>>> Deepayan,
> >>>>
> >>>> Thanks for providing a solution.  While this is close to my goal, I
> want one more change.  The line type (lty) should be the same for long and
> short.  The line type should only change according to ?name? group.  So the
> the graph will have two line types (not four as in your solution).
> >>>>
> >>>> Is it possible?
> >>>>
> >>>> Thanks,
> >>>> Narrsh
> >>>>
> >>>> Sent from my iPhone
> >>>>
> >>>>>> On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <
> deepayan.sarkar at gmail.com> wrote:
> >>>>>
> >>>>> ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
> >>>>> <naresh_gurbuxani at hotmail.com> wrote:
> >>>>>>
> >>>>>>
> >>>>>> I want to merge two panels into one.  Is it possible to do this?
> >>>>>>
> >>>>>> Thanks,
> >>>>>> Naresh
> >>>>>>
> >>>>>> library(lattice)
> >>>>>> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by =
> 1,
> >>>>>> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
> >>>>>> long = runif(20, 2, 10), short = runif(20, -10, 0))
> >>>>>>
> >>>>>> # This plots data in two panels.  I want all four lines in one
> panel.
> >>>>>> xyplot(long + short ~ date, groups = name, data = mydf, type =
> c("l",
> >>>>>> "g"))
> >>>>>
> >>>>> The "extended" formula API (with +) is really only meant as an
> >>>>> alternative to reshape() for simple cases. In your case, you probably
> >>>>> want something like
> >>>>>
> >>>>> mydf.long = reshape(mydf, direction = "long", varying =
> list(c("long",
> >>>>> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
> >>>>>
> >>>>> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long,
> type
> >>>>> = c("l", "g"))
> >>>>>
> >>>>> -Deepayan
> >>>>>
> >>>>>> # This does not work
> >>>>>> # No error in R session
> >>>>>> # Graph window says: "Error using packet 1
> >>>>>> # argument 'subscripts' is missing, with no default"
> >>>>>> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
> >>>>>> panel = function(x, y, ..., subscripts) {
> >>>>>> panel.xyplot(x, y, ...)
> >>>>>> panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From n@re@h_gurbux@n| @end|ng |rom hotm@||@com  Fri Aug 12 21:35:57 2022
From: n@re@h_gurbux@n| @end|ng |rom hotm@||@com (Naresh Gurbuxani)
Date: Fri, 12 Aug 2022 19:35:57 +0000
Subject: [R] lattice question
In-Reply-To: <CADfFDC5qH_e1dh96pfRAViwt5GEu_shgUh=Fj-2eLV6hZsySDg@mail.gmail.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
 <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>
 <BL0PR01MB4036724D69DF755E571D1DD7FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbSkuspyGR1-f=aED+WnR0bEVfxLV_C-pfkA8Xj4yfM9_w@mail.gmail.com>
 <BL0PR01MB403675492C3C426B8607BF8BFA649@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC5qH_e1dh96pfRAViwt5GEu_shgUh=Fj-2eLV6hZsySDg@mail.gmail.com>
Message-ID: <BL0PR01MB4036DA256D99B5B10B73E3E6FA679@BL0PR01MB4036.prod.exchangelabs.com>

This is the solution I was looking for.  Thanks to Deepayan and Bert for sticking with me.

Naresh

Sent from my iPhone

On Aug 12, 2022, at 8:02 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:

?


On Thu, Aug 11, 2022 at 9:03 PM Naresh Gurbuxani <naresh_gurbuxani at hotmail.com<mailto:naresh_gurbuxani at hotmail.com>> wrote:
Bert,

Thanks for providing this solution.  It produces the desired graph.

To see how I want to distinguish groups, you should look at original data (mydf).  There are two groups (Aa and Bb), each with two time series (long and short).  Long is always positive.  Short is always negative.  Therefore, there is no need to distinguish between long and short.  I only need to distinguish between Aa and Bb.

I agree that more than six lines in the graph will make it cluttered.  In fact the above exercise is to avoid clutter in the key.  No need to show Aa.long and Aa.short, because long and short are obvious.

In that case, this alternative approach may be conceptually simpler:

library(latticeExtra)

r <- with(mydf, extendrange(range(long, short)))

plong <- xyplot(long ~ date, groups = name, data = mydf,
                type = "l", grid = TRUE,
                ylim = r)

pshort <- xyplot(short ~ date, groups = name, data = mydf, type = "l")

plong + pshort

The first plot here is the "main" one, so you need to make sure that its 'ylim' is big enough for all the data.

-Deepayan



Thanks,
Naresh

Sent from my iPhone

> On Aug 10, 2022, at 10:02 PM, Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>> wrote:
>
> ?It is unclear to me how you wish to define and distinguish groups.
> Assuming you wish to have separate lines for the interaction as
> Deepayan showed, but want the colors (or line types or both) to differ
> only by by the "name" factor, then is this what you want?
>
> trellis.par.set(superpose.line = list(lty = 1:6)) ## or other line
> types you create
> u.names <- unique(mydf.long$name)
> xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
>       type = c("l","g"),
>       col.line = trellis.par.get("superpose.line")$col[
>          seq_along(u.names)],
>       lty =  trellis.par.get("superpose.line")$lty[
>          seq_along(u.names)]
> )
>
> Notes:
> 1. If this is not what you want, I give up. Others may have better insight.
> 2. If this is what you want, Deepayan may be able to provide you a
> nicer way to do it.
> 3. If you have more different names than 6 or 7, then you may have to
> add more line types or colors to the superpose.line settings. Though I
> would think the plot would be pretty much a mess, if so.
>
> Bert
>
>
>> On Wed, Aug 10, 2022 at 4:57 PM Naresh Gurbuxani
>> <naresh_gurbuxani at hotmail.com<mailto:naresh_gurbuxani at hotmail.com>> wrote:
>>
>> Actually I meant types (e.g., lty = 1:2).  But colors would also work.  But I do not want to specify these in call to xyplot().
>>
>> In my actual problem, there are more than two groups.  Moreover, the number of groups changes from case to case.  In my set up, I use trellis.par.set() to set line styles, colors, pch, etc.
>>
>> I would the call to xyplot() to automatically use the set options.
>>
>> Thanks,
>> Naresh
>>
>> Sent from my iPhone
>>
>>>> On Aug 10, 2022, at 5:40 PM, Bert Gunter <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>> wrote:
>>>
>>> ?I assume you mean two line colors, not types.
>>> Like this?
>>>
>>> xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
>>>      type = c("l", "g"), col.line = c("blue","red"))
>>>
>>>
>>> Cheers,
>>> Bert
>>>
>>>> On Wed, Aug 10, 2022 at 2:12 PM Naresh Gurbuxani
>>>> <naresh_gurbuxani at hotmail.com<mailto:naresh_gurbuxani at hotmail.com>> wrote:
>>>>
>>>> Deepayan,
>>>>
>>>> Thanks for providing a solution.  While this is close to my goal, I want one more change.  The line type (lty) should be the same for long and short.  The line type should only change according to ?name? group.  So the the graph will have two line types (not four as in your solution).
>>>>
>>>> Is it possible?
>>>>
>>>> Thanks,
>>>> Narrsh
>>>>
>>>> Sent from my iPhone
>>>>
>>>>>> On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com<mailto:deepayan.sarkar at gmail.com>> wrote:
>>>>>
>>>>> ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
>>>>> <naresh_gurbuxani at hotmail.com<mailto:naresh_gurbuxani at hotmail.com>> wrote:
>>>>>>
>>>>>>
>>>>>> I want to merge two panels into one.  Is it possible to do this?
>>>>>>
>>>>>> Thanks,
>>>>>> Naresh
>>>>>>
>>>>>> library(lattice)
>>>>>> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
>>>>>> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
>>>>>> long = runif(20, 2, 10), short = runif(20, -10, 0))
>>>>>>
>>>>>> # This plots data in two panels.  I want all four lines in one panel.
>>>>>> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
>>>>>> "g"))
>>>>>
>>>>> The "extended" formula API (with +) is really only meant as an
>>>>> alternative to reshape() for simple cases. In your case, you probably
>>>>> want something like
>>>>>
>>>>> mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
>>>>> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
>>>>>
>>>>> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
>>>>> = c("l", "g"))
>>>>>
>>>>> -Deepayan
>>>>>
>>>>>> # This does not work
>>>>>> # No error in R session
>>>>>> # Graph window says: "Error using packet 1
>>>>>> # argument 'subscripts' is missing, with no default"
>>>>>> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
>>>>>> panel = function(x, y, ..., subscripts) {
>>>>>> panel.xyplot(x, y, ...)
>>>>>> panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> ______________________________________________
>>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri Aug 12 23:29:14 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 12 Aug 2022 14:29:14 -0700
Subject: [R] lattice question
In-Reply-To: <BL0PR01MB4036DA256D99B5B10B73E3E6FA679@BL0PR01MB4036.prod.exchangelabs.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
 <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>
 <BL0PR01MB4036724D69DF755E571D1DD7FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbSkuspyGR1-f=aED+WnR0bEVfxLV_C-pfkA8Xj4yfM9_w@mail.gmail.com>
 <BL0PR01MB403675492C3C426B8607BF8BFA649@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC5qH_e1dh96pfRAViwt5GEu_shgUh=Fj-2eLV6hZsySDg@mail.gmail.com>
 <BL0PR01MB4036DA256D99B5B10B73E3E6FA679@BL0PR01MB4036.prod.exchangelabs.com>
Message-ID: <CAGxFJbRHTpPEa-0KN+f3Qh264=-+SvVgA9QH80Thf9A8ycG7oQ@mail.gmail.com>

As a private coda -- as it is unlikely to be of general interest --
note that it is easy to do this without resorting to the layering
paradigm through the use of the appropriate panel function and
lattice's argument passing protocol (although I stand in awe of Felix
Andrews's clever code!). Here's a version that uses R's underlying
with() function to control the nonstandard evaluation. I've added a
key and y axis label just to be cute:

with(mydf,xyplot(short ~ date, group = name,
type = c("l","g"),
       y2 =long,
       ylim = extendrange(c(short,long)),
       auto.key = list(lines=TRUE, points = FALSE, space = "right"),
       ylab = "Name",
       panel= function(x,y, y2,...){
          panel.superpose(x,y,...)
          panel.superpose(x,y2,...)
       }))

On Fri, Aug 12, 2022 at 12:35 PM Naresh Gurbuxani
<naresh_gurbuxani at hotmail.com> wrote:
>
> This is the solution I was looking for.  Thanks to Deepayan and Bert for sticking with me.
>
> Naresh
>
> Sent from my iPhone
>
> On Aug 12, 2022, at 8:02 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
>
> ?
>
>
> On Thu, Aug 11, 2022 at 9:03 PM Naresh Gurbuxani <naresh_gurbuxani at hotmail.com> wrote:
>>
>> Bert,
>>
>> Thanks for providing this solution.  It produces the desired graph.
>>
>> To see how I want to distinguish groups, you should look at original data (mydf).  There are two groups (Aa and Bb), each with two time series (long and short).  Long is always positive.  Short is always negative.  Therefore, there is no need to distinguish between long and short.  I only need to distinguish between Aa and Bb.
>>
>> I agree that more than six lines in the graph will make it cluttered.  In fact the above exercise is to avoid clutter in the key.  No need to show Aa.long and Aa.short, because long and short are obvious.
>
>
> In that case, this alternative approach may be conceptually simpler:
>
> library(latticeExtra)
>
> r <- with(mydf, extendrange(range(long, short)))
>
> plong <- xyplot(long ~ date, groups = name, data = mydf,
>                 type = "l", grid = TRUE,
>                 ylim = r)
>
> pshort <- xyplot(short ~ date, groups = name, data = mydf, type = "l")
>
> plong + pshort
>
> The first plot here is the "main" one, so you need to make sure that its 'ylim' is big enough for all the data.
>
> -Deepayan
>
>
>>
>>
>> Thanks,
>> Naresh
>>
>> Sent from my iPhone
>>
>> > On Aug 10, 2022, at 10:02 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> >
>> > ?It is unclear to me how you wish to define and distinguish groups.
>> > Assuming you wish to have separate lines for the interaction as
>> > Deepayan showed, but want the colors (or line types or both) to differ
>> > only by by the "name" factor, then is this what you want?
>> >
>> > trellis.par.set(superpose.line = list(lty = 1:6)) ## or other line
>> > types you create
>> > u.names <- unique(mydf.long$name)
>> > xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
>> >       type = c("l","g"),
>> >       col.line = trellis.par.get("superpose.line")$col[
>> >          seq_along(u.names)],
>> >       lty =  trellis.par.get("superpose.line")$lty[
>> >          seq_along(u.names)]
>> > )
>> >
>> > Notes:
>> > 1. If this is not what you want, I give up. Others may have better insight.
>> > 2. If this is what you want, Deepayan may be able to provide you a
>> > nicer way to do it.
>> > 3. If you have more different names than 6 or 7, then you may have to
>> > add more line types or colors to the superpose.line settings. Though I
>> > would think the plot would be pretty much a mess, if so.
>> >
>> > Bert
>> >
>> >
>> >> On Wed, Aug 10, 2022 at 4:57 PM Naresh Gurbuxani
>> >> <naresh_gurbuxani at hotmail.com> wrote:
>> >>
>> >> Actually I meant types (e.g., lty = 1:2).  But colors would also work.  But I do not want to specify these in call to xyplot().
>> >>
>> >> In my actual problem, there are more than two groups.  Moreover, the number of groups changes from case to case.  In my set up, I use trellis.par.set() to set line styles, colors, pch, etc.
>> >>
>> >> I would the call to xyplot() to automatically use the set options.
>> >>
>> >> Thanks,
>> >> Naresh
>> >>
>> >> Sent from my iPhone
>> >>
>> >>>> On Aug 10, 2022, at 5:40 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> >>>
>> >>> ?I assume you mean two line colors, not types.
>> >>> Like this?
>> >>>
>> >>> xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
>> >>>      type = c("l", "g"), col.line = c("blue","red"))
>> >>>
>> >>>
>> >>> Cheers,
>> >>> Bert
>> >>>
>> >>>> On Wed, Aug 10, 2022 at 2:12 PM Naresh Gurbuxani
>> >>>> <naresh_gurbuxani at hotmail.com> wrote:
>> >>>>
>> >>>> Deepayan,
>> >>>>
>> >>>> Thanks for providing a solution.  While this is close to my goal, I want one more change.  The line type (lty) should be the same for long and short.  The line type should only change according to ?name? group.  So the the graph will have two line types (not four as in your solution).
>> >>>>
>> >>>> Is it possible?
>> >>>>
>> >>>> Thanks,
>> >>>> Narrsh
>> >>>>
>> >>>> Sent from my iPhone
>> >>>>
>> >>>>>> On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
>> >>>>>
>> >>>>> ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
>> >>>>> <naresh_gurbuxani at hotmail.com> wrote:
>> >>>>>>
>> >>>>>>
>> >>>>>> I want to merge two panels into one.  Is it possible to do this?
>> >>>>>>
>> >>>>>> Thanks,
>> >>>>>> Naresh
>> >>>>>>
>> >>>>>> library(lattice)
>> >>>>>> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
>> >>>>>> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
>> >>>>>> long = runif(20, 2, 10), short = runif(20, -10, 0))
>> >>>>>>
>> >>>>>> # This plots data in two panels.  I want all four lines in one panel.
>> >>>>>> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
>> >>>>>> "g"))
>> >>>>>
>> >>>>> The "extended" formula API (with +) is really only meant as an
>> >>>>> alternative to reshape() for simple cases. In your case, you probably
>> >>>>> want something like
>> >>>>>
>> >>>>> mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
>> >>>>> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
>> >>>>>
>> >>>>> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
>> >>>>> = c("l", "g"))
>> >>>>>
>> >>>>> -Deepayan
>> >>>>>
>> >>>>>> # This does not work
>> >>>>>> # No error in R session
>> >>>>>> # Graph window says: "Error using packet 1
>> >>>>>> # argument 'subscripts' is missing, with no default"
>> >>>>>> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
>> >>>>>> panel = function(x, y, ..., subscripts) {
>> >>>>>> panel.xyplot(x, y, ...)
>> >>>>>> panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
>> >>>>>>
>> >>>>>> ______________________________________________
>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> >>>>>> and provide commented, minimal, self-contained, reproducible code.
>> >>>> ______________________________________________
>> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> >>>> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Aug 12 23:30:56 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 12 Aug 2022 14:30:56 -0700
Subject: [R] lattice question
In-Reply-To: <CAGxFJbRHTpPEa-0KN+f3Qh264=-+SvVgA9QH80Thf9A8ycG7oQ@mail.gmail.com>
References: <BL0PR01MB40360688E655910F3A8FC982FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC4O+p1FJ5HgmQBqu8mf6kh0fM_Tcmxm_1g64xgVgssfRA@mail.gmail.com>
 <BL0PR01MB403656FFA1AB68A5A0E6FB65FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbRSSVtkZSRY1JBYiePL42HNTWgRW4vcrrfE1h=ZjnDzfg@mail.gmail.com>
 <BL0PR01MB4036724D69DF755E571D1DD7FA659@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbSkuspyGR1-f=aED+WnR0bEVfxLV_C-pfkA8Xj4yfM9_w@mail.gmail.com>
 <BL0PR01MB403675492C3C426B8607BF8BFA649@BL0PR01MB4036.prod.exchangelabs.com>
 <CADfFDC5qH_e1dh96pfRAViwt5GEu_shgUh=Fj-2eLV6hZsySDg@mail.gmail.com>
 <BL0PR01MB4036DA256D99B5B10B73E3E6FA679@BL0PR01MB4036.prod.exchangelabs.com>
 <CAGxFJbRHTpPEa-0KN+f3Qh264=-+SvVgA9QH80Thf9A8ycG7oQ@mail.gmail.com>
Message-ID: <CAGxFJbQn2Tm_91tRk+Qp4k+oCy+Ua4171BcfaqNkabAae2S88A@mail.gmail.com>

I hit the wrong button, unfortunately, so others beside Naresh and
Deepayan can safely ignore my "coda".

On Fri, Aug 12, 2022 at 2:29 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> As a private coda -- as it is unlikely to be of general interest --
> note that it is easy to do this without resorting to the layering
> paradigm through the use of the appropriate panel function and
> lattice's argument passing protocol (although I stand in awe of Felix
> Andrews's clever code!). Here's a version that uses R's underlying
> with() function to control the nonstandard evaluation. I've added a
> key and y axis label just to be cute:
>
> with(mydf,xyplot(short ~ date, group = name,
> type = c("l","g"),
>        y2 =long,
>        ylim = extendrange(c(short,long)),
>        auto.key = list(lines=TRUE, points = FALSE, space = "right"),
>        ylab = "Name",
>        panel= function(x,y, y2,...){
>           panel.superpose(x,y,...)
>           panel.superpose(x,y2,...)
>        }))
>
> On Fri, Aug 12, 2022 at 12:35 PM Naresh Gurbuxani
> <naresh_gurbuxani at hotmail.com> wrote:
> >
> > This is the solution I was looking for.  Thanks to Deepayan and Bert for sticking with me.
> >
> > Naresh
> >
> > Sent from my iPhone
> >
> > On Aug 12, 2022, at 8:02 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> >
> > ?
> >
> >
> > On Thu, Aug 11, 2022 at 9:03 PM Naresh Gurbuxani <naresh_gurbuxani at hotmail.com> wrote:
> >>
> >> Bert,
> >>
> >> Thanks for providing this solution.  It produces the desired graph.
> >>
> >> To see how I want to distinguish groups, you should look at original data (mydf).  There are two groups (Aa and Bb), each with two time series (long and short).  Long is always positive.  Short is always negative.  Therefore, there is no need to distinguish between long and short.  I only need to distinguish between Aa and Bb.
> >>
> >> I agree that more than six lines in the graph will make it cluttered.  In fact the above exercise is to avoid clutter in the key.  No need to show Aa.long and Aa.short, because long and short are obvious.
> >
> >
> > In that case, this alternative approach may be conceptually simpler:
> >
> > library(latticeExtra)
> >
> > r <- with(mydf, extendrange(range(long, short)))
> >
> > plong <- xyplot(long ~ date, groups = name, data = mydf,
> >                 type = "l", grid = TRUE,
> >                 ylim = r)
> >
> > pshort <- xyplot(short ~ date, groups = name, data = mydf, type = "l")
> >
> > plong + pshort
> >
> > The first plot here is the "main" one, so you need to make sure that its 'ylim' is big enough for all the data.
> >
> > -Deepayan
> >
> >
> >>
> >>
> >> Thanks,
> >> Naresh
> >>
> >> Sent from my iPhone
> >>
> >> > On Aug 10, 2022, at 10:02 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >> >
> >> > ?It is unclear to me how you wish to define and distinguish groups.
> >> > Assuming you wish to have separate lines for the interaction as
> >> > Deepayan showed, but want the colors (or line types or both) to differ
> >> > only by by the "name" factor, then is this what you want?
> >> >
> >> > trellis.par.set(superpose.line = list(lty = 1:6)) ## or other line
> >> > types you create
> >> > u.names <- unique(mydf.long$name)
> >> > xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
> >> >       type = c("l","g"),
> >> >       col.line = trellis.par.get("superpose.line")$col[
> >> >          seq_along(u.names)],
> >> >       lty =  trellis.par.get("superpose.line")$lty[
> >> >          seq_along(u.names)]
> >> > )
> >> >
> >> > Notes:
> >> > 1. If this is not what you want, I give up. Others may have better insight.
> >> > 2. If this is what you want, Deepayan may be able to provide you a
> >> > nicer way to do it.
> >> > 3. If you have more different names than 6 or 7, then you may have to
> >> > add more line types or colors to the superpose.line settings. Though I
> >> > would think the plot would be pretty much a mess, if so.
> >> >
> >> > Bert
> >> >
> >> >
> >> >> On Wed, Aug 10, 2022 at 4:57 PM Naresh Gurbuxani
> >> >> <naresh_gurbuxani at hotmail.com> wrote:
> >> >>
> >> >> Actually I meant types (e.g., lty = 1:2).  But colors would also work.  But I do not want to specify these in call to xyplot().
> >> >>
> >> >> In my actual problem, there are more than two groups.  Moreover, the number of groups changes from case to case.  In my set up, I use trellis.par.set() to set line styles, colors, pch, etc.
> >> >>
> >> >> I would the call to xyplot() to automatically use the set options.
> >> >>
> >> >> Thanks,
> >> >> Naresh
> >> >>
> >> >> Sent from my iPhone
> >> >>
> >> >>>> On Aug 10, 2022, at 5:40 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >> >>>
> >> >>> ?I assume you mean two line colors, not types.
> >> >>> Like this?
> >> >>>
> >> >>> xyplot(X ~ date, groups = interaction(name,G), data = mydf.long,
> >> >>>      type = c("l", "g"), col.line = c("blue","red"))
> >> >>>
> >> >>>
> >> >>> Cheers,
> >> >>> Bert
> >> >>>
> >> >>>> On Wed, Aug 10, 2022 at 2:12 PM Naresh Gurbuxani
> >> >>>> <naresh_gurbuxani at hotmail.com> wrote:
> >> >>>>
> >> >>>> Deepayan,
> >> >>>>
> >> >>>> Thanks for providing a solution.  While this is close to my goal, I want one more change.  The line type (lty) should be the same for long and short.  The line type should only change according to ?name? group.  So the the graph will have two line types (not four as in your solution).
> >> >>>>
> >> >>>> Is it possible?
> >> >>>>
> >> >>>> Thanks,
> >> >>>> Narrsh
> >> >>>>
> >> >>>> Sent from my iPhone
> >> >>>>
> >> >>>>>> On Aug 10, 2022, at 9:37 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> >> >>>>>
> >> >>>>> ?On Wed, Aug 10, 2022 at 4:53 PM Naresh Gurbuxani
> >> >>>>> <naresh_gurbuxani at hotmail.com> wrote:
> >> >>>>>>
> >> >>>>>>
> >> >>>>>> I want to merge two panels into one.  Is it possible to do this?
> >> >>>>>>
> >> >>>>>> Thanks,
> >> >>>>>> Naresh
> >> >>>>>>
> >> >>>>>> library(lattice)
> >> >>>>>> mydf <- data.frame(date = rep(seq.Date(as.Date("2022-08-01"), by = 1,
> >> >>>>>> length.out = 10), 2), name = c(rep("Aa", 10), rep("Bb", 10)),
> >> >>>>>> long = runif(20, 2, 10), short = runif(20, -10, 0))
> >> >>>>>>
> >> >>>>>> # This plots data in two panels.  I want all four lines in one panel.
> >> >>>>>> xyplot(long + short ~ date, groups = name, data = mydf, type = c("l",
> >> >>>>>> "g"))
> >> >>>>>
> >> >>>>> The "extended" formula API (with +) is really only meant as an
> >> >>>>> alternative to reshape() for simple cases. In your case, you probably
> >> >>>>> want something like
> >> >>>>>
> >> >>>>> mydf.long = reshape(mydf, direction = "long", varying = list(c("long",
> >> >>>>> "short")), v.names = "X", timevar = "G", times = c("long", "short"))
> >> >>>>>
> >> >>>>> xyplot(X ~ date, groups = interaction(name, G), data = mydf.long, type
> >> >>>>> = c("l", "g"))
> >> >>>>>
> >> >>>>> -Deepayan
> >> >>>>>
> >> >>>>>> # This does not work
> >> >>>>>> # No error in R session
> >> >>>>>> # Graph window says: "Error using packet 1
> >> >>>>>> # argument 'subscripts' is missing, with no default"
> >> >>>>>> xyplot(long ~ date, data = mydf, groups = name, type = c("l", "g"),
> >> >>>>>> panel = function(x, y, ..., subscripts) {
> >> >>>>>> panel.xyplot(x, y, ...)
> >> >>>>>> panel.xyplot(mydf$date[subscripts], mydf$short[subscripts], ...)})
> >> >>>>>>
> >> >>>>>> ______________________________________________
> >> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >> >>>> ______________________________________________
> >> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> >>>> and provide commented, minimal, self-contained, reproducible code.


From bogu@@chr|@to|er @end|ng |rom gm@||@com  Sat Aug 13 19:18:41 2022
From: bogu@@chr|@to|er @end|ng |rom gm@||@com (bogus christofer)
Date: Sat, 13 Aug 2022 22:48:41 +0530
Subject: [R] Fitted values from AR model
In-Reply-To: <CAGgJW74rxrDzwL2CqKy7c-YmdakRqkn8_1Jm=88_Skd_vZX+JA@mail.gmail.com>
References: <CAJKTqP7tOuF7TUiiuMpbRQOVwRCXsZVPXe9R7bQhG2O-Wj6HVg@mail.gmail.com>
 <CAGgJW74rxrDzwL2CqKy7c-YmdakRqkn8_1Jm=88_Skd_vZX+JA@mail.gmail.com>
Message-ID: <CAJKTqP6W5MZmdMDBRsPti=+goCAPDw2vDcs5FfrqLk8mxT3Tiw@mail.gmail.com>

Thanks Eric.

Is it possible to know what R uses as the initial values? Are these
configurable?

On Fri, 12 Aug 2022 at 13:44, Eric Berger <ericjberger at gmail.com> wrote:

> The model that you are fitting to the data is an AR(2) model, which means
>
> y(t) = a0 + a1 * y(t-1) + a2 * y(t-2) + eps(t)                 (1)
>
> The fitting procedure estimates the coefficients a0, a1, a2 (and the
> variance of eps).
>
> After the coefficients have been estimated, the fitted values can be
> calculated using equation (1) (setting eps(t) = 0)
> using
> fitted(t) = a0 + a1 * y(t-1) + a2 * y(t-2)
>
> Assuming the series is given for t=1,2,...,20, there is no problem to
> apply equation (1) to get the
> fitted values for t=3,4,...,20. But there is a problem for t=1 and
> t=2. For example, for t=1, equation (1)
> says
>
> fitted(1) = a0 + a1 * y(0) + a2 * y(-1)
>
> But there are no values given for y(0) and y(-1). So, either no fitted
> values should be given for t=1,2, or some
> other method is being used. Apparently, the arima functions in R and
> python use different methodology
> to generate these two fitted points. (For all the other values, the
> fits are extremely close.)
>
> HTH,
> Eric
>
> On Thu, Aug 11, 2022 at 9:53 PM bogus christofer
> <bogus.christofer at gmail.com> wrote:
> >
> > Hi,
> >
> > I have below AR model and fitted values from the forecast package
> >
> > library(forecast)
> > dta = c(5.0, 11, 16, 23, 36, 58, 29, 20, 10, 8, 3, 0, 0, 2, 11, 27, 47,
> 63,
> > 60, 39)
> > fit <- arima(dta, order=c(2,0,0))
> > fitted(fit)
> >
> > This gives fitted values as
> >
> > Time Series:
> > Start = 1
> > End = 20
> > Frequency = 1
> >  [1] 13.461017  9.073427 18.022166 20.689420 26.352282 38.165635
> 57.502926
> > 9.812106 15.335303  8.298995 11.543320  6.606999  5.800820  7.502621
> > 9.930962 19.723966 34.045298 49.252447 57.333846 44.615067
> >
> >
> > However when I compare this result with Python, I see significant
> > difference particularly in the first few values as below
> >
> > from statsmodels.tsa.arima.model import ARIMA
> > dta = [5.0, 11, 16, 23, 36, 58, 29, 20, 10, 8, 3, 0, 0, 2, 11, 27, 47,
> 63,
> > 60, 39]
> > fit = ARIMA(dta, order=(2, 0, 0)).fit()
> > fit.predict()
> >
> > array([21.24816788, 8.66048306, 18.02197059, 20.68931006,
> > 26.35225759,38.16574655, 57.503278 , 9.81253693, 15.33539514,
> > 8.29894655,11.54316056, 6.60679489, 5.80055038, 7.50232004,
> > 9.93067155,19.72374025, 34.04524337, 49.25265365, 57.3343347 ,
> 44.6157026 ])
> >
> > Any idea why there are such difference between R and Python results will
> be
> > very helpful.
> >
> > Thanks,
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Sat Aug 13 20:46:40 2022
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sat, 13 Aug 2022 21:46:40 +0300
Subject: [R] Fitted values from AR model
In-Reply-To: <CAJKTqP6W5MZmdMDBRsPti=+goCAPDw2vDcs5FfrqLk8mxT3Tiw@mail.gmail.com>
References: <CAJKTqP7tOuF7TUiiuMpbRQOVwRCXsZVPXe9R7bQhG2O-Wj6HVg@mail.gmail.com>
 <CAGgJW74rxrDzwL2CqKy7c-YmdakRqkn8_1Jm=88_Skd_vZX+JA@mail.gmail.com>
 <CAJKTqP6W5MZmdMDBRsPti=+goCAPDw2vDcs5FfrqLk8mxT3Tiw@mail.gmail.com>
Message-ID: <CAGgJW769CYbkpyZkmRu06YcusYArpVyYR+mua7vOf4n-Oh0EiQ@mail.gmail.com>

I have not looked into this. R is open source so you can download the
source code and look at it to try and figure it out.
I do not know if the values are configurable.

If you assume that Equation (1) is applicable for t=1 and t=2, then
you can back out values for y(0) and y(1).
Explicitly, if you enter

> fit

you will get the coefficients of the fit: ar1, ar2 and 'intercept'. In
the notation I used in Equation (1) these map to

a1 = ar1, a2 = ar2 and    a0 = intercept / (1 - ar1 - ar2 )

You then have

fitted(2) = a0 + a1 * y(1) + a2 * y(0)

All the values in this expression are known except for y(0), so you
can back out y(0).
Finally

fitted(1) = a0 + a1 * y(0) + a2 * y(-1)

Again, all the values are known except for y(-1) so you can back out y(-1).

I did this manually and got a0 = 7.502621, y(0) = 7.932861 and y(-1) =
6.475706, but I could easily have made a mistake so you
might want to try this yourself.

Best,
Eric


On Sat, Aug 13, 2022 at 8:18 PM bogus christofer
<bogus.christofer at gmail.com> wrote:
>
> Thanks Eric.
>
> Is it possible to know what R uses as the initial values? Are these configurable?
>
> On Fri, 12 Aug 2022 at 13:44, Eric Berger <ericjberger at gmail.com> wrote:
>>
>> The model that you are fitting to the data is an AR(2) model, which means
>>
>> y(t) = a0 + a1 * y(t-1) + a2 * y(t-2) + eps(t)                 (1)
>>
>> The fitting procedure estimates the coefficients a0, a1, a2 (and the
>> variance of eps).
>>
>> After the coefficients have been estimated, the fitted values can be
>> calculated using equation (1) (setting eps(t) = 0)
>> using
>> fitted(t) = a0 + a1 * y(t-1) + a2 * y(t-2)
>>
>> Assuming the series is given for t=1,2,...,20, there is no problem to
>> apply equation (1) to get the
>> fitted values for t=3,4,...,20. But there is a problem for t=1 and
>> t=2. For example, for t=1, equation (1)
>> says
>>
>> fitted(1) = a0 + a1 * y(0) + a2 * y(-1)
>>
>> But there are no values given for y(0) and y(-1). So, either no fitted
>> values should be given for t=1,2, or some
>> other method is being used. Apparently, the arima functions in R and
>> python use different methodology
>> to generate these two fitted points. (For all the other values, the
>> fits are extremely close.)
>>
>> HTH,
>> Eric
>>
>> On Thu, Aug 11, 2022 at 9:53 PM bogus christofer
>> <bogus.christofer at gmail.com> wrote:
>> >
>> > Hi,
>> >
>> > I have below AR model and fitted values from the forecast package
>> >
>> > library(forecast)
>> > dta = c(5.0, 11, 16, 23, 36, 58, 29, 20, 10, 8, 3, 0, 0, 2, 11, 27, 47, 63,
>> > 60, 39)
>> > fit <- arima(dta, order=c(2,0,0))
>> > fitted(fit)
>> >
>> > This gives fitted values as
>> >
>> > Time Series:
>> > Start = 1
>> > End = 20
>> > Frequency = 1
>> >  [1] 13.461017  9.073427 18.022166 20.689420 26.352282 38.165635 57.502926
>> > 9.812106 15.335303  8.298995 11.543320  6.606999  5.800820  7.502621
>> > 9.930962 19.723966 34.045298 49.252447 57.333846 44.615067
>> >
>> >
>> > However when I compare this result with Python, I see significant
>> > difference particularly in the first few values as below
>> >
>> > from statsmodels.tsa.arima.model import ARIMA
>> > dta = [5.0, 11, 16, 23, 36, 58, 29, 20, 10, 8, 3, 0, 0, 2, 11, 27, 47, 63,
>> > 60, 39]
>> > fit = ARIMA(dta, order=(2, 0, 0)).fit()
>> > fit.predict()
>> >
>> > array([21.24816788, 8.66048306, 18.02197059, 20.68931006,
>> > 26.35225759,38.16574655, 57.503278 , 9.81253693, 15.33539514,
>> > 8.29894655,11.54316056, 6.60679489, 5.80055038, 7.50232004,
>> > 9.93067155,19.72374025, 34.04524337, 49.25265365, 57.3343347 , 44.6157026 ])
>> >
>> > Any idea why there are such difference between R and Python results will be
>> > very helpful.
>> >
>> > Thanks,
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.


From n|ckmwr@y @end|ng |rom gm@||@com  Sun Aug 14 10:37:55 2022
From: n|ckmwr@y @end|ng |rom gm@||@com (Nick Wray)
Date: Sun, 14 Aug 2022 09:37:55 +0100
Subject: [R] von Neumann/Bartels/Serial Correlation
Message-ID: <CABxY9BN8FJiuCRj+sudxnOovB9GseA417GBMceXLUjFiCoxiHg@mail.gmail.com>

Hello  Although there is info on the net about the von Neumann test -
VonNeumannTest:
Von Neumann's Successive Difference Test in DescTools: Tools for
Descriptive Statistics (rdrr.io)
<https://rdrr.io/cran/DescTools/man/VonNeumannTest.html>

I can't make the function run and it doesn't say that the VonNeumannTest()
function is ain a package.  ?VonNeumannTest yields nothing

Likewise, although there is a CRAN page for BartelsRankTest() it doesn't
work and again ?BartelsRankTest gives nothing

And again serialCorrelationTest isn't recognise although again there are
Cran pages

Can anyone point me to where I can find these functions and any info?

Thanks Nick Wray

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Sun Aug 14 10:59:35 2022
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Sun, 14 Aug 2022 11:59:35 +0300
Subject: [R] von Neumann/Bartels/Serial Correlation
In-Reply-To: <CABxY9BN8FJiuCRj+sudxnOovB9GseA417GBMceXLUjFiCoxiHg@mail.gmail.com>
References: <CABxY9BN8FJiuCRj+sudxnOovB9GseA417GBMceXLUjFiCoxiHg@mail.gmail.com>
Message-ID: <CAGgJW75Apj1nha7YKm-V0dPGc+mFJndnsf=4sEmXcrxxjDMkHQ@mail.gmail.com>

Hi Nick,
I have no experience with this package but I just installed it with no
problem and found the help and ran the functions.

> library(DescTools)
> ?VonNeumannTest   <-- works fine
> VonNeumannTest(d.pizza$temperature)  <-- suggested in the help page; works fine
> ?BartelsRankTest  <-- works fine
also the example in the help page for BartelsRankTest works fine.

Did you run
> library(DescTools)

> sessionInfo()
R version 4.2.1 (2022-06-23)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 20.04.4 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] DescTools_0.99.43

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.9        lattice_0.20-45   mvtnorm_1.1-3     class_7.3-20
 [5] MASS_7.3-58       grid_4.2.1        rootSolve_1.8.2.3 e1071_1.7-11
 [9] gld_2.6.2         Exact_3.0         data.table_1.14.2 rstudioapi_0.13
[13] Matrix_1.4-1      boot_1.3-28       tools_4.2.1       proxy_0.4-27
[17] compiler_4.2.1    lmom_2.8          expm_0.999-6

HTH,
Eric

On Sun, Aug 14, 2022 at 11:38 AM Nick Wray <nickmwray at gmail.com> wrote:
>
> Hello  Although there is info on the net about the von Neumann test -
> VonNeumannTest:
> Von Neumann's Successive Difference Test in DescTools: Tools for
> Descriptive Statistics (rdrr.io)
> <https://rdrr.io/cran/DescTools/man/VonNeumannTest.html>
>
> I can't make the function run and it doesn't say that the VonNeumannTest()
> function is ain a package.  ?VonNeumannTest yields nothing
>
> Likewise, although there is a CRAN page for BartelsRankTest() it doesn't
> work and again ?BartelsRankTest gives nothing
>
> And again serialCorrelationTest isn't recognise although again there are
> Cran pages
>
> Can anyone point me to where I can find these functions and any info?
>
> Thanks Nick Wray
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Aug 14 13:50:22 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 14 Aug 2022 12:50:22 +0100
Subject: [R] von Neumann/Bartels/Serial Correlation
In-Reply-To: <CAGgJW75Apj1nha7YKm-V0dPGc+mFJndnsf=4sEmXcrxxjDMkHQ@mail.gmail.com>
References: <CABxY9BN8FJiuCRj+sudxnOovB9GseA417GBMceXLUjFiCoxiHg@mail.gmail.com>
 <CAGgJW75Apj1nha7YKm-V0dPGc+mFJndnsf=4sEmXcrxxjDMkHQ@mail.gmail.com>
Message-ID: <46479d4b-8ad3-486f-eb93-804b5e4dd057@sapo.pt>

Hello,

I cannot reproduce the errpr either.
Maybe exit and restart R or reinstall the package

?remove.packages/?install.packages

Hope this helps,
Rui Barradas

?s 09:59 de 14/08/2022, Eric Berger escreveu:
> Hi Nick,
> I have no experience with this package but I just installed it with no
> problem and found the help and ran the functions.
> 
>> library(DescTools)
>> ?VonNeumannTest   <-- works fine
>> VonNeumannTest(d.pizza$temperature)  <-- suggested in the help page; works fine
>> ?BartelsRankTest  <-- works fine
> also the example in the help page for BartelsRankTest works fine.
> 
> Did you run
>> library(DescTools)
> 
>> sessionInfo()
> R version 4.2.1 (2022-06-23)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 20.04.4 LTS
> 
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3
> 
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] DescTools_0.99.43
> 
> loaded via a namespace (and not attached):
>   [1] Rcpp_1.0.9        lattice_0.20-45   mvtnorm_1.1-3     class_7.3-20
>   [5] MASS_7.3-58       grid_4.2.1        rootSolve_1.8.2.3 e1071_1.7-11
>   [9] gld_2.6.2         Exact_3.0         data.table_1.14.2 rstudioapi_0.13
> [13] Matrix_1.4-1      boot_1.3-28       tools_4.2.1       proxy_0.4-27
> [17] compiler_4.2.1    lmom_2.8          expm_0.999-6
> 
> HTH,
> Eric
> 
> On Sun, Aug 14, 2022 at 11:38 AM Nick Wray <nickmwray at gmail.com> wrote:
>>
>> Hello  Although there is info on the net about the von Neumann test -
>> VonNeumannTest:
>> Von Neumann's Successive Difference Test in DescTools: Tools for
>> Descriptive Statistics (rdrr.io)
>> <https://rdrr.io/cran/DescTools/man/VonNeumannTest.html>
>>
>> I can't make the function run and it doesn't say that the VonNeumannTest()
>> function is ain a package.  ?VonNeumannTest yields nothing
>>
>> Likewise, although there is a CRAN page for BartelsRankTest() it doesn't
>> work and again ?BartelsRankTest gives nothing
>>
>> And again serialCorrelationTest isn't recognise although again there are
>> Cran pages
>>
>> Can anyone point me to where I can find these functions and any info?
>>
>> Thanks Nick Wray
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @n@@j@m@hed1994 @end|ng |rom gm@||@com  Mon Aug 15 17:34:12 2022
From: @n@@j@m@hed1994 @end|ng |rom gm@||@com (Anas Jamshed)
Date: Mon, 15 Aug 2022 20:34:12 +0500
Subject: [R] Normalize GEO Data
Message-ID: <CAG0CrLiNLfdRiKKGz2OB0U5Oq7h-7K_xXjjNiSe7as_4W1PpPQ@mail.gmail.com>

I performed GEO2R analysis on a series dataset and I'm looking to find the
up-regulated and down-regulated genes. I know that to find up-regulated and
down-regulated genes, I should check logFC (Fold-change in log2 scale
(generally)).Consider the value of 1 in log2 is 0. There is optimal cutoff
but log2 > 1 indicates up-regulation and log2 < -1 indicates
down-regulation of genes. Moreover, I should consider adj.p.val which is
the adjusted p-value (corrected p-value dues multiple comparisons). Again
there is no generally accepted cutoff but I should consider values < 0.05
which indicates the test is statistically significant.
But the problem is in this particular GSE series none of the adj.p.value is
< 0.05 - they are all "1" and a "0.636". However, the logFC values are >1,
but none of the samples have a condition of "p <0.05 & logFC > 1".
So, can it be said that in this case, I need to normalise my data to find
out the DEGs, up-regulated and down-regulated genes in a series GEO file?

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Mon Aug 15 17:48:50 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 15 Aug 2022 15:48:50 +0000
Subject: [R] Normalize GEO Data
In-Reply-To: <CAG0CrLiNLfdRiKKGz2OB0U5Oq7h-7K_xXjjNiSe7as_4W1PpPQ@mail.gmail.com>
References: <CAG0CrLiNLfdRiKKGz2OB0U5Oq7h-7K_xXjjNiSe7as_4W1PpPQ@mail.gmail.com>
Message-ID: <BN6PR2201MB1553F25E21731545346A0C1DCF689@BN6PR2201MB1553.namprd22.prod.outlook.com>

One approach for adjusted p-values is to divide by the "magic cut-off" value by the number of tests. Typically one decides significance if the p-value is lass than 0.05. So not it is 0.05/(number of tests). This can be too conservative, but sometimes there are not many options. If you have a multiple comparison procedure (like the mean difference between five treatments) then there are tests like Tukey's HSD test, and many others that try for more balance.

"They are all 1" is possible but unlikely that all p-values would equal 1 except for one vale of 0.636. Maybe some data was not copied correctly, or maybe the model was not built correctly, or data not read in correctly. Please check that the data the computer has in memory matches the data in your file

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Anas Jamshed
Sent: Monday, August 15, 2022 11:34 AM
To: R-help Mailing List <r-help at r-project.org>
Subject: [R] Normalize GEO Data

[External Email]

I performed GEO2R analysis on a series dataset and I'm looking to find the up-regulated and down-regulated genes. I know that to find up-regulated and down-regulated genes, I should check logFC (Fold-change in log2 scale (generally)).Consider the value of 1 in log2 is 0. There is optimal cutoff but log2 > 1 indicates up-regulation and log2 < -1 indicates down-regulation of genes. Moreover, I should consider adj.p.val which is the adjusted p-value (corrected p-value dues multiple comparisons). Again there is no generally accepted cutoff but I should consider values < 0.05 which indicates the test is statistically significant.
But the problem is in this particular GSE series none of the adj.p.value is < 0.05 - they are all "1" and a "0.636". However, the logFC values are >1, but none of the samples have a condition of "p <0.05 & logFC > 1".
So, can it be said that in this case, I need to normalise my data to find out the DEGs, up-regulated and down-regulated genes in a series GEO file?

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ceee701d2e54a41e01f9808da7ed3c46b%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637961745255003828%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=CSx3odPuL1fZhvyyHBuq2p24a%2Fi9V7qqIMC9UMp3yZ0%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Ceee701d2e54a41e01f9808da7ed3c46b%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637961745255003828%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=tlLb%2B9%2BlfexcLPRRpb%2FPvby5%2Bf3cC5jOWB8t5k6JdNU%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Mon Aug 15 17:52:01 2022
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Mon, 15 Aug 2022 10:52:01 -0500
Subject: [R] how to add count to pie chart legend
Message-ID: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>

Hi All,

I have df like this:

> df# A tibble: 2 ? 4
  V1        n  perc labels
  <chr> <int> <dbl> <chr> 1 Yes       8 0.364 36%   2 No       14 0.636 64%

I am making pie chart like this:

library(ggplot2)

ggplot(df, aes(x = "", y = perc, fill = V1)) +
  geom_col(color = "black") +
  geom_label(aes(label = labels),
             position = position_stack(vjust = 0.5),
             show.legend = FALSE) +
  guides(fill = guide_legend(title = "Answer")) +
  coord_polar(theta = "y") +
  theme_void()

How would I add in the legend beside Answer "Yes" count 8 (just number
8) and beside "No" count 14?

Thanks

Ana

	[[alternative HTML version deleted]]


From cry@n @end|ng |rom b|ngh@mton@edu  Mon Aug 15 18:06:28 2022
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W Ryan)
Date: Mon, 15 Aug 2022 12:06:28 -0400
Subject: [R] [External Email]  how to add count to pie chart legend
In-Reply-To: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
References: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
Message-ID: <CAM+rpYkOB5z5JDOcR=5ztknaRYsFznbNSCxCX7U1RoLE0aHGnA@mail.gmail.com>

Perhaps just use a table, and skip the pie chart?

--Chris Ryan

On Mon, Aug 15, 2022 at 11:59 AM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> Hi All,
>
> I have df like this:
>
> > df# A tibble: 2 ? 4
>   V1        n  perc labels
>   <chr> <int> <dbl> <chr> 1 Yes       8 0.364 36%   2 No       14 0.636 64%
>
> I am making pie chart like this:
>
> library(ggplot2)
>
> ggplot(df, aes(x = "", y = perc, fill = V1)) +
>   geom_col(color = "black") +
>   geom_label(aes(label = labels),
>              position = position_stack(vjust = 0.5),
>              show.legend = FALSE) +
>   guides(fill = guide_legend(title = "Answer")) +
>   coord_polar(theta = "y") +
>   theme_void()
>
> How would I add in the legend beside Answer "Yes" count 8 (just number
> 8) and beside "No" count 14?
>
> Thanks
>
> Ana
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Mon Aug 15 18:26:26 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 15 Aug 2022 16:26:26 +0000
Subject: [R] how to add count to pie chart legend
In-Reply-To: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
References: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
Message-ID: <BN6PR2201MB1553790CCA14D06259E52494CF689@BN6PR2201MB1553.namprd22.prod.outlook.com>

V1=c("Yes","No")
n=c(8,14)
perc=c(0.364,0.636)
labels=c("35%","64%")
df=data.frame(V1,n,perc,labels)


library(ggplot2)

ggplot(df, aes(x = "", y = perc, fill = V1)) +
  geom_col(color = "black") +
  geom_label(aes(label = labels),
             position = position_stack(vjust = 0.5),
             show.legend = FALSE) +
  guides(fill = guide_legend(title = "Answer")) +
  coord_polar(theta = "y") +
  theme_void() +
  geom_text(aes(label=n), position=position_stack(vjust=0.25))  


library(ggplot2)

ggplot(df, aes(x = "", y = perc, fill = V1)) +
  geom_col(color = "black") +
  geom_label(aes(label = labels),
             position = position_stack(vjust = 0.5),
             show.legend = FALSE) +
  guides(fill = guide_legend(title = "Answer")) +
  coord_polar(theta = "y") +
  theme_void() +
  geom_text(aes(label=c("n=8","n=14")), position=position_stack(vjust=0.25))  


There are two variants of geom_text. They have slightly different output. Play with vjust to get the location right.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ana Marija
Sent: Monday, August 15, 2022 11:52 AM
To: r-help <r-help at r-project.org>
Subject: [R] how to add count to pie chart legend

[External Email]

Hi All,

I have df like this:

> df# A tibble: 2 ? 4
  V1        n  perc labels
  <chr> <int> <dbl> <chr> 1 Yes       8 0.364 36%   2 No       14 0.636 64%

I am making pie chart like this:

library(ggplot2)

ggplot(df, aes(x = "", y = perc, fill = V1)) +
  geom_col(color = "black") +
  geom_label(aes(label = labels),
             position = position_stack(vjust = 0.5),
             show.legend = FALSE) +
  guides(fill = guide_legend(title = "Answer")) +
  coord_polar(theta = "y") +
  theme_void()

How would I add in the legend beside Answer "Yes" count 8 (just number
8) and beside "No" count 14?

Thanks

Ana

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C63484df8e9fa437560ec08da7ed72970%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637961759829882759%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=K%2BcWZLcFC5eVA5f5nogJvxLVzYadQ1sVgkdJPBVaaGo%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C63484df8e9fa437560ec08da7ed72970%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637961759829882759%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=6qDQiSaEy1WUChkgKjJdV8S%2FnH0bYOv2qloNGzpGt9E%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Mon Aug 15 18:44:51 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 15 Aug 2022 09:44:51 -0700
Subject: [R] Normalize GEO Data
In-Reply-To: <CAG0CrLiNLfdRiKKGz2OB0U5Oq7h-7K_xXjjNiSe7as_4W1PpPQ@mail.gmail.com>
References: <CAG0CrLiNLfdRiKKGz2OB0U5Oq7h-7K_xXjjNiSe7as_4W1PpPQ@mail.gmail.com>
Message-ID: <CAGxFJbTiBOUfN2+=m5EDQZeX669MKYQjfNLM121gd4WQSab28A@mail.gmail.com>

You realise, I presume, that your sample size may be too small to flag
*any* genes, up or down, using p-values? ... and that at a more
fundamental level, the use of hypotheses tests and p values for any of
this is controversial? A discussion of the latter is wayyyyyy off
topic on this list, but if you care to go down that rabbit hole,
online searches should take you there.

Cheers,
Bert

On Mon, Aug 15, 2022 at 8:34 AM Anas Jamshed <anasjamshed1994 at gmail.com> wrote:
>
> I performed GEO2R analysis on a series dataset and I'm looking to find the
> up-regulated and down-regulated genes. I know that to find up-regulated and
> down-regulated genes, I should check logFC (Fold-change in log2 scale
> (generally)).Consider the value of 1 in log2 is 0. There is optimal cutoff
> but log2 > 1 indicates up-regulation and log2 < -1 indicates
> down-regulation of genes. Moreover, I should consider adj.p.val which is
> the adjusted p-value (corrected p-value dues multiple comparisons). Again
> there is no generally accepted cutoff but I should consider values < 0.05
> which indicates the test is statistically significant.
> But the problem is in this particular GSE series none of the adj.p.value is
> < 0.05 - they are all "1" and a "0.636". However, the logFC values are >1,
> but none of the samples have a condition of "p <0.05 & logFC > 1".
> So, can it be said that in this case, I need to normalise my data to find
> out the DEGs, up-regulated and down-regulated genes in a series GEO file?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @n@@j@m@hed1994 @end|ng |rom gm@||@com  Mon Aug 15 18:52:10 2022
From: @n@@j@m@hed1994 @end|ng |rom gm@||@com (Anas Jamshed)
Date: Mon, 15 Aug 2022 21:52:10 +0500
Subject: [R] Normalize GEO Data
In-Reply-To: <CAGxFJbRQh-7JMoVRWumUM5uoqTyq7NYWE1L6gseFtzF+j2YRxQ@mail.gmail.com>
References: <CAG0CrLiNLfdRiKKGz2OB0U5Oq7h-7K_xXjjNiSe7as_4W1PpPQ@mail.gmail.com>
 <CAGxFJbTiBOUfN2+=m5EDQZeX669MKYQjfNLM121gd4WQSab28A@mail.gmail.com>
 <CAG0CrLgEfFJBzE2=xWhZ-EsHaYM4D1OQ4rPffFud2-O6m5wWig@mail.gmail.com>
 <CAGxFJbRQh-7JMoVRWumUM5uoqTyq7NYWE1L6gseFtzF+j2YRxQ@mail.gmail.com>
Message-ID: <CAG0CrLie=YFgNmQuUhu7MN7aaAoiOc7U5YDrP_M5ymq2puEaxQ@mail.gmail.com>

I mean how can I apply RMA to my datasets?

On Mon, Aug 15, 2022 at 9:50 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> I have no idea what you mean. Please do *not* respond to me privately.
>
> On Mon, Aug 15, 2022 at 9:48 AM Anas Jamshed <anasjamshed1994 at gmail.com>
> wrote:
> >
> > but how can I apply for RMA?
> >
> > On Mon, Aug 15, 2022 at 9:45 PM Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >>
> >> You realise, I presume, that your sample size may be too small to flag
> >> *any* genes, up or down, using p-values? ... and that at a more
> >> fundamental level, the use of hypotheses tests and p values for any of
> >> this is controversial? A discussion of the latter is wayyyyyy off
> >> topic on this list, but if you care to go down that rabbit hole,
> >> online searches should take you there.
> >>
> >> Cheers,
> >> Bert
> >>
> >> On Mon, Aug 15, 2022 at 8:34 AM Anas Jamshed <anasjamshed1994 at gmail.com>
> wrote:
> >> >
> >> > I performed GEO2R analysis on a series dataset and I'm looking to
> find the
> >> > up-regulated and down-regulated genes. I know that to find
> up-regulated and
> >> > down-regulated genes, I should check logFC (Fold-change in log2 scale
> >> > (generally)).Consider the value of 1 in log2 is 0. There is optimal
> cutoff
> >> > but log2 > 1 indicates up-regulation and log2 < -1 indicates
> >> > down-regulation of genes. Moreover, I should consider adj.p.val which
> is
> >> > the adjusted p-value (corrected p-value dues multiple comparisons).
> Again
> >> > there is no generally accepted cutoff but I should consider values <
> 0.05
> >> > which indicates the test is statistically significant.
> >> > But the problem is in this particular GSE series none of the
> adj.p.value is
> >> > < 0.05 - they are all "1" and a "0.636". However, the logFC values
> are >1,
> >> > but none of the samples have a condition of "p <0.05 & logFC > 1".
> >> > So, can it be said that in this case, I need to normalise my data to
> find
> >> > out the DEGs, up-regulated and down-regulated genes in a series GEO
> file?
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu  Mon Aug 15 20:03:17 2022
From: mccorm@ck @end|ng |rom mo|b|o@mgh@h@rv@rd@edu (Matthew McCormack)
Date: Mon, 15 Aug 2022 14:03:17 -0400
Subject: [R] Normalize GEO Data
In-Reply-To: <CAG0CrLiNLfdRiKKGz2OB0U5Oq7h-7K_xXjjNiSe7as_4W1PpPQ@mail.gmail.com>
References: <CAG0CrLiNLfdRiKKGz2OB0U5Oq7h-7K_xXjjNiSe7as_4W1PpPQ@mail.gmail.com>
Message-ID: <61f8f171-495e-12ed-ed0a-f091cb5579eb@molbio.mgh.harvard.edu>

Hi Anas,

 ??? How many replicates were there ?? and about how many genes are 
up-regulated (>logFC 1) and downregulated (<logFC1) ? Are there any 
genes with logFC > 4 or more or < logFC 4 ?


Matthew

On 8/15/22 11:34 AM, Anas Jamshed wrote:
>          External Email - Use Caution
>
> I performed GEO2R analysis on a series dataset and I'm looking to find the
> up-regulated and down-regulated genes. I know that to find up-regulated and
> down-regulated genes, I should check logFC (Fold-change in log2 scale
> (generally)).Consider the value of 1 in log2 is 0. There is optimal cutoff
> but log2 > 1 indicates up-regulation and log2 < -1 indicates
> down-regulation of genes. Moreover, I should consider adj.p.val which is
> the adjusted p-value (corrected p-value dues multiple comparisons). Again
> there is no generally accepted cutoff but I should consider values < 0.05
> which indicates the test is statistically significant.
> But the problem is in this particular GSE series none of the adj.p.value is
> < 0.05 - they are all "1" and a "0.636". However, the logFC values are >1,
> but none of the samples have a condition of "p <0.05 & logFC > 1".
> So, can it be said that in this case, I need to normalise my data to find
> out the DEGs, up-regulated and down-regulated genes in a series GEO file?
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://secure-web.cisco.com/1ejCH1ScxpWp3jKUTdhYh6oHp96oRTct5eZw7R78-f8rn9VA1xauGS565x25xXZMaV4xbC6VrjXrLIfB4ai3hKLZEwdZF_iKKTbzItQXqqhu9TOH7iRubx4o5GqRHUF7YFfQRZ1F7Mwi1x2we4pDL2m9DnFYN__1l-p0Cyqsb6R-L4ZURoSqj8VGDFU4073MeUd1Cx5X4PgWo2VvEXS558qllR5ovbJ1jkAEwDuWnH9xJnisDB27QaNn3Pn4Bs-7ryzW4HFvwyWhXVmpE9KXdkA1IcFYCBQy6A_7wtuG2NGlA-a9CW7ag8mfY8oMZhQumu0-2huQ_0V93UKHkHuqgaBYNWg9_HoLTx8lpmBPIjkw/https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help
> PLEASE do read the posting guide http://secure-web.cisco.com/1igXTXMGiWQafW7kci-9bCcUgQPc-ZyZkteyDHvPEdWvwSFYi2z5rFNONA0jn5it4q7YBduGJ_zxapPA126PjgFsgP9AY7TDqz0TaoK5xYGiFODIXiYBvNz70ml1wWiShtNr27rQvNy_gd6GiPbnRXjPQjV4ladYICHSrLjwzcTIYt7cAKfktk4J6pn2WrIEGB1O6uGorHb2ThWnp3ZP_7Ra2qaFkswhDgdmEwzu6PfSm2LZ1HrDNzoEnpk_vFcVZ1_isknajFN91QtM9EtvfWSu0lHOEgan9nTqEuPn_4huzbQxqYcwrtAgzrQ0joJZIxmVkv2C89-ieeKsSLCCI_95kI2FPOrI3JQVobAbnu0o/http%3A%2F%2Fwww.R-project.org%2Fposting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline <https://www.massgeneralbrigham.org/complianceline> .
Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail. 

From drj|m|emon @end|ng |rom gm@||@com  Tue Aug 16 03:43:11 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 16 Aug 2022 11:43:11 +1000
Subject: [R] how to add count to pie chart legend
In-Reply-To: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
References: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
Message-ID: <CA+8X3fWNgVnw6MG+Yuh_=OvygFJob2guW_eb0RLoK9YKd+eeJw@mail.gmail.com>

Hi Ana,
A lot of work for a little pie.

df<-read.table(text="V1 n
 Yes 8
 No 14",
 header=TRUE,
 stringsAsFactors=FALSE)
par(mar=c(5,4,4,4))
pie(df$n,df$V1,col=c(3,2),main="Yes and No",
 xlab="",ylab="",radius=1)
legend(0.75,-0.8,paste(df$V1,df$n),fill=c(3,2),
 xpd=TRUE)

Jim

On Tue, Aug 16, 2022 at 1:59 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hi All,
>
> I have df like this:
>
> > df# A tibble: 2 ? 4
>   V1        n  perc labels
>   <chr> <int> <dbl> <chr> 1 Yes       8 0.364 36%   2 No       14 0.636 64%
>
> I am making pie chart like this:
>
> library(ggplot2)
>
> ggplot(df, aes(x = "", y = perc, fill = V1)) +
>   geom_col(color = "black") +
>   geom_label(aes(label = labels),
>              position = position_stack(vjust = 0.5),
>              show.legend = FALSE) +
>   guides(fill = guide_legend(title = "Answer")) +
>   coord_polar(theta = "y") +
>   theme_void()
>
> How would I add in the legend beside Answer "Yes" count 8 (just number
> 8) and beside "No" count 14?
>
> Thanks
>
> Ana
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Tue Aug 16 04:20:28 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 15 Aug 2022 19:20:28 -0700
Subject: [R] how to add count to pie chart legend
In-Reply-To: <CA+8X3fWNgVnw6MG+Yuh_=OvygFJob2guW_eb0RLoK9YKd+eeJw@mail.gmail.com>
References: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
 <CA+8X3fWNgVnw6MG+Yuh_=OvygFJob2guW_eb0RLoK9YKd+eeJw@mail.gmail.com>
Message-ID: <CAGxFJbRvmbitCKeA-osUhz2vFUrMUF8Gu1jjkr4vdY2gQmS8zA@mail.gmail.com>

Fortune Nomination!

"A lot of work for a little pie." (in response to a query about how to
improve a pie chart)
-- Jim Lemon

On Mon, Aug 15, 2022 at 6:43 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ana,
> A lot of work for a little pie.
>
> df<-read.table(text="V1 n
>  Yes 8
>  No 14",
>  header=TRUE,
>  stringsAsFactors=FALSE)
> par(mar=c(5,4,4,4))
> pie(df$n,df$V1,col=c(3,2),main="Yes and No",
>  xlab="",ylab="",radius=1)
> legend(0.75,-0.8,paste(df$V1,df$n),fill=c(3,2),
>  xpd=TRUE)
>
> Jim
>
> On Tue, Aug 16, 2022 at 1:59 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > Hi All,
> >
> > I have df like this:
> >
> > > df# A tibble: 2 ? 4
> >   V1        n  perc labels
> >   <chr> <int> <dbl> <chr> 1 Yes       8 0.364 36%   2 No       14 0.636 64%
> >
> > I am making pie chart like this:
> >
> > library(ggplot2)
> >
> > ggplot(df, aes(x = "", y = perc, fill = V1)) +
> >   geom_col(color = "black") +
> >   geom_label(aes(label = labels),
> >              position = position_stack(vjust = 0.5),
> >              show.legend = FALSE) +
> >   guides(fill = guide_legend(title = "Answer")) +
> >   coord_polar(theta = "y") +
> >   theme_void()
> >
> > How would I add in the legend beside Answer "Yes" count 8 (just number
> > 8) and beside "No" count 14?
> >
> > Thanks
> >
> > Ana
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Tue Aug 16 04:37:46 2022
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Tue, 16 Aug 2022 14:37:46 +1200
Subject: [R] how to add count to pie chart legend
In-Reply-To: <CAGxFJbRvmbitCKeA-osUhz2vFUrMUF8Gu1jjkr4vdY2gQmS8zA@mail.gmail.com>
References: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
 <CA+8X3fWNgVnw6MG+Yuh_=OvygFJob2guW_eb0RLoK9YKd+eeJw@mail.gmail.com>
 <CAGxFJbRvmbitCKeA-osUhz2vFUrMUF8Gu1jjkr4vdY2gQmS8zA@mail.gmail.com>
Message-ID: <20220816143746.3ff2656a@rolf-Latitude-E7470>


On Mon, 15 Aug 2022 19:20:28 -0700
Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Fortune Nomination!
> 
> "A lot of work for a little pie." (in response to a query about how to
> improve a pie chart)
> -- Jim Lemon

I second the nomination.

cheers,

Rolf

> 
> On Mon, Aug 15, 2022 at 6:43 PM Jim Lemon <drjimlemon at gmail.com>
> wrote:
> >
> > Hi Ana,
> > A lot of work for a little pie.
> >
> > df<-read.table(text="V1 n
> >  Yes 8
> >  No 14",
> >  header=TRUE,
> >  stringsAsFactors=FALSE)
> > par(mar=c(5,4,4,4))
> > pie(df$n,df$V1,col=c(3,2),main="Yes and No",
> >  xlab="",ylab="",radius=1)
> > legend(0.75,-0.8,paste(df$V1,df$n),fill=c(3,2),
> >  xpd=TRUE)
> >
> > Jim
> >
> > On Tue, Aug 16, 2022 at 1:59 AM Ana Marija
> > <sokovic.anamarija at gmail.com> wrote:
> > >
> > > Hi All,
> > >
> > > I have df like this:
> > >
> > > > df# A tibble: 2 ? 4
> > >   V1        n  perc labels
> > >   <chr> <int> <dbl> <chr> 1 Yes       8 0.364 36%   2 No       14
> > > 0.636 64%
> > >
> > > I am making pie chart like this:
> > >
> > > library(ggplot2)
> > >
> > > ggplot(df, aes(x = "", y = perc, fill = V1)) +
> > >   geom_col(color = "black") +
> > >   geom_label(aes(label = labels),
> > >              position = position_stack(vjust = 0.5),
> > >              show.legend = FALSE) +
> > >   guides(fill = guide_legend(title = "Answer")) +
> > >   coord_polar(theta = "y") +
> > >   theme_void()
> > >
> > > How would I add in the legend beside Answer "Yes" count 8 (just
> > > number 8) and beside "No" count 14?
> > >
> > > Thanks
> > >
> > > Ana


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Aug 16 04:40:50 2022
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Mon, 15 Aug 2022 21:40:50 -0500
Subject: [R] how to add count to pie chart legend
In-Reply-To: <CA+8X3fWNgVnw6MG+Yuh_=OvygFJob2guW_eb0RLoK9YKd+eeJw@mail.gmail.com>
References: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
 <CA+8X3fWNgVnw6MG+Yuh_=OvygFJob2guW_eb0RLoK9YKd+eeJw@mail.gmail.com>
Message-ID: <CAF9-5jNwyxzjAJRP=gkri52foBLzfc1LnHZSpRerJeBn296mEQ@mail.gmail.com>

Thank you so much! Indeed a lot of work for a little pie :)

On Mon, Aug 15, 2022 at 8:43 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Ana,
> A lot of work for a little pie.
>
> df<-read.table(text="V1 n
>  Yes 8
>  No 14",
>  header=TRUE,
>  stringsAsFactors=FALSE)
> par(mar=c(5,4,4,4))
> pie(df$n,df$V1,col=c(3,2),main="Yes and No",
>  xlab="",ylab="",radius=1)
> legend(0.75,-0.8,paste(df$V1,df$n),fill=c(3,2),
>  xpd=TRUE)
>
> Jim
>
> On Tue, Aug 16, 2022 at 1:59 AM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >
> > Hi All,
> >
> > I have df like this:
> >
> > > df# A tibble: 2 ? 4
> >   V1        n  perc labels
> >   <chr> <int> <dbl> <chr> 1 Yes       8 0.364 36%   2 No       14 0.636
> 64%
> >
> > I am making pie chart like this:
> >
> > library(ggplot2)
> >
> > ggplot(df, aes(x = "", y = perc, fill = V1)) +
> >   geom_col(color = "black") +
> >   geom_label(aes(label = labels),
> >              position = position_stack(vjust = 0.5),
> >              show.legend = FALSE) +
> >   guides(fill = guide_legend(title = "Answer")) +
> >   coord_polar(theta = "y") +
> >   theme_void()
> >
> > How would I add in the legend beside Answer "Yes" count 8 (just number
> > 8) and beside "No" count 14?
> >
> > Thanks
> >
> > Ana
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Tue Aug 16 13:36:21 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Tue, 16 Aug 2022 07:36:21 -0400
Subject: [R] how to add count to pie chart legend
In-Reply-To: <8031_1660614216_27G1hZmG006175_CA+8X3fWNgVnw6MG+Yuh_=OvygFJob2guW_eb0RLoK9YKd+eeJw@mail.gmail.com>
References: <CAF9-5jMeA_wVvL-Sv2axJJEc1buap17+KyWigcqQhHwsFoZBzg@mail.gmail.com>
 <8031_1660614216_27G1hZmG006175_CA+8X3fWNgVnw6MG+Yuh_=OvygFJob2guW_eb0RLoK9YKd+eeJw@mail.gmail.com>
Message-ID: <56d804f5-3371-9f21-1ad4-7d1a43e6e14e@mcmaster.ca>

Dear Jim and Ana,

Why not skip the legend and put the counts in the labels?

with(df, pie(n, paste0(V1, " (", n, ")"),
     col=c(3, 2), main="Yes and No", radius=1))

Best,
  John

On 2022-08-15 9:43 p.m., Jim Lemon wrote:
> Hi Ana,
> A lot of work for a little pie.
> 
> df<-read.table(text="V1 n
>   Yes 8
>   No 14",
>   header=TRUE,
>   stringsAsFactors=FALSE)
> par(mar=c(5,4,4,4))
> pie(df$n,df$V1,col=c(3,2),main="Yes and No",
>   xlab="",ylab="",radius=1)
> legend(0.75,-0.8,paste(df$V1,df$n),fill=c(3,2),
>   xpd=TRUE)
> 
> Jim
> 
> On Tue, Aug 16, 2022 at 1:59 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>
>> Hi All,
>>
>> I have df like this:
>>
>>> df# A tibble: 2 ? 4
>>    V1        n  perc labels
>>    <chr> <int> <dbl> <chr> 1 Yes       8 0.364 36%   2 No       14 0.636 64%
>>
>> I am making pie chart like this:
>>
>> library(ggplot2)
>>
>> ggplot(df, aes(x = "", y = perc, fill = V1)) +
>>    geom_col(color = "black") +
>>    geom_label(aes(label = labels),
>>               position = position_stack(vjust = 0.5),
>>               show.legend = FALSE) +
>>    guides(fill = guide_legend(title = "Answer")) +
>>    coord_polar(theta = "y") +
>>    theme_void()
>>
>> How would I add in the legend beside Answer "Yes" count 8 (just number
>> 8) and beside "No" count 14?
>>
>> Thanks
>>
>> Ana
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From @h@e|ebrown @end|ng |rom gm@||@com  Mon Aug 15 19:20:59 2022
From: @h@e|ebrown @end|ng |rom gm@||@com (Shael Brown)
Date: Mon, 15 Aug 2022 13:20:59 -0400
Subject: [R] [R-pkgs] TDApplied: Machine Learning and Inference for
 Topological Data Analysis
Message-ID: <CAESJsLvGMY_YJe6fkWgSZAsASGgGeZ8SHD5=NAyjOhCqpFCjug@mail.gmail.com>

Dear R users,

I am very excited to announce my new package TDApplied is now available on
CRAN.

Topological data analysis is a powerful tool for finding non-linear global
structure in whole datasets. 'TDApplied' aims to bridge topological data
analysis with data, statistical and machine learning practitioners so that
more analyses may benefit from the power of topological data analysis. The
main tool of topological data analysis is persistent homology, which
computes a shape descriptor of a dataset, called a persistence diagram.
There are three goals of this package: (1) convert persistence diagrams
computed using the two main R packages for topological data analysis into a
data frame, (2) implement fast versions of both distance and kernel
calculations for pairs of persistence diagrams, and (3) provide scalable
methods for machine learning and inference for persistence diagrams.

For details and examples please check out the github page
https://github.com/shaelebrown/TDApplied or the README file on CRAN
https://cran.r-project.org/web/packages/TDApplied/readme/README.html

I sincerely hope that you will enjoy using this package and that it helps
the field of topological data analysis progress in both academia and
industry.

Best regards,
Shael

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From @n@@j@m@hed1994 @end|ng |rom gm@||@com  Sat Aug 20 23:13:20 2022
From: @n@@j@m@hed1994 @end|ng |rom gm@||@com (Anas Jamshed)
Date: Sun, 21 Aug 2022 02:13:20 +0500
Subject: [R] Microbiome 16S amplicon analysis in dada2
Message-ID: <CAG0CrLiA=0s_b+Wa+_SHhXKwkBJNC9_J4Ur+VpOQUdgLjL1fqQ@mail.gmail.com>

I've 80 FASTQ files ( 40F and 40R) for C57BL6 mice microbiome sequencing in
80 different folders and all are paired-end. How can I upload all of them
in R for dada2 analysis?

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun Aug 21 00:10:23 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 20 Aug 2022 15:10:23 -0700
Subject: [R] Microbiome 16S amplicon analysis in dada2
In-Reply-To: <CAG0CrLiA=0s_b+Wa+_SHhXKwkBJNC9_J4Ur+VpOQUdgLjL1fqQ@mail.gmail.com>
References: <CAG0CrLiA=0s_b+Wa+_SHhXKwkBJNC9_J4Ur+VpOQUdgLjL1fqQ@mail.gmail.com>
Message-ID: <805971D6-437F-4C80-9663-9A7089FFEA35@dcn.davis.ca.us>

1) The Posting Guide warns you to use a search engine to familiarize yourself with the obvious answers before posting. "fastq file r" brought up quite a bunch of promising results. If those results are not useful, do expand on why they weren't when asking anyone for further help.

2) The Posting Guide also warns you that these mailing lists are plain text only ... you sent formatted text and automated systems including your own email program collaborated to strip the formatting. This frequently damages or removes information the sender thinks we will see... so figure out how to tell your email program to send plain text when posting so we won't be left in the dark about what you think is obvious.

3) There is usually more than one way to import files of any type... and the choice you make could be poorly matched to the size of your data or the type of analysis. Do ask this "best practice" question in the Bioconductor help area [1] where you can find many more experts in this specialized use of R than here. Try to be a bit more complete in your question so they can offer informed answers relevant to the size and scope of your analysis.

[1] https://bioconductor.org/help/

On August 20, 2022 2:13:20 PM PDT, Anas Jamshed <anasjamshed1994 at gmail.com> wrote:
>I've 80 FASTQ files ( 40F and 40R) for C57BL6 mice microbiome sequencing in
>80 different folders and all are paired-end. How can I upload all of them
>in R for dada2 analysis?
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @n@@j@m@hed1994 @end|ng |rom gm@||@com  Sun Aug 21 14:13:28 2022
From: @n@@j@m@hed1994 @end|ng |rom gm@||@com (Anas Jamshed)
Date: Sun, 21 Aug 2022 17:13:28 +0500
Subject: [R] Merging different fastq files into one folder
Message-ID: <CAG0CrLibVsUsbC=ajLy6dkL6=NXpiR_SZOSVYckampvuCgnU7g@mail.gmail.com>

I have 80 fastq paired-end data files in 80 different folders but I want
all to be in one folder for analysis. How can I do this through R?

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Aug 21 16:50:48 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 21 Aug 2022 15:50:48 +0100
Subject: [R] Merging different fastq files into one folder
In-Reply-To: <CAG0CrLibVsUsbC=ajLy6dkL6=NXpiR_SZOSVYckampvuCgnU7g@mail.gmail.com>
References: <CAG0CrLibVsUsbC=ajLy6dkL6=NXpiR_SZOSVYckampvuCgnU7g@mail.gmail.com>
Message-ID: <e2c3679a-8783-9d43-f104-ad8f7da4dec6@sapo.pt>

Hello,

If all you want to do is to put all the .fastq files in the same folder, 
please give us an example of your directory structure.

Hope this helps,

Rui Barradas

?s 13:13 de 21/08/2022, Anas Jamshed escreveu:
> I have 80 fastq paired-end data files in 80 different folders but I want
> all to be in one folder for analysis. How can I do this through R?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From v@|kremk @end|ng |rom gm@||@com  Mon Aug 22 17:08:51 2022
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Mon, 22 Aug 2022 10:08:51 -0500
Subject: [R] Correlate
Message-ID: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>

Hi all,

I have a data set with  ~250  variables(columns).  I want to calculate
the correlation of  one variable with the rest of the other variables
and also want  the p-values  for each correlation.  Please see the
sample data and my attempt.  I  have got the correlation but unable to
get the p-values

dat <- read.table(text="x1 x2 x3 x4
           1.68 -0.96 -1.25  0.61
          -0.06  0.41  0.06 -0.96
              .    0.08  1.14  1.42
           0.80 -0.67  0.53 -0.68
           0.23 -0.97 -1.18 -0.78
          -1.03  1.11 -0.61    .
           2.15     .    0.02  0.66
           0.35 -0.37 -0.26  0.39
          -0.66  0.89   .    -1.49
           0.11  1.52  0.73  -1.03",header=TRUE)

#change all to numeric
    dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))

    data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
"pearson", use = "complete.obs")

Result
              [,1]
x2 -0.5845835
x3 -0.4664220
x4  0.7202837

How do I get the p-values ?

Thank you,


From tebert @end|ng |rom u||@edu  Mon Aug 22 17:53:48 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 22 Aug 2022 15:53:48 +0000
Subject: [R] Correlate
In-Reply-To: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
Message-ID: <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>

A somewhat clunky solution:
for(i in colnames(dat)){
  print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
  print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$p.value)
}

Rather than printing you could set up an array or list to save the results. 


Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
Sent: Monday, August 22, 2022 11:09 AM
To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: [R] Correlate

[External Email]

Hi all,

I have a data set with  ~250  variables(columns).  I want to calculate the correlation of  one variable with the rest of the other variables and also want  the p-values  for each correlation.  Please see the sample data and my attempt.  I  have got the correlation but unable to get the p-values

dat <- read.table(text="x1 x2 x3 x4
           1.68 -0.96 -1.25  0.61
          -0.06  0.41  0.06 -0.96
              .    0.08  1.14  1.42
           0.80 -0.67  0.53 -0.68
           0.23 -0.97 -1.18 -0.78
          -1.03  1.11 -0.61    .
           2.15     .    0.02  0.66
           0.35 -0.37 -0.26  0.39
          -0.66  0.89   .    -1.49
           0.11  1.52  0.73  -1.03",header=TRUE)

#change all to numeric
    dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))

    data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method = "pearson", use = "complete.obs")

Result
              [,1]
x2 -0.5845835
x3 -0.4664220
x4  0.7202837

How do I get the p-values ?

Thank you,

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cf0bf7462434f445fdc3808da84505c52%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637967777937186965%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=Oqo1ikNvtAix%2Fj7jax%2Bsf53J5eDHB0LHnRSHEy9O5hM%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cf0bf7462434f445fdc3808da84505c52%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637967777937186965%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=TWJ%2BJxRdA2S7PKBnsYg3DiSdFtSxIit6v1HOAi7Hft8%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Mon Aug 22 18:06:34 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 22 Aug 2022 09:06:34 -0700
Subject: [R] Correlate
In-Reply-To: <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>

... But of course the p-values are essentially meaningless without
some sort of multiplicity adjustment.
(search on "multiplicity adjustment" for details). :-(

-- Bert


On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>
> A somewhat clunky solution:
> for(i in colnames(dat)){
>   print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
>   print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$p.value)
> }
>
> Rather than printing you could set up an array or list to save the results.
>
>
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> Sent: Monday, August 22, 2022 11:09 AM
> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> Subject: [R] Correlate
>
> [External Email]
>
> Hi all,
>
> I have a data set with  ~250  variables(columns).  I want to calculate the correlation of  one variable with the rest of the other variables and also want  the p-values  for each correlation.  Please see the sample data and my attempt.  I  have got the correlation but unable to get the p-values
>
> dat <- read.table(text="x1 x2 x3 x4
>            1.68 -0.96 -1.25  0.61
>           -0.06  0.41  0.06 -0.96
>               .    0.08  1.14  1.42
>            0.80 -0.67  0.53 -0.68
>            0.23 -0.97 -1.18 -0.78
>           -1.03  1.11 -0.61    .
>            2.15     .    0.02  0.66
>            0.35 -0.37 -0.26  0.39
>           -0.66  0.89   .    -1.49
>            0.11  1.52  0.73  -1.03",header=TRUE)
>
> #change all to numeric
>     dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
>
>     data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method = "pearson", use = "complete.obs")
>
> Result
>               [,1]
> x2 -0.5845835
> x3 -0.4664220
> x4  0.7202837
>
> How do I get the p-values ?
>
> Thank you,
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cf0bf7462434f445fdc3808da84505c52%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637967777937186965%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=Oqo1ikNvtAix%2Fj7jax%2Bsf53J5eDHB0LHnRSHEy9O5hM%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cf0bf7462434f445fdc3808da84505c52%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637967777937186965%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=TWJ%2BJxRdA2S7PKBnsYg3DiSdFtSxIit6v1HOAi7Hft8%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Mon Aug 22 19:23:25 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 22 Aug 2022 17:23:25 +0000
Subject: [R] Correlate
In-Reply-To: <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
Message-ID: <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>

I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.

Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.

My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.

A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?

Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.

Large datasets can be very messy.


Tim

-----Original Message-----
From: Bert Gunter <bgunter.4567 at gmail.com> 
Sent: Monday, August 22, 2022 12:07 PM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Correlate

[External Email]

... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
(search on "multiplicity adjustment" for details). :-(

-- Bert


On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>
> A somewhat clunky solution:
> for(i in colnames(dat)){
>   print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
>   print(cor.test(dat[,i], dat$x1, method = "pearson", use = 
> "complete.obs")$p.value) }
>
> Rather than printing you could set up an array or list to save the results.
>
>
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> Sent: Monday, August 22, 2022 11:09 AM
> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> Subject: [R] Correlate
>
> [External Email]
>
> Hi all,
>
> I have a data set with  ~250  variables(columns).  I want to calculate 
> the correlation of  one variable with the rest of the other variables 
> and also want  the p-values  for each correlation.  Please see the 
> sample data and my attempt.  I  have got the correlation but unable to 
> get the p-values
>
> dat <- read.table(text="x1 x2 x3 x4
>            1.68 -0.96 -1.25  0.61
>           -0.06  0.41  0.06 -0.96
>               .    0.08  1.14  1.42
>            0.80 -0.67  0.53 -0.68
>            0.23 -0.97 -1.18 -0.78
>           -1.03  1.11 -0.61    .
>            2.15     .    0.02  0.66
>            0.35 -0.37 -0.26  0.39
>           -0.66  0.89   .    -1.49
>            0.11  1.52  0.73  -1.03",header=TRUE)
>
> #change all to numeric
>     dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
>
>     data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method = 
> "pearson", use = "complete.obs")
>
> Result
>               [,1]
> x2 -0.5845835
> x3 -0.4664220
> x4  0.7202837
>
> How do I get the p-values ?
>
> Thank you,
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> =0 PLEASE do read the posting guide 
> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> =0 PLEASE do read the posting guide 
> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.


From v@|kremk @end|ng |rom gm@||@com  Mon Aug 22 19:33:08 2022
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Mon, 22 Aug 2022 12:33:08 -0500
Subject: [R] Correlate
In-Reply-To: <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>

For the time being  I am assuming the relationship across  variables
is linear.  I want get the values first  and detailed examining  of
the relationship will follow later.

On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>
> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
>
> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
>
> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
>
> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
>
> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
>
> Large datasets can be very messy.
>
>
> Tim
>
> -----Original Message-----
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Monday, August 22, 2022 12:07 PM
> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> Subject: Re: [R] Correlate
>
> [External Email]
>
> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
> (search on "multiplicity adjustment" for details). :-(
>
> -- Bert
>
>
> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >
> > A somewhat clunky solution:
> > for(i in colnames(dat)){
> >   print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
> >   print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> > "complete.obs")$p.value) }
> >
> > Rather than printing you could set up an array or list to save the results.
> >
> >
> > Tim
> >
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> > Sent: Monday, August 22, 2022 11:09 AM
> > To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> > Subject: [R] Correlate
> >
> > [External Email]
> >
> > Hi all,
> >
> > I have a data set with  ~250  variables(columns).  I want to calculate
> > the correlation of  one variable with the rest of the other variables
> > and also want  the p-values  for each correlation.  Please see the
> > sample data and my attempt.  I  have got the correlation but unable to
> > get the p-values
> >
> > dat <- read.table(text="x1 x2 x3 x4
> >            1.68 -0.96 -1.25  0.61
> >           -0.06  0.41  0.06 -0.96
> >               .    0.08  1.14  1.42
> >            0.80 -0.67  0.53 -0.68
> >            0.23 -0.97 -1.18 -0.78
> >           -1.03  1.11 -0.61    .
> >            2.15     .    0.02  0.66
> >            0.35 -0.37 -0.26  0.39
> >           -0.66  0.89   .    -1.49
> >            0.11  1.52  0.73  -1.03",header=TRUE)
> >
> > #change all to numeric
> >     dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
> >
> >     data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
> > "pearson", use = "complete.obs")
> >
> > Result
> >               [,1]
> > x2 -0.5845835
> > x3 -0.4664220
> > x4  0.7202837
> >
> > How do I get the p-values ?
> >
> > Thank you,
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> > .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> > 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> > LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> > &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> > =0 PLEASE do read the posting guide
> > https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> > -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> > 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> > 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> > DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> > sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> > .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> > 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> > LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> > &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> > =0 PLEASE do read the posting guide
> > https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> > -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> > 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> > 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> > DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> > sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> > and provide commented, minimal, self-contained, reproducible code.


From j|ox @end|ng |rom mcm@@ter@c@  Mon Aug 22 20:00:11 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Mon, 22 Aug 2022 14:00:11 -0400
Subject: [R] Correlate
In-Reply-To: <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
Message-ID: <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>

Dear Val,

On 2022-08-22 1:33 p.m., Val wrote:
> For the time being  I am assuming the relationship across  variables
> is linear.  I want get the values first  and detailed examining  of
> the relationship will follow later.

This seems backwards to me, but I'll refrain from commenting further on 
whether what you want to do makes sense and instead address how to do it 
(not, BTW, because I disagree with Bert's and Tim's remarks).

Please see below:

> 
> On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>>
>> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
>>
>> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
>>
>> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
>>
>> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
>>
>> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
>>
>> Large datasets can be very messy.
>>
>>
>> Tim
>>
>> -----Original Message-----
>> From: Bert Gunter <bgunter.4567 at gmail.com>
>> Sent: Monday, August 22, 2022 12:07 PM
>> To: Ebert,Timothy Aaron <tebert at ufl.edu>
>> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
>> Subject: Re: [R] Correlate
>>
>> [External Email]
>>
>> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
>> (search on "multiplicity adjustment" for details). :-(
>>
>> -- Bert
>>
>>
>> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>>>
>>> A somewhat clunky solution:
>>> for(i in colnames(dat)){
>>>    print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
>>>    print(cor.test(dat[,i], dat$x1, method = "pearson", use =
>>> "complete.obs")$p.value) }

Because of missing data, this computes the correlations on different 
subsets of the data. A simple solution is to filter the data for NAs:

D <- na.omit(dat)

More comments below:

>>>
>>> Rather than printing you could set up an array or list to save the results.
>>>
>>>
>>> Tim
>>>
>>> -----Original Message-----
>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
>>> Sent: Monday, August 22, 2022 11:09 AM
>>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
>>> Subject: [R] Correlate
>>>
>>> [External Email]
>>>
>>> Hi all,
>>>
>>> I have a data set with  ~250  variables(columns).  I want to calculate
>>> the correlation of  one variable with the rest of the other variables
>>> and also want  the p-values  for each correlation.  Please see the
>>> sample data and my attempt.  I  have got the correlation but unable to
>>> get the p-values
>>>
>>> dat <- read.table(text="x1 x2 x3 x4
>>>             1.68 -0.96 -1.25  0.61
>>>            -0.06  0.41  0.06 -0.96
>>>                .    0.08  1.14  1.42
>>>             0.80 -0.67  0.53 -0.68
>>>             0.23 -0.97 -1.18 -0.78
>>>            -1.03  1.11 -0.61    .
>>>             2.15     .    0.02  0.66
>>>             0.35 -0.37 -0.26  0.39
>>>            -0.66  0.89   .    -1.49
>>>             0.11  1.52  0.73  -1.03",header=TRUE)
>>>
>>> #change all to numeric
>>>      dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))

This data manipulation is unnecessary. Just specify the argument 
na.strings="." to read.table().

>>>
>>>      data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
>>> "pearson", use = "complete.obs")
>>>
>>> Result
>>>                [,1]
>>> x2 -0.5845835
>>> x3 -0.4664220
>>> x4  0.7202837
>>>
>>> How do I get the p-values ?

Taking a somewhat different approach from cor.test(), you can apply 
Fisher's z-transformation (recall that D is the data filtered for NAs):

 > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
         [,1]
x2 0.2462807
x3 0.3812854
x4 0.1156939

I hope this helps,
  John

>>>
>>> Thank you,
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
>>> =0 PLEASE do read the posting guide
>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
>>> =0 PLEASE do read the posting guide
>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From mr|nm@yk@||t@2 @end|ng |rom gm@||@com  Mon Aug 22 17:16:08 2022
From: mr|nm@yk@||t@2 @end|ng |rom gm@||@com (Mrinmay Kalita)
Date: Mon, 22 Aug 2022 20:46:08 +0530
Subject: [R] Unable to install package "checkpoint" on R3.6.3
Message-ID: <CAM1wLyZoxLKjiueR8gAo35M1M7WRRwF_YV9mrF38mu5VaY0Www@mail.gmail.com>

Hello,

I needed help to install checkpoint package. I tried stackoverflow to no
help. I did conda update -all and conda clean -all as well.

As per the screenshot attachments, some packages did not get installed
while doing "install.packages("checkpoint")" on R console. I retried using
different mirrors for download to no good.

So, I am not able to install Shiny to run my web app as
install.packages("shiny") also says eventually that package checkpoint is
missing.

I have downloaded the package as a zip also from
https://cran.r-project.org/web/packages/checkpoint/index.html. I unzipped
the folder but which file from inside should i run to install checkpoint to
my RStudio in my Windows PC.

i am a novice to R programming; so please help.

Regards

From |@go@g|ne @end|ng |rom @jd@e@  Tue Aug 23 10:20:22 2022
From: |@go@g|ne @end|ng |rom @jd@e@ (=?Windows-1252?Q?IAGO_GIN=C9_V=C1ZQUEZ?=)
Date: Tue, 23 Aug 2022 08:20:22 +0000
Subject: [R] Unable to install package "checkpoint" on R3.6.3
In-Reply-To: <CAM1wLyZoxLKjiueR8gAo35M1M7WRRwF_YV9mrF38mu5VaY0Www@mail.gmail.com>
References: <CAM1wLyZoxLKjiueR8gAo35M1M7WRRwF_YV9mrF38mu5VaY0Www@mail.gmail.com>
Message-ID: <AM6PR02MB44236618AE783CE93093F16A94709@AM6PR02MB4423.eurprd02.prod.outlook.com>

Hi,

Once downloaded the package as a zip, to install checkpoint with your RStudio you don't need to unzip the compressed package. From RStudio, click on Install (in pane Files, Plots, Packages, and Help - Packages tab),select Install from: Package Archive File, choose the downloaded zip file and click Install.

As a side note, it is strange that "install.packages("shiny") also says eventually that package checkpoint is missing" since shiny does neither depends (on) nor imports checkpoint.

Regards





________________________________
De: R-help <r-help-bounces at r-project.org> de part de Mrinmay Kalita <mrinmaykalita2 at gmail.com>
Enviat el: dilluns, 22 d?agost de 2022 17:16
Per a: r-help at r-project.org <r-help at r-project.org>
Tema: [R] Unable to install package "checkpoint" on R3.6.3

Hello,

I needed help to install checkpoint package. I tried stackoverflow to no
help. I did conda update -all and conda clean -all as well.

As per the screenshot attachments, some packages did not get installed
while doing "install.packages("checkpoint")" on R console. I retried using
different mirrors for download to no good.

So, I am not able to install Shiny to run my web app as
install.packages("shiny") also says eventually that package checkpoint is
missing.

I have downloaded the package as a zip also from
https://cran.r-project.org/web/packages/checkpoint/index.html. I unzipped
the folder but which file from inside should i run to install checkpoint to
my RStudio in my Windows PC.

i am a novice to R programming; so please help.

Regards
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Tue Aug 23 10:33:43 2022
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 23 Aug 2022 11:33:43 +0300
Subject: [R] Unable to install package "checkpoint" on R3.6.3
In-Reply-To: <CAM1wLyZoxLKjiueR8gAo35M1M7WRRwF_YV9mrF38mu5VaY0Www@mail.gmail.com>
References: <CAM1wLyZoxLKjiueR8gAo35M1M7WRRwF_YV9mrF38mu5VaY0Www@mail.gmail.com>
Message-ID: <20220823113343.482ce754@Tarkus>

On Mon, 22 Aug 2022 20:46:08 +0530
Mrinmay Kalita <mrinmaykalita2 at gmail.com> wrote:

> As per the screenshot attachments, some packages did not get installed
> while doing "install.packages("checkpoint")" on R console. I retried
> using different mirrors for download to no good.

Attachments didn't make it through: the mailing list filters them (it's
mentioned in the posting guide). Please copy and paste the text
containing the error messages. You're already sending plain text
e-mail, which is the right thing to do with this mailing list, so
you're halfway there.

> I did conda update -all and conda clean -all as well.

This could be an Anaconda-related problem. See
<https://docs.conda.io/en/latest/help-support.html> for Anaconda
support.

-- 
Best regards,
Ivan


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Tue Aug 23 11:00:11 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Tue, 23 Aug 2022 10:00:11 +0100
Subject: [R] Unable to install package "checkpoint" on R3.6.3
In-Reply-To: <CAM1wLyZoxLKjiueR8gAo35M1M7WRRwF_YV9mrF38mu5VaY0Www@mail.gmail.com>
References: <CAM1wLyZoxLKjiueR8gAo35M1M7WRRwF_YV9mrF38mu5VaY0Www@mail.gmail.com>
Message-ID: <6f9dcea7-c899-04f3-51cf-536be01714f6@sapo.pt>

Hello,

Instead of downloading the zip file, download the tar.gz file from [1]

checkpoint_0.4.10.tar.gz

and at a command line run

R CMD INSTALL checkpoint_0.4.10.tar.gz


[1] https://cran.r-project.org/src/contrib/Archive/checkpoint/

Hope this helps,

Rui Barradas


?s 16:16 de 22/08/2022, Mrinmay Kalita escreveu:
> Hello,
> 
> I needed help to install checkpoint package. I tried stackoverflow to no
> help. I did conda update -all and conda clean -all as well.
> 
> As per the screenshot attachments, some packages did not get installed
> while doing "install.packages("checkpoint")" on R console. I retried using
> different mirrors for download to no good.
> 
> So, I am not able to install Shiny to run my web app as
> install.packages("shiny") also says eventually that package checkpoint is
> missing.
> 
> I have downloaded the package as a zip also from
> https://cran.r-project.org/web/packages/checkpoint/index.html. I unzipped
> the folder but which file from inside should i run to install checkpoint to
> my RStudio in my Windows PC.
> 
> i am a novice to R programming; so please help.
> 
> Regards
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @n@@j@m@hed1994 @end|ng |rom gm@||@com  Wed Aug 24 01:49:47 2022
From: @n@@j@m@hed1994 @end|ng |rom gm@||@com (Anas Jamshed)
Date: Wed, 24 Aug 2022 04:49:47 +0500
Subject: [R] Read excel specific column
Message-ID: <CAG0CrLiFZQsM17SRga2wT9s+NF3pUFw8Pdwb-BDjbRKjAoSEyA@mail.gmail.com>

I have .xlsx files with gene names in first column.How can read and load in
R?

	[[alternative HTML version deleted]]


From @kw@|mmo @end|ng |rom gm@||@com  Wed Aug 24 01:55:33 2022
From: @kw@|mmo @end|ng |rom gm@||@com (Andrew Simmons)
Date: Tue, 23 Aug 2022 19:55:33 -0400
Subject: [R] Read excel specific column
In-Reply-To: <CAG0CrLiFZQsM17SRga2wT9s+NF3pUFw8Pdwb-BDjbRKjAoSEyA@mail.gmail.com>
References: <CAG0CrLiFZQsM17SRga2wT9s+NF3pUFw8Pdwb-BDjbRKjAoSEyA@mail.gmail.com>
Message-ID: <CAPcHnpT5NOaJLoCo6zxysN0w_j92gOi-EZ0a0OFbM3ybGjYyhw@mail.gmail.com>

I like package openxlsx, with the function openxlsx::read.xlsx()

Another common package that people use readxl

On Tue., Aug. 23, 2022, 7:51 p.m. Anas Jamshed, <anasjamshed1994 at gmail.com>
wrote:

> I have .xlsx files with gene names in first column.How can read and load in
> R?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Aug 24 07:40:00 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 24 Aug 2022 06:40:00 +0100
Subject: [R] Read excel specific column
In-Reply-To: <CAG0CrLiFZQsM17SRga2wT9s+NF3pUFw8Pdwb-BDjbRKjAoSEyA@mail.gmail.com>
References: <CAG0CrLiFZQsM17SRga2wT9s+NF3pUFw8Pdwb-BDjbRKjAoSEyA@mail.gmail.com>
Message-ID: <21746719-e669-6e39-6435-e263f63ec7e8@sapo.pt>

Hello,

The examples below use packages readxl and cellranger.


# to read the 1st column of a xlsx file named filename:
library(readxl)

read_xlsx(filename, range = cellranger::cell_cols("A"))

# to read 1st column of all files in filenames_vec
# result is a list of data.frames each of them with
# one column only
gene_names_list <- lapply(filenames_vec, \(x) {
   read_xlsx(x, range = cellranger::cell_cols("A"))
})

# to read 1st column of all files in filenames_vec
# result is a vector
gene_names_vec <- lapply(filenames_vec, \(x) {
   read_xlsx(x, range = cellranger::cell_cols("A"))[[1]]
})
gene_names_vec <- unlist(gene_names_vec)



If the files are xls, not xlsx, use read_xls; if you don't know, 
read_excel will call the right function.


Hope this helps,

Rui Barradas


?s 00:49 de 24/08/2022, Anas Jamshed escreveu:
> I have .xlsx files with gene names in first column.How can read and load in
> R?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@b@y@t194 @end|ng |rom gm@||@com  Wed Aug 24 09:44:38 2022
From: j@b@y@t194 @end|ng |rom gm@||@com (javad bayat)
Date: Wed, 24 Aug 2022 12:14:38 +0430
Subject: [R] Getting minimum value of a column according a factor column of
 a dataframe
Message-ID: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>

Dear all;
I am trying to get the minimum value of a column based on a factor column
of the same data frame. My data frame is like the below:
       Code               Y               M                D
 Q
     N              O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6

I want to get the minimum value of the "Q" column with the whole row
values, according to the "Code"  column  which is a factor. Overall it will
give me 4 rows, with the value of "Q". Below is a code that I used but it
did not give me what I wanted.

> x[which(x$Q == min(x$Q)),]

Sincerely



-- 
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Wed Aug 24 18:16:20 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Wed, 24 Aug 2022 16:16:20 +0000
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
Message-ID: <BN6PR2201MB15531357C53E47E2EF408E94CF739@BN6PR2201MB1553.namprd22.prod.outlook.com>

library(dplyr)
library(magrittr)
dat2<-read.table(text="Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6", header=TRUE)
dat3 <- dat2 %>%
  group_by(Code) %>%
  mutate(
    MinByCodeY = min(Y, na.rm = T),
  ) %>%
  arrange(Code)

This returns a dataframe with a new variable "MinByCodeY" which is the minimum value in Y for each group of Code where the minimum value within the group is added to each row of data in the original dataframe.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of javad bayat
Sent: Wednesday, August 24, 2022 3:45 AM
To: R-help at r-project.org
Subject: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

Dear all;
I am trying to get the minimum value of a column based on a factor column of the same data frame. My data frame is like the below:
       Code               Y               M                D
 Q
     N              O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6

I want to get the minimum value of the "Q" column with the whole row values, according to the "Code"  column  which is a factor. Overall it will give me 4 rows, with the value of "Q". Below is a code that I used but it did not give me what I wanted.

> x[which(x$Q == min(x$Q)),]

Sincerely



--
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C9c40fa5f8b354251cf5c08da85e82878%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637969529406639528%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=wRIHPCENQspDLjF9IbGTsyM1kJ5hYhIwfYuFSU9UFDM%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C9c40fa5f8b354251cf5c08da85e82878%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637969529406639528%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=jthiGDRGroo5sh8snIDQHHGsWAndtxp5SCB7HvyhWg8%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Wed Aug 24 18:23:09 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 24 Aug 2022 12:23:09 -0400
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
Message-ID: <00bb01d8b7d5$cd1b5ff0$67521fd0$@gmail.com>

Javad,

If I understood you, you want to use one of many methods to GROUP BY one
column and take the minimum within each group.

If your data is set up right, perhaps using factors, there are base R
versions but many would also suggest using dplyr/tidyverse methods such as
piping your data to group_by then to generating a report per group using the
function(s) you wish. 

In your case, if all four categories were found in your data, you would get
four output lines.

Note in a very low-tech way, if your problem is static and you know the
exact 4 values you want, you can simply make 4 subsets of your data directly
and apply your minimum calculation to each. If you have a situation with the
number of factors not being known in advance, more general methods that
dynamically do the grouping for you are needed.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of javad bayat
Sent: Wednesday, August 24, 2022 3:45 AM
To: R-help at r-project.org
Subject: [R] Getting minimum value of a column according a factor column of
a dataframe

Dear all;
I am trying to get the minimum value of a column based on a factor column of
the same data frame. My data frame is like the below:
       Code               Y               M                D
 Q
     N              O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6

I want to get the minimum value of the "Q" column with the whole row values,
according to the "Code"  column  which is a factor. Overall it will give me
4 rows, with the value of "Q". Below is a code that I used but it did not
give me what I wanted.

> x[which(x$Q == min(x$Q)),]

Sincerely



--
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Aug 24 18:54:55 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 24 Aug 2022 17:54:55 +0100
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
Message-ID: <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>

Hello,

Here are two options, the 1st outputs a vector, the 2nd a data.frame.


x<-'41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6'
df1 <- read.table(textConnection(x))
names(df1) <- scan(what = character(),
                    text = 'Code Y M D Q N O')
df1$Code <- factor(df1$Code)

# 1st option
with(df1, tapply(Q, Code, min))
#  41003 41005 41009 41017
#  0.160 0.210 0.218 0.240

# 2nd option
aggregate(Q ~ Code, df1, min)
#     Code     Q
#  1 41003 0.160
#  2 41005 0.210
#  3 41009 0.218
#  4 41017 0.240


Hope this helps,

Rui Barradas

?s 08:44 de 24/08/2022, javad bayat escreveu:
> Dear all;
> I am trying to get the minimum value of a column based on a factor column
> of the same data frame. My data frame is like the below:
>         Code               Y               M                D
>   Q
>       N              O
> 41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6
> 
> I want to get the minimum value of the "Q" column with the whole row
> values, according to the "Code"  column  which is a factor. Overall it will
> give me 4 rows, with the value of "Q". Below is a code that I used but it
> did not give me what I wanted.
> 
>> x[which(x$Q == min(x$Q)),]
> 
> Sincerely
> 
> 
>


From du|c@|m@ @end|ng |rom b|gpond@com  Thu Aug 25 05:03:02 2022
From: du|c@|m@ @end|ng |rom b|gpond@com (dulcalma dulcalma)
Date: Thu, 25 Aug 2022 13:03:02 +1000 (AEST)
Subject: [R] Unicode chars
Message-ID: <3adb4f19.9ce71.182d2f3fec2.Webtop.91@bigpond.com>


Dear All


I was trying the supplementary file?GS_main.R from
https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.3475

I have tried to prevent latex compilation from failing using Sweave 
after trying all the online fixes I could find including using Rterm?
I could fix it if it was in the input but not in the output?
I am using R version 4.2 on windows 11 with 64 GB memory


Sweave code

\begin{small}
<<r0>>=
library(emdbook) # version 1.3.12
library(bbmle) # version 1.0.23.1
library(pbmcapply) # version 1.5.0?
library(tidyverse) # version 1.3.0
library(ggpubr) # version 0.4.0
@ %%


<<r7>>=
summaryTable <-
tibble(model = m.names,
? ? ? ?dim = m.dims[model],? ? ? ? ? ? ? ? ?
? ? ? ?score = m.loo[model],? ? ? ? ? ? ? ??
? ? ? ?delScore = score - min(score),? ? ? ?
? ? ? ?se_ose = se_ose[model],? ? ? ? ? ? ??
? ? ? ?se_mod = se_mod[model]) %>% arrange(dim) %>%? mutate(index = 
1:length(dim))
summaryTable
@ %%


Output
\begin{Schunk}
\begin{Sinput}
? summaryTable <-
? tibble(model = m.names,
? ? ? ? ?dim = m.dims[model],? ? ? ? ? ? ? ? ?
? ? ? ? ?score = m.loo[model],? ? ? ? ? ? ? ??
? ? ? ? ?delScore = score - min(score),? ? ? ?
? ? ? ? ?se_ose = se_ose[model],? ? ? ? ? ? ??
? ? ? ? ?se_mod = se_mod[model]) %>% arrange(dim) %>%? mutate(index = 
1:length(dim))
? summaryTable
\end{Sinput}
\begin{Soutput}
# A tibble: 10 ? 7
? ?model? ?dim score delScore se_ose se_mod index
? ?<chr> <int> <dbl>? ? <dbl>? <dbl>? <dbl> <int>
?1 zero? ? ? 2? 908.? ? 5.84? ? 40.1? ?4.14? ? ?1
?2 d? ? ? ? ?3? 904.? ? 1.71? ? 40.6? ?2.52? ? ?2
?3 q? ? ? ? ?3? 907.? ? 4.92? ? 40.2? ?3.80? ? ?3
?4 qd? ? ? ? 4? 902.? ? 0? ? ? ?40.7? ?0? ? ? ? 4
?5 qdi? ? ? ?5? 903.? ? 0.632? ?40.5? ?1.60? ? ?5
?6 x? ? ? ? ?6? 908.? ? 5.58? ? 40.2? ?5.53? ? ?6
?7 xq? ? ? ? 7? 907.? ? 4.81? ? 40.3? ?5.36? ? ?7
?8 xd? ? ? ? 7? 905.? ? 2.96? ? 40.5? ?5.04? ? ?8
?9 xqd? ? ? ?8? 903.? ? 0.908? ?40.5? ?4.52? ? ?9
10 xqdi? ? ? 9? 904.? ? 1.89? ? 40.4? ?4.70? ? 10
\end{Soutput}
\end{Schunk}


The problem is the output from tibble?
# A tibble: 10 ? 7


the \times character is Unicode?U+00D7 or hex \xd7 and pdflatex lualatex 
etc fail where this occurs
Is there a way of adding "sanitizing" code in the output before 
compiling?
Or do I have to change it manually before compiling


I do not want to switch to knitr.?


Regards


Duncan Mackay



	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Aug 25 05:25:55 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 24 Aug 2022 20:25:55 -0700
Subject: [R] Unicode chars
In-Reply-To: <3adb4f19.9ce71.182d2f3fec2.Webtop.91@bigpond.com>
References: <3adb4f19.9ce71.182d2f3fec2.Webtop.91@bigpond.com>
Message-ID: <557FD6B7-4725-4C56-8CA6-5F52DA0BB607@dcn.davis.ca.us>

Are you aware that pdfLatex does not support Unicode? You need to use xeLatex. But I don't use Sweave, so I don't know how you go about making that choice.

On August 24, 2022 8:03:02 PM PDT, dulcalma dulcalma <dulcalma at bigpond.com> wrote:
>
>Dear All
>
>
>I was trying the supplementary file?GS_main.R from
>https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.3475
>
>I have tried to prevent latex compilation from failing using Sweave 
>after trying all the online fixes I could find including using Rterm?
>I could fix it if it was in the input but not in the output?
>I am using R version 4.2 on windows 11 with 64 GB memory
>
>
>Sweave code
>
>\begin{small}
><<r0>>=
>library(emdbook) # version 1.3.12
>library(bbmle) # version 1.0.23.1
>library(pbmcapply) # version 1.5.0?
>library(tidyverse) # version 1.3.0
>library(ggpubr) # version 0.4.0
>@ %%
>
>
><<r7>>=
>summaryTable <-
>tibble(model = m.names,
>? ? ? ?dim = m.dims[model],? ? ? ? ? ? ? ? ?
>? ? ? ?score = m.loo[model],? ? ? ? ? ? ? ??
>? ? ? ?delScore = score - min(score),? ? ? ?
>? ? ? ?se_ose = se_ose[model],? ? ? ? ? ? ??
>? ? ? ?se_mod = se_mod[model]) %>% arrange(dim) %>%? mutate(index = 
>1:length(dim))
>summaryTable
>@ %%
>
>
>Output
>\begin{Schunk}
>\begin{Sinput}
>? summaryTable <-
>? tibble(model = m.names,
>? ? ? ? ?dim = m.dims[model],? ? ? ? ? ? ? ? ?
>? ? ? ? ?score = m.loo[model],? ? ? ? ? ? ? ??
>? ? ? ? ?delScore = score - min(score),? ? ? ?
>? ? ? ? ?se_ose = se_ose[model],? ? ? ? ? ? ??
>? ? ? ? ?se_mod = se_mod[model]) %>% arrange(dim) %>%? mutate(index = 
>1:length(dim))
>? summaryTable
>\end{Sinput}
>\begin{Soutput}
># A tibble: 10 ? 7
>? ?model? ?dim score delScore se_ose se_mod index
>? ?<chr> <int> <dbl>? ? <dbl>? <dbl>? <dbl> <int>
>?1 zero? ? ? 2? 908.? ? 5.84? ? 40.1? ?4.14? ? ?1
>?2 d? ? ? ? ?3? 904.? ? 1.71? ? 40.6? ?2.52? ? ?2
>?3 q? ? ? ? ?3? 907.? ? 4.92? ? 40.2? ?3.80? ? ?3
>?4 qd? ? ? ? 4? 902.? ? 0? ? ? ?40.7? ?0? ? ? ? 4
>?5 qdi? ? ? ?5? 903.? ? 0.632? ?40.5? ?1.60? ? ?5
>?6 x? ? ? ? ?6? 908.? ? 5.58? ? 40.2? ?5.53? ? ?6
>?7 xq? ? ? ? 7? 907.? ? 4.81? ? 40.3? ?5.36? ? ?7
>?8 xd? ? ? ? 7? 905.? ? 2.96? ? 40.5? ?5.04? ? ?8
>?9 xqd? ? ? ?8? 903.? ? 0.908? ?40.5? ?4.52? ? ?9
>10 xqdi? ? ? 9? 904.? ? 1.89? ? 40.4? ?4.70? ? 10
>\end{Soutput}
>\end{Schunk}
>
>
>The problem is the output from tibble?
># A tibble: 10 ? 7
>
>
>the \times character is Unicode?U+00D7 or hex \xd7 and pdflatex lualatex 
>etc fail where this occurs
>Is there a way of adding "sanitizing" code in the output before 
>compiling?
>Or do I have to change it manually before compiling
>
>
>I do not want to switch to knitr.?
>
>
>Regards
>
>
>Duncan Mackay
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From e@ @end|ng |rom enr|co@chum@nn@net  Thu Aug 25 09:08:01 2022
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Thu, 25 Aug 2022 09:08:01 +0200
Subject: [R] Unicode chars
In-Reply-To: <3adb4f19.9ce71.182d2f3fec2.Webtop.91@bigpond.com> (dulcalma
 dulcalma's message of "Thu, 25 Aug 2022 13:03:02 +1000 (AEST)")
References: <3adb4f19.9ce71.182d2f3fec2.Webtop.91@bigpond.com>
Message-ID: <87r114er72.fsf@enricoschumann.net>

On Thu, 25 Aug 2022, dulcalma dulcalma writes:

> Dear All
>
>
> I was trying the supplementary file?GS_main.R from
> https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.3475
>
> I have tried to prevent latex compilation from failing using Sweave 
> after trying all the online fixes I could find including using Rterm?
> I could fix it if it was in the input but not in the output?
> I am using R version 4.2 on windows 11 with 64 GB memory
>
>
> Sweave code
>
> \begin{small}
> <<r0>>=
> library(emdbook) # version 1.3.12
> library(bbmle) # version 1.0.23.1
> library(pbmcapply) # version 1.5.0?
> library(tidyverse) # version 1.3.0
> library(ggpubr) # version 0.4.0
> @ %%
>
>
> <<r7>>=
> summaryTable <-
> tibble(model = m.names,
> ? ? ? ?dim = m.dims[model],? ? ? ? ? ? ? ? ?
> ? ? ? ?score = m.loo[model],? ? ? ? ? ? ? ??
> ? ? ? ?delScore = score - min(score),? ? ? ?
> ? ? ? ?se_ose = se_ose[model],? ? ? ? ? ? ??
> ? ? ? ?se_mod = se_mod[model]) %>% arrange(dim) %>%? mutate(index = 
> 1:length(dim))
> summaryTable
> @ %%
>
>
> Output
> \begin{Schunk}
> \begin{Sinput}
> ? summaryTable <-
> ? tibble(model = m.names,
> ? ? ? ? ?dim = m.dims[model],? ? ? ? ? ? ? ? ?
> ? ? ? ? ?score = m.loo[model],? ? ? ? ? ? ? ??
> ? ? ? ? ?delScore = score - min(score),? ? ? ?
> ? ? ? ? ?se_ose = se_ose[model],? ? ? ? ? ? ??
> ? ? ? ? ?se_mod = se_mod[model]) %>% arrange(dim) %>%? mutate(index = 
> 1:length(dim))
> ? summaryTable
> \end{Sinput}
> \begin{Soutput}
> # A tibble: 10 ? 7
> ? ?model? ?dim score delScore se_ose se_mod index
> ? ?<chr> <int> <dbl>? ? <dbl>? <dbl>? <dbl> <int>
> ?1 zero? ? ? 2? 908.? ? 5.84? ? 40.1? ?4.14? ? ?1
> ?2 d? ? ? ? ?3? 904.? ? 1.71? ? 40.6? ?2.52? ? ?2
> ?3 q? ? ? ? ?3? 907.? ? 4.92? ? 40.2? ?3.80? ? ?3
> ?4 qd? ? ? ? 4? 902.? ? 0? ? ? ?40.7? ?0? ? ? ? 4
> ?5 qdi? ? ? ?5? 903.? ? 0.632? ?40.5? ?1.60? ? ?5
> ?6 x? ? ? ? ?6? 908.? ? 5.58? ? 40.2? ?5.53? ? ?6
> ?7 xq? ? ? ? 7? 907.? ? 4.81? ? 40.3? ?5.36? ? ?7
> ?8 xd? ? ? ? 7? 905.? ? 2.96? ? 40.5? ?5.04? ? ?8
> ?9 xqd? ? ? ?8? 903.? ? 0.908? ?40.5? ?4.52? ? ?9
> 10 xqdi? ? ? 9? 904.? ? 1.89? ? 40.4? ?4.70? ? 10
> \end{Soutput}
> \end{Schunk}
>
>
> The problem is the output from tibble?
> # A tibble: 10 ? 7
>
>
> the \times character is Unicode?U+00D7 or hex \xd7 and pdflatex lualatex 
> etc fail where this occurs
> Is there a way of adding "sanitizing" code in the output before 
> compiling?
> Or do I have to change it manually before compiling
>
>
> I do not want to switch to knitr.?
>
>
> Regards
>
>
> Duncan Mackay
>

You could try to automatically clean the code, by using
?iconv, say. But the results by not be satisfactory,
depending on what characters were used.

Sweave itself does not compile the LaTeX code. If you
run (in R) 

    Sweave(<filename.Rnw>, encoding = "utf8")

then it will produce the TeX file, which you can then
compile via LuaLaTeX or XeLaTeX [see
e.g. https://www.ctan.org/pkg/lualatex-doc].

For instance, on the command line, just say

    lualatex <filename.tex>

or another programme (such as latexmk) that your TeX
distribution provides.


If this is a vignette, you can specify a Makefile, see
https://cran.r-project.org/doc/manuals/R-exts.html#Writing-package-vignettes



>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Aug 25 09:22:04 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 25 Aug 2022 08:22:04 +0100
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
Message-ID: <d7c8ab7c-7a67-d178-c14a-e67195f1a780@sapo.pt>

Hello,

OK, what about


res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
do.call(rbind, res)
#         Code  Y  M  D     Q    N    O
#  41003 41003 81  1 19 0.160 7.17 2.50
#  41005 41005 79  8 17 0.210 5.50 7.20
#  41009 41009 79  2 21 0.218 5.56 4.04
#  41017 41017 79 10 20 0.240 5.30 7.10


A dplyr solution.



suppressPackageStartupMessages(library(dplyr))

df1 %>%
   group_by(Code) %>%
   slice_min(Q) %>%
   slice_head(n = 1)
#  # A tibble: 4 ? 7
#  # Groups:   Code [4]
#    Code      Y     M     D     Q     N     O
#    <fct> <int> <int> <int> <dbl> <dbl> <dbl>
#  1 41003    81     1    19 0.16   7.17  2.5
#  2 41005    79     8    17 0.21   5.5   7.2
#  3 41009    79     2    21 0.218  5.56  4.04
#  4 41017    79    10    20 0.24   5.3   7.1



Hope this helps,

Rui Barradas


?s 05:56 de 25/08/2022, javad bayat escreveu:
> Dear all,
> Many thanks for your suggested methods and codes, but unfortunately they
> did not give the desired results.
> All the codes you have provided are correct but they did not represent the
> whole row which is related to the minimum of "Q".
> The code must result in 4 rows, with the minimum value of "Q" and other
> column values, as below:
> 
>         Code
> 
>                Y
> 
>                M
> 
>                 D
> 
>             Q
> 
>              N
> 
>               O
> 
> 41003
> 
> 81
> 
> 1
> 
> 19
> 
> 0.16
> 
> 7.17
> 
> 2.5
> 
> 41005
> 
> 79
> 
> 8
> 
> 17
> 
> 0.21
> 
> 5.5
> 
> 7.2
> 
> 41009
> 
> 79
> 
> 2
> 
> 21
> 
> 0.218
> 
> 5.56
> 
> 4.04
> 41017 79 10 20 0.24 5.3 7.1
> 
> 
> 
> 
> 
> 
> Sincerely
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 41017 79 10 20 0.24 5.3 7.1
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> 
>> Hello,
>>
>> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
>>
>>
>> x<-'41003 81 1 19 0.16 7.17 2.5
>> 41003 77 9 22 0.197 6.8 2.2
>> 41003 79 7 28 0.21 4.7 6.2
>> 41005 79 8 17 0.21 5.5 7.2
>> 41005 80 10 30 0.21 6.84 2.6
>> 41005 80 12 20 0.21 6.84 2.4
>> 41005 79 6 14 0.217 5.61 3.55
>> 41009 79 2 21 0.218 5.56 4.04
>> 41009 79 5 27 0.218 6.4 3.12
>> 41009 80 11 29 0.22 6.84 2.8
>> 41009 78 5 28 0.232 6 3.2
>> 41009 81 8 20 0.233 6.39 1.6
>> 41009 79 9 30 0.24 5.6 7.5
>> 41017 79 10 20 0.24 5.3 7.1
>> 41017 80 7 30 0.24 6.73 2.6'
>> df1 <- read.table(textConnection(x))
>> names(df1) <- scan(what = character(),
>>                      text = 'Code Y M D Q N O')
>> df1$Code <- factor(df1$Code)
>>
>> # 1st option
>> with(df1, tapply(Q, Code, min))
>> #  41003 41005 41009 41017
>> #  0.160 0.210 0.218 0.240
>>
>> # 2nd option
>> aggregate(Q ~ Code, df1, min)
>> #     Code     Q
>> #  1 41003 0.160
>> #  2 41005 0.210
>> #  3 41009 0.218
>> #  4 41017 0.240
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 08:44 de 24/08/2022, javad bayat escreveu:
>>> Dear all;
>>> I am trying to get the minimum value of a column based on a factor column
>>> of the same data frame. My data frame is like the below:
>>>          Code               Y               M                D
>>>    Q
>>>        N              O
>>> 41003 81 1 19 0.16 7.17 2.5
>>> 41003 77 9 22 0.197 6.8 2.2
>>> 41003 79 7 28 0.21 4.7 6.2
>>> 41005 79 8 17 0.21 5.5 7.2
>>> 41005 80 10 30 0.21 6.84 2.6
>>> 41005 80 12 20 0.21 6.84 2.4
>>> 41005 79 6 14 0.217 5.61 3.55
>>> 41009 79 2 21 0.218 5.56 4.04
>>> 41009 79 5 27 0.218 6.4 3.12
>>> 41009 80 11 29 0.22 6.84 2.8
>>> 41009 78 5 28 0.232 6 3.2
>>> 41009 81 8 20 0.233 6.39 1.6
>>> 41009 79 9 30 0.24 5.6 7.5
>>> 41017 79 10 20 0.24 5.3 7.1
>>> 41017 80 7 30 0.24 6.73 2.6
>>>
>>> I want to get the minimum value of the "Q" column with the whole row
>>> values, according to the "Code"  column  which is a factor. Overall it
>> will
>>> give me 4 rows, with the value of "Q". Below is a code that I used but it
>>> did not give me what I wanted.
>>>
>>>> x[which(x$Q == min(x$Q)),]
>>>
>>> Sincerely
>>>
>>>
>>>
>>
> 
>


From j@b@y@t194 @end|ng |rom gm@||@com  Thu Aug 25 06:56:27 2022
From: j@b@y@t194 @end|ng |rom gm@||@com (javad bayat)
Date: Thu, 25 Aug 2022 09:26:27 +0430
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
Message-ID: <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>

Dear all,
Many thanks for your suggested methods and codes, but unfortunately they
did not give the desired results.
All the codes you have provided are correct but they did not represent the
whole row which is related to the minimum of "Q".
The code must result in 4 rows, with the minimum value of "Q" and other
column values, as below:

       Code

              Y

              M

               D

           Q

            N

             O

41003

81

1

19

0.16

7.17

2.5

41005

79

8

17

0.21

5.5

7.2

41009

79

2

21

0.218

5.56

4.04
41017 79 10 20 0.24 5.3 7.1






Sincerely



























































































41017 79 10 20 0.24 5.3 7.1















On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
>
>
> x<-'41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6'
> df1 <- read.table(textConnection(x))
> names(df1) <- scan(what = character(),
>                     text = 'Code Y M D Q N O')
> df1$Code <- factor(df1$Code)
>
> # 1st option
> with(df1, tapply(Q, Code, min))
> #  41003 41005 41009 41017
> #  0.160 0.210 0.218 0.240
>
> # 2nd option
> aggregate(Q ~ Code, df1, min)
> #     Code     Q
> #  1 41003 0.160
> #  2 41005 0.210
> #  3 41009 0.218
> #  4 41017 0.240
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> > Dear all;
> > I am trying to get the minimum value of a column based on a factor column
> > of the same data frame. My data frame is like the below:
> >         Code               Y               M                D
> >   Q
> >       N              O
> > 41003 81 1 19 0.16 7.17 2.5
> > 41003 77 9 22 0.197 6.8 2.2
> > 41003 79 7 28 0.21 4.7 6.2
> > 41005 79 8 17 0.21 5.5 7.2
> > 41005 80 10 30 0.21 6.84 2.6
> > 41005 80 12 20 0.21 6.84 2.4
> > 41005 79 6 14 0.217 5.61 3.55
> > 41009 79 2 21 0.218 5.56 4.04
> > 41009 79 5 27 0.218 6.4 3.12
> > 41009 80 11 29 0.22 6.84 2.8
> > 41009 78 5 28 0.232 6 3.2
> > 41009 81 8 20 0.233 6.39 1.6
> > 41009 79 9 30 0.24 5.6 7.5
> > 41017 79 10 20 0.24 5.3 7.1
> > 41017 80 7 30 0.24 6.73 2.6
> >
> > I want to get the minimum value of the "Q" column with the whole row
> > values, according to the "Code"  column  which is a factor. Overall it
> will
> > give me 4 rows, with the value of "Q". Below is a code that I used but it
> > did not give me what I wanted.
> >
> >> x[which(x$Q == min(x$Q)),]
> >
> > Sincerely
> >
> >
> >
>


-- 
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

	[[alternative HTML version deleted]]


From j@b@y@t194 @end|ng |rom gm@||@com  Thu Aug 25 09:53:09 2022
From: j@b@y@t194 @end|ng |rom gm@||@com (javad bayat)
Date: Thu, 25 Aug 2022 12:23:09 +0430
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <d7c8ab7c-7a67-d178-c14a-e67195f1a780@sapo.pt>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <d7c8ab7c-7a67-d178-c14a-e67195f1a780@sapo.pt>
Message-ID: <CANTxAm+VWvB7BV7YBy7e_Bh_HZD7CgcJiC=FwHYMo1+7rf_=jA@mail.gmail.com>

Dear Rui;
Thank you very much. Both of your codes worked correctly. Now I can see the
whole row's value.
But I found a problem in the results. When I run your codes, the results
are shown in a sorted table. I do not know why the results have been sorted
according to the "Code" column, smallest to largest. Is there any way to
get the results like their order in the first data frame (bilan2)? I used
your codes as follow:

> bilan3 <- lapply(split(bilan2, bilan2$Code), \(x) x[which.min(x$Q),])
> bilan3 = data.frame(do.call(rbind, bilan3))
Sincerely



On Thu, Aug 25, 2022 at 11:52 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> OK, what about
>
>
> res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
> do.call(rbind, res)
> #         Code  Y  M  D     Q    N    O
> #  41003 41003 81  1 19 0.160 7.17 2.50
> #  41005 41005 79  8 17 0.210 5.50 7.20
> #  41009 41009 79  2 21 0.218 5.56 4.04
> #  41017 41017 79 10 20 0.240 5.30 7.10
>
>
> A dplyr solution.
>
>
>
> suppressPackageStartupMessages(library(dplyr))
>
> df1 %>%
>    group_by(Code) %>%
>    slice_min(Q) %>%
>    slice_head(n = 1)
> #  # A tibble: 4 ? 7
> #  # Groups:   Code [4]
> #    Code      Y     M     D     Q     N     O
> #    <fct> <int> <int> <int> <dbl> <dbl> <dbl>
> #  1 41003    81     1    19 0.16   7.17  2.5
> #  2 41005    79     8    17 0.21   5.5   7.2
> #  3 41009    79     2    21 0.218  5.56  4.04
> #  4 41017    79    10    20 0.24   5.3   7.1
>
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 05:56 de 25/08/2022, javad bayat escreveu:
> > Dear all,
> > Many thanks for your suggested methods and codes, but unfortunately they
> > did not give the desired results.
> > All the codes you have provided are correct but they did not represent
> the
> > whole row which is related to the minimum of "Q".
> > The code must result in 4 rows, with the minimum value of "Q" and other
> > column values, as below:
> >
> >         Code
> >
> >                Y
> >
> >                M
> >
> >                 D
> >
> >             Q
> >
> >              N
> >
> >               O
> >
> > 41003
> >
> > 81
> >
> > 1
> >
> > 19
> >
> > 0.16
> >
> > 7.17
> >
> > 2.5
> >
> > 41005
> >
> > 79
> >
> > 8
> >
> > 17
> >
> > 0.21
> >
> > 5.5
> >
> > 7.2
> >
> > 41009
> >
> > 79
> >
> > 2
> >
> > 21
> >
> > 0.218
> >
> > 5.56
> >
> > 4.04
> > 41017 79 10 20 0.24 5.3 7.1
> >
> >
> >
> >
> >
> >
> > Sincerely
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > 41017 79 10 20 0.24 5.3 7.1
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt>
> wrote:
> >
> >> Hello,
> >>
> >> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
> >>
> >>
> >> x<-'41003 81 1 19 0.16 7.17 2.5
> >> 41003 77 9 22 0.197 6.8 2.2
> >> 41003 79 7 28 0.21 4.7 6.2
> >> 41005 79 8 17 0.21 5.5 7.2
> >> 41005 80 10 30 0.21 6.84 2.6
> >> 41005 80 12 20 0.21 6.84 2.4
> >> 41005 79 6 14 0.217 5.61 3.55
> >> 41009 79 2 21 0.218 5.56 4.04
> >> 41009 79 5 27 0.218 6.4 3.12
> >> 41009 80 11 29 0.22 6.84 2.8
> >> 41009 78 5 28 0.232 6 3.2
> >> 41009 81 8 20 0.233 6.39 1.6
> >> 41009 79 9 30 0.24 5.6 7.5
> >> 41017 79 10 20 0.24 5.3 7.1
> >> 41017 80 7 30 0.24 6.73 2.6'
> >> df1 <- read.table(textConnection(x))
> >> names(df1) <- scan(what = character(),
> >>                      text = 'Code Y M D Q N O')
> >> df1$Code <- factor(df1$Code)
> >>
> >> # 1st option
> >> with(df1, tapply(Q, Code, min))
> >> #  41003 41005 41009 41017
> >> #  0.160 0.210 0.218 0.240
> >>
> >> # 2nd option
> >> aggregate(Q ~ Code, df1, min)
> >> #     Code     Q
> >> #  1 41003 0.160
> >> #  2 41005 0.210
> >> #  3 41009 0.218
> >> #  4 41017 0.240
> >>
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> >>> Dear all;
> >>> I am trying to get the minimum value of a column based on a factor
> column
> >>> of the same data frame. My data frame is like the below:
> >>>          Code               Y               M                D
> >>>    Q
> >>>        N              O
> >>> 41003 81 1 19 0.16 7.17 2.5
> >>> 41003 77 9 22 0.197 6.8 2.2
> >>> 41003 79 7 28 0.21 4.7 6.2
> >>> 41005 79 8 17 0.21 5.5 7.2
> >>> 41005 80 10 30 0.21 6.84 2.6
> >>> 41005 80 12 20 0.21 6.84 2.4
> >>> 41005 79 6 14 0.217 5.61 3.55
> >>> 41009 79 2 21 0.218 5.56 4.04
> >>> 41009 79 5 27 0.218 6.4 3.12
> >>> 41009 80 11 29 0.22 6.84 2.8
> >>> 41009 78 5 28 0.232 6 3.2
> >>> 41009 81 8 20 0.233 6.39 1.6
> >>> 41009 79 9 30 0.24 5.6 7.5
> >>> 41017 79 10 20 0.24 5.3 7.1
> >>> 41017 80 7 30 0.24 6.73 2.6
> >>>
> >>> I want to get the minimum value of the "Q" column with the whole row
> >>> values, according to the "Code"  column  which is a factor. Overall it
> >> will
> >>> give me 4 rows, with the value of "Q". Below is a code that I used but
> it
> >>> did not give me what I wanted.
> >>>
> >>>> x[which(x$Q == min(x$Q)),]
> >>>
> >>> Sincerely
> >>>
> >>>
> >>>
> >>
> >
> >
>


-- 
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Thu Aug 25 13:22:25 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 25 Aug 2022 11:22:25 +0000
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAm+VWvB7BV7YBy7e_Bh_HZD7CgcJiC=FwHYMo1+7rf_=jA@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <d7c8ab7c-7a67-d178-c14a-e67195f1a780@sapo.pt>
 <CANTxAm+VWvB7BV7YBy7e_Bh_HZD7CgcJiC=FwHYMo1+7rf_=jA@mail.gmail.com>
Message-ID: <BN6PR2201MB1553D006F68FADE23EC4A822CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>

The data are sorted by the variable(s) in the group_by() statement. If the variable in the group_by() statement is not correct then you can change it to whatever the correct variable may be. If the calculations are correct, but you want a different sort order then arrange() (from dplyr) would work as would order() and sort() commands. There are many help pages on the web for sorting vectors and dataframes.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of javad bayat
Sent: Thursday, August 25, 2022 3:53 AM
To: Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

Dear Rui;
Thank you very much. Both of your codes worked correctly. Now I can see the whole row's value.
But I found a problem in the results. When I run your codes, the results are shown in a sorted table. I do not know why the results have been sorted according to the "Code" column, smallest to largest. Is there any way to get the results like their order in the first data frame (bilan2)? I used your codes as follow:

> bilan3 <- lapply(split(bilan2, bilan2$Code), \(x) x[which.min(x$Q),])
> bilan3 = data.frame(do.call(rbind, bilan3))
Sincerely



On Thu, Aug 25, 2022 at 11:52 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> OK, what about
>
>
> res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),]) 
> do.call(rbind, res)
> #         Code  Y  M  D     Q    N    O
> #  41003 41003 81  1 19 0.160 7.17 2.50 #  41005 41005 79  8 17 0.210 
> 5.50 7.20 #  41009 41009 79  2 21 0.218 5.56 4.04 #  41017 41017 79 10 
> 20 0.240 5.30 7.10
>
>
> A dplyr solution.
>
>
>
> suppressPackageStartupMessages(library(dplyr))
>
> df1 %>%
>    group_by(Code) %>%
>    slice_min(Q) %>%
>    slice_head(n = 1)
> #  # A tibble: 4 ? 7
> #  # Groups:   Code [4]
> #    Code      Y     M     D     Q     N     O
> #    <fct> <int> <int> <int> <dbl> <dbl> <dbl>
> #  1 41003    81     1    19 0.16   7.17  2.5
> #  2 41005    79     8    17 0.21   5.5   7.2
> #  3 41009    79     2    21 0.218  5.56  4.04
> #  4 41017    79    10    20 0.24   5.3   7.1
>
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 05:56 de 25/08/2022, javad bayat escreveu:
> > Dear all,
> > Many thanks for your suggested methods and codes, but unfortunately 
> > they did not give the desired results.
> > All the codes you have provided are correct but they did not 
> > represent
> the
> > whole row which is related to the minimum of "Q".
> > The code must result in 4 rows, with the minimum value of "Q" and 
> > other column values, as below:
> >
> >         Code
> >
> >                Y
> >
> >                M
> >
> >                 D
> >
> >             Q
> >
> >              N
> >
> >               O
> >
> > 41003
> >
> > 81
> >
> > 1
> >
> > 19
> >
> > 0.16
> >
> > 7.17
> >
> > 2.5
> >
> > 41005
> >
> > 79
> >
> > 8
> >
> > 17
> >
> > 0.21
> >
> > 5.5
> >
> > 7.2
> >
> > 41009
> >
> > 79
> >
> > 2
> >
> > 21
> >
> > 0.218
> >
> > 5.56
> >
> > 4.04
> > 41017 79 10 20 0.24 5.3 7.1
> >
> >
> >
> >
> >
> >
> > Sincerely
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > 41017 79 10 20 0.24 5.3 7.1
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt>
> wrote:
> >
> >> Hello,
> >>
> >> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
> >>
> >>
> >> x<-'41003 81 1 19 0.16 7.17 2.5
> >> 41003 77 9 22 0.197 6.8 2.2
> >> 41003 79 7 28 0.21 4.7 6.2
> >> 41005 79 8 17 0.21 5.5 7.2
> >> 41005 80 10 30 0.21 6.84 2.6
> >> 41005 80 12 20 0.21 6.84 2.4
> >> 41005 79 6 14 0.217 5.61 3.55
> >> 41009 79 2 21 0.218 5.56 4.04
> >> 41009 79 5 27 0.218 6.4 3.12
> >> 41009 80 11 29 0.22 6.84 2.8
> >> 41009 78 5 28 0.232 6 3.2
> >> 41009 81 8 20 0.233 6.39 1.6
> >> 41009 79 9 30 0.24 5.6 7.5
> >> 41017 79 10 20 0.24 5.3 7.1
> >> 41017 80 7 30 0.24 6.73 2.6'
> >> df1 <- read.table(textConnection(x))
> >> names(df1) <- scan(what = character(),
> >>                      text = 'Code Y M D Q N O') df1$Code <- 
> >> factor(df1$Code)
> >>
> >> # 1st option
> >> with(df1, tapply(Q, Code, min))
> >> #  41003 41005 41009 41017
> >> #  0.160 0.210 0.218 0.240
> >>
> >> # 2nd option
> >> aggregate(Q ~ Code, df1, min)
> >> #     Code     Q
> >> #  1 41003 0.160
> >> #  2 41005 0.210
> >> #  3 41009 0.218
> >> #  4 41017 0.240
> >>
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> >>> Dear all;
> >>> I am trying to get the minimum value of a column based on a factor
> column
> >>> of the same data frame. My data frame is like the below:
> >>>          Code               Y               M                D
> >>>    Q
> >>>        N              O
> >>> 41003 81 1 19 0.16 7.17 2.5
> >>> 41003 77 9 22 0.197 6.8 2.2
> >>> 41003 79 7 28 0.21 4.7 6.2
> >>> 41005 79 8 17 0.21 5.5 7.2
> >>> 41005 80 10 30 0.21 6.84 2.6
> >>> 41005 80 12 20 0.21 6.84 2.4
> >>> 41005 79 6 14 0.217 5.61 3.55
> >>> 41009 79 2 21 0.218 5.56 4.04
> >>> 41009 79 5 27 0.218 6.4 3.12
> >>> 41009 80 11 29 0.22 6.84 2.8
> >>> 41009 78 5 28 0.232 6 3.2
> >>> 41009 81 8 20 0.233 6.39 1.6
> >>> 41009 79 9 30 0.24 5.6 7.5
> >>> 41017 79 10 20 0.24 5.3 7.1
> >>> 41017 80 7 30 0.24 6.73 2.6
> >>>
> >>> I want to get the minimum value of the "Q" column with the whole 
> >>> row values, according to the "Code"  column  which is a factor. 
> >>> Overall it
> >> will
> >>> give me 4 rows, with the value of "Q". Below is a code that I used 
> >>> but
> it
> >>> did not give me what I wanted.
> >>>
> >>>> x[which(x$Q == min(x$Q)),]
> >>>
> >>> Sincerely
> >>>
> >>>
> >>>
> >>
> >
> >
>


--
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C50403ccd882345d00db908da8689bacb%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970223349266197%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=OrqS9I1qav2ztgWpW6BtMkFU9wa0s8dMrVAEP834%2FaU%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C50403ccd882345d00db908da8689bacb%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970223349266197%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=EzG74%2BCS%2FF2ghpT9FKy%2BDm2Qf%2FAARRCJSXxmYBoqoIQ%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Aug 25 13:44:25 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 25 Aug 2022 12:44:25 +0100
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAm+VWvB7BV7YBy7e_Bh_HZD7CgcJiC=FwHYMo1+7rf_=jA@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <d7c8ab7c-7a67-d178-c14a-e67195f1a780@sapo.pt>
 <CANTxAm+VWvB7BV7YBy7e_Bh_HZD7CgcJiC=FwHYMo1+7rf_=jA@mail.gmail.com>
Message-ID: <79090873-424a-6c92-c5f7-f25a77be1d01@sapo.pt>

Hello,

To keep the original order, try


res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
res <- do.call(rbind, res)
i <- order(unique(df1$Code))
res[order(i), ]


Hope this helps,

Rui Barradas

?s 08:53 de 25/08/2022, javad bayat escreveu:
> Dear Rui;
> Thank you very much. Both of your codes worked correctly. Now I can see the
> whole row's value.
> But I found a problem in the results. When I run your codes, the results
> are shown in a sorted table. I do not know why the results have been sorted
> according to the "Code" column, smallest to largest. Is there any way to
> get the results like their order in the first data frame (bilan2)? I used
> your codes as follow:
> 
>> bilan3 <- lapply(split(bilan2, bilan2$Code), \(x) x[which.min(x$Q),])
>> bilan3 = data.frame(do.call(rbind, bilan3))
> Sincerely
> 
> 
> 
> On Thu, Aug 25, 2022 at 11:52 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> 
>> Hello,
>>
>> OK, what about
>>
>>
>> res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
>> do.call(rbind, res)
>> #         Code  Y  M  D     Q    N    O
>> #  41003 41003 81  1 19 0.160 7.17 2.50
>> #  41005 41005 79  8 17 0.210 5.50 7.20
>> #  41009 41009 79  2 21 0.218 5.56 4.04
>> #  41017 41017 79 10 20 0.240 5.30 7.10
>>
>>
>> A dplyr solution.
>>
>>
>>
>> suppressPackageStartupMessages(library(dplyr))
>>
>> df1 %>%
>>     group_by(Code) %>%
>>     slice_min(Q) %>%
>>     slice_head(n = 1)
>> #  # A tibble: 4 ? 7
>> #  # Groups:   Code [4]
>> #    Code      Y     M     D     Q     N     O
>> #    <fct> <int> <int> <int> <dbl> <dbl> <dbl>
>> #  1 41003    81     1    19 0.16   7.17  2.5
>> #  2 41005    79     8    17 0.21   5.5   7.2
>> #  3 41009    79     2    21 0.218  5.56  4.04
>> #  4 41017    79    10    20 0.24   5.3   7.1
>>
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>>
>> ?s 05:56 de 25/08/2022, javad bayat escreveu:
>>> Dear all,
>>> Many thanks for your suggested methods and codes, but unfortunately they
>>> did not give the desired results.
>>> All the codes you have provided are correct but they did not represent
>> the
>>> whole row which is related to the minimum of "Q".
>>> The code must result in 4 rows, with the minimum value of "Q" and other
>>> column values, as below:
>>>
>>>          Code
>>>
>>>                 Y
>>>
>>>                 M
>>>
>>>                  D
>>>
>>>              Q
>>>
>>>               N
>>>
>>>                O
>>>
>>> 41003
>>>
>>> 81
>>>
>>> 1
>>>
>>> 19
>>>
>>> 0.16
>>>
>>> 7.17
>>>
>>> 2.5
>>>
>>> 41005
>>>
>>> 79
>>>
>>> 8
>>>
>>> 17
>>>
>>> 0.21
>>>
>>> 5.5
>>>
>>> 7.2
>>>
>>> 41009
>>>
>>> 79
>>>
>>> 2
>>>
>>> 21
>>>
>>> 0.218
>>>
>>> 5.56
>>>
>>> 4.04
>>> 41017 79 10 20 0.24 5.3 7.1
>>>
>>>
>>>
>>>
>>>
>>>
>>> Sincerely
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> 41017 79 10 20 0.24 5.3 7.1
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt>
>> wrote:
>>>
>>>> Hello,
>>>>
>>>> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
>>>>
>>>>
>>>> x<-'41003 81 1 19 0.16 7.17 2.5
>>>> 41003 77 9 22 0.197 6.8 2.2
>>>> 41003 79 7 28 0.21 4.7 6.2
>>>> 41005 79 8 17 0.21 5.5 7.2
>>>> 41005 80 10 30 0.21 6.84 2.6
>>>> 41005 80 12 20 0.21 6.84 2.4
>>>> 41005 79 6 14 0.217 5.61 3.55
>>>> 41009 79 2 21 0.218 5.56 4.04
>>>> 41009 79 5 27 0.218 6.4 3.12
>>>> 41009 80 11 29 0.22 6.84 2.8
>>>> 41009 78 5 28 0.232 6 3.2
>>>> 41009 81 8 20 0.233 6.39 1.6
>>>> 41009 79 9 30 0.24 5.6 7.5
>>>> 41017 79 10 20 0.24 5.3 7.1
>>>> 41017 80 7 30 0.24 6.73 2.6'
>>>> df1 <- read.table(textConnection(x))
>>>> names(df1) <- scan(what = character(),
>>>>                       text = 'Code Y M D Q N O')
>>>> df1$Code <- factor(df1$Code)
>>>>
>>>> # 1st option
>>>> with(df1, tapply(Q, Code, min))
>>>> #  41003 41005 41009 41017
>>>> #  0.160 0.210 0.218 0.240
>>>>
>>>> # 2nd option
>>>> aggregate(Q ~ Code, df1, min)
>>>> #     Code     Q
>>>> #  1 41003 0.160
>>>> #  2 41005 0.210
>>>> #  3 41009 0.218
>>>> #  4 41017 0.240
>>>>
>>>>
>>>> Hope this helps,
>>>>
>>>> Rui Barradas
>>>>
>>>> ?s 08:44 de 24/08/2022, javad bayat escreveu:
>>>>> Dear all;
>>>>> I am trying to get the minimum value of a column based on a factor
>> column
>>>>> of the same data frame. My data frame is like the below:
>>>>>           Code               Y               M                D
>>>>>     Q
>>>>>         N              O
>>>>> 41003 81 1 19 0.16 7.17 2.5
>>>>> 41003 77 9 22 0.197 6.8 2.2
>>>>> 41003 79 7 28 0.21 4.7 6.2
>>>>> 41005 79 8 17 0.21 5.5 7.2
>>>>> 41005 80 10 30 0.21 6.84 2.6
>>>>> 41005 80 12 20 0.21 6.84 2.4
>>>>> 41005 79 6 14 0.217 5.61 3.55
>>>>> 41009 79 2 21 0.218 5.56 4.04
>>>>> 41009 79 5 27 0.218 6.4 3.12
>>>>> 41009 80 11 29 0.22 6.84 2.8
>>>>> 41009 78 5 28 0.232 6 3.2
>>>>> 41009 81 8 20 0.233 6.39 1.6
>>>>> 41009 79 9 30 0.24 5.6 7.5
>>>>> 41017 79 10 20 0.24 5.3 7.1
>>>>> 41017 80 7 30 0.24 6.73 2.6
>>>>>
>>>>> I want to get the minimum value of the "Q" column with the whole row
>>>>> values, according to the "Code"  column  which is a factor. Overall it
>>>> will
>>>>> give me 4 rows, with the value of "Q". Below is a code that I used but
>> it
>>>>> did not give me what I wanted.
>>>>>
>>>>>> x[which(x$Q == min(x$Q)),]
>>>>>
>>>>> Sincerely
>>>>>
>>>>>
>>>>>
>>>>
>>>
>>>
>>
> 
>


From tebert @end|ng |rom u||@edu  Thu Aug 25 14:17:53 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 25 Aug 2022 12:17:53 +0000
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
Message-ID: <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>

I missed where you explained how to choose a minimum value if there are several values within a group that are equal to the minimum value. Here is a dplyr code that returns eight values because there are ties for minimum values in Q.

library(dplyr)
library(magrittr)
dat2<-read.table(text="Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6", header=TRUE)
dat2 <- dat2 %>%
  group_by(Code) %>%
  mutate(
    MinByCodeQ = min(Q, na.rm = T),
  ) %>%
  arrange(Code)
dat2<-filter(dat2,Q==MinByCodeQ)



Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of javad bayat
Sent: Thursday, August 25, 2022 12:56 AM
To: Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

Dear all,
Many thanks for your suggested methods and codes, but unfortunately they did not give the desired results.
All the codes you have provided are correct but they did not represent the whole row which is related to the minimum of "Q".
The code must result in 4 rows, with the minimum value of "Q" and other column values, as below:

       Code

              Y

              M

               D

           Q

            N

             O

41003

81

1

19

0.16

7.17

2.5

41005

79

8

17

0.21

5.5

7.2

41009

79

2

21

0.218

5.56

4.04
41017 79 10 20 0.24 5.3 7.1






Sincerely



























































































41017 79 10 20 0.24 5.3 7.1















On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
>
>
> x<-'41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6'
> df1 <- read.table(textConnection(x))
> names(df1) <- scan(what = character(),
>                     text = 'Code Y M D Q N O') df1$Code <- 
> factor(df1$Code)
>
> # 1st option
> with(df1, tapply(Q, Code, min))
> #  41003 41005 41009 41017
> #  0.160 0.210 0.218 0.240
>
> # 2nd option
> aggregate(Q ~ Code, df1, min)
> #     Code     Q
> #  1 41003 0.160
> #  2 41005 0.210
> #  3 41009 0.218
> #  4 41017 0.240
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> > Dear all;
> > I am trying to get the minimum value of a column based on a factor 
> > column of the same data frame. My data frame is like the below:
> >         Code               Y               M                D
> >   Q
> >       N              O
> > 41003 81 1 19 0.16 7.17 2.5
> > 41003 77 9 22 0.197 6.8 2.2
> > 41003 79 7 28 0.21 4.7 6.2
> > 41005 79 8 17 0.21 5.5 7.2
> > 41005 80 10 30 0.21 6.84 2.6
> > 41005 80 12 20 0.21 6.84 2.4
> > 41005 79 6 14 0.217 5.61 3.55
> > 41009 79 2 21 0.218 5.56 4.04
> > 41009 79 5 27 0.218 6.4 3.12
> > 41009 80 11 29 0.22 6.84 2.8
> > 41009 78 5 28 0.232 6 3.2
> > 41009 81 8 20 0.233 6.39 1.6
> > 41009 79 9 30 0.24 5.6 7.5
> > 41017 79 10 20 0.24 5.3 7.1
> > 41017 80 7 30 0.24 6.73 2.6
> >
> > I want to get the minimum value of the "Q" column with the whole row 
> > values, according to the "Code"  column  which is a factor. Overall 
> > it
> will
> > give me 4 rows, with the value of "Q". Below is a code that I used 
> > but it did not give me what I wanted.
> >
> >> x[which(x$Q == min(x$Q)),]
> >
> > Sincerely
> >
> >
> >
>


--
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Caf44a3eea239431cdd9808da8679e392%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970155341721228%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=bSKbIIx0Ce14UZCxV3foH%2FtDjAbc3leysR6Uu6mTSCk%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Caf44a3eea239431cdd9808da8679e392%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970155341721228%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=70hiBxaFp9nl0ihQhqkOnBTxUoOv44l4BabLB%2FtVkTg%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Thu Aug 25 14:28:40 2022
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Fri, 26 Aug 2022 00:28:40 +1200
Subject: [R] Unicode chars
In-Reply-To: <557FD6B7-4725-4C56-8CA6-5F52DA0BB607@dcn.davis.ca.us>
References: <3adb4f19.9ce71.182d2f3fec2.Webtop.91@bigpond.com>
 <557FD6B7-4725-4C56-8CA6-5F52DA0BB607@dcn.davis.ca.us>
Message-ID: <CABcYAdKN+3X6jLRjpx81MXLo-fk=atN+47OkcND17wuKZFMwCQ@mail.gmail.com>

PDFLaTeX does support Latin-1, and this is a Latin-1
character.

On Thu, 25 Aug 2022 at 15:35, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Are you aware that pdfLatex does not support Unicode? You need to use
> xeLatex. But I don't use Sweave, so I don't know how you go about making
> that choice.
>
> On August 24, 2022 8:03:02 PM PDT, dulcalma dulcalma <dulcalma at bigpond.com>
> wrote:
> >
> >Dear All
> >
> >
> >I was trying the supplementary file GS_main.R from
> >https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.3475
> >
> >I have tried to prevent latex compilation from failing using Sweave
> >after trying all the online fixes I could find including using Rterm
> >I could fix it if it was in the input but not in the output
> >I am using R version 4.2 on windows 11 with 64 GB memory
> >
> >
> >Sweave code
> >
> >\begin{small}
> ><<r0>>=
> >library(emdbook) # version 1.3.12
> >library(bbmle) # version 1.0.23.1
> >library(pbmcapply) # version 1.5.0
> >library(tidyverse) # version 1.3.0
> >library(ggpubr) # version 0.4.0
> >@ %%
> >
> >
> ><<r7>>=
> >summaryTable <-
> >tibble(model = m.names,
> >       dim = m.dims[model],
> >       score = m.loo[model],
> >       delScore = score - min(score),
> >       se_ose = se_ose[model],
> >       se_mod = se_mod[model]) %>% arrange(dim) %>%  mutate(index =
> >1:length(dim))
> >summaryTable
> >@ %%
> >
> >
> >Output
> >\begin{Schunk}
> >\begin{Sinput}
> >  summaryTable <-
> >  tibble(model = m.names,
> >         dim = m.dims[model],
> >         score = m.loo[model],
> >         delScore = score - min(score),
> >         se_ose = se_ose[model],
> >         se_mod = se_mod[model]) %>% arrange(dim) %>%  mutate(index =
> >1:length(dim))
> >  summaryTable
> >\end{Sinput}
> >\begin{Soutput}
> ># A tibble: 10 ? 7
> >   model   dim score delScore se_ose se_mod index
> >   <chr> <int> <dbl>    <dbl>  <dbl>  <dbl> <int>
> > 1 zero      2  908.    5.84    40.1   4.14     1
> > 2 d         3  904.    1.71    40.6   2.52     2
> > 3 q         3  907.    4.92    40.2   3.80     3
> > 4 qd        4  902.    0       40.7   0        4
> > 5 qdi       5  903.    0.632   40.5   1.60     5
> > 6 x         6  908.    5.58    40.2   5.53     6
> > 7 xq        7  907.    4.81    40.3   5.36     7
> > 8 xd        7  905.    2.96    40.5   5.04     8
> > 9 xqd       8  903.    0.908   40.5   4.52     9
> >10 xqdi      9  904.    1.89    40.4   4.70    10
> >\end{Soutput}
> >\end{Schunk}
> >
> >
> >The problem is the output from tibble
> ># A tibble: 10 ? 7
> >
> >
> >the \times character is Unicode U+00D7 or hex \xd7 and pdflatex lualatex
> >etc fail where this occurs
> >Is there a way of adding "sanitizing" code in the output before
> >compiling
> >Or do I have to change it manually before compiling
> >
> >
> >I do not want to switch to knitr.
> >
> >
> >Regards
> >
> >
> >Duncan Mackay
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Thu Aug 25 14:27:37 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 25 Aug 2022 12:27:37 +0000
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>

My mistake, I did not change the sort order back to the original order. If you do not like the additional variables they can be dropped using select() or dat2[,-c(RN, MinByCodeQ)] syntax.

library(dplyr)
library(magrittr)
dat2<-read.table(text="Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6", header=TRUE)
dat2$RN <- rownames(dat2)
dat2 <- dat2 %>%
  group_by(Code) %>%
  mutate(
    MinByCodeQ = min(Q, na.rm = T),
  ) %>%
  arrange(Code)
dat2<-filter(dat2,Q==MinByCodeQ)
dat2<-arrange(dat2,as.numeric(RN))

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, August 25, 2022 8:18 AM
To: javad bayat <j.bayat194 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

I missed where you explained how to choose a minimum value if there are several values within a group that are equal to the minimum value. Here is a dplyr code that returns eight values because there are ties for minimum values in Q.

library(dplyr)
library(magrittr)
dat2<-read.table(text="Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6", header=TRUE)
dat2 <- dat2 %>%
  group_by(Code) %>%
  mutate(
    MinByCodeQ = min(Q, na.rm = T),
  ) %>%
  arrange(Code)
dat2<-filter(dat2,Q==MinByCodeQ)



Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of javad bayat
Sent: Thursday, August 25, 2022 12:56 AM
To: Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

Dear all,
Many thanks for your suggested methods and codes, but unfortunately they did not give the desired results.
All the codes you have provided are correct but they did not represent the whole row which is related to the minimum of "Q".
The code must result in 4 rows, with the minimum value of "Q" and other column values, as below:

       Code

              Y

              M

               D

           Q

            N

             O

41003

81

1

19

0.16

7.17

2.5

41005

79

8

17

0.21

5.5

7.2

41009

79

2

21

0.218

5.56

4.04
41017 79 10 20 0.24 5.3 7.1






Sincerely



























































































41017 79 10 20 0.24 5.3 7.1















On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
>
>
> x<-'41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6'
> df1 <- read.table(textConnection(x))
> names(df1) <- scan(what = character(),
>                     text = 'Code Y M D Q N O') df1$Code <-
> factor(df1$Code)
>
> # 1st option
> with(df1, tapply(Q, Code, min))
> #  41003 41005 41009 41017
> #  0.160 0.210 0.218 0.240
>
> # 2nd option
> aggregate(Q ~ Code, df1, min)
> #     Code     Q
> #  1 41003 0.160
> #  2 41005 0.210
> #  3 41009 0.218
> #  4 41017 0.240
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> > Dear all;
> > I am trying to get the minimum value of a column based on a factor 
> > column of the same data frame. My data frame is like the below:
> >         Code               Y               M                D
> >   Q
> >       N              O
> > 41003 81 1 19 0.16 7.17 2.5
> > 41003 77 9 22 0.197 6.8 2.2
> > 41003 79 7 28 0.21 4.7 6.2
> > 41005 79 8 17 0.21 5.5 7.2
> > 41005 80 10 30 0.21 6.84 2.6
> > 41005 80 12 20 0.21 6.84 2.4
> > 41005 79 6 14 0.217 5.61 3.55
> > 41009 79 2 21 0.218 5.56 4.04
> > 41009 79 5 27 0.218 6.4 3.12
> > 41009 80 11 29 0.22 6.84 2.8
> > 41009 78 5 28 0.232 6 3.2
> > 41009 81 8 20 0.233 6.39 1.6
> > 41009 79 9 30 0.24 5.6 7.5
> > 41017 79 10 20 0.24 5.3 7.1
> > 41017 80 7 30 0.24 6.73 2.6
> >
> > I want to get the minimum value of the "Q" column with the whole row 
> > values, according to the "Code"  column  which is a factor. Overall 
> > it
> will
> > give me 4 rows, with the value of "Q". Below is a code that I used 
> > but it did not give me what I wanted.
> >
> >> x[which(x$Q == min(x$Q)),]
> >
> > Sincerely
> >
> >
> >
>


--
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GvFzbrCZcSiitmSGdaeuB9A7I2nVYMILrjYsXOMhX8g%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N9LufwUGQWGKUaPqZ2UYi7yFm32mFp%2BivWCUzKFOTtw%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GvFzbrCZcSiitmSGdaeuB9A7I2nVYMILrjYsXOMhX8g%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N9LufwUGQWGKUaPqZ2UYi7yFm32mFp%2BivWCUzKFOTtw%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Aug 25 17:37:29 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 25 Aug 2022 11:37:29 -0400
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <007f01d8b898$962fbb80$c28f3280$@gmail.com>

I read all the replies and am not sure why nobody used what I see as simpler
and more direct.

Assuming the ORDER of the output matters, it tends to be controlled by the
order of the factor called Code so I have simple code like this:

-------
# Load required libraries

library(dplyr)

# Simulate reading in from a file by using in-line text version

file_contents <- 'Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6'

mydf <- read.table(text=file_contents, header=TRUE)

# Group the results and provide a summary of anything you
# want calculated by group:

mydf %>%
  group_by(Code) %>%
  summarize(minQ=min(Q),
            meanQ=mean(Q),
            maxQ=max(Q),
            sdQ=sd(Q))

----------

Shown as a data.frame, the result is:

   Code  minQ     meanQ  maxQ         sdQ
1 41003 0.160 0.1890000 0.210 0.025942244
2 41005 0.210 0.2117500 0.217 0.003500000
3 41009 0.218 0.2268333 0.240 0.009389711
4 41017 0.240 0.2400000 0.240 0.000000000

You can remove all my extra code to just get the minimum:

mydf %>%
  group_by(Code) %>%
  summarize(minQ=min(Q)) %>% 
  as.data.frame


   Code  minQ
1 41003 0.160
2 41005 0.210
3 41009 0.218
4 41017 0.240

The codes are shown in the same order as the data as it was made into a
factor from raw data in that order.

I may have misunderstood something as others provided an assortment of
methods that, to me, seem less direct. The summarise/summarize function in
dplyr was specifically designed to make a summary as requested, and grouped
if preceded by a request.



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, August 25, 2022 8:28 AM
To: Ebert,Timothy Aaron <tebert at ufl.edu>; javad bayat
<j.bayat194 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column
of a dataframe

My mistake, I did not change the sort order back to the original order. If
you do not like the additional variables they can be dropped using select()
or dat2[,-c(RN, MinByCodeQ)] syntax.

library(dplyr)
library(magrittr)
dat2<-read.table(text="Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6", header=TRUE) dat2$RN <- rownames(dat2)
dat2 <- dat2 %>%
  group_by(Code) %>%
  mutate(
    MinByCodeQ = min(Q, na.rm = T),
  ) %>%
  arrange(Code)
dat2<-filter(dat2,Q==MinByCodeQ)
dat2<-arrange(dat2,as.numeric(RN))

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, August 25, 2022 8:18 AM
To: javad bayat <j.bayat194 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column
of a dataframe

[External Email]

I missed where you explained how to choose a minimum value if there are
several values within a group that are equal to the minimum value. Here is a
dplyr code that returns eight values because there are ties for minimum
values in Q.

library(dplyr)
library(magrittr)
dat2<-read.table(text="Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6", header=TRUE)
dat2 <- dat2 %>%
  group_by(Code) %>%
  mutate(
    MinByCodeQ = min(Q, na.rm = T),
  ) %>%
  arrange(Code)
dat2<-filter(dat2,Q==MinByCodeQ)



Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of javad bayat
Sent: Thursday, August 25, 2022 12:56 AM
To: Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column
of a dataframe

[External Email]

Dear all,
Many thanks for your suggested methods and codes, but unfortunately they did
not give the desired results.
All the codes you have provided are correct but they did not represent the
whole row which is related to the minimum of "Q".
The code must result in 4 rows, with the minimum value of "Q" and other
column values, as below:

       Code

              Y

              M

               D

           Q

            N

             O

41003

81

1

19

0.16

7.17

2.5

41005

79

8

17

0.21

5.5

7.2

41009

79

2

21

0.218

5.56

4.04
41017 79 10 20 0.24 5.3 7.1






Sincerely



























































































41017 79 10 20 0.24 5.3 7.1















On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
>
>
> x<-'41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6'
> df1 <- read.table(textConnection(x))
> names(df1) <- scan(what = character(),
>                     text = 'Code Y M D Q N O') df1$Code <-
> factor(df1$Code)
>
> # 1st option
> with(df1, tapply(Q, Code, min))
> #  41003 41005 41009 41017
> #  0.160 0.210 0.218 0.240
>
> # 2nd option
> aggregate(Q ~ Code, df1, min)
> #     Code     Q
> #  1 41003 0.160
> #  2 41005 0.210
> #  3 41009 0.218
> #  4 41017 0.240
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> > Dear all;
> > I am trying to get the minimum value of a column based on a factor 
> > column of the same data frame. My data frame is like the below:
> >         Code               Y               M                D
> >   Q
> >       N              O
> > 41003 81 1 19 0.16 7.17 2.5
> > 41003 77 9 22 0.197 6.8 2.2
> > 41003 79 7 28 0.21 4.7 6.2
> > 41005 79 8 17 0.21 5.5 7.2
> > 41005 80 10 30 0.21 6.84 2.6
> > 41005 80 12 20 0.21 6.84 2.4
> > 41005 79 6 14 0.217 5.61 3.55
> > 41009 79 2 21 0.218 5.56 4.04
> > 41009 79 5 27 0.218 6.4 3.12
> > 41009 80 11 29 0.22 6.84 2.8
> > 41009 78 5 28 0.232 6 3.2
> > 41009 81 8 20 0.233 6.39 1.6
> > 41009 79 9 30 0.24 5.6 7.5
> > 41017 79 10 20 0.24 5.3 7.1
> > 41017 80 7 30 0.24 6.73 2.6
> >
> > I want to get the minimum value of the "Q" column with the whole row 
> > values, according to the "Code"  column  which is a factor. Overall 
> > it
> will
> > give me 4 rows, with the value of "Q". Below is a code that I used 
> > but it did not give me what I wanted.
> >
> >> x[which(x$Q == min(x$Q)),]
> >
> > Sincerely
> >
> >
> >
>


--
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.
ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547
a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637
970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIi
LCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GvFzbrCZcSiitmSGdae
uB9A7I2nVYMILrjYsXOMhX8g%3D&amp;reserved=0
PLEASE do read the posting guide
https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-proje
ct.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912
984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C63797026
7341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBT
iI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N9LufwUGQWGKUaPqZ2UYi7yF
m32mFp%2BivWCUzKFOTtw%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.
ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547
a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637
970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIi
LCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GvFzbrCZcSiitmSGdae
uB9A7I2nVYMILrjYsXOMhX8g%3D&amp;reserved=0
PLEASE do read the posting guide
https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-proje
ct.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912
984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C63797026
7341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBT
iI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N9LufwUGQWGKUaPqZ2UYi7yF
m32mFp%2BivWCUzKFOTtw%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Aug 25 17:54:21 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 25 Aug 2022 16:54:21 +0100
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <007f01d8b898$962fbb80$c28f3280$@gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
Message-ID: <2a04550c-55c5-439f-7ee2-45dc13103e3e@sapo.pt>

Hello,

Inline.

?s 16:37 de 25/08/2022, avi.e.gross at gmail.com escreveu:
> I read all the replies and am not sure why nobody used what I see as simpler
> and more direct.
> 
> Assuming the ORDER of the output matters, it tends to be controlled by the
> order of the factor called Code so I have simple code like this:
> 
> -------
> # Load required libraries
> 
> library(dplyr)
> 
> # Simulate reading in from a file by using in-line text version
> 
> file_contents <- 'Code Y M D Q N O
> 41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6'
> 
> mydf <- read.table(text=file_contents, header=TRUE)
> 
> # Group the results and provide a summary of anything you
> # want calculated by group:
> 
> mydf %>%
>    group_by(Code) %>%
>    summarize(minQ=min(Q),
>              meanQ=mean(Q),
>              maxQ=max(Q),
>              sdQ=sd(Q))
> 
> ----------
> 
> Shown as a data.frame, the result is:
> 
>     Code  minQ     meanQ  maxQ         sdQ
> 1 41003 0.160 0.1890000 0.210 0.025942244
> 2 41005 0.210 0.2117500 0.217 0.003500000
> 3 41009 0.218 0.2268333 0.240 0.009389711
> 4 41017 0.240 0.2400000 0.240 0.000000000
> 
> You can remove all my extra code to just get the minimum:
> 
> mydf %>%
>    group_by(Code) %>%
>    summarize(minQ=min(Q)) %>%
>    as.data.frame
> 
> 
>     Code  minQ
> 1 41003 0.160
> 2 41005 0.210
> 3 41009 0.218
> 4 41017 0.240
> 
> The codes are shown in the same order as the data as it was made into a
> factor from raw data in that order.
> 
> I may have misunderstood something as others provided an assortment of
> methods that, to me, seem less direct. The summarise/summarize function in
> dplyr was specifically designed to make a summary as requested, and grouped
> if preceded by a request.

Yes, I agree with you.
In the raw data the Code is already ordered so it doesn't matter if it 
is not addressed in the aggregation code. But from the OP's last post I 
conclude (wrongly?) that in the original data this need not be the case. 
Hence the double order() in my last post. I wonder (a) if it's really 
necessary and (b) if it would make a difference if the aggregated output 
is sorted.

Rui Barradas

> 
> 
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
> Sent: Thursday, August 25, 2022 8:28 AM
> To: Ebert,Timothy Aaron <tebert at ufl.edu>; javad bayat
> <j.bayat194 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>
> Cc: R-help at r-project.org
> Subject: Re: [R] Getting minimum value of a column according a factor column
> of a dataframe
> 
> My mistake, I did not change the sort order back to the original order. If
> you do not like the additional variables they can be dropped using select()
> or dat2[,-c(RN, MinByCodeQ)] syntax.
> 
> library(dplyr)
> library(magrittr)
> dat2<-read.table(text="Code Y M D Q N O
> 41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6", header=TRUE) dat2$RN <- rownames(dat2)
> dat2 <- dat2 %>%
>    group_by(Code) %>%
>    mutate(
>      MinByCodeQ = min(Q, na.rm = T),
>    ) %>%
>    arrange(Code)
> dat2<-filter(dat2,Q==MinByCodeQ)
> dat2<-arrange(dat2,as.numeric(RN))
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
> Sent: Thursday, August 25, 2022 8:18 AM
> To: javad bayat <j.bayat194 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>
> Cc: R-help at r-project.org
> Subject: Re: [R] Getting minimum value of a column according a factor column
> of a dataframe
> 
> [External Email]
> 
> I missed where you explained how to choose a minimum value if there are
> several values within a group that are equal to the minimum value. Here is a
> dplyr code that returns eight values because there are ties for minimum
> values in Q.
> 
> library(dplyr)
> library(magrittr)
> dat2<-read.table(text="Code Y M D Q N O
> 41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6", header=TRUE)
> dat2 <- dat2 %>%
>    group_by(Code) %>%
>    mutate(
>      MinByCodeQ = min(Q, na.rm = T),
>    ) %>%
>    arrange(Code)
> dat2<-filter(dat2,Q==MinByCodeQ)
> 
> 
> 
> Tim
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of javad bayat
> Sent: Thursday, August 25, 2022 12:56 AM
> To: Rui Barradas <ruipbarradas at sapo.pt>
> Cc: R-help at r-project.org
> Subject: Re: [R] Getting minimum value of a column according a factor column
> of a dataframe
> 
> [External Email]
> 
> Dear all,
> Many thanks for your suggested methods and codes, but unfortunately they did
> not give the desired results.
> All the codes you have provided are correct but they did not represent the
> whole row which is related to the minimum of "Q".
> The code must result in 4 rows, with the minimum value of "Q" and other
> column values, as below:
> 
>         Code
> 
>                Y
> 
>                M
> 
>                 D
> 
>             Q
> 
>              N
> 
>               O
> 
> 41003
> 
> 81
> 
> 1
> 
> 19
> 
> 0.16
> 
> 7.17
> 
> 2.5
> 
> 41005
> 
> 79
> 
> 8
> 
> 17
> 
> 0.21
> 
> 5.5
> 
> 7.2
> 
> 41009
> 
> 79
> 
> 2
> 
> 21
> 
> 0.218
> 
> 5.56
> 
> 4.04
> 41017 79 10 20 0.24 5.3 7.1
> 
> 
> 
> 
> 
> 
> Sincerely
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 41017 79 10 20 0.24 5.3 7.1
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> 
>> Hello,
>>
>> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
>>
>>
>> x<-'41003 81 1 19 0.16 7.17 2.5
>> 41003 77 9 22 0.197 6.8 2.2
>> 41003 79 7 28 0.21 4.7 6.2
>> 41005 79 8 17 0.21 5.5 7.2
>> 41005 80 10 30 0.21 6.84 2.6
>> 41005 80 12 20 0.21 6.84 2.4
>> 41005 79 6 14 0.217 5.61 3.55
>> 41009 79 2 21 0.218 5.56 4.04
>> 41009 79 5 27 0.218 6.4 3.12
>> 41009 80 11 29 0.22 6.84 2.8
>> 41009 78 5 28 0.232 6 3.2
>> 41009 81 8 20 0.233 6.39 1.6
>> 41009 79 9 30 0.24 5.6 7.5
>> 41017 79 10 20 0.24 5.3 7.1
>> 41017 80 7 30 0.24 6.73 2.6'
>> df1 <- read.table(textConnection(x))
>> names(df1) <- scan(what = character(),
>>                      text = 'Code Y M D Q N O') df1$Code <-
>> factor(df1$Code)
>>
>> # 1st option
>> with(df1, tapply(Q, Code, min))
>> #  41003 41005 41009 41017
>> #  0.160 0.210 0.218 0.240
>>
>> # 2nd option
>> aggregate(Q ~ Code, df1, min)
>> #     Code     Q
>> #  1 41003 0.160
>> #  2 41005 0.210
>> #  3 41009 0.218
>> #  4 41017 0.240
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 08:44 de 24/08/2022, javad bayat escreveu:
>>> Dear all;
>>> I am trying to get the minimum value of a column based on a factor
>>> column of the same data frame. My data frame is like the below:
>>>          Code               Y               M                D
>>>    Q
>>>        N              O
>>> 41003 81 1 19 0.16 7.17 2.5
>>> 41003 77 9 22 0.197 6.8 2.2
>>> 41003 79 7 28 0.21 4.7 6.2
>>> 41005 79 8 17 0.21 5.5 7.2
>>> 41005 80 10 30 0.21 6.84 2.6
>>> 41005 80 12 20 0.21 6.84 2.4
>>> 41005 79 6 14 0.217 5.61 3.55
>>> 41009 79 2 21 0.218 5.56 4.04
>>> 41009 79 5 27 0.218 6.4 3.12
>>> 41009 80 11 29 0.22 6.84 2.8
>>> 41009 78 5 28 0.232 6 3.2
>>> 41009 81 8 20 0.233 6.39 1.6
>>> 41009 79 9 30 0.24 5.6 7.5
>>> 41017 79 10 20 0.24 5.3 7.1
>>> 41017 80 7 30 0.24 6.73 2.6
>>>
>>> I want to get the minimum value of the "Q" column with the whole row
>>> values, according to the "Code"  column  which is a factor. Overall
>>> it
>> will
>>> give me 4 rows, with the value of "Q". Below is a code that I used
>>> but it did not give me what I wanted.
>>>
>>>> x[which(x$Q == min(x$Q)),]
>>>
>>> Sincerely
>>>
>>>
>>>
>>
> 
> 
> --
> Best Regards
> Javad Bayat
> M.Sc. Environment Engineering
> Alternative Mail: bayat194 at yahoo.com
> 
>          [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.
> ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547
> a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637
> 970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIi
> LCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GvFzbrCZcSiitmSGdae
> uB9A7I2nVYMILrjYsXOMhX8g%3D&amp;reserved=0
> PLEASE do read the posting guide
> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-proje
> ct.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912
> 984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C63797026
> 7341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBT
> iI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N9LufwUGQWGKUaPqZ2UYi7yF
> m32mFp%2BivWCUzKFOTtw%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.
> ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547
> a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637
> 970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIi
> LCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GvFzbrCZcSiitmSGdae
> uB9A7I2nVYMILrjYsXOMhX8g%3D&amp;reserved=0
> PLEASE do read the posting guide
> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-proje
> ct.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912
> 984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C63797026
> 7341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBT
> iI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N9LufwUGQWGKUaPqZ2UYi7yF
> m32mFp%2BivWCUzKFOTtw%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Thu Aug 25 17:58:50 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Thu, 25 Aug 2022 15:58:50 +0000
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <007f01d8b898$962fbb80$c28f3280$@gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
Message-ID: <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>

My assumption (maybe wrong) was that we needed to keep the other variables. I want to find the values of Y, M, D, N, and O for the minimum value of Q within each unique value of Code, keeping the data in the original order. All one need to do is filter Q in the original dataframe by your answer for minQ.

Keeping the data in the original order seems unnecessary, but that is what was asked in a later post.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of avi.e.gross at gmail.com
Sent: Thursday, August 25, 2022 11:37 AM
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

I read all the replies and am not sure why nobody used what I see as simpler and more direct.

Assuming the ORDER of the output matters, it tends to be controlled by the order of the factor called Code so I have simple code like this:

-------
# Load required libraries

library(dplyr)

# Simulate reading in from a file by using in-line text version

file_contents <- 'Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6'

mydf <- read.table(text=file_contents, header=TRUE)

# Group the results and provide a summary of anything you # want calculated by group:

mydf %>%
  group_by(Code) %>%
  summarize(minQ=min(Q),
            meanQ=mean(Q),
            maxQ=max(Q),
            sdQ=sd(Q))

----------

Shown as a data.frame, the result is:

   Code  minQ     meanQ  maxQ         sdQ
1 41003 0.160 0.1890000 0.210 0.025942244
2 41005 0.210 0.2117500 0.217 0.003500000
3 41009 0.218 0.2268333 0.240 0.009389711
4 41017 0.240 0.2400000 0.240 0.000000000

You can remove all my extra code to just get the minimum:

mydf %>%
  group_by(Code) %>%
  summarize(minQ=min(Q)) %>%
  as.data.frame


   Code  minQ
1 41003 0.160
2 41005 0.210
3 41009 0.218
4 41017 0.240

The codes are shown in the same order as the data as it was made into a factor from raw data in that order.

I may have misunderstood something as others provided an assortment of methods that, to me, seem less direct. The summarise/summarize function in dplyr was specifically designed to make a summary as requested, and grouped if preceded by a request.



-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, August 25, 2022 8:28 AM
To: Ebert,Timothy Aaron <tebert at ufl.edu>; javad bayat <j.bayat194 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

My mistake, I did not change the sort order back to the original order. If you do not like the additional variables they can be dropped using select() or dat2[,-c(RN, MinByCodeQ)] syntax.

library(dplyr)
library(magrittr)
dat2<-read.table(text="Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6", header=TRUE) dat2$RN <- rownames(dat2)
dat2 <- dat2 %>%
  group_by(Code) %>%
  mutate(
    MinByCodeQ = min(Q, na.rm = T),
  ) %>%
  arrange(Code)
dat2<-filter(dat2,Q==MinByCodeQ)
dat2<-arrange(dat2,as.numeric(RN))

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ebert,Timothy Aaron
Sent: Thursday, August 25, 2022 8:18 AM
To: javad bayat <j.bayat194 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

I missed where you explained how to choose a minimum value if there are several values within a group that are equal to the minimum value. Here is a dplyr code that returns eight values because there are ties for minimum values in Q.

library(dplyr)
library(magrittr)
dat2<-read.table(text="Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6", header=TRUE)
dat2 <- dat2 %>%
  group_by(Code) %>%
  mutate(
    MinByCodeQ = min(Q, na.rm = T),
  ) %>%
  arrange(Code)
dat2<-filter(dat2,Q==MinByCodeQ)



Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of javad bayat
Sent: Thursday, August 25, 2022 12:56 AM
To: Rui Barradas <ruipbarradas at sapo.pt>
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

Dear all,
Many thanks for your suggested methods and codes, but unfortunately they did not give the desired results.
All the codes you have provided are correct but they did not represent the whole row which is related to the minimum of "Q".
The code must result in 4 rows, with the minimum value of "Q" and other column values, as below:

       Code

              Y

              M

               D

           Q

            N

             O

41003

81

1

19

0.16

7.17

2.5

41005

79

8

17

0.21

5.5

7.2

41009

79

2

21

0.218

5.56

4.04
41017 79 10 20 0.24 5.3 7.1






Sincerely



























































































41017 79 10 20 0.24 5.3 7.1















On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
>
>
> x<-'41003 81 1 19 0.16 7.17 2.5
> 41003 77 9 22 0.197 6.8 2.2
> 41003 79 7 28 0.21 4.7 6.2
> 41005 79 8 17 0.21 5.5 7.2
> 41005 80 10 30 0.21 6.84 2.6
> 41005 80 12 20 0.21 6.84 2.4
> 41005 79 6 14 0.217 5.61 3.55
> 41009 79 2 21 0.218 5.56 4.04
> 41009 79 5 27 0.218 6.4 3.12
> 41009 80 11 29 0.22 6.84 2.8
> 41009 78 5 28 0.232 6 3.2
> 41009 81 8 20 0.233 6.39 1.6
> 41009 79 9 30 0.24 5.6 7.5
> 41017 79 10 20 0.24 5.3 7.1
> 41017 80 7 30 0.24 6.73 2.6'
> df1 <- read.table(textConnection(x))
> names(df1) <- scan(what = character(),
>                     text = 'Code Y M D Q N O') df1$Code <-
> factor(df1$Code)
>
> # 1st option
> with(df1, tapply(Q, Code, min))
> #  41003 41005 41009 41017
> #  0.160 0.210 0.218 0.240
>
> # 2nd option
> aggregate(Q ~ Code, df1, min)
> #     Code     Q
> #  1 41003 0.160
> #  2 41005 0.210
> #  3 41009 0.218
> #  4 41017 0.240
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> > Dear all;
> > I am trying to get the minimum value of a column based on a factor 
> > column of the same data frame. My data frame is like the below:
> >         Code               Y               M                D
> >   Q
> >       N              O
> > 41003 81 1 19 0.16 7.17 2.5
> > 41003 77 9 22 0.197 6.8 2.2
> > 41003 79 7 28 0.21 4.7 6.2
> > 41005 79 8 17 0.21 5.5 7.2
> > 41005 80 10 30 0.21 6.84 2.6
> > 41005 80 12 20 0.21 6.84 2.4
> > 41005 79 6 14 0.217 5.61 3.55
> > 41009 79 2 21 0.218 5.56 4.04
> > 41009 79 5 27 0.218 6.4 3.12
> > 41009 80 11 29 0.22 6.84 2.8
> > 41009 78 5 28 0.232 6 3.2
> > 41009 81 8 20 0.233 6.39 1.6
> > 41009 79 9 30 0.24 5.6 7.5
> > 41017 79 10 20 0.24 5.3 7.1
> > 41017 80 7 30 0.24 6.73 2.6
> >
> > I want to get the minimum value of the "Q" column with the whole row 
> > values, according to the "Code"  column  which is a factor. Overall 
> > it
> will
> > give me 4 rows, with the value of "Q". Below is a code that I used 
> > but it did not give me what I wanted.
> >
> >> x[which(x$Q == min(x$Q)),]
> >
> > Sincerely
> >
> >
> >
>


--
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz%2F&amp;data=05%7C01%7Ctebert%40ufl.edu%7C8743a664ee564a89657208da86afd075%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970386928002615%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=G%2BiDn0ShRahazq%2FlGBVgHUe4WxalYK%2B%2BnuEkbS025GU%3D&amp;reserved=0.
ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547
a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637
970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIi
LCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GvFzbrCZcSiitmSGdae
uB9A7I2nVYMILrjYsXOMhX8g%3D&amp;reserved=0
PLEASE do read the posting guide
https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-proje%2F&amp;data=05%7C01%7Ctebert%40ufl.edu%7C8743a664ee564a89657208da86afd075%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970386928002615%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=yb9P82dJF33kgY5ZZiqedyShI16ZQlDA6UWJ6ZJTnQQ%3D&amp;reserved=0
ct.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912
984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C63797026
7341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBT
iI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N9LufwUGQWGKUaPqZ2UYi7yF
m32mFp%2BivWCUzKFOTtw%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz%2F&amp;data=05%7C01%7Ctebert%40ufl.edu%7C8743a664ee564a89657208da86afd075%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970386928002615%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=G%2BiDn0ShRahazq%2FlGBVgHUe4WxalYK%2B%2BnuEkbS025GU%3D&amp;reserved=0.
ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547
a8912984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637
970267341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIi
LCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=GvFzbrCZcSiitmSGdae
uB9A7I2nVYMILrjYsXOMhX8g%3D&amp;reserved=0
PLEASE do read the posting guide
https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-proje%2F&amp;data=05%7C01%7Ctebert%40ufl.edu%7C8743a664ee564a89657208da86afd075%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970386928158848%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=ljNbv686YCryrMQezMVFCA7YXLBmEGbg50JKVA9EzX4%3D&amp;reserved=0
ct.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7Cfc547a8912
984c0b118208da8693f860%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C63797026
7341688438%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBT
iI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=N9LufwUGQWGKUaPqZ2UYi7yF
m32mFp%2BivWCUzKFOTtw%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C8743a664ee564a89657208da86afd075%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970386928158848%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=F5dR15%2BIvRLNJbbq%2Fm6%2B7u9ef6I12HVH4SPEK7fvdNY%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C8743a664ee564a89657208da86afd075%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970386928158848%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=1IcTxl4yCaZWok4Fi6HrOmtoTE78r7Dpfx%2Fod01uifw%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C8743a664ee564a89657208da86afd075%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970386928158848%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=F5dR15%2BIvRLNJbbq%2Fm6%2B7u9ef6I12HVH4SPEK7fvdNY%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C8743a664ee564a89657208da86afd075%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970386928158848%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=1IcTxl4yCaZWok4Fi6HrOmtoTE78r7Dpfx%2Fod01uifw%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From w||||@mwdun|@p @end|ng |rom gm@||@com  Thu Aug 25 18:05:26 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Thu, 25 Aug 2022 09:05:26 -0700
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAm+VWvB7BV7YBy7e_Bh_HZD7CgcJiC=FwHYMo1+7rf_=jA@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <d7c8ab7c-7a67-d178-c14a-e67195f1a780@sapo.pt>
 <CANTxAm+VWvB7BV7YBy7e_Bh_HZD7CgcJiC=FwHYMo1+7rf_=jA@mail.gmail.com>
Message-ID: <CAHqSRuSA4Sx9FS6AsY3ZVqS40GQRWXtNPHsuFuvUUUTwL8Yc9Q@mail.gmail.com>

The order of the rows returned by summarize is controlled by the levels of
the factors given to group_by.  If group_by is given character columns
instead of factors, it converts them to factors with the levels being the
sorted unique values of the character columns.  Convert you character
columns to factors with the desired levels to get the order you want.

E.g.,

> d <- data.frame(charGroup=rep(c("Small","Medium","Large"),3:1), x=101:106)
> d$factorGroup <- factor(d$charGroup, levels=c("Small","Medium","Large","X
Large"))
> d
  charGroup   x factorGroup
1     Small 101       Small
2     Small 102       Small
3     Small 103       Small
4    Medium 104      Medium
5    Medium 105      Medium
6     Large 106       Large
> d |> group_by(charGroup) |> summarize(minX=min(x))
# A tibble: 3 ? 2
  charGroup  minX
  <chr>     <int>
1 Large       106
2 Medium      104
3 Small       101
> d |> group_by(factorGroup) |> summarize(minX=min(x))
# A tibble: 3 ? 2
  factorGroup  minX
  <fct>       <int>
1 Small         101
2 Medium        104
3 Large         106
> # .drop=FALSE outputs a row for each unused level as well
> d |> group_by(factorGroup, .drop=FALSE) |> summarize(minX=min(x))
# A tibble: 4 ? 2
  factorGroup  minX
  <fct>       <dbl>
1 Small         101
2 Medium        104
3 Large         106
4 X Large       Inf
Warning message:
In min(x) : no non-missing arguments to min; returning Inf

-Bill

On Thu, Aug 25, 2022 at 4:05 AM javad bayat <j.bayat194 at gmail.com> wrote:

> Dear Rui;
> Thank you very much. Both of your codes worked correctly. Now I can see the
> whole row's value.
> But I found a problem in the results. When I run your codes, the results
> are shown in a sorted table. I do not know why the results have been sorted
> according to the "Code" column, smallest to largest. Is there any way to
> get the results like their order in the first data frame (bilan2)? I used
> your codes as follow:
>
> > bilan3 <- lapply(split(bilan2, bilan2$Code), \(x) x[which.min(x$Q),])
> > bilan3 = data.frame(do.call(rbind, bilan3))
> Sincerely
>
>
>
> On Thu, Aug 25, 2022 at 11:52 AM Rui Barradas <ruipbarradas at sapo.pt>
> wrote:
>
> > Hello,
> >
> > OK, what about
> >
> >
> > res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
> > do.call(rbind, res)
> > #         Code  Y  M  D     Q    N    O
> > #  41003 41003 81  1 19 0.160 7.17 2.50
> > #  41005 41005 79  8 17 0.210 5.50 7.20
> > #  41009 41009 79  2 21 0.218 5.56 4.04
> > #  41017 41017 79 10 20 0.240 5.30 7.10
> >
> >
> > A dplyr solution.
> >
> >
> >
> > suppressPackageStartupMessages(library(dplyr))
> >
> > df1 %>%
> >    group_by(Code) %>%
> >    slice_min(Q) %>%
> >    slice_head(n = 1)
> > #  # A tibble: 4 ? 7
> > #  # Groups:   Code [4]
> > #    Code      Y     M     D     Q     N     O
> > #    <fct> <int> <int> <int> <dbl> <dbl> <dbl>
> > #  1 41003    81     1    19 0.16   7.17  2.5
> > #  2 41005    79     8    17 0.21   5.5   7.2
> > #  3 41009    79     2    21 0.218  5.56  4.04
> > #  4 41017    79    10    20 0.24   5.3   7.1
> >
> >
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> >
> > ?s 05:56 de 25/08/2022, javad bayat escreveu:
> > > Dear all,
> > > Many thanks for your suggested methods and codes, but unfortunately
> they
> > > did not give the desired results.
> > > All the codes you have provided are correct but they did not represent
> > the
> > > whole row which is related to the minimum of "Q".
> > > The code must result in 4 rows, with the minimum value of "Q" and other
> > > column values, as below:
> > >
> > >         Code
> > >
> > >                Y
> > >
> > >                M
> > >
> > >                 D
> > >
> > >             Q
> > >
> > >              N
> > >
> > >               O
> > >
> > > 41003
> > >
> > > 81
> > >
> > > 1
> > >
> > > 19
> > >
> > > 0.16
> > >
> > > 7.17
> > >
> > > 2.5
> > >
> > > 41005
> > >
> > > 79
> > >
> > > 8
> > >
> > > 17
> > >
> > > 0.21
> > >
> > > 5.5
> > >
> > > 7.2
> > >
> > > 41009
> > >
> > > 79
> > >
> > > 2
> > >
> > > 21
> > >
> > > 0.218
> > >
> > > 5.56
> > >
> > > 4.04
> > > 41017 79 10 20 0.24 5.3 7.1
> > >
> > >
> > >
> > >
> > >
> > >
> > > Sincerely
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > 41017 79 10 20 0.24 5.3 7.1
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt>
> > wrote:
> > >
> > >> Hello,
> > >>
> > >> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
> > >>
> > >>
> > >> x<-'41003 81 1 19 0.16 7.17 2.5
> > >> 41003 77 9 22 0.197 6.8 2.2
> > >> 41003 79 7 28 0.21 4.7 6.2
> > >> 41005 79 8 17 0.21 5.5 7.2
> > >> 41005 80 10 30 0.21 6.84 2.6
> > >> 41005 80 12 20 0.21 6.84 2.4
> > >> 41005 79 6 14 0.217 5.61 3.55
> > >> 41009 79 2 21 0.218 5.56 4.04
> > >> 41009 79 5 27 0.218 6.4 3.12
> > >> 41009 80 11 29 0.22 6.84 2.8
> > >> 41009 78 5 28 0.232 6 3.2
> > >> 41009 81 8 20 0.233 6.39 1.6
> > >> 41009 79 9 30 0.24 5.6 7.5
> > >> 41017 79 10 20 0.24 5.3 7.1
> > >> 41017 80 7 30 0.24 6.73 2.6'
> > >> df1 <- read.table(textConnection(x))
> > >> names(df1) <- scan(what = character(),
> > >>                      text = 'Code Y M D Q N O')
> > >> df1$Code <- factor(df1$Code)
> > >>
> > >> # 1st option
> > >> with(df1, tapply(Q, Code, min))
> > >> #  41003 41005 41009 41017
> > >> #  0.160 0.210 0.218 0.240
> > >>
> > >> # 2nd option
> > >> aggregate(Q ~ Code, df1, min)
> > >> #     Code     Q
> > >> #  1 41003 0.160
> > >> #  2 41005 0.210
> > >> #  3 41009 0.218
> > >> #  4 41017 0.240
> > >>
> > >>
> > >> Hope this helps,
> > >>
> > >> Rui Barradas
> > >>
> > >> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> > >>> Dear all;
> > >>> I am trying to get the minimum value of a column based on a factor
> > column
> > >>> of the same data frame. My data frame is like the below:
> > >>>          Code               Y               M                D
> > >>>    Q
> > >>>        N              O
> > >>> 41003 81 1 19 0.16 7.17 2.5
> > >>> 41003 77 9 22 0.197 6.8 2.2
> > >>> 41003 79 7 28 0.21 4.7 6.2
> > >>> 41005 79 8 17 0.21 5.5 7.2
> > >>> 41005 80 10 30 0.21 6.84 2.6
> > >>> 41005 80 12 20 0.21 6.84 2.4
> > >>> 41005 79 6 14 0.217 5.61 3.55
> > >>> 41009 79 2 21 0.218 5.56 4.04
> > >>> 41009 79 5 27 0.218 6.4 3.12
> > >>> 41009 80 11 29 0.22 6.84 2.8
> > >>> 41009 78 5 28 0.232 6 3.2
> > >>> 41009 81 8 20 0.233 6.39 1.6
> > >>> 41009 79 9 30 0.24 5.6 7.5
> > >>> 41017 79 10 20 0.24 5.3 7.1
> > >>> 41017 80 7 30 0.24 6.73 2.6
> > >>>
> > >>> I want to get the minimum value of the "Q" column with the whole row
> > >>> values, according to the "Code"  column  which is a factor. Overall
> it
> > >> will
> > >>> give me 4 rows, with the value of "Q". Below is a code that I used
> but
> > it
> > >>> did not give me what I wanted.
> > >>>
> > >>>> x[which(x$Q == min(x$Q)),]
> > >>>
> > >>> Sincerely
> > >>>
> > >>>
> > >>>
> > >>
> > >
> > >
> >
>
>
> --
> Best Regards
> Javad Bayat
> M.Sc. Environment Engineering
> Alternative Mail: bayat194 at yahoo.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Aug 25 18:26:19 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 25 Aug 2022 12:26:19 -0400
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <2a04550c-55c5-439f-7ee2-45dc13103e3e@sapo.pt>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <2a04550c-55c5-439f-7ee2-45dc13103e3e@sapo.pt>
Message-ID: <00bb01d8b89f$68c52110$3a4f6330$@gmail.com>

Rui wrote:

<<<<
Yes, I agree with you.
In the raw data the Code is already ordered so it doesn't matter if it is not addressed in the aggregation code. But from the OP's last post I conclude (wrongly?) that in the original data this need not be the case. 
Hence the double order() in my last post. I wonder (a) if it's really necessary and (b) if it would make a difference if the aggregated output is sorted.
>>>>

My reply (truncated but in earlier messages) has to do with the nature of ordering. We were given what looks like a brief version of some data showing 4 unique values for a column contained in "Code" where the data happened to be in complete order showing several rows with values 41003, then several with 41005 then two more in what looked like numerical ascending order.

My personal guess is that if the Code column is a factor, it is left alone, otherwise it is made into a factor and the order of the factor may not be what you want. I did an experiment of changing one factor to a higher number, and it was moved to the last position in the report even though it was near the top. The default ordering is thus going to result in a summary that may not match what is wanted UNLESS you make adjustments.

ONE METHOD I can think of is to take what I call mydf (the data.frame or tibble you read in) and ask for the unique values of Code like so before making it into a factor:

my_order <- unique(mydf$Code)

I had inserted an 88888 into the data in second position so the result is this:

> my_order
[1] 41003 88888 41005 41009 41017

The order has been preserved and I can use it to make the order in the factors be the same by using this:

mydf$Code <- factor(mydf$Code, levels=my_order)

It now produces output in that order in the full code shown below:

   Code  minQ
1 41003 0.160
2 88888 0.160
3 41005 0.210
4 41009 0.218
5 41017 0.240

I quickly mention  a second method where you reorder the rows of the output to whatever order you want. I do not show or do that.

NOTE: the code below shows a changed set of data from the original containing an 88888 in row 2 for illustration:
-----CODE-----

# Load required libraries

library(dplyr)

# Simulate reading in from a file by using in-line text version

file_contents <- 'Code Y M D Q N O
41003 81 1 19 0.16 7.17 2.5
88888 81 1 19 0.16 7.17 2.5
41003 77 9 22 0.197 6.8 2.2
41003 79 7 28 0.21 4.7 6.2
41005 79 8 17 0.21 5.5 7.2
41005 80 10 30 0.21 6.84 2.6
41005 80 12 20 0.21 6.84 2.4
41005 79 6 14 0.217 5.61 3.55
41009 79 2 21 0.218 5.56 4.04
41009 79 5 27 0.218 6.4 3.12
41009 80 11 29 0.22 6.84 2.8
41009 78 5 28 0.232 6 3.2
41009 81 8 20 0.233 6.39 1.6
41009 79 9 30 0.24 5.6 7.5
41017 79 10 20 0.24 5.3 7.1
41017 80 7 30 0.24 6.73 2.6'

mydf <- read.table(text=file_contents, header=TRUE)

# Make a factor of column Code in the same order as
# the numbers are first introduced in rows.

my_order <- unique(mydf$Code)

mydf$Code <- factor(mydf$Code, levels=my_order)

# Group the results and provide a summary of anything you
# want calculated by group:

mydf %>%
  group_by(Code) %>%
  summarize(minQ=min(Q)) %>% 
  as.data.frame


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Aug 25 18:31:35 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 25 Aug 2022 12:31:35 -0400
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
Message-ID: <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>

Yes, Timothy, the request was not seen by all of us as the same.

Indeed if the request was to show a subset of the original data consisting
of only the rows that were the minimum for each Code and also showed ties,
then the solution is a tad more complex. I would then do something along the
lines of what others showed such as generating another column showing the
minimum for each row and then showing only rows that matched their value in
two columns or whatever was needed.

As noted, keeping the output in a specific order was not initially
requested.

Keeping the data in some order is a common enough request but in this
situation, I suspect the order many might want would be the one showing the
minimums in order, not the codes in the original order. 

-----Original Message-----
From: Ebert,Timothy Aaron <tebert at ufl.edu> 
Sent: Thursday, August 25, 2022 11:59 AM
To: avi.e.gross at gmail.com
Cc: R-help at r-project.org
Subject: RE: [R] Getting minimum value of a column according a factor column
of a dataframe

My assumption (maybe wrong) was that we needed to keep the other variables.
I want to find the values of Y, M, D, N, and O for the minimum value of Q
within each unique value of Code, keeping the data in the original order.
All one need to do is filter Q in the original dataframe by your answer for
minQ.

Keeping the data in the original order seems unnecessary, but that is what
was asked in a later post.


From bgunter@4567 @end|ng |rom gm@||@com  Thu Aug 25 19:26:49 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 25 Aug 2022 10:26:49 -0700
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <d7c8ab7c-7a67-d178-c14a-e67195f1a780@sapo.pt>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <d7c8ab7c-7a67-d178-c14a-e67195f1a780@sapo.pt>
Message-ID: <CAGxFJbRM0fjHoY=VE-e7ULDEN93k8FZezRFzSN4T-u_1mM6P6Q@mail.gmail.com>

A slightly slicker solution making use of the handy by() function to
avoid the lapply(split...) construction.

> do.call(rbind,by(df1, df1$Code, \(x)x[which.min(x$Q),]))

       Code  Y  M  D     Q    N    O
41003 41003 81  1 19 0.160 7.17 2.50
41005 41005 79  8 17 0.210 5.50 7.20
41009 41009 79  2 21 0.218 5.56 4.04
41017 41017 79 10 20 0.240 5.30 7.10

This of course ignores the issue of tied minima that Tim Ebert brought
up. That would require a bit more finagling in the anonymous function
code instead of which.min() .

Cheers,
Bert

On Thu, Aug 25, 2022 at 12:22 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> OK, what about
>
>
> res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
> do.call(rbind, res)
> #         Code  Y  M  D     Q    N    O
> #  41003 41003 81  1 19 0.160 7.17 2.50
> #  41005 41005 79  8 17 0.210 5.50 7.20
> #  41009 41009 79  2 21 0.218 5.56 4.04
> #  41017 41017 79 10 20 0.240 5.30 7.10
>
>
> A dplyr solution.
>
>
>
> suppressPackageStartupMessages(library(dplyr))
>
> df1 %>%
>    group_by(Code) %>%
>    slice_min(Q) %>%
>    slice_head(n = 1)
> #  # A tibble: 4 ? 7
> #  # Groups:   Code [4]
> #    Code      Y     M     D     Q     N     O
> #    <fct> <int> <int> <int> <dbl> <dbl> <dbl>
> #  1 41003    81     1    19 0.16   7.17  2.5
> #  2 41005    79     8    17 0.21   5.5   7.2
> #  3 41009    79     2    21 0.218  5.56  4.04
> #  4 41017    79    10    20 0.24   5.3   7.1
>
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> ?s 05:56 de 25/08/2022, javad bayat escreveu:
> > Dear all,
> > Many thanks for your suggested methods and codes, but unfortunately they
> > did not give the desired results.
> > All the codes you have provided are correct but they did not represent the
> > whole row which is related to the minimum of "Q".
> > The code must result in 4 rows, with the minimum value of "Q" and other
> > column values, as below:
> >
> >         Code
> >
> >                Y
> >
> >                M
> >
> >                 D
> >
> >             Q
> >
> >              N
> >
> >               O
> >
> > 41003
> >
> > 81
> >
> > 1
> >
> > 19
> >
> > 0.16
> >
> > 7.17
> >
> > 2.5
> >
> > 41005
> >
> > 79
> >
> > 8
> >
> > 17
> >
> > 0.21
> >
> > 5.5
> >
> > 7.2
> >
> > 41009
> >
> > 79
> >
> > 2
> >
> > 21
> >
> > 0.218
> >
> > 5.56
> >
> > 4.04
> > 41017 79 10 20 0.24 5.3 7.1
> >
> >
> >
> >
> >
> >
> > Sincerely
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > 41017 79 10 20 0.24 5.3 7.1
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > On Wed, Aug 24, 2022 at 9:24 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >
> >> Hello,
> >>
> >> Here are two options, the 1st outputs a vector, the 2nd a data.frame.
> >>
> >>
> >> x<-'41003 81 1 19 0.16 7.17 2.5
> >> 41003 77 9 22 0.197 6.8 2.2
> >> 41003 79 7 28 0.21 4.7 6.2
> >> 41005 79 8 17 0.21 5.5 7.2
> >> 41005 80 10 30 0.21 6.84 2.6
> >> 41005 80 12 20 0.21 6.84 2.4
> >> 41005 79 6 14 0.217 5.61 3.55
> >> 41009 79 2 21 0.218 5.56 4.04
> >> 41009 79 5 27 0.218 6.4 3.12
> >> 41009 80 11 29 0.22 6.84 2.8
> >> 41009 78 5 28 0.232 6 3.2
> >> 41009 81 8 20 0.233 6.39 1.6
> >> 41009 79 9 30 0.24 5.6 7.5
> >> 41017 79 10 20 0.24 5.3 7.1
> >> 41017 80 7 30 0.24 6.73 2.6'
> >> df1 <- read.table(textConnection(x))
> >> names(df1) <- scan(what = character(),
> >>                      text = 'Code Y M D Q N O')
> >> df1$Code <- factor(df1$Code)
> >>
> >> # 1st option
> >> with(df1, tapply(Q, Code, min))
> >> #  41003 41005 41009 41017
> >> #  0.160 0.210 0.218 0.240
> >>
> >> # 2nd option
> >> aggregate(Q ~ Code, df1, min)
> >> #     Code     Q
> >> #  1 41003 0.160
> >> #  2 41005 0.210
> >> #  3 41009 0.218
> >> #  4 41017 0.240
> >>
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >> ?s 08:44 de 24/08/2022, javad bayat escreveu:
> >>> Dear all;
> >>> I am trying to get the minimum value of a column based on a factor column
> >>> of the same data frame. My data frame is like the below:
> >>>          Code               Y               M                D
> >>>    Q
> >>>        N              O
> >>> 41003 81 1 19 0.16 7.17 2.5
> >>> 41003 77 9 22 0.197 6.8 2.2
> >>> 41003 79 7 28 0.21 4.7 6.2
> >>> 41005 79 8 17 0.21 5.5 7.2
> >>> 41005 80 10 30 0.21 6.84 2.6
> >>> 41005 80 12 20 0.21 6.84 2.4
> >>> 41005 79 6 14 0.217 5.61 3.55
> >>> 41009 79 2 21 0.218 5.56 4.04
> >>> 41009 79 5 27 0.218 6.4 3.12
> >>> 41009 80 11 29 0.22 6.84 2.8
> >>> 41009 78 5 28 0.232 6 3.2
> >>> 41009 81 8 20 0.233 6.39 1.6
> >>> 41009 79 9 30 0.24 5.6 7.5
> >>> 41017 79 10 20 0.24 5.3 7.1
> >>> 41017 80 7 30 0.24 6.73 2.6
> >>>
> >>> I want to get the minimum value of the "Q" column with the whole row
> >>> values, according to the "Code"  column  which is a factor. Overall it
> >> will
> >>> give me 4 rows, with the value of "Q". Below is a code that I used but it
> >>> did not give me what I wanted.
> >>>
> >>>> x[which(x$Q == min(x$Q)),]
> >>>
> >>> Sincerely
> >>>
> >>>
> >>>
> >>
> >
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Thu Aug 25 20:10:45 2022
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Thu, 25 Aug 2022 23:40:45 +0530
Subject: [R] Fixing the size of R's Graphic Device
Message-ID: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>

Hi,

I am wondering if there is any way to fix the size (i.e. height and
width) of R's graphic device permanently. Every time I open R, and
create my first plot, the default size of the graphic device is fairly
small, and I need to adjust it manually to make it of comfortable
size.

Any help is really appreciated.

Thanks for your time.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Aug 25 20:24:03 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 25 Aug 2022 11:24:03 -0700
Subject: [R] Fixing the size of R's Graphic Device
In-Reply-To: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
References: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
Message-ID: <1623790F-CCC4-47F4-88A5-50D9715E6CB0@dcn.davis.ca.us>

What is your default graphics device? That is, what OS are you using?

On August 25, 2022 11:10:45 AM PDT, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>Hi,
>
>I am wondering if there is any way to fix the size (i.e. height and
>width) of R's graphic device permanently. Every time I open R, and
>create my first plot, the default size of the graphic device is fairly
>small, and I need to adjust it manually to make it of comfortable
>size.
>
>Any help is really appreciated.
>
>Thanks for your time.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Thu Aug 25 20:30:02 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 25 Aug 2022 11:30:02 -0700
Subject: [R] Fixing the size of R's Graphic Device
In-Reply-To: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
References: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
Message-ID: <CAGxFJbQ53jT0tP4XnDXWOK=ecQpUAmcdSTM4x1wOB2dqbd2V-Q@mail.gmail.com>

See ?Startup for various ways of automatically executing custom code
at R start up.

See ?setHook and ?.onLoad for how to run custom code when packages
(like grDevices) are loaded.

(But Jeff may be able to help you avoid even this if you respond to
his queries).

Cheers,
Bert



On Thu, Aug 25, 2022 at 11:11 AM Christofer Bogaso
<bogaso.christofer at gmail.com> wrote:
>
> Hi,
>
> I am wondering if there is any way to fix the size (i.e. height and
> width) of R's graphic device permanently. Every time I open R, and
> create my first plot, the default size of the graphic device is fairly
> small, and I need to adjust it manually to make it of comfortable
> size.
>
> Any help is really appreciated.
>
> Thanks for your time.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Thu Aug 25 20:30:26 2022
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Fri, 26 Aug 2022 00:00:26 +0530
Subject: [R] Fixing the size of R's Graphic Device
In-Reply-To: <1623790F-CCC4-47F4-88A5-50D9715E6CB0@dcn.davis.ca.us>
References: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
 <1623790F-CCC4-47F4-88A5-50D9715E6CB0@dcn.davis.ca.us>
Message-ID: <CA+dpOJmZq+3qB+acHRn0JTxNr+iLFjAp4xza48wUfDAU+uUGhg@mail.gmail.com>

Hi,

I am using a large screen monitor, the default size comes with fairly
small maybe smaller than a quarter.

Is there any to get exact measure of the default size?

I am using R in WIndows 11 OS

On Thu, Aug 25, 2022 at 11:54 PM Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
>
> What is your default graphics device? That is, what OS are you using?
>
> On August 25, 2022 11:10:45 AM PDT, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> >Hi,
> >
> >I am wondering if there is any way to fix the size (i.e. height and
> >width) of R's graphic device permanently. Every time I open R, and
> >create my first plot, the default size of the graphic device is fairly
> >small, and I need to adjust it manually to make it of comfortable
> >size.
> >
> >Any help is really appreciated.
> >
> >Thanks for your time.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From w||||@mwdun|@p @end|ng |rom gm@||@com  Thu Aug 25 20:44:21 2022
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Thu, 25 Aug 2022 11:44:21 -0700
Subject: [R] Fixing the size of R's Graphic Device
In-Reply-To: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
References: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
Message-ID: <CAHqSRuQTbSLo8eX5nxBU0W3=Nc=h_2g4wayAfDmvnXKumJoDRg@mail.gmail.com>

In one of your R startup files you can set options("device") to a function
that dev.new() will call when a new plot window is requested.  E.g.,

options(device=function()windows(width=3,height=4.5,xpos=-200,ypos=100))
graphics.off()
plot(1:10)


-Bill


On Thu, Aug 25, 2022 at 11:11 AM Christofer Bogaso <
bogaso.christofer at gmail.com> wrote:

> Hi,
>
> I am wondering if there is any way to fix the size (i.e. height and
> width) of R's graphic device permanently. Every time I open R, and
> create my first plot, the default size of the graphic device is fairly
> small, and I need to adjust it manually to make it of comfortable
> size.
>
> Any help is really appreciated.
>
> Thanks for your time.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Thu Aug 25 21:05:00 2022
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Fri, 26 Aug 2022 00:35:00 +0530
Subject: [R] Fixing the size of R's Graphic Device
In-Reply-To: <CAHqSRuQTbSLo8eX5nxBU0W3=Nc=h_2g4wayAfDmvnXKumJoDRg@mail.gmail.com>
References: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
 <CAHqSRuQTbSLo8eX5nxBU0W3=Nc=h_2g4wayAfDmvnXKumJoDRg@mail.gmail.com>
Message-ID: <CA+dpOJktN690GrWwWyhV=_0=pj=6AC-z3ah18pKk+vNTAKqGqw@mail.gmail.com>

Thanks.

But this is woking upto some number

For example

options(device=function()windows(width=303,height=354,xpos=-5,ypos=9))
graphics.off()
plot(1:10)

and

options(device=function()windows(width=303,height=394,xpos=-5,ypos=9))
graphics.off()
plot(1:10)

do not change the height of device

On Fri, Aug 26, 2022 at 12:14 AM Bill Dunlap <williamwdunlap at gmail.com> wrote:
>
> In one of your R startup files you can set options("device") to a function that dev.new() will call when a new plot window is requested.  E.g.,
>
> options(device=function()windows(width=3,height=4.5,xpos=-200,ypos=100))
> graphics.off()
> plot(1:10)
>
>
> -Bill
>
>
> On Thu, Aug 25, 2022 at 11:11 AM Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>
>> Hi,
>>
>> I am wondering if there is any way to fix the size (i.e. height and
>> width) of R's graphic device permanently. Every time I open R, and
>> create my first plot, the default size of the graphic device is fairly
>> small, and I need to adjust it manually to make it of comfortable
>> size.
>>
>> Any help is really appreciated.
>>
>> Thanks for your time.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From j@b@y@t194 @end|ng |rom gm@||@com  Thu Aug 25 20:02:03 2022
From: j@b@y@t194 @end|ng |rom gm@||@com (javad bayat)
Date: Thu, 25 Aug 2022 22:32:03 +0430
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>
Message-ID: <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>

;Dear all
First of all I appreciate you for the answers you have sent. I did the
codes that Rui provided and I got what I wanted.
"
res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
res <- do.call(rbind, res)
i <- order(unique(df1$Code))
res[order(i), ]
"
I think I should explain more about my request. I had a large data frame
(11059 rows and 16 columns). The Code column represented the stations code,
totally the number of stations were 128. At each station I had many
measured variables, like Q and N and O, and these variables were measured
in different years and days. The days that data were measured were
different for each station, and due to this reason I had different rows for
stations. For example, station number one (41009) had 158 rows and station
number 2 (41011) had 113 rows. Note that the station's codes are not in
order format (e.g smallest to largest).
Back to my request, I wanted to extract the minimum value of the Q for each
station (based on the Code column). The problem was that I wanted to have
other column values which were measured for this minimum of the Q.
I hope my explanation was clear enough. As I said before, I used the Rui's
codes and I got what I wanted. Although, other solutions provided by others
were all correct.

Regarding my request, unfortunately I faced another problem. I had to
extract the maximum of the Q and put it exactly under the minimum of the Q.
Something like the below one:
"

Code

              Y

              M

               D

           Q

            N

             O

41003

81

1

19

0.16

7.17

2.5

41003

79

8

17

10.21

5.5

7.2

41009

79

2

21

0.218

5.56

4.04
41009 79 10 20 12.24 5.3 7.1
.
.
.
.
"
I extract both min and max according to the codes, and I have 2 dataframes,
one for the minimum values and another for the max values. Both dataframe
have a Code column which is exactly similar.
Can I extract both min and max simultaneously or I have to combine two
dataframes?
I used the rbind and merge function but they did not give the desired
results.
> df3 = merge (df1, df2, by = "Code")
The result of this code adds a second dataframe as columns to the first
one. I want the first row of the second dataframe put below the first row
of the first dataframe and so on. I used a function to do this but it seems
it does not work correctly.

> fun2 = function(x,y){
                i = 1
                for(i in x) {
                      if (x[i,1] == y[i,1]){
                          rbind(x[i,],y[i,])
                          i = i+1
                }
                }
                }
> fun2(df1, df2)

Sincerely




























On Thu, Aug 25, 2022 at 9:08 PM <avi.e.gross at gmail.com> wrote:

> Yes, Timothy, the request was not seen by all of us as the same.
>
> Indeed if the request was to show a subset of the original data consisting
> of only the rows that were the minimum for each Code and also showed ties,
> then the solution is a tad more complex. I would then do something along
> the
> lines of what others showed such as generating another column showing the
> minimum for each row and then showing only rows that matched their value in
> two columns or whatever was needed.
>
> As noted, keeping the output in a specific order was not initially
> requested.
>
> Keeping the data in some order is a common enough request but in this
> situation, I suspect the order many might want would be the one showing the
> minimums in order, not the codes in the original order.
>
> -----Original Message-----
> From: Ebert,Timothy Aaron <tebert at ufl.edu>
> Sent: Thursday, August 25, 2022 11:59 AM
> To: avi.e.gross at gmail.com
> Cc: R-help at r-project.org
> Subject: RE: [R] Getting minimum value of a column according a factor
> column
> of a dataframe
>
> My assumption (maybe wrong) was that we needed to keep the other variables.
> I want to find the values of Y, M, D, N, and O for the minimum value of Q
> within each unique value of Code, keeping the data in the original order.
> All one need to do is filter Q in the original dataframe by your answer for
> minQ.
>
> Keeping the data in the original order seems unnecessary, but that is what
> was asked in a later post.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Aug 25 21:32:42 2022
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 25 Aug 2022 12:32:42 -0700
Subject: [R] Fixing the size of R's Graphic Device
In-Reply-To: <CA+dpOJktN690GrWwWyhV=_0=pj=6AC-z3ah18pKk+vNTAKqGqw@mail.gmail.com>
References: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
 <CAHqSRuQTbSLo8eX5nxBU0W3=Nc=h_2g4wayAfDmvnXKumJoDRg@mail.gmail.com>
 <CA+dpOJktN690GrWwWyhV=_0=pj=6AC-z3ah18pKk+vNTAKqGqw@mail.gmail.com>
Message-ID: <80F4E62E-DE19-4A7A-8F52-22EB48A6A327@dcn.davis.ca.us>

Please read ?windows. There is a limit to the default size, which I am guessing is related to the default window stacking behavior.

Also, units of width and height are in inches ... do you really have a 300 inch display?

On August 25, 2022 12:05:00 PM PDT, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>Thanks.
>
>But this is woking upto some number
>
>For example
>
>options(device=function()windows(width=303,height=354,xpos=-5,ypos=9))
>graphics.off()
>plot(1:10)
>
>and
>
>options(device=function()windows(width=303,height=394,xpos=-5,ypos=9))
>graphics.off()
>plot(1:10)
>
>do not change the height of device
>
>On Fri, Aug 26, 2022 at 12:14 AM Bill Dunlap <williamwdunlap at gmail.com> wrote:
>>
>> In one of your R startup files you can set options("device") to a function that dev.new() will call when a new plot window is requested.  E.g.,
>>
>> options(device=function()windows(width=3,height=4.5,xpos=-200,ypos=100))
>> graphics.off()
>> plot(1:10)
>>
>>
>> -Bill
>>
>>
>> On Thu, Aug 25, 2022 at 11:11 AM Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>>
>>> Hi,
>>>
>>> I am wondering if there is any way to fix the size (i.e. height and
>>> width) of R's graphic device permanently. Every time I open R, and
>>> create my first plot, the default size of the graphic device is fairly
>>> small, and I need to adjust it manually to make it of comfortable
>>> size.
>>>
>>> Any help is really appreciated.
>>>
>>> Thanks for your time.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Thu Aug 25 22:56:03 2022
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Fri, 26 Aug 2022 02:26:03 +0530
Subject: [R] Fixing the size of R's Graphic Device
In-Reply-To: <80F4E62E-DE19-4A7A-8F52-22EB48A6A327@dcn.davis.ca.us>
References: <CA+dpOJ=Vea5WDjrYwJMpfD6Xoygp08vXMjem6X4doW3btAibaw@mail.gmail.com>
 <CAHqSRuQTbSLo8eX5nxBU0W3=Nc=h_2g4wayAfDmvnXKumJoDRg@mail.gmail.com>
 <CA+dpOJktN690GrWwWyhV=_0=pj=6AC-z3ah18pKk+vNTAKqGqw@mail.gmail.com>
 <80F4E62E-DE19-4A7A-8F52-22EB48A6A327@dcn.davis.ca.us>
Message-ID: <CA+dpOJ=wEq_sm=hgW-jhi+e3eomHoExxdQg-LmJVgHNu03CqTA@mail.gmail.com>

I really dont know what could be the right value. So I am trying to
get the right value based on trial and error.

All I want is that the graphic windows' height should occupy the
height of my screen and width should be ~70% of width of my scream

I am using Mac desktop (21.5 inch 2017 model) but running Windows.

On Fri, Aug 26, 2022 at 1:02 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> Please read ?windows. There is a limit to the default size, which I am guessing is related to the default window stacking behavior.
>
> Also, units of width and height are in inches ... do you really have a 300 inch display?
>
> On August 25, 2022 12:05:00 PM PDT, Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> >Thanks.
> >
> >But this is woking upto some number
> >
> >For example
> >
> >options(device=function()windows(width=303,height=354,xpos=-5,ypos=9))
> >graphics.off()
> >plot(1:10)
> >
> >and
> >
> >options(device=function()windows(width=303,height=394,xpos=-5,ypos=9))
> >graphics.off()
> >plot(1:10)
> >
> >do not change the height of device
> >
> >On Fri, Aug 26, 2022 at 12:14 AM Bill Dunlap <williamwdunlap at gmail.com> wrote:
> >>
> >> In one of your R startup files you can set options("device") to a function that dev.new() will call when a new plot window is requested.  E.g.,
> >>
> >> options(device=function()windows(width=3,height=4.5,xpos=-200,ypos=100))
> >> graphics.off()
> >> plot(1:10)
> >>
> >>
> >> -Bill
> >>
> >>
> >> On Thu, Aug 25, 2022 at 11:11 AM Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> >>>
> >>> Hi,
> >>>
> >>> I am wondering if there is any way to fix the size (i.e. height and
> >>> width) of R's graphic device permanently. Every time I open R, and
> >>> create my first plot, the default size of the graphic device is fairly
> >>> small, and I need to adjust it manually to make it of comfortable
> >>> size.
> >>>
> >>> Any help is really appreciated.
> >>>
> >>> Thanks for your time.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From drj|m|emon @end|ng |rom gm@||@com  Thu Aug 25 23:42:45 2022
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 26 Aug 2022 07:42:45 +1000
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>
 <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
Message-ID: <CA+8X3fXWZSSz-wre+9JOG=jr5kNeGkKfMU1FaawUwReU=ZzUGg@mail.gmail.com>

Hi Javad,
In that case, just modify the function to extract the rows with both
the minimum and maximum Q from each station

df1<-read.table(text="Code Y  M  D  Q  N  O
 41003 81 1 19 0.16 7.17 2.5
 41003 77 9 22 0.197 6.8 2.2
 41003 79 7 28 0.21 4.7 6.2
 41005 79 8 17 0.21 5.5 7.2
 41005 80 10 30 0.21 6.84 2.6
 41005 80 12 20 0.21 6.84 2.4
 41005 79 6 14 0.217 5.61 3.55
 41009 79 2 21 0.218 5.56 4.04
 41009 79 5 27 0.218 6.4 3.12
 41009 80 11 29 0.22 6.84 2.8
 41009 78 5 28 0.232 6 3.2
 41009 81 8 20 0.233 6.39 1.6
 41009 79 9 30 0.24 5.6 7.5
 41017 79 10 20 0.24 5.3 7.1
 41017 80 7 30 0.24 6.73 2.6",
 stringsAsFactors=FALSE,header=TRUE)

# define a function that returns the desired rows
minmaxQrow<-function(x) return(x[c(which.min(x$Q),which.max(x$Q)),])
# apply the function to the data frame
df1a<-by(df1,df1$Code,minmaxQrow)
# set the result to the first element of the list
df1b<-df1a[[1]]
# rbind the remaining rows
for(i in 2:length(df1a)) df1b<-rbind(df1b,df1a[[i]])
# display the result
df1b

Jim

On Fri, Aug 26, 2022 at 5:25 AM javad bayat <j.bayat194 at gmail.com> wrote:
> ...
> I think I should explain more about my request. I had a large data frame
> (11059 rows and 16 columns). The Code column represented the stations code,
> totally the number of stations were 128. At each station I had many
> measured variables, like Q and N and O, and these variables were measured
> in different years and days. The days that data were measured were
> different for each station, and due to this reason I had different rows for
> stations. For example, station number one (41009) had 158 rows and station
> number 2 (41011) had 113 rows. Note that the station's codes are not in
> order format (e.g smallest to largest).
> Back to my request, I wanted to extract the minimum value of the Q for each
> station (based on the Code column). The problem was that I wanted to have
> other column values which were measured for this minimum of the Q.
> I hope my explanation was clear enough. As I said before, I used the Rui's
> codes and I got what I wanted. Although, other solutions provided by others
> were all correct.
>


From @vi@e@gross m@iii@g oii gm@ii@com  Thu Aug 25 23:50:45 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 25 Aug 2022 17:50:45 -0400
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>
 <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
Message-ID: <012e01d8b8cc$bb19f7b0$314de710$@gmail.com>

The requirements keep being clarified and it would have been very useful to know more in advance.

 

To be clear. My earlier suggestion was based on JUST wanting the minimum for each unique version of Code. Then you wanted it in the original order so that was handled by carefully making that a factor column in the order you wanted the output. Now the request is to throw back in ALL the columns for as many rows as are deemed minimums.

 

So, not in any way demeaning the various methods others offer, I suggest you look at database style joins. R has some in the core with names like merge() and other packages such as dplyr have all kinds of variations on a theme.

 

In my case, you can extend the part of my code that makes this as a second data.frame/tibble:

 

   Code  minQ

1 41003 0.160

2 88888 0.160

3 41005 0.210

4 41009 0.218

5 41017 0.240

 

Call it df2 as in:

 

mydf.min <-

  mydf %>%

  group_by(Code) %>%

  summarize(minQ = min(Q))

 

You now have the original thing I called mydf that has a column called Code and lots of other columns and you have a smaller one with fewer rows and columns called mydf.min and they share a single common column.

 

You want to merge these two using whatever kind of join makes sense. Dplyr offers an inner_join(), left_join(), right_join() and full_join() and you can tweak merge() and others to do similar things.

 

What you seem to want is to find all rows that share both a particular value for Code and at the same time a particular value for minQ in one versus Q in the other. You want to ignore all others.  What gets returned can have all the original columns and perhaps also the minQ column (which can be removed) and if two or more rows in one grouping share exactly the same minimum, may get slightly similar but different lines matched. Is that what you want? Do note matching floating point equally can be dangerous but in this case since the numbers were all just read in, should be fine.

 

You can play with it but I tried this:

 

test <- left_join(mydf.min, mydf, by=c("Code", "minQ" = "Q"))

 

The values returned (and not I added an 88888 category earlier in my data) look like this:

 

Code  minQ  Y  M  D    N    O

1 41003 0.160 81  1 19 7.17 2.50

2 88888 0.160 81  1 19 7.17 2.50

3 41005 0.210 79  8 17 5.50 7.20

4 41005 0.210 80 10 30 6.84 2.60

5 41005 0.210 80 12 20 6.84 2.40

6 41009 0.218 79  2 21 5.56 4.04

7 41009 0.218 79  5 27 6.40 3.12

8 41017 0.240 79 10 20 5.30 7.10

9 41017 0.240 80  7 30 6.73 2.60

 

You can see there are three minimum rows for Code 41005 for example and the join keeps them all.

 

You can trivially remove the minQ column by naming the original as Q or just removing it. You can again reorder things if you wish including by sorting on other columns ascending or descending using other functions/verbs.

 

As noted, this has to work for you with your larger code set and sometimes complex enough such requirements can be done many ways as it can be as much art as science. I personally would probably write most of the above as one long pipeline looking a bit like:

 

mydf <-   

  read.table(?) %>%

  mutate(Code=factor(..., levels=unique(...)))

 

result <-

  mydf %>%

  group_by(Code) %>%

  summarize(...) %>%

  left_join(mydf, by=c("Code", "minQ" = "Q")) %>%

  select(-minQ)

 

And of course it may be better to use the new R pipe operator if your version is new and so on, filling in whatever details make sense to you. At the end of the pipeline, you might want to use verbs that sort the data as described above.

 

My guess is you will now tell us about yet another condition suggestions like mine do not fulfill and I will likely then ignore ?

 

 

 

 

From: javad bayat <j.bayat194 at gmail.com> 
Sent: Thursday, August 25, 2022 2:02 PM
To: avi.e.gross at gmail.com
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

 

;Dear all

First of all I appreciate you for the answers you have sent. I did the codes that Rui provided and I got what I wanted.

"

res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
res <- do.call(rbind, res)
i <- order(unique(df1$Code))
res[order(i), ] 

"

I think I should explain more about my request. I had a large data frame (11059 rows and 16 columns). The Code column represented the stations code, totally the number of stations were 128. At each station I had many measured variables, like Q and N and O, and these variables were measured in different years and days. The days that data were measured were different for each station, and due to this reason I had different rows for stations. For example, station number one (41009) had 158 rows and station number 2 (41011) had 113 rows. Note that the station's codes are not in order format (e.g smallest to largest). 

Back to my request, I wanted to extract the minimum value of the Q for each station (based on the Code column). The problem was that I wanted to have other column values which were measured for this minimum of the Q. 

I hope my explanation was clear enough. As I said before, I used the Rui's codes and I got what I wanted. Although, other solutions provided by others were all correct. 

 

Regarding my request, unfortunately I faced another problem. I had to extract the maximum of the Q and put it exactly under the minimum of the Q. Something like the below one:

"


Code

              Y

              M

               D

           Q

            N

             O


41003

81

1

19

0.16

7.17

2.5


41003

79

8

17

10.21

5.5

7.2


41009

79

2

21

0.218

5.56

4.04



41009

79

10

20

12.24

5.3

7.1

.

.

.

.

"

I extract both min and max according to the codes, and I have 2 dataframes, one for the minimum values and another for the max values. Both dataframe have a Code column which is exactly similar.

Can I extract both min and max simultaneously or I have to combine two dataframes?

I used the rbind and merge function but they did not give the desired results.

> df3 = merge (df1, df2, by = "Code")

The result of this code adds a second dataframe as columns to the first one. I want the first row of the second dataframe put below the first row of the first dataframe and so on. I used a function to do this but it seems it does not work correctly.

 

> fun2 = function(x,y){
                i = 1
                for(i in x) {
                      if (x[i,1] == y[i,1]){
                          rbind(x[i,],y[i,])
                          i = i+1
                } 
                }                                        
                }
> fun2(df1, df2)

 

Sincerely

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

On Thu, Aug 25, 2022 at 9:08 PM <avi.e.gross at gmail.com <mailto:avi.e.gross at gmail.com> > wrote:

Yes, Timothy, the request was not seen by all of us as the same.

Indeed if the request was to show a subset of the original data consisting
of only the rows that were the minimum for each Code and also showed ties,
then the solution is a tad more complex. I would then do something along the
lines of what others showed such as generating another column showing the
minimum for each row and then showing only rows that matched their value in
two columns or whatever was needed.

As noted, keeping the output in a specific order was not initially
requested.

Keeping the data in some order is a common enough request but in this
situation, I suspect the order many might want would be the one showing the
minimums in order, not the codes in the original order. 

-----Original Message-----
From: Ebert,Timothy Aaron <tebert at ufl.edu <mailto:tebert at ufl.edu> > 
Sent: Thursday, August 25, 2022 11:59 AM
To: avi.e.gross at gmail.com <mailto:avi.e.gross at gmail.com> 
Cc: R-help at r-project.org <mailto:R-help at r-project.org> 
Subject: RE: [R] Getting minimum value of a column according a factor column
of a dataframe

My assumption (maybe wrong) was that we needed to keep the other variables.
I want to find the values of Y, M, D, N, and O for the minimum value of Q
within each unique value of Code, keeping the data in the original order.
All one need to do is filter Q in the original dataframe by your answer for
minQ.

Keeping the data in the original order seems unnecessary, but that is what
was asked in a later post.

______________________________________________
R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



-- 

Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com <mailto:bayat194 at yahoo.com> 


	[[alternative HTML version deleted]]


From du|c@|m@ @end|ng |rom b|gpond@com  Fri Aug 26 03:31:09 2022
From: du|c@|m@ @end|ng |rom b|gpond@com (dulcalma dulcalma)
Date: Fri, 26 Aug 2022 11:31:09 +1000 (AEST)
Subject: [R] Unicode chars
In-Reply-To: <557FD6B7-4725-4C56-8CA6-5F52DA0BB607@dcn.davis.ca.us>
References: <3adb4f19.9ce71.182d2f3fec2.Webtop.91@bigpond.com>
 <557FD6B7-4725-4C56-8CA6-5F52DA0BB607@dcn.davis.ca.us>
Message-ID: <15eb68de.1cbc7f.182d7c63c17.Webtop.90@bigpond.com>

Thank you for the reply I tried xeLatex at least once and possibly twice 
and it failed to compile

I now tried it again and found that I had missed the inputenc error when 
I tried before.
After removing the line it now compiles.

Thank you

Regards

Duncan


------ Original Message ------
From: "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
To: r-help at r-project.org; "dulcalma dulcalma" <dulcalma at bigpond.com>; 
r-help at R-project.org
Sent: Thursday, 25 Aug, 2022 At 1:25 PM
Subject: Re: [R] Unicode chars

Are you aware that pdfLatex does not support Unicode? You need to use 
xeLatex. But I don't use Sweave, so I don't know how you go about making 
that choice.

On August 24, 2022 8:03:02 PM PDT, dulcalma dulcalma 
<dulcalma at bigpond.com> wrote:
>
> Dear All
>
>
> I was trying the supplementary file?GS_main.R from
> https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.3475
>
> I have tried to prevent latex compilation from failing using Sweave 
> after trying all the online fixes I could find including using Rterm?
> I could fix it if it was in the input but not in the output?
> I am using R version 4.2 on windows 11 with 64 GB memory
>
>
> Sweave code
>
> \begin{small}
> <<r0>>=
> library(emdbook) # version 1.3.12
> library(bbmle) # version 1.0.23.1
> library(pbmcapply) # version 1.5.0?
> library(tidyverse) # version 1.3.0
> library(ggpubr) # version 0.4.0
> @ %%
>
>
> <<r7>>=
> summaryTable <-
> tibble(model = m.names,
> ? ? ? ?dim = m.dims[model],? ? ? ? ? ? ? ? ?
> ? ? ? ?score = m.loo[model],? ? ? ? ? ? ? ??
> ? ? ? ?delScore = score - min(score),? ? ? ?
> ? ? ? ?se_ose = se_ose[model],? ? ? ? ? ? ??
> ? ? ? ?se_mod = se_mod[model]) %>% arrange(dim) %>%? mutate(index = 
> 1:length(dim))
> summaryTable
> @ %%
>
>
> Output
> \begin{Schunk}
> \begin{Sinput}
> ? summaryTable <-
> ? tibble(model = m.names,
> ? ? ? ? ?dim = m.dims[model],? ? ? ? ? ? ? ? ?
> ? ? ? ? ?score = m.loo[model],? ? ? ? ? ? ? ??
> ? ? ? ? ?delScore = score - min(score),? ? ? ?
> ? ? ? ? ?se_ose = se_ose[model],? ? ? ? ? ? ??
> ? ? ? ? ?se_mod = se_mod[model]) %>% arrange(dim) %>%? mutate(index = 
> 1:length(dim))
> ? summaryTable
> \end{Sinput}
> \begin{Soutput}
> # A tibble: 10 ? 7
> ? ?model? ?dim score delScore se_ose se_mod index
> ? ?<chr> <int> <dbl>? ? <dbl>? <dbl>? <dbl> <int>
> ?1 zero? ? ? 2? 908.? ? 5.84? ? 40.1? ?4.14? ? ?1
> ?2 d? ? ? ? ?3? 904.? ? 1.71? ? 40.6? ?2.52? ? ?2
> ?3 q? ? ? ? ?3? 907.? ? 4.92? ? 40.2? ?3.80? ? ?3
> ?4 qd? ? ? ? 4? 902.? ? 0? ? ? ?40.7? ?0? ? ? ? 4
> ?5 qdi? ? ? ?5? 903.? ? 0.632? ?40.5? ?1.60? ? ?5
> ?6 x? ? ? ? ?6? 908.? ? 5.58? ? 40.2? ?5.53? ? ?6
> ?7 xq? ? ? ? 7? 907.? ? 4.81? ? 40.3? ?5.36? ? ?7
> ?8 xd? ? ? ? 7? 905.? ? 2.96? ? 40.5? ?5.04? ? ?8
> ?9 xqd? ? ? ?8? 903.? ? 0.908? ?40.5? ?4.52? ? ?9
> 10 xqdi? ? ? 9? 904.? ? 1.89? ? 40.4? ?4.70? ? 10
> \end{Soutput}
> \end{Schunk}
>
>
> The problem is the output from tibble?
> # A tibble: 10 ? 7
>
>
> the \times character is Unicode?U+00D7 or hex \xd7 and pdflatex 
> lualatex etc fail where this occurs
> Is there a way of adding "sanitizing" code in the output before 
> compiling?
> Or do I have to change it manually before compiling
>
>
> I do not want to switch to knitr.?
>
>
> Regards
>
>
> Duncan Mackay
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Aug 26 06:14:02 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 26 Aug 2022 05:14:02 +0100
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>
 <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
Message-ID: <857c53ca-b20d-a7d1-680e-41982e8fd297@sapo.pt>

Hello,

To return 2 rows for each Code, one for the min and another for the max, 
try the following.

I'm borrowing Bert's by() idea, it makes everything simpler.
There is a hack to have the original Code order kept, since the final 
result res should have two rows for each Code, see what is order()'ed below.


# the output has 2 consecutive rows with
# the same Code, so repeat the unique Codes
i <- order(rep(unique(df1$Code), each = 2))
res <- by(df1, df1$Code, \(x) x[c(which.min(x$Q), which.max(x$Q)), ])
res <- do.call(rbind, res)[order(i), ]

# remake the row names, they're ugly after rbind
row.names(res) <- NULL
res


Hope this helps,

Rui Barradas

?s 19:02 de 25/08/2022, javad bayat escreveu:
> ;Dear all
> First of all I appreciate you for the answers you have sent. I did the
> codes that Rui provided and I got what I wanted.
> "
> res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
> res <- do.call(rbind, res)
> i <- order(unique(df1$Code))
> res[order(i), ]
> "
> I think I should explain more about my request. I had a large data frame
> (11059 rows and 16 columns). The Code column represented the stations code,
> totally the number of stations were 128. At each station I had many
> measured variables, like Q and N and O, and these variables were measured
> in different years and days. The days that data were measured were
> different for each station, and due to this reason I had different rows for
> stations. For example, station number one (41009) had 158 rows and station
> number 2 (41011) had 113 rows. Note that the station's codes are not in
> order format (e.g smallest to largest).
> Back to my request, I wanted to extract the minimum value of the Q for each
> station (based on the Code column). The problem was that I wanted to have
> other column values which were measured for this minimum of the Q.
> I hope my explanation was clear enough. As I said before, I used the Rui's
> codes and I got what I wanted. Although, other solutions provided by others
> were all correct.
> 
> Regarding my request, unfortunately I faced another problem. I had to
> extract the maximum of the Q and put it exactly under the minimum of the Q.
> Something like the below one:
> "
> 
> Code
> 
>                Y
> 
>                M
> 
>                 D
> 
>             Q
> 
>              N
> 
>               O
> 
> 41003
> 
> 81
> 
> 1
> 
> 19
> 
> 0.16
> 
> 7.17
> 
> 2.5
> 
> 41003
> 
> 79
> 
> 8
> 
> 17
> 
> 10.21
> 
> 5.5
> 
> 7.2
> 
> 41009
> 
> 79
> 
> 2
> 
> 21
> 
> 0.218
> 
> 5.56
> 
> 4.04
> 41009 79 10 20 12.24 5.3 7.1
> .
> .
> .
> .
> "
> I extract both min and max according to the codes, and I have 2 dataframes,
> one for the minimum values and another for the max values. Both dataframe
> have a Code column which is exactly similar.
> Can I extract both min and max simultaneously or I have to combine two
> dataframes?
> I used the rbind and merge function but they did not give the desired
> results.
>> df3 = merge (df1, df2, by = "Code")
> The result of this code adds a second dataframe as columns to the first
> one. I want the first row of the second dataframe put below the first row
> of the first dataframe and so on. I used a function to do this but it seems
> it does not work correctly.
> 
>> fun2 = function(x,y){
>                  i = 1
>                  for(i in x) {
>                        if (x[i,1] == y[i,1]){
>                            rbind(x[i,],y[i,])
>                            i = i+1
>                  }
>                  }
>                  }
>> fun2(df1, df2)
> 
> Sincerely
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> On Thu, Aug 25, 2022 at 9:08 PM <avi.e.gross at gmail.com> wrote:
> 
>> Yes, Timothy, the request was not seen by all of us as the same.
>>
>> Indeed if the request was to show a subset of the original data consisting
>> of only the rows that were the minimum for each Code and also showed ties,
>> then the solution is a tad more complex. I would then do something along
>> the
>> lines of what others showed such as generating another column showing the
>> minimum for each row and then showing only rows that matched their value in
>> two columns or whatever was needed.
>>
>> As noted, keeping the output in a specific order was not initially
>> requested.
>>
>> Keeping the data in some order is a common enough request but in this
>> situation, I suspect the order many might want would be the one showing the
>> minimums in order, not the codes in the original order.
>>
>> -----Original Message-----
>> From: Ebert,Timothy Aaron <tebert at ufl.edu>
>> Sent: Thursday, August 25, 2022 11:59 AM
>> To: avi.e.gross at gmail.com
>> Cc: R-help at r-project.org
>> Subject: RE: [R] Getting minimum value of a column according a factor
>> column
>> of a dataframe
>>
>> My assumption (maybe wrong) was that we needed to keep the other variables.
>> I want to find the values of Y, M, D, N, and O for the minimum value of Q
>> within each unique value of Code, keeping the data in the original order.
>> All one need to do is filter Q in the original dataframe by your answer for
>> minQ.
>>
>> Keeping the data in the original order seems unnecessary, but that is what
>> was asked in a later post.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
>


From j@b@y@t194 @end|ng |rom gm@||@com  Fri Aug 26 06:38:50 2022
From: j@b@y@t194 @end|ng |rom gm@||@com (javad bayat)
Date: Fri, 26 Aug 2022 09:08:50 +0430
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <857c53ca-b20d-a7d1-680e-41982e8fd297@sapo.pt>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>
 <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
 <857c53ca-b20d-a7d1-680e-41982e8fd297@sapo.pt>
Message-ID: <CANTxAmLgRzn-N5JBS=DnijN3Q04f+XXgofEiiGKgvg9+vN_mLA@mail.gmail.com>

Dear all;
Many thanks for your suggestions and answers. The problem was solved by the
codes that Rui sent just now. Jim's codes also were correct but did not
give the results in original order.
Dear avi.e.gross at gmail.co,  has provided codes but they just result in
columns with max or min but they did not give the whole row's values
related to the Q min.
Thank you so much again.
Sincerely


On Fri, Aug 26, 2022 at 8:44 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> To return 2 rows for each Code, one for the min and another for the max,
> try the following.
>
> I'm borrowing Bert's by() idea, it makes everything simpler.
> There is a hack to have the original Code order kept, since the final
> result res should have two rows for each Code, see what is order()'ed
> below.
>
>
> # the output has 2 consecutive rows with
> # the same Code, so repeat the unique Codes
> i <- order(rep(unique(df1$Code), each = 2))
> res <- by(df1, df1$Code, \(x) x[c(which.min(x$Q), which.max(x$Q)), ])
> res <- do.call(rbind, res)[order(i), ]
>
> # remake the row names, they're ugly after rbind
> row.names(res) <- NULL
> res
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 19:02 de 25/08/2022, javad bayat escreveu:
> > ;Dear all
> > First of all I appreciate you for the answers you have sent. I did the
> > codes that Rui provided and I got what I wanted.
> > "
> > res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
> > res <- do.call(rbind, res)
> > i <- order(unique(df1$Code))
> > res[order(i), ]
> > "
> > I think I should explain more about my request. I had a large data frame
> > (11059 rows and 16 columns). The Code column represented the stations
> code,
> > totally the number of stations were 128. At each station I had many
> > measured variables, like Q and N and O, and these variables were measured
> > in different years and days. The days that data were measured were
> > different for each station, and due to this reason I had different rows
> for
> > stations. For example, station number one (41009) had 158 rows and
> station
> > number 2 (41011) had 113 rows. Note that the station's codes are not in
> > order format (e.g smallest to largest).
> > Back to my request, I wanted to extract the minimum value of the Q for
> each
> > station (based on the Code column). The problem was that I wanted to have
> > other column values which were measured for this minimum of the Q.
> > I hope my explanation was clear enough. As I said before, I used the
> Rui's
> > codes and I got what I wanted. Although, other solutions provided by
> others
> > were all correct.
> >
> > Regarding my request, unfortunately I faced another problem. I had to
> > extract the maximum of the Q and put it exactly under the minimum of the
> Q.
> > Something like the below one:
> > "
> >
> > Code
> >
> >                Y
> >
> >                M
> >
> >                 D
> >
> >             Q
> >
> >              N
> >
> >               O
> >
> > 41003
> >
> > 81
> >
> > 1
> >
> > 19
> >
> > 0.16
> >
> > 7.17
> >
> > 2.5
> >
> > 41003
> >
> > 79
> >
> > 8
> >
> > 17
> >
> > 10.21
> >
> > 5.5
> >
> > 7.2
> >
> > 41009
> >
> > 79
> >
> > 2
> >
> > 21
> >
> > 0.218
> >
> > 5.56
> >
> > 4.04
> > 41009 79 10 20 12.24 5.3 7.1
> > .
> > .
> > .
> > .
> > "
> > I extract both min and max according to the codes, and I have 2
> dataframes,
> > one for the minimum values and another for the max values. Both dataframe
> > have a Code column which is exactly similar.
> > Can I extract both min and max simultaneously or I have to combine two
> > dataframes?
> > I used the rbind and merge function but they did not give the desired
> > results.
> >> df3 = merge (df1, df2, by = "Code")
> > The result of this code adds a second dataframe as columns to the first
> > one. I want the first row of the second dataframe put below the first row
> > of the first dataframe and so on. I used a function to do this but it
> seems
> > it does not work correctly.
> >
> >> fun2 = function(x,y){
> >                  i = 1
> >                  for(i in x) {
> >                        if (x[i,1] == y[i,1]){
> >                            rbind(x[i,],y[i,])
> >                            i = i+1
> >                  }
> >                  }
> >                  }
> >> fun2(df1, df2)
> >
> > Sincerely
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > On Thu, Aug 25, 2022 at 9:08 PM <avi.e.gross at gmail.com> wrote:
> >
> >> Yes, Timothy, the request was not seen by all of us as the same.
> >>
> >> Indeed if the request was to show a subset of the original data
> consisting
> >> of only the rows that were the minimum for each Code and also showed
> ties,
> >> then the solution is a tad more complex. I would then do something along
> >> the
> >> lines of what others showed such as generating another column showing
> the
> >> minimum for each row and then showing only rows that matched their
> value in
> >> two columns or whatever was needed.
> >>
> >> As noted, keeping the output in a specific order was not initially
> >> requested.
> >>
> >> Keeping the data in some order is a common enough request but in this
> >> situation, I suspect the order many might want would be the one showing
> the
> >> minimums in order, not the codes in the original order.
> >>
> >> -----Original Message-----
> >> From: Ebert,Timothy Aaron <tebert at ufl.edu>
> >> Sent: Thursday, August 25, 2022 11:59 AM
> >> To: avi.e.gross at gmail.com
> >> Cc: R-help at r-project.org
> >> Subject: RE: [R] Getting minimum value of a column according a factor
> >> column
> >> of a dataframe
> >>
> >> My assumption (maybe wrong) was that we needed to keep the other
> variables.
> >> I want to find the values of Y, M, D, N, and O for the minimum value of
> Q
> >> within each unique value of Code, keeping the data in the original
> order.
> >> All one need to do is filter Q in the original dataframe by your answer
> for
> >> minQ.
> >>
> >> Keeping the data in the original order seems unnecessary, but that is
> what
> >> was asked in a later post.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
>


-- 
Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Aug 26 07:31:45 2022
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Fri, 26 Aug 2022 01:31:45 -0400
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <CANTxAmLgRzn-N5JBS=DnijN3Q04f+XXgofEiiGKgvg9+vN_mLA@mail.gmail.com>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>
 <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
 <857c53ca-b20d-a7d1-680e-41982e8fd297@sapo.pt>
 <CANTxAmLgRzn-N5JBS=DnijN3Q04f+XXgofEiiGKgvg9+vN_mLA@mail.gmail.com>
Message-ID: <011e01d8b90d$21fa8810$65ef9830$@gmail.com>

What you mean, Bayat, is you only read and tried my first reply BEFORE you expanded your request! LOL!

 

I really appreciate reading what other?s write here and different approaches. Rui had a beautiful idea of doubling his rows as a way to retain the order of two requests. It could generalize to more.

 

From: javad bayat <j.bayat194 at gmail.com> 
Sent: Friday, August 26, 2022 12:39 AM
To: Rui Barradas <ruipbarradas at sapo.pt>
Cc: avi.e.gross at gmail.com; R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

 

Dear all;

Many thanks for your suggestions and answers. The problem was solved by the codes that Rui sent just now. Jim's codes also were correct but did not give the results in original order. 

Dear avi.e.gross at gmail.co <mailto:avi.e.gross at gmail.co> ,  has provided codes but they just result in columns with max or min but they did not give the whole row's values related to the Q min.

Thank you so much again.

Sincerely

 

 

On Fri, Aug 26, 2022 at 8:44 AM Rui Barradas <ruipbarradas at sapo.pt <mailto:ruipbarradas at sapo.pt> > wrote:

Hello,

To return 2 rows for each Code, one for the min and another for the max, 
try the following.

I'm borrowing Bert's by() idea, it makes everything simpler.
There is a hack to have the original Code order kept, since the final 
result res should have two rows for each Code, see what is order()'ed below.


# the output has 2 consecutive rows with
# the same Code, so repeat the unique Codes
i <- order(rep(unique(df1$Code), each = 2))
res <- by(df1, df1$Code, \(x) x[c(which.min(x$Q), which.max(x$Q)), ])
res <- do.call(rbind, res)[order(i), ]

# remake the row names, they're ugly after rbind
row.names(res) <- NULL
res


Hope this helps,

Rui Barradas

?s 19:02 de 25/08/2022, javad bayat escreveu:
> ;Dear all
> First of all I appreciate you for the answers you have sent. I did the
> codes that Rui provided and I got what I wanted.
> "
> res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),])
> res <- do.call(rbind, res)
> i <- order(unique(df1$Code))
> res[order(i), ]
> "
> I think I should explain more about my request. I had a large data frame
> (11059 rows and 16 columns). The Code column represented the stations code,
> totally the number of stations were 128. At each station I had many
> measured variables, like Q and N and O, and these variables were measured
> in different years and days. The days that data were measured were
> different for each station, and due to this reason I had different rows for
> stations. For example, station number one (41009) had 158 rows and station
> number 2 (41011) had 113 rows. Note that the station's codes are not in
> order format (e.g smallest to largest).
> Back to my request, I wanted to extract the minimum value of the Q for each
> station (based on the Code column). The problem was that I wanted to have
> other column values which were measured for this minimum of the Q.
> I hope my explanation was clear enough. As I said before, I used the Rui's
> codes and I got what I wanted. Although, other solutions provided by others
> were all correct.
> 
> Regarding my request, unfortunately I faced another problem. I had to
> extract the maximum of the Q and put it exactly under the minimum of the Q.
> Something like the below one:
> "
> 
> Code
> 
>                Y
> 
>                M
> 
>                 D
> 
>             Q
> 
>              N
> 
>               O
> 
> 41003
> 
> 81
> 
> 1
> 
> 19
> 
> 0.16
> 
> 7.17
> 
> 2.5
> 
> 41003
> 
> 79
> 
> 8
> 
> 17
> 
> 10.21
> 
> 5.5
> 
> 7.2
> 
> 41009
> 
> 79
> 
> 2
> 
> 21
> 
> 0.218
> 
> 5.56
> 
> 4.04
> 41009 79 10 20 12.24 5.3 7.1
> .
> .
> .
> .
> "
> I extract both min and max according to the codes, and I have 2 dataframes,
> one for the minimum values and another for the max values. Both dataframe
> have a Code column which is exactly similar.
> Can I extract both min and max simultaneously or I have to combine two
> dataframes?
> I used the rbind and merge function but they did not give the desired
> results.
>> df3 = merge (df1, df2, by = "Code")
> The result of this code adds a second dataframe as columns to the first
> one. I want the first row of the second dataframe put below the first row
> of the first dataframe and so on. I used a function to do this but it seems
> it does not work correctly.
> 
>> fun2 = function(x,y){
>                  i = 1
>                  for(i in x) {
>                        if (x[i,1] == y[i,1]){
>                            rbind(x[i,],y[i,])
>                            i = i+1
>                  }
>                  }
>                  }
>> fun2(df1, df2)
> 
> Sincerely
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> On Thu, Aug 25, 2022 at 9:08 PM <avi.e.gross at gmail.com <mailto:avi.e.gross at gmail.com> > wrote:
> 
>> Yes, Timothy, the request was not seen by all of us as the same.
>>
>> Indeed if the request was to show a subset of the original data consisting
>> of only the rows that were the minimum for each Code and also showed ties,
>> then the solution is a tad more complex. I would then do something along
>> the
>> lines of what others showed such as generating another column showing the
>> minimum for each row and then showing only rows that matched their value in
>> two columns or whatever was needed.
>>
>> As noted, keeping the output in a specific order was not initially
>> requested.
>>
>> Keeping the data in some order is a common enough request but in this
>> situation, I suspect the order many might want would be the one showing the
>> minimums in order, not the codes in the original order.
>>
>> -----Original Message-----
>> From: Ebert,Timothy Aaron <tebert at ufl.edu <mailto:tebert at ufl.edu> >
>> Sent: Thursday, August 25, 2022 11:59 AM
>> To: avi.e.gross at gmail.com <mailto:avi.e.gross at gmail.com> 
>> Cc: R-help at r-project.org <mailto:R-help at r-project.org> 
>> Subject: RE: [R] Getting minimum value of a column according a factor
>> column
>> of a dataframe
>>
>> My assumption (maybe wrong) was that we needed to keep the other variables.
>> I want to find the values of Y, M, D, N, and O for the minimum value of Q
>> within each unique value of Code, keeping the data in the original order.
>> All one need to do is filter Q in the original dataframe by your answer for
>> minQ.
>>
>> Keeping the data in the original order seems unnecessary, but that is what
>> was asked in a later post.
>>
>> ______________________________________________
>> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 



-- 

Best Regards
Javad Bayat
M.Sc. Environment Engineering
Alternative Mail: bayat194 at yahoo.com <mailto:bayat194 at yahoo.com> 


	[[alternative HTML version deleted]]


From n|ckmwr@y @end|ng |rom gm@||@com  Fri Aug 26 10:43:56 2022
From: n|ckmwr@y @end|ng |rom gm@||@com (Nick Wray)
Date: Fri, 26 Aug 2022 09:43:56 +0100
Subject: [R] html into R
Message-ID: <CABxY9BMAbMUApNVAhb2-kYBgviUkNss1ZVne_vMVumKuumMNhA@mail.gmail.com>

hello - I need to download flow data for Scottish river catchments.  The
data is available from the Scottish Environmental protection Agency body
and that doesn't present a problem.  For example the API beneath will
access the 96 flow recordings on the River Tweed on Jan 1st 2020 at one
station:

https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0&request=getTimeseriesValues&ts_path=1/14972/Q/15m.Cmd&from=2020-01-01&to=2020-01-07&returnfields=Timestamp,Value,Quality%20Code


But this data comes as HTML.  I can copy and paste it into a text doc which
can then be read into R but that's slow and time-consuming.  I have tried
using the package "rvest" to import the HTML into R but I have got nowhere.

Can anyone give me any pointers as to how to do this?


Thanks Nick Wray

	[[alternative HTML version deleted]]


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Aug 26 10:57:24 2022
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 26 Aug 2022 10:57:24 +0200
Subject: [R] html into R
In-Reply-To: <CABxY9BMAbMUApNVAhb2-kYBgviUkNss1ZVne_vMVumKuumMNhA@mail.gmail.com>
References: <CABxY9BMAbMUApNVAhb2-kYBgviUkNss1ZVne_vMVumKuumMNhA@mail.gmail.com>
Message-ID: <CAJuCY5xATQBM7iPgD4uGi_kQcai-bcjznNo=9iDq=mDxA_yqiQ@mail.gmail.com>

Dear Nick,

A better solution is to add "&format=json" to the URL. Then the query
returns the data in JSON format.

Best regards,

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 26 aug. 2022 om 10:44 schreef Nick Wray <nickmwray at gmail.com>:

> hello - I need to download flow data for Scottish river catchments.  The
> data is available from the Scottish Environmental protection Agency body
> and that doesn't present a problem.  For example the API beneath will
> access the 96 flow recordings on the River Tweed on Jan 1st 2020 at one
> station:
>
>
> https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0&request=getTimeseriesValues&ts_path=1/14972/Q/15m.Cmd&from=2020-01-01&to=2020-01-07&returnfields=Timestamp,Value,Quality%20Code
>
>
> But this data comes as HTML.  I can copy and paste it into a text doc which
> can then be read into R but that's slow and time-consuming.  I have tried
> using the package "rvest" to import the HTML into R but I have got nowhere.
>
> Can anyone give me any pointers as to how to do this?
>
>
> Thanks Nick Wray
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Aug 26 12:53:43 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 26 Aug 2022 11:53:43 +0100
Subject: [R] html into R
In-Reply-To: <CABxY9BMAbMUApNVAhb2-kYBgviUkNss1ZVne_vMVumKuumMNhA@mail.gmail.com>
References: <CABxY9BMAbMUApNVAhb2-kYBgviUkNss1ZVne_vMVumKuumMNhA@mail.gmail.com>
Message-ID: <0c51429d-b30a-14a3-b65c-5e1d53651bde@sapo.pt>

Hello,

You can try the following. It worked with me.
Read from the link and post-process the html data extracting the element 
"table" and then the table itself.

This table has 3 rows before the actual table so the lapply below will 
get the table and its header.


library(httr)
library(rvest)


link <- 
"https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0&request=getTimeseriesValues&ts_path=1/14972/Q/15m.Cmd&from=2020-01-01&to=2020-01-07&returnfields=Timestamp,Value,Quality%20Code"

page <- read_html(link)
page |>
   html_elements("table") |>
   html_table(header = TRUE) |>
   lapply(\(x) {
     hdr <- unlist(x[3, ])
     y <- x[-(1:3), ]
     names(y) <- hdr
     y
   })


Hope this helps,

Rui Barradas

?s 09:43 de 26/08/2022, Nick Wray escreveu:
> hello - I need to download flow data for Scottish river catchments.  The
> data is available from the Scottish Environmental protection Agency body
> and that doesn't present a problem.  For example the API beneath will
> access the 96 flow recordings on the River Tweed on Jan 1st 2020 at one
> station:
> 
> https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0&request=getTimeseriesValues&ts_path=1/14972/Q/15m.Cmd&from=2020-01-01&to=2020-01-07&returnfields=Timestamp,Value,Quality%20Code
> 
> 
> But this data comes as HTML.  I can copy and paste it into a text doc which
> can then be read into R but that's slow and time-consuming.  I have tried
> using the package "rvest" to import the HTML into R but I have got nowhere.
> 
> Can anyone give me any pointers as to how to do this?
> 
> 
> Thanks Nick Wray
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Aug 26 12:57:55 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 26 Aug 2022 11:57:55 +0100
Subject: [R] html into R
In-Reply-To: <0c51429d-b30a-14a3-b65c-5e1d53651bde@sapo.pt>
References: <CABxY9BMAbMUApNVAhb2-kYBgviUkNss1ZVne_vMVumKuumMNhA@mail.gmail.com>
 <0c51429d-b30a-14a3-b65c-5e1d53651bde@sapo.pt>
Message-ID: <36eb9ed8-f158-70ef-9fe1-8c6ad2f42930@sapo.pt>

Sorry, there's simpler code. I used html_elements (plural) and the 
result is a list. Use html_element (singular) and the output is a tibble.


page |>
   html_element("table") |>
   html_table(header = TRUE) |>
   (\(x) {
     hdr <- unlist(x[3, ])
     y <- x[-(1:3), ]
     names(y) <- hdr
     y
   })()


Hope this helps,

Rui Barradas

?s 11:53 de 26/08/2022, Rui Barradas escreveu:
> Hello,
> 
> You can try the following. It worked with me.
> Read from the link and post-process the html data extracting the element 
> "table" and then the table itself.
> 
> This table has 3 rows before the actual table so the lapply below will 
> get the table and its header.
> 
> 
> library(httr)
> library(rvest)
> 
> 
> link <- 
> "https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0&request=getTimeseriesValues&ts_path=1/14972/Q/15m.Cmd&from=2020-01-01&to=2020-01-07&returnfields=Timestamp,Value,Quality%20Code" 
> 
> 
> page <- read_html(link)
> page |>
>  ? html_elements("table") |>
>  ? html_table(header = TRUE) |>
>  ? lapply(\(x) {
>  ??? hdr <- unlist(x[3, ])
>  ??? y <- x[-(1:3), ]
>  ??? names(y) <- hdr
>  ??? y
>  ? })
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 09:43 de 26/08/2022, Nick Wray escreveu:
>> hello - I need to download flow data for Scottish river catchments.? The
>> data is available from the Scottish Environmental protection Agency body
>> and that doesn't present a problem.? For example the API beneath will
>> access the 96 flow recordings on the River Tweed on Jan 1st 2020 at one
>> station:
>>
>> https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0&request=getTimeseriesValues&ts_path=1/14972/Q/15m.Cmd&from=2020-01-01&to=2020-01-07&returnfields=Timestamp,Value,Quality%20Code 
>>
>>
>>
>> But this data comes as HTML.? I can copy and paste it into a text doc 
>> which
>> can then be read into R but that's slow and time-consuming.? I have tried
>> using the package "rvest" to import the HTML into R but I have got 
>> nowhere.
>>
>> Can anyone give me any pointers as to how to do this?
>>
>>
>> Thanks Nick Wray
>>
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Fri Aug 26 14:34:18 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Fri, 26 Aug 2022 12:34:18 +0000
Subject: [R] 
 Getting minimum value of a column according a factor column of
 a dataframe
In-Reply-To: <857c53ca-b20d-a7d1-680e-41982e8fd297@sapo.pt>
References: <CANTxAmKVqHRbdJ+9SP9qsRE-Sarkq7AEH4zOTKedDWjE_G=y8A@mail.gmail.com>
 <7281c139-2b3f-9e72-fd3e-fa2fd78b54ae@sapo.pt>
 <CANTxAmJWMd4_5B7ojB88WpE+PatybnTfZ38e=A1h4iPNZa1Afw@mail.gmail.com>
 <BN6PR2201MB1553CA1ECD3DCDF1F6363DD6CF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <BN6PR2201MB1553B1736B4F2CF8B364B6DCCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <007f01d8b898$962fbb80$c28f3280$@gmail.com>
 <BN6PR2201MB15530F03CA291C34747FE96FCF729@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <00bd01d8b8a0$24f0d230$6ed27690$@gmail.com>
 <CANTxAmKa9nRDk40_r0JcmKdOgO7EpNVma2Mbp=TDDcwStWDARQ@mail.gmail.com>
 <857c53ca-b20d-a7d1-680e-41982e8fd297@sapo.pt>
Message-ID: <BN6PR2201MB15539695CA88EBD799F739E0CF759@BN6PR2201MB1553.namprd22.prod.outlook.com>

What is the point of this data frame? Beyond the fun of being able to morph data into any shape desired, there is usually some next step. I know that the values in Q are the minimum and maximum for each code, and the first value is the minimum and other value the maximum, but what comes next? If I want to get the difference in Y for the min and max Q I will need to reshape the data (or it would be easier that way). I cannot make a graph that identifies min and max Q, or an equivalent statistical analysis. It appears that no thought has gone into what to do about ties for min or max or what to do if min=max. The latter could happen if there is only one observation for Code or if there is no variability in Q.

While it is very educational for me to see other answers to such problems, I just fail to see the point in the final product in this case because it will take more work to make the data useful. The choices in regards to ties might make the analysis results more (or less) meaningful. Maybe the latter works if Q is a non-decreasing function, though in that case an alternative coding is to select the first and last value for each Code. Using min and max is only relevant if Q sometimes decreases. 

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
Sent: Friday, August 26, 2022 12:14 AM
To: javad bayat <j.bayat194 at gmail.com>; avi.e.gross at gmail.com
Cc: R-help at r-project.org
Subject: Re: [R] Getting minimum value of a column according a factor column of a dataframe

[External Email]

Hello,

To return 2 rows for each Code, one for the min and another for the max, try the following.

I'm borrowing Bert's by() idea, it makes everything simpler.
There is a hack to have the original Code order kept, since the final result res should have two rows for each Code, see what is order()'ed below.


# the output has 2 consecutive rows with # the same Code, so repeat the unique Codes i <- order(rep(unique(df1$Code), each = 2)) res <- by(df1, df1$Code, \(x) x[c(which.min(x$Q), which.max(x$Q)), ]) res <- do.call(rbind, res)[order(i), ]

# remake the row names, they're ugly after rbind
row.names(res) <- NULL
res


Hope this helps,

Rui Barradas

?s 19:02 de 25/08/2022, javad bayat escreveu:
> ;Dear all
> First of all I appreciate you for the answers you have sent. I did the 
> codes that Rui provided and I got what I wanted.
> "
> res <- lapply(split(df1, df1$Code), \(x) x[which.min(x$Q),]) res <- 
> do.call(rbind, res) i <- order(unique(df1$Code)) res[order(i), ] "
> I think I should explain more about my request. I had a large data 
> frame
> (11059 rows and 16 columns). The Code column represented the stations 
> code, totally the number of stations were 128. At each station I had 
> many measured variables, like Q and N and O, and these variables were 
> measured in different years and days. The days that data were measured 
> were different for each station, and due to this reason I had 
> different rows for stations. For example, station number one (41009) 
> had 158 rows and station number 2 (41011) had 113 rows. Note that the 
> station's codes are not in order format (e.g smallest to largest).
> Back to my request, I wanted to extract the minimum value of the Q for 
> each station (based on the Code column). The problem was that I wanted 
> to have other column values which were measured for this minimum of the Q.
> I hope my explanation was clear enough. As I said before, I used the 
> Rui's codes and I got what I wanted. Although, other solutions 
> provided by others were all correct.
>
> Regarding my request, unfortunately I faced another problem. I had to 
> extract the maximum of the Q and put it exactly under the minimum of the Q.
> Something like the below one:
> "
>
> Code
>
>                Y
>
>                M
>
>                 D
>
>             Q
>
>              N
>
>               O
>
> 41003
>
> 81
>
> 1
>
> 19
>
> 0.16
>
> 7.17
>
> 2.5
>
> 41003
>
> 79
>
> 8
>
> 17
>
> 10.21
>
> 5.5
>
> 7.2
>
> 41009
>
> 79
>
> 2
>
> 21
>
> 0.218
>
> 5.56
>
> 4.04
> 41009 79 10 20 12.24 5.3 7.1
> .
> .
> .
> .
> "
> I extract both min and max according to the codes, and I have 2 
> dataframes, one for the minimum values and another for the max values. 
> Both dataframe have a Code column which is exactly similar.
> Can I extract both min and max simultaneously or I have to combine two 
> dataframes?
> I used the rbind and merge function but they did not give the desired 
> results.
>> df3 = merge (df1, df2, by = "Code")
> The result of this code adds a second dataframe as columns to the 
> first one. I want the first row of the second dataframe put below the 
> first row of the first dataframe and so on. I used a function to do 
> this but it seems it does not work correctly.
>
>> fun2 = function(x,y){
>                  i = 1
>                  for(i in x) {
>                        if (x[i,1] == y[i,1]){
>                            rbind(x[i,],y[i,])
>                            i = i+1
>                  }
>                  }
>                  }
>> fun2(df1, df2)
>
> Sincerely
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> On Thu, Aug 25, 2022 at 9:08 PM <avi.e.gross at gmail.com> wrote:
>
>> Yes, Timothy, the request was not seen by all of us as the same.
>>
>> Indeed if the request was to show a subset of the original data 
>> consisting of only the rows that were the minimum for each Code and 
>> also showed ties, then the solution is a tad more complex. I would 
>> then do something along the lines of what others showed such as 
>> generating another column showing the minimum for each row and then 
>> showing only rows that matched their value in two columns or whatever 
>> was needed.
>>
>> As noted, keeping the output in a specific order was not initially 
>> requested.
>>
>> Keeping the data in some order is a common enough request but in this 
>> situation, I suspect the order many might want would be the one 
>> showing the minimums in order, not the codes in the original order.
>>
>> -----Original Message-----
>> From: Ebert,Timothy Aaron <tebert at ufl.edu>
>> Sent: Thursday, August 25, 2022 11:59 AM
>> To: avi.e.gross at gmail.com
>> Cc: R-help at r-project.org
>> Subject: RE: [R] Getting minimum value of a column according a factor 
>> column of a dataframe
>>
>> My assumption (maybe wrong) was that we needed to keep the other variables.
>> I want to find the values of Y, M, D, N, and O for the minimum value 
>> of Q within each unique value of Code, keeping the data in the original order.
>> All one need to do is filter Q in the original dataframe by your 
>> answer for minQ.
>>
>> Keeping the data in the original order seems unnecessary, but that is 
>> what was asked in a later post.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fsta
>> t.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40u
>> fl.edu%7C2cba5775104e4a1277fd08da871974f0%7C0d4da0f84a314d76ace60a623
>> 31e1b84%7C0%7C0%7C637970840667682013%7CUnknown%7CTWFpbGZsb3d8eyJWIjoi
>> MC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%
>> 7C%7C&amp;sdata=M1vxIIc8o0fs40Io8KmRUSyEYw1ad8mikRKzBuQd4rg%3D&amp;re
>> served=0
>> PLEASE do read the posting guide
>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.
>> r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.ed
>> u%7C2cba5775104e4a1277fd08da871974f0%7C0d4da0f84a314d76ace60a62331e1b
>> 84%7C0%7C0%7C637970840667682013%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wL
>> jAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>> &amp;sdata=vdMExt9qwLs13FqKl7Q2rUVCvTByUSIPYhVySnb5JI8%3D&amp;reserve
>> d=0 and provide commented, minimal, self-contained, reproducible 
>> code.
>>
>
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C2cba5775104e4a1277fd08da871974f0%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970840667682013%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=M1vxIIc8o0fs40Io8KmRUSyEYw1ad8mikRKzBuQd4rg%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C2cba5775104e4a1277fd08da871974f0%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637970840667682013%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=vdMExt9qwLs13FqKl7Q2rUVCvTByUSIPYhVySnb5JI8%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri Aug 26 15:26:10 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 26 Aug 2022 14:26:10 +0100
Subject: [R] html into R
In-Reply-To: <CABxY9BPU6V3my42c9TG=dZ_Dob8B98DpvYvjmozLmnvY5RUYLQ@mail.gmail.com>
References: <CABxY9BMAbMUApNVAhb2-kYBgviUkNss1ZVne_vMVumKuumMNhA@mail.gmail.com>
 <0c51429d-b30a-14a3-b65c-5e1d53651bde@sapo.pt>
 <36eb9ed8-f158-70ef-9fe1-8c6ad2f42930@sapo.pt>
 <CABxY9BMA+oGfMpd9CrSqHE=rbFLueY6OP3jHW-f0WjHuuWcBLg@mail.gmail.com>
 <CABxY9BPU6V3my42c9TG=dZ_Dob8B98DpvYvjmozLmnvY5RUYLQ@mail.gmail.com>
Message-ID: <66843b22-a1dd-bd73-bb35-d43c25a09886@sapo.pt>

Hello,

You are right, I haven't assigned the return value.
Start the pipe with something like

RiverTweed <- page |>
   rest_of_pipe


If you have more files to download and process, post an example of 2 or 
3 links and I'll see if it can be automated.

Also posting to R-help.


Hope this helps,

Rui Barradas



?s 14:18 de 26/08/2022, Nick Wray escreveu:
> Hi Rui again sorry to have to ask this but although your code prints out a
> tibble I can't seem to be able to identify it ie find its name  I assumed
> that it's "y" but outside of your code R tells me that y is not found.
> I've tried various things but nothing gives me the tibble as an object with
> a name that I can use...?   Thanks Nick
> 
> On Fri, 26 Aug 2022 at 13:37, Nick Wray <nickmwray at gmail.com> wrote:
> 
>> Hi Rui That is brilliant   Thanks v much - what is even better is that I
>> have loads of data from different years, rivers and stations to download,
>> each of which entails a different set of numerical inputs and was thinking
>> about how I could loop through the URL with different inputs to that - but
>> by using paste I can create all the links I need   Thanks again Nick
>>
>> On Fri, 26 Aug 2022 at 11:57, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>
>>> Sorry, there's simpler code. I used html_elements (plural) and the
>>> result is a list. Use html_element (singular) and the output is a tibble.
>>>
>>>
>>> page |>
>>>     html_element("table") |>
>>>     html_table(header = TRUE) |>
>>>     (\(x) {
>>>       hdr <- unlist(x[3, ])
>>>       y <- x[-(1:3), ]
>>>       names(y) <- hdr
>>>       y
>>>     })()
>>>
>>>
>>> Hope this helps,
>>>
>>> Rui Barradas
>>>
>>> ?s 11:53 de 26/08/2022, Rui Barradas escreveu:
>>>> Hello,
>>>>
>>>> You can try the following. It worked with me.
>>>> Read from the link and post-process the html data extracting the
>>> element
>>>> "table" and then the table itself.
>>>>
>>>> This table has 3 rows before the actual table so the lapply below will
>>>> get the table and its header.
>>>>
>>>>
>>>> library(httr)
>>>> library(rvest)
>>>>
>>>>
>>>> link <-
>>>> "
>>> https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0&request=getTimeseriesValues&ts_path=1/14972/Q/15m.Cmd&from=2020-01-01&to=2020-01-07&returnfields=Timestamp,Value,Quality%20Code"
>>>
>>>>
>>>>
>>>> page <- read_html(link)
>>>> page |>
>>>>     html_elements("table") |>
>>>>     html_table(header = TRUE) |>
>>>>     lapply(\(x) {
>>>>       hdr <- unlist(x[3, ])
>>>>       y <- x[-(1:3), ]
>>>>       names(y) <- hdr
>>>>       y
>>>>     })
>>>>
>>>>
>>>> Hope this helps,
>>>>
>>>> Rui Barradas
>>>>
>>>> ?s 09:43 de 26/08/2022, Nick Wray escreveu:
>>>>> hello - I need to download flow data for Scottish river catchments.
>>> The
>>>>> data is available from the Scottish Environmental protection Agency
>>> body
>>>>> and that doesn't present a problem.  For example the API beneath will
>>>>> access the 96 flow recordings on the River Tweed on Jan 1st 2020 at one
>>>>> station:
>>>>>
>>>>>
>>> https://timeseries.sepa.org.uk/KiWIS/KiWIS?service=kisters&type=queryServices&datasource=0&request=getTimeseriesValues&ts_path=1/14972/Q/15m.Cmd&from=2020-01-01&to=2020-01-07&returnfields=Timestamp,Value,Quality%20Code
>>>>>
>>>>>
>>>>>
>>>>> But this data comes as HTML.  I can copy and paste it into a text doc
>>>>> which
>>>>> can then be read into R but that's slow and time-consuming.  I have
>>> tried
>>>>> using the package "rvest" to import the HTML into R but I have got
>>>>> nowhere.
>>>>>
>>>>> Can anyone give me any pointers as to how to do this?
>>>>>
>>>>>
>>>>> Thanks Nick Wray
>>>>>
>>>>>      [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>


From v@|kremk @end|ng |rom gm@||@com  Fri Aug 26 16:41:45 2022
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Fri, 26 Aug 2022 09:41:45 -0500
Subject: [R] Correlate
In-Reply-To: <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
 <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
Message-ID: <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>

Hi John and Timothy

Thank you for your suggestion and help. Using the sample data, I did
carry out a test run and found a difference in the correlation result.

Option 1.
data_cor <- cor(dat[ , colnames(dat) != "x1"],  # Calculate correlations
                    dat$x1, method = "pearson", use = "complete.obs")
resulted
                 [,1]
    x2 -0.5845835
    x3 -0.4664220
    x4  0.7202837

Option 2.
 for(i in colnames(dat)){
      print(cor.test(dat[,i], dat$x1, method = "pearson", use =
"complete.obs")$estimate)
    }
           [,1]
x2  -0.7362030
x3  -0.04935132
x4   0.85766290

This was crosschecked  using Excel and other softwares and all matches
with option 2.
One of the factors that contributed for this difference  is loss of
information when we are using na.rm(). This is because that if x2 has
missing value but x3 and x4 don?t have then  na.rm()  removed  entire
row information including x3 and x4.

My question is there  a way to extract the number of rows (N)  used in
the correlation analysis?.
Thank you,

On Mon, Aug 22, 2022 at 1:00 PM John Fox <jfox at mcmaster.ca> wrote:
>
> Dear Val,
>
> On 2022-08-22 1:33 p.m., Val wrote:
> > For the time being  I am assuming the relationship across  variables
> > is linear.  I want get the values first  and detailed examining  of
> > the relationship will follow later.
>
> This seems backwards to me, but I'll refrain from commenting further on
> whether what you want to do makes sense and instead address how to do it
> (not, BTW, because I disagree with Bert's and Tim's remarks).
>
> Please see below:
>
> >
> > On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>
> >> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
> >>
> >> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
> >>
> >> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
> >>
> >> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
> >>
> >> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
> >>
> >> Large datasets can be very messy.
> >>
> >>
> >> Tim
> >>
> >> -----Original Message-----
> >> From: Bert Gunter <bgunter.4567 at gmail.com>
> >> Sent: Monday, August 22, 2022 12:07 PM
> >> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> >> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >> Subject: Re: [R] Correlate
> >>
> >> [External Email]
> >>
> >> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
> >> (search on "multiplicity adjustment" for details). :-(
> >>
> >> -- Bert
> >>
> >>
> >> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>
> >>> A somewhat clunky solution:
> >>> for(i in colnames(dat)){
> >>>    print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
> >>>    print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> >>> "complete.obs")$p.value) }
>
> Because of missing data, this computes the correlations on different
> subsets of the data. A simple solution is to filter the data for NAs:
>
> D <- na.omit(dat)
>
> More comments below:
>
> >>>
> >>> Rather than printing you could set up an array or list to save the results.
> >>>
> >>>
> >>> Tim
> >>>
> >>> -----Original Message-----
> >>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> >>> Sent: Monday, August 22, 2022 11:09 AM
> >>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>> Subject: [R] Correlate
> >>>
> >>> [External Email]
> >>>
> >>> Hi all,
> >>>
> >>> I have a data set with  ~250  variables(columns).  I want to calculate
> >>> the correlation of  one variable with the rest of the other variables
> >>> and also want  the p-values  for each correlation.  Please see the
> >>> sample data and my attempt.  I  have got the correlation but unable to
> >>> get the p-values
> >>>
> >>> dat <- read.table(text="x1 x2 x3 x4
> >>>             1.68 -0.96 -1.25  0.61
> >>>            -0.06  0.41  0.06 -0.96
> >>>                .    0.08  1.14  1.42
> >>>             0.80 -0.67  0.53 -0.68
> >>>             0.23 -0.97 -1.18 -0.78
> >>>            -1.03  1.11 -0.61    .
> >>>             2.15     .    0.02  0.66
> >>>             0.35 -0.37 -0.26  0.39
> >>>            -0.66  0.89   .    -1.49
> >>>             0.11  1.52  0.73  -1.03",header=TRUE)
> >>>
> >>> #change all to numeric
> >>>      dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
>
> This data manipulation is unnecessary. Just specify the argument
> na.strings="." to read.table().
>
> >>>
> >>>      data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
> >>> "pearson", use = "complete.obs")
> >>>
> >>> Result
> >>>                [,1]
> >>> x2 -0.5845835
> >>> x3 -0.4664220
> >>> x4  0.7202837
> >>>
> >>> How do I get the p-values ?
>
> Taking a somewhat different approach from cor.test(), you can apply
> Fisher's z-transformation (recall that D is the data filtered for NAs):
>
>  > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
>          [,1]
> x2 0.2462807
> x3 0.3812854
> x4 0.1156939
>
> I hope this helps,
>   John
>
> >>>
> >>> Thank you,
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>> =0 PLEASE do read the posting guide
> >>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>> =0 PLEASE do read the posting guide
> >>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://socialsciences.mcmaster.ca/jfox/
>


From j|ox @end|ng |rom mcm@@ter@c@  Fri Aug 26 18:04:08 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 26 Aug 2022 12:04:08 -0400
Subject: [R] Correlate
In-Reply-To: <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
 <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
 <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>
Message-ID: <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>

Dear Val,

On 2022-08-26 10:41 a.m., Val wrote:
> Hi John and Timothy
> 
> Thank you for your suggestion and help. Using the sample data, I did
> carry out a test run and found a difference in the correlation result.
> 
> Option 1.
> data_cor <- cor(dat[ , colnames(dat) != "x1"],  # Calculate correlations
>                      dat$x1, method = "pearson", use = "complete.obs")
> resulted
>                   [,1]
>      x2 -0.5845835
>      x3 -0.4664220
>      x4  0.7202837
> 
> Option 2.
>   for(i in colnames(dat)){
>        print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> "complete.obs")$estimate)
>      }
>             [,1]
> x2  -0.7362030
> x3  -0.04935132
> x4   0.85766290
> 
> This was crosschecked  using Excel and other softwares and all matches
> with option 2.
> One of the factors that contributed for this difference  is loss of
> information when we are using na.rm(). This is because that if x2 has
> missing value but x3 and x4 don?t have then  na.rm()  removed  entire
> row information including x3 and x4.

Yes, I already explained that in my previous message.

As well, cor() is capable of computing pairwise-complete correlations -- 
see ?cor.

There's not an obvious right answer here, however. Using 
pairwise-complete correlations can produce inconsistent (i.e., 
non-positive semi-definite) correlation matrices because correlations 
are computed on different subsets of the data.

There are much better ways to deal with missing data.

> 
> My question is there  a way to extract the number of rows (N)  used in
> the correlation analysis?.

I'm sure that there are many ways, but here is one that is very 
simple-minded and should be reasonably efficient for ~250 variables:

 > (nc <- ncol(dat))
[1] 4

 > R <- N <- matrix(NA, nc, nc)
 > diag(R) <- 1
 > for (i in 1:(nc - 1)){
+   for (j in (i + 1):nc){
+     R[i, j] <- R[j, i] <-cor(dat[, i], dat[, j], use="complete.obs")
+     N[i, j] <- N[j, i] <- nrow(na.omit(dat[, c(i, j)]))
+   }
+ }

 > round(R, 3)
        [,1]   [,2]   [,3]   [,4]
[1,]  1.000 -0.736 -0.049  0.858
[2,] -0.736  1.000  0.458 -0.428
[3,] -0.049  0.458  1.000  0.092
[4,]  0.858 -0.428  0.092  1.000

 > N
      [,1] [,2] [,3] [,4]
[1,]   NA    8    8    8
[2,]    8   NA    8    8
[3,]    8    8   NA    8
[4,]    8    8    8   NA

 > round(cor(dat, use="pairwise.complete.obs"), 3) # check
        x1     x2     x3     x4
x1  1.000 -0.736 -0.049  0.858
x2 -0.736  1.000  0.458 -0.428
x3 -0.049  0.458  1.000  0.092
x4  0.858 -0.428  0.092  1.000

More generally, I think that it's a good idea to learn a little bit 
about R programming if you intend to use R in your work. You'll then be 
able to solve problems like this yourself.

I hope this helps,
  John

> Thank you,
> 
> On Mon, Aug 22, 2022 at 1:00 PM John Fox <jfox at mcmaster.ca> wrote:
>>
>> Dear Val,
>>
>> On 2022-08-22 1:33 p.m., Val wrote:
>>> For the time being  I am assuming the relationship across  variables
>>> is linear.  I want get the values first  and detailed examining  of
>>> the relationship will follow later.
>>
>> This seems backwards to me, but I'll refrain from commenting further on
>> whether what you want to do makes sense and instead address how to do it
>> (not, BTW, because I disagree with Bert's and Tim's remarks).
>>
>> Please see below:
>>
>>>
>>> On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>>>>
>>>> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
>>>>
>>>> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
>>>>
>>>> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
>>>>
>>>> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
>>>>
>>>> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
>>>>
>>>> Large datasets can be very messy.
>>>>
>>>>
>>>> Tim
>>>>
>>>> -----Original Message-----
>>>> From: Bert Gunter <bgunter.4567 at gmail.com>
>>>> Sent: Monday, August 22, 2022 12:07 PM
>>>> To: Ebert,Timothy Aaron <tebert at ufl.edu>
>>>> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
>>>> Subject: Re: [R] Correlate
>>>>
>>>> [External Email]
>>>>
>>>> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
>>>> (search on "multiplicity adjustment" for details). :-(
>>>>
>>>> -- Bert
>>>>
>>>>
>>>> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>>>>>
>>>>> A somewhat clunky solution:
>>>>> for(i in colnames(dat)){
>>>>>     print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
>>>>>     print(cor.test(dat[,i], dat$x1, method = "pearson", use =
>>>>> "complete.obs")$p.value) }
>>
>> Because of missing data, this computes the correlations on different
>> subsets of the data. A simple solution is to filter the data for NAs:
>>
>> D <- na.omit(dat)
>>
>> More comments below:
>>
>>>>>
>>>>> Rather than printing you could set up an array or list to save the results.
>>>>>
>>>>>
>>>>> Tim
>>>>>
>>>>> -----Original Message-----
>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
>>>>> Sent: Monday, August 22, 2022 11:09 AM
>>>>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
>>>>> Subject: [R] Correlate
>>>>>
>>>>> [External Email]
>>>>>
>>>>> Hi all,
>>>>>
>>>>> I have a data set with  ~250  variables(columns).  I want to calculate
>>>>> the correlation of  one variable with the rest of the other variables
>>>>> and also want  the p-values  for each correlation.  Please see the
>>>>> sample data and my attempt.  I  have got the correlation but unable to
>>>>> get the p-values
>>>>>
>>>>> dat <- read.table(text="x1 x2 x3 x4
>>>>>              1.68 -0.96 -1.25  0.61
>>>>>             -0.06  0.41  0.06 -0.96
>>>>>                 .    0.08  1.14  1.42
>>>>>              0.80 -0.67  0.53 -0.68
>>>>>              0.23 -0.97 -1.18 -0.78
>>>>>             -1.03  1.11 -0.61    .
>>>>>              2.15     .    0.02  0.66
>>>>>              0.35 -0.37 -0.26  0.39
>>>>>             -0.66  0.89   .    -1.49
>>>>>              0.11  1.52  0.73  -1.03",header=TRUE)
>>>>>
>>>>> #change all to numeric
>>>>>       dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
>>
>> This data manipulation is unnecessary. Just specify the argument
>> na.strings="." to read.table().
>>
>>>>>
>>>>>       data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
>>>>> "pearson", use = "complete.obs")
>>>>>
>>>>> Result
>>>>>                 [,1]
>>>>> x2 -0.5845835
>>>>> x3 -0.4664220
>>>>> x4  0.7202837
>>>>>
>>>>> How do I get the p-values ?
>>
>> Taking a somewhat different approach from cor.test(), you can apply
>> Fisher's z-transformation (recall that D is the data filtered for NAs):
>>
>>   > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
>>           [,1]
>> x2 0.2462807
>> x3 0.3812854
>> x4 0.1156939
>>
>> I hope this helps,
>>    John
>>
>>>>>
>>>>> Thank you,
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
>>>>> =0 PLEASE do read the posting guide
>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
>>>>> =0 PLEASE do read the posting guide
>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> --
>> John Fox, Professor Emeritus
>> McMaster University
>> Hamilton, Ontario, Canada
>> web: https://socialsciences.mcmaster.ca/jfox/
>>
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From v@|kremk @end|ng |rom gm@||@com  Fri Aug 26 22:06:19 2022
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Fri, 26 Aug 2022 15:06:19 -0500
Subject: [R] Correlate
In-Reply-To: <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
 <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
 <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>
 <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>
Message-ID: <CAJOiR6YL60UeoDUp_zaYRKLQGngWVJbfqEeoZakX7oUkudjikg@mail.gmail.com>

Thank you John for your help and advice.

On Fri, Aug 26, 2022 at 11:04 AM John Fox <jfox at mcmaster.ca> wrote:
>
> Dear Val,
>
> On 2022-08-26 10:41 a.m., Val wrote:
> > Hi John and Timothy
> >
> > Thank you for your suggestion and help. Using the sample data, I did
> > carry out a test run and found a difference in the correlation result.
> >
> > Option 1.
> > data_cor <- cor(dat[ , colnames(dat) != "x1"],  # Calculate correlations
> >                      dat$x1, method = "pearson", use = "complete.obs")
> > resulted
> >                   [,1]
> >      x2 -0.5845835
> >      x3 -0.4664220
> >      x4  0.7202837
> >
> > Option 2.
> >   for(i in colnames(dat)){
> >        print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> > "complete.obs")$estimate)
> >      }
> >             [,1]
> > x2  -0.7362030
> > x3  -0.04935132
> > x4   0.85766290
> >
> > This was crosschecked  using Excel and other softwares and all matches
> > with option 2.
> > One of the factors that contributed for this difference  is loss of
> > information when we are using na.rm(). This is because that if x2 has
> > missing value but x3 and x4 don?t have then  na.rm()  removed  entire
> > row information including x3 and x4.
>
> Yes, I already explained that in my previous message.
>
> As well, cor() is capable of computing pairwise-complete correlations --
> see ?cor.
>
> There's not an obvious right answer here, however. Using
> pairwise-complete correlations can produce inconsistent (i.e.,
> non-positive semi-definite) correlation matrices because correlations
> are computed on different subsets of the data.
>
> There are much better ways to deal with missing data.
>
> >
> > My question is there  a way to extract the number of rows (N)  used in
> > the correlation analysis?.
>
> I'm sure that there are many ways, but here is one that is very
> simple-minded and should be reasonably efficient for ~250 variables:
>
>  > (nc <- ncol(dat))
> [1] 4
>
>  > R <- N <- matrix(NA, nc, nc)
>  > diag(R) <- 1
>  > for (i in 1:(nc - 1)){
> +   for (j in (i + 1):nc){
> +     R[i, j] <- R[j, i] <-cor(dat[, i], dat[, j], use="complete.obs")
> +     N[i, j] <- N[j, i] <- nrow(na.omit(dat[, c(i, j)]))
> +   }
> + }
>
>  > round(R, 3)
>         [,1]   [,2]   [,3]   [,4]
> [1,]  1.000 -0.736 -0.049  0.858
> [2,] -0.736  1.000  0.458 -0.428
> [3,] -0.049  0.458  1.000  0.092
> [4,]  0.858 -0.428  0.092  1.000
>
>  > N
>       [,1] [,2] [,3] [,4]
> [1,]   NA    8    8    8
> [2,]    8   NA    8    8
> [3,]    8    8   NA    8
> [4,]    8    8    8   NA
>
>  > round(cor(dat, use="pairwise.complete.obs"), 3) # check
>         x1     x2     x3     x4
> x1  1.000 -0.736 -0.049  0.858
> x2 -0.736  1.000  0.458 -0.428
> x3 -0.049  0.458  1.000  0.092
> x4  0.858 -0.428  0.092  1.000
>
> More generally, I think that it's a good idea to learn a little bit
> about R programming if you intend to use R in your work. You'll then be
> able to solve problems like this yourself.
>
> I hope this helps,
>   John
>
> > Thank you,
> >
> > On Mon, Aug 22, 2022 at 1:00 PM John Fox <jfox at mcmaster.ca> wrote:
> >>
> >> Dear Val,
> >>
> >> On 2022-08-22 1:33 p.m., Val wrote:
> >>> For the time being  I am assuming the relationship across  variables
> >>> is linear.  I want get the values first  and detailed examining  of
> >>> the relationship will follow later.
> >>
> >> This seems backwards to me, but I'll refrain from commenting further on
> >> whether what you want to do makes sense and instead address how to do it
> >> (not, BTW, because I disagree with Bert's and Tim's remarks).
> >>
> >> Please see below:
> >>
> >>>
> >>> On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>>
> >>>> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
> >>>>
> >>>> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
> >>>>
> >>>> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
> >>>>
> >>>> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
> >>>>
> >>>> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
> >>>>
> >>>> Large datasets can be very messy.
> >>>>
> >>>>
> >>>> Tim
> >>>>
> >>>> -----Original Message-----
> >>>> From: Bert Gunter <bgunter.4567 at gmail.com>
> >>>> Sent: Monday, August 22, 2022 12:07 PM
> >>>> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> >>>> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>>> Subject: Re: [R] Correlate
> >>>>
> >>>> [External Email]
> >>>>
> >>>> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
> >>>> (search on "multiplicity adjustment" for details). :-(
> >>>>
> >>>> -- Bert
> >>>>
> >>>>
> >>>> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>>>
> >>>>> A somewhat clunky solution:
> >>>>> for(i in colnames(dat)){
> >>>>>     print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
> >>>>>     print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> >>>>> "complete.obs")$p.value) }
> >>
> >> Because of missing data, this computes the correlations on different
> >> subsets of the data. A simple solution is to filter the data for NAs:
> >>
> >> D <- na.omit(dat)
> >>
> >> More comments below:
> >>
> >>>>>
> >>>>> Rather than printing you could set up an array or list to save the results.
> >>>>>
> >>>>>
> >>>>> Tim
> >>>>>
> >>>>> -----Original Message-----
> >>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> >>>>> Sent: Monday, August 22, 2022 11:09 AM
> >>>>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>>>> Subject: [R] Correlate
> >>>>>
> >>>>> [External Email]
> >>>>>
> >>>>> Hi all,
> >>>>>
> >>>>> I have a data set with  ~250  variables(columns).  I want to calculate
> >>>>> the correlation of  one variable with the rest of the other variables
> >>>>> and also want  the p-values  for each correlation.  Please see the
> >>>>> sample data and my attempt.  I  have got the correlation but unable to
> >>>>> get the p-values
> >>>>>
> >>>>> dat <- read.table(text="x1 x2 x3 x4
> >>>>>              1.68 -0.96 -1.25  0.61
> >>>>>             -0.06  0.41  0.06 -0.96
> >>>>>                 .    0.08  1.14  1.42
> >>>>>              0.80 -0.67  0.53 -0.68
> >>>>>              0.23 -0.97 -1.18 -0.78
> >>>>>             -1.03  1.11 -0.61    .
> >>>>>              2.15     .    0.02  0.66
> >>>>>              0.35 -0.37 -0.26  0.39
> >>>>>             -0.66  0.89   .    -1.49
> >>>>>              0.11  1.52  0.73  -1.03",header=TRUE)
> >>>>>
> >>>>> #change all to numeric
> >>>>>       dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
> >>
> >> This data manipulation is unnecessary. Just specify the argument
> >> na.strings="." to read.table().
> >>
> >>>>>
> >>>>>       data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
> >>>>> "pearson", use = "complete.obs")
> >>>>>
> >>>>> Result
> >>>>>                 [,1]
> >>>>> x2 -0.5845835
> >>>>> x3 -0.4664220
> >>>>> x4  0.7202837
> >>>>>
> >>>>> How do I get the p-values ?
> >>
> >> Taking a somewhat different approach from cor.test(), you can apply
> >> Fisher's z-transformation (recall that D is the data filtered for NAs):
> >>
> >>   > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
> >>           [,1]
> >> x2 0.2462807
> >> x3 0.3812854
> >> x4 0.1156939
> >>
> >> I hope this helps,
> >>    John
> >>
> >>>>>
> >>>>> Thank you,
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>>>> =0 PLEASE do read the posting guide
> >>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>>>> =0 PLEASE do read the posting guide
> >>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >> --
> >> John Fox, Professor Emeritus
> >> McMaster University
> >> Hamilton, Ontario, Canada
> >> web: https://socialsciences.mcmaster.ca/jfox/
> >>
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://socialsciences.mcmaster.ca/jfox/
>


From j|ox @end|ng |rom mcm@@ter@c@  Fri Aug 26 22:59:01 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 26 Aug 2022 16:59:01 -0400
Subject: [R] Correlate
In-Reply-To: <CAJOiR6YL60UeoDUp_zaYRKLQGngWVJbfqEeoZakX7oUkudjikg@mail.gmail.com>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
 <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
 <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>
 <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>
 <CAJOiR6YL60UeoDUp_zaYRKLQGngWVJbfqEeoZakX7oUkudjikg@mail.gmail.com>
Message-ID: <8fb95cf9-0906-3d73-81eb-65cda52ce17e@mcmaster.ca>

Dear Val,

On 2022-08-26 4:06 p.m., Val wrote:
> Thank you John for your help and advice.\

You're welcome, and it occurred to me that if you want the number of 
non-missing cases for each individual variable, you could add

	diag(N) <- colSums(!is.na(dat))

to the code I sent earlier.

Best,
  John

> 
> On Fri, Aug 26, 2022 at 11:04 AM John Fox <jfox at mcmaster.ca> wrote:
>>
>> Dear Val,
>>
>> On 2022-08-26 10:41 a.m., Val wrote:
>>> Hi John and Timothy
>>>
>>> Thank you for your suggestion and help. Using the sample data, I did
>>> carry out a test run and found a difference in the correlation result.
>>>
>>> Option 1.
>>> data_cor <- cor(dat[ , colnames(dat) != "x1"],  # Calculate correlations
>>>                       dat$x1, method = "pearson", use = "complete.obs")
>>> resulted
>>>                    [,1]
>>>       x2 -0.5845835
>>>       x3 -0.4664220
>>>       x4  0.7202837
>>>
>>> Option 2.
>>>    for(i in colnames(dat)){
>>>         print(cor.test(dat[,i], dat$x1, method = "pearson", use =
>>> "complete.obs")$estimate)
>>>       }
>>>              [,1]
>>> x2  -0.7362030
>>> x3  -0.04935132
>>> x4   0.85766290
>>>
>>> This was crosschecked  using Excel and other softwares and all matches
>>> with option 2.
>>> One of the factors that contributed for this difference  is loss of
>>> information when we are using na.rm(). This is because that if x2 has
>>> missing value but x3 and x4 don?t have then  na.rm()  removed  entire
>>> row information including x3 and x4.
>>
>> Yes, I already explained that in my previous message.
>>
>> As well, cor() is capable of computing pairwise-complete correlations --
>> see ?cor.
>>
>> There's not an obvious right answer here, however. Using
>> pairwise-complete correlations can produce inconsistent (i.e.,
>> non-positive semi-definite) correlation matrices because correlations
>> are computed on different subsets of the data.
>>
>> There are much better ways to deal with missing data.
>>
>>>
>>> My question is there  a way to extract the number of rows (N)  used in
>>> the correlation analysis?.
>>
>> I'm sure that there are many ways, but here is one that is very
>> simple-minded and should be reasonably efficient for ~250 variables:
>>
>>   > (nc <- ncol(dat))
>> [1] 4
>>
>>   > R <- N <- matrix(NA, nc, nc)
>>   > diag(R) <- 1
>>   > for (i in 1:(nc - 1)){
>> +   for (j in (i + 1):nc){
>> +     R[i, j] <- R[j, i] <-cor(dat[, i], dat[, j], use="complete.obs")
>> +     N[i, j] <- N[j, i] <- nrow(na.omit(dat[, c(i, j)]))
>> +   }
>> + }
>>
>>   > round(R, 3)
>>          [,1]   [,2]   [,3]   [,4]
>> [1,]  1.000 -0.736 -0.049  0.858
>> [2,] -0.736  1.000  0.458 -0.428
>> [3,] -0.049  0.458  1.000  0.092
>> [4,]  0.858 -0.428  0.092  1.000
>>
>>   > N
>>        [,1] [,2] [,3] [,4]
>> [1,]   NA    8    8    8
>> [2,]    8   NA    8    8
>> [3,]    8    8   NA    8
>> [4,]    8    8    8   NA
>>
>>   > round(cor(dat, use="pairwise.complete.obs"), 3) # check
>>          x1     x2     x3     x4
>> x1  1.000 -0.736 -0.049  0.858
>> x2 -0.736  1.000  0.458 -0.428
>> x3 -0.049  0.458  1.000  0.092
>> x4  0.858 -0.428  0.092  1.000
>>
>> More generally, I think that it's a good idea to learn a little bit
>> about R programming if you intend to use R in your work. You'll then be
>> able to solve problems like this yourself.
>>
>> I hope this helps,
>>    John
>>
>>> Thank you,
>>>
>>> On Mon, Aug 22, 2022 at 1:00 PM John Fox <jfox at mcmaster.ca> wrote:
>>>>
>>>> Dear Val,
>>>>
>>>> On 2022-08-22 1:33 p.m., Val wrote:
>>>>> For the time being  I am assuming the relationship across  variables
>>>>> is linear.  I want get the values first  and detailed examining  of
>>>>> the relationship will follow later.
>>>>
>>>> This seems backwards to me, but I'll refrain from commenting further on
>>>> whether what you want to do makes sense and instead address how to do it
>>>> (not, BTW, because I disagree with Bert's and Tim's remarks).
>>>>
>>>> Please see below:
>>>>
>>>>>
>>>>> On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>>>>>>
>>>>>> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
>>>>>>
>>>>>> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
>>>>>>
>>>>>> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
>>>>>>
>>>>>> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
>>>>>>
>>>>>> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
>>>>>>
>>>>>> Large datasets can be very messy.
>>>>>>
>>>>>>
>>>>>> Tim
>>>>>>
>>>>>> -----Original Message-----
>>>>>> From: Bert Gunter <bgunter.4567 at gmail.com>
>>>>>> Sent: Monday, August 22, 2022 12:07 PM
>>>>>> To: Ebert,Timothy Aaron <tebert at ufl.edu>
>>>>>> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
>>>>>> Subject: Re: [R] Correlate
>>>>>>
>>>>>> [External Email]
>>>>>>
>>>>>> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
>>>>>> (search on "multiplicity adjustment" for details). :-(
>>>>>>
>>>>>> -- Bert
>>>>>>
>>>>>>
>>>>>> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>>>>>>>
>>>>>>> A somewhat clunky solution:
>>>>>>> for(i in colnames(dat)){
>>>>>>>      print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
>>>>>>>      print(cor.test(dat[,i], dat$x1, method = "pearson", use =
>>>>>>> "complete.obs")$p.value) }
>>>>
>>>> Because of missing data, this computes the correlations on different
>>>> subsets of the data. A simple solution is to filter the data for NAs:
>>>>
>>>> D <- na.omit(dat)
>>>>
>>>> More comments below:
>>>>
>>>>>>>
>>>>>>> Rather than printing you could set up an array or list to save the results.
>>>>>>>
>>>>>>>
>>>>>>> Tim
>>>>>>>
>>>>>>> -----Original Message-----
>>>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
>>>>>>> Sent: Monday, August 22, 2022 11:09 AM
>>>>>>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
>>>>>>> Subject: [R] Correlate
>>>>>>>
>>>>>>> [External Email]
>>>>>>>
>>>>>>> Hi all,
>>>>>>>
>>>>>>> I have a data set with  ~250  variables(columns).  I want to calculate
>>>>>>> the correlation of  one variable with the rest of the other variables
>>>>>>> and also want  the p-values  for each correlation.  Please see the
>>>>>>> sample data and my attempt.  I  have got the correlation but unable to
>>>>>>> get the p-values
>>>>>>>
>>>>>>> dat <- read.table(text="x1 x2 x3 x4
>>>>>>>               1.68 -0.96 -1.25  0.61
>>>>>>>              -0.06  0.41  0.06 -0.96
>>>>>>>                  .    0.08  1.14  1.42
>>>>>>>               0.80 -0.67  0.53 -0.68
>>>>>>>               0.23 -0.97 -1.18 -0.78
>>>>>>>              -1.03  1.11 -0.61    .
>>>>>>>               2.15     .    0.02  0.66
>>>>>>>               0.35 -0.37 -0.26  0.39
>>>>>>>              -0.66  0.89   .    -1.49
>>>>>>>               0.11  1.52  0.73  -1.03",header=TRUE)
>>>>>>>
>>>>>>> #change all to numeric
>>>>>>>        dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
>>>>
>>>> This data manipulation is unnecessary. Just specify the argument
>>>> na.strings="." to read.table().
>>>>
>>>>>>>
>>>>>>>        data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
>>>>>>> "pearson", use = "complete.obs")
>>>>>>>
>>>>>>> Result
>>>>>>>                  [,1]
>>>>>>> x2 -0.5845835
>>>>>>> x3 -0.4664220
>>>>>>> x4  0.7202837
>>>>>>>
>>>>>>> How do I get the p-values ?
>>>>
>>>> Taking a somewhat different approach from cor.test(), you can apply
>>>> Fisher's z-transformation (recall that D is the data filtered for NAs):
>>>>
>>>>    > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
>>>>            [,1]
>>>> x2 0.2462807
>>>> x3 0.3812854
>>>> x4 0.1156939
>>>>
>>>> I hope this helps,
>>>>     John
>>>>
>>>>>>>
>>>>>>> Thank you,
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
>>>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
>>>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
>>>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>>>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
>>>>>>> =0 PLEASE do read the posting guide
>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
>>>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
>>>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
>>>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
>>>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
>>>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
>>>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
>>>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
>>>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>>>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
>>>>>>> =0 PLEASE do read the posting guide
>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
>>>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
>>>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
>>>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
>>>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
>>>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> --
>>>> John Fox, Professor Emeritus
>>>> McMaster University
>>>> Hamilton, Ontario, Canada
>>>> web: https://socialsciences.mcmaster.ca/jfox/
>>>>
>> --
>> John Fox, Professor Emeritus
>> McMaster University
>> Hamilton, Ontario, Canada
>> web: https://socialsciences.mcmaster.ca/jfox/
>>
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From bgunter@4567 @end|ng |rom gm@||@com  Sat Aug 27 00:10:06 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 26 Aug 2022 15:10:06 -0700
Subject: [R] Correlate
In-Reply-To: <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
 <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
 <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>
 <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>
Message-ID: <CAGxFJbQk2RnEbfVS3O=MTBoqzXJ12rQzH9iyeyYE+2Kp+h_R4g@mail.gmail.com>

And just for fun, here is a more "functional" version without the
explicit 'for()' loops that gives the lower triangular values of the
counts of nonmissing values for each correlation:

## (using John's notation)
w <- expand.grid(1:nc,1:nc)
f <- \(x) ifelse(x[1] <= x[2], NA, nrow(na.omit(dat[,x])))
ans <- matrix(apply(w,1,f), ncol = nc)

Note: This is just a matter of taste. I would be amazed if it's any
faster than using the explicit loops (they are implicit here, but
still there -- and it might be slower!). But maybe it reinforces
John's comment about the value of spending some(more) effort to learn
how to program in R. Gives you flexibility to program however you
like, which usually reduces bugs in one's code, I believe.

Cheers,
Bert







Note: this is just

On Fri, Aug 26, 2022 at 9:04 AM John Fox <jfox at mcmaster.ca> wrote:
>
> Dear Val,
>
> On 2022-08-26 10:41 a.m., Val wrote:
> > Hi John and Timothy
> >
> > Thank you for your suggestion and help. Using the sample data, I did
> > carry out a test run and found a difference in the correlation result.
> >
> > Option 1.
> > data_cor <- cor(dat[ , colnames(dat) != "x1"],  # Calculate correlations
> >                      dat$x1, method = "pearson", use = "complete.obs")
> > resulted
> >                   [,1]
> >      x2 -0.5845835
> >      x3 -0.4664220
> >      x4  0.7202837
> >
> > Option 2.
> >   for(i in colnames(dat)){
> >        print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> > "complete.obs")$estimate)
> >      }
> >             [,1]
> > x2  -0.7362030
> > x3  -0.04935132
> > x4   0.85766290
> >
> > This was crosschecked  using Excel and other softwares and all matches
> > with option 2.
> > One of the factors that contributed for this difference  is loss of
> > information when we are using na.rm(). This is because that if x2 has
> > missing value but x3 and x4 don?t have then  na.rm()  removed  entire
> > row information including x3 and x4.
>
> Yes, I already explained that in my previous message.
>
> As well, cor() is capable of computing pairwise-complete correlations --
> see ?cor.
>
> There's not an obvious right answer here, however. Using
> pairwise-complete correlations can produce inconsistent (i.e.,
> non-positive semi-definite) correlation matrices because correlations
> are computed on different subsets of the data.
>
> There are much better ways to deal with missing data.
>
> >
> > My question is there  a way to extract the number of rows (N)  used in
> > the correlation analysis?.
>
> I'm sure that there are many ways, but here is one that is very
> simple-minded and should be reasonably efficient for ~250 variables:
>
>  > (nc <- ncol(dat))
> [1] 4
>
>  > R <- N <- matrix(NA, nc, nc)
>  > diag(R) <- 1
>  > for (i in 1:(nc - 1)){
> +   for (j in (i + 1):nc){
> +     R[i, j] <- R[j, i] <-cor(dat[, i], dat[, j], use="complete.obs")
> +     N[i, j] <- N[j, i] <- nrow(na.omit(dat[, c(i, j)]))
> +   }
> + }
>
>  > round(R, 3)
>         [,1]   [,2]   [,3]   [,4]
> [1,]  1.000 -0.736 -0.049  0.858
> [2,] -0.736  1.000  0.458 -0.428
> [3,] -0.049  0.458  1.000  0.092
> [4,]  0.858 -0.428  0.092  1.000
>
>  > N
>       [,1] [,2] [,3] [,4]
> [1,]   NA    8    8    8
> [2,]    8   NA    8    8
> [3,]    8    8   NA    8
> [4,]    8    8    8   NA
>
>  > round(cor(dat, use="pairwise.complete.obs"), 3) # check
>         x1     x2     x3     x4
> x1  1.000 -0.736 -0.049  0.858
> x2 -0.736  1.000  0.458 -0.428
> x3 -0.049  0.458  1.000  0.092
> x4  0.858 -0.428  0.092  1.000
>
> More generally, I think that it's a good idea to learn a little bit
> about R programming if you intend to use R in your work. You'll then be
> able to solve problems like this yourself.
>
> I hope this helps,
>   John
>
> > Thank you,
> >
> > On Mon, Aug 22, 2022 at 1:00 PM John Fox <jfox at mcmaster.ca> wrote:
> >>
> >> Dear Val,
> >>
> >> On 2022-08-22 1:33 p.m., Val wrote:
> >>> For the time being  I am assuming the relationship across  variables
> >>> is linear.  I want get the values first  and detailed examining  of
> >>> the relationship will follow later.
> >>
> >> This seems backwards to me, but I'll refrain from commenting further on
> >> whether what you want to do makes sense and instead address how to do it
> >> (not, BTW, because I disagree with Bert's and Tim's remarks).
> >>
> >> Please see below:
> >>
> >>>
> >>> On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>>
> >>>> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
> >>>>
> >>>> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
> >>>>
> >>>> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
> >>>>
> >>>> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
> >>>>
> >>>> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
> >>>>
> >>>> Large datasets can be very messy.
> >>>>
> >>>>
> >>>> Tim
> >>>>
> >>>> -----Original Message-----
> >>>> From: Bert Gunter <bgunter.4567 at gmail.com>
> >>>> Sent: Monday, August 22, 2022 12:07 PM
> >>>> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> >>>> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>>> Subject: Re: [R] Correlate
> >>>>
> >>>> [External Email]
> >>>>
> >>>> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
> >>>> (search on "multiplicity adjustment" for details). :-(
> >>>>
> >>>> -- Bert
> >>>>
> >>>>
> >>>> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>>>
> >>>>> A somewhat clunky solution:
> >>>>> for(i in colnames(dat)){
> >>>>>     print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
> >>>>>     print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> >>>>> "complete.obs")$p.value) }
> >>
> >> Because of missing data, this computes the correlations on different
> >> subsets of the data. A simple solution is to filter the data for NAs:
> >>
> >> D <- na.omit(dat)
> >>
> >> More comments below:
> >>
> >>>>>
> >>>>> Rather than printing you could set up an array or list to save the results.
> >>>>>
> >>>>>
> >>>>> Tim
> >>>>>
> >>>>> -----Original Message-----
> >>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> >>>>> Sent: Monday, August 22, 2022 11:09 AM
> >>>>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>>>> Subject: [R] Correlate
> >>>>>
> >>>>> [External Email]
> >>>>>
> >>>>> Hi all,
> >>>>>
> >>>>> I have a data set with  ~250  variables(columns).  I want to calculate
> >>>>> the correlation of  one variable with the rest of the other variables
> >>>>> and also want  the p-values  for each correlation.  Please see the
> >>>>> sample data and my attempt.  I  have got the correlation but unable to
> >>>>> get the p-values
> >>>>>
> >>>>> dat <- read.table(text="x1 x2 x3 x4
> >>>>>              1.68 -0.96 -1.25  0.61
> >>>>>             -0.06  0.41  0.06 -0.96
> >>>>>                 .    0.08  1.14  1.42
> >>>>>              0.80 -0.67  0.53 -0.68
> >>>>>              0.23 -0.97 -1.18 -0.78
> >>>>>             -1.03  1.11 -0.61    .
> >>>>>              2.15     .    0.02  0.66
> >>>>>              0.35 -0.37 -0.26  0.39
> >>>>>             -0.66  0.89   .    -1.49
> >>>>>              0.11  1.52  0.73  -1.03",header=TRUE)
> >>>>>
> >>>>> #change all to numeric
> >>>>>       dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
> >>
> >> This data manipulation is unnecessary. Just specify the argument
> >> na.strings="." to read.table().
> >>
> >>>>>
> >>>>>       data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
> >>>>> "pearson", use = "complete.obs")
> >>>>>
> >>>>> Result
> >>>>>                 [,1]
> >>>>> x2 -0.5845835
> >>>>> x3 -0.4664220
> >>>>> x4  0.7202837
> >>>>>
> >>>>> How do I get the p-values ?
> >>
> >> Taking a somewhat different approach from cor.test(), you can apply
> >> Fisher's z-transformation (recall that D is the data filtered for NAs):
> >>
> >>   > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
> >>           [,1]
> >> x2 0.2462807
> >> x3 0.3812854
> >> x4 0.1156939
> >>
> >> I hope this helps,
> >>    John
> >>
> >>>>>
> >>>>> Thank you,
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>>>> =0 PLEASE do read the posting guide
> >>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>>>> =0 PLEASE do read the posting guide
> >>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >> --
> >> John Fox, Professor Emeritus
> >> McMaster University
> >> Hamilton, Ontario, Canada
> >> web: https://socialsciences.mcmaster.ca/jfox/
> >>
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://socialsciences.mcmaster.ca/jfox/
>


From v@|kremk @end|ng |rom gm@||@com  Sat Aug 27 00:27:50 2022
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Fri, 26 Aug 2022 17:27:50 -0500
Subject: [R] Correlate
In-Reply-To: <8fb95cf9-0906-3d73-81eb-65cda52ce17e@mcmaster.ca>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
 <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
 <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>
 <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>
 <CAJOiR6YL60UeoDUp_zaYRKLQGngWVJbfqEeoZakX7oUkudjikg@mail.gmail.com>
 <8fb95cf9-0906-3d73-81eb-65cda52ce17e@mcmaster.ca>
Message-ID: <CAJOiR6a=Yx+Owsg8LtMemST4TOxXi_pBL6ukE4maJQQxnx26+w@mail.gmail.com>

Thank you John again. I have got my result  in the form as shown below
      Variable   Corr     Pvalue   Nobs
            x1      1.000       0     165425





On Fri, Aug 26, 2022 at 3:59 PM John Fox <jfox at mcmaster.ca> wrote:
>
> Dear Val,
>
> On 2022-08-26 4:06 p.m., Val wrote:
> > Thank you John for your help and advice.\
>
> You're welcome, and it occurred to me that if you want the number of
> non-missing cases for each individual variable, you could add
>
>         diag(N) <- colSums(!is.na(dat))
>
> to the code I sent earlier.
>
> Best,
>   John
>
> >
> > On Fri, Aug 26, 2022 at 11:04 AM John Fox <jfox at mcmaster.ca> wrote:
> >>
> >> Dear Val,
> >>
> >> On 2022-08-26 10:41 a.m., Val wrote:
> >>> Hi John and Timothy
> >>>
> >>> Thank you for your suggestion and help. Using the sample data, I did
> >>> carry out a test run and found a difference in the correlation result.
> >>>
> >>> Option 1.
> >>> data_cor <- cor(dat[ , colnames(dat) != "x1"],  # Calculate correlations
> >>>                       dat$x1, method = "pearson", use = "complete.obs")
> >>> resulted
> >>>                    [,1]
> >>>       x2 -0.5845835
> >>>       x3 -0.4664220
> >>>       x4  0.7202837
> >>>
> >>> Option 2.
> >>>    for(i in colnames(dat)){
> >>>         print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> >>> "complete.obs")$estimate)
> >>>       }
> >>>              [,1]
> >>> x2  -0.7362030
> >>> x3  -0.04935132
> >>> x4   0.85766290
> >>>
> >>> This was crosschecked  using Excel and other softwares and all matches
> >>> with option 2.
> >>> One of the factors that contributed for this difference  is loss of
> >>> information when we are using na.rm(). This is because that if x2 has
> >>> missing value but x3 and x4 don?t have then  na.rm()  removed  entire
> >>> row information including x3 and x4.
> >>
> >> Yes, I already explained that in my previous message.
> >>
> >> As well, cor() is capable of computing pairwise-complete correlations --
> >> see ?cor.
> >>
> >> There's not an obvious right answer here, however. Using
> >> pairwise-complete correlations can produce inconsistent (i.e.,
> >> non-positive semi-definite) correlation matrices because correlations
> >> are computed on different subsets of the data.
> >>
> >> There are much better ways to deal with missing data.
> >>
> >>>
> >>> My question is there  a way to extract the number of rows (N)  used in
> >>> the correlation analysis?.
> >>
> >> I'm sure that there are many ways, but here is one that is very
> >> simple-minded and should be reasonably efficient for ~250 variables:
> >>
> >>   > (nc <- ncol(dat))
> >> [1] 4
> >>
> >>   > R <- N <- matrix(NA, nc, nc)
> >>   > diag(R) <- 1
> >>   > for (i in 1:(nc - 1)){
> >> +   for (j in (i + 1):nc){
> >> +     R[i, j] <- R[j, i] <-cor(dat[, i], dat[, j], use="complete.obs")
> >> +     N[i, j] <- N[j, i] <- nrow(na.omit(dat[, c(i, j)]))
> >> +   }
> >> + }
> >>
> >>   > round(R, 3)
> >>          [,1]   [,2]   [,3]   [,4]
> >> [1,]  1.000 -0.736 -0.049  0.858
> >> [2,] -0.736  1.000  0.458 -0.428
> >> [3,] -0.049  0.458  1.000  0.092
> >> [4,]  0.858 -0.428  0.092  1.000
> >>
> >>   > N
> >>        [,1] [,2] [,3] [,4]
> >> [1,]   NA    8    8    8
> >> [2,]    8   NA    8    8
> >> [3,]    8    8   NA    8
> >> [4,]    8    8    8   NA
> >>
> >>   > round(cor(dat, use="pairwise.complete.obs"), 3) # check
> >>          x1     x2     x3     x4
> >> x1  1.000 -0.736 -0.049  0.858
> >> x2 -0.736  1.000  0.458 -0.428
> >> x3 -0.049  0.458  1.000  0.092
> >> x4  0.858 -0.428  0.092  1.000
> >>
> >> More generally, I think that it's a good idea to learn a little bit
> >> about R programming if you intend to use R in your work. You'll then be
> >> able to solve problems like this yourself.
> >>
> >> I hope this helps,
> >>    John
> >>
> >>> Thank you,
> >>>
> >>> On Mon, Aug 22, 2022 at 1:00 PM John Fox <jfox at mcmaster.ca> wrote:
> >>>>
> >>>> Dear Val,
> >>>>
> >>>> On 2022-08-22 1:33 p.m., Val wrote:
> >>>>> For the time being  I am assuming the relationship across  variables
> >>>>> is linear.  I want get the values first  and detailed examining  of
> >>>>> the relationship will follow later.
> >>>>
> >>>> This seems backwards to me, but I'll refrain from commenting further on
> >>>> whether what you want to do makes sense and instead address how to do it
> >>>> (not, BTW, because I disagree with Bert's and Tim's remarks).
> >>>>
> >>>> Please see below:
> >>>>
> >>>>>
> >>>>> On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>>>>
> >>>>>> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
> >>>>>>
> >>>>>> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
> >>>>>>
> >>>>>> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
> >>>>>>
> >>>>>> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
> >>>>>>
> >>>>>> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
> >>>>>>
> >>>>>> Large datasets can be very messy.
> >>>>>>
> >>>>>>
> >>>>>> Tim
> >>>>>>
> >>>>>> -----Original Message-----
> >>>>>> From: Bert Gunter <bgunter.4567 at gmail.com>
> >>>>>> Sent: Monday, August 22, 2022 12:07 PM
> >>>>>> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> >>>>>> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>>>>> Subject: Re: [R] Correlate
> >>>>>>
> >>>>>> [External Email]
> >>>>>>
> >>>>>> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
> >>>>>> (search on "multiplicity adjustment" for details). :-(
> >>>>>>
> >>>>>> -- Bert
> >>>>>>
> >>>>>>
> >>>>>> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>>>>>
> >>>>>>> A somewhat clunky solution:
> >>>>>>> for(i in colnames(dat)){
> >>>>>>>      print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
> >>>>>>>      print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> >>>>>>> "complete.obs")$p.value) }
> >>>>
> >>>> Because of missing data, this computes the correlations on different
> >>>> subsets of the data. A simple solution is to filter the data for NAs:
> >>>>
> >>>> D <- na.omit(dat)
> >>>>
> >>>> More comments below:
> >>>>
> >>>>>>>
> >>>>>>> Rather than printing you could set up an array or list to save the results.
> >>>>>>>
> >>>>>>>
> >>>>>>> Tim
> >>>>>>>
> >>>>>>> -----Original Message-----
> >>>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> >>>>>>> Sent: Monday, August 22, 2022 11:09 AM
> >>>>>>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>>>>>> Subject: [R] Correlate
> >>>>>>>
> >>>>>>> [External Email]
> >>>>>>>
> >>>>>>> Hi all,
> >>>>>>>
> >>>>>>> I have a data set with  ~250  variables(columns).  I want to calculate
> >>>>>>> the correlation of  one variable with the rest of the other variables
> >>>>>>> and also want  the p-values  for each correlation.  Please see the
> >>>>>>> sample data and my attempt.  I  have got the correlation but unable to
> >>>>>>> get the p-values
> >>>>>>>
> >>>>>>> dat <- read.table(text="x1 x2 x3 x4
> >>>>>>>               1.68 -0.96 -1.25  0.61
> >>>>>>>              -0.06  0.41  0.06 -0.96
> >>>>>>>                  .    0.08  1.14  1.42
> >>>>>>>               0.80 -0.67  0.53 -0.68
> >>>>>>>               0.23 -0.97 -1.18 -0.78
> >>>>>>>              -1.03  1.11 -0.61    .
> >>>>>>>               2.15     .    0.02  0.66
> >>>>>>>               0.35 -0.37 -0.26  0.39
> >>>>>>>              -0.66  0.89   .    -1.49
> >>>>>>>               0.11  1.52  0.73  -1.03",header=TRUE)
> >>>>>>>
> >>>>>>> #change all to numeric
> >>>>>>>        dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
> >>>>
> >>>> This data manipulation is unnecessary. Just specify the argument
> >>>> na.strings="." to read.table().
> >>>>
> >>>>>>>
> >>>>>>>        data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
> >>>>>>> "pearson", use = "complete.obs")
> >>>>>>>
> >>>>>>> Result
> >>>>>>>                  [,1]
> >>>>>>> x2 -0.5845835
> >>>>>>> x3 -0.4664220
> >>>>>>> x4  0.7202837
> >>>>>>>
> >>>>>>> How do I get the p-values ?
> >>>>
> >>>> Taking a somewhat different approach from cor.test(), you can apply
> >>>> Fisher's z-transformation (recall that D is the data filtered for NAs):
> >>>>
> >>>>    > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
> >>>>            [,1]
> >>>> x2 0.2462807
> >>>> x3 0.3812854
> >>>> x4 0.1156939
> >>>>
> >>>> I hope this helps,
> >>>>     John
> >>>>
> >>>>>>>
> >>>>>>> Thank you,
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>>>>>> =0 PLEASE do read the posting guide
> >>>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>>>>>> =0 PLEASE do read the posting guide
> >>>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>> --
> >>>> John Fox, Professor Emeritus
> >>>> McMaster University
> >>>> Hamilton, Ontario, Canada
> >>>> web: https://socialsciences.mcmaster.ca/jfox/
> >>>>
> >> --
> >> John Fox, Professor Emeritus
> >> McMaster University
> >> Hamilton, Ontario, Canada
> >> web: https://socialsciences.mcmaster.ca/jfox/
> >>
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://socialsciences.mcmaster.ca/jfox/
>


From j|ox @end|ng |rom mcm@@ter@c@  Sat Aug 27 00:57:28 2022
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 26 Aug 2022 18:57:28 -0400
Subject: [R] Correlate
In-Reply-To: <CAJOiR6a=Yx+Owsg8LtMemST4TOxXi_pBL6ukE4maJQQxnx26+w@mail.gmail.com>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
 <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
 <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>
 <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>
 <CAJOiR6YL60UeoDUp_zaYRKLQGngWVJbfqEeoZakX7oUkudjikg@mail.gmail.com>
 <8fb95cf9-0906-3d73-81eb-65cda52ce17e@mcmaster.ca>
 <CAJOiR6a=Yx+Owsg8LtMemST4TOxXi_pBL6ukE4maJQQxnx26+w@mail.gmail.com>
Message-ID: <907a5668-a528-ba34-01ee-21c997dc2972@mcmaster.ca>

Dear Val,

On 2022-08-26 6:27 p.m., Val wrote:
> Thank you John again. I have got my result  in the form as shown below
>        Variable   Corr     Pvalue   Nobs
>              x1      1.000       0     165425

This raises two questions: (1) Are p-values interesting when you have n 
 > 165K cases? (2) What's the point of testing a correlation between a 
variable and itself?

Best,
  John

> 
> 
> 
> 
> 
> On Fri, Aug 26, 2022 at 3:59 PM John Fox <jfox at mcmaster.ca> wrote:
>>
>> Dear Val,
>>
>> On 2022-08-26 4:06 p.m., Val wrote:
>>> Thank you John for your help and advice.\
>>
>> You're welcome, and it occurred to me that if you want the number of
>> non-missing cases for each individual variable, you could add
>>
>>          diag(N) <- colSums(!is.na(dat))
>>
>> to the code I sent earlier.
>>
>> Best,
>>    John
>>
>>>
>>> On Fri, Aug 26, 2022 at 11:04 AM John Fox <jfox at mcmaster.ca> wrote:
>>>>
>>>> Dear Val,
>>>>
>>>> On 2022-08-26 10:41 a.m., Val wrote:
>>>>> Hi John and Timothy
>>>>>
>>>>> Thank you for your suggestion and help. Using the sample data, I did
>>>>> carry out a test run and found a difference in the correlation result.
>>>>>
>>>>> Option 1.
>>>>> data_cor <- cor(dat[ , colnames(dat) != "x1"],  # Calculate correlations
>>>>>                        dat$x1, method = "pearson", use = "complete.obs")
>>>>> resulted
>>>>>                     [,1]
>>>>>        x2 -0.5845835
>>>>>        x3 -0.4664220
>>>>>        x4  0.7202837
>>>>>
>>>>> Option 2.
>>>>>     for(i in colnames(dat)){
>>>>>          print(cor.test(dat[,i], dat$x1, method = "pearson", use =
>>>>> "complete.obs")$estimate)
>>>>>        }
>>>>>               [,1]
>>>>> x2  -0.7362030
>>>>> x3  -0.04935132
>>>>> x4   0.85766290
>>>>>
>>>>> This was crosschecked  using Excel and other softwares and all matches
>>>>> with option 2.
>>>>> One of the factors that contributed for this difference  is loss of
>>>>> information when we are using na.rm(). This is because that if x2 has
>>>>> missing value but x3 and x4 don?t have then  na.rm()  removed  entire
>>>>> row information including x3 and x4.
>>>>
>>>> Yes, I already explained that in my previous message.
>>>>
>>>> As well, cor() is capable of computing pairwise-complete correlations --
>>>> see ?cor.
>>>>
>>>> There's not an obvious right answer here, however. Using
>>>> pairwise-complete correlations can produce inconsistent (i.e.,
>>>> non-positive semi-definite) correlation matrices because correlations
>>>> are computed on different subsets of the data.
>>>>
>>>> There are much better ways to deal with missing data.
>>>>
>>>>>
>>>>> My question is there  a way to extract the number of rows (N)  used in
>>>>> the correlation analysis?.
>>>>
>>>> I'm sure that there are many ways, but here is one that is very
>>>> simple-minded and should be reasonably efficient for ~250 variables:
>>>>
>>>>    > (nc <- ncol(dat))
>>>> [1] 4
>>>>
>>>>    > R <- N <- matrix(NA, nc, nc)
>>>>    > diag(R) <- 1
>>>>    > for (i in 1:(nc - 1)){
>>>> +   for (j in (i + 1):nc){
>>>> +     R[i, j] <- R[j, i] <-cor(dat[, i], dat[, j], use="complete.obs")
>>>> +     N[i, j] <- N[j, i] <- nrow(na.omit(dat[, c(i, j)]))
>>>> +   }
>>>> + }
>>>>
>>>>    > round(R, 3)
>>>>           [,1]   [,2]   [,3]   [,4]
>>>> [1,]  1.000 -0.736 -0.049  0.858
>>>> [2,] -0.736  1.000  0.458 -0.428
>>>> [3,] -0.049  0.458  1.000  0.092
>>>> [4,]  0.858 -0.428  0.092  1.000
>>>>
>>>>    > N
>>>>         [,1] [,2] [,3] [,4]
>>>> [1,]   NA    8    8    8
>>>> [2,]    8   NA    8    8
>>>> [3,]    8    8   NA    8
>>>> [4,]    8    8    8   NA
>>>>
>>>>    > round(cor(dat, use="pairwise.complete.obs"), 3) # check
>>>>           x1     x2     x3     x4
>>>> x1  1.000 -0.736 -0.049  0.858
>>>> x2 -0.736  1.000  0.458 -0.428
>>>> x3 -0.049  0.458  1.000  0.092
>>>> x4  0.858 -0.428  0.092  1.000
>>>>
>>>> More generally, I think that it's a good idea to learn a little bit
>>>> about R programming if you intend to use R in your work. You'll then be
>>>> able to solve problems like this yourself.
>>>>
>>>> I hope this helps,
>>>>     John
>>>>
>>>>> Thank you,
>>>>>
>>>>> On Mon, Aug 22, 2022 at 1:00 PM John Fox <jfox at mcmaster.ca> wrote:
>>>>>>
>>>>>> Dear Val,
>>>>>>
>>>>>> On 2022-08-22 1:33 p.m., Val wrote:
>>>>>>> For the time being  I am assuming the relationship across  variables
>>>>>>> is linear.  I want get the values first  and detailed examining  of
>>>>>>> the relationship will follow later.
>>>>>>
>>>>>> This seems backwards to me, but I'll refrain from commenting further on
>>>>>> whether what you want to do makes sense and instead address how to do it
>>>>>> (not, BTW, because I disagree with Bert's and Tim's remarks).
>>>>>>
>>>>>> Please see below:
>>>>>>
>>>>>>>
>>>>>>> On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>>>>>>>>
>>>>>>>> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
>>>>>>>>
>>>>>>>> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
>>>>>>>>
>>>>>>>> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
>>>>>>>>
>>>>>>>> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
>>>>>>>>
>>>>>>>> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
>>>>>>>>
>>>>>>>> Large datasets can be very messy.
>>>>>>>>
>>>>>>>>
>>>>>>>> Tim
>>>>>>>>
>>>>>>>> -----Original Message-----
>>>>>>>> From: Bert Gunter <bgunter.4567 at gmail.com>
>>>>>>>> Sent: Monday, August 22, 2022 12:07 PM
>>>>>>>> To: Ebert,Timothy Aaron <tebert at ufl.edu>
>>>>>>>> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
>>>>>>>> Subject: Re: [R] Correlate
>>>>>>>>
>>>>>>>> [External Email]
>>>>>>>>
>>>>>>>> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
>>>>>>>> (search on "multiplicity adjustment" for details). :-(
>>>>>>>>
>>>>>>>> -- Bert
>>>>>>>>
>>>>>>>>
>>>>>>>> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
>>>>>>>>>
>>>>>>>>> A somewhat clunky solution:
>>>>>>>>> for(i in colnames(dat)){
>>>>>>>>>       print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
>>>>>>>>>       print(cor.test(dat[,i], dat$x1, method = "pearson", use =
>>>>>>>>> "complete.obs")$p.value) }
>>>>>>
>>>>>> Because of missing data, this computes the correlations on different
>>>>>> subsets of the data. A simple solution is to filter the data for NAs:
>>>>>>
>>>>>> D <- na.omit(dat)
>>>>>>
>>>>>> More comments below:
>>>>>>
>>>>>>>>>
>>>>>>>>> Rather than printing you could set up an array or list to save the results.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Tim
>>>>>>>>>
>>>>>>>>> -----Original Message-----
>>>>>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
>>>>>>>>> Sent: Monday, August 22, 2022 11:09 AM
>>>>>>>>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
>>>>>>>>> Subject: [R] Correlate
>>>>>>>>>
>>>>>>>>> [External Email]
>>>>>>>>>
>>>>>>>>> Hi all,
>>>>>>>>>
>>>>>>>>> I have a data set with  ~250  variables(columns).  I want to calculate
>>>>>>>>> the correlation of  one variable with the rest of the other variables
>>>>>>>>> and also want  the p-values  for each correlation.  Please see the
>>>>>>>>> sample data and my attempt.  I  have got the correlation but unable to
>>>>>>>>> get the p-values
>>>>>>>>>
>>>>>>>>> dat <- read.table(text="x1 x2 x3 x4
>>>>>>>>>                1.68 -0.96 -1.25  0.61
>>>>>>>>>               -0.06  0.41  0.06 -0.96
>>>>>>>>>                   .    0.08  1.14  1.42
>>>>>>>>>                0.80 -0.67  0.53 -0.68
>>>>>>>>>                0.23 -0.97 -1.18 -0.78
>>>>>>>>>               -1.03  1.11 -0.61    .
>>>>>>>>>                2.15     .    0.02  0.66
>>>>>>>>>                0.35 -0.37 -0.26  0.39
>>>>>>>>>               -0.66  0.89   .    -1.49
>>>>>>>>>                0.11  1.52  0.73  -1.03",header=TRUE)
>>>>>>>>>
>>>>>>>>> #change all to numeric
>>>>>>>>>         dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
>>>>>>
>>>>>> This data manipulation is unnecessary. Just specify the argument
>>>>>> na.strings="." to read.table().
>>>>>>
>>>>>>>>>
>>>>>>>>>         data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
>>>>>>>>> "pearson", use = "complete.obs")
>>>>>>>>>
>>>>>>>>> Result
>>>>>>>>>                   [,1]
>>>>>>>>> x2 -0.5845835
>>>>>>>>> x3 -0.4664220
>>>>>>>>> x4  0.7202837
>>>>>>>>>
>>>>>>>>> How do I get the p-values ?
>>>>>>
>>>>>> Taking a somewhat different approach from cor.test(), you can apply
>>>>>> Fisher's z-transformation (recall that D is the data filtered for NAs):
>>>>>>
>>>>>>     > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
>>>>>>             [,1]
>>>>>> x2 0.2462807
>>>>>> x3 0.3812854
>>>>>> x4 0.1156939
>>>>>>
>>>>>> I hope this helps,
>>>>>>      John
>>>>>>
>>>>>>>>>
>>>>>>>>> Thank you,
>>>>>>>>>
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>>>>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
>>>>>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
>>>>>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
>>>>>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>>>>>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
>>>>>>>>> =0 PLEASE do read the posting guide
>>>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
>>>>>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
>>>>>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
>>>>>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
>>>>>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
>>>>>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>>
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>>>>>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
>>>>>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
>>>>>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
>>>>>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
>>>>>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
>>>>>>>>> =0 PLEASE do read the posting guide
>>>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
>>>>>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
>>>>>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
>>>>>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
>>>>>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
>>>>>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> --
>>>>>> John Fox, Professor Emeritus
>>>>>> McMaster University
>>>>>> Hamilton, Ontario, Canada
>>>>>> web: https://socialsciences.mcmaster.ca/jfox/
>>>>>>
>>>> --
>>>> John Fox, Professor Emeritus
>>>> McMaster University
>>>> Hamilton, Ontario, Canada
>>>> web: https://socialsciences.mcmaster.ca/jfox/
>>>>
>> --
>> John Fox, Professor Emeritus
>> McMaster University
>> Hamilton, Ontario, Canada
>> web: https://socialsciences.mcmaster.ca/jfox/
>>
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://socialsciences.mcmaster.ca/jfox/


From v@|kremk @end|ng |rom gm@||@com  Sat Aug 27 01:30:13 2022
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Fri, 26 Aug 2022 18:30:13 -0500
Subject: [R] Correlate
In-Reply-To: <907a5668-a528-ba34-01ee-21c997dc2972@mcmaster.ca>
References: <CAJOiR6ZFJfuwko99b5CeeH8EcRy-csGLw=tdx+RfSj7fkU-1Sw@mail.gmail.com>
 <BN6PR2201MB15534F026C1C4AE47CCC6E1ACF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <CAGxFJbR46DDxjeoxchzFZyZQJnA=s2-MgC_O8+iHhHQnY38sSg@mail.gmail.com>
 <BN6PR2201MB1553511E9B6618EF500BA1D6CF719@BN6PR2201MB1553.namprd22.prod.outlook.com>
 <20234_1661189640_27MHY0f9000811_CAJOiR6as5387rVr-ff6gbiTdsyh=15Dvz2ECgqpW1S0-oxwf_A@mail.gmail.com>
 <52b30ece-7746-a64f-b3a2-207a2724611a@mcmaster.ca>
 <CAJOiR6Yc4tR3J+3YNXCtPNxv154X8vWUPuL8-XjZvW60csPjTw@mail.gmail.com>
 <2d76ac52-4493-c345-9719-d28e5a254503@mcmaster.ca>
 <CAJOiR6YL60UeoDUp_zaYRKLQGngWVJbfqEeoZakX7oUkudjikg@mail.gmail.com>
 <8fb95cf9-0906-3d73-81eb-65cda52ce17e@mcmaster.ca>
 <CAJOiR6a=Yx+Owsg8LtMemST4TOxXi_pBL6ukE4maJQQxnx26+w@mail.gmail.com>
 <907a5668-a528-ba34-01ee-21c997dc2972@mcmaster.ca>
Message-ID: <CAJOiR6YzoZ4-CTnyGKNAQjnYGgbzUPu8VEBDpEWs0L6Mv-sw8A@mail.gmail.com>

HI John,

Please see my response below your question,

> (1) Are p-values interesting when you have n  > 165K cases?
 There are some pairwise comparisons that do have few number of
observations. I need to get part of information.

 > (2) What's the point of testing a correlation between a variable and itself?
  No, this part  will be removed.

Thank you

On Fri, Aug 26, 2022 at 5:57 PM John Fox <jfox at mcmaster.ca> wrote:
>
> Dear Val,
>
> On 2022-08-26 6:27 p.m., Val wrote:
> > Thank you John again. I have got my result  in the form as shown below
> >        Variable   Corr     Pvalue   Nobs
> >              x1      1.000       0     165425
>
> This raises two questions: (1) Are p-values interesting when you have n
>  > 165K cases? (2) What's the point of testing a correlation between a
> variable and itself?
>
> Best,
>   John
>
> >
> >
> >
> >
> >
> > On Fri, Aug 26, 2022 at 3:59 PM John Fox <jfox at mcmaster.ca> wrote:
> >>
> >> Dear Val,
> >>
> >> On 2022-08-26 4:06 p.m., Val wrote:
> >>> Thank you John for your help and advice.\
> >>
> >> You're welcome, and it occurred to me that if you want the number of
> >> non-missing cases for each individual variable, you could add
> >>
> >>          diag(N) <- colSums(!is.na(dat))
> >>
> >> to the code I sent earlier.
> >>
> >> Best,
> >>    John
> >>
> >>>
> >>> On Fri, Aug 26, 2022 at 11:04 AM John Fox <jfox at mcmaster.ca> wrote:
> >>>>
> >>>> Dear Val,
> >>>>
> >>>> On 2022-08-26 10:41 a.m., Val wrote:
> >>>>> Hi John and Timothy
> >>>>>
> >>>>> Thank you for your suggestion and help. Using the sample data, I did
> >>>>> carry out a test run and found a difference in the correlation result.
> >>>>>
> >>>>> Option 1.
> >>>>> data_cor <- cor(dat[ , colnames(dat) != "x1"],  # Calculate correlations
> >>>>>                        dat$x1, method = "pearson", use = "complete.obs")
> >>>>> resulted
> >>>>>                     [,1]
> >>>>>        x2 -0.5845835
> >>>>>        x3 -0.4664220
> >>>>>        x4  0.7202837
> >>>>>
> >>>>> Option 2.
> >>>>>     for(i in colnames(dat)){
> >>>>>          print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> >>>>> "complete.obs")$estimate)
> >>>>>        }
> >>>>>               [,1]
> >>>>> x2  -0.7362030
> >>>>> x3  -0.04935132
> >>>>> x4   0.85766290
> >>>>>
> >>>>> This was crosschecked  using Excel and other softwares and all matches
> >>>>> with option 2.
> >>>>> One of the factors that contributed for this difference  is loss of
> >>>>> information when we are using na.rm(). This is because that if x2 has
> >>>>> missing value but x3 and x4 don?t have then  na.rm()  removed  entire
> >>>>> row information including x3 and x4.
> >>>>
> >>>> Yes, I already explained that in my previous message.
> >>>>
> >>>> As well, cor() is capable of computing pairwise-complete correlations --
> >>>> see ?cor.
> >>>>
> >>>> There's not an obvious right answer here, however. Using
> >>>> pairwise-complete correlations can produce inconsistent (i.e.,
> >>>> non-positive semi-definite) correlation matrices because correlations
> >>>> are computed on different subsets of the data.
> >>>>
> >>>> There are much better ways to deal with missing data.
> >>>>
> >>>>>
> >>>>> My question is there  a way to extract the number of rows (N)  used in
> >>>>> the correlation analysis?.
> >>>>
> >>>> I'm sure that there are many ways, but here is one that is very
> >>>> simple-minded and should be reasonably efficient for ~250 variables:
> >>>>
> >>>>    > (nc <- ncol(dat))
> >>>> [1] 4
> >>>>
> >>>>    > R <- N <- matrix(NA, nc, nc)
> >>>>    > diag(R) <- 1
> >>>>    > for (i in 1:(nc - 1)){
> >>>> +   for (j in (i + 1):nc){
> >>>> +     R[i, j] <- R[j, i] <-cor(dat[, i], dat[, j], use="complete.obs")
> >>>> +     N[i, j] <- N[j, i] <- nrow(na.omit(dat[, c(i, j)]))
> >>>> +   }
> >>>> + }
> >>>>
> >>>>    > round(R, 3)
> >>>>           [,1]   [,2]   [,3]   [,4]
> >>>> [1,]  1.000 -0.736 -0.049  0.858
> >>>> [2,] -0.736  1.000  0.458 -0.428
> >>>> [3,] -0.049  0.458  1.000  0.092
> >>>> [4,]  0.858 -0.428  0.092  1.000
> >>>>
> >>>>    > N
> >>>>         [,1] [,2] [,3] [,4]
> >>>> [1,]   NA    8    8    8
> >>>> [2,]    8   NA    8    8
> >>>> [3,]    8    8   NA    8
> >>>> [4,]    8    8    8   NA
> >>>>
> >>>>    > round(cor(dat, use="pairwise.complete.obs"), 3) # check
> >>>>           x1     x2     x3     x4
> >>>> x1  1.000 -0.736 -0.049  0.858
> >>>> x2 -0.736  1.000  0.458 -0.428
> >>>> x3 -0.049  0.458  1.000  0.092
> >>>> x4  0.858 -0.428  0.092  1.000
> >>>>
> >>>> More generally, I think that it's a good idea to learn a little bit
> >>>> about R programming if you intend to use R in your work. You'll then be
> >>>> able to solve problems like this yourself.
> >>>>
> >>>> I hope this helps,
> >>>>     John
> >>>>
> >>>>> Thank you,
> >>>>>
> >>>>> On Mon, Aug 22, 2022 at 1:00 PM John Fox <jfox at mcmaster.ca> wrote:
> >>>>>>
> >>>>>> Dear Val,
> >>>>>>
> >>>>>> On 2022-08-22 1:33 p.m., Val wrote:
> >>>>>>> For the time being  I am assuming the relationship across  variables
> >>>>>>> is linear.  I want get the values first  and detailed examining  of
> >>>>>>> the relationship will follow later.
> >>>>>>
> >>>>>> This seems backwards to me, but I'll refrain from commenting further on
> >>>>>> whether what you want to do makes sense and instead address how to do it
> >>>>>> (not, BTW, because I disagree with Bert's and Tim's remarks).
> >>>>>>
> >>>>>> Please see below:
> >>>>>>
> >>>>>>>
> >>>>>>> On Mon, Aug 22, 2022 at 12:23 PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>>>>>>
> >>>>>>>> I (maybe) agree, but I would go further than that. There are assumptions associated with the test that are missing. It is not clear that the relationships are all linear. Regardless of a "significant outcome" all of the relationships need to be explored in more detail than what is provided in the correlation test.
> >>>>>>>>
> >>>>>>>> Multiplicity adjustment as in : https://www.sciencedirect.com/science/article/pii/S0197245600001069 is not an issue that I can see in these data from the information provided. At least not in the same sense as used in the link.
> >>>>>>>>
> >>>>>>>> My first guess at the meaning of "multiplicity adjustment" was closer to the experimentwise error rate in a multiple comparison procedure. https://dictionary.apa.org/experiment-wise-error-rateEssentially, the type 1 error rate is inflated the more test you do and if you perform enough tests you find significant outcomes by chance alone. There is great significance in the Redskins rule: https://en.wikipedia.org/wiki/Redskins_Rule.
> >>>>>>>>
> >>>>>>>> A simple solution is to apply a Bonferroni correction where alpha is divided by the number of comparisons. If there are 250, then 0.05/250 = 0.0002. Another approach is to try to discuss the outcomes in a way that makes sense. What is the connection between a football team's last home game an the election result that would enable me to take another team and apply their last home game result to the outcome of a different election?
> >>>>>>>>
> >>>>>>>> Another complication is if variables x2 through x250 are themselves correlated. Not enough information was provided in the problem to know if this is an issue, but 250 orthogonal variables in a real dataset would be a bit unusual considering the experimentwise error rate previously mentioned.
> >>>>>>>>
> >>>>>>>> Large datasets can be very messy.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> Tim
> >>>>>>>>
> >>>>>>>> -----Original Message-----
> >>>>>>>> From: Bert Gunter <bgunter.4567 at gmail.com>
> >>>>>>>> Sent: Monday, August 22, 2022 12:07 PM
> >>>>>>>> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> >>>>>>>> Cc: Val <valkremk at gmail.com>; r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>>>>>>> Subject: Re: [R] Correlate
> >>>>>>>>
> >>>>>>>> [External Email]
> >>>>>>>>
> >>>>>>>> ... But of course the p-values are essentially meaningless without some sort of multiplicity adjustment.
> >>>>>>>> (search on "multiplicity adjustment" for details). :-(
> >>>>>>>>
> >>>>>>>> -- Bert
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> On Mon, Aug 22, 2022 at 8:59 AM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:
> >>>>>>>>>
> >>>>>>>>> A somewhat clunky solution:
> >>>>>>>>> for(i in colnames(dat)){
> >>>>>>>>>       print(cor.test(dat[,i], dat$x1, method = "pearson", use = "complete.obs")$estimate)
> >>>>>>>>>       print(cor.test(dat[,i], dat$x1, method = "pearson", use =
> >>>>>>>>> "complete.obs")$p.value) }
> >>>>>>
> >>>>>> Because of missing data, this computes the correlations on different
> >>>>>> subsets of the data. A simple solution is to filter the data for NAs:
> >>>>>>
> >>>>>> D <- na.omit(dat)
> >>>>>>
> >>>>>> More comments below:
> >>>>>>
> >>>>>>>>>
> >>>>>>>>> Rather than printing you could set up an array or list to save the results.
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> Tim
> >>>>>>>>>
> >>>>>>>>> -----Original Message-----
> >>>>>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> >>>>>>>>> Sent: Monday, August 22, 2022 11:09 AM
> >>>>>>>>> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> >>>>>>>>> Subject: [R] Correlate
> >>>>>>>>>
> >>>>>>>>> [External Email]
> >>>>>>>>>
> >>>>>>>>> Hi all,
> >>>>>>>>>
> >>>>>>>>> I have a data set with  ~250  variables(columns).  I want to calculate
> >>>>>>>>> the correlation of  one variable with the rest of the other variables
> >>>>>>>>> and also want  the p-values  for each correlation.  Please see the
> >>>>>>>>> sample data and my attempt.  I  have got the correlation but unable to
> >>>>>>>>> get the p-values
> >>>>>>>>>
> >>>>>>>>> dat <- read.table(text="x1 x2 x3 x4
> >>>>>>>>>                1.68 -0.96 -1.25  0.61
> >>>>>>>>>               -0.06  0.41  0.06 -0.96
> >>>>>>>>>                   .    0.08  1.14  1.42
> >>>>>>>>>                0.80 -0.67  0.53 -0.68
> >>>>>>>>>                0.23 -0.97 -1.18 -0.78
> >>>>>>>>>               -1.03  1.11 -0.61    .
> >>>>>>>>>                2.15     .    0.02  0.66
> >>>>>>>>>                0.35 -0.37 -0.26  0.39
> >>>>>>>>>               -0.66  0.89   .    -1.49
> >>>>>>>>>                0.11  1.52  0.73  -1.03",header=TRUE)
> >>>>>>>>>
> >>>>>>>>> #change all to numeric
> >>>>>>>>>         dat[] <- lapply(dat, function(x) as.numeric(as.character(x)))
> >>>>>>
> >>>>>> This data manipulation is unnecessary. Just specify the argument
> >>>>>> na.strings="." to read.table().
> >>>>>>
> >>>>>>>>>
> >>>>>>>>>         data_cor <- cor(dat[ , colnames(dat) != "x1"],  dat$x1, method =
> >>>>>>>>> "pearson", use = "complete.obs")
> >>>>>>>>>
> >>>>>>>>> Result
> >>>>>>>>>                   [,1]
> >>>>>>>>> x2 -0.5845835
> >>>>>>>>> x3 -0.4664220
> >>>>>>>>> x4  0.7202837
> >>>>>>>>>
> >>>>>>>>> How do I get the p-values ?
> >>>>>>
> >>>>>> Taking a somewhat different approach from cor.test(), you can apply
> >>>>>> Fisher's z-transformation (recall that D is the data filtered for NAs):
> >>>>>>
> >>>>>>     > 2*pnorm(abs(atanh(data_cor)), sd=1/sqrt(nrow(D) - 3), lower.tail=FALSE)
> >>>>>>             [,1]
> >>>>>> x2 0.2462807
> >>>>>> x3 0.3812854
> >>>>>> x4 0.1156939
> >>>>>>
> >>>>>> I hope this helps,
> >>>>>>      John
> >>>>>>
> >>>>>>>>>
> >>>>>>>>> Thank you,
> >>>>>>>>>
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>>>>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>>>>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>>>>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>>>>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>>>>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>>>>>>>> =0 PLEASE do read the posting guide
> >>>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>>>>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>>>>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>>>>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>>>>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>>>>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>>
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> >>>>>>>>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl
> >>>>>>>>> .edu%7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e
> >>>>>>>>> 1b84%7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4w
> >>>>>>>>> LjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C
> >>>>>>>>> &amp;sdata=3iAfMs1QzQARKF3lqUI8s43PX4IIkgEuQ9PUDyUtpqY%3D&amp;reserved
> >>>>>>>>> =0 PLEASE do read the posting guide
> >>>>>>>>> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> >>>>>>>>> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> >>>>>>>>> 7C871d5009dd3c455f398f08da84585e4a%7C0d4da0f84a314d76ace60a62331e1b84%
> >>>>>>>>> 7C0%7C0%7C637967812337328788%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> >>>>>>>>> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> >>>>>>>>> sdata=v3IEonnPgg1xTKUzLK4rJc3cfMFxw5p%2FW6puha5CFz0%3D&amp;reserved=0
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>> --
> >>>>>> John Fox, Professor Emeritus
> >>>>>> McMaster University
> >>>>>> Hamilton, Ontario, Canada
> >>>>>> web: https://socialsciences.mcmaster.ca/jfox/
> >>>>>>
> >>>> --
> >>>> John Fox, Professor Emeritus
> >>>> McMaster University
> >>>> Hamilton, Ontario, Canada
> >>>> web: https://socialsciences.mcmaster.ca/jfox/
> >>>>
> >> --
> >> John Fox, Professor Emeritus
> >> McMaster University
> >> Hamilton, Ontario, Canada
> >> web: https://socialsciences.mcmaster.ca/jfox/
> >>
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://socialsciences.mcmaster.ca/jfox/
>


From t@n@@@ @end|ng |rom gm@||@com  Sat Aug 27 02:55:35 2022
From: t@n@@@ @end|ng |rom gm@||@com (Bogdan Tanasa)
Date: Fri, 26 Aug 2022 17:55:35 -0700
Subject: [R] confidence intervals
Message-ID: <CA+JEM01f88pCikn6gwr20vHx_KgQ7Tnik98hYuzZPcVF1H5k0Q@mail.gmail.com>

Dear all,

Although I know that it is not a statistics mailing list, given my work on
ICeChIP

https://github.com/shah-rohan/icechip/blob/master/Scripts/computeHMDandError

I would appreciate to have the answer to a question :

given two variables a and b (a and b can have 1000 paired-values) and a
calibration number "cal",

why the 95 confidence interval has been calculated as such for each value
a(i) and b(i) :

100 / cal * sqrt (( a/ (b^2) + (a^2) / (b ^3)) * 1.96

Thank you,

Bogdan

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Aug 28 01:44:28 2022
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 27 Aug 2022 16:44:28 -0700
Subject: [R] An intended unintended consequence
Message-ID: <CAGxFJbSvgF=jYcxvbnOXsrx6hViQCthhmpP4+nh_2kd1XLxFgQ@mail.gmail.com>

Note: This is a minor comment on a recent thread, and neither a query
nor an answer to a query.
----
In a recent discussion thread here, it was asked how to compute the
counts for the number of complete cases that are used for computing
the entries in a correlation matrix obtained from
cor(X, use = "pairwise.complete.obs")
when there are missing values (i.e. NA's) in X.
(Whether it is wise to do this is another issue; but here it just
motivates this post).

As part of his solution, John Fox provided the following idiom for
computing the number of complete cases == rows without NA's from pairs
of columns of a matrix Z when Z has NA's. For columns i and j, the
number of rows without NA's is nrow(na.omit (Z[, c(i, j)] )). This
clearly works, because na.omit() is a generic (S3) function designed
to omit rows with NA's in matrix-like objects, and  nrow() then just
counts the rows remaining, which is exactly what is needed.

I would call this "an intended intended consequence", because John
used na.omit() exactly as it's intended to be used.

However, sometimes one can do "better" -- in this case in a speed of
execution sense -- by "misusing" functionality in a way that is not
intended.  Instead of John's "nrow(na.omit...))",  the idiom:
sum(!is.na(rowSums(Z[, c(i,j)])))
turns out to be considerably faster. Here's a little example that
illustrates the point:

>library(microbenchmark)
> Z <- matrix(0, ncol = 2, nrow = 10000) ## 2 columns only for illustration

> is.na(Z) <-sample(seq_len(20000),2000) ## 10% NA's

> ## check that both methods give the same answer
> nrow( na.omit(Z))
[1] 8112
> sum( !is.na( rowSums(Z)))
[1] 8112
## timings ##
> print(microbenchmark( nrow(na.omit(Z)), times = 50), signif = 3)
Unit: microseconds
             expr min  lq mean median  uq max neval
 nrow(na.omit(Z)) 116 122  128    128 132 160    50
> # vs
> print(microbenchmark( sum(!is.na(rowSums(Z, na.rm = TRUE))), times = 50), signif = 3)
Unit: microseconds
                             expr min   lq mean median   uq  max neval
 sum(!is.na( rowSums(Z)))  28 28.9 32.1   32.4 33.5 41.3    50

So a median time of 128 microseconds for nrow(na.omit...) vs. 32 for
sum(!is,na(rowSums(...), i.e. four times as fast. Why? -- the na.omit
approach does its looping at the interpreted R level; the
sum(!is.na...)
does most of its work at the compiled C level. There is a cost to this
efficiency improvement, however: the fast code is more opaque and thus
harder to understand and maintain, because it uses R's functionality
in unintended ways, i.e. for intended unintended consequences. As,
usual, he programmer must decide whether the tradeoff is worthwhile;
but it's nice to know when a tradeoff exists.
==========================================================
For those who may be interested, here is a brief explanation of the
tricks used in the faster solution.

rowSums(Z) gives the sums by row in Z, and will give NA if a row
contains any NA's. Note that this yields just a single vector of NA's
and numeric values.
!is.na (rowSums...) then converts the NA's to FALSE and numeric values
to TRUE, i.e. logicals in this vector.
But (TRUE, FALSE) is treated as (1, 0) by numeric operations, so
sum(...) just sums up the 1's, which is the same as counting the TRUEs
== complete case rows.

Cheers,
Bert


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Aug 28 03:52:06 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sat, 27 Aug 2022 18:52:06 -0700
Subject: [R] confidence intervals
In-Reply-To: <CA+JEM01f88pCikn6gwr20vHx_KgQ7Tnik98hYuzZPcVF1H5k0Q@mail.gmail.com>
References: <CA+JEM01f88pCikn6gwr20vHx_KgQ7Tnik98hYuzZPcVF1H5k0Q@mail.gmail.com>
Message-ID: <61194db7-dcd5-5de0-262d-dc5843490192@comcast.net>

You cross-posted this to StackOverflow and did not say so.? ... and you 
posted in HTML Bad dog squared. I cast one of the close votes on SO, but 
here I can only say ... READ the Posting Guide.

You also give no citation other than someone's Github files with minimal 
comments in that material. You should indicate whether this code has any 
solid support. Why do you think this code is something to depend upon?

After all, you been posting questions on R-help for several months. 
Don't you think you should make a good faith effort to understand the 
principles underlying this resource?


-- 

David.

On 8/26/22 17:55, Bogdan Tanasa wrote:
> Dear all,
>
> Although I know that it is not a statistics mailing list, given my work on
> ICeChIP
>
> https://github.com/shah-rohan/icechip/blob/master/Scripts/computeHMDandError
>
> I would appreciate to have the answer to a question :
>
> given two variables a and b (a and b can have 1000 paired-values) and a
> calibration number "cal",
>
> why the 95 confidence interval has been calculated as such for each value
> a(i) and b(i) :
>
> 100 / cal * sqrt (( a/ (b^2) + (a^2) / (b ^3)) * 1.96
>
> Thank you,
>
> Bogdan
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From t@n@@@ @end|ng |rom gm@||@com  Mon Aug 29 02:55:02 2022
From: t@n@@@ @end|ng |rom gm@||@com (Bogdan Tanasa)
Date: Sun, 28 Aug 2022 17:55:02 -0700
Subject: [R] confidence intervals
In-Reply-To: <61194db7-dcd5-5de0-262d-dc5843490192@comcast.net>
References: <CA+JEM01f88pCikn6gwr20vHx_KgQ7Tnik98hYuzZPcVF1H5k0Q@mail.gmail.com>
 <61194db7-dcd5-5de0-262d-dc5843490192@comcast.net>
Message-ID: <CA+JEM0234ij2_4CUY_xC5CkxdFqbe_qZvdMTM9tNvjq52pTU3A@mail.gmail.com>

Hi David,

Thank you for your comments, and feed-back message. I am very happy to
learn from the experience of the people on R mailing list, and without any
doubt, I am very thankful to you and to everyone for sharing their
knowledge. I do apologize for any confusion that I have created unwillingly
with my previous email.

About my previous email related to the confidence intervals: indeed I have
posted the question with a detailed description on stackoverflow, and the
link is listed below.

I have to admit that I have been in rush willing to have the suggestions of
R-help members by Monday (if that would have been possible), as I have to
make a decision at the beginning of this week on whether I need to re-code
the shell script in R. I have a deadline on Wed. The script itself is less
important per se, I have included it just to point our the origin of my
question.

I do certainly respect the principles of online R-help community, and I
would very much appreciate if I could have your advice on the following :
shall a "R code related emergency" arise, would it be acceptable to post
the question on stackoverflow with the corresponding data tables and
detailed code, and to refer the posting on R-help mailing list ?

If it is acceptable at least for a single email, and if you do not mind, I
could mention the link to stackoverflow, inviting our members to read it,
shall they be comfortable with this topic.

https://stackoverflow.com/questions/73507697/confidence-intervals-of-a-biological-assay?noredirect=1#comment129816241_73507697

Thanks a lot, have  a good week !

~ Bogdan

https://stackoverflow.com/questions/73507697/confidence-intervals-of-a-biological-assay?noredirect=1#comment129816241_73507697

On Sat, Aug 27, 2022, 6:52 PM David Winsemius <dwinsemius at comcast.net>
wrote:

> You cross-posted this to StackOverflow and did not say so.  ... and you
> posted in HTML Bad dog squared. I cast one of the close votes on SO, but
> here I can only say ... READ the Posting Guide.
>
> You also give no citation other than someone's Github files with minimal
> comments in that material. You should indicate whether this code has any
> solid support. Why do you think this code is something to depend upon?
>
> After all, you been posting questions on R-help for several months.
> Don't you think you should make a good faith effort to understand the
> principles underlying this resource?
>
>
> --
>
> David.
>
> On 8/26/22 17:55, Bogdan Tanasa wrote:
> > Dear all,
> >
> > Although I know that it is not a statistics mailing list, given my work
> on
> > ICeChIP
> >
> >
> https://github.com/shah-rohan/icechip/blob/master/Scripts/computeHMDandError
> >
> > I would appreciate to have the answer to a question :
> >
> > given two variables a and b (a and b can have 1000 paired-values) and a
> > calibration number "cal",
> >
> > why the 95 confidence interval has been calculated as such for each value
> > a(i) and b(i) :
> >
> > 100 / cal * sqrt (( a/ (b^2) + (a^2) / (b ^3)) * 1.96
> >
> > Thank you,
> >
> > Bogdan
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Mon Aug 29 03:50:13 2022
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Mon, 29 Aug 2022 01:50:13 +0000
Subject: [R] confidence intervals
In-Reply-To: <CA+JEM0234ij2_4CUY_xC5CkxdFqbe_qZvdMTM9tNvjq52pTU3A@mail.gmail.com>
References: <CA+JEM01f88pCikn6gwr20vHx_KgQ7Tnik98hYuzZPcVF1H5k0Q@mail.gmail.com>
 <61194db7-dcd5-5de0-262d-dc5843490192@comcast.net>
 <CA+JEM0234ij2_4CUY_xC5CkxdFqbe_qZvdMTM9tNvjq52pTU3A@mail.gmail.com>
Message-ID: <BN6PR2201MB1553E6071C9A5CE6769BD45CCF769@BN6PR2201MB1553.namprd22.prod.outlook.com>

I have a general dislike of "analysis emergencies." I would like to see a data emergency wherein someone must cram 3 years of data collection into 18 months so that they have time to work out the correct analysis. I am sure others would suggest working out how analyze the data before starting the experiment.

Our business office gives this advice to faculty members: An emergency on your part is not an emergency on our part. 

How about starting by answering the questions posted by the people you are hoping will help. Focus on David's middle paragraph. However, if you can re-code everything to work, then it would seem that you already know the answer and it might be simpler/faster to write the correct code.

You might spend some time looking for a scientific paper that uses that equation for the confidence interval and thereby get some context to explain why the equation is correct.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bogdan Tanasa
Sent: Sunday, August 28, 2022 8:55 PM
To: David Winsemius <dwinsemius at comcast.net>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] confidence intervals

[External Email]

Hi David,

Thank you for your comments, and feed-back message. I am very happy to learn from the experience of the people on R mailing list, and without any doubt, I am very thankful to you and to everyone for sharing their knowledge. I do apologize for any confusion that I have created unwillingly with my previous email.

About my previous email related to the confidence intervals: indeed I have posted the question with a detailed description on stackoverflow, and the link is listed below.

I have to admit that I have been in rush willing to have the suggestions of R-help members by Monday (if that would have been possible), as I have to make a decision at the beginning of this week on whether I need to re-code the shell script in R. I have a deadline on Wed. The script itself is less important per se, I have included it just to point our the origin of my question.

I do certainly respect the principles of online R-help community, and I would very much appreciate if I could have your advice on the following :
shall a "R code related emergency" arise, would it be acceptable to post the question on stackoverflow with the corresponding data tables and detailed code, and to refer the posting on R-help mailing list ?

If it is acceptable at least for a single email, and if you do not mind, I could mention the link to stackoverflow, inviting our members to read it, shall they be comfortable with this topic.

https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F73507697%2Fconfidence-intervals-of-a-biological-assay%3Fnoredirect%3D1%23comment129816241_73507697&amp;data=05%7C01%7Ctebert%40ufl.edu%7C0ba5d535471b46c05ec508da89592c20%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637973313343894313%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=8sF7j4OCgH12qx4d8NCiw1%2FDbPa6nrui27S9C3ZNuL0%3D&amp;reserved=0

Thanks a lot, have  a good week !

~ Bogdan

https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F73507697%2Fconfidence-intervals-of-a-biological-assay%3Fnoredirect%3D1%23comment129816241_73507697&amp;data=05%7C01%7Ctebert%40ufl.edu%7C0ba5d535471b46c05ec508da89592c20%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637973313343894313%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=8sF7j4OCgH12qx4d8NCiw1%2FDbPa6nrui27S9C3ZNuL0%3D&amp;reserved=0

On Sat, Aug 27, 2022, 6:52 PM David Winsemius <dwinsemius at comcast.net>
wrote:

> You cross-posted this to StackOverflow and did not say so.  ... and 
> you posted in HTML Bad dog squared. I cast one of the close votes on 
> SO, but here I can only say ... READ the Posting Guide.
>
> You also give no citation other than someone's Github files with 
> minimal comments in that material. You should indicate whether this 
> code has any solid support. Why do you think this code is something to depend upon?
>
> After all, you been posting questions on R-help for several months.
> Don't you think you should make a good faith effort to understand the 
> principles underlying this resource?
>
>
> --
>
> David.
>
> On 8/26/22 17:55, Bogdan Tanasa wrote:
> > Dear all,
> >
> > Although I know that it is not a statistics mailing list, given my 
> > work
> on
> > ICeChIP
> >
> >
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgith
> ub.com%2Fshah-rohan%2Ficechip%2Fblob%2Fmaster%2FScripts%2FcomputeHMDan
> dError&amp;data=05%7C01%7Ctebert%40ufl.edu%7C0ba5d535471b46c05ec508da8
> 9592c20%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C63797331334389431
> 3%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6
> Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=9HOH72K0Lj30w7tE%2BMN
> oXakPbO1sasaujUTsnHFwDZo%3D&amp;reserved=0
> >
> > I would appreciate to have the answer to a question :
> >
> > given two variables a and b (a and b can have 1000 paired-values) 
> > and a calibration number "cal",
> >
> > why the 95 confidence interval has been calculated as such for each 
> > value
> > a(i) and b(i) :
> >
> > 100 / cal * sqrt (( a/ (b^2) + (a^2) / (b ^3)) * 1.96
> >
> > Thank you,
> >
> > Bogdan
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fst
> > at.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%4
> > 0ufl.edu%7C0ba5d535471b46c05ec508da89592c20%7C0d4da0f84a314d76ace60a
> > 62331e1b84%7C0%7C0%7C637973313344050547%7CUnknown%7CTWFpbGZsb3d8eyJW
> > IjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C300
> > 0%7C%7C%7C&amp;sdata=KfmZ6PvxUVrbCMzVZcz6S2H2sqfWQmw2WIZHERwKDEk%3D&
> > amp;reserved=0
> > PLEASE do read the posting guide
> https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r
> -project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%
> 7C0ba5d535471b46c05ec508da89592c20%7C0d4da0f84a314d76ace60a62331e1b84%
> 7C0%7C0%7C637973313344050547%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwM
> DAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;
> sdata=127HvzZh7q9Xa7LaESHIEXIy3CdmUyQ3cxAVeOA%2Bukg%3D&amp;reserved=0
> > and provide commented, minimal, self-contained, reproducible code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=05%7C01%7Ctebert%40ufl.edu%7C0ba5d535471b46c05ec508da89592c20%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637973313344050547%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=KfmZ6PvxUVrbCMzVZcz6S2H2sqfWQmw2WIZHERwKDEk%3D&amp;reserved=0
PLEASE do read the posting guide https://nam10.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=05%7C01%7Ctebert%40ufl.edu%7C0ba5d535471b46c05ec508da89592c20%7C0d4da0f84a314d76ace60a62331e1b84%7C0%7C0%7C637973313344050547%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=127HvzZh7q9Xa7LaESHIEXIy3CdmUyQ3cxAVeOA%2Bukg%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


From Den|@@Cou@|ne@u @end|ng |rom uott@w@@c@  Wed Aug 24 16:32:25 2022
From: Den|@@Cou@|ne@u @end|ng |rom uott@w@@c@ (Denis Cousineau)
Date: Wed, 24 Aug 2022 14:32:25 +0000
Subject: [R] [R-pkgs] CohensdpLibrary: Compute Cohen's d and its confidence
 interval in any experimental design
Message-ID: <0175872f-a228-5b53-f9df-0b4bd6e69d18@uottawa.ca>

Dear all,

If you are using Cohen's d, this package is for you. The package "CohensdpLibrary" lets you compute Cohen's d from basic statistics and also get confidence intervals. It expands on MBESS as it is available for data obtained in within-subject designs as well as between-subject designs.

It is based on a simple command, e. g.,

Cohensdp( statistics = list(m1=76, m2=72, n=20, s1=14.8, s2=18.8, r=0.2),
          design = "within"
)

Learn more at

https://dcousin3.github.io/CohensdpLibrary/

Sincerely,
Denis Cousineau.



	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon Aug 29 18:30:58 2022
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 29 Aug 2022 11:30:58 -0500
Subject: [R] About spDataLarge Package
Message-ID: <CAMOcQfMGK4WEDGZ-_ePju8s5ERS3mjWD5ae=r1RA8yfnFToE+g@mail.gmail.com>

Dear friends,

I have just installed R version 4.2.1 for Windows on my machine, and was
trying to install package spDataLarge, but the console threw the following
error message:

Warning in install.packages :
  package 'spDataLarge' is not available for this version of R

A version of this package for your version of R might be available
elsewhere,
see the ideas at
https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages

I used the following command to try to install it:

install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/",
type = "source")
I tried with earlier versions of R (4.2.0 and 4.0.3) but I keep getting the
same error. I was searching to see if I could obtain information regarding
the R version or versions that support this package, but haven't found
anything thus far.

Any suggestions on how to successfully install this package?

Something odd is that I was able to install package spData, but not
spDataLarge (in R version 4.2.1)

Best regards,
Paul

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Aug 29 19:21:47 2022
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 29 Aug 2022 18:21:47 +0100
Subject: [R] About spDataLarge Package
In-Reply-To: <CAMOcQfMGK4WEDGZ-_ePju8s5ERS3mjWD5ae=r1RA8yfnFToE+g@mail.gmail.com>
References: <CAMOcQfMGK4WEDGZ-_ePju8s5ERS3mjWD5ae=r1RA8yfnFToE+g@mail.gmail.com>
Message-ID: <e00ed74e-0ee7-a199-b4b1-10b739feede7@sapo.pt>

Hello,

That package is not a CRAN package, it's not even archived.
Try

remotes::install_github("Nowosad/spDataLarge")


(If you haven't installed it yet, package remotes is a CRAN package.)

Hope this helps,

Rui Barradas

?s 17:30 de 29/08/2022, Paul Bernal escreveu:
> Dear friends,
> 
> I have just installed R version 4.2.1 for Windows on my machine, and was
> trying to install package spDataLarge, but the console threw the following
> error message:
> 
> Warning in install.packages :
>    package 'spDataLarge' is not available for this version of R
> 
> A version of this package for your version of R might be available
> elsewhere,
> see the ideas at
> https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages
> 
> I used the following command to try to install it:
> 
> install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/",
> type = "source")
> I tried with earlier versions of R (4.2.0 and 4.0.3) but I keep getting the
> same error. I was searching to see if I could obtain information regarding
> the R version or versions that support this package, but haven't found
> anything thus far.
> 
> Any suggestions on how to successfully install this package?
> 
> Something odd is that I was able to install package spData, but not
> spDataLarge (in R version 4.2.1)
> 
> Best regards,
> Paul
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon Aug 29 19:31:12 2022
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 29 Aug 2022 12:31:12 -0500
Subject: [R] About spDataLarge Package
In-Reply-To: <e00ed74e-0ee7-a199-b4b1-10b739feede7@sapo.pt>
References: <CAMOcQfMGK4WEDGZ-_ePju8s5ERS3mjWD5ae=r1RA8yfnFToE+g@mail.gmail.com>
 <e00ed74e-0ee7-a199-b4b1-10b739feede7@sapo.pt>
Message-ID: <CAMOcQfOQFzFH3g8FxvtrS_0+L-OvTVNtZazAG3FRNJrXoMEZrA@mail.gmail.com>

Thank you very much dear friend Rui, I will try this and let you know how
it goes.

Best,
Paul

El El lun, 29 de ago. de 2022 a la(s) 12:21 p. m., Rui Barradas <
ruipbarradas at sapo.pt> escribi?:

> Hello,
>
> That package is not a CRAN package, it's not even archived.
> Try
>
> remotes::install_github("Nowosad/spDataLarge")
>
>
> (If you haven't installed it yet, package remotes is a CRAN package.)
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 17:30 de 29/08/2022, Paul Bernal escreveu:
> > Dear friends,
> >
> > I have just installed R version 4.2.1 for Windows on my machine, and was
> > trying to install package spDataLarge, but the console threw the
> following
> > error message:
> >
> > Warning in install.packages :
> >    package 'spDataLarge' is not available for this version of R
> >
> > A version of this package for your version of R might be available
> > elsewhere,
> > see the ideas at
> >
> https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages
> >
> > I used the following command to try to install it:
> >
> > install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/
> ",
> > type = "source")
> > I tried with earlier versions of R (4.2.0 and 4.0.3) but I keep getting
> the
> > same error. I was searching to see if I could obtain information
> regarding
> > the R version or versions that support this package, but haven't found
> > anything thus far.
> >
> > Any suggestions on how to successfully install this package?
> >
> > Something odd is that I was able to install package spData, but not
> > spDataLarge (in R version 4.2.1)
> >
> > Best regards,
> > Paul
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Mon Aug 29 19:32:00 2022
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Mon, 29 Aug 2022 10:32:00 -0700
Subject: [R] About spDataLarge Package
In-Reply-To: <CAMOcQfMGK4WEDGZ-_ePju8s5ERS3mjWD5ae=r1RA8yfnFToE+g@mail.gmail.com>
References: <CAMOcQfMGK4WEDGZ-_ePju8s5ERS3mjWD5ae=r1RA8yfnFToE+g@mail.gmail.com>
Message-ID: <09a2adee-8c31-be68-bb93-367f875c223a@comcast.net>


On 8/29/22 09:30, Paul Bernal wrote:
> Dear friends,
>
> I have just installed R version 4.2.1 for Windows on my machine, and was
> trying to install package spDataLarge, but the console threw the following
> error message:
>
> Warning in install.packages :
>    package 'spDataLarge' is not available for this version of R
>
> A version of this package for your version of R might be available
> elsewhere,
> see the ideas at
> https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages
>
> I used the following command to try to install it:
>
> install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/",
> type = "source")


I'm on a Linux box and I get no difficulty using:

install.packages("spDataLarge",repos  =  "https://nowosad.github.io/drat/",type  =  "source")

That was the third of three options on its webpage at:

https://github.com/Nowosad/spDataLarge


There is an "issue" from Roger Bivand regarding the need for 
type="source: https://github.com/Nowosad/spDataLarge/issues/24

It would seem that `type="source"` might be superfluous since the 
package does not require compilation. At any rate there are two other 
options at that page and you might want to investigate them.


-- 

David.

> I tried with earlier versions of R (4.2.0 and 4.0.3) but I keep getting the
> same error. I was searching to see if I could obtain information regarding
> the R version or versions that support this package, but haven't found
> anything thus far.
>
> Any suggestions on how to successfully install this package?
>
> Something odd is that I was able to install package spData, but not
> spDataLarge (in R version 4.2.1)
>
> Best regards,
> Paul
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon Aug 29 20:42:05 2022
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 29 Aug 2022 13:42:05 -0500
Subject: [R] About spDataLarge Package
In-Reply-To: <e00ed74e-0ee7-a199-b4b1-10b739feede7@sapo.pt>
References: <CAMOcQfMGK4WEDGZ-_ePju8s5ERS3mjWD5ae=r1RA8yfnFToE+g@mail.gmail.com>
 <e00ed74e-0ee7-a199-b4b1-10b739feede7@sapo.pt>
Message-ID: <CAMOcQfPERQoNo_u-hCdFF5iebcK1HT+L8mjA5KQRhV5Zd8KSkw@mail.gmail.com>

Dear friend Rui,

Hope you are doing great. Your suggested workaround works perfectly, I was
able to successfully install package spDataLarge by doing:

install.packages("remotes")
remotes::install_github("Nowosad/spDataLarge")

Thank you so much for your valuable and kind help.

Best,
Paul

El lun, 29 ago 2022 a las 12:21, Rui Barradas (<ruipbarradas at sapo.pt>)
escribi?:

> Hello,
>
> That package is not a CRAN package, it's not even archived.
> Try
>
> remotes::install_github("Nowosad/spDataLarge")
>
>
> (If you haven't installed it yet, package remotes is a CRAN package.)
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 17:30 de 29/08/2022, Paul Bernal escreveu:
> > Dear friends,
> >
> > I have just installed R version 4.2.1 for Windows on my machine, and was
> > trying to install package spDataLarge, but the console threw the
> following
> > error message:
> >
> > Warning in install.packages :
> >    package 'spDataLarge' is not available for this version of R
> >
> > A version of this package for your version of R might be available
> > elsewhere,
> > see the ideas at
> >
> https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages
> >
> > I used the following command to try to install it:
> >
> > install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/
> ",
> > type = "source")
> > I tried with earlier versions of R (4.2.0 and 4.0.3) but I keep getting
> the
> > same error. I was searching to see if I could obtain information
> regarding
> > the R version or versions that support this package, but haven't found
> > anything thus far.
> >
> > Any suggestions on how to successfully install this package?
> >
> > Something odd is that I was able to install package spData, but not
> > spDataLarge (in R version 4.2.1)
> >
> > Best regards,
> > Paul
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon Aug 29 20:43:42 2022
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 29 Aug 2022 13:43:42 -0500
Subject: [R] About spDataLarge Package
In-Reply-To: <09a2adee-8c31-be68-bb93-367f875c223a@comcast.net>
References: <CAMOcQfMGK4WEDGZ-_ePju8s5ERS3mjWD5ae=r1RA8yfnFToE+g@mail.gmail.com>
 <09a2adee-8c31-be68-bb93-367f875c223a@comcast.net>
Message-ID: <CAMOcQfP7iSWck8HWNcbeUi4xPwTb8sGLTS7X1Ci1h5FZMcfzmA@mail.gmail.com>

Dear friend David,

Hope you are doing great. Thank you for your kind reply. I was able to
successfully install package spDataLarge following Rui's suggested
workaround:

installing package remotes and then using the following command:
remotes::install_github("Nowosad/spDataLarge")

Cheers,
Paul

El lun, 29 ago 2022 a las 12:32, David Winsemius (<dwinsemius at comcast.net>)
escribi?:

>
> On 8/29/22 09:30, Paul Bernal wrote:
> > Dear friends,
> >
> > I have just installed R version 4.2.1 for Windows on my machine, and was
> > trying to install package spDataLarge, but the console threw the
> following
> > error message:
> >
> > Warning in install.packages :
> >    package 'spDataLarge' is not available for this version of R
> >
> > A version of this package for your version of R might be available
> > elsewhere,
> > see the ideas at
> >
> https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages
> >
> > I used the following command to try to install it:
> >
> > install.packages("spDataLarge", repos = "https://nowosad.github.io/drat/
> ",
> > type = "source")
>
>
> I'm on a Linux box and I get no difficulty using:
>
> install.packages("spDataLarge",repos  =  "https://nowosad.github.io/drat/",type
> =  "source")
>
> That was the third of three options on its webpage at:
>
> https://github.com/Nowosad/spDataLarge
>
>
> There is an "issue" from Roger Bivand regarding the need for
> type="source: https://github.com/Nowosad/spDataLarge/issues/24
>
> It would seem that `type="source"` might be superfluous since the
> package does not require compilation. At any rate there are two other
> options at that page and you might want to investigate them.
>
>
> --
>
> David.
>
> > I tried with earlier versions of R (4.2.0 and 4.0.3) but I keep getting
> the
> > same error. I was searching to see if I could obtain information
> regarding
> > the R version or versions that support this package, but haven't found
> > anything thus far.
> >
> > Any suggestions on how to successfully install this package?
> >
> > Something odd is that I was able to install package spData, but not
> > spDataLarge (in R version 4.2.1)
> >
> > Best regards,
> > Paul
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


