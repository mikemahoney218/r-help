From cgb at datanalytics.com  Sun Jul  1 01:16:37 2007
From: cgb at datanalytics.com (Carlos J. Gil Bellosta)
Date: Sun, 01 Jul 2007 00:16:37 +0100
Subject: [R] [R-SIG-Finance] Method dispatch in functions?
In-Reply-To: <972441.14518.qm@web35410.mail.mud.yahoo.com>
References: <972441.14518.qm@web35410.mail.mud.yahoo.com>
Message-ID: <1183245397.6000.6.camel@localhost>

Look at the UseMethod function. The help for the "print" method, a
heavily overloaded function, can also help.

Regards,

Carlos J. Gil Bellosta
http://www.datanalytics.com

On Thu, 2007-06-28 at 09:05 -0700, John McHenry wrote:
> Hi,
> 
> Could someone point me in the right direction for documentation on the following question? 
> 
> Let's say I have two objects a and b of classes A and B, respectively.
> Now let's say I write a function foo that does something similar to 
> objects of type A and B. Basically I want to overload the function
> in C++ talk, so if I give foo and object of type A something (and this
> is my question) dispatches the call to, say, foo.A, and if I give foo
> and object of type B something dispatches the call to, say, foo.B.
> 
> I want to write foo.A and foo.B. How to I perform the method 
> dispatch? From what I understand there are two ways in R:
> S3 and S4. What is the simple S3 way?
> 
> Thanks!
> 
> Jack.
> 
>        
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-SIG-Finance at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-sig-finance
> -- Subscriber-posting only. 
> -- If you want to post, subscribe first.


From martin.sikora at upf.edu  Sun Jul  1 01:32:43 2007
From: martin.sikora at upf.edu (martin sikora)
Date: Sun, 1 Jul 2007 01:32:43 +0200
Subject: [R] speed and looping issues; calculations on big datasets
Message-ID: <937EA1A0-0513-4FCD-8036-68EB1503009A@upf.edu>

dear r users,

i'm a little stuck with the following problem(s), hopefully somebody  
can offer some help:

i have data organized in a binary matrix, which can become quite big  
like 60 rows x 10^5 columns (they represent SNP genotypes, for some  
background info). what i need to do is the following:

let's suppose i have a matrix of size n x m. for each of the m  
columns, i want to know the counts of unique rows extended one by one  
from the "core" column, for both values at the "core" separately and  
in both directions. maybe better explained with a little example.

data:

00 0 010
10 1 001
11 1 011
10 0 011
10 0 010

so the extended unique rows & counts taking e.g. column 3 as "core" are:

col 3 = 0:
right:
patterns / counts
00 / 3
001 / 3
0010, 0011 / 2,1

left:
00 / 3
000,001 / 1,2

and that for the other subset ( col3 = 1) as well, then doing the  
whole thing again for the next "core" column. the reason i need this  
counts is that i want to calculate frequencies of the different  
extended sequences to calculate the probability of drawing two  
identical sequences from the core up to an extended position from the  
whole set of sequences.

my main problem is speed of the calculations. i tried different ways  
suggested here in the list of getting the counts of the unique rows,  
all of them using the "table" function. both a combination of table 
( do.call( paste, c( as.data.frame( mymatrix) ) ) ) or table( apply 
( mymatrix , 2 , paste , collapse ="" ) ) work fine, but are too slow  
for bigger matrices that i want to calculate (at least in my not very  
sophisticated function). then i found a great suggestion here to do a  
matrix multiplication with a vector of 2^(0:ncol-1) to convert each  
row into a decimal number, and do table on those. this speeds up  
things quite nicely, although the problem is that it of course does  
not work as soon as i extended for more than 60 columns, because the  
decimal numbers get to large to accurately distinguish between a 0  
and 1 at the smallest digit:

 > 2^60+2 == 2^60
[1] TRUE

another thing is that so far i could not come up with an idea on how  
or if it is possible to do this without the loops i am using, one  
large loop for each column in turn as core, and then another loop  
within that extends the rows by growing column numbers. since i am  
not the best of programmers (and still quite new to R), i was hoping  
that somebody has some advice on doing this calculations in a more  
elegant and more importantly, fast way.
just to get the idea, the approach with the matrix multiplication  
takes 20s for a 60 x 220 matrix on my macbook pro, which is obviously  
not perfect, considering i would like to use this function for  
matrices of size 10^2 x 10^5 or even more.

so i would be very thankful for any ideas, suggestions etc to improve  
this

cheers
martin


From milton_ruser at yahoo.com.br  Sun Jul  1 01:56:45 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Sat, 30 Jun 2007 16:56:45 -0700 (PDT)
Subject: [R] MDS/NMDS: When and Why use or not use?
Message-ID: <441164.91893.qm@web56614.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070630/f9aeb9b6/attachment.pl 

From adschai at optonline.net  Sun Jul  1 02:19:25 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Sun, 01 Jul 2007 00:19:25 +0000 (GMT)
Subject: [R] How to save results from chisq.test or mantelhaen.test to file
Message-ID: <e244c0cf20453.4686f30d@optonline.net>

Hi,

I am new to these functions. I'm wondering if there is anyway to save the entire results (all attributes of the result object) from the chisq.test or mantelhaen.test functions? For example, from chisq.test function, you will have statistic, parameter, p.value, expected, etc. in the result list. How can I save all of them in one shot to, says, a text file or csv file? Thank you.

- adschai


From ck at altaica.de  Sun Jul  1 11:31:20 2007
From: ck at altaica.de (Christoph Krammer)
Date: Sun, 1 Jul 2007 11:31:20 +0200
Subject: [R] Plots from categorial data
In-Reply-To: <009a01c7ba84$416b9f10$0f00a8c0@sydney>
Message-ID: <00c301c7bbc2$9551bd00$0f00a8c0@sydney>

Hello everybody,

Since my first message was caught by the spam filter, I just try to do it
again:

I want to use R to generate plots from categorial data. The data contains
results from OCR scans over images with are preprocessed by different image
filtering techniques. A small sample data set looks as following:

> data <- read.csv("d:/tmp_da/sql_data/filter_d_tool.csv", header=T) 
> data
      ocrtool filter_setting avg.hit.
1  FineReader            2x1    0.383
2  FineReader            2x2    0.488
3  FineReader            3x2    0.268
4  FineReader            3x3    0.198
5  FineReader            4x3    0.081
6  FineReader            4x4    0.056
7        gocr            2x1    0.153
8        gocr            2x2    0.102
9        gocr            3x2    0.047
10       gocr            3x3    0.052
11       gocr            4x3    0.014
12       gocr            4x4    0.002
13      ocrad            2x1    0.085
14      ocrad            2x2    0.094
15      ocrad            3x2    0.045
16      ocrad            3x3    0.050
17      ocrad            4x3    0.025
18      ocrad            4x4    0.009


I now want to draw a plot with the categories (filter_setting) as X axis,
and the avg_hit as Y axis. There should be lines for each ocrtool.

But when I draw a plot, the resulting plot always contains bars, even if I
specify type="n".
> plot(data$filter_setting, data$avg.hit., type="n")

When I only plot the categories, without data, there appear strange grey
(but empty) boxes. 
> plot(data$filter_setting, type="n")

Who do I get a clean white box to draw the different lines in?

Thanks and regards,
 Christoph

---
Christoph Krammer
Student

University of Mannheim
Laboratory for Dependable Distributed Systems A5, 6
68131 Mannheim
Germany


From ccleland at optonline.net  Sun Jul  1 11:53:03 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 01 Jul 2007 05:53:03 -0400
Subject: [R] How to save results from chisq.test or mantelhaen.test to
	file
In-Reply-To: <e244c0cf20453.4686f30d@optonline.net>
References: <e244c0cf20453.4686f30d@optonline.net>
Message-ID: <4687797F.60103@optonline.net>

adschai at optonline.net wrote:
> Hi,
> 
> I am new to these functions. I'm wondering if there is anyway to save the entire results (all attributes of the result object) from the chisq.test or mantelhaen.test functions? For example, from chisq.test function, you will have statistic, parameter, p.value, expected, etc. in the result list. How can I save all of them in one shot to, says, a text file or csv file? Thank you.
> 
> - adschai

  You could unlist() the result, coerce it to a data frame, then use
write.table().  For example, something like this:

write.table(as.data.frame(t(unlist(chisq.test(InsectSprays$count > 7,
InsectSprays$spray)))), quote=FALSE)

or

write.table(as.data.frame(unlist(chisq.test(InsectSprays$count > 7,
InsectSprays$spray))), quote=FALSE)

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From maja.schroeter at gmx.de  Sun Jul  1 11:59:49 2007
From: maja.schroeter at gmx.de (=?iso-8859-1?Q?=22Maja_Schr=F6ter=22?=)
Date: Sun, 01 Jul 2007 11:59:49 +0200
Subject: [R]  How to start a R script from a dos command?
Message-ID: <20070701095949.327520@gmx.net>

Hi everybody,

I want to start a R programm from a dos command.

Are there any possibilities that I can start e.g. the file "Test.R" from dos?

Maybe something like: R.exe -Test.R ?

Thank you very much!

Regards,

Maja 
--


From h.wickham at gmail.com  Sun Jul  1 12:20:39 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 1 Jul 2007 12:20:39 +0200
Subject: [R] Plots from categorial data
In-Reply-To: <00c301c7bbc2$9551bd00$0f00a8c0@sydney>
References: <009a01c7ba84$416b9f10$0f00a8c0@sydney>
	<00c301c7bbc2$9551bd00$0f00a8c0@sydney>
Message-ID: <f8e6ff050707010320s6b2ea9f8r86a47f648810994e@mail.gmail.com>

Perhaps this will do what you want:

library(ggplot2)
qplot(filter_setting, avg.hit, data=data, colour=ocrtool, geom="line")

find out more about ggplot2 at http://had.co.nz/ggplot2

Hadley

On 7/1/07, Christoph Krammer <ck at altaica.de> wrote:
> Hello everybody,
>
> Since my first message was caught by the spam filter, I just try to do it
> again:
>
> I want to use R to generate plots from categorial data. The data contains
> results from OCR scans over images with are preprocessed by different image
> filtering techniques. A small sample data set looks as following:
>
> > data <- read.csv("d:/tmp_da/sql_data/filter_d_tool.csv", header=T)
> > data
>       ocrtool filter_setting avg.hit.
> 1  FineReader            2x1    0.383
> 2  FineReader            2x2    0.488
> 3  FineReader            3x2    0.268
> 4  FineReader            3x3    0.198
> 5  FineReader            4x3    0.081
> 6  FineReader            4x4    0.056
> 7        gocr            2x1    0.153
> 8        gocr            2x2    0.102
> 9        gocr            3x2    0.047
> 10       gocr            3x3    0.052
> 11       gocr            4x3    0.014
> 12       gocr            4x4    0.002
> 13      ocrad            2x1    0.085
> 14      ocrad            2x2    0.094
> 15      ocrad            3x2    0.045
> 16      ocrad            3x3    0.050
> 17      ocrad            4x3    0.025
> 18      ocrad            4x4    0.009
>
>
> I now want to draw a plot with the categories (filter_setting) as X axis,
> and the avg_hit as Y axis. There should be lines for each ocrtool.
>
> But when I draw a plot, the resulting plot always contains bars, even if I
> specify type="n".
> > plot(data$filter_setting, data$avg.hit., type="n")
>
> When I only plot the categories, without data, there appear strange grey
> (but empty) boxes.
> > plot(data$filter_setting, type="n")
>
> Who do I get a clean white box to draw the different lines in?
>
> Thanks and regards,
>  Christoph
>
> ---
> Christoph Krammer
> Student
>
> University of Mannheim
> Laboratory for Dependable Distributed Systems A5, 6
> 68131 Mannheim
> Germany
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jared.oconnell at gmail.com  Sun Jul  1 12:25:00 2007
From: jared.oconnell at gmail.com (Jared O'Connell)
Date: Sun, 1 Jul 2007 18:25:00 +0800
Subject: [R] How to start a R script from a dos command?
In-Reply-To: <20070701095949.327520@gmx.net>
References: <20070701095949.327520@gmx.net>
Message-ID: <8c464e8f0707010325x1392c08bn69acc3c4c9e96ea6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070701/de2cca3a/attachment.pl 

From jim at bitwrit.com.au  Sun Jul  1 13:23:56 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sun, 01 Jul 2007 21:23:56 +1000
Subject: [R] Plots from categorial data
In-Reply-To: <00c301c7bbc2$9551bd00$0f00a8c0@sydney>
References: <00c301c7bbc2$9551bd00$0f00a8c0@sydney>
Message-ID: <46878ECC.2040702@bitwrit.com.au>

Christoph Krammer wrote:
> Hello everybody,
> 
> Since my first message was caught by the spam filter, I just try to do it
> again:
> 
> I want to use R to generate plots from categorial data. The data contains
> results from OCR scans over images with are preprocessed by different image
> filtering techniques. A small sample data set looks as following:
> 
> 
>>data <- read.csv("d:/tmp_da/sql_data/filter_d_tool.csv", header=T) 
>>data
> 
>       ocrtool filter_setting avg.hit.
> 1  FineReader            2x1    0.383
> 2  FineReader            2x2    0.488
> 3  FineReader            3x2    0.268
> 4  FineReader            3x3    0.198
> 5  FineReader            4x3    0.081
> 6  FineReader            4x4    0.056
> 7        gocr            2x1    0.153
> 8        gocr            2x2    0.102
> 9        gocr            3x2    0.047
> 10       gocr            3x3    0.052
> 11       gocr            4x3    0.014
> 12       gocr            4x4    0.002
> 13      ocrad            2x1    0.085
> 14      ocrad            2x2    0.094
> 15      ocrad            3x2    0.045
> 16      ocrad            3x3    0.050
> 17      ocrad            4x3    0.025
> 18      ocrad            4x4    0.009
> 
> 
> I now want to draw a plot with the categories (filter_setting) as X axis,
> and the avg_hit as Y axis. There should be lines for each ocrtool.
> 
> But when I draw a plot, the resulting plot always contains bars, even if I
> specify type="n".
> 
>>plot(data$filter_setting, data$avg.hit., type="n")
> 
> 
> When I only plot the categories, without data, there appear strange grey
> (but empty) boxes. 
> 
>>plot(data$filter_setting, type="n")
> 
> 
> Who do I get a clean white box to draw the different lines in?
> 
Hi Christoph,

How about this?

plot(as.numeric(krammer$filter_setting[1:6]),krammer$avg_hit[1:6],
  type="b",col=2,ylim=c(0,0.5),main="OCR performance",
  xlab="Filter setting",ylab="Average hits",axes=FALSE)
points(as.numeric(krammer$filter_setting[7:12]),krammer$avg_hit[7:12],
  type="b",col=3)
points(as.numeric(krammer$filter_setting[13:18]),krammer$avg_hit[13:18],
  type="b",col=4)
box()
axis(1,at=1:6,labels=c("2x1","2x2","3x2","3x3","4x3","4x4"))
axis(2)

Jim


From carlos.grohmann at gmail.com  Sun Jul  1 13:20:29 2007
From: carlos.grohmann at gmail.com (=?ISO-8859-1?Q?Carlos_"Gu=E2no"_Grohmann?=)
Date: Sun, 1 Jul 2007 12:20:29 +0100
Subject: [R] moving-window (neighborhood) analysis
In-Reply-To: <224562.39105.qm@web56609.mail.re3.yahoo.com>
References: <224562.39105.qm@web56609.mail.re3.yahoo.com>
Message-ID: <bd07447b0707010420h7a1d9de7sdbe0f638a1714ce6@mail.gmail.com>

Hi Milton

thanks for your help

I want to compute a lot of things.. :)
for instance, I want to look at the large scale (regional, non-local)
behavior of slope and aspect, but since aspect is a circular variable,
I can't just go around with mean/median/etc, which are the tools I
have on GIS, so I was hoping I could find some way to define the
moving-window and the apply some function (from a package or
user-defined) to the values within the window (like circular
statistics).

best regards

Carlos
(Brazil / UK)


On 7/1/07, Milton Cezar Ribeiro <milton_ruser at yahoo.com.br> wrote:
>
> Hi Carlos,
>
> What are really you looking for? What you want to compute for the central
> pixel?
> I use FRAGSTATS to compute some landscape metrics using moving windows.
> There you can define circular and rectangular shaped search windows, sized
> as you want.
>
> Kind regards,
>
> Miltinho
> Brazil
>
>
> ----- Mensagem original ----
> De: "Carlos "Gu?no" Grohmann" <carlos.grohmann at gmail.com>
> Para: r-help at stat.math.ethz.ch
> Enviadas: Quarta-feira, 27 de Junho de 2007 12:27:28
> Assunto: [R] moving-window (neighborhood) analysis
>
>
> Hello all
>
> I was wondering what would be the best way to do a moving-window
> analysis of a matrix? By moving-window I mean that kind of analysis
> common in GIS, where each pixel (matrix element) of the resulting map
> is a function of it neighbors, and the neighborhood is a square
> matrix.
> I was hoping there was some function in R that could do that, where I
> could define the size of the neighborhood, and then apply some
> function to the values, some function I don't have in GIS packages
> (like circular statistics).
>
> thanks all.
>
> Carlos
>
>
> --
> +-----------------------------------------------------------+
>               Carlos Henrique Grohmann - Guano
>   Visiting Researcher at Kingston University London - UK
>   Geologist M.Sc  - Doctorate Student at IGc-USP - Brazil
> Linux User #89721  - carlos dot grohmann at gmail dot com
> +-----------------------------------------------------------+
> _________________
> "Good morning, doctors. I have taken the liberty of removing Windows
> 95 from my hard drive."
> --The winning entry in a "What were HAL's first words" contest judged
> by 2001: A SPACE ODYSSEY creator Arthur C. Clarke
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>  ________________________________
> Novo Yahoo! Cad?? - Experimente uma nova busca.


-- 
+-----------------------------------------------------------+
              Carlos Henrique Grohmann - Guano
  Visiting Researcher at Kingston University London - UK
  Geologist M.Sc  - Doctorate Student at IGc-USP - Brazil
Linux User #89721  - carlos dot grohmann at gmail dot com
+-----------------------------------------------------------+
_________________
"Good morning, doctors. I have taken the liberty of removing Windows
95 from my hard drive."
--The winning entry in a "What were HAL's first words" contest judged
by 2001: A SPACE ODYSSEY creator Arthur C. Clarke


From ck at altaica.de  Sun Jul  1 13:57:07 2007
From: ck at altaica.de (Christoph Krammer)
Date: Sun, 1 Jul 2007 13:57:07 +0200
Subject: [R] Plots from categorial data
In-Reply-To: <f8e6ff050707010320s6b2ea9f8r86a47f648810994e@mail.gmail.com>
Message-ID: <00cb01c7bbd6$f43dba30$0f00a8c0@sydney>

Hello Hadley,

Thanks a lot for your help. I got the plot I want out of this module with a
slightly more complicated command.

But now, I have an additional problem: 

In the given case, the "filtersetting" column contains letters, so R takes
the values as categories. But I have other filters, which only have numeric
categories like "0.125", "0.25", "1", and so on. But there is no real
"distance" between these values, so the data is still categorial. But if I
draw a plot from this data, the result is a plot with axis labels like 0.2,
0.4, 0.6, ...

How do I tell R to treat the numbers in the filtersetting column as
categories?

Thanks and regards,
 Christoph


-----Urspr?ngliche Nachricht-----
Von: hadley wickham [mailto:h.wickham at gmail.com] 
Gesendet: Sonntag, 1. Juli 2007 12:21
An: Christoph Krammer
Cc: r-help at stat.math.ethz.ch
Betreff: Re: [R] Plots from categorial data

Perhaps this will do what you want:

library(ggplot2)
qplot(filter_setting, avg.hit, data=data, colour=ocrtool, geom="line")

find out more about ggplot2 at http://had.co.nz/ggplot2

Hadley

On 7/1/07, Christoph Krammer <ck at altaica.de> wrote:
> Hello everybody,
>
> Since my first message was caught by the spam filter, I just try to do 
> it
> again:
>
> I want to use R to generate plots from categorial data. The data 
> contains results from OCR scans over images with are preprocessed by 
> different image filtering techniques. A small sample data set looks as
following:
>
> > data <- read.csv("d:/tmp_da/sql_data/filter_d_tool.csv", header=T) 
> > data
>       ocrtool filter_setting avg.hit.
> 1  FineReader            2x1    0.383
> 2  FineReader            2x2    0.488
> 3  FineReader            3x2    0.268
> 4  FineReader            3x3    0.198
> 5  FineReader            4x3    0.081
> 6  FineReader            4x4    0.056
> 7        gocr            2x1    0.153
> 8        gocr            2x2    0.102
> 9        gocr            3x2    0.047
> 10       gocr            3x3    0.052
> 11       gocr            4x3    0.014
> 12       gocr            4x4    0.002
> 13      ocrad            2x1    0.085
> 14      ocrad            2x2    0.094
> 15      ocrad            3x2    0.045
> 16      ocrad            3x3    0.050
> 17      ocrad            4x3    0.025
> 18      ocrad            4x4    0.009
>
>
> I now want to draw a plot with the categories (filter_setting) as X 
> axis, and the avg_hit as Y axis. There should be lines for each ocrtool.
>
> But when I draw a plot, the resulting plot always contains bars, even 
> if I specify type="n".
> > plot(data$filter_setting, data$avg.hit., type="n")
>
> When I only plot the categories, without data, there appear strange 
> grey (but empty) boxes.
> > plot(data$filter_setting, type="n")
>
> Who do I get a clean white box to draw the different lines in?
>
> Thanks and regards,
>  Christoph
>
> ---
> Christoph Krammer
> Student
>
> University of Mannheim
> Laboratory for Dependable Distributed Systems A5, 6
> 68131 Mannheim
> Germany
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bolker at ufl.edu  Sun Jul  1 14:57:29 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Sun, 1 Jul 2007 12:57:29 +0000 (UTC)
Subject: [R] Standard Probability Distributions.
References: <c0792190706300741j458ed50dm2432b655ffbac794@mail.gmail.com>
	<815b70590706301015l474ae912je59cd220a7594d02@mail.gmail.com>
Message-ID: <loom.20070701T145658-130@post.gmane.org>

David Barron <mothsailor <at> googlemail.com> writes:

 Try RSiteSearch to look for specific
> distributions.
> 

  also try
http://wiki.r-project.org/rwiki/doku.php?id=tips:stats-distri:0verview&s=binomial


From jfox at mcmaster.ca  Sun Jul  1 14:57:46 2007
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 01 Jul 2007 08:57:46 -0400
Subject: [R] SEM model fit
Message-ID: <web-177980259@cgpsrv2.cis.mcmaster.ca>

Dear Frank,

My apologies for the slow response: I'm away from home and checking
r-help infrequently.

To find the confidence interval for the RMSEA it's necessary to compute
two chisquare noncentrality parameters. summary.sem() does this by
one-dimensional optimizations. If the upper bound of the CI is very
large  or the lower bound very close to 0, it might not be possible to
find the values with sufficient precision, and NA is printed.

Looking at the code for summary.sem(), however, I see that the
optimizations could fail spuriously if the sample size is large;
moreover, under these circumstances, both the upper and lower bounds
will be NA, even if the lower bound could have been determined. I've
therefore modified summary.sem() so that it should work more reliably,
and have attached a file with the modified function to this email. Let
me know if it provides more satisfactory results. (Because you didn't
give the input correlation matrix, I can't check myself.) I'll
eventually incorporate the new function is an updated version of the
package.

BTW, I doubt that the RMSEA confidence interval is correct for
polychoric correlations.

Regards,
 John
 
-------- original message ------

 I wonder if someone could explain why, when I perform confirmatory
factor-analysis model using polychoric correlations why I do not get an
estimated confidence interval for the RMSEA.  My experience with these
type
models is that I would obtain a confidence interval estimate.  I did
not get
any warning messages with the output.

RESULTS:

Model Chisquare =  1374   Df =  185 Pr(>Chisq) = 0
 Chisquare (null model) =  12284   Df =  210
 Goodness-of-fit index =  0.903
 Adjusted goodness-of-fit index =  0.88
 RMSEA index =  0.0711   90% CI: (NA, NA)
 Bentler-Bonnett NFI =  0.888
 Tucker-Lewis NNFI =  0.888
 Bentler CFI =  0.902
 SRMR =  0.0682
 BIC =  51.4 


SYNTAX

rm(sem.enf.rq)
mdl.rq <- specify.model()
enf                   -> law2,      NA,       1
enf                   -> law3,      lam2,     1
enf                   -> law4,      lam3,     1
enf                   <-> enf,      psi1,     0.6
law2                  <-> law2,     theta1,   0.3
law3                  <-> law3,     theta2,   0.3
law4                  <-> law4,     theta3,   0.5
gender                -> enf,       a1,       0.2
incomex               -> enf,       a2,       0.2
oftdrnkr              -> enf,       a3,       0.2
attn                  -> nvatt,     NA,       1
attn                  -> crimatt,   lam4,     1.3
attn                  -> asltatt,   lam5,     1.2
attn                  <-> attn,     psi2,     0.5
nvatt                 <-> nvatt,    theta4,   0.5
crimatt               <-> crimatt,  theta5,   0.1
asltatt               <-> asltatt,  theta6,   0.2
gender                -> attn,      a4,       0.2
acon                   -> acon1,    NA,       1
acon                   -> acon2,    lam4,     1.5
acon                   <-> acon,    psi2,     0.1
mcon                   -> mvcon1,   NA,       1
mcon                   -> mvcon2,   lam5,     1
mcon                   <-> mcon,    psi3,     0.3
ocon                   -> oicon1,   NA,       1
ocon                   -> oicon2,   lam6,     1
ocon                   <-> ocon,    psi4,     0.2
con                    -> acon,     NA,       1
con                    -> mcon,     lam7,     0.8
con                    -> ocon,     lam8,     0.9
con                   <-> con,     psi5,     0.3
acon1                 <-> acon1,   theta7,   0.4
acon2                 <-> acon2,   theta8,   0.2
mvcon1                <-> mvcon1,  theta9,   0.2
mvcon2                <-> mvcon2,  theta10,   0.3
oicon1                <-> oicon1,  theta11,   0.2
oicon2                <-> oicon2,  theta12,   0.3
gender                -> con,      a5,       0.1
incomex               -> con,      a6,       -0.1
oftdrnkr              -> con,      a7,       -0.2
attn                  -> con,      gam1,     0.2
sev                   -> aophys,   NA,        1
sev                   -> mvphys,   NA,        1
sev                   -> oiphys,   NA,        1
sev                   <-> sev,     psi6,      0.5
aophys                <-> aophys,  theta13,    0.5
mvphys                <-> mvphys,  theta14,    0.5
oiphys                <-> oiphys,  theta14,    0.5
con                   -> sev,      gam3,       0.8
prev                  -> mvpct,    NA,        1
prev                  -> oipct,    NA,        1
prev                  -> alcpct,   NA,        1
prev                  <-> prev,    psi8,      0.4
mvpct                 <-> mvpct,   theta15,    0.5
oipct                 <-> oipct,   theta15,    0.5
alcpct                <-> alcpct,  theta15,    0.5
con                   -> prev,     gam5,       0.8 
prev                  -> enf,      gam6,       0.4

sem.enf.rq <- sem(ram = mdl.rq, S = hcor(dx),  N = nrow(dx), obs.v =
names(dx), raw = F, fixed = names(dx)[4:6], par.size = 's', maxiter =
1e3,
analytic = F, gradtol = 1e-10)  ##set raw to False
summary(obj = sem.enf.rq, dig = 3, conf = 0.9) 

Respectfully,

Frank Lawrence

--------------------------------
John Fox, Professor
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/

From h.wickham at gmail.com  Sun Jul  1 17:48:44 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 1 Jul 2007 17:48:44 +0200
Subject: [R] Plots from categorial data
In-Reply-To: <00cb01c7bbd6$f43dba30$0f00a8c0@sydney>
References: <f8e6ff050707010320s6b2ea9f8r86a47f648810994e@mail.gmail.com>
	<00cb01c7bbd6$f43dba30$0f00a8c0@sydney>
Message-ID: <f8e6ff050707010848n481cabf4y694b2d8bd58cc9f7@mail.gmail.com>

On 7/1/07, Christoph Krammer <ck at altaica.de> wrote:
> Hello Hadley,
>
> Thanks a lot for your help. I got the plot I want out of this module with a
> slightly more complicated command.
>
> But now, I have an additional problem:
>
> In the given case, the "filtersetting" column contains letters, so R takes
> the values as categories. But I have other filters, which only have numeric
> categories like "0.125", "0.25", "1", and so on. But there is no real
> "distance" between these values, so the data is still categorial. But if I
> draw a plot from this data, the result is a plot with axis labels like 0.2,
> 0.4, 0.6, ...
>
> How do I tell R to treat the numbers in the filtersetting column as
> categories?

Just make it a factor:
qplot(factor(filter_setting), avg.hit, data=data, colour=ocrtool, geom="line")

Hadley


From h.wickham at gmail.com  Sun Jul  1 17:58:54 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 1 Jul 2007 17:58:54 +0200
Subject: [R] [R-pkgs] Clusterfly
Message-ID: <f8e6ff050707010858h7f4b925eu7e5fbecb5a6479f9@mail.gmail.com>

clusterfly
http://had.co.nz/clusterfly/

Typically, there is somewhat of a divide between statistics and
visualisation software. Statistics software, particularly R, provides
implementation of cutting edge research methods, but limited graphics.
Visualisation software will provide sophisticated visual interfaces,
but few statistical algorithms. The clusterfly package presents some
early experimentation aimed at overcoming this deficiency by linking R
and GGobi. Cluster analysis was chosen as it is an exploratory method
that needs sophisticated visualisation and statistical algorithms.

Clusterfly provides some tools that work with all clustering
algorithms, and some that are tailored for particular ones.  Generic
tools allow you to animate between clusterings (see ?cfly_animate) and
produce common static graphics (?cfly_dist, ?cfly_pcp).  Specific
algorithms are available for:

* Self organising maps (aka Kohonen neural networks), ?ggobi.som.
Displays the self organising map/net in the original space of the
data.

* Hierarchical clustering, ?hierfly. Connects data points with lines
like a dendrogram, but in the high-dimensional space of the original
data

 * Model based clustering, ?mefly. Adds ellipsoids from the
multivariate normal distributions the clusters are based on

You will need GGobi (http://www.ggobi.org) and rggobi
(http://www.ggobi.org/rggobi) installed to be able to use clusterfly.

Regards,

Hadley

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From deepayan.sarkar at gmail.com  Sun Jul  1 19:21:46 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Sun, 1 Jul 2007 10:21:46 -0700
Subject: [R] Plots from categorial data
In-Reply-To: <46878ECC.2040702@bitwrit.com.au>
References: <00c301c7bbc2$9551bd00$0f00a8c0@sydney>
	<46878ECC.2040702@bitwrit.com.au>
Message-ID: <eb555e660707011021w65ad8cc9n844b07a74d7a4572@mail.gmail.com>

On 7/1/07, Jim Lemon <jim at bitwrit.com.au> wrote:
> Christoph Krammer wrote:
> > Hello everybody,
> >
> > Since my first message was caught by the spam filter, I just try to do it
> > again:
> >
> > I want to use R to generate plots from categorial data. The data contains
> > results from OCR scans over images with are preprocessed by different
> image
> > filtering techniques. A small sample data set looks as following:
> >
> >
> >>data <- read.csv("d:/tmp_da/sql_data/filter_d_tool.csv", header=T)
> >>data
> >
> >       ocrtool filter_setting avg.hit.
> > 1  FineReader            2x1    0.383
> > 2  FineReader            2x2    0.488
> > 3  FineReader            3x2    0.268
> > 4  FineReader            3x3    0.198
> > 5  FineReader            4x3    0.081
> > 6  FineReader            4x4    0.056
> > 7        gocr            2x1    0.153
> > 8        gocr            2x2    0.102
> > 9        gocr            3x2    0.047
> > 10       gocr            3x3    0.052
> > 11       gocr            4x3    0.014
> > 12       gocr            4x4    0.002
> > 13      ocrad            2x1    0.085
> > 14      ocrad            2x2    0.094
> > 15      ocrad            3x2    0.045
> > 16      ocrad            3x3    0.050
> > 17      ocrad            4x3    0.025
> > 18      ocrad            4x4    0.009
> >
> >
> > I now want to draw a plot with the categories (filter_setting) as X axis,
> > and the avg_hit as Y axis. There should be lines for each ocrtool.
> >
> > But when I draw a plot, the resulting plot always contains bars, even if I
> > specify type="n".
> >
> >>plot(data$filter_setting, data$avg.hit., type="n")
> >
> >
> > When I only plot the categories, without data, there appear strange grey
> > (but empty) boxes.
> >
> >>plot(data$filter_setting, type="n")
> >
> >
> > Who do I get a clean white box to draw the different lines in?
> >
> Hi Christoph,
>
> How about this?
>
> plot(as.numeric(krammer$filter_setting[1:6]),krammer$avg_hit[1:6],
>   type="b",col=2,ylim=c(0,0.5),main="OCR performance",
>   xlab="Filter setting",ylab="Average hits",axes=FALSE)
> points(as.numeric(krammer$filter_setting[7:12]),krammer$avg_hit[7:12],
>   type="b",col=3)
> points(as.numeric(krammer$filter_setting[13:18]),krammer$avg_hit[13:18],
>   type="b",col=4)
> box()
> axis(1,at=1:6,labels=c("2x1","2x2","3x2","3x3","4x3","4x4"))
> axis(2)

And this is mostly equivalent to

with(krammer, interaction.plot(filter_setting, ocrtool, avg_hit))

or (with the original names)

with(data, interaction.plot(filter_setting, ocrtool, avg.hit.))

-Deepayan


From adschai at optonline.net  Sun Jul  1 19:31:21 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Sun, 01 Jul 2007 17:31:21 +0000 (GMT)
Subject: [R] How to save results from chisq.test or mantelhaen.test to
	file
In-Reply-To: <4687797F.60103@optonline.net>
References: <e244c0cf20453.4686f30d@optonline.net>
	<4687797F.60103@optonline.net>
Message-ID: <e4adf54f201f6.4687e4e9@optonline.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070701/b536e3c2/attachment.pl 

From shirley0818 at gmail.com  Sun Jul  1 19:49:12 2007
From: shirley0818 at gmail.com (shirley zhang)
Date: Sun, 1 Jul 2007 13:49:12 -0400
Subject: [R] unequal variance assumption for lme (mixed effect model)
In-Reply-To: <46852C31.80905@pdf.com>
References: <6fb73d020706271855seedb40dobbc0fe2578de94ea@mail.gmail.com>
	<1182998669.4837.11.camel@sib-sblomber01d.sib.uq.edu.au>
	<6fb73d020706272014g18d3abb9ob70ab559883b6fb3@mail.gmail.com>
	<46852C31.80905@pdf.com>
Message-ID: <6fb73d020707011049q32ca88fv599a1dd9cf06735b@mail.gmail.com>

Thanks for Spencer and Simon's help.  I've got very interesting
results based on your suggestions.

One more question,  how to handle unequal variance problme in lm()?
Isn't the weights option also, which means weighted least squares,
right?  Can you give me an example of setting this parameter in lm()
to account for  different variance assumption in each group?

Thanks again,
Shirley


On 6/29/07, Spencer Graves <spencer.graves at pdf.com> wrote:
> <comments in line>
>
> shirley zhang wrote:
> > Hi Simon,
> >
> > Thanks for your reply. Your reply reminds me that book. I've read it
> > long time ago, but haven't  try the weights option in my projects
> > yet:)
> >
> > Is the heteroscedastic test always less powerful because we have to
> > estimate the within group variance from the given data?
> >
> SG:  In general, I suspect we generally lose power when we estimate more
> parameters.
>
> SG:  You can check this using the 'simulate.lme' function, whose use is
> illustrated in the seminal work reported in sect. 2.4 of Pinheiro and
> Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).
> > Should we check whether each group has equal variance before using
> > weights=varIdent()? If we should, what is the function for linear
> > mixed model?
> >
> SG:  The general advice I've seen is to avoid excessive
> overparameterization of heterscedasticity and correlations.  However,
> parsimonious correlation had heterscedasticity models would likely be
> wise.  Years ago, George Box expressed concern about people worrying too
> much about outliers, which are often fairly obvious and relatively easy
> to detect, while they worried too little, he thought, about dependence,
> especially serial dependence, which is generally more difficult to
> detect and creates bigger problems in inference than outliers.  He
> wrote, "Why worry about mice when there are tigers about?"
>
> SG:  Issues of this type can be fairly easily evaluated using
> 'simulate.lme'.
>
>      Hope this helps.
>      Spencer Graves
> > Thanks,
> > Shirley
> >
> > On 6/27/07, Simon Blomberg <s.blomberg1 at uq.edu.au> wrote:
> >
> >> The default settings for lme do assume equal variances within groups.
> >> You can change that by using the various varClasses. see ?varClasses. A
> >> simple example would be to allow unequal variances across groups. So if
> >> your call to lme was:
> >>
> >> lme(...,random=~1|group,...)
> >>
> >> then to allow each group to have its own variance, use:
> >>
> >> lme(...,random=~1|group, weights=varIdent(form=~1|group),...)
> >>
> >> You really really should read Pinheiro & Bates (2000). It's all there.
> >>
> >> HTH,
> >>
> >> Simon.
> >>
> >> , On Wed, 2007-06-27 at 21:55 -0400, shirley zhang wrote:
> >>
> >>> Dear Douglas and R-help,
> >>>
> >>> Does lme assume normal distribution AND equal variance among groups
> >>> like anova() does? If it does, is there any method like unequal
> >>> variance T-test (Welch T) in lme when each group has unequal variance
> >>> in my data?
> >>>
> >>> Thanks,
> >>> Shirley
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >> --
> >> Simon Blomberg, BSc (Hons), PhD, MAppStat.
> >> Lecturer and Consultant Statistician
> >> Faculty of Biological and Chemical Sciences
> >> The University of Queensland
> >> St. Lucia Queensland 4072
> >> Australia
> >>
> >> Room 320, Goddard Building (8)
> >> T: +61 7 3365 2506
> >> email: S.Blomberg1_at_uq.edu.au
> >>
> >> The combination of some data and an aching desire for
> >> an answer does not ensure that a reasonable answer can
> >> be extracted from a given body of data. - John Tukey.
> >>
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ronggui.huang at gmail.com  Sun Jul  1 20:03:11 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Mon, 2 Jul 2007 02:03:11 +0800
Subject: [R] How to save results from chisq.test or mantelhaen.test to
	file
In-Reply-To: <4687797F.60103@optonline.net>
References: <e244c0cf20453.4686f30d@optonline.net>
	<4687797F.60103@optonline.net>
Message-ID: <38b9f0350707011103n56c2fc48le7079cb70aaba190@mail.gmail.com>

Maybe _dput_ is another way, and you can use _dget _ to get it back.

2007/7/1, Chuck Cleland <ccleland at optonline.net>:
> adschai at optonline.net wrote:
> > Hi,
> >
> > I am new to these functions. I'm wondering if there is anyway to save the entire results (all attributes of the result object) from the chisq.test or mantelhaen.test functions? For example, from chisq.test function, you will have statistic, parameter, p.value, expected, etc. in the result list. How can I save all of them in one shot to, says, a text file or csv file? Thank you.
> >
> > - adschai
>
>   You could unlist() the result, coerce it to a data frame, then use
> write.table().  For example, something like this:
>
> write.table(as.data.frame(t(unlist(chisq.test(InsectSprays$count > 7,
> InsectSprays$spray)))), quote=FALSE)
>
> or
>
> write.table(as.data.frame(unlist(chisq.test(InsectSprays$count > 7,
> InsectSprays$spray))), quote=FALSE)
>
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China


From fredrik.bg.lundgren at bredband.net  Sun Jul  1 21:11:26 2007
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Sun, 1 Jul 2007 21:11:26 +0200
Subject: [R]  package with roc, sensitivity, specificity, kappa etc
Message-ID: <000701c7bc13$9fd69830$b4eae455@Larissa>

Dear Guru's,

Is there a package (R of course) with programs for diagnostics - roc, 
sens , spec, kappa etc?

Best wishes Fredrik L


From tobias.verbeke at businessdecision.com  Sun Jul  1 22:22:34 2007
From: tobias.verbeke at businessdecision.com (Tobias Verbeke)
Date: Sun, 01 Jul 2007 22:22:34 +0200
Subject: [R] package with roc, sensitivity, specificity, kappa etc
In-Reply-To: <000701c7bc13$9fd69830$b4eae455@Larissa>
References: <000701c7bc13$9fd69830$b4eae455@Larissa>
Message-ID: <46880D0A.4070403@businessdecision.com>

Fredrik Lundgren wrote:
> Dear Guru's,
> 
> Is there a package (R of course) with programs for diagnostics - roc, 
> sens , spec, kappa etc?

Your question is not very specific, but
you might have a look at the ROCR package
for visualizing classifier performance.

http://cran.r-project.org/src/contrib/Descriptions/ROCR.html

HTH,
Tobias

-- 

Tobias Verbeke - Consultant
Business & Decision Benelux
Rue de la r?volution 8
1000 Brussels - BELGIUM

+32 499 36 33 15
tobias.verbeke at businessdecision.com


From liuwensui at gmail.com  Sun Jul  1 22:49:45 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 1 Jul 2007 16:49:45 -0400
Subject: [R] package with roc, sensitivity, specificity, kappa etc
In-Reply-To: <000701c7bc13$9fd69830$b4eae455@Larissa>
References: <000701c7bc13$9fd69830$b4eae455@Larissa>
Message-ID: <1115a2b00707011349s599ac89r9d31d3efcd8fac1b@mail.gmail.com>

for ROC and AUC calculation, you might try verification package.

On 7/1/07, Fredrik Lundgren <fredrik.bg.lundgren at bredband.net> wrote:
> Dear Guru's,
>
> Is there a package (R of course) with programs for diagnostics - roc,
> sens , spec, kappa etc?
>
> Best wishes Fredrik L
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
WenSui Liu
A lousy statistician who happens to know a little programming
(http://spaces.msn.com/statcompute/blog)


From spencer.graves at pdf.com  Sun Jul  1 23:30:05 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 01 Jul 2007 14:30:05 -0700
Subject: [R] unequal variance assumption for lme (mixed effect model)
In-Reply-To: <6fb73d020707011049q32ca88fv599a1dd9cf06735b@mail.gmail.com>
References: <6fb73d020706271855seedb40dobbc0fe2578de94ea@mail.gmail.com>	
	<1182998669.4837.11.camel@sib-sblomber01d.sib.uq.edu.au>	
	<6fb73d020706272014g18d3abb9ob70ab559883b6fb3@mail.gmail.com>	
	<46852C31.80905@pdf.com>
	<6fb73d020707011049q32ca88fv599a1dd9cf06735b@mail.gmail.com>
Message-ID: <46881CDD.9030605@pdf.com>

      The 'weights' argument on 'lm' is assumed to identify a vector of 
the same length as the response, giving numbers that are inversely 
proportional to the variance for each observation. 

      However, 'lm' provides no capability to estimate weights.  If you 
want to do that, the varFunc capabilities in the 'nlme' package is the 
best tool I know for that purpose. 

      If someone thinks there are better tools available for estimating 
heterscedasticity, I hope s/he will enlighten us both. 

      Hope this helps.
      Spencer Graves   

shirley zhang wrote:
> Thanks for Spencer and Simon's help.  I've got very interesting
> results based on your suggestions.
>
> One more question,  how to handle unequal variance problme in lm()?
> Isn't the weights option also, which means weighted least squares,
> right?  Can you give me an example of setting this parameter in lm()
> to account for  different variance assumption in each group?
>
> Thanks again,
> Shirley
>
>
> On 6/29/07, Spencer Graves <spencer.graves at pdf.com> wrote:
>> <comments in line>
>>
>> shirley zhang wrote:
>> > Hi Simon,
>> >
>> > Thanks for your reply. Your reply reminds me that book. I've read it
>> > long time ago, but haven't  try the weights option in my projects
>> > yet:)
>> >
>> > Is the heteroscedastic test always less powerful because we have to
>> > estimate the within group variance from the given data?
>> >
>> SG:  In general, I suspect we generally lose power when we estimate more
>> parameters.
>>
>> SG:  You can check this using the 'simulate.lme' function, whose use is
>> illustrated in the seminal work reported in sect. 2.4 of Pinheiro and
>> Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).
>> > Should we check whether each group has equal variance before using
>> > weights=varIdent()? If we should, what is the function for linear
>> > mixed model?
>> >
>> SG:  The general advice I've seen is to avoid excessive
>> overparameterization of heterscedasticity and correlations.  However,
>> parsimonious correlation had heterscedasticity models would likely be
>> wise.  Years ago, George Box expressed concern about people worrying too
>> much about outliers, which are often fairly obvious and relatively easy
>> to detect, while they worried too little, he thought, about dependence,
>> especially serial dependence, which is generally more difficult to
>> detect and creates bigger problems in inference than outliers.  He
>> wrote, "Why worry about mice when there are tigers about?"
>>
>> SG:  Issues of this type can be fairly easily evaluated using
>> 'simulate.lme'.
>>
>>      Hope this helps.
>>      Spencer Graves
>> > Thanks,
>> > Shirley
>> >
>> > On 6/27/07, Simon Blomberg <s.blomberg1 at uq.edu.au> wrote:
>> >
>> >> The default settings for lme do assume equal variances within groups.
>> >> You can change that by using the various varClasses. see 
>> ?varClasses. A
>> >> simple example would be to allow unequal variances across groups. 
>> So if
>> >> your call to lme was:
>> >>
>> >> lme(...,random=~1|group,...)
>> >>
>> >> then to allow each group to have its own variance, use:
>> >>
>> >> lme(...,random=~1|group, weights=varIdent(form=~1|group),...)
>> >>
>> >> You really really should read Pinheiro & Bates (2000). It's all 
>> there.
>> >>
>> >> HTH,
>> >>
>> >> Simon.
>> >>
>> >> , On Wed, 2007-06-27 at 21:55 -0400, shirley zhang wrote:
>> >>
>> >>> Dear Douglas and R-help,
>> >>>
>> >>> Does lme assume normal distribution AND equal variance among groups
>> >>> like anova() does? If it does, is there any method like unequal
>> >>> variance T-test (Welch T) in lme when each group has unequal 
>> variance
>> >>> in my data?
>> >>>
>> >>> Thanks,
>> >>> Shirley
>> >>>
>> >>> ______________________________________________
>> >>> R-help at stat.math.ethz.ch mailing list
>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> >>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >> --
>> >> Simon Blomberg, BSc (Hons), PhD, MAppStat.
>> >> Lecturer and Consultant Statistician
>> >> Faculty of Biological and Chemical Sciences
>> >> The University of Queensland
>> >> St. Lucia Queensland 4072
>> >> Australia
>> >>
>> >> Room 320, Goddard Building (8)
>> >> T: +61 7 3365 2506
>> >> email: S.Blomberg1_at_uq.edu.au
>> >>
>> >> The combination of some data and an aching desire for
>> >> an answer does not ensure that a reasonable answer can
>> >> be extracted from a given body of data. - John Tukey.
>> >>
>> >>
>> >>
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>


From lzhtom at hotmail.com  Mon Jul  2 03:47:57 2007
From: lzhtom at hotmail.com (zhihua li)
Date: Mon, 02 Jul 2007 01:47:57 +0000
Subject: [R] working with R graphics remotely
Message-ID: <BAY110-F320CE4503598BABC0AFC93C70D0@phx.gbl>

Hi netters,

Now I'm connecting from my local windows machine to a remote linux machine 
and launch R out there using SSH. When I tried to create grahics, like 
using plot or heatmap, I cannot see the output. Maybe a new R window 
displaying the graphics has popped out in the remote machine? Or I need to 
change some settings for the graphics to display? I don't know. I googled 
it and tried dev.copy but it didn't work. Can anyone help me here? I need 
to be able to see the output graphics and save it to a file (like jpeg)

Thanks a lot!

_________________________________________________________________
?????????????????????????????? MSN Hotmail??  http://www.hotmail.com


From james.milks at wright.edu  Mon Jul  2 05:15:46 2007
From: james.milks at wright.edu (James R. Milks)
Date: Sun, 01 Jul 2007 23:15:46 -0400
Subject: [R] Extracting sums for individual factors in data frames
Message-ID: <CC576A43-BCFC-4656-992A-4DDDCED16CFA@wright.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070701/c3428494/attachment.pl 

From r.turner at auckland.ac.nz  Mon Jul  2 05:30:27 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 2 Jul 2007 15:30:27 +1200
Subject: [R] Extracting sums for individual factors in data frames
In-Reply-To: <CC576A43-BCFC-4656-992A-4DDDCED16CFA@wright.edu>
References: <CC576A43-BCFC-4656-992A-4DDDCED16CFA@wright.edu>
Message-ID: <2CC7007B-6508-4A28-934D-997A5D206422@auckland.ac.nz>


?tapply

######################################################################
Attention:\ This e-mail message is privileged and confidenti...{{dropped}}


From jholtman at gmail.com  Mon Jul  2 05:34:29 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 1 Jul 2007 23:34:29 -0400
Subject: [R] Extracting sums for individual factors in data frames
In-Reply-To: <CC576A43-BCFC-4656-992A-4DDDCED16CFA@wright.edu>
References: <CC576A43-BCFC-4656-992A-4DDDCED16CFA@wright.edu>
Message-ID: <644e1f320707012034v67512fb0of49a573a96e1064e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070701/6aa98d34/attachment.pl 

From ggrothendieck at gmail.com  Mon Jul  2 06:20:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 Jul 2007 00:20:02 -0400
Subject: [R] align() function missing in R ?
In-Reply-To: <200706291302.l5TD2wc9015482@hypatia.math.ethz.ch>
References: <200706281511.l5SFBlat007218@hypatia.math.ethz.ch>
	<18052.43963.748646.703108@stat.math.ethz.ch>
	<Pine.LNX.4.64.0706290840510.1099@gannet.stats.ox.ac.uk>
	<200706291302.l5TD2wc9015482@hypatia.math.ethz.ch>
Message-ID: <971536df0707012120j7862cb2cnb70e8c6b8f2c676@mail.gmail.com>

On 6/29/07, Markus Loecher <markus at insightfromdata.com> wrote:
> Thank you for your responses, I should have given an example of the
> functionality I am looking for, here are three typical scenarios that
> I deal with a lot in my work:
>
> - a regular timeseries with lots of missing values that I want to
> convert to the corresponding regular time series with mssing values
> replaced by NAs, e.g.:
>         x = timeSeries(c(0.5,0.2,0.3,0.4,0.3,0.2,0.3), pos =
> c(1,2,5,8,9,12,14));
>         x.align = align(x, pos = 1:14, method = "NA");
> - a regular timeseries at a coarse scale which I want to linearly
> interpolate to a finer time scale:
>         x = ts(1:10, frequency = 4);
>         x.align = align(x, frequency = 8, method = "interp")
> - an irregular timeseries which I want to linearly interpolate to a
> regular time grid:
>         x = timeSeries(c(0.5,0.2,0.3,0.4,0.3,0.2,0.3), pos =
> c(1,2.5,3.2,4.1,5.7,6.5,7.3));
>         x.align = align(x, pos = 1:7, method = "interp");
>
> I am wondering how to easily code such a function using only window,
> ts.union and ts.intersect.

Here it is using zoo series:


library(zoo)
x <- c(0.5, 0.2, 0.3, 0.4, 0.3, 0.2, 0.3)

x1 <- zoo(x, c(1, 2, 5, 8, 9, 12, 14))
as.zoo(as.ts(x1))

x2 <- zooreg(1:10, frequency = 4)
frequency(x2) <- 8
x2

x3 <- zoo(x,  c(1, 2.5, 3.2, 4.1, 5.7, 6.5, 7.3))
tt <- 1:7
zoo(approx(time(x3), x3, tt)$y, tt)
# or
tt <- as.numeric(1:7) # can omit if warning in next line ok
window(na.approx(cbind(x3, zoo(, tt))), tt)

For more on zoo:

library(zoo)
vignette("zoo")


From s.blomberg1 at uq.edu.au  Mon Jul  2 06:24:08 2007
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Mon, 02 Jul 2007 14:24:08 +1000
Subject: [R] Extracting sums for individual factors in data frames
In-Reply-To: <CC576A43-BCFC-4656-992A-4DDDCED16CFA@wright.edu>
References: <CC576A43-BCFC-4656-992A-4DDDCED16CFA@wright.edu>
Message-ID: <1183350248.4799.57.camel@sib-sblomber01d.sib.uq.edu.au>

Does this do what you want?

> with(dat, tapply(BA, Species, sum))
      ACSA       AEGL       Dead       FRAM       VIPR 
565.172518  11.780972 157.393792 122.993352   3.926991 

Cheers,

Simon.

On Sun, 2007-07-01 at 23:15 -0400, James R. Milks wrote:
> I have a data frame with two columns, one of which is a factor  
> (Species) and the other is numeric (BA, which stands for basal  
> area).  Here's a sample:
> 
> 
> Species	BA
> ACSA	55.7632696
> FRAM	122.9933524
> ACSA	67.54424205
> ACSA	89.22123136
> ACSA	82.46680716
> ACSA	22.46238747
> ACSA	19.94911335
> ACSA	20.42035225
> ACSA	19.00663555
> ACSA	21.67698931
> ACSA	57.80530483
> ACSA	30.31636911
> Dead	43.98229715
> Dead	40.21238597
> Dead	16.49336143
> Dead	40.21238597
> Dead	16.49336143
> ACSA	78.53981634
> VIPR	3.926990817
> AEGL	11.78097245
> AEGL	0
> AEGL	0
> ACSA	0
> ACSA	0
> ACSA	0
> VIPR	0
> 
> I would like to calculate relative basal area for each species in  
> this plot.  For that, I need to divide the total basal area per  
> species by the total basal area in the plot.  Getting the total basal  
> area in the plot is easy.  However, I'm mystified on how to get the  
> total basal area per species.  Is there a way to extract and/or sum  
> the total basal area per species?
> 
> Thank you in advance.
> 
> Jim Milks
> 
> Graduate Student
> Environmental Sciences Ph.D. Program
> Wright State University
> 3640 Colonel Glenn Hwy
> Dayton, OH 45435
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia

Room 320, Goddard Building (8)
T: +61 7 3365 2506 
email: S.Blomberg1_at_uq.edu.au 

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.


From adschai at optonline.net  Mon Jul  2 07:19:46 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Mon, 02 Jul 2007 05:19:46 +0000 (GMT)
Subject: [R] How to set constraints on output layer of Neural Networks
Message-ID: <e7da923b215f9.46888af2@optonline.net>

Hi,

Please bear with me as I never use NN in R before. I have a network whose my output has, says K node. I would like to put a set of constraints on this layer. Indeed, I have two type of constraints. The first type is that their outputs should sum up to one. The second type is monotonic increasing from the first output node to the K-th node. How can I achieve this? Thank you so much in advance.

- adschai


From ripley at stats.ox.ac.uk  Mon Jul  2 07:55:50 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jul 2007 06:55:50 +0100 (BST)
Subject: [R] working with R graphics remotely
In-Reply-To: <BAY110-F320CE4503598BABC0AFC93C70D0@phx.gbl>
References: <BAY110-F320CE4503598BABC0AFC93C70D0@phx.gbl>
Message-ID: <Pine.LNX.4.64.0707020652040.27840@gannet.stats.ox.ac.uk>

You need to forward the X11 window to your local machine, which would need 
to be running an X server.  We do this using Exceed and PUTTY settings, 
but your sysadmins will be able to help you: it is not a question about R 
per se.

On Mon, 2 Jul 2007, zhihua li wrote:

> Hi netters,
>
> Now I'm connecting from my local windows machine to a remote linux machine 
> and launch R out there using SSH. When I tried to create grahics, like using 
> plot or heatmap, I cannot see the output. Maybe a new R window displaying the 
> graphics has popped out in the remote machine? Or I need to change some 
> settings for the graphics to display? I don't know. I googled it and tried 
> dev.copy but it didn't work. Can anyone help me here? I need to be able to 
> see the output graphics and save it to a file (like jpeg)
>
> Thanks a lot!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jari.haukka at ktl.fi  Mon Jul  2 09:11:53 2007
From: jari.haukka at ktl.fi (Jari Haukka)
Date: Mon, 02 Jul 2007 10:11:53 +0300
Subject: [R] G-estimation in R?
Message-ID: <4688A539.8090804@ktl.fi>

Hello,

Have anybody made tools for g-estimation (introduced by J. Robins)  in R.

http://www.ncbi.nlm.nih.gov/sites/entrez?Db=pubmed&Cmd=ShowDetailView&TermToSearch=8235180&ordinalpos=3&itool=EntrezSystem2.PEntrez.Pubmed.Pubmed_ResultsPanel.Pubmed_RVDocSum


Regards, Jari Haukka


From Bill.Venables at csiro.au  Mon Jul  2 09:23:27 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Mon, 2 Jul 2007 17:23:27 +1000
Subject: [R] termplot with uniform y-limits
Message-ID: <B998A44C8986644EA8029CFE6396A924B67FB1@exqld2-bne.nexus.csiro.au>

Does anyone have, or has anyone ever considered making, a version of
'termplot' that allows the user to specify that all plots should have
the same y-limits?

This seems a natural thing to ask for, as the plots share a y-scale.  If
you don't have the same y-axes you can easily misread the comparative
contributions of the different components. 

Notes: the current version of termplot does not allow the user to
specify ylim.  I checked.

	  the plot tools that come with mgcv do this by default.  Thanks
Simon.


Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):  +61 7 3826 7304
Mobile:                         +61 4 8819 4402
Home Phone:                     +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/


From joris.dewolf at cropdesign.com  Mon Jul  2 09:28:26 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Mon, 2 Jul 2007 09:28:26 +0200
Subject: [R] unequal variance assumption for lme (mixed effect model)
In-Reply-To: <46881CDD.9030605@pdf.com>
Message-ID: <OFBDCAFC77.847C6978-ONC125730C.0028F5B6-C125730C.0028F87D@basf-c-s.be>




gls() from the package nlme is similar to lm but is meant for models
without random effects.

Joris





                                                                           
             Spencer Graves                                                
             <spencer.graves at p                                             
             df.com>                                                    To 
             Sent by:                  shirley zhang                       
             r-help-bounces at st         <shirley0818 at gmail.com>             
             at.math.ethz.ch                                            cc 
                                       R-help at stat.math.ethz.ch            
                                                                   Subject 
             01/07/2007 23:30          Re: [R] unequal variance assumption 
                                       for lme (mixed effect model)        
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




      The 'weights' argument on 'lm' is assumed to identify a vector of
the same length as the response, giving numbers that are inversely
proportional to the variance for each observation.

      However, 'lm' provides no capability to estimate weights.  If you
want to do that, the varFunc capabilities in the 'nlme' package is the
best tool I know for that purpose.

      If someone thinks there are better tools available for estimating
heterscedasticity, I hope s/he will enlighten us both.

      Hope this helps.
      Spencer Graves

shirley zhang wrote:
> Thanks for Spencer and Simon's help.  I've got very interesting
> results based on your suggestions.
>
> One more question,  how to handle unequal variance problme in lm()?
> Isn't the weights option also, which means weighted least squares,
> right?  Can you give me an example of setting this parameter in lm()
> to account for  different variance assumption in each group?
>
> Thanks again,
> Shirley
>
>
> On 6/29/07, Spencer Graves <spencer.graves at pdf.com> wrote:
>> <comments in line>
>>
>> shirley zhang wrote:
>> > Hi Simon,
>> >
>> > Thanks for your reply. Your reply reminds me that book. I've read it
>> > long time ago, but haven't  try the weights option in my projects
>> > yet:)
>> >
>> > Is the heteroscedastic test always less powerful because we have to
>> > estimate the within group variance from the given data?
>> >
>> SG:  In general, I suspect we generally lose power when we estimate more
>> parameters.
>>
>> SG:  You can check this using the 'simulate.lme' function, whose use is
>> illustrated in the seminal work reported in sect. 2.4 of Pinheiro and
>> Bates (2000) Mixed-Effects Models in S and S-Plus (Springer).
>> > Should we check whether each group has equal variance before using
>> > weights=varIdent()? If we should, what is the function for linear
>> > mixed model?
>> >
>> SG:  The general advice I've seen is to avoid excessive
>> overparameterization of heterscedasticity and correlations.  However,
>> parsimonious correlation had heterscedasticity models would likely be
>> wise.  Years ago, George Box expressed concern about people worrying too
>> much about outliers, which are often fairly obvious and relatively easy
>> to detect, while they worried too little, he thought, about dependence,
>> especially serial dependence, which is generally more difficult to
>> detect and creates bigger problems in inference than outliers.  He
>> wrote, "Why worry about mice when there are tigers about?"
>>
>> SG:  Issues of this type can be fairly easily evaluated using
>> 'simulate.lme'.
>>
>>      Hope this helps.
>>      Spencer Graves
>> > Thanks,
>> > Shirley
>> >
>> > On 6/27/07, Simon Blomberg <s.blomberg1 at uq.edu.au> wrote:
>> >
>> >> The default settings for lme do assume equal variances within groups.
>> >> You can change that by using the various varClasses. see
>> ?varClasses. A
>> >> simple example would be to allow unequal variances across groups.
>> So if
>> >> your call to lme was:
>> >>
>> >> lme(...,random=~1|group,...)
>> >>
>> >> then to allow each group to have its own variance, use:
>> >>
>> >> lme(...,random=~1|group, weights=varIdent(form=~1|group),...)
>> >>
>> >> You really really should read Pinheiro & Bates (2000). It's all
>> there.
>> >>
>> >> HTH,
>> >>
>> >> Simon.
>> >>
>> >> , On Wed, 2007-06-27 at 21:55 -0400, shirley zhang wrote:
>> >>
>> >>> Dear Douglas and R-help,
>> >>>
>> >>> Does lme assume normal distribution AND equal variance among groups
>> >>> like anova() does? If it does, is there any method like unequal
>> >>> variance T-test (Welch T) in lme when each group has unequal
>> variance
>> >>> in my data?
>> >>>
>> >>> Thanks,
>> >>> Shirley
>> >>>
>> >>> ______________________________________________
>> >>> R-help at stat.math.ethz.ch mailing list
>> >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >>> and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >> --
>> >> Simon Blomberg, BSc (Hons), PhD, MAppStat.
>> >> Lecturer and Consultant Statistician
>> >> Faculty of Biological and Chemical Sciences
>> >> The University of Queensland
>> >> St. Lucia Queensland 4072
>> >> Australia
>> >>
>> >> Room 320, Goddard Building (8)
>> >> T: +61 7 3365 2506
>> >> email: S.Blomberg1_at_uq.edu.au
>> >>
>> >> The combination of some data and an aching desire for
>> >> an answer does not ensure that a reasonable answer can
>> >> be extracted from a given body of data. - John Tukey.
>> >>
>> >>
>> >>
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From d.vanderelst at tue.nl  Mon Jul  2 11:08:46 2007
From: d.vanderelst at tue.nl (Dieter Vanderelst)
Date: Mon, 02 Jul 2007 11:08:46 +0200
Subject: [R] Combine graphical and textual output
Message-ID: <4688C09E.7080400@tue.nl>

Hi,

I would like to know whether anybody knows a simple way to combine 
textual and graphical output in R.

A typical analysis produces textual output (e.g. model fits) and plots 
in R. I would like to know whether R has the possibility of combining 
these into a single 'report' or output. An example of a program that 
does this is SPSS. After running the analysis you have a combination of 
textual output (tables) and plots that are easy to print and distribute.

I know of the possibility to embed R code into LATEX (using Sweave), but 
I wouldn't call this quick since a lot of coding will go into writing 
the Sweave file.

Any other suggestions?

Regards,
Dieter


From petr.pikal at precheza.cz  Mon Jul  2 11:23:05 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Mon, 2 Jul 2007 11:23:05 +0200
Subject: [R] Odp:  Combine graphical and textual output
In-Reply-To: <4688C09E.7080400@tue.nl>
Message-ID: <OF5A811232.3734B156-ONC125730C.00334605-C125730C.00338DA0@precheza.cz>

Hi

try to look at R2HTML package, it can produce report like output.

Petr
petr.pikal at precheza.cz

r-help-bounces at stat.math.ethz.ch napsal dne 02.07.2007 11:08:46:

> Hi,
> 
> I would like to know whether anybody knows a simple way to combine 
> textual and graphical output in R.
> 
> A typical analysis produces textual output (e.g. model fits) and plots 
> in R. I would like to know whether R has the possibility of combining 
> these into a single 'report' or output. An example of a program that 
> does this is SPSS. After running the analysis you have a combination of 
> textual output (tables) and plots that are easy to print and distribute.
> 
> I know of the possibility to embed R code into LATEX (using Sweave), but 

> I wouldn't call this quick since a lot of coding will go into writing 
> the Sweave file.
> 
> Any other suggestions?
> 
> Regards,
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Bernhard_Pfaff at fra.invesco.com  Mon Jul  2 11:33:41 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Mon, 2 Jul 2007 10:33:41 +0100
Subject: [R] Combine graphical and textual output
In-Reply-To: <4688C09E.7080400@tue.nl>
References: <4688C09E.7080400@tue.nl>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C3097BB87C@GBHENXMB02.corp.amvescap.net>

Hello Dieter,

aside of Petr's suggestion, you might want to look at the 'relax'
package. Here, you can produce html as well as tex for on-the-fly
reports. The reports are set up within a tcl/tk window.

Best,
Bernhard 

>
>Hi,
>
>I would like to know whether anybody knows a simple way to combine 
>textual and graphical output in R.
>
>A typical analysis produces textual output (e.g. model fits) and plots 
>in R. I would like to know whether R has the possibility of combining 
>these into a single 'report' or output. An example of a program that 
>does this is SPSS. After running the analysis you have a 
>combination of 
>textual output (tables) and plots that are easy to print and 
>distribute.
>
>I know of the possibility to embed R code into LATEX (using 
>Sweave), but 
>I wouldn't call this quick since a lot of coding will go into writing 
>the Sweave file.
>
>Any other suggestions?
>
>Regards,
>Dieter
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From NKrichene at isdb.org  Sun Jul  1 10:50:51 2007
From: NKrichene at isdb.org (NKrichene at isdb.org)
Date: Sun, 1 Jul 2007 11:50:51 +0300
Subject: [R]  Package submission---HyperbolicDist
Message-ID: <OFE710415F.C93FA695-ON4325730B.0030759A-4325730B.003099F4@isdb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070701/2873a4ab/attachment.pl 

From Jan.Wijffels at ucs.kuleuven.be  Mon Jul  2 12:25:27 2007
From: Jan.Wijffels at ucs.kuleuven.be (Jan Wijffels)
Date: Mon, 2 Jul 2007 12:25:27 +0200
Subject: [R] working with R graphics remotely
In-Reply-To: <Pine.LNX.4.64.0707020652040.27840@gannet.stats.ox.ac.uk>
Message-ID: <002601c7bc93$4ebdbb60$2c70210a@UCSPC32>

?pdf
?jpeg
and load the files back to your windows machine unless you need to work
interactively through X11

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: maandag 2 juli 2007 7:56
To: zhihua li
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] working with R graphics remotely

You need to forward the X11 window to your local machine, which would
need 
to be running an X server.  We do this using Exceed and PUTTY settings, 
but your sysadmins will be able to help you: it is not a question about
R 
per se.

On Mon, 2 Jul 2007, zhihua li wrote:

> Hi netters,
>
> Now I'm connecting from my local windows machine to a remote linux
machine 
> and launch R out there using SSH. When I tried to create grahics, like
using 
> plot or heatmap, I cannot see the output. Maybe a new R window
displaying the 
> graphics has popped out in the remote machine? Or I need to change
some 
> settings for the graphics to display? I don't know. I googled it and
tried 
> dev.copy but it didn't work. Can anyone help me here? I need to be
able to 
> see the output graphics and save it to a file (like jpeg)
>
> Thanks a lot!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595




Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ebfernandes at fc.ul.pt  Mon Jul  2 12:53:01 2007
From: ebfernandes at fc.ul.pt (ElisabeteFernandes)
Date: Mon, 2 Jul 2007 11:53:01 +0100
Subject: [R] Interface R and C - windows
References: <ED0CEB28C60A4B458DF365134CEB4D000BE53E@fc-mailserver01.ul.pt>
	<554C46A9-4C8B-4C62-ABEE-F46D0566F8BD@jhsph.edu>
	<ED0CEB28C60A4B458DF365134CEB4D001C9B5A@fc-mailserver01.ul.pt>
	<C709DFFD-F5AE-4B9A-9F85-B302A57A7327@jhsph.edu>
	<ED0CEB28C60A4B458DF365134CEB4D0004115794@fc-mailserver01.ul.pt>
	<9B19CF94-610C-426C-BF33-63ADE6E339C8@jhsph.edu>
	<ED0CEB28C60A4B458DF365134CEB4D00041157BD@fc-mailserver01.ul.pt>
	<C374B369-53A0-454C-AB1B-D5A725E67A3D@jhsph.edu>
Message-ID: <ED0CEB28C60A4B458DF365134CEB4D00041157D8@fc-mailserver01.ul.pt>

Hello,

I want use the interface between R and C.  I usually work in windows.

I installed the Rtools.exe and ActivePerl 5.8.8 in the path  C:\R\R-2.3.1 like it is in the Appendix F the windows tools (Manual).  However, I do not have sucess. How should I do this ? Which type of path should I use?
 
Other question is about how compiling C code using the Dev-C++ and librarys of packages R. For example, I want use the function dnorm and I include in compiler options the library Rmath.h and dnorm.c:
 
#include <stdio.h> 
#include <Rmath.h>
#include <dnorm.c>

        main()
        {
           int a, b, s;
 
           printf("Introduza um n?mero:\n");
           scanf("%d", &a);
           printf("Introduza outro n?mero:\n");
           scanf("%d", &b);
           s = a + b;
           s=dnorm(2,1,1,1);
           printf("A densidade de %d com %d d? %d\n", a, b, s);
           system("PAUSE");
        }

 
However, this code do not work. 


 thanks

the best regard

 Elisabete Fernandes


From florian.dubuisson at googlemail.com  Mon Jul  2 13:06:15 2007
From: florian.dubuisson at googlemail.com (Florian Dubuisson)
Date: Mon, 2 Jul 2007 12:06:15 +0100
Subject: [R] Problem with the function plot and multipage
Message-ID: <8fc9ea000707020406m5dc77a51j3a7163362b22864d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/4829137c/attachment.pl 

From jim at bitwrit.com.au  Mon Jul  2 13:13:03 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 02 Jul 2007 21:13:03 +1000
Subject: [R] Combine graphical and textual output
In-Reply-To: <4688C09E.7080400@tue.nl>
References: <4688C09E.7080400@tue.nl>
Message-ID: <4688DDBF.3070607@bitwrit.com.au>

Dieter Vanderelst wrote:
> Hi,
> 
> I would like to know whether anybody knows a simple way to combine 
> textual and graphical output in R.
> 
> A typical analysis produces textual output (e.g. model fits) and plots 
> in R. I would like to know whether R has the possibility of combining 
> these into a single 'report' or output. An example of a program that 
> does this is SPSS. After running the analysis you have a combination of 
> textual output (tables) and plots that are easy to print and distribute.
> 
> I know of the possibility to embed R code into LATEX (using Sweave), but 
> I wouldn't call this quick since a lot of coding will go into writing 
> the Sweave file.
> 
> Any other suggestions?
> 
And to further complicate the issue, you could look at the htmlize and 
R2html in the prettyR package, which allows you to run an R script and 
produce HTML output.

Jim


From ramasamy at cancer.org.uk  Mon Jul  2 13:17:12 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 02 Jul 2007 12:17:12 +0100
Subject: [R] speed and looping issues; calculations on big datasets
In-Reply-To: <937EA1A0-0513-4FCD-8036-68EB1503009A@upf.edu>
References: <937EA1A0-0513-4FCD-8036-68EB1503009A@upf.edu>
Message-ID: <4688DEB8.3040506@cancer.org.uk>

I don't fully understand what your objective here, but I would try a 
combination of cut and grep in a shell to see if it works. For example, 
if your data was saved as a tab-delimited file and you have some 
predefined patterns you seek, then try the untested code below

  cut -f3-6 | gsub 's/ //g' > tmp
  grep "^00" tmp | wc >> rightA
  grep "^001" tmp | wc >> rightB
  grep "^010|^0011" tmp | wc >> rightC

  cut -f1-3 | | gsub 's/ //g'
  grep "00$ | wc > leftA
  grep "000$|001$" | wc > leftB

Then you got to write a loop and generalise the codes. You can try this 
in bash, perl or rewrite it in C.

If you want more help, the provide more explanation on what the types of 
pattern you are looking for. You might want to try checking the 
BioConductor packages as well.

Regards, Adai



martin sikora wrote:
> dear r users,
> 
> i'm a little stuck with the following problem(s), hopefully somebody  
> can offer some help:
> 
> i have data organized in a binary matrix, which can become quite big  
> like 60 rows x 10^5 columns (they represent SNP genotypes, for some  
> background info). what i need to do is the following:
> 
> let's suppose i have a matrix of size n x m. for each of the m  
> columns, i want to know the counts of unique rows extended one by one  
> from the "core" column, for both values at the "core" separately and  
> in both directions. maybe better explained with a little example.
> 
> data:
> 
> 00 0 010
> 10 1 001
> 11 1 011
> 10 0 011
> 10 0 010
> 
> so the extended unique rows & counts taking e.g. column 3 as "core" are:
> 
> col 3 = 0:
> right:
> patterns / counts
> 00 / 3
> 001 / 3
> 010, 0011 / 2,1
> 
> left:
> 00 / 3
> 000,001 / 1,2
> 
> and that for the other subset ( col3 = 1) as well, then doing the  
> whole thing again for the next "core" column. the reason i need this  
> counts is that i want to calculate frequencies of the different  
> extended sequences to calculate the probability of drawing two  
> identical sequences from the core up to an extended position from the  
> whole set of sequences.
> 
> my main problem is speed of the calculations. i tried different ways  
> suggested here in the list of getting the counts of the unique rows,  
> all of them using the "table" function. both a combination of table 
> ( do.call( paste, c( as.data.frame( mymatrix) ) ) ) or table( apply 
> ( mymatrix , 2 , paste , collapse ="" ) ) work fine, but are too slow  
> for bigger matrices that i want to calculate (at least in my not very  
> sophisticated function). then i found a great suggestion here to do a  
> matrix multiplication with a vector of 2^(0:ncol-1) to convert each  
> row into a decimal number, and do table on those. this speeds up  
> things quite nicely, although the problem is that it of course does  
> not work as soon as i extended for more than 60 columns, because the  
> decimal numbers get to large to accurately distinguish between a 0  
> and 1 at the smallest digit:
> 
>  > 2^60+2 == 2^60
> [1] TRUE
> 
> another thing is that so far i could not come up with an idea on how  
> or if it is possible to do this without the loops i am using, one  
> large loop for each column in turn as core, and then another loop  
> within that extends the rows by growing column numbers. since i am  
> not the best of programmers (and still quite new to R), i was hoping  
> that somebody has some advice on doing this calculations in a more  
> elegant and more importantly, fast way.
> just to get the idea, the approach with the matrix multiplication  
> takes 20s for a 60 x 220 matrix on my macbook pro, which is obviously  
> not perfect, considering i would like to use this function for  
> matrices of size 10^2 x 10^5 or even more.
> 
> so i would be very thankful for any ideas, suggestions etc to improve  
> this
> 
> cheers
> martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From ripley at stats.ox.ac.uk  Mon Jul  2 13:18:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Jul 2007 12:18:44 +0100 (BST)
Subject: [R] working with R graphics remotely
In-Reply-To: <002601c7bc93$4ebdbb60$2c70210a@UCSPC32>
References: <002601c7bc93$4ebdbb60$2c70210a@UCSPC32>
Message-ID: <Pine.LNX.4.64.0707021216470.5294@gannet.stats.ox.ac.uk>

On Mon, 2 Jul 2007, Jan Wijffels wrote:

> ?pdf
> ?jpeg
> and load the files back to your windows machine unless you need to work
> interactively through X11

But they don't work under Linux unless you have an X server to access, and 
a remote Windows user is very unlikely to.

Please see the help page of the devices you recommended, which did warn 
about this.

>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: maandag 2 juli 2007 7:56
> To: zhihua li
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] working with R graphics remotely
>
> You need to forward the X11 window to your local machine, which would
> need
> to be running an X server.  We do this using Exceed and PUTTY settings,
> but your sysadmins will be able to help you: it is not a question about
> R
> per se.
>
> On Mon, 2 Jul 2007, zhihua li wrote:
>
>> Hi netters,
>>
>> Now I'm connecting from my local windows machine to a remote linux
> machine
>> and launch R out there using SSH. When I tried to create grahics, like
> using
>> plot or heatmap, I cannot see the output. Maybe a new R window
> displaying the
>> graphics has popped out in the remote machine? Or I need to change
> some
>> settings for the graphics to display? I don't know. I googled it and
> tried
>> dev.copy but it didn't work. Can anyone help me here? I need to be
> able to
>> see the output graphics and save it to a file (like jpeg)
>>
>> Thanks a lot!
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From heltertwo at care2.com  Mon Jul  2 14:38:22 2007
From: heltertwo at care2.com (Helter Two)
Date: Mon, 2 Jul 2007 14:38:22 +0200
Subject: [R] estimating a generalized autocorrelated model
Message-ID: <09901CEE35549DD45B24FA3FF3E7F153@heltertwo.care2.com>


Hi, I want to estimate a model of the following form:

y = a1W1y + X1beta1 + e
e = a2W2e + X2beta2 + nu
with y a vector of proportions or counts and W a matrix of weights.

In other words, autocorrelation occurs for the response variable y and for the disturbance term e. Ideally, at either level multiple W matrices could be included and X1 and X2 could be (partly) different (or X2 could be absent). But the most important thing is to be able to include a count/proportion response variable in the autocorrelation model. 

Does anyone know of a function in a package to estimate such a model? I have found some functions that capture small parts of this, but especially the option of a "generalized" model with a count reponse hasn't shown up yet. But perhaps it exists under names I am not familiar with.

thanks a lot,
Hank Stevens (PhD. student)




http://toolbar.Care2.com  Make your computer carbon-neutral (free).

http://www.Care2.com  Green Living, Human Rights and more - 7 million members!


From jfox at mcmaster.ca  Mon Jul  2 14:40:26 2007
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 02 Jul 2007 08:40:26 -0400
Subject: [R] termplot with uniform y-limits
Message-ID: <web-178017759@cgpsrv2.cis.mcmaster.ca>

Dear Bill,

Functions in the effects package are somewhat similar to termplot, and
allow you to specify the y-axis limits. For example, modify the last
line of the last example in example(all.effects) to 

plot(eff.pres, ask=FALSE, ylim=c(10, 70))

I hope this helps,
 John

------- original message -------

Does anyone have, or has anyone ever considered making, a version of
'termplot' that allows the user to specify that all plots should have
the same y-limits?

This seems a natural thing to ask for, as the plots share a y-scale.
 If
you don't have the same y-axes you can easily misread the comparative
contributions of the different components. 

Notes: the current version of termplot does not allow the user to
specify ylim.  I checked.

	  the plot tools that come with mgcv do this by default.  Thanks
Simon.

--------------------------------
John Fox, Professor
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From phhs80 at gmail.com  Mon Jul  2 14:51:16 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Mon, 2 Jul 2007 13:51:16 +0100
Subject: [R] Looking for easy way of getting the starting value for
	constrOptim
Message-ID: <6ade6f6c0707020551gf8e2bd9o5d3c3a0de70fcee5@mail.gmail.com>

Dear All,

I am using constrOptim, but facing a difficulty: how can I get the
starting value? Is there some automatic way of getting it? Since I
have many inequalities as constraints (all linear), it is quite
difficult to find a feasible value. I have tried to use rgenoud
solution as a feasible value, but rgenoud seems not being converging.

Any ideas?

Thanks in advance,

Paul


From chrisbowringgg at gmail.com  Mon Jul  2 15:10:05 2007
From: chrisbowringgg at gmail.com (Chris Bowring)
Date: Mon, 2 Jul 2007 14:10:05 +0100
Subject: [R] ARIMA prediction
Message-ID: <39da0ad40707020610m3ae3631an6e53e0494f583faa@mail.gmail.com>

Hi

This is my first post to this group, so apologies in advance if I get it wrong.

I would like to know how the prediction for arima models works in R. I
have a time series to which I fit an arima model, of varying AR and MA
orders. I then use the predict function to project it forward. I have
also written my own function to perform the prediction, but it gives
different answers to Arima.predict when the MA order is non-zero.


I use the residuals from the arima function in my custom prediction
function. I think this may be my problem. In the arima model:


x{t} = a(1)x{t-1} + a(2)x{t-2} + ... + a(p)x{t-p} + e{t} + b(1)e{t-1}
+ b(2)e{t-2} + ... + b(q)e{t-q}


I am treating the residuals (i.e. arima(....)$res)  as the e{t} terms.
This gives different answers both in the region of the simulation and
in the region of the prediction, so I'm guessing they're not what I
think they are. Indeed, after q intervals in the prediction, the
prediction proceeds as I would expect, presumably because all the
residuals that have an effect are zero by this stage.


Any help greatly appreciated - my code is below.


Thanks


Chris


--------------------------------------------------------------

The code to produce the two predictions is as follows:


AR <- 5
MA <- 3


sim <- arima.sim(list(order=c(AR,0,MA), ar=c(.1, .1, .1, .1, .1),
ma=c(.1, .1, .1)), n=100) + 50


fit <- arima(sim, order = c(AR, 0, MA))


coefs <- fit$coef
series <- sim
innov <- fit$res
pred <- 100


fit.predict <- predict(fit, n.ahead = pred)


fit.r <- c(sim, fit.predict$pred)


fit.custom <- ProjectCentralArima(AR = AR, MA = MA, d = 0, coefs =
coefs, series = sim, innov = innov, pred = pred)$ser


ProjectCentralArima function:


ProjectCentralArima <- function(AR, MA, d, coefs, series, innov, pred)
{


  if(d==0){
    series.diff <- series
  }
  else {
    series.diff <- diff(series, lag=1, differences=d)
  }


  intercept <- coefs[length(coefs)]


  for(i in 1:pred){
    temp <- intercept
    l.s <- length(series.diff)
    if(AR > 0){
      for(j in 1:AR){
        temp <- temp + coefs[j] * (series.diff[l.s - j + 1] -
intercept)
      }
    }
    if(MA > 0){
      for(j in (1:MA)){
        temp <- temp + coefs[j + AR] * innov[l.s - j + 1]
      }
    }


    innov <- c(innov, 0)
    series.diff <- c(series.diff, temp)
  }


  if(d==0){
     series.undiff <- series.diff
  }
  else {
    series.undiff <- diffinv(series.diff, lag=1, differences=d, xi =
series[1:d])
  }


  return(list(series = series.undiff, innov = innov))
}


From tlumley at u.washington.edu  Mon Jul  2 16:16:38 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 2 Jul 2007 07:16:38 -0700 (PDT)
Subject: [R] Determining whether a function's return value is assigned
In-Reply-To: <aba1c4400706300344m3a62826scf6752d6201eb0da@mail.gmail.com>
References: <aba1c4400706300344m3a62826scf6752d6201eb0da@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707020716000.10864@homer23.u.washington.edu>

On Sat, 30 Jun 2007, Paul Laub wrote:

> Dear all,
>
> Does R offer a means by which a function can determine
> whether its return value is assigned? I am using R
> 2.4.1 for Windows.

No

> Suppose what I am looking for is called
> "return.value.assigned". Then one might use it like
> this
>
>    myfunction <- function () {
>        # Create bigobject here
>
>        if (return.value.assigned()) {
>            bigobject
>        } else {
>            summary(bigobject)
>        }
>    }

That's what the print() function is for. Write a print() method.

 	-thomas


From fasidfas at yahoo.com  Mon Jul  2 15:46:04 2007
From: fasidfas at yahoo.com (faisal afzal siddiqui)
Date: Mon, 2 Jul 2007 06:46:04 -0700 (PDT)
Subject: [R] Homals, PsychoR, Bayesm
Message-ID: <779104.71664.qm@web53303.mail.re2.yahoo.com>

hi,

I am working on conjoint analysis, how I can install
Homals, PsychoR, and Bayesm in my computer, also I
need some examples in R, would u please help me?

also advise if there are other programmes/modules in R
for conjoint analysis?

Regds 
Faisal Afzal Siddiqui


From tobischlot2002 at yahoo.com  Mon Jul  2 16:43:55 2007
From: tobischlot2002 at yahoo.com (Tobias Schlottmann)
Date: Mon, 2 Jul 2007 07:43:55 -0700 (PDT)
Subject: [R] QP for solving Support Vector Regression
Message-ID: <600820.32047.qm@web45013.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/6cd716e7/attachment.pl 

From statadat at gmail.com  Mon Jul  2 16:48:28 2007
From: statadat at gmail.com (domenico pestalozzi)
Date: Mon, 2 Jul 2007 16:48:28 +0200
Subject: [R] how to solve a min problem
Message-ID: <e591a95b0707020748p2650666bs972a7f28c8001a31@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/a267e33a/attachment.pl 

From ggrothendieck at gmail.com  Mon Jul  2 16:52:16 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 Jul 2007 10:52:16 -0400
Subject: [R] working with R graphics remotely
In-Reply-To: <BAY110-F320CE4503598BABC0AFC93C70D0@phx.gbl>
References: <BAY110-F320CE4503598BABC0AFC93C70D0@phx.gbl>
Message-ID: <971536df0707020752x70d07cd8y39456f2cd1b892b@mail.gmail.com>

On your Windows machine you need to run (1) an X server and (2) an ssh client.
Xming and putty, respectively, are among the free ones.

First start Xming (you can configure it using XLaunch, which is included with
Xming, or you can run Xming from the Windows command line specifying the
configuration parameters via command line flags).  Then run
putty.   Be sure you have X11 forwarding enabled in putty's
  Connection | SSH | Tunnel
screen.  Googling will locate much info for these programs.

On 7/1/07, zhihua li <lzhtom at hotmail.com> wrote:
> Hi netters,
>
> Now I'm connecting from my local windows machine to a remote linux machine
> and launch R out there using SSH. When I tried to create grahics, like
> using plot or heatmap, I cannot see the output. Maybe a new R window
> displaying the graphics has popped out in the remote machine? Or I need to
> change some settings for the graphics to display? I don't know. I googled
> it and tried dev.copy but it didn't work. Can anyone help me here? I need
> to be able to see the output graphics and save it to a file (like jpeg)
>
> Thanks a lot!
>
> _________________________________________________________________
> ?????????????????????????????? MSN Hotmail??  http://www.hotmail.com
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From tobischlot2002 at yahoo.com  Mon Jul  2 17:06:26 2007
From: tobischlot2002 at yahoo.com (Tobias Schlottmann)
Date: Mon, 2 Jul 2007 08:06:26 -0700 (PDT)
Subject: [R] Implementing Support Vector Regression by ipop in kernlab
Message-ID: <689223.1490.qm@web45010.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/e567a7d7/attachment.pl 

From weller at erdw.ethz.ch  Mon Jul  2 17:17:12 2007
From: weller at erdw.ethz.ch (Andy Weller)
Date: Mon, 02 Jul 2007 17:17:12 +0200
Subject: [R] Salient feature selection
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A834401957411@NYWEXMB23.msad.ms.com>
References: <467B87A1.3090808@erdw.ethz.ch>
	<D3AEEDA31E57474B840BEBC25A8A834401957411@NYWEXMB23.msad.ms.com>
Message-ID: <468916F8.1080207@erdw.ethz.ch>

I am relatively new to R. I am hoping that someone will be able to point 
me in the right direction and/or suggest a technique/package/reference 
that will help me with the following. I have:

a) Some explanatory variables (integers, real) - these are "real world" 
physical descriptions, i.e. counts of features, etc

b) Some response variables (integers, real) - these are image analysis 
measurements (gray-value distributions, textural descriptors, etc) of 
the same things represented in a

and I want to find out which between the two correlate best - i.e. the 
salient features from BOTH sets (i.e. not for classification purposes).

For example, if a has 10 explanatory variables and b has 10 response 
variables, I want to test the complete set of explanatory variables with 
each individual response (or vice versa). So, explanatory 1-10 with 
response 1, explanatory 1-10 with response 2, explanatory 1-10 with 
response 3, etc...

This should ultimately tell me which "real world" physical features are 
related best with the image analysis measurements (with the confidence 
level between them).

I hope this makes sense?

I have used SPSS AnswerTree's "Exhaustive CHAID" before to select a 
subset of input features for a complete set of output features to aid 
the creation of artificial neural networks. I want to do a similar 
thing, but it is not important for ALL explanatory and response 
variables are used/selected.

I hope that I have been clear in my intentions and I look forward to 
your replies, Andy


From jamesrgraham at mac.com  Mon Jul  2 17:24:00 2007
From: jamesrgraham at mac.com (James R. Graham)
Date: Mon, 2 Jul 2007 11:24:00 -0400
Subject: [R] Question about PCA with prcomp
Message-ID: <D8F26047-45B9-4E1F-B813-DCD1F4901623@mac.com>

Hello All,

The basic premise of what I want to do is the following:

I have 20 "entities" for which I have ~500 measurements each. So, I  
have a matrix of 20 rows by ~500 columns.

The 20 entities fall into two classes: "good" and "bad."

I eventually would like to derive a model that would then be able to  
classify new entities as being in "good territory" or "bad territory"  
based upon my existing data set.

I know that not all ~500 measurements are meaningful, so I thought  
the best place to begin would be to do a PCA in order to reduce the  
amount of data with which I have to work.

I did this using the prcomp function and found that nearly 90% of the  
variance in the data is explained by PC1 and 2.

So far, so good.

I would now like to find out which of the original ~500 measurements  
contribute to PC1 and 2 and by how much.

Any tips would be greatly appreciated! And apologies in advance if  
this turns out to be an idiotic question.


james


From johannes_graumann at web.de  Mon Jul  2 17:29:25 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 02 Jul 2007 17:29:25 +0200
Subject: [R] "S3method" equivalent to "exportPattern"
Message-ID: <f6b5kl$pgb$1@sea.gmane.org>

Hi,

I'm just trying to figure this out ... for any function I want my package to
generally provide, I need to export it and define it as an "S3method" (?).
"exportPattern" is really convenient. Is there a regex based analogon
for "S3method"?

Thanks, Joh


From hao.liu at bms.com  Mon Jul  2 17:30:54 2007
From: hao.liu at bms.com (Hao Liu)
Date: Mon, 02 Jul 2007 11:30:54 -0400
Subject: [R] focus to tkwindow after a PDF window pop up
Message-ID: <46891A2E.6050709@bms.com>

Dear All:

I currently have a TK window start a acroread window: However, when the 
acroread window is open, I can't get back to the TK window unless I 
close the acroead.

I invoked the acroread window using: system(paste("acroread ",file, sep=""))

anything I can do to make them both available to users?

Thanks
Hao


From gunter.berton at gene.com  Mon Jul  2 17:38:02 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 2 Jul 2007 08:38:02 -0700
Subject: [R] Salient feature selection
In-Reply-To: <468916F8.1080207@erdw.ethz.ch>
Message-ID: <001f01c7bcbe$f9e82fe0$4d908980@gne.windows.gene.com>

Andy:

See e.g. the pls package. However, be forewarned: this is a vague problem
(what kind of predictors/responses do you want? -- linear combinations?
nonlinear combinations? ...). The problem is also NP-Hard I believe, so
solutions are very algorithm (and even starting value)-dependent. For these
reasons, statistical inference is difficult, at best, and probably not even
meaningful in your context, as I doubt that you have a random sample of
anything. A personal recommendation (with which many disagree, I know): seek
extreme parsimony in both predictors and responses for results to be
replicable/scientifically meaningful.


Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andy Weller
Sent: Monday, July 02, 2007 8:17 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Salient feature selection

I am relatively new to R. I am hoping that someone will be able to point 
me in the right direction and/or suggest a technique/package/reference 
that will help me with the following. I have:

a) Some explanatory variables (integers, real) - these are "real world" 
physical descriptions, i.e. counts of features, etc

b) Some response variables (integers, real) - these are image analysis 
measurements (gray-value distributions, textural descriptors, etc) of 
the same things represented in a

and I want to find out which between the two correlate best - i.e. the 
salient features from BOTH sets (i.e. not for classification purposes).

For example, if a has 10 explanatory variables and b has 10 response 
variables, I want to test the complete set of explanatory variables with 
each individual response (or vice versa). So, explanatory 1-10 with 
response 1, explanatory 1-10 with response 2, explanatory 1-10 with 
response 3, etc...

This should ultimately tell me which "real world" physical features are 
related best with the image analysis measurements (with the confidence 
level between them).

I hope this makes sense?

I have used SPSS AnswerTree's "Exhaustive CHAID" before to select a 
subset of input features for a complete set of output features to aid 
the creation of artificial neural networks. I want to do a similar 
thing, but it is not important for ALL explanatory and response 
variables are used/selected.

I hope that I have been clear in my intentions and I look forward to 
your replies, Andy

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From drobin at sandia.gov  Mon Jul  2 17:54:48 2007
From: drobin at sandia.gov (drobin)
Date: Mon, 02 Jul 2007 09:54:48 -0600
Subject: [R] CCDF plotting - axis labels+overlay
Message-ID: <C2AE7BE8.BC13%drobin@sandia.gov>

My apologies in advance for what will probably be a poor post, but I'm very
new to R and, obviously, the R-help group.

What I am trying to do is plot the complementary cumulative distribution
function (aka survivor function) on log-log axes.

The cdf function with the oneminus=TRUE option in the heR.Misc package
almost does what I want to do with two problems. First, labeling the y-axis
seems to be a problem - I cannot get it to change.

Second, I would like to overlay ccdf's on the same plot. I can get them on
the same plot with new=TRUE.  However, when I try to scale the quartz window
(I'm on a Mac), the two ccdf's do not scale the same and they no longer line
up.  

I've wandered through the archives, but have not hit on solutions to the
above nor alternatives to heR.Misc for plotting the ccdf. Any insight would
be appreciated.


Many thanks, 
Dave


From p.dalgaard at biostat.ku.dk  Mon Jul  2 17:59:52 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 02 Jul 2007 17:59:52 +0200
Subject: [R] focus to tkwindow after a PDF window pop up
In-Reply-To: <46891A2E.6050709@bms.com>
References: <46891A2E.6050709@bms.com>
Message-ID: <468920F8.3000804@biostat.ku.dk>

Hao Liu wrote:
> Dear All:
>
> I currently have a TK window start a acroread window: However, when the 
> acroread window is open, I can't get back to the TK window unless I 
> close the acroead.
>
> I invoked the acroread window using: system(paste("acroread ",file, sep=""))
>
> anything I can do to make them both available to users?
>   
Tell system() not to _wait_ for command to complete.


From hao.liu at bms.com  Mon Jul  2 18:02:36 2007
From: hao.liu at bms.com (Hao Liu)
Date: Mon, 02 Jul 2007 12:02:36 -0400
Subject: [R] focus to tkwindow after a PDF window pop up
In-Reply-To: <468920F8.3000804@biostat.ku.dk>
References: <46891A2E.6050709@bms.com> <468920F8.3000804@biostat.ku.dk>
Message-ID: <4689219C.5040307@bms.com>

Thanks...
I finally used BrowseURL(), which is how Rcmdr does this... it starts 
firefox and things are ok...

Thanks
Hao

Peter Dalgaard wrote:

> Hao Liu wrote:
>
>> Dear All:
>>
>> I currently have a TK window start a acroread window: However, when 
>> the acroread window is open, I can't get back to the TK window unless 
>> I close the acroead.
>>
>> I invoked the acroread window using: system(paste("acroread ",file, 
>> sep=""))
>>
>> anything I can do to make them both available to users?
>>   
>
> Tell system() not to _wait_ for command to complete.
>


From skip at pobox.com  Mon Jul  2 18:00:00 2007
From: skip at pobox.com (Skip Montanaro)
Date: Mon, 2 Jul 2007 16:00:00 +0000 (UTC)
Subject: [R] relocation error in grDevices.so
Message-ID: <loom.20070702T154902-879@post.gmane.org>

(Warning: I'm not an R guy.  I'm a Python guy trying to get the
R-Python interface working again after some upgrades.)

I'm trying to upgrade our numpy/rpy/matplotlib environment (Solaris
10/Intel, Python 2.4).  In the process I found I needed to rebuild R
(2.1.1) because it was compiled with gcc 3.3.2 and we have since
migrated to gcc 3.4.1.  I'm using this configure setup:

    export CPPFLAGS=-I/opt/app/include
    export LDFLAGS='-L/opt/app/lib -R/opt/app/lib'
    ./configure --prefix=/opt/app/R-2.1.1-gcc-3.4.1

After building and installing R I downloaded and installed the
contributed quadprog library.  I ran this simple solver example using
Python+rpy which works (well, except for some R error I haven't
figured out yet) in the old installation:

    import numpy
    import rpy
    rpy.r("library(quadprog)")

    Dmat = numpy.identity(3, numpy.float_)
    print Dmat
    rpy.r.assign("Dmat", Dmat)

    dvec = numpy.array([0,5,0], numpy.float_)
    print dvec
    rpy.r.assign("dvec", dvec)

    Amat = numpy.array([[-4,-3,0], [2,1,0], [0,-2,1]], numpy.float_)
    print Amat
    rpy.r.assign("Amat", Amat.transpose())

    bvec = numpy.array([-8,2,0], numpy.float_)
    print bvec
    rpy.r.assign("bvec", bvec)

    result = rpy.r("solve.QP(Dmat, dvec, Amat, bvec=bvec)")
    print result['solution']

and get this error:

    Error in dyn.load(x, as.logical(local), as.logical(now)) :
	    unable to load shared library
'/opt/app/R-2.1.1-gcc-3.4.1/lib/R/library/grDevices/libs/grDevices.so':
      ld.so.1: python: fatal: relocation error: file
/opt/app/R-2.1.1-gcc-3.4.1/lib/R/library/grDevices/libs/grDevices.so: symbol
R_NilValue: referenced symbol not found
    Loading required package: grDevices
    Error in dyn.load(x, as.logical(local), as.logical(now)) :
	    unable to load shared library
'/opt/app/R-2.1.1-gcc-3.4.1/lib/R/library/grDevices/libs/grDevices.so':
      ld.so.1: python: fatal: relocation error: file
/opt/app/R-2.1.1-gcc-3.4.1/lib/R/library/grDevices/libs/grDevices.so: symbol
R_NilValue: referenced symbol not found
    In addition: Warning message:
    package grDevices in options("defaultPackages") was not found
    Error: package 'grDevices' could not be loaded
    Traceback (most recent call last):
      File "rpytest.py", line 3, in ?
	rpy.r("library(quadprog)")
      File
"/opt/app/g++lib6/python-2.4-pre-release/lib/python2.4/site-packages/rpy.py",
line 299, in __call__
	return self.eval(self.parse(text=s))
    rpy.RException: Error: package/namespace load failed for 'quadprog'

Looking at grDevices.so I see that it wasn't even linked against the R
library:

    % ldd /opt/app/R-2.1.1-gcc-3.4.1/lib/R/library/grDevices/libs/grDevices.so
	    libc.so.1 =>     /lib/libc.so.1
	    libm.so.2 =>     /lib/libm.so.2

so I can't see how it could possibly resolve the R_NilValue symbol.

Can someone tell me where I've gone awry?

Thanks,

Skip Montanaro
skip at pobox.com


From cumarporn at gmail.com  Mon Jul  2 18:22:17 2007
From: cumarporn at gmail.com (umarporn charusombat)
Date: Mon, 2 Jul 2007 12:22:17 -0400
Subject: [R] help again
Message-ID: <904d37580707020922k10303719hbfb06a5a115ce375@mail.gmail.com>

hi
i try to use arima and holtwinter to predict drought from 1895-2006
but i cannot read whole period of time and i try to do the exponent fitting,
but it comes out as the coordinate x-y error
i send the source code and data to take a look
if anyone can help me, i am really new in R

thank u so much
jam

From cumarporn at gmail.com  Mon Jul  2 18:22:17 2007
From: cumarporn at gmail.com (umarporn charusombat)
Date: Mon, 2 Jul 2007 12:22:17 -0400
Subject: [R] help again
Message-ID: <904d37580707020922k10303719hbfb06a5a115ce375@mail.gmail.com>

hi
i try to use arima and holtwinter to predict drought from 1895-2006
but i cannot read whole period of time and i try to do the exponent fitting,
but it comes out as the coordinate x-y error
i send the source code and data to take a look
if anyone can help me, i am really new in R

thank u so much
jam

From Zava.Aydemir at morganstanley.com  Mon Jul  2 18:23:54 2007
From: Zava.Aydemir at morganstanley.com (Aydemir, Zava (FID))
Date: Mon, 2 Jul 2007 12:23:54 -0400
Subject: [R] compute time span in months between two dates
Message-ID: <755261CA22782948B1C42ACDC83912A10443605D@NYWEXMB27.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/ba10944c/attachment.pl 

From georgehret at gmail.com  Mon Jul  2 18:55:43 2007
From: georgehret at gmail.com (Georg Ehret)
Date: Mon, 2 Jul 2007 12:55:43 -0400
Subject: [R] basics: changing the directory
Message-ID: <e4dda3890707020955sc8c2abdqbb58655c45f128d6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/158f3fe4/attachment.pl 

From wwwhsd at gmail.com  Mon Jul  2 18:58:44 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 2 Jul 2007 13:58:44 -0300
Subject: [R] basics: changing the directory
In-Reply-To: <e4dda3890707020955sc8c2abdqbb58655c45f128d6@mail.gmail.com>
References: <e4dda3890707020955sc8c2abdqbb58655c45f128d6@mail.gmail.com>
Message-ID: <da79af330707020958p24d0e2fcuecaa7926cf531387@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/43f0e05a/attachment.pl 

From p.dalgaard at biostat.ku.dk  Mon Jul  2 18:59:35 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 02 Jul 2007 18:59:35 +0200
Subject: [R] compute time span in months between two dates
In-Reply-To: <755261CA22782948B1C42ACDC83912A10443605D@NYWEXMB27.msad.ms.com>
References: <755261CA22782948B1C42ACDC83912A10443605D@NYWEXMB27.msad.ms.com>
Message-ID: <46892EF7.6070304@biostat.ku.dk>

Aydemir, Zava (FID) wrote:
> Hi,
>  
> I am just starting to play with R. What is the recommended manner for
> calculating time spans between 2 dates? In particular, should I be using
> the "chron" or the "date" package (so far I just found how to calculate
> a timespan in terms of days)?
>  
> Thanks
>   
I'd recommend something along these lines:

d1 <- "11/03-1959"
d2 <- "2/7-2007"
f <- "%d/%m-%Y"
as.numeric(as.Date(d2, f) - as.Date(d1, f), units="days")

(The format in f needs to be adjusted to the actual format, of course. 
For some formats, it can be omitted altogether).

>  
> Zava
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From singularitaet at gmx.net  Mon Jul  2 19:15:01 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Mon, 02 Jul 2007 19:15:01 +0200
Subject: [R] help again
In-Reply-To: <904d37580707020922k10303719hbfb06a5a115ce375@mail.gmail.com>
References: <904d37580707020922k10303719hbfb06a5a115ce375@mail.gmail.com>
Message-ID: <46893295.3070204@gmx.net>


> hi
> i try to use arima and holtwinter to predict drought from 1895-2006
> but i cannot read whole period of time and i try to do the exponent
> fitting,
> but it comes out as the coordinate x-y error
> i send the source code and data to take a look
> if anyone can help me, i am really new in R
>



"PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html

and provide commented, minimal, self-contained, reproducible code."

This makes it much easier to help! (how do your numbers look like e.g.) You don't need to send the whole dataset but a few lines would be nice plus what commands you've done to receive those errors...

Stefan


-=-=-
... Lotteries are a tax on ignorance. (A. Smith - attributed)


From Greg.Snow at intermountainmail.org  Mon Jul  2 19:36:12 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 2 Jul 2007 11:36:12 -0600
Subject: [R] Combine graphical and textual output
In-Reply-To: <4688C09E.7080400@tue.nl>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBA5B0E9@LP-EXCHVS07.CO.IHC.COM>

The R2HTML package can be used to create and html transcript of your
session (you do need to start, end, and tell it to include graphs).  I
use this when doing an R demonstration (teaching), so that the students
can have a transcript that shows the commands I typed along with their
output (including the graphs).  

Another approach is the odfWeave package.  This is similar to sweave,
except that you use openoffice writer (can read MSWord docs) to set up
the text and set of commands (less overhead/startup than LaTeX).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dieter 
> Vanderelst
> Sent: Monday, July 02, 2007 3:09 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Combine graphical and textual output
> 
> Hi,
> 
> I would like to know whether anybody knows a simple way to 
> combine textual and graphical output in R.
> 
> A typical analysis produces textual output (e.g. model fits) 
> and plots in R. I would like to know whether R has the 
> possibility of combining these into a single 'report' or 
> output. An example of a program that does this is SPSS. After 
> running the analysis you have a combination of textual output 
> (tables) and plots that are easy to print and distribute.
> 
> I know of the possibility to embed R code into LATEX (using 
> Sweave), but I wouldn't call this quick since a lot of coding 
> will go into writing the Sweave file.
> 
> Any other suggestions?
> 
> Regards,
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Mon Jul  2 19:40:15 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Mon, 2 Jul 2007 15:40:15 -0200
Subject: [R] help again
In-Reply-To: <46893295.3070204@gmx.net>
References: <904d37580707020922k10303719hbfb06a5a115ce375@mail.gmail.com>
	<46893295.3070204@gmx.net>
Message-ID: <20070702173828.M98619@centroin.com.br>

Stefan Grosse wrote:
> 
> This makes it much easier to help! (how do your numbers look like 
> e.g.) You don't need to send the whole dataset but a few lines would 
> be nice plus what commands you've done to receive those errors...
> 
In fact, I noticed that, most of the time, when I reduce the
problematic code to a minimum code that reproduces the bug,
it's much easier _for me_ to spot (and fix) the error :-)

Alberto Monteiro


From jrkrideau at yahoo.ca  Mon Jul  2 19:44:08 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 2 Jul 2007 13:44:08 -0400 (EDT)
Subject: [R] basics: changing the directory
In-Reply-To: <e4dda3890707020955sc8c2abdqbb58655c45f128d6@mail.gmail.com>
Message-ID: <463415.69313.qm@web32807.mail.mud.yahoo.com>

RSiteSearch("change directory")  will take you to an
archive site that should help. 

or type
?setwd  to get the relevant man page.  


--- Georg Ehret <georgehret at gmail.com> wrote:

> Dear Ms. R,
>    I struggle with a very basic command for most of
> you: How to change the
> working-directory by command-line?
> 
> Thank you!
> Georg.


From mark_difford at yahoo.co.uk  Mon Jul  2 19:55:15 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Mon, 2 Jul 2007 10:55:15 -0700 (PDT)
Subject: [R] Question about PCA with prcomp
In-Reply-To: <D8F26047-45B9-4E1F-B813-DCD1F4901623@mac.com>
References: <D8F26047-45B9-4E1F-B813-DCD1F4901623@mac.com>
Message-ID: <11398608.post@talk.nabble.com>


Hi James,

Have a look at Cadima et al.'s subselect package [Cadima worked with/was a
student of Prof Jolliffe, one of _the_ experts on PCA; Jolliffe devotes part
of a Chapter to this question in his text (Principal Component Analysis,
pub. Springer)].  Then you should look at psychometric stuff: a good place
to start would be Professor Revelle's psych package.

BestR,
Mark.


James R. Graham wrote:
> 
> Hello All,
> 
> The basic premise of what I want to do is the following:
> 
> I have 20 "entities" for which I have ~500 measurements each. So, I  
> have a matrix of 20 rows by ~500 columns.
> 
> The 20 entities fall into two classes: "good" and "bad."
> 
> I eventually would like to derive a model that would then be able to  
> classify new entities as being in "good territory" or "bad territory"  
> based upon my existing data set.
> 
> I know that not all ~500 measurements are meaningful, so I thought  
> the best place to begin would be to do a PCA in order to reduce the  
> amount of data with which I have to work.
> 
> I did this using the prcomp function and found that nearly 90% of the  
> variance in the data is explained by PC1 and 2.
> 
> So far, so good.
> 
> I would now like to find out which of the original ~500 measurements  
> contribute to PC1 and 2 and by how much.
> 
> Any tips would be greatly appreciated! And apologies in advance if  
> this turns out to be an idiotic question.
> 
> 
> james
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Question-about-PCA-with-prcomp-tf4012919.html#a11398608
Sent from the R help mailing list archive at Nabble.com.


From ksorensen84 at yahoo.com  Mon Jul  2 20:05:16 2007
From: ksorensen84 at yahoo.com (Kevin Sorensen)
Date: Mon, 2 Jul 2007 11:05:16 -0700 (PDT)
Subject: [R] gam function & time trend splines
Message-ID: <789984.89002.qm@web44910.mail.sp1.yahoo.com>

I've been doing a simple time-series analysis looking
at the relationship between daily pneumonia
hospitalizations and daily temperature.  To mimic some
of the literature, I've been including a time-trend to
try to account for normal cyclical trends in
hospitalization.  So I've been using a function that
looks something like this:

gam(pneucount ~ temp_f +
s(day,bs="cr",k=(4*totalyears)+1),

day being the enumerated day in the analysis (1-365
for a 1 year analysis). 

This seems to work well enough.  What troubles me is
when I think about doing an analysis focusing on
winter days using more than one year of data.  If I
just delete the summer days from the dataset, the time
trend spline is trying to anneal counts from the end
of one winter with the beginning of another, which
doesn't seem right to me.  

What's the route to a statistically defensible result?
 Is it as simple as using the subset option?  Or would
I need to create indicator variables for each winter
I'm interested and work in a by statement somehow
(with an extra term for the levels of that indicator,
I assume)?  

Thanks in advance for helping a Epi student who's
being exposed to all this for the first time.

Sincerely,

Kevin Sorensen 


      ____________________________________________________________________________________
Park yourself in front of a world of choices in alternative vehicles. Visit the Yahoo! Auto Green Center.


From jrkrideau at yahoo.ca  Mon Jul  2 20:06:02 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 2 Jul 2007 14:06:02 -0400 (EDT)
Subject: [R] Homals, PsychoR, Bayesm
In-Reply-To: <779104.71664.qm@web53303.mail.re2.yahoo.com>
Message-ID: <487418.61743.qm@web32809.mail.mud.yahoo.com>

You need to tell us what operating system you are
using.


--- faisal afzal siddiqui <fasidfas at yahoo.com> wrote:

> hi,
> 
> I am working on conjoint analysis, how I can install
> Homals, PsychoR, and Bayesm in my computer, also I
> need some examples in R, would u please help me?
> 
> also advise if there are other programmes/modules in
> R > for conjoint analysis?
> 
> Regds 
> Faisal Afzal Siddiqui
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jizhang at chori.org  Mon Jul  2 19:17:54 2007
From: jizhang at chori.org (Jiong Zhang, PhD)
Date: Mon, 02 Jul 2007 10:17:54 -0700
Subject: [R]  displaying one page at a time in the console
Message-ID: <9911b82c3fadb84182141a14f713b6a0@mail.chori.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/ca6dfc31/attachment.pl 

From Roy.Mendelssohn at noaa.gov  Mon Jul  2 20:42:32 2007
From: Roy.Mendelssohn at noaa.gov (Roy Mendelssohn)
Date: Mon, 02 Jul 2007 11:42:32 -0700
Subject: [R] download.file - it works on my Mac but not on Windows.
Message-ID: <3B7EDFF8-3569-4D44-832B-FAA15E7DAADE@noaa.gov>

Hi:

I am working with someone remotely to allow them access to our data.  
The follow command using "download.file" works perfectly on my Mac:


> > download.file(url="http://oceanwatch.pfeg.noaa.gov:8081/thredds/ 
> wcs/satellite/AG/ssta/14day? 
> request=GetCoverage&version=1.0.0&service=WCS&format=NetCDF3&coverage= 
> AGssta&Vertical=.0&time=2006-01-06T00:00:00Z&bbox=220,20,250,50",  
> destfile="/users/rmendels/desktop/carrie.nc")
> trying URL 'http://oceanwatch.pfeg.noaa.gov:8081/thredds/wcs/ 
> satellite/AG/ssta/14day? 
> request=GetCoverage&version=1.0.0&service=WCS&format=NetCDF3&coverage= 
> AGssta&Vertical=.0&time=2006-01-06T00:00:00Z&bbox=220,20,250,50'
> Content type 'application/x-netcdf' length 369144 bytes
> opened URL
> ==================================================
> downloaded 360Kb
>
>

On Windows, which this person is using, the following fails:

> download.file(url="http://oceanwatch.pfeg.noaa.gov:8081/thredds/wcs/ 
> satellite/AG/ssta/14day? 
> request=GetCoverage&version=1.0.0&service=WCS&format=NetCDF3&coverage= 
> AGssta&Vertical=.0&time=2006-01-06T00:00:00Z&bbox=220,20,250,50",  
> destfile="C:\Users\Catherine Holt\test.nc")
>


The error message is:

> Error: Uxxxxxxx sequences are not supported on Windows
>

Relevant Info:

Mac:

 > version
                _
platform       powerpc-apple-darwin8.9.1
arch           powerpc
os             darwin8.9.1
system         powerpc, darwin8.9.1
status
major          2
minor          5.1
year           2007
month          06
day            27
svn rev        42083
language       R
version.string R version 2.5.1 (2007-06-27)


Windows:

Here's my Version information:
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          5.0
year           2007
month          04
day            23
svn rev        41293
language       R
version.string R version 2.5.0 (2007-04-23)


Any help or workarounds appreciated.

-Roy M.






**********************
"The contents of this message do not reflect any position of the U.S.  
Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division	
Southwest Fisheries Science Center
1352 Lighthouse Avenue
Pacific Grove, CA 93950-2097

e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
voice: (831)-648-9029
fax: (831)-648-8440
www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."


From roland.rproject at gmail.com  Mon Jul  2 20:46:49 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Mon, 02 Jul 2007 14:46:49 -0400
Subject: [R] GPL Question and Updated Link
Message-ID: <46894819.9040904@gmail.com>

Dear all,

I saw that the Free Software Foundation (www.fsf.org) released Version 3 
of their General Public Licence on Friday.
Just out of curiosity, are there any plans to 'upgrade' the licensing 
terms of R?

Maybe one can update the licence() function since the link given there 
(http://www.gnu.org/copyleft/gpl.html) leads to the main page of Version 
3 of the GPL.
The main page for Version 2 is now:
http://www.gnu.org/licenses/old-licenses/gpl-2.0.html

All the best,
Roland


From wwwhsd at gmail.com  Mon Jul  2 21:11:00 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 2 Jul 2007 16:11:00 -0300
Subject: [R] download.file - it works on my Mac but not on Windows.
In-Reply-To: <3B7EDFF8-3569-4D44-832B-FAA15E7DAADE@noaa.gov>
References: <3B7EDFF8-3569-4D44-832B-FAA15E7DAADE@noaa.gov>
Message-ID: <da79af330707021211h4795a350u85ce254198a22a78@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/52c204d8/attachment.pl 

From rvaradhan at jhmi.edu  Mon Jul  2 21:16:26 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 2 Jul 2007 15:16:26 -0400
Subject: [R] Question about PCA with prcomp
In-Reply-To: <11398608.post@talk.nabble.com>
References: <D8F26047-45B9-4E1F-B813-DCD1F4901623@mac.com>
	<11398608.post@talk.nabble.com>
Message-ID: <000001c7bcdd$7c8f9f50$7c94100a@win.ad.jhu.edu>

Mark,

What you are referring to deals with the selection of covariates, since PC
doesn't do dimensionality reduction in the sense of covariate selection.
But what Mark is asking for is to identify how much each data point
contributes to individual PCs.  I don't think that Mark's query makes much
sense, unless he meant to ask: which individuals have high/low scores on
PC1/PC2.  Here are some comments that may be tangentially related to Mark's
question:

1.  If one is worried about a few data points contributing heavily to the
estimation of PCs, then one can use robust PCA, for example, using robust
covariance matrices.  MASS has some tools for this.
2.  The "biplot" for the first 2 PCs can give some insights
3. PCs, especially, the last few PCs, can be used to identify "outliers".
  
Hope this is helpful,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Difford
Sent: Monday, July 02, 2007 1:55 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Question about PCA with prcomp


Hi James,

Have a look at Cadima et al.'s subselect package [Cadima worked with/was a
student of Prof Jolliffe, one of _the_ experts on PCA; Jolliffe devotes part
of a Chapter to this question in his text (Principal Component Analysis,
pub. Springer)].  Then you should look at psychometric stuff: a good place
to start would be Professor Revelle's psych package.

BestR,
Mark.


James R. Graham wrote:
> 
> Hello All,
> 
> The basic premise of what I want to do is the following:
> 
> I have 20 "entities" for which I have ~500 measurements each. So, I  
> have a matrix of 20 rows by ~500 columns.
> 
> The 20 entities fall into two classes: "good" and "bad."
> 
> I eventually would like to derive a model that would then be able to  
> classify new entities as being in "good territory" or "bad territory"  
> based upon my existing data set.
> 
> I know that not all ~500 measurements are meaningful, so I thought  
> the best place to begin would be to do a PCA in order to reduce the  
> amount of data with which I have to work.
> 
> I did this using the prcomp function and found that nearly 90% of the  
> variance in the data is explained by PC1 and 2.
> 
> So far, so good.
> 
> I would now like to find out which of the original ~500 measurements  
> contribute to PC1 and 2 and by how much.
> 
> Any tips would be greatly appreciated! And apologies in advance if  
> this turns out to be an idiotic question.
> 
> 
> james
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context:
http://www.nabble.com/Question-about-PCA-with-prcomp-tf4012919.html#a1139860
8
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Mon Jul  2 22:01:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 2 Jul 2007 16:01:57 -0400
Subject: [R] displaying one page at a time in the console
In-Reply-To: <9911b82c3fadb84182141a14f713b6a0@mail.chori.org>
References: <9911b82c3fadb84182141a14f713b6a0@mail.chori.org>
Message-ID: <971536df0707021301i1b1bbfc5u5064d548c57af347@mail.gmail.com>

View(names(x)) will display the names in a scrollable fashion.  Also
head(names(x)) will show just the first few if that's good enough.

On 7/2/07, Jiong Zhang, PhD <jizhang at chori.org> wrote:
> Hi all,
>
> How do I ask the console to display one page at a time for commands such
> as "names()"?
>
> Thanks.
>
> jiong


From p_connolly at ihug.co.nz  Mon Jul  2 22:22:31 2007
From: p_connolly at ihug.co.nz (Patrick Connolly)
Date: Tue, 3 Jul 2007 08:22:31 +1200
Subject: [R] Question about PCA with prcomp
In-Reply-To: <000001c7bcdd$7c8f9f50$7c94100a@win.ad.jhu.edu>
References: <D8F26047-45B9-4E1F-B813-DCD1F4901623@mac.com>
	<11398608.post@talk.nabble.com>
	<000001c7bcdd$7c8f9f50$7c94100a@win.ad.jhu.edu>
Message-ID: <20070702202231.GA9728@ihug.co.nz>

On Mon, 02-Jul-2007 at 03:16PM -0400, Ravi Varadhan wrote:

|> Mark,
|> 
|> What you are referring to deals with the selection of covariates, since PC
|> doesn't do dimensionality reduction in the sense of covariate selection.
|> But what Mark is asking for is to identify how much each data point
|> contributes to individual PCs.  I don't think that Mark's query makes much
|> sense, unless he meant to ask: which individuals have high/low scores on
|> PC1/PC2.  Here are some comments that may be tangentially related to Mark's
|> question:
|> 
|> 1.  If one is worried about a few data points contributing heavily to the
|> estimation of PCs, then one can use robust PCA, for example, using robust
|> covariance matrices.  MASS has some tools for this.
|> 2.  The "biplot" for the first 2 PCs can give some insights
|> 3. PCs, especially, the last few PCs, can be used to identify "outliers".

What is meant by "last few PCs"?

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From Roy.Mendelssohn at noaa.gov  Mon Jul  2 22:28:34 2007
From: Roy.Mendelssohn at noaa.gov (Roy Mendelssohn)
Date: Mon, 02 Jul 2007 13:28:34 -0700
Subject: [R] download.file - it works on my Mac but not on Windows.
In-Reply-To: <da79af330707021211h4795a350u85ce254198a22a78@mail.gmail.com>
References: <3B7EDFF8-3569-4D44-832B-FAA15E7DAADE@noaa.gov>
	<da79af330707021211h4795a350u85ce254198a22a78@mail.gmail.com>
Message-ID: <39A538B5-9777-456C-8BE5-D0686C7705B1@noaa.gov>

Thanks muchly.  That fixed the problem.

-Roy M.

On Jul 2, 2007, at 12:11 PM, Henrique Dallazuanna wrote:

> Try whit:
>
> destfile="C:/Users/Catherine Holt/test.nc"
>
> --  
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
>
> On 02/07/07, Roy Mendelssohn <Roy.Mendelssohn at noaa.gov> wrote: Hi:
>
> I am working with someone remotely to allow them access to our data.
> The follow command using "download.file" works perfectly on my Mac:
>
>
> > > download.file(url=" http://oceanwatch.pfeg.noaa.gov:8081/thredds/
> > wcs/satellite/AG/ssta/14day?
> >  
> request=GetCoverage&version=1.0.0&service=WCS&format=NetCDF3&coverage=
> > AGssta&Vertical=.0&time=2006-01-06T00:00:00Z&bbox=220,20,250,50",
> > destfile="/users/rmendels/desktop/carrie.nc")
> > trying URL 'http://oceanwatch.pfeg.noaa.gov:8081/thredds/wcs/
> > satellite/AG/ssta/14day?
> >  
> request=GetCoverage&version=1.0.0&service=WCS&format=NetCDF3&coverage=
> > AGssta&Vertical=.0&time=2006-01-06T00:00:00Z&bbox=220,20,250,50'
> > Content type 'application/x-netcdf' length 369144 bytes
> > opened URL
> > ==================================================
> > downloaded 360Kb
> >
> >
>
> On Windows, which this person is using, the following fails:
>
> > download.file(url=" http://oceanwatch.pfeg.noaa.gov:8081/thredds/ 
> wcs/
> > satellite/AG/ssta/14day?
> >  
> request=GetCoverage&version=1.0.0&service=WCS&format=NetCDF3&coverage=
> > AGssta&Vertical=.0&time=2006-01-06T00:00:00Z&bbox=220,20,250,50",
> > destfile="C:\Users\Catherine Holt\test.nc")
> >
>
>
> The error message is:
>
> > Error: Uxxxxxxx sequences are not supported on Windows
> >
>
> Relevant Info:
>
> Mac:
>
> > version
>                 _
> platform       powerpc-apple-darwin8.9.1
> arch           powerpc
> os             darwin8.9.1
> system         powerpc, darwin8.9.1
> status
> major          2
> minor          5.1
> year           2007
> month          06
> day            27
> svn rev        42083
> language       R
> version.string R version 2.5.1 (2007-06-27)
>
>
> Windows:
>
> Here's my Version information:
>                _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor           5.0
> year           2007
> month          04
> day            23
> svn rev        41293
> language       R
> version.string R version 2.5.0 (2007-04-23)
>
>
> Any help or workarounds appreciated.
>
> -Roy M.
>
>
>
>
>
>
> **********************
> "The contents of this message do not reflect any position of the U.S.
> Government or NOAA."
> **********************
> Roy Mendelssohn
> Supervisory Operations Research Analyst
> NOAA/NMFS
> Environmental Research Division
> Southwest Fisheries Science Center
> 1352 Lighthouse Avenue
> Pacific Grove, CA 93950-2097
>
> e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
> voice: (831)-648-9029
> fax: (831)-648-8440
> www: http://www.pfeg.noaa.gov/
>
> "Old age and treachery will overcome youth and skill."
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

**********************
"The contents of this message do not reflect any position of the U.S.  
Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division	
Southwest Fisheries Science Center
1352 Lighthouse Avenue
Pacific Grove, CA 93950-2097

e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
voice: (831)-648-9029
fax: (831)-648-8440
www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."


From rvaradhan at jhmi.edu  Mon Jul  2 22:29:13 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 2 Jul 2007 16:29:13 -0400
Subject: [R] Question about PCA with prcomp
In-Reply-To: <20070702202231.GA9728@ihug.co.nz>
References: <D8F26047-45B9-4E1F-B813-DCD1F4901623@mac.com>
	<11398608.post@talk.nabble.com>
	<000001c7bcdd$7c8f9f50$7c94100a@win.ad.jhu.edu>
	<20070702202231.GA9728@ihug.co.nz>
Message-ID: <000001c7bce7$a76d8bb0$7c94100a@win.ad.jhu.edu>

The PCs that are associated with the smaller eigenvalues. 

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: Patrick Connolly [mailto:p_connolly at ihug.co.nz] 
Sent: Monday, July 02, 2007 4:23 PM
To: Ravi Varadhan
Cc: 'Mark Difford'; r-help at stat.math.ethz.ch
Subject: Re: [R] Question about PCA with prcomp

On Mon, 02-Jul-2007 at 03:16PM -0400, Ravi Varadhan wrote:

|> Mark,
|> 
|> What you are referring to deals with the selection of covariates, since
PC
|> doesn't do dimensionality reduction in the sense of covariate selection.
|> But what Mark is asking for is to identify how much each data point
|> contributes to individual PCs.  I don't think that Mark's query makes
much
|> sense, unless he meant to ask: which individuals have high/low scores on
|> PC1/PC2.  Here are some comments that may be tangentially related to
Mark's
|> question:
|> 
|> 1.  If one is worried about a few data points contributing heavily to the
|> estimation of PCs, then one can use robust PCA, for example, using robust
|> covariance matrices.  MASS has some tools for this.
|> 2.  The "biplot" for the first 2 PCs can give some insights
|> 3. PCs, especially, the last few PCs, can be used to identify "outliers".

What is meant by "last few PCs"?

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From mark_difford at yahoo.co.uk  Mon Jul  2 22:44:29 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Mon, 2 Jul 2007 13:44:29 -0700 (PDT)
Subject: [R] Question about PCA with prcomp
In-Reply-To: <000001c7bcdd$7c8f9f50$7c94100a@win.ad.jhu.edu>
References: <D8F26047-45B9-4E1F-B813-DCD1F4901623@mac.com>
	<11398608.post@talk.nabble.com>
	<000001c7bcdd$7c8f9f50$7c94100a@win.ad.jhu.edu>
Message-ID: <11401504.post@talk.nabble.com>


Hi James, Ravi:

James wrote:
...
>> I have 20 "entities" for which I have ~500 measurements each. So, I   
>> have a matrix of 20 rows by ~500 columns.
...

Perhaps I misread James' question, but I don't think so.  As James described
it, we have ~500 measurements made on 20 objects.  A PCA on this [20
rows/observations by ~ 500 columns/descriptors/variables] should return ~
500 eigenvalues.  And each of these columns/descriptors/variables will have
a loading on each PC.

James wants to reduce his descriptors/measurements/variables to the "most
important" (variance).  A primitive way of doing this would be to examine
the loadings on the first 2--3 PCs and choose those
columns/descriptors/variables with the highest loadings, and throw away the
rest.  [He has already decided that he can throw away all but the first two
PCs.]  In fact, it would be a very good idea to do a coinertia analysis on
the pre- and post-selected sets, and look at the RV value.  If this is above
[thumbsuck] 0.9, then you're doing very well (there's a good plot method for
this in ade4, cf coinertia &c).

But see Cadima et al. (+refs for caution; and elsewhere) for more
sophisticated methods of subsetting.

Regards,
Mark.


Ravi Varadhan wrote:
> 
> Mark,
> 
> What you are referring to deals with the selection of covariates, since PC
> doesn't do dimensionality reduction in the sense of covariate selection.
> But what Mark is asking for is to identify how much each data point
> contributes to individual PCs.  I don't think that Mark's query makes much
> sense, unless he meant to ask: which individuals have high/low scores on
> PC1/PC2.  Here are some comments that may be tangentially related to
> Mark's
> question:
> 
> 1.  If one is worried about a few data points contributing heavily to the
> estimation of PCs, then one can use robust PCA, for example, using robust
> covariance matrices.  MASS has some tools for this.
> 2.  The "biplot" for the first 2 PCs can give some insights
> 3. PCs, especially, the last few PCs, can be used to identify "outliers".
>   
> Hope this is helpful,
> Ravi.
> 
> ----------------------------------------------------------------------------
> -------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor, The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology 
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email: rvaradhan at jhmi.edu
> 
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
> 
>  
> 
> ----------------------------------------------------------------------------
> --------
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark Difford
> Sent: Monday, July 02, 2007 1:55 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Question about PCA with prcomp
> 
> 
> Hi James,
> 
> Have a look at Cadima et al.'s subselect package [Cadima worked with/was a
> student of Prof Jolliffe, one of _the_ experts on PCA; Jolliffe devotes
> part
> of a Chapter to this question in his text (Principal Component Analysis,
> pub. Springer)].  Then you should look at psychometric stuff: a good place
> to start would be Professor Revelle's psych package.
> 
> BestR,
> Mark.
> 
> 
> James R. Graham wrote:
>> 
>> Hello All,
>> 
>> The basic premise of what I want to do is the following:
>> 
>> I have 20 "entities" for which I have ~500 measurements each. So, I  
>> have a matrix of 20 rows by ~500 columns.
>> 
>> The 20 entities fall into two classes: "good" and "bad."
>> 
>> I eventually would like to derive a model that would then be able to  
>> classify new entities as being in "good territory" or "bad territory"  
>> based upon my existing data set.
>> 
>> I know that not all ~500 measurements are meaningful, so I thought  
>> the best place to begin would be to do a PCA in order to reduce the  
>> amount of data with which I have to work.
>> 
>> I did this using the prcomp function and found that nearly 90% of the  
>> variance in the data is explained by PC1 and 2.
>> 
>> So far, so good.
>> 
>> I would now like to find out which of the original ~500 measurements  
>> contribute to PC1 and 2 and by how much.
>> 
>> Any tips would be greatly appreciated! And apologies in advance if  
>> this turns out to be an idiotic question.
>> 
>> 
>> james
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> 
> -- 
> View this message in context:
> http://www.nabble.com/Question-about-PCA-with-prcomp-tf4012919.html#a1139860
> 8
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Question-about-PCA-with-prcomp-tf4012919.html#a11401504
Sent from the R help mailing list archive at Nabble.com.


From Bill.Venables at csiro.au  Mon Jul  2 23:03:02 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Tue, 3 Jul 2007 07:03:02 +1000
Subject: [R] Question about PCA with prcomp
Message-ID: <B998A44C8986644EA8029CFE6396A924B67FBB@exqld2-bne.nexus.csiro.au>

...but with 500 variables and only 20 'entities' (observations) you will
have 481 PCs with dead zero eigenvalues.  How small is 'smaller' and how
many is "a few"?

Everyone who has responded to this seems to accept the idea that PCA is
the way to go here, but that is not clear to me at all.  There is a
2-sample structure in the 20 observations that you have.  If you simply
ignore that in doing your PCA you are making strong assumptions about
sampling that would seem to me unlikely to be met.  If you allow for the
structure and project orthogonal to it then you are probably throwing
the baby out with the bathwater - you want to choose variables which
maximise separation between the 2 samples (and now you are up to 482
zero principal variances, if that matters...).

I think this problem probably needs a bit of a re-think.  Some variant
on singular LDA, for example, may be a more useful way to think about
it.

Bill Venables.  

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ravi Varadhan
Sent: Monday, 2 July 2007 1:29 PM
To: 'Patrick Connolly'
Cc: r-help at stat.math.ethz.ch; 'Mark Difford'
Subject: Re: [R] Question about PCA with prcomp

The PCs that are associated with the smaller eigenvalues. 

------------------------------------------------------------------------
----
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:
http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

------------------------------------------------------------------------
----
--------

-----Original Message-----
From: Patrick Connolly [mailto:p_connolly at ihug.co.nz]
Sent: Monday, July 02, 2007 4:23 PM
To: Ravi Varadhan
Cc: 'Mark Difford'; r-help at stat.math.ethz.ch
Subject: Re: [R] Question about PCA with prcomp

On Mon, 02-Jul-2007 at 03:16PM -0400, Ravi Varadhan wrote:

|> Mark,
|> 
|> What you are referring to deals with the selection of covariates, 
|> since
PC
|> doesn't do dimensionality reduction in the sense of covariate
selection.
|> But what Mark is asking for is to identify how much each data point 
|> contributes to individual PCs.  I don't think that Mark's query makes
much
|> sense, unless he meant to ask: which individuals have high/low scores

|> on PC1/PC2.  Here are some comments that may be tangentially related 
|> to
Mark's
|> question:
|> 
|> 1.  If one is worried about a few data points contributing heavily to

|> the estimation of PCs, then one can use robust PCA, for example, 
|> using robust covariance matrices.  MASS has some tools for this.
|> 2.  The "biplot" for the first 2 PCs can give some insights 3. PCs, 
|> especially, the last few PCs, can be used to identify "outliers".

What is meant by "last few PCs"?

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.

   ___    Patrick Connolly   
 {~._.~}          		 Great minds discuss ideas    
 _( Y )_  	  	        Middle minds discuss events 
(:_~*~_:) 	       		 Small minds discuss people  
 (_)-(_)  	                           ..... Anon
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mark_difford at yahoo.co.uk  Mon Jul  2 23:18:07 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Mon, 2 Jul 2007 14:18:07 -0700 (PDT)
Subject: [R] Question about PCA with prcomp
In-Reply-To: <B998A44C8986644EA8029CFE6396A924B67FBB@exqld2-bne.nexus.csiro.au>
References: <D8F26047-45B9-4E1F-B813-DCD1F4901623@mac.com>
	<B998A44C8986644EA8029CFE6396A924B67FBB@exqld2-bne.nexus.csiro.au>
Message-ID: <11402204.post@talk.nabble.com>


To all ...,

Bill's "lateral" wisdom is almost certainly a better solution.  So thanks
for the advice (and everything else that went before it [Bill: apropos of
termplot, what happened to tplot ?]).  And I will [almost] desist from
asking the obvious: and if there were 10 000 observations ?

BestR,
Mark.


Bill.Venables wrote:
> 
> ...but with 500 variables and only 20 'entities' (observations) you will
> have 481 PCs with dead zero eigenvalues.  How small is 'smaller' and how
> many is "a few"?
> 
> Everyone who has responded to this seems to accept the idea that PCA is
> the way to go here, but that is not clear to me at all.  There is a
> 2-sample structure in the 20 observations that you have.  If you simply
> ignore that in doing your PCA you are making strong assumptions about
> sampling that would seem to me unlikely to be met.  If you allow for the
> structure and project orthogonal to it then you are probably throwing
> the baby out with the bathwater - you want to choose variables which
> maximise separation between the 2 samples (and now you are up to 482
> zero principal variances, if that matters...).
> 
> I think this problem probably needs a bit of a re-think.  Some variant
> on singular LDA, for example, may be a more useful way to think about
> it.
> 
> Bill Venables.  
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ravi Varadhan
> Sent: Monday, 2 July 2007 1:29 PM
> To: 'Patrick Connolly'
> Cc: r-help at stat.math.ethz.ch; 'Mark Difford'
> Subject: Re: [R] Question about PCA with prcomp
> 
> The PCs that are associated with the smaller eigenvalues. 
> 
> ------------------------------------------------------------------------
> ----
> -------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor, The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology 
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email: rvaradhan at jhmi.edu
> 
> Webpage:
> http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
> 
>  
> 
> ------------------------------------------------------------------------
> ----
> --------
> 
> -----Original Message-----
> From: Patrick Connolly [mailto:p_connolly at ihug.co.nz]
> Sent: Monday, July 02, 2007 4:23 PM
> To: Ravi Varadhan
> Cc: 'Mark Difford'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Question about PCA with prcomp
> 
> On Mon, 02-Jul-2007 at 03:16PM -0400, Ravi Varadhan wrote:
> 
> |> Mark,
> |> 
> |> What you are referring to deals with the selection of covariates, 
> |> since
> PC
> |> doesn't do dimensionality reduction in the sense of covariate
> selection.
> |> But what Mark is asking for is to identify how much each data point 
> |> contributes to individual PCs.  I don't think that Mark's query makes
> much
> |> sense, unless he meant to ask: which individuals have high/low scores
> 
> |> on PC1/PC2.  Here are some comments that may be tangentially related 
> |> to
> Mark's
> |> question:
> |> 
> |> 1.  If one is worried about a few data points contributing heavily to
> 
> |> the estimation of PCs, then one can use robust PCA, for example, 
> |> using robust covariance matrices.  MASS has some tools for this.
> |> 2.  The "biplot" for the first 2 PCs can give some insights 3. PCs, 
> |> especially, the last few PCs, can be used to identify "outliers".
> 
> What is meant by "last few PCs"?
> 
> -- 
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> 
>    ___    Patrick Connolly   
>  {~._.~}          		 Great minds discuss ideas    
>  _( Y )_  	  	        Middle minds discuss events 
> (:_~*~_:) 	       		 Small minds discuss people  
>  (_)-(_)  	                           ..... Anon
> 	  
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Question-about-PCA-with-prcomp-tf4012919.html#a11402204
Sent from the R help mailing list archive at Nabble.com.


From bthatcher at att.net  Mon Jul  2 23:19:32 2007
From: bthatcher at att.net (Bruce)
Date: Mon, 2 Jul 2007 15:19:32 -0600
Subject: [R] Problem installing R packages in OpenBSD
Message-ID: <200707021519.33520.bthatcher@att.net>

OS:  OpenBSD version 4.1 i386
R version 2.4.1 (2006-12-18) installed as a binary package

$ ls
mapproj_1.1-7.1.tar.gz     maps_2.0-36.tar.gz

$ sudo R CMD INSTALL mapproj_1.1-7.1.tar.gz

/usr/local/lib/R/bin/INSTALL[118]: 
  .: /usr/obj/i386/R-2.4.1/fake-i386/usr/local/lib/R/share/sh/dcf.sh: not 
found

I get the same error message from the R command prompt using 
install.packages("mapproj", dep=TRUE)

Questions regarding  /usr/obj/i386/R-2.4.1/fake-i386:  
o Where did this path come from?  
o Is there a conflict between OpenBSD ports and R packages?
o Is OpenBSD ports trying to do the package install?  If so, how do I stop it?

Please note that I have installed R packages successfully on Linux, Windows XP 
and FreeBSD.

Background information
-------------------------------------------
Environment variables:
$ echo $R_HOME
/usr/local/lib/R
$ echo $R_LIBS
/usr/local/lib/R/library

R install:
$ pwd
/usr/local/lib/R/share/sh
$ ls -l
total 16
-r--r--r--  1 root  bin   392 Mar  9 00:57 dcf.sh
-r--r--r--  1 root  bin    27 Mar  9 00:57 echo.sh
-r--r--r--  1 root  bin  1506 Mar  9 00:57 help-links.sh
-r--r--r--  1 root  bin   825 Mar  9 00:57 help-print.sh

Thanks


From juryef at yahoo.com  Tue Jul  3 01:01:07 2007
From: juryef at yahoo.com (Judith Flores)
Date: Mon, 2 Jul 2007 16:01:07 -0700 (PDT)
Subject: [R] Substitution of Variables
Message-ID: <25077.11252.qm@web34704.mail.mud.yahoo.com>

Hi,

   I need to run a script under the variable (that
comes from a csv file) that is assigned to another
variable. This is very a simplified version of what I
want to do:

data<-read.csv('name.csv')
names(data)<-("VA","VB","VC")

v<-VA

mn(v)


Thank you in advance,

Judith
   




       
____________________________________________________________________________________
Got a little couch potato? 
Check out fun summer activities for kids.


From jholtman at gmail.com  Tue Jul  3 01:25:46 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 2 Jul 2007 19:25:46 -0400
Subject: [R] Substitution of Variables
In-Reply-To: <25077.11252.qm@web34704.mail.mud.yahoo.com>
References: <25077.11252.qm@web34704.mail.mud.yahoo.com>
Message-ID: <644e1f320707021625o3fac9f9ewd7f9810f395370d9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/4371ec03/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Tue Jul  3 01:32:43 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 03 Jul 2007 00:32:43 +0100 (BST)
Subject: [R] Substitution of Variables
In-Reply-To: <25077.11252.qm@web34704.mail.mud.yahoo.com>
Message-ID: <XFMail.070703003243.ted.harding@nessie.mcc.ac.uk>

On 02-Jul-07 23:01:07, Judith Flores wrote:
> Hi,
>    I need to run a script under the variable (that
> comes from a csv file) that is assigned to another
> variable. This is very a simplified version of what I
> want to do:
> 
> data<-read.csv('name.csv')
> names(data)<-("VA","VB","VC")
> 
> v<-VA
> 
> mn(v)
> 
> Thank you in advance,
> Judith

read.csv() makes 'data' into a dataframe. This is essentially
a list with named components. So VA is not yet available as
a variable in your program. The data for the variable you want
to call "VA" is stored as the component 'data$VA' of 'data'.
The names "VA", "VB", "VC" are the names you have given to
the *components* of 'data'; you have not created separate
variables with these names.

So you can assign the data for VA directly to 'v' with:

  v<-data$VA

or (if you think you will need it later) create a variable VA
using

  VA<-data$VA

and, if you want a variable called 'v' as well, then:

  v<-VA

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 03-Jul-07                                       Time: 00:32:40
------------------------------ XFMail ------------------------------


From rdpeng at gmail.com  Tue Jul  3 02:43:27 2007
From: rdpeng at gmail.com (Roger Peng)
Date: Mon, 2 Jul 2007 20:43:27 -0400
Subject: [R] gam function & time trend spline
In-Reply-To: <789984.89002.qm@web44910.mail.sp1.yahoo.com>
References: <789984.89002.qm@web44910.mail.sp1.yahoo.com>
Message-ID: <66f3bd910707021743i2fd90323vf62dd4b8891f9494@mail.gmail.com>

If you're looking only at winter days then you probably don't need to
remove seasonal trends, do you?

-roger

On 7/2/07, Kevin Sorensen <ksorensen84 at yahoo.com> wrote:
> I've been doing a simple time-series analysis looking
> at the relationship between daily pneumonia
> hospitalizations and daily temperature.  To mimic some
> of the literature, I've been including a time-trend to
> try to account for normal cyclical trends in
> hospitalization.  So I've been using a function that
> looks something like this:
>
> gam(pneucount ~ temp_f +
> s(day,bs="cr",k=(4*totalyears)+1),
>
> day being the enumerated day in the analysis (1-365
> for a 1 year analysis).
>
> This seems to work well enough.  What troubles me is
> when I think about doing an analysis focusing on
> winter days using more than one year of data.  If I
> just delete the summer days from the dataset, the time
> trend spline is trying to anneal counts from the end
> of one winter with the beginning of another, which
> doesn't seem right to me.
>
> What's the route to a statistically defensible result?
>  Is it as simple as using the subset option?  Or would
> I need to create indicator variables for each winter
> I'm interested and work in a by statement somehow
> (with an extra term for the levels of that indicator,
> I assume)?
>
> Thanks in advance for helping a Epi student who's
> being exposed to all this for the first time.
>
> Sincerely,
>
> Kevin Sorensen
>
>
>       ____________________________________________________________________________________
> Park yourself in front of a world of choices in alternative vehicles. Visit the Yahoo! Auto Green Center.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From ivineet at gmail.com  Tue Jul  3 03:28:00 2007
From: ivineet at gmail.com (Vineet Kumar)
Date: Mon, 2 Jul 2007 21:28:00 -0400
Subject: [R] generating correlated Bernoulli random variables
Message-ID: <4ad306f30707021828o790b3ec1j49a932cf97b39b95@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070702/ba800b32/attachment.pl 

From r.turner at auckland.ac.nz  Tue Jul  3 03:58:08 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 3 Jul 2007 13:58:08 +1200
Subject: [R] generating correlated Bernoulli random variables
In-Reply-To: <4ad306f30707021828o790b3ec1j49a932cf97b39b95@mail.gmail.com>
References: <4ad306f30707021828o790b3ec1j49a932cf97b39b95@mail.gmail.com>
Message-ID: <E59A0BB7-FC46-4B68-B244-D452B6884E36@auckland.ac.nz>


On 3/07/2007, at 1:28 PM, Vineet Kumar wrote:

> Hi all,
> I was wondering how to generate samples for two RVs X1 and X2.
>
> X1 ~ Bernoulli (p1)
> X2 ~ Bernoulli (p2)
>
> Also, X1 and X2 are correlated with correlation \rho.

Back-of-envelope calculation, not checked, not tested:

	Let q1 = p2 + (rho/p1) * sqrt((1-p1)*(1-p2))

                q2 = (p2 - q1*p1)/(1-p1)

	Generate X1 with success probability p1.
	If X1 = 1, generate X2 with probability q1, else generate X2 with  
probability q2.

	Clearly there must be restrictions on the value of rho (in terms of  
p1 and p2) in
	order that the values of q1 and q2 lie between 0 and 1.

	I.e. for some values of rho your goal will be impossible to achieve.

				cheers,

					Rolf Turner

######################################################################
Attention:\ This e-mail message is privileged and confidenti...{{dropped}}


From r.turner at auckland.ac.nz  Tue Jul  3 04:01:54 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 3 Jul 2007 14:01:54 +1200
Subject: [R] generating correlated Bernoulli random variables
In-Reply-To: <4ad306f30707021828o790b3ec1j49a932cf97b39b95@mail.gmail.com>
References: <4ad306f30707021828o790b3ec1j49a932cf97b39b95@mail.gmail.com>
Message-ID: <9B251B37-5013-43DD-8C55-54CA5C4522BE@auckland.ac.nz>


Typo in my previous email:

	sqrt((1-p1)*(1-p2)) should have read sqrt(p1*(1-p1)*p2*(1-p2))

I think!

			cheers,

				Rolf

######################################################################
Attention:\ This e-mail message is privileged and confidenti...{{dropped}}


From dkaplan at education.wisc.edu  Tue Jul  3 04:18:03 2007
From: dkaplan at education.wisc.edu (David Kaplan)
Date: Mon, 02 Jul 2007 21:18:03 -0500
Subject: [R] labeling the screeplot
Message-ID: <4689B1DB.5050003@education.wisc.edu>

Greetings,

I'm using the screeplot command and in the R documentation it shows the 
general setup as

screeplot(x, npcs = min(10, length(x$sdev)),
           type = c("barplot", "lines"),
           main = deparse(substitute(x)), ...)

My question concerns labeling the screeplot and I see from the 
documentation that the main= function is referring to graphics 
parameters.  I don't know what this means or where to find information 
on this.  The plot automatically appears with the label "variances" on 
y-axis (which are actually the eigenvalues), and the label PCA on the 
top.  I wish to relabel.

Thanks in advance,

David
-- 
=======================================================================
David Kaplan, Ph.D.
Professor
Department of Educational Psychology
University of Wisconsin - Madison
Educational Sciences, Room 1061
1025 W. Johnson Street
Madison, WI 53706

email: dkaplan at education.wisc.edu
Web:   http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
Phone: 608-262-0836
Fax:   608-262-0843


From b3i4old02 at sneakemail.com  Tue Jul  3 04:24:20 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Tue, 03 Jul 2007 03:24:20 +0100
Subject: [R] Lattice: shifting strips to left of axes
Message-ID: <f6cc16$p0a$1@sea.gmane.org>

Consider this plot:

xyplot(mpg ~ disp | cyl, mtcars, strip=F, strip.left=T, layout=c(1, 3),
        scales=list(relation="free"),
        par.settings=list(strip.background=list(col="transparent")))

I want to have the "cyl" strip labels on the left side of the axis. Is 
this possible?

Failing that, is it possible to remove the left axis and display it on 
the right instead, despite relation="free"?
-- 
Michael Hoffman


From deepayan.sarkar at gmail.com  Tue Jul  3 05:40:55 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Mon, 2 Jul 2007 20:40:55 -0700
Subject: [R] Lattice: shifting strips to left of axes
In-Reply-To: <f6cc16$p0a$1@sea.gmane.org>
References: <f6cc16$p0a$1@sea.gmane.org>
Message-ID: <eb555e660707022040qf4ec6d0p4479498600ceead2@mail.gmail.com>

On 7/2/07, Michael Hoffman <b3i4old02 at sneakemail.com> wrote:
> Consider this plot:
>
> xyplot(mpg ~ disp | cyl, mtcars, strip=F, strip.left=T, layout=c(1, 3),
>         scales=list(relation="free"),
>         par.settings=list(strip.background=list(col="transparent")))
>
> I want to have the "cyl" strip labels on the left side of the axis. Is
> this possible?

No. (It's possible to have a legend there, which could be used to put
row-specific ylab-s, for example, but it will be hard to make it look
like strips)

> Failing that, is it possible to remove the left axis and display it on
> the right instead, despite relation="free"?

Yes, as long as you are willing to manually manage the space required for it:

xyplot(mpg ~ disp | cyl, mtcars, strip=F, strip.left=T, layout=c(1, 3),
       scales=list(relation="free", y = list(draw = FALSE)),
       axis = function(side, ...) {
           if (side == "right")
               panel.axis(side = "right", outside = TRUE)
           else axis.default(side = side, ...)
       },
       par.settings=
       list(strip.background=list(col="transparent"),
            layout.widths = list(axis.key.padding = 5)))

-Deepayan


From adschai at optonline.net  Tue Jul  3 06:34:56 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Tue, 03 Jul 2007 04:34:56 +0000 (GMT)
Subject: [R] SVM for ordinal regression
Message-ID: <e4bff3372243b.4689d1f0@optonline.net>

Hi

I come across an idea of using SVM to fit ordinal label data rather than nominal ones. I am wondering if there is any pacakge that does that. Or if you could please recommend me the package that is flexible enough the let me do so, I'd really appreciate. Thank you.

- adschai


From jenifer at unt.edu  Tue Jul  3 06:53:50 2007
From: jenifer at unt.edu (Jenifer Larson-Hall)
Date: Mon, 02 Jul 2007 23:53:50 -0500
Subject: [R] MASS library rob.cov ellipse
Message-ID: <4689900E020000BB00013C4B@GWIA.unt.edu>

Hi,
I used to get a really useful graph when I ran the following command
using the MASS library:

> cov.rob(cbind(dekeyser$AGE,dekeyser$GJTSCORE),cor=T)

Besides the regular output, a graph appeared that had the classical
correlation and the robust correlation, and two ellipses, one
surrounding the data that would be used in the classical correlation and
the other surrounding the data in the robust correlation. 

I've searched through the MASS library but don't see a separate command
that could produce this graph. Does anyone know whether one exists, or
did the graph just disappear in the newer version of R?

Thanks for any help,

Dr. Jenifer Larson-Hall
Assistant Professor of Linguistics
University of North Texas
(940)369-8950


From ripley at stats.ox.ac.uk  Tue Jul  3 07:08:50 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jul 2007 06:08:50 +0100 (BST)
Subject: [R] Problem installing R packages in OpenBSD
In-Reply-To: <200707021519.33520.bthatcher@att.net>
References: <200707021519.33520.bthatcher@att.net>
Message-ID: <Pine.LNX.4.64.0707030604180.12715@gannet.stats.ox.ac.uk>

1) You are not supposed to have R_HOME set in your environment: that is 
the job of the R front-end script.

2) R 2.4.1 is obsolete, but I believe the relevant line is

. "${R_SHARE_DIR}/sh/dcf.sh"    # get_dcf_field()

R_SHARE_DIR is set in the R front-end script, so that's where to 
investigate.

On Mon, 2 Jul 2007, Bruce wrote:

> OS:  OpenBSD version 4.1 i386
> R version 2.4.1 (2006-12-18) installed as a binary package

The R project does not distribute binary packages for OpenBSD, so this is 
really an issue for whoever does.

> $ ls
> mapproj_1.1-7.1.tar.gz     maps_2.0-36.tar.gz
>
> $ sudo R CMD INSTALL mapproj_1.1-7.1.tar.gz
>
> /usr/local/lib/R/bin/INSTALL[118]:
>  .: /usr/obj/i386/R-2.4.1/fake-i386/usr/local/lib/R/share/sh/dcf.sh: not
> found
>
> I get the same error message from the R command prompt using
> install.packages("mapproj", dep=TRUE)
>
> Questions regarding  /usr/obj/i386/R-2.4.1/fake-i386:
> o Where did this path come from?
> o Is there a conflict between OpenBSD ports and R packages?
> o Is OpenBSD ports trying to do the package install?  If so, how do I stop it?
>
> Please note that I have installed R packages successfully on Linux, Windows XP
> and FreeBSD.
>
> Background information
> -------------------------------------------
> Environment variables:
> $ echo $R_HOME
> /usr/local/lib/R
> $ echo $R_LIBS
> /usr/local/lib/R/library
>
> R install:
> $ pwd
> /usr/local/lib/R/share/sh
> $ ls -l
> total 16
> -r--r--r--  1 root  bin   392 Mar  9 00:57 dcf.sh
> -r--r--r--  1 root  bin    27 Mar  9 00:57 echo.sh
> -r--r--r--  1 root  bin  1506 Mar  9 00:57 help-links.sh
> -r--r--r--  1 root  bin   825 Mar  9 00:57 help-print.sh
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jul  3 07:32:51 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jul 2007 06:32:51 +0100 (BST)
Subject: [R] MASS library rob.cov ellipse
In-Reply-To: <4689900E020000BB00013C4B@GWIA.unt.edu>
References: <4689900E020000BB00013C4B@GWIA.unt.edu>
Message-ID: <Pine.LNX.4.64.0707030617000.12715@gannet.stats.ox.ac.uk>

On Mon, 2 Jul 2007, Jenifer Larson-Hall wrote:

> Hi,
> I used to get a really useful graph when I ran the following command
> using the MASS library:
>
>> cov.rob(cbind(dekeyser$AGE,dekeyser$GJTSCORE),cor=T)
>
> Besides the regular output, a graph appeared that had the classical
> correlation and the robust correlation, and two ellipses, one
> surrounding the data that would be used in the classical correlation and
> the other surrounding the data in the robust correlation.

But all the data are 'used in the classical correlation'.  Maybe you mean 
a tolerance ellipse for the implied bivariate normal distribution?

> I've searched through the MASS library but don't see a separate command
> that could produce this graph. Does anyone know whether one exists, or
> did the graph just disappear in the newer version of R?

It was never in the MASS *package*.  cov.rob() is mainly intended for 
multidimensional problems, not 2D ones.   It is possible you were using 
methods from package 'robustbase', such as covPlot?


The author of cov.rob.


> Thanks for any help,

> Dr. Jenifer Larson-Hall
> Assistant Professor of Linguistics
> University of North Texas
> (940)369-8950

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ibaxter at purdue.edu  Tue Jul  3 07:59:45 2007
From: ibaxter at purdue.edu (Ivan Baxter)
Date: Tue, 03 Jul 2007 01:59:45 -0400
Subject: [R] loop causes syntax error in print()
Message-ID: <4689E5D1.2070202@purdue.edu>

I am having trouble printing a table out to the GUI display when the 
table is created and printed within a loop.

I get a "Error: syntax error message"

If I comment out the print statement, the loop runs fine and I can print 
out the last iteration of the table.

...[multiple loops and calculations ending with.....]...

+             print(paste(mutType,"sim",sim,"hmm",hmm))      
+             # print(acctab[,10:15])
+                
+             nummod <- nummod +1
+         }  #end hmmMats loop
+         }  #end tmats loop   
+         }  #end mut type loop
[1] "dup sim Imod hmm Jmod"

 > print(acctab[,10:15])
    hitrate falsepos   multrate      avghit avgmiss avgfalsepos
1 0.0000000        1        NaN         NaN       2          NA
2 1.0000000        0 0.00000000    5.333333     NaN          NA
3 0.0000000      NaN        NaN         NaN       9          NA
4 0.7777778        0 0.00000000   15.571429      11          NA
5 1.0000000        0 0.00000000   24.083333     NaN          NA
6 1.0000000        0 0.07692308   64.538462     NaN          NA
7 1.0000000        0 0.39207048 1088.454846     NaN          NA

you can see that
a) the print statement above it works
b) the print command works with this table

but if I uncomment it and try to run the loops again.....

+             print(paste(mutType,"sim",sim,"hmm",hmm))         
              print(acctab[,10:15])
Error: syntax error
 >        
 >                
 >             nummod <- nummod +1
 >         }  #end hmmMats loop
Error: syntax error
 >         }  #end tmats loop   
Error: syntax error
 >         }  #end mut type loop
Error: syntax error


I have tried print.data.frame, but that doesn't work either....


Any suggestions would be appreciated  (session info below)


Ivan


 > sessionInfo()
R version 2.5.1 (2007-06-27)
i386-apple-darwin8.9.1

locale:
C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  
"methods"   "base"  

-- 
**************************************************************
Ivan Baxter
Research Scientist
Bindley Bioscience Center
Purdue University
Office: Hort 305
765-543-7288
ibaxter at purdue.edu


From dieter.menne at menne-biomed.de  Tue Jul  3 08:45:19 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 3 Jul 2007 08:45:19 +0200
Subject: [R] What is parameter "display" (windows/grDevices)
Message-ID: <LPEJLJACLINDNMBMFAFIMEFPCIAA.dieter.menne@menne-biomed.de>

In the docs of window/grDevices:

>All these devices are implemented as windows devices, the _display_
parameter selects which is actually used.>>

What is the "display" parameter?


Dieter Menne


From res90sx5 at verizon.net  Tue Jul  3 08:47:54 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Mon, 02 Jul 2007 23:47:54 -0700
Subject: [R] MatchIt package on Ubuntu 7.04 (Feisty Fawn)
Message-ID: <024401c7bd3e$159cad40$0201a8c0@Aragorn>

UseRs,

I tried to install the MatchIt package on Ubuntu 7.04.   When loading required dependencies I got the following error message:

Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library '/usr/local/lib/R/site-library/optmatch/libs/optmatch.so':
  /usr/local/lib/R/site-library/optmatch/libs/optmatch.so: cannot map zero-fill pages: Cannot allocate memory
Error: package/namespace load failed for 'optmatch'

optmatch.so appears to be where R is looking for it so I don't know what has gone wrong.   Have I missed something in the installation process, or is this a known Ubuntu problem, or a licensing issue?  Should I direct this question to the maintainer?

I have installed MatchIt along with optmatch on a WinXP Pro system without any problem.   Any assistance/suggestions are welcome.  I running R-2.5.1 and downloading packages from the CRAN mirror at FHCRC in Seattle, WA, USA.

Dan

Daniel Nordlund
Bothell, WA USA


From i.visser at uva.nl  Tue Jul  3 08:55:50 2007
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 3 Jul 2007 08:55:50 +0200
Subject: [R] help again
In-Reply-To: <904d37580707020922k10303719hbfb06a5a115ce375@mail.gmail.com>
References: <904d37580707020922k10303719hbfb06a5a115ce375@mail.gmail.com>
Message-ID: <87DEA982-9CBA-4333-AC25-A251F6D92AF9@uva.nl>


On Jul 2, 2007, at 6:22 PM, umarporn charusombat wrote:

> hi
> i try to use arima and holtwinter to predict drought from 1895-2006

unless you have access to time-travel isn't this better called  
retrodiction?

ingmar

> but i cannot read whole period of time and i try to do the exponent  
> fitting,
> but it comes out as the coordinate x-y error
> i send the source code and data to take a look
> if anyone can help me, i am really new in R
>
> thank u so much
> jam
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Tue Jul  3 09:06:56 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Jul 2007 08:06:56 +0100 (BST)
Subject: [R] What is parameter "display" (windows/grDevices)
In-Reply-To: <LPEJLJACLINDNMBMFAFIMEFPCIAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFIMEFPCIAA.dieter.menne@menne-biomed.de>
Message-ID: <Pine.LNX.4.64.0707030802100.15880@gannet.stats.ox.ac.uk>

On Tue, 3 Jul 2007, Dieter Menne wrote:

> In the docs of window/grDevices:

Do you mean 'windows' (sic) in package 'grDevices'?

>
>> All these devices are implemented as windows devices, the _display_
> parameter selects which is actually used.>>
>
> What is the "display" parameter?

The second argument to the .External call to "devga".
Take a closer look at the R code to see what the comment is talking about.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From linpan1975 at yahoo.com  Mon Jul  2 23:23:17 2007
From: linpan1975 at yahoo.com (Lin Pan)
Date: Mon, 2 Jul 2007 14:23:17 -0700 (PDT)
Subject: [R] how to use mle with a defined function
Message-ID: <11402268.post@talk.nabble.com>


Hi all,

I am trying to use mle() to find a self-defined function. Here is my
function:

test <- function(a=0.1, b=0.1, c=0.001, e=0.2){

# omega is the known covariance matrix, Y is the response vector, X is the
explanatory matrix

odet = unlist(determinant(omega))[1]
 
# do cholesky decomposition

C = chol(omega)

# transform data

U = t(C)%*%Y
WW=t(C)%*%X

beta = lm(U~W)$coef

Z=Y-X%*%beta
V=solve(t(C), Z)

0.5*odet + 0.5*(t(V)%*%V)

}

and I am trying to call mle() to calculate the maximum likelihood estimates
for function (0.5*odet+0.5*(t(V)%*%V)) by

result = mle(test, method="Nelder-Mead")

But I get the following error message:

Error in optim(start, f, method = method, hessian = TRUE, ...) : 
        (list) object cannot be coerced to 'double'

I am pretty sure that the matrices, parameters etc are numerical before
importing to the function. But why I still get such error message? Could
anybody give some help on this? thanks a lot.


Lin
-- 
View this message in context: http://www.nabble.com/how-to-use-mle-with-a-defined-function-tf4015002.html#a11402268
Sent from the R help mailing list archive at Nabble.com.


From peter.robinson at charite.de  Tue Jul  3 11:20:45 2007
From: peter.robinson at charite.de (Dr. med. Peter Robinson)
Date: Tue, 3 Jul 2007 11:20:45 +0200 (CEST)
Subject: [R] Plotting very skewed data in barplot
Message-ID: <56860.87.187.103.4.1183454445.squirrel@webmail.charite.de>

Dear R'ers,

I would like to use barplot or a similar function to plot data
demonstrating the distribution of the length of a kind of conservation in
about 25000 DNA sequences. My data look like this:

#Total sequences: 23873
0	19936
1	218
2	391
3	477
4	360
5	431
6	294
7	215
8	320
9	209
10	160
 (.....)
99	0
100	1
101	0
102	0
103	1
104	0
105	0
106	0
107	0
108	0
109	1


Therefore, I would like to show the column representing 0 (with 19936
sequences) "cut" so it doesn't dominate the rest of the plot. Also,
starting from about 10 sequences, I would like to group the rest of the
sequences into groups of 5 each (for instance, 10-15, 16-20, 21-25 etc).

I have looked extensively in the help pages and in some online fora, but
have not found an answer to this question. I would greatly appreciate any
tips. Thanks, Peter


From lami at faunalia.it  Tue Jul  3 11:33:33 2007
From: lami at faunalia.it (Leonardo Lami)
Date: Tue, 03 Jul 2007 11:33:33 +0200
Subject: [R] select row
Message-ID: <468A17ED.7000607@faunalia.it>

Hi all,
I have a little problem selecting some rows from a data.frame.
I'd like to select the rows where a determinated column take a
partivolar value.

For example:
tab
          id       x       y
1    24-2005 1621814 4834991
2    24-2005 1621856 4834907
3    24-2005 1621763 4834956
4   25-2006 1622330 4835189
5   25-2006 1622533 4834834
6   25-2006 1622535 4834909
7   25-2006 1622543 4834803
8   28-2005 1622798 4835043
9   28-2005 1622299 4835129

I'd like to select the row where id=25-2006

I searched on the search of the R site but I did'nt find anything of simple.
Can someone help me?

Thank you very much
Leonardo


From Jon.Butchar at osumc.edu  Tue Jul  3 11:36:38 2007
From: Jon.Butchar at osumc.edu (Butchar, Jon)
Date: Tue, 3 Jul 2007 05:36:38 -0400
Subject: [R] Problem installing R packages in OpenBSD
References: <200707021519.33520.bthatcher@att.net>
Message-ID: <CE0706430C3B1642B60076B6A4A12F6619944D@msxc05.OSUMC.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/d58b6cc1/attachment.pl 

From Thierry.ONKELINX at inbo.be  Tue Jul  3 11:40:51 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 3 Jul 2007 11:40:51 +0200
Subject: [R] select row
In-Reply-To: <468A17ED.7000607@faunalia.it>
References: <468A17ED.7000607@faunalia.it>
Message-ID: <2E9C414912813E4EB981326983E0A104032D953C@inboexch.inbo.be>

Assuming that id is a character.

tab[tab$id == "25-2006", ]

Cheers,

Thierry
------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Leonardo Lami
> Verzonden: dinsdag 3 juli 2007 11:34
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] select row
> 
> Hi all,
> I have a little problem selecting some rows from a data.frame.
> I'd like to select the rows where a determinated column take 
> a partivolar value.
> 
> For example:
> tab
>           id       x       y
> 1    24-2005 1621814 4834991
> 2    24-2005 1621856 4834907
> 3    24-2005 1621763 4834956
> 4   25-2006 1622330 4835189
> 5   25-2006 1622533 4834834
> 6   25-2006 1622535 4834909
> 7   25-2006 1622543 4834803
> 8   28-2005 1622798 4835043
> 9   28-2005 1622299 4835129
> 
> I'd like to select the row where id=25-2006
> 
> I searched on the search of the R site but I did'nt find 
> anything of simple.
> Can someone help me?
> 
> Thank you very much
> Leonardo
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Tue Jul  3 11:43:37 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 3 Jul 2007 05:43:37 -0400 (EDT)
Subject: [R] select row
In-Reply-To: <468A17ED.7000607@faunalia.it>
Message-ID: <998936.50820.qm@web32803.mail.mud.yahoo.com>

 ?which

Use it to find the rows and then extract the rows 

 selection  <- mydata[which(mydata$id=="25-2006"), ]

 should work.

--- Leonardo Lami <lami at faunalia.it> wrote:

> Hi all,
> I have a little problem selecting some rows from a
> data.frame.
> I'd like to select the rows where a determinated
> column take a
> partivolar value.
> 
> For example:
> tab
>           id       x       y
> 1    24-2005 1621814 4834991
> 2    24-2005 1621856 4834907
> 3    24-2005 1621763 4834956
> 4   25-2006 1622330 4835189
> 5   25-2006 1622533 4834834
> 6   25-2006 1622535 4834909
> 7   25-2006 1622543 4834803
> 8   28-2005 1622798 4835043
> 9   28-2005 1622299 4835129
> 
> I'd like to select the row where id=25-2006
> 
> I searched on the search of the R site but I did'nt
> find anything of simple.
> Can someone help me?
> 
> Thank you very much
> Leonardo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From scruveil at genoscope.cns.fr  Tue Jul  3 11:46:41 2007
From: scruveil at genoscope.cns.fr (Stephane Cruveiller)
Date: Tue, 03 Jul 2007 11:46:41 +0200
Subject: [R] possible bug in ggplot2 v0.5.2???
Message-ID: <468A1B01.9090100@genoscope.cns.fr>

Dear R-Users,

I recently gave a try to the nice package ggplot2. Everything  went
well until I tried to add a smoother (using lm method for instance).
On the graphic device the regression line is displayed but not confidence
intervals as it should be (at least on ggplot website). I tried to do 
the job on
both MS winXP and Linux i586: same result. Did anyone encountered this
problem? Did I miss something?


My R version is 2.4.1.



Thanks,

St?phane.


-- 
==========================================================
Stephane CRUVEILLER Ph. D.
Genoscope - Centre National de Sequencage
Atelier de Genomique Comparative
2, Rue Gaston Cremieux   CP 5706
91057 Evry Cedex - France
Phone: +33 (0)1 60 87 84 58
Fax: +33 (0)1 60 87 25 14
EMails: scruveil at genoscope.cns.fr ,scruvell at infobiogen.fr
===========================================================


From lami at faunalia.it  Tue Jul  3 12:13:26 2007
From: lami at faunalia.it (Leonardo Lami)
Date: Tue, 03 Jul 2007 12:13:26 +0200
Subject: [R] select row
In-Reply-To: <998936.50820.qm@web32803.mail.mud.yahoo.com>
References: <998936.50820.qm@web32803.mail.mud.yahoo.com>
Message-ID: <468A2146.30803@faunalia.it>

Thanks all of you for the help!
>   
>> Hi all,
>> I have a little problem selecting some rows from a
>> data.frame.
>> I'd like to select the rows where a determinated
>> column take a
>> partivolar value.
>>
>> For example:
>> tab
>>           id       x       y
>> 1    24-2005 1621814 4834991
>> 2    24-2005 1621856 4834907
>> 3    24-2005 1621763 4834956
>> 4   25-2006 1622330 4835189
>> 5   25-2006 1622533 4834834
>> 6   25-2006 1622535 4834909
>> 7   25-2006 1622543 4834803
>> 8   28-2005 1622798 4835043
>> 9   28-2005 1622299 4835129
>>
>> I'd like to select the row where id=25-2006
>>
>> I searched on the search of the R site but I did'nt
>> find anything of simple.
>> Can someone help me?
>>
>> Thank you very much
>> Leonardo
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained,
>> reproducible code.
>>
>>     
>
>
>
>       Get a sneak peak at messages with a handy reading pane with All new Yahoo! Mail: http://mrd.mail.yahoo.com/try_beta?.intl=ca
>
>


From h.wickham at gmail.com  Tue Jul  3 12:14:29 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 3 Jul 2007 12:14:29 +0200
Subject: [R] possible bug in ggplot2 v0.5.2???
In-Reply-To: <468A1B01.9090100@genoscope.cns.fr>
References: <468A1B01.9090100@genoscope.cns.fr>
Message-ID: <f8e6ff050707030314h21fa2898m1dc1ce35b91dbb54@mail.gmail.com>

Hi Stephane,

The problem is that the windows graphics device doesn't support
transparent colours.  You can get around this in two ways:

 * export to a device that does support transparency (eg. pdf)
 * use a solid fill colour : + stat_smooth(method="lm", fill="grey50")

Hadley

On 7/3/07, Stephane Cruveiller <scruveil at genoscope.cns.fr> wrote:
> Dear R-Users,
>
> I recently gave a try to the nice package ggplot2. Everything  went
> well until I tried to add a smoother (using lm method for instance).
> On the graphic device the regression line is displayed but not confidence
> intervals as it should be (at least on ggplot website). I tried to do
> the job on
> both MS winXP and Linux i586: same result. Did anyone encountered this
> problem? Did I miss something?
>
>
> My R version is 2.4.1.
>
>
>
> Thanks,
>
> St?phane.
>
>
> --
> ==========================================================
> Stephane CRUVEILLER Ph. D.
> Genoscope - Centre National de Sequencage
> Atelier de Genomique Comparative
> 2, Rue Gaston Cremieux   CP 5706
> 91057 Evry Cedex - France
> Phone: +33 (0)1 60 87 84 58
> Fax: +33 (0)1 60 87 25 14
> EMails: scruveil at genoscope.cns.fr ,scruvell at infobiogen.fr
> ===========================================================
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From yn19832 at msn.com  Tue Jul  3 12:51:07 2007
From: yn19832 at msn.com (livia)
Date: Tue, 3 Jul 2007 03:51:07 -0700 (PDT)
Subject: [R] Exponentially Weighted Moving Average
Message-ID: <11409874.post@talk.nabble.com>


Hi, I have got a series of data "x" and some parameter "a", and I would like
to take some Exponentially Weighted Moving Average to the data in the
following fomula, and obtain the return series y

y1=a^265*x[2]+a^264*x[3]+a^263*x[4]+...+a^0*x[267]

y2=a^264*x[4]+a^263*x[5]+a^263*x[6]+...+a^0*x[268]

....

y265=a^1*x[530]+a^0*x[531]

y266=a^0*x[532]

Could anyone give me some advice how can I achieve this?
Many thanks

-- 
View this message in context: http://www.nabble.com/Exponentially-Weighted-Moving-Average-tf4017572.html#a11409874
Sent from the R help mailing list archive at Nabble.com.


From jim at bitwrit.com.au  Tue Jul  3 13:30:39 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 03 Jul 2007 21:30:39 +1000
Subject: [R] Plotting very skewed data in barplot
In-Reply-To: <56860.87.187.103.4.1183454445.squirrel@webmail.charite.de>
References: <56860.87.187.103.4.1183454445.squirrel@webmail.charite.de>
Message-ID: <468A335F.6050109@bitwrit.com.au>

Dr. med. Peter Robinson wrote:
> Dear R'ers,
> 
> I would like to use barplot or a similar function to plot data
> demonstrating the distribution of the length of a kind of conservation in
> about 25000 DNA sequences. My data look like this:
> ...
> 
> Therefore, I would like to show the column representing 0 (with 19936
> sequences) "cut" so it doesn't dominate the rest of the plot. Also,
> starting from about 10 sequences, I would like to group the rest of the
> sequences into groups of 5 each (for instance, 10-15, 16-20, 21-25 etc).
> 

  Hi Peter,

Have a look at gap.barplot in the plotrix package. I would suggest 
something like this:

gap.barplot(y,c(500,19800),main="Skewed distribution",
  yaxlab=c(200,400,19900),ytics=c(200,400,19900))

where y is the right column of your data.

Jim


From pietrzyk at research.ge.com  Tue Jul  3 14:22:30 2007
From: pietrzyk at research.ge.com (Pietrzykowski, Matthew (GE, Research))
Date: Tue, 3 Jul 2007 08:22:30 -0400
Subject: [R] The R Book by M. J. Crawley
Message-ID: <1EB58414BAB4014DB2C3E289FDF55FBB019A0A30@CINMLVEM15.e2k.ad.ge.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/11ca2300/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Jul  3 14:33:05 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 03 Jul 2007 14:33:05 +0200
Subject: [R] The R Book by M. J. Crawley
In-Reply-To: <1EB58414BAB4014DB2C3E289FDF55FBB019A0A30@CINMLVEM15.e2k.ad.ge.com>
References: <1EB58414BAB4014DB2C3E289FDF55FBB019A0A30@CINMLVEM15.e2k.ad.ge.com>
Message-ID: <468A4201.1080401@statistik.uni-dortmund.de>



Pietrzykowski, Matthew (GE, Research) wrote:
> Hello all-
> 
> I would appreciate any guidance that can be provided.  I am new to R and
> am 
> using it exclusively in a statistics program I am undertaking that
> mainly references
> Minitab.  My focus is on data modeling and further more multivariate
> data analysis
> as much of my work in involves chemical measurements from custom sensors
> using
> all sorts of transduction methods.   I am looking for a reference that
> has sound statistical
> foundations with relevant R commands as well as multivariate support.  I
> saw the new book,
> "The R Book", by Michael J. Crawley and wanted to know what R users
> thoughts of it.

The author seems to be an expert in (almost?) all available statistical 
programming languages and able to write almost 1000 pages about these 
languages. He also seems to be a perfect R programmer, since the title 
is "The R book".

Uwe Ligges


> Thanks in advance,
> 
> Matt
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Bernhard.Klingenberg at williams.edu  Tue Jul  3 14:37:29 2007
From: Bernhard.Klingenberg at williams.edu (Bernhard Klingenberg)
Date: Tue, 03 Jul 2007 14:37:29 +0200
Subject: [R] generating correlated Bernoulli random variables
Message-ID: <468A4309.5020506@williams.edu>

>
> > Hi all,
> > I was wondering how to generate samples for two RVs X1 and X2.
> >
> > X1 ~ Bernoulli (p1)
> > X2 ~ Bernoulli (p2)
> >
> > Also, X1 and X2 are correlated with correlation \rho.
>   

You can use the rmvbin() function in the bindata package, e.g.,

require(bindata)
n=10
p1=0.5
p2=0.3
rho=0.2
rmvbin(n, c(p1,p2), bincorr=(1-rho)*diag(2)+rho)
?rmvbin

However, as pointed out before, rho is bounded below and above by some 
function of the marginal probabilities. (Try above code with rho=0.9)
You may want to use the odds ratio (which is unrestricted) to specify 
the association between the two binary variables and then convert this 
odds ratio, for given marginal probabilities p1 and p2, into a (valid) 
correlation rho to be used in rmvbin().

Here is some ad hoc code to do that:

bincorr <- function(OR, p1, p2) {    #from odds ratio to binary correlation
    if (OR==1) p11=p2-p2+p1*p2 else {
        p11_1=p2-(1/2/(1-OR)*(1-p1+OR*p1+p2-OR*p2-
              sqrt((-1+p1-OR*p1-p2+OR*p2)^2-4*(1-OR)*(p2-p1*p2))))
        p11_2=p2-(1/2/(1-OR)*(1-p1+OR*p1+p2-OR*p2-
              sqrt((-1+p1-OR*p1-p2+OR*p2)^2)-4*(1-OR)*(p2-p1*p2)))
        if (p11_1>0 && p11_1<=p1 && p11_1<p2) p11=p11_1 else p11=p11_2
    }
    bincorr=(p11-p1*p2)/sqrt(p1*(1-p1)*p2*(1-p2))
    return(bincorr)
}

For instance, try

sapply(c(0,0.5,1,1.5,3,10,100),function(x) bincorr(x,p1,p2))

to see the range of valid correlations for odds ratios between 0 and 
100, with p1 and p2 as above.

Bernhard Klingenberg
Dept. of Mathematics and Statistics
Williams College, MA
www.williams.edu/~bklingen


From murdoch at stats.uwo.ca  Tue Jul  3 14:48:49 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 03 Jul 2007 08:48:49 -0400
Subject: [R] loop causes syntax error in print()
In-Reply-To: <4689E5D1.2070202@purdue.edu>
References: <4689E5D1.2070202@purdue.edu>
Message-ID: <468A45B1.3030304@stats.uwo.ca>

On 7/3/2007 1:59 AM, Ivan Baxter wrote:
> I am having trouble printing a table out to the GUI display when the 
> table is created and printed within a loop.
> 
> I get a "Error: syntax error message"
> 
> If I comment out the print statement, the loop runs fine and I can print 
> out the last iteration of the table.

You should simplify your loop until it's something you can post for us 
to try.  Chances are you'll notice the error when you do that, but if 
not, someone else will be able to tell you what's going on.

Without a reproducible example, it's more or less hopeless.

Duncan Murdoch

> 
> ...[multiple loops and calculations ending with.....]...
> 
> +             print(paste(mutType,"sim",sim,"hmm",hmm))      
> +             # print(acctab[,10:15])
> +                
> +             nummod <- nummod +1
> +         }  #end hmmMats loop
> +         }  #end tmats loop   
> +         }  #end mut type loop
> [1] "dup sim Imod hmm Jmod"
> 
>  > print(acctab[,10:15])
>     hitrate falsepos   multrate      avghit avgmiss avgfalsepos
> 1 0.0000000        1        NaN         NaN       2          NA
> 2 1.0000000        0 0.00000000    5.333333     NaN          NA
> 3 0.0000000      NaN        NaN         NaN       9          NA
> 4 0.7777778        0 0.00000000   15.571429      11          NA
> 5 1.0000000        0 0.00000000   24.083333     NaN          NA
> 6 1.0000000        0 0.07692308   64.538462     NaN          NA
> 7 1.0000000        0 0.39207048 1088.454846     NaN          NA
> 
> you can see that
> a) the print statement above it works
> b) the print command works with this table
> 
> but if I uncomment it and try to run the loops again.....
> 
> +             print(paste(mutType,"sim",sim,"hmm",hmm))         
>               print(acctab[,10:15])
> Error: syntax error
>  >        
>  >                
>  >             nummod <- nummod +1
>  >         }  #end hmmMats loop
> Error: syntax error
>  >         }  #end tmats loop   
> Error: syntax error
>  >         }  #end mut type loop
> Error: syntax error
> 
> 
> I have tried print.data.frame, but that doesn't work either....
> 
> 
> Any suggestions would be appreciated  (session info below)
> 
> 
> Ivan
> 
> 
>  > sessionInfo()
> R version 2.5.1 (2007-06-27)
> i386-apple-darwin8.9.1
> 
> locale:
> C
> 
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  
> "methods"   "base"  
>


From bthatcher at att.net  Tue Jul  3 15:17:17 2007
From: bthatcher at att.net (Bruce)
Date: Tue, 3 Jul 2007 07:17:17 -0600
Subject: [R] Problem installing R packages in OpenBSD
In-Reply-To: <CE0706430C3B1642B60076B6A4A12F6619944D@msxc05.OSUMC.EDU>
References: <200707021519.33520.bthatcher@att.net>
	<CE0706430C3B1642B60076B6A4A12F6619944D@msxc05.OSUMC.EDU>
Message-ID: <200707030717.18591.bthatcher@att.net>

Thank you both for your responses.

I am now able to install R packages.

The front-end R scripts were changed at /usr/local/bin 
and /usr/local/lib/R/bin as follows:
From:
R_HOME_DIR=/usr/obj/i386/R-2.4.1/fake-i386/usr/local/lib/R
R_SHARE_DIR=/usr/obj/i386/R-2.4.1/fake-i386/usr/local/lib/R/share
export R_SHARE_DIR
R_INCLUDE_DIR=/usr/obj/i386/R-2.4.1/fake-i386/usr/local/lib/R/include
export R_INCLUDE_DIR
R_DOC_DIR=/usr/obj/i386/R-2.4.1/fake-i386/usr/local/lib/R/doc
export R_DOC_DIR
To:
R_HOME_DIR=/usr/local/lib/R
R_SHARE_DIR=/usr/local/lib/R/share
export R_SHARE_DIR
R_INCLUDE_DIR=/usr/local/lib/R/include
export R_INCLUDE_DIR
R_DOC_DIR=/usr/local/lib/R/doc
export R_DOC_DIR

The R_HOME environment variable has been removed from my .profile 
and /etc/profile.

Bruce

On Tuesday 03 July 2007 03:36:38 am you wrote:
> Hello.
>
> It's been quite a while since using OpenBSD (and no OpenBSD here for me to
> check this out), but I do remember having to reset some of the path
> entries. Can you check the directory settings in the main R script and see
> if that's where the "fake-..." is coming from?  This looks like a setting
> for the OpenBSD package creation that didn't get changed after package
> installation.
>
> Hope this helps,
>
> Jon
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch on behalf of Bruce
> Sent: Mon 7/2/2007 5:19 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Problem installing R packages in OpenBSD
>
> OS:  OpenBSD version 4.1 i386
> R version 2.4.1 (2006-12-18) installed as a binary package
>
> $ ls
> mapproj_1.1-7.1.tar.gz     maps_2.0-36.tar.gz
>
> $ sudo R CMD INSTALL mapproj_1.1-7.1.tar.gz
>
> /usr/local/lib/R/bin/INSTALL[118]:
>   .: /usr/obj/i386/R-2.4.1/fake-i386/usr/local/lib/R/share/sh/dcf.sh: not
> found
>
> I get the same error message from the R command prompt using
> install.packages("mapproj", dep=TRUE)
>
> Questions regarding  /usr/obj/i386/R-2.4.1/fake-i386:
> o Where did this path come from?
> o Is there a conflict between OpenBSD ports and R packages?
> o Is OpenBSD ports trying to do the package install?  If so, how do I stop
> it?
>
> Please note that I have installed R packages successfully on Linux, Windows
> XP
> and FreeBSD.
>
> Background information
> -------------------------------------------
> Environment variables:
> $ echo $R_HOME
> /usr/local/lib/R
> $ echo $R_LIBS
> /usr/local/lib/R/library
>
> R install:
> $ pwd
> /usr/local/lib/R/share/sh
> $ ls -l
> total 16
> -r--r--r--  1 root  bin   392 Mar  9 00:57 dcf.sh
> -r--r--r--  1 root  bin    27 Mar  9 00:57 echo.sh
> -r--r--r--  1 root  bin  1506 Mar  9 00:57 help-links.sh
> -r--r--r--  1 root  bin   825 Mar  9 00:57 help-print.sh
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From yn19832 at msn.com  Tue Jul  3 15:36:53 2007
From: yn19832 at msn.com (livia)
Date: Tue, 3 Jul 2007 06:36:53 -0700 (PDT)
Subject: [R] EWMA procedure to forecast variance
Message-ID: <11412324.post@talk.nabble.com>


Hello,

I would like to use the Exponential Weighted Moving Average procedure to get
the variance. Is there any R function for doing this?

Many thanks.
-- 
View this message in context: http://www.nabble.com/EWMA-procedure-to-forecast-variance-tf4018317.html#a11412324
Sent from the R help mailing list archive at Nabble.com.


From davidr at rhotrading.com  Tue Jul  3 15:42:14 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 3 Jul 2007 08:42:14 -0500
Subject: [R] Exponentially Weighted Moving Average
In-Reply-To: <11409874.post@talk.nabble.com>
References: <11409874.post@talk.nabble.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE77395243@rhopost.rhotrading.com>

You could use rollFun from fMultivar package.
You need to define your EWMA function separately.
(Usually the EWMA moves along with a constant window size, though ....)

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of livia
Sent: Tuesday, July 03, 2007 5:51 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Exponentially Weighted Moving Average


Hi, I have got a series of data "x" and some parameter "a", and I would
like
to take some Exponentially Weighted Moving Average to the data in the
following fomula, and obtain the return series y

y1=a^265*x[2]+a^264*x[3]+a^263*x[4]+...+a^0*x[267]

y2=a^264*x[4]+a^263*x[5]+a^263*x[6]+...+a^0*x[268]

....

y265=a^1*x[530]+a^0*x[531]

y266=a^0*x[532]

Could anyone give me some advice how can I achieve this?
Many thanks

-- 
View this message in context:
http://www.nabble.com/Exponentially-Weighted-Moving-Average-tf4017572.ht
ml#a11409874
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From sunnyside500 at gmail.com  Tue Jul  3 15:52:46 2007
From: sunnyside500 at gmail.com (runner)
Date: Tue, 3 Jul 2007 06:52:46 -0700 (PDT)
Subject: [R] regexpr
In-Reply-To: <914427.39695.qm@web39712.mail.mud.yahoo.com>
References: <11363041.post@talk.nabble.com>
	<914427.39695.qm@web39712.mail.mud.yahoo.com>
Message-ID: <11412603.post@talk.nabble.com>


using lapply is so great. That help me a lot.
thanks.


Stephen Tucker wrote:
> 
> I think you are looking for paste().
> 
> And you can replace your for loop with lapply(), which will apply regexpr
> to
> every element of 'mylist' (as the first argument, which is 'pattern').
> 'text'
> can be a vector also:
> 
> mylist <- c("MN","NY","FL")
> lapply(paste(mylist,"$",sep=""),regexpr,text="Those from MN:")
> 
> 
> 
> --- runner <sunnyside500 at gmail.com> wrote:
> 
>> 
>> Hi, 
>> 
>> I 'd like to match each member of a list to a target string, e.g.
>> ------------------------------
>> mylist=c("MN","NY","FL")
>> g=regexpr(mylist[1], "Those from MN:")
>> if (g>0)
>> {
>> "On list"
>> }
>> ------------------------------
>> My question is:
>> 
>> How to add an end-of-string symbol '$' to the to-match string? so that
>> 'M'
>> won't match.
>> 
>> Of course, "MN$" will work, but i want to use it in a loop; "mylist[i]"
>> is
>> what i need. I tried "mylist[1]$", but didn't work. So why it doesn't
>> extrapolate? How to do it?
>> 
>> Thanks a lot!
>> -- 
>> View this message in context:
>> http://www.nabble.com/regexpr-tf4000743.html#a11363041
>> Sent from the R help mailing list archive at Nabble.com.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 
> 
>  
> ____________________________________________________________________________________
> Bored stiff? Loosen up...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/regexpr-tf4000743.html#a11412603
Sent from the R help mailing list archive at Nabble.com.


From edd at debian.org  Tue Jul  3 15:56:42 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 3 Jul 2007 08:56:42 -0500
Subject: [R] MatchIt package on Ubuntu 7.04 (Feisty Fawn)
In-Reply-To: <024401c7bd3e$159cad40$0201a8c0@Aragorn>
References: <024401c7bd3e$159cad40$0201a8c0@Aragorn>
Message-ID: <18058.21914.430998.829044@basebud.nulle.part>


On 2 July 2007 at 23:47, Daniel Nordlund wrote:
| I tried to install the MatchIt package on Ubuntu 7.04. 

How? Via 'sudo apt-get install r-cran-matchit', or using R?

| When loading required dependencies I got the following error message:
| 
| Error in dyn.load(x, as.logical(local), as.logical(now)) :
|         unable to load shared library
| '/usr/local/lib/R/site-library/optmatch/libs/optmatch.so':

/usr/local, so you did this using via 'R CMD INSTALL ' or from inside
R. Consider removing this directory and install the Ubuntu-supplied package. 
Also, you didn't mention whether this is an older install of matchit, or
whether you just did it.

|   /usr/local/lib/R/site-library/optmatch/libs/optmatch.so: cannot map zero-fill pages: Cannot allocate memory
| Error: package/namespace load failed for 'optmatch'
| 
| optmatch.so appears to be where R is looking for it so I don't know what
| has gone wrong.   Have I missed something in the installation process, or

Looks like a runtime error, possible due to mismatched libraries. 

| is this a known Ubuntu problem, or a licensing issue?  Should I direct this
| question to the maintainer? 

Possibly, the r-sig-debian list is also 'on topic' for Debian/Ubuntu questions.

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From wayne.w.jones at shell.com  Tue Jul  3 16:03:32 2007
From: wayne.w.jones at shell.com (Wayne_Betws)
Date: Tue, 3 Jul 2007 07:03:32 -0700 (PDT)
Subject: [R] R Icon dulled out
Message-ID: <11412808.post@talk.nabble.com>


Hi there, 

For some strange reason the R icon on my quickstart menu and desktop icon
has suddenly dulled out to grey instead of the usual blue colour! I didnt do
anything out of the ordinary which might cause this! R seems to be working
fine but this problem is still bugging me!!
I have tried uninstalling and reinstalling R but with no luck!
Has anyone else had the same problem?

version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          5.0                         
year           2007                        
month          04                          
day            23                          
svn rev        41293                       
language       R                           
version.string R version 2.5.0 (2007-04-23)


Regards

Wayne

-- 
View this message in context: http://www.nabble.com/R-Icon-dulled-out-tf4018482.html#a11412808
Sent from the R help mailing list archive at Nabble.com.


From benoitchemineau at gmail.com  Tue Jul  3 16:12:05 2007
From: benoitchemineau at gmail.com (Benoit Chemineau)
Date: Tue, 3 Jul 2007 16:12:05 +0200
Subject: [R] how to get the position of an element in a vector ?
Message-ID: <50c8fbc90707030712v3e9a878fx698f77717c93dbc1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/cd4bd78d/attachment.pl 

From amsa36060 at yahoo.com  Tue Jul  3 16:20:28 2007
From: amsa36060 at yahoo.com (Amir Safari)
Date: Tue, 3 Jul 2007 07:20:28 -0700 (PDT)
Subject: [R] EWMA procedure to forecast variance
In-Reply-To: <11412324.post@talk.nabble.com>
Message-ID: <769223.64223.qm@web60422.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/775f8a67/attachment.pl 

From ibaxter at purdue.edu  Tue Jul  3 16:23:40 2007
From: ibaxter at purdue.edu (Ivan Baxter)
Date: Tue, 03 Jul 2007 10:23:40 -0400
Subject: [R] loop causes syntax error in print()
In-Reply-To: <468A45B1.3030304@stats.uwo.ca>
References: <4689E5D1.2070202@purdue.edu> <468A45B1.3030304@stats.uwo.ca>
Message-ID: <468A5BEC.1000407@purdue.edu>



Duncan Murdoch wrote:
> On 7/3/2007 1:59 AM, Ivan Baxter wrote:
>> I am having trouble printing a table out to the GUI display when the 
>> table is created and printed within a loop.
>>
>> I get a "Error: syntax error message"
>>
>> If I comment out the print statement, the loop runs fine and I can 
>> print out the last iteration of the table.
>
> You should simplify your loop until it's something you can post for us 
> to try.  Chances are you'll notice the error when you do that, but if 
> not, someone else will be able to tell you what's going on.
>
> Without a reproducible example, it's more or less hopeless.
>
> Duncan Murdoch
Right- well I did as you suggested and seem to have fixed the problem.

The problem appeared to be that some carriage returns were not 
registering in the Mac Os X R editor. So even though it looked like it 
was on a new line, it wasn't. It wasn't a line wrap issue, these were 
actually returns that I had hit that just weren't registering. I had one 
chunk of code which showed what I thought was the problem when R 
crashed. When I opened up the script again after restarting, the problem 
disappeared, so somehow the returns started to be seen.

thanks for your help.

Ivan




>
>>
>> ...[multiple loops and calculations ending with.....]...
>>
>> +             print(paste(mutType,"sim",sim,"hmm",hmm))      
>> +             # print(acctab[,10:15])
>> +                +             nummod <- nummod +1
>> +         }  #end hmmMats loop
>> +         }  #end tmats loop   +         }  #end mut type loop
>> [1] "dup sim Imod hmm Jmod"
>>
>>  > print(acctab[,10:15])
>>     hitrate falsepos   multrate      avghit avgmiss avgfalsepos
>> 1 0.0000000        1        NaN         NaN       2          NA
>> 2 1.0000000        0 0.00000000    5.333333     NaN          NA
>> 3 0.0000000      NaN        NaN         NaN       9          NA
>> 4 0.7777778        0 0.00000000   15.571429      11          NA
>> 5 1.0000000        0 0.00000000   24.083333     NaN          NA
>> 6 1.0000000        0 0.07692308   64.538462     NaN          NA
>> 7 1.0000000        0 0.39207048 1088.454846     NaN          NA
>>
>> you can see that
>> a) the print statement above it works
>> b) the print command works with this table
>>
>> but if I uncomment it and try to run the loops again.....
>>
>> +             print(paste(mutType,"sim",sim,"hmm",hmm))         
>>               print(acctab[,10:15])
>> Error: syntax error
>>  >         >                 >             nummod <- nummod +1
>>  >         }  #end hmmMats loop
>> Error: syntax error
>>  >         }  #end tmats loop   Error: syntax error
>>  >         }  #end mut type loop
>> Error: syntax error
>>
>>
>> I have tried print.data.frame, but that doesn't work either....
>>
>>
>> Any suggestions would be appreciated  (session info below)
>>
>>
>> Ivan
>>
>>
>>  > sessionInfo()
>> R version 2.5.1 (2007-06-27)
>> i386-apple-darwin8.9.1
>>
>> locale:
>> C
>>
>> attached base packages:
>> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  
>> "methods"   "base" 
>

-- 
**************************************************************
Ivan Baxter
Research Scientist
Bindley Bioscience Center
Purdue University
Office: Hort 305
765-543-7288
ibaxter at purdue.edu


From wwwhsd at gmail.com  Tue Jul  3 16:26:05 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Tue, 3 Jul 2007 11:26:05 -0300
Subject: [R] how to get the position of an element in a vector ?
In-Reply-To: <50c8fbc90707030712v3e9a878fx698f77717c93dbc1@mail.gmail.com>
References: <50c8fbc90707030712v3e9a878fx698f77717c93dbc1@mail.gmail.com>
Message-ID: <da79af330707030726w7adfbea2m1aa8b252c4e05ba7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/789d56a9/attachment.pl 

From Thierry.ONKELINX at inbo.be  Tue Jul  3 16:30:37 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 3 Jul 2007 16:30:37 +0200
Subject: [R] how to get the position of an element in a vector ?
In-Reply-To: <50c8fbc90707030712v3e9a878fx698f77717c93dbc1@mail.gmail.com>
References: <50c8fbc90707030712v3e9a878fx698f77717c93dbc1@mail.gmail.com>
Message-ID: <2E9C414912813E4EB981326983E0A104032D96CB@inboexch.inbo.be>

?which.max

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Benoit Chemineau
> Verzonden: dinsdag 3 juli 2007 16:12
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] how to get the position of an element in a vector ?
> 
> Hi, dear R developers,
> 
> I've got a vector of monthly volatilities and i would like to 
> get the position of the highest volatility of the vector 
> without computing a loop.
> Is there a function that could give me such a result ?
> 
> a<-c(1,2,4,100,3)
> 
> the highest value is the fourth of the vector.
> how can i get "4" without a loop going through the vector ?
> 
> Thanks !
> 
> Benoit.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cumarporn at gmail.com  Tue Jul  3 16:34:13 2007
From: cumarporn at gmail.com (umarporn charusombat)
Date: Tue, 3 Jul 2007 10:34:13 -0400
Subject: [R] Fwd:  help again
In-Reply-To: <904d37580707021850y133e2ed1nae9227ce08d5be00@mail.gmail.com>
References: <904d37580707020922k10303719hbfb06a5a115ce375@mail.gmail.com>
	<46893295.3070204@gmx.net> <20070702173828.M98619@centroin.com.br>
	<904d37580707021850y133e2ed1nae9227ce08d5be00@mail.gmail.com>
Message-ID: <904d37580707030734x7030b0fcrf0f4152871d15b20@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/ede18eb4/attachment.pl 

From isaac_kohane at harvard.edu  Tue Jul  3 16:38:21 2007
From: isaac_kohane at harvard.edu (Isaac Kohane)
Date: Tue, 3 Jul 2007 10:38:21 -0400
Subject: [R] Formula syntax question
Message-ID: <EC825581-93CB-4057-B6F3-B0C24286CB77@harvard.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/cc4b5a56/attachment.pl 

From f.harrell at vanderbilt.edu  Tue Jul  3 16:49:46 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 03 Jul 2007 09:49:46 -0500
Subject: [R] Formula syntax question
In-Reply-To: <EC825581-93CB-4057-B6F3-B0C24286CB77@harvard.edu>
References: <EC825581-93CB-4057-B6F3-B0C24286CB77@harvard.edu>
Message-ID: <468A620A.2000203@vanderbilt.edu>

Isaac Kohane wrote:
> Forgive me if this is obvious:
> 
> 	I have a frame of data with the variables in each column (e.g.  
> Discrete_Variable1, ContinuousVariable_1, ContinuousVariable_2,  ...   
> ContinuousVariable_n)
> 
> 	and I want to create a model using lrm i.e.
> 	model <- lrm(Discrete_Variable1 ~ ContinuousVariable_1,  
> data=lotsofdata)
> 
> 	Is there a syntax for having all the continuous variables referenced  
> in the formula without having to enumerate them all?
> 
> 	I've seen the ~ . notation but when I try
> 
> 
> 	model <- lrm(Discrete_Variable1 ~  ., data=lotsofdata)
> 
> 	I get this error:
> 
> 	Error in terms.formula(formula, specials = "strat") :
> 	'.' in formula and no 'data' argument
> 	
> 
> 	Any help is appreciated.
> 
> -Zak

It may be best to write a function to determine what is continuous (>= 
10 unique values for example, and numeric) and to run sapply on that 
function, over your data frame.  Then you could use lrm(y ~ ., 
data=mydata[continuous]) if it were not for a problem with lrm which 
Charles Thomas Dupont (the Design package maintainer) and I will work 
on.  Until then you can write a command to compose a formula, e.g.,

form <- as.formula(paste('y', paste(names(mydata)[continuous], 
collapse='+'), sep='~'))
lrm(form, data=mydata)


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From yn19832 at msn.com  Tue Jul  3 16:59:00 2007
From: yn19832 at msn.com (livia)
Date: Tue, 3 Jul 2007 07:59:00 -0700 (PDT)
Subject: [R] EWMA in fMultivar
Message-ID: <11414114.post@talk.nabble.com>


Hello, I would like to use the function EWMA() in the fMultivar Package and I
have a series of data x, which is the returns series. Basically, I would
like to get the variance estimation using EWMA.

I am trying something like EWMA(x, lambda) and I have a couple of questions:

 
Should x be the returns series or price series in my case?

When I get the result, there are the same numbers of data points as in the
returns series. I was expecting there would be one less data points than the
original data series, or are they one period lagged data?

Could anyone give me some advice? Many thanks

-- 
View this message in context: http://www.nabble.com/EWMA-in-fMultivar-tf4018921.html#a11414114
Sent from the R help mailing list archive at Nabble.com.


From vineetk at cmu.edu  Tue Jul  3 16:59:30 2007
From: vineetk at cmu.edu (Vineet Kumar)
Date: Tue, 3 Jul 2007 10:59:30 -0400
Subject: [R] generating correlated Bernoulli random variables
In-Reply-To: <468A4309.5020506@williams.edu>
References: <468A4309.5020506@williams.edu>
Message-ID: <4ad306f30707030759x1f756273v532e9b9fc0dc42aa@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/9f55edd7/attachment.pl 

From milton_ruser at yahoo.com.br  Tue Jul  3 16:59:47 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Tue, 3 Jul 2007 07:59:47 -0700 (PDT)
Subject: [R] Parsimony Analysis of Encemism in R?
Message-ID: <813951.67055.qm@web56615.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/8b0ff0bc/attachment.pl 

From bolker at ufl.edu  Tue Jul  3 17:10:26 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 3 Jul 2007 15:10:26 +0000 (UTC)
Subject: [R] how to use mle with a defined function
References: <11402268.post@talk.nabble.com>
Message-ID: <loom.20070703T170313-189@post.gmane.org>

Lin Pan <linpan1975 <at> yahoo.com> writes:

> 
> 
> Hi all,
> 
> I am trying to use mle() to find a self-defined function. Here is my
> function:
> 
> test <- function(a=0.1, b=0.1, c=0.001, e=0.2){
> 
> # omega is the known covariance matrix, Y is the response vector, X is the
> explanatory matrix
> 
> odet = unlist(determinant(omega))[1]
> 
> # do cholesky decomposition
> 
> C = chol(omega)
> 
> # transform data
> 
> U = t(C)%*%Y
> WW=t(C)%*%X
> 
> beta = lm(U~W)$coef
> 
> Z=Y-X%*%beta
> V=solve(t(C), Z)
> 
> 0.5*odet + 0.5*(t(V)%*%V)
> 
> }
> 
> and I am trying to call mle() to calculate the maximum likelihood estimates
> for function (0.5*odet+0.5*(t(V)%*%V)) by
> 
> result = mle(test, method="Nelder-Mead")
> 
> But I get the following error message:
> 
> Error in optim(start, f, method = method, hessian = TRUE, ...) : 
>         (list) object cannot be coerced to 'double'


  Can you give a self-contained example (e.g., make up a small
matrix?)  There are a few places where I don't understand what
you're doing:

  - is the WW above a typo for W?

  - if omega is fixed, why calculate its (log) determinant
every time the function is called?  (this shouldn't change
the answer, just slow things down)
 
  - is X a single vector or a design matrix?
it seems like you might want lm(U~W-1) instead of lm(U~W)

  - it doesn't look like your parameters are used in the
function at all -- are they really parameters for calculating
omega?  [because of this, I can get as far as computing
the Hessian, and then I get "system is exactly singular"]

   [ off topic: somewhat disturbingly, the CAPTCHA
word for posting from Gmane was mother f***ers ... ]


From murdoch at stats.uwo.ca  Tue Jul  3 17:20:50 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 03 Jul 2007 11:20:50 -0400
Subject: [R] loop causes syntax error in print()
In-Reply-To: <468A5BEC.1000407@purdue.edu>
References: <4689E5D1.2070202@purdue.edu> <468A45B1.3030304@stats.uwo.ca>
	<468A5BEC.1000407@purdue.edu>
Message-ID: <468A6952.6020902@stats.uwo.ca>

On 7/3/2007 10:23 AM, Ivan Baxter wrote:
> 
> Duncan Murdoch wrote:
>> On 7/3/2007 1:59 AM, Ivan Baxter wrote:
>>> I am having trouble printing a table out to the GUI display when the 
>>> table is created and printed within a loop.
>>>
>>> I get a "Error: syntax error message"
>>>
>>> If I comment out the print statement, the loop runs fine and I can 
>>> print out the last iteration of the table.
>>
>> You should simplify your loop until it's something you can post for us 
>> to try.  Chances are you'll notice the error when you do that, but if 
>> not, someone else will be able to tell you what's going on.
>>
>> Without a reproducible example, it's more or less hopeless.
>>
>> Duncan Murdoch
> Right- well I did as you suggested and seem to have fixed the problem.
> 
> The problem appeared to be that some carriage returns were not 
> registering in the Mac Os X R editor. So even though it looked like it 
> was on a new line, it wasn't. It wasn't a line wrap issue, these were 
> actually returns that I had hit that just weren't registering. I had one 
> chunk of code which showed what I thought was the problem when R 
> crashed. When I opened up the script again after restarting, the problem 
> disappeared, so somehow the returns started to be seen.
> 

If you can reproduce this in the future, it might be worth trying to use 
cut and paste to save a record of the offending text, and sending it to 
the Mac GUI maintainers.  I would guess there's some special code that 
is shown by the editor as a newline but seen by the parser as just 
another character.

Duncan Murdoch


From yvonnick.noel at free.fr  Tue Jul  3 15:04:30 2007
From: yvonnick.noel at free.fr (NOEL Yvonnick)
Date: Tue, 03 Jul 2007 15:04:30 +0200
Subject: [R] Search a function name in a string
Message-ID: <468A495E.2060509@free.fr>

Hello,

I am trying to find a function name in a string that expresses a 
functional form :

 > s = "blabla...S(var)...blabla"

I would like to detect the pattern "S(*)" in s.

I am no guru at regular expressions. Just tried :

 > grep("S(.*)",c("S(a)","CSP"))
[1] 1 2
 >

I expected the pattern to be retrieved only in the first string, so 
obviously this is not correct. Any idea ?

Thank you very much in advance,

Yvonnick Noel, PhD
U. of Rennes 2
France


From grenife at hotmail.com  Tue Jul  3 15:37:44 2007
From: grenife at hotmail.com (GWeiss)
Date: Tue, 3 Jul 2007 06:37:44 -0700 (PDT)
Subject: [R] Empirical copula in R
Message-ID: <11412335.post@talk.nabble.com>


Hi,

I would like to implement the empirical copula in R, does anyone know if it
is included in a package? I know it is not in the "Copula" package. This one
only includes a gof-test based on the empirical copula process.

Thanks for your help!
Gregor
-- 
View this message in context: http://www.nabble.com/Empirical-copula-in-R-tf4018319.html#a11412335
Sent from the R help mailing list archive at Nabble.com.


From b3i4old02 at sneakemail.com  Tue Jul  3 17:21:56 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Tue, 03 Jul 2007 16:21:56 +0100
Subject: [R] Lattice: shifting strips to left of axes
In-Reply-To: <eb555e660707022040qf4ec6d0p4479498600ceead2@mail.gmail.com>
References: <f6cc16$p0a$1@sea.gmane.org>
	<eb555e660707022040qf4ec6d0p4479498600ceead2@mail.gmail.com>
Message-ID: <f6dpj7$3pa$1@sea.gmane.org>

deepayan.sarkar at gmail.com wrote:
> On 7/2/07, Michael Hoffman <b3i4old02 at sneakemail.com> wrote:
>> Consider this plot:
>>
>> xyplot(mpg ~ disp | cyl, mtcars, strip=F, strip.left=T, layout=c(1, 3),
>>         scales=list(relation="free"),
>>         par.settings=list(strip.background=list(col="transparent")))
>>
>> I want to have the "cyl" strip labels on the left side of the axis. Is
>> this possible?
> 
> No. (It's possible to have a legend there, which could be used to put
> row-specific ylab-s, for example, but it will be hard to make it look
> like strips)

Thanks for the response.

Not looking like a real strip is fine. What I want is essentially a 
secondary ylab for each row, and don't care about niceties such as 
shingle markings (I should have made the conditional factor(cyl) in the 
above plot).

But it looks like the legend goes to the left of the plot's ylab, and 
what I really want is for the secondary ylab to be between the primary 
ylab and the panel. So looks like I would have to eliminate the primary 
ylab from being drawn automatically and draw it myself in the legend? 
And I think I would have to manually calculate the panel heights as 
well, right? I don't see a way for the legend to get this out of the 
trellis object.

> xyplot(mpg ~ disp | cyl, mtcars, strip=F, strip.left=T, layout=c(1, 3),
>        scales=list(relation="free", y = list(draw = FALSE)),
>        axis = function(side, ...) {
>            if (side == "right")
>                panel.axis(side = "right", outside = TRUE)
>            else axis.default(side = side, ...)
>        },
>        par.settings=
>        list(strip.background=list(col="transparent"),
>             layout.widths = list(axis.key.padding = 5)))

This seems a lot easier.


From bessa_ricardo at hotmail.com  Tue Jul  3 12:00:43 2007
From: bessa_ricardo at hotmail.com (Ricardo Bessa)
Date: Tue, 3 Jul 2007 11:00:43 +0100
Subject: [R] Blank margin in plot figures
Message-ID: <BAY103-W90165EE5387ABB6A72922960C0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/81e3907b/attachment.pl 

From Mark.Leeds at morganstanley.com  Tue Jul  3 17:34:05 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Tue, 3 Jul 2007 11:34:05 -0400
Subject: [R] EWMA in fMultivar
In-Reply-To: <11414114.post@talk.nabble.com>
References: <11414114.post@talk.nabble.com>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401957470@NYWEXMB23.msad.ms.com>

there is an ewma example in ?filter I think that might be more useful
because then you can see better
What's happening in terms of the smoothing. Actually, I just looked and
it's not there. It must have been in S+.

I include my ewma below but you have to modify it because it assumes a
zoo object. Of coure,  The other
option you have is to look at the source code for the ewma function in
fMultvar.

ewma<-function(x,lambda = 1, init = (1-lambda)*.raw[good.ind][1]) {

   # work with 'non-zoo' data for speed and then recombine
   .raw <- unclass(coredata(x))

   good.ind <- !is.na(.raw)  # determine good values

   .raw[good.ind] <- filter(lambda * .raw[good.ind], filter=(1-lambda),
       method='recursive', init=coredata(init))
   zoo(.raw, index(x)) # create zoo object for return



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of livia
Sent: Tuesday, July 03, 2007 10:59 AM
To: r-help at stat.math.ethz.ch
Subject: [R] EWMA in fMultivar


Hello, I would like to use the function EWMA() in the fMultivar Package
and I have a series of data x, which is the returns series. Basically, I
would like to get the variance estimation using EWMA.

I am trying something like EWMA(x, lambda) and I have a couple of
questions:

 
Should x be the returns series or price series in my case?

When I get the result, there are the same numbers of data points as in
the returns series. I was expecting there would be one less data points
than the original data series, or are they one period lagged data?

Could anyone give me some advice? Many thanks

--
View this message in context:
http://www.nabble.com/EWMA-in-fMultivar-tf4018921.html#a11414114
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From ligges at statistik.uni-dortmund.de  Tue Jul  3 17:43:38 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 03 Jul 2007 17:43:38 +0200
Subject: [R] Search a function name in a string
In-Reply-To: <468A495E.2060509@free.fr>
References: <468A495E.2060509@free.fr>
Message-ID: <468A6EAA.4010702@statistik.uni-dortmund.de>



NOEL Yvonnick wrote:
> Hello,
> 
> I am trying to find a function name in a string that expresses a 
> functional form :
> 
>  > s = "blabla...S(var)...blabla"
> 
> I would like to detect the pattern "S(*)" in s.
> 
> I am no guru at regular expressions. Just tried :
> 
>  > grep("S(.*)",c("S(a)","CSP"))

grep("S\\(.*\\)",c("S(a)","CSP"))

Uwe Ligges

> [1] 1 2
>  >
> 
> I expected the pattern to be retrieved only in the first string, so 
> obviously this is not correct. Any idea ?
> 
> Thank you very much in advance,
> 
> Yvonnick Noel, PhD
> U. of Rennes 2
> France
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From davidr at rhotrading.com  Tue Jul  3 17:53:26 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Tue, 3 Jul 2007 10:53:26 -0500
Subject: [R] EWMA in fMultivar
In-Reply-To: <11414114.post@talk.nabble.com>
References: <11414114.post@talk.nabble.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE7739525F@rhopost.rhotrading.com>

To calculate variance (assuming zero mean, as is usual), you would use
returns^2. 
You will have to examine the code by typing EWMA to see what it's doing
and how to change it. The code is clear enough that you could make your
own version to achieve what you want.
HTH,

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of livia
Sent: Tuesday, July 03, 2007 9:59 AM
To: r-help at stat.math.ethz.ch
Subject: [R] EWMA in fMultivar


Hello, I would like to use the function EWMA() in the fMultivar Package
and I
have a series of data x, which is the returns series. Basically, I would
like to get the variance estimation using EWMA.

I am trying something like EWMA(x, lambda) and I have a couple of
questions:

 
Should x be the returns series or price series in my case?

When I get the result, there are the same numbers of data points as in
the
returns series. I was expecting there would be one less data points than
the
original data series, or are they one period lagged data?

Could anyone give me some advice? Many thanks

-- 
View this message in context:
http://www.nabble.com/EWMA-in-fMultivar-tf4018921.html#a11414114
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Tue Jul  3 18:03:50 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 3 Jul 2007 12:03:50 -0400
Subject: [R] Search a function name in a string
In-Reply-To: <468A495E.2060509@free.fr>
References: <468A495E.2060509@free.fr>
Message-ID: <971536df0707030903r352c0f23p2fd28a382c7a9fb@mail.gmail.com>

Parnetheses have special meaning so they must be escaped or
specify a character class of 1:

> grep("S\\(.*\\)",c("S(a)","CSP"))
[1] 1
> grep("S[(].*[)]",c("S(a)","CSP"))
[1] 1


On 7/3/07, NOEL Yvonnick <yvonnick.noel at free.fr> wrote:
> Hello,
>
> I am trying to find a function name in a string that expresses a
> functional form :
>
>  > s = "blabla...S(var)...blabla"
>
> I would like to detect the pattern "S(*)" in s.
>
> I am no guru at regular expressions. Just tried :
>
>  > grep("S(.*)",c("S(a)","CSP"))
> [1] 1 2
>  >
>
> I expected the pattern to be retrieved only in the first string, so
> obviously this is not correct. Any idea ?
>
> Thank you very much in advance,
>
> Yvonnick Noel, PhD
> U. of Rennes 2
> France
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From yn19832 at msn.com  Tue Jul  3 18:04:46 2007
From: yn19832 at msn.com (livia)
Date: Tue, 3 Jul 2007 09:04:46 -0700 (PDT)
Subject: [R] sequences
Message-ID: <11414836.post@talk.nabble.com>


Hi, I would like to generate a series in the following form (0.8^1, 0.8^2,
..., 0.8^600)
Could anyone tell me how can I achieve that? I am really new to R.
-- 
View this message in context: http://www.nabble.com/sequences-tf4019146.html#a11414836
Sent from the R help mailing list archive at Nabble.com.


From res90sx5 at verizon.net  Tue Jul  3 18:22:15 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Tue, 03 Jul 2007 09:22:15 -0700
Subject: [R] MatchIt package on Ubuntu 7.04 (Feisty Fawn)
In-Reply-To: <18058.21914.430998.829044@basebud.nulle.part>
Message-ID: <027e01c7bd8e$51e275f0$0201a8c0@Aragorn>



Dirk,

Thanks for the assistance.

> -----Original Message-----
> From: Dirk Eddelbuettel [mailto:edd at debian.org]
> Sent: Tuesday, July 03, 2007 6:57 AM
> To: Daniel Nordlund
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] MatchIt package on Ubuntu 7.04 (Feisty Fawn)
> 
> 
> On 2 July 2007 at 23:47, Daniel Nordlund wrote:
> | I tried to install the MatchIt package on Ubuntu 7.04.
> 
> How? Via 'sudo apt-get install r-cran-matchit', or using R?

Using install.packages() inside R.

> 
> | When loading required dependencies I got the following error message:
> |
> | Error in dyn.load(x, as.logical(local), as.logical(now)) :
> |         unable to load shared library
> | '/usr/local/lib/R/site-library/optmatch/libs/optmatch.so':
> 
> /usr/local, so you did this using via 'R CMD INSTALL ' or from inside
> R. Consider removing this directory and install the Ubuntu-supplied package.
> Also, you didn't mention whether this is an older install of matchit, or
> whether you just did it.

This is a new install using install.packages().  I will try your suggestions and maybe move further questions to r-sig-debian.

> 
> |   /usr/local/lib/R/site-library/optmatch/libs/optmatch.so: cannot map zero-fill pages:
> Cannot allocate memory
> | Error: package/namespace load failed for 'optmatch'
> |
> | optmatch.so appears to be where R is looking for it so I don't know what
> | has gone wrong.   Have I missed something in the installation process, or
> 
> Looks like a runtime error, possible due to mismatched libraries.
> 
> | is this a known Ubuntu problem, or a licensing issue?  Should I direct this
> | question to the maintainer?
> 
> Possibly, the r-sig-debian list is also 'on topic' for Debian/Ubuntu questions.
> 
> Hth, Dirk
> 
Thanks again,

Dan

Daniel Nordlund
Bothell, WA USA


From afshart at exchange.sba.miami.edu  Tue Jul  3 18:35:08 2007
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Tue, 3 Jul 2007 12:35:08 -0400
Subject: [R] xyplot and autokey,
	maintaining colors specified via "col" in key
In-Reply-To: <Pine.LNX.4.64.0706281419450.21694@springer.Berkeley.EDU>
Message-ID: <6BCB4D493A447546A8126F24332056E8063BC45D@school1.business.edu>

 
All,

When specifying colors to xyplot w/ a groups argument, using
auto.key no longer maintains the colors properly.  I've searched
the docs and help but haven't found exactly what I need ... I saw
a few examples in the archives involving par.settings but that doesn't
seem to do it. I also saw some people using key instead of auto.key, but
that didn't seem consistent.  Is there a quick fix to the example code
below?

cheers,
Dave

dat.ex = data.frame(  rep(c(1:6), each=6), c(rnorm(12), rnorm(12, 1),
rnorm(12, 2)), rep(c(1:6), 6),
rep(c("Drug1", "Drug2", "Placebo"), each=12) )
names(dat.ex) = c("patient.no", "outcome", "time", "drug")


## colors in xyplot agree w/ colors in key, colors not specified:
xyplot(outcome ~ time, dat.ex, groups=drug, type=c("g", "smooth"), 
auto.key = list(space = "top",  text = levels(dat.ex$drug),
points=FALSE, lines=TRUE ) )

## colors in xyplot do NOT agree w/ colors in key when colors changed:
xyplot(outcome ~ time, dat.ex, groups=drug, type=c("g", "smooth"), 
auto.key = list(space = "top",  text = levels(dat.ex$drug),
points=FALSE, lines=TRUE ),
col = c(1,2,3)  )

i.e., the key is still generated w/ the "old" colors.


From wwwhsd at gmail.com  Tue Jul  3 18:46:01 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Tue, 3 Jul 2007 13:46:01 -0300
Subject: [R] sequences
In-Reply-To: <11414836.post@talk.nabble.com>
References: <11414836.post@talk.nabble.com>
Message-ID: <da79af330707030946h1ca71a4ej352c40f518c73a63@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/b4bdf466/attachment.pl 

From HDoran at air.org  Tue Jul  3 18:47:04 2007
From: HDoran at air.org (Doran, Harold)
Date: Tue, 3 Jul 2007 12:47:04 -0400
Subject: [R] sequences
In-Reply-To: <11414836.post@talk.nabble.com>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EE579EE@dc1ex01.air.org>

Just take advantage of R's vectorized calculations as

x <- seq(1:600)

.8^x

Harold


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of livia
> Sent: Tuesday, July 03, 2007 12:05 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] sequences
> 
> 
> Hi, I would like to generate a series in the following form 
> (0.8^1, 0.8^2, ..., 0.8^600) Could anyone tell me how can I 
> achieve that? I am really new to R.
> --
> View this message in context: 
> http://www.nabble.com/sequences-tf4019146.html#a11414836
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ccleland at optonline.net  Tue Jul  3 18:47:17 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 03 Jul 2007 12:47:17 -0400
Subject: [R] sequences
In-Reply-To: <11414836.post@talk.nabble.com>
References: <11414836.post@talk.nabble.com>
Message-ID: <468A7D95.402@optonline.net>

livia wrote:
> Hi, I would like to generate a series in the following form (0.8^1, 0.8^2,
> ..., 0.8^600)
> Could anyone tell me how can I achieve that? I am really new to R.

8^(1:600)

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ggrothendieck at gmail.com  Tue Jul  3 19:00:44 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 3 Jul 2007 13:00:44 -0400
Subject: [R] xyplot and autokey,
	maintaining colors specified via "col" in key
In-Reply-To: <6BCB4D493A447546A8126F24332056E8063BC45D@school1.business.edu>
References: <Pine.LNX.4.64.0706281419450.21694@springer.Berkeley.EDU>
	<6BCB4D493A447546A8126F24332056E8063BC45D@school1.business.edu>
Message-ID: <971536df0707031000n17aabcb8kc72825a4b2e26a22@mail.gmail.com>

Don't know what you did regarding par.settings to not get the desired
result but that is, in fact, the way to go since both the key and the plot
lines will get their colors from that:

xyplot(outcome ~ time, dat.ex, groups=drug, type=c("g", "smooth"),
auto.key = list(space = "top",  text = levels(dat.ex$drug),
points=FALSE, lines=TRUE ),
par.settings = list(superpose.line = list(col = rainbow(3))))


On 7/3/07, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
>
> All,
>
> When specifying colors to xyplot w/ a groups argument, using
> auto.key no longer maintains the colors properly.  I've searched
> the docs and help but haven't found exactly what I need ... I saw
> a few examples in the archives involving par.settings but that doesn't
> seem to do it. I also saw some people using key instead of auto.key, but
> that didn't seem consistent.  Is there a quick fix to the example code
> below?
>
> cheers,
> Dave
>
> dat.ex = data.frame(  rep(c(1:6), each=6), c(rnorm(12), rnorm(12, 1),
> rnorm(12, 2)), rep(c(1:6), 6),
> rep(c("Drug1", "Drug2", "Placebo"), each=12) )
> names(dat.ex) = c("patient.no", "outcome", "time", "drug")
>
>
> ## colors in xyplot agree w/ colors in key, colors not specified:
> xyplot(outcome ~ time, dat.ex, groups=drug, type=c("g", "smooth"),
> auto.key = list(space = "top",  text = levels(dat.ex$drug),
> points=FALSE, lines=TRUE ) )
>
> ## colors in xyplot do NOT agree w/ colors in key when colors changed:
> xyplot(outcome ~ time, dat.ex, groups=drug, type=c("g", "smooth"),
> auto.key = list(space = "top",  text = levels(dat.ex$drug),
> points=FALSE, lines=TRUE ),
> col = c(1,2,3)  )
>
> i.e., the key is still generated w/ the "old" colors.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dreiss at systemsbiology.org  Tue Jul  3 19:37:55 2007
From: dreiss at systemsbiology.org (David Reiss)
Date: Tue, 3 Jul 2007 10:37:55 -0700
Subject: [R] bug in closing gzfile-opened connections?
Message-ID: <fd913b0d0707031037o37629382ke6cb28af5fc56b76@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/4acb66d6/attachment.pl 

From deepayan.sarkar at gmail.com  Tue Jul  3 19:53:34 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Tue, 3 Jul 2007 10:53:34 -0700
Subject: [R] Lattice: shifting strips to left of axes
In-Reply-To: <f6dpj7$3pa$1@sea.gmane.org>
References: <f6cc16$p0a$1@sea.gmane.org>
	<eb555e660707022040qf4ec6d0p4479498600ceead2@mail.gmail.com>
	<f6dpj7$3pa$1@sea.gmane.org>
Message-ID: <eb555e660707031053w4f853b5anb194bc7a9a1daf3d@mail.gmail.com>

On 7/3/07, Michael Hoffman <b3i4old02 at sneakemail.com> wrote:
> deepayan.sarkar at gmail.com wrote:
> > On 7/2/07, Michael Hoffman <b3i4old02 at sneakemail.com> wrote:
> >> Consider this plot:
> >>
> >> xyplot(mpg ~ disp | cyl, mtcars, strip=F, strip.left=T, layout=c(1, 3),
> >>         scales=list(relation="free"),
> >>         par.settings=list(strip.background=list(col="transparent")))
> >>
> >> I want to have the "cyl" strip labels on the left side of the axis. Is
> >> this possible?
> >
> > No. (It's possible to have a legend there, which could be used to put
> > row-specific ylab-s, for example, but it will be hard to make it look
> > like strips)
>
> Thanks for the response.
>
> Not looking like a real strip is fine. What I want is essentially a
> secondary ylab for each row, and don't care about niceties such as
> shingle markings (I should have made the conditional factor(cyl) in the
> above plot).

I thought this might be the case.

> But it looks like the legend goes to the left of the plot's ylab, and
> what I really want is for the secondary ylab to be between the primary
> ylab and the panel. So looks like I would have to eliminate the primary
> ylab from being drawn automatically and draw it myself in the legend?
> And I think I would have to manually calculate the panel heights as
> well, right? I don't see a way for the legend to get this out of the
> trellis object.

It's possible, although it requires some advanced grid features.
Luckily, this has come up before (search the r-help archives for
"myXlabGrob"). Basically, you can use the fact that 'ylab' can be a
"grob" to get what you want (I think). Here is a modified version of
the original function (adapted to include a 'primary' ylab):


library(grid)
library(lattice)

myYlabGrob <-
    function(..., main.ylab = "") ## ...is lab1, lab2, etc
{
    ## you can add arguments to textGrob for more control
    ## in the next line
    labs <- lapply(list(...), textGrob, rot=90)
    main.ylab <- textGrob(main.ylab, rot = 90)
    nlabs <- length(labs)
    lab.heights <-
        lapply(labs,
               function(lab) unit(1, "grobheight",
                                  data=list(lab)))
    unit1 <- unit(1.2, "grobheight", data = list(main.ylab))
    unit2 <- do.call(max, lab.heights)
    lab.layout <-
        grid.layout(ncol = 2, nrow = nlabs,
                    heights = unit(1, "null"),
                    widths = unit.c(unit1, unit2),
                    respect = TRUE)
    lab.gf <- frameGrob(layout=lab.layout)
    for (i in seq_len(nlabs))
    {
        lab.gf <- placeGrob(lab.gf, labs[[i]], row = i, col = 2)
    }
    lab.gf <- placeGrob(lab.gf, main.ylab, col = 1)
    lab.gf
}

xyplot(mpg ~ disp | cyl, mtcars, strip=F, strip.left=F, layout=c(1, 3),
       scales=list(relation="free"),
       ylab = myYlabGrob("4", "6", "8", main.ylab = "mpg"))

-Deepayan


From Zava.Aydemir at morganstanley.com  Tue Jul  3 20:00:33 2007
From: Zava.Aydemir at morganstanley.com (Aydemir, Zava (FID))
Date: Tue, 3 Jul 2007 14:00:33 -0400
Subject: [R] vertically concatenating data frames
Message-ID: <755261CA22782948B1C42ACDC83912A1046140A2@NYWEXMB27.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/d616ebb9/attachment.pl 

From otter at otter-rsch.com  Tue Jul  3 19:34:10 2007
From: otter at otter-rsch.com (dave fournier)
Date: Tue, 03 Jul 2007 10:34:10 -0700
Subject: [R]  Using odesolve to produce non-negative solutions
Message-ID: <468A8892.1080503@otter-rsch.com>

If you didn't get this solved.

I have done parameter estimation with models
defined by ODE's where negative solutions are a problem
and one can only avoid them with great difficulty if the
standard explicit methods for solving the ODE are used. I found that
using implicit methods could be a great help.

For example in the exponential case

     dN/dt = -k*N

the simple finite difference approximation is

     N_{t+1}-N+t
     --------------  = -k*N_t ,  k>=0
        h

   or
     N_{t+1} = N_t -k*h*N_t

   and if  k*h gets too large N_{t+1} goes negative and you are in trouble.

   Consider instead the implicit formulation where the second
   N_t on the RHS is replaced by N_{t+1}  and one gets

     N_{t+1} = N_t/(1+k*h)

    which is correct  for k*h=0 and as k*h--> infinity

   For a more complicated example see

    http://otter-rsch.com/admodel/cc4.html

   for something I called "semi-implicit".
   I hope these ideas will be useful for your problem.


         Cheers,

          Dave





-- 
David A. Fournier
P.O. Box 2040,
Sidney, B.C. V8l 3S3
Canada
Phone/FAX 250-655-3364
http://otter-rsch.com


From Greg.Snow at intermountainmail.org  Tue Jul  3 20:06:36 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 3 Jul 2007 12:06:36 -0600
Subject: [R] vertically concatenating data frames
In-Reply-To: <755261CA22782948B1C42ACDC83912A1046140A2@NYWEXMB27.msad.ms.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAACF13@LP-EXCHVS07.CO.IHC.COM>

Use rbind instead of c:

> df <- rbind(df1,df2)

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Aydemir, Zava (FID)
> Sent: Tuesday, July 03, 2007 12:01 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] vertically concatenating data frames
> 
> Hi,
>  
> what is the recommended way to vertically concatenate 2 data 
> frames with the same column names but different number of rows?
>  
> My problem is something along these lines:
>  
> df1 <- data.frame(var1=var1,var2=var2,var3=var3)  # nrow(df1)=1000
> df2 <- data.frame(var1=var4,var2=var5,var3=var6)  # nrow(df2)=2000
>  
> I tried df <- c(df1,df2), no success. stack does not seem to 
> be appropriate either for my problem.
>  
>  
> Thanks
>  
> Zava
> --------------------------------------------------------
> 
> This is not an offer (or solicitation of an offer) to 
> buy/se...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Mark.Leeds at morganstanley.com  Tue Jul  3 20:28:47 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Tue, 3 Jul 2007 14:28:47 -0400
Subject: [R] Convetring a dataframe  so that it just has one column
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBAACF13@LP-EXCHVS07.CO.IHC.COM>
References: <755261CA22782948B1C42ACDC83912A1046140A2@NYWEXMB27.msad.ms.com>
	<07E228A5BE53C24CAD490193A7381BBBAACF13@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401957472@NYWEXMB23.msad.ms.com>

I have two dataframes, yendog and datasub as below and I want to do the
same thing to both of them, namely convert
them so that they retain their dataframeness but stack the columns into
1 column. I don't need names
on the results.

I did try 

temp1<-do.call("rbind",datasub)
temp2<-do.call("rbind",yendog)

and also

temp1<-do.call("rbind",lapply(datasub,function(x) { t(x) }))
temp2<-do.call("rbind",lapply(yendog, function(x) { t(x) }))

but neither of them did the trick.  I'm probably making it more
complicated
than it needs to be. Thanks for any sugggestions.


datasub

          AAA.l1        BBB.l1        CCC.l1        DDD.l1        EEE.l1
FFF.l1
2  -1.937171e-04  1.391873e-03  1.713845e-04  2.595525e-04  2.635428e-05
2.047188e-04
3   3.228410e-04  6.047229e-05  1.713551e-04  7.414547e-05 -4.744833e-04
-3.070939e-04
4   0.000000e+00  1.209336e-04 -1.142335e-04 -7.414547e-05 -1.318409e-04
0.000000e+00
5  -1.291239e-04 -4.838222e-04 -6.856751e-04 -2.966369e-04 -7.650202e-04
2.047397e-04
6  -1.937171e-04 -1.814937e-04  0.000000e+00  0.000000e+00 -5.278159e-05
2.046978e-04
7   0.000000e+00 -6.050522e-05  2.286106e-04  5.190568e-04  4.749340e-04
5.115613e-04
8   1.291489e-04  3.629764e-04  4.570645e-04  1.482525e-04  1.582612e-04
2.045513e-04
9  -3.229036e-04 -1.814717e-04 -7.428360e-04 -5.931418e-04 -7.123729e-04
-2.045513e-04
10  8.393325e-04  7.256894e-04  1.714727e-04  3.708167e-05  7.917761e-05
-2.557479e-04
11  6.451613e-04  0.000000e+00  1.142988e-04  2.224612e-04 -2.639497e-04
-4.093328e-04
12  1.934673e-04  0.000000e+00 -1.142988e-04 -3.707342e-05 -5.279831e-05
5.117576e-05
13 -1.096810e-03  3.022152e-04  0.000000e+00 -1.483074e-04 -2.376269e-04
1.023437e-04
14  7.098377e-04 -1.208751e-04  6.855968e-04  3.707274e-04  2.640578e-05
7.672438e-04
16  3.224454e-04  1.087613e-03 -5.712490e-05  2.223870e-04  3.695101e-04
3.066858e-04
17  9.022945e-04  2.171685e-03  6.281945e-04  6.668643e-04  5.803831e-04
4.087681e-04
18 -6.442261e-05 -7.233709e-04  1.712573e-04 -7.407407e-05  1.054880e-04
-1.021764e-04
19 -1.224819e-03 -1.206782e-03 -1.142270e-03 -8.151772e-04 -7.911497e-05
0.000000e+00
20  3.869470e-04 -1.811430e-04 -5.144474e-04 -7.416472e-04  1.318548e-04
-5.621567e-04
21 -1.934548e-04  1.811430e-04  2.858368e-04  1.483735e-04 -7.911079e-05
5.110646e-04
22 -1.934922e-04  6.035732e-04  5.715755e-05 -1.112780e-04  5.274123e-05
-3.066074e-04
23 -7.097919e-04  0.000000e+00  1.714531e-04  1.854565e-04 -1.054852e-04
0.000000e+00
24 -7.748935e-04  1.206709e-04  1.142857e-04  3.708717e-05  1.845919e-04
0.000000e+00
25 -5.815644e-04 -4.827711e-04 -4.000572e-04 -1.112656e-04 -1.054769e-04
1.022129e-04
26 -5.172302e-04  1.207146e-04 -4.574042e-04 -4.080800e-04  1.318444e-04
-3.577909e-04
27  2.586486e-04 -6.641109e-04 -4.004004e-04 -5.196155e-04 -5.273566e-05
5.112082e-05
28  1.292992e-04  1.207802e-04  0.000000e+00  1.856080e-04 -1.582237e-04
5.111821e-05
29  1.292825e-04  0.000000e+00 -5.721315e-05  1.113482e-04  1.582237e-04
-3.578824e-04
30  2.585148e-04 -3.623845e-04  1.716296e-04  3.710713e-04 -2.636887e-05
5.113389e-05


yendog

             AAA           BBB           CCC           DDD           EEE
FFF
2   3.228410e-04  6.047229e-05  1.713551e-04  7.414547e-05 -4.744833e-04
-3.070939e-04
3   0.000000e+00  1.209336e-04 -1.142335e-04 -7.414547e-05 -1.318409e-04
0.000000e+00
4  -1.291239e-04 -4.838222e-04 -6.856751e-04 -2.966369e-04 -7.650202e-04
2.047397e-04
5  -1.937171e-04 -1.814937e-04  0.000000e+00  0.000000e+00 -5.278159e-05
2.046978e-04
6   0.000000e+00 -6.050522e-05  2.286106e-04  5.190568e-04  4.749340e-04
5.115613e-04
7   1.291489e-04  3.629764e-04  4.570645e-04  1.482525e-04  1.582612e-04
2.045513e-04
8  -3.229036e-04 -1.814717e-04 -7.428360e-04 -5.931418e-04 -7.123729e-04
-2.045513e-04
9   8.393325e-04  7.256894e-04  1.714727e-04  3.708167e-05  7.917761e-05
-2.557479e-04
10  6.451613e-04  0.000000e+00  1.142988e-04  2.224612e-04 -2.639497e-04
-4.093328e-04
11  1.934673e-04  0.000000e+00 -1.142988e-04 -3.707342e-05 -5.279831e-05
5.117576e-05
12 -1.096810e-03  3.022152e-04  0.000000e+00 -1.483074e-04 -2.376269e-04
1.023437e-04
13  7.098377e-04 -1.208751e-04  6.855968e-04  3.707274e-04  2.640578e-05
7.672438e-04
14  1.290073e-04 -2.417941e-04 -1.713551e-04 -7.413448e-05  2.640194e-04
1.533782e-04
16  9.022945e-04  2.171685e-03  6.281945e-04  6.668643e-04  5.803831e-04
4.087681e-04
17 -6.442261e-05 -7.233709e-04  1.712573e-04 -7.407407e-05  1.054880e-04
-1.021764e-04
18 -1.224819e-03 -1.206782e-03 -1.142270e-03 -8.151772e-04 -7.911497e-05
0.000000e+00
19  3.869470e-04 -1.811430e-04 -5.144474e-04 -7.416472e-04  1.318548e-04
-5.621567e-04
20 -1.934548e-04  1.811430e-04  2.858368e-04  1.483735e-04 -7.911079e-05
5.110646e-04
21 -1.934922e-04  6.035732e-04  5.715755e-05 -1.112780e-04  5.274123e-05
-3.066074e-04
22 -7.097919e-04  0.000000e+00  1.714531e-04  1.854565e-04 -1.054852e-04
0.000000e+00
23 -7.748935e-04  1.206709e-04  1.142857e-04  3.708717e-05  1.845919e-04
0.000000e+00
24 -5.815644e-04 -4.827711e-04 -4.000572e-04 -1.112656e-04 -1.054769e-04
1.022129e-04
25 -5.172302e-04  1.207146e-04 -4.574042e-04 -4.080800e-04  1.318444e-04
-3.577909e-04
26  2.586486e-04 -6.641109e-04 -4.004004e-04 -5.196155e-04 -5.273566e-05
5.112082e-05
27  1.292992e-04  1.207802e-04  0.000000e+00  1.856080e-04 -1.582237e-04
5.111821e-05
28  1.292825e-04  0.000000e+00 -5.721315e-05  1.113482e-04  1.582237e-04
-3.578824e-04
29  2.585148e-04 -3.623845e-04  1.716296e-04  3.710713e-04 -2.636887e-05
5.113389e-05
30  6.461827e-05  3.623845e-04 -5.720660e-05 -7.420324e-05  1.054713e-04
-2.045513e-04
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From rmh at temple.edu  Tue Jul  3 20:38:39 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Tue,  3 Jul 2007 14:38:39 -0400 (EDT)
Subject: [R] Convetring a dataframe  so that it just has one column
Message-ID: <20070703143839.CFO17360@po-d.temple.edu>

?stack

tmp <- data.frame(a=1:10, b=11:20)
stack(tmp)


From selenezarate at gmail.com  Tue Jul  3 20:39:48 2007
From: selenezarate at gmail.com (=?ISO-8859-1?Q?Selene_Z=E1rate?=)
Date: Tue, 3 Jul 2007 11:39:48 -0700
Subject: [R] exact AIC
Message-ID: <9ed3fa0707031139t6a3e8deu63272fcb91a2a477@mail.gmail.com>

Hi,

I am using the stepAIC function on the MASS package that does model
selection by exact AIC. I'm not sure what the term 'exact AIC' means,
and how it differs from  AIC.

Thank you,

Selene Zarate


From szhan at uoguelph.ca  Tue Jul  3 20:45:56 2007
From: szhan at uoguelph.ca (szhan at uoguelph.ca)
Date: Tue, 03 Jul 2007 14:45:56 -0400
Subject: [R] how to calculate interaction contrast
Message-ID: <20070703144556.56h6gdu5q88g4wkk@webmail.uoguelph.ca>


Hello, R experts,
Sorry for asking this question again since I really want a help!

I have a two-factor experiment data and like to calculate estimates of
interation contrasts say factor A has levels of a1, a2, and B has
levels of b1, b2, b3, b4, and b5 with 3 replicates. I am not sure the
constrast estimate I got is right using the script below:

score<-c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)

A <- gl(2, 15, labels=c("a1", "a2"))
B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)

contrasts(B)<-cbind(c(-4,rep(1,4)),c(rep(-3,2),rep(2,3)),
+  c(rep(-2,3),rep(3,2)),c(rep(-1,4), rep(4,1)))
fit1 <- aov(score ~ A*B)
summary(fit1, split=list(B=1:4), expand.split = TRUE)
               Df Sum Sq Mean Sq F value    Pr(>F)
A            1 3.2013  3.2013 15.1483 0.0009054 ***
B            4 8.7780  2.1945 10.3841 0.0001019 ***
     B: C1      1 0.0301  0.0301  0.1424 0.7099296
     B: C2      1 2.0335  2.0335  9.6221 0.0056199 **
     B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
     B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
A:B          4 5.3420  1.3355  6.3194 0.0018616 **
     A:B: C1    1 0.7207  0.7207  3.4105 0.0796342 .
     A:B: C2    1 2.6068  2.6068 12.3350 0.0021927 **
     A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
     A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
Residuals   20 4.2267  0.2113
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Now I like to get interaction contrast estimate for b1 and b2 vs b3, b4 and b5
cont <- c(1, -1)[A] * c(-3, -3, 2, 2, 2)[B]

estimat<-sum(cont*score) # value of the contrast estimate for A:B C2

> estimat
[1] -24.1

I am not sure the estimate for A:B C2 contrast  (-24.1) is correct
because the F value given the output above(12.3350) does not equal to
those I calculate below (15.2684):

t.stat <- sum(cont*score)/se.contrast(fit1, as.matrix(cont))
> t.stat^2
Contrast 1
     15.2684

Could you please help me calculate the correct the estimate of
interaction contrast and corresponding F value?
Thanks in advance!
Joshua

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


----- End forwarded message -----


From helprhelp at gmail.com  Tue Jul  3 20:55:19 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 3 Jul 2007 14:55:19 -0400
Subject: [R] Installing packages...
In-Reply-To: <45B66B6D.6060707@itu.edu.tr>
References: <45B66B6D.6060707@itu.edu.tr>
Message-ID: <cdf817830707031155t2e2f0814v94e8898ca383b283@mail.gmail.com>

I have the exact same problem as you had when I customerized some
package: the way I go around this is using

install.packages("~/Documents/projects/R_customerized_packages/supclust_1.0-5.tar.gz",
repos=NULL, type="source")

in which supclust_... is built from
R CMD build supclust

and in your case, you just need to download the package if you insist
on using downloaded package.

HTH,

Weiwei

On 1/23/07, N?zhet Dalfes <dalfes at itu.edu.tr> wrote:
> Hi,
>
> I am a total newbie to R. I am using R (2.4.1) on Mac OS X 10.4.8 and
> trying to install some packages using GUI Packages & Data/Package Installer
> interface...
>
> Every time I get:
>
> trying URL
> 'http://umfragen.sowi.uni-mainz.de/CRAN/bin/macosx/universal/contrib/2.4/neural_1.4.1.tgz'
> Content type 'application/x-tar' length 18920 bytes
> opened URL
> ==================================================
> downloaded 18Kb
>
> Error in gzfile(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file 'neural/DESCRIPTION'
> >
>
> What am I doing wrong?
>
> Any help will be much appreciated.
>
> N?zhet Dalfes
>
> Istanbul Tech. Univ.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From jrkrideau at yahoo.ca  Tue Jul  3 21:07:46 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Tue, 3 Jul 2007 15:07:46 -0400 (EDT)
Subject: [R] vertically concatenating data frames
In-Reply-To: <755261CA22782948B1C42ACDC83912A1046140A2@NYWEXMB27.msad.ms.com>
Message-ID: <77414.85106.qm@web32813.mail.mud.yahoo.com>

?rbind


--- "Aydemir, Zava (FID)"
<Zava.Aydemir at morganstanley.com> wrote:

> Hi,
>  
> what is the recommended way to vertically
> concatenate 2 data frames with
> the same column names but different number of rows?
>  
> My problem is something along these lines:
>  
> df1 <- data.frame(var1=var1,var2=var2,var3=var3)  #
> nrow(df1)=1000
> df2 <- data.frame(var1=var4,var2=var5,var3=var6)  #
> nrow(df2)=2000
>  
> I tried df <- c(df1,df2), no success. stack does not
> seem to be
> appropriate either for my problem.
>  
>  
> Thanks
>  
> Zava
>
--------------------------------------------------------
> 
> This is not an offer (or solicitation of an offer)
> to buy/se...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From spencer.graves at pdf.com  Tue Jul  3 21:06:30 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 Jul 2007 12:06:30 -0700
Subject: [R] how to solve a min problem
In-Reply-To: <e591a95b0707020748p2650666bs972a7f28c8001a31@mail.gmail.com>
References: <e591a95b0707020748p2650666bs972a7f28c8001a31@mail.gmail.com>
Message-ID: <468A9E36.3030307@pdf.com>

      Do you mean

      minimize mu with 0 < b_func(S+mu) < 800? 

      For this kind of problem, I'd first want to know the nature of 
"b_func".  Without knowing more, I might try to plot b_func(S+mu) vs. 
mu, then maybe use 'optimize'. 

      If this is not what you mean, please be more specific:  I'm 
confused. 

      Hope this helps. 
      Spencer Graves

domenico pestalozzi wrote:
> I know it's possible to solve max e min problems  by using these functions:
>
> nlm, optimize, optim
>
> but I don't know how to use them (...if possible...) to solve this problem.
>
> I have a personal function called  b_func(S) where S is an input array (1 X
> n)  and I'd like:
>
> minimize mean(S) with 0 < b_funct < 800.
>
> I know that the solution exists, but It's possible to calculate it in R?
> The b_func is non linear and it calculates a particular value using S as
> input and applying a convergent iterative algorithm.
>
> thanks
>
>
> domenico
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From helprhelp at gmail.com  Tue Jul  3 21:20:15 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 3 Jul 2007 15:20:15 -0400
Subject: [R] dlda{supclust} 's output
In-Reply-To: <033101c7909e$842a84c0$7f14a8c0@win.ad.jhu.edu>
References: <cdf817830705011450m51381074s82e3d5d4f42c1c61@mail.gmail.com>
	<033101c7909e$842a84c0$7f14a8c0@win.ad.jhu.edu>
Message-ID: <cdf817830707031220r21a8661j17c92ad6aff481b8@mail.gmail.com>

Hi,

I modified dlda{supclust} so that the original example in ?dlda gives
the following output:

> set.seed(342)
> xlearn <- matrix(rnorm(200), nrow = 20, ncol = 10)
>
> ## Generating random test data: 8 observations and 10 variables(clusters)
> xtest  <- matrix(rnorm(80),  nrow = 8,  ncol = 10)
>
> ## Generating random class labels for the learning data
> ylearn <- as.numeric(runif(20)>0.5)
>
> ## Predicting the class labels for the test data
>
> t0 = dlda(xlearn, xtest, ylearn)
> t0
          [,1]     [,2]
[1,] 17.595758 21.20141
[2,] 11.882305 20.34470
[3,]  7.837422 12.47240
[4,] 11.025810 12.04523
[5,] 18.167740 15.91930
[6,] 11.396010  9.26949
[7,] 33.911010 26.06992
[8,] 16.140149 19.83915

(to be noticed: the above is anti-probabilities, which means the
smaller, the higher prob for being the label of colname, for example,
sample 5, the class label is predicted as 1 instead of 0)

Here I have one question about it:

since apply(t0, 1, sum) does not give the same sum, I am wondering if
standardization is a proper way to compare the probabilities "BETWEEN"
samples, following the understanding of dlda algorithm.



Thanks,

Weiwei

On 5/7/07, Marcel Dettling <mdettling at bluewin.ch> wrote:
> Hi Weiwei,
>
> it would be possible to obtain probabilities instead of just a 0/1
> output. The code needs to be altered though. Sorry I don't have the time
> to do that. But R is open source and contributions are most welcome.
>
> I'm sorry not to be able of more help,
>
> Marcel
>
> --------------------------------------
> Marcel Dettling
> Phone:  +41 79 489 72 04
> E-Mail: mdettling at bluewin.ch
> Web:    http://stat.ethz.ch/~dettling
> --------------------------------------
> ----- Original Message -----
> From: "Weiwei Shi" <helprhelp at gmail.com>
> To: "R Help" <R-help at stat.math.ethz.ch>
> Cc: <dettling at stat.math.ethz.ch>
> Sent: Tuesday, May 01, 2007 11:50 PM
> Subject: dlda{supclust} 's output
>
>
> > Hi,
> >
> > I am using dlda algorithm from supclust package and I am wondering if
> > the output can be a continuous probability instead of discrete class
> > label (zero or one) since it puts some restriction on convariance
> > matrix, compared with lda, while the latter can.
> >
> > thanks,
> >
> > --
> > Weiwei Shi, Ph.D
> > Research Scientist
> > GeneGO, Inc.
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
> >
> > !DSPAM:4637b61518111667610022!
> >
>
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From helprhelp at gmail.com  Tue Jul  3 21:27:00 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 3 Jul 2007 15:27:00 -0400
Subject: [R] reinforce library to re-load
Message-ID: <cdf817830707031227y7e076470nd8597967935c5e6@mail.gmail.com>

Hi,

I am wondering if there is a parameter in library() so that it can
reinforce package to be reloaded. It helps when you test your modified
package by yourself. Otherwise, my way is to re-start Rgui.

(by reading ?library, I understand this option is not implemented)
"...Both functions check and update the list of currently loaded
packages and do not reload a package which is already loaded.
(Furthermore, if the package has a name space and a name space of that
name is already loaded, they work from the existing names space rather
than reloading from the file system.)"

Thanks.

-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From HDoran at air.org  Tue Jul  3 21:35:11 2007
From: HDoran at air.org (Doran, Harold)
Date: Tue, 3 Jul 2007 15:35:11 -0400
Subject: [R] reinforce library to re-load
In-Reply-To: <cdf817830707031227y7e076470nd8597967935c5e6@mail.gmail.com>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EE57A07@dc1ex01.air.org>

I think you want to use detach() 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
> Sent: Tuesday, July 03, 2007 3:27 PM
> To: r-help at stat.math.ethz.ch
> Cc: R-devel at stat.math.ethz.ch
> Subject: [R] reinforce library to re-load
> 
> Hi,
> 
> I am wondering if there is a parameter in library() so that 
> it can reinforce package to be reloaded. It helps when you 
> test your modified package by yourself. Otherwise, my way is 
> to re-start Rgui.
> 
> (by reading ?library, I understand this option is not 
> implemented) "...Both functions check and update the list of 
> currently loaded packages and do not reload a package which 
> is already loaded.
> (Furthermore, if the package has a name space and a name 
> space of that name is already loaded, they work from the 
> existing names space rather than reloading from the file system.)"
> 
> Thanks.
> 
> --
> Weiwei Shi, Ph.D
> Research Scientist
> GeneGO, Inc.
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From skewsie at yahoo.com  Tue Jul  3 21:40:58 2007
From: skewsie at yahoo.com (Susie Iredale)
Date: Tue, 3 Jul 2007 12:40:58 -0700 (PDT)
Subject: [R] Dealing with imported data
Message-ID: <733142.45961.qm@web34512.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/79eb5bb7/attachment.pl 

From skewsie at yahoo.com  Tue Jul  3 21:49:44 2007
From: skewsie at yahoo.com (Susie Iredale)
Date: Tue, 3 Jul 2007 12:49:44 -0700 (PDT)
Subject: [R] Problems using imported data
Message-ID: <20070703194944.88895.qmail@web34501.mail.mud.yahoo.com>




(Repeat of previous HTML version)

Hello all,

I am a new R user and I have finally imported my data using
>read.delim("Filename.txt", header=TRUE) after some difficulty, by changing file directories (a hint to anyone who might be stuck there).

However, I am now stuck trying to use my data.  When I try to use data.frame("filename.txt") it tells me object not found, which makes it difficult to use attach() or with().  How do I get R to recognize my data?  

Thanks,
Susie
PhD Student UCI




      ____________________________________________________________________________________
Luggage? GPS? Comic books?


From pcrutcher at gmail.com  Tue Jul  3 21:52:50 2007
From: pcrutcher at gmail.com (Patrick C.)
Date: Tue, 3 Jul 2007 15:52:50 -0400
Subject: [R] Non-linear constraints under Markowitz
Message-ID: <7d4c2810707031252v9c30fa5wa4f397b368d2297d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/2201f719/attachment.pl 

From jholtman at gmail.com  Tue Jul  3 21:59:36 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 3 Jul 2007 15:59:36 -0400
Subject: [R] Problems using imported data
In-Reply-To: <20070703194944.88895.qmail@web34501.mail.mud.yahoo.com>
References: <20070703194944.88895.qmail@web34501.mail.mud.yahoo.com>
Message-ID: <644e1f320707031259v9ac77d3ibb38c41f256d1475@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/111df51d/attachment.pl 

From phhs80 at gmail.com  Tue Jul  3 22:10:19 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 3 Jul 2007 21:10:19 +0100
Subject: [R] Fine tunning rgenoud
Message-ID: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>

Dear All,

I am trying to solve the following maximization problem, but I cannot
have rgenoud giving me a reliable solution.

Any ideas?

Thanks in advance,

Paul

----------------------------
library(rgenoud)

v <- 0.90
O1 <- 10
O2 <- 20
O0 <- v*O1+(1-v)*O2

myfunc <- function(x) {
  U0 <- x[1]
  U1 <- x[2]
  U2 <- x[3]
  q0 <- x[4]
  q1 <- x[5]
  q2 <- x[6]
  p <- x[7]

  if (U0 < 0)
    return(-1e+200)
  else if (U1 < 0)
    return(-1e+200)
  else if (U2 < 0)
    return(-1e+200)
  else if ((U0-(U1+(O1-O0)*q1)) < 0)
    return(-1e+200)
  else if ((U0-(U2+(O2-O0)*q2)) < 0)
    return(-1e+200)
  else if ((U1-(U0+(O0-O1)*q0)) < 0)
    return(-1e+200)
  else if ((U1-(U2+(O2-O1)*q2)) < 0)
    return(-1e+200)
  else if((U2-(U0+(O0-O2)*q0)) < 0)
    return(-1e+200)
  else if((U2-(U1+(O1-O2)*q1)) < 0)
    return(-1e+200)
  else if(p < 0)
    return(-1e+200)
  else if(p > 1)
    return(-1e+200)
  else if(q0 < 0)
    return(-1e+200)
  else if(q1 < 0)
    return(-1e+200)
  else if(q2 < 0)
    return(-1e+200)
  else return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2)-(O2*q2+U2))))

}
genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.generations=150,max.generations=300,boundary.enforcement=2)


From Mark.Leeds at morganstanley.com  Tue Jul  3 22:16:01 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Tue, 3 Jul 2007 16:16:01 -0400
Subject: [R] Non-linear constraints under Markowitz
In-Reply-To: <7d4c2810707031252v9c30fa5wa4f397b368d2297d@mail.gmail.com>
References: <7d4c2810707031252v9c30fa5wa4f397b368d2297d@mail.gmail.com>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401957475@NYWEXMB23.msad.ms.com>

I think that problem has a complicated closed form solution but I'm not
sure which text it is in.
It might be in Ingersoll, Financial Decision Making. I'm sorry that I
can't be less vague. 
It's also possible to derive it using a Langrange Multiplier. I did it
once but that was a long time ago.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick C.
Sent: Tuesday, July 03, 2007 3:53 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Non-linear constraints under Markowitz

I am hoping to do some portfolio optimization where I want to maximize
my possible return subject to the constraint that my variance is below a
certain value and no short positions. Is there a way I can use optim to
do this ? thanks

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From ssj1364 at gmail.com  Tue Jul  3 22:29:29 2007
From: ssj1364 at gmail.com (sj)
Date: Tue, 3 Jul 2007 14:29:29 -0600
Subject: [R] Statistics Question not R question: competing risks and
	non-informative censoring
Message-ID: <1c6126db0707031329h20606b79u73e8f3edbf70f3fb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/ec3e6f33/attachment.pl 

From Roy.Mendelssohn at noaa.gov  Tue Jul  3 22:30:00 2007
From: Roy.Mendelssohn at noaa.gov (Roy Mendelssohn)
Date: Tue, 03 Jul 2007 13:30:00 -0700
Subject: [R] (no subject)
Message-ID: <28C8977E-B2E9-4190-A527-40DE08392AEF@noaa.gov>

Hi:

We are having some very strange problems with R on WIndows, both with  
R 2.4.1 and R 2.5.0.  The following command:

> download.file(url="http://oceanwatch.pfeg.noaa.gov:8081/thredds/wcs/ 
> satellite/AG/ssta/14day? 
> request=GetCoverage&version=1.0.0&service=WCS&format=NetCDF3&coverage= 
> AGssta&Vertical=.0&time=2006-01-06T00:00:00Z&bbox=220,20,250,50",  
> destfile="junk.nc")
>

on my Mac downloads a netcdf file, which I can open just fine using  
the ncdf package, and displays just fine using the netcdf utility  
"ncdump -h"

The same command under Windows downloads a file with that name and  
with the correct size.  A cmp on the two files says there are  
differences, and using a hex editor confirms this.  The file produces  
a simple error  when using "open.ncdf" and in fact the ncdump utility  
produces:

  rmendels% /sw/bin/ncdump -h junk1.nc
/sw/bin/ncdump: junk1.nc: Invalid argument


There is clearly something corrupted in the Wndows version, and yet  
it works perfectly on my Mac.  I have produced this error on two  
different Windows machines with two different versions of R.

Any help appreciated

-Roy M.

**********************
"The contents of this message do not reflect any position of the U.S.  
Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division	
Southwest Fisheries Science Center
1352 Lighthouse Avenue
Pacific Grove, CA 93950-2097

e-mail: Roy.Mendelssohn at noaa.gov (Note new e-mail address)
voice: (831)-648-9029
fax: (831)-648-8440
www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."


From rmadero.hulp at salud.madrid.org  Tue Jul  3 22:41:57 2007
From: rmadero.hulp at salud.madrid.org (Madero Jarabo, Rosario)
Date: Tue, 3 Jul 2007 22:41:57 +0200
Subject: [R] Harrell's C
Message-ID: <D014BFACDE98D04A959B126EB9934FDC02E54B0E@cexmadbe0301.tdatamail.tde>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/c92a39c1/attachment.pl 

From Terrence.Murphy at yale.edu  Tue Jul  3 22:42:36 2007
From: Terrence.Murphy at yale.edu (Terrence Murphy)
Date: Tue, 03 Jul 2007 16:42:36 -0400
Subject: [R] Kullback-Leibler divergence
Message-ID: <6.0.1.1.2.20070703163334.01cbcdf0@email.med.yale.edu>

Dear R users,

Is anyone willing to share some sample code using R that calculates the 
Kullback-Leibler divergence in information contained in the the posterior 
distribution relative to the prior for some simple continuous distribution 
such as a normal?  We are thinking of an informative prior and using 
Kullback-Leibler to quantify how much information is gained from the 
merging of the data with the prior.




Terrence E. Murphy, Ph.D.
Program on Aging
Yale University
300 George St., Suite 775
New Haven, CT 06511
terrence.murphy at yale.edu
phone: 203-737-2295
fax:      203-785-4823


From rvaradhan at jhmi.edu  Tue Jul  3 22:50:03 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 3 Jul 2007 16:50:03 -0400
Subject: [R] Fine tunning rgenoud
In-Reply-To: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
Message-ID: <000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>

Paul,

You had indicated in your previous email that you are having trouble finding
a feasible starting value for constrOptim().  So, you basically need to
solve a system of linear inequalities to obtain a starting point.  Have you
considered using linear programming? Either simplex() in the "boot" package
or solveLP() in "linprog" would work.  It seems to me that you could use any
linear objective function in solveLP to obtain a feasible starting point.
This is not the most efficient solution, but it might be worth a try. 

I am aware of other methods for generating n-tuples that satisfy linear
inequality constraints, but AFAIK those are not available in R.

Best,
Ravi.


----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
Sent: Tuesday, July 03, 2007 4:10 PM
To: R-help
Subject: [R] Fine tunning rgenoud

Dear All,

I am trying to solve the following maximization problem, but I cannot
have rgenoud giving me a reliable solution.

Any ideas?

Thanks in advance,

Paul

----------------------------
library(rgenoud)

v <- 0.90
O1 <- 10
O2 <- 20
O0 <- v*O1+(1-v)*O2

myfunc <- function(x) {
  U0 <- x[1]
  U1 <- x[2]
  U2 <- x[3]
  q0 <- x[4]
  q1 <- x[5]
  q2 <- x[6]
  p <- x[7]

  if (U0 < 0)
    return(-1e+200)
  else if (U1 < 0)
    return(-1e+200)
  else if (U2 < 0)
    return(-1e+200)
  else if ((U0-(U1+(O1-O0)*q1)) < 0)
    return(-1e+200)
  else if ((U0-(U2+(O2-O0)*q2)) < 0)
    return(-1e+200)
  else if ((U1-(U0+(O0-O1)*q0)) < 0)
    return(-1e+200)
  else if ((U1-(U2+(O2-O1)*q2)) < 0)
    return(-1e+200)
  else if((U2-(U0+(O0-O2)*q0)) < 0)
    return(-1e+200)
  else if((U2-(U1+(O1-O2)*q1)) < 0)
    return(-1e+200)
  else if(p < 0)
    return(-1e+200)
  else if(p > 1)
    return(-1e+200)
  else if(q0 < 0)
    return(-1e+200)
  else if(q1 < 0)
    return(-1e+200)
  else if(q2 < 0)
    return(-1e+200)
  else
return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
)-(O2*q2+U2))))

}
genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
rations=150,max.generations=300,boundary.enforcement=2)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From phhs80 at gmail.com  Tue Jul  3 23:09:30 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 3 Jul 2007 22:09:30 +0100
Subject: [R] Fine tunning rgenoud
In-Reply-To: <000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
	<000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
Message-ID: <6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>

On 7/3/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> You had indicated in your previous email that you are having trouble finding
> a feasible starting value for constrOptim().  So, you basically need to
> solve a system of linear inequalities to obtain a starting point.  Have you
> considered using linear programming? Either simplex() in the "boot" package
> or solveLP() in "linprog" would work.  It seems to me that you could use any
> linear objective function in solveLP to obtain a feasible starting point.
> This is not the most efficient solution, but it might be worth a try.
>
> I am aware of other methods for generating n-tuples that satisfy linear
> inequality constraints, but AFAIK those are not available in R.

Thanks, Ravi. I had already conceived the solution that you suggest,
actually using "lpSolve". I am able to get a solution for my problem
with constrOptim, but I am not enough confident that the solution is
right. That is why I am trying to get a solution with rgenoud, but
unsuccessfully until now.

Paul



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> Sent: Tuesday, July 03, 2007 4:10 PM
> To: R-help
> Subject: [R] Fine tunning rgenoud
>
> Dear All,
>
> I am trying to solve the following maximization problem, but I cannot
> have rgenoud giving me a reliable solution.
>
> Any ideas?
>
> Thanks in advance,
>
> Paul
>
> ----------------------------
> library(rgenoud)
>
> v <- 0.90
> O1 <- 10
> O2 <- 20
> O0 <- v*O1+(1-v)*O2
>
> myfunc <- function(x) {
>   U0 <- x[1]
>   U1 <- x[2]
>   U2 <- x[3]
>   q0 <- x[4]
>   q1 <- x[5]
>   q2 <- x[6]
>   p <- x[7]
>
>   if (U0 < 0)
>     return(-1e+200)
>   else if (U1 < 0)
>     return(-1e+200)
>   else if (U2 < 0)
>     return(-1e+200)
>   else if ((U0-(U1+(O1-O0)*q1)) < 0)
>     return(-1e+200)
>   else if ((U0-(U2+(O2-O0)*q2)) < 0)
>     return(-1e+200)
>   else if ((U1-(U0+(O0-O1)*q0)) < 0)
>     return(-1e+200)
>   else if ((U1-(U2+(O2-O1)*q2)) < 0)
>     return(-1e+200)
>   else if((U2-(U0+(O0-O2)*q0)) < 0)
>     return(-1e+200)
>   else if((U2-(U1+(O1-O2)*q1)) < 0)
>     return(-1e+200)
>   else if(p < 0)
>     return(-1e+200)
>   else if(p > 1)
>     return(-1e+200)
>   else if(q0 < 0)
>     return(-1e+200)
>   else if(q1 < 0)
>     return(-1e+200)
>   else if(q2 < 0)
>     return(-1e+200)
>   else
> return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
> )-(O2*q2+U2))))
>
> }
> genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
> rations=150,max.generations=300,boundary.enforcement=2)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.harrell at vanderbilt.edu  Tue Jul  3 23:18:21 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 03 Jul 2007 16:18:21 -0500
Subject: [R] Harrell's C
In-Reply-To: <D014BFACDE98D04A959B126EB9934FDC02E54B0E@cexmadbe0301.tdatamail.tde>
References: <D014BFACDE98D04A959B126EB9934FDC02E54B0E@cexmadbe0301.tdatamail.tde>
Message-ID: <468ABD1D.2060205@vanderbilt.edu>

Madero Jarabo, Rosario wrote:
> I need to calculate Harrell's C for some survival analyses using Design package with R version 2.4.1. ?How can I try or do it?
> 
> Rosario Madero
> Secci?n de Bioestad?stica
> Hospital Universitario La Paz
> P?de la Castellana, 261
> 28046 Madrid, Espa?a
> Tfno: 917277112
> rmadero.hulp at salud.madrid.org

It's in the documention with the Hmisc package.  Type
?rcorr.cens
?rcorrp.cens

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From s.blomberg1 at uq.edu.au  Wed Jul  4 01:04:46 2007
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Wed, 04 Jul 2007 09:04:46 +1000
Subject: [R] Parsimony Analysis of Encemism in R?
In-Reply-To: <813951.67055.qm@web56615.mail.re3.yahoo.com>
References: <813951.67055.qm@web56615.mail.re3.yahoo.com>
Message-ID: <1183503886.4806.2.camel@sib-sblomber01d.sib.uq.edu.au>

Short answer: No.

Longer answer:

R currently has some useful but relatively limited functions for
phylogenetics. See the "Statistical genetics" task view on CRAN. The ape
package is the most full-featured.

Simon.

On Tue, 2007-07-03 at 07:59 -0700, Milton Cezar Ribeiro wrote:
> Hi R-gurus, 
> 
> Is there a package for "Parsimony Analysis of Endemism" (Cladist) in R?
> 
> Kind regards,
> 
> Miltinho
> Brazil
> 
> 
>        
> ____________________________________________________________________________________
> 
> http://yahoo.com.br/oqueeuganhocomisso 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia

Room 320, Goddard Building (8)
T: +61 7 3365 2506 
email: S.Blomberg1_at_uq.edu.au 

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.


From rvaradhan at jhmi.edu  Wed Jul  4 01:02:53 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 3 Jul 2007 19:02:53 -0400
Subject: [R] Fine tunning rgenoud
In-Reply-To: <6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
	<000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>
Message-ID: <000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>

Paul,

It should be easy enough to check that your solution is valid (i.e. a local
minimum):  first, check to see if the solution satisfies all the
constraints; secondly, check to see if it is an interior point (i.e. none of
the constraints become equality); and finally, if the solution is an
interior point, check to see whether the gradient there is close to zero.
Note that if the solution is one of the vertices of the polyhedron, then the
gradient may not be zero.

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
Sent: Tuesday, July 03, 2007 5:10 PM
To: R-help
Subject: Re: [R] Fine tunning rgenoud

On 7/3/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> You had indicated in your previous email that you are having trouble
finding
> a feasible starting value for constrOptim().  So, you basically need to
> solve a system of linear inequalities to obtain a starting point.  Have
you
> considered using linear programming? Either simplex() in the "boot"
package
> or solveLP() in "linprog" would work.  It seems to me that you could use
any
> linear objective function in solveLP to obtain a feasible starting point.
> This is not the most efficient solution, but it might be worth a try.
>
> I am aware of other methods for generating n-tuples that satisfy linear
> inequality constraints, but AFAIK those are not available in R.

Thanks, Ravi. I had already conceived the solution that you suggest,
actually using "lpSolve". I am able to get a solution for my problem
with constrOptim, but I am not enough confident that the solution is
right. That is why I am trying to get a solution with rgenoud, but
unsuccessfully until now.

Paul



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> Sent: Tuesday, July 03, 2007 4:10 PM
> To: R-help
> Subject: [R] Fine tunning rgenoud
>
> Dear All,
>
> I am trying to solve the following maximization problem, but I cannot
> have rgenoud giving me a reliable solution.
>
> Any ideas?
>
> Thanks in advance,
>
> Paul
>
> ----------------------------
> library(rgenoud)
>
> v <- 0.90
> O1 <- 10
> O2 <- 20
> O0 <- v*O1+(1-v)*O2
>
> myfunc <- function(x) {
>   U0 <- x[1]
>   U1 <- x[2]
>   U2 <- x[3]
>   q0 <- x[4]
>   q1 <- x[5]
>   q2 <- x[6]
>   p <- x[7]
>
>   if (U0 < 0)
>     return(-1e+200)
>   else if (U1 < 0)
>     return(-1e+200)
>   else if (U2 < 0)
>     return(-1e+200)
>   else if ((U0-(U1+(O1-O0)*q1)) < 0)
>     return(-1e+200)
>   else if ((U0-(U2+(O2-O0)*q2)) < 0)
>     return(-1e+200)
>   else if ((U1-(U0+(O0-O1)*q0)) < 0)
>     return(-1e+200)
>   else if ((U1-(U2+(O2-O1)*q2)) < 0)
>     return(-1e+200)
>   else if((U2-(U0+(O0-O2)*q0)) < 0)
>     return(-1e+200)
>   else if((U2-(U1+(O1-O2)*q1)) < 0)
>     return(-1e+200)
>   else if(p < 0)
>     return(-1e+200)
>   else if(p > 1)
>     return(-1e+200)
>   else if(q0 < 0)
>     return(-1e+200)
>   else if(q1 < 0)
>     return(-1e+200)
>   else if(q2 < 0)
>     return(-1e+200)
>   else
>
return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
> )-(O2*q2+U2))))
>
> }
>
genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
> rations=150,max.generations=300,boundary.enforcement=2)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Wed Jul  4 01:10:35 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 03 Jul 2007 19:10:35 -0400
Subject: [R] bug in closing gzfile-opened connections?
In-Reply-To: <fd913b0d0707031037o37629382ke6cb28af5fc56b76@mail.gmail.com>
References: <fd913b0d0707031037o37629382ke6cb28af5fc56b76@mail.gmail.com>
Message-ID: <468AD76B.7060209@stats.uwo.ca>

On 03/07/2007 1:37 PM, David Reiss wrote:
> Hi,
> I am making multiple calls to gzfile() via read.table(), e.g.
> 
>> x <- read.table( gzfile( "xxx.gz" ) )
> 
> After i do this many times (I haven't counted, but probably between 50 and
> 100 times) I get the error message:
> 
> Error in open.connection(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open compressed file 'xxx.gz'
> 
> however, I also find that:
> 
>> showConnections()
>      description class mode text isopen can read can write
> 
> so there are no (apparently) open connections. Calling closeAllConnections()
> does not fix the problem. I have to quit and re-start R.
> I am using R 2.5.0 on a Mac (OSX 10.4.9).
> 
> Anyone know if this is a bug or a 'feature'? I see from the gzfile help
> that:
> 
>  In general functions using connections
>      will open them if they are not open, but then close them again, so
>      to leave a connection open call 'open' explicitly.

You didn't give a reproducible example, so I couldn't say.  When I 
create a gzipped version of a write.table output and run

for(i in 1:1000) read.table(gzfile(f))

in R 2.5.0 I don't see a problem.  This is on Windows, but I doubt that 
makes a difference.

Duncan Murdoch


From phhs80 at gmail.com  Wed Jul  4 01:13:49 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 4 Jul 2007 00:13:49 +0100
Subject: [R] Fine tunning rgenoud
In-Reply-To: <000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
	<000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>
	<000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>
Message-ID: <6ade6f6c0707031613r4e54ba71mbc25b5356c614c59@mail.gmail.com>

On 7/4/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> It should be easy enough to check that your solution is valid (i.e. a local
> minimum):  first, check to see if the solution satisfies all the
> constraints; secondly, check to see if it is an interior point (i.e. none of
> the constraints become equality); and finally, if the solution is an
> interior point, check to see whether the gradient there is close to zero.
> Note that if the solution is one of the vertices of the polyhedron, then the
> gradient may not be zero.

That is a very good idea, Ravi! Thanks!

Paul



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> Sent: Tuesday, July 03, 2007 5:10 PM
> To: R-help
> Subject: Re: [R] Fine tunning rgenoud
>
> On 7/3/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> > You had indicated in your previous email that you are having trouble
> finding
> > a feasible starting value for constrOptim().  So, you basically need to
> > solve a system of linear inequalities to obtain a starting point.  Have
> you
> > considered using linear programming? Either simplex() in the "boot"
> package
> > or solveLP() in "linprog" would work.  It seems to me that you could use
> any
> > linear objective function in solveLP to obtain a feasible starting point.
> > This is not the most efficient solution, but it might be worth a try.
> >
> > I am aware of other methods for generating n-tuples that satisfy linear
> > inequality constraints, but AFAIK those are not available in R.
>
> Thanks, Ravi. I had already conceived the solution that you suggest,
> actually using "lpSolve". I am able to get a solution for my problem
> with constrOptim, but I am not enough confident that the solution is
> right. That is why I am trying to get a solution with rgenoud, but
> unsuccessfully until now.
>
> Paul
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> > Sent: Tuesday, July 03, 2007 4:10 PM
> > To: R-help
> > Subject: [R] Fine tunning rgenoud
> >
> > Dear All,
> >
> > I am trying to solve the following maximization problem, but I cannot
> > have rgenoud giving me a reliable solution.
> >
> > Any ideas?
> >
> > Thanks in advance,
> >
> > Paul
> >
> > ----------------------------
> > library(rgenoud)
> >
> > v <- 0.90
> > O1 <- 10
> > O2 <- 20
> > O0 <- v*O1+(1-v)*O2
> >
> > myfunc <- function(x) {
> >   U0 <- x[1]
> >   U1 <- x[2]
> >   U2 <- x[3]
> >   q0 <- x[4]
> >   q1 <- x[5]
> >   q2 <- x[6]
> >   p <- x[7]
> >
> >   if (U0 < 0)
> >     return(-1e+200)
> >   else if (U1 < 0)
> >     return(-1e+200)
> >   else if (U2 < 0)
> >     return(-1e+200)
> >   else if ((U0-(U1+(O1-O0)*q1)) < 0)
> >     return(-1e+200)
> >   else if ((U0-(U2+(O2-O0)*q2)) < 0)
> >     return(-1e+200)
> >   else if ((U1-(U0+(O0-O1)*q0)) < 0)
> >     return(-1e+200)
> >   else if ((U1-(U2+(O2-O1)*q2)) < 0)
> >     return(-1e+200)
> >   else if((U2-(U0+(O0-O2)*q0)) < 0)
> >     return(-1e+200)
> >   else if((U2-(U1+(O1-O2)*q1)) < 0)
> >     return(-1e+200)
> >   else if(p < 0)
> >     return(-1e+200)
> >   else if(p > 1)
> >     return(-1e+200)
> >   else if(q0 < 0)
> >     return(-1e+200)
> >   else if(q1 < 0)
> >     return(-1e+200)
> >   else if(q2 < 0)
> >     return(-1e+200)
> >   else
> >
> return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
> > )-(O2*q2+U2))))
> >
> > }
> >
> genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
> > rations=150,max.generations=300,boundary.enforcement=2)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From phhs80 at gmail.com  Wed Jul  4 01:31:35 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 4 Jul 2007 00:31:35 +0100
Subject: [R] Fine tunning rgenoud
In-Reply-To: <000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
	<000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>
	<000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>
Message-ID: <6ade6f6c0707031631r5034777bk11b3597f1e9ae37@mail.gmail.com>

On 7/4/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> It should be easy enough to check that your solution is valid (i.e. a local
> minimum):  first, check to see if the solution satisfies all the
> constraints; secondly, check to see if it is an interior point (i.e. none of
> the constraints become equality); and finally, if the solution is an
> interior point, check to see whether the gradient there is close to zero.
> Note that if the solution is one of the vertices of the polyhedron, then the
> gradient may not be zero.

I am having bad luck: all constraints are satisfied, but the solution
given by constrOptim is not interior; the gradient is not equal to
zero.

Paul


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> Sent: Tuesday, July 03, 2007 5:10 PM
> To: R-help
> Subject: Re: [R] Fine tunning rgenoud
>
> On 7/3/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> > You had indicated in your previous email that you are having trouble
> finding
> > a feasible starting value for constrOptim().  So, you basically need to
> > solve a system of linear inequalities to obtain a starting point.  Have
> you
> > considered using linear programming? Either simplex() in the "boot"
> package
> > or solveLP() in "linprog" would work.  It seems to me that you could use
> any
> > linear objective function in solveLP to obtain a feasible starting point.
> > This is not the most efficient solution, but it might be worth a try.
> >
> > I am aware of other methods for generating n-tuples that satisfy linear
> > inequality constraints, but AFAIK those are not available in R.
>
> Thanks, Ravi. I had already conceived the solution that you suggest,
> actually using "lpSolve". I am able to get a solution for my problem
> with constrOptim, but I am not enough confident that the solution is
> right. That is why I am trying to get a solution with rgenoud, but
> unsuccessfully until now.
>
> Paul
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Smith
> > Sent: Tuesday, July 03, 2007 4:10 PM
> > To: R-help
> > Subject: [R] Fine tunning rgenoud
> >
> > Dear All,
> >
> > I am trying to solve the following maximization problem, but I cannot
> > have rgenoud giving me a reliable solution.
> >
> > Any ideas?
> >
> > Thanks in advance,
> >
> > Paul
> >
> > ----------------------------
> > library(rgenoud)
> >
> > v <- 0.90
> > O1 <- 10
> > O2 <- 20
> > O0 <- v*O1+(1-v)*O2
> >
> > myfunc <- function(x) {
> >   U0 <- x[1]
> >   U1 <- x[2]
> >   U2 <- x[3]
> >   q0 <- x[4]
> >   q1 <- x[5]
> >   q2 <- x[6]
> >   p <- x[7]
> >
> >   if (U0 < 0)
> >     return(-1e+200)
> >   else if (U1 < 0)
> >     return(-1e+200)
> >   else if (U2 < 0)
> >     return(-1e+200)
> >   else if ((U0-(U1+(O1-O0)*q1)) < 0)
> >     return(-1e+200)
> >   else if ((U0-(U2+(O2-O0)*q2)) < 0)
> >     return(-1e+200)
> >   else if ((U1-(U0+(O0-O1)*q0)) < 0)
> >     return(-1e+200)
> >   else if ((U1-(U2+(O2-O1)*q2)) < 0)
> >     return(-1e+200)
> >   else if((U2-(U0+(O0-O2)*q0)) < 0)
> >     return(-1e+200)
> >   else if((U2-(U1+(O1-O2)*q1)) < 0)
> >     return(-1e+200)
> >   else if(p < 0)
> >     return(-1e+200)
> >   else if(p > 1)
> >     return(-1e+200)
> >   else if(q0 < 0)
> >     return(-1e+200)
> >   else if(q1 < 0)
> >     return(-1e+200)
> >   else if(q2 < 0)
> >     return(-1e+200)
> >   else
> >
> return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
> > )-(O2*q2+U2))))
> >
> > }
> >
> genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
> > rations=150,max.generations=300,boundary.enforcement=2)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Alexander.Herr at csiro.au  Wed Jul  4 02:01:00 2007
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Wed, 4 Jul 2007 10:01:00 +1000
Subject: [R] for loop doesn't stop with upper loop value
Message-ID: <80C7911E901E7E4797B3F88D106CB25D14A302@exqld2-bne.nexus.csiro.au>

Hi list,

could anyone please educate me on the following:

lst<-seq(47, 239, by=12)

for(n in lst)
{
  lower=n; upper=lower+10
   for(i in lower+2 : upper) 
   { 
     print(paste(n, " i: ", i, " lower: ",lower, " upper :", upper))
   }  
}

does not stop when i = upper


A while loop fixes this but, I still don't understand why the for loop
doesn't stop when I has the value of upper


for(n in lst)
{
  lower=n; upper=lower+10
   while(lower !=upper +1)
   { 
    print(paste(n, " lower: ",lower, " upper :", upper))
     lower=lower+1
   }  
}

Any enlightment would be most welcome.

Thankx 
Herry


From Hong.Ooi at iag.com.au  Wed Jul  4 02:21:03 2007
From: Hong.Ooi at iag.com.au (Hong Ooi)
Date: Wed, 4 Jul 2007 10:21:03 +1000
Subject: [R] for loop doesn't stop with upper loop value
In-Reply-To: <80C7911E901E7E4797B3F88D106CB25D14A302@exqld2-bne.nexus.csiro.au>
References: <80C7911E901E7E4797B3F88D106CB25D14A302@exqld2-bne.nexus.csiro.au>
Message-ID: <200707040019.l640JXkt001138@hypatia.math.ethz.ch>


_______________________________________________________________________________________


lower+2 : upper parses as lower + (2:upper). The colon operator has
fairly high precedence. What you want is (lower + 2):upper


-- 
Hong Ooi
Senior Research Analyst, IAG Limited
388 George St, Sydney NSW 2000
+61 (2) 9292 1566
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Alexander.Herr at csiro.au
Sent: Wednesday, 4 July 2007 10:01 AM
To: r-help at stat.math.ethz.ch
Subject: [R] for loop doesn't stop with upper loop value

Hi list,

could anyone please educate me on the following:

lst<-seq(47, 239, by=12)

for(n in lst)
{
  lower=n; upper=lower+10
   for(i in lower+2 : upper) 
   { 
     print(paste(n, " i: ", i, " lower: ",lower, " upper :", upper))
   }  
}

does not stop when i = upper


A while loop fixes this but, I still don't understand why the for loop
doesn't stop when I has the value of upper


for(n in lst)
{
  lower=n; upper=lower+10
   while(lower !=upper +1)
   { 
    print(paste(n, " lower: ",lower, " upper :", upper))
     lower=lower+1
   }  
}

Any enlightment would be most welcome.

Thankx 
Herry

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

_______________________________________________________________________________________

The information transmitted in this message and its attachme...{{dropped}}


From p.murrell at auckland.ac.nz  Wed Jul  4 02:32:26 2007
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 04 Jul 2007 12:32:26 +1200
Subject: [R] Print grid/ggplot to a metafile
In-Reply-To: <LPEJLJACLINDNMBMFAFIIEFICIAA.dieter.menne@menne-biomed.de>
References: <LPEJLJACLINDNMBMFAFIIEFICIAA.dieter.menne@menne-biomed.de>
Message-ID: <468AEA9A.7010500@stat.auckland.ac.nz>

Hi


Dieter Menne wrote:
> Dear UseRs called Hadley, or Paul,
> 
> I am trying to print an edited ggplot2/grid graphics to a metafile. With the
> commented line below it works, but when I edit the plot by uncommenting the
> line, it fails, because it's illegal to have 2 graphics in a metafile. It
> works with pdf, but even then I get two plots, which is a nuisance.
> 
> I found a workaround by using windows(); savePlot, but it only works in
> interactive mode, not when called with something like (Windows)
> 
> rterm --no-save < printit.r
> 
> Any ideas?


You can capture the ggplot drawing as a grid grob (gTree), edit that (no 
drawing occurs to this point), and then draw it ...

gridggplot <- grid.grabExpr(print(ggplot(mtcars, aes(x=cyls)) +
                                   geom_bar()))
modgridggplot <- editGrob(gridggplot,
                           "xaxis::labels::label.text",
                           just=c("center","center"),
                           grep=TRUE, global=TRUE)

win.metafile(file="bar.emf")
grid.draw(modgridggplot)
dev.off()

Paul


> Dieter
> 
> #------
> library(ggplot2)
> win.metafile(file="bar.emf")
> mtcars$cyls = factor(mtcars$cyl,
>   labels=c("four\ncylinders","six\ncylinders","eight\ncylinders"))
> ggplot(mtcars, aes(x=cyls)) + geom_bar()
> #grid.gedit("xaxis::labels::label.text",just=c("center","center"))
> dev.off()
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From smyth at wehi.EDU.AU  Wed Jul  4 02:48:31 2007
From: smyth at wehi.EDU.AU (Gordon Smyth)
Date: Wed, 04 Jul 2007 10:48:31 +1000
Subject: [R] Sorting the levels of a factor
In-Reply-To: <F2235647AC878D438F09255C39842FBC1B27317F@SJMEMXMB03.stjude
	.sjcrh.local>
References: <F2235647AC878D438F09255C39842FBC1B134BCA@SJMEMXMB03.stjude.sjcrh.local>
	<6.2.5.6.1.20070629084432.02316d10@wehi.edu.au>
	<F2235647AC878D438F09255C39842FBC1B135307@SJMEMXMB03.stjude.sjcrh.local>
	<6.2.5.6.1.20070630123452.0231a780@wehi.edu.au>
	<F2235647AC878D438F09255C39842FBC10CC566B@SJMEMXMB03.stjude.sjcrh.local>
	<6.2.5.6.1.20070701215133.023a10b0@wehi.edu.au>
	<F2235647AC878D438F09255C39842FBC1B272FE9@SJMEMXMB03.stjude.sjcrh.local>
	<6.2.5.6.1.20070703171619.02353b10@wehi.edu.au>
	<F2235647AC878D438F09255C39842FBC1B27317F@SJMEMXMB03.stjude.sjcrh.local>
Message-ID: <6.2.5.6.1.20070704094917.022f4880@wehi.edu.au>

Dear Chunxu,

I'm cc'ing my reply to the r-help mailing list. As I said in one of 
my previous replies to you, your questions really have to do with use 
of factors in R rather than anything specifically to do with the 
limma package, so they should go to a help list.

The levels() function in R, when applied to a factor, is simply an 
extractor function. It extracts the levels attribute of the factor (a 
short character vector) which was setup when the factor itself was 
created. It doesn't change the levels in any way. For example, 
consider the following code:

   >  f <- factor( c("a","b","a","b"), levels=c("b","a"))
   >  levels(f)
   [1] "b" "a"

The factor() function sets up the factor with levels in a certain 
order. The levels() function simply extracts the previously defined 
levels without resorting them.

If targets$Class is a factor, the correct way to extract the levels is

    levels(targets$Class)

Your use of unique() is superfluous. There is no need for you to use 
unique() anywhere in conjunction with factors if you are using 
factors correctly in R.

When you use read.table() to read data into R, that function will 
automatically convert any character column that it finds in your file 
into a factor. By default, the factor levels will be the unique 
values of the character column in alphabetical order. If that's not 
what you want, you can define your factors explicitly, with levels in 
any order that you like, or else you can use read.table() with 
as.is=TRUE to suppress the creation of factors altogether.

You last comment suggests that you might not be clear on the 
difference between the values of a factor and its levels. You refer 
to an 'array', but there is no array in your example. Your example 
shows the levels of the factor twice, in exactly the same order each 
time. The first row of your output is actually the values of the 
factor created by unique(), not the levels attribute of the factor.

The reason that the levels of your factor are in alphabetical order 
is because you read your data using read.table() in the first place.

Hope this helps
Best wishes
Gordon

At 11:21 PM 3/07/2007, Qu, Chunxu wrote:
>In our Linux cluster, unique did not sort the array but levels did.
>See below.
>Chunxu
>
> > unique(targets$Class)
>[1] Normal    Tumor     Tumor_CN  Normal_CN
>Levels: Normal Normal_CN Tumor Tumor_CN
> > levels(unique(targets$Class))
>[1] "Normal"    "Normal_CN" "Tumor"     "Tumor_CN"
>
>
>
>-----Original Message-----
>From: Gordon Smyth [mailto:smyth at wehi.EDU.AU]
>Sent: Tuesday, July 03, 2007 2:42 AM
>To: Qu, Chunxu
>Subject: RE: limma 2.9.17
>
>Dear Chunxu,
>
>At 06:32 AM 3/07/2007, Qu, Chunxu wrote:
...
> >         if (is.factor(levels))
> >                 levels = levels(levels)
> >
> >This WILL sort the levels.
>
>No, this doesn't sort the levels. It simply extracts the levels
>without changing their order.
...
>Best wishes
>Gordon
>
> >Chunxu


From toby909 at gmail.com  Wed Jul  4 02:54:05 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Tue, 03 Jul 2007 17:54:05 -0700
Subject: [R] constraining residual variance to zero in lme or lmer
Message-ID: <f6er1d$k6f$1@sea.gmane.org>

Hi All

I havent seen that lme or lmer allows to specify certain constraints (although I 
heard one could program ones own covariance structure). Specific constraints, 
including one on the residual variance can be specified in sas proc mixed. In my 
model I want to fix the residual variance to the known value of 0. If anyone 
having an idea about that, I would appreciate letting me know.

Thanks Toby


From r.turner at auckland.ac.nz  Wed Jul  4 03:40:09 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 4 Jul 2007 13:40:09 +1200
Subject: [R] for loop doesn't stop with upper loop value
In-Reply-To: <80C7911E901E7E4797B3F88D106CB25D14A302@exqld2-bne.nexus.csiro.au>
References: <80C7911E901E7E4797B3F88D106CB25D14A302@exqld2-bne.nexus.csiro.au>
Message-ID: <99B29308-3A1A-46EC-AB5F-C74882F4E4A0@auckland.ac.nz>


On 4/07/2007, at 12:01 PM, <Alexander.Herr at csiro.au>  
<Alexander.Herr at csiro.au> wrote:

> Hi list,
>
> could anyone please educate me on the following:
>
> lst<-seq(47, 239, by=12)
>
> for(n in lst)
> {
>   lower=n; upper=lower+10
>    for(i in lower+2 : upper)
>    {
>      print(paste(n, " i: ", i, " lower: ",lower, " upper :", upper))
>    }
> }
>
> does not stop when i = upper


	To see what's going on, type

		239+2:249

	Hints:

			o precedence of operators (this ain't APL)
			o parentheses are Good Things.

				cheers,

					Rolf Turner

######################################################################
Attention:\ This e-mail message is privileged and confidenti...{{dropped}}


From brown_emu at yahoo.com  Wed Jul  4 05:39:43 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 3 Jul 2007 20:39:43 -0700 (PDT)
Subject: [R] Problems using imported data
In-Reply-To: <20070703194944.88895.qmail@web34501.mail.mud.yahoo.com>
Message-ID: <705140.68539.qm@web39710.mail.mud.yahoo.com>

Actually, I believe attach() and detached() is discouraged nowadays...

x <- read.delim("Filename.txt", header=TRUE)

You can access your data by column:
x[,1]
x[,c(1,3)]

or if your first column is named "Col1" and the third "Col3",
x[,"Col1"]
x[,c("Col1","Col3")]

and you can do the same to access by row - by indices or rownames [which you
can set with "rownames<-", see help("rownames<-")]

Alternatively, with this type of data [created by the read.delim() function]
you can also access with the following syntax:
x$Col1
x$Col3
with(x,Col1)
with(x,cbind(Col1,Col3))

...hope this helps

ST


--- Susie Iredale <skewsie at yahoo.com> wrote:

> 
> 
> 
> (Repeat of previous HTML version)
> 
> Hello all,
> 
> I am a new R user and I have finally imported my data using
> >read.delim("Filename.txt", header=TRUE) after some difficulty, by changing
> file directories (a hint to anyone who might be stuck there).
> 
> However, I am now stuck trying to use my data.  When I try to use
> data.frame("filename.txt") it tells me object not found, which makes it
> difficult to use attach() or with().  How do I get R to recognize my data? 
> 
> 
> Thanks,
> Susie
> PhD Student UCI
> 
> 
> 
> 
>      
>
____________________________________________________________________________________
> Luggage? GPS? Comic books?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rvaradhan at jhmi.edu  Wed Jul  4 06:30:24 2007
From: rvaradhan at jhmi.edu (RAVI VARADHAN)
Date: Wed, 04 Jul 2007 00:30:24 -0400
Subject: [R] Fine tunning rgenoud
In-Reply-To: <6ade6f6c0707031631r5034777bk11b3597f1e9ae37@mail.gmail.com>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
	<000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>
	<000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031631r5034777bk11b3597f1e9ae37@mail.gmail.com>
Message-ID: <f69d97acd06.468aea20@johnshopkins.edu>

Paul,

Here is another approach: I wrote an R function that would generate interior points as starting values for constrOptim.  This might work better than the LP approach, since the LP approach gives you a starting value that is on the boundary of the feasible region, i.e a vertex of the polyhedron, whereas this new approach gives you points on the interior.  You can generate as many points as you wish, but the approach is brute-force and is very inefficient - it takes on the order of a 1000 tries to find one feasible point.

Hope this helps,
Ravi. 

----- Original Message -----
From: Paul Smith <phhs80 at gmail.com>
Date: Tuesday, July 3, 2007 7:32 pm
Subject: Re: [R] Fine tunning rgenoud
To: R-help <r-help at stat.math.ethz.ch>


> On 7/4/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
>  > It should be easy enough to check that your solution is valid (i.e. 
> a local
>  > minimum):  first, check to see if the solution satisfies all the
>  > constraints; secondly, check to see if it is an interior point 
> (i.e. none of
>  > the constraints become equality); and finally, if the solution is an
>  > interior point, check to see whether the gradient there is close to 
> zero.
>  > Note that if the solution is one of the vertices of the polyhedron, 
> then the
>  > gradient may not be zero.
>  
>  I am having bad luck: all constraints are satisfied, but the solution
>  given by constrOptim is not interior; the gradient is not equal to
>  zero.
>  
>  Paul
>  
>  
>  > -----Original Message-----
>  > From: r-help-bounces at stat.math.ethz.ch
>  > [ On Behalf Of Paul Smith
>  > Sent: Tuesday, July 03, 2007 5:10 PM
>  > To: R-help
>  > Subject: Re: [R] Fine tunning rgenoud
>  >
>  > On 7/3/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
>  > > You had indicated in your previous email that you are having trouble
>  > finding
>  > > a feasible starting value for constrOptim().  So, you basically 
> need to
>  > > solve a system of linear inequalities to obtain a starting point. 
>  Have
>  > you
>  > > considered using linear programming? Either simplex() in the "boot"
>  > package
>  > > or solveLP() in "linprog" would work.  It seems to me that you 
> could use
>  > any
>  > > linear objective function in solveLP to obtain a feasible 
> starting point.
>  > > This is not the most efficient solution, but it might be worth a 
> try.
>  > >
>  > > I am aware of other methods for generating n-tuples that satisfy 
> linear
>  > > inequality constraints, but AFAIK those are not available in R.
>  >
>  > Thanks, Ravi. I had already conceived the solution that you suggest,
>  > actually using "lpSolve". I am able to get a solution for my problem
>  > with constrOptim, but I am not enough confident that the solution is
>  > right. That is why I am trying to get a solution with rgenoud, but
>  > unsuccessfully until now.
>  >
>  > Paul
>  >
>  >
>  >
>  > > -----Original Message-----
>  > > From: r-help-bounces at stat.math.ethz.ch
>  > > [ On Behalf Of Paul Smith
>  > > Sent: Tuesday, July 03, 2007 4:10 PM
>  > > To: R-help
>  > > Subject: [R] Fine tunning rgenoud
>  > >
>  > > Dear All,
>  > >
>  > > I am trying to solve the following maximization problem, but I cannot
>  > > have rgenoud giving me a reliable solution.
>  > >
>  > > Any ideas?
>  > >
>  > > Thanks in advance,
>  > >
>  > > Paul
>  > >
>  > > ----------------------------
>  > > library(rgenoud)
>  > >
>  > > v <- 0.90
>  > > O1 <- 10
>  > > O2 <- 20
>  > > O0 <- v*O1+(1-v)*O2
>  > >
>  > > myfunc <- function(x) {
>  > >   U0 <- x[1]
>  > >   U1 <- x[2]
>  > >   U2 <- x[3]
>  > >   q0 <- x[4]
>  > >   q1 <- x[5]
>  > >   q2 <- x[6]
>  > >   p <- x[7]
>  > >
>  > >   if (U0 < 0)
>  > >     return(-1e+200)
>  > >   else if (U1 < 0)
>  > >     return(-1e+200)
>  > >   else if (U2 < 0)
>  > >     return(-1e+200)
>  > >   else if ((U0-(U1+(O1-O0)*q1)) < 0)
>  > >     return(-1e+200)
>  > >   else if ((U0-(U2+(O2-O0)*q2)) < 0)
>  > >     return(-1e+200)
>  > >   else if ((U1-(U0+(O0-O1)*q0)) < 0)
>  > >     return(-1e+200)
>  > >   else if ((U1-(U2+(O2-O1)*q2)) < 0)
>  > >     return(-1e+200)
>  > >   else if((U2-(U0+(O0-O2)*q0)) < 0)
>  > >     return(-1e+200)
>  > >   else if((U2-(U1+(O1-O2)*q1)) < 0)
>  > >     return(-1e+200)
>  > >   else if(p < 0)
>  > >     return(-1e+200)
>  > >   else if(p > 1)
>  > >     return(-1e+200)
>  > >   else if(q0 < 0)
>  > >     return(-1e+200)
>  > >   else if(q1 < 0)
>  > >     return(-1e+200)
>  > >   else if(q2 < 0)
>  > >     return(-1e+200)
>  > >   else
>  > >
>  > return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
>  > > )-(O2*q2+U2))))
>  > >
>  > > }
>  > >
>  > genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
>  > > rations=150,max.generations=300,boundary.enforcement=2)
>  > >
>  > > ______________________________________________
>  > > R-help at stat.math.ethz.ch mailing list
>  > > 
>  > > PLEASE do read the posting guide
>  > 
>  > > and provide commented, minimal, self-contained, reproducible code.
>  > >
>  >
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > 
>  > PLEASE do read the posting guide 
>  > and provide commented, minimal, self-contained, reproducible code.
>  >
>  
>  ______________________________________________
>  R-help at stat.math.ethz.ch mailing list
>  
>  PLEASE do read the posting guide 
>  and provide commented, minimal, self-contained, reproducible code. 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: lineq_R.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/eb3535e6/attachment.txt 

From hvillalo at ipn.mx  Wed Jul  4 06:42:57 2007
From: hvillalo at ipn.mx (=?ISO-8859-1?Q?H=E9ctor_Villalobos?=)
Date: Tue, 03 Jul 2007 22:42:57 -0600
Subject: [R] retrieving stats from bwplot
Message-ID: <468AD0F1.31216.3147CF7@hvillalo.ipn.mx>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070703/00ba709d/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul  4 07:41:52 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jul 2007 06:41:52 +0100 (BST)
Subject: [R] exact AIC
In-Reply-To: <9ed3fa0707031139t6a3e8deu63272fcb91a2a477@mail.gmail.com>
References: <9ed3fa0707031139t6a3e8deu63272fcb91a2a477@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707040637240.12086@gannet.stats.ox.ac.uk>

On Tue, 3 Jul 2007, Selene Z?rate wrote:

> I am using the stepAIC function on the MASS package that does model
> selection by exact AIC. I'm not sure what the term 'exact AIC' means,
> and how it differs from  AIC.

It doesn't, but there are approximations to AIC in use.  This is explained 
in the book stepAIC supports (one is C_p, for example).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Wed Jul  4 08:09:20 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jul 2007 07:09:20 +0100 (BST)
Subject: [R] bug in closing gzfile-opened connections?
In-Reply-To: <468AD76B.7060209@stats.uwo.ca>
References: <fd913b0d0707031037o37629382ke6cb28af5fc56b76@mail.gmail.com>
	<468AD76B.7060209@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0707040658170.12086@gannet.stats.ox.ac.uk>

Note that the use of read.table() does make a difference.  If you did

x <- scan(gzfile("xxx.gz"), list("","",""))

you would leave an unused connection, and showConnections(all=TRUE) would 
show this.  There is a finite pool of connections, and in general the 
correct way to use them is

con <- gzfile("xxx.gz")
x <- scan(con, list("","",""))
close(con)

read.table() is the exception, so I suspect it is other things that have 
been done in the session that have used up the pool of connections.

On Tue, 3 Jul 2007, Duncan Murdoch wrote:

> On 03/07/2007 1:37 PM, David Reiss wrote:
>> Hi,
>> I am making multiple calls to gzfile() via read.table(), e.g.
>>
>>> x <- read.table( gzfile( "xxx.gz" ) )
>>
>> After i do this many times (I haven't counted, but probably between 50 and
>> 100 times) I get the error message:
>>
>> Error in open.connection(file, "r") : unable to open connection
>> In addition: Warning message:
>> cannot open compressed file 'xxx.gz'
>>
>> however, I also find that:
>>
>>> showConnections()
>>      description class mode text isopen can read can write
>>
>> so there are no (apparently) open connections. Calling closeAllConnections()
>> does not fix the problem. I have to quit and re-start R.
>> I am using R 2.5.0 on a Mac (OSX 10.4.9).
>>
>> Anyone know if this is a bug or a 'feature'? I see from the gzfile help
>> that:
>>
>>  In general functions using connections
>>      will open them if they are not open, but then close them again, so
>>      to leave a connection open call 'open' explicitly.
>
> You didn't give a reproducible example, so I couldn't say.  When I
> create a gzipped version of a write.table output and run
>
> for(i in 1:1000) read.table(gzfile(f))
>
> in R 2.5.0 I don't see a problem.  This is on Windows, but I doubt that
> makes a difference.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Wed Jul  4 08:55:50 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Jul 2007 07:55:50 +0100 (BST)
Subject: [R] possible bug in ggplot2 v0.5.2???
In-Reply-To: <f8e6ff050707030314h21fa2898m1dc1ce35b91dbb54@mail.gmail.com>
References: <468A1B01.9090100@genoscope.cns.fr>
	<f8e6ff050707030314h21fa2898m1dc1ce35b91dbb54@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707031255400.25506@gannet.stats.ox.ac.uk>

On Tue, 3 Jul 2007, hadley wickham wrote:

> Hi Stephane,
>
> The problem is that the windows graphics device doesn't support
> transparent colours.  You can get around this in two ways:

It certainly does!  Try col="transparent" (and perhaps consult your 
dictionary).  It was news to me that the windows() graphics device worked 
on 
Linux i586.

What it does not support as yet is translucent colours, and that is a 
restriction imposed by Windows (translucency support was introduced for 
Windows XP, and we still try to support older versions of Windows, unlike 
the MacOS people).  I have been working on a workaround, so translucency 
support is likely to be implemented in R 2.6.0 for users of XP or later.

Given that neither of the two main screen devices and neither of the 
standard print devices support translucency, the subject line looks 
correct to me: the problem surely lies in the assumptions made in ggplot2.

> * export to a device that does support transparency (eg. pdf)
> * use a solid fill colour : + stat_smooth(method="lm", fill="grey50")
>
> Hadley
>
> On 7/3/07, Stephane Cruveiller <scruveil at genoscope.cns.fr> wrote:
>> Dear R-Users,
>>
>> I recently gave a try to the nice package ggplot2. Everything  went
>> well until I tried to add a smoother (using lm method for instance).
>> On the graphic device the regression line is displayed but not confidence
>> intervals as it should be (at least on ggplot website). I tried to do
>> the job on
>> both MS winXP and Linux i586: same result. Did anyone encountered this
>> problem? Did I miss something?
>>
>>
>> My R version is 2.4.1.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From elyakhlifi_mustapha at yahoo.fr  Wed Jul  4 08:59:21 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Wed, 4 Jul 2007 06:59:21 +0000 (GMT)
Subject: [R] read items
Message-ID: <184782.44197.qm@web27504.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/117db8c9/attachment.pl 

From deepayan.sarkar at gmail.com  Wed Jul  4 09:04:58 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Wed, 4 Jul 2007 00:04:58 -0700
Subject: [R] retrieving stats from bwplot
In-Reply-To: <468AD0F1.31216.3147CF7@hvillalo.ipn.mx>
References: <468AD0F1.31216.3147CF7@hvillalo.ipn.mx>
Message-ID: <eb555e660707040004ld77c11v65ac78ace4d89f8e@mail.gmail.com>

On 7/3/07, H?ctor Villalobos <hvillalo at ipn.mx> wrote:
> Hi all,
>
> I want to retrieve the stats from a 'bwplot' with one factor. I have read
> the help for 'panel'
> function and I'm aware of the option 'stats' which defaults to
> 'boxplot.stats' but I didn't
> understand it well and therefore I am unable to get what I need.

I'm not sure what bwplot has to do with this. Perhaps this will help:

> foo <- with(OrchardSprays, split(decrease, treatment))
> str(foo)
List of 8
 $ A: num [1:8] 2 2 5 4 5 12 4 3
 $ B: num [1:8] 8 6 4 10 7 4 8 14
 $ C: num [1:8] 15 84 16 9 17 29 13 19
 $ D: num [1:8] 57 36 22 51 28 27 20 39
 $ E: num [1:8] 95 51 39 114 43 47 61 55
 $ F: num [1:8] 90 69 87 20 71 44 57 114
 $ G: num [1:8] 92 71 72 24 60 77 72 80
 $ H: num [1:8] 69 127 72 130 81 76 81 86
> boxplot.stats(foo$A)
$stats
[1] 2.0 2.5 4.0 5.0 5.0

$n
[1] 8

$conf
[1] 2.603464 5.396536

$out
[1] 12

> bxp.stats <- lapply(foo, boxplot.stats)
> str(bxp.stats)
List of 8
 $ A:List of 4
  ..$ stats: num [1:5] 2 2.5 4 5 5
  ..$ n    : int 8
  ..$ conf : num [1:2] 2.60 5.40
  ..$ out  : num 12
 $ B:List of 4
  ..$ stats: num [1:5] 4 5 7.5 9 14
  ..$ n    : int 8
  ..$ conf : num [1:2] 5.27 9.73
  ..$ out  : num(0)
 $ C:List of 4
  ..$ stats: num [1:5] 9 14 16.5 24 29
  ..$ n    : int 8
  ..$ conf : num [1:2] 10.9 22.1
  ..$ out  : num 84
 $ D:List of 4
  ..$ stats: num [1:5] 20 24.5 32 45 57
  ..$ n    : int 8
  ..$ conf : num [1:2] 20.5 43.5
  ..$ out  : num(0)
 $ E:List of 4
  ..$ stats: num [1:5] 39 45 53 78 114
  ..$ n    : int 8
  ..$ conf : num [1:2] 34.6 71.4
  ..$ out  : num(0)
 $ F:List of 4
  ..$ stats: num [1:5] 20 50.5 70 88.5 114
  ..$ n    : int 8
  ..$ conf : num [1:2] 48.8 91.2
  ..$ out  : num(0)
 $ G:List of 4
  ..$ stats: num [1:5] 60 65.5 72 78.5 92
  ..$ n    : int 8
  ..$ conf : num [1:2] 64.7 79.3
  ..$ out  : num 24
 $ H:List of 4
  ..$ stats: num [1:5]  69  74  81 106 130
  ..$ n    : int 8
  ..$ conf : num [1:2] 62.8 99.2
  ..$ out  : num(0)


If you want combinations defined by more than one factor, you could
use something like

with(OrchardSprays, split(decrease, interaction(treatment, colpos)))

(although this is a bad example, since there is only one observation
per combination)

-Deepayan


From xh.along at gmail.com  Wed Jul  4 09:20:17 2007
From: xh.along at gmail.com (along zeng)
Date: Wed, 4 Jul 2007 15:20:17 +0800
Subject: [R] probabilty plot
Message-ID: <21add9100707040020r3e3378e6o6d34d8fe73192e40@mail.gmail.com>

Hi all,
   I am a freshman of R,but I am interested  in it! Those days,I am
learning   pages on NIST,with url
http://www.itl.nist.gov/div898/handbook/eda/section3/probplot.htm,
I am meeting  a problem about probability plot and I don't know how to
plot a data set with R.
Could somebody tell me the answer,and a example is the best!  I will
look forward to your answer.
 Thank you very much.


From h.wickham at gmail.com  Wed Jul  4 09:37:33 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 4 Jul 2007 09:37:33 +0200
Subject: [R] possible bug in ggplot2 v0.5.2???
In-Reply-To: <Pine.LNX.4.64.0707031255400.25506@gannet.stats.ox.ac.uk>
References: <468A1B01.9090100@genoscope.cns.fr>
	<f8e6ff050707030314h21fa2898m1dc1ce35b91dbb54@mail.gmail.com>
	<Pine.LNX.4.64.0707031255400.25506@gannet.stats.ox.ac.uk>
Message-ID: <f8e6ff050707040037h58fba7a1w67c2b829dc9b2cb0@mail.gmail.com>

On 7/4/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Tue, 3 Jul 2007, hadley wickham wrote:
>
> > Hi Stephane,
> >
> > The problem is that the windows graphics device doesn't support
> > transparent colours.  You can get around this in two ways:
>
> It certainly does!  Try col="transparent" (and perhaps consult your
> dictionary).  It was news to me that the windows() graphics device worked
> on
> Linux i586.

Well my dictionary defines transparent as "allowing light to pass
through so that objects behind can be distinctly seen" which I believe
applies here (ie. stained glass windows and blue points with alpha 0.5
are both transparent).  What does your dictionary say?

> What it does not support as yet is translucent colours, and that is a
> restriction imposed by Windows (translucency support was introduced for
> Windows XP, and we still try to support older versions of Windows, unlike
> the MacOS people).  I have been working on a workaround, so translucency
> support is likely to be implemented in R 2.6.0 for users of XP or later.

I am confused by your implication that windows (prior to XP) does not
support translucency.  Perhaps it is not supported at the operating
system level, but it has certainly been available at the application
level for a very long time.

> Given that neither of the two main screen devices and neither of the
> standard print devices support translucency, the subject line looks
> correct to me: the problem surely lies in the assumptions made in ggplot2.

The features of the windows and X11 devices clearly lag behind the
quartz and pdf devices.  I can program for the lowest common
denominator or I can use modern features that support the tasks I am
working on.  I choose the later, and it is certainly your prerogative
to declare that a bug in me.

Hadley


From msmith at ebi.ac.uk  Wed Jul  4 09:38:14 2007
From: msmith at ebi.ac.uk (Grimbough)
Date: Wed, 4 Jul 2007 00:38:14 -0700 (PDT)
Subject: [R] How to install R 2.5 with Synaptic in Ubuntu?
In-Reply-To: <20070629072923.172990@gmx.net>
References: <1183099152.5873.5.camel@zhurx-desktop>
	<20070629072923.172990@gmx.net>
Message-ID: <11426235.post@talk.nabble.com>


Hi,
I can't get this approach to work.  When I first added the repsitory line to
/etc/apt/sources.list synaptic complained that it was a malformed line.  I
fixed this
by adding "main" to end of the entry making it:

 deb http://my.favorite.cran.mirror/bin/linux/ubuntu feisty main

However after this it still complains that it can't find "packages.gz"

It appears to be looking in 
http://my.favorite.cran.mirror/bin/linux/ubuntu/distsfeisty
which isn't the directory structure of the cran repository, but 
I can see anyway to modify this behaviour.  Every other Ubuntu repositoy
I have looked at contains the dists directory.

Any suggestions for modifying this behaviour are gratefully recieved.
Many thanks

Mike Smith


&quot;Stefan Gro?e&quot; wrote:
> 
>>I'm using Ubuntu dapper, which only have R of Version 2.2.1.
> 
>>Would anybody tell me how to install the latest version of R with 
> 
> 
> from the CRAN Ubuntu readme- works for synaptic as well:
> 
> * UBUNTU
> 
> R packages for Ubuntu on i386 are available. The plans are to support at
> least the latest Ubuntu release and the latest LTS release. Currently
> (April 2007), these are Feisty Fawn (7.04) and Dapper Drake (6.06),
> respectively. Since Feisty was released very shortly before R 2.5.0,
> binary packages *for this release of R* are also available for Edgy
> Eft (6.10).
> 
> To obtain the latest R packages, add an entry like
> 
>   deb http://my.favorite.cran.mirror/bin/linux/ubuntu feisty/
> 
> or
> 
>   deb http://my.favorite.cran.mirror/bin/linux/ubuntu edgy/
> 
> or
> 
>   deb http://my.favorite.cran.mirror/bin/linux/ubuntu dapper/
> 
> in your /etc/apt/sources.list file. See 
> http://cran.r-project.org/mirrors.html
> for the list of CRAN mirrors. To install the complete R system, use
> 
>   sudo apt-get update
>   sudo apt-get install r-base
> 
> Users who need to compile packages should also install the r-base-dev
> package:
> 
>   sudo apt-get install r-base-dev
> 
> The R packages for Ubuntu should otherwise behave like the Debian ones.
> For
> more information, see the README file in
> 
>   http://cran.R-project.org/bin/linux/debian/
> 
> * SECURE APT
> 
> The Ubuntu archives on CRAN are signed with the key of "Vincent Goulet
> <vincent.goulet at act.ulaval.ca>" with key ID E2A11821. You can fetch
> this with
> 
>   gpg --keyserver subkeys.pgp.net --recv-key E2A11821
> 
> and then you feed the key to apt-key with
> 
>   gpg -a --export E2A11821 | sudo apt-key add -
> 
> Some people have reported difficulties using this approach. The issue
> was usually related to a firewall blocking port 11371. An alternative
> approach is to search for the key at http://keyserver.noreply.org/ and
> copy the key to a plain text file, say key.txt. Then, feed the key to
> apt-key with
> 
>   sudo apt-key add key.txt
> 
> 
> * ACKNOWLEDGEMENT
> 
> The Debian R packages are maintained by Dirk Eddelbuettel and Doug Bates.
> The Ubuntu packages are compiled for i386 by Vincent Goulet.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/How-to-install-R-2.5-with-Synaptic-in-Ubuntu--tf3998481.html#a11426235
Sent from the R help mailing list archive at Nabble.com.


From paul.lemmens at gmail.com  Wed Jul  4 09:48:15 2007
From: paul.lemmens at gmail.com (Paul Lemmens)
Date: Wed, 4 Jul 2007 09:48:15 +0200
Subject: [R] Adding data to existing plot with new=TRUE does not appear to
	work
Message-ID: <b9065fc0707040048v1c24635bl164bd85f1bf7db62@mail.gmail.com>

Dear all,

I am trying to shove a number of cmdscale() results into a single plot
(k=1 so I'm trying to get multiple columns in the plot).  From ?par I
learned that I can/should set new=TRUE in either par() or the plot
function itself. However with the following reduced code, I get only a
plot with a column of data points with x==2.

plot(1,10, xlim=range(0,3), ylim=range(0,10), type='n')
aa <- rep(1,10)
bb <- 1:10
plot(aa,bb, xlim=range(0,3), ylim=range(0,10), new=TRUE)
aa <- rep(2,10)
plot(aa,bb, xlim=range(0,3), ylim=range(0,10), new=TRUE)

Also, when I insert a op <- par(new=TRUE) either before or immediately
after the first plot statement (the type='n' one) in the above code
fragment, the resulting graph still only shows one column of data.

Have I misinterpreted the instructions or the functionality of new=TRUE?

Thank you,
Paul Lemmens


From Jan.Verbesselt at csiro.au  Wed Jul  4 09:58:41 2007
From: Jan.Verbesselt at csiro.au (Jan.Verbesselt at csiro.au)
Date: Wed, 4 Jul 2007 17:58:41 +1000
Subject: [R] how to plot a monthplot from a ts object where all individual
	years are shown (e.g. as lines) and can be compared with a
	"average or median " year?
Message-ID: <0393960FC6AFA142B0234E5CC8DA3B0C3FE3A4@exnswn2-syd.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/dd7cd129/attachment.pl 

From pburns at pburns.seanet.com  Wed Jul  4 10:20:19 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 04 Jul 2007 09:20:19 +0100
Subject: [R] Fine tunning rgenoud
In-Reply-To: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
Message-ID: <468B5843.8060708@pburns.seanet.com>

I think fine tuning the function might be in order.

The function has just a single penalty for not meeting
the constraints no matter how close it is to meeting
them.  A better approach is to have a penalty that
depends on the amount by which all of the constraints
are breached.


Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")


Paul Smith wrote:

>Dear All,
>
>I am trying to solve the following maximization problem, but I cannot
>have rgenoud giving me a reliable solution.
>
>Any ideas?
>
>Thanks in advance,
>
>Paul
>
>----------------------------
>library(rgenoud)
>
>v <- 0.90
>O1 <- 10
>O2 <- 20
>O0 <- v*O1+(1-v)*O2
>
>myfunc <- function(x) {
>  U0 <- x[1]
>  U1 <- x[2]
>  U2 <- x[3]
>  q0 <- x[4]
>  q1 <- x[5]
>  q2 <- x[6]
>  p <- x[7]
>
>  if (U0 < 0)
>    return(-1e+200)
>  else if (U1 < 0)
>    return(-1e+200)
>  else if (U2 < 0)
>    return(-1e+200)
>  else if ((U0-(U1+(O1-O0)*q1)) < 0)
>    return(-1e+200)
>  else if ((U0-(U2+(O2-O0)*q2)) < 0)
>    return(-1e+200)
>  else if ((U1-(U0+(O0-O1)*q0)) < 0)
>    return(-1e+200)
>  else if ((U1-(U2+(O2-O1)*q2)) < 0)
>    return(-1e+200)
>  else if((U2-(U0+(O0-O2)*q0)) < 0)
>    return(-1e+200)
>  else if((U2-(U1+(O1-O2)*q1)) < 0)
>    return(-1e+200)
>  else if(p < 0)
>    return(-1e+200)
>  else if(p > 1)
>    return(-1e+200)
>  else if(q0 < 0)
>    return(-1e+200)
>  else if(q1 < 0)
>    return(-1e+200)
>  else if(q2 < 0)
>    return(-1e+200)
>  else return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2)-(O2*q2+U2))))
>
>}
>genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.generations=150,max.generations=300,boundary.enforcement=2)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From singularitaet at gmx.net  Wed Jul  4 11:01:16 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Wed, 04 Jul 2007 11:01:16 +0200
Subject: [R] How to install R 2.5 with Synaptic in Ubuntu?
In-Reply-To: <11426235.post@talk.nabble.com>
References: <1183099152.5873.5.camel@zhurx-desktop>
	<20070629072923.172990@gmx.net> <11426235.post@talk.nabble.com>
Message-ID: <468B61DC.5080004@gmx.net>


>  to end of the entry making it:
>
>  deb http://my.favorite.cran.mirror/bin/linux/ubuntu feisty main
>
> However after this it still complains that it can't find "packages.gz"
>
>   

Just a guess: have you replaced the my.favorite.cran.mirror by a mirror
which is close to you? If you're in UK it would be for example

deb http://www.stats.bris.ac.uk/R/bin/linux/ubuntu feisty main

;o)
Stefan

> It appears to be looking in 
> http://my.favorite.cran.mirror/bin/linux/ubuntu/distsfeisty
> which isn't the directory structure of the cran repository, but 
> I can see anyway to modify this behaviour.  Every other Ubuntu repositoy
> I have looked at contains the dists directory.
>
> Any suggestions for modifying this behaviour are gratefully recieved.
> Many thanks
>
> Mike Smith
>
>
>   




-=-=-
... The simple truth is that interstellar distances will not fit into
the human imagination - (The Hitchhiker's Guide to the Galaxy)


From petr.pikal at precheza.cz  Wed Jul  4 11:22:31 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Wed, 4 Jul 2007 11:22:31 +0200
Subject: [R] Odp: Adding data to existing plot with new=TRUE does not appear
	to	work
In-Reply-To: <b9065fc0707040048v1c24635bl164bd85f1bf7db62@mail.gmail.com>
Message-ID: <OF7834B48A.F508F047-ONC125730E.00333703-C125730E.00337DE5@precheza.cz>

Hi

if you change your code

plot(1,10, xlim=range(0,3), ylim=range(0,10), type='n')
aa <- rep(1,10)
bb <- 1:10
plot(aa,bb, xlim=range(0,3), ylim=range(0,10), new=TRUE)
aa <- rep(2,10)
par(new=T)
plot(aa,bb, xlim=range(0,3), ylim=range(0,10), new=TRUE)

you will get both columns plotted.

However you can get similar result with using points

plot(1,10, xlim=range(0,3), ylim=range(0,10), type='n')
aa <- rep(1,10)
bb <- 1:10
points(aa,bb)
aa <- rep(2,10)
points(aa,bb)

Regards

Petr
petr.pikal at precheza.cz

r-help-bounces at stat.math.ethz.ch napsal dne 04.07.2007 09:48:15:

> Dear all,
> 
> I am trying to shove a number of cmdscale() results into a single plot
> (k=1 so I'm trying to get multiple columns in the plot).  From ?par I
> learned that I can/should set new=TRUE in either par() or the plot
> function itself. However with the following reduced code, I get only a
> plot with a column of data points with x==2.
> 
> plot(1,10, xlim=range(0,3), ylim=range(0,10), type='n')
> aa <- rep(1,10)
> bb <- 1:10
> plot(aa,bb, xlim=range(0,3), ylim=range(0,10), new=TRUE)
> aa <- rep(2,10)
> plot(aa,bb, xlim=range(0,3), ylim=range(0,10), new=TRUE)
> 
> Also, when I insert a op <- par(new=TRUE) either before or immediately
> after the first plot statement (the type='n' one) in the above code
> fragment, the resulting graph still only shows one column of data.
> 
> Have I misinterpreted the instructions or the functionality of new=TRUE?
> 
> Thank you,
> Paul Lemmens
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From msmith at ebi.ac.uk  Wed Jul  4 11:34:37 2007
From: msmith at ebi.ac.uk (msmith)
Date: Wed, 4 Jul 2007 02:34:37 -0700 (PDT)
Subject: [R] How to install R 2.5 with Synaptic in Ubuntu?
In-Reply-To: <468B61DC.5080004@gmx.net>
References: <1183099152.5873.5.camel@zhurx-desktop>
	<20070629072923.172990@gmx.net> <11426235.post@talk.nabble.com>
	<468B61DC.5080004@gmx.net>
Message-ID: <11427837.post@talk.nabble.com>


Hi,

Thanks for the suggestion and I wish the solution was that obvious, but I
have changed it to really point at my favourite mirror.

Using your example Synaptic reports the following error when I try to update
the repositories:

http://www.stats.bris.ac.uk/R/bin/linux/ubuntu/dists/feisty/main/binary-i386/Packages.gz:
404 Not Found

This is understandable since that location doesn't exist, but it makes me
think that the directory structure of the R mirrors is not compatible with
Ubuntu and Synaptic, since it automatically seeks /dists/feisty/ rather than
just /feisty/ as it is on the CRAN mirrors.

Thanks again
Mike Smith


Stefan Grosse-2 wrote:
> 
> 
>>  to end of the entry making it:
>>
>>  deb http://my.favorite.cran.mirror/bin/linux/ubuntu feisty main
>>
>> However after this it still complains that it can't find "packages.gz"
>>
>>   
> 
> Just a guess: have you replaced the my.favorite.cran.mirror by a mirror
> which is close to you? If you're in UK it would be for example
> 
> deb http://www.stats.bris.ac.uk/R/bin/linux/ubuntu feisty main
> 
> ;o)
> Stefan
> 
>> It appears to be looking in 
>> http://my.favorite.cran.mirror/bin/linux/ubuntu/distsfeisty
>> which isn't the directory structure of the cran repository, but 
>> I can see anyway to modify this behaviour.  Every other Ubuntu repositoy
>> I have looked at contains the dists directory.
>>
>> Any suggestions for modifying this behaviour are gratefully recieved.
>> Many thanks
>>
>> Mike Smith
>>
>>
>>   
> 
> 
> 
> 
> -=-=-
> ... The simple truth is that interstellar distances will not fit into
> the human imagination - (The Hitchhiker's Guide to the Galaxy)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/How-to-install-R-2.5-with-Synaptic-in-Ubuntu--tf3998481.html#a11427837
Sent from the R help mailing list archive at Nabble.com.


From jemwa at sun.ac.za  Wed Jul  4 11:35:11 2007
From: jemwa at sun.ac.za (Gorden T Jemwa)
Date: Wed, 04 Jul 2007 11:35:11 +0200
Subject: [R] QP for solving Support Vector Regression
Message-ID: <468B69CF.7060001@sun.ac.za>

The ksvm object is probably what you need to use.

[quote]
Dear R users,
   I'm trying to run the Support Vector Regression by a general 
quadratic programming function like ipop ( ) in kernlab or solve.QP ( ) 
in quadprog packages.

   Since they are general, their application in Support Vector 
Regression can lead to misunderstanding, particularly when constructing 
matrices. Even their examples are general and applied in Support Vector 
Classification.

   Could anybody please introduce an example code for regression case.
[\quote]


From phhs80 at gmail.com  Wed Jul  4 11:59:48 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 4 Jul 2007 10:59:48 +0100
Subject: [R] Fine tunning rgenoud
In-Reply-To: <f69d97acd06.468aea20@johnshopkins.edu>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
	<000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>
	<000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031631r5034777bk11b3597f1e9ae37@mail.gmail.com>
	<f69d97acd06.468aea20@johnshopkins.edu>
Message-ID: <6ade6f6c0707040259k638eeb8bv126d883bbeee619a@mail.gmail.com>

On 7/4/07, RAVI VARADHAN <rvaradhan at jhmi.edu> wrote:
> Here is another approach: I wrote an R function that would generate interior points as starting values for constrOptim.  This might work better than the LP approach, since the LP approach gives you a starting value that is on the boundary of the feasible region, i.e a vertex of the polyhedron, whereas this new approach gives you points on the interior.  You can generate as many points as you wish, but the approach is brute-force and is very inefficient - it takes on the order of a 1000 tries to find one feasible point.

Thanks again, Ravi. Actually, the LP approach also works here. Let
g(X) >= k be the constraints. Then, by solving a LP problem with the
constraints

g(X) >= (k+0.2)

returns an interior starting value for constrOptim. I am aware that
the new set of constraints may correspond to an impossible linear
system, but it works in many cases.

Paul

> ----- Original Message -----
> From: Paul Smith <phhs80 at gmail.com>
> Date: Tuesday, July 3, 2007 7:32 pm
> Subject: Re: [R] Fine tunning rgenoud
> To: R-help <r-help at stat.math.ethz.ch>
>
>
> > On 7/4/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> >  > It should be easy enough to check that your solution is valid (i.e.
> > a local
> >  > minimum):  first, check to see if the solution satisfies all the
> >  > constraints; secondly, check to see if it is an interior point
> > (i.e. none of
> >  > the constraints become equality); and finally, if the solution is an
> >  > interior point, check to see whether the gradient there is close to
> > zero.
> >  > Note that if the solution is one of the vertices of the polyhedron,
> > then the
> >  > gradient may not be zero.
> >
> >  I am having bad luck: all constraints are satisfied, but the solution
> >  given by constrOptim is not interior; the gradient is not equal to
> >  zero.
> >
> >  Paul
> >
> >
> >  > -----Original Message-----
> >  > From: r-help-bounces at stat.math.ethz.ch
> >  > [ On Behalf Of Paul Smith
> >  > Sent: Tuesday, July 03, 2007 5:10 PM
> >  > To: R-help
> >  > Subject: Re: [R] Fine tunning rgenoud
> >  >
> >  > On 7/3/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> >  > > You had indicated in your previous email that you are having trouble
> >  > finding
> >  > > a feasible starting value for constrOptim().  So, you basically
> > need to
> >  > > solve a system of linear inequalities to obtain a starting point.
> >  Have
> >  > you
> >  > > considered using linear programming? Either simplex() in the "boot"
> >  > package
> >  > > or solveLP() in "linprog" would work.  It seems to me that you
> > could use
> >  > any
> >  > > linear objective function in solveLP to obtain a feasible
> > starting point.
> >  > > This is not the most efficient solution, but it might be worth a
> > try.
> >  > >
> >  > > I am aware of other methods for generating n-tuples that satisfy
> > linear
> >  > > inequality constraints, but AFAIK those are not available in R.
> >  >
> >  > Thanks, Ravi. I had already conceived the solution that you suggest,
> >  > actually using "lpSolve". I am able to get a solution for my problem
> >  > with constrOptim, but I am not enough confident that the solution is
> >  > right. That is why I am trying to get a solution with rgenoud, but
> >  > unsuccessfully until now.
> >  >
> >  > Paul
> >  >
> >  >
> >  >
> >  > > -----Original Message-----
> >  > > From: r-help-bounces at stat.math.ethz.ch
> >  > > [ On Behalf Of Paul Smith
> >  > > Sent: Tuesday, July 03, 2007 4:10 PM
> >  > > To: R-help
> >  > > Subject: [R] Fine tunning rgenoud
> >  > >
> >  > > Dear All,
> >  > >
> >  > > I am trying to solve the following maximization problem, but I cannot
> >  > > have rgenoud giving me a reliable solution.
> >  > >
> >  > > Any ideas?
> >  > >
> >  > > Thanks in advance,
> >  > >
> >  > > Paul
> >  > >
> >  > > ----------------------------
> >  > > library(rgenoud)
> >  > >
> >  > > v <- 0.90
> >  > > O1 <- 10
> >  > > O2 <- 20
> >  > > O0 <- v*O1+(1-v)*O2
> >  > >
> >  > > myfunc <- function(x) {
> >  > >   U0 <- x[1]
> >  > >   U1 <- x[2]
> >  > >   U2 <- x[3]
> >  > >   q0 <- x[4]
> >  > >   q1 <- x[5]
> >  > >   q2 <- x[6]
> >  > >   p <- x[7]
> >  > >
> >  > >   if (U0 < 0)
> >  > >     return(-1e+200)
> >  > >   else if (U1 < 0)
> >  > >     return(-1e+200)
> >  > >   else if (U2 < 0)
> >  > >     return(-1e+200)
> >  > >   else if ((U0-(U1+(O1-O0)*q1)) < 0)
> >  > >     return(-1e+200)
> >  > >   else if ((U0-(U2+(O2-O0)*q2)) < 0)
> >  > >     return(-1e+200)
> >  > >   else if ((U1-(U0+(O0-O1)*q0)) < 0)
> >  > >     return(-1e+200)
> >  > >   else if ((U1-(U2+(O2-O1)*q2)) < 0)
> >  > >     return(-1e+200)
> >  > >   else if((U2-(U0+(O0-O2)*q0)) < 0)
> >  > >     return(-1e+200)
> >  > >   else if((U2-(U1+(O1-O2)*q1)) < 0)
> >  > >     return(-1e+200)
> >  > >   else if(p < 0)
> >  > >     return(-1e+200)
> >  > >   else if(p > 1)
> >  > >     return(-1e+200)
> >  > >   else if(q0 < 0)
> >  > >     return(-1e+200)
> >  > >   else if(q1 < 0)
> >  > >     return(-1e+200)
> >  > >   else if(q2 < 0)
> >  > >     return(-1e+200)
> >  > >   else
> >  > >
> >  > return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
> >  > > )-(O2*q2+U2))))
> >  > >
> >  > > }
> >  > >
> >  > genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
> >  > > rations=150,max.generations=300,boundary.enforcement=2)
> >  > >
> >  > > ______________________________________________
> >  > > R-help at stat.math.ethz.ch mailing list
> >  > >
> >  > > PLEASE do read the posting guide
> >  >
> >  > > and provide commented, minimal, self-contained, reproducible code.
> >  > >
> >  >
> >  > ______________________________________________
> >  > R-help at stat.math.ethz.ch mailing list
> >  >
> >  > PLEASE do read the posting guide
> >  > and provide commented, minimal, self-contained, reproducible code.
> >  >
> >
> >  ______________________________________________
> >  R-help at stat.math.ethz.ch mailing list
> >
> >  PLEASE do read the posting guide
> >  and provide commented, minimal, self-contained, reproducible code.
>
>


From yn19832 at msn.com  Wed Jul  4 12:19:24 2007
From: yn19832 at msn.com (livia)
Date: Wed, 4 Jul 2007 03:19:24 -0700 (PDT)
Subject: [R] sequences
In-Reply-To: <11414836.post@talk.nabble.com>
References: <11414836.post@talk.nabble.com>
Message-ID: <11428415.post@talk.nabble.com>


Hi all, thank you very much.

livia wrote:
> 
> Hi, I would like to generate a series in the following form (0.8^1, 0.8^2,
> ..., 0.8^600)
> Could anyone tell me how can I achieve that? I am really new to R.
> 

-- 
View this message in context: http://www.nabble.com/sequences-tf4019146.html#a11428415
Sent from the R help mailing list archive at Nabble.com.


From b3i4old02 at sneakemail.com  Wed Jul  4 12:18:56 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Wed, 04 Jul 2007 11:18:56 +0100
Subject: [R] Lattice: shifting strips to left of axes
In-Reply-To: <eb555e660707031053w4f853b5anb194bc7a9a1daf3d@mail.gmail.com>
References: <f6cc16$p0a$1@sea.gmane.org>	<eb555e660707022040qf4ec6d0p4479498600ceead2@mail.gmail.com>	<f6dpj7$3pa$1@sea.gmane.org>
	<eb555e660707031053w4f853b5anb194bc7a9a1daf3d@mail.gmail.com>
Message-ID: <f6fs75$82u$1@sea.gmane.org>

deepayan.sarkar at gmail.com wrote:

> myYlabGrob <-
>     function(..., main.ylab = "") ## ...is lab1, lab2, etc
> {
>     ## you can add arguments to textGrob for more control
>     ## in the next line
>     labs <- lapply(list(...), textGrob, rot=90)
>     main.ylab <- textGrob(main.ylab, rot = 90)
>     nlabs <- length(labs)
>     lab.heights <-
>         lapply(labs,
>                function(lab) unit(1, "grobheight",
>                                   data=list(lab)))
>     unit1 <- unit(1.2, "grobheight", data = list(main.ylab))
>     unit2 <- do.call(max, lab.heights)
>     lab.layout <-
>         grid.layout(ncol = 2, nrow = nlabs,
>                     heights = unit(1, "null"),
>                     widths = unit.c(unit1, unit2),
>                     respect = TRUE)
>     lab.gf <- frameGrob(layout=lab.layout)
>     for (i in seq_len(nlabs))
>     {
>         lab.gf <- placeGrob(lab.gf, labs[[i]], row = i, col = 2)
>     }
>     lab.gf <- placeGrob(lab.gf, main.ylab, col = 1)
>     lab.gf
> }

Wow. I don't think I would have been able to come up with that on my 
own. Thank you!
-- 
Michael Hoffman


From paul.lemmens at gmail.com  Wed Jul  4 13:00:23 2007
From: paul.lemmens at gmail.com (Paul Lemmens)
Date: Wed, 4 Jul 2007 13:00:23 +0200
Subject: [R] Adding data to existing plot with new=TRUE does not appear
	to work
In-Reply-To: <OF7834B48A.F508F047-ONC125730E.00333703-C125730E.00337DE5@precheza.cz>
References: <b9065fc0707040048v1c24635bl164bd85f1bf7db62@mail.gmail.com>
	<OF7834B48A.F508F047-ONC125730E.00333703-C125730E.00337DE5@precheza.cz>
Message-ID: <b9065fc0707040400j4d0b5c1o202865df60a8242f@mail.gmail.com>

Hi Petr,

On 7/4/07, Petr PIKAL <petr.pikal at precheza.cz> wrote:
> par(new=T)
> plot(aa,bb, xlim=range(0,3), ylim=range(0,10), new=TRUE)
>
So I need to activate the par(new=T) really just ahead of time when I
need it, not as sort of a general clause at the beginning of my
script?

>
> However you can get similar result with using points
>
Yes I new that, but I wanted to try and go without an if() for
deciding between the first and consecutive columns.

Thnx for helping out!
Paul Lemmens


From statba at nus.edu.sg  Wed Jul  4 13:20:20 2007
From: statba at nus.edu.sg (Berwin A Turlach)
Date: Wed, 4 Jul 2007 19:20:20 +0800
Subject: [R] The R Book by M. J. Crawley
In-Reply-To: <468A4201.1080401@statistik.uni-dortmund.de>
References: <1EB58414BAB4014DB2C3E289FDF55FBB019A0A30@CINMLVEM15.e2k.ad.ge.com>
	<468A4201.1080401@statistik.uni-dortmund.de>
Message-ID: <20070704192020.5021739c@berwin5>

G'day Uwe,

On Tue, 03 Jul 2007 14:33:05 +0200
Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:

> Pietrzykowski, Matthew (GE, Research) wrote:
> >  I saw the new book,
> > "The R Book", by Michael J. Crawley and wanted to know what R users
> > thoughts of it.
> 
> The author seems to be an expert in (almost?) all available
> statistical programming languages 

I would have thought that honour would go to Brian S. Everitt. :-)

M.J. Crawley only seems to write books using S-PLus and R.  His book on
Statistical Computing (using S-Plus) is about 750 pages and his book
"Statistics: An introduction using R" is about 320 pages.  

I do not know and have not seen "The R Book" yet, so I cannot comment
on it.  The statistical material presented in the other two books is
pretty sound and well explained.  M.J. Crawley definitely has some
strong opinions on how certain data should be analysed and how
statistics should be used. The S-Plus code or R code he uses can, on
occasions, be somewhat improved.

Cheers,

	Berwin


From debmidya at yahoo.com  Wed Jul  4 13:39:18 2007
From: debmidya at yahoo.com (Deb Midya)
Date: Wed, 4 Jul 2007 04:39:18 -0700 (PDT)
Subject: [R] Calling C Code from R
Message-ID: <489626.40553.qm@web50401.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/8c6839d9/attachment.pl 

From csardi at rmki.kfki.hu  Wed Jul  4 13:48:12 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Wed, 4 Jul 2007 13:48:12 +0200
Subject: [R] Calling C Code from R
In-Reply-To: <489626.40553.qm@web50401.mail.re2.yahoo.com>
References: <489626.40553.qm@web50401.mail.re2.yahoo.com>
Message-ID: <20070704114758.GA5713@guzu>

On Wed, Jul 04, 2007 at 04:39:18AM -0700, Deb Midya wrote:
> Hi R Users,
>    
>   Thanks in advance.
>    
>   I am using R-2.5.1 on Windows XP.
>    
>   I am trying to call C code (testCX1.C) from R. testCX1.c calls another C code (funcC1.c) and returning a value to testCX1.c. I like to have this value in R.
>    
>   My steps are below:
>    
>   1. R CMD SHLIB testCX1.c funcC1.c (at command propmt)
>    
>   2. It creates testCX1.dll with warning (but testCX1.dll works):
>    
>   testCX1.c:38: warning: implicit declaration of function 'func'
>    
>   How to get rid off this error ?

By adding the prototype of 'func' to testCX1.c:

SEXP func(SEXP a);

Probably it is simplest to collect all prototypes in a single header file
and include that from all .c files.

>   What is the best way to call funcC1.c from testCX1.c?

See .C and .Call and in particular the 'Writing R Extensions' manual,
5 System and foreign language interfaces.

Gabor

[...]

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From singularitaet at gmx.net  Wed Jul  4 13:52:47 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Wed, 04 Jul 2007 13:52:47 +0200
Subject: [R] How to install R 2.5 with Synaptic in Ubuntu?
In-Reply-To: <11427837.post@talk.nabble.com>
References: <1183099152.5873.5.camel@zhurx-desktop>
	<20070629072923.172990@gmx.net> <11426235.post@talk.nabble.com>
	<468B61DC.5080004@gmx.net> <11427837.post@talk.nabble.com>
Message-ID: <468B8A0F.4090000@gmx.net>

msmith schrieb:
> Hi,
>
> Thanks for the suggestion and I wish the solution was that obvious, but I
> have changed it to really point at my favourite mirror.
>
> Using your example Synaptic reports the following error when I try to update
> the repositories:
>
> http://www.stats.bris.ac.uk/R/bin/linux/ubuntu/dists/feisty/main/binary-i386/Packages.gz:
> 404 Not Found
>
> This is understandable since that location doesn't exist, but it makes me
> think that the directory structure of the R mirrors is not compatible with
> Ubuntu and Synaptic, since it automatically seeks /dists/feisty/ rather than
> just /feisty/ as it is on the CRAN mirrors.
>
>   

Hm. I have Fedora 7 so I cannot really check what my entry would look
like. I recently installed Kubuntu on a friends notebook so there the
readme from http://www.stats.bris.ac.uk/R/bin/linux/ubuntu/README.html
did actually work. I just had a second look at your mail so could it be
that there is a missing slash after feisty? If that does not work try
another mirror like

deb http://stat.ethz.ch/CRAN/bin/linux/ubuntu/ feisty/

there is also no main stated there in the readme... sorry I copied that
from your mail so thats obviously not following the readme...

I hope this works now.

Stefan




-=-=-
... Satisfaction does not come with achievement, but with effort. Full
effort is full victory. (M.Gandhi)


From singularitaet at gmx.net  Wed Jul  4 13:54:25 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Wed, 04 Jul 2007 13:54:25 +0200
Subject: [R] How to install R 2.5 with Synaptic in Ubuntu?
In-Reply-To: <468B61DC.5080004@gmx.net>
References: <1183099152.5873.5.camel@zhurx-desktop>
	<20070629072923.172990@gmx.net> <11426235.post@talk.nabble.com>
	<468B61DC.5080004@gmx.net>
Message-ID: <468B8A71.7090901@gmx.net>

Stefan Grosse schrieb:
> deb http://www.stats.bris.ac.uk/R/bin/linux/ubuntu feisty main
>   

Sorry, I copied a mistake there, it should be:

deb http://www.stats.bris.ac.uk/R/bin/linux/ubuntu feisty/

Stefan



-=-=-
... The simple truth is that interstellar distances will not fit into
the human imagination - (The Hitchhiker's Guide to the Galaxy)


From jrkrideau at yahoo.ca  Wed Jul  4 14:09:43 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 4 Jul 2007 08:09:43 -0400 (EDT)
Subject: [R] probabilty plot
In-Reply-To: <21add9100707040020r3e3378e6o6d34d8fe73192e40@mail.gmail.com>
Message-ID: <787900.26883.qm@web32801.mail.mud.yahoo.com>


Is this what you mean ? 

-------------------------------
mydata <- c(1,2,3,4,5,7,5,4,3)

plot(mydata)
-----------------------------------


--- along zeng <xh.along at gmail.com> wrote:

> Hi all,
>    I am a freshman of R,but I am interested  in it!
> Those days,I am
> learning   pages on NIST,with url
>
http://www.itl.nist.gov/div898/handbook/eda/section3/probplot.htm,
> I am meeting  a problem about probability plot and I
> don't know how to
> plot a data set with R.
> Could somebody tell me the answer,and a example is
> the best!  I will
> look forward to your answer.
>  Thank you very much.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From debmidya at yahoo.com  Wed Jul  4 14:15:15 2007
From: debmidya at yahoo.com (Deb Midya)
Date: Wed, 4 Jul 2007 05:15:15 -0700 (PDT)
Subject: [R] Calling C Code from R
In-Reply-To: <20070704114758.GA5713@guzu>
Message-ID: <573577.71818.qm@web50403.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/92915d0b/attachment.pl 

From csardi at rmki.kfki.hu  Wed Jul  4 14:19:27 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Wed, 4 Jul 2007 14:19:27 +0200
Subject: [R] Calling C Code from R
In-Reply-To: <573577.71818.qm@web50403.mail.re2.yahoo.com>
References: <20070704114758.GA5713@guzu>
	<573577.71818.qm@web50403.mail.re2.yahoo.com>
Message-ID: <20070704121927.GB5713@guzu>


On Wed, Jul 04, 2007 at 05:15:15AM -0700, Deb Midya wrote:
>    Gabor,
> 
>    Thank you very much for such a quick response.
> 
>    As I am new to this area, will you please explain where can I put SEXP
>    func(SEXP a);
>    in my program.

Deb, anywhere before calling it. (Well outside a function definition.)
Typically after the #include lines.
Or put all these prototypes into a header file called myfuncs.h
and add line 

#include "myfuncs.h"

just after the other #include lines at the beginning of the file.

Gabor

>    Once again, thank you very much for your quick response.
> 
>    Regards,
> 
>    Deb
> 

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From yn19832 at msn.com  Wed Jul  4 14:57:38 2007
From: yn19832 at msn.com (livia)
Date: Wed, 4 Jul 2007 05:57:38 -0700 (PDT)
Subject: [R] Loop and cbind
Message-ID: <11430500.post@talk.nabble.com>


Hi, I would like to apply the following function for i between 1 and 12, and
then construct a list of the return series.

for (i in 1:12){
ewma[i] <- emaTA(calm[[i]]^2,0.03)
standard[i]<- calm[[i]]/sqrt(ewma[i])
standard <- cbind(standard[i])
}

But it does not work. Could anyone give me some advice how can I achieve
this? Many thanks
-- 
View this message in context: http://www.nabble.com/Loop-and-cbind-tf4024291.html#a11430500
Sent from the R help mailing list archive at Nabble.com.


From stonemonkey at web.de  Wed Jul  4 14:58:44 2007
From: stonemonkey at web.de (Hubertus)
Date: Wed, 4 Jul 2007 14:58:44 +0200
Subject: [R] Problem/bug with smooth.spline and all.knots=T
Message-ID: <20070704125844.GA31772@p15097509.pureserver.info>

Dear list,
if I do
  smooth.spline(tmpSec, tmpT, all.knots=T)
with the attached data, I get this error-message:
  Error in smooth.spline(tmpSec, tmpT, all.knots = T) :
        smoothing parameter value too small
If I do
  smooth.spline(tmpSec[-single arbitrary number], tmpT[-single arbitrary number], all.knots=T)
it works!

I just don't see it. It works for hundrets other datasets, but not for this one.
Would be glad if anyone could help!

Hubertus

From john.seers at bbsrc.ac.uk  Wed Jul  4 15:18:50 2007
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Wed, 4 Jul 2007 14:18:50 +0100
Subject: [R] Loop and cbind
In-Reply-To: <11430500.post@talk.nabble.com>
References: <11430500.post@talk.nabble.com>
Message-ID: <AAD49F46EAE3F6479E1D46428FAC31CB0181AB4D@NBIE2KSRV1.nbi.bbsrc.ac.uk>




Hi 

In what way does it not work?

My guess is that you have not declared your values outside the for loop.
As they are local they will be lost on exit.

You need to declare them before:

ewma<-vector(length=12)
standard<-vector(length=12)

for ... {
	....
}

John Seers
 


 
---

Hi, I would like to apply the following function for i between 1 and 12,
and then construct a list of the return series.

for (i in 1:12){
ewma[i] <- emaTA(calm[[i]]^2,0.03)
standard[i]<- calm[[i]]/sqrt(ewma[i])
standard <- cbind(standard[i])
}

But it does not work. Could anyone give me some advice how can I achieve
this? Many thanks
--
View this message in context:
http://www.nabble.com/Loop-and-cbind-tf4024291.html#a11430500
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Thierry.ONKELINX at inbo.be  Wed Jul  4 15:29:49 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 4 Jul 2007 15:29:49 +0200
Subject: [R] Loop and cbind
In-Reply-To: <AAD49F46EAE3F6479E1D46428FAC31CB0181AB4D@NBIE2KSRV1.nbi.bbsrc.ac.uk>
References: <11430500.post@talk.nabble.com>
	<AAD49F46EAE3F6479E1D46428FAC31CB0181AB4D@NBIE2KSRV1.nbi.bbsrc.ac.uk>
Message-ID: <2E9C414912813E4EB981326983E0A104033E48DB@inboexch.inbo.be>

A more elegant way to do this is

standard <- sapply(calm, function(calmi){calmi / sqrt(emaTA(calmi ^ 2,
0.03))})

Cheers,

Thierry
------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens john seers (IFR)
> Verzonden: woensdag 4 juli 2007 15:19
> Aan: livia; r-help op stat.math.ethz.ch
> Onderwerp: Re: [R] Loop and cbind
> 
> 
> 
> 
> Hi 
> 
> In what way does it not work?
> 
> My guess is that you have not declared your values outside 
> the for loop.
> As they are local they will be lost on exit.
> 
> You need to declare them before:
> 
> ewma<-vector(length=12)
> standard<-vector(length=12)
> 
> for ... {
> 	....
> }
> 
> John Seers
>  
> 
> 
>  
> ---
> 
> Hi, I would like to apply the following function for i 
> between 1 and 12, and then construct a list of the return series.
> 
> for (i in 1:12){
> ewma[i] <- emaTA(calm[[i]]^2,0.03)
> standard[i]<- calm[[i]]/sqrt(ewma[i])
> standard <- cbind(standard[i])
> }
> 
> But it does not work. Could anyone give me some advice how 
> can I achieve this? Many thanks
> --
> View this message in context:
> http://www.nabble.com/Loop-and-cbind-tf4024291.html#a11430500
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From phhs80 at gmail.com  Wed Jul  4 16:15:00 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 4 Jul 2007 15:15:00 +0100
Subject: [R] Fine tunning rgenoud
In-Reply-To: <f69de9c12c48.468b6a9d@johnshopkins.edu>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
	<000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>
	<000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031631r5034777bk11b3597f1e9ae37@mail.gmail.com>
	<f69d97acd06.468aea20@johnshopkins.edu>
	<6ade6f6c0707040259k638eeb8bv126d883bbeee619a@mail.gmail.com>
	<f69de9c12c48.468b6a9d@johnshopkins.edu>
Message-ID: <6ade6f6c0707040715p6b8e7732y3014000138a60174@mail.gmail.com>

On 7/4/07, RAVI VARADHAN <rvaradhan at jhmi.edu> wrote:
> My point is that it might be better to try multiple (feasible) starting values for constrOptim to ensure that you have a good local minimum, since it appears that constrOptim converges to a boundary solution where the gradient is non-zero.  That is why my code could be useful.

Thanks, Ravi. I have used your function, which works pretty fine.
However, constrOptim returns solutions markedly different, depending
on the starting values. That is true that I am expecting a solution in
the boundary, but should not constrOptim find boundary solutions
correctly? The set of solution that I got is below.

Paul

--------------------------------

2.67682495728743e-08	0.676401684216637	5.18627076390355e-09	0.00206463986063195	0.87185968612836	4.32039325909089e-11	0.999999999996234
3.71711020733097e-08	0.539853580957444	1.82592937615235e-08	0.00206941041763503	0.93305250393447	2.08076621230984e-11	0.999999999995774
1.55648443014316e-08	0.356047772992972	8.61341165816411e-09	0.00207149128044574	0.939531540703735	2.55211186629222e-12	0.999999999999424
2.20685747493755e-07	0.575689534431218	5.30976753476747e-08	0.00210500604605837	0.588947341576757	3.1310360048386e-10	0.999999999998789
1.92961662926727e-08	0.773588030510204	1.04841835042200e-08	0.00206723852358352	0.816755014708394	3.89478290348532e-11	0.999999999997794
0.000279824051289082	0.000310992385522886	1.01467522935252e-06	3.11645639181419e-05	0.00249801538651552	3.0978819115532e-05	7.11821104872585e-06
2.81901448690893e-07	0.381718731525906	4.72860507882539e-08	0.00206807672109157	0.769178513763055	1.39278079797628e-09	0.999999999967123
5.58938545019597e-05	0.00171253668169328	4.54005998518212e-09	0.00165663757292733	0.00247994862102590	6.20992250482468e-06	0.419169641865998
1.03300938985890e-08	0.438357835603591	6.89854079723234e-09	0.00206693286138396	0.977554885433201	1.17209206267609e-10	0.99999999996921
7.63336821363444e-05	0.00177141538041517	1.88050423143828e-10	0.00169507950991094	0.00249739505142207	8.4814984916537e-06	0.470929220605509
9.16005846107533e-09	0.682179815036755	1.63255733785783e-09	0.00206922107327189	0.919323193130209	5.71436138398897e-11	0.99999999999629
1.40968913167328e-08	0.343606628343661	1.33227447885302e-08	0.00206789984370423	0.343671264496824	1.11679312116211e-11	0.999999999999822
4.76054734844857e-09	0.593022549313178	2.28102966623129e-09	0.00206625165098398	0.947562121256448	8.9437610753173e-11	0.999999999999992
1.96950784184139e-07	0.579488113726155	1.61915231214025e-07	0.00208000350528798	1.00891340595040	1.22248906754713e-10	0.999999999996493
8.1448937742933e-09	0.441088618716555	4.54846390087941e-09	0.00207634940425852	0.446155700100820	4.81439647816238e-12	0.99999999999939
4.82439218405912e-08	0.557771049256698	3.53737879481732e-08	0.0020663035737319	0.588137767965923	2.6568947800491e-11	0.999999999988615
2.43086751126363e-08	0.522927598354163	2.26886829089137e-08	0.00206533531066324	0.611696593543814	4.51226610050184e-11	0.999999999999087
3.05498959434100e-08	0.465522202845817	1.09246302124670e-08	0.00207004066920179	0.465583376966915	3.24213847202457e-11	0.999999999997366
1.88687179088788e-07	0.783614197203923	4.51346471059839e-08	0.00222403775221293	0.786422171740329	8.17865794171933e-10	0.999999999986103
1.0154423824979e-08	0.302657777579883	9.06923080122203e-09	0.00206615353968094	0.359722316646974	8.27866320956902e-12	0.99999999998461
8.91008717665837e-08	0.0020661526864997	3.08619455858999e-09	0.00206579199039568	0.00275523149199496	9.55650084108725e-09	0.985185595958656
1.25320647920029e-07	0.635217955401437	7.44627883600107e-08	0.00206656250455391	0.855937507707323	3.70326032870889e-10	0.999999999998375
2.57618374406559e-08	0.636499151952225	1.09822023878715e-08	0.00206677354204888	0.772636071860102	8.99370944431481e-11	0.999999999978744
1.09474196877990e-08	0.501469973722704	1.19992915868609e-10	0.00206117941606503	0.501594064757161	1.34320044786225e-11	0.999999999991232
5.24203710193977e-05	0.000127998340144109	3.33258623630601e-09	7.55779680724378e-05	0.00248898574263025	5.82411313482383e-06	0.0221497278110802
3.80217498132259e-07	0.57664568703189	1.01755510162620e-08	0.00207232950382402	0.944031557945531	5.30703662426069e-10	0.999999999995957
1.45159816281038e-09	0.391742001993341	1.13492553980291e-09	0.00206615324883312	0.73041632635671	2.05351961803669e-11	0.99999999997684
8.74006318627465e-05	0.00176059707830211	1.94489765002966e-09	0.00167319599216734	0.00234472182706612	9.710963192258e-06	0.358282221037878
0.000238275046444342	0.000264776586825777	4.40904795740029e-10	2.64911355650347e-05	0.00259858019547791	2.64749446522330e-05	1.83580740341509e-07
7.39730651399581e-10	0.00908091738218919	2.48425610406978e-10	0.00206667909063917	0.00928420674840742	5.18131986840764e-11	0.99999875366957
6.76489417503509e-08	0.66948409231674	3.03852881114497e-08	0.00207327594614506	0.82038550137061	2.64031575134214e-11	0.999999999984622
2.47578596556463e-05	0.00192273935278028	1.23710433232059e-10	0.00189797520967688	0.00249714264227544	2.75085723641778e-06	0.654395200885401
1.89137722020867e-08	0.611106915103611	3.89703191870382e-09	0.00210184590871973	0.624411027730728	8.5135512646264e-11	0.99999999999684
0.00019176743398086	0.000561608946531094	3.86469382998825e-10	0.000369840972771214	0.00250444089363261	2.13074457599853e-05	0.0571352017772342
0.000185606084629350	0.000206269128591342	1.56795453162326e-11	2.06627190417154e-05	0.00249993097016385	2.06228887085015e-05	1.71605199821395e-06
6.07957336199056e-08	0.483682520205442	5.5863930138716e-09	0.00206322767692575	0.483682695616455	5.34362969147144e-11	0.999999999999643
2.32535804244419e-08	0.479995736120651	1.71422519965926e-08	0.00207338945556393	0.510141538832558	1.48964674702430e-11	0.999999999993732
8.9183702782689e-06	0.00206653282852738	1.75762003328053e-10	0.00205761314408722	0.00252353219843752	9.9090949737504e-07	0.833268509557228
1.98549228223182e-06	0.251617257564206	1.05359270191879e-06	0.0022260243792503	0.251624161164046	9.60072596752024e-08	0.99999999999983
3.98028438135354e-08	0.547004412551238	3.46420325883517e-08	0.00206593124574590	0.780233389945956	4.31912137883551e-11	0.999999999999248
7.415833973742e-09	0.411955745727431	6.63826778226352e-09	0.00206692655795474	0.422974654599724	6.0360458732987e-12	0.999999999999735
2.31198818168677e-09	0.00651214491263961	1.32346818065092e-09	0.00206600061742381	0.0308298577099849	1.09702273280814e-10	0.99987783901442
4.17312889043775e-08	0.535841994679291	2.16650907799191e-08	0.00206804295529002	0.59999577506709	1.18576996500002e-10	0.999999999999237
3.06571185479614e-08	0.210504230265054	2.05714086443132e-08	0.00206707615217700	0.218748559037881	2.15131848511398e-14	0.999999999993557
2.75615889053932e-09	0.410462493146348	1.61078464820130e-09	0.00206420001019636	0.410565659594555	9.40738331890563e-11	0.999999999999952
4.56613193593397e-09	0.408415058997493	2.33397373332447e-09	0.00206711241339086	0.532349316444773	6.74887208834456e-11	0.99999999999929
7.18189347926295e-08	0.456169544603848	5.38996419605601e-08	0.00206990174706466	1.10900729793115	1.09308927282691e-13	0.999999999990783
0.000188732057072893	0.000209745553735604	2.65570357321341e-11	2.10123232762018e-05	0.00249946399527847	2.09702247828353e-05	7.56804739408697e-07
5.85086685972612e-08	0.586106422015639	3.34740271798516e-08	0.00207017375045705	1.02476378656892	7.7586558103274e-11	0.999999999999751
0.000110963331385117	0.000134315151780862	1.07242866306447e-06	2.33497881113803e-05	0.00255535434807175	1.22100977811675e-05	0.0100715104652649
6.74756547544261e-05	0.00181277651670570	2.06882429964314e-10	0.0017453002881811	0.00248364499775618	7.49727080120661e-06	0.477865304416143
2.12169576142447e-09	0.258696699048681	3.83834275215625e-10	0.00206572907235368	0.593016720449582	2.47234004478185e-11	0.999999999976093
4.90290042793427e-08	0.845133894257049	4.26659941495083e-08	0.00206573635916222	0.846607006638287	2.63753283209851e-10	0.999999999994938
1.82215438136793e-08	0.80366504831158	1.78340820866931e-08	0.00225471081235068	0.805133180734986	2.65837229120276e-12	0.999999999992972
4.73449331282602e-08	0.284890209400889	3.18447541813436e-08	0.00206840481685483	0.808166776706798	4.05949074947464e-10	0.999999999996061
1.09692979675704e-08	0.504203767689845	1.91188777015599e-09	0.00206788476221373	0.706912076703946	4.06894266747559e-11	0.99999999999973
4.34112012988195e-08	0.458125660334097	6.46770375936903e-09	0.00207007623328698	0.625303714142111	1.01347377674089e-10	0.999999999992636
1.16509068624767e-07	0.598772124413396	1.83783072861908e-08	0.00207311616484422	0.649412674861061	1.72090788969858e-10	0.999999999982887
1.72904399298124e-08	0.58859841513479	5.56864060572789e-09	0.00206902943325815	0.834770114923498	4.44826394221343e-11	0.999999999998404
1.95167889112074e-07	0.680051903362475	4.07870548730064e-08	0.0021048787521164	0.758987053084469	3.35901569072651e-10	0.999999999999464
1.06657670445553e-08	0.705185947467634	5.468246700687e-09	0.00206717328235464	0.852353580488024	9.79781097317382e-11	0.999999999997673
3.38906655480421e-08	0.675066389889348	5.7356420037523e-09	0.00206939927716748	0.864714866878808	3.50701456652884e-10	0.999999999999926
4.72130896241037e-09	0.193480077858215	4.09642232386814e-09	0.00206639690357456	0.193772609031377	2.22379321541448e-11	0.999999999992884
3.55665252909232e-08	0.620669750121032	1.90641809388101e-08	0.00206864461846135	0.622207402604423	3.55684392133601e-11	0.999999999999635
3.43643575170166e-08	0.508032144170851	3.05576064051123e-08	0.0020666442030495	0.537473527046566	5.41317633959989e-12	0.999999999991207
2.07413577081273e-08	0.517006009937055	1.78794895322523e-08	0.00206609160101216	0.723546217083115	4.25770563404542e-11	0.999999999977351
2.9634111809255e-08	0.678167734702809	2.87136255305397e-08	0.00207005922065176	0.680058941948453	4.86294997869349e-11	0.999999999997674
2.70007729246723e-08	0.777109777637737	2.23198659183296e-08	0.00211784135373727	0.821904778895813	2.72480529179635e-10	0.999999999999417
1.21638582096581e-05	0.685066257234478	3.90995182181942e-06	0.00201558375343728	0.685516429216796	4.73765370536012e-08	0.999999999999993
6.21341332802845e-08	0.45153366532873	4.28836207599489e-08	0.00205228864113157	0.630886659065685	2.01031448944481e-10	0.999999999976095
1.19446880731148e-06	0.00204849378641721	5.41758686583886e-10	0.00204660358124349	0.00250002927315376	1.32657483581135e-07	0.926358671695374
4.23724996341138e-07	0.76589882078562	1.07976473375288e-07	0.00231084075578484	0.772123021534594	4.222800306051e-09	0.99999999999554
9.76490929801592e-09	0.611133237912937	3.40654501481527e-09	0.00206753317060016	0.624278286034974	1.35688861199835e-11	0.999999999991987
0.000185800168507657	0.000206604414024173	1.51257663200075e-10	2.08040548465085e-05	0.00249987187225613	2.06444402710859e-05	1.00090667437022e-05
4.87431144092831e-09	0.486985270937825	1.01147265793908e-09	0.00206873578985202	0.494229783529316	2.1163667211661e-12	0.999999999999217
1.58678140422157e-08	0.458475824861338	3.728478508397e-09	0.00206665340523142	0.822538576890515	6.73647051811321e-11	0.999999999992949
7.78797943133767e-07	0.00201714819466376	1.00271036821046e-09	0.00201517307886584	0.00243247068677390	8.6420943116519e-08	0.946415585057681
1.79734244345473e-07	0.618114350664356	8.41196223101054e-08	0.00209424634015987	1.09808725088463	1.36311179064841e-09	0.999999999999325
4.04750622932535e-06	0.00202435649740235	9.55231839434103e-11	0.00202030658534268	0.00249780762721203	4.49711157881299e-07	0.878051982789172
3.27944009298396e-08	0.710329613922295	2.02989735589882e-08	0.00206702754480486	0.713789321637634	4.43500632164969e-12	0.99999999998325
3.6211203640154e-08	0.486990872484964	2.23029819494578e-08	0.00206834819984283	0.925447188571729	1.43913590315638e-10	0.999999999999852
1.52117049757253e-08	0.499368102973816	5.70289700988348e-09	0.00206632041653347	0.688163413575525	6.86450078797379e-11	0.99999999998881
2.14084286484963e-08	0.669902287842233	1.64604388157258e-08	0.00206787121557579	0.675669448264383	3.05725070183645e-11	0.99999999999783
4.64383080200508e-05	0.00188125145190771	2.92580309919556e-10	0.00183481310287054	0.00250971056566956	5.15977640139516e-06	0.571340025768215
3.11099577796555e-08	0.319299482089934	8.84023352637676e-09	0.00206855023112842	0.441077204370492	1.10446087116967e-09	0.999999999995467
0.000102965203269986	0.0016493488546921	2.58445958349384e-10	0.00154638327053218	0.00250288634336625	1.14405481683120e-05	0.342341363715383
5.87912519479874e-09	0.00301357388024528	2.75211293457077e-10	0.00206603126487256	0.00686204622812863	6.1682829968701e-10	0.999931564740833
8.28694064868836e-08	0.673413934917665	5.55691871872657e-08	0.00206648968616976	0.690994737517309	8.53762213679993e-11	0.99999999999553
2.78964308878308e-08	0.631906605077265	1.87081150212841e-08	0.00206682302780812	1.04912853459322	1.19798263785958e-10	0.999999999998404
0.000186009655330342	0.000206938973112988	1.18525336022574e-09	2.09286186935030e-05	0.00249989443096258	2.06676012144089e-05	2.57319855839169e-06
5.79914576203232e-05	0.00181271191072469	1.15884908131972e-09	0.00175471081855071	0.0024812702174432	6.44334507197721e-06	0.509698530097913
2.0503183260964e-08	0.546450460261978	1.27059638843222e-08	0.00206791702172114	1.14082199649402	8.5751965370755e-12	0.99999999999683
3.66836690280629e-08	0.481874078118496	3.13611734401948e-08	0.00207354422550728	0.990376379663234	2.13707863395726e-10	0.999999999975761
1.77224405556878e-08	0.493128575789259	8.63845133717168e-09	0.00207307974398005	0.493553411995952	3.19150428367562e-11	0.999999999999902
0.000119234853016321	0.00194143059603129	2.65621637099599e-10	0.00182214114564480	0.00258385562559042	1.32482776896270e-05	0.564809333872708
1.91321783130778e-07	0.00205131977291163	2.45737561775750e-10	0.00205112100576657	0.00251498989989795	2.12303674145017e-08	0.981038377325434
2.04970869809e-09	0.61540968024267	1.80692316850342e-10	0.00206758160218205	0.616881623876956	9.88879753424694e-11	0.999999999998682
2.28380664111838e-07	0.495196927735637	2.03520983522517e-07	0.00206484911183903	0.495729865740038	3.10063601627842e-10	0.999999999998029
0.000142663207352814	0.00136226156979156	2.47224893929883e-11	0.00121959537442013	0.00249964872330666	1.58514632925203e-05	0.213308634647407



> ----- Original Message -----
> From: Paul Smith <phhs80 at gmail.com>
> Date: Wednesday, July 4, 2007 6:00 am
> Subject: Re: [R] Fine tunning rgenoud
> To: R-help <r-help at stat.math.ethz.ch>
>
>
> > On 7/4/07, RAVI VARADHAN <rvaradhan at jhmi.edu> wrote:
> >  > Here is another approach: I wrote an R function that would generate
> > interior points as starting values for constrOptim.  This might work
> > better than the LP approach, since the LP approach gives you a
> > starting value that is on the boundary of the feasible region, i.e a
> > vertex of the polyhedron, whereas this new approach gives you points
> > on the interior.  You can generate as many points as you wish, but the
> > approach is brute-force and is very inefficient - it takes on the
> > order of a 1000 tries to find one feasible point.
> >
> >  Thanks again, Ravi. Actually, the LP approach also works here. Let
> >  g(X) >= k be the constraints. Then, by solving a LP problem with the
> >  constraints
> >
> >  g(X) >= (k+0.2)
> >
> >  returns an interior starting value for constrOptim. I am aware that
> >  the new set of constraints may correspond to an impossible linear
> >  system, but it works in many cases.
> >
> >  Paul
> >
> >  > ----- Original Message -----
> >  > From: Paul Smith <phhs80 at gmail.com>
> >  > Date: Tuesday, July 3, 2007 7:32 pm
> >  > Subject: Re: [R] Fine tunning rgenoud
> >  > To: R-help <r-help at stat.math.ethz.ch>
> >  >
> >  >
> >  > > On 7/4/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> >  > >  > It should be easy enough to check that your solution is valid
> > (i.e.
> >  > > a local
> >  > >  > minimum):  first, check to see if the solution satisfies all the
> >  > >  > constraints; secondly, check to see if it is an interior point
> >  > > (i.e. none of
> >  > >  > the constraints become equality); and finally, if the solution
> > is an
> >  > >  > interior point, check to see whether the gradient there is
> > close to
> >  > > zero.
> >  > >  > Note that if the solution is one of the vertices of the polyhedron,
> >  > > then the
> >  > >  > gradient may not be zero.
> >  > >
> >  > >  I am having bad luck: all constraints are satisfied, but the solution
> >  > >  given by constrOptim is not interior; the gradient is not equal
> > to
> >  > >  zero.
> >  > >
> >  > >  Paul
> >  > >
> >  > >
> >  > >  > -----Original Message-----
> >  > >  > From: r-help-bounces at stat.math.ethz.ch
> >  > >  > [ On Behalf Of Paul Smith
> >  > >  > Sent: Tuesday, July 03, 2007 5:10 PM
> >  > >  > To: R-help
> >  > >  > Subject: Re: [R] Fine tunning rgenoud
> >  > >  >
> >  > >  > On 7/3/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> >  > >  > > You had indicated in your previous email that you are having
> > trouble
> >  > >  > finding
> >  > >  > > a feasible starting value for constrOptim().  So, you basically
> >  > > need to
> >  > >  > > solve a system of linear inequalities to obtain a starting point.
> >  > >  Have
> >  > >  > you
> >  > >  > > considered using linear programming? Either simplex() in the
> > "boot"
> >  > >  > package
> >  > >  > > or solveLP() in "linprog" would work.  It seems to me that you
> >  > > could use
> >  > >  > any
> >  > >  > > linear objective function in solveLP to obtain a feasible
> >  > > starting point.
> >  > >  > > This is not the most efficient solution, but it might be
> > worth a
> >  > > try.
> >  > >  > >
> >  > >  > > I am aware of other methods for generating n-tuples that satisfy
> >  > > linear
> >  > >  > > inequality constraints, but AFAIK those are not available in
> > R.
> >  > >  >
> >  > >  > Thanks, Ravi. I had already conceived the solution that you suggest,
> >  > >  > actually using "lpSolve". I am able to get a solution for my problem
> >  > >  > with constrOptim, but I am not enough confident that the
> > solution is
> >  > >  > right. That is why I am trying to get a solution with rgenoud,
> > but
> >  > >  > unsuccessfully until now.
> >  > >  >
> >  > >  > Paul
> >  > >  >
> >  > >  >
> >  > >  >
> >  > >  > > -----Original Message-----
> >  > >  > > From: r-help-bounces at stat.math.ethz.ch
> >  > >  > > [ On Behalf Of Paul Smith
> >  > >  > > Sent: Tuesday, July 03, 2007 4:10 PM
> >  > >  > > To: R-help
> >  > >  > > Subject: [R] Fine tunning rgenoud
> >  > >  > >
> >  > >  > > Dear All,
> >  > >  > >
> >  > >  > > I am trying to solve the following maximization problem, but
> > I cannot
> >  > >  > > have rgenoud giving me a reliable solution.
> >  > >  > >
> >  > >  > > Any ideas?
> >  > >  > >
> >  > >  > > Thanks in advance,
> >  > >  > >
> >  > >  > > Paul
> >  > >  > >
> >  > >  > > ----------------------------
> >  > >  > > library(rgenoud)
> >  > >  > >
> >  > >  > > v <- 0.90
> >  > >  > > O1 <- 10
> >  > >  > > O2 <- 20
> >  > >  > > O0 <- v*O1+(1-v)*O2
> >  > >  > >
> >  > >  > > myfunc <- function(x) {
> >  > >  > >   U0 <- x[1]
> >  > >  > >   U1 <- x[2]
> >  > >  > >   U2 <- x[3]
> >  > >  > >   q0 <- x[4]
> >  > >  > >   q1 <- x[5]
> >  > >  > >   q2 <- x[6]
> >  > >  > >   p <- x[7]
> >  > >  > >
> >  > >  > >   if (U0 < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if (U1 < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if (U2 < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if ((U0-(U1+(O1-O0)*q1)) < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if ((U0-(U2+(O2-O0)*q2)) < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if ((U1-(U0+(O0-O1)*q0)) < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if ((U1-(U2+(O2-O1)*q2)) < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if((U2-(U0+(O0-O2)*q0)) < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if((U2-(U1+(O1-O2)*q1)) < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if(p < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if(p > 1)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if(q0 < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if(q1 < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else if(q2 < 0)
> >  > >  > >     return(-1e+200)
> >  > >  > >   else
> >  > >  > >
> >  > >  > return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
> >  > >  > > )-(O2*q2+U2))))
> >  > >  > >
> >  > >  > > }
> >  > >  > >
> >  > >  > genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
> >  > >  > > rations=150,max.generations=300,boundary.enforcement=2)
> >  > >  > >
> >  > >  > > ______________________________________________
> >  > >  > > R-help at stat.math.ethz.ch mailing list
> >  > >  > >
> >  > >  > > PLEASE do read the posting guide
> >  > >  >
> >  > >  > > and provide commented, minimal, self-contained, reproducible
> > code.
> >  > >  > >
> >  > >  >
> >  > >  > ______________________________________________
> >  > >  > R-help at stat.math.ethz.ch mailing list
> >  > >  >
> >  > >  > PLEASE do read the posting guide
> >  > >  > and provide commented, minimal, self-contained, reproducible code.
> >  > >  >
> >  > >
> >  > >  ______________________________________________
> >  > >  R-help at stat.math.ethz.ch mailing list
> >  > >
> >  > >  PLEASE do read the posting guide
> >  > >  and provide commented, minimal, self-contained, reproducible code.
> >  >
> >  >
> >
> >  ______________________________________________
> >  R-help at stat.math.ethz.ch mailing list
> >
> >  PLEASE do read the posting guide
> >  and provide commented, minimal, self-contained, reproducible code.
>


From phhs80 at gmail.com  Wed Jul  4 16:22:38 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Wed, 4 Jul 2007 15:22:38 +0100
Subject: [R] Fine tunning rgenoud
In-Reply-To: <6ade6f6c0707040715p6b8e7732y3014000138a60174@mail.gmail.com>
References: <6ade6f6c0707031310m4bf83d91xb8f61264caa12645@mail.gmail.com>
	<000501c7bdb3$bbc47ac0$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031409i5a9ba74cs4567e4aa994a0749@mail.gmail.com>
	<000d01c7bdc6$4967c460$7c94100a@win.ad.jhu.edu>
	<6ade6f6c0707031631r5034777bk11b3597f1e9ae37@mail.gmail.com>
	<f69d97acd06.468aea20@johnshopkins.edu>
	<6ade6f6c0707040259k638eeb8bv126d883bbeee619a@mail.gmail.com>
	<f69de9c12c48.468b6a9d@johnshopkins.edu>
	<6ade6f6c0707040715p6b8e7732y3014000138a60174@mail.gmail.com>
Message-ID: <6ade6f6c0707040722w7dd45a2fs8e3819eadb5fef2a@mail.gmail.com>

On 7/4/07, Paul Smith <phhs80 at gmail.com> wrote:
> On 7/4/07, RAVI VARADHAN <rvaradhan at jhmi.edu> wrote:
> > My point is that it might be better to try multiple (feasible) starting values for constrOptim to ensure that you have a good local minimum, since it appears that constrOptim converges to a boundary solution where the gradient is non-zero.  That is why my code could be useful.
>
> Thanks, Ravi. I have used your function, which works pretty fine.
> However, constrOptim returns solutions markedly different, depending
> on the starting values. That is true that I am expecting a solution in
> the boundary, but should not constrOptim find boundary solutions
> correctly? The set of solution that I got is below.

Unless, there are many local optimal solutions...

Paul


> --------------------------------
>
> 2.67682495728743e-08    0.676401684216637       5.18627076390355e-09    0.00206463986063195     0.87185968612836        4.32039325909089e-11    0.999999999996234
> 3.71711020733097e-08    0.539853580957444       1.82592937615235e-08    0.00206941041763503     0.93305250393447        2.08076621230984e-11    0.999999999995774
> 1.55648443014316e-08    0.356047772992972       8.61341165816411e-09    0.00207149128044574     0.939531540703735       2.55211186629222e-12    0.999999999999424
> 2.20685747493755e-07    0.575689534431218       5.30976753476747e-08    0.00210500604605837     0.588947341576757       3.1310360048386e-10     0.999999999998789
> 1.92961662926727e-08    0.773588030510204       1.04841835042200e-08    0.00206723852358352     0.816755014708394       3.89478290348532e-11    0.999999999997794
> 0.000279824051289082    0.000310992385522886    1.01467522935252e-06    3.11645639181419e-05    0.00249801538651552     3.0978819115532e-05     7.11821104872585e-06
> 2.81901448690893e-07    0.381718731525906       4.72860507882539e-08    0.00206807672109157     0.769178513763055       1.39278079797628e-09    0.999999999967123
> 5.58938545019597e-05    0.00171253668169328     4.54005998518212e-09    0.00165663757292733     0.00247994862102590     6.20992250482468e-06    0.419169641865998
> 1.03300938985890e-08    0.438357835603591       6.89854079723234e-09    0.00206693286138396     0.977554885433201       1.17209206267609e-10    0.99999999996921
> 7.63336821363444e-05    0.00177141538041517     1.88050423143828e-10    0.00169507950991094     0.00249739505142207     8.4814984916537e-06     0.470929220605509
> 9.16005846107533e-09    0.682179815036755       1.63255733785783e-09    0.00206922107327189     0.919323193130209       5.71436138398897e-11    0.99999999999629
> 1.40968913167328e-08    0.343606628343661       1.33227447885302e-08    0.00206789984370423     0.343671264496824       1.11679312116211e-11    0.999999999999822
> 4.76054734844857e-09    0.593022549313178       2.28102966623129e-09    0.00206625165098398     0.947562121256448       8.9437610753173e-11     0.999999999999992
> 1.96950784184139e-07    0.579488113726155       1.61915231214025e-07    0.00208000350528798     1.00891340595040        1.22248906754713e-10    0.999999999996493
> 8.1448937742933e-09     0.441088618716555       4.54846390087941e-09    0.00207634940425852     0.446155700100820       4.81439647816238e-12    0.99999999999939
> 4.82439218405912e-08    0.557771049256698       3.53737879481732e-08    0.0020663035737319      0.588137767965923       2.6568947800491e-11     0.999999999988615
> 2.43086751126363e-08    0.522927598354163       2.26886829089137e-08    0.00206533531066324     0.611696593543814       4.51226610050184e-11    0.999999999999087
> 3.05498959434100e-08    0.465522202845817       1.09246302124670e-08    0.00207004066920179     0.465583376966915       3.24213847202457e-11    0.999999999997366
> 1.88687179088788e-07    0.783614197203923       4.51346471059839e-08    0.00222403775221293     0.786422171740329       8.17865794171933e-10    0.999999999986103
> 1.0154423824979e-08     0.302657777579883       9.06923080122203e-09    0.00206615353968094     0.359722316646974       8.27866320956902e-12    0.99999999998461
> 8.91008717665837e-08    0.0020661526864997      3.08619455858999e-09    0.00206579199039568     0.00275523149199496     9.55650084108725e-09    0.985185595958656
> 1.25320647920029e-07    0.635217955401437       7.44627883600107e-08    0.00206656250455391     0.855937507707323       3.70326032870889e-10    0.999999999998375
> 2.57618374406559e-08    0.636499151952225       1.09822023878715e-08    0.00206677354204888     0.772636071860102       8.99370944431481e-11    0.999999999978744
> 1.09474196877990e-08    0.501469973722704       1.19992915868609e-10    0.00206117941606503     0.501594064757161       1.34320044786225e-11    0.999999999991232
> 5.24203710193977e-05    0.000127998340144109    3.33258623630601e-09    7.55779680724378e-05    0.00248898574263025     5.82411313482383e-06    0.0221497278110802
> 3.80217498132259e-07    0.57664568703189        1.01755510162620e-08    0.00207232950382402     0.944031557945531       5.30703662426069e-10    0.999999999995957
> 1.45159816281038e-09    0.391742001993341       1.13492553980291e-09    0.00206615324883312     0.73041632635671        2.05351961803669e-11    0.99999999997684
> 8.74006318627465e-05    0.00176059707830211     1.94489765002966e-09    0.00167319599216734     0.00234472182706612     9.710963192258e-06      0.358282221037878
> 0.000238275046444342    0.000264776586825777    4.40904795740029e-10    2.64911355650347e-05    0.00259858019547791     2.64749446522330e-05    1.83580740341509e-07
> 7.39730651399581e-10    0.00908091738218919     2.48425610406978e-10    0.00206667909063917     0.00928420674840742     5.18131986840764e-11    0.99999875366957
> 6.76489417503509e-08    0.66948409231674        3.03852881114497e-08    0.00207327594614506     0.82038550137061        2.64031575134214e-11    0.999999999984622
> 2.47578596556463e-05    0.00192273935278028     1.23710433232059e-10    0.00189797520967688     0.00249714264227544     2.75085723641778e-06    0.654395200885401
> 1.89137722020867e-08    0.611106915103611       3.89703191870382e-09    0.00210184590871973     0.624411027730728       8.5135512646264e-11     0.99999999999684
> 0.00019176743398086     0.000561608946531094    3.86469382998825e-10    0.000369840972771214    0.00250444089363261     2.13074457599853e-05    0.0571352017772342
> 0.000185606084629350    0.000206269128591342    1.56795453162326e-11    2.06627190417154e-05    0.00249993097016385     2.06228887085015e-05    1.71605199821395e-06
> 6.07957336199056e-08    0.483682520205442       5.5863930138716e-09     0.00206322767692575     0.483682695616455       5.34362969147144e-11    0.999999999999643
> 2.32535804244419e-08    0.479995736120651       1.71422519965926e-08    0.00207338945556393     0.510141538832558       1.48964674702430e-11    0.999999999993732
> 8.9183702782689e-06     0.00206653282852738     1.75762003328053e-10    0.00205761314408722     0.00252353219843752     9.9090949737504e-07     0.833268509557228
> 1.98549228223182e-06    0.251617257564206       1.05359270191879e-06    0.0022260243792503      0.251624161164046       9.60072596752024e-08    0.99999999999983
> 3.98028438135354e-08    0.547004412551238       3.46420325883517e-08    0.00206593124574590     0.780233389945956       4.31912137883551e-11    0.999999999999248
> 7.415833973742e-09      0.411955745727431       6.63826778226352e-09    0.00206692655795474     0.422974654599724       6.0360458732987e-12     0.999999999999735
> 2.31198818168677e-09    0.00651214491263961     1.32346818065092e-09    0.00206600061742381     0.0308298577099849      1.09702273280814e-10    0.99987783901442
> 4.17312889043775e-08    0.535841994679291       2.16650907799191e-08    0.00206804295529002     0.59999577506709        1.18576996500002e-10    0.999999999999237
> 3.06571185479614e-08    0.210504230265054       2.05714086443132e-08    0.00206707615217700     0.218748559037881       2.15131848511398e-14    0.999999999993557
> 2.75615889053932e-09    0.410462493146348       1.61078464820130e-09    0.00206420001019636     0.410565659594555       9.40738331890563e-11    0.999999999999952
> 4.56613193593397e-09    0.408415058997493       2.33397373332447e-09    0.00206711241339086     0.532349316444773       6.74887208834456e-11    0.99999999999929
> 7.18189347926295e-08    0.456169544603848       5.38996419605601e-08    0.00206990174706466     1.10900729793115        1.09308927282691e-13    0.999999999990783
> 0.000188732057072893    0.000209745553735604    2.65570357321341e-11    2.10123232762018e-05    0.00249946399527847     2.09702247828353e-05    7.56804739408697e-07
> 5.85086685972612e-08    0.586106422015639       3.34740271798516e-08    0.00207017375045705     1.02476378656892        7.7586558103274e-11     0.999999999999751
> 0.000110963331385117    0.000134315151780862    1.07242866306447e-06    2.33497881113803e-05    0.00255535434807175     1.22100977811675e-05    0.0100715104652649
> 6.74756547544261e-05    0.00181277651670570     2.06882429964314e-10    0.0017453002881811      0.00248364499775618     7.49727080120661e-06    0.477865304416143
> 2.12169576142447e-09    0.258696699048681       3.83834275215625e-10    0.00206572907235368     0.593016720449582       2.47234004478185e-11    0.999999999976093
> 4.90290042793427e-08    0.845133894257049       4.26659941495083e-08    0.00206573635916222     0.846607006638287       2.63753283209851e-10    0.999999999994938
> 1.82215438136793e-08    0.80366504831158        1.78340820866931e-08    0.00225471081235068     0.805133180734986       2.65837229120276e-12    0.999999999992972
> 4.73449331282602e-08    0.284890209400889       3.18447541813436e-08    0.00206840481685483     0.808166776706798       4.05949074947464e-10    0.999999999996061
> 1.09692979675704e-08    0.504203767689845       1.91188777015599e-09    0.00206788476221373     0.706912076703946       4.06894266747559e-11    0.99999999999973
> 4.34112012988195e-08    0.458125660334097       6.46770375936903e-09    0.00207007623328698     0.625303714142111       1.01347377674089e-10    0.999999999992636
> 1.16509068624767e-07    0.598772124413396       1.83783072861908e-08    0.00207311616484422     0.649412674861061       1.72090788969858e-10    0.999999999982887
> 1.72904399298124e-08    0.58859841513479        5.56864060572789e-09    0.00206902943325815     0.834770114923498       4.44826394221343e-11    0.999999999998404
> 1.95167889112074e-07    0.680051903362475       4.07870548730064e-08    0.0021048787521164      0.758987053084469       3.35901569072651e-10    0.999999999999464
> 1.06657670445553e-08    0.705185947467634       5.468246700687e-09      0.00206717328235464     0.852353580488024       9.79781097317382e-11    0.999999999997673
> 3.38906655480421e-08    0.675066389889348       5.7356420037523e-09     0.00206939927716748     0.864714866878808       3.50701456652884e-10    0.999999999999926
> 4.72130896241037e-09    0.193480077858215       4.09642232386814e-09    0.00206639690357456     0.193772609031377       2.22379321541448e-11    0.999999999992884
> 3.55665252909232e-08    0.620669750121032       1.90641809388101e-08    0.00206864461846135     0.622207402604423       3.55684392133601e-11    0.999999999999635
> 3.43643575170166e-08    0.508032144170851       3.05576064051123e-08    0.0020666442030495      0.537473527046566       5.41317633959989e-12    0.999999999991207
> 2.07413577081273e-08    0.517006009937055       1.78794895322523e-08    0.00206609160101216     0.723546217083115       4.25770563404542e-11    0.999999999977351
> 2.9634111809255e-08     0.678167734702809       2.87136255305397e-08    0.00207005922065176     0.680058941948453       4.86294997869349e-11    0.999999999997674
> 2.70007729246723e-08    0.777109777637737       2.23198659183296e-08    0.00211784135373727     0.821904778895813       2.72480529179635e-10    0.999999999999417
> 1.21638582096581e-05    0.685066257234478       3.90995182181942e-06    0.00201558375343728     0.685516429216796       4.73765370536012e-08    0.999999999999993
> 6.21341332802845e-08    0.45153366532873        4.28836207599489e-08    0.00205228864113157     0.630886659065685       2.01031448944481e-10    0.999999999976095
> 1.19446880731148e-06    0.00204849378641721     5.41758686583886e-10    0.00204660358124349     0.00250002927315376     1.32657483581135e-07    0.926358671695374
> 4.23724996341138e-07    0.76589882078562        1.07976473375288e-07    0.00231084075578484     0.772123021534594       4.222800306051e-09      0.99999999999554
> 9.76490929801592e-09    0.611133237912937       3.40654501481527e-09    0.00206753317060016     0.624278286034974       1.35688861199835e-11    0.999999999991987
> 0.000185800168507657    0.000206604414024173    1.51257663200075e-10    2.08040548465085e-05    0.00249987187225613     2.06444402710859e-05    1.00090667437022e-05
> 4.87431144092831e-09    0.486985270937825       1.01147265793908e-09    0.00206873578985202     0.494229783529316       2.1163667211661e-12     0.999999999999217
> 1.58678140422157e-08    0.458475824861338       3.728478508397e-09      0.00206665340523142     0.822538576890515       6.73647051811321e-11    0.999999999992949
> 7.78797943133767e-07    0.00201714819466376     1.00271036821046e-09    0.00201517307886584     0.00243247068677390     8.6420943116519e-08     0.946415585057681
> 1.79734244345473e-07    0.618114350664356       8.41196223101054e-08    0.00209424634015987     1.09808725088463        1.36311179064841e-09    0.999999999999325
> 4.04750622932535e-06    0.00202435649740235     9.55231839434103e-11    0.00202030658534268     0.00249780762721203     4.49711157881299e-07    0.878051982789172
> 3.27944009298396e-08    0.710329613922295       2.02989735589882e-08    0.00206702754480486     0.713789321637634       4.43500632164969e-12    0.99999999998325
> 3.6211203640154e-08     0.486990872484964       2.23029819494578e-08    0.00206834819984283     0.925447188571729       1.43913590315638e-10    0.999999999999852
> 1.52117049757253e-08    0.499368102973816       5.70289700988348e-09    0.00206632041653347     0.688163413575525       6.86450078797379e-11    0.99999999998881
> 2.14084286484963e-08    0.669902287842233       1.64604388157258e-08    0.00206787121557579     0.675669448264383       3.05725070183645e-11    0.99999999999783
> 4.64383080200508e-05    0.00188125145190771     2.92580309919556e-10    0.00183481310287054     0.00250971056566956     5.15977640139516e-06    0.571340025768215
> 3.11099577796555e-08    0.319299482089934       8.84023352637676e-09    0.00206855023112842     0.441077204370492       1.10446087116967e-09    0.999999999995467
> 0.000102965203269986    0.0016493488546921      2.58445958349384e-10    0.00154638327053218     0.00250288634336625     1.14405481683120e-05    0.342341363715383
> 5.87912519479874e-09    0.00301357388024528     2.75211293457077e-10    0.00206603126487256     0.00686204622812863     6.1682829968701e-10     0.999931564740833
> 8.28694064868836e-08    0.673413934917665       5.55691871872657e-08    0.00206648968616976     0.690994737517309       8.53762213679993e-11    0.99999999999553
> 2.78964308878308e-08    0.631906605077265       1.87081150212841e-08    0.00206682302780812     1.04912853459322        1.19798263785958e-10    0.999999999998404
> 0.000186009655330342    0.000206938973112988    1.18525336022574e-09    2.09286186935030e-05    0.00249989443096258     2.06676012144089e-05    2.57319855839169e-06
> 5.79914576203232e-05    0.00181271191072469     1.15884908131972e-09    0.00175471081855071     0.0024812702174432      6.44334507197721e-06    0.509698530097913
> 2.0503183260964e-08     0.546450460261978       1.27059638843222e-08    0.00206791702172114     1.14082199649402        8.5751965370755e-12     0.99999999999683
> 3.66836690280629e-08    0.481874078118496       3.13611734401948e-08    0.00207354422550728     0.990376379663234       2.13707863395726e-10    0.999999999975761
> 1.77224405556878e-08    0.493128575789259       8.63845133717168e-09    0.00207307974398005     0.493553411995952       3.19150428367562e-11    0.999999999999902
> 0.000119234853016321    0.00194143059603129     2.65621637099599e-10    0.00182214114564480     0.00258385562559042     1.32482776896270e-05    0.564809333872708
> 1.91321783130778e-07    0.00205131977291163     2.45737561775750e-10    0.00205112100576657     0.00251498989989795     2.12303674145017e-08    0.981038377325434
> 2.04970869809e-09       0.61540968024267        1.80692316850342e-10    0.00206758160218205     0.616881623876956       9.88879753424694e-11    0.999999999998682
> 2.28380664111838e-07    0.495196927735637       2.03520983522517e-07    0.00206484911183903     0.495729865740038       3.10063601627842e-10    0.999999999998029
> 0.000142663207352814    0.00136226156979156     2.47224893929883e-11    0.00121959537442013     0.00249964872330666     1.58514632925203e-05    0.213308634647407
>
>
>
> > ----- Original Message -----
> > From: Paul Smith <phhs80 at gmail.com>
> > Date: Wednesday, July 4, 2007 6:00 am
> > Subject: Re: [R] Fine tunning rgenoud
> > To: R-help <r-help at stat.math.ethz.ch>
> >
> >
> > > On 7/4/07, RAVI VARADHAN <rvaradhan at jhmi.edu> wrote:
> > >  > Here is another approach: I wrote an R function that would generate
> > > interior points as starting values for constrOptim.  This might work
> > > better than the LP approach, since the LP approach gives you a
> > > starting value that is on the boundary of the feasible region, i.e a
> > > vertex of the polyhedron, whereas this new approach gives you points
> > > on the interior.  You can generate as many points as you wish, but the
> > > approach is brute-force and is very inefficient - it takes on the
> > > order of a 1000 tries to find one feasible point.
> > >
> > >  Thanks again, Ravi. Actually, the LP approach also works here. Let
> > >  g(X) >= k be the constraints. Then, by solving a LP problem with the
> > >  constraints
> > >
> > >  g(X) >= (k+0.2)
> > >
> > >  returns an interior starting value for constrOptim. I am aware that
> > >  the new set of constraints may correspond to an impossible linear
> > >  system, but it works in many cases.
> > >
> > >  Paul
> > >
> > >  > ----- Original Message -----
> > >  > From: Paul Smith <phhs80 at gmail.com>
> > >  > Date: Tuesday, July 3, 2007 7:32 pm
> > >  > Subject: Re: [R] Fine tunning rgenoud
> > >  > To: R-help <r-help at stat.math.ethz.ch>
> > >  >
> > >  >
> > >  > > On 7/4/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> > >  > >  > It should be easy enough to check that your solution is valid
> > > (i.e.
> > >  > > a local
> > >  > >  > minimum):  first, check to see if the solution satisfies all the
> > >  > >  > constraints; secondly, check to see if it is an interior point
> > >  > > (i.e. none of
> > >  > >  > the constraints become equality); and finally, if the solution
> > > is an
> > >  > >  > interior point, check to see whether the gradient there is
> > > close to
> > >  > > zero.
> > >  > >  > Note that if the solution is one of the vertices of the polyhedron,
> > >  > > then the
> > >  > >  > gradient may not be zero.
> > >  > >
> > >  > >  I am having bad luck: all constraints are satisfied, but the solution
> > >  > >  given by constrOptim is not interior; the gradient is not equal
> > > to
> > >  > >  zero.
> > >  > >
> > >  > >  Paul
> > >  > >
> > >  > >
> > >  > >  > -----Original Message-----
> > >  > >  > From: r-help-bounces at stat.math.ethz.ch
> > >  > >  > [ On Behalf Of Paul Smith
> > >  > >  > Sent: Tuesday, July 03, 2007 5:10 PM
> > >  > >  > To: R-help
> > >  > >  > Subject: Re: [R] Fine tunning rgenoud
> > >  > >  >
> > >  > >  > On 7/3/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> > >  > >  > > You had indicated in your previous email that you are having
> > > trouble
> > >  > >  > finding
> > >  > >  > > a feasible starting value for constrOptim().  So, you basically
> > >  > > need to
> > >  > >  > > solve a system of linear inequalities to obtain a starting point.
> > >  > >  Have
> > >  > >  > you
> > >  > >  > > considered using linear programming? Either simplex() in the
> > > "boot"
> > >  > >  > package
> > >  > >  > > or solveLP() in "linprog" would work.  It seems to me that you
> > >  > > could use
> > >  > >  > any
> > >  > >  > > linear objective function in solveLP to obtain a feasible
> > >  > > starting point.
> > >  > >  > > This is not the most efficient solution, but it might be
> > > worth a
> > >  > > try.
> > >  > >  > >
> > >  > >  > > I am aware of other methods for generating n-tuples that satisfy
> > >  > > linear
> > >  > >  > > inequality constraints, but AFAIK those are not available in
> > > R.
> > >  > >  >
> > >  > >  > Thanks, Ravi. I had already conceived the solution that you suggest,
> > >  > >  > actually using "lpSolve". I am able to get a solution for my problem
> > >  > >  > with constrOptim, but I am not enough confident that the
> > > solution is
> > >  > >  > right. That is why I am trying to get a solution with rgenoud,
> > > but
> > >  > >  > unsuccessfully until now.
> > >  > >  >
> > >  > >  > Paul
> > >  > >  >
> > >  > >  >
> > >  > >  >
> > >  > >  > > -----Original Message-----
> > >  > >  > > From: r-help-bounces at stat.math.ethz.ch
> > >  > >  > > [ On Behalf Of Paul Smith
> > >  > >  > > Sent: Tuesday, July 03, 2007 4:10 PM
> > >  > >  > > To: R-help
> > >  > >  > > Subject: [R] Fine tunning rgenoud
> > >  > >  > >
> > >  > >  > > Dear All,
> > >  > >  > >
> > >  > >  > > I am trying to solve the following maximization problem, but
> > > I cannot
> > >  > >  > > have rgenoud giving me a reliable solution.
> > >  > >  > >
> > >  > >  > > Any ideas?
> > >  > >  > >
> > >  > >  > > Thanks in advance,
> > >  > >  > >
> > >  > >  > > Paul
> > >  > >  > >
> > >  > >  > > ----------------------------
> > >  > >  > > library(rgenoud)
> > >  > >  > >
> > >  > >  > > v <- 0.90
> > >  > >  > > O1 <- 10
> > >  > >  > > O2 <- 20
> > >  > >  > > O0 <- v*O1+(1-v)*O2
> > >  > >  > >
> > >  > >  > > myfunc <- function(x) {
> > >  > >  > >   U0 <- x[1]
> > >  > >  > >   U1 <- x[2]
> > >  > >  > >   U2 <- x[3]
> > >  > >  > >   q0 <- x[4]
> > >  > >  > >   q1 <- x[5]
> > >  > >  > >   q2 <- x[6]
> > >  > >  > >   p <- x[7]
> > >  > >  > >
> > >  > >  > >   if (U0 < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if (U1 < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if (U2 < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if ((U0-(U1+(O1-O0)*q1)) < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if ((U0-(U2+(O2-O0)*q2)) < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if ((U1-(U0+(O0-O1)*q0)) < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if ((U1-(U2+(O2-O1)*q2)) < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if((U2-(U0+(O0-O2)*q0)) < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if((U2-(U1+(O1-O2)*q1)) < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if(p < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if(p > 1)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if(q0 < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if(q1 < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else if(q2 < 0)
> > >  > >  > >     return(-1e+200)
> > >  > >  > >   else
> > >  > >  > >
> > >  > >  > return(p*(sqrt(q0)-(O0*q0+U0))+(1-p)*(v*(sqrt(q1)-(O1*q1+U1))+(1-v)*(sqrt(q2
> > >  > >  > > )-(O2*q2+U2))))
> > >  > >  > >
> > >  > >  > > }
> > >  > >  > >
> > >  > >  > genoud(myfunc,nvars=7,max=T,pop.size=6000,starting.values=runif(7),wait.gene
> > >  > >  > > rations=150,max.generations=300,boundary.enforcement=2)
> > >  > >  > >
> > >  > >  > > ______________________________________________
> > >  > >  > > R-help at stat.math.ethz.ch mailing list
> > >  > >  > >
> > >  > >  > > PLEASE do read the posting guide
> > >  > >  >
> > >  > >  > > and provide commented, minimal, self-contained, reproducible
> > > code.
> > >  > >  > >
> > >  > >  >
> > >  > >  > ______________________________________________
> > >  > >  > R-help at stat.math.ethz.ch mailing list
> > >  > >  >
> > >  > >  > PLEASE do read the posting guide
> > >  > >  > and provide commented, minimal, self-contained, reproducible code.
> > >  > >  >
> > >  > >
> > >  > >  ______________________________________________
> > >  > >  R-help at stat.math.ethz.ch mailing list
> > >  > >
> > >  > >  PLEASE do read the posting guide
> > >  > >  and provide commented, minimal, self-contained, reproducible code.
> > >  >
> > >  >
> > >
> > >  ______________________________________________
> > >  R-help at stat.math.ethz.ch mailing list
> > >
> > >  PLEASE do read the posting guide
> > >  and provide commented, minimal, self-contained, reproducible code.
> >
>


From pzs6 at CDC.GOV  Tue Jul  3 18:13:28 2007
From: pzs6 at CDC.GOV (Smith, Phil (CDC/CCID/NCIRD))
Date: Tue, 3 Jul 2007 12:13:28 -0400
Subject: [R] Please help with legend command
Message-ID: <392FF8243BA9634084F5AC5EF07B5CDF018AFCC6@LTA3VS002.ees.hhs.gov>

Hi R-ers:

I'm drawing a plot and have used different line types (lty) for
different race/ethnicity groups. I want a legend that explains what line
types correspond to the different race/ethnicity groups. I used the
following code:


legend( 1992 , 42  , c("Hispanic" , "non-Hispanic white (NHW)" ,
"non-Hispanic black" , "AI/AN" , "Asian" ) , lty=1:5 ,cex = .6 , bty='n'
)

Guess what? The legend "box" was so narrow that the line types that show
up in that legend box look essentially the same, because they are short.
I.e, although a line type might be a long dash followed by a short dash,
only the long dash shows up in the box. The consequence of this is that
the race/ethnic group that corresponds to the line type that is only a
long dash cannot be distinguished from the legend.

How do I stretch that legend box out so as to allow lty to draw longer
line segments?

Please reply to: pzs6 at cdc.gov

Many thanks!
Phil Smith
Centers for Disease Control and Prevention


From ben_it at libero.it  Wed Jul  4 14:18:02 2007
From: ben_it at libero.it (ben_it at libero.it)
Date: Wed,  4 Jul 2007 14:18:02 +0200
Subject: [R] copula estimation wih time series marginals
Message-ID: <JKNM62$46F95B93BA537A19F763D2E348B10BD9@libero.it>

I am using R 2.5.1 for windows and my purpose is to estimate a clayton copula . 
Since I have two time series marginals, I found that the most appropriate model was an ARMA(1,0)+GARCH(1,1) model for both with sstd as conditional distribution. Can anyone give me some tips about the code to estimate the copula?
Thanks in advance

Gaetano Rossi


------------------------------------------------------
Scegli infostrada: ADSL gratis per tutta l?estate e telefoni senza canone Telecom
http://click.libero.it/infostrada


From Keith.Chamberlain at Colorado.EDU  Wed Jul  4 15:44:44 2007
From: Keith.Chamberlain at Colorado.EDU (Keith Alan Chamberlain)
Date: Wed, 4 Jul 2007 07:44:44 -0600
Subject: [R] A More efficient method?
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch> 
Message-ID: <000101c7be41$7b2a4d30$6501a8c0@kLab>

Dear Rhelpers,

Is there a faster way than below to set a vector based on values from
another vector? I'd like to call a pre-existing function for this, but one
which can also handle an arbitrarily large number of categories. Any ideas?

Cat=c('a','a','a','b','b','b','a','a','b')	# Categorical variable
C1=vector(length=length(Cat))	# New vector for numeric values

# Cycle through each column and set C1 to corresponding value of Cat.
for(i in 1:length(C1)){
	if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
}

C1
[1] -1 -1 -1  1  1  1 -1 -1  1
Cat
[1] "a" "a" "a" "b" "b" "b" "a" "a" "b"

Sincerely,
KeithC.
Psych Undergrad, CU Boulder (US)
RE McNair Scholar


From knoblauch at lyon.inserm.fr  Wed Jul  4 17:11:59 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Wed, 4 Jul 2007 15:11:59 +0000 (UTC)
Subject: [R] A More efficient method?
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
Message-ID: <loom.20070704T171011-929@post.gmane.org>

Keith Alan Chamberlain <Keith.Chamberlain <at> Colorado.EDU> writes:
> Cat=c('a','a','a','b','b','b','a','a','b')	# Categorical variable
> C1=vector(length=length(Cat))	# New vector for numeric values

> for(i in 1:length(C1)){
> 	if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> }
> 
> C1
> [1] -1 -1 -1  1  1  1 -1 -1  1
> Cat
> [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"

 ifelse(Cat == "a", -1, 1)
[1] -1 -1 -1  1  1  1 -1 -1  1

HTH


From bcarvalh at jhsph.edu  Wed Jul  4 17:12:39 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Wed, 4 Jul 2007 11:12:39 -0400
Subject: [R] A More efficient method?
In-Reply-To: <000101c7be41$7b2a4d30$6501a8c0@kLab>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
Message-ID: <2E743078-E164-4B8F-88F8-27C5D0C4675D@jhsph.edu>

C1 <- rep(-1, length(Cat))
C1[Cat == "b"]] <- 1

b

On Jul 4, 2007, at 9:44 AM, Keith Alan Chamberlain wrote:

> Dear Rhelpers,
>
> Is there a faster way than below to set a vector based on values from
> another vector? I'd like to call a pre-existing function for this,  
> but one
> which can also handle an arbitrarily large number of categories.  
> Any ideas?
>
> Cat=c('a','a','a','b','b','b','a','a','b')	# Categorical variable
> C1=vector(length=length(Cat))	# New vector for numeric values
>
> # Cycle through each column and set C1 to corresponding value of Cat.
> for(i in 1:length(C1)){
> 	if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> }
>
> C1
> [1] -1 -1 -1  1  1  1 -1 -1  1
> Cat
> [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"
>
> Sincerely,
> KeithC.
> Psych Undergrad, CU Boulder (US)
> RE McNair Scholar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From singularitaet at gmx.net  Wed Jul  4 17:16:51 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Wed, 04 Jul 2007 17:16:51 +0200
Subject: [R] A More efficient method?
In-Reply-To: <000101c7be41$7b2a4d30$6501a8c0@kLab>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
Message-ID: <468BB9E3.2010309@gmx.net>


> Cat=c('a','a','a','b','b','b','a','a','b')	# Categorical variable
> C1=vector(length=length(Cat))	# New vector for numeric values
>
> # Cycle through each column and set C1 to corresponding value of Cat.
> for(i in 1:length(C1)){
> 	if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> }
>
> C1
> [1] -1 -1 -1  1  1  1 -1 -1  1
> Cat
> [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"
>
>   
how about:

Cat<-c('a','a','a','b','b','b','a','a','b')
c1<- -2*(Cat=="a")+1



-=-=-
... Time is an illusion, lunchtime doubly so. (Ford Prefect)


From Thierry.ONKELINX at inbo.be  Wed Jul  4 17:17:30 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 4 Jul 2007 17:17:30 +0200
Subject: [R] A More efficient method?
In-Reply-To: <000101c7be41$7b2a4d30$6501a8c0@kLab>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
Message-ID: <2E9C414912813E4EB981326983E0A104033E493D@inboexch.inbo.be>

Cat <- c('a','a','a','b','b','b','a','a','b')
C1 <- ifelse(Cat == 'a', -1, 1)

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Keith Alan 
> Chamberlain
> Verzonden: woensdag 4 juli 2007 15:45
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] A More efficient method?
> 
> Dear Rhelpers,
> 
> Is there a faster way than below to set a vector based on 
> values from another vector? I'd like to call a pre-existing 
> function for this, but one which can also handle an 
> arbitrarily large number of categories. Any ideas?
> 
> Cat=c('a','a','a','b','b','b','a','a','b')	# Categorical variable
> C1=vector(length=length(Cat))	# New vector for numeric values
> 
> # Cycle through each column and set C1 to corresponding value of Cat.
> for(i in 1:length(C1)){
> 	if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> }
> 
> C1
> [1] -1 -1 -1  1  1  1 -1 -1  1
> Cat
> [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"
> 
> Sincerely,
> KeithC.
> Psych Undergrad, CU Boulder (US)
> RE McNair Scholar
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From statadat at gmail.com  Wed Jul  4 17:21:39 2007
From: statadat at gmail.com (domenico pestalozzi)
Date: Wed, 4 Jul 2007 17:21:39 +0200
Subject: [R] how to solve a min problem
In-Reply-To: <468A9E36.3030307@pdf.com>
References: <e591a95b0707020748p2650666bs972a7f28c8001a31@mail.gmail.com>
	<468A9E36.3030307@pdf.com>
Message-ID: <e591a95b0707040821u991b9d1p63e5dbb17df7570a@mail.gmail.com>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/b03c714f/attachment.pl 

From aconesa at ochoa.fib.es  Wed Jul  4 17:21:53 2007
From: aconesa at ochoa.fib.es (Ana Conesa)
Date: Wed, 4 Jul 2007 17:21:53 +0200
Subject: [R] questions on lme function
Message-ID: <380-22007734152153148@ochoa.fib.es>


Dear list,

I am using the lme funcion to fit a mixed model for the time response
of a number of physiological variables. The random variable would be
the individual on which physiological variables are measured at
different time points. I have 4 time points, 5 individuals and 3
replicates per condition (time/individual),  and I would like to fit
a quadratic model on time. The model I am using is

> mm <- lme(myvar ~ time + time2, random= ~ time|individual,
data=clinical)

being time2 = time*time

I have a number of questions

1) I am not very sure the random effect is correctly modeled. Would I
need to include the time2 variable aswell?

2) I would like to extract the F statistics of the model, and I do
not find a function for this. Is this possible?

3) depending of the variable I take, I frequently obtain a
convergence error as a result of the lme funcion. Any ideas on what
to do to improve convergence?

Thank you

Ana Conesa, PhD
Centro de Investigacion Principe Felipe
Avda. Autopista Saler 16 46013 Valencia
http://bioinfo.cipf.es


From ggrothendieck at gmail.com  Wed Jul  4 17:29:12 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jul 2007 11:29:12 -0400
Subject: [R] A More efficient method?
In-Reply-To: <000101c7be41$7b2a4d30$6501a8c0@kLab>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
Message-ID: <971536df0707040829w55bc833y47ff9d085c21949e@mail.gmail.com>

Here are two ways.  The second way is more than 10x faster.

> set.seed(1)
> C <- sample(c("a", "b"), 100000, replace = TRUE)
> system.time(s1 <- ifelse(C == "a", 1, -1))
   user  system elapsed
   0.37    0.01    0.38
> system.time(s2 <- 2 * (C == "a") - 1)
   user  system elapsed
   0.02    0.00    0.02
> identical(s1, s2)
[1] TRUE

On 7/4/07, Keith Alan Chamberlain <Keith.Chamberlain at colorado.edu> wrote:
> Dear Rhelpers,
>
> Is there a faster way than below to set a vector based on values from
> another vector? I'd like to call a pre-existing function for this, but one
> which can also handle an arbitrarily large number of categories. Any ideas?
>
> Cat=c('a','a','a','b','b','b','a','a','b')      # Categorical variable
> C1=vector(length=length(Cat))   # New vector for numeric values
>
> # Cycle through each column and set C1 to corresponding value of Cat.
> for(i in 1:length(C1)){
>        if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> }
>
> C1
> [1] -1 -1 -1  1  1  1 -1 -1  1
> Cat
> [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"
>
> Sincerely,
> KeithC.
> Psych Undergrad, CU Boulder (US)
> RE McNair Scholar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From joris.dewolf at cropdesign.com  Wed Jul  4 17:30:58 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Wed, 4 Jul 2007 17:30:58 +0200
Subject: [R] A More efficient method?
In-Reply-To: <2E9C414912813E4EB981326983E0A104033E493D@inboexch.inbo.be>
Message-ID: <OF1EFB3160.9E831BEE-ONC125730E.0055316B-C125730E.00553CC6@basf-c-s.be>



or
Cat <- c('a','a','a','b','b','b','a','a','b')
C1 <- (Cat=='a')*1







                                                                           
             "ONKELINX,                                                    
             Thierry"                                                      
             <Thierry.ONKELINX                                          To 
             @inbo.be>                 "Keith Alan Chamberlain"            
             Sent by:                  <Keith.Chamberlain at Colorado.EDU>,   
             r-help-bounces at st         <r-help at stat.math.ethz.ch>          
             at.math.ethz.ch                                            cc 
                                                                           
                                                                   Subject 
             04/07/2007 17:17          Re: [R] A More efficient method?    
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Cat <- c('a','a','a','b','b','b','a','a','b')
C1 <- ifelse(Cat == 'a', -1, 1)

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be
www.inbo.be

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney



> -----Oorspronkelijk bericht-----
> Van: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] Namens Keith Alan
> Chamberlain
> Verzonden: woensdag 4 juli 2007 15:45
> Aan: r-help at stat.math.ethz.ch
> Onderwerp: [R] A More efficient method?
>
> Dear Rhelpers,
>
> Is there a faster way than below to set a vector based on
> values from another vector? I'd like to call a pre-existing
> function for this, but one which can also handle an
> arbitrarily large number of categories. Any ideas?
>
> Cat=c('a','a','a','b','b','b','a','a','b')           # Categorical
variable
> C1=vector(length=length(Cat))            # New vector for numeric values
>
> # Cycle through each column and set C1 to corresponding value of Cat.
> for(i in 1:length(C1)){
>            if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> }
>
> C1
> [1] -1 -1 -1  1  1  1 -1 -1  1
> Cat
> [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"
>
> Sincerely,
> KeithC.
> Psych Undergrad, CU Boulder (US)
> RE McNair Scholar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From efh at nessie.mcc.ac.uk  Wed Jul  4 17:44:22 2007
From: efh at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 04 Jul 2007 16:44:22 +0100 (BST)
Subject: [R] A More efficient method?
In-Reply-To: <000101c7be41$7b2a4d30$6501a8c0@kLab>
Message-ID: <XFMail.070704164422.efh@nessie.mcc.ac.uk>

On 04-Jul-07 13:44:44, Keith Alan Chamberlain wrote:
> Dear Rhelpers,
> 
> Is there a faster way than below to set a vector based on values
> from another vector? I'd like to call a pre-existing function for
> this, but one which can also handle an arbitrarily large number
> of categories. Any ideas?
> 
> Cat=c('a','a','a','b','b','b','a','a','b')    # Categorical variable
> C1=vector(length=length(Cat)) # New vector for numeric values
> 
># Cycle through each column and set C1 to corresponding value of Cat.
> for(i in 1:length(C1)){
>       if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> }
> 
> C1
> [1] -1 -1 -1  1  1  1 -1 -1  1
> Cat
> [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"

> Cat=c('a','a','a','b','b','b','a','a','b')

> Cat=="b" 
[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE

> (Cat=="b") - 0.5
[1] -0.5 -0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5  0.5

> 2*((Cat=="b") - 0.5)
[1] -1 -1 -1  1  1  1 -1 -1  1

to give one example of a way to do it. But you don't say why you
really want to do this. You may really want factors. And what do
you want to see if there is "an arbitrarily large number of
categories"?

For instance:

> factor(Cat,labels=c(-1,1))
[1] -1 -1 -1 1  1  1  -1 -1 1 

but this is not a vector, but a "factor" object. To get the vector,
you need to convert Cat to an integer:

> as.integer(factor(Cat))
[1] 1 1 1 2 2 2 1 1 2

where (unless you've specified otherwise in factor()) the values
will correspond to the elements of Cat in "natural" order, in this
case first "a" (-> 1), then "b" (-> 2).

E.g.

> Cat2<-c("a","a","c","b","a","b")
> as.integer(factor(Cat2))
[1] 1 1 3 2 1 2

so, with C2<-as.integer(factor(Cat2)), you get a vector of distinct
integers 91,2,3) for the distinct levels ("a","b","c") of Cat2.
If you want integer values for these levels, you can write a function
to change them.

Hoping this helps to beark the ice!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <efh at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 04-Jul-07                                       Time: 16:44:20
------------------------------ XFMail ------------------------------


From efh at nessie.mcc.ac.uk  Wed Jul  4 17:48:03 2007
From: efh at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 04 Jul 2007 16:48:03 +0100 (BST)
Subject: [R] A More efficient method?
In-Reply-To: <000101c7be41$7b2a4d30$6501a8c0@kLab>
Message-ID: <XFMail.070704164422.efh@nessie.mcc.ac.uk>

[Sorry, there were silly typose in the previous version. Corrected below]

On 04-Jul-07 13:44:44, Keith Alan Chamberlain wrote:
> Dear Rhelpers,
> 
> Is there a faster way than below to set a vector based on values
> from another vector? I'd like to call a pre-existing function for
> this, but one which can also handle an arbitrarily large number
> of categories. Any ideas?
> 
> Cat=c('a','a','a','b','b','b','a','a','b')    # Categorical variable
> C1=vector(length=length(Cat)) # New vector for numeric values
> 
># Cycle through each column and set C1 to corresponding value of Cat.
> for(i in 1:length(C1)){
>       if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> }
> 
> C1
> [1] -1 -1 -1  1  1  1 -1 -1  1
> Cat
> [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"

> Cat=c('a','a','a','b','b','b','a','a','b')

> Cat=="b" 
[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE

> (Cat=="b") - 0.5
[1] -0.5 -0.5 -0.5  0.5  0.5  0.5 -0.5 -0.5  0.5

> 2*((Cat=="b") - 0.5)
[1] -1 -1 -1  1  1  1 -1 -1  1

to give one example of a way to do it. But you don't say why you
really want to do this. You may really want factors. And what do
you want to see if there is "an arbitrarily large number of
categories"?

For instance:

> factor(Cat,labels=c(-1,1))
[1] -1 -1 -1 1  1  1  -1 -1 1 

but this is not a vector, but a "factor" object. To get the vector,
you need to convert Cat to an integer:

> as.integer(factor(Cat))
[1] 1 1 1 2 2 2 1 1 2

where (unless you've specified otherwise in factor()) the values
will correspond to the elements of Cat in "natural" order, in this
case first "a" (-> 1), then "b" (-> 2).

E.g.

> Cat2<-c("a","a","c","b","a","b")
> as.integer(factor(Cat2))
[1] 1 1 3 2 1 2

so, with C2<-as.integer(factor(Cat2)), you get a vector of distinct
integers [1,2,3) for the distinct levels ("a","b","c") of Cat2.
If you want different integer values for these levels, you can write
a function to change them.

Hoping this helps to break the ice!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <efh at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 04-Jul-07                                       Time: 16:44:20
------------------------------ XFMail ------------------------------


From S.Ellison at lgc.co.uk  Wed Jul  4 17:57:35 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Wed, 04 Jul 2007 16:57:35 +0100
Subject: [R] A More efficient method?
Message-ID: <s68bd19b.038@tedmail2.lgc.co.uk>

#Given
Cat=c('a','a','a','b','b','b','a','a','b')	# Categorical variable

#and defining 
coding<-array(c(-1,1), dimnames=list(unique(Cat) ))

#(ie an array of values corresponding to your character array levels, and with names set to those levels)

coding[Cat]

#does what you want.

>>> Keith Alan Chamberlain <Keith.Chamberlain at Colorado.EDU> 04/07/2007 14:44:44 >>>
Dear Rhelpers,

Is there a faster way than below to set a vector based on values from
another vector? I'd like to call a pre-existing function for this, but one
which can also handle an arbitrarily large number of categories. Any ideas?

Cat=c('a','a','a','b','b','b','a','a','b')	# Categorical variable
C1=vector(length=length(Cat))	# New vector for numeric values

# Cycle through each column and set C1 to corresponding value of Cat.
for(i in 1:length(C1)){
	if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
}

C1
[1] -1 -1 -1  1  1  1 -1 -1  1
Cat
[1] "a" "a" "a" "b" "b" "b" "a" "a" "b"

Sincerely,
KeithC.
Psych Undergrad, CU Boulder (US)
RE McNair Scholar

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}


From singularitaet at gmx.net  Wed Jul  4 18:03:27 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Wed, 04 Jul 2007 18:03:27 +0200
Subject: [R] A More efficient method?
In-Reply-To: <971536df0707040829w55bc833y47ff9d085c21949e@mail.gmail.com>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
	<971536df0707040829w55bc833y47ff9d085c21949e@mail.gmail.com>
Message-ID: <468BC4CF.9090002@gmx.net>

Gabor Grothendieck wrote:
>> set.seed(1)
>> C <- sample(c("a", "b"), 100000, replace = TRUE)
>> system.time(s1 <- ifelse(C == "a", 1, -1))
>>     
>    user  system elapsed
>    0.37    0.01    0.38
>   
>> system.time(s2 <- 2 * (C == "a") - 1)
>>     
>    user  system elapsed
>    0.02    0.00    0.02
>   
> system.time(s1 <- ifelse(C == "a", 1, -1))
   user  system elapsed
   0.04    0.01    0.08
> system.time(s2 <- 2 * (C == "a") - 1)
   user  system elapsed
      0       0       0


I am just wondering: how comes the time does add up to 0.05 while
elapsed states 0.08 on my system? (Vista+R2.5.1)

Stefan


-=-=-
... Time is an illusion, lunchtime doubly so. (Ford Prefect)


From gmoyeyemi at yahoo.com  Wed Jul  4 18:18:41 2007
From: gmoyeyemi at yahoo.com (Yemi Oyeyemi)
Date: Wed, 4 Jul 2007 09:18:41 -0700 (PDT)
Subject: [R] working with matrix
Message-ID: <252974.4745.qm@web63907.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/763858ca/attachment.pl 

From csardi at rmki.kfki.hu  Wed Jul  4 18:31:02 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Wed, 4 Jul 2007 18:31:02 +0200
Subject: [R] working with matrix
In-Reply-To: <252974.4745.qm@web63907.mail.re1.yahoo.com>
References: <252974.4745.qm@web63907.mail.re1.yahoo.com>
Message-ID: <20070704163102.GC5713@guzu>

which.max(apply(mat, 1, prod))

Gabor

On Wed, Jul 04, 2007 at 09:18:41AM -0700, Yemi Oyeyemi wrote:
> I am new in R and I want to solve this problem;
>   I have a matrix X (with n-rows and p-colums) my problem is to obtain the products of the vectors of rows and print out only the maximum value and identify the row that gives the maximum value. Thanks
>   Oyeyemi, G.M
> 
>  
> ---------------------------------
> Don't pick lemons.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From Keith.Chamberlain at Colorado.EDU  Wed Jul  4 18:37:03 2007
From: Keith.Chamberlain at Colorado.EDU (Keith Alan Chamberlain)
Date: Wed, 4 Jul 2007 10:37:03 -0600
Subject: [R] A More efficient method?
In-Reply-To: <XFMail.070704164422.efh@nessie.mcc.ac.uk>
References: <000101c7be41$7b2a4d30$6501a8c0@kLab>
	<XFMail.070704164422.efh@nessie.mcc.ac.uk>
Message-ID: <001101c7be59$8e7b23b0$6501a8c0@kLab>

Dear Ted,

You are correct in that factors are probably what I had in mind since I
would be using them as predictors in a regression. I didn't know the syntax
to get R to do the arithmetic.

Many thanks to everyone who replied! 

Sincerely,
KeithC.
Psych Undergrad, CU Boulder (US)
RE McNair Scholar


From S.Ellison at lgc.co.uk  Wed Jul  4 18:36:11 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Wed, 04 Jul 2007 17:36:11 +0100
Subject: [R] working with matrix
Message-ID: <s68bdaab.056@tedmail2.lgc.co.uk>

Yemi,

Try

which.max(apply(X,1,prod))

(or possibly abs(apply(X,1,prod)) if you're only interested in unsigned product max.

S

>>> Yemi Oyeyemi <gmoyeyemi at yahoo.com> 04/07/2007 17:18:41 >>>
I am new in R and I want to solve this problem;
  I have a matrix X (with n-rows and p-colums) my problem is to obtain the products of the vectors of rows and print out only the maximum value and identify the row that gives the maximum value. Thanks
  Oyeyemi, G.M

 
---------------------------------
Don't pick lemons.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}


From nitish at imtech.res.in  Wed Jul  4 18:32:03 2007
From: nitish at imtech.res.in (Nitish Kumar Mishra)
Date: Wed, 4 Jul 2007 22:02:03 +0530 (IST)
Subject: [R] About dataset
Message-ID: <37151.59.160.112.42.1183566723.squirrel@webmail.imtech.res.in>

Hi R hep group member,
I want to know that how I can call my data in R for princomp function.
I want to calculate PCA of 200 descriptors of 4000 molecule(I am using
Linux). How I can call this in R.
Thanking you.


-- 
Nitish Kumar Mishra
Junior Research Fellow
BIC, IMTECH, Chandigarh, India
E-Mail Address:
nitish_km at yahoo.com
nitish at imtech.res.in


From ggrothendieck at gmail.com  Wed Jul  4 18:45:37 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jul 2007 12:45:37 -0400
Subject: [R] A More efficient method?
In-Reply-To: <971536df0707040829w55bc833y47ff9d085c21949e@mail.gmail.com>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
	<971536df0707040829w55bc833y47ff9d085c21949e@mail.gmail.com>
Message-ID: <971536df0707040945i262607fdk1fa7c33a11ceac7a@mail.gmail.com>

In thinking about this a bit more I have found a slightly faster one still.
See s3.  Also I have added s0, the original solution, to the timings.

> set.seed(1)
> C <- sample(c("a", "b"), 1000000, replace = TRUE)
> system.time({
+ s0 <- vector(length = length(C))
+ for(i in seq_along(C)) s0[i] <- if (C[i] == "a") 1 else -1
+ s0
+ })
   user  system elapsed
  21.75    0.02   25.99
> system.time(s1 <- ifelse(C == "a", 1, -1))
   user  system elapsed
   2.32    0.17    2.54
> system.time(s2 <- 2 * (C == "a") - 1)
   user  system elapsed
   0.29    0.02    0.32
> system.time({tmp <- C == "a"; tmp - !tmp})
   user  system elapsed
   0.21    0.00    0.21
> identical(s0, s1)
[1] TRUE
> identical(s0, s2)
[1] TRUE
> identical(s0, s3)
[1] TRUE

On 7/4/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Here are two ways.  The second way is more than 10x faster.
>
> > set.seed(1)
> > C <- sample(c("a", "b"), 100000, replace = TRUE)
> > system.time(s1 <- ifelse(C == "a", 1, -1))
>   user  system elapsed
>   0.37    0.01    0.38
> > system.time(s2 <- 2 * (C == "a") - 1)
>   user  system elapsed
>   0.02    0.00    0.02
> > identical(s1, s2)
> [1] TRUE
>
> On 7/4/07, Keith Alan Chamberlain <Keith.Chamberlain at colorado.edu> wrote:
> > Dear Rhelpers,
> >
> > Is there a faster way than below to set a vector based on values from
> > another vector? I'd like to call a pre-existing function for this, but one
> > which can also handle an arbitrarily large number of categories. Any ideas?
> >
> > Cat=c('a','a','a','b','b','b','a','a','b')      # Categorical variable
> > C1=vector(length=length(Cat))   # New vector for numeric values
> >
> > # Cycle through each column and set C1 to corresponding value of Cat.
> > for(i in 1:length(C1)){
> >        if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> > }
> >
> > C1
> > [1] -1 -1 -1  1  1  1 -1 -1  1
> > Cat
> > [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"
> >
> > Sincerely,
> > KeithC.
> > Psych Undergrad, CU Boulder (US)
> > RE McNair Scholar
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From ggrothendieck at gmail.com  Wed Jul  4 18:49:53 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 4 Jul 2007 12:49:53 -0400
Subject: [R] A More efficient method?
In-Reply-To: <971536df0707040945i262607fdk1fa7c33a11ceac7a@mail.gmail.com>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
	<971536df0707040829w55bc833y47ff9d085c21949e@mail.gmail.com>
	<971536df0707040945i262607fdk1fa7c33a11ceac7a@mail.gmail.com>
Message-ID: <971536df0707040949q21750e52sa27004d23746bf55@mail.gmail.com>

This was in error since s3 was not set.  The as.numeric in the calculation
of s3 can be omitted if its ok to have an integer rather than numeric result
and in that case its still faster yet.

> set.seed(1)
> C <- sample(c("a", "b"), 1000000, replace = TRUE)
> system.time({
+ s0 <- vector(length = length(C))
+ for(i in seq_along(C)) s0[i] <- if (C[i] == "a") 1 else -1
+ s0
+ })
   user  system elapsed
  21.32    0.02   26.10
> system.time(s1 <- ifelse(C == "a", 1, -1))
   user  system elapsed
   2.37    0.26    2.64
> system.time(s2 <- 2 * (C == "a") - 1)
   user  system elapsed
   0.32    0.02    0.35
> system.time({tmp <- C == "a"; s3 <- as.numeric(tmp - !tmp)})
   user  system elapsed
   0.28    0.02    0.31
> identical(s0, s1)
[1] TRUE
> identical(s0, s2)
[1] TRUE
> identical(s0, s3)
[1] TRUE
>


On 7/4/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> In thinking about this a bit more I have found a slightly faster one still.
> See s3.  Also I have added s0, the original solution, to the timings.
>
> > set.seed(1)
> > C <- sample(c("a", "b"), 1000000, replace = TRUE)
> > system.time({
> + s0 <- vector(length = length(C))
> + for(i in seq_along(C)) s0[i] <- if (C[i] == "a") 1 else -1
> + s0
> + })
>   user  system elapsed
>  21.75    0.02   25.99
> > system.time(s1 <- ifelse(C == "a", 1, -1))
>   user  system elapsed
>   2.32    0.17    2.54
> > system.time(s2 <- 2 * (C == "a") - 1)
>   user  system elapsed
>   0.29    0.02    0.32
> > system.time({tmp <- C == "a"; tmp - !tmp})
>   user  system elapsed
>   0.21    0.00    0.21
> > identical(s0, s1)
> [1] TRUE
> > identical(s0, s2)
> [1] TRUE
> > identical(s0, s3)
> [1] TRUE
>
> On 7/4/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > Here are two ways.  The second way is more than 10x faster.
> >
> > > set.seed(1)
> > > C <- sample(c("a", "b"), 100000, replace = TRUE)
> > > system.time(s1 <- ifelse(C == "a", 1, -1))
> >   user  system elapsed
> >   0.37    0.01    0.38
> > > system.time(s2 <- 2 * (C == "a") - 1)
> >   user  system elapsed
> >   0.02    0.00    0.02
> > > identical(s1, s2)
> > [1] TRUE
> >
> > On 7/4/07, Keith Alan Chamberlain <Keith.Chamberlain at colorado.edu> wrote:
> > > Dear Rhelpers,
> > >
> > > Is there a faster way than below to set a vector based on values from
> > > another vector? I'd like to call a pre-existing function for this, but one
> > > which can also handle an arbitrarily large number of categories. Any ideas?
> > >
> > > Cat=c('a','a','a','b','b','b','a','a','b')      # Categorical variable
> > > C1=vector(length=length(Cat))   # New vector for numeric values
> > >
> > > # Cycle through each column and set C1 to corresponding value of Cat.
> > > for(i in 1:length(C1)){
> > >        if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
> > > }
> > >
> > > C1
> > > [1] -1 -1 -1  1  1  1 -1 -1  1
> > > Cat
> > > [1] "a" "a" "a" "b" "b" "b" "a" "a" "b"
> > >
> > > Sincerely,
> > > KeithC.
> > > Psych Undergrad, CU Boulder (US)
> > > RE McNair Scholar
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>


From gmoyeyemi at yahoo.com  Wed Jul  4 19:18:33 2007
From: gmoyeyemi at yahoo.com (Yemi Oyeyemi)
Date: Wed, 4 Jul 2007 10:18:33 -0700 (PDT)
Subject: [R] Working with matrix
Message-ID: <350908.65517.qm@web63904.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/86f6b16d/attachment.pl 

From ocelma at iua.upf.edu  Wed Jul  4 19:25:45 2007
From: ocelma at iua.upf.edu (ocelma at iua.upf.edu)
Date: Wed, 4 Jul 2007 19:25:45 +0200 (CEST)
Subject: [R] Long-tail model in R ... anyone?
Message-ID: <32298.148.204.211.251.1183569945.squirrel@iua-mail.upf.es>

Dear all,

first I would like to tell you that I've been using R for two days... (so,
you can predict my knowledge of the language!).

Yet, I managed to implement some stuff related with the Long-Tail model [1].
I did some tests with the data in table 1 (from [1]), and plotted figure 2
(from [1]). (See R code and CSV file at the end of the email)

Now, I'm stuck in the nonlinear regression model of F(x). I got a nice error:
"
Error in nls(~F(r, N50, beta, alfa), data = dataset, start = list(N50 =
N50,  : singular gradient
"

And, yes, I've been looking for how to solve this (via this mailing list +
some google), and I could not come across to a proper solution. That's why
I am asking the experts to help me! :-)

So, any help would be much appreciated...

Cheers, Oscar
[1] http://www.firstmonday.org/issues/issue12_5/kilkki/

PS: R code and CVS file

FILE: "data.R" (data taken from [1] Table 1, columns 1 and 2)
--8=<-------------------
"rank","cum_value"
10,     17396510
32,     31194809
96,     53447300
420,    100379331
1187,   152238166
24234,  432238757
91242,  581332371
294180, 650880870
1242185,665227287
-->=8-------------------

R CODE:

#
# F(x). The long-tail model
# Reference: http://www.firstmonday.org/issues/issue12_5/kilkki/
# Params:
#       x   :   Rank (either an integer or a list)
#       N50 :   the number of objects that cover half of the whole volume
#       beta:   total volume
#       alfa:   the factor that defines the form of the function
F <- function (x, N50, beta=1.0, alfa=0.49)
{
        xx <- as.numeric(x) # as.numeric() prevents overflow
        Fx = beta / ( (N50/xx)^alfa + 1 )
        Fx
}

# Read CSV file (rank, cum_value)
lt <- read.csv(file="data.R",head=TRUE,sep=",")

r <- lt$rank
v <- lt$cum_value
pcnt <- v/v[length(v)] *100 # get cumulative percentage
plot(r, pcnt, log="x", type='l', xlab='Ranking', ylab='Cumulative
percentatge of sales', main="Books Popularity", sub="The long-tail
effect", col='blue')

# Set some default values to be used by F(x)...
alfa = 0.49
beta = 1.38
N50 = 30714

# Start using F(x). Results are in 'f' ...
f <- c(0) # oops! is this the best initialization for 'f'?
for (i in 1:24234) f[i] <- F(i, N50, beta, alfa)*100

# Plot some estimated values from F(x) (N50, beta, and alfa values come
from the paper. See ref. [1])
plot(f, log="x", type='l', xlab='Ranking', ylab='Cumulative percentatge of
sales', main="Books Popularity", sub="Plotting first values of F(x) and
some real points")
points(r, pcnt, col="blue") # adding the "real" points

# Create a dataset to be used by nls()
dataset <- data.frame(r, pcnt)

# Verifying that F(x) works fine... (comparing with the "real" values
contained in the dataset)

dataset
F(10, N50, beta, alfa) * 100
F(32, N50, beta, alfa) * 100
F(96, N50, beta, alfa) * 100
F(420, N50, beta, alfa) * 100
F(1187, N50, beta, alfa) * 100
F(24234, N50, beta, alfa) * 100
F(91242, N50, beta, alfa) * 100
F(294180, N50, beta, alfa) * 100
F(1242185, N50, beta, alfa) * 100

#dataset <- data.frame(pcnt) # which dataset should I use? Should I
include the ranks in it?
nls( ~ F(r, N50, beta, alfa), data = dataset, start = list(N50=N50,
beta=beta, alfa=alfa), trace = TRUE )


From pinard at iro.umontreal.ca  Wed Jul  4 19:51:56 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Wed, 4 Jul 2007 13:51:56 -0400
Subject: [R] A More efficient method?
In-Reply-To: <000101c7be41$7b2a4d30$6501a8c0@kLab>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
Message-ID: <20070704175156.GA9154@alcyon.progiciels-bpi.ca>

[Keith Alan Chamberlain]

>Is there a faster way than below to set a vector based on values
>from another vector? I'd like to call a pre-existing function for
>this, but one which can also handle an arbitrarily large number of
>categories. Any ideas?

>Cat=c('a','a','a','b','b','b','a','a','b')	# Categorical variable
>C1=vector(length=length(Cat))	# New vector for numeric values

># Cycle through each column and set C1 to corresponding value of Cat.
>for(i in 1:length(C1)){
>	if(Cat[i]=='a') C1[i]=-1 else C1[i]=1
>}

>C1
>[1] -1 -1 -1  1  1  1 -1 -1  1
>Cat
>[1] "a" "a" "a" "b" "b" "b" "a" "a" "b"

For handling an arbitrarily large number of categories, one may go
through a recoding vector, like this for the example above:

> Cat <- c('a', 'a', 'a', 'b', 'b', 'b', 'a', 'a', 'b')
> C1 <- c(a=-1, b=1)[Cat]
> C1
 a  a  a  b  b  b  a  a  b
-1 -1 -1  1  1  1 -1 -1  1

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From rvaradhan at jhmi.edu  Wed Jul  4 20:18:56 2007
From: rvaradhan at jhmi.edu (RAVI VARADHAN)
Date: Wed, 04 Jul 2007 14:18:56 -0400
Subject: [R] how to solve a min problem
In-Reply-To: <e591a95b0707040821u991b9d1p63e5dbb17df7570a@mail.gmail.com>
References: <e591a95b0707020748p2650666bs972a7f28c8001a31@mail.gmail.com>
	<468A9E36.3030307@pdf.com>
	<e591a95b0707040821u991b9d1p63e5dbb17df7570a@mail.gmail.com>
Message-ID: <f579fbffd76.468bac50@johnshopkins.edu>

Whether you can use "optim" or not depends on the nature of the constraints on S.  If you have simple box constraints, you can use the "L-BFGS-B" method in optim.  If not, optim may not be directly applicable, unless you can somehow transform your problem into an unconstrained minimization problem. 

Ravi.

----- Original Message -----
From: domenico pestalozzi <statadat at gmail.com>
Date: Wednesday, July 4, 2007 11:26 am
Subject: Re: [R] how to solve a min problem
To: R-help <r-help at stat.math.ethz.ch>


> S is an array 1-dimensional, for example 1 X 10, and mean(S) is the 
> mean of
>  these 10 elements.
>  
>  So, I want to do:
>  
>  minimize mean(S) with 0 < b_func(S) < 800.
>  That is, there are some boundaries on S according the b_funct
>  
>  The function apply an iterative convergent criterion:
>  
>  f_1=g(S), f_2=g(f_1), f_3=g(f_2), ecc....
>  The function stops when
>  f_n - f_n-1 <=0.1e-09
>  and g(S) is a non-linear function of S and the convergence is mathematically
>  assured.
>  
>  Is it possible to use  'optimize'?
>  
>  thanks
>  
>  domenico
>  
>  
>  2007/7/3, Spencer Graves <spencer.graves at pdf.com>:
>  >
>  >      Do you mean
>  >
>  >      minimize mu with 0 < b_func(S+mu) < 800?
>  >
>  >      For this kind of problem, I'd first want to know the nature of
>  > "b_func".  Without knowing more, I might try to plot b_func(S+mu) vs.
>  > mu, then maybe use 'optimize'.
>  >
>  >      If this is not what you mean, please be more specific:  I'm
>  > confused.
>  >
>  >      Hope this helps.
>  >      Spencer Graves
>  >
>  > domenico pestalozzi wrote:
>  > > I know it's possible to solve max e min problems  by using these
>  > functions:
>  > >
>  > > nlm, optimize, optim
>  > >
>  > > but I don't know how to use them (...if possible...) to solve this
>  > problem.
>  > >
>  > > I have a personal function called  b_func(S) where S is an input 
> array
>  > (1 X
>  > > n)  and I'd like:
>  > >
>  > > minimize mean(S) with 0 < b_funct < 800.
>  > >
>  > > I know that the solution exists, but It's possible to calculate 
> it in R?
>  > > The b_func is non linear and it calculates a particular value 
> using S as
>  > > input and applying a convergent iterative algorithm.
>  > >
>  > > thanks
>  > >
>  > >
>  > > domenico
>  > >
>  > >       [[alternative HTML version deleted]]
>  > >
>  > > ______________________________________________
>  > > R-help at stat.math.ethz.ch mailing list
>  > > 
>  > > PLEASE do read the posting guide
>  > 
>  > > and provide commented, minimal, self-contained, reproducible code.
>  > >
>  >
>  
>  	[[alternative HTML version deleted]]
>  
>  ______________________________________________
>  R-help at stat.math.ethz.ch mailing list
>  
>  PLEASE do read the posting guide 
>  and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Wed Jul  4 20:19:04 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 4 Jul 2007 14:19:04 -0400
Subject: [R] A More efficient method?
In-Reply-To: <468BC4CF.9090002@gmx.net>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
	<971536df0707040829w55bc833y47ff9d085c21949e@mail.gmail.com>
	<468BC4CF.9090002@gmx.net>
Message-ID: <644e1f320707041119s4a8ab682x5e41884e66fd7120@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/6d0f3b7c/attachment.pl 

From jholtman at gmail.com  Wed Jul  4 20:20:28 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 4 Jul 2007 14:20:28 -0400
Subject: [R] A More efficient method?
In-Reply-To: <644e1f320707041119s4a8ab682x5e41884e66fd7120@mail.gmail.com>
References: <mailman.11.1183543206.28203.r-help@stat.math.ethz.ch>
	<000101c7be41$7b2a4d30$6501a8c0@kLab>
	<971536df0707040829w55bc833y47ff9d085c21949e@mail.gmail.com>
	<468BC4CF.9090002@gmx.net>
	<644e1f320707041119s4a8ab682x5e41884e66fd7120@mail.gmail.com>
Message-ID: <644e1f320707041120g3aa644d9tfbb15557c844d0f5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/14b8f2f3/attachment.pl 

From rvaradhan at jhmi.edu  Wed Jul  4 20:21:26 2007
From: rvaradhan at jhmi.edu (RAVI VARADHAN)
Date: Wed, 04 Jul 2007 14:21:26 -0400
Subject: [R] how to solve a min problem
In-Reply-To: <e591a95b0707040821u991b9d1p63e5dbb17df7570a@mail.gmail.com>
References: <e591a95b0707020748p2650666bs972a7f28c8001a31@mail.gmail.com>
	<468A9E36.3030307@pdf.com>
	<e591a95b0707040821u991b9d1p63e5dbb17df7570a@mail.gmail.com>
Message-ID: <f494c671327a.468bace6@johnshopkins.edu>

If the constraints on S are linear inequalities, then linear programming methods would work.  See function solveLP in package "linprog" or simplex in "boot" or package "lpSolve".

Ravi.

----- Original Message -----
From: domenico pestalozzi <statadat at gmail.com>
Date: Wednesday, July 4, 2007 11:26 am
Subject: Re: [R] how to solve a min problem
To: R-help <r-help at stat.math.ethz.ch>


> S is an array 1-dimensional, for example 1 X 10, and mean(S) is the 
> mean of
>  these 10 elements.
>  
>  So, I want to do:
>  
>  minimize mean(S) with 0 < b_func(S) < 800.
>  That is, there are some boundaries on S according the b_funct
>  
>  The function apply an iterative convergent criterion:
>  
>  f_1=g(S), f_2=g(f_1), f_3=g(f_2), ecc....
>  The function stops when
>  f_n - f_n-1 <=0.1e-09
>  and g(S) is a non-linear function of S and the convergence is mathematically
>  assured.
>  
>  Is it possible to use  'optimize'?
>  
>  thanks
>  
>  domenico
>  
>  
>  2007/7/3, Spencer Graves <spencer.graves at pdf.com>:
>  >
>  >      Do you mean
>  >
>  >      minimize mu with 0 < b_func(S+mu) < 800?
>  >
>  >      For this kind of problem, I'd first want to know the nature of
>  > "b_func".  Without knowing more, I might try to plot b_func(S+mu) vs.
>  > mu, then maybe use 'optimize'.
>  >
>  >      If this is not what you mean, please be more specific:  I'm
>  > confused.
>  >
>  >      Hope this helps.
>  >      Spencer Graves
>  >
>  > domenico pestalozzi wrote:
>  > > I know it's possible to solve max e min problems  by using these
>  > functions:
>  > >
>  > > nlm, optimize, optim
>  > >
>  > > but I don't know how to use them (...if possible...) to solve this
>  > problem.
>  > >
>  > > I have a personal function called  b_func(S) where S is an input 
> array
>  > (1 X
>  > > n)  and I'd like:
>  > >
>  > > minimize mean(S) with 0 < b_funct < 800.
>  > >
>  > > I know that the solution exists, but It's possible to calculate 
> it in R?
>  > > The b_func is non linear and it calculates a particular value 
> using S as
>  > > input and applying a convergent iterative algorithm.
>  > >
>  > > thanks
>  > >
>  > >
>  > > domenico
>  > >
>  > >       [[alternative HTML version deleted]]
>  > >
>  > > ______________________________________________
>  > > R-help at stat.math.ethz.ch mailing list
>  > > 
>  > > PLEASE do read the posting guide
>  > 
>  > > and provide commented, minimal, self-contained, reproducible code.
>  > >
>  >
>  
>  	[[alternative HTML version deleted]]
>  
>  ______________________________________________
>  R-help at stat.math.ethz.ch mailing list
>  
>  PLEASE do read the posting guide 
>  and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Wed Jul  4 20:31:26 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 4 Jul 2007 14:31:26 -0400
Subject: [R] read items
In-Reply-To: <184782.44197.qm@web27504.mail.ukl.yahoo.com>
References: <184782.44197.qm@web27504.mail.ukl.yahoo.com>
Message-ID: <644e1f320707041131v41604ed3k2684cc6529325cdf@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/4ab115e3/attachment.pl 

From ymoisan at groupesm.com  Wed Jul  4 20:58:15 2007
From: ymoisan at groupesm.com (Moisan Yves)
Date: Wed, 4 Jul 2007 14:58:15 -0400
Subject: [R] Kmeans performance difference
Message-ID: <8FAF06163E22434EACB677AD054EF89101FDD94D@sm-exchange.groupesm.com>

Hi All,

A question from a newbie using R 2-5-0 on windows XP.  Why is it that
kmeans clustering with apparently the exact same parameters behaves so
differently between the two following examples :

> cl1 <- kmeans(subset(pointsUXO15555, select = c(2:4)), 10)

Takes about 2 seconds to deliver a result

> cl1 <- clust(subset(pointsUXO15555, select = c(2:4)), k=10,
method="kmeansHartigan") 

Dies after about 10 minutes and fills up RAM :   

*** running kmeansHartigan cluster algorithm...

 *** calculating validity measure... 
Erreur : impossible d'allouer un vecteur de taille 922.9 Mo
De plus : Warning messages:
1: Reached total allocation of 1023Mb: see help(memory.size) 
2: Reached total allocation of 1023Mb: see help(memory.size) 
3: Reached total allocation of 1023Mb: see help(memory.size) 
4: Reached total allocation of 1023Mb: see help(memory.size)

If I understand correctly, both methods should give the sameish results
(modulo the initial random locations) since the default in kmeans is
"Hartigan-Wong".  My data frame is 3 columns X 15555 lines.  It must be
that kmeans is more a "core" R function whereas clust id from the
clustTool package, but isn't clustTool simply wrapping the core kmeans
method ?  Why such a difference ?

TIA,

Yves Moisan


From michael at frumin.net  Wed Jul  4 21:01:41 2007
From: michael at frumin.net (mfrumin)
Date: Wed, 4 Jul 2007 12:01:41 -0700 (PDT)
Subject: [R] Lookups in R
Message-ID: <11435994.post@talk.nabble.com>


Hey all; I'm a beginner++ user of R, trying to use it to do some processing
of data sets of over 1M rows, and running into a snafu.  imagine that my
input is a huge table of transactions, each linked to a specif user id.  as
I run through the transactions, I need to update a separate table for the
users, but I am finding that the traditional ways of doing a table lookup
are way too slow to support this kind of operation.

i.e:

for(i in 1:1000000) {
   userid = transactions$userid[i];
   amt = transactions$amounts[i];
   users[users$id == userid,'amt'] += amt;
}

I assume this is a linear lookup through the users table (in which there are
10's of thousands of rows), when really what I need is O(constant time), or
at worst O(log(# users)).

is there any way to manage a list of ID's (be they numeric, string, etc) and
have them efficiently mapped to some other table index?

I see the CRAN package for SQLite hashes, but that seems to be going a bit
too far.

thanks,
Mike

Intern, Oyster Card Group, Transport for London
(feel free to email back to this address, I'm posting through NAbble so I
hope it works).
-- 
View this message in context: http://www.nabble.com/Lookups-in-R-tf4026062.html#a11435994
Sent from the R help mailing list archive at Nabble.com.


From edd at debian.org  Wed Jul  4 21:15:49 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 4 Jul 2007 14:15:49 -0500
Subject: [R] Long-tail model in R ... anyone?
In-Reply-To: <32298.148.204.211.251.1183569945.squirrel@iua-mail.upf.es>
References: <32298.148.204.211.251.1183569945.squirrel@iua-mail.upf.es>
Message-ID: <18059.61925.16253.545535@basebud.nulle.part>


I think you simply had your nls() syntax wrong.  Works here:


## first a neat trick to read the data from embedded text
> fmdata <- read.csv(textConnection("
+ rank,cum_value
10,     17396510
32,     31194809
96,     53447300
420,    100379331
1187,   152238166
24234,  432238757
91242,  581332371
294180, 650880870
1242185,665227287"))
> 


## then compute cumulative share
> fmdata[,"cumshare"] <- fmdata[,"cum_value"] / fmdata[nrow(fmdata),"cum_value"]
> 


## then check the data, just in case
> summary(fmdata)
      rank           cum_value            cumshare      
 Min.   :     10   Min.   : 17396510   Min.   :0.02615  
 1st Qu.:     96   1st Qu.: 53447300   1st Qu.:0.08034  
 Median :   1187   Median :152238166   Median :0.22885  
 Mean   : 183732   Mean   :298259489   Mean   :0.44836  
 3rd Qu.:  91242   3rd Qu.:581332371   3rd Qu.:0.87389  
 Max.   :1242185   Max.   :665227287   Max.   :1.00000  
> 

## finally estimate the model, using only the first seven rows of data
## using the parametric form from the paper and some wild guesses as
## starting values:
> fit <- nls(cumshare ~ Beta / ((N50 / rank)^Alpha + 1), data=fmdata[1:7,], start=list(Alpha=1, Beta=1, N50=1e4))
> summary(fit)

Formula: cumshare ~ Beta/((N50/rank)^Alpha + 1)

Parameters:
       Estimate Std. Error t value Pr(>|t|)    
Alpha 4.829e-01  5.374e-03   89.86 9.20e-08 ***
Beta  1.429e+00  2.745e-02   52.07 8.14e-07 ***
N50   3.560e+04  3.045e+03   11.69 0.000306 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.002193 on 4 degrees of freedom

Number of iterations to convergence: 8 
Achieved convergence tolerance: 1.297e-06 

> 

which is reasonably close to the quoted 
	N50 = 30714, ? = 0.49, and ? = 1.38.

You can probably play a little with the nls options to see what effect this
has. 

That said, seven observations for three parameters in non-linear model may be
a little hazardous.  One indication is that the estimated parameters values
are not too stable once you add the eights and nineth row of data.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From p.dalgaard at biostat.ku.dk  Wed Jul  4 21:47:36 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 04 Jul 2007 21:47:36 +0200
Subject: [R] Lookups in R
In-Reply-To: <11435994.post@talk.nabble.com>
References: <11435994.post@talk.nabble.com>
Message-ID: <468BF958.9060102@biostat.ku.dk>

mfrumin wrote:
> Hey all; I'm a beginner++ user of R, trying to use it to do some processing
> of data sets of over 1M rows, and running into a snafu.  imagine that my
> input is a huge table of transactions, each linked to a specif user id.  as
> I run through the transactions, I need to update a separate table for the
> users, but I am finding that the traditional ways of doing a table lookup
> are way too slow to support this kind of operation.
>
> i.e:
>
> for(i in 1:1000000) {
>    userid = transactions$userid[i];
>    amt = transactions$amounts[i];
>    users[users$id == userid,'amt'] += amt;
> }
>
> I assume this is a linear lookup through the users table (in which there are
> 10's of thousands of rows), when really what I need is O(constant time), or
> at worst O(log(# users)).
>
> is there any way to manage a list of ID's (be they numeric, string, etc) and
> have them efficiently mapped to some other table index?
>
> I see the CRAN package for SQLite hashes, but that seems to be going a bit
> too far.
>   
Sometimes you need a bit of lateral thinking. I suspect that you could 
do it like this:

tbl <- with(transactions, tapply(amount, userid, sum))
users$amt <- users$amt + tbl[users$id]

one catch is that there could be users with no transactions, in which 
case you may need to replace userid by factor(userid, levels=users$id). 
None of this is tested, of course.


From attenka at utu.fi  Wed Jul  4 22:59:37 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Wed, 04 Jul 2007 23:59:37 +0300
Subject: [R] New function combinations using tree structures?
Message-ID: <f6e6c92cbce8.468c3469@utu.fi>

Hi everybody,

I'm still interesting the possibility to use R for genetic programming (see https://stat.ethz.ch/pipermail/r-help/2007-April/128782.html)
and I'd like to know, how to express for instance this kind of functions (x^2+3x+1 etc, see the picture) using some kind of tree structures and what are needed to make similar recombination operations with them as seen in the picture:

http://users.utu.fi/attenka/Kuva1.png

I know the basic principles of mutation and mating etc, as far as genetic algorithms are concerned, but now I'd like to know the practical issues considering R and mating of these functions using "trees".

All suggestions are warmly appreciated.

With best wishes,

Atte


From thomas.harte at yahoo.com  Wed Jul  4 23:32:33 2007
From: thomas.harte at yahoo.com (Thomas Harte)
Date: Wed, 4 Jul 2007 14:32:33 -0700 (PDT)
Subject: [R] How to install R 2.5 with Synaptic in Ubuntu?
Message-ID: <848601.88486.qm@web43133.mail.sp1.yahoo.com>

mike,

try installing directly using apt-get instead of Synaptic. 

in my /etc/apt/sources.list i added the line:

	deb http://cran.R-project.org/bin/linux/ubuntu/ dapper/ 

and then i did:

	bash$ sudo apt-get install r-base r-doc-info r-doc-pdf r-doc-html r-mathlib  r-base-html
r-base-latex r-base-dev r-gnome

recently to update to R 2.5.1 on my version of Ubuntu (6.06).

cheers,

thomas.


> Message: 98
> Date: Wed, 4 Jul 2007 02:34:37 -0700 (PDT)
> From: msmith <msmith at ebi.ac.uk>
> Subject: Re: [R] How to install R 2.5 with Synaptic in Ubuntu?
> To: r-help at stat.math.ethz.ch
> Message-ID: <11427837.post at talk.nabble.com>
> Content-Type: text/plain; charset=us-ascii
> 
> 
> Hi,
> 
> Thanks for the suggestion and I wish the solution was that obvious, but I
> have changed it to really point at my favourite mirror.
> 
> Using your example Synaptic reports the following error when I try to update
> the repositories:
> 
>
http://www.stats.bris.ac.uk/R/bin/linux/ubuntu/dists/feisty/main/binary-i386/Packages.gz:
> 404 Not Found
> 
> This is understandable since that location doesn't exist, but it makes me
> think that the directory structure of the R mirrors is not compatible with
> Ubuntu and Synaptic, since it automatically seeks /dists/feisty/ rather than
> just /feisty/ as it is on the CRAN mirrors.
> 
> Thanks again
> Mike Smith
> 
> 
> Stefan Grosse-2 wrote:
> > 
> > 
> >>  to end of the entry making it:
> >>
> >>  deb http://my.favorite.cran.mirror/bin/linux/ubuntu feisty main
> >>
> >> However after this it still complains that it can't find "packages.gz"
> >>
> >>   
> > 
> > Just a guess: have you replaced the my.favorite.cran.mirror by a mirror
> > which is close to you? If you're in UK it would be for example
> > 
> > deb http://www.stats.bris.ac.uk/R/bin/linux/ubuntu feisty main
> > 
> > ;o)
> > Stefan
> > 
> >> It appears to be looking in 
> >> http://my.favorite.cran.mirror/bin/linux/ubuntu/distsfeisty
> >> which isn't the directory structure of the cran repository, but 
> >> I can see anyway to modify this behaviour.  Every other Ubuntu repositoy
> >> I have looked at contains the dists directory.
> >>
> >> Any suggestions for modifying this behaviour are gratefully recieved.
> >> Many thanks
> >>
> >> Mike Smith
> >>
> >>
> >>   
> > 
> > 
> > 
> > 
> > -=-=-
> > ... The simple truth is that interstellar distances will not fit into
> > the human imagination - (The Hitchhiker's Guide to the Galaxy)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
> -- 
> View this message in context:
> 
http://www.nabble.com/How-to-install-R-2.5-with-Synaptic-in-Ubuntu--tf3998481.html#a11427837
> Sent from the R help mailing list archive at Nabble.com.
>


From michael at frumin.net  Wed Jul  4 23:40:19 2007
From: michael at frumin.net (Michael Frumin)
Date: Wed, 04 Jul 2007 22:40:19 +0100
Subject: [R] Lookups in R
In-Reply-To: <468BF958.9060102@biostat.ku.dk>
References: <11435994.post@talk.nabble.com> <468BF958.9060102@biostat.ku.dk>
Message-ID: <468C13C3.7070701@frumin.net>

i wish it were that simple.  unfortunately the logic i have to do on 
each transaction is substantially more complicated, and involves 
referencing the existing values of the user table through a number of 
conditions.

any other thoughts on how to get better-than-linear performance time?  
is there a recommended binary searching/sorting (i.e. BTree) module that 
I could use to maintain my own index?

thanks,
mike

Peter Dalgaard wrote:
> mfrumin wrote:
>> Hey all; I'm a beginner++ user of R, trying to use it to do some 
>> processing
>> of data sets of over 1M rows, and running into a snafu.  imagine that my
>> input is a huge table of transactions, each linked to a specif user 
>> id.  as
>> I run through the transactions, I need to update a separate table for 
>> the
>> users, but I am finding that the traditional ways of doing a table 
>> lookup
>> are way too slow to support this kind of operation.
>>
>> i.e:
>>
>> for(i in 1:1000000) {
>>    userid = transactions$userid[i];
>>    amt = transactions$amounts[i];
>>    users[users$id == userid,'amt'] += amt;
>> }
>>
>> I assume this is a linear lookup through the users table (in which 
>> there are
>> 10's of thousands of rows), when really what I need is O(constant 
>> time), or
>> at worst O(log(# users)).
>>
>> is there any way to manage a list of ID's (be they numeric, string, 
>> etc) and
>> have them efficiently mapped to some other table index?
>>
>> I see the CRAN package for SQLite hashes, but that seems to be going 
>> a bit
>> too far.
>>   
> Sometimes you need a bit of lateral thinking. I suspect that you could 
> do it like this:
>
> tbl <- with(transactions, tapply(amount, userid, sum))
> users$amt <- users$amt + tbl[users$id]
>
> one catch is that there could be users with no transactions, in which 
> case you may need to replace userid by factor(userid, 
> levels=users$id). None of this is tested, of course.


From mtmorgan at fhcrc.org  Wed Jul  4 23:54:30 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Wed, 04 Jul 2007 14:54:30 -0700
Subject: [R] Lookups in R
In-Reply-To: <468BF958.9060102@biostat.ku.dk> (Peter Dalgaard's message of
	"Wed, 04 Jul 2007 21:47:36 +0200")
References: <11435994.post@talk.nabble.com> <468BF958.9060102@biostat.ku.dk>
Message-ID: <6phy7hv3kdl.fsf@gopher4.fhcrc.org>

Michael,

A hash provides constant-time access, though the resulting perl-esque
data structures (a hash of lists, e.g.) are not convenient for other
manipulations

> n_accts <- 10^3
> n_trans <- 10^4
> t <- list()
> t$amt <- runif(n_trans)
> t$acct <- as.character(round(runif(n_trans, 1, n_accts)))
> 
> uhash <- new.env(hash=TRUE, parent=emptyenv(), size=n_accts)
> ## keys, presumably account ids
> for (acct in as.character(1:n_accts)) uhash[[acct]] <- list(amt=0, n=0)
> 
> system.time(for (i in seq_along(t$amt)) {
+     acct <- t$acct[i]
+     x <- uhash[[acct]]
+     uhash[[acct]] <- list(amt=x$amt + t$amt[i], n=x$n + 1)
+ })
   user  system elapsed 
  0.264   0.000   0.262 
> udf <- data.frame(amt=0, n=rep(0L, n_accts),
+                   row.names=as.character(1:n_accts))
> system.time(for (i in seq_along(t$amt)) {
+     idx <- row.names(udf)==t$acct[i]
+     udf[idx, ] <- c(udf[idx,"amt"], udf[idx, "n"]) + c(t$amt[i], 1)
+ })
   user  system elapsed 
 18.398   0.000  18.394 

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> mfrumin wrote:
>> Hey all; I'm a beginner++ user of R, trying to use it to do some processing
>> of data sets of over 1M rows, and running into a snafu.  imagine that my
>> input is a huge table of transactions, each linked to a specif user id.  as
>> I run through the transactions, I need to update a separate table for the
>> users, but I am finding that the traditional ways of doing a table lookup
>> are way too slow to support this kind of operation.
>>
>> i.e:
>>
>> for(i in 1:1000000) {
>>    userid = transactions$userid[i];
>>    amt = transactions$amounts[i];
>>    users[users$id == userid,'amt'] += amt;
>> }
>>
>> I assume this is a linear lookup through the users table (in which there are
>> 10's of thousands of rows), when really what I need is O(constant time), or
>> at worst O(log(# users)).
>>
>> is there any way to manage a list of ID's (be they numeric, string, etc) and
>> have them efficiently mapped to some other table index?
>>
>> I see the CRAN package for SQLite hashes, but that seems to be going a bit
>> too far.
>>   
> Sometimes you need a bit of lateral thinking. I suspect that you could 
> do it like this:
>
> tbl <- with(transactions, tapply(amount, userid, sum))
> users$amt <- users$amt + tbl[users$id]
>
> one catch is that there could be users with no transactions, in which 
> case you may need to replace userid by factor(userid, levels=users$id). 
> None of this is tested, of course.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Bioconductor / Computational Biology
http://bioconductor.org


From michael at frumin.net  Wed Jul  4 23:58:09 2007
From: michael at frumin.net (Michael Frumin)
Date: Wed, 04 Jul 2007 22:58:09 +0100
Subject: [R] Lookups in R
In-Reply-To: <6phy7hv3kdl.fsf@gopher4.fhcrc.org>
References: <11435994.post@talk.nabble.com> <468BF958.9060102@biostat.ku.dk>
	<6phy7hv3kdl.fsf@gopher4.fhcrc.org>
Message-ID: <468C17F1.6040607@frumin.net>


From spencer.graves at pdf.com  Thu Jul  5 00:15:50 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 04 Jul 2007 15:15:50 -0700
Subject: [R] Empirical copula in R
In-Reply-To: <11412335.post@talk.nabble.com>
References: <11412335.post@talk.nabble.com>
Message-ID: <468C1C16.5040905@pdf.com>

      I just got 203 hits from RSiteSearch("copula") and 60 from 
RSiteSearch("copula", "fun").  Most if not all of the first 24 or the 
hits in the latter referred to the 'copula' package.  Have you reviewed 
these?  The 25th hit in the latter referred to an 'fgac' package for 
'Generalized Archimedean Copula', with a Brazilian author and 
maintainer, who presumably is not related to the 'copula' author and 
maintainer at U. Iowa. 

      Also, have you tried contacting the official "maintainers" of 
these packages?  You can get an email address for them from 
help(package="copula") and help (package="fgac"). 

      Hope this helps. 
      Spencer Graves

GWeiss wrote:
> Hi,
>
> I would like to implement the empirical copula in R, does anyone know if it
> is included in a package? I know it is not in the "Copula" package. This one
> only includes a gof-test based on the empirical copula process.
>
> Thanks for your help!
> Gregor
>


From edna.bell01 at gmail.com  Thu Jul  5 00:43:31 2007
From: edna.bell01 at gmail.com (Edna Bell)
Date: Wed, 4 Jul 2007 17:43:31 -0500
Subject: [R]  Newbie creating package with compiled code
Message-ID: <2d1ebb110707041543v2148c2c5nb58d55e8bb51a656@mail.gmail.com>

Hi R Gurus!

I'm trying to create a test package using the package.skeleton function.
I wanted to  add some compiled code too.
In the src library, I put together a baby subroutine, compiled it and created
a test.dll

When I use the R cmd build, it works fine.  But I get into trouble
with the R CMD check section.


/home/Desktop/R-2.5.1/bin # ./R CMD check mypkg
* checking for working latex ... OK
* using log directory '/home/Desktop/R-2.5.1/bin/mypkg.Rcheck'
* using R version 2.5.1 (2007-06-27)
* checking for file 'mypkg/DESCRIPTION' ... OK
* checking extension type ... Package
* this is package 'mypkg' version '1.0'
* checking package dependencies ... OK
* checking if this is a source package ... OK
* checking whether package 'mypkg' can be installed ... OK
* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking top-level files ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for non-ASCII characters ... OK
* checking R files for syntax errors ... OK
* checking whether the package can be loaded ... ERROR
Error in .find.package(package, lib.loc, verbose = verbose) :
        there are no packages called 'mypkg', 'stats', 'graphics',
'grDevices', 'utils', 'datasets', 'methods', 'base'
Error in library(mypkg) : .First.lib failed for 'mypkg'
Execution halted

It looks like this package has a loading problem: see the messages for
details.

Here is the mypkg.R file
sss <- "/home/hodgesse/Desktop/R-2.5.1"
.First.lib <- function(lib=sss,pkg="mypkg")
	   library.dynam("mypkg.so",pkg="mypkg",lib=sss)


f <- function(x,y) x+y

g <-function(x,y) x-y


Thanks for any help

Edna
mailto: edna.bell01 at gmail.com


From murdoch at stats.uwo.ca  Thu Jul  5 00:58:27 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 04 Jul 2007 18:58:27 -0400
Subject: [R] Newbie creating package with compiled code
In-Reply-To: <2d1ebb110707041543v2148c2c5nb58d55e8bb51a656@mail.gmail.com>
References: <2d1ebb110707041543v2148c2c5nb58d55e8bb51a656@mail.gmail.com>
Message-ID: <468C2613.1060308@stats.uwo.ca>

On 04/07/2007 6:43 PM, Edna Bell wrote:
> Hi R Gurus!
> 
> I'm trying to create a test package using the package.skeleton function.
> I wanted to  add some compiled code too.
> In the src library, I put together a baby subroutine, compiled it and created
> a test.dll
> 
> When I use the R cmd build, it works fine.  But I get into trouble
> with the R CMD check section.
> 
> 
> /home/Desktop/R-2.5.1/bin # ./R CMD check mypkg
> * checking for working latex ... OK
> * using log directory '/home/Desktop/R-2.5.1/bin/mypkg.Rcheck'
> * using R version 2.5.1 (2007-06-27)
> * checking for file 'mypkg/DESCRIPTION' ... OK
> * checking extension type ... Package
> * this is package 'mypkg' version '1.0'
> * checking package dependencies ... OK
> * checking if this is a source package ... OK
> * checking whether package 'mypkg' can be installed ... OK
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking for sufficient/correct file permissions ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking top-level files ... OK
> * checking index information ... OK
> * checking package subdirectories ... OK
> * checking R files for non-ASCII characters ... OK
> * checking R files for syntax errors ... OK
> * checking whether the package can be loaded ... ERROR
> Error in .find.package(package, lib.loc, verbose = verbose) :
>         there are no packages called 'mypkg', 'stats', 'graphics',
> 'grDevices', 'utils', 'datasets', 'methods', 'base'
> Error in library(mypkg) : .First.lib failed for 'mypkg'
> Execution halted
> 
> It looks like this package has a loading problem: see the messages for
> details.
> 
> Here is the mypkg.R file
> sss <- "/home/hodgesse/Desktop/R-2.5.1"
> .First.lib <- function(lib=sss,pkg="mypkg")
> 	   library.dynam("mypkg.so",pkg="mypkg",lib=sss)

That's a very strange .First.lib.  I think you'll have more success with 
a simpler one:

.First.lib <- function(libname, pkgname)
   library.dynam("mypkg", package=pkgname, lib.loc=libname)

(and sss is not needed at all).

Duncan Murdoch

> 
> 
> f <- function(x,y) x+y
> 
> g <-function(x,y) x-y
> 
> 
> Thanks for any help
> 
> Edna
> mailto: edna.bell01 at gmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Thu Jul  5 01:04:49 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 05 Jul 2007 01:04:49 +0200
Subject: [R] Lookups in R
In-Reply-To: <468C13C3.7070701@frumin.net>
References: <11435994.post@talk.nabble.com> <468BF958.9060102@biostat.ku.dk>
	<468C13C3.7070701@frumin.net>
Message-ID: <468C2791.4040505@biostat.ku.dk>

Michael Frumin wrote:
> i wish it were that simple.  unfortunately the logic i have to do on 
> each transaction is substantially more complicated, and involves 
> referencing the existing values of the user table through a number of 
> conditions.
>
> any other thoughts on how to get better-than-linear performance time?  
> is there a recommended binary searching/sorting (i.e. BTree) module that 
> I could use to maintain my own index?
>   
The point remains: To do anything efficient in R, you need to get rid of 
that for loop and use something vectorized. Notice that you can expand 
values from the user table into the transaction table by indexing with 
transactions$userid, or you can use a merge operation.

> thanks,
> mike
>
> Peter Dalgaard wrote:
>   
>> mfrumin wrote:
>>     
>>> Hey all; I'm a beginner++ user of R, trying to use it to do some 
>>> processing
>>> of data sets of over 1M rows, and running into a snafu.  imagine that my
>>> input is a huge table of transactions, each linked to a specif user 
>>> id.  as
>>> I run through the transactions, I need to update a separate table for 
>>> the
>>> users, but I am finding that the traditional ways of doing a table 
>>> lookup
>>> are way too slow to support this kind of operation.
>>>
>>> i.e:
>>>
>>> for(i in 1:1000000) {
>>>    userid = transactions$userid[i];
>>>    amt = transactions$amounts[i];
>>>    users[users$id == userid,'amt'] += amt;
>>> }
>>>
>>> I assume this is a linear lookup through the users table (in which 
>>> there are
>>> 10's of thousands of rows), when really what I need is O(constant 
>>> time), or
>>> at worst O(log(# users)).
>>>
>>> is there any way to manage a list of ID's (be they numeric, string, 
>>> etc) and
>>> have them efficiently mapped to some other table index?
>>>
>>> I see the CRAN package for SQLite hashes, but that seems to be going 
>>> a bit
>>> too far.
>>>   
>>>       
>> Sometimes you need a bit of lateral thinking. I suspect that you could 
>> do it like this:
>>
>> tbl <- with(transactions, tapply(amount, userid, sum))
>> users$amt <- users$amt + tbl[users$id]
>>
>> one catch is that there could be users with no transactions, in which 
>> case you may need to replace userid by factor(userid, 
>> levels=users$id). None of this is tested, of course.
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Thu Jul  5 01:16:40 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 4 Jul 2007 16:16:40 -0700 (PDT)
Subject: [R] nls() lower/upper bound specification
Message-ID: <571264.78421.qm@web39710.mail.mud.yahoo.com>

Dear all,

In optim() all parameters of a function to be adjusted is stored in a single
vector, with lower/upper bounds can be specified by a vector of the same
length.

In nls(), is it true that if I want to specify lower/upper bounds, functions
must be re-written so that each parameter is contained in a single-valued
vector?

## data input
x <- 1:10
y <- 3*x+4*x^2+rnorm(10,250)

## this one does not work
f <- function(x)
  function(beta)
  beta[1]+ beta[2]*x+beta[3]*x^2

out <- nls(y~f(x)(beta),data=data.frame(x,y),
           alg="port",
           start=list(beta=1:3),
           lower=list(beta=rep(0,3)))

(However, this works if I do not specify a lower bound)

## this one works
g <- function(x)
  function(beta1,beta2,beta3)
  beta1+ beta2*x+beta3*x^2

out <- nls(y~g(x)(beta1,beta2,beta3),data=data.frame(x,y),
           alg="port",
           start=list(beta1=1,beta2=1,beta3=1),
           lower=list(beta1=1,beta2=1,beta3=1))

Thanks in advance!

Stephen


From deepayan.sarkar at gmail.com  Thu Jul  5 02:15:25 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Wed, 4 Jul 2007 17:15:25 -0700
Subject: [R] Lookups in R
In-Reply-To: <6phy7hv3kdl.fsf@gopher4.fhcrc.org>
References: <11435994.post@talk.nabble.com> <468BF958.9060102@biostat.ku.dk>
	<6phy7hv3kdl.fsf@gopher4.fhcrc.org>
Message-ID: <eb555e660707041715j4e0906f2i93e03fbb77104284@mail.gmail.com>

On 7/4/07, Martin Morgan <mtmorgan at fhcrc.org> wrote:
> Michael,
>
> A hash provides constant-time access, though the resulting perl-esque
> data structures (a hash of lists, e.g.) are not convenient for other
> manipulations
>
> > n_accts <- 10^3
> > n_trans <- 10^4
> > t <- list()
> > t$amt <- runif(n_trans)
> > t$acct <- as.character(round(runif(n_trans, 1, n_accts)))
> >
> > uhash <- new.env(hash=TRUE, parent=emptyenv(), size=n_accts)
> > ## keys, presumably account ids
> > for (acct in as.character(1:n_accts)) uhash[[acct]] <- list(amt=0, n=0)
> >
> > system.time(for (i in seq_along(t$amt)) {
> +     acct <- t$acct[i]
> +     x <- uhash[[acct]]
> +     uhash[[acct]] <- list(amt=x$amt + t$amt[i], n=x$n + 1)
> + })
>    user  system elapsed
>   0.264   0.000   0.262
> > udf <- data.frame(amt=0, n=rep(0L, n_accts),
> +                   row.names=as.character(1:n_accts))
> > system.time(for (i in seq_along(t$amt)) {
> +     idx <- row.names(udf)==t$acct[i]
> +     udf[idx, ] <- c(udf[idx,"amt"], udf[idx, "n"]) + c(t$amt[i], 1)
> + })
>    user  system elapsed
>  18.398   0.000  18.394

I don't think that's a fair comparison--- much of the overhead comes
from the use of data frames and the creation of the indexing vector. I
get

> n_accts <- 10^3
> n_trans <- 10^4
> t <- list()
> t$amt <- runif(n_trans)
> t$acct <- as.character(round(runif(n_trans, 1, n_accts)))
> uhash <- new.env(hash=TRUE, parent=emptyenv(), size=n_accts)
> for (acct in as.character(1:n_accts)) uhash[[acct]] <- list(amt=0, n=0)
> system.time(for (i in seq_along(t$amt)) {
+     acct <- t$acct[i]
+     x <- uhash[[acct]]
+     uhash[[acct]] <- list(amt=x$amt + t$amt[i], n=x$n + 1)
+ }, gcFirst = TRUE)
   user  system elapsed
  0.508   0.008   0.517
> udf <- matrix(0, nrow = n_accts, ncol = 2)
> rownames(udf) <- as.character(1:n_accts)
> colnames(udf) <- c("amt", "n")
> system.time(for (i in seq_along(t$amt)) {
+     idx <- t$acct[i]
+     udf[idx, ] <- udf[idx, ] + c(t$amt[i], 1)
+ }, gcFirst = TRUE)
   user  system elapsed
  1.872   0.008   1.883

The loop is still going to be the problem for realistic examples.

-Deepayan


From r.darnell at uq.edu.au  Thu Jul  5 02:50:40 2007
From: r.darnell at uq.edu.au (Dr Ross Darnell)
Date: Thu, 05 Jul 2007 10:50:40 +1000
Subject: [R] questions on lme function
Message-ID: <643d126444a7.6444a7643d12@uq.edu.au>

Ana

You are estimating a random coefficient model on 5 individuals (mean 
and variance). Are you sure this is wise?

Ross Darnell 

----- Original Message -----
From: Ana Conesa <aconesa at ochoa.fib.es>
Date: Thursday, July 5, 2007 1:21 am
Subject: [R] questions on lme function
> 
> Dear list,
> 
> I am using the lme funcion to fit a mixed model for the time response
> of a number of physiological variables. The random variable would be
> the individual on which physiological variables are measured at
> different time points. I have 4 time points, 5 individuals and 3
> replicates per condition (time/individual),  and I would like to fit
> a quadratic model on time. The model I am using is
> 
> > mm <- lme(myvar ~ time + time2, random= ~ time|individual,
> data=clinical)
> 
> being time2 = time*time
> 
> I have a number of questions
> 
> 1) I am not very sure the random effect is correctly modeled. 
> Would I
> need to include the time2 variable aswell?
> 
> 2) I would like to extract the F statistics of the model, and I do
> not find a function for this. Is this possible?
> 
> 3) depending of the variable I take, I frequently obtain a
> convergence error as a result of the lme funcion. Any ideas on what
> to do to improve convergence?
> 
> Thank you
> 
> Ana Conesa, PhD
> Centro de Investigacion Principe Felipe
> Avda. Autopista Saler 16 46013 Valencia
> http://bioinfo.cipf.es
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.htmland provide commented, minimal, self-contained, 
> reproducible code.
>


From adschai at optonline.net  Thu Jul  5 04:14:41 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Thu, 05 Jul 2007 02:14:41 +0000 (GMT)
Subject: [R] (Statistics question) - Nonlinear regression and simultaneous
	equation
Message-ID: <e4ada7f122a58.468c5411@optonline.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/bfe94e0c/attachment.pl 

From adschai at optonline.net  Thu Jul  5 04:56:24 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Thu, 05 Jul 2007 02:56:24 +0000 (GMT)
Subject: [R] Question about framework to weighting different classes in SVM
Message-ID: <e4ffcfef23189.468c5dd8@optonline.net>

Hi gurus,

I have a doubt about multiclass classification SVM. The population in my data includes a couple of class labels that have relatively small proportion of the entire population compared to other classes. I would like SVM to pay more attention to these classes. However, the question I am having here is that is there any systematic/theoretic framework to determine the weights for each class? 

My second question is directly related to R. I would like to use the class.weights attribute in svm function. However, I'm quite confused a bit about how to use it from the description I got from ?svm. Below is the quote.

'a named vector of weights for the different classes, used for asymetric class sizes. Not all factor levels have to be supplied (default weight: 1). All components have to be named.'

Is the name of the vector has to match the levels in my factor used as target labels for my classification? Any simple example would be really appreciated. Thank you!

- adschai


From ssj1364 at gmail.com  Thu Jul  5 06:29:08 2007
From: ssj1364 at gmail.com (sj)
Date: Wed, 4 Jul 2007 22:29:08 -0600
Subject: [R] speed up crr function in cmprsk package
Message-ID: <1c6126db0707042129o25799611o7862b6d3f8fdc1e4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070704/189265f1/attachment.pl 

From adschai at optonline.net  Thu Jul  5 06:35:48 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Thu, 05 Jul 2007 04:35:48 +0000 (GMT)
Subject: [R] Question for svm function in e1071
Message-ID: <e2a7c72e2133f.468c7524@optonline.net>

Hi,

Sorry that I have many questions today. I am using svm function on about 180,000 points of training set. It takes very long time to run. However, I would like it to spit out something to make sure that the run is not dead in between.  Would you please suggest anyway to do so? 

And is there anyway to speed up the performance of this svm function? Thank you.

- adschai


From jk528 at cornell.edu  Thu Jul  5 07:23:21 2007
From: jk528 at cornell.edu (=?ks_c_5601-1987?B?sejB2Mjx?=)
Date: Thu, 5 Jul 2007 01:23:21 -0400
Subject: [R] about stableFit() and hypFit() of fBasics package
Message-ID: <002901c7bec4$a9fd0f70$ac995480@yoursvvosven3r>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/e5cf2e81/attachment.pl 

From n.nguyen at garvan.org.au  Thu Jul  5 08:12:16 2007
From: n.nguyen at garvan.org.au (Nguyen Dinh Nguyen)
Date: Thu, 5 Jul 2007 16:12:16 +1000
Subject: [R] Incidence estimated from Kaplan-Meier
Message-ID: <002901c7becb$701e7030$0fe05e81@D145LD1S>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/5e8738c6/attachment.pl 

From paulmatthias.diderichsen at abbott.com  Thu Jul  5 10:17:52 2007
From: paulmatthias.diderichsen at abbott.com (Paul Matthias Diderichsen)
Date: Thu, 5 Jul 2007 10:17:52 +0200
Subject: [R] pgup/pgdown in R Graphics Window under Linux
Message-ID: <OFE1604883.E944131C-ONC125730F.002D0933-C125730F.002D9323@abbott.com>

Dear S-users.
This should be an easy one: How do I change pages on an X11 graphics 
device under linux?

I thought that the page-up/page-down keys were supposed to do the trick, 
but the frame (window) seems to be kind of immune to any kind of keyboard 
input. The only reaction I ever see is that the mouse pointer changes to a 
"+" when moved into the frame.

I issue these commands:

------------------------------ QUOTE START 
----------------------------------------

[pmd at vmware_diderpm ~]$ R

R version 2.5.1 (2007-06-27)
Copyright (C) 2007 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(lattice)
> xyplot(speed~dist|speed, data=cars, layout=c(3,3))
> sessionInfo()
R version 2.5.1 (2007-06-27)
i686-redhat-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
  lattice
"0.15-11"
> 
-----------------------------QUOTE END 
-----------------------------------------
Which produces the following frame:




Is it necessary to initialize the graphics device so that old pages are 
stored and accessible for paging? Or am I just pressing the wrong buttons.

Any input is appreciated! Please let me know if further info re. 
versions/installed packages/etc is needed.

Thanks, PMD.



***********Abbott GmbH & Co. KG ***********
Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRA 4888
Pers?nlich haftende Gesellschafterin: Abbott Management GmbH
Sitz der Gesellschaft: Wiesbaden, Amtsgericht Wiesbaden HRB 12889

Gesch?ftsf?hrer: Siegfried Brune, Jaime Contreras, Rodolfo Viana
Vorsitzender des Aufsichtsrates: John Landgraf



***********  L e g a l   D is c l a i m e r  ***********
Der Inhalt dieser Nachricht ist vertraulich, kann gesetzlichen Bestimmungen unterliegen, kann vertrauliche Informationen beinhalten und ist nur f?r den direkten Empf?nger bestimmt.Sie ist Eigentum von Abbott Laboratories bzw. der betreffenden Niederlassung. Nicht authorisierte Benutzung, unbefugte Weitergabe sowie Kopieren jeglicher Bestandteile dieser Information ist streng verboten und kann als rechtswidrige Handlung eingestuft werden. Sollten Sie diese Nachricht f?lschlicherweise erhalten haben, informieren Sie bitte Abbott Laboratories umgehend, indem Sie die Email zur?ckschicken und diese dann zusammen mit allen zugeh?rigen Kopien oder Dateianh?ngen zerst?ren.

The information contained in this communication is confidential, may be subject to legal privileges, may constitute inside information, and is intended only for the use of the addressee.  It is the property of Abbott Laboratories or its relevant affiliate.  Unauthorized use, disclosure or copying of this communication or any part thereof is strictly prohibited and may be unlawful.  If you have received this communication in error, please notify Abbott Laboratories immediately by return e-mail and destroy this communication and all copies thereof, including all attachments.

From tamir at imp.univie.ac.at  Thu Jul  5 10:44:46 2007
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Thu, 5 Jul 2007 04:44:46 -0400
Subject: [R] ggplot2 customizing
Message-ID: <200707050444.46124.tamir@imp.univie.ac.at>

Dear all,

I know that ggplot2 documentation is coming along,
but at the moment I can't find how to do the following:
a) change the title of the legend
b) get rid of the closing line at the bottom of the 
density line.

I also observed that the density lines (after limiting the
x-scale) extend a little bit into the surrounding of the plot, 
which can be seen very strong when plotted as pdf.
They extend into the white space between the tick and the
plotting panel.


p <- ggplot(df, aes(x=distance))
p + stat_density(aes(colour=factor(mark),y=..scaled..), size=1, fill=FALSE) + 
scale_x_continuous(limits=c(0, 1e4)) + scale_y_continuous("scaled density")


thank you very much,
ido


From eewwaaww at interia.pl  Thu Jul  5 11:32:58 2007
From: eewwaaww at interia.pl (eewwaaww at interia.pl)
Date: 05 Jul 2007 11:32:58 +0200
Subject: [R] model-based quesiton
Message-ID: <20070705093258.D00D7D5503@poczta.interia.pl>


It%u2019s going to be easy question to you. I%u2019ve started to interest in model-based clustering.
Adrian E. Raftery %u201CRecent Advances in Model-Based Clustering: Image Segmentation and Variable Selection%u201D (www.stat.washington.edu/Raftery)  showed that we can compare different classification methods using BIC statistic. For %u201Cdiabetes%u201D dataset the best model is VVV model with 3 classes- for this model the BIC curve reaches the highest value and the error rate=12%
BIC curve for EII model %u2248k-means is much under the VVV model curve and the error rate equals 18%, so k-means (EII)  is worse then  VVV, what%u2019s clear for me.

I would like to apply model-based to economic data set (GDP, life expectancy 
 data of UE countries), because I%u2019m PhD  student of University of Economics in Poland.
Using this data (standardized) I get the best model EEV (2 classes), EII (k-means) curve is under EEVcurve what suggests that k-means is worse then EEV, but class error for EII equals 0 and for EEV= 6% (and more for another variables), why?

Even applying %u201Ciris%u201D data we get lower class error for EII model (10%)  than for VEV (33%) for 2 classes,   in spite of another models curve are above EII model at the BIC plot.
For this data BIC doesn%u2019t choose the right number of clusters- it chooses VEV for 2 clusters while the right number of classes, given in column five equals 3.

When model-based clustering (for which data sets, are there any special type of data)  is better than k-means (kmeans), hierarchical clustering (hclust)?

I%u2019m looking forward to hearing from you.       

Best regards, 
              Ewa


----------------------------------------------------------------------
O Twoich stronach juz si? m?wi...
Na >>> http://link.interia.pl/f1ad3


From h.wickham at gmail.com  Thu Jul  5 11:33:59 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 5 Jul 2007 11:33:59 +0200
Subject: [R] ggplot2 customizing
In-Reply-To: <200707050444.46124.tamir@imp.univie.ac.at>
References: <200707050444.46124.tamir@imp.univie.ac.at>
Message-ID: <f8e6ff050707050233j239b27b2led486310592cccca@mail.gmail.com>

Hi Ido,

On 7/5/07, Ido M. Tamir <tamir at imp.univie.ac.at> wrote:
> Dear all,
>
> I know that ggplot2 documentation is coming along,
> but at the moment I can't find how to do the following:
> a) change the title of the legend

There's lot of examples in the documentation - and you seem to have
figured how to change the axis labels - so you should find it pretty
easy!

 + scale_colour("new legend name")

> b) get rid of the closing line at the bottom of the
> density line.

Try:

 + stat_density(..., geom="path")

> I also observed that the density lines (after limiting the
> x-scale) extend a little bit into the surrounding of the plot,
> which can be seen very strong when plotted as pdf.
> They extend into the white space between the tick and the
> plotting panel.

Yes, this is a bug - I'll try and get it fixed in the next version.

Thanks,

Hadley


From michael at frumin.net  Thu Jul  5 11:56:20 2007
From: michael at frumin.net (Michael Frumin)
Date: Thu, 05 Jul 2007 05:56:20 -0400
Subject: [R] Lookups in R
In-Reply-To: <eb555e660707041715j4e0906f2i93e03fbb77104284@mail.gmail.com>
References: <11435994.post@talk.nabble.com> <468BF958.9060102@biostat.ku.dk>
	<6phy7hv3kdl.fsf@gopher4.fhcrc.org>
	<eb555e660707041715j4e0906f2i93e03fbb77104284@mail.gmail.com>
Message-ID: <1183629380.13417.1198603635@webmail.messagingengine.com>

the problem I have is that userid's are not just sequential from
1:n_users.  if they were, of course I'd have made a big matrix that was
n_users x n_fields and that would be that.  but, I think what I cando is
just use the hash to store the index into the result matrix, nothing
more. then the rest of it will be easy.

but please tell me more about eliminating loops.  In many cases in R I
have used lapply and derivatives to avoid loops, but in this case they
seem to give me extra overhead simply by the generation of their result
lists:

> system.time(lapply(1:10^4, mean))
   user  system elapsed 
   1.31    0.00    1.31 
> system.time(for(i in 1:10^4) mean(i))
   user  system elapsed 
   0.33    0.00    0.32 


thanks,
mike


> I don't think that's a fair comparison--- much of the overhead comes
> from the use of data frames and the creation of the indexing vector. I
> get
> 
> > n_accts <- 10^3
> > n_trans <- 10^4
> > t <- list()
> > t$amt <- runif(n_trans)
> > t$acct <- as.character(round(runif(n_trans, 1, n_accts)))
> > uhash <- new.env(hash=TRUE, parent=emptyenv(), size=n_accts)
> > for (acct in as.character(1:n_accts)) uhash[[acct]] <- list(amt=0, n=0)
> > system.time(for (i in seq_along(t$amt)) {
> +     acct <- t$acct[i]
> +     x <- uhash[[acct]]
> +     uhash[[acct]] <- list(amt=x$amt + t$amt[i], n=x$n + 1)
> + }, gcFirst = TRUE)
>    user  system elapsed
>   0.508   0.008   0.517
> > udf <- matrix(0, nrow = n_accts, ncol = 2)
> > rownames(udf) <- as.character(1:n_accts)
> > colnames(udf) <- c("amt", "n")
> > system.time(for (i in seq_along(t$amt)) {
> +     idx <- t$acct[i]
> +     udf[idx, ] <- udf[idx, ] + c(t$amt[i], 1)
> + }, gcFirst = TRUE)
>    user  system elapsed
>   1.872   0.008   1.883
> 
> The loop is still going to be the problem for realistic examples.
> 
> -Deepayan


From rb373 at cam.ac.uk  Thu Jul  5 11:58:50 2007
From: rb373 at cam.ac.uk (R. Baker)
Date: 05 Jul 2007 10:58:50 +0100
Subject: [R] t-values for two-way interactions
Message-ID: <Prayer.1.0.18.0707051058500.754@hermes-1.csi.cam.ac.uk>

I have a model with 3 fixed factors (type, stress, MorD) and two 
significant two-way interactions (type*stress, stress*MorD).

x$summary

#                  Estimate Std.Error  DF t.value pvals ci950 ci990 ci999

#(Intercept)        241.738     8.757 994  27.606 0e+00  TRUE  TRUE  TRUE

#typePsPr           -26.516     5.905 994  -4.490 1e-05  TRUE  TRUE  TRUE

#stressPN           -21.820     1.776 994 -12.284 0e+00  TRUE  TRUE  TRUE

#MorDMis            -26.588     5.905 994  -4.503 1e-05  TRUE  TRUE  TRUE

#typePsPr:stressPN   13.904     2.051 994   6.779 0e+00  TRUE  TRUE  TRUE

#stressPN:MorDMis     9.168     2.051 994   4.470 1e-05  TRUE  TRUE  TRUE


I would like to find out more about the type*stress interaction, e.g. how 
the levels of type differ between stress. More specifically I want to find 
a t-value for the pairwise comparisons:

typePr:stressN vs. typePsPr:stressN
typePr:stressPN vs. typePsPr:stressPN

I can't do this with the summary table (above) as it always includes the 
factor MorD (mis/dis) in each pairwise comparison, i.e. it provides 
t-values for

typePr:stressN:MorDmis and typePsPr:stressN:MorDmis 
typePr:stressN:MorDdis and typePsPr:stressN:MorDdis 

Is it possible to look at interaction of type*stress over MorD and NOT 
within MorDmis or MorDdis?

Any advice would be greatly appreciated!

Rachel Baker

-- 
--------------------------------------------------------------------------
PhD student                
Dept of Linguistics        
Sidgwick Avenue
University of Cambridge              
Cambridge


From yn19832 at msn.com  Thu Jul  5 12:16:44 2007
From: yn19832 at msn.com (livia)
Date: Thu, 5 Jul 2007 03:16:44 -0700 (PDT)
Subject: [R] Loop and function
Message-ID: <11443955.post@talk.nabble.com>


Hi All, I am trying to make a loop for a function and I am using the
following codes. "p" and "var" are some matrix obtained before. I would like
to apply the function  "gpdlow" for i in 1:12 and get the "returnlow" for i
in 1:12. But when I ask for "returnlow" there are warnings and it turns out
some strange result. 

for (i in 1:12){  
gpdlow <- function(u){      
p[,i]$beta -u*p[,i][[2]]
}
returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
}


-- 
View this message in context: http://www.nabble.com/Loop-and-function-tf4028854.html#a11443955
Sent from the R help mailing list archive at Nabble.com.


From jabezwuk at yahoo.co.uk  Thu Jul  5 12:20:43 2007
From: jabezwuk at yahoo.co.uk (Jabez Wilson)
Date: Thu, 5 Jul 2007 10:20:43 +0000 (GMT)
Subject: [R] getting values from arrays using which()
Message-ID: <515463.69341.qm@web27408.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/5e1d7546/attachment.pl 

From FredeA.Togersen at agrsci.dk  Thu Jul  5 12:36:02 2007
From: FredeA.Togersen at agrsci.dk (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Thu, 5 Jul 2007 12:36:02 +0200
Subject: [R] getting values from arrays using which()
In-Reply-To: <515463.69341.qm@web27408.mail.ukl.yahoo.com>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC05429FE9@DJFPOST01.djf.agrsci.dk>


ndx <- which(myArray>=99 , ind.arr=T)

cbind(ndx, myArray[ndx])



Best regards

Frede Aakmann T?gersen
Scientist


UNIVERSITY OF AARHUS
Faculty of Agricultural Sciences
Dept. of Genetics and Biotechnology
Blichers All? 20, P.O. BOX 50
DK-8830 Tjele

Phone:   +45 8999 1900
Direct:  +45 8999 1878

E-mail:  FredeA.Togersen at agrsci.dk
Web:	   http://www.agrsci.org				

This email may contain information that is confidential.
Any use or publication of this email without written permission from Faculty of Agricultural Sciences is not allowed.
If you are not the intended recipient, please notify Faculty of Agricultural Sciences immediately and delete this email.



Med venlig hilsen
Frede Aakmann T?gersen
 

 

> -----Oprindelig meddelelse-----
> Fra: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] P? vegne af Jabez Wilson
> Sendt: 5. juli 2007 12:21
> Til: R-Help
> Emne: [R] getting values from arrays using which()
> 
> Dear R-Help,
> 
> I have an array 1260x1260, upper triangle consisting of 
> numbers between 0 and 100, and lower triangle all NA. I can 
> extract the index of those values say above 99 using the 
> following code:
> which(myArray>=99 , ind.arr=T)
> 
> which returns:
>        row  col
> 5475   252  253
> 45423  764  765
> 46902  777  778
> 34146  611  962
> 50681 1220 1221
> 
> Now I would like to if poss print the actual value contained 
> in the index as well so it would look like:
> 
>        row  col
> 5475   252  253    99  
> 45423  764  765    96.5    
> 46902  777  778    99
> 34146  611  962    100    
> 50681 1220 1221    100
> 
> Can this be done?
> 
> 
>       ___________________________________________________________ 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Thu Jul  5 13:31:59 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Jul 2007 07:31:59 -0400
Subject: [R] Loop and function
In-Reply-To: <11443955.post@talk.nabble.com>
References: <11443955.post@talk.nabble.com>
Message-ID: <644e1f320707050431l5d8b3ab2s1e6b94c5fb6171a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/641994ef/attachment.pl 

From jholtman at gmail.com  Thu Jul  5 13:44:59 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Jul 2007 07:44:59 -0400
Subject: [R] Lookups in R
In-Reply-To: <1183629380.13417.1198603635@webmail.messagingengine.com>
References: <11435994.post@talk.nabble.com> <468BF958.9060102@biostat.ku.dk>
	<6phy7hv3kdl.fsf@gopher4.fhcrc.org>
	<eb555e660707041715j4e0906f2i93e03fbb77104284@mail.gmail.com>
	<1183629380.13417.1198603635@webmail.messagingengine.com>
Message-ID: <644e1f320707050444t37fc21b1g84a3a7470b6869e1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/e2118f04/attachment.pl 

From eb99lamo at kth.se  Thu Jul  5 13:54:10 2007
From: eb99lamo at kth.se (Lars Modig)
Date: Thu, 5 Jul 2007 13:54:10 +0200 (CEST)
Subject: [R] select data from large CSV file
Message-ID: <8299.193.183.79.7.1183636450.squirrel@webmail.sys.kth.se>

Hello


I?ve got a large CSV file (>500M) with statistical data. It?s devided in
12 columns and I don?t know how many lines.
The second column is the date and the second is a unique code for the
location, the rest is (lets say different whether data.  See example
below.
070704, 25, --,--,--,temperature, 22, --,--,30, 20,Y
070705, 25, --,--,--,temperature, 22, --,--,30, 20,Y
070705, 25, --,--,--,pressure, 1200, --,--,1000, 1100,N
070705, 26, --,--,--,temperature, 22, --,--,30, 20,Y


First I tried with data <- read.csv. and of course the memory got full.
Then I found in the archive that you could use scan. So then I wrote the
following lines below to search for location and store one location with
all different data in one variable.

# collect the different pnc's
 b=2                                        #compare from second number
 alike=TRUE                                 #Dim alike like a boolean
 stored = 910286609                         #first number is known
  for(i in 1: 100){                         #start counting and scaning
     data_final <- matrix(unlist(scan("C:/Documents and
Settings/modiglar/Desktop/temp/et.csv",sep="," ,
what=list("","","","","","","","","","","",""), skip=i ,
n=12)),ncol=12, byrow=TRUE)


      a=1                                     #compare from the 1:th stored
      while( a < b){                          #---
                                              #
        if(as.numeric(data_final[2] != stored[a])) #compare
          { a=a+1                                  #
          alike=FALSE  }                           #
        else{                                      #
           alike=TRUE                              #
           break }                                 #
      }                                            # ---

      if (alike==FALSE){                           #
         stored[b]=as.numeric(data_final[2])       # Store new data
         b=b+1                                     #
      }
  }

#------------------------------------------------------------
# save 1 pnc at the time
d=1
saved_data = 1:1200 ; dim(saved_data) <- c(12,100)
save_data_nr = 1                               #Stored number
  for(i in 1: 100){                            #start counting and scaning
     data_final <- matrix(unlist(scan("C:/Documents and
Settings/modiglar/Desktop/temp/et.csv",sep="," ,
what=list("","","","","","","","","","","",""), skip=i ,
n=12)),ncol=12, byrow=TRUE)


      if(as.numeric(data_final[2] == stored[save_data_nr])) #compare
        { saved_data[,d] <-  matrix(unlist(data_final),ncol=12,
byrow=TRUE)  #Store new data
         d=d+1 }                                         #
                                                         #
                                                         #
 }
As you can see I?m not so familiar with R, and therefore I have probably
done this the wrong way.

As I understand when running this, is that scan opens up the file count
down to the line that should be read and read it, then closing the file
again. So when I?m starting to come to line number at 10000 then it
starting to take time. I let the computer run over night, but it was still
far from finished when I stopped the loop.

So how should I do this? Maybe I also need to sort on the date, and that
is hopefully in order so then you should be able to cut the file every
time you hit a new month but that will also take time if I do it like
this.

Thank you for your help in advance.

Lars


From signethompson at gmail.com  Thu Jul  5 13:55:37 2007
From: signethompson at gmail.com (Signe Thompson)
Date: Thu, 5 Jul 2007 13:55:37 +0200
Subject: [R] (no subject)
Message-ID: <259d42ad0707050455h19f2fd51u49946b5f56b732ef@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/fc0d0ab3/attachment.pl 

From jim at bitwrit.com.au  Thu Jul  5 14:09:18 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 05 Jul 2007 22:09:18 +1000
Subject: [R] Please help with legend command
In-Reply-To: <392FF8243BA9634084F5AC5EF07B5CDF018AFCC6@LTA3VS002.ees.hhs.gov>
References: <392FF8243BA9634084F5AC5EF07B5CDF018AFCC6@LTA3VS002.ees.hhs.gov>
Message-ID: <468CDF6E.2070703@bitwrit.com.au>

Smith, Phil (CDC/CCID/NCIRD) wrote:
> Hi R-ers:
> 
> I'm drawing a plot and have used different line types (lty) for
> different race/ethnicity groups. I want a legend that explains what line
> types correspond to the different race/ethnicity groups. I used the
> following code:
> 
> 
> legend( 1992 , 42  , c("Hispanic" , "non-Hispanic white (NHW)" ,
> "non-Hispanic black" , "AI/AN" , "Asian" ) , lty=1:5 ,cex = .6 , bty='n'
> )
> 
> Guess what? The legend "box" was so narrow that the line types that show
> up in that legend box look essentially the same, because they are short.
> I.e, although a line type might be a long dash followed by a short dash,
> only the long dash shows up in the box. The consequence of this is that
> the race/ethnic group that corresponds to the line type that is only a
> long dash cannot be distinguished from the legend.
> 
> How do I stretch that legend box out so as to allow lty to draw longer
> line segments?
> 
Hi Phil,
A quick hack is to get a copy of the function:

sink("newlegend.R")
legend
sink()

Get the file into a text editor and first get rid of the

<environment: namespace:graphics>

at the bottom, then change line 161 to:

w0 <- w0 + (4 + x.off) * xchar

and line 212 to:

seg.len <- 4

source the new function:

source("newlegend.R")

and your line segments will be twice as long.

Jim


From j.logsdon at quantex-research.com  Thu Jul  5 14:05:46 2007
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Thu, 5 Jul 2007 13:05:46 +0100
Subject: [R] working with R graphics remotely
In-Reply-To: <971536df0707020752x70d07cd8y39456f2cd1b892b@mail.gmail.com>
References: <BAY110-F320CE4503598BABC0AFC93C70D0@phx.gbl>
	<971536df0707020752x70d07cd8y39456f2cd1b892b@mail.gmail.com>
Message-ID: <200707051305.46178.j.logsdon@quantex-research.com>

Better still IMHO - try NX or FreeNX.  That runs an xclient (also of course an 
xserver underneath) on your WIndows desktop.   Windows pop up as required.  
NX compression means that you get very impressive speed, mouse response etc.

A server also needs installing on the Linux box so you may have to negotiate 
with the sysadmins.   I don't think you can install it under an ordinary 
user.

NX: www.nomachine.com
FreeNX: freenx.berlios.de
FreeNX uses core libraries kindly provided by NX

On Monday 02 July 2007 15:52:16 Gabor Grothendieck wrote:
> On your Windows machine you need to run (1) an X server and (2) an ssh
> client. Xming and putty, respectively, are among the free ones.
>
> First start Xming (you can configure it using XLaunch, which is included
> with Xming, or you can run Xming from the Windows command line specifying
> the configuration parameters via command line flags).  Then run
> putty.   Be sure you have X11 forwarding enabled in putty's
>   Connection | SSH | Tunnel
> screen.  Googling will locate much info for these programs.
>
> On 7/1/07, zhihua li <lzhtom at hotmail.com> wrote:
> > Hi netters,
> >
> > Now I'm connecting from my local windows machine to a remote linux
> > machine and launch R out there using SSH. When I tried to create grahics,
> > like using plot or heatmap, I cannot see the output. Maybe a new R window
> > displaying the graphics has popped out in the remote machine? Or I need
> > to change some settings for the graphics to display? I don't know. I
> > googled it and tried dev.copy but it didn't work. Can anyone help me
> > here? I need to be able to see the output graphics and save it to a file
> > (like jpeg)
> >
> > Thanks a lot!
> >
> > _________________________________________________________________
> > ??????????????? MSN Hotmail?  http://www.hotmail.com
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.



-- 
Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


From ligges at statistik.uni-dortmund.de  Thu Jul  5 14:12:12 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jul 2007 14:12:12 +0200
Subject: [R] Problem/bug with smooth.spline and all.knots=T
In-Reply-To: <20070704125844.GA31772@p15097509.pureserver.info>
References: <20070704125844.GA31772@p15097509.pureserver.info>
Message-ID: <468CE01C.1070102@statistik.uni-dortmund.de>



Hubertus wrote:
> Dear list,
> if I do
>   smooth.spline(tmpSec, tmpT, all.knots=T)
> with the attached data, I get this error-message:

Note that you cannot attach data that way. You migth want to upload it 
to some web space and send us the link.

Uwe Ligges




>   Error in smooth.spline(tmpSec, tmpT, all.knots = T) :
>         smoothing parameter value too small
> If I do
>   smooth.spline(tmpSec[-single arbitrary number], tmpT[-single arbitrary number], all.knots=T)
> it works!
> 
> I just don't see it. It works for hundrets other datasets, but not for this one.
> Would be glad if anyone could help!
> 
> Hubertus
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From brown_emu at yahoo.com  Thu Jul  5 14:16:34 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Thu, 5 Jul 2007 05:16:34 -0700 (PDT)
Subject: [R] Loop and function
In-Reply-To: <11443955.post@talk.nabble.com>
Message-ID: <522166.61148.qm@web39712.mail.mud.yahoo.com>

You do not have matching parentheses in this line
   returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
most likely there is a syntax error that halts the execution of the
assignment statement?



--- livia <yn19832 at msn.com> wrote:

> 
> Hi All, I am trying to make a loop for a function and I am using the
> following codes. "p" and "var" are some matrix obtained before. I would
> like
> to apply the function  "gpdlow" for i in 1:12 and get the "returnlow" for i
> in 1:12. But when I ask for "returnlow" there are warnings and it turns out
> some strange result. 
> 
> for (i in 1:12){  
> gpdlow <- function(u){      
> p[,i]$beta -u*p[,i][[2]]
> }
> returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
> }
> 
> 
> -- 
> View this message in context:
> http://www.nabble.com/Loop-and-function-tf4028854.html#a11443955
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Zava.Aydemir at morganstanley.com  Thu Jul  5 14:24:57 2007
From: Zava.Aydemir at morganstanley.com (Aydemir, Zava (FID))
Date: Thu, 5 Jul 2007 08:24:57 -0400
Subject: [R] converting list to an array
Message-ID: <755261CA22782948B1C42ACDC83912A104614119@NYWEXMB27.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/39d9a288/attachment.pl 

From stonemonkey at web.de  Thu Jul  5 14:33:12 2007
From: stonemonkey at web.de (Hubertus)
Date: Thu, 5 Jul 2007 14:33:12 +0200
Subject: [R] Problem/bug with smooth.spline and all.knots=T
In-Reply-To: <468CE01C.1070102@statistik.uni-dortmund.de>
References: <20070704125844.GA31772@p15097509.pureserver.info>
	<468CE01C.1070102@statistik.uni-dortmund.de>
Message-ID: <20070705123312.GA12186@p15097509.pureserver.info>

Hi again,
Uwe mentioned that attachments cannot be posted. I uploaded the file on
http://phi.stonemonkey.org/smoothspline.rda

Thanks for your help!

Hubertus


From bren at juanantonio.info  Thu Jul  5 14:33:35 2007
From: bren at juanantonio.info (=?UTF-8?Q?Juan_Antonio_Bre=C3=B1a_Moral?=)
Date: Thu, 5 Jul 2007 05:33:35 -0700 (PDT)
Subject: [R] R(DCOM) / StatConnector and PHP
In-Reply-To: <000501c678d4$4af7ee00$6400a8c0@IBM>
References: <000501c678d4$4af7ee00$6400a8c0@IBM>
Message-ID: <11445559.post@talk.nabble.com>


Yes I recommend you the following reading:

http://www.juanantonio.info/jab_cms.php?id=144

Best Regards.

Juan Antonio Bre?a Moral
http://www.juanantonio.info


S?ren Merser wrote:
> 
> hi
> does anyone have knowledge on how to connected to StatConnector from PHP
> I have tried:
>      new COM("STATCONNECTORSRVLib.StatConnector") or die("Unable to 
> instantiate StatConnector");
> with no succes
> regards soren
> 
> ______________________________________________
> R-help en stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 

-- 
View this message in context: http://www.nabble.com/R%28DCOM%29---StatConnector-and-PHP-tf1626352.html#a11445559
Sent from the R help mailing list archive at Nabble.com.


From f.harrell at vanderbilt.edu  Thu Jul  5 14:48:00 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 05 Jul 2007 07:48:00 -0500
Subject: [R] Incidence estimated from Kaplan-Meier
In-Reply-To: <002901c7becb$701e7030$0fe05e81@D145LD1S>
References: <002901c7becb$701e7030$0fe05e81@D145LD1S>
Message-ID: <468CE880.2040500@vanderbilt.edu>

Nguyen Dinh Nguyen wrote:
> Dear all,
> 
> I have a stat question that may not be related to R, but I would like to
> have your advice.
> 
>  
> 
> I have just read a medical paper in which the authors report the 1-p (where
> p is the cumulative survival probability from the Kaplan Meier curve) as
> incidence of disease.  
> 
>  
> 
> Specifically, the study followed ~12000 women on drug A and ~20000 women on
> drug B for 12 months.  During that period 29 women on drug A and 80 on drug
> B had the disease.  The incidence of disease for A and B was 0.24% and 0.30%
> respectively.  However, instead of reporting these numbers, they report the
> 1-p figure which was 0.3% for A and 0.6% for B. 
> 
>  
> 
> So, the incidence from 1-p was substantially higher than the actual
> incidence.  My question is: is it appropriate to use 1-p estimated from
> Kaplan-Meier as the incidence of disease?  If not, why not? 
> 
>  
> 
> Regards,
> 
> Nguyen

Yes it's appropriate, and it makes you state the cumulative incidence by 
time t rather than leaving time unspecified.  In your example it is 
likely that all women weren't followed completely, so simple incidences 
are not appropriate to compute because the denominator is not constant.

Frank

> 
>  
> 
> ____________________________ 
> Nguyen Dinh Nguyen, 
> 
> Bone and Mineral Research Program 
> Garvan Institute of Medical Research 
> St Vincent's Hospital 
> 384 Victoria Street, Darlinghurst 
> Sydney, NSW 2010 
> Australia 
> Tel; 61-2-9295 8274 
> Fax: 61-2-9295 8241 
> E-mail: n.nguyen at garvan.org.au 
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From yn19832 at msn.com  Thu Jul  5 14:48:41 2007
From: yn19832 at msn.com (livia)
Date: Thu, 5 Jul 2007 05:48:41 -0700 (PDT)
Subject: [R] Loop and function
In-Reply-To: <522166.61148.qm@web39712.mail.mud.yahoo.com>
References: <11443955.post@talk.nabble.com>
	<522166.61148.qm@web39712.mail.mud.yahoo.com>
Message-ID: <11445807.post@talk.nabble.com>


Thanks a lot. I have corrected this. But it still does not work. Any thought?

Stephen Tucker wrote:
> 
> You do not have matching parentheses in this line
>    returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
> most likely there is a syntax error that halts the execution of the
> assignment statement?
> 
> 
> 
> --- livia <yn19832 at msn.com> wrote:
> 
>> 
>> Hi All, I am trying to make a loop for a function and I am using the
>> following codes. "p" and "var" are some matrix obtained before. I would
>> like
>> to apply the function  "gpdlow" for i in 1:12 and get the "returnlow" for
>> i
>> in 1:12. But when I ask for "returnlow" there are warnings and it turns
>> out
>> some strange result. 
>> 
>> for (i in 1:12){  
>> gpdlow <- function(u){      
>> p[,i]$beta -u*p[,i][[2]]
>> }
>> returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
>> }
>> 
>> 
>> -- 
>> View this message in context:
>> http://www.nabble.com/Loop-and-function-tf4028854.html#a11443955
>> Sent from the R help mailing list archive at Nabble.com.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Return-valus-for-different-numbr-of-rows-tf4028854.html#a11445807
Sent from the R help mailing list archive at Nabble.com.


From jholtman at gmail.com  Thu Jul  5 15:11:02 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Jul 2007 09:11:02 -0400
Subject: [R] Loop and function
In-Reply-To: <11445807.post@talk.nabble.com>
References: <11443955.post@talk.nabble.com>
	<522166.61148.qm@web39712.mail.mud.yahoo.com>
	<11445807.post@talk.nabble.com>
Message-ID: <644e1f320707050611y762ac236u354b3be736985db4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/681b65dd/attachment.pl 

From pedrazzi at unipr.it  Thu Jul  5 11:50:18 2007
From: pedrazzi at unipr.it (Giuseppe PEDRAZZI)
Date: Thu, 5 Jul 2007 11:50:18 +0200
Subject: [R] Is it a bug ?
Message-ID: <000601c7bee9$e5ba7cd0$47544ea0@pedrak>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/11325faa/attachment.pl 

From yn19832 at msn.com  Thu Jul  5 15:23:03 2007
From: yn19832 at msn.com (livia)
Date: Thu, 5 Jul 2007 06:23:03 -0700 (PDT)
Subject: [R] Loop and function
In-Reply-To: <644e1f320707050611y762ac236u354b3be736985db4@mail.gmail.com>
References: <11443955.post@talk.nabble.com>
	<522166.61148.qm@web39712.mail.mud.yahoo.com>
	<11445807.post@talk.nabble.com>
	<644e1f320707050611y762ac236u354b3be736985db4@mail.gmail.com>
Message-ID: <11446408.post@talk.nabble.com>


Thanks. Yes, gpdlow is indeed return a vector longer than one. The length of
the vector is different for i in 1:12, each equals to
length(var[,i][var[,i]<(p[,i][[2]]).





jim holtman wrote:
> 
> What does
> 
> gpdlow(var[,i][var[,i]<(p[,i][[2]])
> 
> return?  Is it a vector; if so, how long?  Your declaration of
> 
>> returnlow<- matrix(,12)
>>
>> str(returnlow)
>  logi [1:12, 1] NA NA NA NA NA NA ...
>>
> 
> is a matrix of 12 rows and one column.  You may be getting the error
> message
> is gpdlow is returning a vector longer than one.  Do
> 
> str(gpdlow(var[,i][var[,i]<(p[,i][[2]]))
> 
> so that we can see what the data looks like.  You still haven't provided a
> self-contained example, so we can only guess at what is happening.
> 
> 
> 
> On 7/5/07, livia <yn19832 at msn.com> wrote:
>>
>>
>> Thanks a lot. I have corrected this. But it still does not work. Any
>> thought?
>>
>> Stephen Tucker wrote:
>> >
>> > You do not have matching parentheses in this line
>> >    returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
>> > most likely there is a syntax error that halts the execution of the
>> > assignment statement?
>> >
>> >
>> >
>> > --- livia <yn19832 at msn.com> wrote:
>> >
>> >>
>> >> Hi All, I am trying to make a loop for a function and I am using the
>> >> following codes. "p" and "var" are some matrix obtained before. I
>> would
>> >> like
>> >> to apply the function  "gpdlow" for i in 1:12 and get the "returnlow"
>> for
>> >> i
>> >> in 1:12. But when I ask for "returnlow" there are warnings and it
>> turns
>> >> out
>> >> some strange result.
>> >>
>> >> for (i in 1:12){
>> >> gpdlow <- function(u){
>> >> p[,i]$beta -u*p[,i][[2]]
>> >> }
>> >> returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
>> >> }
>> >>
>> >>
>> >> --
>> >> View this message in context:
>> >> http://www.nabble.com/Loop-and-function-tf4028854.html#a11443955
>> >> Sent from the R help mailing list archive at Nabble.com.
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Return-valus-for-different-numbr-of-rows-tf4028854.html#a11445807
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Return-valus-for-different-numbr-of-rows-tf4028854.html#a11446408
Sent from the R help mailing list archive at Nabble.com.


From rvaradhan at jhmi.edu  Thu Jul  5 15:25:22 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 5 Jul 2007 09:25:22 -0400
Subject: [R] Incidence estimated from Kaplan-Meier
In-Reply-To: <468CE880.2040500@vanderbilt.edu>
References: <002901c7becb$701e7030$0fe05e81@D145LD1S>
	<468CE880.2040500@vanderbilt.edu>
Message-ID: <002d01c7bf07$f0a749c0$7c94100a@win.ad.jhu.edu>

The 1-Pr(disease free survival) estimate from KM is not appropriate if
competing risk of mortality (from causes other than the disease of interest)
are present.  In that case, 1-Pr(disease free survival) over-estimates the
cumulative incidence of disease.  The larger the hazard of mortality, the
larger the over-estimation.  This is a well-known phenomenon in the
competing risks literature.  See, for example, Gooley et al. (Stats in Med
1999).

Ravi. 

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank E Harrell Jr
Sent: Thursday, July 05, 2007 8:48 AM
To: Nguyen Dinh Nguyen
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Incidence estimated from Kaplan-Meier

Nguyen Dinh Nguyen wrote:
> Dear all,
> 
> I have a stat question that may not be related to R, but I would like to
> have your advice.
> 
>  
> 
> I have just read a medical paper in which the authors report the 1-p
(where
> p is the cumulative survival probability from the Kaplan Meier curve) as
> incidence of disease.  
> 
>  
> 
> Specifically, the study followed ~12000 women on drug A and ~20000 women
on
> drug B for 12 months.  During that period 29 women on drug A and 80 on
drug
> B had the disease.  The incidence of disease for A and B was 0.24% and
0.30%
> respectively.  However, instead of reporting these numbers, they report
the
> 1-p figure which was 0.3% for A and 0.6% for B. 
> 
>  
> 
> So, the incidence from 1-p was substantially higher than the actual
> incidence.  My question is: is it appropriate to use 1-p estimated from
> Kaplan-Meier as the incidence of disease?  If not, why not? 
> 
>  
> 
> Regards,
> 
> Nguyen

Yes it's appropriate, and it makes you state the cumulative incidence by 
time t rather than leaving time unspecified.  In your example it is 
likely that all women weren't followed completely, so simple incidences 
are not appropriate to compute because the denominator is not constant.

Frank

> 
>  
> 
> ____________________________ 
> Nguyen Dinh Nguyen, 
> 
> Bone and Mineral Research Program 
> Garvan Institute of Medical Research 
> St Vincent's Hospital 
> 384 Victoria Street, Darlinghurst 
> Sydney, NSW 2010 
> Australia 
> Tel; 61-2-9295 8274 
> Fax: 61-2-9295 8241 
> E-mail: n.nguyen at garvan.org.au 
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Jul  5 15:38:06 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Jul 2007 15:38:06 +0200
Subject: [R] Is it a bug ?
In-Reply-To: <000601c7bee9$e5ba7cd0$47544ea0@pedrak>
References: <000601c7bee9$e5ba7cd0$47544ea0@pedrak>
Message-ID: <468CF43E.8040100@statistik.uni-dortmund.de>

I don't get your point, because

 > exp(-(-3)^2.2)
[1] NaN

is correct. A negative value to the power of a non-integer is undefined 
in IR. Of course it is defined as a complex number:

 > exp(-(-3+0i)^2.2)
[1] 1.096538e-04-3.47404e-05i

Uwe Ligges




Giuseppe PEDRAZZI wrote:
> 		[[diverted from R-bugs to R-help by the list maintainer]]
> 
> Dear Friend and distinguished R gurus,
> 
> first of all really thank you very much for the marvellous tool that is R.
> 
> I am using R 2.5.0,  windows XP - italian language.
> 
> I was perfoming some calculation on fractional exponential and
> I found a strange behaviour. I do not know if it is really a bug, but I would expect
> a different answer from R.
> 
> I was trying the following :
> 
> x <- seq(-3,3, by =0.1)
> n <- 2.2
> y <- exp(-x^n)
> 
> well, the y vector contains (NaN for all negative value of x)
> 
> but if you ask for single value calculation like
> 
> y <- exp(-(-3)^2.2) or 
> 
> y <- exp(-(-2.9)^2.2)
> 
> the answer is correct. 
> It seem it does not make the calculation in vector form.
> 
> I got the same behaviour (NaN)  in a for loop
> 
>> for(i in 1:length(x)) y[i]=exp(x[i]^n)
>> y
>  [1]           NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN
> [10]          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN
> [19]          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN
> [28]          NaN          NaN          NaN     1.000000     1.006330     1.029416     1.073302     1.142488     1.243137
> [37]     1.384082     1.578166     1.844237     2.210260     2.718282     3.432491     4.452553     5.936068     8.137120
> [46]    11.473746    16.648415    24.867680    38.251295    60.611092    98.967689   166.572985   289.077778   517.425935
> [55]   955.487320  1820.793570  3581.521323  7273.674928 15255.446778 33050.861013 73982.100407
> 
> Is it strange or did I miss something ?
> 
> Many thanks for the attention.
> 
> 
> Very best regards
> 
> Giuseppe Pedrazzi
> Dept Public Health, Physics Division
> University of Parma, Italy
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From yn19832 at msn.com  Thu Jul  5 15:49:06 2007
From: yn19832 at msn.com (livia)
Date: Thu, 5 Jul 2007 06:49:06 -0700 (PDT)
Subject: [R] Loop and function
In-Reply-To: <644e1f320707050611y762ac236u354b3be736985db4@mail.gmail.com>
References: <11443955.post@talk.nabble.com>
	<522166.61148.qm@web39712.mail.mud.yahoo.com>
	<11445807.post@talk.nabble.com>
	<644e1f320707050611y762ac236u354b3be736985db4@mail.gmail.com>
Message-ID: <11446873.post@talk.nabble.com>


I tried str(gpdlow(var[,i][var[,i]<(p[,i][[2]])) and it returns num [1:49]
-1.92 -1.69 -2.20 -1.65 -2.13 ...
It is the number when i=1, I guess it does not loop. In fact, the number
should be different when loop between i.


jim holtman wrote:
> 
> What does
> 
> gpdlow(var[,i][var[,i]<(p[,i][[2]])
> 
> return?  Is it a vector; if so, how long?  Your declaration of
> 
>> returnlow<- matrix(,12)
>>
>> str(returnlow)
>  logi [1:12, 1] NA NA NA NA NA NA ...
>>
> 
> is a matrix of 12 rows and one column.  You may be getting the error
> message
> is gpdlow is returning a vector longer than one.  Do
> 
> str(gpdlow(var[,i][var[,i]<(p[,i][[2]]))
> 
> so that we can see what the data looks like.  You still haven't provided a
> self-contained example, so we can only guess at what is happening.
> 
> 
> 
> On 7/5/07, livia <yn19832 at msn.com> wrote:
>>
>>
>> Thanks a lot. I have corrected this. But it still does not work. Any
>> thought?
>>
>> Stephen Tucker wrote:
>> >
>> > You do not have matching parentheses in this line
>> >    returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
>> > most likely there is a syntax error that halts the execution of the
>> > assignment statement?
>> >
>> >
>> >
>> > --- livia <yn19832 at msn.com> wrote:
>> >
>> >>
>> >> Hi All, I am trying to make a loop for a function and I am using the
>> >> following codes. "p" and "var" are some matrix obtained before. I
>> would
>> >> like
>> >> to apply the function  "gpdlow" for i in 1:12 and get the "returnlow"
>> for
>> >> i
>> >> in 1:12. But when I ask for "returnlow" there are warnings and it
>> turns
>> >> out
>> >> some strange result.
>> >>
>> >> for (i in 1:12){
>> >> gpdlow <- function(u){
>> >> p[,i]$beta -u*p[,i][[2]]
>> >> }
>> >> returnlow <- gpdlow(var[,i][var[,i]<(p[,i][[2]])
>> >> }
>> >>
>> >>
>> >> --
>> >> View this message in context:
>> >> http://www.nabble.com/Loop-and-function-tf4028854.html#a11443955
>> >> Sent from the R help mailing list archive at Nabble.com.
>> >>
>> >> ______________________________________________
>> >> R-help at stat.math.ethz.ch mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Return-valus-for-different-numbr-of-rows-tf4028854.html#a11445807
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Return-valus-for-different-numbr-of-rows-tf4028854.html#a11446873
Sent from the R help mailing list archive at Nabble.com.


From eewwaaww at interia.pl  Thu Jul  5 15:54:43 2007
From: eewwaaww at interia.pl (eewwaaww at interia.pl)
Date: 05 Jul 2007 15:54:43 +0200
Subject: [R] model-based question-better readable version
Message-ID: <20070705135443.F27482BD89F@poczta.interia.pl>

It is going to be easy question to you. I've  started to interest in model-based clustering.

Adrian E. Raftery "Recent Advances in Model-Based Clustering: Image Segmentation and Variable Selection" (www.stat.washington.edu/Raftery)showed that we can compare different classification methods using BIC
statistic. For diabetes dataset the best model is VVV model with 3 classes- for this model the BIC curve reaches the highest value and the error rate=12%
BIC curve for EII model~k-means is much under the VVV model curve and the error rate equals 18%, so k-means (EII) is worse then VVV, what is clear for me.

I would like to apply model-based to economic data set (GDP, life expectancy data of UE countries), because I am PhD  student of University of Economics in Poland.
Using this data (standardized) I get the best model EEV (2 classes), EII (k-means) curve is under EEV curve what suggests that k-means is worse then EEV, but class error for EII equals 0 and for EEV= 6% (and more for another variables), why?

Even applying  iris data we get lower class error for EII model (10%)  than for VEV (33%) for 2 classes,   in spite of that VEV model and others models curves are above EII model at the BIC plot.

For this data BIC chooses VEV for 2 clusters while the right number of classes, given in
column "Species"

My second question is: when model-based clustering (for which data sets, are there any special type of data) is better than k-means (kmeans), hierarchical clustering(hclust)?

I am looking forward to hearing from you.       

Best regards, 
              Ewa


-----------------------------------------
Rozdajemy bilety na koncert
wi?cej na >> http://link.interia.pl/f1ae9


From Mark.Leeds at morganstanley.com  Thu Jul  5 16:31:58 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 5 Jul 2007 10:31:58 -0400
Subject: [R] Question for svm function in e1071
In-Reply-To: <e2a7c72e2133f.468c7524@optonline.net>
References: <e2a7c72e2133f.468c7524@optonline.net>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401957494@NYWEXMB23.msad.ms.com>

adschai : this isn't particularly helpful but when I am using a
function from a package called xxx that
I have little knowledge about, I take the source as is and create my own
function out of
It called my.xxx and then put print statements
Inside it to see what's going on. This is probably an extremely kludgy
way of dealing with that problem
But it is a way. I'm not R expert enough to know any other way but I
would
Be interested if you get a better private reply. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
adschai at optonline.net
Sent: Thursday, July 05, 2007 12:36 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Question for svm function in e1071

Hi,

Sorry that I have many questions today. I am using svm function on about
180,000 points of training set. It takes very long time to run. However,
I would like it to spit out something to make sure that the run is not
dead in between.  Would you please suggest anyway to do so? 

And is there anyway to speed up the performance of this svm function?
Thank you.

- adschai

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From xh.along at gmail.com  Thu Jul  5 16:36:01 2007
From: xh.along at gmail.com (along zeng)
Date: Thu, 5 Jul 2007 22:36:01 +0800
Subject: [R] Levene Test with R
Message-ID: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>

Hi All,
     is there Levene' test in R ? If not ,Could you give me some
advice about Levene test with R?
    Thanks a lot! I am waiting for yours.


From wwwhsd at gmail.com  Thu Jul  5 16:44:24 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 5 Jul 2007 11:44:24 -0300
Subject: [R] Levene Test with R
In-Reply-To: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
References: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
Message-ID: <da79af330707050744q308c837v31a14d8b36f54028@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/2c604536/attachment.pl 

From tlumley at u.washington.edu  Thu Jul  5 16:48:39 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 5 Jul 2007 07:48:39 -0700 (PDT)
Subject: [R] Is it a bug ?
In-Reply-To: <000601c7bee9$e5ba7cd0$47544ea0@pedrak>
References: <000601c7bee9$e5ba7cd0$47544ea0@pedrak>
Message-ID: <Pine.LNX.4.64.0707050743580.29705@homer23.u.washington.edu>

On Thu, 5 Jul 2007, Giuseppe PEDRAZZI wrote:
> I am using R 2.5.0,  windows XP - italian language.
>
> I was perfoming some calculation on fractional exponential and
> I found a strange behaviour. I do not know if it is really a bug, but I would expect
> a different answer from R.
>
> I was trying the following :
>
> x <- seq(-3,3, by =0.1)
> n <- 2.2
> y <- exp(-x^n)
>
> well, the y vector contains (NaN for all negative value of x)

Yes. Non-integer powers of negative numbers are undefined (unless you use 
complex numbers).

> but if you ask for single value calculation like
>
> y <- exp(-(-3)^2.2) or
>
> y <- exp(-(-2.9)^2.2)
>
> the answer is correct.

I get NaN for both of these.  Perhaps you mean exp(-2.9^2.2)? This gives 
a valid answer, but that is because it is exp(-(2.9^2.2)) not 
exp((-2.9)^2.2)

> It seem it does not make the calculation in vector form.
>
> I got the same behaviour (NaN)  in a for loop
>
>> for(i in 1:length(x)) y[i]=exp(x[i]^n)
>> y
> [1]           NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN
> [10]          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN
> [19]          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN          NaN
> [28]          NaN          NaN          NaN     1.000000     1.006330     1.029416     1.073302     1.142488     1.243137
> [37]     1.384082     1.578166     1.844237     2.210260     2.718282     3.432491     4.452553     5.936068     8.137120
> [46]    11.473746    16.648415    24.867680    38.251295    60.611092    98.967689   166.572985   289.077778   517.425935
> [55]   955.487320  1820.793570  3581.521323  7273.674928 15255.446778 33050.861013 73982.100407
>>
>
> Is it strange or did I miss something ?

You missed something. It is not clear what you missed because some of your 
examples do not give the answer you say they give.

 	-thomas


From ccleland at optonline.net  Thu Jul  5 16:48:44 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 05 Jul 2007 10:48:44 -0400
Subject: [R] Levene Test with R
In-Reply-To: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
References: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
Message-ID: <468D04CC.9020800@optonline.net>

along zeng wrote:
> Hi All,
>      is there Levene' test in R ? If not ,Could you give me some
> advice about Levene test with R?
>     Thanks a lot! I am waiting for yours.

  Did you try RSiteSearch("levene", restrict="function"), which points
to a funtion in the car package?

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From mark.lyman at gmail.com  Thu Jul  5 16:53:37 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Thu, 5 Jul 2007 14:53:37 +0000 (UTC)
Subject: [R] Adding points to a wireframe with conditioning variable
Message-ID: <loom.20070705T164554-595@post.gmane.org>

I would like to add points to a wireframe but with a conditioning variable. I 
found a solution for this without a conditioning variable here, 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/65321.html. Does anyone know 
how to plot a wireframe conditioned on a variable and add the points 
conditioned on the same variable, similar to the solution at the link above?


From tomas.goicoa at unavarra.es  Thu Jul  5 16:56:37 2007
From: tomas.goicoa at unavarra.es (Tomas Goicoa)
Date: Thu, 05 Jul 2007 16:56:37 +0200
Subject: [R] Levene Test with R
In-Reply-To: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.co
 m>
References: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
Message-ID: <20070705145616.6D7A619E604@cartero1.unavarra.es>


Hi,

  library(car)
  ?levene.test











At 16:36 5/7/2007, along zeng wrote:
>Hi All,
>      is there Levene' test in R ? If not ,Could you give me some
>advice about Levene test with R?
>     Thanks a lot! I am waiting for yours.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From Mark.Leeds at morganstanley.com  Thu Jul  5 16:59:35 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Thu, 5 Jul 2007 10:59:35 -0400
Subject: [R] converting list to an array
In-Reply-To: <755261CA22782948B1C42ACDC83912A104614119@NYWEXMB27.msad.ms.com>
References: <755261CA22782948B1C42ACDC83912A104614119@NYWEXMB27.msad.ms.com>
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401957497@NYWEXMB23.msad.ms.com>

if by array, you just mean vector, then the following would work .

stack(myList)

If you want to take off the names of myList that get put in the second
column

stack(myList)[,-2,drop=FALSE]



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aydemir, Zava
(FID)
Sent: Thursday, July 05, 2007 8:25 AM
To: r-help at stat.math.ethz.ch
Subject: [R] converting list to an array

Hi,
 
I have a list myList (see below) that consists of id's $'7',....,$'10'
and each id has an individual array, the length of which can vary from
id to id.
 
How can I convert this list into an array, ie. stacking the individual
arrays into one large array?
 
 
Thanks
 
Zava
 
 
myList:
 
$`7`
[1] 20050201 20050301 20050401 20050501
 
$`8`
[1] 20050201 20050301 20050401 20050501
 
$`9`
[1] 20050201 20050301 20050401 20050501
 
$`10`
[1] 20050101 20050201 20050301 20050401 20050501
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to
buy/se...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From xh.along at gmail.com  Thu Jul  5 17:08:51 2007
From: xh.along at gmail.com (along zeng)
Date: Thu, 5 Jul 2007 23:08:51 +0800
Subject: [R] Levene Test with R
In-Reply-To: <20070705145616.6D7A619E604@cartero1.unavarra.es>
References: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
	<20070705145616.6D7A619E604@cartero1.unavarra.es>
Message-ID: <21add9100707050808s1cdae5f5l9e9a0453112292d0@mail.gmail.com>

I got it!
Thank all of you,Sorry I am a freshman of R.

2007/7/5, Tomas Goicoa <tomas.goicoa at unavarra.es>:
>
> Hi,
>
>   library(car)
>   ?levene.test
>
>
>
>
>
>
>
>
>
>
>
> At 16:36 5/7/2007, along zeng wrote:
> >Hi All,
> >      is there Levene' test in R ? If not ,Could you give me some
> >advice about Levene test with R?
> >     Thanks a lot! I am waiting for yours.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
>


From xh.along at gmail.com  Thu Jul  5 17:09:29 2007
From: xh.along at gmail.com (along zeng)
Date: Thu, 5 Jul 2007 23:09:29 +0800
Subject: [R] Levene Test with R
In-Reply-To: <468D04CC.9020800@optonline.net>
References: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
	<468D04CC.9020800@optonline.net>
Message-ID: <21add9100707050809n5f9cac08y2772617947e12b72@mail.gmail.com>

I got it!
Thank all of you,Sorry I am a freshman of R.

2007/7/5, Chuck Cleland <ccleland at optonline.net>:
> along zeng wrote:
> > Hi All,
> >      is there Levene' test in R ? If not ,Could you give me some
> > advice about Levene test with R?
> >     Thanks a lot! I am waiting for yours.
>
>   Did you try RSiteSearch("levene", restrict="function"), which points
> to a funtion in the car package?
>
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>
>


From elyakhlifi_mustapha at yahoo.fr  Thu Jul  5 17:35:11 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Thu, 5 Jul 2007 15:35:11 +0000 (GMT)
Subject: [R] sink() and source()
Message-ID: <752418.57479.qm@web27506.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/ecf1c6d2/attachment.pl 

From supton at referentia.com  Thu Jul  5 17:40:20 2007
From: supton at referentia.com (Stephen C. Upton)
Date: Thu, 05 Jul 2007 11:40:20 -0400
Subject: [R] select data from large CSV file
In-Reply-To: <8299.193.183.79.7.1183636450.squirrel@webmail.sys.kth.se>
References: <8299.193.183.79.7.1183636450.squirrel@webmail.sys.kth.se>
Message-ID: <468D10E4.1070306@referentia.com>

Hi Lars,

I haven't tried this, but I believe there were a couple of messages on 
the list recently on reading large files that basically used scan with 
connections, and reading in by blocks.

see ?scan, ?connections

HTH
steve

Lars Modig wrote:
> Hello
>
>
> I?ve got a large CSV file (>500M) with statistical data. It?s devided in
> 12 columns and I don?t know how many lines.
> The second column is the date and the second is a unique code for the
> location, the rest is (lets say different whether data.  See example
> below.
> 070704, 25, --,--,--,temperature, 22, --,--,30, 20,Y
> 070705, 25, --,--,--,temperature, 22, --,--,30, 20,Y
> 070705, 25, --,--,--,pressure, 1200, --,--,1000, 1100,N
> 070705, 26, --,--,--,temperature, 22, --,--,30, 20,Y
> ?
> First I tried with data <- read.csv. and of course the memory got full.
> Then I found in the archive that you could use scan. So then I wrote the
> following lines below to search for location and store one location with
> all different data in one variable.
>
> # collect the different pnc's
>  b=2                                        #compare from second number
>  alike=TRUE                                 #Dim alike like a boolean
>  stored = 910286609                         #first number is known
>   for(i in 1: 100){                         #start counting and scaning
>      data_final <- matrix(unlist(scan("C:/Documents and
> Settings/modiglar/Desktop/temp/et.csv",sep="," ,
> what=list("","","","","","","","","","","",""), skip=i ,
> n=12)),ncol=12, byrow=TRUE)
>
>
>       a=1                                     #compare from the 1:th stored
>       while( a < b){                          #---
>                                               #
>         if(as.numeric(data_final[2] != stored[a])) #compare
>           { a=a+1                                  #
>           alike=FALSE  }                           #
>         else{                                      #
>            alike=TRUE                              #
>            break }                                 #
>       }                                            # ---
>
>       if (alike==FALSE){                           #
>          stored[b]=as.numeric(data_final[2])       # Store new data
>          b=b+1                                     #
>       }
>   }
>
> #------------------------------------------------------------
> # save 1 pnc at the time
> d=1
> saved_data = 1:1200 ; dim(saved_data) <- c(12,100)
> save_data_nr = 1                               #Stored number
>   for(i in 1: 100){                            #start counting and scaning
>      data_final <- matrix(unlist(scan("C:/Documents and
> Settings/modiglar/Desktop/temp/et.csv",sep="," ,
> what=list("","","","","","","","","","","",""), skip=i ,
> n=12)),ncol=12, byrow=TRUE)
>
>
>       if(as.numeric(data_final[2] == stored[save_data_nr])) #compare
>         { saved_data[,d] <-  matrix(unlist(data_final),ncol=12,
> byrow=TRUE)  #Store new data
>          d=d+1 }                                         #
>                                                          #
>                                                          #
>  }
> As you can see I?m not so familiar with R, and therefore I have probably
> done this the wrong way.
>
> As I understand when running this, is that scan opens up the file count
> down to the line that should be read and read it, then closing the file
> again. So when I?m starting to come to line number at 10000 then it
> starting to take time. I let the computer run over night, but it was still
> far from finished when I stopped the loop.
>
> So how should I do this? Maybe I also need to sort on the date, and that
> is hopefully in order so then you should be able to cut the file every
> time you hit a new month but that will also take time if I do it like
> this.
>
> Thank you for your help in advance.
>
> Lars
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From david.meyer at wu-wien.ac.at  Thu Jul  5 17:58:10 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Thu, 05 Jul 2007 17:58:10 +0200
Subject: [R] Question about framework to weighting different classes in
 SVM
Message-ID: <468D1512.4070605@wu-wien.ac.at>

Adschai:

here is an example for class.weights (isn't it on the help page?):

      data(iris)
      i2 <- iris
      levels(i2$Species)[3] <- "versicolor"
      summary(i2$Species)
      wts <- 100 / table(i2$Species)
      wts
      m <- svm(Species ~ ., data = i2, class.weights = wts)

Cheers,
David


From afshart at exchange.sba.miami.edu  Thu Jul  5 18:17:55 2007
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Thu, 5 Jul 2007 12:17:55 -0400
Subject: [R] summarizing dataframe at variable/factor levels
In-Reply-To: <mailman.11.1183111206.10613.r-help@stat.math.ethz.ch>
Message-ID: <6BCB4D493A447546A8126F24332056E8063E886E@school1.business.edu>


All,

Is there an efficient way to apply say "mean" or "median" to a dataframe

according to say all combinations of two variables in the dataframe?
Below is a simple example and the outline of a "manual" solution that
will work but is not very efficient
(could also generalize this to a function).  Searched the archives and
docs but didn't see anything close to this question.

Cheers,
dave

dat.ex = data.frame(  rep(c(1:6), each=6), c(rnorm(12), rnorm(12, 1),
rnorm(12, 2)), rnorm(36, 5), rep(c(1:6), 6),
rep(c("Drug1", "Drug2", "Placebo"), each=12) )
names(dat.ex) = c("patient.no", "outcome", "x", "time", "drug")

mean of first 2 time pts on Drug1:
mean.time.1.drug.1 = mean( dat.ex[dat.ex$time==1 & dat.ex$drug=="Drug1",
c(2,3)])
mean.time.2.drug.1 = mean( dat.ex[dat.ex$time==2 & dat.ex$drug=="Drug1",
c(2,3)])

dat.ex.reduced = as.data.frame(rbind(mean.time.1.drug.1,
mean.time.2.drug.1))
dat.ex.reduced$Drug = c("Drug1", "Drug1")  ## add back Drug variable and
time variable
dat.ex.reduced$time = c(1,2)


From Greg.Snow at intermountainmail.org  Thu Jul  5 18:19:15 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 5 Jul 2007 10:19:15 -0600
Subject: [R] sink() and source()
In-Reply-To: <752418.57479.qm@web27506.mail.ukl.yahoo.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAAD05C@LP-EXCHVS07.CO.IHC.COM>

I don't know what is causing your problem,  But if you goal is to
produce html then you may want to look at the R2HTML package.  It may do
what you want without using sink.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> elyakhlifi mustapha
> Sent: Thursday, July 05, 2007 9:35 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] sink() and source()
> 
> hello,
> I have  a problem running a R script actually I'm using 
> source() and sink() and it doesn't work
> 
> source("T:/agents/melyakhlifi/R/essai_rep.r")
> 
> to execute a file and the file contain
> 
> sink("T:/agents/melyakhlifi/R/sortie.html")
> cat("<html><body><pre>\n")
> matrix.merge2
> cat("</pre></body></html>\n")
> sink()
> 
> 
> I don't understand why when I execute just the syntax with 
> sink() it work but in using source() it doesn't work thanks
> 
> 
>       
> ______________________________________________________________
> _______________ 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Thu Jul  5 18:30:17 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Jul 2007 12:30:17 -0400
Subject: [R] Loop and function
In-Reply-To: <11446873.post@talk.nabble.com>
References: <11443955.post@talk.nabble.com>
	<522166.61148.qm@web39712.mail.mud.yahoo.com>
	<11445807.post@talk.nabble.com>
	<644e1f320707050611y762ac236u354b3be736985db4@mail.gmail.com>
	<11446873.post@talk.nabble.com>
Message-ID: <644e1f320707050930m505b353cxb5e7be2a83e6b8d5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/406c0c41/attachment.pl 

From price_ja at hotmail.com  Thu Jul  5 18:31:14 2007
From: price_ja at hotmail.com (Jim Price)
Date: Thu, 5 Jul 2007 09:31:14 -0700 (PDT)
Subject: [R] summarizing dataframe at variable/factor levels
In-Reply-To: <6BCB4D493A447546A8126F24332056E8063E886E@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8063E886E@school1.business.edu>
Message-ID: <11450001.post@talk.nabble.com>


my.data <- data.frame(
	trts <- rep(c('Drug 1','Drug2'), each = 10),
	doses <- rep(c('Low dose','High dose'), 10),
	resp <- rnorm(20)
)


tapply(my.data$resp, list(my.data$trts, my.data$doses), mean)




Jim





Afshartous, David wrote:
> 
> 
> All,
> 
> Is there an efficient way to apply say "mean" or "median" to a dataframe
> 
> according to say all combinations of two variables in the dataframe?
> Below is a simple example and the outline of a "manual" solution that
> will work but is not very efficient
> (could also generalize this to a function).  Searched the archives and
> docs but didn't see anything close to this question.
> 
> Cheers,
> dave
> 
> dat.ex = data.frame(  rep(c(1:6), each=6), c(rnorm(12), rnorm(12, 1),
> rnorm(12, 2)), rnorm(36, 5), rep(c(1:6), 6),
> rep(c("Drug1", "Drug2", "Placebo"), each=12) )
> names(dat.ex) = c("patient.no", "outcome", "x", "time", "drug")
> 
> mean of first 2 time pts on Drug1:
> mean.time.1.drug.1 = mean( dat.ex[dat.ex$time==1 & dat.ex$drug=="Drug1",
> c(2,3)])
> mean.time.2.drug.1 = mean( dat.ex[dat.ex$time==2 & dat.ex$drug=="Drug1",
> c(2,3)])
> 
> dat.ex.reduced = as.data.frame(rbind(mean.time.1.drug.1,
> mean.time.2.drug.1))
> dat.ex.reduced$Drug = c("Drug1", "Drug1")  ## add back Drug variable and
> time variable
> dat.ex.reduced$time = c(1,2)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/summarizing-dataframe-at-variable-factor-levels-tf4030788.html#a11450001
Sent from the R help mailing list archive at Nabble.com.


From deepayan.sarkar at gmail.com  Thu Jul  5 18:42:22 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Thu, 5 Jul 2007 09:42:22 -0700
Subject: [R] Lookups in R
In-Reply-To: <644e1f320707050444t37fc21b1g84a3a7470b6869e1@mail.gmail.com>
References: <11435994.post@talk.nabble.com> <468BF958.9060102@biostat.ku.dk>
	<6phy7hv3kdl.fsf@gopher4.fhcrc.org>
	<eb555e660707041715j4e0906f2i93e03fbb77104284@mail.gmail.com>
	<1183629380.13417.1198603635@webmail.messagingengine.com>
	<644e1f320707050444t37fc21b1g84a3a7470b6869e1@mail.gmail.com>
Message-ID: <eb555e660707050942h643be1a1ua205a51930966c34@mail.gmail.com>

On 7/5/07, jim holtman <jholtman at gmail.com> wrote:
> You are getting two very different results in what you are comparing.
>
> > system.time(lapply(1:10^4, mean))
>   user  system elapsed
>   1.31    0.00    1.31
> is returning a list with 10,000 values in it.  It is taking time to allocate
> the space and such.
>
> > system.time(for(i in 1:10^4) mean(i))
>   user  system elapsed
>   0.33    0.00    0.32
> is just returning a single value (mean(10^4)) and is not having to allocate
> space and setup the structure for a list.  Typically you use 'lapply' not
> only for 'looping', but more importantly returning the values associated
> with the processing.

The point still holds:

> system.time(lapply(1:10^4, mean))
   user  system elapsed
  3.748   2.404   6.161
> system.time({ a = numeric(10^4); for (i in 1:10^4) a[i] = mean(i) })
   user  system elapsed
  0.716   0.004   0.720

To really get rid of the for loop, you need to move the loop to pure C
code, e.g.

> system.time(rowMeans(matrix(1:10^4, ncol = 1)))
   user  system elapsed
  0.004   0.000   0.004

Sometimes you can do this using functions available in R, e.g. using
tapply() in your original question and rowMeans() in this example.
Sometimes you cannot, and the only way to gain efficiency is to write
custom C code (we do not have enough information to decide which is
the case in your real example, since we don't know what it is).

-Deepayan

> On 7/5/07, Michael Frumin <michael at frumin.net> wrote:
> >
> > the problem I have is that userid's are not just sequential from
> > 1:n_users.  if they were, of course I'd have made a big matrix that was
> > n_users x n_fields and that would be that.  but, I think what I cando is
> > just use the hash to store the index into the result matrix, nothing
> > more. then the rest of it will be easy.
> >
> > but please tell me more about eliminating loops.  In many cases in R I
> > have used lapply and derivatives to avoid loops, but in this case they
> > seem to give me extra overhead simply by the generation of their result
> > lists:
> >
> > > system.time(lapply(1:10^4, mean))
> >   user  system elapsed
> >   1.31    0.00    1.31
> > > system.time(for(i in 1:10^4) mean(i))
> >   user  system elapsed
> >   0.33    0.00    0.32
> >
> >
> > thanks,
> > mike
> >
> >
> > > I don't think that's a fair comparison--- much of the overhead comes
> > > from the use of data frames and the creation of the indexing vector. I
> > > get
> > >
> > > > n_accts <- 10^3
> > > > n_trans <- 10^4
> > > > t <- list()
> > > > t$amt <- runif(n_trans)
> > > > t$acct <- as.character(round(runif(n_trans, 1, n_accts)))
> > > > uhash <- new.env(hash=TRUE, parent=emptyenv(), size=n_accts)
> > > > for (acct in as.character(1:n_accts)) uhash[[acct]] <- list(amt=0,
> > n=0)
> > > > system.time(for (i in seq_along(t$amt)) {
> > > +     acct <- t$acct[i]
> > > +     x <- uhash[[acct]]
> > > +     uhash[[acct]] <- list(amt=x$amt + t$amt[i], n=x$n + 1)
> > > + }, gcFirst = TRUE)
> > >    user  system elapsed
> > >   0.508   0.008   0.517
> > > > udf <- matrix(0, nrow = n_accts, ncol = 2)
> > > > rownames(udf) <- as.character(1:n_accts)
> > > > colnames(udf) <- c("amt", "n")
> > > > system.time(for (i in seq_along(t$amt)) {
> > > +     idx <- t$acct[i]
> > > +     udf[idx, ] <- udf[idx, ] + c(t$amt[i], 1)
> > > + }, gcFirst = TRUE)
> > >    user  system elapsed
> > >   1.872   0.008   1.883
> > >
> > > The loop is still going to be the problem for realistic examples.
> > >
> > > -Deepayan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>


From dmca at ucla.edu  Thu Jul  5 18:45:34 2007
From: dmca at ucla.edu (D L McArthur)
Date: Thu, 5 Jul 2007 16:45:34 +0000 (UTC)
Subject: [R] summarizing dataframe at variable/factor levels
References: <mailman.11.1183111206.10613.r-help@stat.math.ethz.ch>
	<6BCB4D493A447546A8126F24332056E8063E886E@school1.business.edu>
Message-ID: <loom.20070705T184245-799@post.gmane.org>

Afshartous, David <afshart <at> exchange.sba.miami.edu> writes:

> All,
> Is there an efficient way to apply say "mean" or "median" to a dataframe
> according to say all combinations of two variables in the dataframe?
> ..[snip]..
> 
See function summaryBy in package doBy


From ggrothendieck at gmail.com  Thu Jul  5 18:47:20 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 5 Jul 2007 12:47:20 -0400
Subject: [R] summarizing dataframe at variable/factor levels
In-Reply-To: <6BCB4D493A447546A8126F24332056E8063E886E@school1.business.edu>
References: <mailman.11.1183111206.10613.r-help@stat.math.ethz.ch>
	<6BCB4D493A447546A8126F24332056E8063E886E@school1.business.edu>
Message-ID: <971536df0707050947l1461d6ebvc60af2e3bd7360c7@mail.gmail.com>

Try this:

aggregate(dat.ex[2:3], dat.ex[4:5], mean)


On 7/5/07, Afshartous, David <afshart at exchange.sba.miami.edu> wrote:
>
> All,
>
> Is there an efficient way to apply say "mean" or "median" to a dataframe
>
> according to say all combinations of two variables in the dataframe?
> Below is a simple example and the outline of a "manual" solution that
> will work but is not very efficient
> (could also generalize this to a function).  Searched the archives and
> docs but didn't see anything close to this question.
>
> Cheers,
> dave
>
> dat.ex = data.frame(  rep(c(1:6), each=6), c(rnorm(12), rnorm(12, 1),
> rnorm(12, 2)), rnorm(36, 5), rep(c(1:6), 6),
> rep(c("Drug1", "Drug2", "Placebo"), each=12) )
> names(dat.ex) = c("patient.no", "outcome", "x", "time", "drug")
>
> mean of first 2 time pts on Drug1:
> mean.time.1.drug.1 = mean( dat.ex[dat.ex$time==1 & dat.ex$drug=="Drug1",
> c(2,3)])
> mean.time.2.drug.1 = mean( dat.ex[dat.ex$time==2 & dat.ex$drug=="Drug1",
> c(2,3)])
>
> dat.ex.reduced = as.data.frame(rbind(mean.time.1.drug.1,
> mean.time.2.drug.1))
> dat.ex.reduced$Drug = c("Drug1", "Drug1")  ## add back Drug variable and
> time variable
> dat.ex.reduced$time = c(1,2)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gvrocha at stat.Berkeley.EDU  Thu Jul  5 18:48:54 2007
From: gvrocha at stat.Berkeley.EDU (Guilherme Veiga da Rocha)
Date: Thu, 5 Jul 2007 11:48:54 -0500
Subject: [R] Information on objects of "class by"
Message-ID: <7CFF2F2B-4F18-4E90-A43C-8BBD2060DDE1@stat.berkeley.edu>

	Hi all,
	
	Does anyone know where to find more information on the "by" class?
	
	OR
	
	Does anyone know how to coerce an object of the "by" class into a  
data.frame containing the results of FUN and the values of the  
grouping variables?
	
	OR
	
	Does anyone know how to do the following:
	
	I have a big table with the results of many replications of an  
experiment and I want to summarize the medians according to the  
experimental parameters used.
	Columns 1-6 index the parameters used in that particular replication.
	Column 7 is just an index to the replication number.
	Column 8 is the variable I want to summarize
	
	I did:
	x=by(model.errors[, 8], model.errors[, 1:6], median)
	
	I now have an object of class "by" that seems to contain everything  
I need.
	If I just output "x" to the screen, I see all I need: the median for  
each unique combination of values in model.errors[, 1:6].
	However, I only know how to access the information on the medians  
themselves by getting x[i] where i is the index of the median I want.
	
	The problem is:
	- How do I retrieve the combination of values in model.errors[, 1:6]  
for a given index "i".
	OR
	- How do I determine the correct index "i" for a particular  
combination of the values in model.errors[, 1:6]?
	
	Just to help the thinking: I bypassed this problem for the mean by  
using lm and predict.
	I could in principle do the same using the L1 loss instead of the L2  
loss in lm by I do not know how to do that...
	This seems a rather convoluted solution for a problem "by" is meant  
to solve easily...
	
	Any help is appreciated.
	
	Thanks a lot,
	
	Guilherme Rocha


From mike.prager at noaa.gov  Thu Jul  5 19:04:07 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Thu, 05 Jul 2007 13:04:07 -0400
Subject: [R] The R Book by M. J. Crawley
References: <1EB58414BAB4014DB2C3E289FDF55FBB019A0A30@CINMLVEM15.e2k.ad.ge.com>
Message-ID: <7b8q83p3ogen0bsfjj5la23a77vm39jmkh@4ax.com>

"Pietrzykowski, Matthew (GE, Research)"
<pietrzyk at research.ge.com> wrote:

> Hello all-
> 
> I would appreciate any guidance that can be provided.  
>I am new to R and am using it exclusively in a statistics 
> program I am undertaking that mainly references
> Minitab.  My focus is on data modeling and further 
> more multivariate data analysis [...]
> I I am looking for a reference that has sound statistical
> foundations with relevant R commands as well as 
> multivariate support.  I saw the new book,
> "The R Book", by Michael J. Crawley and wanted to 
> know what R users thoughts of it.

I can't comment on The R Book, as I haven't seen it. This is to
point out some other references for your consideration.  In
order of most technical to most relaxed:

The standard reference for many R users is "Modern Applied
Statistics with S" by Venables and Ripley, two important
contributors to R.  This has a language introduction and a great
variety of statistical material in its 500 pages. Though I
haven't read every word, I would not for a nanosecond doubt its
"sound statistical foundations."  It seems to me that every R
user would benefit from having MASS (as it's called) on his
shelf.

More relaxed in presentation but still with some multivariate
coverage is "Data Analysis and Graphics using R," by Maindonald
and Braun, also names quite familiar to most R users. This is
more typical of a introductory statistics textbook, and shorter
(about 350 p.).

Even more relaxed but with less breadth of statistical topics is
"Introductory Statistics with R" by Dalgaard, yet another
familiar contributor to R.  This is an excellent introductory
book.  After I had been using R for 5 years, I bought a copy and
learned several good things immediately. About 270 p.

You may want to examine those (along with Crawley's) before
settling on the what you want to buy.

I hope that helps.

Mike

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From jpfededa at gmail.com  Thu Jul  5 19:29:42 2007
From: jpfededa at gmail.com (Juan Pablo Fededa)
Date: Thu, 5 Jul 2007 14:29:42 -0300
Subject: [R] help with vector construction
Message-ID: <7bff68f40707051029p6b3bff0ib0a0e7325674dcb@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/620aaf5a/attachment.pl 

From james.foadi at diamond.ac.uk  Thu Jul  5 19:39:17 2007
From: james.foadi at diamond.ac.uk (James Foadi)
Date: Thu, 5 Jul 2007 18:39:17 +0100
Subject: [R] unexpected result in function valuation
Message-ID: <009201c7bf2b$6acae810$661d17ac@JEEG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/69d4e35c/attachment.pl 

From wwwhsd at gmail.com  Thu Jul  5 20:01:27 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 5 Jul 2007 15:01:27 -0300
Subject: [R] help with vector construction
In-Reply-To: <7bff68f40707051029p6b3bff0ib0a0e7325674dcb@mail.gmail.com>
References: <7bff68f40707051029p6b3bff0ib0a0e7325674dcb@mail.gmail.com>
Message-ID: <da79af330707051101od8ffcf0m6bd76adb24034ed7@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/608b03ea/attachment.pl 

From k.lockyear at ucl.ac.uk  Thu Jul  5 20:02:51 2007
From: k.lockyear at ucl.ac.uk (Kris Lockyear)
Date: Thu, 05 Jul 2007 19:02:51 +0100
Subject: [R] test of CA axis
Message-ID: <200707051802.l65I2srF032366@hypatia.math.ethz.ch>

Dear All,

I am not a statistician, and was wondering if anyone could help me 
with the following.

Greenacre, in his Correspondence Analysis in Practice (1993, p.173) 
gives a method for testing the significance of an axis in CA where:

$\chi^2 = \lambda \times n$ where \lambda is the the eigenvalue for 
the principal axis and n is the number of objects in the 
analysis.  The value for \chi^2 is then compared to a table of 
critical values.  The table in his book is a subset of Table 51 in 
Pearson and Hartley 1976, Biometrica Tables for Statisticians vol II, 
described as "Percentage points of the extreme roots of 
$|\text{\textbf{S}}\Sigma^{-1}-c\text{\textbf{I}}|=0$"

Is there an easy way of doing this test in R?  My main problem in 
that Table 51 only gives values for a maximum of a p=10, \nu = 200 
table and mine are regularly much bigger than that (although it would 
be also nice to be able to put in the figures for lambda, n, p and 
\nu and get the probability back).

Many thanks in advance, Kris Lockyear.

Dr Kris Lockyear
Institute of Archaeology
31-34 Gordon Square
London

phone: 020 7679 4568
email: k.lockyear at ucl.ac.uk


From gerifalte28 at hotmail.com  Thu Jul  5 20:11:53 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 05 Jul 2007 12:11:53 -0600
Subject: [R] probabilty plot
In-Reply-To: <21add9100707040020r3e3378e6o6d34d8fe73192e40@mail.gmail.com>
References: <21add9100707040020r3e3378e6o6d34d8fe73192e40@mail.gmail.com>
Message-ID: <468D3469.8040703@hotmail.com>

Hi Zeng,

I just glanced at the link, but I think this is what you are after:

x=rnorm(1000)#1000 random samples from N(0,1)
y=rlnorm(1000)#1000 random samples from Lognormal(0,1)
fx=ecdf(x)#Empirical cumulative density function of x
fy=ecdf(y)#Empirical cumulative density function of y

#Histogram of data
hist(x)
hist(y)

n=seq(-4,30,.1)#Quantiles to be applied to the F(x)

plot(fx(n), fy(n))#Probability plot

If you are testing data against a known distribution (i.e. Normal) you 
may want to use the distribution function for that distribution (i.e. 
pnorm for the Normal distr) instead of the ecdf since that will provide 
you with an exact answer. i.e.

plot(pnorm(n), fy(n))


Now, QQ plots are usually more useful to compare distributions since 
they are more sensitive to small discrepancies in the data.  Take a look 
at qqplot and qqnorm for examples of how to create qqplots in R

I hope this helps.

Francisco


along zeng wrote:
> Hi all,
>    I am a freshman of R,but I am interested  in it! Those days,I am
> learning   pages on NIST,with url
> http://www.itl.nist.gov/div898/handbook/eda/section3/probplot.htm,
> I am meeting  a problem about probability plot and I don't know how to
> plot a data set with R.
> Could somebody tell me the answer,and a example is the best!  I will
> look forward to your answer.
>  Thank you very much.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gerifalte28 at hotmail.com  Thu Jul  5 20:11:53 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 05 Jul 2007 12:11:53 -0600
Subject: [R] probabilty plot
In-Reply-To: <21add9100707040020r3e3378e6o6d34d8fe73192e40@mail.gmail.com>
References: <21add9100707040020r3e3378e6o6d34d8fe73192e40@mail.gmail.com>
Message-ID: <468D3469.8040703@hotmail.com>

Hi Zeng,

I just glanced at the link, but I think this is what you are after:

x=rnorm(1000)#1000 random samples from N(0,1)
y=rlnorm(1000)#1000 random samples from Lognormal(0,1)
fx=ecdf(x)#Empirical cumulative density function of x
fy=ecdf(y)#Empirical cumulative density function of y

#Histogram of data
hist(x)
hist(y)

n=seq(-4,30,.1)#Quantiles to be applied to the F(x)

plot(fx(n), fy(n))#Probability plot

If you are testing data against a known distribution (i.e. Normal) you 
may want to use the distribution function for that distribution (i.e. 
pnorm for the Normal distr) instead of the ecdf since that will provide 
you with an exact answer. i.e.

plot(pnorm(n), fy(n))


Now, QQ plots are usually more useful to compare distributions since 
they are more sensitive to small discrepancies in the data.  Take a look 
at qqplot and qqnorm for examples of how to create qqplots in R

I hope this helps.

Francisco


along zeng wrote:
> Hi all,
>    I am a freshman of R,but I am interested  in it! Those days,I am
> learning   pages on NIST,with url
> http://www.itl.nist.gov/div898/handbook/eda/section3/probplot.htm,
> I am meeting  a problem about probability plot and I don't know how to
> plot a data set with R.
> Could somebody tell me the answer,and a example is the best!  I will
> look forward to your answer.
>  Thank you very much.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From John.Scillieri at constellation.com  Thu Jul  5 20:18:41 2007
From: John.Scillieri at constellation.com (Scillieri, John)
Date: Thu, 5 Jul 2007 14:18:41 -0400
Subject: [R] R Logging Package
Message-ID: <7B145F644E353D48A1FC22F09D9776770264CADD@EXM-OMF-22.Ceg.Corp.Net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/4129cbef/attachment.pl 

From deepayan.sarkar at gmail.com  Thu Jul  5 20:24:37 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 5 Jul 2007 11:24:37 -0700
Subject: [R] Adding points to a wireframe with conditioning variable
In-Reply-To: <loom.20070705T164554-595@post.gmane.org>
References: <loom.20070705T164554-595@post.gmane.org>
Message-ID: <eb555e660707051124k40e8f89byb48fb4a5b9db4601@mail.gmail.com>

On 7/5/07, Mark Lyman <mark.lyman at gmail.com> wrote:
> I would like to add points to a wireframe but with a conditioning variable. I
> found a solution for this without a conditioning variable here,
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/65321.html. Does anyone know
> how to plot a wireframe conditioned on a variable and add the points
> conditioned on the same variable, similar to the solution at the link above?

Depends on what form you have your points in. Inside a panel function,
packet.number() will give you the packet number in sequential order
(column major order if you think of the conditioning variables as
defining a multiway cross-tabulation), and which.packet() will give
you a vector with the current level of each conditioning variable. You
can use these to extract the appropriate subset of points.

-Deepayan


From jrkrideau at yahoo.ca  Thu Jul  5 20:24:14 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 5 Jul 2007 14:24:14 -0400 (EDT)
Subject: [R] help with vector construction
In-Reply-To: <7bff68f40707051029p6b3bff0ib0a0e7325674dcb@mail.gmail.com>
Message-ID: <654618.2080.qm@web32810.mail.mud.yahoo.com>

Or an alternative to Henrique's if you want to select
all the rows from row 2 up to the  3*n row this may
work.

 n  <- 2
 myvector <- data1[2:(2*n), 3]


--- Juan Pablo Fededa <jpfededa at gmail.com> wrote:

> Hi all,
> 
> I want to make a vector with the third column of a
> matrix, but only for the
> 2+3n rows of the matrix, with n being an entire
> number from 0 to a million.
> How can I do that in an easy way?
> Thanks in advance,
> 
> Juan Pablo
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From greenberg at ucdavis.edu  Thu Jul  5 21:07:10 2007
From: greenberg at ucdavis.edu (Jonathan Greenberg)
Date: Thu, 5 Jul 2007 12:07:10 -0700
Subject: [R] (no subject)
Message-ID: <002001c7bf37$b095eaf0$b0a6eda9@terra.cstars.ucdavis.edu>

I'm trying to hunt down an appropriate kriging package for my specific
application, and I was hoping someone on the R list might have some pointers
--  I'm interested in performing kriging and related spatial interpolations
with one of the R packages, but I need to be able to provide my own
point-to-point distances (e.g. I do not want to use standard between point
distances, as calculated by Euclidean or other similar distance measures).
Is there an R package that *I* can provide the matrix of distances between
every pair of points (e.g. for 10 points, I would have a 10 x 10 matrix of
distances)?  Similarly, if this is possible, can I then provide this package
a vector of distances from each point to an arbitrary unknown location (in
my example, this would be a 1x10 vector) and apply the model to this vector
to predict a single unknown point?  It seems most (if not all) of the
kriging packages I'm finding for R take the x,y,z location as the inputs and
calculate these distances themselves.  Thanks!

Sincerely,

Jonathan Greenberg

--
Jonathan A. Greenberg, PhD
Postdoctoral Scholar
Center for Spatial Technologies and Remote Sensing (CSTARS)
University of California, Davis
One Shields Avenue
The Barn, Room 250N 
Davis, CA 95616
Cell: 415-794-5043
AIM: jgrn307
MSN: jgrn307 at hotmail.com


From greenberg at ucdavis.edu  Thu Jul  5 21:20:53 2007
From: greenberg at ucdavis.edu (Jonathan Greenberg)
Date: Thu, 5 Jul 2007 12:20:53 -0700
Subject: [R] Kriging with precalculated point-to-point distances?
Message-ID: <002101c7bf39$9b18bfc0$b0a6eda9@terra.cstars.ucdavis.edu>

I'm trying to hunt down an appropriate kriging package for my specific
application, and I was hoping someone on the R list might have some pointers
--  I'm interested in performing kriging and related spatial interpolations
with one of the R packages, but I need to be able to provide my own
point-to-point distances (e.g. I do not want to use standard between point
distances, as calculated by Euclidean or other similar distance measures).
Is there an R package that *I* can provide the matrix of distances between
every pair of points (e.g. for 10 points, I would have a 10 x 10 matrix of
distances)?  Similarly, if this is possible, can I then provide this package
a vector of distances from each point to an arbitrary unknown location (in
my example, this would be a 1x10 vector) and apply the model to this vector
to predict a single unknown point?  It seems most (if not all) of the
kriging packages I'm finding for R take the x,y,z location as the inputs and
calculate these distances themselves.  Thanks!

Sincerely,

Jonathan Greenberg

--
Jonathan A. Greenberg, PhD
Postdoctoral Scholar
Center for Spatial Technologies and Remote Sensing (CSTARS)
University of California, Davis
One Shields Avenue
The Barn, Room 250N 
Davis, CA 95616
Cell: 415-794-5043
AIM: jgrn307
MSN: jgrn307 at hotmail.com


From murdoch at stats.uwo.ca  Thu Jul  5 21:31:04 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 05 Jul 2007 15:31:04 -0400
Subject: [R] sink() and source()
In-Reply-To: <752418.57479.qm@web27506.mail.ukl.yahoo.com>
References: <752418.57479.qm@web27506.mail.ukl.yahoo.com>
Message-ID: <468D46F8.9030909@stats.uwo.ca>

On 7/5/2007 11:35 AM, elyakhlifi mustapha wrote:
 > hello,
 > I have  a problem running a R script actually I'm using source() and 
sink() and it doesn't work
 >
 > source("T:/agents/melyakhlifi/R/essai_rep.r")
 >
 > to execute a file and the file contain
 >
 > sink("T:/agents/melyakhlifi/R/sortie.html")
 > cat("<html><body><pre>\n")
 > matrix.merge2
 > cat("</pre></body></html>\n")
 > sink()
 >
 >
 > I don't understand why when I execute just the syntax with sink() it 
work but in using source() it doesn't work
 > thanks

You don't say what is going wrong, but I suspect your problem is that 
you're not printing matrix.merge2.  Listing a variable name on a line by 
itself only causes it to be printed when you're typing at the console, 
not when it's a line in a function or a line sourced from a file.  You 
need to call print() explicitly in those cases.

Duncan Murdoch


From mark.lyman at gmail.com  Thu Jul  5 21:40:50 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Thu, 5 Jul 2007 19:40:50 +0000 (UTC)
Subject: [R] Adding points to a wireframe with conditioning variable
References: <loom.20070705T164554-595@post.gmane.org>
	<eb555e660707051124k40e8f89byb48fb4a5b9db4601@mail.gmail.com>
Message-ID: <loom.20070705T213616-529@post.gmane.org>

Deepayan Sarkar <deepayan.sarkar <at> gmail.com> writes:

> 
> On 7/5/07, Mark Lyman <mark.lyman <at> gmail.com> wrote:
> > I would like to add points to a wireframe but with a conditioning 
variable. I
> > found a solution for this without a conditioning variable here,
> > http://finzi.psych.upenn.edu/R/Rhelp02a/archive/65321.html. Does anyone 
know
> > how to plot a wireframe conditioned on a variable and add the points
> > conditioned on the same variable, similar to the solution at the link 
above?
> 
> Depends on what form you have your points in. Inside a panel function,
> packet.number() will give you the packet number in sequential order
> (column major order if you think of the conditioning variables as
> defining a multiway cross-tabulation), and which.packet() will give
> you a vector with the current level of each conditioning variable. You
> can use these to extract the appropriate subset of points.
> 
> -Deepayan

Thank you Deepayan. I modified the panel function you wrote in the post 
referenced above according to your suggestion. Here is the panel function, I 
came up with


panel.3dwire.points <- function(x, y, z, xlim, ylim, zlim, xlim.scaled,
	ylim.scaled, zlim.scaled, pts, ...)
{
	panel.3dwire(x=x, y=y, z=z, xlim=xlim, ylim=ylim, zlim=zlim,
		xlim.scaled=xlim.scaled, ylim.scaled=ylim.scaled,
		zlim.scaled=zlim.scaled, ...)
	pts.sub <- subset(pts, g==levels(g)[which.packet()])
	xx <- xlim.scaled[1] + diff(xlim.scaled)*(pts.sub$x - xlim[1])/diff
(xlim)
	yy <- ylim.scaled[1] + diff(ylim.scaled)*(pts.sub$y - ylim[1])/diff
(ylim)
	zz <- zlim.scaled[1] + diff(zlim.scaled)*(pts.sub$z - zlim[1])/diff
(zlim)
	panel.3dscatter(x=xx, y=yy, z=zz, xlim=xlim, ylim=ylim, zlim=zlim,
		xlim.scaled=xlim.scaled, ylim.scaled=ylim.scaled,
		zlim.scaled=zlim.scaled, ...)
}

where pts is a dataframe with the three dimensional points (x,y,z) and the 
conditioning variable g.

Mark


From ripley at stats.ox.ac.uk  Thu Jul  5 21:44:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jul 2007 20:44:23 +0100 (BST)
Subject: [R] unexpected result in function valuation
In-Reply-To: <009201c7bf2b$6acae810$661d17ac@JEEG>
References: <009201c7bf2b$6acae810$661d17ac@JEEG>
Message-ID: <Pine.LNX.4.64.0707052022240.23499@gannet.stats.ox.ac.uk>

What value should your formula give when x is a multiple of 2*pi?
You seem to believe 9 is correct but in fact NaN is.

Element 701 of x is approximately but not exactly 2*pi: on my system 
it is about 7*.Machine$double.eps different.  You cannot expect sin(N*pi) 
to be exactly zero for N != 0.


On Thu, 5 Jul 2007, James Foadi wrote:

> Dear all,
> I have a very small script to plot a function. Here it is:
>
> ##########################################
> sinca <- function(N,th)
>
> {
>
> return(sin((N+0.5)*th)/sin(0.5*th))
>
> }
>
> plot_sinca <- function(N)
>
> {
>
> x <- seq(-5*pi,5*pi,by=pi/100)
>
> y <- rep(0,length=length(x))
>
> for (i in 1:length(x))y[i] <- sinca(N,x[i])
>
> plot(x,y,type="l",ylim=c(0,2*N+4))
>
> return(c(x,y))
>
> }
>
> ##########################################
>
> When I load the script and run the function like this:
>
> ###########################################
>> data <- plot_sinca(4)
>> y <- data[1002:2002]
> ###########################################
>
> I notice a spike on the plot which should not be there.
> In fact I have checked and:
> ###########################################
>> y[701]
> [1] 10.07404
>> sinca(4,2*pi)
> [1] 9
> ###########################################
>
> The second result is the correct one. Why, then do
> I get the y[701]=10.07404? This function is not supposed
> to be higher than 9...
>
> Any help is greatly appreciated.
>
> Regards,
>
> J
>
> Dr James Foadi
> Membrane Protein Laboratory
> Diamond Light Source Ltd
> Chilton, Didcot
> Oxfordshire OX11 0DE
> ---
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jholtman at gmail.com  Thu Jul  5 21:44:47 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Jul 2007 15:44:47 -0400
Subject: [R] unexpected result in function valuation
In-Reply-To: <009201c7bf2b$6acae810$661d17ac@JEEG>
References: <009201c7bf2b$6acae810$661d17ac@JEEG>
Message-ID: <644e1f320707051244w79856b85r16b03de3564d024b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/8628dfdb/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Jul  5 21:46:20 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Jul 2007 20:46:20 +0100 (BST)
Subject: [R] pgup/pgdown in R Graphics Window under Linux
In-Reply-To: <OFE1604883.E944131C-ONC125730F.002D0933-C125730F.002D9323@abbott.com>
References: <OFE1604883.E944131C-ONC125730F.002D0933-C125730F.002D9323@abbott.com>
Message-ID: <Pine.LNX.4.64.0707050935270.18325@auk.stats>

On Thu, 5 Jul 2007, Paul Matthias Diderichsen wrote:

> Dear S-users.

This is the help forum for R users ....

> This should be an easy one: How do I change pages on an X11 graphics
> device under linux?
>
> I thought that the page-up/page-down keys were supposed to do the trick,

It is baffling, rather than easy.  What did you find in your homework that 
told you that the X11() device had 'pages' and responded to those keys?

> but the frame (window) seems to be kind of immune to any kind of keyboard
> input. The only reaction I ever see is that the mouse pointer changes to a
> "+" when moved into the frame.

I am unaware of any documentation that says otherwise, so please enlighten 
us with the fruits of your researches.

The windows() device has this capability, but it is not enabled by 
default.  I don't believe any of the third-party graphics devices in 
CRAN package do.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From james.foadi at diamond.ac.uk  Thu Jul  5 21:55:14 2007
From: james.foadi at diamond.ac.uk (James Foadi)
Date: Thu, 5 Jul 2007 20:55:14 +0100
Subject: [R] unexpected result in function valuation
References: <009201c7bf2b$6acae810$661d17ac@JEEG>
	<644e1f320707051244w79856b85r16b03de3564d024b@mail.gmail.com>
Message-ID: <00d801c7bf3e$68614070$661d17ac@JEEG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/bae68e92/attachment.pl 

From albmont at centroin.com.br  Thu Jul  5 22:08:11 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Thu, 5 Jul 2007 18:08:11 -0200
Subject: [R] Me again, about the horrible documentation of tcltk
Message-ID: <20070705200046.M65248@centroin.com.br>

How on Earth can I know what are the arguments of any of the functions of 
the tcl/tk package? I tried hard to find, using all search engines 
available, looking deep into keywords of R, python's tkinter and tcl/tk, but 
nowhere I found anything remotely similar to a help.

For example, what are the possible arguments to tkgetOpenFile?

I know that this works:

library(tcltk)
filename <- tclvalue(tkgetOpenFile(
  filetypes="{{Porn Files} {.jpg}} {{All files} {*}}"))
if (filename != "") cat("Selected file:", filename, "\n")

but, besides filetypes, what are the other arguments to
tkgetOpenFile? I would like to force the files to be sorted by
time, with most recent files coming first (and no, the purpose is
not to use for porn files).

Alberto Monteiro


From Greg.Snow at intermountainmail.org  Thu Jul  5 22:07:47 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 5 Jul 2007 14:07:47 -0600
Subject: [R] help with vector construction
In-Reply-To: <7bff68f40707051029p6b3bff0ib0a0e7325674dcb@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAAD0DE@LP-EXCHVS07.CO.IHC.COM>

One approach is to use the fact that vectors are automatically
replicated to the correct length when subscripting, so you can do
something like:

> my.matrix[ c(FALSE,TRUE,FALSE), 3 ] 

To get every 3rd element starting at the 2nd element, and the 3rd
column.

Hope this helps,


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Juan 
> Pablo Fededa
> Sent: Thursday, July 05, 2007 11:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] help with vector construction
> 
> Hi all,
> 
> I want to make a vector with the third column of a matrix, 
> but only for the
> 2+3n rows of the matrix, with n being an entire number from 0 
> to a million.
> How can I do that in an easy way?
> Thanks in advance,
> 
> Juan Pablo
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From maechler at stat.math.ethz.ch  Thu Jul  5 22:17:50 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 5 Jul 2007 22:17:50 +0200
Subject: [R] Problem/bug with smooth.spline and all.knots=T
In-Reply-To: <20070704125844.GA31772@p15097509.pureserver.info>
References: <20070704125844.GA31772@p15097509.pureserver.info>
Message-ID: <18061.20974.176693.44517@stat.math.ethz.ch>

>>>>> "H" == Hubertus  <stonemonkey at web.de>
>>>>>     on Wed, 4 Jul 2007 14:58:44 +0200 writes:

    H> Dear list,
    H> if I do

    H> smooth.spline(tmpSec, tmpT, all.knots=T)

    H> with the attached data,
Thanks for providing the data via URL (see below)

    H> with the attached data, I get this error-message:
    H> Error in smooth.spline(tmpSec, tmpT, all.knots = T) :
    H> smoothing parameter value too small
    H> If I do
    H> smooth.spline(tmpSec[-single arbitrary number], tmpT[-single arbitrary number], all.knots=T)

    H> it works!

not quite (see below)

I have transformed your reports into something reproducible
(as *all* such R-help  messages should be!):

dFile <- "..../sspl_Hub_data.rda"
## use a file name you can write to

if(file.exists(dFile)) {
    load(dFile)
} else {
    load(url("http://phi.stonemonkey.org/smoothspline.rda"))
    d.tmp <- data.frame(Sec = tmpSec, T = tmpT)
    save(d.tmp, file = dFile)
}
str(d.tmp)
## 'data.frame':	2055 obs. of  2 variables:
##  $ Sec: num  25743 25744 25745 25746 25747 ...
##  $ T  : num  288 288 288 288 288 ...

## Dear list,
## if I do

ssT <- with(d.tmp, smooth.spline(Sec, T, all.knots=TRUE))

## with the attached data, I get this error-message:
##   Error in smooth.spline(tmpSec, tmpT, all.knots = T) :
##         smoothing parameter value too small

## MM: it works "fine" for me on 64-bit, RH5 ("lynne"),
##     I but can confirm the problem on my 32-bit Pentium M notebook:
ssT <- with(d.tmp, smooth.spline(Sec, T, all.knots=TRUE,
                                 control.spar = list(trace = TRUE)))
## gives (on nb-mm):
## sbart (ratio =   1.1399039e-11) iterations; initial tol1 = 3.334042e-05 :
##        spar            GCV      b - a           e  Kind   NEW lspar         crit
##  -------------------------------------------------------------------------------
## -0.35410197 0.000220934951 3.0000e+00           0 GS -- 1.61033e-11   0.00027127
## -0.35410197 0.000220934951 1.8541e+00      1.8541 FP GS 8.47468e-20  1.57859e-05
## -0.79179607 1.57858803e-05 1.1459e+00     -1.1459 FP GS 9.41382e-22    3.555e-12
## -1.06230590 3.55500244e-12 7.0820e-01     -0.7082 FP PI 3.86481e-21  2.08386e-10
## -1.06230590 3.55500244e-12 5.2259e-01    -0.27051 FP PI  1.9073e-21  1.13107e-09
## -1.06230590 3.55500244e-12 4.8014e-01    0.084898 FP GS 5.83319e-23  1.04372e-18
## -1.22949017 1.04371872e-18 4.3769e-01    -0.43769 FP PI 2.30006e-22  4.72251e-15
## -1.22949017 1.04371872e-18 3.5298e-01    -0.16718 FP PI  1.1561e-22  2.37808e-16
## -1.22949017 1.04371872e-18 3.1163e-01    0.082471 FP PI 7.90222e-23  2.03459e-15
## -1.22949017 1.04371872e-18 2.8876e-01    0.041121 FP GS  1.0457e-23  2.03459e-15
##   >>>   1.0457e-23  1.04372e-18
## Error in smooth.spline(Sec, T, all.knots = TRUE, control.spar = list(trace = 2)) :
## 	smoothing parameter value too small

## Where it works, I get
ssT
## Call:
## smooth.spline(x = Sec, y = T, all.knots = TRUE)

## Smoothing Parameter  spar= -1.044387  lambda= 1.266990e-21 (22 iterations)
## Equivalent Degrees of Freedom (Df): 2054.954
## Penalized Criterion: 4.420969e-20


## OTOH, need a relaxed convergence criterion:
ssT. <- with(d.tmp,
             smooth.spline(Sec, T, all.knots=TRUE,
                           control.spar = list(trace = TRUE,
                           tol=1e-4, eps= 0.01)))
ssT.
##-> Df: 2055.163 --- interpolation !!!!

##------> really almost fails to do spline *interpolation* as a
##        limiting case.

In principle that's well know:
the smoothing spline solution tends to an interpolation spline
when lambda -> 0, however the formula (algorithm) that is used
for smoothing spline fitting( given lambda) is not applicable to
the limiting case, lambda=0.

I agree that this is a small remaining flaw of an already
somewhat sophisticated algorithm for determining the smoothing
parameter.
I'm sure we could fix the algorithm to "work" in this case,
however to do the fix in such a way that all other cases remain
returning the exact same solution as now, seems a harder task.
And changing the default behavior of smooth.spline() even if only
slightly, is quite a bit problematic.
I know it, because I did it many many R versions back... 

For you data, see many possibilities;
- don't use all.knots = TRUE
- really use spline interpolation,
  library(splines)
  ?interpSpline 
- use 'df = <something reasonable>' if you want to apply it to
  many datasets

- don't use 'all.knots = TRUE'  or use 'cv=TRUE' or ... :

## What about "proper" crossvalidation instead of GCV (default)
## which I'd recommend anyway on todays fast computers :
ssTg <- with(d.tmp,
             smooth.spline(Sec, T, all.knots=TRUE, cv= TRUE,
                           control.spar = list(trace = TRUE)))

## *does* work (interestingly)
ssTg # hmm, also DF = 2055
plot(T ~ Sec, data = d.tmp, pch = ".")
lines(predict(ssTg), col=2)

## don't use all.knots:
ssTg. <- with(d.tmp,
              smooth.spline(Sec, T, cv= TRUE,
                            control.spar = list(trace = TRUE)))
## *does* work too
ssTg. # DF = 103.8
plot(T ~ Sec, data = d.tmp, pch = ".")
lines(predict(ssTg.), col=2)

sfsmisc::TA.plot(ssTg., labels="o")
sfsmisc::p.ts(residuals(ssTg.))
## hmm, the residuals *do* look like a time-series
acf(residuals(ssTg.))
plot(spectrum(residuals(ssTg.), method = "ar"))

--------------------
Regards,
Maritn Maechler, ETH Zurich


## If I do
## If I do
##   smooth.spline(tmpSec[-single arbitrary number], tmpT[-single arbitrary number], all.knots=T)
## it works!

## MM: *reproducible* code, please!
N <- nrow(d.tmp)
set.seed(1)# or any(?) other
for(n in 1:100) {
    i <- sample(N, 1)
    cat(sprintf("i =%5d : ", i))
    sst.i <- with(d.tmp, smooth.spline(Sec[-i], T[-i], all.knots=TRUE))
    cat(" -> df = ", format(sst.i$df),"\n")
}

## what Hubertus says is not true, already the 3rd example
## also gives error:

## i =  546 :  -> df =  2054.019
## i =  765 :  -> df =  2053.997
## i = 1178 : Error in smooth.spline(Sec[-i], T[-i], all.knots = TRUE) :
## 	smoothing parameter value too small


From jholtman at gmail.com  Thu Jul  5 22:27:23 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Jul 2007 16:27:23 -0400
Subject: [R] unexpected result in function valuation
In-Reply-To: <00d801c7bf3e$68614070$661d17ac@JEEG>
References: <009201c7bf2b$6acae810$661d17ac@JEEG>
	<644e1f320707051244w79856b85r16b03de3564d024b@mail.gmail.com>
	<00d801c7bf3e$68614070$661d17ac@JEEG>
Message-ID: <644e1f320707051327o74eaa747we48548427d32489a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/d0da5698/attachment.pl 

From rvaradhan at jhmi.edu  Thu Jul  5 22:37:00 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 5 Jul 2007 16:37:00 -0400
Subject: [R] unexpected result in function valuation
In-Reply-To: <009201c7bf2b$6acae810$661d17ac@JEEG>
References: <009201c7bf2b$6acae810$661d17ac@JEEG>
Message-ID: <000601c7bf44$3d5c77e0$7c94100a@win.ad.jhu.edu>

The problem is that you are dividing two numbers that are both very small.
Any small imprecision in the denominator creates a big error. 

Here you can re-write your function using a trig identity to get accurate
results:

sinca <- function(N,th) {
#return(sin((N+0.5)* th)/sin(0.5*th))
return( (sin(N*th)/tan(th/2)) + cos(N*th))
}

This function works well, as you had expected.

Ravi. 

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of James Foadi
Sent: Thursday, July 05, 2007 1:39 PM
To: r-help at stat.math.ethz.ch
Subject: [R] unexpected result in function valuation

Dear all,
I have a very small script to plot a function. Here it is:

##########################################
sinca <- function(N,th)

{

return(sin((N+0.5)*th)/sin(0.5*th))

}

plot_sinca <- function(N)

{

x <- seq(-5*pi,5*pi,by=pi/100)

y <- rep(0,length=length(x))

for (i in 1:length(x))y[i] <- sinca(N,x[i])

plot(x,y,type="l",ylim=c(0,2*N+4))

return(c(x,y))

}

##########################################

When I load the script and run the function like this:

###########################################
> data <- plot_sinca(4)
> y <- data[1002:2002]
###########################################

I notice a spike on the plot which should not be there.
In fact I have checked and:
###########################################
> y[701]
[1] 10.07404
> sinca(4,2*pi)
[1] 9
###########################################

The second result is the correct one. Why, then do
I get the y[701]=10.07404? This function is not supposed
to be higher than 9...

Any help is greatly appreciated.

Regards,

J

Dr James Foadi
Membrane Protein Laboratory
Diamond Light Source Ltd
Chilton, Didcot
Oxfordshire OX11 0DE
---

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Thu Jul  5 22:38:35 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 05 Jul 2007 22:38:35 +0200
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <20070705200046.M65248@centroin.com.br>
References: <20070705200046.M65248@centroin.com.br>
Message-ID: <468D56CB.8070703@biostat.ku.dk>

Alberto Monteiro wrote:
> How on Earth can I know what are the arguments of any of the functions of 
> the tcl/tk package? I tried hard to find, using all search engines 
> available, looking deep into keywords of R, python's tkinter and tcl/tk, but 
> nowhere I found anything remotely similar to a help.
>
> For example, what are the possible arguments to tkgetOpenFile?
>
> I know that this works:
>
> library(tcltk)
> filename <- tclvalue(tkgetOpenFile(
>   filetypes="{{Porn Files} {.jpg}} {{All files} {*}}"))
> if (filename != "") cat("Selected file:", filename, "\n")
>
> but, besides filetypes, what are the other arguments to
> tkgetOpenFile? I would like to force the files to be sorted by
> time, with most recent files coming first (and no, the purpose is
> not to use for porn files).
>
>   
man n tk_getOpenFile

or if you are not on Unix/Linux, find it online with Google
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mike.prager at noaa.gov  Thu Jul  5 22:40:18 2007
From: mike.prager at noaa.gov (Mike Prager)
Date: Thu, 05 Jul 2007 16:40:18 -0400
Subject: [R] Me again, about the horrible documentation of tcltk
References: <20070705200046.M65248@centroin.com.br>
Message-ID: <smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>

"Alberto Monteiro" <albmont at centroin.com.br> wrote:

> How on Earth can I know what are the arguments of any of the functions of 
> the tcl/tk package? [...]

My impression is that you as supposed to look in tck/tk manuals.
For example, Googling on

tk tck getopenfile

pointed to this Web page:

http://www.tcl.tk/man/tcl8.5/TkCmd/getOpenFile.htm

Hope that helps.

-- 
Mike Prager, NOAA, Beaufort, NC
* Opinions expressed are personal and not represented otherwise.
* Any use of tradenames does not constitute a NOAA endorsement.


From jmacdon at med.umich.edu  Thu Jul  5 22:45:36 2007
From: jmacdon at med.umich.edu (James MacDonald)
Date: Thu, 05 Jul 2007 16:45:36 -0400
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <20070705200046.M65248@centroin.com.br>
References: <20070705200046.M65248@centroin.com.br>
Message-ID: <468D20E3.47B0.00EE.0@med.umich.edu>

Hi Alberto,

It took me approximately 20 seconds to find all the arguments for this function. Here were the steps I took.

1.) Look at R help page ?tkgetOpenFile
2.) Hmmm. Lots of functions, but little info. But wait, what's this?

Details:

    [snip]

     There are far too many of these commands to describe them and
     their arguments in full.  Please refer to the Tcl/Tk documentation
     for details. With a few exceptions, the pattern is that Tk
     subcommands like 'pack configure' are converted to function names
     like 'tkpack.configure', and Tcl subcommands are like
     'tclfile.dir'.

3.) Type tkgetOpenFile at R prompt.

> tkgetOpenFile
function (...) 
tcl("tk_getOpenFile", ...)
<environment: namespace:tcltk>

4.) Google tk_getOpenFile.
5.) http://www.tcl.tk/man/tcl8.5/TkCmd/getOpenFile.htm 

Best,

Jim


-- 

James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Alberto Monteiro" <albmont at centroin.com.br> wrote:
> How on Earth can I know what are the arguments of any of the functions of 
> the tcl/tk package? I tried hard to find, using all search engines 
> available, looking deep into keywords of R, python's tkinter and tcl/tk, but 
> 
> nowhere I found anything remotely similar to a help.
> 
> For example, what are the possible arguments to tkgetOpenFile?
> 
> I know that this works:
> 
> library(tcltk)
> filename <- tclvalue(tkgetOpenFile(
>   filetypes="{{Porn Files} {.jpg}} {{All files} {*}}"))
> if (filename != "") cat("Selected file:", filename, "\n")
> 
> but, besides filetypes, what are the other arguments to
> tkgetOpenFile? I would like to force the files to be sorted by
> time, with most recent files coming first (and no, the purpose is
> not to use for porn files).
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code.



**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From kathy at virginia.edu  Thu Jul  5 23:04:52 2007
From: kathy at virginia.edu (Kathy Gerber)
Date: Thu, 05 Jul 2007 17:04:52 -0400
Subject: [R] Question on Rmpi looping
Message-ID: <web-14318517@neon.mail.virginia.edu>

Dear R list,

In the course of learning to work with Rmpi, we are confused about a few 
points.  The following simple program is based on some examples we retrieved 
from the web. Each slave is writing the same output line multiple times (a 
multiple equal to the number of slaves). In other words, the write 
statements are being executed a number of times equal to the number of 
slaves.

I am trying to print out the slave number to a file (once), but it is 
printing it 4 times (since the number of slaves is 4). The code is as 
follows:

# Initialize the Rmpi environment
library("Rmpi")

# We are spawning four slaves
mpi.spawn.Rslaves(nslaves=4)

if (mpi.comm.size() != 5)
     {
     print("Please initialize an MPI cluster of at least 5 processors.")
     print("Then, try again")
     mpi.quit()
     }

.Last <- function()
     {
     if (is.loaded("mpi_initialize"))
         {
         if (mpi.comm.size(1) > 0)
             {
             print("Please use mpi.close.Rslaves() to close slaves.")
             mpi.close.Rslaves()
             }
         print("Please use mpi.quit() to quit R")
         .Call("mpi_finalize")
         }
     }


# Define the base directory
basedir <- "/home/user/directory/"

fcnfishtest <- function()
   {
          wrout = paste(basedir, paste("processor",my_rank, sep=""), sep="")
          write(my_rank, wrout, append=TRUE)
   }
################## END OF SLAVES ##################


# We're in the parent.

#Have each slave get its rank
  mpi.bcast.cmd(my_rank <- mpi.comm.rank())
  mpi.bcast.Robj2slave(basedir)


# Send the function to the slaves
   
  mpi.bcast.Robj2slave(fcnfishtest)

# Call the function

x<- mpi.remote.exec(fcnfishtest())
x

# close slaves and exit
  mpi.close.Rslaves()
  mpi.quit(save = "no")

##### end code


The output is as follows:

file 1:
1
1
1
1
  
file 2:
2
2
2
2

file 3:
3
3
3
3

file 4:
4
4
4
4


We would like to call four slaves with output files like:

file 1:
1

file 2:
2

file 3:
3

file 4:
4


Any help would be greatly appreciated. Thank you!

Kathy Gerber
University of Virginia - ITC - Research Computing Support
kathy at virginia.edu                         434-982-4986


From albmont at centroin.com.br  Thu Jul  5 23:23:58 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Thu, 5 Jul 2007 19:23:58 -0200
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <468D20E3.47B0.00EE.0@med.umich.edu>
References: <20070705200046.M65248@centroin.com.br>
	<468D20E3.47B0.00EE.0@med.umich.edu>
Message-ID: <20070705212128.M50696@centroin.com.br>

James MacDonald wrote:
> 
> 3.) Type tkgetOpenFile at R prompt.
> 
> > tkgetOpenFile
> function (...) 
> tcl("tk_getOpenFile", ...)
> <environment: namespace:tcltk>
> 
> 4.) Google tk_getOpenFile.
> 5.) http://www.tcl.tk/man/tcl8.5/TkCmd/getOpenFile.htm
> 
Thanks, you (all who helped) are so nice. I even incorporated your 
(MacDonald's) suggestion in the R-Wiki, at...
http://wiki.r-project.org/rwiki/doku.php?id=rdoc:tcltk:tkcommands

Alberto Monteiro


From Zava.Aydemir at morganstanley.com  Fri Jul  6 00:53:45 2007
From: Zava.Aydemir at morganstanley.com (Aydemir, Zava (FID))
Date: Thu, 5 Jul 2007 18:53:45 -0400
Subject: [R] generating lower triangular matrix
Message-ID: <755261CA22782948B1C42ACDC83912A104614225@NYWEXMB27.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/346a674b/attachment.pl 

From murdoch at stats.uwo.ca  Fri Jul  6 01:41:25 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 05 Jul 2007 19:41:25 -0400
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <468D56CB.8070703@biostat.ku.dk>
References: <20070705200046.M65248@centroin.com.br>
	<468D56CB.8070703@biostat.ku.dk>
Message-ID: <468D81A5.5070308@stats.uwo.ca>

On 05/07/2007 4:38 PM, Peter Dalgaard wrote:
> Alberto Monteiro wrote:
>> How on Earth can I know what are the arguments of any of the functions of 
>> the tcl/tk package? I tried hard to find, using all search engines 
>> available, looking deep into keywords of R, python's tkinter and tcl/tk, but 
>> nowhere I found anything remotely similar to a help.
>>
>> For example, what are the possible arguments to tkgetOpenFile?
>>
>> I know that this works:
>>
>> library(tcltk)
>> filename <- tclvalue(tkgetOpenFile(
>>   filetypes="{{Porn Files} {.jpg}} {{All files} {*}}"))
>> if (filename != "") cat("Selected file:", filename, "\n")
>>
>> but, besides filetypes, what are the other arguments to
>> tkgetOpenFile? I would like to force the files to be sorted by
>> time, with most recent files coming first (and no, the purpose is
>> not to use for porn files).
>>
>>   
> man n tk_getOpenFile
> 
> or if you are not on Unix/Linux, find it online with Google

Chances are, if Alberto is not on Unix/Linux he is on Windows (assuming 
Mac OSX counts as an *ix), and TCL and TK help files are distributed 
with R.  See RHOME/Tcl/doc.  He'll still have the problem of converting 
TCL/TK documentation conventions into their R equivalents, but I think 
you've given reasonable documentation on how to do that.

I wish we had a good way to refer to these files from the rest of the 
help system.

Duncan Murdoch


>> Alberto Monteiro
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From michael.drescher at ontario.ca  Fri Jul  6 01:59:26 2007
From: michael.drescher at ontario.ca (Drescher, Michael (MNR))
Date: Thu, 5 Jul 2007 19:59:26 -0400
Subject: [R] problem assigning to indexed data frame element
Message-ID: <76D2AA307C39054DBA8BD42DE44E71A4031409FD@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>

Hi All,

Sorry if I ask an obvious thing, I am still new to R ...

I created a data frame of given dimensions to which I gave strings as
column names. I want to write to elements of the data frame by indexing
them with the row number and column name (string). The problem is that I
can read elements from the data frame in this way, but I cannot assign
to elements in this way. Instead, I get the following error message:

Error in Summary.factor(..., na.rm = na.rm) : 
        min not meaningful for factors

Please find the code I used farther below. It would be great if someone
could help me.

Best regards, Michael

PS: Coincidentally, I found this same error message mentioned in another
context (levelplot) as indicating a bug (original bug report PR# 6005 on
Mon, 22 Dec 2003)

------------------------
Michael Drescher
Ontario Forest Research Institute
Ontario Ministry of Natural Resources
1235 Queen St East
Sault Ste Marie, ON, P6A 2E3
Tel: (705) 946-7406
Fax: (705) 946-2030

------------------------

Code:
> sfalls.plot.comp <- matrix(nrow=plot.count, ncol=spec.count, byrow=T)
> colnames(sfalls.plot.comp) <- levels(SPECIES)
### SPECIES, SPP_VOL, & PLOT are columns/variables in a previously read
data file
> sfalls.plot.comp <- data.frame(sfalls.plot.comp)
> attach(sfalls.plot.comp)
> sfalls.plot.comp[is.na(sfalls.plot.comp)] <- 0

> sfalls.plot.comp
  Bf Bw Pj Po Sb
1  0  0  0  0  0
2  0  0  0  0  0

> hh <- 1
> current.spec <- SPECIES[hh]; current.vol <- SPP_VOL[hh]; current.plot
<- PLOT[hh]

> current.spec
[1] Bf
Levels: Bf Bw Pj Po Sb

> current.vol
[1] 2

> current.plot
[1] 1

> sfalls.plot.comp[current.plot,current.spec]
### thus, reading from the data frame in this way (using the column
name/string) works fine
[1] 0

> sfalls.plot.comp[current.plot,current.spec] <- current.vol	### but
assigning in this way does not work
Error in Summary.factor(..., na.rm = na.rm) : 
        min not meaningful for factors

> sfalls.plot.comp[current.plot,1] <- current.vol
### assigning by using the column number instead of the column name of
course does work
> sfalls.plot.comp[current.plot,current.spec]
[1] 2

> sfalls.plot.comp[current.plot,"Bw"] <- current.vol
### as does assigning when replacing 'current.spec' for its assigned
value in quotes, e.g., "Bw"
> sfalls.plot.comp[current.plot,"Bw"]
[1] 2

> sfalls.plot.comp
  Bf Bw Pj Po Sb
1  2  2  0  0  0
2  0  0  0  0  0


From anup_nandialath at yahoo.com  Fri Jul  6 02:04:40 2007
From: anup_nandialath at yahoo.com (Anup Nandialath)
Date: Thu, 5 Jul 2007 17:04:40 -0700 (PDT)
Subject: [R] row index
Message-ID: <287789.89572.qm@web53307.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/bd6ecc8b/attachment.pl 

From deepayan.sarkar at gmail.com  Fri Jul  6 02:05:02 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 5 Jul 2007 17:05:02 -0700
Subject: [R] pgup/pgdown in R Graphics Window under Linux
In-Reply-To: <OFE1604883.E944131C-ONC125730F.002D0933-C125730F.002D9323@abbott.com>
References: <OFE1604883.E944131C-ONC125730F.002D0933-C125730F.002D9323@abbott.com>
Message-ID: <eb555e660707051705i42e1f726q23fae2cd6c407056@mail.gmail.com>

On 7/5/07, Paul Matthias Diderichsen
<paulmatthias.diderichsen at abbott.com> wrote:
> Dear S-users.
> This should be an easy one: How do I change pages on an X11 graphics
> device under linux?
>
> I thought that the page-up/page-down keys were supposed to do the trick,
> but the frame (window) seems to be kind of immune to any kind of keyboard
> input. The only reaction I ever see is that the mouse pointer changes to a
> "+" when moved into the frame.
>
> I issue these commands:

[...]

> > library(lattice)
> > xyplot(speed~dist|speed, data=cars, layout=c(3,3))

If this is your use case, you might be interested in

http://cran.r-project.org/src/contrib/Descriptions/plotAndPlayGTK.html

-Deepayan


From jholtman at gmail.com  Fri Jul  6 02:58:47 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 5 Jul 2007 20:58:47 -0400
Subject: [R] row index
In-Reply-To: <287789.89572.qm@web53307.mail.re2.yahoo.com>
References: <287789.89572.qm@web53307.mail.re2.yahoo.com>
Message-ID: <644e1f320707051758x4175382aj86ad50622dbfdd2c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070705/4494200b/attachment.pl 

From edna.bell01 at gmail.com  Fri Jul  6 03:19:41 2007
From: edna.bell01 at gmail.com (Edna Bell)
Date: Thu, 5 Jul 2007 20:19:41 -0500
Subject: [R]  Rmpi installation
Message-ID: <2d1ebb110707051819m6a9ac7dcx35c41d54426d2d15@mail.gmail.com>

Hi R Gurus.

I'm trying to install the Rmpi package.

Here are the errors:
checking for stdint.h... yes
checking for unistd.h... yes
checking mpi.h usability... no
checking mpi.h presence... no
checking for mpi.h... no
Try to find libmpi or libmpich ...
checking for main in -lmpi... no
libmpi not found. exiting...
ERROR: configuration failed for package 'Rmpi'
** Removing '/home/Desktop/R-2.5.1/library/Rmpi'

The downloaded packages are in
        /tmp/Rtmp90H1Vp/downloaded_packages
Warning messages:
1: installation of package 'rsprng' had non-zero exit status in:
install.packages("Rmpi", depend = T)
2: installation of package 'Rmpi' had non-zero exit status in:
install.packages("Rmpi", depend = T)

I'm thinking that there might be things like "MPI_HOME" or something
like to be set, maybe?

Thanks for any help.
Edna Bell
mailto: edna.bell01 at gmail.com


From edna.bell01 at gmail.com  Fri Jul  6 03:24:02 2007
From: edna.bell01 at gmail.com (Edna Bell)
Date: Thu, 5 Jul 2007 20:24:02 -0500
Subject: [R] The R Book by M. J. Crawley
In-Reply-To: <7b8q83p3ogen0bsfjj5la23a77vm39jmkh@4ax.com>
References: <1EB58414BAB4014DB2C3E289FDF55FBB019A0A30@CINMLVEM15.e2k.ad.ge.com>
	<7b8q83p3ogen0bsfjj5la23a77vm39jmkh@4ax.com>
Message-ID: <2d1ebb110707051824s5ab97dc6s28963ad3da6fd806@mail.gmail.com>

Have you seen "The Basics of S Plus", by Krause and Olson?  It's really good too
.
On 7/5/07, Mike Prager <mike.prager at noaa.gov> wrote:
> "Pietrzykowski, Matthew (GE, Research)"
> <pietrzyk at research.ge.com> wrote:
>
> > Hello all-
> >
> > I would appreciate any guidance that can be provided.
> >I am new to R and am using it exclusively in a statistics
> > program I am undertaking that mainly references
> > Minitab.  My focus is on data modeling and further
> > more multivariate data analysis [...]
> > I I am looking for a reference that has sound statistical
> > foundations with relevant R commands as well as
> > multivariate support.  I saw the new book,
> > "The R Book", by Michael J. Crawley and wanted to
> > know what R users thoughts of it.
>
> I can't comment on The R Book, as I haven't seen it. This is to
> point out some other references for your consideration.  In
> order of most technical to most relaxed:
>
> The standard reference for many R users is "Modern Applied
> Statistics with S" by Venables and Ripley, two important
> contributors to R.  This has a language introduction and a great
> variety of statistical material in its 500 pages. Though I
> haven't read every word, I would not for a nanosecond doubt its
> "sound statistical foundations."  It seems to me that every R
> user would benefit from having MASS (as it's called) on his
> shelf.
>
> More relaxed in presentation but still with some multivariate
> coverage is "Data Analysis and Graphics using R," by Maindonald
> and Braun, also names quite familiar to most R users. This is
> more typical of a introductory statistics textbook, and shorter
> (about 350 p.).
>
> Even more relaxed but with less breadth of statistical topics is
> "Introductory Statistics with R" by Dalgaard, yet another
> familiar contributor to R.  This is an excellent introductory
> book.  After I had been using R for 5 years, I bought a copy and
> learned several good things immediately. About 270 p.
>
> You may want to examine those (along with Crawley's) before
> settling on the what you want to buy.
>
> I hope that helps.
>
> Mike
>
> --
> Mike Prager, NOAA, Beaufort, NC
> * Opinions expressed are personal and not represented otherwise.
> * Any use of tradenames does not constitute a NOAA endorsement.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Alexander.Herr at csiro.au  Fri Jul  6 04:40:49 2007
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Fri, 6 Jul 2007 12:40:49 +1000
Subject: [R] ?replace characters within vector data
Message-ID: <80C7911E901E7E4797B3F88D106CB25D14A48B@exqld2-bne.nexus.csiro.au>

Hi List,

I want  replace characters within a vector. Outside R I could use sed,
but I'd like to automate it in R. For example

vectorx
xxxyyz
xxxyyza
xxxyyzzb

I want to change to: 

vectorx
aaayyz
aaayyza
aaayyzzb

The obvious replace command only deals with whole data entries?
Any hints would be appreciated.

Thanks
Herry


From marc_schwartz at comcast.net  Fri Jul  6 04:48:28 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 05 Jul 2007 21:48:28 -0500
Subject: [R] ?replace characters within vector data
In-Reply-To: <80C7911E901E7E4797B3F88D106CB25D14A48B@exqld2-bne.nexus.csiro.au>
References: <80C7911E901E7E4797B3F88D106CB25D14A48B@exqld2-bne.nexus.csiro.au>
Message-ID: <1183690108.3676.28.camel@Bellerophon.localdomain>

On Fri, 2007-07-06 at 12:40 +1000, Alexander.Herr at csiro.au wrote:
> Hi List,
> 
> I want  replace characters within a vector. Outside R I could use sed,
> but I'd like to automate it in R. For example
> 
> vectorx
> xxxyyz
> xxxyyza
> xxxyyzzb
> 
> I want to change to: 
> 
> vectorx
> aaayyz
> aaayyza
> aaayyzzb
> 
> The obvious replace command only deals with whole data entries?
> Any hints would be appreciated.
> 
> Thanks
> Herry

See ?gsub and also ?regex

> vectorx
[1] "xxxyyz"   "xxxyyza"  "xxxyyzzb"

> gsub("x", "a", vectorx)
[1] "aaayyz"   "aaayyza"  "aaayyzzb"

HTH,

Marc Schwartz


From edd at debian.org  Fri Jul  6 04:59:28 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 5 Jul 2007 21:59:28 -0500
Subject: [R] Rmpi installation
In-Reply-To: <2d1ebb110707051819m6a9ac7dcx35c41d54426d2d15@mail.gmail.com>
References: <2d1ebb110707051819m6a9ac7dcx35c41d54426d2d15@mail.gmail.com>
Message-ID: <18061.45072.630998.45163@basebud.nulle.part>


On 5 July 2007 at 20:19, Edna Bell wrote:
| Hi R Gurus.
| 
| I'm trying to install the Rmpi package.
| 
| Here are the errors:
| checking for stdint.h... yes
| checking for unistd.h... yes
| checking mpi.h usability... no
| checking mpi.h presence... no
| checking for mpi.h... no
| Try to find libmpi or libmpich ...
| checking for main in -lmpi... no

You need to be able to compile normal MPI programs. Rmpi seems to like
LAM/MPI, in particular the older version 7.1.1.  I've had problems with newer
releases 7.1.2 to 7.1.4.

On Debian or Ubuntu, 'apt-get install r-cran-rmpi' will install rmpi (and all
required dependencies) for you.

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From ggrothendieck at gmail.com  Fri Jul  6 05:07:42 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 5 Jul 2007 23:07:42 -0400
Subject: [R] ?replace characters within vector data
In-Reply-To: <80C7911E901E7E4797B3F88D106CB25D14A48B@exqld2-bne.nexus.csiro.au>
References: <80C7911E901E7E4797B3F88D106CB25D14A48B@exqld2-bne.nexus.csiro.au>
Message-ID: <971536df0707052007ma4887f8l8b71082aebfc7c40@mail.gmail.com>

Check out ?chartr

On 7/5/07, Alexander.Herr at csiro.au <Alexander.Herr at csiro.au> wrote:
> Hi List,
>
> I want  replace characters within a vector. Outside R I could use sed,
> but I'd like to automate it in R. For example
>
> vectorx
> xxxyyz
> xxxyyza
> xxxyyzzb
>
> I want to change to:
>
> vectorx
> aaayyz
> aaayyza
> aaayyzzb
>
> The obvious replace command only deals with whole data entries?
> Any hints would be appreciated.
>
> Thanks
> Herry
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From s.blomberg1 at uq.edu.au  Fri Jul  6 05:20:25 2007
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Fri, 06 Jul 2007 13:20:25 +1000
Subject: [R] ?replace characters within vector data
In-Reply-To: <80C7911E901E7E4797B3F88D106CB25D14A48B@exqld2-bne.nexus.csiro.au>
References: <80C7911E901E7E4797B3F88D106CB25D14A48B@exqld2-bne.nexus.csiro.au>
Message-ID: <1183692025.4684.59.camel@sib-sblomber01d.sib.uq.edu.au>

sub("xxx", "aaa", vectorx)

or maybe gsub, depending on your application.

Cheers,

Simon.

On Fri, 2007-07-06 at 12:40 +1000, Alexander.Herr at csiro.au wrote:
> Hi List,
> 
> I want  replace characters within a vector. Outside R I could use sed,
> but I'd like to automate it in R. For example
> 
> vectorx
> xxxyyz
> xxxyyza
> xxxyyzzb
> 
> I want to change to: 
> 
> vectorx
> aaayyz
> aaayyza
> aaayyzzb
> 
> The obvious replace command only deals with whole data entries?
> Any hints would be appreciated.
> 
> Thanks
> Herry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia

Room 320, Goddard Building (8)
T: +61 7 3365 2506 
email: S.Blomberg1_at_uq.edu.au 

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.


From ripley at stats.ox.ac.uk  Fri Jul  6 08:12:27 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jul 2007 07:12:27 +0100 (BST)
Subject: [R] possible bug in ggplot2 v0.5.2???
In-Reply-To: <f8e6ff050707040037h58fba7a1w67c2b829dc9b2cb0@mail.gmail.com>
References: <468A1B01.9090100@genoscope.cns.fr> 
	<f8e6ff050707030314h21fa2898m1dc1ce35b91dbb54@mail.gmail.com> 
	<Pine.LNX.4.64.0707031255400.25506@gannet.stats.ox.ac.uk>
	<f8e6ff050707040037h58fba7a1w67c2b829dc9b2cb0@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707060630110.9948@gannet.stats.ox.ac.uk>

On Wed, 4 Jul 2007, hadley wickham wrote:

> On 7/4/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>> On Tue, 3 Jul 2007, hadley wickham wrote:
>> 
>> > Hi Stephane,
>> >
>> > The problem is that the windows graphics device doesn't support
>> > transparent colours.  You can get around this in two ways:
>> 
>> It certainly does!  Try col="transparent" (and perhaps consult your
>> dictionary).  It was news to me that the windows() graphics device worked
>> on
>> Linux i586.
>
> Well my dictionary defines transparent as "allowing light to pass
> through so that objects behind can be distinctly seen" which I believe
> applies here (ie. stained glass windows and blue points with alpha 0.5
> are both transparent).  What does your dictionary say?

Not quite the same, but even by your definition col="transparent" is 
transparent.  In this context

http://en.wikipedia.org/wiki/Transparency_%28graphic%29

seems more pertinent.

>> What it does not support as yet is translucent colours, and that is a
>> restriction imposed by Windows (translucency support was introduced for
>> Windows XP, and we still try to support older versions of Windows, unlike
>> the MacOS people).  I have been working on a workaround, so translucency
>> support is likely to be implemented in R 2.6.0 for users of XP or later.
>
> I am confused by your implication that windows (prior to XP) does not
> support translucency.  Perhaps it is not supported at the operating
> system level, but it has certainly been available at the application
> level for a very long time.

Really? It's hard to reply to unspecific assertions.  But remember XP has 
been out since 2001, almost as long as PDF has supported translucency.

One possibility is that you are talking about game-type applications, 
which have had alpha-blending for a long time via DirectX.  However, that 
was an add-on for a long time, is a function of the graphics card and is 
normally done on the GPU.

The standard Windows way of plotting is GDI, which is a vector painting 
language like postscript.  Like postscript, it also supports bitmaps but 
using them loses a lot of the flexibility.  Alpha blending of bitmaps was 
added for Windows 98, 2000 and later, but not for all devices: in 
particular not for metafiles and optionally for printers (and none of the 
printer drivers I have support it).  GDI+ (introduced with XP) adds 
translucency, but how widely it is supported is unclear to me.

For example, Cairo internally uses alpha-blending of bitmaps, but excludes 
Windows 98 as too buggy.  I've chosen to support Win2000 and later in 
windows().

>> Given that neither of the two main screen devices and neither of the
>> standard print devices support translucency, the subject line looks
>> correct to me: the problem surely lies in the assumptions made in ggplot2.
>
> The features of the windows and X11 devices clearly lag behind the
> quartz and pdf devices.  I can program for the lowest common
> denominator or I can use modern features that support the tasks I am
> working on.  I choose the later, and it is certainly your prerogative
> to declare that a bug in me.

I think to make undocumented assumptions about the environment is unkind 
to your would-be users.  Ideally the graphics devices would detect and 
report that, but that is not how support for semi-transparency was added. 
As a by-product of adding limited translucency support on the windows() 
family of devices, they do now warn.

You also need to check that the extra features work correctly.  I found 
some problems with all the devices I tried that support translucency (or 
at least with device+viewer combinations for pdf and svg).  Issues include 
whether translucent fills are rendered at all, blending translucent 
colours with transparent backgrounds, and the model used (is it the light 
intensity or the perceptual colours that are being blended?).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From r.hankin at noc.soton.ac.uk  Fri Jul  6 09:13:07 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 6 Jul 2007 08:13:07 +0100
Subject: [R] (no subject)
In-Reply-To: <002001c7bf37$b095eaf0$b0a6eda9@terra.cstars.ucdavis.edu>
References: <002001c7bf37$b095eaf0$b0a6eda9@terra.cstars.ucdavis.edu>
Message-ID: <E0FC1837-82A1-48A7-96F3-1CAE534062A3@noc.soton.ac.uk>

Jonathan

the BACCO bundle (emulator package) does exactly what you require.

(it's easier to think in terms of the variance matrix than a matrix
of distances, tho')

HTH

Robin



On 5 Jul 2007, at 20:07, Jonathan Greenberg wrote:

> I'm trying to hunt down an appropriate kriging package for my specific
> application, and I was hoping someone on the R list might have some  
> pointers
> --  I'm interested in performing kriging and related spatial  
> interpolations
> with one of the R packages, but I need to be able to provide my own
> point-to-point distances (e.g. I do not want to use standard  
> between point
> distances, as calculated by Euclidean or other similar distance  
> measures).
> Is there an R package that *I* can provide the matrix of distances  
> between
> every pair of points (e.g. for 10 points, I would have a 10 x 10  
> matrix of
> distances)?  Similarly, if this is possible, can I then provide  
> this package
> a vector of distances from each point to an arbitrary unknown  
> location (in
> my example, this would be a 1x10 vector) and apply the model to  
> this vector
> to predict a single unknown point?  It seems most (if not all) of the
> kriging packages I'm finding for R take the x,y,z location as the  
> inputs and
> calculate these distances themselves.  Thanks!
>
> Sincerely,
>
> Jonathan Greenberg
>
> --
> Jonathan A. Greenberg, PhD
> Postdoctoral Scholar
> Center for Spatial Technologies and Remote Sensing (CSTARS)
> University of California, Davis
> One Shields Avenue
> The Barn, Room 250N
> Davis, CA 95616
> Cell: 415-794-5043
> AIM: jgrn307
> MSN: jgrn307 at hotmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From pieter.claassen at cs.ru.nl  Fri Jul  6 11:29:13 2007
From: pieter.claassen at cs.ru.nl (pieter claassen)
Date: Fri, 06 Jul 2007 11:29:13 +0200
Subject: [R] How does the r-distribution function work
Message-ID: <1183714153.16308.2.camel@saphire.cs.ru.nl>

I am trying to understand what rbinom function does.

Here is some sample code. Are both the invocations of bfunc effectively
doing the same or I am missing the point?

Thanks,
Pieter

bfunc <- function(n1,p1,sims) {
c<-rbinom(sims,n1,p1)
c
}

a=c()
b=c()
p1=.5
for (i in 1:10000){
a[i]=bfunc(30,p1,1)
}
b=bfunc(30,p1,10000)


From tavpritesh at gmail.com  Fri Jul  6 09:26:30 2007
From: tavpritesh at gmail.com (Tavpritesh Sethi)
Date: Fri, 6 Jul 2007 12:56:30 +0530
Subject: [R] Multiple stripcharts?
Message-ID: <33846cd50707060026x282af85bv7ab4de96fc597a95@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/bdd87cee/attachment.pl 

From mmeredith at wcs.org  Fri Jul  6 09:51:09 2007
From: mmeredith at wcs.org (Mike Meredith)
Date: Fri, 6 Jul 2007 00:51:09 -0700 (PDT)
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>
References: <20070705200046.M65248@centroin.com.br>
	<smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>
Message-ID: <11460111.post@talk.nabble.com>



I think it would help if the tcl/tk manuals were added to the RGui Help
menu. Why google when they are on your hard drive already?

Cheers,  Mike


Mike Prager wrote:
> 
> "Alberto Monteiro" <albmont at centroin.com.br> wrote:
> 
>> How on Earth can I know what are the arguments of any of the functions of 
>> the tcl/tk package? [...]
> 
> My impression is that you as supposed to look in tck/tk manuals.
> For example, Googling on
> 
> tk tck getopenfile
> 
> pointed to this Web page:
> 
> http://www.tcl.tk/man/tcl8.5/TkCmd/getOpenFile.htm
> 
> Hope that helps.
> 
> -- 
> Mike Prager, NOAA, Beaufort, NC
> * Opinions expressed are personal and not represented otherwise.
> * Any use of tradenames does not constitute a NOAA endorsement.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Me-again%2C-about-the-horrible-documentation-of-tcltk-tf4032019.html#a11460111
Sent from the R help mailing list archive at Nabble.com.


From p.dalgaard at biostat.ku.dk  Fri Jul  6 09:54:19 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 06 Jul 2007 09:54:19 +0200
Subject: [R] How does the r-distribution function work
In-Reply-To: <1183714153.16308.2.camel@saphire.cs.ru.nl>
References: <1183714153.16308.2.camel@saphire.cs.ru.nl>
Message-ID: <468DF52B.4060006@biostat.ku.dk>

pieter claassen wrote:
> I am trying to understand what rbinom function does.
>
> Here is some sample code. Are both the invocations of bfunc effectively
> doing the same or I am missing the point?
>
>   
There are some "newbie" issues with your code (you are extending a on 
every iteration, and your bfunc is just rbinom with the parameters in a 
different order), but basically, yes: They are conceptually the same. 
Both give 10000 independent binomial samples.

In fact, if you reset the random number generator in between, they also 
give the same results (this is an implementation issue and not obviously 
guaranteed for any distribution) . Here's an example with smaller values 
than 10000 and 30.

 > set.seed(123)
 > rbinom(10,1,.5)
 [1] 0 1 0 1 1 0 1 1 1 0

 > set.seed(123)
 > for (i in 1:10) print(rbinom(1,1,.5))
[1] 0
[1] 1
[1] 0
[1] 1
[1] 1
[1] 0
[1] 1
[1] 1
[1] 1
[1] 0

 > set.seed(123)
 > replicate(10, rbinom(1,1,.5))
 [1] 0 1 0 1 1 0 1 1 1 0


> Thanks,
> Pieter
>
> bfunc <- function(n1,p1,sims) {
> c<-rbinom(sims,n1,p1)
> c
> }
>
> a=c()
> b=c()
> p1=.5
> for (i in 1:10000){
> a[i]=bfunc(30,p1,1)
> }
> b=bfunc(30,p1,10000)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From james.foadi at diamond.ac.uk  Fri Jul  6 10:19:43 2007
From: james.foadi at diamond.ac.uk (James Foadi)
Date: Fri, 6 Jul 2007 09:19:43 +0100
Subject: [R] unexpected result in function valuation
References: <009201c7bf2b$6acae810$661d17ac@JEEG>
	<000601c7bf44$3d5c77e0$7c94100a@win.ad.jhu.edu>
Message-ID: <003201c7bfa6$694cb6a0$661d17ac@JEEG>

----- Original Message ----- 
From: "Ravi Varadhan" <rvaradhan at jhmi.edu>
To: "'James Foadi'" <james.foadi at diamond.ac.uk>; <r-help at stat.math.ethz.ch>
Sent: Thursday, July 05, 2007 9:37 PM
Subject: RE: [R] unexpected result in function valuation


> The problem is that you are dividing two numbers that are both very small.
> Any small imprecision in the denominator creates a big error.
>
> Here you can re-write your function using a trig identity to get accurate
> results:
>
> sinca <- function(N,th) {
> #return(sin((N+0.5)* th)/sin(0.5*th))
> return( (sin(N*th)/tan(th/2)) + cos(N*th))
> }
>
> This function works well, as you had expected.
>
> Ravi.
>

Rounding off errors and error propagation...
I forgot all about it...Sorry for wasting your time, but thanks a lot for 
your help, Brian, Jim and Ravi.

J


From tavpritesh at gmail.com  Fri Jul  6 10:21:16 2007
From: tavpritesh at gmail.com (Tavpritesh Sethi)
Date: Fri, 6 Jul 2007 13:51:16 +0530
Subject: [R] Multiple Stripcharts
Message-ID: <33846cd50707060121o76c3f962u6b6f07c3f4667b7a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/245e1ca0/attachment.pl 

From matthew_laurence at hotmail.it  Fri Jul  6 10:26:50 2007
From: matthew_laurence at hotmail.it (matthew5555)
Date: Fri, 6 Jul 2007 01:26:50 -0700 (PDT)
Subject: [R] How does the r-distribution function work
In-Reply-To: <1183714153.16308.2.camel@saphire.cs.ru.nl>
References: <1183714153.16308.2.camel@saphire.cs.ru.nl>
Message-ID: <11460431.post@talk.nabble.com>


Hi, I have a problem..... how can I solve a problem without t.test????

for example:
x<-c(1,2,3,4,5,6)
y<-c(7,8,9)
t.test(x,y,alternative="less",paired=FALSE,var.equal=TRUE,con.level=0.95)

sorry for my english :)
-- 
View this message in context: http://www.nabble.com/How-does-the-r-distribution-function-work-tf4034026.html#a11460431
Sent from the R help mailing list archive at Nabble.com.


From matthew_laurence at hotmail.it  Fri Jul  6 10:40:11 2007
From: matthew_laurence at hotmail.it (matthew5555)
Date: Fri, 6 Jul 2007 01:40:11 -0700 (PDT)
Subject: [R] t.test
Message-ID: <11460445.post@talk.nabble.com>


Hi, how can I solve a problem without the function t.test???

for example:
x<-(1,3,5,7)
y<-(2,4,6)
t.test(x,y,alternative="less",paired=FALSE,var.equal=TRUE,conf.level=0.95)


-- 
View this message in context: http://www.nabble.com/t.test-tf4034225.html#a11460445
Sent from the R help mailing list archive at Nabble.com.


From sdl.web at gmail.com  Fri Jul  6 10:59:59 2007
From: sdl.web at gmail.com (Leo)
Date: Fri, 06 Jul 2007 09:59:59 +0100
Subject: [R] Warning message: cannot create HTML package index
Message-ID: <m28x9tvre8.fsf@cam.ac.uk>

I have set R_LIBS=~/R_lib as I don't have root access.

The following message shown up every time after installing a package:

  ......
  The downloaded packages are in
  	/tmp/RtmpBoIPoz/downloaded_packages
  Warning message:
  cannot create HTML package index in: tools:::unix.packages.html(.Library) 

Any ideas?

-- 
Leo <sdl.web AT gmail.com>                         (GPG Key: 9283AA3F)


From P.Dalgaard at biostat.ku.dk  Fri Jul  6 11:25:09 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 06 Jul 2007 11:25:09 +0200
Subject: [R] t.test
In-Reply-To: <11460445.post@talk.nabble.com>
References: <11460445.post@talk.nabble.com>
Message-ID: <468E0A75.9010800@biostat.ku.dk>

matthew5555 wrote:
> Hi, how can I solve a problem without the function t.test???
>
> for example:
> x<-(1,3,5,7)
> y<-(2,4,6)
> t.test(x,y,alternative="less",paired=FALSE,var.equal=TRUE,conf.level=0.95)
>
>
>   
Homework?

Hints: Take out your statistics textbook and look up the formulas for
the two-sample t.
You'll probably (there can be some variation depending on the book) find
that you need to compute

- difference of means
- sd for each group
- pooled sd
- s.e. of differences of means

all of which you can do easily in R, once you have the formulas. Then
calculate the t statistic and the corresponding p value, either using a
table or R's function for the t distibution.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Bernhard.Wellhoefer at gaia-group.com  Fri Jul  6 11:45:29 2007
From: Bernhard.Wellhoefer at gaia-group.com (=?iso-8859-1?Q?Bernhard_Wellh=F6fer?=)
Date: Fri, 6 Jul 2007 11:45:29 +0200
Subject: [R] RODBC problem
Message-ID: <7C36DCD0D35F9A4A8D89A7A8916E755D6D5376@GAIA-SERVER.gaia-group.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/a6d7c9ad/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jul  6 11:46:43 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jul 2007 10:46:43 +0100 (BST)
Subject: [R] Warning message: cannot create HTML package index
In-Reply-To: <m28x9tvre8.fsf@cam.ac.uk>
References: <m28x9tvre8.fsf@cam.ac.uk>
Message-ID: <Pine.LNX.4.64.0707061043200.22312@gannet.stats.ox.ac.uk>

On Fri, 6 Jul 2007, Leo wrote:

> I have set R_LIBS=~/R_lib as I don't have root access.
>
> The following message shown up every time after installing a package:
>
>  ......
>  The downloaded packages are in
>  	/tmp/RtmpBoIPoz/downloaded_packages
>  Warning message:
>  cannot create HTML package index in: tools:::unix.packages.html(.Library)
>
> Any ideas?

It is a correct warning.  What is the problem with being warned?

R tries to maintain an HTML page of installed packages, but you don't have 
permission to update it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sdl.web at gmail.com  Fri Jul  6 11:57:11 2007
From: sdl.web at gmail.com (Leo)
Date: Fri, 06 Jul 2007 10:57:11 +0100
Subject: [R] Warning message: cannot create HTML package index
References: <m28x9tvre8.fsf@cam.ac.uk>
	<Pine.LNX.4.64.0707061043200.22312@gannet.stats.ox.ac.uk>
Message-ID: <m2odipua6g.fsf@cam.ac.uk>

On 06/07/2007, Prof Brian Ripley wrote:
> On Fri, 6 Jul 2007, Leo wrote:
>
>> I have set R_LIBS=~/R_lib as I don't have root access.
>>
>> The following message shown up every time after installing a package:
>>
>>  ......
>>  The downloaded packages are in
>>  	/tmp/RtmpBoIPoz/downloaded_packages
>>  Warning message:
>>  cannot create HTML package index in: tools:::unix.packages.html(.Library)
>>
>> Any ideas?
>
> It is a correct warning.  What is the problem with being warned?
>
> R tries to maintain an HTML page of installed packages, but you don't have 
> permission to update it.

Where is that HTML page located on a GNU/Linux system?

Is it possible to maintain a user HTML page of installed packages?

Thanks,
-- 
Leo <sdl.web AT gmail.com>                         (GPG Key: 9283AA3F)


From cedric.duprez at ifn.fr  Fri Jul  6 11:58:38 2007
From: cedric.duprez at ifn.fr (=?iso-8859-1?Q?DUPREZ_C=E9dric?=)
Date: Fri, 6 Jul 2007 11:58:38 +0200
Subject: [R] RODBC problem
Message-ID: <5E3D22A4869BB94AA1138AB97660D8B69196B0@POPULUS.ifn.fr>

Hello,

The problem seems to be in the query syntax.
Can you show us the query you are trying to perform ?

Regards,

Cedric

-----Message d'origine-----
De?: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] De la part de Bernhard Wellh?fer
Envoy??: vendredi 6 juillet 2007 11:45
??: r-help at stat.math.ethz.ch
Objet?: [R] RODBC problem

Hello,
 
I use a RODBC connection to a MySQL server on a Debian machine. The call to odbcConnect() seems to be ok, but the result of the first sqlFetch(channel,"t_studie") retrieves this data frame:
 
[1] "[RODBC] ERROR: Could not SQLExecDirect"
[2] "42000 1064 [MySQL][ODBC 3.51 Driver][mysqld-5.0.22-Debian_1bpo1-log]You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '\"t_studi(\004"

Please note the funny character at the end of the table name in the error message.
 
The "Test Data Source" option on the ODBC Data Source Name configuration panel report success.
 
Who can help me here?
 
Regards,
 
Bernhard
 
 
 
 
 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From murdoch at stats.uwo.ca  Fri Jul  6 12:33:52 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 06 Jul 2007 06:33:52 -0400
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <11460111.post@talk.nabble.com>
References: <20070705200046.M65248@centroin.com.br>	<smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>
	<11460111.post@talk.nabble.com>
Message-ID: <468E1A90.4060606@stats.uwo.ca>

On 06/07/2007 3:51 AM, Mike Meredith wrote:
> 
> I think it would help if the tcl/tk manuals were added to the RGui Help
> menu. Why google when they are on your hard drive already?

I'd say they are too specialized for that.  There are dozens of topics 
that are as important as this, but a GUI menu with more than a few 
entries is just unwieldy.

We do have a text reference to the help files in the ?tcltk topic.

Duncan Murdoch

> 
> Cheers,  Mike
> 
> 
> Mike Prager wrote:
>> "Alberto Monteiro" <albmont at centroin.com.br> wrote:
>>
>>> How on Earth can I know what are the arguments of any of the functions of 
>>> the tcl/tk package? [...]
>> My impression is that you as supposed to look in tck/tk manuals.
>> For example, Googling on
>>
>> tk tck getopenfile
>>
>> pointed to this Web page:
>>
>> http://www.tcl.tk/man/tcl8.5/TkCmd/getOpenFile.htm
>>
>> Hope that helps.
>>
>> -- 
>> Mike Prager, NOAA, Beaufort, NC
>> * Opinions expressed are personal and not represented otherwise.
>> * Any use of tradenames does not constitute a NOAA endorsement.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>


From Bernhard.Wellhoefer at gaia-group.com  Fri Jul  6 12:43:12 2007
From: Bernhard.Wellhoefer at gaia-group.com (=?iso-8859-1?Q?Bernhard_Wellh=F6fer?=)
Date: Fri, 6 Jul 2007 12:43:12 +0200
Subject: [R] RODBC problem
In-Reply-To: <5E3D22A4869BB94AA1138AB97660D8B69196B0@POPULUS.ifn.fr>
References: <5E3D22A4869BB94AA1138AB97660D8B69196B0@POPULUS.ifn.fr>
Message-ID: <7C36DCD0D35F9A4A8D89A7A8916E755D6D5384@GAIA-SERVER.gaia-group.local>

Hello,

as I wrote I call

  sqlFetch(channel,"t_studie")

and this function generates in the background the concrete query. How can I view/log/... the concrete query?

Regards,

Bernhard 

> -----Original Message-----
> From: DUPREZ C?dric [mailto:cedric.duprez at ifn.fr] 
> Sent: Friday, July 06, 2007 11:59 AM
> To: Bernhard Wellh?fer
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] RODBC problem
> 
> Hello,
> 
> The problem seems to be in the query syntax.
> Can you show us the query you are trying to perform ?
> 
> Regards,
> 
> Cedric
> 
> -----Message d'origine-----
> De?: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] De la part de 
> Bernhard Wellh?fer
> Envoy??: vendredi 6 juillet 2007 11:45
> ??: r-help at stat.math.ethz.ch
> Objet?: [R] RODBC problem
> 
> Hello,
>  
> I use a RODBC connection to a MySQL server on a Debian 
> machine. The call to odbcConnect() seems to be ok, but the 
> result of the first sqlFetch(channel,"t_studie") retrieves 
> this data frame:
>  
> [1] "[RODBC] ERROR: Could not SQLExecDirect"
> [2] "42000 1064 [MySQL][ODBC 3.51 
> Driver][mysqld-5.0.22-Debian_1bpo1-log]You have an error in 
> your SQL syntax; check the manual that corresponds to your 
> MySQL server version for the right syntax to use near '\"t_studi(\004"
> 
> Please note the funny character at the end of the table name 
> in the error message.
>  
> The "Test Data Source" option on the ODBC Data Source Name 
> configuration panel report success.
>  
> Who can help me here?
>  
> Regards,
>  
> Bernhard
>  
>  
>  
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From nitish at imtech.res.in  Fri Jul  6 12:41:37 2007
From: nitish at imtech.res.in (Nitish Kumar Mishra)
Date: Fri, 6 Jul 2007 16:11:37 +0530 (IST)
Subject: [R] about R, RMSEP, R2, PCR
Message-ID: <45339.59.160.112.42.1183718497.squirrel@webmail.imtech.res.in>

Hi,
I want to calculate PLS package in R. Now I want to calculate R, MSEP,
RMSEP and R2 of PLSR and PCR using this.
I also add this in library of R. How I can calculate R, MSEP, RMSEP and R2
of PLSR and PCR in R.
I s any other method then please also suggest me. Simply I want to
calculate these value.
Thanking you.
-- 
Nitish Kumar Mishra
Junior Research Fellow
BIC, IMTECH, Chandigarh, India
E-Mail Address:
nitish_km at yahoo.com
nitish at imtech.res.in


From P.Dalgaard at biostat.ku.dk  Fri Jul  6 12:49:21 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 06 Jul 2007 12:49:21 +0200
Subject: [R] Warning message: cannot create HTML package index
In-Reply-To: <m2odipua6g.fsf@cam.ac.uk>
References: <m28x9tvre8.fsf@cam.ac.uk>	<Pine.LNX.4.64.0707061043200.22312@gannet.stats.ox.ac.uk>
	<m2odipua6g.fsf@cam.ac.uk>
Message-ID: <468E1E31.4090409@biostat.ku.dk>

Leo wrote:
> On 06/07/2007, Prof Brian Ripley wrote:
>   
>> On Fri, 6 Jul 2007, Leo wrote:
>>
>>     
>>> I have set R_LIBS=~/R_lib as I don't have root access.
>>>
>>> The following message shown up every time after installing a package:
>>>
>>>  ......
>>>  The downloaded packages are in
>>>  	/tmp/RtmpBoIPoz/downloaded_packages
>>>  Warning message:
>>>  cannot create HTML package index in: tools:::unix.packages.html(.Library)
>>>
>>> Any ideas?
>>>       
>> It is a correct warning.  What is the problem with being warned?
>>
>> R tries to maintain an HTML page of installed packages, but you don't have 
>> permission to update it.
>>     
>
> Where is that HTML page located on a GNU/Linux system?
>
> Is it possible to maintain a user HTML page of installed packages?
>
> Thanks,
>   
This confuses me a bit too. I had gotten used to the warning without
thinking about it. It tries to update $RHOME/doc/html/packages.html,
which starts like this:

.....
<p><h3>Packages in the standard library</h3>
.....

However, if I run help.start, I get

> help.start()
Making links in per-session dir ...
If 'firefox' is already running, it is *not* restarted, and you must
    switch to its window.
Otherwise, be patient ...

and then it opens (say)
  file:///tmp/RtmpXyp5Cg/.R/doc/html/index.html
which has a link to
  file:///tmp/RtmpXyp5Cg/.R/doc/html/packages.html

which looks like this

....
<p><h3>Packages in /home/bs/pd/Rlibrary</h3>
....
<p><h3>Packages in /usr/lib64/R/library</h3>

I.e. it is autogenerated by help.start and doesn't even look at the file
in $RHOME. So what puzzles me is

(a) why we maintain $RHOME/doc/html/packages.html at all

One argument could be that this is browseable for everyone on a system,
even without starting R. But then

(b) why do we even try updating it when packages are installed in a
private location?

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From phgrosjean at sciviews.org  Fri Jul  6 12:57:17 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 06 Jul 2007 12:57:17 +0200
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <468E1A90.4060606@stats.uwo.ca>
References: <20070705200046.M65248@centroin.com.br>	<smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>	<11460111.post@talk.nabble.com>
	<468E1A90.4060606@stats.uwo.ca>
Message-ID: <468E200D.7000808@sciviews.org>

For those who are interested, I just cook a little tcltkHelp() function 
to ease access to the Tcl/Tk documentation under Windows. This is on the 
Wiki discussion of the TkCommands help page at: 
http://wiki.r-project.org/rwiki/doku.php?id=rdoc:tcltk:tkcommands
Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Duncan Murdoch wrote:
> On 06/07/2007 3:51 AM, Mike Meredith wrote:
>> I think it would help if the tcl/tk manuals were added to the RGui Help
>> menu. Why google when they are on your hard drive already?
> 
> I'd say they are too specialized for that.  There are dozens of topics 
> that are as important as this, but a GUI menu with more than a few 
> entries is just unwieldy.
> 
> We do have a text reference to the help files in the ?tcltk topic.
> 
> Duncan Murdoch
> 
>> Cheers,  Mike
>>
>>
>> Mike Prager wrote:
>>> "Alberto Monteiro" <albmont at centroin.com.br> wrote:
>>>
>>>> How on Earth can I know what are the arguments of any of the functions of 
>>>> the tcl/tk package? [...]
>>> My impression is that you as supposed to look in tck/tk manuals.
>>> For example, Googling on
>>>
>>> tk tck getopenfile
>>>
>>> pointed to this Web page:
>>>
>>> http://www.tcl.tk/man/tcl8.5/TkCmd/getOpenFile.htm
>>>
>>> Hope that helps.
>>>
>>> -- 
>>> Mike Prager, NOAA, Beaufort, NC
>>> * Opinions expressed are personal and not represented otherwise.
>>> * Any use of tradenames does not constitute a NOAA endorsement.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Joao.Fadista at agrsci.dk  Fri Jul  6 13:12:04 2007
From: Joao.Fadista at agrsci.dk (=?iso-8859-1?Q?Jo=E3o_Fadista?=)
Date: Fri, 6 Jul 2007 13:12:04 +0200
Subject: [R] number of permutations required
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F34@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/53d21748/attachment.pl 

From murdoch at stats.uwo.ca  Fri Jul  6 13:23:08 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 06 Jul 2007 07:23:08 -0400
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <468E200D.7000808@sciviews.org>
References: <20070705200046.M65248@centroin.com.br>	<smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>	<11460111.post@talk.nabble.com>
	<468E1A90.4060606@stats.uwo.ca> <468E200D.7000808@sciviews.org>
Message-ID: <468E261C.2050809@stats.uwo.ca>

On 06/07/2007 6:57 AM, Philippe Grosjean wrote:
> For those who are interested, I just cook a little tcltkHelp() function 
> to ease access to the Tcl/Tk documentation under Windows. This is on the 
> Wiki discussion of the TkCommands help page at: 
> http://wiki.r-project.org/rwiki/doku.php?id=rdoc:tcltk:tkcommands

Thanks, Philippe.  That's a useful function.

Duncan Murdoch

> Best,
> 
> Philippe Grosjean
> 
> ..............................................<?}))><........
>   ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>   ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>   ) ) ) ) )   Mons-Hainaut University, Belgium
> ( ( ( ( (
> ..............................................................
> 
> Duncan Murdoch wrote:
>> On 06/07/2007 3:51 AM, Mike Meredith wrote:
>>> I think it would help if the tcl/tk manuals were added to the RGui Help
>>> menu. Why google when they are on your hard drive already?
>> I'd say they are too specialized for that.  There are dozens of topics 
>> that are as important as this, but a GUI menu with more than a few 
>> entries is just unwieldy.
>>
>> We do have a text reference to the help files in the ?tcltk topic.
>>
>> Duncan Murdoch
>>
>>> Cheers,  Mike
>>>
>>>
>>> Mike Prager wrote:
>>>> "Alberto Monteiro" <albmont at centroin.com.br> wrote:
>>>>
>>>>> How on Earth can I know what are the arguments of any of the functions of 
>>>>> the tcl/tk package? [...]
>>>> My impression is that you as supposed to look in tck/tk manuals.
>>>> For example, Googling on
>>>>
>>>> tk tck getopenfile
>>>>
>>>> pointed to this Web page:
>>>>
>>>> http://www.tcl.tk/man/tcl8.5/TkCmd/getOpenFile.htm
>>>>
>>>> Hope that helps.
>>>>
>>>> -- 
>>>> Mike Prager, NOAA, Beaufort, NC
>>>> * Opinions expressed are personal and not represented otherwise.
>>>> * Any use of tradenames does not constitute a NOAA endorsement.
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From csardi at rmki.kfki.hu  Fri Jul  6 13:26:14 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Fri, 6 Jul 2007 13:26:14 +0200
Subject: [R] generating lower triangular matrix
In-Reply-To: <755261CA22782948B1C42ACDC83912A104614225@NYWEXMB27.msad.ms.com>
References: <755261CA22782948B1C42ACDC83912A104614225@NYWEXMB27.msad.ms.com>
Message-ID: <20070706112614.GB6145@guzu>

You might want to check lower.tri. Also, keep in mind that a matrix
in R is just a vector with a 'dim' attribute. The elements are stored 
columnwise.

Gabor

On Thu, Jul 05, 2007 at 06:53:45PM -0400, Aydemir, Zava (FID) wrote:
> Hi,
>  
> I would like to generate below triangular matrix for some  a.
>  
> 0
> 0
> 0
> 0
[...]
>  
> What's an efficient way to do this? (this matrix being KxK, K very
> large)
>  
> Thanks
>  
> Zava
> --------------------------------------------------------
> 
> This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From albmont at centroin.com.br  Fri Jul  6 13:26:47 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 6 Jul 2007 09:26:47 -0200
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <468E200D.7000808@sciviews.org>
References: <20070705200046.M65248@centroin.com.br>	<smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>	<11460111.post@talk.nabble.com>
	<468E1A90.4060606@stats.uwo.ca> <468E200D.7000808@sciviews.org>
Message-ID: <20070706112643.M35134@centroin.com.br>




---------- Original Message -----------
From: Philippe Grosjean <phgrosjean at sciviews.org>
To: Duncan Murdoch <murdoch at stats.uwo.ca>
Cc: r-help at stat.math.ethz.ch, Mike Meredith <mmeredith at wcs.org>
Sent: Fri, 06 Jul 2007 12:57:17 +0200
Subject: Re: [R] Me again, about the horrible documentation of tcltk

> For those who are interested, I just cook a little tcltkHelp() 
> function to ease access to the Tcl/Tk documentation under Windows. 
> This is on the Wiki discussion of the TkCommands help page at: 
> http://wiki.r-project.org/rwiki/doku.php?id=rdoc:tcltk:tkcommands 
> Best,
> 
> Philippe Grosjean
> 
> ..............................................<?}))><........
>   ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>   ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>   ) ) ) ) )   Mons-Hainaut University, Belgium
> ( ( ( ( (
> ..............................................................
> 
> Duncan Murdoch wrote:
> > On 06/07/2007 3:51 AM, Mike Meredith wrote:
> >> I think it would help if the tcl/tk manuals were added to the RGui Help
> >> menu. Why google when they are on your hard drive already?
> > 
> > I'd say they are too specialized for that.  There are dozens of topics 
> > that are as important as this, but a GUI menu with more than a few 
> > entries is just unwieldy.
> > 
> > We do have a text reference to the help files in the ?tcltk topic.
> > 
> > Duncan Murdoch
> > 
> >> Cheers,  Mike
> >>
> >>
> >> Mike Prager wrote:
> >>> "Alberto Monteiro" <albmont at centroin.com.br> wrote:
> >>>
> >>>> How on Earth can I know what are the arguments of any of the 
functions of 
> >>>> the tcl/tk package? [...]
> >>> My impression is that you as supposed to look in tck/tk manuals.
> >>> For example, Googling on
> >>>
> >>> tk tck getopenfile
> >>>
> >>> pointed to this Web page:
> >>>
> >>> http://www.tcl.tk/man/tcl8.5/TkCmd/getOpenFile.htm
> >>>
> >>> Hope that helps.
> >>>
> >>> -- 
> >>> Mike Prager, NOAA, Beaufort, NC
> >>> * Opinions expressed are personal and not represented otherwise.
> >>> * Any use of tradenames does not constitute a NOAA endorsement.
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>>
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
guide.html
> and provide commented, minimal, self-contained, reproducible code.
------- End of Original Message -------


From albmont at centroin.com.br  Fri Jul  6 13:32:29 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 6 Jul 2007 09:32:29 -0200
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <468E200D.7000808@sciviews.org>
References: <20070705200046.M65248@centroin.com.br>	<smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>	<11460111.post@talk.nabble.com>
	<468E1A90.4060606@stats.uwo.ca> <468E200D.7000808@sciviews.org>
Message-ID: <20070706112704.M87449@centroin.com.br>

[sorry for the previous mis-typed message... my mouse is
playing evil tricks against me [*]]

Philippe Grosjean wrote:
>
> For those who are interested, I just cook a little tcltkHelp() 
> function to ease access to the Tcl/Tk documentation under Windows. 
> This is on the Wiki discussion of the TkCommands help page at: 
> http://wiki.r-project.org/rwiki/doku.php?id=rdoc:tcltk:tkcommands 
> Best,
> 
The help is Windows-only, isn't it? I use Windows (at work and 
eventually at home) and Linux (at home). Maybe this is off-topic
(it's more a bug of tcl/tk then of the tcltk R library), but this 
tk_getOpenFile opens a nice window in Windows, but a mutilated
window in Linux, that does not show any file information except
filename. That's why I wanted to know if there was a way to
improve the function call - I think there isn't.

Alberto Monteiro

[*] I am neither paranoid nor animistic!


From david.meyer at wu-wien.ac.at  Fri Jul  6 13:34:26 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Fri, 06 Jul 2007 13:34:26 +0200
Subject: [R] Question for svm function in e1071
Message-ID: <468E28C2.4090906@wu-wien.ac.at>

Adschai:

The function is written in C++, so debugging the source code of the R 
svm() function will not really help. What you can do to make the C-code 
more verbose is the following:

- get the sources of e1071
- in the src/ directory, look up the svm.cpp file
- In line 37, there is:


#if 0
void info(char *fmt,...)

[...]

replace the first line by:

#if 1

- build and install the package again.

Best
David

----------------------------

Sorry that I have many questions today. I am using svm function on about
180,000 points of training set. It takes very long time to run. However,
I would like it to spit out something to make sure that the run is not
dead in between.  Would you please suggest anyway to do so?


From Angela.Moss at dudley.nhs.uk  Fri Jul  6 13:39:31 2007
From: Angela.Moss at dudley.nhs.uk (Moss, Angela (Dudley PCT))
Date: Fri, 6 Jul 2007 12:39:31 +0100
Subject: [R] svyglm
Message-ID: <3381AE41D6EE5344B20DC65B83D7320C063056@EVS01.dudley.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/2c1d1ea2/attachment.pl 

From John.Scillieri at constellation.com  Fri Jul  6 13:51:31 2007
From: John.Scillieri at constellation.com (Scillieri, John)
Date: Fri, 6 Jul 2007 07:51:31 -0400
Subject: [R] R Logging Package
In-Reply-To: <7B145F644E353D48A1FC22F09D977677012F2E56@EXM-OMF-22.Ceg.Corp.Net>
References: <7B145F644E353D48A1FC22F09D977677012F2E56@EXM-OMF-22.Ceg.Corp.Net>
Message-ID: <7B145F644E353D48A1FC22F09D9776770264CE79@EXM-OMF-22.Ceg.Corp.Net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/78c115c0/attachment.pl 

From dusa.adrian at gmail.com  Fri Jul  6 13:55:11 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Fri, 6 Jul 2007 14:55:11 +0300
Subject: [R] Levene Test with R
In-Reply-To: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
References: <21add9100707050736s53f09578k960d758a2c9886e6@mail.gmail.com>
Message-ID: <200707061455.12533.dusa.adrian@gmail.com>

On Thursday 05 July 2007, along zeng wrote:
> Hi All,
>      is there Levene' test in R ? If not ,Could you give me some
> advice about Levene test with R?
>     Thanks a lot! I am waiting for yours.


>From what I found in the archives, Levene is not very well suited for the 
homogeneity of variances test, and the recommended ones are:

Ansari-Bradley for two groups (i.e. for t.test)
Fligner-Killeen for three or more groups (i.e. for ANOVA)

hth,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From schimpanski at gmx.de  Fri Jul  6 14:04:22 2007
From: schimpanski at gmx.de (Will)
Date: Fri, 06 Jul 2007 14:04:22 +0200
Subject: [R] .Rprofile: Set package path for downloads
Message-ID: <468E2FC6.3030004@gmx.de>

Hi,

I'd like to configure my .Rprofile so that additional packages are 
installed and loaded from my personal package repository (let's say 
d:/R/MyPackages/). I am working with R 2.4.1 under Windows XP.
How do I do this?

Thanks in advance,

Will


From bhs2 at mevik.net  Fri Jul  6 14:27:58 2007
From: bhs2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 06 Jul 2007 14:27:58 +0200
Subject: [R] about R, RMSEP, R2, PCR
In-Reply-To: <45339.59.160.112.42.1183718497.squirrel@webmail.imtech.res.in>
	(Nitish
	Kumar Mishra's message of "Fri, 6 Jul 2007 16:11:37 +0530 (IST)")
References: <45339.59.160.112.42.1183718497.squirrel@webmail.imtech.res.in>
Message-ID: <m03b013eep.fsf@bar.nemo-project.org>

Nitish Kumar Mishra wrote:

> I want to calculate PLS package in R. Now I want to calculate R, MSEP,
> RMSEP and R2 of PLSR and PCR using this.
> I also add this in library of R. How I can calculate R, MSEP, RMSEP and R2
> of PLSR and PCR in R.
> I s any other method then please also suggest me. Simply I want to
> calculate these value.

I'm not entirely sure what you are asking about, but if you want to
calculate R, MSEP, RMSEP and R2 for PLSRs and PCRs with the pls
package, this should work:

library(pls)
data(yarn)
mymodel <- plsr(density ~ NIR, ncomp = 10, data = yarn) # or pcr()

See ?plsr for further options, especially "validation" for using
cross-validation.

## R2:
R2(mymodel)

## MSEP:
MSEP(mymodel)

## RMSEP:
RMSEP(mymodel)

See ?R2, etc. for further arguments, especially "estimate" for
selecting the estimator (test set, CV, or train).

The objects returned by these functions have a plot method, som
plot(RMSEP(...)) does "what you'd expect".  See ?R2 for details about
the returned objects.

To get R (I presume you mean the correlation between predicted and
measured values), you can use

sqrt(R2(mymodel)$val)


-- 
HTH,
Bj?rn-Helge Mevik


From elyakhlifi_mustapha at yahoo.fr  Fri Jul  6 14:25:59 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Fri, 6 Jul 2007 12:25:59 +0000 (GMT)
Subject: [R] HTML.data.frame
Message-ID: <363599.67568.qm@web27506.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/dfef2285/attachment.pl 

From jrkrideau at yahoo.ca  Fri Jul  6 14:35:50 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 6 Jul 2007 08:35:50 -0400 (EDT)
Subject: [R] problem assigning to indexed data frame element
In-Reply-To: <76D2AA307C39054DBA8BD42DE44E71A4031409FD@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <991654.14588.qm@web32812.mail.mud.yahoo.com>

Check what is happening with current.spec.  It looks
to me as if you are trying to use a factor as an
index.  See below

--- "Drescher, Michael (MNR)"
<michael.drescher at ontario.ca> wrote:

> Hi All,
> 
> Sorry if I ask an obvious thing, I am still new to R
> ...
> 
> I created a data frame of given dimensions to which
> I gave strings as
> column names. I want to write to elements of the
> data frame by indexing
> them with the row number and column name (string).
> The problem is that I
> can read elements from the data frame in this way,
> but I cannot assign
> to elements in this way. Instead, I get the
> following error message:
> 
> Error in Summary.factor(..., na.rm = na.rm) : 
>         min not meaningful for factors
> 
> Please find the code I used farther below. It would
> be great if someone
> could help me.
> 
> Best regards, Michael
> 
> PS: Coincidentally, I found this same error message
> mentioned in another
> context (levelplot) as indicating a bug (original
> bug report PR# 6005 on
> Mon, 22 Dec 2003)
> 
> ------------------------
> Michael Drescher
> Ontario Forest Research Institute
> Ontario Ministry of Natural Resources
> 1235 Queen St East
> Sault Ste Marie, ON, P6A 2E3
> Tel: (705) 946-7406
> Fax: (705) 946-2030
> 
> ------------------------
> 
> Code:
> > sfalls.plot.comp <- matrix(nrow=plot.count,
> ncol=spec.count, byrow=T)
> > colnames(sfalls.plot.comp) <- levels(SPECIES)
> ### SPECIES, SPP_VOL, & PLOT are columns/variables
> in a previously read
> data file
> > sfalls.plot.comp <- data.frame(sfalls.plot.comp)
> > attach(sfalls.plot.comp)
> > sfalls.plot.comp[is.na(sfalls.plot.comp)] <- 0
> 
> > sfalls.plot.comp
>   Bf Bw Pj Po Sb
> 1  0  0  0  0  0
> 2  0  0  0  0  0
> 
> > hh <- 1
> > current.spec <- SPECIES[hh]; current.vol <-
> SPP_VOL[hh]; current.plot
> <- PLOT[hh]
> 
> > current.spec
> [1] Bf
> Levels: Bf Bw Pj Po Sb
> 
> > current.vol
> [1] 2
> 
> > current.plot
> [1] 1
> 
> > sfalls.plot.comp[current.plot,current.spec]
> ### thus, reading from the data frame in this way
> (using the column
> name/string) works fine
> [1] 0
> 
> > sfalls.plot.comp[current.plot,current.spec] <-
> current.vol	### but
> assigning in this way does not work
> Error in Summary.factor(..., na.rm = na.rm) : 
>         min not meaningful for factors

If I am reading this correctly 
current.spec is not a column name or a number. It is a
factor as the levels indicate.  You might be able to
get around this by  

current.spec <- as.character(SPECIES[hh])

R has,what to me is, an annoying tendency to read in
many columns of characters as factors.  This can be
changed by changing something in the startup file.
Exactly what I have forgotten.

> 
> > sfalls.plot.comp[current.plot,1] <- current.vol
> ### assigning by using the column number instead of
> the column name of
> course does work
> > sfalls.plot.comp[current.plot,current.spec]
> [1] 2
> 
> > sfalls.plot.comp[current.plot,"Bw"] <- current.vol
> ### as does assigning when replacing 'current.spec'
> for its assigned
> value in quotes, e.g., "Bw"
> > sfalls.plot.comp[current.plot,"Bw"]
> [1] 2
> 
> > sfalls.plot.comp
>   Bf Bw Pj Po Sb
> 1  2  2  0  0  0
> 2  0  0  0  0  0
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From murdoch at stats.uwo.ca  Fri Jul  6 14:38:49 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 06 Jul 2007 08:38:49 -0400
Subject: [R] Me again, about the horrible documentation of tcltk
In-Reply-To: <20070706112704.M87449@centroin.com.br>
References: <20070705200046.M65248@centroin.com.br>	<smlq83pt2q6v35ev85ospij90389nn4n6j@4ax.com>	<11460111.post@talk.nabble.com>
	<468E1A90.4060606@stats.uwo.ca> <468E200D.7000808@sciviews.org>
	<20070706112704.M87449@centroin.com.br>
Message-ID: <468E37D9.40004@stats.uwo.ca>

On 7/6/2007 7:32 AM, Alberto Monteiro wrote:
> [sorry for the previous mis-typed message... my mouse is
> playing evil tricks against me [*]]
> 
> Philippe Grosjean wrote:
>>
>> For those who are interested, I just cook a little tcltkHelp() 
>> function to ease access to the Tcl/Tk documentation under Windows. 
>> This is on the Wiki discussion of the TkCommands help page at: 
>> http://wiki.r-project.org/rwiki/doku.php?id=rdoc:tcltk:tkcommands 
>> Best,
>> 
> The help is Windows-only, isn't it? I use Windows (at work and 
> eventually at home) and Linux (at home). Maybe this is off-topic
> (it's more a bug of tcl/tk then of the tcltk R library), but this 
> tk_getOpenFile opens a nice window in Windows, but a mutilated
> window in Linux, that does not show any file information except
> filename. That's why I wanted to know if there was a way to
> improve the function call - I think there isn't.

But on Linux you can just use "the system man pages", as ?tcltk tells 
you.  (And by the way, tkgetOpenFile opens a reasonably nice window for 
me when run on a Linux system:  so this is probably a problem with your 
local installation of TCL/TK.)

Duncan Murdoch


From murdoch at stats.uwo.ca  Fri Jul  6 14:43:41 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 06 Jul 2007 08:43:41 -0400
Subject: [R] .Rprofile: Set package path for downloads
In-Reply-To: <468E2FC6.3030004@gmx.de>
References: <468E2FC6.3030004@gmx.de>
Message-ID: <468E38FD.3030208@stats.uwo.ca>

On 7/6/2007 8:04 AM, Will wrote:
> Hi,
> 
> I'd like to configure my .Rprofile so that additional packages are 
> installed and loaded from my personal package repository (let's say 
> d:/R/MyPackages/). I am working with R 2.4.1 under Windows XP.
> How do I do this?

See the repos option in ?options, or ?setRepositories.

Duncan Murdoch


From regina.verghis at gmail.com  Fri Jul  6 15:04:27 2007
From: regina.verghis at gmail.com (Regina Verghis)
Date: Fri, 6 Jul 2007 18:34:27 +0530
Subject: [R] loading package in LINUX
Message-ID: <49f266b90707060604i2ca8e0c0w313858cdbaea678e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/351c874c/attachment.pl 

From francois.morneau at ifn.fr  Fri Jul  6 15:29:21 2007
From: francois.morneau at ifn.fr (=?iso-8859-1?Q?MORNEAU_Fran=E7ois?=)
Date: Fri, 6 Jul 2007 15:29:21 +0200
Subject: [R] HTML.data.frame
References: <363599.67568.qm@web27506.mail.ukl.yahoo.com>
Message-ID: <5E3D22A4869BB94AA1138AB97660D8B6AA77E8@POPULUS.ifn.fr>

You "wanna" many things but seem to have forgotten the posting guide and perhaps to have a look at fortune(122) ;)
Could you provide a short reproducible example please ?
Regards,
Fran?ois

-----Message d'origine-----
De?: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] De la part de elyakhlifi mustapha
Envoy??: vendredi 6 juillet 2007 14:26
??: R-help at stat.math.ethz.ch
Objet?: [R] HTML.data.frame

hello,
I installed the R2HTML package but when I wanna execute the HTML.data.frame function I can't and I don't know why
can you help me please?
thanks.


      _____________________________________________________________________________ 

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Jul  6 15:45:15 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Jul 2007 14:45:15 +0100 (BST)
Subject: [R] .Rprofile: Set package path for downloads
In-Reply-To: <468E38FD.3030208@stats.uwo.ca>
References: <468E2FC6.3030004@gmx.de> <468E38FD.3030208@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0707061443141.28289@gannet.stats.ox.ac.uk>

On Fri, 6 Jul 2007, Duncan Murdoch wrote:

> On 7/6/2007 8:04 AM, Will wrote:
>> Hi,
>>
>> I'd like to configure my .Rprofile so that additional packages are
>> installed and loaded from my personal package repository (let's say
>> d:/R/MyPackages/). I am working with R 2.4.1 under Windows XP.
>> How do I do this?
>
> See the repos option in ?options, or ?setRepositories.

Do you think he really means that?  A repository would need to be a 
file:// URL, for a start, and people who know how to set one up have 
surely read the manual.

My guess is he means a personal library, for which see the rw-FAQ.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From spencer.graves at pdf.com  Fri Jul  6 16:38:31 2007
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 06 Jul 2007 07:38:31 -0700
Subject: [R] (Statistics question) - Nonlinear regression and
 simultaneous equation
In-Reply-To: <e4ada7f122a58.468c5411@optonline.net>
References: <e4ada7f122a58.468c5411@optonline.net>
Message-ID: <468E53E7.9020408@pdf.com>

      Not all parameters are estimable in some systems of equations like 
the classical "errors in X" regression. 

      Consistency is an asymptotic property:  On average, as the sample 
size increases without bound, a consistent estimator will converge to 
what you want.  I'm no expert in asymptotics, but I recall theorems that 
suggest that the estimator obtained from a single step in a maximum 
likelihood estimation can be consistent -- provided the information is 
available in the data and the structure of the model.  The issue is not 
whether you use SVM (support vector machine?), FIML (full information 
maximum likelihood?) or the 2SLS (2 stage least squares?) or only the 
first step. 

      Is there information in your data for estimating all the 
parameters in your model?  By "information" here, I mean something like 
Fisher information, the negative expectation of the matrix of second 
partial derivatives with respect to parameters you want to estimate of a 
log(likelihood) for your model.  Is this matrix ill conditioned?  What 
happens to its eigenvalues as your hypothetical sample size increases 
without bound? 

      If these comments do not seem relevant to your question, please 
provide more detail of your specific application, preferably including 
"commented, minimal, self-contained, reproducible code", as requested at 
the end of every email forwarded by r-help. 

      Hope this helps. 
      Spencer Graves
    
adschai at optonline.net wrote:
> Hi,I have a fundamental questions that I'm a bit confused. If any guru from this circle could help me out, I would really appreciate.I have a system of equations in which some of the endogs appear on right hand sides of some equations. To solve this, one needs a technique like 2SLS or FIML to circumvent inconsistency of the estimated coefficients. My question is that if I apply the nonlinear regression like SVM regression. Do I still need to worry about endogeneity? Meaning, what I only need to care is the 1st step of 2SLS. That would mean that I only need to carry out the SVM regression on all the exogs. Am I missing anything here? Thank you so much.Regards,- adschai
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From matteo.dallamico at ing.unitn.it  Fri Jul  6 17:15:24 2007
From: matteo.dallamico at ing.unitn.it (Matteo Dall'Amico)
Date: Fri, 6 Jul 2007 17:15:24 +0200
Subject: [R] import DTM with readRAST6()
Message-ID: <15794C14-BDAA-481C-BF95-1B079B737E17@ing.unitn.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/9e3aaf5b/attachment.pl 

From lepape.gilles at neuf.fr  Fri Jul  6 17:51:28 2007
From: lepape.gilles at neuf.fr (LE PAPE Gilles)
Date: Fri, 06 Jul 2007 17:51:28 +0200
Subject: [R] Text Mining
Message-ID: <02f001c7bfe5$8451ae30$8b28fea9@nom17f32728f76>

Hi everybody,
I am a new R user. Is there any package devoted to "text mining" analysis in 
R ?
Thanks
Gilles
lepape.gilles at neuf.fr


From ggrothendieck at gmail.com  Fri Jul  6 18:02:06 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 6 Jul 2007 12:02:06 -0400
Subject: [R] Text Mining
In-Reply-To: <02f001c7bfe5$8451ae30$8b28fea9@nom17f32728f76>
References: <02f001c7bfe5$8451ae30$8b28fea9@nom17f32728f76>
Message-ID: <971536df0707060902h6bda374p522a22a5aa2ba481@mail.gmail.com>

The gsubfn package has been used for linguistic analysis.  See the following
demo:

library(gsubfn)
demo("gsubfn-gries")

The gsubfn home page is at:

   http://code.google.com/p/gsubfn/

That will point you to more info including a vignette (i.e. pdf document).

On 7/6/07, LE PAPE Gilles <lepape.gilles at neuf.fr> wrote:
> Hi everybody,
> I am a new R user. Is there any package devoted to "text mining" analysis in
> R ?
> Thanks
> Gilles
> lepape.gilles at neuf.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jporzak at gmail.com  Fri Jul  6 18:26:46 2007
From: jporzak at gmail.com (Jim Porzak)
Date: Fri, 6 Jul 2007 09:26:46 -0700
Subject: [R] Text Mining
In-Reply-To: <02f001c7bfe5$8451ae30$8b28fea9@nom17f32728f76>
References: <02f001c7bfe5$8451ae30$8b28fea9@nom17f32728f76>
Message-ID: <2a9c000c0707060926x4fb3a073s56c421af7451baa8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/70ac084a/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Fri Jul  6 18:54:30 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 06 Jul 2007 17:54:30 +0100 (BST)
Subject: [R] Recursion in R ...
Message-ID: <XFMail.070706175430.ted.harding@nessie.mcc.ac.uk>

Hi Folks,

R has known speed issues for recursive definitions.
There was a thread "Extremely slow recursion in R?" in
August 2006 (24 Aug, from Jason Liao), with some
interesting comparisons between programming languages.

I'm re-opening the topic, with an ulterior motive
(stated at the end).

The starting point was the question "How many distinct
increasing sequences of length n can be made from k
integers (1:k)?", i.e. what is the size of the sample
space of

  sort(sample((1:k),n,replace=TRUE))

Let this number be Nnk(n,k). I aptly observed that

  Nnk(1,k) = k
  Nnk(n,k) = Nnk(n-1,k) + sum[r= 1 to k](Nnk(n-1,r))

So I slickly wrote a recursive definition:

Nnk<-function(n,k){
  if(n==1) {return(k)} else {
    R<-0;
    for(r in (1:k)) R<-(R+Nnk(n-1,k-r+1,depth))
  }
  return(R)
}

I then ran a few test cases, to check timing etc.:

Nnk(21, 7)   34 seconds
Nnk(21, 8)  120 seconds
Nnk(21, 9)  384 seconds
Nnk(21,10) 1141 seconds

I then complacently set it to work out the number I happened
to really want:

Nnk(21,20)

24 hours later, no result; and an extra "test line" (not
shown above) at the start of the function had prodced no
output, so the function had not even returned to the top
level for the first time.

Then, with heavy thunderstorms rolling in, and my mains
power beginning to flicker, I closed down the computer.

A quick extrapolation based on the above results, and a
few others, suggested that the time for completion of
Nnk(21,20) would be about 2 years.

Then I thought about it.

The recursion relation in fact shows that Nnk(n,k) is
the maximum value in cumsum(Nnk(n,r)) over r = 1:k.

Hence (for n>=1 and r>=1):

Nnk<-function(n,k){
  K<-rep(1,k)
  for(i in (1:n)){
    K<-cumsum(K)
  }
  return(max(K))
}

This definition then gave me, in so brief a blink of the
eye that I could make no estimate of it, the result:

Nnk(21,20) = 131282408400

In fact, I had to go up to things like

Nnk(1000,200) = 3.335367e+232

before I could sensibly perceive the delay (about 0.5 seconds
in this case).

On the old recursive definition, the computation might have
outlasted the Universe.



ON THAT BASIS: I hereby claim the all-time record for inefficient
programming in R.

Challengers are invited to strut their stuff ...

Best wishes to all, and happy end of week,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Jul-07                                       Time: 17:53:55
------------------------------ XFMail ------------------------------


From wssecn at uol.com.br  Fri Jul  6 19:24:40 2007
From: wssecn at uol.com.br (wssecn)
Date: Fri,  6 Jul 2007 14:24:40 -0300
Subject: [R] Text Mining
Message-ID: <JKRPP4$105321D2B2DC2E5385A3CD0B73246365@uol.com.br>

See the tm package.

Washington S. Silva


> Hi everybody,
> I am a new R user. Is there any package devoted to "text mining" analysis in 
> R ?
> Thanks
> Gilles
> lepape.gilles at neuf.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From albmont at centroin.com.br  Fri Jul  6 20:19:27 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Fri, 6 Jul 2007 16:19:27 -0200
Subject: [R] Recursion in R ...
In-Reply-To: <XFMail.070706175430.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070706175430.ted.harding@nessie.mcc.ac.uk>
Message-ID: <20070706181429.M26735@centroin.com.br>

Ted Harding wrote:
>
> So I slickly wrote a recursive definition:
> 
> Nnk<-function(n,k){
>   if(n==1) {return(k)} else {
>     R<-0;
>     for(r in (1:k)) R<-(R+Nnk(n-1,k-r+1)) # ,depth))
>   }
>   return(R)
> }
> 
You are aware that this is equivalent to:

Nnk1 <- function(n, k) { prod(1:(n+k-1)) / prod(1:n) / prod(1:(k-1)) }

aren't you?
 
> ON THAT BASIS: I hereby claim the all-time record for inefficient
> programming in R.
> 
> Challengers are invited to strut their stuff ...
> 
No, I don't think I can bet you unintentionally, even though
my second computer program that I ever wrote in life had to be
aborted, because it consumed all the resources of the computer.

Alberto Monteiro


From Mark.Leeds at morganstanley.com  Fri Jul  6 20:48:09 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Fri, 6 Jul 2007 14:48:09 -0400
Subject: [R] algebra/moving average question - NOTHING TO DO WITH R
Message-ID: <D3AEEDA31E57474B840BEBC25A8A8344019574B2@NYWEXMB23.msad.ms.com>

This has ABSOLUTELY nothing to do with R but I was hoping that someone
might know because there are obviously a lot of very bright people on
this list.

Suppose I had a time series of data and at each point in time t, I was
calculating x bar + plus minus sigma where x bar was based on a 
moving window of size n and so was sigma.

So, if I was at time t , then x bar t plus minus sigma_t would be based
on the values of x_t-n+1 through x_t.
 
This is the hard part : Is there a way to back out what the next
value(s), x_t+1 would have to be in order to for that value to
be either 

greater than  x bar_t+1 plus Z*sigma_t+1 

or 

less than  x bar_t+1 plus minus Z*sigma_t+1. 

where Z is whatever constant ?

I started to try to figure this out but the data window changes and the
x bar is in the formula for sigma so the algebra got overwhelming but I
was never an algebra whiz. thanks.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From maitra at iastate.edu  Fri Jul  6 21:13:03 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Fri, 6 Jul 2007 14:13:03 -0500
Subject: [R] loading package in LINUX
In-Reply-To: <49f266b90707060604i2ca8e0c0w313858cdbaea678e@mail.gmail.com>
References: <49f266b90707060604i2ca8e0c0w313858cdbaea678e@mail.gmail.com>
Message-ID: <20070706141303.2ba0c75b@triveni.stat.iastate.edu>

On Fri, 6 Jul 2007 18:34:27 +0530 "Regina Verghis"
<regina.verghis at gmail.com> wrote:

> I am comfortable with windows based R. But recently I had shifted to
> LINUX(Red Hat Linux Enterprise Guide 4)
> 1) I want to load J K Lindsey's repeated library in R. How to install
> the packge?

Hi,

Use install.packages() which is the command line interface to what that
Windows gui does....


> 2) How to create the shared library if I ve the fortran codes(I
> haven't done creation of shared library in windows also).

Isn't is R CMD SHLIB etc etc..... two separate words with a space in
between R and CMD? Please note that case matters in the linux/unix
world.

HTH,
Ranjan

> I had run the command Rcmd in bin directory but an error message
> "bash: Rcmd: command not found" is produced.
> Thank You
> With Regards,
> Regina
> 
> 
> -- 
> regina.verghis at gmail.com
> Regina M.Verghis,
> Post Graduate Student
> Department of Biostatistics,
> Christian Medical College,
> Vellore, Tamilnadu- 632002
> India.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.
>


From bartjoosen at hotmail.com  Fri Jul  6 14:50:13 2007
From: bartjoosen at hotmail.com (Bartjoosen)
Date: Fri, 6 Jul 2007 05:50:13 -0700 (PDT)
Subject: [R] HTML.data.frame
In-Reply-To: <363599.67568.qm@web27506.mail.ukl.yahoo.com>
References: <363599.67568.qm@web27506.mail.ukl.yahoo.com>
Message-ID: <11463723.post@talk.nabble.com>


Hi,

Don't know what you are trying to do, but did you load the package after
installing?
eg: library(R2HTML)


Bart


elyakhlifi mustapha wrote:
> 
> hello,
> I installed the R2HTML package but when I wanna execute the
> HTML.data.frame function I can't and I don't know why
> can you help me please?
> thanks.
> 
> 
>      
> _____________________________________________________________________________ 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/HTML.data.frame-tf4035290.html#a11463723
Sent from the R help mailing list archive at Nabble.com.


From bartjoosen at hotmail.com  Fri Jul  6 14:53:05 2007
From: bartjoosen at hotmail.com (Bartjoosen)
Date: Fri, 6 Jul 2007 05:53:05 -0700 (PDT)
Subject: [R] t.test
In-Reply-To: <11460445.post@talk.nabble.com>
References: <11460445.post@talk.nabble.com>
Message-ID: <11463812.post@talk.nabble.com>


and do read the R-manual about how to make a vector....


matthew5555 wrote:
> 
> Hi, how can I solve a problem without the function t.test???
> 
> for example:
> x<-(1,3,5,7)
> y<-(2,4,6)
> t.test(x,y,alternative="less",paired=FALSE,var.equal=TRUE,conf.level=0.95)
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/t.test-tf4034225.html#a11463812
Sent from the R help mailing list archive at Nabble.com.


From ferri.leberl at gmx.at  Fri Jul  6 12:32:54 2007
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Fri, 06 Jul 2007 12:32:54 +0200
Subject: [R] histogram with absolute figures
Message-ID: <1183717974.8176.1.camel@localhost>

Dear everybody!
Is ist easily possible to make up a histogram with absolute numbers
instead of percentages?
Thank you in advance!
Yours, Mag. Ferri Leberl


From juliane_willert at web.de  Fri Jul  6 00:31:32 2007
From: juliane_willert at web.de (Juliane Willert)
Date: Fri, 06 Jul 2007 00:31:32 +0200
Subject: [R] Markov Chain Model Simulation
Message-ID: <468D7144.9080302@web.de>

Hi everybody,

I have not worked yet very much with R and must investigate a Monte 
Carlo Simulation.
My model contains an autoregression(1) and a two state markov chain.

For example:
X_t  = Tau_t + u_t
Tau_t = nu_t + Tau_{t-1}
nu_t = nu_1 * S_t + nu_0 * (1-S_t)
phi(L) u_t = epsilon_t, epsilon_t is i.i.d.(0, sigma^2_epsilon)
S_t is the markov switching variable and is either 0 or 1. phi(L) is the 
first lag of the residuals.

I would appreciate to get any hint how to simulate such a model in R .

Thank you,
Juliane


From stefano.colucci1 at libero.it  Fri Jul  6 09:31:53 2007
From: stefano.colucci1 at libero.it (stefano.colucci1 at libero.it)
Date: Fri,  6 Jul 2007 09:31:53 +0200
Subject: [R] Fees to use R
Message-ID: <JKQY95$CBC9924C2D4045736CEA5404B9A15221@libero.it>

Good morning to all,

I work for a bank in Italy, I want to know if i can install R and relative add on like Rbloomberg for free or my company has to pay some fee.
tanks to all.
Stefano Colucci


------------------------------------------------------
Scegli infostrada: ADSL gratis per tutta l?estate e telefoni senza canone Telecom
http://click.libero.it/infostrada


From rizwanyounis at hotmail.com  Fri Jul  6 14:34:08 2007
From: rizwanyounis at hotmail.com (Rizwan Younis)
Date: Fri, 6 Jul 2007 08:34:08 -0400
Subject: [R] VGLM cumulative logit/probit models
Message-ID: <BAY116-DAV100FD536E4C88EC250C545C1010@phx.gbl>

Dear R experts:

I have some confusion about vglm function for cumulative logit/probit
models. Does the function checks for parallelism? I was wondering what
happens if we say parallel = FALSE in case of cumulative probit/logit model.
Does vglm fits dichotomous models in that case ? 

Thanks
Reez You
Graduate Student
Dept. of Civil and Environmental Engineering
University of Waterloo
Waterloo, ON Canada


From sbearer at tnc.org  Fri Jul  6 18:43:23 2007
From: sbearer at tnc.org (Scott Bearer)
Date: Fri, 6 Jul 2007 12:43:23 -0400
Subject: [R] Clustering nested data
Message-ID: <FOEPJOMFPIAGCACENKDMMEIICEAA.sbearer@tnc.org>

Hi all,

I am interested in performing a cluster analysis on ecological data from
forests in Pennsylvania.  I would like to develop definitions for forest
types (red maple forests, upland oak forests, etc.(AH AR in attached table))
based on measured attributes in each forest type.  To do this, I would like
to 'draw clusters' around forest types based on information from various
tree species (red maple, red oak, etc.(837, 832 in attached table))
occurring in those forests.  Each row of data includes mean values on a
particular species occurring within a forest type at a particular site.  In
other words, if we monitored 10 sites in red maple forests, we would only
have 10 rows of data for the tree species 'red maple', even though we
measured 100 trees.

I have used classification trees to examine this data, which I like because
of it's predictive abilities for later 'unknown' datasets.  However, my
concern is that the mean species attributes (columns Diameter:Avgnumtrees in
attached table) are associated with the tree species (nested?)(column
Treespecies in attached table) and are not independent attributes, but are
directly associated with the species listed in that row.

My question is, what is the best way to conduct a clustering (I have also
tried hclust, cclust and flexclust) or CART model with this sort of nested
data?
Also, what is the preferrable method for predicting a new dataset once these
clusters or CART models have been developed?

Any help would be greatly appreciated.

Kind regards,
Scott


----------------------------------------------------------------------------
----

      Scott L. Bearer, Ph.D.
      Forest Ecologist

      sbearer at tnc.org
      (570) 321-9092 (Office)
      (570) 321-9096 (Fax)
      (570) 460-0778 (Mobile)       The Nature Conservancy
        in Pennsylvania


      Community Arts Center
      220 West Fourth Street, 3rd Floor
      Williamsport, PA  17701


      nature.org


From peterson at heritage.nv.gov  Fri Jul  6 20:42:48 2007
From: peterson at heritage.nv.gov (Eric Peterson)
Date: Fri, 6 Jul 2007 11:42:48 -0700
Subject: [R] access to 'formula' terms in a user function
Message-ID: <200707061842.l66IgsPa010751@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/16545a22/attachment.pl 

From sarah.goslee at gmail.com  Fri Jul  6 22:17:03 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 6 Jul 2007 16:17:03 -0400
Subject: [R] histogram with absolute figures
In-Reply-To: <1183717974.8176.1.camel@localhost>
References: <1183717974.8176.1.camel@localhost>
Message-ID: <efb536d50707061317w3d9da10dn155d0d3b74489e92@mail.gmail.com>

The default of hist() is counts rather than percentages.

Sarah

On 7/6/07, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Dear everybody!
> Is ist easily possible to make up a histogram with absolute numbers
> instead of percentages?
> Thank you in advance!
> Yours, Mag. Ferri Leberl
>
> ___________________
-- 
Sarah Goslee
http://www.functionaldiversity.org


From compusale2007 at gmail.com  Fri Jul  6 22:33:44 2007
From: compusale2007 at gmail.com (Computer City)
Date: Fri, 06 Jul 2007 23:33:44 +0300
Subject: [R] Safe Buy Original Laptops
Message-ID: <200707062033.l66KXdHA011245@hypatia.math.ethz.ch>

Dear Sir/Madam,
           We are a laptop seller, all our products is original, we sell all products Brand new original and with its full accessories along with 3 years international warranty, some of our prices is as follows

Sony VAIO B100B - $420:00
Sony VAIO FS790 - $440:00
Sony VAIO FJ170/B - $750:00
Toshiba satellite r15-s822 - $380:00
Toshiba satellite m45-s265 - $320:00
Toshiba satellite m45-s355 - $400:00
Toshiba satellite a75-s213 - $410:00
dell xps M2010 - $900
Dell XPS M1710 - $699:00
Dell Inspiron E1705 $435:00
Dell Inspiron E1505 $295:00 USD
Lenovo ThinkPad R60e 0657 - $280
ThinkPad T42 2372 Pentium M 735 - $980
ThinkPad T43 2668 Pentium M 750 - $1010
Acer Aspire 1353 XC Home $ 240.00  
Acer Aspire 1353 XC Pro  $ 500.00  
Acer Aspire 1501LCe      $ 580.00 	
HP Evo 620C DE262A       $ 700.00  
Compaq HP Evo 620C DE271A$ 800.00  
HP NX9010 316T           $ 890.00  
FUJITSU SIEMENS Amilo D1840 $ 850.00  
FUJITSU SIEMENS Lifebook T 3010 $ 700.00  
FUJITSU SIEMENS Lifebook S 6120 $ 880.00  

If you need the full price list or you would like to ask about any other products as we also sell
Mobile phones (Nokia made in Finland) , PlayStation3 (PAL, NTSC,SECAM), plasma screen, Nintendo,.....
we have the safest payment terms in the online market

Contact Details
Web Site
www.computercityegypt.info
MSN
compu_sale_2007 at hotmail.com
Email
compusale2007 at gmail.com
  

From gavin.simpson at ucl.ac.uk  Fri Jul  6 22:36:44 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 06 Jul 2007 21:36:44 +0100
Subject: [R] Fees to use R
In-Reply-To: <JKQY95$CBC9924C2D4045736CEA5404B9A15221@libero.it>
References: <JKQY95$CBC9924C2D4045736CEA5404B9A15221@libero.it>
Message-ID: <1183754204.3508.6.camel@graptoleberis.geog.ucl.ac.uk>

On Fri, 2007-07-06 at 09:31 +0200, stefano.colucci1 at libero.it wrote:
> Good morning to all,
> 
> I work for a bank in Italy, I want to know if i can install R and
> relative add on like Rbloomberg for free or my company has to pay some
> fee.
> tanks to all.
> Stefano Colucci

R is released under the GNU GPL licence version 2. You can read the
licence online here:

http://www.gnu.org/copyleft/gpl.html

As such R is free (as in beer) and you can install it without paying a
fee. The source code is also free (as in speech) and is available from
www.r-project.org as are pre-compiled binaries for various systems.

You are however bound by the GPL licence and you should evaluate the
implication of the GPL for the use you/your employer has in mind.

HTH

G


From baron at psych.upenn.edu  Fri Jul  6 22:41:31 2007
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 6 Jul 2007 16:41:31 -0400
Subject: [R] loading package in LINUX
In-Reply-To: <20070706141303.2ba0c75b@triveni.stat.iastate.edu>
References: <49f266b90707060604i2ca8e0c0w313858cdbaea678e@mail.gmail.com>
	<20070706141303.2ba0c75b@triveni.stat.iastate.edu>
Message-ID: <20070706204131.GA3329@psych.upenn.edu>

On 07/06/07 14:13, Ranjan Maitra wrote:
> On Fri, 6 Jul 2007 18:34:27 +0530 "Regina Verghis"
> <regina.verghis at gmail.com> wrote:
> 
> > I am comfortable with windows based R. But recently I had shifted to
> > LINUX(Red Hat Linux Enterprise Guide 4)
> > 1) I want to load J K Lindsey's repeated library in R. How to install
> > the packge?
> 
> Hi,
> 
> Use install.packages() which is the command line interface to what that
> Windows gui does....

I don't think this will work for Lindsey's packages, as they are not
on CRAN.  You have to get them from his web page, e.g.:
wget http://popgen.unimaas.nl/~jlindsey/rcode/repeated.tgz

Then something like
R CMD INSTALL repeated.tgz
after you become root.

But I'm pretty sure that this package depends on others.  See
http://popgen.unimaas.nl/~jlindsey/rcode.html

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron


From deepayan.sarkar at gmail.com  Fri Jul  6 23:01:02 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 6 Jul 2007 14:01:02 -0700
Subject: [R] algebra/moving average question - NOTHING TO DO WITH R
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A8344019574B2@NYWEXMB23.msad.ms.com>
References: <D3AEEDA31E57474B840BEBC25A8A8344019574B2@NYWEXMB23.msad.ms.com>
Message-ID: <eb555e660707061401ga25ff91yad0cc4fbeb8bcef8@mail.gmail.com>

On 7/6/07, Leeds, Mark (IED) <Mark.Leeds at morganstanley.com> wrote:
> This has ABSOLUTELY nothing to do with R but I was hoping that someone
> might know because there are obviously a lot of very bright people on
> this list.
>
> Suppose I had a time series of data and at each point in time t, I was
> calculating x bar + plus minus sigma where x bar was based on a
> moving window of size n and so was sigma.
>
> So, if I was at time t , then x bar t plus minus sigma_t would be based
> on the values of x_t-n+1 through x_t.
>
> This is the hard part : Is there a way to back out what the next
> value(s), x_t+1 would have to be in order to for that value to
> be either
>
> greater than  x bar_t+1 plus Z*sigma_t+1
>
> or
>
> less than  x bar_t+1 plus minus Z*sigma_t+1.
>
> where Z is whatever constant ?
>
> I started to try to figure this out but the data window changes and the
> x bar is in the formula for sigma so the algebra got overwhelming but I
> was never an algebra whiz. thanks.

Here's one approach:

Define

T1(t, n) = \sum_{k=t-n+1}^t X_k
T2(t, n) = \sum_{k=t-n+1}^t {X_k}^2

Then, dropping the dependence on (t, n),

xbar = T1 / n
sigma^2 = (T2 - T1^2 / n) / (n-1)

The bounds you want are

X_{t+1} = xbar(t + 1, n) +/- Z sigma(t + 1, n)

Note that

T1(t + 1, n) = T1(t, n - 1) + X_{t+1}
T2(t + 1, n) = T2(t, n - 1) + {X_{t+1}}^2

This should give you a quadratic equation for X_{t+1}.  If my
calculations are correct (they may not be, so you should check), this
equation is

Ax^2 + Bx + C = 0, where

A = (n-1)^3 - Z^2 * n * (n-1)
B = -2 * (Z^2 * n + (n-1)^2) * T1(t, n-1)
C = (Z^2 * n + n - 1) * T1(t, n-1)^2 - Z^2 * n^2 * T2(t, n-1)

Hope this helps,
-Deepayan


From ggrothendieck at gmail.com  Fri Jul  6 23:12:54 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 6 Jul 2007 17:12:54 -0400
Subject: [R] access to 'formula' terms in a user function
In-Reply-To: <200707061842.l66IgsPa010751@hypatia.math.ethz.ch>
References: <200707061842.l66IgsPa010751@hypatia.math.ethz.ch>
Message-ID: <971536df0707061412w4c374b60p9a5213f8954dfacd@mail.gmail.com>

Try this:


MyFunction <- function(formula, data) {
	vars <- all.vars(formula)
	data.name <- deparse(substitute(data))
	for(v in vars)
		cat("Mean of ", data.name, "$", v, ": ",
			mean(data[[v]]), "\n", sep = "")
	invisible(colMeans(data[vars]))
}
MyFunction(~ Sepal.Length + Petal.Length, iris)
out <- MyFunction(~ Sepal.Length + Petal.Length, iris)
out




On 7/6/07, Eric Peterson <peterson at heritage.nv.gov> wrote:
> This is probably buried somewhere in the R help archives, but I've been
> unable to find it, so perhaps the keywords I use here will help bring the
> topic to the surface more easily for future users.
>
> I want to write my own modeling function (ultimately for some
> multidimensional windowing - but this question is on scripting basics).  For
> purposes of figuring out my needs, lets just consider writing a function
> that takes a formula and a dataset, and outputs the mean of each variable in
> the formula model.  So:
>
> MyFunction <- function(AFormula, ADataFrame) {
>  ...
>  }
>
> >MyFunction(Y ~ A + B, TheData)
>
> would give results something like:
>
> Mean of MyData$Y is 5.3
> Mean of MyData$A is 3.4
> Mean of MyData$B is 8.2
>
> QUESTION 1:
> How do I determine the terms in the formula within the function?
>
> QUESTION 2:
> How do I use one of those terms to get to the individual columns, like
> MyData$Y ?
>
> Thanks!
> -Eric
> ---
> Eric Peterson
> Vegetation Ecologist
> Nevada Natural Heritage Program
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From apezzola at reed.edu  Fri Jul  6 23:31:41 2007
From: apezzola at reed.edu (Anthony Pezzola)
Date: Fri, 06 Jul 2007 14:31:41 -0700
Subject: [R] Changing Tick Mark Values for lattice / wireframe
Message-ID: <20070706143141.txjjoea8jvlwo48s@unagi.reed.edu>

How can I change the tick mark values in lattice, specifically wireframe?

I have a 11*46 matrix of values that I am plotting using wireframe.   
Unfortunely, the values range from 0.1-1.1 and 0.5-5.  Using the code  
below the tick marks have are (2,4,6,8,10) and (10,20,30,40).

Thanks in advance.

graphic5 <- wireframe(output.matrix, shade= TRUE,
scales = list(arrows = FALSE,
cex=.6, col="black", font= 3, tck=1),
xlab=list("Employees", cex=.65, col="black", rot=30),
ylab=list("Political Concentration", cex=.65, col="black", rot=-40),
zlab=list(label="Probability of Protection", cex=.65, rot=90, col="black")
)


From afshart at exchange.sba.miami.edu  Fri Jul  6 23:44:21 2007
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Fri, 6 Jul 2007 17:44:21 -0400
Subject: [R] maintaining specified factor contrasts when subsetting in lmer
Message-ID: <6BCB4D493A447546A8126F24332056E8063E8B57@school1.business.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/f62917f3/attachment.pl 

From deepayan.sarkar at gmail.com  Fri Jul  6 23:51:30 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 6 Jul 2007 14:51:30 -0700
Subject: [R] Changing Tick Mark Values for lattice / wireframe
In-Reply-To: <20070706143141.txjjoea8jvlwo48s@unagi.reed.edu>
References: <20070706143141.txjjoea8jvlwo48s@unagi.reed.edu>
Message-ID: <eb555e660707061451qe281324tb4d9f193f132d879@mail.gmail.com>

On 7/6/07, Anthony Pezzola <apezzola at reed.edu> wrote:
> How can I change the tick mark values in lattice, specifically wireframe?
>
> I have a 11*46 matrix of values that I am plotting using wireframe.
> Unfortunely, the values range from 0.1-1.1 and 0.5-5.  Using the code
> below the tick marks have are (2,4,6,8,10) and (10,20,30,40).

You will need to use the formula method. Here's one possibility:

mydata <-
    expand.grid(row = rowvals,
                column = colvals)
mydata$z <- as.vector(as.numeric(output.matrix))

wireframe(z ~ x * y, data = mydata, ...)

It probably makes sense to allow this for matrices directly, and I'll
add something in the next update.

-Deepayan


> Thanks in advance.
>
> graphic5 <- wireframe(output.matrix, shade= TRUE,
> scales = list(arrows = FALSE,
> cex=.6, col="black", font= 3, tck=1),
> xlab=list("Employees", cex=.65, col="black", rot=30),
> ylab=list("Political Concentration", cex=.65, col="black", rot=-40),
> zlab=list(label="Probability of Protection", cex=.65, rot=90, col="black")
> )
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gerifalte28 at hotmail.com  Sat Jul  7 00:40:28 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 06 Jul 2007 16:40:28 -0600
Subject: [R] maintaining specified factor contrasts when subsetting in
	lmer
In-Reply-To: <6BCB4D493A447546A8126F24332056E8063E8B57@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8063E8B57@school1.business.edu>
Message-ID: <468EC4DC.6060401@hotmail.com>

See ?relevel

Regards,

Francisco

Afshartous, David wrote:
> All,
>  
> I'm using lmer for some repeated measures data and have specified 
> the contrasts for a time factor such that say time 3 is the base.   This
> works fine.  However, when 
> I next use the subset argument to remove the last two time values, the
> output indicates that
> the specified contrast is not maintained (see below).   I can solve this
> by creating a new dataframe
> for the subset of interest and redefining the constrasts, but I was
> wondering if there is a direct method that
> allows me to continue w/ the subset argument?  (perhaps via supplying a
> contrast argument to lmer 
> directly, but this doesn't seem possible based on the defintion of this
> argument in model.matrix.default).
>  
> Thanks,
> Dave
>  
>  
> z <- rnorm(24, mean=0, sd=1)
> time <- factor(paste("Time-", rep(1:6, 4), sep="")) 
> Patient <- rep(1:4, each = 6) 
> drug <- factor(rep(c("D", "P"), each = 6, times = 2)) ## P = placebo, D
> = Drug
> dat.new <- data.frame(time, drug, z, Patient) 
>  
> ### specify the contrast as time 3:
> contrasts(dat.new$time) <- contr.treatment(6, base=3)     
> dimnames(contrasts(dat.new$time))[[1]] <-
> as.character(levels(dat.new$time))    
> dimnames(contrasts(dat.new$time))[[2]] <-
> as.character(levels(dat.new$time)[-3])
>  
> fm1 <- lmer(z ~ drug + time + (1 | Patient), data = dat.new )
> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept) -0.182774   0.464014 -0.39390
> drugP       -0.281103   0.352309 -0.79789
> timeTime-1   0.150505   0.606462  0.24817
> timeTime-2   0.612016   0.606462  1.00916
> timeTime-4   0.775342   0.606462  1.27847
> timeTime-5   0.093741   0.606462  0.15457
> timeTime-6   0.452442   0.606462  0.74604
>  
> ## time 3 is the base as specified
> 
> fm2 <-  lmer(z ~ drug + time + (1 | Patient), data = dat.new, 
> subset = dat.new$time !="Time-6" & dat.new$time != "Time-5")
> 
> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept)  0.052975   0.500675  0.10581
> drugP       -0.451593   0.447818 -1.00843
> timeTime-2   0.461511   0.633310  0.72873
> timeTime-3  -0.150505   0.633310 -0.23765
> timeTime-4   0.624837   0.633310  0.98662
>  
> ### time 3 no longer the base; was expecting to see the fixed effects
> for time-1, time-2, and time-4, w/ Intercept 
> ### representing time-3
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gerifalte28 at hotmail.com  Sat Jul  7 00:40:28 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 06 Jul 2007 16:40:28 -0600
Subject: [R] maintaining specified factor contrasts when subsetting in
	lmer
In-Reply-To: <6BCB4D493A447546A8126F24332056E8063E8B57@school1.business.edu>
References: <6BCB4D493A447546A8126F24332056E8063E8B57@school1.business.edu>
Message-ID: <468EC4DC.6060401@hotmail.com>

See ?relevel

Regards,

Francisco

Afshartous, David wrote:
> All,
>  
> I'm using lmer for some repeated measures data and have specified 
> the contrasts for a time factor such that say time 3 is the base.   This
> works fine.  However, when 
> I next use the subset argument to remove the last two time values, the
> output indicates that
> the specified contrast is not maintained (see below).   I can solve this
> by creating a new dataframe
> for the subset of interest and redefining the constrasts, but I was
> wondering if there is a direct method that
> allows me to continue w/ the subset argument?  (perhaps via supplying a
> contrast argument to lmer 
> directly, but this doesn't seem possible based on the defintion of this
> argument in model.matrix.default).
>  
> Thanks,
> Dave
>  
>  
> z <- rnorm(24, mean=0, sd=1)
> time <- factor(paste("Time-", rep(1:6, 4), sep="")) 
> Patient <- rep(1:4, each = 6) 
> drug <- factor(rep(c("D", "P"), each = 6, times = 2)) ## P = placebo, D
> = Drug
> dat.new <- data.frame(time, drug, z, Patient) 
>  
> ### specify the contrast as time 3:
> contrasts(dat.new$time) <- contr.treatment(6, base=3)     
> dimnames(contrasts(dat.new$time))[[1]] <-
> as.character(levels(dat.new$time))    
> dimnames(contrasts(dat.new$time))[[2]] <-
> as.character(levels(dat.new$time)[-3])
>  
> fm1 <- lmer(z ~ drug + time + (1 | Patient), data = dat.new )
> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept) -0.182774   0.464014 -0.39390
> drugP       -0.281103   0.352309 -0.79789
> timeTime-1   0.150505   0.606462  0.24817
> timeTime-2   0.612016   0.606462  1.00916
> timeTime-4   0.775342   0.606462  1.27847
> timeTime-5   0.093741   0.606462  0.15457
> timeTime-6   0.452442   0.606462  0.74604
>  
> ## time 3 is the base as specified
> 
> fm2 <-  lmer(z ~ drug + time + (1 | Patient), data = dat.new, 
> subset = dat.new$time !="Time-6" & dat.new$time != "Time-5")
> 
> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept)  0.052975   0.500675  0.10581
> drugP       -0.451593   0.447818 -1.00843
> timeTime-2   0.461511   0.633310  0.72873
> timeTime-3  -0.150505   0.633310 -0.23765
> timeTime-4   0.624837   0.633310  0.98662
>  
> ### time 3 no longer the base; was expecting to see the fixed effects
> for time-1, time-2, and time-4, w/ Intercept 
> ### representing time-3
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From s.blomberg1 at uq.edu.au  Sat Jul  7 02:36:22 2007
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Sat, 7 Jul 2007 10:36:22 +1000
Subject: [R] Fees to use R
References: <JKQY95$CBC9924C2D4045736CEA5404B9A15221@libero.it>
	<1183754204.3508.6.camel@graptoleberis.geog.ucl.ac.uk>
Message-ID: <DE3D1F203DAF7A4CB259560D2801DF8B3B2AA1@UQEXMB2.soe.uq.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/f9e37f62/attachment.pl 

From david.farrelly at gmail.com  Sat Jul  7 04:01:40 2007
From: david.farrelly at gmail.com (David Farrelly)
Date: Fri, 6 Jul 2007 20:01:40 -0600
Subject: [R] color scale in rgl plots
Message-ID: <96cc741a0707061901y3bf261a5i5620b0669598f447@mail.gmail.com>

Hello,

I'm trying to make a 3d plot using rgl in which the size and color of
each point corresponds to certain attributes of each data point. The color
attribute, let's call it X, is scaled to go from 0 to 1. The
rainbow(64,start=0.7,end=0.1) palette is perfect for what I want but I
don't know how to take that palette and pick a color from it based on
the value of X for a given data point. I'm fairly new to R and any
suggestions would be greatly appreciated.

Here's what I do - it's how to do the color mapping that has me stumped.

plot3d(th1,ph,th2,type='s',size=wd1,col=????(rp1,0,0,1),cex=2,ylab=NULL,xlab=NULL,zlab=NULL,xlim=c(0,1),ylim=c(0,2),zlim=c(0,1),box=TRUE,axes=FALSE)

I have also tried the more obvious col = rgb(a,b,c,d) where a,b,c,d are
functions of X but I can't manage to come up with a nice looking color
scale.

Thanks in advance,

David


From rmh at temple.edu  Sat Jul  7 05:59:54 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri,  6 Jul 2007 23:59:54 -0400 (EDT)
Subject: [R] color scale in rgl plots
Message-ID: <20070706235954.CFV26497@po-d.temple.edu>

> I can't manage to come up with a nice looking color scale. 


install package RColorBrewer

Description: The packages provides palettes for drawing nice maps
        shaded according to a variable.


?display.brewer.all
display.brewer.all()


From srinee.16 at gmail.com  Sat Jul  7 06:45:59 2007
From: srinee.16 at gmail.com (Neelima Aitha)
Date: Fri, 6 Jul 2007 23:45:59 -0500
Subject: [R] Error Coefficient matrix not invertible using nlme
	correlation
Message-ID: <f4ebac200707062145i702700d0l736e90a931632d7d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070706/f2257b75/attachment.pl 

From ripley at stats.ox.ac.uk  Sat Jul  7 08:46:30 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 Jul 2007 07:46:30 +0100 (BST)
Subject: [R] access to 'formula' terms in a user function
In-Reply-To: <200707061842.l66IgsPa010751@hypatia.math.ethz.ch>
References: <200707061842.l66IgsPa010751@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.64.0707070741180.24881@gannet.stats.ox.ac.uk>

On Fri, 6 Jul 2007, Eric Peterson wrote:

> This is probably buried somewhere in the R help archives, but I've been
> unable to find it, so perhaps the keywords I use here will help bring the
> topic to the surface more easily for future users.
>
> I want to write my own modeling function (ultimately for some
> multidimensional windowing - but this question is on scripting basics).  For
> purposes of figuring out my needs, lets just consider writing a function
> that takes a formula and a dataset, and outputs the mean of each variable in
> the formula model.  So:
>
> MyFunction <- function(AFormula, ADataFrame) {
> ...
> }
>
>> MyFunction(Y ~ A + B, TheData)
>
> would give results something like:
>
> Mean of MyData$Y is 5.3
> Mean of MyData$A is 3.4
> Mean of MyData$B is 8.2
>
> QUESTION 1:
> How do I determine the terms in the formula within the function?

The function you seem to be missing is called terms(), strangely enough.

> QUESTION 2:
> How do I use one of those terms to get to the individual columns, like
> MyData$Y ?

Use model.frame to collect the data you want.  What

 	MyFunction(Y ~ A + B, TheData)

conventionally means is to find Y, A, B, looking first in TheData (not 
MyData), then in the environment of the formula and then in the search 
path (see ?model.frame and 
http://developer.r-project.org/nonstandard-eval.pdf).

See also http://developer.r-project.org/model-fitting-functions.txt.


> Eric Peterson
> Vegetation Ecologist
> Nevada Natural Heritage Program

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From brown_emu at yahoo.com  Sat Jul  7 09:27:59 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 7 Jul 2007 00:27:59 -0700 (PDT)
Subject: [R] Multiple Stripcharts
In-Reply-To: <33846cd50707060121o76c3f962u6b6f07c3f4667b7a@mail.gmail.com>
Message-ID: <917474.48297.qm@web39715.mail.mud.yahoo.com>

I'm not able to make out your data but something like this?

df <- data.frame(A=rnorm(10),B=rnorm(10),C=runif(10))
stripchart(df,method="jitter")


--- Tavpritesh Sethi <tavpritesh at gmail.com> wrote:

> Hi all,
> I have 205 rows with measurements for three categories of people. I want to
> generate stripplots for each of these rows. How can I do it without having
> to do them one by one. I am giving a sample dataset:-
> 
>      A
>  B
>  C
>  A
>  B
>  C
>  A
>  B
>  C
>  A
>  B
>  C
>  10.34822
>  10.18426
>  9.837874
>  9.65047
>  8.020482
>  9.17312
>  6.349595
>  13.55664
>  5.286697
>  11.85409
>  2.827027
>  7.002696
>  11.54984
>  12.14591
>  14.88955
>  12.26134
>  11.74262
>  11.13481
>  15.11849
>  14.97857
>  14.12973
>  14.23219
>  15.36582
>  15.4698
>  10.59222
>  11.22417
>  13.34279
>  12.2538
>  11.02348
>  11.59403
>  9.933778
>  10.45499
>  8.884345
>  8.465186
>  9.72647
>  10.44469
> 
> 
> Thanks,
> Tavpritesh
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



 
____________________________________________________________________________________
Finding fabulous fares is fun.


From jim at bitwrit.com.au  Sat Jul  7 10:25:04 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 07 Jul 2007 18:25:04 +1000
Subject: [R] color scale in rgl plots
In-Reply-To: <96cc741a0707061901y3bf261a5i5620b0669598f447@mail.gmail.com>
References: <96cc741a0707061901y3bf261a5i5620b0669598f447@mail.gmail.com>
Message-ID: <468F4DE0.5000401@bitwrit.com.au>

David Farrelly wrote:
> Hello,
> 
> I'm trying to make a 3d plot using rgl in which the size and color of
> each point corresponds to certain attributes of each data point. The color
> attribute, let's call it X, is scaled to go from 0 to 1. The
> rainbow(64,start=0.7,end=0.1) palette is perfect for what I want but I
> don't know how to take that palette and pick a color from it based on
> the value of X for a given data point. I'm fairly new to R and any
> suggestions would be greatly appreciated.
> 
> Here's what I do - it's how to do the color mapping that has me stumped.
> 
> plot3d(th1,ph,th2,type='s',size=wd1,col=????(rp1,0,0,1),cex=2,ylab=NULL,xlab=NULL,zlab=NULL,xlim=c(0,1),ylim=c(0,2),zlim=c(0,1),box=TRUE,axes=FALSE)
> 
> I have also tried the more obvious col = rgb(a,b,c,d) where a,b,c,d are
> functions of X but I can't manage to come up with a nice looking color
> scale.
> 
Hi David,
You could check out color.scale in the plotrix package, which does just 
this.

Jim


From brown_emu at yahoo.com  Sat Jul  7 10:31:50 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 7 Jul 2007 01:31:50 -0700 (PDT)
Subject: [R] color scale in rgl plots
In-Reply-To: <96cc741a0707061901y3bf261a5i5620b0669598f447@mail.gmail.com>
Message-ID: <354787.53901.qm@web39706.mail.mud.yahoo.com>

Hi David,

I'm not an expert in 'rgl', but to determine data-dependent color for points
I often use cut().

# using a very simple example,
x <- 1:2; y <- 1:2; z <- matrix(1:4,ncol=2)

# the following image will be a projection of my intended 3-D 'rgl' plot
# into 2-D space (if we don't consider color to be a dimension):
library(fields)
image.plot(x,y,z)

# the 3-D rgl plot will be as follows:
df <- data.frame(x=rep(x,times=length(y)),
                 y=rep(y,each=length(x)),
                 z=as.vector(z))
plot3d(x=df,col=1:4,type="s")

## looks okay so moving onto bigger example:
x <- 1:10; y <- 1:10; z <- matrix(1:100,ncol=10)

# 2-D projection:
image.plot(x,y,z)

# 3-D plot in rgl
df <- data.frame(x=rep(x,times=length(y)),
                 y=rep(y,each=length(x)),
                 z=as.vector(z))
# This is how I determine color:
nColors <- 64
colindex <- as.integer(cut(df$z,breaks=nColors))
plot3d(x=df,type="s",col=tim.colors(nColors)[colindex])

===
tim.colors(nColors)[colindex] will return a vector of colors the same length
as 'df'.

I don't think as.integer() on cut() is entirely necessary because cut()
returns a factor... in any case, I use these integers as indices for
tim.colors() [you will need the 'fields' package for this set of colors].

Hope this helps.

ST


--- David Farrelly <david.farrelly at gmail.com> wrote:

> Hello,
> 
> I'm trying to make a 3d plot using rgl in which the size and color of
> each point corresponds to certain attributes of each data point. The color
> attribute, let's call it X, is scaled to go from 0 to 1. The
> rainbow(64,start=0.7,end=0.1) palette is perfect for what I want but I
> don't know how to take that palette and pick a color from it based on
> the value of X for a given data point. I'm fairly new to R and any
> suggestions would be greatly appreciated.
> 
> Here's what I do - it's how to do the color mapping that has me stumped.
> 
>
plot3d(th1,ph,th2,type='s',size=wd1,col=????(rp1,0,0,1),cex=2,ylab=NULL,xlab=NULL,zlab=NULL,xlim=c(0,1),ylim=c(0,2),zlim=c(0,1),box=TRUE,axes=FALSE)
> 
> I have also tried the more obvious col = rgb(a,b,c,d) where a,b,c,d are
> functions of X but I can't manage to come up with a nice looking color
> scale.
> 
> Thanks in advance,
> 
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tavpritesh at gmail.com  Sat Jul  7 11:19:00 2007
From: tavpritesh at gmail.com (Tavpritesh Sethi)
Date: Sat, 7 Jul 2007 14:49:00 +0530
Subject: [R] suggestions
Message-ID: <33846cd50707070219g6d527440j5dedc7784d8c49fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/e0a4dcd6/attachment.pl 

From ligges at statistik.uni-dortmund.de  Sat Jul  7 12:34:03 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 07 Jul 2007 12:34:03 +0200
Subject: [R] Recursion in R ...
In-Reply-To: <20070706181429.M26735@centroin.com.br>
References: <XFMail.070706175430.ted.harding@nessie.mcc.ac.uk>
	<20070706181429.M26735@centroin.com.br>
Message-ID: <468F6C1B.6050506@statistik.uni-dortmund.de>



Alberto Monteiro wrote:
> Ted Harding wrote:
>> So I slickly wrote a recursive definition:
>>
>> Nnk<-function(n,k){
>>   if(n==1) {return(k)} else {
>>     R<-0;
>>     for(r in (1:k)) R<-(R+Nnk(n-1,k-r+1)) # ,depth))
>>   }
>>   return(R)
>> }
>>
> You are aware that this is equivalent to:
> 
> Nnk1 <- function(n, k) { prod(1:(n+k-1)) / prod(1:n) / prod(1:(k-1)) }

or

Nnk2 <- function(n, k) { gamma(n+k) / gamma(n+1) / gamma(k) }

or most easily:

Nnk3 <- function(n, k) choose(n+k-1, n)


Uwe Ligges




> aren't you?
>  
>> ON THAT BASIS: I hereby claim the all-time record for inefficient
>> programming in R.
>>
>> Challengers are invited to strut their stuff ...
>>
> No, I don't think I can bet you unintentionally, even though
> my second computer program that I ever wrote in life had to be
> aborted, because it consumed all the resources of the computer.
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cgenolin at u-paris10.fr  Sat Jul  7 12:52:49 2007
From: cgenolin at u-paris10.fr (Christophe Genolini)
Date: Sat, 07 Jul 2007 12:52:49 +0200
Subject: [R] Changing integer class
Message-ID: <468F7081.2040308@u-paris10.fr>

Hi all

I define a function with two methods, one for numeric or integer. 
Sometime, I need the numeric method to be applied on integer. So I try 
to change the class of the integer but it does not work... Does someone 
know why ?

>  x<-1:3
>  class(x)
[1] "integer"
>  class(x)<- "numeric"
>  class(x)
[1] "integer"

Thanks

Christophe


From jholtman at gmail.com  Sat Jul  7 13:01:11 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 7 Jul 2007 07:01:11 -0400
Subject: [R] Changing integer class
In-Reply-To: <468F7081.2040308@u-paris10.fr>
References: <468F7081.2040308@u-paris10.fr>
Message-ID: <644e1f320707070401q1e485379l53416a193294132c@mail.gmail.com>

?mode

> x <- 1:10
> str(x)
 int [1:10] 1 2 3 4 5 6 7 8 9 10
> class(x) <- 'numeric'
> str(x)
 int [1:10] 1 2 3 4 5 6 7 8 9 10
> mode(x) <- 'numeric'
> str(x)
 num [1:10] 1 2 3 4 5 6 7 8 9 10



On 7/7/07, Christophe Genolini <cgenolin at u-paris10.fr> wrote:
> Hi all
>
> I define a function with two methods, one for numeric or integer.
> Sometime, I need the numeric method to be applied on integer. So I try
> to change the class of the integer but it does not work... Does someone
> know why ?
>
> >  x<-1:3
> >  class(x)
> [1] "integer"
> >  class(x)<- "numeric"
> >  class(x)
> [1] "integer"
>
> Thanks
>
> Christophe
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From unwin at math.uni-augsburg.de  Sat Jul  7 13:01:04 2007
From: unwin at math.uni-augsburg.de (Antony Unwin)
Date: Sat, 7 Jul 2007 13:01:04 +0200
Subject: [R] Multiple Stripcharts: Better parallel coordinates?
Message-ID: <D8B15F21-8583-4917-84DC-E31CD82D43A5@math.uni-augsburg.de>

Like Stephen Tucker I am not sure what your data are and what you  
actually want.  In the iPlots package you can draw interactive  
parallel coordinate plots or parallel boxplots of x by y (where y is  
a groups variable).

Antony


From ted.harding at nessie.mcc.ac.uk  Sat Jul  7 13:15:24 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 07 Jul 2007 12:15:24 +0100 (BST)
Subject: [R] Recursion in R ...
In-Reply-To: <468F6C1B.6050506@statistik.uni-dortmund.de>
Message-ID: <XFMail.070707121524.ted.harding@nessie.mcc.ac.uk>

On 07-Jul-07 10:34:03, Uwe Ligges wrote:
> Alberto Monteiro wrote:
>> Ted Harding wrote:
>>> So I slickly wrote a recursive definition:
>>>
>>> Nnk<-function(n,k){
>>>   if(n==1) {return(k)} else {
>>>     R<-0;
>>>     for(r in (1:k)) R<-(R+Nnk(n-1,k-r+1)) # ,depth))
>>>   }
>>>   return(R)
>>> }
>>>
>> You are aware that this is equivalent to:
>> 
>> Nnk1 <- function(n, k) { prod(1:(n+k-1)) / prod(1:n) / prod(1:(k-1)) }
> 
> or
> 
> Nnk2 <- function(n, k) { gamma(n+k) / gamma(n+1) / gamma(k) }
> 
> or most easily:
> 
> Nnk3 <- function(n, k) choose(n+k-1, n)
> 
> Uwe Ligges

OK, I can recognise a negative binomial coefficient when I see one!

I'm wondering, though, what is the "natural" connection between
choose(n+k-1, n) and the statement of the original question:

  What is the number of distinct non-decreasing sequences of length n
  which can be made using integers chosen from (1:k)?
  (repetition allowed, of course)

(The fact that this leads to a recurrence relationship which is
satisfied by choose(n+k-1,n) is not what I mean by "natural"
-- I'm looking for a correspondence between such a sequence, and
a choice of n out of (n+k-1) somethings).

Ted.

>> aren't you?
>>  
>>> ON THAT BASIS: I hereby claim the all-time record for inefficient
>>> programming in R.
>>>
>>> Challengers are invited to strut their stuff ...
>>>
>> No, I don't think I can bet you unintentionally, even though
>> my second computer program that I ever wrote in life had to be
>> aborted, because it consumed all the resources of the computer.
>> 
>> Alberto Monteiro
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 07-Jul-07                                       Time: 12:15:21
------------------------------ XFMail ------------------------------


From murdoch at stats.uwo.ca  Sat Jul  7 13:55:55 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 07 Jul 2007 07:55:55 -0400
Subject: [R] Recursion in R ...
In-Reply-To: <XFMail.070707121524.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070707121524.ted.harding@nessie.mcc.ac.uk>
Message-ID: <468F7F4B.2030107@stats.uwo.ca>

On 07/07/2007 7:15 AM, (Ted Harding) wrote:
> On 07-Jul-07 10:34:03, Uwe Ligges wrote:
>> Alberto Monteiro wrote:
>>> Ted Harding wrote:
>>>> So I slickly wrote a recursive definition:
>>>>
>>>> Nnk<-function(n,k){
>>>>   if(n==1) {return(k)} else {
>>>>     R<-0;
>>>>     for(r in (1:k)) R<-(R+Nnk(n-1,k-r+1)) # ,depth))
>>>>   }
>>>>   return(R)
>>>> }
>>>>
>>> You are aware that this is equivalent to:
>>>
>>> Nnk1 <- function(n, k) { prod(1:(n+k-1)) / prod(1:n) / prod(1:(k-1)) }
>> or
>>
>> Nnk2 <- function(n, k) { gamma(n+k) / gamma(n+1) / gamma(k) }
>>
>> or most easily:
>>
>> Nnk3 <- function(n, k) choose(n+k-1, n)
>>
>> Uwe Ligges
> 
> OK, I can recognise a negative binomial coefficient when I see one!
> 
> I'm wondering, though, what is the "natural" connection between
> choose(n+k-1, n) and the statement of the original question:
> 
>   What is the number of distinct non-decreasing sequences of length n
>   which can be made using integers chosen from (1:k)?
>   (repetition allowed, of course)
> 
> (The fact that this leads to a recurrence relationship which is
> satisfied by choose(n+k-1,n) is not what I mean by "natural"
> -- I'm looking for a correspondence between such a sequence, and
> a choice of n out of (n+k-1) somethings).

Colour the integers 1:k red and the integers 1:(n-1) black, and throw 
them in a hat.  Select n things out of the hat.

Put the red things in order:  that's the strictly increasing subsequence.

Put the black things in order.  Proceeding from smallest to largest, if 
you see a black i, duplicate the i'th element in the current version of 
the sequence.

For example, if k=5, n=4, you might draw red 2, 3 and black 1, 2, so 
you'd build your sequence as

2 3
2 2 3
2 2 2 3

or you might draw red 1, 4, 5 and black 2, so you'd output

1 4 4 5

Duncan Murdoch


From pomchip at free.fr  Sat Jul  7 14:45:39 2007
From: pomchip at free.fr (=?ISO-8859-1?Q?S=E9bastien?=)
Date: Sat, 07 Jul 2007 08:45:39 -0400
Subject: [R] Several quick questions
Message-ID: <468F8AF3.6010200@free.fr>

Dear R users,

Here is a couple a quick questions, for which I was unable to not find 
any answer in the list archives and in the help:

1- Is there any R equivalents of the VB functions Cint, CStr, etc... 
(for non VB users, these functions transform the category of a specified 
variable and smartly adapt the value of this variable) ?

I have tried to use the as.numeric, as.factor and as.vector commands but 
the result is not exactly what I want ([1] 1, 3, 5, 6)
 >a<-as.factor(cbind(1,3,5,6))      # creates a dummy factor
 >a
[1] 1 3 5 6
Levels: 1 3 5 6
 >> a<-as.vector(as.numeric(a))
 > a
[1] 1 2 3 4

2- When a log scale is called in a graph, the label takes a format like 
10^n. Is there a way to come back to a regular number format like 1, 10, 
100... without having to create a custom axis ?

3- In lattice graphics, how does the default value of the "axs" argument 
influence the values of "limits" ?
This question should be considered in the following context. The help 
states that a 4% extension is applied by default to the axis range in 
base graphics. So, I have tried to apply this 4 % extension to create 
some custom lattice graphics. I worked on a dataset in which the 
independent variable ranged from 0  to 120, so I basically customized my 
axis using limits=c(-4.8,124.8). The results of the graphics with and 
without the limits command were not identical...

Thanks in advance for your help.

Sebastien


From istazahn at gmail.com  Sat Jul  7 16:06:21 2007
From: istazahn at gmail.com (Ista Zahn)
Date: Sat, 7 Jul 2007 10:06:21 -0400
Subject: [R] Text Mining
In-Reply-To: <mailman.10.1183802404.2803.r-help@stat.math.ethz.ch>
References: <mailman.10.1183802404.2803.r-help@stat.math.ethz.ch>
Message-ID: <B060D3E6-A440-4D53-A347-EDCB1B8932BA@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/9d615725/attachment.pl 

From afshart at exchange.sba.miami.edu  Sat Jul  7 17:39:47 2007
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Sat, 7 Jul 2007 11:39:47 -0400
Subject: [R] maintaining specified factor contrasts when subsetting in
	lmer
In-Reply-To: <468EC4DC.6060401@hotmail.com>
Message-ID: <6BCB4D493A447546A8126F24332056E8063E8B78@school1.business.edu>


I'm familiar w/ relevel but that doesn't seem to help here 
(e.g., tried setting the reference level to 3 but the second
model w/ the subset argument doesn't seem to estimate the 
desired model).

Possibly another contrasts statement needs to be provided prior 
to the second model, this time w/ a reduced number of levels, but
that seems to introduce problems w.r.t. the total number of levels
prior to subsetting.   

Is the only way around this to define a new dataframe for the subset
and define the contrasts for that dataframe accordingly?

 

-----Original Message-----
From: Francisco J. Zagmutt [mailto:gerifalte28 at hotmail.com] 
Sent: Friday, July 06, 2007 6:40 PM
To: Afshartous, David
Cc: r-help at stat.math.ethz.ch
Subject: Re: maintaining specified factor contrasts when subsetting in
lmer

See ?relevel

Regards,

Francisco

Afshartous, David wrote:
> All,
>  
> I'm using lmer for some repeated measures data and have specified 
> the contrasts for a time factor such that say time 3 is the base.
This
> works fine.  However, when
> I next use the subset argument to remove the last two time values, the

> output indicates that
> the specified contrast is not maintained (see below).   I can solve
this
> by creating a new dataframe
> for the subset of interest and redefining the constrasts, but I was 
> wondering if there is a direct method that allows me to continue w/ 
> the subset argument?  (perhaps via supplying a contrast argument to 
> lmer directly, but this doesn't seem possible based on the defintion 
> of this argument in model.matrix.default).
>  
> Thanks,
> Dave
>  
>  
> z <- rnorm(24, mean=0, sd=1)
> time <- factor(paste("Time-", rep(1:6, 4), sep="")) Patient <- 
> rep(1:4, each = 6) drug <- factor(rep(c("D", "P"), each = 6, times = 
> 2)) ## P = placebo, D = Drug dat.new <- data.frame(time, drug, z, 
> Patient)
>  
> ### specify the contrast as time 3:
> contrasts(dat.new$time) <- contr.treatment(6, base=3)     
> dimnames(contrasts(dat.new$time))[[1]] <-
> as.character(levels(dat.new$time))    
> dimnames(contrasts(dat.new$time))[[2]] <-
> as.character(levels(dat.new$time)[-3])
>  
> fm1 <- lmer(z ~ drug + time + (1 | Patient), data = dat.new ) Fixed 
> effects:
>              Estimate Std. Error  t value
> (Intercept) -0.182774   0.464014 -0.39390
> drugP       -0.281103   0.352309 -0.79789
> timeTime-1   0.150505   0.606462  0.24817
> timeTime-2   0.612016   0.606462  1.00916
> timeTime-4   0.775342   0.606462  1.27847
> timeTime-5   0.093741   0.606462  0.15457
> timeTime-6   0.452442   0.606462  0.74604
>  
> ## time 3 is the base as specified
> 
> fm2 <-  lmer(z ~ drug + time + (1 | Patient), data = dat.new, subset =

> dat.new$time !="Time-6" & dat.new$time != "Time-5")
> 
> Fixed effects:
>              Estimate Std. Error  t value
> (Intercept)  0.052975   0.500675  0.10581
> drugP       -0.451593   0.447818 -1.00843
> timeTime-2   0.461511   0.633310  0.72873
> timeTime-3  -0.150505   0.633310 -0.23765
> timeTime-4   0.624837   0.633310  0.98662
>  
> ### time 3 no longer the base; was expecting to see the fixed effects 
> for time-1, time-2, and time-4, w/ Intercept ### representing time-3
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From attenka at utu.fi  Sat Jul  7 17:41:09 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Sat, 07 Jul 2007 18:41:09 +0300
Subject: [R] from character string to function body?
Message-ID: <f774d1cebc20.468fde45@utu.fi>

Dear R users,

I wonder if it is possible to form a function from a character string. Here is an example:


> x=3
> `-`(`+`(`^`(x,3),`^`(x,2)),1) # Here is my function evaluated.
[1] 35

> V=list("`-`","(","`+`","(","`^`","(","x",",",3,")",",","`^`","(","x",",",2,")",")",",",1,")") # Here I construct the string, it could be vector as well?
> 
> S=noquote(paste(V,collapse=""))
> 
> S
[1] `-`(`+`(`^`(x,3),`^`(x,2)),1) # Here is the same as a character string.

Now I'd like to create a function using this string, something like this, but of course, this doesn't work:

S=as.expression(S)

F1<-function(x){S}

Is there some way to do this?

Cheers,

Atte Tenkanen
University of Turku, Finland


From sundar.dorai-raj at pdf.com  Sat Jul  7 18:02:29 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sat, 07 Jul 2007 09:02:29 -0700
Subject: [R] from character string to function body?
In-Reply-To: <f774d1cebc20.468fde45@utu.fi>
References: <f774d1cebc20.468fde45@utu.fi>
Message-ID: <468FB915.4070802@pdf.com>



Atte Tenkanen said the following on 7/7/2007 8:41 AM:
> Dear R users,
> 
> I wonder if it is possible to form a function from a character string. Here is an example:
> 
> 
>> x=3
>> `-`(`+`(`^`(x,3),`^`(x,2)),1) # Here is my function evaluated.
> [1] 35
> 
>> V=list("`-`","(","`+`","(","`^`","(","x",",",3,")",",","`^`","(","x",",",2,")",")",",",1,")") # Here I construct the string, it could be vector as well?
>>
>> S=noquote(paste(V,collapse=""))
>>
>> S
> [1] `-`(`+`(`^`(x,3),`^`(x,2)),1) # Here is the same as a character string.
> 
> Now I'd like to create a function using this string, something like this, but of course, this doesn't work:
> 
> S=as.expression(S)
> 
> F1<-function(x){S}
> 
> Is there some way to do this?
> 
> Cheers,
> 
> Atte Tenkanen
> University of Turku, Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

How about:

V <- 
list("`-`","(","`+`","(","`^`","(","x",",",3,")",",","`^`","(","x",",",2,")",")",",",1,")") 
# Here I construct the string, it could be vector as well?
S <- paste(V, collapse = "")

F1 <- function(x) {}
body(F1) <- parse(text = S)
F1(3)
# [1] 35
F1(2)
# [1] 11

HTH,

--sundar


From Philip.Turk at NAU.EDU  Sat Jul  7 21:20:05 2007
From: Philip.Turk at NAU.EDU (Philip Turk)
Date: Sat, 07 Jul 2007 12:20:05 -0700
Subject: [R] No convergence using ADAPT
Message-ID: <46981A60@webmail.nau.edu>

I am trying calculate a probability using numerical integration. The first 
program I ran spit out an answer in a very short time. The program is below:

## START PROGRAM

trial <- function(input)

{
pmvnorm(lower = c(0,0), upper = c(2, 2), mean = input, sigma = matrix(c(.1, 0, 
0, .1), nrow = 2, ncol = 2, byrow = FALSE)) 
}

require(mvtnorm)
require(adapt)

bottomB <- -5*sqrt(.1)
topB <- 2 + 5*sqrt(.1)
areaB <- (topB - bottomB)^2

unscaled.Po.in.a <- adapt(2, lo = c(bottomB, bottomB), up = c(topB, topB), 
minpts = 1000, eps = 1e-4, functn = trial)

(1/areaB)*unscaled.Po.in.a$value

## FINISH PROGRAM

I tried to run the program again changing a.) sigma in the trial function, b.) 
upper in the trial function, and c.) the bounds of integration; that is, 
bottomB and topB.  The new program is below:

## START PROGRAM

trial <- function(input)

{
pmvnorm(lower = c(0,0), upper = c(10, 10), mean = input, sigma = matrix(c(.01, 
0, 0, .01), nrow = 2, ncol = 2, byrow = FALSE)) 
}

require(mvtnorm)
require(adapt)

bottomB <- -5*sqrt(.01)
topB <- 10 + 5*sqrt(.01)
areaB <- (topB - bottomB)^2

unscaled.Po.in.a <- adapt(2, lo = c(bottomB, bottomB), up = c(topB, topB), 
minpts = 1000, eps = 1e-4, functn = trial)

(1/areaB)*unscaled.Po.in.a$value

## FINISH PROGRAM

Now, the program just runs and runs (48 hours at last count!).  By playing 
around with the program, I have deduced the program is highly sensitive to 
changing the upper option in the trial function.  For example, using a vector 
like (4, 4) causes no problems and the program quickly yields an answer.  I 
have a couple of other programs where I can easily obtain a simulation-based 
answer, but I would ultimately like to know what's up with this program before 
I give up on it so I can learn a thing or two.  Does anyone have any clues or 
tricks to get around this problem?  My guess is that it will simply be very 
difficult (impossible?) to obtain this type of relative error (eps = 1e-4) and 
I will have no choice but to pursue the simulation approach.

Thanks for any responses (philip.turk at nau.edu)!

-- Phil

Philip Turk
Assistant Professor of Statistics
Northern Arizona University
Department of Mathematics and Statistics
PO Box 5717
Flagstaff, AZ 86011
Phone: 928-523-6884
Fax: 928-523-5847
E-mail: Philip.Turk at nau.edu
Web Site: http://jan.ucc.nau.edu/~stapjt-p/


From jzhang1982 at gmail.com  Sat Jul  7 21:30:44 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sat, 7 Jul 2007 13:30:44 -0600
Subject: [R] random sampling with some limitive conditions?
Message-ID: <3f2938d50707071230n3974be62x368a660762e674f4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/343f020f/attachment.pl 

From attenka at utu.fi  Sat Jul  7 21:42:26 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Sat, 07 Jul 2007 22:42:26 +0300
Subject: [R] from character string to function body?
In-Reply-To: <468FB915.4070802@pdf.com>
References: <f774d1cebc20.468fde45@utu.fi> <468FB915.4070802@pdf.com>
Message-ID: <fd6fd247de60.469016d2@utu.fi>

Thanks Sundar!

I wonder that - when evaluating F1 - the right mathematical formula is now also printed!

> F1
function (x) 
x^3 + x^2 - 1

Atte

> 
> 
> Atte Tenkanen said the following on 7/7/2007 8:41 AM:
> > Dear R users,
> > 
> > I wonder if it is possible to form a function from a character 
> string. Here is an example:
> > 
> > 
> >> x=3
> >> `-`(`+`(`^`(x,3),`^`(x,2)),1) # Here is my function evaluated.
> > [1] 35
> > 
> >> V=list("`-
> `","(","`+`","(","`^`","(","x",",",3,")",",","`^`","(","x",",",2,")",")",",",1,")") # Here I construct the string, it could be vector as well?
> >>
> >> S=noquote(paste(V,collapse=""))
> >>
> >> S
> > [1] `-`(`+`(`^`(x,3),`^`(x,2)),1) # Here is the same as a 
> character string.
> > 
> > Now I'd like to create a function using this string, something 
> like this, but of course, this doesn't work:
> > 
> > S=as.expression(S)
> > 
> > F1<-function(x){S}
> > 
> > Is there some way to do this?
> > 
> > Cheers,
> > 
> > Atte Tenkanen
> > University of Turku, Finland
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html> and provide commented, minimal, self-contained, 
> reproducible code.
> 
> How about:
> 
> V <- 
> list("`-
> `","(","`+`","(","`^`","(","x",",",3,")",",","`^`","(","x",",",2,")",")",",",1,")") 
> # Here I construct the string, it could be vector as well?
> S <- paste(V, collapse = "")
> 
> F1 <- function(x) {}
> body(F1) <- parse(text = S)
> F1(3)
> # [1] 35
> F1(2)
> # [1] 11
> 
> HTH,
> 
> --sundar
> 
>


From thomas.pujol at yahoo.com  Sat Jul  7 22:35:12 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Sat, 7 Jul 2007 13:35:12 -0700 (PDT)
Subject: [R] calculating p-values of columns in a dataframe
Message-ID: <455639.16597.qm@web59308.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/8f9a74ac/attachment.pl 

From res90sx5 at verizon.net  Sat Jul  7 23:08:06 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Sat, 07 Jul 2007 14:08:06 -0700
Subject: [R] random sampling with some limitive conditions?
In-Reply-To: <3f2938d50707071230n3974be62x368a660762e674f4@mail.gmail.com>
Message-ID: <002701c7c0da$ea9e60c0$0201a8c0@Aragorn>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Zhang Jian
> Sent: Saturday, July 07, 2007 12:31 PM
> To: r-help
> Subject: [R] random sampling with some limitive conditions?
> 
> I want to gain thousands of random sampling data by randomizing the
> presence-absence data. Meantime, one important limition is that the row and
> column sums must be fixed. For example, the data "tst" is following:
>    site1 site2 site3 site4 site5 site6 site7 site8 1 0 0 0 1 1 0 0 0 1 1 1 0
> 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0
> 0 0 0 0 0 0 0 0 1 0 1 0 1
> 
> sum(tst[1,]) = 3, sum(tst[,1])=4, and so on. When I randomize the data, the
> first row sums must equal to 3, and the first column sums must equal to 4.
> The rules need to be applied to each row and column.
> How to get the new random sampling data? I have no idea.
> Thanks.
> 

You could reorder your table by stepping through your table a column at a time, and for each column randomly deciding to swap the current column with a column that has the same column total.  Repeat this process for each row, i.e. for each row, randomly choose a row with the same row total to swap with.

Here is some example code which is neither efficient nor general, but does demonstrate the basic idea.  You will need to decide if this approach meets you needs. 

# I created a data file with your table (8x8) and read from it
sites <- read.table("c:/R/R-examples/site_random_sample.txt", header=TRUE)
sites
# get row and column totals
colsums <- apply(sites,2,sum)
rowsums <- apply(sites,1,sum)
# randomly swap columns
for(i in 1:8) {
  if (runif(1) > .5) {
    swapcol<-sample(which(colsums==colsums[i]),1)
    temp<-sites[,swapcol]
    sites[,swapcol]<-sites[,i]
    sites[,i]<-temp
    }
  }
# randomly swap rows
for(i in 1:8) {
  if (runif(1) > .5) {
    swaprow<-sample(which(rowsums==rowsums[i]),1)
    temp<-sites[swaprow,]
    sites[swaprow,]<-sites[i,]
    sites[i,]<-temp
    }
  }
sites
    

Hope this is helpful,

Dan

Daniel Nordlund
Bothell, WA USA


From jzhang1982 at gmail.com  Sun Jul  8 00:18:42 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sat, 7 Jul 2007 16:18:42 -0600
Subject: [R] How to calculate the index "the number of species combinations"?
Message-ID: <3f2938d50707071518r52a33499te91130fc38b2efd8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/7bccc3d7/attachment.pl 

From sarah.goslee at gmail.com  Sun Jul  8 00:56:32 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Sat, 7 Jul 2007 18:56:32 -0400
Subject: [R] How to calculate the index "the number of species
	combinations"?
In-Reply-To: <3f2938d50707071518r52a33499te91130fc38b2efd8@mail.gmail.com>
References: <3f2938d50707071518r52a33499te91130fc38b2efd8@mail.gmail.com>
Message-ID: <efb536d50707071556h55bd069du340a763abfa8bdd@mail.gmail.com>

It should be the number of unique sites. In this case, the number of
unique columns in the data frame. See ?unique. (Interestingly,
convention is usually that species are columns and sites are rows.)

For your sample data you only see 10 of the 2^17 possible combinations
of 17 species (not 2n).

Sarah

On 7/7/07, Zhang Jian <jzhang1982 at gmail.com> wrote:
> I want to analyze the co-occurrence of some species. In some papers, the
> authors said that the index"the number of species combinations (COMBO)" is a
> good index. I try to calculate the index by R language. But I can not get
> the right value. I think that I do not understand the concept of the index
> because my english is not good.
>
> The concept:
> *The number of species combinations   *This index scans the columns of the
> presence-absence matrix and keeps track of the number of unique species
> combinations that are represented in different sites. For an assemblage of n
> species, there are 2n possible species combinations, including the
> combination of no species being present (Pielou and Pielou 1968). In most
> real matrices, the number of sites (= columns) is usually substantially less
> than 2n, which places an upper bound on the number of species combinations
> that can be found in both the observed and the simulated matrices.
>
> Presence-absence Data (Each row represents different species and each column
> represents a different site. A "1" indicates a species is present at a
> particular site, and a "0" indicates that a species is absent from a
> particular site):
> Species Cuba Hispaniola Jamaica Puerto_Rico Guadeloupe Martinique Dominica
> St._Lucia Barbados St._Vincent Grenada Antigua St._Croix Grand_Cayman
> St._Kitts Barbuda Montserrat St._Martin St._Thomas
> Carduelis_dominicensis 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Loxia_leucoptera 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Volatinia_jacarina 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
> Sporophila_nigricollis 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
> Melopyrrha_nigra 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
> Loxigilla_portoricensis 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Loxigilla_violacea 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Loxigilla_noxis 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0
> Melanospiza_richardsoni 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
> Tiara_olivacea 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
> Tiara_bicolor 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1
> Tiara_canora 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Loxipasser_anoxanthus 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Saltator_albicollis 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
> Torreornis_inexpectata 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Ammodramus_savannarum 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> Zonotrichia_capensis 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From jzhang1982 at gmail.com  Sun Jul  8 01:19:56 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sat, 7 Jul 2007 17:19:56 -0600
Subject: [R] How to calculate the index "the number of species
	combinations"?
In-Reply-To: <efb536d50707071556h55bd069du340a763abfa8bdd@mail.gmail.com>
References: <3f2938d50707071518r52a33499te91130fc38b2efd8@mail.gmail.com>
	<efb536d50707071556h55bd069du340a763abfa8bdd@mail.gmail.com>
Message-ID: <3f2938d50707071619x108eda0atc2c15bd12ba5a835@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/ca160188/attachment.pl 

From jzhang1982 at gmail.com  Sun Jul  8 01:41:01 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sat, 7 Jul 2007 17:41:01 -0600
Subject: [R] one question about the loop
Message-ID: <3f2938d50707071641j5e189dfenece984279cd58cb4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/f6c37fa1/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Sun Jul  8 01:46:11 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 08 Jul 2007 00:46:11 +0100 (BST)
Subject: [R] How to calculate the index "the number of species combin
In-Reply-To: <3f2938d50707071518r52a33499te91130fc38b2efd8@mail.gmail.com>
Message-ID: <XFMail.070708004611.ted.harding@nessie.mcc.ac.uk>

On 07-Jul-07 22:18:42, Zhang Jian wrote:
> I want to analyze the co-occurrence of some species. In some
> papers, the authors said that the index"the number of species
> combinations (COMBO)" is a good index. I try to calculate the
> index by R language. But I can not get the right value. I think
> that I do not understand the concept of the index because my
> english is not good.
> 
> The concept:
> *The number of species combinations   *This index scans the
> columns of the presence-absence matrix and keeps track of the
> number of unique species combinations that are represented in
> different sites. For an assemblage of n species, there are 2n

[I think this should be 2^n]

> possible species combinations, including the combination of no
> species being present (Pielou and Pielou 1968). In most real
> matrices, the number of sites (= columns) is usually substantially
> less than 2n, which places an upper bound on the number of species
> combinations that can be found in both the observed and the
> simulated matrices.

English good or bad, I found the above description not easy to
follow! But, since I could see only one thing it could mean if
it was intended to gice a unique definition, I decided to test
on you data the hypothesis that Species Combinations means the
number of distinct columns in the data matrix.

I took your species-incidence data (not reproduced here) and
converted it to a matrix S of 0 and 1 with 19 columns and 17
rows (though the following would work just as well if it is a
dataframe):

S
   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19
1   0  1  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
2   0  1  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
3   0  0  0  0  0  0  0  0  0   0   1   0   0   0   0   0   0   0   0
4   0  0  0  0  0  0  0  0  0   0   1   0   0   0   0   0   0   0   0
5   1  0  0  0  0  0  0  0  0   0   0   0   0   1   0   0   0   0   0
6   0  0  0  1  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
7   0  1  1  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
8   0  0  0  0  1  1  1  1  1   1   1   1   0   0   1   1   1   1   0
9   0  0  0  0  0  0  0  1  0   0   0   0   0   0   0   0   0   0   0
10  1  1  1  1  0  0  0  0  0   0   0   0   0   1   0   0   0   0   0
11  0  1  1  1  1  1  1  1  1   1   1   1   1   0   1   1   1   1   1
12  1  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
13  0  0  1  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
14  0  0  0  0  1  1  1  1  0   0   0   0   0   0   0   0   0   0   0
15  1  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
16  0  1  1  1  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0
17  0  1  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0

There are 10 different columns in there, as can be found by

t(unique(t(S)))
   V1  V2  V3  V4  V5  V8  V9 V11 V13 V14
1   0   1   0   0   0   0   0   0   0   0
2   0   1   0   0   0   0   0   0   0   0
3   0   0   0   0   0   0   0   1   0   0
4   0   0   0   0   0   0   0   1   0   0
5   1   0   0   0   0   0   0   0   0   1
6   0   0   0   1   0   0   0   0   0   0
7   0   1   1   0   0   0   0   0   0   0
8   0   0   0   0   1   1   1   1   0   0
9   0   0   0   0   0   1   0   0   0   0
10  1   1   1   1   0   0   0   0   0   1
11  0   1   1   1   1   1   1   1   1   0
12  1   0   0   0   0   0   0   0   0   0
13  0   0   1   0   0   0   0   0   0   0
14  0   0   0   0   1   1   0   0   0   0
15  1   0   0   0   0   0   0   0   0   0
16  0   1   1   1   0   0   0   0   0   0
17  0   1   0   0   0   0   0   0   0   0

Of the "missing" columns, it can be seen that V6 and V7 are the
same as V5, and V10, V12, V15, V16, V17, V18, V19 are the same
as V9. Hence the interpretation that "COMBO" means the number of
distinct columns is confirmed. If that is really the case, then
a very simple R function can be written for it:

COMBO<-function(S){ncol(t(unique(t(S))))}

COMBO(S)
[1] 10

> About the data, I calculated the index "COMBO" by other software.
> The value of the index is 10.
> Can you help me to calculate or give me some detalied explain about
> the concept of the index?

See above! I hope it helps.
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Jul-07                                       Time: 00:46:08
------------------------------ XFMail ------------------------------


From jholtman at gmail.com  Sun Jul  8 02:08:04 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 7 Jul 2007 20:08:04 -0400
Subject: [R] one question about the loop
In-Reply-To: <3f2938d50707071641j5e189dfenece984279cd58cb4@mail.gmail.com>
References: <3f2938d50707071641j5e189dfenece984279cd58cb4@mail.gmail.com>
Message-ID: <644e1f320707071708v578a3e32i8f1b1d1f3f8cf355@mail.gmail.com>

Is this what you want?

> t(combn(5,2))
      [,1] [,2]
 [1,]    1    2
 [2,]    1    3
 [3,]    1    4
 [4,]    1    5
 [5,]    2    3
 [6,]    2    4
 [7,]    2    5
 [8,]    3    4
 [9,]    3    5
[10,]    4    5
>


On 7/7/07, Zhang Jian <jzhang1982 at gmail.com> wrote:
> Hi.
> I want to get a series data just like this:
> sp1 sp2
> 1   2
> 1   3
> 1   4
> 1   5
> 2   3
> 2   4
> 2   5
> 3   4
> 3   5
> 4   5
>
> I can get one part of the data every time, but I can get the data directly.
> I try to use the loop, but it can not work. How to get it ? Thanks.
>
> pair.fn=function(i,sp=5){res=data.frame(sp1=rep(i,(sp-i)),sp2=(i+1):sp)
> return(res)}
> pair.fn(1)
> pair.fn(2)
> .....
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From luo_weijun at yahoo.com  Sun Jul  8 02:09:17 2007
From: luo_weijun at yahoo.com (Luo Weijun)
Date: Sat, 7 Jul 2007 17:09:17 -0700 (PDT)
Subject: [R] Loading problem with XML_1.9
In-Reply-To: <6phir97j2jf.fsf@gopher4.fhcrc.org>
Message-ID: <967571.55520.qm@web32511.mail.mud.yahoo.com>

Hello Dr. Lang and all,
I posted this message in R-help mail list, but haven?t
solved my problem so far. Therefore, could you help me
look at it?
I have loading problem with XML_1.9 under 64 bit
R2.3.1 for Mac OS X, which I got from
http://R.research.att.com/. XML_1.9 works fine under
32 bit R2.5.0. I thought that could be installation
problem, and I tried install.packages or biocLite,
every time the package installed fine, except some
warning messages below:
ld64 warning: in /usr/lib/libxml2.dylib, file does not
contain requested architecture
ld64 warning: in /usr/lib/libz.dylib, file does not
contain requested architecture
ld64 warning: in /usr/lib/libiconv.dylib, file does
not contain requested architecture
ld64 warning: in /usr/lib/libz.dylib, file does not
contain requested architecture
ld64 warning: in /usr/lib/libxml2.dylib, file does not
contain requested architecture

Here is the error messages I got, when XML is loaded:
> library(XML)
Error in dyn.load(x, as.logical(local),
as.logical(now)) : 
        unable to load shared library
'/usr/local/lib64/R/library/XML/libs/XML.so':
  dlopen(/usr/local/lib64/R/library/XML/libs/XML.so,
6): Symbol not found: _xmlMemDisplay
  Referenced from:
/usr/local/lib64/R/library/XML/libs/XML.so
  Expected in: flat namespace
Error: .onLoad failed in 'loadNamespace' for 'XML'
Error: package/namespace load failed for 'XML'

Session information
> sessionInfo()
Version 2.3.1 Patched (2006-06-27 r38447) 
powerpc64-apple-darwin8.7.0 

attached base packages:
[1] "methods"   "stats"     "graphics"  "grDevices"
"utils"     "datasets" 
[7] "base"     

Prof Brian Ripley also suggested that this could be
that I don?t have a 64-bit version of libxml2
installed. Where I get it and where/how to install it,
if that?s the problem? 
The reason I need to use R64 is that I have memory
limitation issue with R 32 bit version when I load
some very large XML trees (the data file is about
800M). And Martin suggested me to use 'handler'
argument of xmlTreeParse, tried 'handler' with
useInternalNodes=T, but I still got this memory
problem with R 32 bit version. Please tell me what I
can do now. Thank you so much!
Weijun



       
____________________________________________________________________________________

Comedy with an Edge to see what's on, when.


From jbustosm at udec.cl  Sun Jul  8 02:15:16 2007
From: jbustosm at udec.cl (=?iso-8859-1?Q?Jos=E9_Ignacio_Bustos_Melo?=)
Date: Sat, 7 Jul 2007 20:15:16 -0400 (CLT)
Subject: [R] Making Gehan-Breslow test for Survival data
Message-ID: <2711.190.54.28.217.1183853716.squirrel@webmail.udec.cl>

Hi all,

The survivals functions can be tested by the Log-rank test and others, for
example the Gehan-Breslow. The graham breslow work with the alpha values.

But I don't know how is the Gehan-Breslow test with R. Somebody know a
type function?.. or other suggestions?  Any help will be really
appreciated

Jos? Bustos
Marine Biologist
Master Apllied Stat Program
University of Concepci?n


From guyader at agrocampus-rennes.fr  Sun Jul  8 02:36:46 2007
From: guyader at agrocampus-rennes.fr (vincent guyader)
Date: Sun, 8 Jul 2007 03:36:46 +0300
Subject: [R] change the "coeffcients approach" on an anova
Message-ID: <45551220707071736p43dafeafhea687f6a7816a77@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/1e54b3a8/attachment.pl 

From ggrothendieck at gmail.com  Sun Jul  8 02:52:30 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 7 Jul 2007 20:52:30 -0400
Subject: [R] change the "coeffcients approach" on an anova
In-Reply-To: <45551220707071736p43dafeafhea687f6a7816a77@mail.gmail.com>
References: <45551220707071736p43dafeafhea687f6a7816a77@mail.gmail.com>
Message-ID: <971536df0707071752w3ca768f5v143329103feb168b@mail.gmail.com>

On 7/7/07, vincent guyader <guyader at agrocampus-rennes.fr> wrote:
> hi everybody
>
> I have to do a lot of Anova with R and I would like to have another type of
> coefficients coding.. I explain.
>
> by default if I have 2 temperatures for an experience. 100?C or 130?C and I
> want to see the temperature effect on the presure
> I want to estimate the coefficient of each temperature.
>
> I will obtain ,with the anova, juste one coefficients for example +3,56 (for
> 100?C), and the other (for 130?C) is always  zero.
>
> but I prefer to obtain + 1,78 or the first effect and -1,78 for the second
> (the intercept is also different too)
>
> my script is (it s just an example)
>
> rinfo2 <- (lm(pression~ temp, data=rebe))

Try:

lm(pression ~ temp + 0, rebe)



> anova(rinfo2)
> summary(rinfo2)
>
> what can I change to obtain what I need?
>
> best regards
>
>
>
> --
> _________________________________
> Vincent GUYADER
> ?cole nationale sup?rieure d'agronomie de Rennes (ENSAR)
> EN156 - Sp?cialit? statistiques appliqu?es
> 06.22.85.34.27
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From Bill.Venables at csiro.au  Sun Jul  8 03:05:13 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Sun, 8 Jul 2007 11:05:13 +1000
Subject: [R] How to calculate the index "the number of
	speciescombinations"?
Message-ID: <B998A44C8986644EA8029CFE6396A924B68026@exqld2-bne.nexus.csiro.au>

Here is a step by step explanation.  

The way you present the data is as species (rows) by sites (columns)
data frame

> dim(species_x_sites)
[1] 17 20

There are in fact only 19 sites as one of the columns of the data frame
is the species name:

> names(species_x_sites)
 [1] "Species"      "Cuba"         "Hispaniola"   "Jamaica"
"Puerto_Rico" 
 [6] "Guadeloupe"   "Martinique"   "Dominica"     "St._Lucia"
"Barbados"    
[11] "St._Vincent"  "Grenada"      "Antigua"      "St._Croix"
"Grand_Cayman"
[16] "St._Kitts"    "Barbuda"      "Montserrat"   "St._Martin"
"St._Thomas"  

To use the standard tools you need to turn it around and make a
site_x_species matrix

> site_x_species <- t(as.matrix(species_x_sites[, -1]))

(The [, -1] simply omits the species column and the t() transoses it)

> dim(site_x_species)
[1] 19 17

Now how many unique combinations are there?

> unique_combos <- unique(site_x_species) # unique *rows*

Not to find out how many we could use

> dim(unique_combos)
[1] 10 17

or

> nrow(unique_combos)
[1] 10

Which I believe corresponds to your index.



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Zhang Jian
Sent: Saturday, 7 July 2007 4:20 PM
To: Sarah Goslee; r-help
Subject: Re: [R] How to calculate the index "the number of
speciescombinations"?

Sorry, I can not understand your reply very clearly.
How to compute the number of unique sites ?
Can you give me a simply example or do a simply analyse using one data?
Thanks very much.
                                       Jian Zhang


On 7/7/07, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>
> It should be the number of unique sites. In this case, the number of
> unique columns in the data frame. See ?unique. (Interestingly,
> convention is usually that species are columns and sites are rows.)
>
> For your sample data you only see 10 of the 2^17 possible combinations
> of 17 species (not 2n).
>
> Sarah
>
> On 7/7/07, Zhang Jian <jzhang1982 at gmail.com> wrote:
> > I want to analyze the co-occurrence of some species. In some papers,
the
> > authors said that the index"the number of species combinations
(COMBO)"
> is a
> > good index. I try to calculate the index by R language. But I can
not
> get
> > the right value. I think that I do not understand the concept of the
> index
> > because my english is not good.
> >
> > The concept:
> > *The number of species combinations   *This index scans the columns
of
> the
> > presence-absence matrix and keeps track of the number of unique
species
> > combinations that are represented in different sites. For an
assemblage
> of n
> > species, there are 2n possible species combinations, including the
> > combination of no species being present (Pielou and Pielou 1968). In
> most
> > real matrices, the number of sites (= columns) is usually
substantially
> less
> > than 2n, which places an upper bound on the number of species
> combinations
> > that can be found in both the observed and the simulated matrices.
> >
> > Presence-absence Data (Each row represents different species and
each
> column
> > represents a different site. A "1" indicates a species is present at
a
> > particular site, and a "0" indicates that a species is absent from a
> > particular site):
> > Species Cuba Hispaniola Jamaica Puerto_Rico Guadeloupe Martinique
> Dominica
> > St._Lucia Barbados St._Vincent Grenada Antigua St._Croix
Grand_Cayman
> > St._Kitts Barbuda Montserrat St._Martin St._Thomas
> > Carduelis_dominicensis 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> > Loxia_leucoptera 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> > Volatinia_jacarina 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
> > Sporophila_nigricollis 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
> > Melopyrrha_nigra 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
> > Loxigilla_portoricensis 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> > Loxigilla_violacea 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> > Loxigilla_noxis 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0
> > Melanospiza_richardsoni 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
> > Tiara_olivacea 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
> > Tiara_bicolor 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1
> > Tiara_canora 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> > Loxipasser_anoxanthus 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> > Saltator_albicollis 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
> > Torreornis_inexpectata 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> > Ammodramus_savannarum 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> > Zonotrichia_capensis 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
> >
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bakalegum at gmail.com  Sun Jul  8 03:08:09 2007
From: bakalegum at gmail.com (BaKaLeGuM)
Date: Sun, 8 Jul 2007 04:08:09 +0300
Subject: [R] change the "coeffcients approach" on an anova
In-Reply-To: <45551220707071736p43dafeafhea687f6a7816a77@mail.gmail.com>
References: <45551220707071736p43dafeafhea687f6a7816a77@mail.gmail.com>
Message-ID: <45551220707071808i119e38eal7383cd3a8729e94b@mail.gmail.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070708/16c270e0/attachment.pl 

From ted.harding at nessie.mcc.ac.uk  Sun Jul  8 03:18:27 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 08 Jul 2007 02:18:27 +0100 (BST)
Subject: [R] change the "coeffcients approach" on an anova
In-Reply-To: <45551220707071736p43dafeafhea687f6a7816a77@mail.gmail.com>
Message-ID: <XFMail.070708021827.ted.harding@nessie.mcc.ac.uk>


On 08-Jul-07 00:36:46, vincent guyader wrote:
> hi everybody
> 
> I have to do a lot of Anova with R and I would like to have another
> type of
> coefficients coding.. I explain.
> 
> by default if I have 2 temperatures for an experience. 100?C or 130?C
> and I
> want to see the temperature effect on the presure
> I want to estimate the coefficient of each temperature.
> 
> I will obtain ,with the anova, juste one coefficients for example +3,56
> (for
> 100?C), and the other (for 130?C) is always  zero.
> 
> but I prefer to obtain + 1,78 or the first effect and -1,78 for the
> second
> (the intercept is also different too)
> 
> my script is (it s just an example)
> 
> rinfo2 <- (lm(pression~ temp, data=rebe))
> anova(rinfo2)
> summary(rinfo2)
> 
> what can I change to obtain what I need?

Try

  rinfo2 <- (lm(pression~temp-1, data=rebe))

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 08-Jul-07                                       Time: 02:18:24
------------------------------ XFMail ------------------------------


From ral at lcfltd.com  Sun Jul  8 05:43:12 2007
From: ral at lcfltd.com (Robert A LaBudde)
Date: Sat, 07 Jul 2007 23:43:12 -0400
Subject: [R] No convergence using ADAPT
In-Reply-To: <46981A60@webmail.nau.edu>
References: <46981A60@webmail.nau.edu>
Message-ID: <0JKU009P7D00L7K9@vms042.mailsrvcs.net>

What versions of "adapt" and R are you using? The current package was 
built with R-2.5.1.

I tried your program with R-2.5.0, and got the answer 0.1501053 in 
just a few seconds.

At 03:20 PM 7/7/2007, Philip wrote:
>I am trying calculate a probability using numerical integration. The first
>program I ran spit out an answer in a very short time. The program is below:
>
>## START PROGRAM
>
>trial <- function(input)
>
>{
>pmvnorm(lower = c(0,0), upper = c(2, 2), mean = input, sigma = 
>matrix(c(.1, 0,
>0, .1), nrow = 2, ncol = 2, byrow = FALSE))
>}
>
>require(mvtnorm)
>require(adapt)
>
>bottomB <- -5*sqrt(.1)
>topB <- 2 + 5*sqrt(.1)
>areaB <- (topB - bottomB)^2
>
>unscaled.Po.in.a <- adapt(2, lo = c(bottomB, bottomB), up = c(topB, topB),
>minpts = 1000, eps = 1e-4, functn = trial)
>
>(1/areaB)*unscaled.Po.in.a$value
>
>## FINISH PROGRAM
>
>I tried to run the program again changing a.) sigma in the trial 
>function, b.)
>upper in the trial function, and c.) the bounds of integration; that is,
>bottomB and topB.  The new program is below:
>
>## START PROGRAM
>
>trial <- function(input)
>
>{
>pmvnorm(lower = c(0,0), upper = c(10, 10), mean = input, sigma = 
>matrix(c(.01,
>0, 0, .01), nrow = 2, ncol = 2, byrow = FALSE))
>}
>
>require(mvtnorm)
>require(adapt)
>
>bottomB <- -5*sqrt(.01)
>topB <- 10 + 5*sqrt(.01)
>areaB <- (topB - bottomB)^2
>
>unscaled.Po.in.a <- adapt(2, lo = c(bottomB, bottomB), up = c(topB, topB),
>minpts = 1000, eps = 1e-4, functn = trial)
>
>(1/areaB)*unscaled.Po.in.a$value
>
>## FINISH PROGRAM
>
>Now, the program just runs and runs (48 hours at last count!).  By playing
>around with the program, I have deduced the program is highly sensitive to
>changing the upper option in the trial function.  For example, using a vector
>like (4, 4) causes no problems and the program quickly yields an answer.  I
>have a couple of other programs where I can easily obtain a simulation-based
>answer, but I would ultimately like to know what's up with this 
>program before
>I give up on it so I can learn a thing or two.  Does anyone have any clues or
>tricks to get around this problem?  My guess is that it will simply be very
>difficult (impossible?) to obtain this type of relative error (eps = 
>1e-4) and
>I will have no choice but to pursue the simulation approach.
>
>Thanks for any responses (philip.turk at nau.edu)!
>
>-- Phil
>
>Philip Turk
>Assistant Professor of Statistics
>Northern Arizona University
>Department of Mathematics and Statistics
>PO Box 5717
>Flagstaff, AZ 86011
>Phone: 928-523-6884
>Fax: 928-523-5847
>E-mail: Philip.Turk at nau.edu
>Web Site: http://jan.ucc.nau.edu/~stapjt-p/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

================================================================
Robert A. LaBudde, PhD, PAS, Dpl. ACAFS  e-mail: ral at lcfltd.com
Least Cost Formulations, Ltd.            URL: http://lcfltd.com/
824 Timberlake Drive                     Tel: 757-467-0954
Virginia Beach, VA 23464-3239            Fax: 757-467-2947

"Vere scire est per causas scire"


From haotang1982 at gmail.com  Sun Jul  8 05:43:29 2007
From: haotang1982 at gmail.com (Hao Tang)
Date: Sat, 7 Jul 2007 23:43:29 -0400
Subject: [R] Is there Discriminant Adaptive Nearest Neighbor classification?
Message-ID: <005201c7c112$29c29aa0$6501a8c0@Tracy>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/9b0d3516/attachment.pl 

From duncan at wald.ucdavis.edu  Sun Jul  8 08:12:52 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Sat, 07 Jul 2007 23:12:52 -0700
Subject: [R] Loading problem with XML_1.9
In-Reply-To: <967571.55520.qm@web32511.mail.mud.yahoo.com>
References: <967571.55520.qm@web32511.mail.mud.yahoo.com>
Message-ID: <46908064.605@wald.ucdavis.edu>


Well, as you mention at the end of the mail,
several people have given you suggestions about
how to solve the problem using different approaches.
You might search on the Web for how to install a 64 bit version of libxml2?
Using xmlTreeParse(, useInternalNodes = TRUE) is an approach
to reducing the memory consumption as is using the handlers
argument. And if size is really the issue, you should consider
the SAX model which is very memory efficient and made available
via the xmlEventParse() function in the XML package.
And it even provides the concepts of branches to provide a
hybrid of SAX and DOM-style parsing together.

However, to solve the problem of the xmlMemDisplay
symbol not being found, you can look for where
that is used and remove it.    It is in src/DocParse.c
in the routine RS_XML_MemoryShow().  You can remove
the line
  xmlMemDisplay(stderr)
or indeed the entire routine.  Then re-install and
reload the package.

 D.


Luo Weijun wrote:
> Hello Dr. Lang and all,
> I posted this message in R-help mail list, but haven?t
> solved my problem so far. Therefore, could you help me
> look at it?
> I have loading problem with XML_1.9 under 64 bit
> R2.3.1 for Mac OS X, which I got from
> http://R.research.att.com/. XML_1.9 works fine under
> 32 bit R2.5.0. I thought that could be installation
> problem, and I tried install.packages or biocLite,
> every time the package installed fine, except some
> warning messages below:
> ld64 warning: in /usr/lib/libxml2.dylib, file does not
> contain requested architecture
> ld64 warning: in /usr/lib/libz.dylib, file does not
> contain requested architecture
> ld64 warning: in /usr/lib/libiconv.dylib, file does
> not contain requested architecture
> ld64 warning: in /usr/lib/libz.dylib, file does not
> contain requested architecture
> ld64 warning: in /usr/lib/libxml2.dylib, file does not
> contain requested architecture
> 
> Here is the error messages I got, when XML is loaded:
>> library(XML)
> Error in dyn.load(x, as.logical(local),
> as.logical(now)) : 
>         unable to load shared library
> '/usr/local/lib64/R/library/XML/libs/XML.so':
>   dlopen(/usr/local/lib64/R/library/XML/libs/XML.so,
> 6): Symbol not found: _xmlMemDisplay
>   Referenced from:
> /usr/local/lib64/R/library/XML/libs/XML.so
>   Expected in: flat namespace
> Error: .onLoad failed in 'loadNamespace' for 'XML'
> Error: package/namespace load failed for 'XML'
> 
> Session information
>> sessionInfo()
> Version 2.3.1 Patched (2006-06-27 r38447) 
> powerpc64-apple-darwin8.7.0 
> 
> attached base packages:
> [1] "methods"   "stats"     "graphics"  "grDevices"
> "utils"     "datasets" 
> [7] "base"     
> 
> Prof Brian Ripley also suggested that this could be
> that I don?t have a 64-bit version of libxml2
> installed. Where I get it and where/how to install it,
> if that?s the problem? 
> The reason I need to use R64 is that I have memory
> limitation issue with R 32 bit version when I load
> some very large XML trees (the data file is about
> 800M). And Martin suggested me to use 'handler'
> argument of xmlTreeParse, tried 'handler' with
> useInternalNodes=T, but I still got this memory
> problem with R 32 bit version. Please tell me what I
> can do now. Thank you so much!
> Weijun
> 
> 
> 
>        
> ____________________________________________________________________________________
> 
> Comedy with an Edge to see what's on, when.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jzhang1982 at gmail.com  Sun Jul  8 08:58:27 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sun, 8 Jul 2007 00:58:27 -0600
Subject: [R] random sampling with some limitive conditions?
In-Reply-To: <002701c7c0da$ea9e60c0$0201a8c0@Aragorn>
References: <3f2938d50707071230n3974be62x368a660762e674f4@mail.gmail.com>
	<002701c7c0da$ea9e60c0$0201a8c0@Aragorn>
Message-ID: <3f2938d50707072358k40b8604eu3effe26f3b57b299@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/53ebbd88/attachment.pl 

From paulmatthias.diderichsen at abbott.com  Sun Jul  8 09:49:41 2007
From: paulmatthias.diderichsen at abbott.com (Paul Matthias Diderichsen)
Date: Sun, 8 Jul 2007 09:49:41 +0200
Subject: [R] Antwort: Re: pgup/pgdown in R Graphics Window under Linux
	['Watchdog': checked]
In-Reply-To: <Pine.LNX.4.64.0707050935270.18325@auk.stats>
Message-ID: <OF47B03972.23FA8B35-ONC1257312.0029E98B-C1257312.002B0020@abbott.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/eecf2418/attachment.pl 

From paulmatthias.diderichsen at abbott.com  Sun Jul  8 10:00:20 2007
From: paulmatthias.diderichsen at abbott.com (Paul Matthias Diderichsen)
Date: Sun, 8 Jul 2007 10:00:20 +0200
Subject: [R] Antwort: Re: pgup/pgdown in R Graphics Window under Linux
	['Watchdog': checked]
In-Reply-To: <eb555e660707051705i42e1f726q23fae2cd6c407056@mail.gmail.com>
Message-ID: <OFE35E52C7.14DC3AAB-ONC1257312.002B1719-C1257312.002BF9D4@abbott.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/959781f4/attachment.pl 

From cgenolin at u-paris10.fr  Sun Jul  8 11:14:30 2007
From: cgenolin at u-paris10.fr (cgenolin at u-paris10.fr)
Date: Sun, 08 Jul 2007 11:14:30 +0200
Subject: [R] one question about the loop
In-Reply-To: <644e1f320707071708v578a3e32i8f1b1d1f3f8cf355@mail.gmail.com>
References: <3f2938d50707071641j5e189dfenece984279cd58cb4@mail.gmail.com>
	<644e1f320707071708v578a3e32i8f1b1d1f3f8cf355@mail.gmail.com>
Message-ID: <20070708111430.v1p3dubmoko8o0sg@icare.u-paris10.fr>

jim holtman <jholtman at gmail.com> a ??crit? :

> Is this what you want?
>
>> t(combn(5,2))
>

Well, it seems nice, but from which library does it come ?
I try help.search("combn"), but that did not give me any valuable 
information...

Christophe



----------------------------------------------------------------
Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre


From sigiml at netvision.net.il  Sun Jul  8 12:45:38 2007
From: sigiml at netvision.net.il (sigalit mangut-leiba)
Date: Sun, 08 Jul 2007 12:45:38 +0200
Subject: [R] longitudinal data
Message-ID: <000001c7c14d$2009bc60$0200000a@fox1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/77ed8c31/attachment.pl 

From r.t.a.j.leenders at rug.nl  Sun Jul  8 12:20:01 2007
From: r.t.a.j.leenders at rug.nl (Roger Leenders)
Date: Sun, 08 Jul 2007 12:20:01 +0200
Subject: [R] one question about the loop
In-Reply-To: <20070708111430.v1p3dubmoko8o0sg@icare.u-paris10.fr>
References: <3f2938d50707071641j5e189dfenece984279cd58cb4@mail.gmail.com>	<644e1f320707071708v578a3e32i8f1b1d1f3f8cf355@mail.gmail.com>
	<20070708111430.v1p3dubmoko8o0sg@icare.u-paris10.fr>
Message-ID: <4690BA51.3030905@rug.nl>


combinat
http://cran.r-project.org/src/contrib/Descriptions/combinat.html


cgenolin at u-paris10.fr schreef:
> jim holtman <jholtman at gmail.com> a ??crit? :
>
>   
>> Is this what you want?
>>
>>     
>>> t(combn(5,2))
>>>       
>
> Well, it seems nice, but from which library does it come ?
> I try help.search("combn"), but that did not give me any valuable 
> information...
>
> Christophe
>
>
>
> ----------------------------------------------------------------
> Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Sun Jul  8 13:03:13 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 08 Jul 2007 13:03:13 +0200
Subject: [R] calculating p-values of columns in a dataframe
In-Reply-To: <455639.16597.qm@web59308.mail.re1.yahoo.com>
References: <455639.16597.qm@web59308.mail.re1.yahoo.com>
Message-ID: <4690C471.3060603@statistik.uni-dortmund.de>



Thomas Pujol wrote:
> I have a dataframe ("mydf") that contains "differences of means".
> I wish to test whether these differences are significantly different from zero.
> 
> Below, I calculate the t-statistic for each column.
> 
> What is a "good" method to calculate/look-up the p-value for each column?
> 
> 
> mydf=data.frame(a=c(1,-22,3,-4),b=c(5,-6,-7,9))
> 
> mymean=mean(mydf)
> mysd=sd(mydf)
> mynn=sapply(mydf, function(x) {sum ( as.numeric(x) >= -Inf) })
> myse=mysd/sqrt(mynn)
> myt=mymean/myse
> myt

You can do the whole lot with
   L <- lapply(mydf, t.test)
or if you only want the t statistics and p-values now:
   sapply(L, "[", c("statistic", "p.value"))

If you want to follow your initial approach quickly, you can calculate 
the probability function of the t distribution with 3 degrees of freedom 
(for your data) with
   2 * pt(-abs(myt), df = nrow(mydf) - 1)

Uwe Ligges




> 
>  
> ---------------------------------
> Food fight? Enjoy some healthy debate
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From decoder at wjpserver.cs.uni-sb.de  Sun Jul  8 13:55:07 2007
From: decoder at wjpserver.cs.uni-sb.de (Christian Holler)
Date: Sun, 08 Jul 2007 13:55:07 +0200
Subject: [R] Problems with e1071 and SparseM
Message-ID: <4690D09B.3090807@wjpserver.cs.uni-sb.de>

Hello all,


I am trying to use the "svm" method provided by e1071 (Version: 1.5-16)
together with a matrix provided by the SparseM package (Version: 0.73)
but it fails with this message:

> model <- svm(lm, lv, scale = TRUE, type = 'C-classification', kernel =
'linear')
Error in t.default(x) : argument is not a matrix

although lm was created before with read.matrix.csr (from the e1071)
package.

I also tried to simply convert a normal matrix to a SparseM matrix and
then pass it, but I get the same error again.

According to the manual of svm(), this is supposed to work though:

"       x: a data matrix, a vector, or a sparse matrix (object of class
          'matrix.csr' as provided by the package 'SparseM').    "

Used R version: R version 2.4.0 Patched (2006-11-25 r39997)

Does anyone know how I can use Sparse Matrices with e1071? This would be
really important because the matrix is simply too large to write it out.


Best regards,


Chris


-- 

Christian Holler
System Administrator

Chair of Prof. Dr. W.J. Paul
Saarland University
Germany
Building E1 3, Room 3.20

phone: +49 - 681 / 302 - 5537
fax:   +49 - 681 / 302 - 4290


From MayaB at tauex.tau.ac.il  Sun Jul  8 14:29:41 2007
From: MayaB at tauex.tau.ac.il (Maya Bercovich)
Date: Sun, 8 Jul 2007 15:29:41 +0300
Subject: [R] how to revert to an older limma version?
Message-ID: <FE6D3F1920752643B6A9EF0307CBD80E03238844@EX2.tau-centaur.tau.ac.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/70bd7f34/attachment.pl 

From mtmorgan at fhcrc.org  Sun Jul  8 15:50:49 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Sun, 08 Jul 2007 06:50:49 -0700
Subject: [R] how to revert to an older limma version?
In-Reply-To: <FE6D3F1920752643B6A9EF0307CBD80E03238844@EX2.tau-centaur.tau.ac.il>
	(Maya Bercovich's message of "Sun, 8 Jul 2007 15:29:41 +0300")
References: <FE6D3F1920752643B6A9EF0307CBD80E03238844@EX2.tau-centaur.tau.ac.il>
Message-ID: <6phir8vroli.fsf@gopher4.fhcrc.org>

"Maya Bercovich" <MayaB at tauex.tau.ac.il> writes:

> Dear Sirs,

Please post to the Bioconductor list (see http://bioconductor.org for
instructions)

> How can I revert to an older limma version?
> Typing "install.packages("limma")"  in R gives a list of mirrors. How
> can I install the version I want after I obtain and untar the file (e.g,
> limma_2.9.1.tar.gz)?
> I am running R 2.5.0 on a Linux machine (CentOS 5).  When using limma it
> will not go past the read.maimages command. 
> I get this error:
> Error in readGenericHeader(fullname, columns = columns, sep = sep) :
>          Specified column headings not found in file
>  In addition: Warning message:
> input string 1 is invalid in this locale in: grep(pattern, x,
> ignore.case, extended, value, fixed, useBytes)

Please provide a short section of code that shows how you invoke the
function. It is very hard to tell what the cause of your problem is
without this.

read.maimages has a 'columns' argument. Do you supply it? If so, does
it contain the correct column names for the file type you are reading?
For instance, is the capitalization correct? The help page for
read.maimages provides some guidance.

> I was told by a colleague that this may be due to my limma version. 
> I try to use limma 2.10.5 and he uses 2.9.1

Specific Bioconductor package versions work with specific R
versions. Instead of using install.packages, use

> source("http://bioconductor.org/biocLite.R")
> biocLite("limma")

to get the right version for your R. For R 2.5.0, limma 2.10.5 is the
correct version. The limma author is very responsive to bug reports, so
seek additional help and if necessary report bugs rather than revert
to previous versions.

> Could this be the reason?

Please always provide the output of the command

> sessionInfo()

to provide a concise summary of your system.

> Thanks in advance
> Maya
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Bioconductor / Computational Biology
http://bioconductor.org


From ripley at stats.ox.ac.uk  Sun Jul  8 16:08:03 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 8 Jul 2007 15:08:03 +0100 (BST)
Subject: [R] how to revert to an older limma version?
In-Reply-To: <FE6D3F1920752643B6A9EF0307CBD80E03238844@EX2.tau-centaur.tau.ac.il>
References: <FE6D3F1920752643B6A9EF0307CBD80E03238844@EX2.tau-centaur.tau.ac.il>
Message-ID: <Pine.LNX.4.64.0707081506170.3028@gannet.stats.ox.ac.uk>

That sounds as if you are running in a UTF-8 locale and your colleague is 
not.  We do ask for the results of sessionInfo(), which would have helped.

I suggest you try an English 8-bit locale and see what happens.

On Sun, 8 Jul 2007, Maya Bercovich wrote:

> Dear Sirs,
> How can I revert to an older limma version?
> Typing "install.packages("limma")"  in R gives a list of mirrors. How
> can I install the version I want after I obtain and untar the file (e.g,
> limma_2.9.1.tar.gz)?
> I am running R 2.5.0 on a Linux machine (CentOS 5).  When using limma it
> will not go past the read.maimages command.
> I get this error:
> Error in readGenericHeader(fullname, columns = columns, sep = sep) :
>         Specified column headings not found in file
> In addition: Warning message:
> input string 1 is invalid in this locale in: grep(pattern, x,
> ignore.case, extended, value, fixed, useBytes)
> I was told by a colleague that this may be due to my limma version.
> I try to use limma 2.10.5 and he uses 2.9.1
> Could this be the reason?
> Thanks in advance
> Maya
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jholtman at gmail.com  Sun Jul  8 16:08:45 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 8 Jul 2007 10:08:45 -0400
Subject: [R] one question about the loop
In-Reply-To: <20070708111430.v1p3dubmoko8o0sg@icare.u-paris10.fr>
References: <3f2938d50707071641j5e189dfenece984279cd58cb4@mail.gmail.com>
	<644e1f320707071708v578a3e32i8f1b1d1f3f8cf355@mail.gmail.com>
	<20070708111430.v1p3dubmoko8o0sg@icare.u-paris10.fr>
Message-ID: <644e1f320707080708u15a63593q7cbb4eaa4a929064@mail.gmail.com>

It is part of the standard 'util' library that comes with R

?combn
help.search('combination')


On 7/8/07, cgenolin at u-paris10.fr <cgenolin at u-paris10.fr> wrote:
> jim holtman <jholtman at gmail.com> a ?(c)crit? :
>
> > Is this what you want?
> >
> >> t(combn(5,2))
> >
>
> Well, it seems nice, but from which library does it come ?
> I try help.search("combn"), but that did not give me any valuable
> information...
>
> Christophe
>
>
>
> ----------------------------------------------------------------
> Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Sun Jul  8 16:13:22 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 8 Jul 2007 10:13:22 -0400
Subject: [R] longitudinal data
In-Reply-To: <000001c7c14d$2009bc60$0200000a@fox1>
References: <000001c7c14d$2009bc60$0200000a@fox1>
Message-ID: <644e1f320707080713m56e78071u7cde197972e4bd5d@mail.gmail.com>

The question is, how is the missing data accounted for?  Is this a CSV
file where the missing data is left blank?  If it is just separated by
white space, how do you know that var1 is missing is var2 is there?

If it is the case that just the initial values are there, then you can
use fill=TRUE on read.table which will supply NAs for the trailing
values in uneven rows.

You need to provide a reproducible script/data so that we have a
better chance of answering your questions.

On 7/8/07, sigalit mangut-leiba <sigiml at netvision.net.il> wrote:
> Hello all,
>
> I want to analyze data that looks like this:
>
> Id var1 var2 var3..
>
> 1   0      1       0
>
> 1   0      1       1
>
> 2
>
> 2
>
> 2
>
> 2
>
> Not all id's have the same no. of observations.
>
> At the first stage I want to count how many people in the survey, how many
> have 1 in var1, etc.
>
> How do I do that?
>
> Thank you,
>
> Sigalit.
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jzhang1982 at gmail.com  Sun Jul  8 18:03:45 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sun, 8 Jul 2007 10:03:45 -0600
Subject: [R] random sampling with some limitive conditions?
In-Reply-To: <3f2938d50707072358k40b8604eu3effe26f3b57b299@mail.gmail.com>
References: <3f2938d50707071230n3974be62x368a660762e674f4@mail.gmail.com>
	<002701c7c0da$ea9e60c0$0201a8c0@Aragorn>
	<3f2938d50707072358k40b8604eu3effe26f3b57b299@mail.gmail.com>
Message-ID: <3f2938d50707080903mc341bc1nc2ecdcf3e85aec50@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/37fb8274/attachment.pl 

From francogrex at mail.com  Sun Jul  8 18:07:33 2007
From: francogrex at mail.com (francogrex)
Date: Sun, 8 Jul 2007 09:07:33 -0700 (PDT)
Subject: [R] Extracting S code from a C program
Message-ID: <11489962.post@talk.nabble.com>


There is a C program called GPS: 'gamma poisson shrinker' at
ftp://ftp.research.att.com/dist/gps/
The algorithms in GPS are based on S-Plus programs written by William
DuMouchel with support from Columbia University and AT&T Labs. 
My question is: is there a relatively easy way to extract some of the S code
from this windows program? Thanks.



-- 
View this message in context: http://www.nabble.com/Extracting-S-code-from-a-C-program-tf4044952.html#a11489962
Sent from the R help mailing list archive at Nabble.com.


From arjunravinarayan at gmail.com  Sun Jul  8 18:08:47 2007
From: arjunravinarayan at gmail.com (Arjun Ravi Narayan)
Date: Sun, 8 Jul 2007 12:08:47 -0400
Subject: [R] xmlOutputBuffer vs xmlOutputDOM
Message-ID: <8029eca10707080908w5bae3f09n3b8211b91f2d3ea1@mail.gmail.com>

Hi,

I am trying to use the XML package to write some data (pretty large
amounts of data) into XML files. I experimented with a few variations,
using xmlOutputBuffer and xmlOutputDOM.

xmlOutputDOM provides neat formatted, indented output, but takes very
long. xmlOutputBuffer is incompatible (in my experiences) with the
saveXML function, and so i hacked around it by outputting its $value()
to cat. This unfortunately makes it lose all proper formatting, and so
gives me an XML file with new lines after every tag or entry, and with
no indenting at all.

However, xmlOutputDOM takes very long - I am outputting rather large
files, and where xmlOutputBuffer takes about 10-15 seconds,
xmlOutputDOM takes about 20 minutes.

Am I using xmlOutputDOM in some wrong way? Is there a way to get
proper formatting out of xmlOutputBuffer? Either of these solutions
would be useful, as I see no advantage to using one over the other for
just outputting lots of data (>10000 fields at minimum)

Below is my code, and after that, an output of the times that were
reported on a sample run:


library(XML)

buffer <- xmlOutputBuffer()
buffer2 <- xmlOutputDOM()

buffer$addTag("outside", close = FALSE)
buffer2$addTag("outside", close = FALSE)

for(i in 1:1000) {
 buffer$addTag("tag", i)
 buffer2$addTag("tag", i)
}

buffer$closeTag()
buffer2$closeTag()

system.time(cat(buffer$value(), file = "foo2.xml"))
system.time(saveXML(buffer2$value(), file = "foo.xml"))


Times reported : the xmlOutputDOM is more than 100x slower.

> system.time(cat(buffer$value(), file = "foo2.xml"))
  user  system elapsed
 0.004   0.000   0.001
> system.time(saveXML(buffer2$value(), file = "foo.xml"))
  user  system elapsed
 0.476   0.024   0.516
>


I am using R version 2.5.1, and XML package version 1.9-0


Yours sincerely,
Arjun Ravi Narayan


From zaslavsk at hcp.med.harvard.edu  Sun Jul  8 18:40:08 2007
From: zaslavsk at hcp.med.harvard.edu (Alan Zaslavsky)
Date: Sun, 8 Jul 2007 12:40:08 -0400 (EDT)
Subject: [R]  random sampling with some limitive conditions?
Message-ID: <Pine.GSO.4.60.0707081237350.13671@hcp>

If I understand your problem, this might be a solution.  Assign 
independent random numbers for row and column and use the corresponding 
ordering to assign the row and column indices.  Thus row and column 
assignments are independent and the row and column totals are fixed.  If 
cc and rr are respectively the desired row and column totals, with 
sum(cc)==sum(rr), then

n = sum(cc)
row.assign = rep(1:length(rr),rr)[order(runif(n))]
col.assign = rep(1:length(cc),cc)[order(runif(n))]

If you want many such sets of random assignments to be generated at once 
you can use a few more rep() calls in the expressions to generate multiple 
sets in the same way.  (Do you actually want the assignments or just the 
tables?) Of course there are many other possible solutions since you have 
not fully specified the distribution you want.

 	Alan Zaslavsky
 	Harvard U

> From: "Zhang Jian" <jzhang1982 at gmail.com>
> Subject: [R] random sampling with some limitive conditions?
> To: r-help <r-help at stat.math.ethz.ch>
> 
> I want to gain thousands of random sampling data by randomizing the
> presence-absence data. Meantime, one important limition is that the row and
> column sums must be fixed. For example, the data "tst" is following:
>    site1 site2 site3 site4 site5 site6 site7 site8 1 0 0 0 1 1 0 0 0 1 1 1 0
> 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0
> 0 0 0 0 0 0 0 0 1 0 1 0 1
> 
> sum(tst[1,]) = 3, sum(tst[,1])=4, and so on. When I randomize the data, the
> first row sums must equal to 3, and the first column sums must equal to 4.
> The rules need to be applied to each row and column.
> How to get the new random sampling data? I have no idea.
> Thanks.


From murdoch at stats.uwo.ca  Sun Jul  8 18:59:46 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 08 Jul 2007 12:59:46 -0400
Subject: [R] Extracting S code from a C program
In-Reply-To: <11489962.post@talk.nabble.com>
References: <11489962.post@talk.nabble.com>
Message-ID: <46911802.3020901@stats.uwo.ca>

On 08/07/2007 12:07 PM, francogrex wrote:
> There is a C program called GPS: 'gamma poisson shrinker' at
> ftp://ftp.research.att.com/dist/gps/
> The algorithms in GPS are based on S-Plus programs written by William
> DuMouchel with support from Columbia University and AT&T Labs. 
> My question is: is there a relatively easy way to extract some of the S code
> from this windows program? Thanks.

No.  From the description, there's no S code there to extract, it's been 
translated to C, and you don't even have the source code.

I'd recommend contacting Dr. DuMouchel to see if he is willing to let 
you have his S code.

Duncan Murdoch


From james.milks at wright.edu  Sun Jul  8 19:08:27 2007
From: james.milks at wright.edu (James R. Milks)
Date: Sun, 08 Jul 2007 13:08:27 -0400
Subject: [R] generating a data frame with a subset from another data frame
Message-ID: <36D5F0F6-E65F-4BB7-B1C9-7C654056DD8E@wright.edu>

R gurus,

I have a data set that looks something like this:

Site	Species	DBH	#Vines
G	PLOC	45.9	4
G	ACNE	23.3	1
G	ACNE	12.0	0
G	FRAM	35.9	5
G	AEGL	11.2	2
N	PLOC	77.3	12
N	JUNI	78.6	7
N	ACNE	18.9	1
N	ACNE	15.7	3
N	ACRU	35.5	4
H	ACSA2	24.1	6
H	ULAM	35.2	7

There are 730 individual trees (22 species) from four sites in the  
actual data set.  I would like to create a second data frame that  
contains just the most common species (mainly ACNE, PLOC, ULAM, FRAM,  
and ACSA2).  Here's some of my attempts:

 >study.1<-subset(study,study$Species=c 
("ACNE","PLOC","FRAM","ULAM","ACSA2))
Error: syntax error

 >study.1<-study[study$Species==,c("ACNE","PLOC","FRAM","ULAM","ACSA2)]
Error: syntax error

 >study.1<-study[c("ACNE","PLOC","FRAM","ULAM","ACSA2"),]
#This one appeared to work, but upon inspection, it just copied the  
entire "study" data frame instead of just copying the data I wanted,  
as study.1$Species had a length of 22 (the same as the original)  
instead of the desired length of 5.

I've already consulted a book on R as well as spent the last three  
hours searching the R-help archives.  There must be a way to get the  
subset I desire but it is not obvious to me.

Thanks in advance for your help.

Jim Milks

Graduate Student
Environmental Sciences Ph.D. Program
Wright State University
3640 Colonel Glenn Hwy
Dayton, OH 45431


From jzhang1982 at gmail.com  Sun Jul  8 19:12:42 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sun, 8 Jul 2007 11:12:42 -0600
Subject: [R] random sampling with some limitive conditions?
In-Reply-To: <Pine.GSO.4.60.0707081237350.13671@hcp>
References: <Pine.GSO.4.60.0707081237350.13671@hcp>
Message-ID: <3f2938d50707081012i2fff4919u218be88294bdcfc9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/b07a1676/attachment.pl 

From valiente at lsi.upc.edu  Sun Jul  8 19:30:40 2007
From: valiente at lsi.upc.edu (Gabriel Valiente)
Date: Sun, 8 Jul 2007 19:30:40 +0200
Subject: [R] Efficient matrix slices
Message-ID: <BB632C5D-51B7-4ADA-9DD2-69DB445BAB4A@lsi.upc.edu>

Indexing matrices by subsets of rows and columns is quite convenient,  
but it seems to take time linear in the size of the matrix (even for  
a small slice of the matrix):

 > dim(y)
[1]  732 1332
 > length(which(a[1,]==1))
[1] 4
 > length(which(b[1,]==1))
[1] 12
 > proc.time(y[which(a[1,]==1),which(b[1,]==1)])
[1]  32.596   1.809 510.928   0.000   0.000
 > proc.time(sum(y))
[1]  33.082   1.914 547.469   0.000   0.000

Does anybody know how matrix slices are actually implemented in R?  
Thanks a lot,

Gabriel


From afshart at exchange.sba.miami.edu  Sun Jul  8 19:50:20 2007
From: afshart at exchange.sba.miami.edu (Afshartous, David)
Date: Sun, 8 Jul 2007 13:50:20 -0400
Subject: [R] random effect variance per treatment group in lmer
Message-ID: <6BCB4D493A447546A8126F24332056E8063E8BAA@school1.business.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/6d308cac/attachment.pl 

From amnakhan493 at gmail.com  Sun Jul  8 19:52:17 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 8 Jul 2007 10:52:17 -0700
Subject: [R] Windows Binary for ncdf package
Message-ID: <3ffd3bb60707081052t5c210333rdb980de31ae281ad@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/b4bfa554/attachment.pl 

From amnakhan493 at gmail.com  Sun Jul  8 19:52:17 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 8 Jul 2007 10:52:17 -0700
Subject: [R] Windows Binary for ncdf package
Message-ID: <3ffd3bb60707081052t5c210333rdb980de31ae281ad@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/b4bfa554/attachment-0001.pl 

From jholtman at gmail.com  Sun Jul  8 19:55:08 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 8 Jul 2007 13:55:08 -0400
Subject: [R] generating a data frame with a subset from another data
	frame
In-Reply-To: <36D5F0F6-E65F-4BB7-B1C9-7C654056DD8E@wright.edu>
References: <36D5F0F6-E65F-4BB7-B1C9-7C654056DD8E@wright.edu>
Message-ID: <644e1f320707081055p52a5bf6ax62c1d31b950202a5@mail.gmail.com>

?"%in%"

I think what you want is:

study.1<-subset(study,study$Species %in% c("ACNE","PLOC","FRAM","ULAM","ACSA2))


On 7/8/07, James R. Milks <james.milks at wright.edu> wrote:
> R gurus,
>
> I have a data set that looks something like this:
>
> Site    Species DBH     #Vines
> G       PLOC    45.9    4
> G       ACNE    23.3    1
> G       ACNE    12.0    0
> G       FRAM    35.9    5
> G       AEGL    11.2    2
> N       PLOC    77.3    12
> N       JUNI    78.6    7
> N       ACNE    18.9    1
> N       ACNE    15.7    3
> N       ACRU    35.5    4
> H       ACSA2   24.1    6
> H       ULAM    35.2    7
>
> There are 730 individual trees (22 species) from four sites in the
> actual data set.  I would like to create a second data frame that
> contains just the most common species (mainly ACNE, PLOC, ULAM, FRAM,
> and ACSA2).  Here's some of my attempts:
>
>  >study.1<-subset(study,study$Species=c
> ("ACNE","PLOC","FRAM","ULAM","ACSA2))
> Error: syntax error
>
>  >study.1<-study[study$Species==,c("ACNE","PLOC","FRAM","ULAM","ACSA2)]
> Error: syntax error
>
>  >study.1<-study[c("ACNE","PLOC","FRAM","ULAM","ACSA2"),]
> #This one appeared to work, but upon inspection, it just copied the
> entire "study" data frame instead of just copying the data I wanted,
> as study.1$Species had a length of 22 (the same as the original)
> instead of the desired length of 5.
>
> I've already consulted a book on R as well as spent the last three
> hours searching the R-help archives.  There must be a way to get the
> subset I desire but it is not obvious to me.
>
> Thanks in advance for your help.
>
> Jim Milks
>
> Graduate Student
> Environmental Sciences Ph.D. Program
> Wright State University
> 3640 Colonel Glenn Hwy
> Dayton, OH 45431
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From duncan at wald.ucdavis.edu  Sun Jul  8 20:13:32 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Sun, 08 Jul 2007 11:13:32 -0700
Subject: [R] xmlOutputBuffer vs xmlOutputDOM
In-Reply-To: <8029eca10707080908w5bae3f09n3b8211b91f2d3ea1@mail.gmail.com>
References: <8029eca10707080908w5bae3f09n3b8211b91f2d3ea1@mail.gmail.com>
Message-ID: <4691294C.6030205@wald.ucdavis.edu>

Hi Arjun

Have you tried using xmlTree() which uses an opaque
C representation of the document and I expect
will serialize the contents relatively rapidly.
The interface for creating the tree is intended to be
the same, and is at least similar to, as xmlOutputDOM.
The intent is that the representations are easily interchangeable.

xmlOutputDOM is slow because it is representing a tree in R as
a list of lists. You might also use xmlHashTree() which uses
a more efficient representation in R.  But the nature of
the C representation (and not just the fact that it uses C code)
will probably speed things up considerably.


A question that comes to mind is why you really care about
pretty printing of the resulting document if it is very large?
Will a human read it?  If so and it is just for verifying it is correct,
read it back into R and validate the contents programmatically.


  D.


Arjun Ravi Narayan wrote:
> Hi,
> 
> I am trying to use the XML package to write some data (pretty large
> amounts of data) into XML files. I experimented with a few variations,
> using xmlOutputBuffer and xmlOutputDOM.
> 
> xmlOutputDOM provides neat formatted, indented output, but takes very
> long. xmlOutputBuffer is incompatible (in my experiences) with the
> saveXML function, and so i hacked around it by outputting its $value()
> to cat. This unfortunately makes it lose all proper formatting, and so
> gives me an XML file with new lines after every tag or entry, and with
> no indenting at all.
> 
> However, xmlOutputDOM takes very long - I am outputting rather large
> files, and where xmlOutputBuffer takes about 10-15 seconds,
> xmlOutputDOM takes about 20 minutes.
> 
> Am I using xmlOutputDOM in some wrong way? Is there a way to get
> proper formatting out of xmlOutputBuffer? Either of these solutions
> would be useful, as I see no advantage to using one over the other for
> just outputting lots of data (>10000 fields at minimum)
> 
> Below is my code, and after that, an output of the times that were
> reported on a sample run:
> 
> 
> library(XML)
> 
> buffer <- xmlOutputBuffer()
> buffer2 <- xmlOutputDOM()
> 
> buffer$addTag("outside", close = FALSE)
> buffer2$addTag("outside", close = FALSE)
> 
> for(i in 1:1000) {
>  buffer$addTag("tag", i)
>  buffer2$addTag("tag", i)
> }
> 
> buffer$closeTag()
> buffer2$closeTag()
> 
> system.time(cat(buffer$value(), file = "foo2.xml"))
> system.time(saveXML(buffer2$value(), file = "foo.xml"))
> 
> 
> Times reported : the xmlOutputDOM is more than 100x slower.
> 
>> system.time(cat(buffer$value(), file = "foo2.xml"))
>   user  system elapsed
>  0.004   0.000   0.001
>> system.time(saveXML(buffer2$value(), file = "foo.xml"))
>   user  system elapsed
>  0.476   0.024   0.516
> 
> 
> I am using R version 2.5.1, and XML package version 1.9-0
> 
> 
> Yours sincerely,
> Arjun Ravi Narayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Sun Jul  8 20:19:00 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 08 Jul 2007 20:19:00 +0200
Subject: [R] Windows Binary for ncdf package
In-Reply-To: <3ffd3bb60707081052t5c210333rdb980de31ae281ad@mail.gmail.com>
References: <3ffd3bb60707081052t5c210333rdb980de31ae281ad@mail.gmail.com>
Message-ID: <46912A94.8080507@statistik.uni-dortmund.de>



amna khan wrote:
> Dear Sir
> 
> There is no window binary version of package ncdf in the latest release of R
> 2.5.1. i dont have any information about the old versions.
> Please guid in thie regard
> Thank you


See the ReadMe in the CRAN repository. It tells you that "ncdf" does not 
build out of the box on Windows and is hence not available on CRAN. 
Nevertheless, Brian kindly provides the binary in his "CRAN (extras)" 
repository (URL http://www.stats.ox.ac.uk/pub/RWin). Just type
   install.packages("ncdf")
and it will be installed ...

Uwe Ligges


From h.wickham at gmail.com  Sun Jul  8 21:10:49 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 8 Jul 2007 21:10:49 +0200
Subject: [R] [R-pkgs] Scagnostics - scatterplot diagnostics
Message-ID: <f8e6ff050707081210n29e29847y372e02fd220c5d3b@mail.gmail.com>

The scagnostics package implements the graph theoretic scagnostics
described by Leland Wilkinson, Anushka Anand and Robert Grossman
(http://www.ncdm.uic.edu/publications/files/proc-094.pdf), building on
an old idea of Tukey's to define indices of "interestingness" to help
guide the search for interesting features in the pair-wise
scatterplots of a highly multivariate dataset.

The scagnostics package currently only supports two methods, one which
computes the scagnostics for a pair of variables, and the other for
all pairs of variables in a data.frame.

If you are attending the JSM, there is a session on scagnostics.
Details are available at http://tinyurl.com/324yb5

(The package has just been added to CRAN, it may be a couple of days
before it is available on your local mirror)

Regards,

Hadley

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From singularitaet at gmx.net  Sun Jul  8 21:15:46 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Sun, 08 Jul 2007 21:15:46 +0200
Subject: [R] Windows Binary for ncdf package
In-Reply-To: <3ffd3bb60707081052t5c210333rdb980de31ae281ad@mail.gmail.com>
References: <3ffd3bb60707081052t5c210333rdb980de31ae281ad@mail.gmail.com>
Message-ID: <469137E2.80905@gmx.net>

-------- Original Message  --------
Subject: [R] Windows Binary for ncdf package
From: amna khan <amnakhan493 at gmail.com>
To: R-help at stat.math.ethz.ch, R-help at stat.math.ethz.ch
Date: 08.07.2007 19:52
> Dear Sir
>
> There is no window binary version of package ncdf in the latest release of R
> 2.5.1. i dont have any information about the old versions.
> Please guid in thie regard
> Thank you
>
>
>   
It is there!
install.packages("ncdf")

trying URL
'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/2.5/ncdf_1.6.zip'
Content type 'application/zip' length 231140 bytes
opened URL
downloaded 225Kb

package 'ncdf' successfully unpacked and MD5 sums checked

Maybe you should try another mirror.

Stefan

> version
               _                          
platform       i386-pc-mingw32            
arch           i386                       
os             mingw32                    
system         i386, mingw32              
status                                    
major          2                          
minor          5.1                        
year           2007                       
month          06                         
day            27                         
svn rev        42083                      
language       R                          
version.string R version 2.5.1 (2007-06-27)
-=-=-
... Money: There's nothing in the world so demoralizing as money.
(Sophocles)


From luo_weijun at yahoo.com  Sun Jul  8 21:30:11 2007
From: luo_weijun at yahoo.com (Luo Weijun)
Date: Sun, 8 Jul 2007 12:30:11 -0700 (PDT)
Subject: [R] Loading problem with XML_1.9
In-Reply-To: <46908064.605@wald.ucdavis.edu>
Message-ID: <701633.8753.qm@web32503.mail.mud.yahoo.com>

Thanks, Dr. Lang,
I used xmlEventParse() + branches concept as you
suggested, it really works, and the memory issue is
gone. Now I can query large XML files from within R.
but here is another problem: it is too slows (a simple
query has not finished for 1.5h), even though the
number of relevant records is very limited, but the
whole XML file has more than 500 thousand
similarly-structured records. And the parser has to go
through all of them as to find the matches. Attached
is part of the XML files with two records. I am trying
to retrieve the content of <moleculeName> nodes from
<molecule> records where <name> nodes bear specific
gene names.
Is it possible to locate based on node content (or
xmlValue) rather than node names (since they are the
same in all records) first and then parse the xml
record locally? Would query based on XPath be faster
in this case? I understand that we do have the
facility in the XML package for XPath based queries,
called getNodeSet(). But that requires reading the
whole XML tree into the memory first, which is not
feasible for my large XML file. Or can I call
XML::XPath statements using your R-Perl interface
package? Any suggestions/thoughts? Thank you!
Weijun


Part of my XML file: 

<molecule>
<prov><im><imid>20</imid></im></prov><moleculeID>119043</moleculeID>
<moleculeType>protein<prov><im><imid>20</imid></im></prov></moleculeType>
<organismID>10090<prov><im><imid>20</imid></im></prov></organismID>
<id><prov><im><imid>20</imid></im></prov><idType>GI</idType><idValue>6677981</idValue></id>
<name>SKD1<prov><im><imid>20</imid></im></prov></name>
<name>Vps4b<prov><im><imid>20</imid></im></prov></name>
<name>8030489C12Rik<prov><im><imid>20</imid></im></prov></name>
<description><distribution><value>Mouse homologue of
yeast Vacuolar protein sorting 4 (Vps4); Suppressor of
potassium transport defect 1. Mem
ber of mammalian class E Vps proteins involved in
endosomal transport; AAA-type
ATPase.<prov><im><imid>20</imid></im></prov></value><value>Mo
use homologue of yeast  Vacuolar protein sorting 4
(Vps4); Suppressor of potassium  transport defect 1.
Member of  mammalian class E Vps prot
eins involved in endosomal transport; AAA-type
ATPase.<prov><im><imid>20</imid></im></prov></value></distribution></description>
<orthologue>
<method><methodID>337974</methodID><methodName>miClust80</methodName></method>
</orthologue>
<variant>
<prov><im><imid>20</imid></im></prov><variantID>0</variantID>
</variant>
<interaction><interactionRef>201581</interactionRef><moleculeRef>89434</moleculeRef><moleculeName>SBP1</moleculeName>
<selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
<interaction><interactionRef>201582</interactionRef><moleculeRef>17953</moleculeRef><moleculeName>mVps2</moleculeName>
<selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
</molecule>

<molecule>
<prov><im><imid>30</imid></im></prov><moleculeID>116226</moleculeID>
<moleculeType>protein<prov><im><imid>30</imid></im></prov></moleculeType>
<organismID>9606<prov><im><imid>30</imid></im></prov></organismID>
<id><prov><im><imid>30</imid></im></prov><idType>HGNC</idType><idValue>9859</idValue></id>
<name>RAP1GDS1<prov><im><imid>30</imid></im></prov></name>
<name>GDS1<prov><im><imid>30</imid></im></prov></name>
<name>MGC118859<prov><im><imid>30</imid></im></prov></name>
<name>MGC118861<prov><im><imid>30</imid></im></prov></name>
<variant>
<prov><im><imid>30</imid></im></prov><variantID>0</variantID>
</variant>
<interaction><interactionRef>93569</interactionRef><moleculeRef>116280</moleculeRef><moleculeName>RAC1</moleculeName>
<selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
<interaction><interactionRef>104132</interactionRef><moleculeRef>103040</moleculeRef><moleculeName>RHOA</moleculeName>
<selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
<interaction><interactionRef>121818</interactionRef><moleculeRef>74726</moleculeRef><moleculeName>MBIP</moleculeName>
<selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
</molecule>

--- Duncan Temple Lang <duncan at wald.ucdavis.edu>
wrote:

> 
> Well, as you mention at the end of the mail,
> several people have given you suggestions about
> how to solve the problem using different approaches.
> You might search on the Web for how to install a 64
> bit version of libxml2?
> Using xmlTreeParse(, useInternalNodes = TRUE) is an
> approach
> to reducing the memory consumption as is using the
> handlers
> argument. And if size is really the issue, you
> should consider
> the SAX model which is very memory efficient and
> made available
> via the xmlEventParse() function in the XML package.
> And it even provides the concepts of branches to
> provide a
> hybrid of SAX and DOM-style parsing together.
> 
> However, to solve the problem of the xmlMemDisplay
> symbol not being found, you can look for where
> that is used and remove it.    It is in
> src/DocParse.c
> in the routine RS_XML_MemoryShow().  You can remove
> the line
>   xmlMemDisplay(stderr)
> or indeed the entire routine.  Then re-install and
> reload the package.
> 
>  D.
> 
> 
> Luo Weijun wrote:
> > Hello Dr. Lang and all,
> > I posted this message in R-help mail list, but
> haven???t
> > solved my problem so far. Therefore, could you
> help me
> > look at it?
> > I have loading problem with XML_1.9 under 64 bit
> > R2.3.1 for Mac OS X, which I got from
> > http://R.research.att.com/. XML_1.9 works fine
> under
> > 32 bit R2.5.0. I thought that could be
> installation
> > problem, and I tried install.packages or biocLite,
> > every time the package installed fine, except some
> > warning messages below:
> > ld64 warning: in /usr/lib/libxml2.dylib, file does
> not
> > contain requested architecture
> > ld64 warning: in /usr/lib/libz.dylib, file does
> not
> > contain requested architecture
> > ld64 warning: in /usr/lib/libiconv.dylib, file
> does
> > not contain requested architecture
> > ld64 warning: in /usr/lib/libz.dylib, file does
> not
> > contain requested architecture
> > ld64 warning: in /usr/lib/libxml2.dylib, file does
> not
> > contain requested architecture
> > 
> > Here is the error messages I got, when XML is
> loaded:
> >> library(XML)
> > Error in dyn.load(x, as.logical(local),
> > as.logical(now)) : 
> >         unable to load shared library
> > '/usr/local/lib64/R/library/XML/libs/XML.so':
> >  
> dlopen(/usr/local/lib64/R/library/XML/libs/XML.so,
> > 6): Symbol not found: _xmlMemDisplay
> >   Referenced from:
> > /usr/local/lib64/R/library/XML/libs/XML.so
> >   Expected in: flat namespace
> > Error: .onLoad failed in 'loadNamespace' for 'XML'
> > Error: package/namespace load failed for 'XML'
> > 
> > Session information
> >> sessionInfo()
> > Version 2.3.1 Patched (2006-06-27 r38447) 
> > powerpc64-apple-darwin8.7.0 
> > 
> > attached base packages:
> > [1] "methods"   "stats"     "graphics" 
> "grDevices"
> > "utils"     "datasets" 
> > [7] "base"     
> > 
> > Prof Brian Ripley also suggested that this could
> be
> > that I don???t have a 64-bit version of libxml2
> > installed. Where I get it and where/how to install
> it,
> > if that???s the problem? 
> > The reason I need to use R64 is that I have memory
> > limitation issue with R 32 bit version when I load
> > some very large XML trees (the data file is about
> > 800M). And Martin suggested me to use 'handler'
> > argument of xmlTreeParse, tried 'handler' with
> > useInternalNodes=T, but I still got this memory
> > problem with R 32 bit version. Please tell me what
> I
> > can do now. Thank you so much!
> > Weijun
> > 
> > 
> > 
> >        
> >
>
____________________________________________________________________________________
> > 
> > Comedy with an Edge to see what's on, when.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> 



       
____________________________________________________________________________________
Pinpoint customers who are looking for what you sell.


From marc_schwartz at comcast.net  Sun Jul  8 22:20:37 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Sun, 08 Jul 2007 15:20:37 -0500
Subject: [R] Writing Excel (.xls) files on non-Windows OSs using Perl
Message-ID: <1183926037.3706.89.camel@Bellerophon.localdomain>

Hi all,

There have been quite a few threads in the recent months pertaining to
the ability to directly write native Excel (.xls) files from R. For
example, exporting R matrices and/or data frames to an Excel file, with
perhaps the ability to create multiple tabs (worksheets) within a single
file, with one tab/sheet per R object.

There exists the xlsReadWrite package on CRAN by Hans-Peter Suter, which
is restricted to Windows, since it utilizes the non-FOSS MS Office API
to write the Excel formats.

I recently had the need, under Linux (FC6/F7) to create an Excel file
containing multiple worksheets, each worksheet containing an 'exported'
data frame from R. While one could export the data frames to delimited
files (ie. using write.table() ) and then open those files from Excel
(or OO.org's Calc), it was rather tedious to do so with a larger number
of R objects. Since I would now have the need to engage in this process
with some level of frequency, the preceding approach would not be time
efficient.

I thus embarked on a mini-project to create a Perl script utilizing
openly available functions from CPAN and then facilitate the calling of
the script directly from R.

I am posting the Perl code here for the benefit of others who may have
similar requirements. Please note that I am providing this 'as is' and
don't have any plans to substantively modify or enhance the code. It
does what I need it to do. Feel free to modify for other needs as may be
required.

The basic calling schema is:

WriteXLS.pl [--CSVpath] [--CSVfiles] ExcelFileName

Where:

CSVpath = Path to the csv files created in R, typically done
          using write.table()

CSVfiles = globbed file name specification (ie. *.csv)

ExcelFileName = FULL name of Excel .xls file to create


When the Excel file is created, a new worksheet (tab) will be created
for each CSV file imported. The worksheet name will be the basename (no
path or extension) of the CSV file, up to the first 31 characters, which
is a limitation for Excel worksheet names.

Note of course that Excel has certain (version specific) limitations
with respect to file formats. I list the MS link below for Excel 2007.
Similar specs are available for earlier versions:

  http://office.microsoft.com/en-us/excel/HP100738491033.aspx

Finally, note that I use 'Spreadsheet::WriteExcel::Big', as the regular
version of the Perl package has a constraint where the ENTIRE Excel file
cannot be larger than 7 Mb, which was a problem for my application.


Here is the Perl code:


#!/usr/bin/perl -w

use strict;
use Spreadsheet::WriteExcel::Big;
use Getopt::Long;
use File::Glob;
use File::Basename;
use Text::CSV_XS;


# Initialize and get command line arguments
my $CSVPath = '.';
my $CSVFiles = "*.csv";

GetOptions ('CSVpath=s' => \$CSVPath, 
            'CSVfiles=s' => \$CSVFiles);

my $ExcelFileName = $ARGV[0];


# Create Excel XLS File
print "Creating Excel File: $ExcelFileName\n\n";
my $XLSFile  = Spreadsheet::WriteExcel::Big->new($ExcelFileName);

# Glob file path and names
my @FileNames = <$CSVPath/$CSVFiles>;


foreach my $FileName (@FileNames) {

  print "Reading: $FileName\n";

  # Open CSV File
  my $csv = Text::CSV_XS->new();
  open (CSVFILE, "$FileName") || die "ERROR: cannot open $FileName. $!\n";

  # Create new sheet with filename prefix
  # ($base, $dir, $ext) = fileparse ($FileName, '..*');
  my $FName = (fileparse ($FileName, '\..*'))[0];

  # Only take the first 31 chars, which is the
  # limit for a worksheet name
  my $SheetName = substr($FName, 0, 31);

  print "Creating New WorkSheet: $SheetName\n\n";

  my $WorkSheet = $XLSFile->add_worksheet($SheetName);

  # Rows and columns are zero indexed
  my $Row = 0;

  # Write to Sheet
  while (<CSVFILE>) {

    if ($csv->parse($_)) {
      my @Fields = $csv->fields();

      my $Col = 0;

      foreach my $Fld (@Fields) {
          $WorkSheet->write($Row, $Col, $Fld);
          $Col++;
      }

      $Row++;
    }
  }

  close CSVFILE;
}



A 'typical' sequence for the use of the code from within R might be:

# Create a character vector of R objects to be exported
RObjects <- c(VectorOfRObjectNames, ...)

# Now loop through the vector, creating CSV files
# In this case, export to a 'CSVFILES' sub-directory
for (i in RObjects)
{
  write.table(get(i), file = paste("CSVFILES/", i, ".csv", sep = ""),
              sep = ",", quote = TRUE, na = "", row.names = FALSE)
}


# Now call the Perl script from within R, presuming
# that the script is in the current default directory
system("./WriteXLS.pl --CSVPath CSVFILES RExport.xls")



This process has worked for me, given the current functional
requirements for my project. I hope that this is of some help to others.

Regards,

Marc Schwartz


From sharif_math2000 at yahoo.com  Mon Jul  9 00:57:19 2007
From: sharif_math2000 at yahoo.com (Mozumder Sharif)
Date: Sun, 8 Jul 2007 15:57:19 -0700 (PDT)
Subject: [R] need some help on Inverse Gaussian distribution
Message-ID: <880176.24823.qm@web60217.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/26474c02/attachment.pl 

From macq at llnl.gov  Mon Jul  9 01:09:11 2007
From: macq at llnl.gov (Don MacQueen)
Date: Sun, 8 Jul 2007 16:09:11 -0700
Subject: [R] Several quick questions
In-Reply-To: <468F8AF3.6010200@free.fr>
References: <468F8AF3.6010200@free.fr>
Message-ID: <p06240800c2b71e4754cc@[192.168.11.7]>

At 8:45 AM -0400 7/7/07, S?bastien wrote:
>Dear R users,
>
>Here is a couple a quick questions, for which I was unable to not find
>any answer in the list archives and in the help:
>
>1- Is there any R equivalents of the VB functions Cint, CStr, etc...
>(for non VB users, these functions transform the category of a specified
>variable and smartly adapt the value of this variable) ?
>
>I have tried to use the as.numeric, as.factor and as.vector commands but
>the result is not exactly what I want ([1] 1, 3, 5, 6)
>  >a<-as.factor(cbind(1,3,5,6))      # creates a dummy factor
>  >a
>[1] 1 3 5 6
>Levels: 1 3 5 6
>  >> a<-as.vector(as.numeric(a))
>  > a
>[1] 1 2 3 4
>

Does this give what you want?

>  a <- factor(c(1,3,5,6))
>  a
[1] 1 3 5 6
Levels: 1 3 5 6
>  as.numeric(format(a))
[1] 1 3 5 6
>  as.numeric(as.character(a))   ## an alternative
[1] 1 3 5 6
>

<--- remainder omitted --->

>Thanks in advance for your help.
>
>Sebastien
>


-- 
---------------------------------
Don MacQueen
Lawrence Livermore National Laboratory
Livermore, CA, USA
925-423-1062
macq at llnl.gov


From michaeltiemann at mac.com  Mon Jul  9 01:56:01 2007
From: michaeltiemann at mac.com (Michael Tiemann)
Date: Sun, 08 Jul 2007 19:56:01 -0400
Subject: [R] patch to enhance sound module for 96 kHz/24 bit sample sizes
Message-ID: <46917991.8090103@mac.com>

Greetings Matthias,

Thanks again for your sound module.  I did not ever manage to find the 
time to play with phase equations, but I found I needed the module for a 
new project involving bats.  I needed to do some work @ 96 kHz/24 bit 
sample size, and found the limitations of the sound package stop at 48 
kHz and 16 bit samples.  Here's a patch to bring things up to 96/24.  
Sorry I cannot test 192/24.  I am copying r-help in case others have 
more advanced equipment and an interest in testing it out.  Hope this helps!

BTW, if you are curious about the bats, you can check here: 
http://blogs.cnet.com/8301-13507_1-9738110-18.html?tag=more
I will be writing a follow-up that uses sound and seewave in the next 
few days.

[tiemann at localhost Desktop]$ diff -ru sound-orig/ sound
diff -ru sound-orig/man/bits.Rd sound/man/bits.Rd
--- sound-orig/man/bits.Rd    2006-02-20 12:50:53.000000000 -0500
+++ sound/man/bits.Rd    2007-07-08 19:36:08.000000000 -0400
@@ -12,13 +12,13 @@
 }
 \arguments{
   \item{s}{ a Sample object, or a string giving the name of a wav file. }
-  \item{value}{ the number of bits per sample, 8 or 16. }
+  \item{value}{ the number of bits per sample, 8, 16, or 24. }
 }
 \details{
 The replacement form can be used to reset the sampling quality of a 
Sample object, that is the number of bits per sample (8 or 16). Here, 
filenames are not accepted.
 }
 \value{
-  For \code{bits}, the bits parameter (number of bits per sample) of 
the Sample object, 8 or 16.
+  For \code{bits}, the bits parameter (number of bits per sample) of 
the Sample object, 8, 16, or 24.
 
   For \code{setBits}, a Sample object with the new \code{bits} parameter.
 }
Only in sound/man: bits.Rd~
diff -ru sound-orig/man/loadSample.Rd sound/man/loadSample.Rd
--- sound-orig/man/loadSample.Rd    2006-02-20 12:57:00.000000000 -0500
+++ sound/man/loadSample.Rd    2007-07-08 19:35:31.000000000 -0400
@@ -11,7 +11,8 @@
   \item{filecheck}{ logical. If FALSE, no check for existance and read 
permission of the file will be performed. }
 }
 \details{
-All kinds of wav files are supported: mono / stereo, 8 / 16 bits per 
sample, 1000 to 48000 samples/second.
+All kinds of wav files are supported: mono / stereo, 8 / 16 / 24 bits 
per sample, 1000 to 96000 samples/second,
+but no compressed formats are supported.
 }
 \value{
   the Sample object that is equivalent to the wav file.
Only in sound/man: loadSample.Rd~
diff -ru sound-orig/man/nullSample.Rd sound/man/nullSample.Rd
--- sound-orig/man/nullSample.Rd    2006-02-20 12:56:37.000000000 -0500
+++ sound/man/nullSample.Rd    2007-07-08 19:37:03.000000000 -0400
@@ -7,8 +7,8 @@
 \usage{nullSample(rate=44100, bits=16, channels=1)
 }
 \arguments{
-  \item{rate}{ the sampling rate, between 1000 and 48000. }
-  \item{bits}{ the sample quality (number of bits per sample), 8 or 16. }
+  \item{rate}{ the sampling rate, between 1000 and 96000. }
+  \item{bits}{ the sample quality (number of bits per sample), 8, 16, 
or 24. }
   \item{channels}{ 1 for mono, or 2 for stereo. }
 }
 \value{
Only in sound/man: nullSample.Rd~
diff -ru sound-orig/man/rate.Rd sound/man/rate.Rd
--- sound-orig/man/rate.Rd    2006-02-20 12:59:34.000000000 -0500
+++ sound/man/rate.Rd    2007-07-08 19:39:22.000000000 -0400
@@ -12,7 +12,7 @@
 }
 \arguments{
   \item{s}{ a Sample object, or a string giving the name of a wav file. }
-  \item{value}{ an integer between 1000 and 48000 giving the sampling 
rate. }
+  \item{value}{ an integer between 1000 and 96000 giving the sampling 
rate. }
 }
 \details{
 The replacement form can be used to reset the sampling rate. Here, 
filenames are not accepted.
@@ -26,7 +26,7 @@
 }
 \author{ Matthias Heymann }
 
-\note{ Common sampling rates are between 8000 and 44100 (CD quality). 
The sampling rate of DAT recorders is 48000. Not every rate is 
guaranteed to be supported by every wav file player.
+\note{ Common sampling rates are between 8000 and 44100 (CD quality). 
The sampling rate of DAT recorders is 48000.  DVD Audio supports rates 
up to 96000 (and perhaps 192000, though this has not been tested).  Not 
every rate is guaranteed to be supported by every wav file player.
 
 Future versions may use a different algorithm for sampling rate 
conversion to achieve a better sound quality for the returned sample.
 }
Only in sound/man: rate.Rd~
diff -ru sound-orig/man/Sample.Rd sound/man/Sample.Rd
--- sound-orig/man/Sample.Rd    2006-02-20 12:59:24.000000000 -0500
+++ sound/man/Sample.Rd    2007-07-08 19:39:52.000000000 -0400
@@ -14,7 +14,7 @@
 \arguments{
   \item{sound}{ a \code{channels(s)} x \code{sampleLength(s)} matrix or 
a vector of doubles describing the waveform(s) of the sample. }
   \item{rate}{ the sampling rate (number of samples per second). }
-  \item{bits}{ the sampling quality (the number of bits per sample), 8 
or 16. }
+  \item{bits}{ the sampling quality (the number of bits per sample), 8, 
16, or 24. }
   \item{s}{ an R object to be tested.}
   \item{argname}{ a string giving the name of the object that is 
tested. It is used for creating an error message. }
 }
Only in sound/man: Sample.Rd~
diff -ru sound-orig/man/Sine.Rd sound/man/Sine.Rd
--- sound-orig/man/Sine.Rd    2006-02-20 12:58:04.000000000 -0500
+++ sound/man/Sine.Rd    2007-07-08 19:40:16.000000000 -0400
@@ -17,8 +17,8 @@
 \arguments{
   \item{freq}{ the frequency (a double). }
   \item{dur}{ the duration in seconds (a double). }
-  \item{rate}{ the sampling rate, an integer between 1000 and 48000. }
-  \item{bits}{ the sampling quality in bits per sample, 8 or 16. }
+  \item{rate}{ the sampling rate, an integer between 1000 and 96000. }
+  \item{bits}{ the sampling quality in bits per sample, 8, 16, or 24. }
   \item{channels}{ 1 for mono, or 2 for stereo. }
   \item{reverse}{ logical. If \code{TRUE}, the waveform will be 
mirrored vertically. }
   \item{upPerc}{ a number between 0 and 100 giving the percentage of 
the waveform with value +1. }
Only in sound/man: Sine.Rd~
diff -ru sound-orig/R/sound.R sound/R/sound.R
--- sound-orig/R/sound.R    2007-04-24 08:12:47.000000000 -0400
+++ sound/R/sound.R    2007-07-08 11:13:03.000000000 -0400
@@ -71,10 +71,10 @@
 as.Sample <- function(sound,rate=44100,bits=16){
   if (mode(sound)!="numeric")
     stop("Argument 'sound' must be a numeric vectors.")
-  if (mode(rate)!="numeric" || rate<1000 || rate>48000)
-    stop("Parameter 'rate' must be an number between 1000 and 48000.")
-  if (mode(bits)!="numeric" || bits!=8 && bits!=16)
-    stop("Parameter 'bits' must be 8 or 16.")
+  if (mode(rate)!="numeric" || rate<1000 || rate>96000)
+    stop("Parameter 'rate' must be an number between 1000 and 96000.")
+  if (mode(bits)!="numeric" || bits!=8 && bits!=16 && bits!=24)
+    stop("Parameter 'bits' must be 8, 16, or 24.")
   if (is.null(dim(sound)))
     sound <- matrix(sound,nrow=1)
   if (dim(sound)[1]>2){
@@ -125,23 +125,44 @@
   if(readChar(fileR, nchars=4) != 'WAVE')
     stop("File is not WAVE format.")
 
-              readBin(fileR,"integer",n=10,size=1)
+  # "fmt " (4 bytes) + Chunk Data Size (4 bytes) + Compression Code (2 
bytes)
+              readBin(fileR,"integer",n=8,size=1)
+
+  compressionCode = readBin(fileR,"integer", size=2, endian='little')
+  if (compressionCode > 1)
+    stop ("unknown compression code.")
+
   channels <- readBin(fileR,"integer",     size=2, endian='little')
   rate     <- readBin(fileR,"integer",     size=4, endian='little')
+
+  # avg. bytes per second (4 bytes) + Block align (2 bytes)
               readBin(fileR,"integer",n= 6,size=1)
+
   bits     <- readBin(fileR,"integer",     size=2, endian='little')
-              readBin(fileR,"integer",n= 4,size=1)
+
+  # "data" (4 bytes)
+  dataMarker <- readChar(fileR, 4)
+  if (dataMarker != "data")
+    stop ("'data' marker missing.")
+
   Length   <- readBin(fileR,"integer",     size=4, endian='little')
+
+  print (Length)
+
   if (bits==8)
       data <- readBin(fileR,"integer",n=Length  ,size=1,signed=FALSE, 
endian='little')
-  else
+  else if (bits==16)
       data <- readBin(fileR,"integer",n=Length/2,size=2,signed=TRUE , 
endian='little')
+  else
+      data <- read.fwf(fileR,width=3,n=Length/3)
   close(fileR)
 
   if (bits==8)
     data   <- data/128-1
-  else
+  else if (bits==16)
     data   <- data/32768
+  else
+    data   <- data/16777216
 
   if (channels==2)
     dim(data) <- c(channels,length(data)/channels)
@@ -166,7 +187,8 @@
     else  {data <- array(sound(s),dim=c(1,2*sampleLength(s)))}
 
   if (bits(s)==8) data <- data*127+128
-  else data <- data*32767
+  else if (bits(s)==16) data <- data*32767
+  else data <- data*16777216
 
   dataLength <- length(data)*bits(s)/8
 
@@ -182,7 +204,7 @@
   writeBin(as.integer(channels(s)),fileA,size=2, 
endian='little')             # 1=mono / 2=stereo
   writeBin(as.integer(rate(s)),fileA, 
endian='little')                        # sample rate
   writeBin(as.integer(rate(s)*channels(s)*bits(s)/8),fileA, 
endian='little')  # bytes/second
-  writeBin(as.integer(channels(s)*bits(s)/8),fileA,size=2, 
endian='little')   # bytes/sample
+  writeBin(as.integer(channels(s)*bits(s)/8),fileA,size=bits(s)/8, 
endian='little')   # bytes/sample
   writeBin(as.integer(bits(s)),fileA,size=2, 
endian='little')                 # bits/sample
 
   
writeChar("data",fileA,eos=NULL)                                            
# "data"
@@ -366,8 +388,8 @@
 "bits<-" <- function(s,value){
   if (is.null(class(s)) || class(s)!="Sample")
     stop("Argument 's' must be of class 'Sample'.")
-  if (mode(value)!="numeric" || (value!=8 && value!=16))
-    stop("Number of bits must be 8 or 16.")
+  if (mode(value)!="numeric" || (value!=8 && value!=16 && value!=24))
+    stop("Number of bits must be 8, 16, or 24.")
   else s$bits <- value
   return(s)
 }
@@ -375,8 +397,8 @@
 "rate<-" <- function(s,value){
   if (is.null(class(s)) || class(s)!="Sample")
     stop("Argument 's' must be of class 'Sample'.")
-  if (mode(value)!="numeric" || value<1000 || value>48000)
-    stop("Rate must be an number between 1000 and 48000.")
+  if (mode(value)!="numeric" || value<1000 || value>96000)
+    stop("Rate must be an number between 1000 and 96000.")
   if (rate(s)==value) return(s)
   ch <- channels(s)
   sound(s) <- 
sound(s)[,as.integer(seq(1,sampleLength(s)+.9999,by=rate(s)/value))]
@@ -433,8 +455,8 @@
 setBits <- function(s,value){
   sampletest <- is.Sample(s)
   if (!sampletest$test) stop(sampletest$error)
-  if (mode(value)!="numeric" || (value!=8 && value!=16))
-    stop("Number of bits must be 8 or 16.")
+  if (mode(value)!="numeric" || (value!=8 && value!=16 && value!=24))
+    stop("Number of bits must be 8, 16, or 24.")
   if (is.null(class(s))) s <- loadSample(s,filecheck=FALSE)
   bits(s) <- value
   return(s)
@@ -443,8 +465,8 @@
 setRate <- function(s,value){
   sampletest <- is.Sample(s)
   if (!sampletest$test) stop(sampletest$error)
-  if (mode(value)!="numeric" || value<1000 || value>48000)
-    stop("Rate must be a number between 1000 and 48000.")
+  if (mode(value)!="numeric" || value<1000 || value>96000)
+    stop("Rate must be a number between 1000 and 96000.")
   if (is.null(class(s))) s <- loadSample(s,filecheck=FALSE)
   rate(s) <- value
   return(s)
Only in sound/R: sound.R~
[tiemann at localhost Desktop]$

I did this for a personal project I'm doing for fun.  Let me know 
whether you need a more formal copyright disclaimer than "I hereby offer 
this patch to be included in any software licensed under the GNU General 
Public Lincese (version 2 or later)".

Michael Tiemann


From yihsuc at gmail.com  Mon Jul  9 02:26:54 2007
From: yihsuc at gmail.com (YIHSU CHEN)
Date: Sun, 8 Jul 2007 17:26:54 -0700
Subject: [R] ca.jo
Message-ID: <31beaa9a0707081726t2ccca012t12b7128f837fa2ca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/552763db/attachment.pl 

From deepayan.sarkar at gmail.com  Mon Jul  9 02:38:00 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Sun, 8 Jul 2007 17:38:00 -0700
Subject: [R] Several quick questions
In-Reply-To: <468F8AF3.6010200@free.fr>
References: <468F8AF3.6010200@free.fr>
Message-ID: <eb555e660707081738o791ebcbbw53c393bd76600ac7@mail.gmail.com>

On 7/7/07, S?bastien <pomchip at free.fr> wrote:
> Dear R users,
>
> Here is a couple a quick questions, for which I was unable to not find
> any answer in the list archives and in the help:

[...]

> 2- When a log scale is called in a graph, the label takes a format like
> 10^n.

That's true for lattice, but not traditional graphics, as far as I know.

> Is there a way to come back to a regular number format like 1, 10,
> 100... without having to create a custom axis ?

Depends on what you mean by "custom axis". You don't need to manually
choose the tick positions etc, but you still need to define the rules
that determine how they are calculated. See example(axis.default) for
an example where the tick positions remain the same (as the defaults),
but the labels change.  The slightly different rule used in
traditional graphics is available through the axTicks() function,
which basically boils down to this:

logTicks <- function (lim, loc = c(1, 5))
{
    ii <- floor(log10(range(lim))) + c(-1, 2)
    main <- 10^(ii[1]:ii[2])
    r <- as.numeric(outer(loc, main, "*"))
    r[lim[1] <= r & r <= lim[2]]
}

where 'lim' is the limits in the original scale. So we have

> logTicks(c(1, 100))
[1]   1   5  10  50 100
> logTicks(c(1, 100), loc = c(2, 5, 10))
[1]   1   2   5  10  20  50 100

> 3- In lattice graphics, how does the default value of the "axs" argument
> influence the values of "limits" ?
> This question should be considered in the following context. The help
> states that a 4% extension is applied by default to the axis range in
> base graphics. So, I have tried to apply this 4 % extension to create
> some custom lattice graphics. I worked on a dataset in which the
> independent variable ranged from 0  to 120, so I basically customized my
> axis using limits=c(-4.8,124.8). The results of the graphics with and
> without the limits command were not identical...

The extension is user-settable in lattice, and defaults to 7% (I think
this value came from Trellis specs, but I don't remember the exact
details).

> lattice.getOption("axis.padding")
$numeric
[1] 0.07

$factor
[1] 0.6

-Deepayan


From aaront at uniserve.com  Mon Jul  9 03:02:27 2007
From: aaront at uniserve.com (aaront)
Date: Sun, 8 Jul 2007 18:02:27 -0700 (PDT)
Subject: [R] transform excel data into graph
In-Reply-To: <11493073.post@talk.nabble.com>
References: <11493073.post@talk.nabble.com>
Message-ID: <11494545.post@talk.nabble.com>


There are numerous ways of importing data from excel. One is to save as a
.csv and use the read.csv function. Or, you can copy to the clipboard and
use the read.delim("clipboard",header=T) function.

Are you looking at a bar graph where the lessons have the names nested below
them on the x axis, and the numbers on the y?

As these are all introductory elements of using R, going through the
numerous intro R manuals available online is your best bet. Try:
http://cran.r-project.org/manuals.html
 
and also under "Contributed Documentation" at the above site.


cross123 wrote:
> 
> Hello everyone,
> I have a set of data in the following form, which are stored in an Excel
> file:
>                  nick       john       peter                 
> lesson1       0.465     0.498     0.473    
> lesson2       0.422      0.44      0.134           
> lesson3       0.45       0.35       0.543                      
> lesson4       0.590      0.64      0.11                      
> lesson5       0.543      0.5        0.32                      
> 
> What I want to do is a 2d-graph plot where I will have  the name of the
> student in the X-axis and the name of the lesson in the Y-axis and the
> number from each pair will be used to construct the plot.
> I am newbie with R and I don't know which package shall I use nor the
> commands with which I will import my data in R so that the plot will be
> created...
> 
> Any help would be greatly appreciated.
> 
> 

-- 
View this message in context: http://www.nabble.com/transform-excel-data-into-graph-tf4046056.html#a11494545
Sent from the R help mailing list archive at Nabble.com.


From mvinic at gmail.com  Mon Jul  9 04:20:47 2007
From: mvinic at gmail.com (Marcus Vinicius)
Date: Sun, 8 Jul 2007 23:20:47 -0300
Subject: [R] EM algorithm for Missing Data.
Message-ID: <c0792190707081920p5698a162ya664b40fd8d7e48c@mail.gmail.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070708/6a87c824/attachment.pl 

From s.blomberg1 at uq.edu.au  Mon Jul  9 04:40:11 2007
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Mon, 09 Jul 2007 12:40:11 +1000
Subject: [R] EM algorithm for Missing Data.
In-Reply-To: <c0792190707081920p5698a162ya664b40fd8d7e48c@mail.gmail.com>
References: <c0792190707081920p5698a162ya664b40fd8d7e48c@mail.gmail.com>
Message-ID: <1183948811.6531.43.camel@sib-sblomber01d.sib.uq.edu.au>

Sure! Read this:

 MAXIMUM LIKELIHOOD FROM INCOMPLETE DATA VIA EM ALGORITHM
Author(s): DEMPSTER AP, LAIRD NM, RUBIN DB
Source: JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-METHODOLOGICAL
39 (1): 1-38 1977

then read the posting guide.

Simon.

On Sun, 2007-07-08 at 23:20 -0300, Marcus Vinicius wrote:
> Dear all,
> I need to use the EM algorithm where data are missing.
> Example:
> x<- c(60.87, NA, 61.53, 72.20, 68.96, NA, 68.35, 68.11, NA, 71.38)
> 
> May anyone help me?
> 
> Thanks.
> 
> Marcus Vinicius
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia

Room 320, Goddard Building (8)
T: +61 7 3365 2506 
email: S.Blomberg1_at_uq.edu.au 

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.


From krcabrer at une.net.co  Mon Jul  9 05:15:39 2007
From: krcabrer at une.net.co (Kenneth Cabrera)
Date: Sun, 08 Jul 2007 22:15:39 -0500
Subject: [R] Help in installing rggobi in ubuntu linux
Message-ID: <op.tu6godk9lgnhok@davinci.une.net.co>

Hi R users.

I am experimenting with ubuntu 7.04 Feisty.

I install the ggobi package with apt-get.

I got almost all the packages, but
when I try to obtain rggobi, I got
this message:

-------------------------------------------------------------------------------------------------
install.packages("rggobi")
Aviso en install.packages("rggobi") : argument 'lib' is missing: using  
'/usr/local/lib/R/site-library'
--- Please select a CRAN mirror for use in this session ---
Loading Tcl/Tk interface ... done
probando la URL  
'http://cran.at.r-project.org/src/contrib/rggobi_2.1.4-4.tar.gz'
Content type 'application/x-gzip' length 401451 bytes
URL abierta
==================================================
downloaded 392Kb

* Installing *source* package 'rggobi' ...
checking for pkg-config... /usr/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
checking for GGOBI... configure: creating ./config.status
config.status: creating src/Makevars
** libs
gcc -std=gnu99 -I/usr/share/R/include -I/usr/share/R/include -g  
-DUSE_EXT_PTR=1 -D_R_=1      -fpic  -g -O2 -c brush.c -o brush.o
En el fichero inclu??do de brush.c:1:
RSGGobi.h:5:22: error: GGobiAPI.h: No existe el fichero ?? directorio
In file included from RSGGobi.h:6,
                  from brush.c:1:
conversion.h:174: error: expected ???=???, ???,???, ???;???, ???asm??? or  
???__attribute__??? before ???asCLogical???
conversion.h:176: error: expected ???=???, ???,???, ???;???, ???asm??? or  
???__attribute__??? before ???asCRaw???

--- snip ---

brush.c:124: error: ???t??? no se declar?? aqu?? (primer uso en esta  
funci??n)
brush.c:124: error: ???s??? no se declar?? aqu?? (primer uso en esta  
funci??n)
brush.c:124: error: el objeto ???GGOBI(<erroneous-expression>)??? llamado  
no es una funci??n
brush.c: En el nivel principal:
brush.c:135: error: expected ???)??? before ???cid???
make: *** [brush.o] Error 1
chmod: no se puede acceder a  
`/usr/local/lib/R/site-library/rggobi/libs/*': No existe el fichero ??  
directorio
ERROR: compilation failed for package 'rggobi'
** Removing '/usr/local/lib/R/site-library/rggobi'

The downloaded packages are in
         /tmp/RtmpVCacJd/downloaded_packages
Warning message:
installation of package 'rggobi' had non-zero exit status in:  
install.packages("rggobi")
-------------------------------------------------------------------------------------------------------------------

What am I doing wrong?

Thank you for your help.
-- 
Kenneth Roy Cabrera Torres
Cel 315 504 9339


From edna.bell01 at gmail.com  Mon Jul  9 05:55:00 2007
From: edna.bell01 at gmail.com (Edna Bell)
Date: Sun, 8 Jul 2007 22:55:00 -0500
Subject: [R]  adding latex, html docs. to new packag
Message-ID: <2d1ebb110707082055x38e9f9b3q4e6dbb25d4216c76@mail.gmail.com>

Hi again!

How do I create the Latex and HTML files for documentation for a new
package, please?

Is there something in the R CMD stuff that would do it, or do I need
to produce by hand, pleaes?

thanks,
eb


From epistat at gmail.com  Mon Jul  9 09:03:18 2007
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 9 Jul 2007 15:03:18 +0800
Subject: [R] help on fisher.test(stats)?
Message-ID: <2fc17e30707090003vc0aece6v3d2e349684d9b9cc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/9c53d4c3/attachment.ksh 

From debmidya at yahoo.com  Sat Jul  7 15:47:17 2007
From: debmidya at yahoo.com (Deb Midya)
Date: Sat, 7 Jul 2007 06:47:17 -0700 (PDT)
Subject: [R] Calling R function in C
Message-ID: <430878.2306.qm@web50404.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070707/5da8ca9f/attachment.ksh 

From fredrik.bg.lundgren at bredband.net  Sun Jul  8 22:56:52 2007
From: fredrik.bg.lundgren at bredband.net (Fredrik Lundgren)
Date: Sun, 8 Jul 2007 22:56:52 +0200
Subject: [R]  boot.ci
Message-ID: <000501c7c1a2$83a3c7c0$57eae455@Larissa>

Dear boot.ers,

I ran a small program for training purposes and ended with problems in 
boot.ci

bush <- c(rep(1, 840), rep(0, 660))
> f.mean <- function(y, id) {mean(y[id])}
> bushB <- boot(bush, f.mean, 1000)
> boot.ci(bushB, conf = 0.95, type = c('perc', 'bca'))
Error in bca.ci(boot.out, conf, index[1], L = L, t = t.o, t0 = t0.o, h = 
h,  :
 estimated adjustment 'a' is NA
>

What's wrong in my setup? boot appears to work OK

Sincerly Fredrik Lundgren

PS. By the way - anyone who has "Resampling stats, standalone" for sale? 
DS


From ctsirigos at yahoo.com  Sun Jul  8 23:38:16 2007
From: ctsirigos at yahoo.com (cross123)
Date: Sun, 8 Jul 2007 14:38:16 -0700 (PDT)
Subject: [R] transform excel data into graph
Message-ID: <11493073.post@talk.nabble.com>


Hello everyone,
I have a set of data in the following form, which are stored in an Excel
file:
                 nick       john       peter                 
lesson1       0.465     0.498     0.473    
lesson2       0.422      0.44      0.134           
lesson3       0.45       0.35       0.543                      
lesson4       0.590      0.64      0.11                      
lesson5       0.543      0.5        0.32                      

What I want to do is a 2d-graph plot where I will have  the name of the
student in the X-axis and the name of the lesson in the Y-axis and the
number from each pair will be used to construct the plot.
I am newbie with R and I don't know which package shall I use nor the
commands with which I will import my data in R so that the plot will be
created...

Any help would be greatly appreciated.

-- 
View this message in context: http://www.nabble.com/transform-excel-data-into-graph-tf4046056.html#a11493073
Sent from the R help mailing list archive at Nabble.com.


From davranche at tourduvalat.org  Sun Jul  8 13:22:08 2007
From: davranche at tourduvalat.org (=?ISO-8859-1?Q?Aur=E9lie_Davranche?=)
Date: Sun, 08 Jul 2007 13:22:08 +0200
Subject: [R] rpart weight prior
Message-ID: <4690C8E0.2000005@tourduvalat.org>

Hi!

Could you please explain the difference between "prior" and "weight" in 
rpart? It seems to be the same. But in this case why including a weight 
option in the latest versions? For an unbalanced sampling what is the 
best to use : weight, prior or the both together?

Thanks a lot.

Aur?lie Davranche.

From singularitaet at gmx.net  Mon Jul  9 09:53:41 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Mon, 09 Jul 2007 09:53:41 +0200
Subject: [R] help on fisher.test(stats)?
In-Reply-To: <2fc17e30707090003vc0aece6v3d2e349684d9b9cc@mail.gmail.com>
References: <2fc17e30707090003vc0aece6v3d2e349684d9b9cc@mail.gmail.com>
Message-ID: <4691E985.6030307@gmx.net>

-------- Original Message  --------
Subject: [R] help on fisher.test(stats)?
From: zhijie zhang <epistat at gmail.com>
To: R-help at stat.math.ethz.ch
Date: 09.07.2007 09:03
> Dear friends,
>   My dataset have many zeros, so i must use fisher exact test .
> Unfortunately, the fisher.test(stats) function fail to do it.
>   Anybody knows how to do the fisher exact test with many zeros in the
> dataset?
> My dataset is:
> a<-matrix(c(0,1,0,0,0,0,1,0,1,0,0,0,0,1,0,1,1,0,2,1,5,1,1,6,4,4,1,17,2,8,5,7,1,1,24,3,6,1,1,3,2,16,7,4,0,2,4,0,17,0,1,0,0,0,1,2),nrow=8,byrow=TRUE)
> data.frame(a)
> b<-a[,-7]
> as.matrix(b)
> c<-as.matrix(b)
>
>   
>> c
>>     
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    0    1    0    0    0    0
> [2,]    0    1    0    0    0    0
> [3,]    0    1    1    0    2    1
> [4,]    1    1    6    4    4    1
> [5,]    2    8    5    7    1    1
> [6,]    3    6    1    1    3    2
> [7,]    7    4    0    2    4    0
> [8,]    0    1    0    0    0    1
>   
>> fisher.test(c,workspace=200000000000000000)
>>     
> ??????fisher.test(c, workspace = 2e+17) :
>         ????????????????????NA(arg10)
> ????: Warning message:
> ????????????????????NA
>
> Any suggestion or help are greatly appreciated.
>   
Your workspace is by far to large. I have done it with
> fisher.test(c,workspace=40000000)

        Fisher's Exact Test for Count Data

data:  c
p-value = 0.01548
alternative hypothesis: two.sided

(btw. it took half an hour...)

Simulation would also be an alternative approach:

> fisher.test(c,simulate=T)

        Fisher's Exact Test for Count Data with simulated p-value (based
on 2000 replicates)

data:  c
p-value = 0.01349
alternative hypothesis: two.sided

As you see the p-value is not that different, you could use more
replications:

> fisher.test(c,simulate=T,B=1000000)

        Fisher's Exact Test for Count Data with simulated p-value (based
on 1e+06 replicates)

data:  c
p-value = 0.01514
alternative hypothesis: two.sided

and it is still much faster...

Stefan
-=-=-
... The most incomprehensible thing about the world is that it is
comprehensible. (A. Einstein)


From elyakhlifi_mustapha at yahoo.fr  Mon Jul  9 10:03:41 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Mon, 9 Jul 2007 08:03:41 +0000 (GMT)
Subject: [R] display data.frame with lines and colimns
Message-ID: <823968.93449.qm@web27511.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/df94452f/attachment.pl 

From gavin.simpson at ucl.ac.uk  Mon Jul  9 10:05:45 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 09 Jul 2007 09:05:45 +0100
Subject: [R] random sampling with some limitive conditions?
In-Reply-To: <3f2938d50707081012i2fff4919u218be88294bdcfc9@mail.gmail.com>
References: <Pine.GSO.4.60.0707081237350.13671@hcp>
	<3f2938d50707081012i2fff4919u218be88294bdcfc9@mail.gmail.com>
Message-ID: <1183968345.3080.21.camel@dhcppc2.my.nat.localnet>

Dear Jian,

This came up on R-help recently; I haven't got time to find the thread
for you, but it shouldn't take too much hunting out.

I suggested the following approach, based on the swap method of Roberts
and Stone (1990) Island-sharing by archipelago species. Oecologia 85:
560-567. The code for this is appended at the end of this message. Below
is an example:

> dat <- matrix(scan(), ncol = 8, byrow = TRUE)
1: 1 0 0 0 1 1 0 0
9: 0 1 1 1 0 1 0 1
17: 1 0 0 0 1 0 1 0
25: 0 0 0 1 0 1 0 1
33: 1 0 1 0 0 0 0 0
41: 0 1 0 1 1 1 1 1
49: 1 0 0 0 0 0 0 0
57: 0 0 0 1 0 1 0 1
65:
Read 64 items
> dat ## your data
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    0    0    0    1    1    0    0
[2,]    0    1    1    1    0    1    0    1
[3,]    1    0    0    0    1    0    1    0
[4,]    0    0    0    1    0    1    0    1
[5,]    1    0    1    0    0    0    0    0
[6,]    0    1    0    1    1    1    1    1
[7,]    1    0    0    0    0    0    0    0
[8,]    0    0    0    1    0    1    0    1

rBinMat is the function appended below. We search for elements of dat
that that are diagonal:

10  or 01
01     10

We do this by randomly selecting two rows and columns to find a 2x2
sub-matrix of dat. We check if it is diagonal, and if it is, we swap the
1's and 0's. This retains the row and column sums. If the sub-matrix is
non-diagonal, we simply leave it as it is. This process is repeated many
times in a "burn in" period, after which, random binary matrices can be
produced.

In the line below we use a burn in of 30000 swaps - which takes about 9
seconds to prime the matrix. (in fact this matrix is achieved after
making a further 1000 swaps as per argument skip = 1000)

> system.time(ran.mat1 <- rBinMat(dat, burn.in = 30000))
   user  system elapsed
  9.556   0.104   9.683
> ran.mat1
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    0    1    0    0    1    0    0    1
[2,]    0    1    1    0    1    1    1    0
[3,]    1    0    1    1    0    0    0    0
[4,]    0    0    0    1    0    1    0    1
[5,]    1    0    0    1    0    0    0    0
[6,]    1    0    0    1    1    1    1    1
[7,]    0    0    0    0    0    1    0    0
[8,]    1    0    0    0    0    1    0    1
> rowSums(ran.mat1)
[1] 3 5 3 3 2 6 1 3
> colSums(ran.mat1)
[1] 4 2 2 4 3 5 2 4

Having primed our random matrix, we can use this matrix and produce a
new random matrix from it by making a smaller number of swaps (default
1000), by setting the burn in to be 0.

> ran.mat2 <- rBinMat(ran.mat1, burn.in = 0)
> ran.mat2
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    0    0    0    1    1    0    0
[2,]    1    0    1    1    0    1    0    1
[3,]    1    0    1    0    0    0    1    0
[4,]    0    0    0    1    1    0    0    1
[5,]    0    1    0    0    0    0    0    1
[6,]    0    1    0    1    1    1    1    1
[7,]    0    0    0    0    0    1    0    0
[8,]    1    0    0    1    0    1    0    0
> rowSums(ran.mat2)
[1] 3 5 3 3 2 6 1 3
> colSums(ran.mat2)
[1] 4 2 2 4 3 5 2 4
>

As you can see, the row and column sums are as per your original matrix.

There are other methods to achieve these random binary matrices - some
of which were mentioned in the recent thread.

HTH

G

Ps: I cooked this up over lunch in response to the earlier thread. It
could no doubt be improved upon.

#######################################################################
##                                                                   ##
## rBinMat()                                                         ##
##                                                                   ##
## Function:                                                         ##
## =========                                                         ##
## Generates random binary matrices that preserve row and column     ##
## sums using the swap method of Roberts and Stone (1990) Island-    ##
## sharing by archipelago species. Oecologia 85: 560-567.            ##
##                                                                   ##
## Arguments:                                                        ##
## ==========                                                        ##
## x       : Species presence/absence matrix (binary, 1/0s)          ##
## burn.in : The "burn in" period required --- how many swaps to     ##
##           make to 'x' before a random matrix can be generated?    ##
## skip    : The number of swaps to skip before drawing a matrix.    ##
##           Once a matrix is burned in, it can be used in sub-      ##
##           sequent calls to rBinMat() [using "burn.in = 0"] for    ##
##           generating further random matrices. In such cases, one  ##
##           might wish to skip n swaps before drawing the next      ##
##           matrix.                                                 ##
##                                                                   ##
## Author  : Gavin L. Simpson                                        ##
## Date    : 27 April 2007                                           ##
## Licence : GPL Version 2                                           ##
##                                                                   ##
## Copyright (c) Gavin L. Simpson, 2007                              ##
##                                                                   ##
## History : 27-April-2007 - Created                                 ##
##                                                                   ##
#######################################################################
rBinMat <- function(x, burn.in = 10000, skip = 1000) {
  dim.nams <- dimnames(x)
  x <- as.matrix(x)
  dimnames(x) <- NULL
  ## number rows/cols
  n.col <- ncol(x)
  n.row <- nrow(x)
  ## is the 2x2 matrix diagonal or anti-diagonal
  isDiag <- function(x) {
    X <- as.vector(x)
    if(.Internal(identical(X, c(1,0,0,1))))
      return(TRUE)
    else if(.Internal(identical(X, c(0,1,1,0))))
      return(TRUE)
    else
      return(FALSE)
  }
  changed <- 0
  ## do the burn in changes, then skip, then a final swap,
  ## this is the first random matrix
  while(changed <= (burn.in + skip + 1)) {
    ran.row <- .Internal(sample(n.row, 2, replace = FALSE, prob = NULL))
    ran.col <- .Internal(sample(n.col, 2, replace = FALSE, prob = NULL))
    if(isDiag(x[ran.row, ran.col])) {
      x[ran.row, ran.col] <- c(1,0)[x[ran.row, ran.col] + 1]
      changed <- changed + 1}
  }
  dimnames(x) <- dim.nams
  return(x)
}


On Sun, 2007-07-08 at 11:12 -0600, Zhang Jian wrote:
> It is not right. My data is the presence-absence data. And I want to get
> thousands of presence-absence random data which length of rows and columns
> is the same with the former data. Meantime, the new data needs to have the
> fixed sums for each row and column with the former data.
> For example:
> The data "sites":
> site1 site2 site3 site4 site5 site6 site7 site8
> 1 0 0 0 1 1 0 0
> 0 1 1 1 0 1 0 1
> 1 0 0 0 1 0 1 0
> 0 0 0 1 0 1 0 1
> 1 0 1 0 0 0 0 0
> 0 1 0 1 1 1 1 1
> 1 0 0 0 0 0 0 0
> 0 0 0 1 0 1 0 1
> > apply(sites,2,sum)
> site1 site2 site3 site4 site5 site6 site7 site8
>     4     2     2     4     3     5     2     4
> > apply(sites,1,sum)
> [1] 3 5 3 3 2 6 1 3
> 
> If I get the new data "sites.random":
> site1 site2 site3 site4 site5 site6 site7 site8
> 1 0 0 0 1 1 0 0
> 1 1 1 1 0 1 0 0
> 1 0 0 0 1 0 1 0
> 0 0 0 1 0 1 0 1
> 0 0 1 0 0 0 0 1
> 0 1 0 1 1 1 1 1
> 1 0 0 0 0 0 0 0
> 0 0 0 1 0 1 0 1
> > apply(sites.random,2,sum) # the same with the former data
> site1 site2 site3 site4 site5 site6 site7 site8
>     4     2     2     4     3     5     2     4
> > apply(sites.random,1,sum) # the same with the former data
> [1] 3 5 3 3 2 6 1 3
> 
> How can I get the new random data? Thanks.
> 
> 
> 
> On 7/8/07, Alan Zaslavsky <zaslavsk at hcp.med.harvard.edu> wrote:
> >
> > If I understand your problem, this might be a solution.  Assign
> > independent random numbers for row and column and use the corresponding
> > ordering to assign the row and column indices.  Thus row and column
> > assignments are independent and the row and column totals are fixed.  If
> > cc and rr are respectively the desired row and column totals, with
> > sum(cc)==sum(rr), then
> >
> > n = sum(cc)
> > row.assign = rep(1:length(rr),rr)[order(runif(n))]
> > col.assign = rep(1:length(cc),cc)[order(runif(n))]
> >
> > If you want many such sets of random assignments to be generated at once
> > you can use a few more rep() calls in the expressions to generate multiple
> > sets in the same way.  (Do you actually want the assignments or just the
> > tables?) Of course there are many other possible solutions since you have
> > not fully specified the distribution you want.
> >
> >        Alan Zaslavsky
> >        Harvard U
> >
> > > From: "Zhang Jian" <jzhang1982 at gmail.com>
> > > Subject: [R] random sampling with some limitive conditions?
> > > To: r-help <r-help at stat.math.ethz.ch>
> > >
> > > I want to gain thousands of random sampling data by randomizing the
> > > presence-absence data. Meantime, one important limition is that the row
> > and
> > > column sums must be fixed. For example, the data "tst" is following:
> > >    site1 site2 site3 site4 site5 site6 site7 site8 1 0 0 0 1 1 0 0 0 1 1
> > 1 0
> > > 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 1
> > 0 0
> > > 0 0 0 0 0 0 0 0 1 0 1 0 1
> > >
> > > sum(tst[1,]) = 3, sum(tst[,1])=4, and so on. When I randomize the data,
> > the
> > > first row sums must equal to 3, and the first column sums must equal to
> > 4.
> > > The rules need to be applied to each row and column.
> > > How to get the new random sampling data? I have no idea.
> > > Thanks.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [t] +44 (0)20 7679 0522
ECRC                              [f] +44 (0)20 7679 0565
UCL Department of Geography
Pearson Building                  [e] gavin.simpsonATNOSPAMucl.ac.uk
Gower Street
London, UK                        [w] http://www.ucl.ac.uk/~ucfagls/
WC1E 6BT                          [w] http://www.freshwaters.org.uk/
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ted.harding at nessie.mcc.ac.uk  Mon Jul  9 10:19:00 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 09 Jul 2007 09:19:00 +0100 (BST)
Subject: [R] EM algorithm for Missing Data.
In-Reply-To: <c0792190707081920p5698a162ya664b40fd8d7e48c@mail.gmail.com>
Message-ID: <XFMail.070709091900.ted.harding@nessie.mcc.ac.uk>

On 09-Jul-07 02:20:47, Marcus Vinicius wrote:
>  Dear all,
> I need to use the EM algorithm where data are missing.
> Example:
> x<- c(60.87, NA, 61.53, 72.20, 68.96, NA, 68.35, 68.11, NA, 71.38)
> 
> May anyone help me?
> 
> Thanks.
> 
> Marcus Vinicius

The Dempster, Laird & Rubin reference given by Simon Blomberg
is the classical account of the EM Algorithm for incomplete
information, though there has been a lot more published since.

However, more to the point in the present case: If the above
is typical of your data, you had better state what you want to
do with the data.

Do you want to fit a distribution by estimating parameters?
Are they observations of a "response" variable with covariates
and you want to fit a linear model estimating the coefficients?
Are they data from a time-series and you need to interpolate
at the missing values?
Etc.??

Depending on what you want to do, the way you apply the general
EM Algorithm procedure may be very different; and a lot of
applications are not covered by Dempster, Laird & Rubin (1977).

And there may possibly be no point anyway: If all you want to do
is estimate the mean of the distribution of the data, then the
best procedure may simply be to ignore the missing data.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 09-Jul-07                                       Time: 09:18:56
------------------------------ XFMail ------------------------------


From ligges at statistik.uni-dortmund.de  Mon Jul  9 11:09:12 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Jul 2007 11:09:12 +0200
Subject: [R] adding latex, html docs. to new packag
In-Reply-To: <2d1ebb110707082055x38e9f9b3q4e6dbb25d4216c76@mail.gmail.com>
References: <2d1ebb110707082055x38e9f9b3q4e6dbb25d4216c76@mail.gmail.com>
Message-ID: <4691FB38.4080800@statistik.uni-dortmund.de>



Edna Bell wrote:
> Hi again!
> 
> How do I create the Latex and HTML files for documentation for a new
> package, please?
> 
> Is there something in the R CMD stuff that would do it, or do I need
> to produce by hand, pleaes?


As the manual "Writing R Extensions" suggests, please write your 
documentation in the Rd format. Those files are converted automatically 
to latex and html (among other format) when the package is installed.

Skeletons for the help files can be generated using prompt() or for a 
whole package, see ?package.skeleton.

Uwe Ligges







> thanks,
> eb
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Mon Jul  9 11:12:28 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 9 Jul 2007 11:12:28 +0200
Subject: [R] [R-pkgs] ggplot 0.5.4
Message-ID: <f8e6ff050707090212r7925d60ck2ece466d98500731@mail.gmail.com>

ggplot2
===================================

ggplot2 is a plotting system for R, based on the grammar of graphics,
which tries to take the good parts of base and lattice graphics and
avoid bad parts. It takes care of many of the fiddly details
that make plotting a hassle (like drawing legends) as well as
providing a powerful model of graphics that makes it easy to produce
complex multi-layered graphics.

Find out more at http://had.co.nz/ggplot2, and check out the over 500
examples of ggplot in use.

Changes in version 0.5.4 ------------------------------

* border now drawn on top of geoms, instead of below - this results in
better appearance when adjusting scale limits
* ggplot() + aes() now modifies existing default aesthetic mapping,
rather than overwriting
* polished examples in facet_grid

Changes in version 0.5.3 ------------------------------

* added experimental scatterplot matrix, see ?plotmatrix
* added new border.colour and grid.minor.colour options for better
control over plot appearance
* updated theme_bw to do better when drawing a plot with white background
* better default colour choices for gradients (and more discussion in examples)
* fixed bug in ScaleGradient2 where scales with different positive and
negative ranges were not scaled correctly
* allow expressions as result from strip.text
* fixed rare bug in geom_vline and geom_hline
* fixed example in geom_abline
* tweaked display of multiline axis labels

Regards,

Hadley

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From epistat at gmail.com  Mon Jul  9 11:42:40 2007
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 9 Jul 2007 17:42:40 +0800
Subject: [R] a little problem on selecting a subset from dataset A according
	to dataset B?
Message-ID: <2fc17e30707090242r2db112ddm5656a09e96c4ef2f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/c92682d6/attachment.pl 

From OrderInfo at HomeVisions.com  Mon Jul  9 12:17:43 2007
From: OrderInfo at HomeVisions.com (OrderInfo at HomeVisions.com)
Date: Mon, 9 Jul 2007 05:17:43 -0500 (CDT)
Subject: [R] Message Acknowledgement
Message-ID: <16102116.801183976263685.JavaMail.tomcat@127.0.0.1>

 
This is just a quick confirmation to let you know that we received your  
message below at OrderInfo at HomeVisions.com 
 
Your input is important to us and we will get back to you as soon as we can. 
If you have any other questions please feel free to send us email or call 
our customer service department at 1-800-651-1415. Email customer service is 
availible 8:00 AM - 5:00 PM Central Time, Monday through Friday. 
 
Sincerely, 
 
Customer Service
HomeVisions
www.HomeVisions.com 
 
Your Message: 
-------------- 
CI:00864890

Your message was not delivered due to the following reason(s):

Your message was not delivered because the destination computer was
unreachable within the allowed queue period. The amount of time
a message is queued before it is returned depends on local configura-
tion parameters.

Most likely there is a network problem that prevented delivery, but
it is also possible that the computer is turned off, or does not
have a mail system running right now.

Your message was not delivered within 1 days:
Mail server 178.23.220.84 is not responding.

The following recipients did not receive this message:
<customerservice at homevisions.com>

Please reply to postmaster at stat.math.ethz.ch
if you feel this message to be in error.





File attachment: homevisions.com
A file attached to this email was removed
because it was infected with a virus.

Virus Name:  W32.Mydoom.M at mm
Virus ID:    34256

 


From ccleland at optonline.net  Mon Jul  9 12:20:33 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 09 Jul 2007 06:20:33 -0400
Subject: [R] a little problem on selecting a subset from dataset A
 according to dataset B?
In-Reply-To: <2fc17e30707090242r2db112ddm5656a09e96c4ef2f@mail.gmail.com>
References: <2fc17e30707090242r2db112ddm5656a09e96c4ef2f@mail.gmail.com>
Message-ID: <46920BF1.5080203@optonline.net>

zhijie zhang wrote:
> Dear Friends,
>    I want to extract the records from A according to B, but the results are
> not correct because R says :
>   The length of long object is not integer times on the length of short
> object.
>   Anybody have met the same problem? How to do it correctly?
> 
> length(A)=47
> length(B)=6
> 
> A[A$coords.x1==B$X1,]   #the program for the above task. I should get 6
> records, but i only get former 4 records for the above reason.
> 
> Thanks.

A[A$coords.x1 %in% B$X1,]
   coords.x1 coords.x2
1   542250.9   3392404
8   541512.5   3394722
9   541479.3   3394878
10  538903.4   3395943
19  543274.0   3389919
20  543840.8   3392012

?is.element

>  The folloing shows dataset A and B.
> 
> 
>> A
>    coords.x1 coords.x2
> 0  542250.89 3392404.1
> 1  538813.87 3388339.0
> 2  536049.19 3385821.6
> 3  533659.62 3383194.2
> 4  530642.30 3376834.9
> 5  529573.15 3378177.8
> 6  530853.82 3394838.8
> 7  541512.51 3394721.6
> 8  541479.33 3394877.8
> 9  538903.39 3395942.5
> 10 536019.95 3396286.1
> 11 538675.23 3384213.2
> 12 535127.95 3381255.4
> 13 533852.24 3378660.4
> 14 531360.91 3379273.8
> 15 539289.14 3375759.8
> 16 543410.51 3384353.1
> 17 543089.27 3388170.1
> 18 543274.03 3389919.2
> 19 543840.77 3392012.4
> 20 553383.55 3402401.8
> 21 554621.51 3397938.9
> 22 564096.42 3397524.4
> 23 567529.64 3398702.9
> 24 561798.76 3404864.0
> 25 562868.34 3405502.2
> 26 563145.22 3403192.1
> 27 562419.87 3404090.4
> 28 558321.85 3403879.9
> 29 567050.74 3404973.1
> 30 570609.70 3408742.4
> 31 556777.57 3397858.0
> 32 531353.38 3368596.6
> 33 533513.50 3372749.3
> 34 537543.19 3364284.8
> 35 538779.41 3368224.8
> 36 525930.09 3374067.7
> 37 522990.85 3369213.1
> 38 528826.37 3359019.0
> 39 533865.85 3362595.4
> 40 531200.25 3365053.0
> 41 551054.10 3377181.3
> 42 546974.19 3369284.8
> 43 572315.59 3359541.1
> 44 562703.63 3355173.4
> 45 558959.31 3357804.4
> 46 558531.39 3361741.1
> 
> 
>> B
>          X1        X2
> 1 542250.89 3392404.1
> 2 541512.51 3394721.6
> 3 541479.33 3394877.8
> 4 538903.39 3395942.5
> 5 543274.03 3389919.2
> 6 543840.77 3392012.4

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ripley at stats.ox.ac.uk  Mon Jul  9 13:27:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jul 2007 12:27:07 +0100 (BST)
Subject: [R] rpart weight prior
In-Reply-To: <4690C8E0.2000005@tourduvalat.org>
References: <4690C8E0.2000005@tourduvalat.org>
Message-ID: <Pine.LNX.4.64.0707091221080.18947@gannet.stats.ox.ac.uk>

On Sun, 8 Jul 2007, Aur?lie Davranche wrote:

> Hi!
>
> Could you please explain the difference between "prior" and "weight" in 
> rpart? It seems to be the same. But in this case why including a weight 
> option in the latest versions? For an unbalanced sampling what is the best to 
> use : weight, prior or the both together?

The 'weight' argument (sic) has been there for a decade, and is not the 
same as the 'prior' param.

The help file (which you seem unfamiliar with) says

  weights: optional case weights.

    parms: optional parameters for the splitting function. Anova
           splitting has no parameters. Poisson splitting has a single
           parameter, the coefficient of variation of the prior
           distribution on the rates.  The default value is 1.
           Exponential splitting has the same parameter as Poisson. For
           classification splitting, the list can contain any of: the
           vector of prior probabilities (component 'prior'), the loss
           matrix (component 'loss') or the splitting index (component
           'split').  The priors must be positive and sum to 1.  The
           loss matrix must have zeros on the diagonal and positive
           off-diagonal elements.  The splitting index can be 'gini' or
           'information'.  The default priors are proportional to the
           data counts, the losses default to 1, and the split defaults
           to 'gini'.

The rpart technical report at

http://mayoresearch.mayo.edu/mayo/research/biostat/upload/61.pdf

may help you understand this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Bill.Venables at csiro.au  Mon Jul  9 13:49:02 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Mon, 9 Jul 2007 21:49:02 +1000
Subject: [R] a little problem on selecting a subset from dataset A
	accordingto dataset B?
Message-ID: <B998A44C8986644EA8029CFE6396A924B6803D@exqld2-bne.nexus.csiro.au>

> AB <- with(B, subset(A, coords.x1 %in% X1))
> AB
   coords.x1 coords.x2
0   542250.9   3392404
7   541512.5   3394722
8   541479.3   3394878
9   538903.4   3395943
18  543274.0   3389919
19  543840.8   3392012

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of zhijie zhang
Sent: Monday, 9 July 2007 2:43 AM
To: R-help at stat.math.ethz.ch
Subject: [R] a little problem on selecting a subset from dataset A
accordingto dataset B?

Dear Friends,
   I want to extract the records from A according to B, but the results
are
not correct because R says :
  The length of long object is not integer times on the length of short
object.
  Anybody have met the same problem? How to do it correctly?

length(A)=47
length(B)=6

A[A$coords.x1==B$X1,]   #the program for the above task. I should get 6
records, but i only get former 4 records for the above reason.

Thanks.
 The folloing shows dataset A and B.


> A
   coords.x1 coords.x2
0  542250.89 3392404.1
1  538813.87 3388339.0
2  536049.19 3385821.6
3  533659.62 3383194.2
4  530642.30 3376834.9
5  529573.15 3378177.8
6  530853.82 3394838.8
7  541512.51 3394721.6
8  541479.33 3394877.8
9  538903.39 3395942.5
10 536019.95 3396286.1
11 538675.23 3384213.2
12 535127.95 3381255.4
13 533852.24 3378660.4
14 531360.91 3379273.8
15 539289.14 3375759.8
16 543410.51 3384353.1
17 543089.27 3388170.1
18 543274.03 3389919.2
19 543840.77 3392012.4
20 553383.55 3402401.8
21 554621.51 3397938.9
22 564096.42 3397524.4
23 567529.64 3398702.9
24 561798.76 3404864.0
25 562868.34 3405502.2
26 563145.22 3403192.1
27 562419.87 3404090.4
28 558321.85 3403879.9
29 567050.74 3404973.1
30 570609.70 3408742.4
31 556777.57 3397858.0
32 531353.38 3368596.6
33 533513.50 3372749.3
34 537543.19 3364284.8
35 538779.41 3368224.8
36 525930.09 3374067.7
37 522990.85 3369213.1
38 528826.37 3359019.0
39 533865.85 3362595.4
40 531200.25 3365053.0
41 551054.10 3377181.3
42 546974.19 3369284.8
43 572315.59 3359541.1
44 562703.63 3355173.4
45 558959.31 3357804.4
46 558531.39 3361741.1


> B
         X1        X2
1 542250.89 3392404.1
2 541512.51 3394721.6
3 541479.33 3394877.8
4 538903.39 3395942.5
5 543274.03 3389919.2
6 543840.77 3392012.4

-- 
With Kind Regards,

oooO:::::::::
(..):::::::::
:\.(:::Oooo::
::\_)::(..)::
:::::::)./:::
::::::(_/::::
:::::::::::::
[***********************************************************************
]
Zhi Jie,Zhang ,PHD
Tel:86-21-54237149
Dept. of Epidemiology,School of Public Health,Fudan University
Address:No. 138 Yi Xue Yuan Road,Shanghai,China
Postcode:200032
Email:epistat at gmail.com
Website: www.statABC.com
[***********************************************************************
]
oooO:::::::::
(..):::::::::
:\.(:::Oooo::
::\_)::(..)::
:::::::)./:::
::::::(_/::::
:::::::::::::

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Thierry.ONKELINX at inbo.be  Mon Jul  9 13:55:23 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 9 Jul 2007 13:55:23 +0200
Subject: [R] Problem with Sweave and pdf version 1.4
Message-ID: <2E9C414912813E4EB981326983E0A104033E4FB6@inboexch.inbo.be>

Dear useRs,

I'm trying to use ggplot2 in Sweave (R 2.5.1). The plots use the alpha
channel, so I need to use pdf version 1.4. Search the mailinglist
archive I found two solutions: \SweaveOpts{echo = FALSE,
pdf.version=1.4} and explicit writing to a pdf 1.4 file. The latter
works but the first doesn't. Probably I'm doing something wrong (see the
Rnw file below). The file tmp_tmp.pdf is generated but is corrupt.
Using the second option, is there a way to create the filename based on
some variable. Something like 
x <- "sometext"
filename <- paste(x, ".pdf", sep = "")
pdf(file=filename, version="1.4") 

But how can I pass the filename to \includegraphics?

The Rnw file

\documentclass[11pt]{report}
\usepackage{Sweave}
\SweaveOpts{echo = FALSE, pdf.version=1.4, eps = FALSE}
\begin{document}
\begin{figure}
\centering
<<tmp, fig = T, width = 5, height = 4>>=
  library(ggplot2)
  x <- runif(100)
  dataset <- data.frame(x = x, y = 5 * x + rnorm(100))
  ggplot(dataset, aes(y = y, x = x)) + stat_smooth() + geom_jitter()
@
\caption{Here comes a caption.}
\label{fig:tmp}
\end{figure}
\begin{figure}
\centering
<<fig = F>>=
  pdf(file="foo.pdf", version="1.4") 
  ggplot(dataset, aes(y = y, x = x)) + stat_smooth() + geom_jitter()
  dev.off() 
@ 
\incudegraphics{foo} 
\caption{Here comes a caption.}
\label{fig:foo}
\end{figure}
\end{document}


Thanks in advance,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


From jim at bitwrit.com.au  Mon Jul  9 14:18:40 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 09 Jul 2007 22:18:40 +1000
Subject: [R] character string to name
Message-ID: <469227A0.4070902@bitwrit.com.au>

Hi folks,

I thought I recalled a request for turning a character string into an 
object name as in:

x$as.name("y")<-1:4

OR

x<-data.frame(as.name("y")=1:4)

However, as.name and a few other uninformed attempts didn't even come 
close. A search of "character to name" produced no helpful functions. 
This isn't a very urgent request, but if anyone knows some trick to 
perform this transformation, I would like to hear about it. Thanks.

Jim


From jingjiangyan at gmail.com  Mon Jul  9 14:14:17 2007
From: jingjiangyan at gmail.com (jingjiang yan)
Date: Mon, 9 Jul 2007 20:14:17 +0800
Subject: [R] why function 'sum' within 'aggregate' need an 'index'?
Message-ID: <edc929b40707090514x5a761ed0l42b36fb9a0a4c756@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/7a81710d/attachment.pl 

From gavin.simpson at ucl.ac.uk  Mon Jul  9 14:27:39 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 09 Jul 2007 13:27:39 +0100
Subject: [R] why function 'sum' within 'aggregate' need an 'index'?
In-Reply-To: <edc929b40707090514x5a761ed0l42b36fb9a0a4c756@mail.gmail.com>
References: <edc929b40707090514x5a761ed0l42b36fb9a0a4c756@mail.gmail.com>
Message-ID: <1183984059.21961.33.camel@gsimpson.geog.ucl.ac.uk>

On Mon, 2007-07-09 at 20:14 +0800, jingjiang yan wrote:
> Hi, people.
> I am using R-2.5.0 now, when tried the function aggregate with sum, it
> showed an  error as following:
> > a <- gl(3,10)
> > b <- rnorm(30)
> > aggregate(b,list(a),sum)
> #  here is the error message,  it complained that an error in FUN(X[[1L]],
> missing "INDEX", and no defaults value.
> 
> but the tapply function will be okay.
> > tapply(b,list(a),sum)
>         1         2         3
>  2.113349 -5.972195  4.854731
> 
> furthermore, when I was using the R-2.5.0 pre-release version before.
> it could work well.
> > a <- gl(3,10)
> > b <- rnorm(30)
> > aggregate(b,list(a),sum)      # it works well
>   Group.1          x
> 1       1 -1.0330482
> 2       2  0.1235796
> 3       3 -1.0086930
> > tapply(b,list(a),sum)            # so does tapply
>          1          2          3
> -1.0330482  0.1235796 -1.0086930
> 
> So, who can tell what should I do to overcome this?
> thanks a lot.

Update to R 2.5.1 as your first example works for me [version info
below]. If that is not possible, just use the tapply version for now.

G

> version
               _
platform       i686-pc-linux-gnu
arch           i686
os             linux-gnu
system         i686, linux-gnu
status         Patched
major          2
minor          5.1
year           2007
month          07
day            05
svn rev        42131
language       R
version.string R version 2.5.1 Patched (2007-07-05 r42131)

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ggrothendieck at gmail.com  Mon Jul  9 14:32:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 9 Jul 2007 08:32:15 -0400
Subject: [R] character string to name
In-Reply-To: <469227A0.4070902@bitwrit.com.au>
References: <469227A0.4070902@bitwrit.com.au>
Message-ID: <971536df0707090532y683b15daoefbf27de9dcfc135@mail.gmail.com>

Is one of these alternatives what you want?

# 1
x <- list()
x[["y"]] <- 1:4
x <- as.data.frame(x)
x

# 2
x <- data.frame(1:4)
names(x) <- "y"
x

# 3
x <- as.data.frame(sapply("y", function(x, y) y, 1:4, simplify = FALSE))
x

On 7/9/07, Jim Lemon <jim at bitwrit.com.au> wrote:
> Hi folks,
>
> I thought I recalled a request for turning a character string into an
> object name as in:
>
> x$as.name("y")<-1:4
>
> OR
>
> x<-data.frame(as.name("y")=1:4)
>
> However, as.name and a few other uninformed attempts didn't even come
> close. A search of "character to name" produced no helpful functions.
> This isn't a very urgent request, but if anyone knows some trick to
> perform this transformation, I would like to hear about it. Thanks.
>
> Jim
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jrkrideau at yahoo.ca  Mon Jul  9 15:05:35 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Mon, 9 Jul 2007 09:05:35 -0400 (EDT)
Subject: [R] transform excel data into graph
In-Reply-To: <11493073.post@talk.nabble.com>
Message-ID: <416372.78582.qm@web32812.mail.mud.yahoo.com>

I am not sure exactly what you want but see if this
will work. It will give you a dotchart.  

Assume your data.frame is dat
First give the lessons column a name. See ?names

convert numbers in data.frame to a matrix for dotchart

dmat <- as.matrix(dat[,2:4])

Draw dotchart using dat$lessons as a 
dotchart(dmat, labels = dat[,1])

--- cross123 <ctsirigos at yahoo.com> wrote:

> 
> Hello everyone,
> I have a set of data in the following form, which
> are stored in an Excel
> file:
>                  nick       john       peter        
>         
> lesson1       0.465     0.498     0.473    
> lesson2       0.422      0.44      0.134           
> lesson3       0.45       0.35       0.543           
>           
> lesson4       0.590      0.64      0.11             
>         
> lesson5       0.543      0.5        0.32            
>          
> 
> What I want to do is a 2d-graph plot where I will
> have  the name of the
> student in the X-axis and the name of the lesson in
> the Y-axis and the
> number from each pair will be used to construct the
> plot.
> I am newbie with R and I don't know which package
> shall I use nor the
> commands with which I will import my data in R so
> that the plot will be
> created...
> 
> Any help would be greatly appreciated.


From yee at post.harvard.edu  Mon Jul  9 15:12:16 2007
From: yee at post.harvard.edu (Andrew Yee)
Date: Mon, 9 Jul 2007 09:12:16 -0400
Subject: [R] using the function unique(),
	but asking it to ignore a column of a data.frame
Message-ID: <5dff5a0d0707090612r2dfd72d9t5533d817c8db9cdf@mail.gmail.com>

Take for example the following data.frame:

a<-c(1,1,5)
b<-c(3,2,3)
c<-c(5,1,5)
sample.data.frame<-data.frame(a=a,b=b,c=c)

I'd like to be able to use unique(sample.data.frame), but have
unique() ignore column a when determining the unique elements.

However, I figured that this would be setting for incomparables=, but
it appears that this funcationality hasn't been incorporated.  Is
there a work around for this, i.e. to be able to get unique to only
look at selected columns of a data frame?

Thanks,
Andrew


From bjorn.vancampenhout at ua.ac.be  Mon Jul  9 15:12:13 2007
From: bjorn.vancampenhout at ua.ac.be (Van Campenhout Bjorn)
Date: Mon, 9 Jul 2007 15:12:13 +0200
Subject: [R] Speeding up
Message-ID: <0EE866100C01984EAE6AC3AE56EDFE535A7C0F@xmail05.ad.ua.ac.be>

Hi R-helpers,

I am new in programming and R, so this may be basic.  I need to speed up
a procedure where I minimize some function on different partitions of
the data. I can do this with a loop, as for instance in:

i<-1
meanmin<-Inf
while (i<length(x)) {
	ind<-x<xord[i]
	if (some function to be minimized<meanmin) {
		meanmin<-some function to be minimized
		indmin<-xord[i]
	}
i<-i+1
}
print(indmin)

I learned that I should avoid loops, so I found the following
alternative:

dmat<-outer(x,xord,"<")*1 
ss<-apply(dmat,2,function (z) some function to be minimized)
indmin<-xord[which.min(ss)]
print(indmin)

But this does not lead to spectacular improvements (obviously, this is
dependent on the function and the length of x, and this dmat does not
seem to be an elegant solution to me).  Is there scope for substantial
improvement? Any pointers will be greatly appreciated.  Below an example
with some timings.


> set.seed(12345)
> x <- rnorm(1000, mean = 5, sd = 2)
> xord<-x[order(x)]
> 
> system.time({i<-1
+ meanmin<-Inf
+ while (i<length(x)) {
+ ind<-x<xord[i]
+ if ((mean(x*ind)+mean(x*!ind))<meanmin) {
+ meanmin<-mean(x*ind)+mean(x*!ind)
+ indmin<-xord[i]
+ }
+ i<-i+1
+ }
+ print(indmin)})
[1] 3.826595
   user  system elapsed 
   0.14    0.00    0.14 
> 
> 
> 
> system.time({dmat<-outer(x,xord,"<")*1 
+ ss<-apply(dmat,2,function (z) mean(x*z)+mean(x*!z))
+ indmin<-xord[which.min(ss)]
+ print(indmin)})
[1] 3.826595
   user  system elapsed 
   0.42    0.06    0.49 
> 
>


From h.wickham at gmail.com  Mon Jul  9 15:28:23 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 9 Jul 2007 15:28:23 +0200
Subject: [R] using the function unique(),
	but asking it to ignore a column of a data.frame
In-Reply-To: <5dff5a0d0707090612r2dfd72d9t5533d817c8db9cdf@mail.gmail.com>
References: <5dff5a0d0707090612r2dfd72d9t5533d817c8db9cdf@mail.gmail.com>
Message-ID: <f8e6ff050707090628m1a422e63v2a7c9715bc571403@mail.gmail.com>

On 7/9/07, Andrew Yee <yee at post.harvard.edu> wrote:
> Take for example the following data.frame:
>
> a<-c(1,1,5)
> b<-c(3,2,3)
> c<-c(5,1,5)
> sample.data.frame<-data.frame(a=a,b=b,c=c)
>
> I'd like to be able to use unique(sample.data.frame), but have
> unique() ignore column a when determining the unique elements.
>
> However, I figured that this would be setting for incomparables=, but
> it appears that this funcationality hasn't been incorporated.  Is
> there a work around for this, i.e. to be able to get unique to only
> look at selected columns of a data frame?

unique(df[,c("a","c")]) ?

Hadley


From yee at post.harvard.edu  Mon Jul  9 15:33:00 2007
From: yee at post.harvard.edu (Andrew Yee)
Date: Mon, 9 Jul 2007 09:33:00 -0400
Subject: [R] using the function unique(),
	but asking it to ignore a column of a data.frame
In-Reply-To: <f8e6ff050707090628m1a422e63v2a7c9715bc571403@mail.gmail.com>
References: <5dff5a0d0707090612r2dfd72d9t5533d817c8db9cdf@mail.gmail.com>
	<f8e6ff050707090628m1a422e63v2a7c9715bc571403@mail.gmail.com>
Message-ID: <5dff5a0d0707090633t74206396u9c522bf20720b0d1@mail.gmail.com>

Thanks.  But in this specific case, I would like the output to include
all three columns, including the "ignored" column (in this case, I'd
like it to ignore column a).

Thanks,
Andrew

On 7/9/07, hadley wickham <h.wickham at gmail.com> wrote:
> On 7/9/07, Andrew Yee <yee at post.harvard.edu> wrote:
> > Take for example the following data.frame:
> >
> > a<-c(1,1,5)
> > b<-c(3,2,3)
> > c<-c(5,1,5)
> > sample.data.frame<-data.frame(a=a,b=b,c=c)
> >
> > I'd like to be able to use unique(sample.data.frame), but have
> > unique() ignore column a when determining the unique elements.
> >
> > However, I figured that this would be setting for incomparables=, but
> > it appears that this funcationality hasn't been incorporated.  Is
> > there a work around for this, i.e. to be able to get unique to only
> > look at selected columns of a data frame?
>
> unique(df[,c("a","c")]) ?
>
> Hadley
>


From gchappi at gmail.com  Mon Jul  9 15:42:45 2007
From: gchappi at gmail.com (Hans-Peter)
Date: Mon, 9 Jul 2007 16:42:45 +0300
Subject: [R] Writing Excel (.xls) files on non-Windows OSs using Perl
In-Reply-To: <1183926037.3706.89.camel@Bellerophon.localdomain>
References: <1183926037.3706.89.camel@Bellerophon.localdomain>
Message-ID: <47fce0650707090642j1bf4a0a3q45c2eb59ea94bd1c@mail.gmail.com>

Hi,

2007/7/8, Marc Schwartz <marc_schwartz at comcast.net>:
> [snip]
> There exists the xlsReadWrite package on CRAN by Hans-Peter Suter, which
> is restricted to Windows, since it utilizes the non-FOSS MS Office API
> to write the Excel formats.

The non-FOSS API is not the problem(#) but its implementation is:

The 3rd party library I use is written in Pascal and supports Delphi
and Kylix. Kylix would allow to port the package to Linux but as Kylix
has unfortunately been abandoned by CodeGear (Borland) I am not
ready/interested to spend my time on this dead road. Though it
probably could be done quickly.

A much more interesting way is to port the package using FreePascal.
--> I plan to do this since long but...
--> Maybe someone fluent on Linux and FreePascal could have a look at
the pascal header files (treetron.googlepages.com) and make the demos
run on Linux..., that would be great and speed up an eventual
xlsReadWrite port!

-- 
Regards,
Hans-Peter

(#) at least for people who are not in principle opposed to run a
package which contains a non-FOSS code part.


From olivier.eterradossi at ema.fr  Mon Jul  9 15:44:53 2007
From: olivier.eterradossi at ema.fr (Olivier ETERRADOSSI)
Date: Mon, 09 Jul 2007 15:44:53 +0200
Subject: [R] about scagnostics
Message-ID: <46923BD5.5000204@ema.fr>

Hi Hadley,
thank you for providing this "scagnostics" primer....
I was trying to do some basic testing, and I see that I probably missed 
some points :
first it's not clear for me if the argument of "scagnostics" should be 
raw data or "processed" data (results of calling "splom" or whatever...).
If the first, I thought (from Wilkinson & al.) that if taking as an 
example variables x and y being the coordinates of a circle, I should 
find in scagnostics(x,y)$s :
Skinny = 0 and Convex =1.
I get Skinny = 1 and Convex =0.... What am I missing ?
(My God, I'm feeling myself going to be "Ripleyed" !.....)
Regards, Olivier

-- 
Olivier ETERRADOSSI
Ma?tre-Assistant
CMGD / Equipe "Propri?t?s Psycho-Sensorielles des Mat?riaux"
Ecole des Mines d'Al?s
H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
tel std: +33 (0)5.59.30.54.25
tel direct: +33 (0)5.59.30.90.35 
fax: +33 (0)5.59.30.63.68
http://www.ema.fr


From elyakhlifi_mustapha at yahoo.fr  Mon Jul  9 15:52:04 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Mon, 9 Jul 2007 15:52:04 +0200 (CEST)
Subject: [R] HTML.data.frame
Message-ID: <250150.95284.qm@web27505.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/802c1aa4/attachment.pl 

From pomchip at free.fr  Mon Jul  9 15:53:55 2007
From: pomchip at free.fr (=?UTF-8?B?U8OpYmFzdGllbg==?=)
Date: Mon, 09 Jul 2007 09:53:55 -0400
Subject: [R] Several quick questions
In-Reply-To: <eb555e660707081738o791ebcbbw53c393bd76600ac7@mail.gmail.com>
References: <468F8AF3.6010200@free.fr>
	<eb555e660707081738o791ebcbbw53c393bd76600ac7@mail.gmail.com>
Message-ID: <46923DF3.8030707@free.fr>

Thanks all of you guys for your help. It will most helpful.


deepayan.sarkar at gmail.com a ?crit :
> On 7/7/07, S?bastien <pomchip at free.fr> wrote:
>> Dear R users,
>>
>> Here is a couple a quick questions, for which I was unable to not find
>> any answer in the list archives and in the help:
>
> [...]
>
>> 2- When a log scale is called in a graph, the label takes a format like
>> 10^n.
>
> That's true for lattice, but not traditional graphics, as far as I know.
>
>> Is there a way to come back to a regular number format like 1, 10,
>> 100... without having to create a custom axis ?
>
> Depends on what you mean by "custom axis". You don't need to manually
> choose the tick positions etc, but you still need to define the rules
> that determine how they are calculated. See example(axis.default) for
> an example where the tick positions remain the same (as the defaults),
> but the labels change.  The slightly different rule used in
> traditional graphics is available through the axTicks() function,
> which basically boils down to this:
>
> logTicks <- function (lim, loc = c(1, 5))
> {
>    ii <- floor(log10(range(lim))) + c(-1, 2)
>    main <- 10^(ii[1]:ii[2])
>    r <- as.numeric(outer(loc, main, "*"))
>    r[lim[1] <= r & r <= lim[2]]
> }
>
> where 'lim' is the limits in the original scale. So we have
>
>> logTicks(c(1, 100))
> [1]   1   5  10  50 100
>> logTicks(c(1, 100), loc = c(2, 5, 10))
> [1]   1   2   5  10  20  50 100
>
>> 3- In lattice graphics, how does the default value of the "axs" argument
>> influence the values of "limits" ?
>> This question should be considered in the following context. The help
>> states that a 4% extension is applied by default to the axis range in
>> base graphics. So, I have tried to apply this 4 % extension to create
>> some custom lattice graphics. I worked on a dataset in which the
>> independent variable ranged from 0  to 120, so I basically customized my
>> axis using limits=c(-4.8,124.8). The results of the graphics with and
>> without the limits command were not identical...
>
> The extension is user-settable in lattice, and defaults to 7% (I think
> this value came from Trellis specs, but I don't remember the exact
> details).
>
>> lattice.getOption("axis.padding")
> $numeric
> [1] 0.07
>
> $factor
> [1] 0.6
>
> -Deepayan


From Thierry.ONKELINX at inbo.be  Mon Jul  9 15:55:52 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 9 Jul 2007 15:55:52 +0200
Subject: [R] Speeding up
In-Reply-To: <0EE866100C01984EAE6AC3AE56EDFE535A7C0F@xmail05.ad.ua.ac.be>
References: <0EE866100C01984EAE6AC3AE56EDFE535A7C0F@xmail05.ad.ua.ac.be>
Message-ID: <2E9C414912813E4EB981326983E0A104033E5037@inboexch.inbo.be>

Dear Bjorn,

Do you know that mean(xy*ind)+mean(xy*!ind) yields the same value for
all i? Maybe you meant mean(xy[ind]) + mean(xy[!ind])

sapply(xord, function(xordi, xy = x){
  ind <- xy < xordi
  mean(xy*ind)+mean(xy*!ind)
})

Cheers,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Van Campenhout Bjorn
> Verzonden: maandag 9 juli 2007 15:12
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] Speeding up
> 
> Hi R-helpers,
> 
> I am new in programming and R, so this may be basic.  I need 
> to speed up a procedure where I minimize some function on 
> different partitions of the data. I can do this with a loop, 
> as for instance in:
> 
> i<-1
> meanmin<-Inf
> while (i<length(x)) {
> 	ind<-x<xord[i]
> 	if (some function to be minimized<meanmin) {
> 		meanmin<-some function to be minimized
> 		indmin<-xord[i]
> 	}
> i<-i+1
> }
> print(indmin)
> 
> I learned that I should avoid loops, so I found the following
> alternative:
> 
> dmat<-outer(x,xord,"<")*1
> ss<-apply(dmat,2,function (z) some function to be minimized) 
> indmin<-xord[which.min(ss)]
> print(indmin)
> 
> But this does not lead to spectacular improvements 
> (obviously, this is dependent on the function and the length 
> of x, and this dmat does not seem to be an elegant solution 
> to me).  Is there scope for substantial improvement? Any 
> pointers will be greatly appreciated.  Below an example with 
> some timings.
> 
> 
> > set.seed(12345)
> > x <- rnorm(1000, mean = 5, sd = 2)
> > xord<-x[order(x)]
> > 
> > system.time({i<-1
> + meanmin<-Inf
> + while (i<length(x)) {
> + ind<-x<xord[i]
> + if ((mean(x*ind)+mean(x*!ind))<meanmin) {
> + meanmin<-mean(x*ind)+mean(x*!ind)
> + indmin<-xord[i]
> + }
> + i<-i+1
> + }
> + print(indmin)})
> [1] 3.826595
>    user  system elapsed 
>    0.14    0.00    0.14 
> > 
> > 
> > 
> > system.time({dmat<-outer(x,xord,"<")*1
> + ss<-apply(dmat,2,function (z) mean(x*z)+mean(x*!z)) 
> + indmin<-xord[which.min(ss)]
> + print(indmin)})
> [1] 3.826595
>    user  system elapsed 
>    0.42    0.06    0.49 
> > 
> >
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From P.Dalgaard at biostat.ku.dk  Mon Jul  9 15:59:01 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 09 Jul 2007 15:59:01 +0200
Subject: [R] using the function unique(),
 but asking it to ignore a column of a data.frame
In-Reply-To: <5dff5a0d0707090633t74206396u9c522bf20720b0d1@mail.gmail.com>
References: <5dff5a0d0707090612r2dfd72d9t5533d817c8db9cdf@mail.gmail.com>	<f8e6ff050707090628m1a422e63v2a7c9715bc571403@mail.gmail.com>
	<5dff5a0d0707090633t74206396u9c522bf20720b0d1@mail.gmail.com>
Message-ID: <46923F25.90707@biostat.ku.dk>

Andrew Yee wrote:
> Thanks.  But in this specific case, I would like the output to include
> all three columns, including the "ignored" column (in this case, I'd
> like it to ignore column a).
>   
df[!duplicated(df[,c("a","c")]),]

or perhaps

df[!duplicated(df[-2]),]
> Thanks,
> Andrew
>
> On 7/9/07, hadley wickham <h.wickham at gmail.com> wrote:
>   
>> On 7/9/07, Andrew Yee <yee at post.harvard.edu> wrote:
>>     
>>> Take for example the following data.frame:
>>>
>>> a<-c(1,1,5)
>>> b<-c(3,2,3)
>>> c<-c(5,1,5)
>>> sample.data.frame<-data.frame(a=a,b=b,c=c)
>>>
>>> I'd like to be able to use unique(sample.data.frame), but have
>>> unique() ignore column a when determining the unique elements.
>>>
>>> However, I figured that this would be setting for incomparables=, but
>>> it appears that this funcationality hasn't been incorporated.  Is
>>> there a work around for this, i.e. to be able to get unique to only
>>> look at selected columns of a data frame?
>>>       
>> unique(df[,c("a","c")]) ?
>>
>> Hadley
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From david.meyer at wu-wien.ac.at  Mon Jul  9 15:49:29 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Mon, 09 Jul 2007 15:49:29 +0200
Subject: [R] [R-pkgs] New package "proxy" for distances and similiarities
Message-ID: <46923CE9.4090802@wu-wien.ac.at>

Dear useRs,

a new package for computing distance and similarity matrices made it to 
CRAN, and will propagate to the mirrors soon.

It includes an enhanced version of "dist()" with support for more than 
40 popular similarity and distance measures, both for auto- and 
cross-distances. Some important ones are implemented in C.

The proximity measures are stored in a registry which can easily be 
queried and extended by users at run-time. For adding a new measure, the 
simplest way is to provide the distance measure as a small R function, 
the package code will do the loops on the C code level to create the 
proximity matrix. It is of course also possible to use more efficient C 
implementations---either for the distance measure alone, or the whole 
matrix computation.

Input data is not restricted to matrices: provided the proximity measure 
can handle it, lists and data frames are also accepted.

The formulas for binary proximities can conveniently be specified in the 
a/b/c/d/n format, where the number of concordant/discordant pairs is 
precomputed on the C code level.

We are currently working on support for sparse data.

This is also a "Call for Measures": if you feel that a particular 
similarity of distance measure is missing, please send the formula and a 
reference (or, ideally, the whole registry entry) to one of the package 
maintainers who will happily add it.

David and Christian.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From h.wickham at gmail.com  Mon Jul  9 16:13:59 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 9 Jul 2007 16:13:59 +0200
Subject: [R] about scagnostics
In-Reply-To: <46923BD5.5000204@ema.fr>
References: <46923BD5.5000204@ema.fr>
Message-ID: <f8e6ff050707090713n4b3a17cg96eb175e2314ac65@mail.gmail.com>

Hi Olivier,

You can call scagnostics either with two vectors, or a data.frame (in
which case it computes all pairwise scagnostics).

I just double checked to make sure I didn't accidentally misname the
vector of scagnostics in R, and it doesn't look like I did, so could
you please send me a reproducible example so I can look into it more
closely.

Thanks,

Hadly

On 7/9/07, Olivier ETERRADOSSI <olivier.eterradossi at ema.fr> wrote:
> Hi Hadley,
> thank you for providing this "scagnostics" primer....
> I was trying to do some basic testing, and I see that I probably missed
> some points :
> first it's not clear for me if the argument of "scagnostics" should be
> raw data or "processed" data (results of calling "splom" or whatever...).
> If the first, I thought (from Wilkinson & al.) that if taking as an
> example variables x and y being the coordinates of a circle, I should
> find in scagnostics(x,y)$s :
> Skinny = 0 and Convex =1.
> I get Skinny = 1 and Convex =0.... What am I missing ?
> (My God, I'm feeling myself going to be "Ripleyed" !.....)
> Regards, Olivier
>
> --
> Olivier ETERRADOSSI
> Ma?tre-Assistant
> CMGD / Equipe "Propri?t?s Psycho-Sensorielles des Mat?riaux"
> Ecole des Mines d'Al?s
> H?lioparc, 2 av. P. Angot, F-64053 PAU CEDEX 9
> tel std: +33 (0)5.59.30.54.25
> tel direct: +33 (0)5.59.30.90.35
> fax: +33 (0)5.59.30.63.68
> http://www.ema.fr
>
>


From h.wickham at gmail.com  Mon Jul  9 16:16:58 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 9 Jul 2007 16:16:58 +0200
Subject: [R] using the function unique(),
	but asking it to ignore a column of a data.frame
In-Reply-To: <46923F25.90707@biostat.ku.dk>
References: <5dff5a0d0707090612r2dfd72d9t5533d817c8db9cdf@mail.gmail.com>
	<f8e6ff050707090628m1a422e63v2a7c9715bc571403@mail.gmail.com>
	<5dff5a0d0707090633t74206396u9c522bf20720b0d1@mail.gmail.com>
	<46923F25.90707@biostat.ku.dk>
Message-ID: <f8e6ff050707090716s251be12bsbc58b80ea9761a0a@mail.gmail.com>

On 7/9/07, Peter Dalgaard <P.Dalgaard at biostat.ku.dk> wrote:
> Andrew Yee wrote:
> > Thanks.  But in this specific case, I would like the output to include
> > all three columns, including the "ignored" column (in this case, I'd
> > like it to ignore column a).
> >
> df[!duplicated(df[,c("a","c")]),]
>
> or perhaps
>
> df[!duplicated(df[-2]),]

Yes - of course.  I was momentarily confused about unique vs. duplicated.  Oops!

Hadley


From tlumley at u.washington.edu  Mon Jul  9 16:25:07 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Jul 2007 07:25:07 -0700 (PDT)
Subject: [R] character string to name
In-Reply-To: <469227A0.4070902@bitwrit.com.au>
References: <469227A0.4070902@bitwrit.com.au>
Message-ID: <Pine.LNX.4.64.0707090724560.11041@homer24.u.washington.edu>

On Mon, 9 Jul 2007, Jim Lemon wrote:

> Hi folks,
>
> I thought I recalled a request for turning a character string into an
> object name as in:

Yes. It's a FAQ.

 	-thomas


> x$as.name("y")<-1:4
>
> OR
>
> x<-data.frame(as.name("y")=1:4)
>
> However, as.name and a few other uninformed attempts didn't even come
> close. A search of "character to name" produced no helpful functions.
> This isn't a very urgent request, but if anyone knows some trick to
> perform this transformation, I would like to hear about it. Thanks.
>
> Jim
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ripley at stats.ox.ac.uk  Mon Jul  9 16:45:27 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jul 2007 15:45:27 +0100 (BST)
Subject: [R] using the function unique(),
 but asking it to ignore a column of a data.frame
In-Reply-To: <5dff5a0d0707090633t74206396u9c522bf20720b0d1@mail.gmail.com>
References: <5dff5a0d0707090612r2dfd72d9t5533d817c8db9cdf@mail.gmail.com>
	<f8e6ff050707090628m1a422e63v2a7c9715bc571403@mail.gmail.com>
	<5dff5a0d0707090633t74206396u9c522bf20720b0d1@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707091543280.22123@gannet.stats.ox.ac.uk>

On Mon, 9 Jul 2007, Andrew Yee wrote:

> Thanks.  But in this specific case, I would like the output to include
> all three columns, including the "ignored" column (in this case, I'd
> like it to ignore column a).

sample.data.frame[!duplicated(sample.data.frame[-1]), ]

(index to exclude columns as you wish).

> Thanks,
> Andrew
>
> On 7/9/07, hadley wickham <h.wickham at gmail.com> wrote:
>> On 7/9/07, Andrew Yee <yee at post.harvard.edu> wrote:
>>> Take for example the following data.frame:
>>>
>>> a<-c(1,1,5)
>>> b<-c(3,2,3)
>>> c<-c(5,1,5)
>>> sample.data.frame<-data.frame(a=a,b=b,c=c)
>>>
>>> I'd like to be able to use unique(sample.data.frame), but have
>>> unique() ignore column a when determining the unique elements.
>>>
>>> However, I figured that this would be setting for incomparables=, but
>>> it appears that this funcationality hasn't been incorporated.  Is
>>> there a work around for this, i.e. to be able to get unique to only
>>> look at selected columns of a data frame?
>>
>> unique(df[,c("a","c")]) ?
>>
>> Hadley
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h.wickham at gmail.com  Mon Jul  9 16:47:43 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 9 Jul 2007 16:47:43 +0200
Subject: [R] possible bug in ggplot2 v0.5.2???
In-Reply-To: <Pine.LNX.4.64.0707060630110.9948@gannet.stats.ox.ac.uk>
References: <468A1B01.9090100@genoscope.cns.fr>
	<f8e6ff050707030314h21fa2898m1dc1ce35b91dbb54@mail.gmail.com>
	<Pine.LNX.4.64.0707031255400.25506@gannet.stats.ox.ac.uk>
	<f8e6ff050707040037h58fba7a1w67c2b829dc9b2cb0@mail.gmail.com>
	<Pine.LNX.4.64.0707060630110.9948@gannet.stats.ox.ac.uk>
Message-ID: <f8e6ff050707090747t7b39af95k7b00822edf62a893@mail.gmail.com>

On 7/6/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Wed, 4 Jul 2007, hadley wickham wrote:
>
> > On 7/4/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> >> On Tue, 3 Jul 2007, hadley wickham wrote:
> >>
> >> > Hi Stephane,
> >> >
> >> > The problem is that the windows graphics device doesn't support
> >> > transparent colours.  You can get around this in two ways:
> >>
> >> It certainly does!  Try col="transparent" (and perhaps consult your
> >> dictionary).  It was news to me that the windows() graphics device worked
> >> on
> >> Linux i586.
> >
> > Well my dictionary defines transparent as "allowing light to pass
> > through so that objects behind can be distinctly seen" which I believe
> > applies here (ie. stained glass windows and blue points with alpha 0.5
> > are both transparent).  What does your dictionary say?
>
> Not quite the same, but even by your definition col="transparent" is
> transparent.  In this context
>
> http://en.wikipedia.org/wiki/Transparency_%28graphic%29
>
> seems more pertinent.

col="transparent" is transparent by any reasonable definition, but
does it make sense to claim that a graphics device "supports"
transparency?  How can you tell the difference between a transparent
object and nothing?

> >> What it does not support as yet is translucent colours, and that is a
> >> restriction imposed by Windows (translucency support was introduced for
> >> Windows XP, and we still try to support older versions of Windows, unlike
> >> the MacOS people).  I have been working on a workaround, so translucency
> >> support is likely to be implemented in R 2.6.0 for users of XP or later.
> >
> > I am confused by your implication that windows (prior to XP) does not
> > support translucency.  Perhaps it is not supported at the operating
> > system level, but it has certainly been available at the application
> > level for a very long time.
>
> Really? It's hard to reply to unspecific assertions.  But remember XP has
> been out since 2001, almost as long as PDF has supported translucency.

Yes, I agree, and thank you for providing some support to your
statements.  Java has supported transparency since 1.2 (with the
Graphics2D class), and was released on Dec 4, 1998, so certainly some
applications were drawing transparent graphics on windows.

> >> Given that neither of the two main screen devices and neither of the
> >> standard print devices support translucency, the subject line looks
> >> correct to me: the problem surely lies in the assumptions made in ggplot2.
> >
> > The features of the windows and X11 devices clearly lag behind the
> > quartz and pdf devices.  I can program for the lowest common
> > denominator or I can use modern features that support the tasks I am
> > working on.  I choose the later, and it is certainly your prerogative
> > to declare that a bug in me.
>
> I think to make undocumented assumptions about the environment is unkind
> to your would-be users.  Ideally the graphics devices would detect and

I have tried to point that out in most places where I used
alpha-blending in the documentation, but I did miss a few.  I think
part of my job is to educate users about what is possible with R, even
though it might be currently available for their default set up.

> report that, but that is not how support for semi-transparency was added.
> As a by-product of adding limited translucency support on the windows()
> family of devices, they do now warn.

That's great news.

> You also need to check that the extra features work correctly.  I found
> some problems with all the devices I tried that support translucency (or
> at least with device+viewer combinations for pdf and svg).  Issues include
> whether translucent fills are rendered at all, blending translucent
> colours with transparent backgrounds, and the model used (is it the light
> intensity or the perceptual colours that are being blended?).

Could you provide more details about these bugs so that I can look
into the implications for my code?  I haven't seen any problems with
preview or acrobat on the mac.

Regards,

Hadley


From marc_schwartz at comcast.net  Mon Jul  9 16:49:07 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 09 Jul 2007 09:49:07 -0500
Subject: [R] Writing Excel (.xls) files on non-Windows OSs using Perl
In-Reply-To: <47fce0650707090642j1bf4a0a3q45c2eb59ea94bd1c@mail.gmail.com>
References: <1183926037.3706.89.camel@Bellerophon.localdomain>
	<47fce0650707090642j1bf4a0a3q45c2eb59ea94bd1c@mail.gmail.com>
Message-ID: <1183992547.3731.16.camel@Bellerophon.localdomain>

On Mon, 2007-07-09 at 16:42 +0300, Hans-Peter wrote:
> Hi,
> 
> 2007/7/8, Marc Schwartz <marc_schwartz at comcast.net>:
> > [snip]
> > There exists the xlsReadWrite package on CRAN by Hans-Peter Suter, which
> > is restricted to Windows, since it utilizes the non-FOSS MS Office API
> > to write the Excel formats.
> 
> The non-FOSS API is not the problem(#) but its implementation is:
> 
> The 3rd party library I use is written in Pascal and supports Delphi
> and Kylix. Kylix would allow to port the package to Linux but as Kylix
> has unfortunately been abandoned by CodeGear (Borland) I am not
> ready/interested to spend my time on this dead road. Though it
> probably could be done quickly.
> 
> A much more interesting way is to port the package using FreePascal.
> --> I plan to do this since long but...
> --> Maybe someone fluent on Linux and FreePascal could have a look at
> the pascal header files (treetron.googlepages.com) and make the demos
> run on Linux..., that would be great and speed up an eventual
> xlsReadWrite port!

Thanks for the clarification.

However, I think that if you are going to pursue a cross-platform
solution, providing source code requiring compilation (as opposed to a
pre-compiled Windows binary), you should consider what the installation
requirements for your package would then be.

If you are going to take the step of requiring a prospective end-user to
have a particular Pascal compiler in place, you may as well have the
requirement for a Perl interpreter and associated packages. Since Perl
is widely available and you are more likely to find Perl-fluent coders
as opposed to Pascal-fluent coders (eg. I have not used Pascal since the
late 80's), I would urge you to consider Perl as a future substrate for
your functions.

While compiled code will run faster than interpreted code, for these
types of file I/O functions, I am not sure that you lose much with Perl
from a performance standpoint and you certainly gain the eyes of a wider
audience with respect to use, debugging and enhancements.

To that end, you (or any other interested parties) are free to utilize
my code in any way you deem appropriate. I did not state this in my
original post, but I make the code available under GPL(v2), freeing you
from any restrictions in its use, including your "Pro" version, as long
as you make the source available in a fashion consistent with the GPL
requirements.

Regards,

Marc Schwartz


From elw at stderr.org  Mon Jul  9 16:56:45 2007
From: elw at stderr.org (elw at stderr.org)
Date: Mon, 9 Jul 2007 09:56:45 -0500 (CDT)
Subject: [R] Planet R - a weblog aggregator for statistical computing
Message-ID: <Pine.LNX.4.64.0707051542220.9568@illuminati.stderr.org>


Announcing...

Planet R - a weblog aggregator for statistical computing

Q: What is it?

A: An aggregator for weblog posts about statistical computing topics,
    focused primarily around the R community.

Q2: Where is it?

A2: For now, at http://planetr.stderr.org

Q3: What's it good for?

A3: Hopefully, collecting resources and weblog posts from people who might
     otherwise not know about each other.  Community-building, you see?

Q4: How do I get my stuff on there?

A4: Send mail to elw at stderr.org, with a representative subject line (e.g.
     'please add to planet r' will be fine..); I'll need the link to the
     RSS feed of your blog or other resource.  If you'd like a personal
     icon in the style of Planet Debian or Planet Gnome (or one of those
     other planetplanet-based sites..) feel free to send along a smallish
     picture or icon as well. (Think 80-by-80 pixels...)

Q5: What sort of things are there?

A5: A selection of weblogs about statistical computing, a feed from
     omegahat, some bioconductor-related content, feeds from a couple of
     journals (including JoSS content, as a subset of the J Computational
     and Graphical Statistics, as well as Royal Statistical Society
     content sourced from IngentaConnect...), the changes feed from the R
     Wiki, and a few other things of great utility to the R community.


Please feel free to contact me directly with further content, media, 
suggestions, et cetera, that you think would enhance the utility of this 
site as a resource to the community.


thanks,

--elijah wright
indiana university bloomington
school of library and information science


From yee at post.harvard.edu  Mon Jul  9 17:07:39 2007
From: yee at post.harvard.edu (Andrew Yee)
Date: Mon, 9 Jul 2007 11:07:39 -0400
Subject: [R] using the function unique(),
	but asking it to ignore a column of a data.frame
In-Reply-To: <Pine.LNX.4.64.0707091543280.22123@gannet.stats.ox.ac.uk>
References: <5dff5a0d0707090612r2dfd72d9t5533d817c8db9cdf@mail.gmail.com>
	<f8e6ff050707090628m1a422e63v2a7c9715bc571403@mail.gmail.com>
	<5dff5a0d0707090633t74206396u9c522bf20720b0d1@mail.gmail.com>
	<Pine.LNX.4.64.0707091543280.22123@gannet.stats.ox.ac.uk>
Message-ID: <5dff5a0d0707090807l726c7901wab0375b23b885545@mail.gmail.com>

Thanks, this works perfectly.

On 7/9/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Mon, 9 Jul 2007, Andrew Yee wrote:
>
> > Thanks.  But in this specific case, I would like the output to include
> > all three columns, including the "ignored" column (in this case, I'd
> > like it to ignore column a).
>
> sample.data.frame[!duplicated(sample.data.frame[-1]), ]
>
> (index to exclude columns as you wish).
>
> > Thanks,
> > Andrew
> >
> > On 7/9/07, hadley wickham <h.wickham at gmail.com> wrote:
> >> On 7/9/07, Andrew Yee <yee at post.harvard.edu> wrote:
> >>> Take for example the following data.frame:
> >>>
> >>> a<-c(1,1,5)
> >>> b<-c(3,2,3)
> >>> c<-c(5,1,5)
> >>> sample.data.frame<-data.frame(a=a,b=b,c=c)
> >>>
> >>> I'd like to be able to use unique(sample.data.frame), but have
> >>> unique() ignore column a when determining the unique elements.
> >>>
> >>> However, I figured that this would be setting for incomparables=, but
> >>> it appears that this funcationality hasn't been incorporated.  Is
> >>> there a work around for this, i.e. to be able to get unique to only
> >>> look at selected columns of a data frame?
> >>
> >> unique(df[,c("a","c")]) ?
> >>
> >> Hadley
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From ggrothendieck at gmail.com  Mon Jul  9 17:10:51 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 9 Jul 2007 11:10:51 -0400
Subject: [R] Writing Excel (.xls) files on non-Windows OSs using Perl
In-Reply-To: <1183992547.3731.16.camel@Bellerophon.localdomain>
References: <1183926037.3706.89.camel@Bellerophon.localdomain>
	<47fce0650707090642j1bf4a0a3q45c2eb59ea94bd1c@mail.gmail.com>
	<1183992547.3731.16.camel@Bellerophon.localdomain>
Message-ID: <971536df0707090810q645abd13o15352e73f99ffa70@mail.gmail.com>

Note that there already is a function, read.xls, in gdata that uses Perl.

On 7/9/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> On Mon, 2007-07-09 at 16:42 +0300, Hans-Peter wrote:
> > Hi,
> >
> > 2007/7/8, Marc Schwartz <marc_schwartz at comcast.net>:
> > > [snip]
> > > There exists the xlsReadWrite package on CRAN by Hans-Peter Suter, which
> > > is restricted to Windows, since it utilizes the non-FOSS MS Office API
> > > to write the Excel formats.
> >
> > The non-FOSS API is not the problem(#) but its implementation is:
> >
> > The 3rd party library I use is written in Pascal and supports Delphi
> > and Kylix. Kylix would allow to port the package to Linux but as Kylix
> > has unfortunately been abandoned by CodeGear (Borland) I am not
> > ready/interested to spend my time on this dead road. Though it
> > probably could be done quickly.
> >
> > A much more interesting way is to port the package using FreePascal.
> > --> I plan to do this since long but...
> > --> Maybe someone fluent on Linux and FreePascal could have a look at
> > the pascal header files (treetron.googlepages.com) and make the demos
> > run on Linux..., that would be great and speed up an eventual
> > xlsReadWrite port!
>
> Thanks for the clarification.
>
> However, I think that if you are going to pursue a cross-platform
> solution, providing source code requiring compilation (as opposed to a
> pre-compiled Windows binary), you should consider what the installation
> requirements for your package would then be.
>
> If you are going to take the step of requiring a prospective end-user to
> have a particular Pascal compiler in place, you may as well have the
> requirement for a Perl interpreter and associated packages. Since Perl
> is widely available and you are more likely to find Perl-fluent coders
> as opposed to Pascal-fluent coders (eg. I have not used Pascal since the
> late 80's), I would urge you to consider Perl as a future substrate for
> your functions.
>
> While compiled code will run faster than interpreted code, for these
> types of file I/O functions, I am not sure that you lose much with Perl
> from a performance standpoint and you certainly gain the eyes of a wider
> audience with respect to use, debugging and enhancements.
>
> To that end, you (or any other interested parties) are free to utilize
> my code in any way you deem appropriate. I did not state this in my
> original post, but I make the code available under GPL(v2), freeing you
> from any restrictions in its use, including your "Pro" version, as long
> as you make the source available in a fashion consistent with the GPL
> requirements.
>
> Regards,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From shirley0818 at gmail.com  Mon Jul  9 17:16:07 2007
From: shirley0818 at gmail.com (shirley zhang)
Date: Mon, 9 Jul 2007 11:16:07 -0400
Subject: [R] similar limma's contrasts.fit() for lme (mixed effect model)
	object
Message-ID: <6fb73d020707090816u4868ac5dq4176313c868d854a@mail.gmail.com>

Dear R help,

In limma package, contrasts.fit() function is very useful. I am
wondering whether there is a similar function for lme object, which
means given a mixed linear model fit, compute estimated coefficients
and standard errors for a given set of contrasts.

Thanks,
Shirley


From rvaradhan at jhmi.edu  Mon Jul  9 17:27:06 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Mon, 9 Jul 2007 11:27:06 -0400
Subject: [R] No convergence using ADAPT
In-Reply-To: <46981A60@webmail.nau.edu>
References: <46981A60@webmail.nau.edu>
Message-ID: <000301c7c23d$9c0ed5c0$7c94100a@win.ad.jhu.edu>

Since the covariance is zero (i.e. you have independent normals), you can
simplify your problem so that you just need to perform one-dimensional
integration.  Here is how you can do it:


trial2 <- function(input) {
#pmvnorm(lower = c(0,0), upper = c(10, 10), mean = input, sigma =
matrix(c(.01, 0, 0, .01), nrow = 2, ncol = 2, byrow = FALSE)) 
pnorm(q=10, mean = input, sd = sqrt(.01)) - pnorm(q=0, mean = input, sd =
sqrt(.01))
}

bottomB <- -5*sqrt(.01)
topB <- 10 + 5*sqrt(.01)
areaB <- (topB - bottomB)^2

new.ans <- 1/areaB * (integrate(f=trial2, lo = bottomB, up = topB)$val)^2
> new.ans
[1] 0.8264463

Hope this is helpful,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Philip Turk
Sent: Saturday, July 07, 2007 3:20 PM
To: r-help at stat.math.ethz.ch
Subject: [R] No convergence using ADAPT

I am trying calculate a probability using numerical integration. The first 
program I ran spit out an answer in a very short time. The program is below:

## START PROGRAM

trial <- function(input)

{
pmvnorm(lower = c(0,0), upper = c(2, 2), mean = input, sigma = matrix(c(.1,
0, 
0, .1), nrow = 2, ncol = 2, byrow = FALSE)) 
}

require(mvtnorm)
require(adapt)

bottomB <- -5*sqrt(.1)
topB <- 2 + 5*sqrt(.1)
areaB <- (topB - bottomB)^2

unscaled.Po.in.a <- adapt(2, lo = c(bottomB, bottomB), up = c(topB, topB), 
minpts = 1000, eps = 1e-4, functn = trial)

(1/areaB)*unscaled.Po.in.a$value

## FINISH PROGRAM

I tried to run the program again changing a.) sigma in the trial function,
b.) 
upper in the trial function, and c.) the bounds of integration; that is, 
bottomB and topB.  The new program is below:

## START PROGRAM

trial <- function(input)

{
pmvnorm(lower = c(0,0), upper = c(10, 10), mean = input, sigma =
matrix(c(.01, 
0, 0, .01), nrow = 2, ncol = 2, byrow = FALSE)) 
}

require(mvtnorm)
require(adapt)

bottomB <- -5*sqrt(.01)
topB <- 10 + 5*sqrt(.01)
areaB <- (topB - bottomB)^2

unscaled.Po.in.a <- adapt(2, lo = c(bottomB, bottomB), up = c(topB, topB), 
minpts = 1000, eps = 1e-4, functn = trial)

(1/areaB)*unscaled.Po.in.a$value

## FINISH PROGRAM

Now, the program just runs and runs (48 hours at last count!).  By playing 
around with the program, I have deduced the program is highly sensitive to 
changing the upper option in the trial function.  For example, using a
vector 
like (4, 4) causes no problems and the program quickly yields an answer.  I 
have a couple of other programs where I can easily obtain a simulation-based

answer, but I would ultimately like to know what's up with this program
before 
I give up on it so I can learn a thing or two.  Does anyone have any clues
or 
tricks to get around this problem?  My guess is that it will simply be very 
difficult (impossible?) to obtain this type of relative error (eps = 1e-4)
and 
I will have no choice but to pursue the simulation approach.

Thanks for any responses (philip.turk at nau.edu)!

-- Phil

Philip Turk
Assistant Professor of Statistics
Northern Arizona University
Department of Mathematics and Statistics
PO Box 5717
Flagstaff, AZ 86011
Phone: 928-523-6884
Fax: 928-523-5847
E-mail: Philip.Turk at nau.edu
Web Site: http://jan.ucc.nau.edu/~stapjt-p/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Max.Kuhn at pfizer.com  Mon Jul  9 17:27:50 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Mon, 9 Jul 2007 11:27:50 -0400
Subject: [R] similar limma's contrasts.fit() for lme (mixed effect
	model)object
In-Reply-To: <6fb73d020707090816u4868ac5dq4176313c868d854a@mail.gmail.com>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D308DE6E6F@groamrexm03.amer.pfizer.com>

Shirley,

The contrast package can do this. The method of specifying the contrast
conditions/coefficients is different (and I think easier).

Max


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of shirley zhang
Sent: Monday, July 09, 2007 11:16 AM
To: R-help at stat.math.ethz.ch
Subject: [R] similar limma's contrasts.fit() for lme (mixed effect
model)object

Dear R help,

In limma package, contrasts.fit() function is very useful. I am
wondering whether there is a similar function for lme object, which
means given a mixed linear model fit, compute estimated coefficients
and standard errors for a given set of contrasts.

Thanks,
Shirley

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From sbearer at tnc.org  Mon Jul  9 17:38:07 2007
From: sbearer at tnc.org (Scott Bearer)
Date: Mon, 9 Jul 2007 11:38:07 -0400
Subject: [R] Clustering nested data
Message-ID: <FOEPJOMFPIAGCACENKDMAEJECEAA.sbearer@tnc.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/0b7fb683/attachment.pl 

From shirley0818 at gmail.com  Mon Jul  9 17:49:40 2007
From: shirley0818 at gmail.com (shirley zhang)
Date: Mon, 9 Jul 2007 11:49:40 -0400
Subject: [R] similar limma's contrasts.fit() for lme (mixed effect
	model)object
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D308DE6E6F@groamrexm03.amer.pfizer.com>
References: <6fb73d020707090816u4868ac5dq4176313c868d854a@mail.gmail.com>
	<71257D09F114DA4A8E134DEAC70F25D308DE6E6F@groamrexm03.amer.pfizer.com>
Message-ID: <6fb73d020707090849l11e7e2b8r446c6f9f57d4a724@mail.gmail.com>

Hi Max,

Thanks for your prompt reply. Actually I have already checked contrast
package, but I still could not figure out how to set the contrast
matrix for a 2x2 factorial design.

I would like to set a contrast exactly similar to the limma's user
guide, 8.7 Factorial Designs (page 45). For example, after a mixed
linear model fit with the default contr.treatment, I want to compute
an estimated coefficient as the sum of the third and fourth
coefficients of the original lme model (2x2 factorial design).

Could you give me an example how to implement this using  the contrast package?

Thanks,
Shirley

On 7/9/07, Kuhn, Max <Max.Kuhn at pfizer.com> wrote:
> Shirley,
>
> The contrast package can do this. The method of specifying the contrast
> conditions/coefficients is different (and I think easier).
>
> Max
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of shirley zhang
> Sent: Monday, July 09, 2007 11:16 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] similar limma's contrasts.fit() for lme (mixed effect
> model)object
>
> Dear R help,
>
> In limma package, contrasts.fit() function is very useful. I am
> wondering whether there is a similar function for lme object, which
> means given a mixed linear model fit, compute estimated coefficients
> and standard errors for a given set of contrasts.
>
> Thanks,
> Shirley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ----------------------------------------------------------------------
> LEGAL NOTICE
> Unless expressly stated otherwise, this message is confidential and may be privileged.  It is intended for the addressee(s) only.  Access to this E-mail by anyone else is unauthorized.  If you are not an addressee, any disclosure or copying of the contents of this E-mail or any action taken (or not taken) in reliance on it is unauthorized and may be unlawful.  If you are not an addressee, please inform the sender immediately.
>


From ligges at statistik.uni-dortmund.de  Mon Jul  9 18:15:41 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Jul 2007 18:15:41 +0200
Subject: [R] Writing Excel (.xls) files on non-Windows OSs using Perl
In-Reply-To: <971536df0707090810q645abd13o15352e73f99ffa70@mail.gmail.com>
References: <1183926037.3706.89.camel@Bellerophon.localdomain>	<47fce0650707090642j1bf4a0a3q45c2eb59ea94bd1c@mail.gmail.com>	<1183992547.3731.16.camel@Bellerophon.localdomain>
	<971536df0707090810q645abd13o15352e73f99ffa70@mail.gmail.com>
Message-ID: <46925F2D.1060902@statistik.uni-dortmund.de>



Gabor Grothendieck wrote:
> Note that there already is a function, read.xls, in gdata that uses Perl.

Note that Marc talked about *writing* in his original message.

Uwe Ligges


> On 7/9/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
>> On Mon, 2007-07-09 at 16:42 +0300, Hans-Peter wrote:
>>> Hi,
>>>
>>> 2007/7/8, Marc Schwartz <marc_schwartz at comcast.net>:
>>>> [snip]
>>>> There exists the xlsReadWrite package on CRAN by Hans-Peter Suter, which
>>>> is restricted to Windows, since it utilizes the non-FOSS MS Office API
>>>> to write the Excel formats.
>>> The non-FOSS API is not the problem(#) but its implementation is:
>>>
>>> The 3rd party library I use is written in Pascal and supports Delphi
>>> and Kylix. Kylix would allow to port the package to Linux but as Kylix
>>> has unfortunately been abandoned by CodeGear (Borland) I am not
>>> ready/interested to spend my time on this dead road. Though it
>>> probably could be done quickly.
>>>
>>> A much more interesting way is to port the package using FreePascal.
>>> --> I plan to do this since long but...
>>> --> Maybe someone fluent on Linux and FreePascal could have a look at
>>> the pascal header files (treetron.googlepages.com) and make the demos
>>> run on Linux..., that would be great and speed up an eventual
>>> xlsReadWrite port!
>> Thanks for the clarification.
>>
>> However, I think that if you are going to pursue a cross-platform
>> solution, providing source code requiring compilation (as opposed to a
>> pre-compiled Windows binary), you should consider what the installation
>> requirements for your package would then be.
>>
>> If you are going to take the step of requiring a prospective end-user to
>> have a particular Pascal compiler in place, you may as well have the
>> requirement for a Perl interpreter and associated packages. Since Perl
>> is widely available and you are more likely to find Perl-fluent coders
>> as opposed to Pascal-fluent coders (eg. I have not used Pascal since the
>> late 80's), I would urge you to consider Perl as a future substrate for
>> your functions.
>>
>> While compiled code will run faster than interpreted code, for these
>> types of file I/O functions, I am not sure that you lose much with Perl
>> from a performance standpoint and you certainly gain the eyes of a wider
>> audience with respect to use, debugging and enhancements.
>>
>> To that end, you (or any other interested parties) are free to utilize
>> my code in any way you deem appropriate. I did not state this in my
>> original post, but I make the code available under GPL(v2), freeing you
>> from any restrictions in its use, including your "Pro" version, as long
>> as you make the source available in a fashion consistent with the GPL
>> requirements.
>>
>> Regards,
>>
>> Marc Schwartz
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From thomas.pujol at yahoo.com  Mon Jul  9 18:31:27 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Mon, 9 Jul 2007 09:31:27 -0700 (PDT)
Subject: [R] calculating p-values of columns in a dataframe
In-Reply-To: <4690C471.3060603@statistik.uni-dortmund.de>
Message-ID: <991666.65864.qm@web59308.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/77284103/attachment.pl 

From edd at debian.org  Mon Jul  9 18:41:55 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 9 Jul 2007 11:41:55 -0500
Subject: [R] [R-pkgs] CRANberries -- An RSS feed about New and Updated
	CRAN	packages
Message-ID: <18066.25939.77583.800991@basebud.nulle.part>


Announcing CRANberries -- An RSS feed about New and Updated CRAN packages

A new RSS feed [1] is now available that summarizes uploads to CRAN.  This
makes it possibly to quickly obtain concise information about which (of the
now over one thousand !!) packages were added or updated at CRAN and its
mirrors.

To this end, two basic variants are provided:
 - a feed for new packages where we display the DESCRIPTION file
 - a feed for updated packages where we display the output of diffstat(1) 
   between the old and new source tar archives.
  
As the URLs for these are in a hierarchy, one can subscribe to both or
individual feeds.  The URLs are as follows:

   Everything
	http://dirk.eddelbuettel.com/cranberries/index.rss

   Just CRAN (so far the same as All)
	http://dirk.eddelbuettel.com/cranberries/cran/index.rss

   New CRAN packages
	http://dirk.eddelbuettel.com/cranberries/cran/new/index.rss

   Updated CRAN packages
	http://dirk.eddelbuettel.com/cranberries/cran/updated/index.rss

but the easiest way may just be to subscribe to Elijah's wonderful 'Planet R'
feed aggregator which already sources the 'Everything' variant above.  Beside
giving you lots of other R information, it also points to a more reliable
back-end than my small server at home.

Lastly, I could add other repositories. However, to provide updates in the
current format, my code relies on some CRAN features not available on all
other repos (i.e an Archive/ section with old tarballs, and the various
Descriptions/$package.DESCRIPTION files).

For the technically inclined, this is implemented using a few lines of R
executed by littler [2] storing data via R/DBI in a SQLite db and writing
simple text files that are then aggregated by the Blosxom [3] blog engine.

Comments, questions, criticism most welcome.

Best regards,  Dirk

[1] See the Wikipedia entry at http://en.wikipedia.org/wiki/Rss if that term
    is unfamiliar. RSS feeds can be read in web browsers, numerous stand-alone
    applications, or web-services such as Google Reader.

[2] See http://dirk.eddelbuettel.com/code/littler.html

[3] See http://blosxom.sourceforge.net/ and http://blosxom.sourceforge.net/
    but not that Blosxom development seems to have ceased. There are many 
    alternatives such as PyBlosxom and Nanoblogger.


-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From b.rowlingson at lancaster.ac.uk  Mon Jul  9 19:10:13 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 09 Jul 2007 18:10:13 +0100
Subject: [R] [R-pkgs] CRANberries -- An RSS feed about New and Updated
 CRAN	packages
In-Reply-To: <18066.25939.77583.800991@basebud.nulle.part>
References: <18066.25939.77583.800991@basebud.nulle.part>
Message-ID: <46926BF5.9010802@lancaster.ac.uk>

Dirk Eddelbuettel wrote:

> but the easiest way may just be to subscribe to Elijah's wonderful 'Planet R'
> feed aggregator 

My favourite RSS reader at the moment is the RSS cat caption generator:

http://lol.ianloic.com/feed/dirk.eddelbuettel.com/cranberries/index.rss

Barry


From h.wickham at gmail.com  Mon Jul  9 19:23:21 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 9 Jul 2007 19:23:21 +0200
Subject: [R] [R-pkgs] Reshape version 0.8
Message-ID: <f8e6ff050707091023j2818c1d6gcfb763e1c5e81b09@mail.gmail.com>

Reshape version 0.8
http://had.co.nz/reshape

Reshape is an R package for flexibly restructuring and aggregating
data.  It's inspired by Excel's pivot tables, and it (hopefully) makes
it very easy to get your data into the shape that you want.  You can find out
more at http://had.co.nz/reshape

This version brings a few minor changes to make the output more
attractive and less surprising.  If you have any code that relies on
the exact output structure you might need to tweak it a little.

* preserve.na renamed to na.rm to be consistent with other R functions

* Column names are no longer automatically converted to valid R names.
You may need to use `` (those are backticks) to access these names.

* Margins now displayed with (all) instead of NA

* melt.array can now deal with cases where there are partial dimnames
- Thanks to Roberto Ugoccioni

 * Added the Smiths dataset to the package

 * Fixed a bug when displaying margins with multiple result variables

Regards,

Hadley

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From Max.Kuhn at pfizer.com  Mon Jul  9 19:34:18 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Mon, 9 Jul 2007 13:34:18 -0400
Subject: [R] similar limma's contrasts.fit() for lme (mixed effect
	model)object
In-Reply-To: <6fb73d020707090849l11e7e2b8r446c6f9f57d4a724@mail.gmail.com>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D308DE70B1@groamrexm03.amer.pfizer.com>

Shirley,

Well, you picked the type of contrast that proves my statement wrong.
The interface in the contrast package (and the Design package) may not
be well suited for the type that you are interested in (or other types,
such as SAS's "lsmeans" type contrasts).

The good news is that the function contrast:::contrastCalc currently
does most of the work for all types of models. A modified version of
that function can be used for your situation. The top part of the
function works to generate two objects: xa and xb. Looking at the limma
documentation, this might correspond to vectors given for WT.SvsU and
Mu.SvsU. 

I'll attach the modified version in an off-list email with an lme
example. (and I'll think about how to let the contrasts be specified by
the coefficient vectors for a future release)

Max


-----Original Message-----
From: shirley zhang [mailto:shirley0818 at gmail.com] 
Sent: Monday, July 09, 2007 11:50 AM
To: Kuhn, Max
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] similar limma's contrasts.fit() for lme (mixed effect
model)object

Hi Max,

Thanks for your prompt reply. Actually I have already checked contrast
package, but I still could not figure out how to set the contrast
matrix for a 2x2 factorial design.

I would like to set a contrast exactly similar to the limma's user
guide, 8.7 Factorial Designs (page 45). For example, after a mixed
linear model fit with the default contr.treatment, I want to compute
an estimated coefficient as the sum of the third and fourth
coefficients of the original lme model (2x2 factorial design).

Could you give me an example how to implement this using  the contrast
package?

Thanks,
Shirley

On 7/9/07, Kuhn, Max <Max.Kuhn at pfizer.com> wrote:
> Shirley,
>
> The contrast package can do this. The method of specifying the
contrast
> conditions/coefficients is different (and I think easier).
>
> Max
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of shirley zhang
> Sent: Monday, July 09, 2007 11:16 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] similar limma's contrasts.fit() for lme (mixed effect
> model)object
>
> Dear R help,
>
> In limma package, contrasts.fit() function is very useful. I am
> wondering whether there is a similar function for lme object, which
> means given a mixed linear model fit, compute estimated coefficients
> and standard errors for a given set of contrasts.
>
> Thanks,
> Shirley
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ----------------------------------------------------------------------
> LEGAL NOTICE
> Unless expressly stated otherwise, this message is confidential and
may be privileged.  It is intended for the addressee(s) only.  Access to
this E-mail by anyone else is unauthorized.  If you are not an
addressee, any disclosure or copying of the contents of this E-mail or
any action taken (or not taken) in reliance on it is unauthorized and
may be unlawful.  If you are not an addressee, please inform the sender
immediately.
>


From abastos at berkeley.edu  Mon Jul  9 20:04:27 2007
From: abastos at berkeley.edu (Andre Bastos)
Date: Mon, 9 Jul 2007 11:04:27 -0700
Subject: [R] When is the periodogram is consistent with white noise?
Message-ID: <00a601c7c253$98bd85e0$ca3891a0$@edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/554cc8e3/attachment.pl 

From sbearer at tnc.org  Mon Jul  9 20:06:24 2007
From: sbearer at tnc.org (Scott Bearer)
Date: Mon, 9 Jul 2007 14:06:24 -0400
Subject: [R] Clustering nested data
Message-ID: <FOEPJOMFPIAGCACENKDMKEJGCEAA.sbearer@tnc.org>

My apologies for cross-postings

Hi all,

I am interested in performing a cluster analysis on ecological data from
forests in Pennsylvania.  I would like to develop definitions for forest
types (red maple forests, upland oak forests, etc.(AH AR in attached table))
based on measured attributes in each forest type.  To do this, I would like
to 'draw clusters' around forest types based on information from various
tree species (red maple, red oak, etc.(837, 832 in attached table))
occurring in those forests.  Each row of data includes mean values on a
particular species occurring within a forest type at a particular site.  In
other words, if we monitored 10 sites in red maple forests, we would only
have 10 rows of data for the tree species 'red maple', even though we
measured 100 trees.

I have used classification trees to examine this data, which I like because
of it's predictive abilities for later 'unknown' datasets.  However, my
concern is that the mean species attributes (columns Diameter:Avgnumtrees in
attached table) are associated with the tree species (nested?)(column
Treespecies in attached table) and are not independent attributes, but are
directly associated with the species listed in that row.

My question is, what is the best way to conduct a clustering (I have also
tried hclust, cclust and flexclust) or CART model with this sort of nested
data?
Also, what is the preferrable method for predicting a new dataset once these
clusters or CART models have been developed?

Any help would be greatly appreciated.

Kind regards,
Scott

> head(data_hal_dom, 15)
ForestType	COMMON_NAME	BasalArea	TreesperAcre	DeadperAcre	VolumeperAcre
BiomassperAcre	AverageDiameter		STDERRDIAM	AVGHT	STDERRHT	AVGNUMTREES
AH	blackoak	50	31.5	25.1	NA	950.9	47955	15.1	1.1	86.8	15.2	4
AH	chestnutoak	50	11.2	12	NA	231.9	16713.8	13.1	0.3	55	4.2	2
AH	northern	oak	50	45.3	37.6	NA	1319.7	82508.2	14.7	0.9	81.5	7	6
AH	redmaple	50	51.9	66.2	NA	1564.4	60960.9	12	0.2	70.3	2.5	3
AH	redpine	50	8.8	9.3	NA	189.4	8106.9	13.2	0	42	0	1
AH	scarletoak	50	41.2	27.9	NA	1211	67645.6	16.3	1.5	80.3	12.4	3
AH	whiteoak	50	10.4	9.2	NA	264.1	15738.6	14.4	0.3	73.3	0	1.3
AR	northern	oak	50	47.2	30.1	12	1506.4	93490	16.9	0.9	84.2	10.7	5
AR	paperbirch	50	7.5	6	NA	243.7	9637	15.1	0	77	0	1
AR	redmaple	50	7.1	6	6	226.7	9102.2	14.6	0	75	0	1
AR	sweetbirch	50	4.7	6	NA	146.3	6676.2	12	0	75.5	0	1
AR	whiteash	50	6.8	6	NA	261.5	9474.5	14.4	0	106	0	1
AR	yellow-poplar	50	23.8	18.1	NA	962.1	28302.8	15.3	2.1	99.3	6.8	3
AR	easternhemlock	70	16.6	6	NA	512.6	17125.8	22.5	0	94	0	1
AR	northern	oak	70	16.2	6	12	583.4	38060.4	22.2	0	110	0	1

Scott Bearer
Forest Ecologist
The Nature Conservancy
 in Pennsylvania
Community Arts Center
220 West Fourth Street, 3rd Floor
Williamsport, PA  17701


From jim at bitwrit.com.au  Mon Jul  9 20:24:12 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 10 Jul 2007 04:24:12 +1000
Subject: [R] character string to name
In-Reply-To: <469227A0.4070902@bitwrit.com.au>
References: <469227A0.4070902@bitwrit.com.au>
Message-ID: <46927D4C.9060701@bitwrit.com.au>

Jim Lemon wrote:
> Hi folks,
> 
> I thought I recalled a request for turning a character string into an 
> object name as in:
> 
> x$as.name("y")<-1:4
 > ...
Thanks to those who replied to this eminently dumb question.

OT:
I put this down to the Euthermic Model of Cognitive Efficiency. This 
model, recently developed in Sydney, Australia, proposes that cognitive 
efficiency is maximized at the most comfortable ambient temperature for 
the thinker. Thus after riding home in the cold Sydney rain last night, 
I was unable to think of the simplest answer:

names(x)<-"y"

until I had gone to bed and warmed up. Then I awoke in the early morning 
and thought of Gabor's suggestion:

x[["y"]]<-...

I felt compelled to arise and publish this amazing theory, that not only 
explains why the rate of intellectual and technical innovation is 
greatest in the temperate climes, but more importantly, gives me an 
excuse for asking the question.

Jim


From topkatz at msn.com  Mon Jul  9 21:10:17 2007
From: topkatz at msn.com (Talbot Katz)
Date: Mon, 09 Jul 2007 15:10:17 -0400
Subject: [R] factanal frustration!
Message-ID: <BAY108-F222CF1A0249C26466BE997AA060@phx.gbl>

Hi.

It seems that nearly every time I try to use factanal I get the following 
response:

>faa2db1<-factanal(mretdb1,factors=2,method="mle",control=list(nstart=25))
Error in factanal(mretdb1, factors = 2, method = "mle", control = 
list(nstart = 25)) :
        unable to optimize from these starting value(s)
>

In the case cited above, mretdb1 is synthetic data created from two factors, 
that is, mretdb1 has dimensions 3050*1000, and each of the thousand columns 
has the form
c[i]=a1[i]*f1 + a2[i]*f2 + a3[i]*fn[,i]
where a1, a2, a3 are each vectors of length 1000, f1 and f2 are vectors of 
length 3050, and fn is a matrix of dimension 3050*1000, such that f1, f2, 
and all the columns of fn are mutually uncorrelated.  I wanted to see how 
closely the factors returned by factanal are to the ones that generated the 
data.  But, as you can see, factanal hasn't cooperated!

I would greatly appreciate advice from factanal users, especially those who 
have experienced the same error.  Thanks!

--  TMK  --
917-656-5351	cell
212-460-5430	home


From alex.baugh at gmail.com  Mon Jul  9 21:12:53 2007
From: alex.baugh at gmail.com (Alex Baugh)
Date: Mon, 9 Jul 2007 14:12:53 -0500
Subject: [R] ANOVA: Does a Between-Subjects Factor belong in the Error Term?
Message-ID: <b91a7cd80707091212h1bb26c2aj5d12aa9d9bf6061a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/3f7e2674/attachment.pl 

From jholtman at gmail.com  Mon Jul  9 21:33:44 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 9 Jul 2007 15:33:44 -0400
Subject: [R] a little problem on selecting a subset from dataset A
	accordingto dataset B?
In-Reply-To: <B998A44C8986644EA8029CFE6396A924B6803D@exqld2-bne.nexus.csiro.au>
References: <B998A44C8986644EA8029CFE6396A924B6803D@exqld2-bne.nexus.csiro.au>
Message-ID: <644e1f320707091233n7c397e9esdd20654e7c27f073@mail.gmail.com>

You might want to be careful since what you are comparing is floating
point numbers.  You might want to scale them and then convert to
integers to make sure that you are getting the numbers you think you
should be getting. (FAQ 7.31)

On 7/9/07, Bill.Venables at csiro.au <Bill.Venables at csiro.au> wrote:
> > AB <- with(B, subset(A, coords.x1 %in% X1))
> > AB
>   coords.x1 coords.x2
> 0   542250.9   3392404
> 7   541512.5   3394722
> 8   541479.3   3394878
> 9   538903.4   3395943
> 18  543274.0   3389919
> 19  543840.8   3392012
>
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of zhijie zhang
> Sent: Monday, 9 July 2007 2:43 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] a little problem on selecting a subset from dataset A
> accordingto dataset B?
>
> Dear Friends,
>   I want to extract the records from A according to B, but the results
> are
> not correct because R says :
>  The length of long object is not integer times on the length of short
> object.
>  Anybody have met the same problem? How to do it correctly?
>
> length(A)=47
> length(B)=6
>
> A[A$coords.x1==B$X1,]   #the program for the above task. I should get 6
> records, but i only get former 4 records for the above reason.
>
> Thanks.
>  The folloing shows dataset A and B.
>
>
> > A
>   coords.x1 coords.x2
> 0  542250.89 3392404.1
> 1  538813.87 3388339.0
> 2  536049.19 3385821.6
> 3  533659.62 3383194.2
> 4  530642.30 3376834.9
> 5  529573.15 3378177.8
> 6  530853.82 3394838.8
> 7  541512.51 3394721.6
> 8  541479.33 3394877.8
> 9  538903.39 3395942.5
> 10 536019.95 3396286.1
> 11 538675.23 3384213.2
> 12 535127.95 3381255.4
> 13 533852.24 3378660.4
> 14 531360.91 3379273.8
> 15 539289.14 3375759.8
> 16 543410.51 3384353.1
> 17 543089.27 3388170.1
> 18 543274.03 3389919.2
> 19 543840.77 3392012.4
> 20 553383.55 3402401.8
> 21 554621.51 3397938.9
> 22 564096.42 3397524.4
> 23 567529.64 3398702.9
> 24 561798.76 3404864.0
> 25 562868.34 3405502.2
> 26 563145.22 3403192.1
> 27 562419.87 3404090.4
> 28 558321.85 3403879.9
> 29 567050.74 3404973.1
> 30 570609.70 3408742.4
> 31 556777.57 3397858.0
> 32 531353.38 3368596.6
> 33 533513.50 3372749.3
> 34 537543.19 3364284.8
> 35 538779.41 3368224.8
> 36 525930.09 3374067.7
> 37 522990.85 3369213.1
> 38 528826.37 3359019.0
> 39 533865.85 3362595.4
> 40 531200.25 3365053.0
> 41 551054.10 3377181.3
> 42 546974.19 3369284.8
> 43 572315.59 3359541.1
> 44 562703.63 3355173.4
> 45 558959.31 3357804.4
> 46 558531.39 3361741.1
>
>
> > B
>         X1        X2
> 1 542250.89 3392404.1
> 2 541512.51 3394721.6
> 3 541479.33 3394877.8
> 4 538903.39 3395942.5
> 5 543274.03 3389919.2
> 6 543840.77 3392012.4
>
> --
> With Kind Regards,
>
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::
> [***********************************************************************
> ]
> Zhi Jie,Zhang ,PHD
> Tel:86-21-54237149
> Dept. of Epidemiology,School of Public Health,Fudan University
> Address:No. 138 Yi Xue Yuan Road,Shanghai,China
> Postcode:200032
> Email:epistat at gmail.com
> Website: www.statABC.com
> [***********************************************************************
> ]
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From duncan at wald.ucdavis.edu  Mon Jul  9 21:54:23 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 09 Jul 2007 12:54:23 -0700
Subject: [R] Loading problem with XML_1.9
In-Reply-To: <701633.8753.qm@web32503.mail.mud.yahoo.com>
References: <701633.8753.qm@web32503.mail.mud.yahoo.com>
Message-ID: <4692926F.9080306@wald.ucdavis.edu>


Weijun and I corresponded off-list so that I could
get a copy of the data.

On a relatively modest machine with 2G of RAM, 10G swap,
dual core 1Ghz 64bit AMDs, the code below takes approximately
100 seconds. It is not optimized in any particular way, so
there is room for improvement.

doc <- xmlTreeParse("mi1.txt.gz", useInternal = TRUE)
mols <- getNodeSet(doc, "//molecule")

ans =
lapply(mols,
        function(node, targets) {

           names = as.character(xpathApply(node, "//name/text()",
                                             xmlValue))
           if(any(names %in% targets))
             xpathApply(node, "//moleculeName", xmlValue)
           else
             character()
        }, c("Vps4b", "SKD1", "frm-1"))

ans = ans[sapply(ans, length) > 0]


We can read the file without uncompressing which probably
slows things down slightly.
The parsing of the tree takes about 20 seconds and occupies
approximately 1G (very roughly).
Then we find all the <molecule> nodes of which there
are 25452.  Then we loop over each of these and
do a sub-query using XPath and see if
the text child in the <name> nodes are in the set
of interest (entirely made up for my test), and if so
fetch the content of any <moleculeName> within this
<molecule> node.

It would be nice if we could build the hash for targets
just once.
And we could get clever with the XPath query to try do
the matching and selection in one query. This might actually slow things
down.

(There are garbage collection issues with XPath sub-queries
for which I am still deciding on the optimal strategy.)

So perhaps the lesson her is that for those working with XML,
XPath is worth using before more specialized approaches
and large XML data files can fit into memory. The tree
is not using contiguous memory so nodes can be squeezed into
available spaces.

D.


Luo Weijun wrote:
> Thanks, Dr. Lang,
> I used xmlEventParse() + branches concept as you
> suggested, it really works, and the memory issue is
> gone. Now I can query large XML files from within R.
> but here is another problem: it is too slows (a simple
> query has not finished for 1.5h), even though the
> number of relevant records is very limited, but the
> whole XML file has more than 500 thousand
> similarly-structured records. And the parser has to go
> through all of them as to find the matches. Attached
> is part of the XML files with two records. I am trying
> to retrieve the content of <moleculeName> nodes from
> <molecule> records where <name> nodes bear specific
> gene names.
> Is it possible to locate based on node content (or
> xmlValue) rather than node names (since they are the
> same in all records) first and then parse the xml
> record locally? Would query based on XPath be faster
> in this case? I understand that we do have the
> facility in the XML package for XPath based queries,
> called getNodeSet(). But that requires reading the
> whole XML tree into the memory first, which is not
> feasible for my large XML file. Or can I call
> XML::XPath statements using your R-Perl interface
> package? Any suggestions/thoughts? Thank you!
> Weijun
> 
> 
> Part of my XML file: 
> 
> <molecule>
> <prov><im><imid>20</imid></im></prov><moleculeID>119043</moleculeID>
> <moleculeType>protein<prov><im><imid>20</imid></im></prov></moleculeType>
> <organismID>10090<prov><im><imid>20</imid></im></prov></organismID>
> <id><prov><im><imid>20</imid></im></prov><idType>GI</idType><idValue>6677981</idValue></id>
> <name>SKD1<prov><im><imid>20</imid></im></prov></name>
> <name>Vps4b<prov><im><imid>20</imid></im></prov></name>
> <name>8030489C12Rik<prov><im><imid>20</imid></im></prov></name>
> <description><distribution><value>Mouse homologue of
> yeast Vacuolar protein sorting 4 (Vps4); Suppressor of
> potassium transport defect 1. Mem
> ber of mammalian class E Vps proteins involved in
> endosomal transport; AAA-type
> ATPase.<prov><im><imid>20</imid></im></prov></value><value>Mo
> use homologue of yeast  Vacuolar protein sorting 4
> (Vps4); Suppressor of potassium  transport defect 1.
> Member of  mammalian class E Vps prot
> eins involved in endosomal transport; AAA-type
> ATPase.<prov><im><imid>20</imid></im></prov></value></distribution></description>
> <orthologue>
> <method><methodID>337974</methodID><methodName>miClust80</methodName></method>
> </orthologue>
> <variant>
> <prov><im><imid>20</imid></im></prov><variantID>0</variantID>
> </variant>
> <interaction><interactionRef>201581</interactionRef><moleculeRef>89434</moleculeRef><moleculeName>SBP1</moleculeName>
> <selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
> <interaction><interactionRef>201582</interactionRef><moleculeRef>17953</moleculeRef><moleculeName>mVps2</moleculeName>
> <selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
> </molecule>
> 
> <molecule>
> <prov><im><imid>30</imid></im></prov><moleculeID>116226</moleculeID>
> <moleculeType>protein<prov><im><imid>30</imid></im></prov></moleculeType>
> <organismID>9606<prov><im><imid>30</imid></im></prov></organismID>
> <id><prov><im><imid>30</imid></im></prov><idType>HGNC</idType><idValue>9859</idValue></id>
> <name>RAP1GDS1<prov><im><imid>30</imid></im></prov></name>
> <name>GDS1<prov><im><imid>30</imid></im></prov></name>
> <name>MGC118859<prov><im><imid>30</imid></im></prov></name>
> <name>MGC118861<prov><im><imid>30</imid></im></prov></name>
> <variant>
> <prov><im><imid>30</imid></im></prov><variantID>0</variantID>
> </variant>
> <interaction><interactionRef>93569</interactionRef><moleculeRef>116280</moleculeRef><moleculeName>RAC1</moleculeName>
> <selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
> <interaction><interactionRef>104132</interactionRef><moleculeRef>103040</moleculeRef><moleculeName>RHOA</moleculeName>
> <selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
> <interaction><interactionRef>121818</interactionRef><moleculeRef>74726</moleculeRef><moleculeName>MBIP</moleculeName>
> <selfVariantRef>0</selfVariantRef><partnerVariantRef>0</partnerVariantRef></interaction>
> </molecule>
> 
> --- Duncan Temple Lang <duncan at wald.ucdavis.edu>
> wrote:
> 
>> Well, as you mention at the end of the mail,
>> several people have given you suggestions about
>> how to solve the problem using different approaches.
>> You might search on the Web for how to install a 64
>> bit version of libxml2?
>> Using xmlTreeParse(, useInternalNodes = TRUE) is an
>> approach
>> to reducing the memory consumption as is using the
>> handlers
>> argument. And if size is really the issue, you
>> should consider
>> the SAX model which is very memory efficient and
>> made available
>> via the xmlEventParse() function in the XML package.
>> And it even provides the concepts of branches to
>> provide a
>> hybrid of SAX and DOM-style parsing together.
>>
>> However, to solve the problem of the xmlMemDisplay
>> symbol not being found, you can look for where
>> that is used and remove it.    It is in
>> src/DocParse.c
>> in the routine RS_XML_MemoryShow().  You can remove
>> the line
>>   xmlMemDisplay(stderr)
>> or indeed the entire routine.  Then re-install and
>> reload the package.
>>
>>  D.
>>
>>
>> Luo Weijun wrote:
>>> Hello Dr. Lang and all,
>>> I posted this message in R-help mail list, but
>> haven???t
>>> solved my problem so far. Therefore, could you
>> help me
>>> look at it?
>>> I have loading problem with XML_1.9 under 64 bit
>>> R2.3.1 for Mac OS X, which I got from
>>> http://R.research.att.com/. XML_1.9 works fine
>> under
>>> 32 bit R2.5.0. I thought that could be
>> installation
>>> problem, and I tried install.packages or biocLite,
>>> every time the package installed fine, except some
>>> warning messages below:
>>> ld64 warning: in /usr/lib/libxml2.dylib, file does
>> not
>>> contain requested architecture
>>> ld64 warning: in /usr/lib/libz.dylib, file does
>> not
>>> contain requested architecture
>>> ld64 warning: in /usr/lib/libiconv.dylib, file
>> does
>>> not contain requested architecture
>>> ld64 warning: in /usr/lib/libz.dylib, file does
>> not
>>> contain requested architecture
>>> ld64 warning: in /usr/lib/libxml2.dylib, file does
>> not
>>> contain requested architecture
>>>
>>> Here is the error messages I got, when XML is
>> loaded:
>>>> library(XML)
>>> Error in dyn.load(x, as.logical(local),
>>> as.logical(now)) : 
>>>         unable to load shared library
>>> '/usr/local/lib64/R/library/XML/libs/XML.so':
>>>  
>> dlopen(/usr/local/lib64/R/library/XML/libs/XML.so,
>>> 6): Symbol not found: _xmlMemDisplay
>>>   Referenced from:
>>> /usr/local/lib64/R/library/XML/libs/XML.so
>>>   Expected in: flat namespace
>>> Error: .onLoad failed in 'loadNamespace' for 'XML'
>>> Error: package/namespace load failed for 'XML'
>>>
>>> Session information
>>>> sessionInfo()
>>> Version 2.3.1 Patched (2006-06-27 r38447) 
>>> powerpc64-apple-darwin8.7.0 
>>>
>>> attached base packages:
>>> [1] "methods"   "stats"     "graphics" 
>> "grDevices"
>>> "utils"     "datasets" 
>>> [7] "base"     
>>>
>>> Prof Brian Ripley also suggested that this could
>> be
>>> that I don???t have a 64-bit version of libxml2
>>> installed. Where I get it and where/how to install
>> it,
>>> if that???s the problem? 
>>> The reason I need to use R64 is that I have memory
>>> limitation issue with R 32 bit version when I load
>>> some very large XML trees (the data file is about
>>> 800M). And Martin suggested me to use 'handler'
>>> argument of xmlTreeParse, tried 'handler' with
>>> useInternalNodes=T, but I still got this memory
>>> problem with R 32 bit version. Please tell me what
>> I
>>> can do now. Thank you so much!
>>> Weijun
>>>
>>>
>>>
>>>        
>>>
> ____________________________________________________________________________________
>>> Comedy with an Edge to see what's on, when.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained,
>> reproducible code.
>>
> 
> 
> 
>        
> ____________________________________________________________________________________
> Pinpoint customers who are looking for what you sell.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From deepayan.sarkar at gmail.com  Mon Jul  9 22:20:02 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 9 Jul 2007 13:20:02 -0700
Subject: [R] Antwort: Re: pgup/pgdown in R Graphics Window under Linux
	['Watchdog': checked]
In-Reply-To: <OFE35E52C7.14DC3AAB-ONC1257312.002B1719-C1257312.002BF9D4@abbott.com>
References: <eb555e660707051705i42e1f726q23fae2cd6c407056@mail.gmail.com>
	<OFE35E52C7.14DC3AAB-ONC1257312.002B1719-C1257312.002BF9D4@abbott.com>
Message-ID: <eb555e660707091320q6a81e02aw624f098dce269d10@mail.gmail.com>

On 7/8/07, Paul Matthias Diderichsen
<paulmatthias.diderichsen at abbott.com> wrote:
> Hi Deepayan,
>
> "Deepayan Sarkar" <deepayan.sarkar at gmail.com> schrieb am 06.07.2007
> 02:05:02:
> > On 7/5/07, Paul Matthias Diderichsen
> > <paulmatthias.diderichsen at abbott.com> wrote:
> >> library(lattice)
> >>> xyplot(speed~dist|speed, data=cars, layout=c(3,3))
> > If this is your use case, you might be interested in
> > http://cran.r-project.org/src/contrib/Descriptions/plotAndPlayGTK.html
>
> Thanks a lot for the pointer; this package seems to be very useful when
> coding your own plots. However, it's not exactly my use case - rather an
> example to illustrate the the X11 graphics device is apparently not too
> useful for multi-page plots.
>
> The motivation for my question was that I want to use xpose4 (
> http://xpose.sourceforge.net/) under linux. Xpose is a program that
> provides functions for producing diagnostic plots for population PKPD
> model evaluation. I am not able to rewrite the entire package, wrapping
> every call to multi-page plot functions with plotAndPlayGTK.
>
> That's why I was hoping that there exist some obscure configuration option
> for X11 (seems not to be the case, cf. Prof Ripley's reply) or an
> alternative graphic device that runs under linux.

The tools seem to be there already (recordPlot and replayPlot), and it
seems mostly a matter of capturing the relevant keypresses etc. I have
no idea how hard that would be with the X11 device, but I have added
some basic functionality to the Qt based device I've been playing
with. You are welcome to try it --- details can be found at

http://dsarkar.fhcrc.org/R/R-Qt.html

I'm sure doing something similar with the Gtk/Cairo devices wouldn't
be too hard (for those who know what they are doing).

-Deepayan


From p.dalgaard at biostat.ku.dk  Mon Jul  9 22:23:09 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 09 Jul 2007 22:23:09 +0200
Subject: [R] ANOVA: Does a Between-Subjects Factor belong in the Error
 Term?
In-Reply-To: <b91a7cd80707091212h1bb26c2aj5d12aa9d9bf6061a@mail.gmail.com>
References: <b91a7cd80707091212h1bb26c2aj5d12aa9d9bf6061a@mail.gmail.com>
Message-ID: <4692992D.6000004@biostat.ku.dk>

Alex Baugh wrote:
> I am executing a Repeated Measures Analysis of Variance with 1 DV (LOCOMOTOR
> RESPONSE),  2 Within-Subjects Factors (AGE, ACOUSTIC CONDITION), and 1
> Between-Subjects Factor (SEX).
>
> Does anyone know whether the between-subjects factor (SEX) belongs in the
> Error Term of the aov or not? And if it does belong, where in the Error Term
> does it go? The 3 possible scenarios are listed below:
>
>
>
> e.g.,
>
> 1. Omit Sex from the Error Term:
>
>   
>> My.aov = aov(Locomotor.Response~(Age*AcousticCond*Sex) + Error
>>     
> (Subject/(Timepoint*Acx.Cond)), data=locomotor.tab)
>
>   note: Placing SEX outside the double paretheses of the Error Term has the
> same statistical outcome effect as omitting it all together from the Error
> Term (as shown above in #1).
>
>
>
> 2.  Include SEX inside the Error Term (inside Double parentheses):
>
>   
>> My.aov = aov(Locomotor.Response~(Age*AcousticCond*Sex) + Error
>>     
> (Subject/(Timepoint*Acx.Cond+Sex)), data=locomotor.tab)
>
>
>
> 3.  Include SEX inside the Error Term (inside Single parentheses):
>
>
>   
>> My.aov = aov(Locomotor.Response~(Age*AcousticCond*Sex) + Error
>>     
> (Subject/(Timepoint*Acx.Cond)+Sex), data=locomotor.tab)
>
> note: Placing SEX inside the single parentheses (as shown above in #3)
> generates no main effect of Sex. Thus, I'm fairly confident that option #3
> is incorrect.
>
>
>
> Scenarios 1,2, and 3 yield different results in the aov summary.
>
>   
You don't generally want terms with systematic effects to appear as 
error terms also, so 3 is wrong.

In 2 you basically have a random effect of sex within subject, which is 
nonsensical since the subjects presumably have only one sex each. This 
presumably generates an error stratum with 0 DF, which may well be harmless.

That leaves 1 as the likely solution.

You'll probably do yourself a favour if you learn to expand error terms, 
a/b == a + a:b, etc.; that's considerably more constructive than trying 
to think in terms of whether things are inside or outside parentheses.

>
> Thanks for your help!
>
> Alex
>
>
>
>
>
>


From ripley at stats.ox.ac.uk  Mon Jul  9 22:32:20 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Jul 2007 21:32:20 +0100 (BST)
Subject: [R] factanal frustration!
In-Reply-To: <BAY108-F222CF1A0249C26466BE997AA060@phx.gbl>
References: <BAY108-F222CF1A0249C26466BE997AA060@phx.gbl>
Message-ID: <Pine.LNX.4.64.0707092124360.23355@auk.stats>

factanal() does not have an argument method="mle".

Trying to do factor analysis on 1000 columns is quite unrealistic, but you 
may find rescaling the matrix helps.

On Mon, 9 Jul 2007, Talbot Katz wrote:

> Hi.
>
> It seems that nearly every time I try to use factanal I get the following
> response:
>
>> faa2db1<-factanal(mretdb1,factors=2,method="mle",control=list(nstart=25))
> Error in factanal(mretdb1, factors = 2, method = "mle", control =
> list(nstart = 25)) :
>        unable to optimize from these starting value(s)
>>
>
> In the case cited above, mretdb1 is synthetic data created from two factors,
> that is, mretdb1 has dimensions 3050*1000, and each of the thousand columns
> has the form
> c[i]=a1[i]*f1 + a2[i]*f2 + a3[i]*fn[,i]
> where a1, a2, a3 are each vectors of length 1000, f1 and f2 are vectors of
> length 3050, and fn is a matrix of dimension 3050*1000, such that f1, f2,
> and all the columns of fn are mutually uncorrelated.  I wanted to see how
> closely the factors returned by factanal are to the ones that generated the
> data.  But, as you can see, factanal hasn't cooperated!
>
> I would greatly appreciate advice from factanal users, especially those who
> have experienced the same error.  Thanks!
>
> --  TMK  --
> 917-656-5351	cell
> 212-460-5430	home
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ferri.leberl at gmx.at  Mon Jul  9 23:52:05 2007
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Mon, 09 Jul 2007 23:52:05 +0200
Subject: [R] histogram with absolute figures
In-Reply-To: <efb536d50707061317w3d9da10dn155d0d3b74489e92@mail.gmail.com>
References: <1183717974.8176.1.camel@localhost>
	<efb536d50707061317w3d9da10dn155d0d3b74489e92@mail.gmail.com>
Message-ID: <1184017925.6806.2.camel@localhost>

Meanwhile I have recognized, that the breaks-option enforces density as
the default. But if I try to force frequencies (freq=TRUE) I get the
following feedback:

Warning message:
the AREAS in the plot are wrong -- rather use freq=FALSE in:
plot.histogram(r, freq = freq, col = col, border = border, angle =
angle,

And the machine hasn't promised too much: the result IS wrong.
Yours,
Mag. Ferri Leberl



Am Freitag, den 06.07.2007, 16:17 -0400 schrieb Sarah Goslee:
> The default of hist() is counts rather than percentages.
> 
> Sarah
> 
> On 7/6/07, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> > Dear everybody!
> > Is ist easily possible to make up a histogram with absolute numbers
> > instead of percentages?
> > Thank you in advance!
> > Yours, Mag. Ferri Leberl
> >
> > ___________________


From gregory.warnes at mac.com  Mon Jul  9 23:53:44 2007
From: gregory.warnes at mac.com (Gregory Warnes)
Date: Mon, 9 Jul 2007 17:53:44 -0400
Subject: [R] Writing Excel (.xls) files on non-Windows OSs using Perl
In-Reply-To: <46925F2D.1060902@statistik.uni-dortmund.de>
References: <1183926037.3706.89.camel@Bellerophon.localdomain>	<47fce0650707090642j1bf4a0a3q45c2eb59ea94bd1c@mail.gmail.com>	<1183992547.3731.16.camel@Bellerophon.localdomain><971536df0707090810q645abd13o15352e73f99ffa70@mail.gmail.com>
	<46925F2D.1060902@statistik.uni-dortmund.de>
Message-ID: <D0D84D35-7AC9-43B0-B745-633B763ED51B@mac.com>


Since I wrote the xls2csv.pl and read.xls() code for gdata, a perl  
module for writing MS-Excel files has come on the scene.  I don't  
have the time at the moment to create an csv2xls.pl file, but it  
should be straightforward, and I would gladly add it to the gdata  
package.

-G


On Jul 9, 2007, at 12:15PM , Uwe Ligges wrote:

>
>
> Gabor Grothendieck wrote:
>> Note that there already is a function, read.xls, in gdata that  
>> uses Perl.
>
> Note that Marc talked about *writing* in his original message.
>
> Uwe Ligges
>
>
>> On 7/9/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
>>> On Mon, 2007-07-09 at 16:42 +0300, Hans-Peter wrote:
>>>> Hi,
>>>>
>>>> 2007/7/8, Marc Schwartz <marc_schwartz at comcast.net>:
>>>>> [snip]
>>>>> There exists the xlsReadWrite package on CRAN by Hans-Peter  
>>>>> Suter, which
>>>>> is restricted to Windows, since it utilizes the non-FOSS MS  
>>>>> Office API
>>>>> to write the Excel formats.
>>>> The non-FOSS API is not the problem(#) but its implementation is:
>>>>
>>>> The 3rd party library I use is written in Pascal and supports  
>>>> Delphi
>>>> and Kylix. Kylix would allow to port the package to Linux but as  
>>>> Kylix
>>>> has unfortunately been abandoned by CodeGear (Borland) I am not
>>>> ready/interested to spend my time on this dead road. Though it
>>>> probably could be done quickly.
>>>>
>>>> A much more interesting way is to port the package using  
>>>> FreePascal.
>>>> --> I plan to do this since long but...
>>>> --> Maybe someone fluent on Linux and FreePascal could have a  
>>>> look at
>>>> the pascal header files (treetron.googlepages.com) and make the  
>>>> demos
>>>> run on Linux..., that would be great and speed up an eventual
>>>> xlsReadWrite port!
>>> Thanks for the clarification.
>>>
>>> However, I think that if you are going to pursue a cross-platform
>>> solution, providing source code requiring compilation (as opposed  
>>> to a
>>> pre-compiled Windows binary), you should consider what the  
>>> installation
>>> requirements for your package would then be.
>>>
>>> If you are going to take the step of requiring a prospective end- 
>>> user to
>>> have a particular Pascal compiler in place, you may as well have the
>>> requirement for a Perl interpreter and associated packages. Since  
>>> Perl
>>> is widely available and you are more likely to find Perl-fluent  
>>> coders
>>> as opposed to Pascal-fluent coders (eg. I have not used Pascal  
>>> since the
>>> late 80's), I would urge you to consider Perl as a future  
>>> substrate for
>>> your functions.
>>>
>>> While compiled code will run faster than interpreted code, for these
>>> types of file I/O functions, I am not sure that you lose much  
>>> with Perl
>>> from a performance standpoint and you certainly gain the eyes of  
>>> a wider
>>> audience with respect to use, debugging and enhancements.
>>>
>>> To that end, you (or any other interested parties) are free to  
>>> utilize
>>> my code in any way you deem appropriate. I did not state this in my
>>> original post, but I make the code available under GPL(v2),  
>>> freeing you
>>> from any restrictions in its use, including your "Pro" version,  
>>> as long
>>> as you make the source available in a fashion consistent with the  
>>> GPL
>>> requirements.
>>>
>>> Regards,
>>>
>>> Marc Schwartz
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From david.meyer at wu-wien.ac.at  Mon Jul  9 23:54:35 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Mon, 09 Jul 2007 23:54:35 +0200
Subject: [R]  Problems with e1071 and SparseM
Message-ID: <4692AE9B.6040908@wu-wien.ac.at>

Chris:

yes, this is indeed a bug (in predict.svm) - will be fixed in the next 
release of e1071.

Thanks for pointing this out,

David

------------------------------

Hello all,


I am trying to use the "svm" method provided by e1071 (Version: 1.5-16)
together with a matrix provided by the SparseM package (Version: 0.73)
but it fails with this message:

 > > model <- svm(lm, lv, scale = TRUE, type = 'C-classification', kernel =
'linear')
Error in t.default(x) : argument is not a matrix

although lm was created before with read.matrix.csr (from the e1071)
package.

I also tried to simply convert a normal matrix to a SparseM matrix and
then pass it, but I get the same error again.

According to the manual of svm(), this is supposed to work though:

"       x: a data matrix, a vector, or a sparse matrix (object of class
           'matrix.csr' as provided by the package 'SparseM').    "

Used R version: R version 2.4.0 Patched (2006-11-25 r39997)

Does anyone know how I can use Sparse Matrices with e1071? This would be
really important because the matrix is simply too large to write it out.


Best regards,


Chris


From n.nguyen at garvan.org.au  Tue Jul 10 00:54:46 2007
From: n.nguyen at garvan.org.au (Nguyen Dinh Nguyen)
Date: Tue, 10 Jul 2007 08:54:46 +1000
Subject: [R] Subject:  transform excel data into graph
In-Reply-To: <mailman.9.1183975203.984.r-help@stat.math.ethz.ch>
Message-ID: <000701c7c27c$25b6aad0$0fe05e81@D145LD1S>

Is this you want?
library(gplots)

n
<-(0.465,0.422,0.45,0.59,0.543,0.498,0.44,0.35,0.64,0.5,0.473,0.134,0.543,0.
11,0.32)
graph <- matrix(n, nrow=5, ncol=3)

colnames(graph) <- c("Nick", "John", "Peter")
rownames(graph) <- c("Lesson1","Lesson2","Lesson3", "Lesson4","Lesson5")

g <- barplot2(graph, beside = TRUE,
        col = "black",
        angle=c(45,45,90,-45,180),
        density=c(0,20,20,20,20),
        legend = rownames(graph),
        ylim = c(0, 1),
        ylab="lable here",
        plot.grid = TRUE)

N.Nguyen
____________________
What I want to do is a 2d-graph plot where I will have  the name of the
student in the X-axis and the name of the lesson in the Y-axis and the
number from each pair will be used to construct the plot. I am newbie with R
and I don't know which package shall I use nor the commands with which I
will import my data in R so that the plot will be created...


From ferri.leberl at gmx.at  Mon Jul  9 21:05:46 2007
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Mon, 09 Jul 2007 21:05:46 +0200
Subject: [R] making groups
Message-ID: <1184007947.5095.8.camel@localhost>

Dear everybody!
If I have an array of numbers e.g. the points my students got at an
examination, and a  key to group the numbers, e.g. the key which
interval corresponds with which mark (two arrays of the same length or
one 2x(number of marks)), how can I get the array of absolute
frequencies of marks?
I hope I have expressed my problem clearly.
Thank you in advance.
Mag. Ferri Leberl


From jessietian917 at yahoo.com  Mon Jul  9 09:43:22 2007
From: jessietian917 at yahoo.com (tian shen)
Date: Mon, 9 Jul 2007 00:43:22 -0700 (PDT)
Subject: [R]  Split  graphs
Message-ID: <541588.97503.qm@web63010.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/9998787c/attachment.pl 

From linpan1975 at yahoo.com  Tue Jul 10 00:06:13 2007
From: linpan1975 at yahoo.com (Lin Pan)
Date: Mon, 9 Jul 2007 15:06:13 -0700 (PDT)
Subject: [R] how to use mle with a defined function
In-Reply-To: <11402268.post@talk.nabble.com>
References: <11402268.post@talk.nabble.com>
Message-ID: <11511446.post@talk.nabble.com>


Hi all,

sorry for the misleading in the previous email. here is my function to
calculate the maximum likelihood function for a multinormial distribution:

mymle <- function (sigmaX, sigmaY, constraints, env){

	# build omega
	omegaX = abs(sigmaX) * kin + abs(env) * diag(1.0, n, n)
	omegaY = abs(sigmaY) * kin + abs(env) * diag(1.0, n, n)
	omegaXY = (sqrt(sigmaX * sigmaY) - abs(constraints)) * kin
	omega = array(0, c(2*n, 2*n))
	for(i in 1:n){
		for(j in 1:n){
			omega[i, j] = omegaX[i, j]
			omega[i+n, j+n] = omegaY[i, j]
			omega[i+n, j] = omegaXY[i, j]
			omega[i, j+n] = omegaXY[i, j]
		}
	}

	# obtain det of omega
	odet = unlist(determinant(omega))[1]

	# Cholesky decomposition
	C = chol(omega)

	# beta parameter estimates
	newY = t(C)%*%Y
	newX = t(C)%*%X

	# maximum likelihood estimates
	Z = Y - X%*%(lm(newY~newX-1)$coef)
	V = solve(t(C), Z)

	# compute the -2log-likelihood
	square = t(V)%*%V
	0.5*odet + 0.5*square
}

here kin, n, X and Y are known, for example

> kin
           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]
[1,] 1.02276611 0.04899597 0.05076599 0.06727600 0.05561066 0.05561066
[2,] 0.04899597 1.02087402 0.11497498 0.07623291 0.06423950 0.06423950
[3,] 0.05076599 0.11497498 1.02291870 0.07189941 0.11162567 0.11162567
[4,] 0.06727600 0.07623291 0.07189941 1.03277588 0.05522156 0.05522156
[5,] 0.05561066 0.06423950 0.11162567 0.05522156 1.03971863 0.54769897
[6,] 0.05561066 0.06423950 0.11162567 0.05522156 0.54769897 1.03971863
> Y
 [1] 0.4054651 0.6931472 0.7884574 0.6931472 0.5306283 0.5306283 3.1696856
3.5467397 3.5862929 2.5494452 3.1354942 3.2188758
> X
      [,1] [,2] [,3] [,4]
 [1,]    1   69    0    0
 [2,]    1   65    0    0
 [3,]    1   50    0    0
 [4,]    1   54    0    0
 [5,]    1   48    0    0
 [6,]    1   42    0    0
 [7,]    0    0    1    1
 [8,]    0    0    1    2
 [9,]    0    0    1    2
[10,]    0    0    1    2
[11,]    0    0    1    1
[12,]    0    0    1    1
> n
[1] 6


when I call the function
fit <- mle(corr_add, start=list(sigmaX=0.855, sigmaY=0.5, constraints=0.15,
env=0.199), 
	method = "L-BFGS-B", lower=c(0.001, 0.001, 0.001, 0.001),
upper=c(1000,1000,1000,1000))

it always gave me error message something like
"Error in chol(omega) : the leading minor of order 8 is not positive
definite"

I checked the eigenvalues at each iteration and found that when it stopped
there existed negative eigenvalues. I don't know why this is not working
since omega supposed to be positive definite at each iteration. Any hints
would be very appreciated.



Lin





Lin Pan wrote:
> 
> Hi all,
> 
> I am trying to use mle() to find a self-defined function. Here is my
> function:
> 
> test <- function(a=0.1, b=0.1, c=0.001, e=0.2){
> 
> # omega is the known covariance matrix, Y is the response vector, X is the
> explanatory matrix
> 
> odet = unlist(determinant(omega))[1]
>  
> # do cholesky decomposition
> 
> C = chol(omega)
> 
> # transform data
> 
> U = t(C)%*%Y
> WW=t(C)%*%X
> 
> beta = lm(U~W)$coef
> 
> Z=Y-X%*%beta
> V=solve(t(C), Z)
> 
> 0.5*odet + 0.5*(t(V)%*%V)
> 
> }
> 
> and I am trying to call mle() to calculate the maximum likelihood
> estimates for function (0.5*odet+0.5*(t(V)%*%V)) by
> 
> result = mle(test, method="Nelder-Mead")
> 
> But I get the following error message:
> 
> Error in optim(start, f, method = method, hessian = TRUE, ...) : 
>         (list) object cannot be coerced to 'double'
> 
> I am pretty sure that the matrices, parameters etc are numerical before
> importing to the function. But why I still get such error message? Could
> anybody give some help on this? thanks a lot.
> 
> 
> Lin
> 

-- 
View this message in context: http://www.nabble.com/how-to-use-mle-with-a-defined-function-tf4015002.html#a11511446
Sent from the R help mailing list archive at Nabble.com.


From michael.drescher at ontario.ca  Mon Jul  9 23:55:24 2007
From: michael.drescher at ontario.ca (Drescher, Michael (MNR))
Date: Mon, 9 Jul 2007 17:55:24 -0400
Subject: [R] parsing strings
Message-ID: <76D2AA307C39054DBA8BD42DE44E71A403140A07@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/873399f8/attachment.pl 

From sarah.goslee at gmail.com  Tue Jul 10 01:18:31 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 9 Jul 2007 19:18:31 -0400
Subject: [R] histogram with absolute figures
In-Reply-To: <1184017925.6806.2.camel@localhost>
References: <1183717974.8176.1.camel@localhost>
	<efb536d50707061317w3d9da10dn155d0d3b74489e92@mail.gmail.com>
	<1184017925.6806.2.camel@localhost>
Message-ID: <efb536d50707091618t3e1cf930pe5f3b1f4f63bb672@mail.gmail.com>

Well, how about an example of what you are doing, and a
description of what the results you get and the results you
want are?

When I do a histogram, I get frequencies.

Sarah

On 7/9/07, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Meanwhile I have recognized, that the breaks-option enforces density as
> the default. But if I try to force frequencies (freq=TRUE) I get the
> following feedback:
>
> Warning message:
> the AREAS in the plot are wrong -- rather use freq=FALSE in:
> plot.histogram(r, freq = freq, col = col, border = border, angle =
> angle,
>
> And the machine hasn't promised too much: the result IS wrong.
> Yours,
> Mag. Ferri Leberl
>
>
>
> Am Freitag, den 06.07.2007, 16:17 -0400 schrieb Sarah Goslee:
> > The default of hist() is counts rather than percentages.
> >
> > Sarah
> >
> > On 7/6/07, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> > > Dear everybody!
> > > Is ist easily possible to make up a histogram with absolute numbers
> > > instead of percentages?
> > > Thank you in advance!
> > > Yours, Mag. Ferri Leberl
> > >
> > > ___________________
>
>


-- 
Sarah Goslee
http://www.functionaldiversity.org


From jholtman at gmail.com  Tue Jul 10 01:20:52 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 9 Jul 2007 19:20:52 -0400
Subject: [R] Split graphs
In-Reply-To: <541588.97503.qm@web63010.mail.re1.yahoo.com>
References: <541588.97503.qm@web63010.mail.re1.yahoo.com>
Message-ID: <644e1f320707091620pa30450h80382ba8e7d2b26a@mail.gmail.com>

How many columns do you have?  Is it 2 or 1000; can not tell from your
email.  A histogram of 2 values does not seem meaningful.

Do you want 1000 separate histograms, one per page, or multiple per
page?  Yes you can do it, the question is what/how do you want to do
it.

On 7/9/07, tian shen <jessietian917 at yahoo.com> wrote:
> Hello All,
>  I have a question, which somehow I think it is easy, however, I just couldn't get it.
>  I want to histogram each row of a 1000*2 matrix( means it has 1000 rows), and I want to see those 1000 pictures together. How can I do this? Am I able to split a graph into 1000 parts and in each parts it contains a histogram for one row?
>
>  Thank you very much
>
>  Jessie
>
>
> ---------------------------------
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From Charles.Annis at StatisticalEngineering.com  Tue Jul 10 01:23:05 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Mon, 9 Jul 2007 19:23:05 -0400
Subject: [R] Split  graphs
In-Reply-To: <541588.97503.qm@web63010.mail.re1.yahoo.com>
References: <541588.97503.qm@web63010.mail.re1.yahoo.com>
Message-ID: <038601c7c280$1a825520$6400a8c0@DD4XFW31>

Jessie:

How many pixels would you need to allocate for each of these 1000 parts?  Is
that feasible?

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tian shen
Sent: Monday, July 09, 2007 3:43 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Split graphs

Hello All,
  I have a question, which somehow I think it is easy, however, I just
couldn't get it. 
  I want to histogram each row of a 1000*2 matrix( means it has 1000 rows),
and I want to see those 1000 pictures together. How can I do this? Am I able
to split a graph into 1000 parts and in each parts it contains a histogram
for one row? 
   
  Thank you very much
   
  Jessie

 
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From cgenolin at u-paris10.fr  Tue Jul 10 01:25:00 2007
From: cgenolin at u-paris10.fr (cgenolin at u-paris10.fr)
Date: Tue, 10 Jul 2007 01:25:00 +0200
Subject: [R] Within matrix
In-Reply-To: <000001c7c14d$2009bc60$0200000a@fox1>
References: <000001c7c14d$2009bc60$0200000a@fox1>
Message-ID: <20070710012500.0jz5dxpaockg08gg@icare.u-paris10.fr>

Hi all,

I am working on cluster, I am trying to evaluate a within and between 
matrix. Is there any facility for that ? I did my own function, but I 
am not a programmer, so I am affraid I am not really able to programme 
efficiant and fast function...

Thanks

Christophe

----------------------------------------------------------------
Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre


From ggrothendieck at gmail.com  Tue Jul 10 01:36:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 9 Jul 2007 19:36:41 -0400
Subject: [R] parsing strings
In-Reply-To: <76D2AA307C39054DBA8BD42DE44E71A403140A07@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
References: <76D2AA307C39054DBA8BD42DE44E71A403140A07@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <971536df0707091636l78d87dcbha6107c7f25593e58@mail.gmail.com>

strapply in package gsubfn can do that.  The following matches the
indicated regular expression against x and applies the function
given in formula notation (which removes spaces) to each match,
outputting the result as a list:

library(gsubfn)
x <- c("AB10B10", "A10 B5", "AB 10 CD 12", "A10CD2EF3")
strapply(x, "[[:alpha:]] *[[:digit:]]+", ~ gsub(" ", "", x))


For more info, see the gsubfn home page at:
   http://code.google.com/p/gsubfn/
and the various Links there.


On 7/9/07, Drescher, Michael (MNR) <michael.drescher at ontario.ca> wrote:
> Hi All,
>
>
>
> I have strings made up of an unknown number of letters, digits, and
> spaces. Strings always start with one or two letters, and always end
> with one or two digits. A set of letters (one or two letters) is always
> followed by a set of digits (one or two digits), possibly with one or
> more spaces between the sets of letters and digits. A set of letters
> always belongs to the following set of digits and I want to parse the
> strings into these groups. As an example, the strings and the desired
> parsing results could look like this:
>
>
>
> A10B10, desired parsing result: A10 and B10
>
> A10  B5, desired parsing result: A10 and B5
>
> AB 10 CD 12, desired parsing result: AB10 and CD12
>
> A10CD2EF3, desired parsing result: A10, CD2, and EF3
>
>
>
> I assume that it is possible to search a string for letters and digits
> and then break the string where letters are followed by digits, however
> I am a bit clueless about how I could use, e.g., the 'charmatch' or
> 'parse' commands to achieve this.
>
>
>
> Thanks a lot in advance for your help.
>
>
>
> Best, Michael
>
>
>
>
>
>
>
> Michael Drescher
>
> Ontario Forest Research Institute
>
> Ontario Ministry of Natural Resources
>
> 1235 Queen St East
>
> Sault Ste Marie, ON, P6A 2E3
>
> Tel: (705) 946-7406
>
> Fax: (705) 946-2030
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Tue Jul 10 01:40:25 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 9 Jul 2007 19:40:25 -0400
Subject: [R] parsing strings
In-Reply-To: <76D2AA307C39054DBA8BD42DE44E71A403140A07@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
References: <76D2AA307C39054DBA8BD42DE44E71A403140A07@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <644e1f320707091640k382f6878of104678cb6b37aa9@mail.gmail.com>

Is this what you want:

> x <- "A10B10A10  B5AB 10 CD 12A10CD2EF3"
> x <- gsub(" ", "", x)  # remove blanks
> y <- gregexpr("[A-Z]+\\s*[0-9]+", x )[[1]]
>
> substring(x, y, y + attr(y, 'match.length') - 1)
[1] "A10"  "B10"  "A10"  "B5"   "AB10" "CD12" "A10"  "CD2"  "EF3"
>


On 7/9/07, Drescher, Michael (MNR) <michael.drescher at ontario.ca> wrote:
> Hi All,
>
>
>
> I have strings made up of an unknown number of letters, digits, and
> spaces. Strings always start with one or two letters, and always end
> with one or two digits. A set of letters (one or two letters) is always
> followed by a set of digits (one or two digits), possibly with one or
> more spaces between the sets of letters and digits. A set of letters
> always belongs to the following set of digits and I want to parse the
> strings into these groups. As an example, the strings and the desired
> parsing results could look like this:
>
>
>
> A10B10, desired parsing result: A10 and B10
>
> A10  B5, desired parsing result: A10 and B5
>
> AB 10 CD 12, desired parsing result: AB10 and CD12
>
> A10CD2EF3, desired parsing result: A10, CD2, and EF3
>
>
>
> I assume that it is possible to search a string for letters and digits
> and then break the string where letters are followed by digits, however
> I am a bit clueless about how I could use, e.g., the 'charmatch' or
> 'parse' commands to achieve this.
>
>
>
> Thanks a lot in advance for your help.
>
>
>
> Best, Michael
>
>
>
>
>
>
>
> Michael Drescher
>
> Ontario Forest Research Institute
>
> Ontario Ministry of Natural Resources
>
> 1235 Queen St East
>
> Sault Ste Marie, ON, P6A 2E3
>
> Tel: (705) 946-7406
>
> Fax: (705) 946-2030
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Tue Jul 10 01:44:24 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 9 Jul 2007 19:44:24 -0400
Subject: [R] making groups
In-Reply-To: <1184007947.5095.8.camel@localhost>
References: <1184007947.5095.8.camel@localhost>
Message-ID: <644e1f320707091644y502b6438xe8f8b547aaf22a69@mail.gmail.com>

It would be nice if you could supply an example of what your input
looks like and then what you would like your output to look like.  You
would probably use 'tapply', but I would have to see what you data
looks like.

On 7/9/07, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Dear everybody!
> If I have an array of numbers e.g. the points my students got at an
> examination, and a  key to group the numbers, e.g. the key which
> interval corresponds with which mark (two arrays of the same length or
> one 2x(number of marks)), how can I get the array of absolute
> frequencies of marks?
> I hope I have expressed my problem clearly.
> Thank you in advance.
> Mag. Ferri Leberl
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From adschai at optonline.net  Tue Jul 10 02:13:14 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Tue, 10 Jul 2007 00:13:14 +0000 (GMT)
Subject: [R] Question for svm function in e1071
In-Reply-To: <468E28C2.4090906@wu-wien.ac.at>
References: <468E28C2.4090906@wu-wien.ac.at>
Message-ID: <e32a8a622d89f.4692cf1a@optonline.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/8d6aec07/attachment.pl 

From Norm.Good at csiro.au  Tue Jul 10 02:16:31 2007
From: Norm.Good at csiro.au (Norm.Good at csiro.au)
Date: Tue, 10 Jul 2007 10:16:31 +1000
Subject: [R]  Extracting sums for individual factors in data frames
Message-ID: <B998A44C8986644EA8029CFE6396A924CC0E81@exqld2-bne.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/d6a00ca0/attachment.pl 

From Bill.Venables at csiro.au  Tue Jul 10 02:43:55 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Tue, 10 Jul 2007 10:43:55 +1000
Subject: [R] Within matrix
References: <000001c7c14d$2009bc60$0200000a@fox1>
	<20070710012500.0jz5dxpaockg08gg@icare.u-paris10.fr>
Message-ID: <B998A44C8986644EA8029CFE6396A924B68047@exqld2-bne.nexus.csiro.au>

Unless you want to do this millions of times, efficiecy is probably not
a big issue here, but simplicity always pays off.

I'm presuming you are dealing with a single classification setup.

Let f (n x 1) be a *factor* defining the classes
Let X (n x p) be the data matrix.

Then the steps I would use to find the between and within SSP matrices,
'by hand' are as follows:

Tot <- scale(X, scale = FALSE)  	# sweep out the grand means
Res <- resid(aov(X ~ f))		# sweep out the class means

WSS <- crossprod(Res)			# within SSP matrix
BSS <- crossprod(Tot - Res)		# between SSP matrix

 


Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):  +61 7 3826 7304
Mobile:                         +61 4 8819 4402
Home Phone:                     +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
cgenolin at u-paris10.fr
Sent: Tuesday, 10 July 2007 9:25 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Within matrix

Hi all,

I am working on cluster, I am trying to evaluate a within and between 
matrix. Is there any facility for that ? I did my own function, but I 
am not a programmer, so I am affraid I am not really able to programme 
efficiant and fast function...

Thanks

Christophe

----------------------------------------------------------------
Ce message a ete envoye par IMP, grace a l'Universite Paris 10 Nanterre

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rdpeng at gmail.com  Tue Jul 10 03:37:50 2007
From: rdpeng at gmail.com (Roger Peng)
Date: Mon, 9 Jul 2007 21:37:50 -0400
Subject: [R] [R-pkgs] CRANberries -- An RSS feed about New and Updated
	CRAN packages
In-Reply-To: <46926BF5.9010802@lancaster.ac.uk>
References: <18066.25939.77583.800991@basebud.nulle.part>
	<46926BF5.9010802@lancaster.ac.uk>
Message-ID: <66f3bd910707091837g29c87b93w454f3ca8eab00490@mail.gmail.com>

LOL!

-roger

On 7/9/07, Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
> Dirk Eddelbuettel wrote:
>
> > but the easiest way may just be to subscribe to Elijah's wonderful 'Planet R'
> > feed aggregator
>
> My favourite RSS reader at the moment is the RSS cat caption generator:
>
> http://lol.ianloic.com/feed/dirk.eddelbuettel.com/cranberries/index.rss
>
> Barry
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Roger D. Peng  |  http://www.biostat.jhsph.edu/~rpeng/


From lawremi at iastate.edu  Tue Jul 10 05:38:14 2007
From: lawremi at iastate.edu (Michael Lawrence)
Date: Mon, 9 Jul 2007 22:38:14 -0500
Subject: [R] Help in installing rggobi in ubuntu linux
In-Reply-To: <op.tu6godk9lgnhok@davinci.une.net.co>
References: <op.tu6godk9lgnhok@davinci.une.net.co>
Message-ID: <509e0620707092038s18097633m3c36351a20840ac1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070709/16d9bf5c/attachment.pl 

From edd at debian.org  Tue Jul 10 06:00:41 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 9 Jul 2007 23:00:41 -0500
Subject: [R] Help in installing rggobi in ubuntu linux
In-Reply-To: <509e0620707092038s18097633m3c36351a20840ac1@mail.gmail.com>
References: <op.tu6godk9lgnhok@davinci.une.net.co>
	<509e0620707092038s18097633m3c36351a20840ac1@mail.gmail.com>
Message-ID: <18067.1129.592428.88958@basebud.nulle.part>


On 9 July 2007 at 22:38, Michael Lawrence wrote:
| Looks like rggobi can't find GGobi. Make sure that PKG_CONFIG_PATH contains
| the path to your ggobi.pc file. For example:
| 
| export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig
| 
| I would have assumed, however, that the ggobi package would have installed
| to the /usr prefix, in which case pkg-config should have no problem finding
| GGobi.
| 
| On 7/8/07, Kenneth Cabrera <krcabrer at une.net.co> wrote:
| >
| > Hi R users.
| >
| > I am experimenting with ubuntu 7.04 Feisty.
| >
| > I install the ggobi package with apt-get.
| >
| > I got almost all the packages, but
| > when I try to obtain rggobi, I got
| > this message:

Why don;t you install the Rggobi that is provided via Ubuntu?  It is version
2.1.4-4-1 and it corresponds to the 2.1.4-2 version of Ggobi you just
installed. Just do 'sudo apt-get install r-omegahat-ggobi'

On Debian, we are now at 2.1.5-* for both (and we renamed it r-cran-rggobi as
it now resides on CRAN).

Hth, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From christophe at pallier.org  Tue Jul 10 08:13:26 2007
From: christophe at pallier.org (Christophe Pallier)
Date: Tue, 10 Jul 2007 08:13:26 +0200
Subject: [R] ANOVA: Does a Between-Subjects Factor belong in the Error
	Term?
In-Reply-To: <b91a7cd80707091212h1bb26c2aj5d12aa9d9bf6061a@mail.gmail.com>
References: <b91a7cd80707091212h1bb26c2aj5d12aa9d9bf6061a@mail.gmail.com>
Message-ID: <dea6cb960707092313g2f5c5645iee8d566f2272f846@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/fa5c882a/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jul 10 09:00:24 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Jul 2007 08:00:24 +0100 (BST)
Subject: [R] character string to name
In-Reply-To: <Pine.LNX.4.64.0707090724560.11041@homer24.u.washington.edu>
References: <469227A0.4070902@bitwrit.com.au>
	<Pine.LNX.4.64.0707090724560.11041@homer24.u.washington.edu>
Message-ID: <Pine.LNX.4.64.0707091820040.24008@gannet.stats.ox.ac.uk>

On Mon, 9 Jul 2007, Thomas Lumley wrote:

> On Mon, 9 Jul 2007, Jim Lemon wrote:
>
>> Hi folks,
>>
>> I thought I recalled a request for turning a character string into an
>> object name as in:
>
> Yes. It's a FAQ.

There is an FAQ about turning character strings into objects (Q7.21), but 
this seems a bit different.  In the first example, he really does want a 
name.  Since $ does not evaluate its rhs, ways to do that are

yy <- "y"
x$[[yy]] <- 1:4
eval(substitute(x$yy <- 1:4, list(yy=yy)))

and of course parse(text=).

For the second, I think something like

e <- quote(data.frame(yy=1:4))
names(e)[2] <- yy
x <- eval(e)

is probably the simplest way.

>
> 	-thomas
>
>
>> x$as.name("y")<-1:4
>>
>> OR
>>
>> x<-data.frame(as.name("y")=1:4)
>>
>> However, as.name and a few other uninformed attempts didn't even come
>> close. A search of "character to name" produced no helpful functions.
>> This isn't a very urgent request, but if anyone knows some trick to
>> perform this transformation, I would like to hear about it. Thanks.
>>
>> Jim

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From albertosantini at gmail.com  Tue Jul 10 09:21:47 2007
From: albertosantini at gmail.com (Alberto Santini)
Date: Tue, 10 Jul 2007 09:21:47 +0200
Subject: [R] sspir: how to forecast data?
Message-ID: <6a066ca20707100021k39e95ff2n4b3a3d04d088fd67@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/c082b367/attachment.pl 

From martin.bleichner at web.de  Tue Jul 10 10:42:00 2007
From: martin.bleichner at web.de (mb2)
Date: Tue, 10 Jul 2007 01:42:00 -0700 (PDT)
Subject: [R] Repeated Measure different results to spss
Message-ID: <11516870.post@talk.nabble.com>


Hi, 

I have some problems with my repeated measures analysis. When I compute it
with SPSS I get different results than with R. Probably I am doing something
wrong in R. 
I have two groups (1,2) both having to solve a task under two conditions
(1,2). That is one between subject factor (group) and one within subject
factor (task). I tried the following:
 
 aov(Score ~factor(Group)*factor(Task)+Error(Id)))
 aov(Score ~factor(Group)*factor(Task))
but it leads to different results than my spss. I definitely miss some point
here .

Thanks for you help. 

Id	Group	Task	Score
1	1	1	0.39
2	1	1	0.48
3	1	1	0.59
4	1	1	0.33
5	1	1	0.38
6	1	1	0.37
7	1	1	0.47
8	1	1	0.2
9	1	1	0.29
10	1	1	0.41
11	1	1	0.24
12	1	1	0.28
13	1	1	0.32
14	1	1	0.26
15	2	1	0.65
16	2	1	0.41
17	2	1	0.62
18	2	1	0.39
19	2	1	0.81
20	2	1	0.34
21	2	1	0.32
22	2	1	0.33
23	2	1	0.33
24	2	1	0.38
1	1	2	0.46
2	1	2	0.27
3	1	2	0.41
4	1	2	0.13
5	1	2	0.41
6	1	2	0.36
7	1	2	0.32
8	1	2	0.33
9	1	2	0.44
10	1	2	0.36
11	1	2	0.2
12	1	2	0.3
13	1	2	0.27
14	1	2	0.4
15	2	2	0.35
16	2	2	0.37
17	2	2	0.34
18	2	2	0.24
19	2	2	0.44
20	2	2	0.34
21	2	2	0.4
22	2	2	0.28
23	2	2	0.32
24	2	2	0.33
-- 
View this message in context: http://www.nabble.com/Repeated-Measure-different-results-to-spss-tf4054506.html#a11516870
Sent from the R help mailing list archive at Nabble.com.


From mark_difford at yahoo.co.uk  Tue Jul 10 11:13:52 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Tue, 10 Jul 2007 02:13:52 -0700 (PDT)
Subject: [R] histogram with absolute figures
In-Reply-To: <efb536d50707091618t3e1cf930pe5f3b1f4f63bb672@mail.gmail.com>
References: <1183717974.8176.1.camel@localhost>
	<efb536d50707061317w3d9da10dn155d0d3b74489e92@mail.gmail.com>
	<1184017925.6806.2.camel@localhost>
	<efb536d50707091618t3e1cf930pe5f3b1f4f63bb672@mail.gmail.com>
Message-ID: <11517345.post@talk.nabble.com>


In the absence of a data set, it may help to read the help file carefully:

?hist

Note, in particular, that the argument freq defaults to TRUE "if and only if
breaks are equidistant (and probability is not specified)."

Regards,
Mark.


Sarah Goslee wrote:
> 
> Well, how about an example of what you are doing, and a
> description of what the results you get and the results you
> want are?
> 
> When I do a histogram, I get frequencies.
> 
> Sarah
> 
> On 7/9/07, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
>> Meanwhile I have recognized, that the breaks-option enforces density as
>> the default. But if I try to force frequencies (freq=TRUE) I get the
>> following feedback:
>>
>> Warning message:
>> the AREAS in the plot are wrong -- rather use freq=FALSE in:
>> plot.histogram(r, freq = freq, col = col, border = border, angle =
>> angle,
>>
>> And the machine hasn't promised too much: the result IS wrong.
>> Yours,
>> Mag. Ferri Leberl
>>
>>
>>
>> Am Freitag, den 06.07.2007, 16:17 -0400 schrieb Sarah Goslee:
>> > The default of hist() is counts rather than percentages.
>> >
>> > Sarah
>> >
>> > On 7/6/07, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
>> > > Dear everybody!
>> > > Is ist easily possible to make up a histogram with absolute numbers
>> > > instead of percentages?
>> > > Thank you in advance!
>> > > Yours, Mag. Ferri Leberl
>> > >
>> > > ___________________
>>
>>
> 
> 
> -- 
> Sarah Goslee
> http://www.functionaldiversity.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/histogram-with-absolute-figures-tf4037852.html#a11517345
Sent from the R help mailing list archive at Nabble.com.


From calldimple at yahoo.com  Tue Jul 10 11:45:41 2007
From: calldimple at yahoo.com (dimple thyagarajan)
Date: Tue, 10 Jul 2007 02:45:41 -0700 (PDT)
Subject: [R] The results of your email commands
In-Reply-To: <mailman.974.1184060566.2069.r-help@stat.math.ethz.ch>
Message-ID: <261295.82758.qm@web34613.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/329dece9/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jul 10 11:56:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Jul 2007 10:56:07 +0100 (BST)
Subject: [R] Repeated Measure different results to spss
In-Reply-To: <11516870.post@talk.nabble.com>
References: <11516870.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0707101047270.19131@gannet.stats.ox.ac.uk>

We don't know what you are after (or what you did in SPSS), but

> dat <- read.table("mb2.dat", header=TRUE,
                     colClasses=c(rep("factor",3), "double"))
> summary(aov(Score ~ Group * Task + Error(Id), dat))

would seem to be the sort of thing your description indicates.

If you tell us what you are looking for (and why) we may be able to tell 
you how to get it in R.

On Tue, 10 Jul 2007, mb2 wrote:

>
> Hi,
>
> I have some problems with my repeated measures analysis. When I compute it
> with SPSS I get different results than with R. Probably I am doing something
> wrong in R.
> I have two groups (1,2) both having to solve a task under two conditions
> (1,2). That is one between subject factor (group) and one within subject
> factor (task). I tried the following:
>
> aov(Score ~factor(Group)*factor(Task)+Error(Id)))
> aov(Score ~factor(Group)*factor(Task))
> but it leads to different results than my spss. I definitely miss some point
> here .
>
> Thanks for you help.
>
> Id	Group	Task	Score
> 1	1	1	0.39
> 2	1	1	0.48
> 3	1	1	0.59
> 4	1	1	0.33
> 5	1	1	0.38
> 6	1	1	0.37
> 7	1	1	0.47
> 8	1	1	0.2
> 9	1	1	0.29
> 10	1	1	0.41
> 11	1	1	0.24
> 12	1	1	0.28
> 13	1	1	0.32
> 14	1	1	0.26
> 15	2	1	0.65
> 16	2	1	0.41
> 17	2	1	0.62
> 18	2	1	0.39
> 19	2	1	0.81
> 20	2	1	0.34
> 21	2	1	0.32
> 22	2	1	0.33
> 23	2	1	0.33
> 24	2	1	0.38
> 1	1	2	0.46
> 2	1	2	0.27
> 3	1	2	0.41
> 4	1	2	0.13
> 5	1	2	0.41
> 6	1	2	0.36
> 7	1	2	0.32
> 8	1	2	0.33
> 9	1	2	0.44
> 10	1	2	0.36
> 11	1	2	0.2
> 12	1	2	0.3
> 13	1	2	0.27
> 14	1	2	0.4
> 15	2	2	0.35
> 16	2	2	0.37
> 17	2	2	0.34
> 18	2	2	0.24
> 19	2	2	0.44
> 20	2	2	0.34
> 21	2	2	0.4
> 22	2	2	0.28
> 23	2	2	0.32
> 24	2	2	0.33
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From P.Dalgaard at biostat.ku.dk  Tue Jul 10 12:05:09 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 10 Jul 2007 12:05:09 +0200
Subject: [R] Repeated Measure different results to spss
In-Reply-To: <11516870.post@talk.nabble.com>
References: <11516870.post@talk.nabble.com>
Message-ID: <469359D5.3080600@biostat.ku.dk>

mb2 wrote:
> Hi, 
>
> I have some problems with my repeated measures analysis. When I compute it
> with SPSS I get different results than with R. Probably I am doing something
> wrong in R. 
> I have two groups (1,2) both having to solve a task under two conditions
> (1,2). That is one between subject factor (group) and one within subject
> factor (task). I tried the following:
>  
>  aov(Score ~factor(Group)*factor(Task)+Error(Id)))
>  aov(Score ~factor(Group)*factor(Task))
> but it leads to different results than my spss. I definitely miss some point
> here .
>
>   
Did you mean Error(factor(Id)) ?

With that modification, things look sane. Can't vouch for SPSS...

(As a general matter, I prefer to do the factor conversions up front,
rather than inside model formulas.)


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jim at bitwrit.com.au  Tue Jul 10 12:14:47 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Tue, 10 Jul 2007 20:14:47 +1000
Subject: [R] The results of your email commands
Message-ID: <46935C17.3050408@bitwrit.com.au>

dimple thyagarajan wrote:

 > ...
 > Building a website is a piece of cake.

But sending an email is another matter altogether.

Jim


From amnakhan493 at gmail.com  Tue Jul 10 12:21:03 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 10 Jul 2007 03:21:03 -0700
Subject: [R] iid.test package
Message-ID: <3ffd3bb60707100321w1ff15693na0a30256314a6ba8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/bc64106a/attachment.pl 

From amnakhan493 at gmail.com  Tue Jul 10 12:21:03 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 10 Jul 2007 03:21:03 -0700
Subject: [R] iid.test package
Message-ID: <3ffd3bb60707100321w1ff15693na0a30256314a6ba8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/bc64106a/attachment-0001.pl 

From Bernhard_Pfaff at fra.invesco.com  Tue Jul 10 12:46:48 2007
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Tue, 10 Jul 2007 11:46:48 +0100
Subject: [R] ca.jo
In-Reply-To: <31beaa9a0707081726t2ccca012t12b7128f837fa2ca@mail.gmail.com>
References: <31beaa9a0707081726t2ccca012t12b7128f837fa2ca@mail.gmail.com>
Message-ID: <B89F0CE41D45644A97CCC93DF548C1C309B4B2DC@GBHENXMB02.corp.amvescap.net>

Hello Yihsu,

have a look at ?cajorls. With this function a VECM is estimated, whence
the cointegration rank has been determined (ca.jo). For further
analysis, you might want to consider the function vec2var in package
vars and methods irf, fevd and predict, as well as the diagnostic tests
that are available in vars.

Best,
Bernhard

>
>Dear R users;
>
>I'm using ca.jo for a VECM model.  Is there a way that I can 
>get sd/p-value
>to see whether coefficients estimated are statistical 
>significant?   Thank
>you
>
>Yours,
>
>Yihsu
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
*****************************************************************
Confidentiality Note: The information contained in this mess...{{dropped}}


From r.hankin at noc.soton.ac.uk  Tue Jul 10 12:57:25 2007
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 10 Jul 2007 11:57:25 +0100
Subject: [R] integration over a simplex
Message-ID: <3CC4D963-CFFC-4B7E-8605-3A40374968A7@noc.soton.ac.uk>

Hello

The excellent adapt package integrates over multi-dimensional
hypercubes.

I want to integrate over a multidimensional simplex.  Has anyone
implemented such a thing in R?

I can transform an n-simplex to a hyperrectangle
but the Jacobian is a rapidly-varying (and very lopsided)
function and this is making adapt() slow.

[
A \dfn{simplex} is an n-dimensional analogue of a triangle or  
tetrahedron.
It is the convex hull of (n+1) points in an n-dimensional Euclidean  
space.

My application is a variant of the Dirichlet distribution:
With p~D(a), if length(p) = n+1 then the requirement that
all(p>0) and sum(p)=1 mean that the support of the
Dirichlet distribution is an n-simplex.
]


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743


From carsten_jaeger at web.de  Tue Jul 10 13:14:36 2007
From: carsten_jaeger at web.de (Carsten Jaeger)
Date: Tue, 10 Jul 2007 13:14:36 +0200
Subject: [R] type III ANOVA for a nested linear model
Message-ID: <1184066076.3890.31.camel@Amilo>

Hello,

is it possible to obtain type III sums of squares for a nested model as
in the following:

lmod <- lm(resp ~ A * B + (C %in% A), mydata))

I have tried

library(car)
Anova(lmod, type="III")

but this gives me an error (and I also understand from the documentation
of Anova as well as from a previous request
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/64477.html) that it is
not possible to specify nested models with car's Anova).

anova(lmod) works, of course.

My data (given below) is balanced so I expect the results to be similar
for both type I and type III sums of squares. But are they *exactly* the
same? The editor of the journal which I'm sending my manuscript to
requests what he calls "conventional" type III tests and I'm not sure if
can convince him to accept my type I analysis.

R> mydata
      A     B     C  resp
1     1     1      1 34.12
2     1     1      2 32.45
3     1     1      3 44.55
4     1     2      1 20.88
5     1     2      2 22.32
6     1     2      3 27.71
7     2     1      6 38.20
8     2     1      7 31.62
9     2     1      8 38.71
10    2     2      6 18.93
11    2     2      7 20.57
12    2     2      8 31.55
13    3     1      9 40.81
14    3     1     10 42.23
15    3     1     11 41.26
16    3     2      9 28.41
17    3     2     10 24.07
18    3     2     11 21.16

Thanks a lot,

Carsten


From noel.magnin at free.fr  Tue Jul 10 13:22:22 2007
From: noel.magnin at free.fr (Noel Magnin)
Date: Tue, 10 Jul 2007 13:22:22 +0200
Subject: [R] Barplot with "multiple categories"
Message-ID: <46936BEE.1060206@free.fr>

Dear all
Thanks in advance for replies
I am trying to make a barplot out of this data which I read from a file

tb <- read.table("tmp.dat"
        , na.string=c("-"))

tmp.dat: ############
(the file is much longer and includes NAs as "-")

#    A1    A2    A3    B1    B2    B3    C1    C2    C3    D1    D2    D3
    2    P    12    2    P    11    2    F    1    2    P    7
    2    S    6    2    S    6    2    P    11    2    x    6
    2    x    34    2    x    21    2    S    2    3    I    7
    3    I    25    3    I    21    2    T    1    3    S    2
    3    N    2    3    S    1    2    x    15    3    x    4
    3    x    25    3    x    16    3    I    15    4    S    8
    4    C    1    4    S    26    3    x    15    4    Y    1
    4    S    32    4    x    12    4    S    15    4    x    4
    4    x    19    5    P    26    4    x    15    5    P    8
    5    L    1    5    R    2    5    P    15    5    S    1
    5    P    31    5    x    10    5    x    15    5    x    4
    5    R    1    6    I    27    6    I    13    6    I    9
    5    x    19    6    N    1    6    M    1    6    x    4
    6    I    32    6    x    10    6    N    1    7    E    9
    6    N    1    7    E    29    6    x    15    7    x    4
    6    x    19    7    x    9    7    D    1    8    T    9
    7    D    1    8    R    1    7    E    14    8    x    4
    7    E    32    8    T    29    7    x    15    9    V    10
    7    x    19    8    x    8    8    T    15    9    x    3
    8    T    34    9    V    30    8    x    15    10    P    11
    8    x    18    9    x    8    9    V    17    10    x    2
    9    I    1    10    H    1    9    x    13    11    V    11
    9    V    35    10    P    29    10    A    2    11    x    2
    9    x    16    10    x    8    10    P    15    12    K    13
    10    P    36    11    V    30    10    S    1    13    L    13
    10    x    16    11    x    8    10    x    12    14    K    13
    11    V    36    12    K    30    11    V    20    15    P    12
    11    x    16    12    R    1    11    x    10    15    [PA]    1
    12    K    41    12    x    7    12    K    23    16    G    13
    12    x    11    13    L    33    12    R    1    17    M    13
    13    I    2    13    x    5    12    x    6    18    D    13
    13    L    42    14    K    32    13    L    27    19    G    13
    13    x    8    14    N    1    13    x    3    20    P    13
    14    K    46    14    x    5    14    K    27    21    K    12

######### end Data
Data explanation :
"position" : A1, B1, C1, D1 for different samples  [these are 
amino-acids positions in biological sequences]
"value found" : A2, B2, ...
"number of occurences" : C1, C2 ....

the barplot type I would like to obtain :
 |
 |
y||
 || |
 || |
 |||| ...  ie : number of occurences (A3, B3, ...)
 |_______________________________________
  PSx PSx FPSTx Px | INx ISx Ix ISx ...   ie : values in A2, B2 ....(not 
always the same "length")
           2       |        3   ...           ie : position in A1, B1, ...

I have tried to read as matrix, simple vectors, but I cannot manage to 
reach this type of barplot.
Any hint towards this goal would be much appreciated
All the best
Noel


From P.Dalgaard at biostat.ku.dk  Tue Jul 10 13:54:35 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 10 Jul 2007 13:54:35 +0200
Subject: [R] type III ANOVA for a nested linear model
In-Reply-To: <1184066076.3890.31.camel@Amilo>
References: <1184066076.3890.31.camel@Amilo>
Message-ID: <4693737B.7070208@biostat.ku.dk>

Carsten Jaeger wrote:
> Hello,
>
> is it possible to obtain type III sums of squares for a nested model as
> in the following:
>
> lmod <- lm(resp ~ A * B + (C %in% A), mydata))
>
> I have tried
>
> library(car)
> Anova(lmod, type="III")
>
> but this gives me an error (and I also understand from the documentation
> of Anova as well as from a previous request
> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/64477.html) that it is
> not possible to specify nested models with car's Anova).
>
> anova(lmod) works, of course.
>
> My data (given below) is balanced so I expect the results to be similar
> for both type I and type III sums of squares. But are they *exactly* the
> same? The editor of the journal which I'm sending my manuscript to
> requests what he calls "conventional" type III tests and I'm not sure if
>   
> can convince him to accept my type I analysis.
In balanced designs, type I-IV SSD's are all identical. However, I don't think the model does what I think you think it does. 

Notice that "nesting" is used with two diferent meanings, in R it would be that the codings of C only makes sense within levels of A - e.g. if they were numbered 1:3 within each group, but with C==1 when A==1 having nothing to do with C==1 when A==2.  SAS does something. er. else...

What I think you want is a model where C is a random terms so that main effects of A can be tested, like in

> summary(aov(resp ~ A * B + Error(C), dd))

Error: C
          Df  Sum Sq Mean Sq F value Pr(>F)
A          2  33.123  16.562  0.4981 0.6308
Residuals  6 199.501  33.250

Error: Within
          Df Sum Sq Mean Sq F value   Pr(>F)
B          1 915.21  915.21 83.7846 9.57e-05 ***
A:B        2  16.13    8.07  0.7384   0.5168
Residuals  6  65.54   10.92
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1


(This is essentially the same structure as Martin Bleichner had earlier today, also @web.de. What is this? an epidemic? ;-))


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From edward.oakeley at fmi.ch  Tue Jul 10 14:36:59 2007
From: edward.oakeley at fmi.ch (Oakeley, Edward)
Date: Tue, 10 Jul 2007 14:36:59 +0200
Subject: [R] Building R on Interix 6.0
Message-ID: <4850F76274F80D4D8F6D043FAD35F8270194E27D@ex2.fmi.ch>

Dear all,

I have been trying to build R-2.5.1 on the Interix-6.0 Unix subsystem
that ships with Vista and everything looks fine during the configure
except towards the end when "sed" throws an error: 

sed: 1: ""s/\*/\\\*/g"": invalid command code "

A few lines later I then get:

./configure: : bad substitution

So I guess sed is trying to do something and because it failed the rest
of the configure process breaks. It would be very helpful to get R to
build (or at least the "standalone" bits) because then I can link it in
to perl scripts running on Interix that make use of R libraries.

If anyone can make a suggestion I would really appreciate it as the
alternative (cygwin) is REALLY slow compared to Interix.

The output from the ./configure command is listed below in case I missed
something.

Thanks

Ed

----------
checking build system type... i586-pc-interix6.0
checking host system type... i586-pc-interix6.0
loading site script './config.site'
loading build specific script './config.site'
checking for pwd... /bin/pwd
checking whether builddir is srcdir... yes
checking for working aclocal... found
checking for working autoconf... found
checking for working automake... found
checking for working autoheader... found
checking for gawk... gawk
checking for grep that handles long lines and -e... /bin/grep
checking for egrep... /bin/grep -E
checking whether ln -s works... yes
checking for ranlib... ranlib
checking for bison... bison -y
checking for ar... ar
checking for a BSD-compatible install... /bin/install -c
checking for sed... /bin/sed
checking for less... /bin/less
checking for perl... /usr/local/bin/perl
checking whether perl version is at least 5.004... yes
checking for dvips... no
checking for tex... no
checking for latex... no
checking for makeindex... no
checking for pdftex... no
checking for pdflatex... no
checking for makeinfo... /usr/local/bin/makeinfo
checking whether makeinfo version is at least 4.7... yes
checking for unzip... /usr/contrib/bin/unzip
checking for zip... /usr/contrib/bin/zip
checking for gzip... /usr/local/bin/gzip
checking for firefox... no
checking for mozilla... no
checking for netscape... no
checking for galeon... no
checking for kfmclient... no
checking for opera... no
checking for gnome-moz-remote... no
checking for open... no
checking for acroread... no
checking for acroread4... no
checking for xpdf... no
checking for gv... no
checking for gnome-gv... no
checking for ggv... no
checking for kghostview... no
checking for open... no
checking for gpdf... no
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking how to run the C preprocessor... gcc -E
checking whether gcc needs -traditional... no
checking how to run the C preprocessor... gcc -E
checking for g77... g77
checking whether we are using the GNU Fortran 77 compiler... yes
checking whether g77 accepts -g... yes
checking for g++... g++
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
checking how to run the C++ preprocessor... g++ -E
checking whether __attribute__((visibility())) is supported... no
checking whether gcc accepts -fvisibility... no
checking whether g77 accepts -fvisibility... no
checking for gcc... gcc
checking whether we are using the GNU Objective C compiler... yes
checking whether gcc accepts -g... yes
checking whether g++ can compile ObjC++... yes
checking for Objective C++ compiler... g++
checking for a sed that does not truncate output... /bin/sed
checking for ld used by gcc... /opt/gcc.3.3/i586-pc-interix3/bin/ld
checking if the linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld) is GNU
ld... yes
checking for /opt/gcc.3.3/i586-pc-interix3/bin/ld option to reload
object files... -r
checking for BSD-compatible nm... /bin/nm -B
checking how to recognise dependent libraries... unknown
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking dlfcn.h usability... yes
checking dlfcn.h presence... yes
checking for dlfcn.h... yes
checking the maximum length of command line arguments... 262144
checking command to parse /bin/nm -B output from gcc object... ok
checking for objdir... .libs
checking for ranlib... (cached) ranlib
checking for strip... strip
checking if gcc static flag  works... yes
checking if gcc supports -fno-rtti -fno-exceptions... no
checking for gcc option to produce PIC... -fPIC
checking if gcc PIC flag -fPIC works... yes
checking if gcc supports -c -o file.o... yes
checking whether the gcc linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld)
supports shared libraries... no
checking dynamic linker characteristics... no
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
checking if libtool supports shared libraries... no
checking whether to build shared libraries... no
checking whether to build static libraries... yes
configure: creating libtool
appending configuration tag "CXX" to libtool
checking for ld used by g++... /opt/gcc.3.3/i586-pc-interix3/bin/ld
checking if the linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld) is GNU
ld... yes
checking whether the g++ linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld)
supports shared libraries... no
sed: 1: ""s/\*/\\\*/g"": invalid command code "
checking for g++ option to produce PIC... -fPIC
checking if g++ PIC flag -fPIC works... yes
checking if g++ supports -c -o file.o... yes
checking whether the g++ linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld)
supports shared libraries... no
checking dynamic linker characteristics... no
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
appending configuration tag "F77" to libtool
checking if libtool supports shared libraries... no
checking whether to build shared libraries... no
checking whether to build static libraries... yes
checking for g77 option to produce PIC... -fPIC
checking if g77 PIC flag -fPIC works... yes
checking if g77 supports -c -o file.o... yes
checking whether the g77 linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld)
supports shared libraries... no
checking dynamic linker characteristics... no
checking how to hardcode library paths into programs... immediate
checking whether stripping libraries is possible... yes
./configure: : bad substitution


From Bill.Venables at csiro.au  Tue Jul 10 14:44:16 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Tue, 10 Jul 2007 22:44:16 +1000
Subject: [R] type III ANOVA for a nested linear model
Message-ID: <B998A44C8986644EA8029CFE6396A924B68056@exqld2-bne.nexus.csiro.au>

The message from this cute little data set is very clear.  Consider

> fm <- aov(resp ~ A*B + A/C, mydata)
> 
> drop1(fm, test = "F")
Single term deletions

Model:
resp ~ A * B + A/C
       Df Sum of Sq     RSS     AIC F value  Pr(F)
<none>               65.540  47.261               
A:B     2    16.132  81.672  47.222  0.7384 0.5168
A:C     6   199.501 265.041  60.411  3.0440 0.1007

So neither of the non-marginal terms is significant.  To address
questions about the main effects the natural next step is to remove the
interactions.  By orthogonality you can safely cut a few corners and do
both at once:


> drop1(update(fm, .~A+B), test = "F")
Single term deletions

Model:
resp ~ A + B
       Df Sum of Sq     RSS     AIC F value     Pr(F)
<none>               281.17   57.47                  
A       2     33.12  314.30   55.48  0.8246    0.4586
B       1    915.21 1196.38   81.54 45.5695 9.311e-06

There is a very obvious, even trivial, B main effect, but nothing else.
All this becomes even more glaring if you take the unusal step of
plotting the data.

What sort of editor would overlook this clear and demonstrable message
leaping out from the data in favour of some arcane argument about "types
of sums of squares"?  Several answers come to mind: A power freak, a SAS
afficianado, an idiot.

If you get nowhere with this editor, my suggestion, hard as it may seem,
is that you do not submit to that kind of midnless idealogy and make
fatuous compromises for the sake of immediate publication. If necessary,
part company with that editor and find somewhere else to publish where
the editor has some inkling of what statistical inference is all about.

Bill Venables.
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Carsten Jaeger
Sent: Tuesday, 10 July 2007 4:15 AM
To: R help list
Subject: [R] type III ANOVA for a nested linear model

Hello,

is it possible to obtain type III sums of squares for a nested model as
in the following:

lmod <- lm(resp ~ A * B + (C %in% A), mydata))

I have tried

library(car)
Anova(lmod, type="III")

but this gives me an error (and I also understand from the documentation
of Anova as well as from a previous request
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/64477.html) that it is
not possible to specify nested models with car's Anova).

anova(lmod) works, of course.

My data (given below) is balanced so I expect the results to be similar
for both type I and type III sums of squares. But are they *exactly* the
same? The editor of the journal which I'm sending my manuscript to
requests what he calls "conventional" type III tests and I'm not sure if
can convince him to accept my type I analysis.

R> mydata
      A     B     C  resp
1     1     1      1 34.12
2     1     1      2 32.45
3     1     1      3 44.55
4     1     2      1 20.88
5     1     2      2 22.32
6     1     2      3 27.71
7     2     1      6 38.20
8     2     1      7 31.62
9     2     1      8 38.71
10    2     2      6 18.93
11    2     2      7 20.57
12    2     2      8 31.55
13    3     1      9 40.81
14    3     1     10 42.23
15    3     1     11 41.26
16    3     2      9 28.41
17    3     2     10 24.07
18    3     2     11 21.16

Thanks a lot,

Carsten

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From elyakhlifi_mustapha at yahoo.fr  Tue Jul 10 14:51:53 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Tue, 10 Jul 2007 12:51:53 +0000 (GMT)
Subject: [R] TukeyHSD test
Message-ID: <989690.38420.qm@web27502.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/88f097f1/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Tue Jul 10 15:23:03 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 10 Jul 2007 15:23:03 +0200
Subject: [R] Building R on Interix 6.0
In-Reply-To: <4850F76274F80D4D8F6D043FAD35F8270194E27D@ex2.fmi.ch>
References: <4850F76274F80D4D8F6D043FAD35F8270194E27D@ex2.fmi.ch>
Message-ID: <46938837.3090504@biostat.ku.dk>

Oakeley, Edward wrote:
> Dear all,
>
> I have been trying to build R-2.5.1 on the Interix-6.0 Unix subsystem
> that ships with Vista and everything looks fine during the configure
> except towards the end when "sed" throws an error: 
>
> sed: 1: ""s/\*/\\\*/g"": invalid command code "
>
> A few lines later I then get:
>
> ./configure: : bad substitution
>
> So I guess sed is trying to do something and because it failed the rest
> of the configure process breaks. It would be very helpful to get R to
> build (or at least the "standalone" bits) because then I can link it in
> to perl scripts running on Interix that make use of R libraries.
>
> If anyone can make a suggestion I would really appreciate it as the
> alternative (cygwin) is REALLY slow compared to Interix.
>
>   
Ouch. Good luck...

Looks like you also have some serious issues with shared libraries,
without which you will have trouble loading R packages (i.e. with
everything, basically).

The sed issue appears to involve quoting; could it be related to
whichever shell is being used?

> The output from the ./configure command is listed below in case I missed
> something.
>
> Thanks
>
> Ed
>
> ----------
> checking build system type... i586-pc-interix6.0
> checking host system type... i586-pc-interix6.0
> loading site script './config.site'
> loading build specific script './config.site'
> checking for pwd... /bin/pwd
> checking whether builddir is srcdir... yes
> checking for working aclocal... found
> checking for working autoconf... found
> checking for working automake... found
> checking for working autoheader... found
> checking for gawk... gawk
> checking for grep that handles long lines and -e... /bin/grep
> checking for egrep... /bin/grep -E
> checking whether ln -s works... yes
> checking for ranlib... ranlib
> checking for bison... bison -y
> checking for ar... ar
> checking for a BSD-compatible install... /bin/install -c
> checking for sed... /bin/sed
> checking for less... /bin/less
> checking for perl... /usr/local/bin/perl
> checking whether perl version is at least 5.004... yes
> checking for dvips... no
> checking for tex... no
> checking for latex... no
> checking for makeindex... no
> checking for pdftex... no
> checking for pdflatex... no
> checking for makeinfo... /usr/local/bin/makeinfo
> checking whether makeinfo version is at least 4.7... yes
> checking for unzip... /usr/contrib/bin/unzip
> checking for zip... /usr/contrib/bin/zip
> checking for gzip... /usr/local/bin/gzip
> checking for firefox... no
> checking for mozilla... no
> checking for netscape... no
> checking for galeon... no
> checking for kfmclient... no
> checking for opera... no
> checking for gnome-moz-remote... no
> checking for open... no
> checking for acroread... no
> checking for acroread4... no
> checking for xpdf... no
> checking for gv... no
> checking for gnome-gv... no
> checking for ggv... no
> checking for kghostview... no
> checking for open... no
> checking for gpdf... no
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables... 
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ISO C89... none needed
> checking how to run the C preprocessor... gcc -E
> checking whether gcc needs -traditional... no
> checking how to run the C preprocessor... gcc -E
> checking for g77... g77
> checking whether we are using the GNU Fortran 77 compiler... yes
> checking whether g77 accepts -g... yes
> checking for g++... g++
> checking whether we are using the GNU C++ compiler... yes
> checking whether g++ accepts -g... yes
> checking how to run the C++ preprocessor... g++ -E
> checking whether __attribute__((visibility())) is supported... no
> checking whether gcc accepts -fvisibility... no
> checking whether g77 accepts -fvisibility... no
> checking for gcc... gcc
> checking whether we are using the GNU Objective C compiler... yes
> checking whether gcc accepts -g... yes
> checking whether g++ can compile ObjC++... yes
> checking for Objective C++ compiler... g++
> checking for a sed that does not truncate output... /bin/sed
> checking for ld used by gcc... /opt/gcc.3.3/i586-pc-interix3/bin/ld
> checking if the linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld) is GNU
> ld... yes
> checking for /opt/gcc.3.3/i586-pc-interix3/bin/ld option to reload
> object files... -r
> checking for BSD-compatible nm... /bin/nm -B
> checking how to recognise dependent libraries... unknown
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking dlfcn.h usability... yes
> checking dlfcn.h presence... yes
> checking for dlfcn.h... yes
> checking the maximum length of command line arguments... 262144
> checking command to parse /bin/nm -B output from gcc object... ok
> checking for objdir... .libs
> checking for ranlib... (cached) ranlib
> checking for strip... strip
> checking if gcc static flag  works... yes
> checking if gcc supports -fno-rtti -fno-exceptions... no
> checking for gcc option to produce PIC... -fPIC
> checking if gcc PIC flag -fPIC works... yes
> checking if gcc supports -c -o file.o... yes
> checking whether the gcc linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld)
> supports shared libraries... no
> checking dynamic linker characteristics... no
> checking how to hardcode library paths into programs... immediate
> checking whether stripping libraries is possible... yes
> checking if libtool supports shared libraries... no
> checking whether to build shared libraries... no
> checking whether to build static libraries... yes
> configure: creating libtool
> appending configuration tag "CXX" to libtool
> checking for ld used by g++... /opt/gcc.3.3/i586-pc-interix3/bin/ld
> checking if the linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld) is GNU
> ld... yes
> checking whether the g++ linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld)
> supports shared libraries... no
> sed: 1: ""s/\*/\\\*/g"": invalid command code "
> checking for g++ option to produce PIC... -fPIC
> checking if g++ PIC flag -fPIC works... yes
> checking if g++ supports -c -o file.o... yes
> checking whether the g++ linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld)
> supports shared libraries... no
> checking dynamic linker characteristics... no
> checking how to hardcode library paths into programs... immediate
> checking whether stripping libraries is possible... yes
> appending configuration tag "F77" to libtool
> checking if libtool supports shared libraries... no
> checking whether to build shared libraries... no
> checking whether to build static libraries... yes
> checking for g77 option to produce PIC... -fPIC
> checking if g77 PIC flag -fPIC works... yes
> checking if g77 supports -c -o file.o... yes
> checking whether the g77 linker (/opt/gcc.3.3/i586-pc-interix3/bin/ld)
> supports shared libraries... no
> checking dynamic linker characteristics... no
> checking how to hardcode library paths into programs... immediate
> checking whether stripping libraries is possible... yes
> ./configure: : bad substitution
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From deepa1121 at yahoo.com  Tue Jul 10 15:29:36 2007
From: deepa1121 at yahoo.com (deepa gupta)
Date: Tue, 10 Jul 2007 06:29:36 -0700 (PDT)
Subject: [R] Help Needed!!
Message-ID: <120991.61017.qm@web36108.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/d0f18efe/attachment.pl 

From wwwhsd at gmail.com  Tue Jul 10 15:40:01 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Tue, 10 Jul 2007 10:40:01 -0300
Subject: [R] Help Needed!!
In-Reply-To: <120991.61017.qm@web36108.mail.mud.yahoo.com>
References: <120991.61017.qm@web36108.mail.mud.yahoo.com>
Message-ID: <da79af330707100640y1c7fa94fr8e804386ed42f600@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/c5ab53b4/attachment.pl 

From natekupp at gmail.com  Tue Jul 10 16:05:48 2007
From: natekupp at gmail.com (natekupp)
Date: Tue, 10 Jul 2007 07:05:48 -0700 (PDT)
Subject: [R] Simple table generation question
Message-ID: <11521582.post@talk.nabble.com>


Hey all,

I'm doing some work with machine learning on R (I'm a fairly new user of R),
and I have a question about generating new tables from existing tables.  I'm
currently using a table of measurements I read in from a CSV file to
generate training and validation data set tables for future use in a machine
learning algorithm using the code:

#generate probabilities to divide up training / validation data sets
randomly
device_Prob_Vector <- runif(num_Devices)

#NULL-initialize training and validation sets.  This seems like a bit of a
hack...
training_Set <- measurements[0]
validation_Set <- measurements[0]

#divide up the training and validation data sets from measurements.
for ( i in 1:num_Devices)
{
     	if ( device_Prob_Vector[i] > 0.5 )
     	{
     		training_Set <- rbind(training_Set, measurements[i,])
     	}
     	else
     	{
     		validation_Set <- rbind(validation_Set, measurements[i,])
     	}
}

This code works correctly, but takes quite a long time to execute.  I
suspect this is because rbind() is dynamically resizing the tables as it
adds new rows to each table of data.  Is there a way to pre-allocate memory
for each of the two tables, and then shrink them after the loop has
completed?  Thanks for the help.

~Nate
-- 
View this message in context: http://www.nabble.com/Simple-table-generation-question-tf4056042.html#a11521582
Sent from the R help mailing list archive at Nabble.com.


From b3i4old02 at sneakemail.com  Tue Jul 10 16:06:23 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Tue, 10 Jul 2007 15:06:23 +0100
Subject: [R] Lattice: vertical barchart
Message-ID: <f703pk$8om$1@sea.gmane.org>

barchart(Titanic, stack=F) produces a very nice horizontal barchart. 
Each panel has four groups of two bars.

barchart(Titanic, stack=F, horizontal=F) doesn't produce the results I 
would have expected, as it produces this warning message:

Warning message:
y should be numeric in: bwplot.formula(x = as.formula(form), data = 
list(Class = c(1,

And it results in each panel having 22 groups of 0-2 bars.

How can I produce something just like the original except with the 
orientation changed?

Thanks in advance.


From hao.liu at bms.com  Tue Jul 10 16:18:47 2007
From: hao.liu at bms.com (Hao Liu)
Date: Tue, 10 Jul 2007 10:18:47 -0400
Subject: [R] overlay boxplot
Message-ID: <46939547.6080700@bms.com>

hi, All:

I need to overlay two boxplot, I played around with points() but found 
it does not seem to work with boxplot, it works fine with other. Is 
there a way to overlay two boxplot (using different color) in R?

There was a thread talking about using ggplot package, however, I don't 
think there is a final solution... the answer give does not give overlay 
but a new plot.

Thanks
Hao


From h.wickham at gmail.com  Tue Jul 10 16:25:52 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 10 Jul 2007 16:25:52 +0200
Subject: [R] overlay boxplot
In-Reply-To: <46939547.6080700@bms.com>
References: <46939547.6080700@bms.com>
Message-ID: <f8e6ff050707100725m62212afak1c870fb87356deb6@mail.gmail.com>

You will get more useful answers if you specify exactly how you want
to overlay the boxplots (overlay them on what?).  You can certainly do
this with the ggplot2 package, or lattice or base graphics.

Hadley

On 7/10/07, Hao Liu <hao.liu at bms.com> wrote:
> hi, All:
>
> I need to overlay two boxplot, I played around with points() but found
> it does not seem to work with boxplot, it works fine with other. Is
> there a way to overlay two boxplot (using different color) in R?
>
> There was a thread talking about using ggplot package, however, I don't
> think there is a final solution... the answer give does not give overlay
> but a new plot.
>
> Thanks
> Hao
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Tue Jul 10 16:36:13 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 10 Jul 2007 09:36:13 -0500
Subject: [R] Writing Excel (.xls) files on non-Windows OSs using Perl
In-Reply-To: <D0D84D35-7AC9-43B0-B745-633B763ED51B@mac.com>
References: <1183926037.3706.89.camel@Bellerophon.localdomain>
	<47fce0650707090642j1bf4a0a3q45c2eb59ea94bd1c@mail.gmail.com>
	<1183992547.3731.16.camel@Bellerophon.localdomain>
	<971536df0707090810q645abd13o15352e73f99ffa70@mail.gmail.com>
	<46925F2D.1060902@statistik.uni-dortmund.de>
	<D0D84D35-7AC9-43B0-B745-633B763ED51B@mac.com>
Message-ID: <1184078173.3646.21.camel@Bellerophon.localdomain>

Greg,

You are certainly welcome to use my Perl script as the basis for a
write.xls() function for gdata.  You can even change the name of the
script to csv2xls.pl if you wish, for consistency with the existing
function.

If you might want to use the script largely 'as is', I won't have time
for a couple of weeks until I finish an interim analysis in progress,
but I could take a look at crafting a version of write.xls() as a
wrapper to the Perl script and create a .Rd file for it.

For ease of use and installation, we would need to think about including
the Perl modules that the script currently utilizes, which I see you do
with others for gdata in the 'perl' sub-dir. It looks like you have
OLE::Storage_Lite.pm there, which means that I could modify the code to
use Spreadsheet::WriteExcel rather than the 'Big' version, since the
former (as of version 2.17) supports .xls files > 7Mb with Storage_Lite
installed.

These would be:

http://search.cpan.org/~jmcnamara/Spreadsheet-WriteExcel
http://search.cpan.org/dist/Getopt-Long/
http://search.cpan.org/~nwclark/perl-5.8.8/ext/File/Glob/Glob.pm
http://search.cpan.org/~nwclark/perl-5.8.8/lib/File/Basename.pm
http://search.cpan.org/~hmbrand/Text-CSV_XS-0.29/CSV_XS.pm


HTH,

Marc

On Mon, 2007-07-09 at 17:53 -0400, Gregory Warnes wrote:
> Since I wrote the xls2csv.pl and read.xls() code for gdata, a perl  
> module for writing MS-Excel files has come on the scene.  I don't  
> have the time at the moment to create an csv2xls.pl file, but it  
> should be straightforward, and I would gladly add it to the gdata  
> package.
> 
> -G
> 
> 
> On Jul 9, 2007, at 12:15PM , Uwe Ligges wrote:
> 
> >
> >
> > Gabor Grothendieck wrote:
> >> Note that there already is a function, read.xls, in gdata that  
> >> uses Perl.
> >
> > Note that Marc talked about *writing* in his original message.
> >
> > Uwe Ligges
> >
> >
> >> On 7/9/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> >>> On Mon, 2007-07-09 at 16:42 +0300, Hans-Peter wrote:
> >>>> Hi,
> >>>>
> >>>> 2007/7/8, Marc Schwartz <marc_schwartz at comcast.net>:
> >>>>> [snip]
> >>>>> There exists the xlsReadWrite package on CRAN by Hans-Peter  
> >>>>> Suter, which
> >>>>> is restricted to Windows, since it utilizes the non-FOSS MS  
> >>>>> Office API
> >>>>> to write the Excel formats.
> >>>> The non-FOSS API is not the problem(#) but its implementation is:
> >>>>
> >>>> The 3rd party library I use is written in Pascal and supports  
> >>>> Delphi
> >>>> and Kylix. Kylix would allow to port the package to Linux but as  
> >>>> Kylix
> >>>> has unfortunately been abandoned by CodeGear (Borland) I am not
> >>>> ready/interested to spend my time on this dead road. Though it
> >>>> probably could be done quickly.
> >>>>
> >>>> A much more interesting way is to port the package using  
> >>>> FreePascal.
> >>>> --> I plan to do this since long but...
> >>>> --> Maybe someone fluent on Linux and FreePascal could have a  
> >>>> look at
> >>>> the pascal header files (treetron.googlepages.com) and make the  
> >>>> demos
> >>>> run on Linux..., that would be great and speed up an eventual
> >>>> xlsReadWrite port!
> >>> Thanks for the clarification.
> >>>
> >>> However, I think that if you are going to pursue a cross-platform
> >>> solution, providing source code requiring compilation (as opposed  
> >>> to a
> >>> pre-compiled Windows binary), you should consider what the  
> >>> installation
> >>> requirements for your package would then be.
> >>>
> >>> If you are going to take the step of requiring a prospective end- 
> >>> user to
> >>> have a particular Pascal compiler in place, you may as well have the
> >>> requirement for a Perl interpreter and associated packages. Since  
> >>> Perl
> >>> is widely available and you are more likely to find Perl-fluent  
> >>> coders
> >>> as opposed to Pascal-fluent coders (eg. I have not used Pascal  
> >>> since the
> >>> late 80's), I would urge you to consider Perl as a future  
> >>> substrate for
> >>> your functions.
> >>>
> >>> While compiled code will run faster than interpreted code, for these
> >>> types of file I/O functions, I am not sure that you lose much  
> >>> with Perl
> >>> from a performance standpoint and you certainly gain the eyes of  
> >>> a wider
> >>> audience with respect to use, debugging and enhancements.
> >>>
> >>> To that end, you (or any other interested parties) are free to  
> >>> utilize
> >>> my code in any way you deem appropriate. I did not state this in my
> >>> original post, but I make the code available under GPL(v2),  
> >>> freeing you
> >>> from any restrictions in its use, including your "Pro" version,  
> >>> as long
> >>> as you make the source available in a fashion consistent with the  
> >>> GPL
> >>> requirements.
> >>>
> >>> Regards,
> >>>
> >>> Marc Schwartz
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting- 
> >>> guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting- 
> >> guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting- 
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From yn19832 at msn.com  Tue Jul 10 16:36:35 2007
From: yn19832 at msn.com (livia)
Date: Tue, 10 Jul 2007 07:36:35 -0700 (PDT)
Subject: [R] Fraction ECDF
Message-ID: <11522204.post@talk.nabble.com>


Hi all,

I would like to plot part of the emperical CDF. Suppose the variable is x, I
just need the part when x>1,therefore, I am using the following codes. 

tail <- x>1
plot(ecdf(x[tail]), do.points=FALSE, verticals=TRUE)

The "x" value starts from 1, but the yaxs still begins from 0, not the
corresponding value when "x" is 1. How can I make it match?

Could anyone give me some advice? Many thanks.
-- 
View this message in context: http://www.nabble.com/Fraction-ECDF-tf4056229.html#a11522204
Sent from the R help mailing list archive at Nabble.com.


From sundar.dorai-raj at pdf.com  Tue Jul 10 16:51:31 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 10 Jul 2007 07:51:31 -0700
Subject: [R] Lattice: vertical barchart
In-Reply-To: <f703pk$8om$1@sea.gmane.org>
References: <f703pk$8om$1@sea.gmane.org>
Message-ID: <46939CF3.6050304@pdf.com>



Michael Hoffman said the following on 7/10/2007 7:06 AM:
> barchart(Titanic, stack=F) produces a very nice horizontal barchart. 
> Each panel has four groups of two bars.
> 
> barchart(Titanic, stack=F, horizontal=F) doesn't produce the results I 
> would have expected, as it produces this warning message:
> 
> Warning message:
> y should be numeric in: bwplot.formula(x = as.formula(form), data = 
> list(Class = c(1,
> 
> And it results in each panel having 22 groups of 0-2 bars.
> 
> How can I produce something just like the original except with the 
> orientation changed?
> 
> Thanks in advance.
> 

Hi, Michael,

It seems that barchart.table doesn't allow the horizontal = FALSE 
argument. With a slight modification to barchart.table this can be 
accomplished. Also, I don't get a warning with your original code using 
R-2.5.1 and lattice 0.16-1.

HTH,

--sundar

barchart.table <-
function (x, data = NULL, groups = TRUE, origin = 0, stack = TRUE,
     horizontal = TRUE, ...) ## add horizontal argument
{
     formula <- x
     ocall <- sys.call(sys.parent())
     if (!is.null(data))
         warning("explicit 'data' specification ignored")
     data <- as.data.frame(formula)
     nms <- names(data)
     freq <- which(nms == "Freq")
     nms <- nms[-freq]

     ## SD: change formula if horizontal == FALSE
     form <- if(horizontal) {
       paste(nms[1], "Freq", sep = "~")
     } else {
       paste("Freq", nms[1], sep = "~")
     }
     ## SD: end change

     nms <- nms[-1]
     len <- length(nms)
     if (is.logical(groups) && groups && len > 0) {
         groups <- as.name(nms[len])
         nms <- nms[-len]
         len <- length(nms)
     }
     else groups <- NULL
     if (len > 0) {
         rest <- paste(nms, collapse = "+")
         form <- paste(form, rest, sep = "|")
     }
     ans <- barchart(as.formula(form), data, groups = eval(groups),
         origin = origin, stack = stack, ...)
     ans$call <- ocall
     ans
}


barchart(Titanic, stack = FALSE)
barchart(Titanic, stack = FALSE, horizontal = FALSE)


From rvaradhan at jhmi.edu  Tue Jul 10 16:54:24 2007
From: rvaradhan at jhmi.edu (RAVI VARADHAN)
Date: Tue, 10 Jul 2007 10:54:24 -0400
Subject: [R] integration over a simplex
In-Reply-To: <3CC4D963-CFFC-4B7E-8605-3A40374968A7@noc.soton.ac.uk>
References: <3CC4D963-CFFC-4B7E-8605-3A40374968A7@noc.soton.ac.uk>
Message-ID: <f579a02711b8.46936560@johnshopkins.edu>

Hi Robin,

A Monte-Carlo approach could be attempted, if one could generate samples that are either uniformly distributed over the simplex.  There is a small section in Luc Devroye's book (Generation of Non-uniform random deviates) on random uniform sampling from a simplex, if I remeber correctly.
Another approach is importance sampling, where the sampling points have a characterized distribution.  I have seen a technique called polyEDA, based on Gibbs sampling and truncated multivariate normal distribution.  I had previously emailed the authors of this approach for the code, but haven't received a reply yet.  You can google "polyEDA" for more info.  

I am interested in various computational problems related to polyhedra (e.g. enumeration of vertices, locating extrema, random sampling).  I would appreciate if you'd keep me posted on how you solved this problem.  

Best,
Ravi.

----- Original Message -----
From: Robin Hankin <r.hankin at noc.soton.ac.uk>
Date: Tuesday, July 10, 2007 6:58 am
Subject: [R] integration over a simplex
To: RHelp help <r-help at stat.math.ethz.ch>


> Hello
>  
>  The excellent adapt package integrates over multi-dimensional
>  hypercubes.
>  
>  I want to integrate over a multidimensional simplex.  Has anyone
>  implemented such a thing in R?
>  
>  I can transform an n-simplex to a hyperrectangle
>  but the Jacobian is a rapidly-varying (and very lopsided)
>  function and this is making adapt() slow.
>  
>  [
>  A \dfn{simplex} is an n-dimensional analogue of a triangle or  
>  tetrahedron.
>  It is the convex hull of (n+1) points in an n-dimensional Euclidean  
> 
>  space.
>  
>  My application is a variant of the Dirichlet distribution:
>  With p~D(a), if length(p) = n+1 then the requirement that
>  all(p>0) and sum(p)=1 mean that the support of the
>  Dirichlet distribution is an n-simplex.
>  ]
>  
>  
>  --
>  Robin Hankin
>  Uncertainty Analyst
>  National Oceanography Centre, Southampton
>  European Way, Southampton SO14 3ZH, UK
>    tel  023-8059-7743
>  
>  ______________________________________________
>  R-help at stat.math.ethz.ch mailing list
>  
>  PLEASE do read the posting guide 
>  and provide commented, minimal, self-contained, reproducible code.


From hao.liu at bms.com  Tue Jul 10 16:54:31 2007
From: hao.liu at bms.com (Hao Liu)
Date: Tue, 10 Jul 2007 10:54:31 -0400
Subject: [R] overlay boxplot
In-Reply-To: <f8e6ff050707100725m62212afak1c870fb87356deb6@mail.gmail.com>
References: <46939547.6080700@bms.com>
	<f8e6ff050707100725m62212afak1c870fb87356deb6@mail.gmail.com>
Message-ID: <46939DA7.6010001@bms.com>

Thanks... I just realized using add=TRUE will work...
Best
Hao

hadley wickham wrote:

> You will get more useful answers if you specify exactly how you want
> to overlay the boxplots (overlay them on what?).  You can certainly do
> this with the ggplot2 package, or lattice or base graphics.
>
> Hadley
>
> On 7/10/07, Hao Liu <hao.liu at bms.com> wrote:
>
>> hi, All:
>>
>> I need to overlay two boxplot, I played around with points() but found
>> it does not seem to work with boxplot, it works fine with other. Is
>> there a way to overlay two boxplot (using different color) in R?
>>
>> There was a thread talking about using ggplot package, however, I don't
>> think there is a final solution... the answer give does not give overlay
>> but a new plot.
>>
>> Thanks
>> Hao
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From mazatlanmexico at yahoo.com  Tue Jul 10 16:58:13 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Tue, 10 Jul 2007 07:58:13 -0700 (PDT)
Subject: [R] How to plot two variables using a secondary Y axis
Message-ID: <195957.24352.qm@web56612.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/cfae199d/attachment.pl 

From murdoch at stats.uwo.ca  Tue Jul 10 17:03:48 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 10 Jul 2007 11:03:48 -0400
Subject: [R] Fraction ECDF
In-Reply-To: <11522204.post@talk.nabble.com>
References: <11522204.post@talk.nabble.com>
Message-ID: <46939FD4.2060801@stats.uwo.ca>

On 7/10/2007 10:36 AM, livia wrote:
> Hi all,
> 
> I would like to plot part of the emperical CDF. Suppose the variable is x, I
> just need the part when x>1,therefore, I am using the following codes. 
> 
> tail <- x>1
> plot(ecdf(x[tail]), do.points=FALSE, verticals=TRUE)
> 
> The "x" value starts from 1, but the yaxs still begins from 0, not the
> corresponding value when "x" is 1. How can I make it match?
> 
> Could anyone give me some advice? Many thanks.

Rather than subsetting the x, I'd just use xlim and ylim arguments to 
plot() to change the range.  For example,

plot(ecdf(x), do.points=FALSE, verticals=TRUE, xlim=c(1, max(x)), 
ylim=c(1-sum(x>1)/length(x), 1))

Duncan Murdoch


From sundar.dorai-raj at pdf.com  Tue Jul 10 17:14:48 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 10 Jul 2007 08:14:48 -0700
Subject: [R] How to plot two variables using a secondary Y axis
In-Reply-To: <195957.24352.qm@web56612.mail.re3.yahoo.com>
References: <195957.24352.qm@web56612.mail.re3.yahoo.com>
Message-ID: <4693A268.7080900@pdf.com>



Felipe Carrillo said the following on 7/10/2007 7:58 AM:
>           Date  Fo  Co    6/27/2007  57.1  13.9    6/28/2007  57.7  14.3    6/29/2007  57.8  14.3    6/30/2007  57  13.9    7/1/2007  57.1  13.9    7/2/2007  57.2  14.0    7/3/2007  57.3  14.1    7/4/2007  57.6  14.2    7/5/2007  58  14.4    7/6/2007  58.1  14.5    7/7/2007  58.2  14.6    7/8/2007  58.4  14.7    7/9/2007    58.7
>     14.8
>    
>   Hello all:
>   I am a newbie to R, and I was wondering how can I plot the Temperature values above using Lattice or ggplot2 code. I want Date(X axis), Degrees F(Y axis) and Degrees C( on a secondary Y axis). 
>   Thanks
>    
>    
>    

For lattice, see this thread:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/102768.html

HTH,

--sundar


From b3i4old02 at sneakemail.com  Tue Jul 10 17:28:56 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Tue, 10 Jul 2007 16:28:56 +0100
Subject: [R] Lattice: vertical barchart
In-Reply-To: <46939CF3.6050304@pdf.com>
References: <f703pk$8om$1@sea.gmane.org> <46939CF3.6050304@pdf.com>
Message-ID: <f708kd$rru$1@sea.gmane.org>

Sundar Dorai-Raj wrote:

> It seems that barchart.table doesn't allow the horizontal = FALSE 
> argument. With a slight modification to barchart.table this can be 
> accomplished.

Thanks for supplying that.

> Also, I don't get a warning with your original code using 
> R-2.5.1 and lattice 0.16-1.

Thanks. I should have specified I am using R-2.4.0.


From torma at sztaki.hu  Tue Jul 10 17:32:16 2007
From: torma at sztaki.hu (Balazs Torma)
Date: Tue, 10 Jul 2007 17:32:16 +0200
Subject: [R] matrix of bins with different length
Message-ID: <20070710173216.sry923ws8w440kkg@webmail.sztaki.hu>

Dear users,

    please help to define the following data structure:

I would like to have a matrix, where every element is a container of  
different size , containing real numbers. The containers (bins) are  
addressed by an index pair [i,j] (i is number of corresponding row of  
the matrix, j is the coloumn of the matrix). The containers are  
initially empty, I would like to fill them dynamically (put certain  
numbers into different bins in each iteration).

I can not define a 3 dimensional array, because I don't know the  
length of the third dimension in advance, and because the vectors  
(containers) in the matrix are usually of different length.

Any help greatly appreciated,
Balazs Torma


From ggrothendieck at gmail.com  Tue Jul 10 17:38:16 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 10 Jul 2007 11:38:16 -0400
Subject: [R] matrix of bins with different length
In-Reply-To: <20070710173216.sry923ws8w440kkg@webmail.sztaki.hu>
References: <20070710173216.sry923ws8w440kkg@webmail.sztaki.hu>
Message-ID: <971536df0707100838m2adbfcd8sf4cf194cc9c7e9e1@mail.gmail.com>

Try this:

> m <- matrix(list(1, 1:2, 1:3, 1:4), 2)
> m[[1,1]]
[1] 1
> m[[2,1]]
[1] 1 2
> m
     [,1]      [,2]
[1,] 1         Integer,3
[2,] Integer,2 Integer,4


On 7/10/07, Balazs Torma <torma at sztaki.hu> wrote:
> Dear users,
>
>    please help to define the following data structure:
>
> I would like to have a matrix, where every element is a container of
> different size , containing real numbers. The containers (bins) are
> addressed by an index pair [i,j] (i is number of corresponding row of
> the matrix, j is the coloumn of the matrix). The containers are
> initially empty, I would like to fill them dynamically (put certain
> numbers into different bins in each iteration).
>
> I can not define a 3 dimensional array, because I don't know the
> length of the third dimension in advance, and because the vectors
> (containers) in the matrix are usually of different length.
>
> Any help greatly appreciated,
> Balazs Torma
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sapsi at pobox.com  Tue Jul 10 17:41:13 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 10 Jul 2007 11:41:13 -0400
Subject: [R] How to preserve data across function calls in a library package
Message-ID: <6F70270F-4DF1-4D34-8444-FCBF8F035030@pobox.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/11bf0ee4/attachment.pl 

From yee at post.harvard.edu  Tue Jul 10 17:56:41 2007
From: yee at post.harvard.edu (Andrew Yee)
Date: Tue, 10 Jul 2007 11:56:41 -0400
Subject: [R] why doesn't as.character of this factor create a vector of
	characters?
Message-ID: <5dff5a0d0707100856n3b804f2bl72a9f99a45058072@mail.gmail.com>

I'm trying to figure out why when I use as.character() on one row of a
data.frame, I get factor numbers instead of a character vector.  Any
suggestions?

See the following code:

a<-c("Abraham","Jonah","Moses")
b<-c("Sarah","Hannah","Mary")
c<-c("Billy","Joe","Bob")

df<-data.frame(a=a,b=b,c=c)

#Suppose I'm interested in one line of this data frame but as a vector

one.line <- df[df$a=="Abraham",]

#However the following illustrates the problem I'm having

one.line <- as.vector(df[df$a=="Abraham",]) #Creates a one row
data.frame instead of a vector!

#compare above to

one.line <- as.character(df[df$a=="Abraham",]) #Creates a vector of 1, 3, 1!

#In the end, this creates the output that I'd like:

one.line <-as.vector(t(df[df$a=="Abraham",])) #but it seems like a lot of work!


From Greg.Snow at intermountainmail.org  Tue Jul 10 18:02:08 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 10 Jul 2007 10:02:08 -0600
Subject: [R] type III ANOVA for a nested linear model
In-Reply-To: <B998A44C8986644EA8029CFE6396A924B68056@exqld2-bne.nexus.csiro.au>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAAD4EA@LP-EXCHVS07.CO.IHC.COM>


I nominate the following 2 pieces from Bill's reply for fortunes
(probably 2 separate fortunes):
 


> All this becomes even more glaring if you take the unusal 
> step of plotting the data.

and

> What sort of editor would overlook this clear and 
> demonstrable message leaping out from the data in favour of 
> some arcane argument about "types of sums of squares"?  
> Several answers come to mind: A power freak, a SAS 
> afficianado, an idiot.


-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111


From yn19832 at msn.com  Tue Jul 10 18:15:46 2007
From: yn19832 at msn.com (livia)
Date: Tue, 10 Jul 2007 09:15:46 -0700 (PDT)
Subject: [R] Fraction ECDF
In-Reply-To: <46939FD4.2060801@stats.uwo.ca>
References: <11522204.post@talk.nabble.com> <46939FD4.2060801@stats.uwo.ca>
Message-ID: <11524206.post@talk.nabble.com>


Thank you very much.


Duncan Murdoch-2 wrote:
> 
> On 7/10/2007 10:36 AM, livia wrote:
>> Hi all,
>> 
>> I would like to plot part of the emperical CDF. Suppose the variable is
>> x, I
>> just need the part when x>1,therefore, I am using the following codes. 
>> 
>> tail <- x>1
>> plot(ecdf(x[tail]), do.points=FALSE, verticals=TRUE)
>> 
>> The "x" value starts from 1, but the yaxs still begins from 0, not the
>> corresponding value when "x" is 1. How can I make it match?
>> 
>> Could anyone give me some advice? Many thanks.
> 
> Rather than subsetting the x, I'd just use xlim and ylim arguments to 
> plot() to change the range.  For example,
> 
> plot(ecdf(x), do.points=FALSE, verticals=TRUE, xlim=c(1, max(x)), 
> ylim=c(1-sum(x>1)/length(x), 1))
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Fraction-ECDF-tf4056229.html#a11524206
Sent from the R help mailing list archive at Nabble.com.


From sapsi at pobox.com  Tue Jul 10 18:33:26 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 10 Jul 2007 12:33:26 -0400
Subject: [R] How to preserve data across function calls in a library
	package
In-Reply-To: <6F70270F-4DF1-4D34-8444-FCBF8F035030@pobox.com>
References: <6F70270F-4DF1-4D34-8444-FCBF8F035030@pobox.com>
Message-ID: <A00673FA-F462-4CD4-8D3C-03DE3B9CC2C6@pobox.com>

Hi,
	Some progress: I am using
	SEXP retty;
	book=Calloc(1,int);
	*book=10;
	PROTECT(retty=R_MakeExternalPtr(book,R_NilValue,R_NilValue));
	
	then UNPROTECTING and returning retty.

	In a another function,
	foo(SEXP s){
	 int* f=(int *)R_ExternalPtrAddr(p);
	 Rprintf("many times %d\n",*f);	
}

	When called do_foo(p) where do_foo calls foo and p is the pointer  
returned by the former code snippet, the Rprintf successfully prints  
the correct value but subsequently crashes
*** caught bus error ***
address 0x0, cause 'invalid alignment'.

	I can't figure out why... I would appreciate any advice provided.
	Rgds
	Saptarshi




On Jul 10, 2007, at 11:41 AM, Saptarshi Guha wrote:

> Hi,
> 	I am writing an R package with two functions in C++. So far
> everything works.
> 	Now, i would like to write a third function which would use a pointer
> (it is a pointer to a class object) created by first function.
> 	I tried placing this pointer outside of the function definitions
> (i.e to make it global) but when called in the 3rd function i get
>> 	 *** caught bus error ***
> address 0x0, cause 'invalid alignment'"
>
> 	I tried Callocing it in the 1st function but to no avail. Here is a
> quick summary. When foo is called (through do_foo, **after** having
> called do_kNN_e) i get the aforementioned error.
> 	Can anyone provide some pointers (no pun intended) on this?
>
> 	Thanks
> 	Saptarshi
>
> ANN* book;
> int* foot;
>
> void foo(void){
>    Rprintf("many times\n");
>    Rprintf("%p\n",book);
>    Rprintf("%p\n",foot);
> }
>
> SEXP
> kNN_e(SEXP data, SEXP Nrow, SEXP Ncol,SEXP K,SEXP Eps)
> {
>    int nrow=asInteger(Nrow);
>    int ncol=asInteger(Ncol);
>    int k=asInteger(K);
>    double eps=asReal(Eps);
>
>    SEXP ans,distance;
>    SEXP retlist;
>    PROTECT(ans=allocMatrix(INTSXP,nrow,k)); //The 2nd argument gives
> the number of rows, and the last the number of cols see http://cran.r-
> project.org/doc/manuals/R-exts.html
>    PROTECT(distance=allocMatrix(REALSXP,nrow,k));
>    ANNpointArray datapoints;
>    ANNpoint qpoint;
>    ANNkd_tree* kdTree;
>    book=Calloc(1,ANN*);
>    foot=Calloc(1,int);
>    book=kdTree;
>   *foot=10;
>
>   .......
> }
>
> extern "C" {
>    void do_foo(void){
>      foo();
>    }
>
> SEXP
> do_kNN_e(SEXP data, SEXP Nrow, SEXP Ncol,SEXP k,SEXP eps)
> {
>    return kNN_e(data,Nrow, Ncol,
> 	     k,eps);
>
> }
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
Would you people stop playing these stupid games?!?!?!!!!


From yn19832 at msn.com  Tue Jul 10 18:35:04 2007
From: yn19832 at msn.com (livia)
Date: Tue, 10 Jul 2007 09:35:04 -0700 (PDT)
Subject: [R] ECDF, distribution of Pareto, distribution of Normal
Message-ID: <11524560.post@talk.nabble.com>


Hello all,

I would like to plot the emperical CDF, normal CDF and pareto CDF in the
same graph and I amusing the following codes. "z" is a vector and I just
need the part when z between 1.6 and 3.

plot(ecdf(z), do.points=FALSE, verticals=TRUE,
xlim=c(1.6,3),ylim=c(1-sum(z>1.6)/length(z), 1))

x <- seq(1.6, 3, 0.1)
lines(x,pgpd(x, 1.544,0.4373,-0.2398), col="red")

y <- seq(1.6, 3, 0.1)
lines(y,pnorm(y, mean(z),sqrt(var(z))), col="blue")

The emperical CDF and normal CDF look rather resonable, but the pareto CDF
looks quite odd. I am not sure whether I plot the pareto CDF correctly e.g.
in the right yaxs or any other mistake?

At the same time, let "t" represents the vector whose values are larger than
1.6(the part we want). If I implement the following codes and plot the
emperical CDF and pareto CDF, the pareto CDF seems fit.

plot(ecdf(t), do.points=FALSE, verticals=TRUE)
x <- seq(1.6, 3, 0.1)
lines(x,pgpd(x, 1.544,0.4373,-0.2398), col="red")

Could anyone give me some advice on this? Many thanks.
-- 
View this message in context: http://www.nabble.com/ECDF%2C-distribution-of-Pareto%2C-distribution-of-Normal-tf4056943.html#a11524560
Sent from the R help mailing list archive at Nabble.com.


From bartjoosen at hotmail.com  Tue Jul 10 16:55:56 2007
From: bartjoosen at hotmail.com (Bartjoosen)
Date: Tue, 10 Jul 2007 07:55:56 -0700 (PDT)
Subject: [R] Simple table generation question
In-Reply-To: <11521582.post@talk.nabble.com>
References: <11521582.post@talk.nabble.com>
Message-ID: <11522530.post@talk.nabble.com>


Maybe this is what you want:

you are right about the re-allocating the tables, but you can subset your
table into a new one:

selection <- which(device_Prob_Vector > 0.5)
# or via sample: selection <- sample(num_Devices)
training_Set <- measurements[selection]
validation_Set <- measurements[-selection]

good luck

Bart



natekupp wrote:
> 
> Hey all,
> 
> I'm doing some work with machine learning on R (I'm a fairly new user of
> R), and I have a question about generating new tables from existing
> tables.  I'm currently using a table of measurements I read in from a CSV
> file to generate training and validation data set tables for future use in
> a machine learning algorithm using the code:
> 
> #generate probabilities to divide up training / validation data sets
> randomly
> device_Prob_Vector <- runif(num_Devices)
> 
> #NULL-initialize training and validation sets.  This seems like a bit of a
> hack...
> training_Set <- measurements[0]
> validation_Set <- measurements[0]
> 
> #divide up the training and validation data sets from measurements.
> for ( i in 1:num_Devices)
> {
>      	if ( device_Prob_Vector[i] > 0.5 )
>      	{
>      		training_Set <- rbind(training_Set, measurements[i,])
>      	}
>      	else
>      	{
>      		validation_Set <- rbind(validation_Set, measurements[i,])
>      	}
> }
> 
> This code works correctly, but takes quite a long time to execute.  I
> suspect this is because rbind() is dynamically resizing the tables as it
> adds new rows to each table of data.  Is there a way to pre-allocate
> memory for each of the two tables, and then shrink them after the loop has
> completed?  Thanks for the help.
> 
> ~Nate
> 

-- 
View this message in context: http://www.nabble.com/Simple-table-generation-question-tf4056042.html#a11522530
Sent from the R help mailing list archive at Nabble.com.


From milton_ruser at yahoo.com.br  Tue Jul 10 18:43:42 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Tue, 10 Jul 2007 09:43:42 -0700 (PDT)
Subject: [R] Crossing native ArcGis GRID with a XY coordinate table
Message-ID: <419570.66946.qm@web56612.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/dd200c04/attachment.pl 

From murdoch at stats.uwo.ca  Tue Jul 10 19:25:06 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 10 Jul 2007 13:25:06 -0400
Subject: [R] integration over a simplex
In-Reply-To: <3CC4D963-CFFC-4B7E-8605-3A40374968A7@noc.soton.ac.uk>
References: <3CC4D963-CFFC-4B7E-8605-3A40374968A7@noc.soton.ac.uk>
Message-ID: <4693C0F2.5080306@stats.uwo.ca>

On 7/10/2007 6:57 AM, Robin Hankin wrote:
> Hello
> 
> The excellent adapt package integrates over multi-dimensional
> hypercubes.
> 
> I want to integrate over a multidimensional simplex.  Has anyone
> implemented such a thing in R?
> 
> I can transform an n-simplex to a hyperrectangle
> but the Jacobian is a rapidly-varying (and very lopsided)
> function and this is making adapt() slow.
> 
> [
> A \dfn{simplex} is an n-dimensional analogue of a triangle or  
> tetrahedron.
> It is the convex hull of (n+1) points in an n-dimensional Euclidean  
> space.
> 
> My application is a variant of the Dirichlet distribution:
> With p~D(a), if length(p) = n+1 then the requirement that
> all(p>0) and sum(p)=1 mean that the support of the
> Dirichlet distribution is an n-simplex.

I don't know what shape of simplex you're working with, but I believe 
the subset of an n-cube with coordinates ordered x[1] < x[2] < ... < 
x[n] is a simplex, and the cube can be tiled with n! of those, by 
permuting the order of the coordinates.  So if your function is smooth 
enough at the edges you might be able to map n! copies of it onto a 
cube, and use adapt to integrate over that.

That is:  if f() is your function, defined on 0 < x[1] < x[2] < ... < 
x[n] < 1, define g <- function(x) f(sort(x)), and the integral you want 
is (1/n!) times the integral of g over the unit cube.

Duncan Murdoch


From singularitaet at gmx.net  Tue Jul 10 19:30:53 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Tue, 10 Jul 2007 19:30:53 +0200
Subject: [R] ECDF, distribution of Pareto, distribution of Normal
In-Reply-To: <11524560.post@talk.nabble.com>
References: <11524560.post@talk.nabble.com>
Message-ID: <4693C24D.6060605@gmx.net>



-------- Original Message  --------
Subject: [R] ECDF, distribution of Pareto, distribution of Normal
From: livia <yn19832 at msn.com>
To: r-help at stat.math.ethz.ch
Date: Tue Jul 10 2007 18:35:04 GMT+0200
> Hello all,
>
> I would like to plot the emperical CDF, normal CDF and pareto CDF in the
> same graph and I amusing the following codes. "z" is a vector and I just
> need the part when z between 1.6 and 3.
>
> plot(ecdf(z), do.points=FALSE, verticals=TRUE,
> xlim=c(1.6,3),ylim=c(1-sum(z>1.6)/length(z), 1))
>
> x <- seq(1.6, 3, 0.1)
> lines(x,pgpd(x, 1.544,0.4373,-0.2398), col="red")
>   

There is something wrong with your pgpd function, see ?pgpd for help and
parameters... (I wonder how you got something plotted here...)


> y <- seq(1.6, 3, 0.1)
> lines(y,pnorm(y, mean(z),sqrt(var(z))), col="blue")
>
> The emperical CDF and normal CDF look rather resonable, but the pareto CDF
> looks quite odd. I am not sure whether I plot the pareto CDF correctly e.g.
> in the right yaxs or any other mistake?
>
> At the same time, let "t" represents the vector whose values are larger than
> 1.6(the part we want). If I implement the following codes and plot the
> emperical CDF and pareto CDF, the pareto CDF seems fit.
>
> plot(ecdf(t), do.points=FALSE, verticals=TRUE)
> x <- seq(1.6, 3, 0.1)
> lines(x,pgpd(x, 1.544,0.4373,-0.2398), col="red")
>
> Could anyone give me some advice on this? Many thanks.
>


From kdestler at u.washington.edu  Tue Jul 10 19:37:54 2007
From: kdestler at u.washington.edu (kdestler)
Date: Tue, 10 Jul 2007 10:37:54 -0700 (PDT)
Subject: [R] Help with write.foreign (exporting data to Stata)
Message-ID: <11525796.post@talk.nabble.com>


Hi.  I'm trying to export a dataframe from R into Stata to use a statistical
function I have there.  I attached library write.foreign and renamed my
variables to get them to match Stata's required format, and now have the
following error:  "file /tmp/Rtmps7rmrM/file1c06dac8.raw not found"  Other
than typing write.foreign, do I need to do something in R to get it to save
the file on my hard drive?  When I search for the file name on my computer
nothing comes up.  I'm using a Mac in case that makes a difference.

Thanks,
Kate
-- 
View this message in context: http://www.nabble.com/Help-with-write.foreign-%28exporting-data-to-Stata%29-tf4057346.html#a11525796
Sent from the R help mailing list archive at Nabble.com.


From gunter.berton at gene.com  Tue Jul 10 19:53:44 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Tue, 10 Jul 2007 10:53:44 -0700
Subject: [R] why doesn't as.character of this factor create a vector
	ofcharacters?
In-Reply-To: <5dff5a0d0707100856n3b804f2bl72a9f99a45058072@mail.gmail.com>
Message-ID: <005b01c7c31b$425ef080$4d908980@gne.windows.gene.com>

Andrew:

As you haven't received a reply yet ...

?factor,?UseMethod, and "An Introduction to R" may help. But it's a bit
subtle.

Factors are objects that are integer vectors (codes) with a levels attribute
that associates the codes with levels as character names. So
df[df$a=="Abraham",] is a data.frame in which the columns are still factors.
as.character() is a S3 generic function that calls the (internal) default
method on a data.frame. This obviously just turns the vector of integers
into characters and ignores the levels attribute.

t() is also a S3 generic with a data.frame method. This merely converts the
data.frame to a matrix via as.matrix and then applies t() to the matrix. The
as.matrix() method for data.frames captures the levels and converts the
data.frame to a character matrix with the level names, not their numeric
codes.So another perhaps more intuitive but also more storage intensive way
(I think) of doing what you wantthat avoids the transpose and as.vector()
conversion would be:

mx <- as.matrix(df)
mx[mx[,"a"]=="Abraham",,drop=TRUE]

HTH.

Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andrew Yee
Sent: Tuesday, July 10, 2007 8:57 AM
To: r-help at stat.math.ethz.ch
Subject: [R] why doesn't as.character of this factor create a vector
ofcharacters?

I'm trying to figure out why when I use as.character() on one row of a
data.frame, I get factor numbers instead of a character vector.  Any
suggestions?

See the following code:

a<-c("Abraham","Jonah","Moses")
b<-c("Sarah","Hannah","Mary")
c<-c("Billy","Joe","Bob")

df<-data.frame(a=a,b=b,c=c)

#Suppose I'm interested in one line of this data frame but as a vector

one.line <- df[df$a=="Abraham",]

#However the following illustrates the problem I'm having

one.line <- as.vector(df[df$a=="Abraham",]) #Creates a one row
data.frame instead of a vector!

#compare above to

one.line <- as.character(df[df$a=="Abraham",]) #Creates a vector of 1, 3, 1!

#In the end, this creates the output that I'd like:

one.line <-as.vector(t(df[df$a=="Abraham",])) #but it seems like a lot of
work!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From hillyons at stat.washington.edu  Tue Jul 10 19:56:16 2007
From: hillyons at stat.washington.edu (Hil Lyons)
Date: Tue, 10 Jul 2007 10:56:16 -0700 (PDT)
Subject: [R] Formatting panel borders in lattice package
Message-ID: <Pine.LNX.4.64.0707101042480.480@madrid2.stat.washington.edu>

Hello all --

I would like to thicken the borders between panels -- or more generally, 
all borders -- in a plot generated using lattice (specifically, 
levelplot).  Something similar perhaps to box() function in graphics. 
I haven't been successful in reviewing available documentation.

The problem stems from using a grayscale colorscheme that puts black 
colors near the borders, thus obscuring them.  Then, another alternative 
would be to somehow inject white space between panels.

In summary:
1) Does anybody know of a good way to increase the line thickness of 
borders in lattice, specifically those separating panels?
2) Does anybody know of a way to separate panels with whitespace?

Advice is very much appreciated.

Thanks,
Hil Lyons
Graduate Student
Univ. of Washington Dept of Statistics


From singularitaet at gmx.net  Tue Jul 10 20:00:28 2007
From: singularitaet at gmx.net (Stefan Grosse)
Date: Tue, 10 Jul 2007 20:00:28 +0200
Subject: [R] Help with write.foreign (exporting data to Stata)
In-Reply-To: <11525796.post@talk.nabble.com>
References: <11525796.post@talk.nabble.com>
Message-ID: <4693C93C.7090206@gmx.net>

I am not sure what you are doing there but what you need is
library(foreign)
and
write.dta()

see
?write.dta once you have loaded the foreign package

Stefan

-------- Original Message  --------
Subject: [R] Help with write.foreign (exporting data to Stata)
From: kdestler <kdestler at u.washington.edu>
To: r-help at stat.math.ethz.ch
Date: Tue Jul 10 2007 19:37:54 GMT+0200
> Hi.  I'm trying to export a dataframe from R into Stata to use a statistical
> function I have there.  I attached library write.foreign and renamed my
> variables to get them to match Stata's required format, and now have the
> following error:  "file /tmp/Rtmps7rmrM/file1c06dac8.raw not found"  Other
> than typing write.foreign, do I need to do something in R to get it to save
> the file on my hard drive?  When I search for the file name on my computer
> nothing comes up.  I'm using a Mac in case that makes a difference.
>
> Thanks,
> Kate
>


From sapsi at pobox.com  Tue Jul 10 20:01:18 2007
From: sapsi at pobox.com (Saptarshi Guha)
Date: Tue, 10 Jul 2007 14:01:18 -0400
Subject: [R] How to preserve data across function calls in a library
	package
In-Reply-To: <A00673FA-F462-4CD4-8D3C-03DE3B9CC2C6@pobox.com>
References: <6F70270F-4DF1-4D34-8444-FCBF8F035030@pobox.com>
	<A00673FA-F462-4CD4-8D3C-03DE3B9CC2C6@pobox.com>
Message-ID: <CC14F374-4A9E-42D3-B9E0-2DA277D61032@pobox.com>


On Jul 10, 2007, at 12:33 PM, Saptarshi Guha wrote:

> Hi,
> 	Some progress: I am using
> 	SEXP retty;
> 	book=Calloc(1,int);
> 	*book=10;
> 	PROTECT(retty=R_MakeExternalPtr(book,R_NilValue,R_NilValue));
> 	
> 	then UNPROTECTING and returning retty.
>
> 	In a another function,
> 	foo(SEXP s){
> 	 int* f=(int *)R_ExternalPtrAddr(p);
> 	 Rprintf("many times %d\n",*f);	
> }
>
> 	When called do_foo(p) where do_foo calls foo and p is the pointer
> returned by the former code snippet, the Rprintf successfully prints
> the correct value but subsequently crashes
> *** caught bus error ***
> address 0x0, cause 'invalid alignment'.
>
> 	I can't figure out why... I would appreciate any advice provided.
> 	Rgds
> 	Saptarshi
>

One quick solution, change the function foo, to
SEXP foo(SEXP s){
...
return(s)
}
and make corresponding changes elsewhere. This should work.
Regards
Saptarshi










>
>
>
> On Jul 10, 2007, at 11:41 AM, Saptarshi Guha wrote:
>
>> Hi,
>> 	I am writing an R package with two functions in C++. So far
>> everything works.
>> 	Now, i would like to write a third function which would use a  
>> pointer
>> (it is a pointer to a class object) created by first function.
>> 	I tried placing this pointer outside of the function definitions
>> (i.e to make it global) but when called in the 3rd function i get
>>> 	 *** caught bus error ***
>> address 0x0, cause 'invalid alignment'"
>>
>> 	I tried Callocing it in the 1st function but to no avail. Here is a
>> quick summary. When foo is called (through do_foo, **after** having
>> called do_kNN_e) i get the aforementioned error.
>> 	Can anyone provide some pointers (no pun intended) on this?
>>
>> 	Thanks
>> 	Saptarshi
>>
>> ANN* book;
>> int* foot;
>>
>> void foo(void){
>>    Rprintf("many times\n");
>>    Rprintf("%p\n",book);
>>    Rprintf("%p\n",foot);
>> }
>>
>> SEXP
>> kNN_e(SEXP data, SEXP Nrow, SEXP Ncol,SEXP K,SEXP Eps)
>> {
>>    int nrow=asInteger(Nrow);
>>    int ncol=asInteger(Ncol);
>>    int k=asInteger(K);
>>    double eps=asReal(Eps);
>>
>>    SEXP ans,distance;
>>    SEXP retlist;
>>    PROTECT(ans=allocMatrix(INTSXP,nrow,k)); //The 2nd argument gives
>> the number of rows, and the last the number of cols see http:// 
>> cran.r-
>> project.org/doc/manuals/R-exts.html
>>    PROTECT(distance=allocMatrix(REALSXP,nrow,k));
>>    ANNpointArray datapoints;
>>    ANNpoint qpoint;
>>    ANNkd_tree* kdTree;
>>    book=Calloc(1,ANN*);
>>    foot=Calloc(1,int);
>>    book=kdTree;
>>   *foot=10;
>>
>>   .......
>> }
>>
>> extern "C" {
>>    void do_foo(void){
>>      foo();
>>    }
>>
>> SEXP
>> do_kNN_e(SEXP data, SEXP Nrow, SEXP Ncol,SEXP k,SEXP eps)
>> {
>>    return kNN_e(data,Nrow, Ncol,
>> 	     k,eps);
>>
>> }
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
> Would you people stop playing these stupid games?!?!?!!!!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

Saptarshi Guha | sapsi at pobox.com | http://www.stat.purdue.edu/~sguha
What ever happened to happily ever after?


From deepayan.sarkar at gmail.com  Tue Jul 10 20:15:29 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 10 Jul 2007 11:15:29 -0700
Subject: [R] Lattice: vertical barchart
In-Reply-To: <46939CF3.6050304@pdf.com>
References: <f703pk$8om$1@sea.gmane.org> <46939CF3.6050304@pdf.com>
Message-ID: <eb555e660707101115w63b493c0t50a1aa54c6bc5a99@mail.gmail.com>

On 7/10/07, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> wrote:
>
>
> Michael Hoffman said the following on 7/10/2007 7:06 AM:
> > barchart(Titanic, stack=F) produces a very nice horizontal barchart.
> > Each panel has four groups of two bars.
> >
> > barchart(Titanic, stack=F, horizontal=F) doesn't produce the results I
> > would have expected, as it produces this warning message:
> >
> > Warning message:
> > y should be numeric in: bwplot.formula(x = as.formula(form), data =
> > list(Class = c(1,
> >
> > And it results in each panel having 22 groups of 0-2 bars.
> >
> > How can I produce something just like the original except with the
> > orientation changed?
> >
> > Thanks in advance.
> >
>
> Hi, Michael,
>
> It seems that barchart.table doesn't allow the horizontal = FALSE
> argument. With a slight modification to barchart.table this can be
> accomplished.

Thanks Sundar. This seems like a reasonable feature; I'll add it in
the next update.

-Deepayan


From deepayan.sarkar at gmail.com  Tue Jul 10 20:32:25 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 10 Jul 2007 11:32:25 -0700
Subject: [R] Formatting panel borders in lattice package
In-Reply-To: <Pine.LNX.4.64.0707101042480.480@madrid2.stat.washington.edu>
References: <Pine.LNX.4.64.0707101042480.480@madrid2.stat.washington.edu>
Message-ID: <eb555e660707101132m6f788e6eobf37d2a8421992fd@mail.gmail.com>

On 7/10/07, Hil Lyons <hillyons at stat.washington.edu> wrote:
> Hello all --
>
> I would like to thicken the borders between panels -- or more generally,
> all borders -- in a plot generated using lattice (specifically,
> levelplot).  Something similar perhaps to box() function in graphics.
> I haven't been successful in reviewing available documentation.
>
> The problem stems from using a grayscale colorscheme that puts black
> colors near the borders, thus obscuring them.  Then, another alternative
> would be to somehow inject white space between panels.
>
> In summary:
> 1) Does anybody know of a good way to increase the line thickness of
> borders in lattice, specifically those separating panels?

qqmath(~height | voice.part, singer, par.settings = list(axis.line =
list(lwd = 3)))

(or probably closer to what you want)

qqmath(~height | voice.part, singer, par.settings = list(axis.line =
list(lwd = 3)),
       scales = list(lwd = 1))

> 2) Does anybody know of a way to separate panels with whitespace?

qqmath(~height | voice.part, singer, between = list(x = 0.5, y = 0.5))

-Deepayan


From tlumley at u.washington.edu  Tue Jul 10 21:57:56 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 10 Jul 2007 12:57:56 -0700 (PDT)
Subject: [R] Help with write.foreign (exporting data to Stata)
In-Reply-To: <4693C93C.7090206@gmx.net>
Message-ID: <Pine.LNX.4.43.0707101257560.7218@hymn09.u.washington.edu>

On Tue, 10 Jul 2007, Stefan Grosse wrote:

> I am not sure what you are doing there but what you need is
> library(foreign)
> and
> write.dta()


write.foreign should also work, though.

My guess is that Kate used tempfile() to specify the filenames, and that the data file would then have been deleted on leaving R.  This is only a guess, of course.

The syntax for write.dta is
   write.dta(the.data.set, file="dataset.dta")
and for write.foreign is
   write.foreign(the.data.set,codefile="dataset.do", datafile="dataset.raw",
        package="Stata")

     -thomas


> see
> ?write.dta once you have loaded the foreign package
>
> Stefan
>
> -------- Original Message  --------
> Subject: [R] Help with write.foreign (exporting data to Stata)
> From: kdestler <kdestler at u.washington.edu>
> To: r-help at stat.math.ethz.ch
> Date: Tue Jul 10 2007 19:37:54 GMT+0200
>> Hi.  I'm trying to export a dataframe from R into Stata to use a statistical
>> function I have there.  I attached library write.foreign and renamed my
>> variables to get them to match Stata's required format, and now have the
>> following error:  "file /tmp/Rtmps7rmrM/file1c06dac8.raw not found"  Other
>> than typing write.foreign, do I need to do something in R to get it to save
>> the file on my hard drive?  When I search for the file name on my computer
>> nothing comes up.  I'm using a Mac in case that makes a difference.
>>
>> Thanks,
>> Kate
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From hansenfrank at yahoo.com  Tue Jul 10 22:01:43 2007
From: hansenfrank at yahoo.com (Frank Hansen)
Date: Tue, 10 Jul 2007 13:01:43 -0700 (PDT)
Subject: [R] exces return by mktcap decile for each year
Message-ID: <737561.92010.qm@web51101.mail.re2.yahoo.com>

I have a data frame, lets call it dat,
with 3 columns ( mc, yr, ret) which represent market
cap, year, and return. mc is a factor, mc, and ret are
real numbers.

I want to add a column to the data calculated as
follows.

For each year, I want to split the data by mc decile,
then calculate the mean ret within that mc decile, and
finally subtract that year's decile mean from the raw
return. Then I want that mean adjusted ret to be the
new column.

I can get the market cap deciles with

my.cut <- function(x) {
  cut( x, quantile( x, probs=seq(0,1,0.1),
na.rm=TRUE))
}
mc.deciles <- by( dat$mc, dat$yr, my.cut)

I don't know how to associate the values in mc.deciles
with a particular row of the original data frame dat.

I don't think I can unlist mc.deciles because the
order won't be the same.

If I could append mc.deciles as a new column to dat,
then I could do something like

by( dat$ret, list( dat$yr, dat$decile), mean)

In which case I would still be faced with finding the
right mean to subtract from each entry in dat$ret.

Maybe I'm just stuck on by() and cut() when there is
an easier way to handle this. Any suggestions? Thanks.


From vokey at uleth.ca  Tue Jul 10 22:11:53 2007
From: vokey at uleth.ca (John Vokey)
Date: Tue, 10 Jul 2007 14:11:53 -0600
Subject: [R] Repeated Measure different results to spss
Message-ID: <668E4827-5C42-4AA3-A89A-47166C188EC6@uleth.ca>

This should work (with x containing the dataframe):

 > x$Id=factor(x$Id)
 > x$Group=factor(x$Group)
 > x$Task=factor(x$Task)
 > str(x)
'data.frame':	48 obs. of  4 variables:
$ Id   : Factor w/ 24 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9  
10 ...
$ Group: Factor w/ 2 levels "1","2": 1 1 1 1 1 1 1 1 1 1 ...
$ Task : Factor w/ 2 levels "1","2": 1 1 1 1 1 1 1 1 1 1 ...
$ Score: num  0.39 0.48 0.59 0.33 0.38 0.37 0.47 0.2 0.29 0.41 ...
 > out.aov = aov(Score~Group*Task+Error(Id+Id:Task),data=x)
 > summary(out.aov)

Error: Id
           Df  Sum Sq Mean Sq F value Pr(>F)
Group      1 0.03420 0.03420  2.1382 0.1578
Residuals 22 0.35189 0.01600

Error: Id:Task
            Df   Sum Sq  Mean Sq F value  Pr(>F)
Task        1 0.048133 0.048133  5.2144 0.03242 *
Group:Task  1 0.024687 0.024687  2.6743 0.11621
Residuals  22 0.203080 0.009231
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
 >

out.aov = aov(Score~Group*Task+Error(Id),data=x) will work as well,  
but the error-term will be labelled simply as ``Within'', rather than  
as the interaction of Id*Task

>
> Hi,
>
> I have some problems with my repeated measures analysis. When I  
> compute it
> with SPSS I get different results than with R. Probably I am doing  
> something
> wrong in R.
> I have two groups (1,2) both having to solve a task under two  
> conditions
> (1,2). That is one between subject factor (group) and one within  
> subject
> factor (task). I tried the following:
>
>  aov(Score ~factor(Group)*factor(Task)+Error(Id)))
>  aov(Score ~factor(Group)*factor(Task))
> but it leads to different results than my spss. I definitely miss  
> some point
> here .
>
> Thanks for you help.
>
> Id	Group	Task	Score
> 1	1	1	0.39
> 2	1	1	0.48


From FolkesM at pac.dfo-mpo.gc.ca  Tue Jul 10 22:23:09 2007
From: FolkesM at pac.dfo-mpo.gc.ca (Folkes, Michael)
Date: Tue, 10 Jul 2007 13:23:09 -0700
Subject: [R] Plot SpatialLinesDataFrame with xlim & ylim
Message-ID: <63F107BCC37AEA49A75FD94AA3E07CB099F499@pacpbsex01.pac.dfo-mpo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/9e255a2c/attachment.pl 

From Cody_Hamilton at Edwards.com  Tue Jul 10 23:33:29 2007
From: Cody_Hamilton at Edwards.com (Cody Hamilton)
Date: Tue, 10 Jul 2007 14:33:29 -0700
Subject: [R]  Making Gehan-Breslow test for Survival data
Message-ID: <4C7D0185E1C6D64AAFB18A2281B262B805A613BEDD@EXIRV01.am.edwards.lcl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/88ca48f7/attachment.pl 

From ggrothendieck at gmail.com  Wed Jul 11 00:25:38 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 10 Jul 2007 18:25:38 -0400
Subject: [R] How to plot two variables using a secondary Y axis
In-Reply-To: <195957.24352.qm@web56612.mail.re3.yahoo.com>
References: <195957.24352.qm@web56612.mail.re3.yahoo.com>
Message-ID: <971536df0707101525i1f22bdcfyc86cbd2d736b2775@mail.gmail.com>

We assume the Fo and Co represent the same data except in different
units (this seems to be approximately the case) so there is really
only one variable being measured here.  If that's not the case let
me know. Below we read the data, define enough padding around plot
to do what we want, call xyplot, draw the right axis and add the right
y lablel.

library(lattice)
library(grid)  # needed for grid.text

# data

Lines.raw <- "Date  Fo  Co
6/27/2007  57.1  13.9
6/28/2007  57.7  14.3
6/29/2007  57.8  14.3
6/30/2007  57  13.9
7/1/2007  57.1  13.9
7/2/2007  57.2  14.0
7/3/2007  57.3  14.1
7/4/2007  57.6  14.2
7/5/2007  58  14.4
7/6/2007  58.1  14.5
7/7/2007  58.2  14.6
7/8/2007  58.4  14.7
7/9/2007    58.7 14.8
"
# in reality next stmt would be DF <- read.table("myfile.dat", header = TRUE)
DF <- read.table(textConnection(Lines.raw), header = TRUE)
DF$Date <- as.Date(DF$Date, "%m/%d/%Y")

par.settings <- list(
	layout.widths = list(left.padding = 10, right.padding = 10),
	layout.heights = list(bottom.padding = 10, top.padding = 10)
)

xyplot(Co ~ Date, DF, default.scales = list(y = list(relation = "free")),
	ylab = "C", par.settings = par.settings)

trellis.focus("panel", 1, 1, clip.off = TRUE)
  pr <- pretty(DF$Fo)
  at <- 5/9 * (pr - 32)
  panel.axis("right", at = at, lab = pr, outside = TRUE)
  grid.text("F", x = 1.1, rot = 90) # right y axis label
trellis.unfocus()



On 7/10/07, Felipe Carrillo <mazatlanmexico at yahoo.com> wrote:
>          Date  Fo  Co    6/27/2007  57.1  13.9    6/28/2007  57.7  14.3    6/29/2007  57.8  14.3    6/30/2007  57  13.9    7/1/2007  57.1  13.9    7/2/2007  57.2  14.0    7/3/2007  57.3  14.1    7/4/2007  57.6  14.2    7/5/2007  58  14.4    7/6/2007  58.1  14.5    7/7/2007  58.2  14.6    7/8/2007  58.4  14.7    7/9/2007    58.7
>    14.8
>
>  Hello all:
>  I am a newbie to R, and I was wondering how can I plot the Temperature values above using Lattice or ggplot2 code. I want Date(X axis), Degrees F(Y axis) and Degrees C( on a secondary Y axis).
>  Thanks
>
>
>
>
>
>
>
>
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From david.meyer at wu-wien.ac.at  Wed Jul 11 00:49:20 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Wed, 11 Jul 2007 00:49:20 +0200
Subject: [R] [R-pkgs] package "relations" updated
Message-ID: <46940CF0.5040205@wu-wien.ac.at>

Dear useRs,

Version 0.2 of package "relations" appeared on CRAN and is currently 
propagating to the mirrors. In addition to some bug fixes, the new 
release includes:

   o an introductory vignette showing the main features;

   o new SD fitters for the C ("complete") and A ("antisymmetric")
     families of relations;

   o a fitter for Copeland's method;

   o the relation_classes() function to extract and pretty-print
     (ordered) classes from preferences and equivalences;

   o the function relation_violations() to compute a measure of
     remoteness from a specified property (e.g., symmetry,
     transitivity, etc.).

David and Kurt.





-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Tel: +43-1-313 36 4393
Fax: +43-1-313 36 90 4393
HP:  http://wi.wu-wien.ac.at/~meyer/

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From s.blomberg1 at uq.edu.au  Wed Jul 11 00:58:03 2007
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Wed, 11 Jul 2007 08:58:03 +1000
Subject: [R] type III ANOVA for a nested linear model
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBAAD4EA@LP-EXCHVS07.CO.IHC.COM>
References: <07E228A5BE53C24CAD490193A7381BBBAAD4EA@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <1184108283.4812.6.camel@sib-sblomber01d.sib.uq.edu.au>

I second the nomination!

Simon.

On Tue, 2007-07-10 at 10:02 -0600, Greg Snow wrote:
> I nominate the following 2 pieces from Bill's reply for fortunes
> (probably 2 separate fortunes):
>  
> 
> 
> > All this becomes even more glaring if you take the unusal 
> > step of plotting the data.
> 
> and
> 
> > What sort of editor would overlook this clear and 
> > demonstrable message leaping out from the data in favour of 
> > some arcane argument about "types of sums of squares"?  
> > Several answers come to mind: A power freak, a SAS 
> > afficianado, an idiot.
> 
> 
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia
Room 320 Goddard Building (8)
T: +61 7 3365 2506 
email: S.Blomberg1_at_uq.edu.au

Policies:
1.  I will NOT analyse your data for you.
2.  Your deadline is your problem.

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.


From bjs379 at psu.edu  Wed Jul 11 01:07:10 2007
From: bjs379 at psu.edu (Byran Smucker)
Date: Tue, 10 Jul 2007 19:07:10 -0400
Subject: [R] error using lp function in linux
Message-ID: <1184108829l.2445514l.0l@psu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070710/3ddb8875/attachment.pl 

From Alexander.Herr at csiro.au  Wed Jul 11 04:19:25 2007
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Wed, 11 Jul 2007 12:19:25 +1000
Subject: [R] Gap statistics (Tibshirani et al 2001) for Categorical data
Message-ID: <80C7911E901E7E4797B3F88D106CB25D14A6E3@exqld2-bne.nexus.csiro.au>

Hi list,

has anyone implemented the Gap statistic for clusters based on
categorical/mixed data?

Slmisc (and SAGx) only work on numerical data.

Any suggestions welcome

Thanks
Herry


From h.wickham at gmail.com  Wed Jul 11 07:57:53 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 11 Jul 2007 07:57:53 +0200
Subject: [R] How to plot two variables using a secondary Y axis
In-Reply-To: <195957.24352.qm@web56612.mail.re3.yahoo.com>
References: <195957.24352.qm@web56612.mail.re3.yahoo.com>
Message-ID: <f8e6ff050707102257v10b708c7w7f218b4efe38035d@mail.gmail.com>

On 7/10/07, Felipe Carrillo <mazatlanmexico at yahoo.com> wrote:
>           Date  Fo  Co    6/27/2007  57.1  13.9    6/28/2007  57.7  14.3    6/29/2007  57.8  14.3    6/30/2007  57  13.9    7/1/2007  57.1  13.9    7/2/2007  57.2  14.0    7/3/2007  57.3  14.1    7/4/2007  57.6  14.2    7/5/2007  58  14.4    7/6/2007  58.1  14.5    7/7/2007  58.2  14.6    7/8/2007  58.4  14.7    7/9/2007    58.7
>     14.8
>
>   Hello all:
>   I am a newbie to R, and I was wondering how can I plot the Temperature values above using Lattice or ggplot2 code. I want Date(X axis), Degrees F(Y axis) and Degrees C( on a secondary Y axis).

Hi Felipe,

It's not currently possible with ggplot2, but it is something on my to do list.

Hadley


From amnakhan493 at gmail.com  Wed Jul 11 08:24:40 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Wed, 11 Jul 2007 11:24:40 +0500
Subject: [R] Window Version for permtest
Message-ID: <3ffd3bb60707102324u3e13d074q84d98cfb1c2fbc04@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/9baf29d6/attachment.pl 

From amnakhan493 at gmail.com  Wed Jul 11 08:24:40 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Wed, 11 Jul 2007 11:24:40 +0500
Subject: [R] Window Version for permtest
Message-ID: <3ffd3bb60707102324u3e13d074q84d98cfb1c2fbc04@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/9baf29d6/attachment-0001.pl 

From ripley at stats.ox.ac.uk  Wed Jul 11 08:42:39 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Jul 2007 07:42:39 +0100 (BST)
Subject: [R] Window Version for permtest
In-Reply-To: <3ffd3bb60707102324u3e13d074q84d98cfb1c2fbc04@mail.gmail.com>
References: <3ffd3bb60707102324u3e13d074q84d98cfb1c2fbc04@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707110740230.1119@gannet.stats.ox.ac.uk>

On Wed, 11 Jul 2007, amna khan wrote:

> Hi Sir
>
> There is no Window Binary versin of package permtest.Please provide
> information in this regard.
>

See

http://cran.r-project.org/bin/windows/contrib/2.5/check/permtest-check.log

[Sending this once was quite sufficient.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From e.pebesma at geo.uu.nl  Wed Jul 11 08:56:14 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Wed, 11 Jul 2007 08:56:14 +0200
Subject: [R]  Plot SpatialLinesDataFrame with xlim & ylim
Message-ID: <46947F0E.9020102@geo.uu.nl>

Michael,

The plot method for SpatialLinesDataFrame objects resides in package sp, 
and questions regarding it are easier noticed on the r-sig-geo mailing 
list.

The reason why they are plotted with aspect ratio 1 is that they are 
assumed to be spatial (geographical) data, and assume that 1 m north 
equals 1 m west -- think of a map. The exception is when the projection 
argument is set to longlat data (i.e. decimal degrees North/East), where 
the aspect ratio is computed differently, such that the argument above 
more or less holds.

You should be able to override the default aspect setting by explicitly 
passing the e.g. asp=0.5 argument to plot.

Here's the comment in the documentation of plot for Spatial objects 
(such as SpatialLinesDataFrame):

The default aspect for map plots is 1; if however data are not projected 
(coordinates are longlat), the aspect is by default set to 1/cos(My * 
pi)/180) with My the y coordinate of the middle of the map (the mean of 
ylim, which defaults to the y range of bounding box).

The argument |setParUsrBB| may be used to pass the logical value |TRUE| 
to functions within |plot.Spatial|. When set to |TRUE|, par(?usr?) will 
be overwritten with |c(xlim, ylim)|, which defaults to the bounding box 
of the spatial object. This is only needed in the particular context of 
graphic output to a specified device with given width and height, to be 
matched to the spatial object, when using par(?xaxs?) and par(?yaxs?) in 
addition to |par(mar=c(0,0,0,0))|.
--

Edzer

I'm running windows xp, R 2.3.1 with maptools 0.6-6, I guess. 
When plotting from a large SpatialLinesDataFrame and using xlim & ylim to reduce the area, the plot axes automatically have the same scale size, even if xlim and ylim ranges differ.  
E.g.:
tmp <- readShapeLines(filepath)
plot(tmp,xlim=c(-126,-119),ylim=c(50,51))

The y-axis range is actually 47-54, same range as the x-axis.  What am I doing wrong?  Should I be using a different object for simple coastline & river data?
Thanks in advance!
Michael


From ripley at stats.ox.ac.uk  Wed Jul 11 09:19:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Jul 2007 08:19:13 +0100 (BST)
Subject: [R] Improved Windows Vista compatibility
Message-ID: <Pine.LNX.4.64.0707110802350.1268@gannet.stats.ox.ac.uk>

Duncan Murdoch and I have been working on Vista compatibility.

The first build of R-2.5.1-win32.exe was built with a version of the 
installer that had some compatibility issues, and it has been rebuilt 
under the latest version.  What now happens is that if you want to install 
R in an adminstrative area you need to select 'Run as Administrator' when 
running the installer: otherwise the default installation directory 
offered will be in your user area (e.g. c:/Users/ripley/Documents).

The Tcl support files came with obsolete Winhelp 4 .hlp files that are not 
readable under Vista, and .chm help has been added.  (As from the next 
release .hlp will be dropped.)

Rtools.exe has also been rebuilt with newer versions of the tools that 
work on Vista.  There is still a compiler path issue, and you need to add

c:\Rtools\MinGW\libexec\gcc\mingw\3.4.5

(or similar) to your path.  When that is done I have successfully built 
and run R-patched and R-devel on 64-bit Vista.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From helin.gai at gmail.com  Wed Jul 11 09:16:43 2007
From: helin.gai at gmail.com (Helin Gai)
Date: Wed, 11 Jul 2007 15:16:43 +0800
Subject: [R] Installation on Leopard
Message-ID: <8570be060707110016y52a241eavd9b1bd3a6783f9dd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/dd9ba3db/attachment.pl 

From Cody_Hamilton at Edwards.com  Tue Jul 10 23:55:10 2007
From: Cody_Hamilton at Edwards.com (Cody Hamilton)
Date: Tue, 10 Jul 2007 14:55:10 -0700
Subject: [R]  Making Gehan-Breslow test for Survival data
Message-ID: <4C7D0185E1C6D64AAFB18A2281B262B805A613BF77@EXIRV01.am.edwards.lcl>

Jose,

The Gehan-Breslow test provides a generalization of the Kruskal-Wallis test for censored data.  As an alternative, try using survdiff with rho=1.  This method uses weights w(ti) = S(ti) (where S is the Kaplan-Meier estimate of survival) which yields Fleming and Harrington's version of the Kruskal-Wallis test for censored data.  This test will give more weight to early differences in the hazards.

Regards,
   -Cody

Cody Hamilton
Edwards Lifesciences

Hi all,

The survivals functions can be tested by the Log-rank test and others, for
example the Gehan-Breslow. The graham breslow work with the alpha values.

But I don't know how is the Gehan-Breslow test with R. Somebody know a
type function?.. or other suggestions?  Any help will be really
appreciated

Jos? Bustos
Marine Biologist
Master Apllied Stat Program
University of Concepci?n


From kristi.glover at hotmail.com  Wed Jul 11 06:18:15 2007
From: kristi.glover at hotmail.com (Kristi Glover)
Date: Wed, 11 Jul 2007 01:18:15 -0300
Subject: [R] Previously saved workspace restored
Message-ID: <BAY111-W190CC2530BBAA68A849DC8FA040@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/3d653998/attachment.pl 

From Roger.Bivand at nhh.no  Wed Jul 11 10:30:00 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 11 Jul 2007 10:30:00 +0200 (CEST)
Subject: [R] [R-sig-Geo]  Plot SpatialLinesDataFrame with xlim & ylim
In-Reply-To: <46947F0E.9020102@geo.uu.nl>
References: <46947F0E.9020102@geo.uu.nl>
Message-ID: <Pine.LNX.4.64.0707111026020.31223@reclus.nhh.no>

On Wed, 11 Jul 2007, Edzer J. Pebesma wrote:

> Michael,
>
> The plot method for SpatialLinesDataFrame objects resides in package sp,
> and questions regarding it are easier noticed on the r-sig-geo mailing
> list.
>
> The reason why they are plotted with aspect ratio 1 is that they are
> assumed to be spatial (geographical) data, and assume that 1 m north
> equals 1 m west -- think of a map. The exception is when the projection
> argument is set to longlat data (i.e. decimal degrees North/East), where
> the aspect ratio is computed differently, such that the argument above
> more or less holds.
>
> You should be able to override the default aspect setting by explicitly
> passing the e.g. asp=0.5 argument to plot.
>
> Here's the comment in the documentation of plot for Spatial objects
> (such as SpatialLinesDataFrame):
>
> The default aspect for map plots is 1; if however data are not projected
> (coordinates are longlat), the aspect is by default set to 1/cos(My *
> pi)/180) with My the y coordinate of the middle of the map (the mean of
> ylim, which defaults to the y range of bounding box).
>
> The argument |setParUsrBB| may be used to pass the logical value |TRUE|
> to functions within |plot.Spatial|. When set to |TRUE|, par(?usr?) will
> be overwritten with |c(xlim, ylim)|, which defaults to the bounding box
> of the spatial object. This is only needed in the particular context of
> graphic output to a specified device with given width and height, to be
> matched to the spatial object, when using par(?xaxs?) and par(?yaxs?) in
> addition to |par(mar=c(0,0,0,0))|.


Yes, if you look at how the GE_SpatialGrid() function in maptools works - 
you'll see how it meddles with the actual regional extents and the device 
size. I think Michael's data should also have been set to longlat:

proj4string(tmp) <- CRS("+proj=longlat")

The key is realising that the axes are driven by the device shape, not by 
the xlim/ylim as such, as Edzer says.

Roger


> --
>
> Edzer
>
> I'm running windows xp, R 2.3.1 with maptools 0.6-6, I guess.
> When plotting from a large SpatialLinesDataFrame and using xlim & ylim to reduce the area, the plot axes automatically have the same scale size, even if xlim and ylim ranges differ.
> E.g.:
> tmp <- readShapeLines(filepath)
> plot(tmp,xlim=c(-126,-119),ylim=c(50,51))
>
> The y-axis range is actually 47-54, same range as the x-axis.  What am I doing wrong?  Should I be using a different object for simple coastline & river data?
> Thanks in advance!
> Michael
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From yn19832 at msn.com  Wed Jul 11 10:32:28 2007
From: yn19832 at msn.com (livia)
Date: Wed, 11 Jul 2007 01:32:28 -0700 (PDT)
Subject: [R] ECDF, distribution of Pareto, distribution of Normal
In-Reply-To: <4693C24D.6060605@gmx.net>
References: <11524560.post@talk.nabble.com> <4693C24D.6060605@gmx.net>
Message-ID: <11536305.post@talk.nabble.com>


Thank you very much for your reply. I am afraid I have no idea what is wrong
with the pgpg function. The parameters are generated from pre-fitted GPD
distribution. 1.544 is the location parameter, 0.4373 is the scale parameter
and -0.2398 is the shape parameter.

Cound you please give me some hint?

Stefan Grosse-2 wrote:
> 
> 
> 
> -------- Original Message  --------
> Subject: [R] ECDF, distribution of Pareto, distribution of Normal
> From: livia <yn19832 at msn.com>
> To: r-help at stat.math.ethz.ch
> Date: Tue Jul 10 2007 18:35:04 GMT+0200
>> Hello all,
>>
>> I would like to plot the emperical CDF, normal CDF and pareto CDF in the
>> same graph and I amusing the following codes. "z" is a vector and I just
>> need the part when z between 1.6 and 3.
>>
>> plot(ecdf(z), do.points=FALSE, verticals=TRUE,
>> xlim=c(1.6,3),ylim=c(1-sum(z>1.6)/length(z), 1))
>>
>> x <- seq(1.6, 3, 0.1)
>> lines(x,pgpd(x, 1.544,0.4373,-0.2398), col="red")
>>   
> 
> There is something wrong with your pgpd function, see ?pgpd for help and
> parameters... (I wonder how you got something plotted here...)
> 
> 
>> y <- seq(1.6, 3, 0.1)
>> lines(y,pnorm(y, mean(z),sqrt(var(z))), col="blue")
>>
>> The emperical CDF and normal CDF look rather resonable, but the pareto
>> CDF
>> looks quite odd. I am not sure whether I plot the pareto CDF correctly
>> e.g.
>> in the right yaxs or any other mistake?
>>
>> At the same time, let "t" represents the vector whose values are larger
>> than
>> 1.6(the part we want). If I implement the following codes and plot the
>> emperical CDF and pareto CDF, the pareto CDF seems fit.
>>
>> plot(ecdf(t), do.points=FALSE, verticals=TRUE)
>> x <- seq(1.6, 3, 0.1)
>> lines(x,pgpd(x, 1.544,0.4373,-0.2398), col="red")
>>
>> Could anyone give me some advice on this? Many thanks.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/ECDF%2C-distribution-of-Pareto%2C-distribution-of-Normal-tf4056943.html#a11536305
Sent from the R help mailing list archive at Nabble.com.


From mark_difford at yahoo.co.uk  Wed Jul 11 10:48:10 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Wed, 11 Jul 2007 01:48:10 -0700 (PDT)
Subject: [R] type III ANOVA for a nested linear model
In-Reply-To: <1184108283.4812.6.camel@sib-sblomber01d.sib.uq.edu.au>
References: <1184066076.3890.31.camel@Amilo>
	<B998A44C8986644EA8029CFE6396A924B68056@exqld2-bne.nexus.csiro.au>
	<07E228A5BE53C24CAD490193A7381BBBAAD4EA@LP-EXCHVS07.CO.IHC.COM>
	<1184108283.4812.6.camel@sib-sblomber01d.sib.uq.edu.au>
Message-ID: <11536509.post@talk.nabble.com>


Indeed!  And, apropos of the expression, "to be Ripleyed" (and so be
condemned to eating cookies for a long, long time), what about being
"Billasted"?

BestR,
Mark.


Simon Blomberg-4 wrote:
> 
> I second the nomination!
> 
> Simon.
> 
> On Tue, 2007-07-10 at 10:02 -0600, Greg Snow wrote:
>> I nominate the following 2 pieces from Bill's reply for fortunes
>> (probably 2 separate fortunes):
>>  
>> 
>> 
>> > All this becomes even more glaring if you take the unusal 
>> > step of plotting the data.
>> 
>> and
>> 
>> > What sort of editor would overlook this clear and 
>> > demonstrable message leaping out from the data in favour of 
>> > some arcane argument about "types of sums of squares"?  
>> > Several answers come to mind: A power freak, a SAS 
>> > afficianado, an idiot.
>> 
>> 
> -- 
> Simon Blomberg, BSc (Hons), PhD, MAppStat. 
> Lecturer and Consultant Statistician 
> Faculty of Biological and Chemical Sciences 
> The University of Queensland 
> St. Lucia Queensland 4072 
> Australia
> Room 320 Goddard Building (8)
> T: +61 7 3365 2506 
> email: S.Blomberg1_at_uq.edu.au
> 
> Policies:
> 1.  I will NOT analyse your data for you.
> 2.  Your deadline is your problem.
> 
> The combination of some data and an aching desire for 
> an answer does not ensure that a reasonable answer can 
> be extracted from a given body of data. - John Tukey.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/type-III-ANOVA-for-a-nested-linear-model-tf4055192.html#a11536509
Sent from the R help mailing list archive at Nabble.com.


From steve at promente.org  Wed Jul 11 11:04:18 2007
From: steve at promente.org (Steve Powell)
Date: Wed, 11 Jul 2007 11:04:18 +0200
Subject: [R] variable and value labels
Message-ID: <46949D12.8000705@promente.org>

Dear list
I am new here - enjoying the power of R compared to SPSS.
Looking for sets of tips and tricks for people with old SPSS habits.
In particular, would like to know an easy way to set variable labels 
across a dataset and to set value labels for sets of variables.
Grateful for any help,
Steve Powell


From heiden at math.TU-Berlin.DE  Wed Jul 11 10:47:40 2007
From: heiden at math.TU-Berlin.DE (Matthias an der Heiden)
Date: Wed, 11 Jul 2007 10:47:40 +0200
Subject: [R] question about gamm models
Message-ID: <jUsT.aNoTheR.mEsSaGe.iD.11841448659635@www.math.tu-berlin.de>

Dear R-Users,

I have a question concerning mixed models in R.
The strata of my model are the counties of Germany. The differencies 
between these counties should be modelled as realizations of a normally 
distributed random variable X.
Moreover, the model contains a 0/1 variable A that enters as a fixed 
effect.
The only special feature that should be additionally in the model 
is the following:  

In a usual mixed model the (constant) variance of X will be chosen 
in an optimal way, but I want to fit 2 constant variances, one for 
the subset {A=0} and the other one for the subset {A=1}. Nevertheless 
it should be one and the same random variable X.

I know that it is possible to fit a model with two independent random 
variables X1 and X2 for the subsets {A=0} and {A=1} respectively.
But I want it to be the same! Equivalently the correlation between 
X1 and X2 should be 1.

Can anybody help me in this respect?

Yours sincerely     Matthias an der Heiden


From jbarnier at ens-lsh.fr  Wed Jul 11 11:22:52 2007
From: jbarnier at ens-lsh.fr (Julien Barnier)
Date: Wed, 11 Jul 2007 11:22:52 +0200
Subject: [R] variable and value labels
References: <46949D12.8000705@promente.org>
Message-ID: <87bqeji9ar.fsf@ens-lsh.fr>

Hi Steve,

> I am new here - enjoying the power of R compared to SPSS.
> Looking for sets of tips and tricks for people with old SPSS habits.
> In particular, would like to know an easy way to set variable labels 
> across a dataset and to set value labels for sets of variables.

Maybe you should take a look at the following document :

http://oit.utk.edu/scc/RforSAS&SPSSusers.pdf

HTH,

Julien

-- 
Julien Barnier
Groupe de recherche sur la socialisation
ENS-LSH - Lyon, France


From Corinna.Schmitt at igb.fraunhofer.de  Wed Jul 11 11:31:11 2007
From: Corinna.Schmitt at igb.fraunhofer.de (Schmitt, Corinna)
Date: Wed, 11 Jul 2007 11:31:11 +0200
Subject: [R] Changing default library
Message-ID: <8B7B0FD99E8AF541A21609104D1961587D5942@izs-xchg01.izs.fraunhofer.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/f92d80d1/attachment.pl 

From jbarnier at ens-lsh.fr  Wed Jul 11 11:36:45 2007
From: jbarnier at ens-lsh.fr (Julien Barnier)
Date: Wed, 11 Jul 2007 11:36:45 +0200
Subject: [R] Changing default library
References: <8B7B0FD99E8AF541A21609104D1961587D5942@izs-xchg01.izs.fraunhofer.de>
Message-ID: <877ip7i8nm.fsf@ens-lsh.fr>

Hi,

> When I than do the command .libPaths() the result is:
> "/usr/lib/R/library" "/home/csc/usr/lib/R/library". But if I start R
> the next time the result of the command .libPaths() is again just
> "/usr/lib/R/library".

>From libPaths help page :

,----
| The library search path is initialized at startup from the environment
| variable R_LIBS (which should be a colon-separated list of directories
| at which R library trees are rooted) by calling .libPaths with the
| directories specified in R_LIBS.
`----

HTH,

Julien

-- 
Julien Barnier
Groupe de recherche sur la socialisation
ENS-LSH - Lyon, France


From vladosr at gmail.com  Wed Jul 11 11:42:51 2007
From: vladosr at gmail.com (vladosr at gmail.com)
Date: Wed, 11 Jul 2007 19:42:51 +1000
Subject: [R] p-value from survreg, library(survival)
Message-ID: <854756c60707110242t702d141cg47add8dd68da7f5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/b3146c7d/attachment.pl 

From Achim.Zeileis at wu-wien.ac.at  Wed Jul 11 11:47:18 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 11 Jul 2007 11:47:18 +0200 (CEST)
Subject: [R] Changing default library
In-Reply-To: <8B7B0FD99E8AF541A21609104D1961587D5942@izs-xchg01.izs.fraunhofer.de>
Message-ID: <Pine.LNX.4.44.0707111143330.24205-100000@disco.wu-wien.ac.at>

Corinna:

> I got a question concerning the .libPaths(). if I do the command
> .libPaths() than the result is /usr/lib/R/library. This is the default
> folder. I now want to change this one into /home/csc/usr/lib/R/library.
> I thought it would work with the command
> .libPaths("/home/csc/usr/lib/R/library"). When I than do the command
> .libPaths() the result is: "/usr/lib/R/library"
> "/home/csc/usr/lib/R/library". But if I start R the next time the result
> of the command .libPaths() is again just "/usr/lib/R/library".
>
> Can anyone help me?

You should set the R_LIBS environment variable (that the man page of
.libPaths points you to). See also FAQ 5.2.

hth,
Z


From mary-jane.anderson at isd.csa.scot.nhs.uk  Wed Jul 11 11:50:57 2007
From: mary-jane.anderson at isd.csa.scot.nhs.uk (Anderson, Mary-Jane)
Date: Wed, 11 Jul 2007 10:50:57 +0100
Subject: [R] inquiry about anova and ancova
Message-ID: <4C0877E4E2FEEA4DB2722C3B703BD6C3091CC143@wizzle.isd.csa.scot.nhs.uk>

Dear R users,
	 I have a rather knotty analysis problem and I was hoping that
someone on this list would be able to help. I was advised to try this list
by a colleague who uses R but it is a statistical inquiry not about how to
use R.
 In brief I have a 3x2 anova, 2 tasks under 3 conditions, within subjects. I
also took a variety of personality measures that might influence the results
under the different conditions. I had thought that an ancova would be the
best test, but it might be the case that this would not work with a within
subjects design. I have not found anything that explicitly states whether or
not it would, but all the examples I have read are between subjects design.
 I also thought of investigating a manova, but it is not really the case
that I have more than one DV, it is the same DV in 6 different combinations
of task and condition. 
 There were 4 personality measures and I wanted to look at the degree to
which they affected the task/ condition interaction. 
 I have explained this briefly here, but I can of course provied more
details to anyone who can advise me further with this.
Thanks,
Mary-Jane Anderson 
Information Analyst 
Platform Project
Information Services Division, 
NHS National Services Scotland, 
Gyle Square, 
1 South Gyle Crescent, 
Edinburgh, 
EH12 9EB. 
0131 275 7163.


_________________________________________________________________ 
NHS National Services Scotland Disclaimer 

The information contained in this message may be confidentia...{{dropped}}


From P.Dalgaard at biostat.ku.dk  Wed Jul 11 12:22:09 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 11 Jul 2007 12:22:09 +0200
Subject: [R] inquiry about anova and ancova
In-Reply-To: <4C0877E4E2FEEA4DB2722C3B703BD6C3091CC143@wizzle.isd.csa.scot.nhs.uk>
References: <4C0877E4E2FEEA4DB2722C3B703BD6C3091CC143@wizzle.isd.csa.scot.nhs.uk>
Message-ID: <4694AF51.6050005@biostat.ku.dk>

Anderson, Mary-Jane wrote:
> Dear R users,
> 	 I have a rather knotty analysis problem and I was hoping that
> someone on this list would be able to help. I was advised to try this list
> by a colleague who uses R but it is a statistical inquiry not about how to
> use R.
>  In brief I have a 3x2 anova, 2 tasks under 3 conditions, within subjects. I
> also took a variety of personality measures that might influence the results
> under the different conditions. I had thought that an ancova would be the
> best test, but it might be the case that this would not work with a within
> subjects design. I have not found anything that explicitly states whether or
> not it would, but all the examples I have read are between subjects design.
>  I also thought of investigating a manova, but it is not really the case
> that I have more than one DV, it is the same DV in 6 different combinations
> of task and condition. 
>  There were 4 personality measures and I wanted to look at the degree to
> which they affected the task/ condition interaction. 
>  I have explained this briefly here, but I can of course provied more
> details to anyone who can advise me further with this.
>   
This sounds like a job for a Multivariate Linear Model (assuming that
you have complete data for each subject or are prepared to throw away
subjects with missing values).

This lets you decompose the response into mean, effects of task and
condition, and the interaction effect. Each component can then be
separately tested for effect of predictors, using multivariate tests, or
F tests under sphericity assumptions.

Have a look at example(anova.mlm); this mostly looks at cases where
effects are tested against zero, but the last example involves a (bogus)
between subject factor f.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From A.Robinson at ms.unimelb.edu.au  Wed Jul 11 12:22:41 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 11 Jul 2007 20:22:41 +1000
Subject: [R] type III ANOVA for a nested linear model
In-Reply-To: <11536509.post@talk.nabble.com>
References: <1184066076.3890.31.camel@Amilo>
	<B998A44C8986644EA8029CFE6396A924B68056@exqld2-bne.nexus.csiro.au>
	<07E228A5BE53C24CAD490193A7381BBBAAD4EA@LP-EXCHVS07.CO.IHC.COM>
	<1184108283.4812.6.camel@sib-sblomber01d.sib.uq.edu.au>
	<11536509.post@talk.nabble.com>
Message-ID: <20070711102241.GH5296@ms.unimelb.edu.au>

Billasted sounds too brutal.  How about "Billeted", as in what one
does to one's breshly-caught bish?  

Andrew


On Wed, Jul 11, 2007 at 01:48:10AM -0700, Mark Difford wrote:
> 
> Indeed!  And, apropos of the expression, "to be Ripleyed" (and so be
> condemned to eating cookies for a long, long time), what about being
> "Billasted"?
> 
> BestR,
> Mark.
> 
> 
> Simon Blomberg-4 wrote:
> > 
> > I second the nomination!
> > 
> > Simon.
> > 
> > On Tue, 2007-07-10 at 10:02 -0600, Greg Snow wrote:
> >> I nominate the following 2 pieces from Bill's reply for fortunes
> >> (probably 2 separate fortunes):
> >>  
> >> 
> >> 
> >> > All this becomes even more glaring if you take the unusal 
> >> > step of plotting the data.
> >> 
> >> and
> >> 
> >> > What sort of editor would overlook this clear and 
> >> > demonstrable message leaping out from the data in favour of 
> >> > some arcane argument about "types of sums of squares"?  
> >> > Several answers come to mind: A power freak, a SAS 
> >> > afficianado, an idiot.
> >> 
> >> 
> > -- 
> > Simon Blomberg, BSc (Hons), PhD, MAppStat. 
> > Lecturer and Consultant Statistician 
> > Faculty of Biological and Chemical Sciences 
> > The University of Queensland 
> > St. Lucia Queensland 4072 
> > Australia
> > Room 320 Goddard Building (8)
> > T: +61 7 3365 2506 
> > email: S.Blomberg1_at_uq.edu.au
> > 
> > Policies:
> > 1.  I will NOT analyse your data for you.
> > 2.  Your deadline is your problem.
> > 
> > The combination of some data and an aching desire for 
> > an answer does not ensure that a reasonable answer can 
> > be extracted from a given body of data. - John Tukey.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
> -- 
> View this message in context: http://www.nabble.com/type-III-ANOVA-for-a-nested-linear-model-tf4055192.html#a11536509
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From A.Robinson at ms.unimelb.edu.au  Wed Jul 11 12:29:41 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Wed, 11 Jul 2007 20:29:41 +1000
Subject: [R] inquiry about anova and ancova
In-Reply-To: <4C0877E4E2FEEA4DB2722C3B703BD6C3091CC143@wizzle.isd.csa.scot.nhs.uk>
References: <4C0877E4E2FEEA4DB2722C3B703BD6C3091CC143@wizzle.isd.csa.scot.nhs.uk>
Message-ID: <20070711102941.GI5296@ms.unimelb.edu.au>

Hi Mary,

it sounds like you have a split-plot design, or more gruesomely, a
split-subject design.  

The model that I infer from your description of the design can be fit
using the lme() function of the nlme() package, along the lines of a
similar analysis documented in section 1.6 of Pinheiro and Bates
(2000).  It should also be possible by using the aov() function in
base R.

Cheers,

Andrew

On Wed, Jul 11, 2007 at 10:50:57AM +0100, Anderson, Mary-Jane wrote:
> Dear R users,
> 	 I have a rather knotty analysis problem and I was hoping that
> someone on this list would be able to help. I was advised to try this list
> by a colleague who uses R but it is a statistical inquiry not about how to
> use R.
>  In brief I have a 3x2 anova, 2 tasks under 3 conditions, within subjects. I
> also took a variety of personality measures that might influence the results
> under the different conditions. I had thought that an ancova would be the
> best test, but it might be the case that this would not work with a within
> subjects design. I have not found anything that explicitly states whether or
> not it would, but all the examples I have read are between subjects design.
>  I also thought of investigating a manova, but it is not really the case
> that I have more than one DV, it is the same DV in 6 different combinations
> of task and condition. 
>  There were 4 personality measures and I wanted to look at the degree to
> which they affected the task/ condition interaction. 
>  I have explained this briefly here, but I can of course provied more
> details to anyone who can advise me further with this.
> Thanks,
> Mary-Jane Anderson 
> Information Analyst 
> Platform Project
> Information Services Division, 
> NHS National Services Scotland, 
> Gyle Square, 
> 1 South Gyle Crescent, 
> Edinburgh, 
> EH12 9EB. 
> 0131 275 7163.
> 
> 
> _________________________________________________________________ 
> NHS National Services Scotland Disclaimer 
> 
> The information contained in this message may be confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From ripley at stats.ox.ac.uk  Wed Jul 11 12:45:37 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Jul 2007 11:45:37 +0100 (BST)
Subject: [R] Warning message: cannot create HTML package index
In-Reply-To: <468E1E31.4090409@biostat.ku.dk>
References: <m28x9tvre8.fsf@cam.ac.uk>
	<Pine.LNX.4.64.0707061043200.22312@gannet.stats.ox.ac.uk>
	<m2odipua6g.fsf@cam.ac.uk> <468E1E31.4090409@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0707061610580.13863@gannet.stats.ox.ac.uk>

On Fri, 6 Jul 2007, Peter Dalgaard wrote:

> Leo wrote:
>> On 06/07/2007, Prof Brian Ripley wrote:
>>
>>> On Fri, 6 Jul 2007, Leo wrote:
>>>
>>>
>>>> I have set R_LIBS=~/R_lib as I don't have root access.
>>>>
>>>> The following message shown up every time after installing a package:
>>>>
>>>>  ......
>>>>  The downloaded packages are in
>>>>  	/tmp/RtmpBoIPoz/downloaded_packages
>>>>  Warning message:
>>>>  cannot create HTML package index in: tools:::unix.packages.html(.Library)
>>>>
>>>> Any ideas?
>>>>
>>> It is a correct warning.  What is the problem with being warned?
>>>
>>> R tries to maintain an HTML page of installed packages, but you don't have
>>> permission to update it.
>>>
>>
>> Where is that HTML page located on a GNU/Linux system?
>>
>> Is it possible to maintain a user HTML page of installed packages?
>>
>> Thanks,
>>
> This confuses me a bit too. I had gotten used to the warning without
> thinking about it. It tries to update $RHOME/doc/html/packages.html,
> which starts like this:
>
> .....
> <p><h3>Packages in the standard library</h3>
> .....
>
> However, if I run help.start, I get
>
>> help.start()
> Making links in per-session dir ...
> If 'firefox' is already running, it is *not* restarted, and you must
>    switch to its window.
> Otherwise, be patient ...
>
> and then it opens (say)
>  file:///tmp/RtmpXyp5Cg/.R/doc/html/index.html
> which has a link to
>  file:///tmp/RtmpXyp5Cg/.R/doc/html/packages.html
>
> which looks like this
>
> ....
> <p><h3>Packages in /home/bs/pd/Rlibrary</h3>
> ....
> <p><h3>Packages in /usr/lib64/R/library</h3>
>
> I.e. it is autogenerated by help.start and doesn't even look at the file
> in $RHOME. So what puzzles me is
>
> (a) why we maintain $RHOME/doc/html/packages.html at all
>
> One argument could be that this is browseable for everyone on a system,
> even without starting R. But then

Some front-end GUIs use it (or used to use it).

> (b) why do we even try updating it when packages are installed in a
> private location?

We don't know that.  Given the use of symbolic links, it is not clear 
which libraries are private and which are links to .Library.  However, we 
could be less cautious about this, and I've altered the code to update 
only if the path matches .Library exactly.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bates at stat.wisc.edu  Wed Jul 11 13:13:02 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 11 Jul 2007 06:13:02 -0500
Subject: [R] Installation on Leopard
In-Reply-To: <8570be060707110016y52a241eavd9b1bd3a6783f9dd@mail.gmail.com>
References: <8570be060707110016y52a241eavd9b1bd3a6783f9dd@mail.gmail.com>
Message-ID: <40e66e0b0707110413q30b22992l18120cecee09b6d5@mail.gmail.com>

On 7/11/07, Helin Gai <helin.gai at gmail.com> wrote:
> Hi all,
> I'm wondering whether anyone has installed R on Mac OS X leopard with
> success. I downloaded the latest version, but couldn't have it installed.
> Any idea as to how I get around the problem? Thanks!

Simon Urbanek has comments about R for Leopard on his Wiki.  Check
R.research.att.com and wiki.Urbanek.info


From vladosr at gmail.com  Wed Jul 11 13:22:23 2007
From: vladosr at gmail.com (Vlado Sremac)
Date: Wed, 11 Jul 2007 21:22:23 +1000
Subject: [R] p-value from survreg(), library(survival)
Message-ID: <854756c60707110422o1235e668h55681f97971d787e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/7a77c31c/attachment.pl 

From amsa36060 at yahoo.com  Wed Jul 11 13:29:38 2007
From: amsa36060 at yahoo.com (Amir Safari)
Date: Wed, 11 Jul 2007 04:29:38 -0700 (PDT)
Subject: [R] Some questions about quadratic programming (QP)
Message-ID: <332637.34905.qm@web60425.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/ab969c3f/attachment.pl 

From yn19832 at msn.com  Wed Jul 11 13:56:22 2007
From: yn19832 at msn.com (livia)
Date: Wed, 11 Jul 2007 04:56:22 -0700 (PDT)
Subject: [R] CDF for pareto distribution
Message-ID: <11538272.post@talk.nabble.com>


Hi, I would like to use the following codes to plot the CDF for pareto
distribution. Before doing this, I have plot the emperical one. 

x <- seq(1.6, 3, 0.1) 
lines(x,pgpd(x, 1.544,0.4477557,), col="red") 

Could anyone give me some advice whether the above codes are correct?

Many thanks.
-- 
View this message in context: http://www.nabble.com/CDF-for-pareto-distribution-tf4061253.html#a11538272
Sent from the R help mailing list archive at Nabble.com.


From Antigen_UM-TSMTPOUT1 at stat.math.ethz.ch  Wed Jul 11 14:04:28 2007
From: Antigen_UM-TSMTPOUT1 at stat.math.ethz.ch (Antigen_UM-TSMTPOUT1 at stat.math.ethz.ch)
Date: 11 Jul 2007 07:04:28 -0500
Subject: [R] Antigen found FILE FILTER=  *.pif file
Message-ID: <UM-TSMTPOUT1oBJHsxP00000362@um-tsmtpout1.um.umsystem.edu>

Antigen for Exchange found instruction.zip__instruction.doc                                                                                                                                                                                                      .pif matching FILE FILTER=  *.pif file filter.
The file is currently Purged.  The message, "DELIVERY REPORTS ABOUT YOUR E_MAIL", was
sent from r_help at stat.math.ethz.ch and was discovered in SMTP Messages\Inbound And Outbound
located at University of Missouri/UM System/UM-TSMTPOUT1.


From strinz at freenet.de  Wed Jul 11 14:16:14 2007
From: strinz at freenet.de (strinz at freenet.de)
Date: Wed, 11 Jul 2007 14:16:14 +0200
Subject: [R] RWeka control parameters classifiers interface
Message-ID: <E1I8b78-0004BL-VJ@www19.emo.freenet-rz.de>

Hello,

  I have some trouble in achieving the desired parametrisation
  for the weka classifier functions, using the package RWeka.

  The problem is, that the functions
  result=classifier(formula, data, subset, na.action, control = Weka_control(mycontrol))
  do not seem to be manipulated by the mycontrol- arguments

  Perhaps this should be resepected via the handlers- argument ,
  but the documentation in this regard is rather sparse. 

# ------------- Examples

file      =system.file("arff","iris.arff",package="RWeka") 
data      =read.arff(file=file)     		
rownames(data)=1:nrow(data)           	
colnames(data)[ncol(data)]  ="class"  	

library(RWeka)

# Example: no parameter influence
mySMO =make_Weka_classifier(name="weka/classifiers/functions/SMO",class=NULL,handlers=list());     
# Using control =Weka_control()
m1 =mySMO(formula=class~.,data=data[,],control=Weka_control(K="weka.classifiers.functions.supportVector.PolyKernel",E=2))    
m2 =mySMO(formula=class~.,data=data[,],control=Weka_control(K="weka.classifiers.functions.supportVector.PolyKernel",E=3)) 
m3 =mySMO(formula=class~.,data=data[,],control=c("K","weka.classifiers.functions.supportVector.PolyKernel","E",3)) 
# Using predefinded interface, does not work 
x1 	=SMO(formula=class~.,data=data[,],control=Weka_control(K="weka.classifiers.functions.supportVector.PolyKernel",E=2))    
x2 	=SMO(formula=class~.,data=data[,],control=Weka_control(K="weka.classifiers.functions.supportVector.PolyKernel",E=3))    
m1$call   
m2$call   
m3$call		
x1$call
x2$call
# no differences:
m1 
m2 
m3
x1
x2

Any suggestions?
Many thanks
Bjoern

many thanks
bjoern


From john.seers at bbsrc.ac.uk  Wed Jul 11 14:19:06 2007
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Wed, 11 Jul 2007 13:19:06 +0100
Subject: [R] Previously saved workspace restored
In-Reply-To: <BAY111-W190CC2530BBAA68A849DC8FA040@phx.gbl>
References: <BAY111-W190CC2530BBAA68A849DC8FA040@phx.gbl>
Message-ID: <AAD49F46EAE3F6479E1D46428FAC31CB0181AB5F@NBIE2KSRV1.nbi.bbsrc.ac.uk>

 

Hi

If you enter the command "ls()"you will see a list of names that have
come with the .Rdata file you double-clicked.

If you enter one of these names at the command prompt you will see the
data.

So, for example if you have some data called "mydata":



> ls()
[1] "mydata" "repos" 
> mydata
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9
> 


Regards

John


 
---

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kristi Glover
Sent: 11 July 2007 05:18
To: r-help at stat.math.ethz.ch
Subject: [R] Previously saved workspace restored

hi there,
i an beginner of R. some one have sent me a file (extension is .Rdata).
i have  installed R in my computer and i just double clicked the data.
then it automatically opened R programme and displayed that [previously
saved workspace restored]. the following message was displayed. 
 
Type 'demo()' for some demos, 'help()' for on-line help,
or'help.start()' for an HTML browser interface to help.Type 'q()' to
quit R.
 
[Previously saved workspace restored]
 
but how  can I see the data (table) which is saved (in R format) in R?,

 
i hope you will help me. 
 
Kristi Glover
 
 
 
 
_________________________________________________________________
Explore the seven wonders of the world

BRE
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From carsten_jaeger at web.de  Wed Jul 11 14:37:41 2007
From: carsten_jaeger at web.de (Carsten Jaeger)
Date: Wed, 11 Jul 2007 14:37:41 +0200
Subject: [R] type III ANOVA for a nested linear model
In-Reply-To: <4693737B.7070208@biostat.ku.dk>
References: <1184066076.3890.31.camel@Amilo> <4693737B.7070208@biostat.ku.dk>
Message-ID: <1184157461.3775.19.camel@Amilo>

Hello Peter,

thanks for your help. I'm quite sure that I specified the right model.
Factor C is indeed nested within factor A. I think you were confused by
the numbering of C (1..11), and it is easier to understand when I code
it as you suggested (1,2,3 within each level of A, as in mydata1 [see
below]). However, it does not matter which numbering I choose for
carrying the analysis, as

anova(lm(resp ~ A * B + (C %in% A), mydata))
anova(lm(resp ~ A * B + (C %in% A), mydata1))

both give the same results (as at least I had expected because of the
nesting).

However, I found that Anova() from the car package only accepts the
second version. So,

Anova(lm(resp ~ A * B + (C %in% A), mydata)) does not work (giving an
error) but
Anova(lm(resp ~ A * B + (C %in% A), mydata1)) does.

This behaviour is rather confusing, or is there anything I'm missing?

Thanks for your help again, 

Carsten


R> mydata
      A     B      C resp
1     1     1      1 34.12
2     1     1      2 32.45
3     1     1      3 44.55
4     1     2      1 20.88
5     1     2      2 22.32
6     1     2      3 27.71
7     2     1      6 38.20
8     2     1      7 31.62
9     2     1      8 38.71
10    2     2      6 18.93
11    2     2      7 20.57
12    2     2      8 31.55
13    3     1      9 40.81
14    3     1     10 42.23
15    3     1     11 41.26
16    3     2      9 28.41
17    3     2     10 24.07
18    3     2     11 21.16

R> mydata1
      A     B      C resp
1     1     1      1 34.12
2     1     1      2 32.45
3     1     1      3 44.55
4     1     2      1 20.88
5     1     2      2 22.32
6     1     2      3 27.71
7     2     1      1 38.20
8     2     1      2 31.62
9     2     1      3 38.71
10    2     2      1 18.93
11    2     2      2 20.57
12    2     2      3 31.55
13    3     1      1 40.81
14    3     1      2 42.23
15    3     1      3 41.26
16    3     2      1 28.41
17    3     2      2 24.07
18    3     2      3 21.16


On Tue, 2007-07-10 at 13:54 +0200, Peter Dalgaard wrote:
> Carsten Jaeger wrote:
> > Hello,
> >
> > is it possible to obtain type III sums of squares for a nested model as
> > in the following:
> >
> > lmod <- lm(resp ~ A * B + (C %in% A), mydata))
> >
> > I have tried
> >
> > library(car)
> > Anova(lmod, type="III")
> >
> > but this gives me an error (and I also understand from the documentation
> > of Anova as well as from a previous request
> > (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/64477.html) that it is
> > not possible to specify nested models with car's Anova).
> >
> > anova(lmod) works, of course.
> >
> > My data (given below) is balanced so I expect the results to be similar
> > for both type I and type III sums of squares. But are they *exactly* the
> > same? The editor of the journal which I'm sending my manuscript to
> > requests what he calls "conventional" type III tests and I'm not sure if
> >   
> > can convince him to accept my type I analysis.
> In balanced designs, type I-IV SSD's are all identical. However, I don't think the model does what I think you think it does. 
> 
> Notice that "nesting" is used with two diferent meanings, in R it would be that the codings of C only makes sense within levels of A - e.g. if they were numbered 1:3 within each group, but with C==1 when A==1 having nothing to do with C==1 when A==2.  SAS does something. er. else...
> 
> What I think you want is a model where C is a random terms so that main effects of A can be tested, like in
> 
> > summary(aov(resp ~ A * B + Error(C), dd))
> 
> Error: C
>           Df  Sum Sq Mean Sq F value Pr(>F)
> A          2  33.123  16.562  0.4981 0.6308
> Residuals  6 199.501  33.250
> 
> Error: Within
>           Df Sum Sq Mean Sq F value   Pr(>F)
> B          1 915.21  915.21 83.7846 9.57e-05 ***
> A:B        2  16.13    8.07  0.7384   0.5168
> Residuals  6  65.54   10.92
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> 
> (This is essentially the same structure as Martin Bleichner had earlier today, also @web.de. What is this? an epidemic? ;-))
> 
>


From Achim.Zeileis at wu-wien.ac.at  Wed Jul 11 14:44:46 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 11 Jul 2007 14:44:46 +0200 (CEST)
Subject: [R] RWeka control parameters classifiers interface
In-Reply-To: <E1I8b78-0004BL-VJ@www19.emo.freenet-rz.de>
Message-ID: <Pine.LNX.4.44.0707111436330.24205-100000@disco.wu-wien.ac.at>

On Wed, 11 Jul 2007 strinz at freenet.de wrote:

>   The problem is, that the functions
>   result=classifier(formula, data, subset, na.action, control = Weka_control(mycontrol))
>   do not seem to be manipulated by the mycontrol- arguments

Yes, they are...not all parameter changes have always an effect on the
specified learner.

>   Perhaps this should be resepected via the handlers- argument ,
>   but the documentation in this regard is rather sparse.

Handlers are not needed here.

Re: sparse docs. In case you have not seen that paper already, there is a
technical report on the ideas behind RWeka:
  http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_ba6

Re: SMO. Compare

m1 <- SMO(Species ~ ., data = iris)
m2 <- SMO(Species ~ ., data = iris, control = Weka_control(
  K = "weka.classifiers.functions.supportVector.RBFKernel"))

which yield different results so the Weka_control() works.

The same happens if you register the mySMO() interface yourself. I'm not
sure why the "E = ..." argument has no influence on the SMO, please check
the Weka docs for this particular learner.

Best,
Z


From f.harrell at vanderbilt.edu  Wed Jul 11 14:50:45 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 11 Jul 2007 07:50:45 -0500
Subject: [R] variable and value labels
In-Reply-To: <46949D12.8000705@promente.org>
References: <46949D12.8000705@promente.org>
Message-ID: <4694D225.8050204@vanderbilt.edu>

Steve Powell wrote:
> Dear list
> I am new here - enjoying the power of R compared to SPSS.
> Looking for sets of tips and tricks for people with old SPSS habits.
> In particular, would like to know an easy way to set variable labels 
> across a dataset and to set value labels for sets of variables.
> Grateful for any help,
> Steve Powell

In the Hmisc package see the functions spss.get, label, upData, and 
describe.
Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From bruno.c at inwind.it  Wed Jul 11 15:18:47 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Wed, 11 Jul 2007 15:18:47 +0200
Subject: [R] 3D plot and interactive PDFs
Message-ID: <JL0NNB$587AE36308A459832D2DCF9EF092E606@libero.it>

With version 8 of acrobat reader, it is now possible to have 3D in PDf documents.
Does it exist already an R package who manage to produce 3D plots which can be saved as interactive 3D graphs in a PDF file?

Best Regards
Bruno Cavestro


------------------------------------------------------
Leggi GRATIS le tue mail con il telefonino i-mode? di Wind
http://i-mode.wind.it/


From dgvirtual at akl.lt  Wed Jul 11 15:27:25 2007
From: dgvirtual at akl.lt (Donatas G.)
Date: Wed, 11 Jul 2007 16:27:25 +0300
Subject: [R] elementary statistics with R (rkward?)
Message-ID: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>

Hi, I am trying to learn some basic statistics stuff but I cannot find any
elementary statistics exercises using R language. Using RKward would be even
better...

I need that in analysing sociological data, obtained through questionnairres -
findind corelations between variables, relations between different types of
data, etc.

Could anyone recommend simple tutorials/exercises, available on www for me to
work on?

I realize it would be much simple to do this introductory stuff with spss, that
everyone around me is using here in Lithuania, but I'd really like to learn to
do it with R instead...

-- 
Donatas G.


From sarah.goslee at gmail.com  Wed Jul 11 15:29:13 2007
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 11 Jul 2007 09:29:13 -0400
Subject: [R] elementary statistics with R (rkward?)
In-Reply-To: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
References: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
Message-ID: <efb536d50707110629w5350cd0cv6ffa3bbd5124c489@mail.gmail.com>

I don't know anything about RKward, but there are many, many
tutorials, guides and other documents written for people learning R
available online.
Try the introduction to R at:
http://www.r-project.org/
under manuals, or some of the many fine contributions at:

http://cran.r-project.org/other-docs.html

Sarah

On 7/11/07, Donatas G. <dgvirtual at akl.lt> wrote:
> Hi, I am trying to learn some basic statistics stuff but I cannot find any
> elementary statistics exercises using R language. Using RKward would be even
> better...
>
> I need that in analysing sociological data, obtained through questionnairres -
> findind corelations between variables, relations between different types of
> data, etc.
>
> Could anyone recommend simple tutorials/exercises, available on www for me to
> work on?
>
> I realize it would be much simple to do this introductory stuff with spss, that
> everyone around me is using here in Lithuania, but I'd really like to learn to
> do it with R instead...
>
> --
> Donatas G.
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From Charles.Annis at StatisticalEngineering.com  Wed Jul 11 15:33:07 2007
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 11 Jul 2007 09:33:07 -0400
Subject: [R] elementary statistics with R (rkward?)
In-Reply-To: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
References: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
Message-ID: <003801c7c3c0$04608500$6400a8c0@DD4XFW31>

Face the music and buy the book: _Introductory Statistics with R_ by Peter
Dalgaard.  It's perfect for what you need.  It's clear and concise and will
teach you statistics AND R as painlessly as such a thing can be.  It's
inexpensive and you can get it on Amazon.com and every other major
bookseller, including the nearest university bookstore.

Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  614-455-3265
http://www.StatisticalEngineering.com
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Donatas G.
Sent: Wednesday, July 11, 2007 9:27 AM
To: r-help at stat.math.ethz.ch
Subject: [R] elementary statistics with R (rkward?)

Hi, I am trying to learn some basic statistics stuff but I cannot find any
elementary statistics exercises using R language. Using RKward would be even
better...

I need that in analysing sociological data, obtained through questionnairres
-
findind corelations between variables, relations between different types of
data, etc.

Could anyone recommend simple tutorials/exercises, available on www for me
to
work on?

I realize it would be much simple to do this introductory stuff with spss,
that
everyone around me is using here in Lithuania, but I'd really like to learn
to
do it with R instead...

-- 
Donatas G.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From vincent.goulet at act.ulaval.ca  Wed Jul 11 15:48:35 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Wed, 11 Jul 2007 09:48:35 -0400
Subject: [R] CDF for pareto distribution
In-Reply-To: <11538272.post@talk.nabble.com>
References: <11538272.post@talk.nabble.com>
Message-ID: <D64F51F5-EC2A-4A9E-A9BD-F0BB495151D1@act.ulaval.ca>

Le 07-07-11 ? 07:56, livia a ?crit :

>
> Hi, I would like to use the following codes to plot the CDF for pareto
> distribution. Before doing this, I have plot the emperical one.
>
> x <- seq(1.6, 3, 0.1)
> lines(x,pgpd(x, 1.544,0.4477557,), col="red")
>
> Could anyone give me some advice whether the above codes are correct?
>
> Many thanks.

livia,

You seem to be struggling with the Pareto distribution... The above  
code seems correct, but you do not say where you took the pdpd()  
function from. This makes it harder for us to help you.

In you other message (https://stat.ethz.ch/pipermail/r-help/2007-July/ 
136137.html) you quote a negative scale parameter. The Pareto I know  
has strictly positive shape and scale parameters.

Perhaps can you retry with functions ppareto() or pgenpareto() of  
package actuar.

---
   Vincent Goulet, Associate Professor
   ?cole d'actuariat
   Universit? Laval, Qu?bec
   Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From kristi.glover at hotmail.com  Wed Jul 11 16:00:20 2007
From: kristi.glover at hotmail.com (Kristi Glover)
Date: Wed, 11 Jul 2007 11:00:20 -0300
Subject: [R] Previously saved workspace restored
Message-ID: <BAY111-W2459E5E4660369DA6E908CFA040@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/0c22269c/attachment.pl 

From yn19832 at msn.com  Wed Jul 11 16:01:46 2007
From: yn19832 at msn.com (livia)
Date: Wed, 11 Jul 2007 07:01:46 -0700 (PDT)
Subject: [R] CDF for pareto distribution
In-Reply-To: <D64F51F5-EC2A-4A9E-A9BD-F0BB495151D1@act.ulaval.ca>
References: <11538272.post@talk.nabble.com>
	<D64F51F5-EC2A-4A9E-A9BD-F0BB495151D1@act.ulaval.ca>
Message-ID: <11540928.post@talk.nabble.com>


Hi, thank you very much for your reply. The function pgpd() is from the
package POT, and the 1.544 is the location parameter, 0.4477557 is the scale
parameter and -0.50113 is the shape parameter, which can be both negtive or
positive.


Vincent Goulet wrote:
> 
> Le 07-07-11 ? 07:56, livia a ?crit :
> 
>>
>> Hi, I would like to use the following codes to plot the CDF for pareto
>> distribution. Before doing this, I have plot the emperical one.
>>
>> x <- seq(1.6, 3, 0.1)
>> lines(x,pgpd(x, 1.544,0.4477557,-0.50113), col="red")
>>
>> Could anyone give me some advice whether the above codes are correct?
>>
>> Many thanks.
> 
> livia,
> 
> You seem to be struggling with the Pareto distribution... The above  
> code seems correct, but you do not say where you took the pdpd()  
> function from. This makes it harder for us to help you.
> 
> In you other message (https://stat.ethz.ch/pipermail/r-help/2007-July/ 
> 136137.html) you quote a negative scale parameter. The Pareto I know  
> has strictly positive shape and scale parameters.
> 
> Perhaps can you retry with functions ppareto() or pgenpareto() of  
> package actuar.
> 
> ---
>    Vincent Goulet, Associate Professor
>    ?cole d'actuariat
>    Universit? Laval, Qu?bec
>    Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/CDF-for-pareto-distribution-tf4061253.html#a11540928
Sent from the R help mailing list archive at Nabble.com.


From h.wickham at gmail.com  Wed Jul 11 16:11:08 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 11 Jul 2007 16:11:08 +0200
Subject: [R] p-value from survreg(), library(survival)
In-Reply-To: <854756c60707110422o1235e668h55681f97971d787e@mail.gmail.com>
References: <854756c60707110422o1235e668h55681f97971d787e@mail.gmail.com>
Message-ID: <f8e6ff050707110711g74d1a9c3w25af42cfe0faed62@mail.gmail.com>

str(survreg(s~groups, dist="gaussian"))

is probably a good place to start.

Hadley

On 7/11/07, Vlado Sremac <vladosr at gmail.com> wrote:
> dear r experts:
> It seems my message got spam filtered, another try:
> i would appreciate advice on how to get the p-value from the object 'sr'
> created  with the function survreg() as given below.
> vlad
>
> sr<-survreg(s~groups, dist="gaussian")
> Coefficients:
> (Intercept)      groups
> -0.02138485  0.03868351
>
> Scale= 0.01789372
>
> Loglik(model)= 31.1   Loglik(intercept only)= 25.4
>         Chisq= 11.39 on 1 degrees of freedom, p= 0.00074
> n= 16
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ezhil02 at yahoo.com  Wed Jul 11 16:14:34 2007
From: ezhil02 at yahoo.com (A Ezhil)
Date: Wed, 11 Jul 2007 07:14:34 -0700 (PDT)
Subject: [R] Power calculation for the time series experiment
Message-ID: <831006.67400.qm@web32405.mail.mud.yahoo.com>

Hi All,

We are planning to run an experiment, where samples
will be taken at different time points (say, 0, 4, 8,
16, 24). If I am interested in the effect size of 1.5
for a reasonably large samples (say 500), what will be
the power? Is it a good idea to use F-test (one-way
ANOVA) as my test statistics?  How can we include
correlation structure among samples in the power
analysis, if I use one-way ANOVA design? 

I am aware of power.anova.test() in R that will help
me to do power calculation for one-way ANOVA.  It will
be of great help if you send me some related articles
or pointers to some useful resources.

Thanks in advance.

Kind regards,
Ezhil


From murdoch at stats.uwo.ca  Wed Jul 11 16:19:17 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 11 Jul 2007 10:19:17 -0400
Subject: [R] 3D plot and interactive PDFs
In-Reply-To: <JL0NNB$587AE36308A459832D2DCF9EF092E606@libero.it>
References: <JL0NNB$587AE36308A459832D2DCF9EF092E606@libero.it>
Message-ID: <4694E6E5.9050401@stats.uwo.ca>

On 7/11/2007 9:18 AM, Bruno C. wrote:
> With version 8 of acrobat reader, it is now possible to have 3D in PDf documents.
> Does it exist already an R package who manage to produce 3D plots which can be saved as interactive 3D graphs in a PDF file?

No, not as far as I know.  If you want to help to make it happen, I'd 
suggest working to add it to the GL2PS project 
(http://www.geuz.org/gl2ps) and then it should be automatically 
incorporated into the rgl package.

Duncan Murdoch


From jh910 at juno.com  Wed Jul 11 16:19:50 2007
From: jh910 at juno.com (J. R. M. Hosking)
Date: Wed, 11 Jul 2007 10:19:50 -0400
Subject: [R] ECDF, distribution of Pareto, distribution of Normal
In-Reply-To: <11524560.post@talk.nabble.com>
References: <11524560.post@talk.nabble.com>
Message-ID: <f72ou6$22a$1@sea.gmane.org>

livia wrote:
> Hello all,
> 
> I would like to plot the emperical CDF, normal CDF and pareto CDF in the
> same graph and I amusing the following codes. "z" is a vector and I just
> need the part when z between 1.6 and 3.
> 
> plot(ecdf(z), do.points=FALSE, verticals=TRUE,
> xlim=c(1.6,3),ylim=c(1-sum(z>1.6)/length(z), 1))
> 
> x <- seq(1.6, 3, 0.1)
> lines(x,pgpd(x, 1.544,0.4373,-0.2398), col="red")
> 
> y <- seq(1.6, 3, 0.1)
> lines(y,pnorm(y, mean(z),sqrt(var(z))), col="blue")
> 
> The emperical CDF and normal CDF look rather resonable, but the pareto CDF
> looks quite odd. I am not sure whether I plot the pareto CDF correctly e.g.
> in the right yaxs or any other mistake?
> 
> At the same time, let "t" represents the vector whose values are larger than
> 1.6(the part we want). If I implement the following codes and plot the
> emperical CDF and pareto CDF, the pareto CDF seems fit.
> 
> plot(ecdf(t), do.points=FALSE, verticals=TRUE)
> x <- seq(1.6, 3, 0.1)
> lines(x,pgpd(x, 1.544,0.4373,-0.2398), col="red")
> 
> Could anyone give me some advice on this? Many thanks.

If any of your data points are less than 1.6, ecdf(z) and ecdf(t)
will be different functions: for arguments greater than 1.6,
the former will take values in c(mean(z<1.6),1) and the latter
will cover the range (0,1).  It is not surprising that your
pgpd function will fit only one of these empirical cdf's closely.

Assuming that those GPD parameters were obtained by fitting to just
the data values greater than 1.6, the GPD curve in your first plot
should be

   u<-mean(z<1.6)
   x<-seq(1.6,3,0.1)
   lines(x, u + (1-u)*pgpd(x, <parameters> )


J. R. M. Hosking


From wwwhsd at gmail.com  Wed Jul 11 16:32:59 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 11 Jul 2007 11:32:59 -0300
Subject: [R] p-value from survreg(), library(survival)
In-Reply-To: <854756c60707110422o1235e668h55681f97971d787e@mail.gmail.com>
References: <854756c60707110422o1235e668h55681f97971d787e@mail.gmail.com>
Message-ID: <da79af330707110732j282b8ce1wfa89e07b04466a3@mail.gmail.com>

Try also:

pchisq(summary(sr)$chi, degrees_freedom, lower=FALSE)

*You need know your degrees of freedom

-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


On 11/07/07, Vlado Sremac <vladosr at gmail.com> wrote:
> dear r experts:
> It seems my message got spam filtered, another try:
> i would appreciate advice on how to get the p-value from the object 'sr'
> created  with the function survreg() as given below.
> vlad
>
> sr<-survreg(s~groups, dist="gaussian")
> Coefficients:
> (Intercept)      groups
> -0.02138485  0.03868351
>
> Scale= 0.01789372
>
> Loglik(model)= 31.1   Loglik(intercept only)= 25.4
>         Chisq= 11.39 on 1 degrees of freedom, p= 0.00074
> n= 16
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Wed Jul 11 16:34:47 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 11 Jul 2007 09:34:47 -0500
Subject: [R] p-value from survreg(), library(survival)
In-Reply-To: <f8e6ff050707110711g74d1a9c3w25af42cfe0faed62@mail.gmail.com>
References: <854756c60707110422o1235e668h55681f97971d787e@mail.gmail.com>
	<f8e6ff050707110711g74d1a9c3w25af42cfe0faed62@mail.gmail.com>
Message-ID: <1184164488.3705.4.camel@Bellerophon.localdomain>

Actually, in this case, looking at the code for:

  survival:::print.survreg

would be better, as the p value is calculate there, rather than being
part of the survreg object. As with many R functions, the p value is
calculated in the print method for the object.

In this case, it is a pretty straightforward p value for the chi-square
statistic.  Using the output of Vlado's example below:

> format(signif(1 - pchisq(11.39, 1), 2))
[1] "0.00074"

HTH,

Marc Schwartz

On Wed, 2007-07-11 at 16:11 +0200, hadley wickham wrote:
> str(survreg(s~groups, dist="gaussian"))
> 
> is probably a good place to start.
> 
> Hadley
> 
> On 7/11/07, Vlado Sremac <vladosr at gmail.com> wrote:
> > dear r experts:
> > It seems my message got spam filtered, another try:
> > i would appreciate advice on how to get the p-value from the object 'sr'
> > created  with the function survreg() as given below.
> > vlad
> >
> > sr<-survreg(s~groups, dist="gaussian")
> > Coefficients:
> > (Intercept)      groups
> > -0.02138485  0.03868351
> >
> > Scale= 0.01789372
> >
> > Loglik(model)= 31.1   Loglik(intercept only)= 25.4
> >         Chisq= 11.39 on 1 degrees of freedom, p= 0.00074
> > n= 16
> >


From sellis at uwindsor.ca  Wed Jul 11 16:11:28 2007
From: sellis at uwindsor.ca (Sandra Ellis)
Date: Wed, 11 Jul 2007 10:11:28 -0400
Subject: [R] survfit for interval censored data
Message-ID: <web-74209605@uwindsor.ca>

<p>Hello,</p><p>I am a new R-user and would like to use survfit for interval censored data.?
Whenever I try I get an error message that states I can only use survfit for right censored or
counting process data.? I was wondering if anyone knows if there is an additional package available
that can?calculate KM curves for interval censored data, or another program with this
capability?</p><p>Thank you.</p><p>S. Ellis (student)</p>


From h.wickham at gmail.com  Wed Jul 11 16:41:18 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 11 Jul 2007 16:41:18 +0200
Subject: [R] p-value from survreg(), library(survival)
In-Reply-To: <1184164488.3705.4.camel@Bellerophon.localdomain>
References: <854756c60707110422o1235e668h55681f97971d787e@mail.gmail.com>
	<f8e6ff050707110711g74d1a9c3w25af42cfe0faed62@mail.gmail.com>
	<1184164488.3705.4.camel@Bellerophon.localdomain>
Message-ID: <f8e6ff050707110741g2d35723fg17b15fb5d077cabe@mail.gmail.com>

On 7/11/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> Actually, in this case, looking at the code for:
>
>   survival:::print.survreg
>
> would be better, as the p value is calculate there, rather than being
> part of the survreg object. As with many R functions, the p value is
> calculated in the print method for the object.

I wish print methods wouldn't do that. Printing is supposed to be
about displaying existing create, not creating new values.

Hadley


From vincent.goulet at act.ulaval.ca  Wed Jul 11 16:46:10 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Wed, 11 Jul 2007 10:46:10 -0400
Subject: [R] CDF for pareto distribution
In-Reply-To: <11540928.post@talk.nabble.com>
References: <11538272.post@talk.nabble.com>
	<D64F51F5-EC2A-4A9E-A9BD-F0BB495151D1@act.ulaval.ca>
	<11540928.post@talk.nabble.com>
Message-ID: <4F508D05-73B6-4DF8-8322-BE88D975F784@act.ulaval.ca>

Le 07-07-11 ? 10:01, livia a ?crit :

>
> Hi, thank you very much for your reply. The function pgpd() is from  
> the
> package POT, and the 1.544 is the location parameter, 0.4477557 is  
> the scale
> parameter and -0.50113 is the shape parameter, which can be both  
> negtive or
> positive.

The documentation of POT states that the scale parameter must be  
positive. You seem right about the shape parameter, though; POT uses  
a different parametrization from the one I'm used to.

The problem with

 > x <- seq(1.6, 3, 0.1)
 > lines(x,pgpd(x, 1.544,0.4477557,-0.50113), col="red")

is that lines() must be used to add to an already existing graphic.  
For a new plot, use

 > x <- seq(1.6, 3, 0.1)
 > plot(x,pgpd(x, 1.544,0.4477557,-0.50113), col="red")

which works just fine here.

We cannot help you more if you do not give a *reproducible* example  
of what is going wrong.

If you have issues with the pgpd() function, you should contact the  
maintainer of package POT, as the Posting Guide asks.

HTH

>
>
> Vincent Goulet wrote:
>>
>> Le 07-07-11 ? 07:56, livia a ?crit :
>>
>>>
>>> Hi, I would like to use the following codes to plot the CDF for  
>>> pareto
>>> distribution. Before doing this, I have plot the emperical one.
>>>
>>> x <- seq(1.6, 3, 0.1)
>>> lines(x,pgpd(x, 1.544,0.4477557,-0.50113), col="red")
>>>
>>> Could anyone give me some advice whether the above codes are  
>>> correct?
>>>
>>> Many thanks.
>>
>> livia,
>>
>> You seem to be struggling with the Pareto distribution... The above
>> code seems correct, but you do not say where you took the pdpd()
>> function from. This makes it harder for us to help you.
>>
>> In you other message (https://stat.ethz.ch/pipermail/r-help/2007- 
>> July/
>> 136137.html) you quote a negative scale parameter. The Pareto I know
>> has strictly positive shape and scale parameters.
>>
>> Perhaps can you retry with functions ppareto() or pgenpareto() of
>> package actuar.
>>
>> ---
>>    Vincent Goulet, Associate Professor
>>    ?cole d'actuariat
>>    Universit? Laval, Qu?bec
>>    Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> -- 
> View this message in context: http://www.nabble.com/CDF-for-pareto- 
> distribution-tf4061253.html#a11540928
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From vmuggeo at dssm.unipa.it  Wed Jul 11 16:47:53 2007
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Wed, 11 Jul 2007 16:47:53 +0200
Subject: [R] 3D plot and interactive PDFs
In-Reply-To: <JL0NNB$587AE36308A459832D2DCF9EF092E606@libero.it>
References: <JL0NNB$587AE36308A459832D2DCF9EF092E606@libero.it>
Message-ID: <4694ED99.9060004@dssm.unipa.it>

This does not answer exactly to your question, anyway..:

If you are planning to use latex, the package movie15 allows to include 
media files in your document (to be processed via pdflatex)

vito



Bruno C. wrote:
> With version 8 of acrobat reader, it is now possible to have 3D in PDf documents.
> Does it exist already an R package who manage to produce 3D plots which can be saved as interactive 3D graphs in a PDF file?
> 
> Best Regards
> Bruno Cavestro
> 
> 
> ------------------------------------------------------
> Leggi GRATIS le tue mail con il telefonino i-mode? di Wind
> http://i-mode.wind.it/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit? di Palermo
viale delle Scienze, edificio 13
90128 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612


From cryan at binghamton.edu  Wed Jul 11 17:57:13 2007
From: cryan at binghamton.edu (Christopher W. Ryan)
Date: Wed, 11 Jul 2007 10:57:13 -0500
Subject: [R] installing, removing, upgrading, and downgrading packages
Message-ID: <4694FDD9.2040808@binghamton.edu>

I'm very new to R, trying to learn it.  I started with R 2.4, but I have
since upgraded to 2.5.0, on WindowsXP.  I understand that 2.5.1 is now
available.

Last night in the course of things I loaded libraries "coin" and
"survival".  I received warning messages that they had been built under
version 2.5.1.  However, as far as I could tell, they worked OK.  But
this brought some questions to mind:

I found the remove.packages() command.  Is there a way to revert a
package back to a previous version without removing it?  If not, and I
remove one version, how do I specify downloading a particular (earlier)
version?

Was that warning of any consequence?  Are there times when a package
will not work properly with an earlier version of R, and will this be
obvious to me when I try to use it?  Or in a more general sense, are
there foreseeable circumstances in which an older version of a package
would be necessary, rather than the newest one?

Should I keep my packages in a library folder outside of the R install
folder?  Right now, it appears that all the packages I download get
installed into a library subdirectory under my R250 directory.
Advantages and disadvantages of either method?

Thanks.
-- 
Christopher W. Ryan, MD
SUNY Upstate Medical University Clinical Campus at Binghamton
40 Arch Street, Johnson City, NY  13790
cryanatbinghamtondotedu
PGP public keys available at http://home.stny.rr.com/ryancw/

"If you want to build a ship, don't drum up the men to gather wood,
divide the work and give orders. Instead, teach them to yearn for the
vast and endless sea."  [Antoine de St. Exupery]


From cryan at binghamton.edu  Wed Jul 11 18:02:29 2007
From: cryan at binghamton.edu (Christopher W. Ryan)
Date: Wed, 11 Jul 2007 11:02:29 -0500
Subject: [R] elementary statistics with R (rkward?)
In-Reply-To: <003801c7c3c0$04608500$6400a8c0@DD4XFW31>
References: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
	<003801c7c3c0$04608500$6400a8c0@DD4XFW31>
Message-ID: <4694FF15.5030704@binghamton.edu>

As a fellow beginner, I also found Handbook of Statistical Analyses
Using R, by Brian Everitt, to be a very useful book.  There is an
accompanying R package, "HSAUR".

Also Using R for Introductory Statistics, by John Verzani.  There is an
accompanying R package, "UsingR".

Christopher W. Ryan, MD
SUNY Upstate Medical University Clinical Campus at Binghamton
40 Arch Street, Johnson City, NY  13790
cryanatbinghamtondotedu
PGP public keys available at http://home.stny.rr.com/ryancw/

"If you want to build a ship, don't drum up the men to gather wood,
divide the work and give orders. Instead, teach them to yearn for the
vast and endless sea."  [Antoine de St. Exupery]

Charles Annis, P.E. wrote:
> Face the music and buy the book: _Introductory Statistics with R_ by Peter
> Dalgaard.  It's perfect for what you need.  It's clear and concise and will
> teach you statistics AND R as painlessly as such a thing can be.  It's
> inexpensive and you can get it on Amazon.com and every other major
> bookseller, including the nearest university bookstore.
> 
> Charles Annis, P.E.
> 
> Charles.Annis at StatisticalEngineering.com
> phone: 561-352-9699
> eFax:  614-455-3265
> http://www.StatisticalEngineering.com
>  
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Donatas G.
> Sent: Wednesday, July 11, 2007 9:27 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] elementary statistics with R (rkward?)
> 
> Hi, I am trying to learn some basic statistics stuff but I cannot find any
> elementary statistics exercises using R language. Using RKward would be even
> better...
> 
> I need that in analysing sociological data, obtained through questionnairres
> -
> findind corelations between variables, relations between different types of
> data, etc.
> 
> Could anyone recommend simple tutorials/exercises, available on www for me
> to
> work on?
> 
> I realize it would be much simple to do this introductory stuff with spss,
> that
> everyone around me is using here in Lithuania, but I'd really like to learn
> to
> do it with R instead...
>


From P.Dalgaard at biostat.ku.dk  Wed Jul 11 17:03:13 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 11 Jul 2007 17:03:13 +0200
Subject: [R] type III ANOVA for a nested linear model
In-Reply-To: <1184157461.3775.19.camel@Amilo>
References: <1184066076.3890.31.camel@Amilo> <4693737B.7070208@biostat.ku.dk>
	<1184157461.3775.19.camel@Amilo>
Message-ID: <4694F131.502@biostat.ku.dk>

Carsten Jaeger wrote:
> Hello Peter,
>
> thanks for your help. I'm quite sure that I specified the right model.
> Factor C is indeed nested within factor A. I think you were confused by
> the numbering of C (1..11), and it is easier to understand when I code
> it as you suggested (1,2,3 within each level of A, as in mydata1 [see
> below]). However, it does not matter which numbering I choose for
> carrying the analysis, as
>
> anova(lm(resp ~ A * B + (C %in% A), mydata))
> anova(lm(resp ~ A * B + (C %in% A), mydata1))
>
> both give the same results (as at least I had expected because of the
> nesting).
>
> However, I found that Anova() from the car package only accepts the
> second version. So,
>
> Anova(lm(resp ~ A * B + (C %in% A), mydata)) does not work (giving an
> error) but
> Anova(lm(resp ~ A * B + (C %in% A), mydata1)) does.
>
> This behaviour is rather confusing, or is there anything I'm missing?
>
>   
You're not listening to what I told you!

A term C %in% A  (or A/C) is not a _specification_ that C is nested in
A, it is a _directive_ to include the terms A and C:A. Now, C:A involves
a term for each combination of A and C, of which many are empty if C is
strictly coarser than A. This may well be what is confusing Anova().

In fact, with this (c(1:3,6:11)) coding of C, A:C is completely
equivalent to C, but if you look at summary(lm(....)) you will see a lot
of NA coefficients in the A:C case. If you use resp ~ A*B+C, then you
still get a couple of missing coefficients in the C terms because of
collinearity with the A terms. (Notice that this is one case where the
order inside the model formula will matter; C+A*B is not the same.)

Whether you'd want C as a random factor is a different matter. It is
often the natural model if C is "subject" and A is "group". Let's assume
that this is the case: In an ordinary linear model, you can test whether
you can remove C (or A:C) , which implies that all subjects in the same
group have the same level of the response. In your case, the hypothesis
is accepted, but the F statistic is around 3 (on (6, 6) DF) , which
suggests that there might be some variation of subjects within groups.
In a mixed-effects model, you assume that this variation exists and
therefore you use the SSD for C as the denominator when testing A, which
is arguably safer than pooling it with the somewhat smaller residual SSD.

> Thanks for your help again, 
>
> Carsten
>
>
> R> mydata
>       A     B      C resp
> 1     1     1      1 34.12
> 2     1     1      2 32.45
> 3     1     1      3 44.55
> 4     1     2      1 20.88
> 5     1     2      2 22.32
> 6     1     2      3 27.71
> 7     2     1      6 38.20
> 8     2     1      7 31.62
> 9     2     1      8 38.71
> 10    2     2      6 18.93
> 11    2     2      7 20.57
> 12    2     2      8 31.55
> 13    3     1      9 40.81
> 14    3     1     10 42.23
> 15    3     1     11 41.26
> 16    3     2      9 28.41
> 17    3     2     10 24.07
> 18    3     2     11 21.16
>
> R> mydata1
>       A     B      C resp
> 1     1     1      1 34.12
> 2     1     1      2 32.45
> 3     1     1      3 44.55
> 4     1     2      1 20.88
> 5     1     2      2 22.32
> 6     1     2      3 27.71
> 7     2     1      1 38.20
> 8     2     1      2 31.62
> 9     2     1      3 38.71
> 10    2     2      1 18.93
> 11    2     2      2 20.57
> 12    2     2      3 31.55
> 13    3     1      1 40.81
> 14    3     1      2 42.23
> 15    3     1      3 41.26
> 16    3     2      1 28.41
> 17    3     2      2 24.07
> 18    3     2      3 21.16
>
>
> On Tue, 2007-07-10 at 13:54 +0200, Peter Dalgaard wrote:
>   
>> Carsten Jaeger wrote:
>>     
>>> Hello,
>>>
>>> is it possible to obtain type III sums of squares for a nested model as
>>> in the following:
>>>
>>> lmod <- lm(resp ~ A * B + (C %in% A), mydata))
>>>
>>> I have tried
>>>
>>> library(car)
>>> Anova(lmod, type="III")
>>>
>>> but this gives me an error (and I also understand from the documentation
>>> of Anova as well as from a previous request
>>> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/64477.html) that it is
>>> not possible to specify nested models with car's Anova).
>>>
>>> anova(lmod) works, of course.
>>>
>>> My data (given below) is balanced so I expect the results to be similar
>>> for both type I and type III sums of squares. But are they *exactly* the
>>> same? The editor of the journal which I'm sending my manuscript to
>>> requests what he calls "conventional" type III tests and I'm not sure if
>>>   
>>> can convince him to accept my type I analysis.
>>>       
>> In balanced designs, type I-IV SSD's are all identical. However, I don't think the model does what I think you think it does. 
>>
>> Notice that "nesting" is used with two diferent meanings, in R it would be that the codings of C only makes sense within levels of A - e.g. if they were numbered 1:3 within each group, but with C==1 when A==1 having nothing to do with C==1 when A==2.  SAS does something. er. else...
>>
>> What I think you want is a model where C is a random terms so that main effects of A can be tested, like in
>>
>>     
>>> summary(aov(resp ~ A * B + Error(C), dd))
>>>       
>> Error: C
>>           Df  Sum Sq Mean Sq F value Pr(>F)
>> A          2  33.123  16.562  0.4981 0.6308
>> Residuals  6 199.501  33.250
>>
>> Error: Within
>>           Df Sum Sq Mean Sq F value   Pr(>F)
>> B          1 915.21  915.21 83.7846 9.57e-05 ***
>> A:B        2  16.13    8.07  0.7384   0.5168
>> Residuals  6  65.54   10.92
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>>
>> (This is essentially the same structure as Martin Bleichner had earlier today, also @web.de. What is this? an epidemic? ;-))
>>
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From marc_schwartz at comcast.net  Wed Jul 11 17:10:50 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 11 Jul 2007 10:10:50 -0500
Subject: [R] p-value from survreg(), library(survival)
In-Reply-To: <f8e6ff050707110741g2d35723fg17b15fb5d077cabe@mail.gmail.com>
References: <854756c60707110422o1235e668h55681f97971d787e@mail.gmail.com>
	<f8e6ff050707110711g74d1a9c3w25af42cfe0faed62@mail.gmail.com>
	<1184164488.3705.4.camel@Bellerophon.localdomain>
	<f8e6ff050707110741g2d35723fg17b15fb5d077cabe@mail.gmail.com>
Message-ID: <1184166650.3705.22.camel@Bellerophon.localdomain>

On Wed, 2007-07-11 at 16:41 +0200, hadley wickham wrote:
> On 7/11/07, Marc Schwartz <marc_schwartz at comcast.net> wrote:
> > Actually, in this case, looking at the code for:
> >
> >   survival:::print.survreg
> >
> > would be better, as the p value is calculate there, rather than being
> > part of the survreg object. As with many R functions, the p value is
> > calculated in the print method for the object.
> 
> I wish print methods wouldn't do that. Printing is supposed to be
> about displaying existing create, not creating new values.
> 
> Hadley

It has been occasionally confusing and I am not sure of the history
behind the diverse approach. To borrow a phrase from the DoD TCSEC
books[1], I don't have my S/R "Rainbow Books" at hand to research it.
Both sets of colors are on shelves in my home office.

The first time I came across this years ago, was with the p value for
the F statistic in a simple linear model. it is calculated in:

  stats:::print.summary.lm

while the individual term p values are calculated in

  summary.lm

versus being part of the returned lm object itself.

I have just become "behaviorally modified" to look in more than one
place for such things...  :-)

Regards,

Marc

[1] http://en.wikipedia.org/wiki/Rainbow_Series


From zur at uchicago.edu  Wed Jul 11 17:23:52 2007
From: zur at uchicago.edu (Richard Zur)
Date: Wed, 11 Jul 2007 10:23:52 -0500
Subject: [R] R CMD SHLIB problem with -lg2c and make:
Message-ID: <4694F608.8060203@uchicago.edu>

Hello,

I have 2 problems with R CMD SHLIB.  The first problem didn't resolve, 
it just morphed into the new problem.  I first have to admit that even 
though I've used R CMD SHLIB for a few years, I don't understand C 
compiling.

The problem arose after our linux guy upgraded from Mandriva 2005 to 
Mandriva 2007.  I was using R 2.4.1, but now it's upgraded to the 
current version.  I used R CMD SHLIB to compile a C file that also 
linked to some fortran code.  The error was something along the lines of 
not being able to find -lg2c.  The gcc version I'm using is now 4.1.1, 
so our linux guy suggested the problem might be related to gfortran and 
f2c.  I'm not sure... is this enough information to know what might be 
going on?

So, I started looking around the FAQs online to try and figure out the 
problem and stumbled upon the R CMD COMPILE command.  I ran it on my C 
and fortran code and didn't see any errors.  I thought that was funny, 
so I erased the .o files it had created and tried again, with COMPILE 
and SHLIB... but now I get the error

make: *** No rule to make target 'mhroc.o'. Stop.

I have no idea what I did or how to correct it.  Can anyone help?

Thank you,
Richard Zur


From hao.liu at bms.com  Wed Jul 11 17:43:10 2007
From: hao.liu at bms.com (Hao Liu)
Date: Wed, 11 Jul 2007 11:43:10 -0400
Subject: [R] tkfocus issue
Message-ID: <4694FA8E.5040309@bms.com>

Dear All:

I am stuck with this issue:

I have a button on a TK window, once click it, it pops up a individual 
plot device:

 individual_plot <- function() {
     tkconfigure(overlay.button, state="normal")
    options(locatorBell = FALSE)
     plotfuntype()
     trellis.focus("panel", 1, 1,highlight=FALSE)
     panel.identify(labels=colnames(dataplot))
}

Now I have another button, originally state="disabled", but activated by 
: tkconfigure(overlay.button, state="normal") in previous function:

The overlay function was writen to overlay another plot to the original 
plot.

--The problem:

1.    once the plot device is out, I can't go back to the Tk window, it 
is take hostage by the plot image some how. As a result, overlay button 
won't work.

2. Is there a mechanism to detect close/destroy of the plotting device 
in R? I want to use that even to make overlay.button state to "disabled" 
so user won't be able to click overlay just to get an error message if 
there is no plot already exist...


Thanks
Hao


From ccleland at optonline.net  Wed Jul 11 17:44:43 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 11 Jul 2007 11:44:43 -0400
Subject: [R] elementary statistics with R (rkward?)
In-Reply-To: <4694FF15.5030704@binghamton.edu>
References: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
	<003801c7c3c0$04608500$6400a8c0@DD4XFW31>
	<4694FF15.5030704@binghamton.edu>
Message-ID: <4694FAEB.6040104@optonline.net>

Christopher W. Ryan wrote:
> As a fellow beginner, I also found Handbook of Statistical Analyses
> Using R, by Brian Everitt, to be a very useful book.  There is an
> accompanying R package, "HSAUR".
> 
> Also Using R for Introductory Statistics, by John Verzani.  There is an
> accompanying R package, "UsingR".

  And for Peter Dalgaard's book there is the ISwR package:

http://cran.r-project.org/src/contrib/Descriptions/ISwR.html

> Christopher W. Ryan, MD
> SUNY Upstate Medical University Clinical Campus at Binghamton
> 40 Arch Street, Johnson City, NY  13790
> cryanatbinghamtondotedu
> PGP public keys available at http://home.stny.rr.com/ryancw/
> 
> "If you want to build a ship, don't drum up the men to gather wood,
> divide the work and give orders. Instead, teach them to yearn for the
> vast and endless sea."  [Antoine de St. Exupery]
> 
> Charles Annis, P.E. wrote:
>> Face the music and buy the book: _Introductory Statistics with R_ by Peter
>> Dalgaard.  It's perfect for what you need.  It's clear and concise and will
>> teach you statistics AND R as painlessly as such a thing can be.  It's
>> inexpensive and you can get it on Amazon.com and every other major
>> bookseller, including the nearest university bookstore.
>>
>> Charles Annis, P.E.
>>
>> Charles.Annis at StatisticalEngineering.com
>> phone: 561-352-9699
>> eFax:  614-455-3265
>> http://www.StatisticalEngineering.com
>>  
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Donatas G.
>> Sent: Wednesday, July 11, 2007 9:27 AM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] elementary statistics with R (rkward?)
>>
>> Hi, I am trying to learn some basic statistics stuff but I cannot find any
>> elementary statistics exercises using R language. Using RKward would be even
>> better...
>>
>> I need that in analysing sociological data, obtained through questionnairres
>> -
>> findind corelations between variables, relations between different types of
>> data, etc.
>>
>> Could anyone recommend simple tutorials/exercises, available on www for me
>> to
>> work on?
>>
>> I realize it would be much simple to do this introductory stuff with spss,
>> that
>> everyone around me is using here in Lithuania, but I'd really like to learn
>> to
>> do it with R instead...
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From bruno.c at inwind.it  Wed Jul 11 17:50:16 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Wed, 11 Jul 2007 17:50:16 +0200
Subject: [R] 3D plot and interactive PDFs
Message-ID: <JL0UNS$DDAEDCAB096F15777DB19A55B27E592D@libero.it>

Thanks vito 

I was aware of movie15 but the point is how to get a U3D or VRLM file out of R :/
I don't know those two standards, nor I know the usual format for R 3D plots ...
And unfortunately I am a bit in a rush so no way, right now, to do some reverse engineering about plot file format in order to convert them in VRLM :D

> This does not answer exactly to your question, anyway..:
> 
> If you are planning to use latex, the package movie15 allows to include 
> media files in your document (to be processed via pdflatex)
> 
> vito
> 
> 
> 
> Bruno C. wrote:
> > With version 8 of acrobat reader, it is now possible to have 3D in PDf documents.
> > Does it exist already an R package who manage to produce 3D plots which can be saved as interactive 3D graphs in a PDF file?
> > 
> > Best Regards
> > Bruno Cavestro
> > 
> > 
> > ------------------------------------------------------
> > Leggi GRATIS le tue mail con il telefonino i-modeTM di Wind
> > http://i-mode.wind.it/
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> 
> -- 
> ====================================
> Vito M.R. Muggeo
> Dip.to Sc Statist e Matem `Vianelli'
> Universit? di Palermo
> viale delle Scienze, edificio 13
> 90128 Palermo - ITALY
> tel: 091 6626240
> fax: 091 485726/485612
> ====================================
> 


------------------------------------------------------
Leggi GRATIS le tue mail con il telefonino i-mode? di Wind
http://i-mode.wind.it/


From jholtman at gmail.com  Wed Jul 11 18:20:49 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 11 Jul 2007 12:20:49 -0400
Subject: [R] exces return by mktcap decile for each year
In-Reply-To: <608398.92952.qm@web51105.mail.re2.yahoo.com>
References: <644e1f320707101620oee53704m90ce02f4009ef489@mail.gmail.com>
	<608398.92952.qm@web51105.mail.re2.yahoo.com>
Message-ID: <644e1f320707110920t6df91d67hf96f6634bf071ddf@mail.gmail.com>

here is one way of doing it using 'ave':

> dat <- read.table(textConnection("        mc         yr    ret
+  32902.233 01/01/1995  0.426
+  15793.691 01/01/1995  0.024
+   2375.868 01/01/1995  0.660
+  54586.558 01/01/1996  0.497
+  10674.900 01/01/1996  0.405
+    859.656 01/01/1996 -0.033
+    770.963 01/01/1995 -1.248
+    423.480 01/01/1995  0.654
+   2135.504 01/01/1995  0.394
+    696.599 01/01/1995 -0.482
+   5115.476 01/01/1995  0.352
+    821.347 01/01/1995  0.869
+  43329.695 01/01/1995  0.495
+   7975.151 01/01/1995  0.112
+    396.450 01/01/1995  0.956
+    843.870 01/01/1995  0.172
+   2727.037 01/01/1995 -0.358
+    114.584 01/01/1995 -1.015
+   1347.327 01/01/1995 -0.083
+   4592.049 01/01/1995 -0.251
+    674.305 01/01/1995 -0.327
+  39424.887 01/01/1996  0.198
+   4447.383 01/01/1996 -0.045
+   1608.540 01/01/1996 -0.109
+    217.151 01/01/1996  0.539
+   1813.320 01/01/1996  0.754
+    145.170 01/01/1996  0.249
+   3176.298 01/01/1996 -0.202
+  14379.686 01/01/1996  0.013
+   3009.059 01/01/1996 -0.328
+   1781.406 01/01/1996 -0.158
+   2576.215 01/01/1996  0.514
+   1236.317 01/01/1996  0.346
+   3003.735 01/01/1996  0.151
+   1544.003 01/01/1996  0.482
+   7588.657 01/01/1996  0.306
+   1516.625 01/01/1996  0.183
+   1596.098 01/01/1996  0.674
+   2792.192 01/01/1996  0.528
+   1276.702 01/01/1996  0.010
+    875.716 01/01/1996  0.189
+   4858.450 01/01/1995  0.250
+   2033.623 01/01/1995 -0.582
+   2164.125 01/01/1995  0.631"), header=TRUE)
> # quantiles by year (need as grouping in next statement
> dat$qByYr <- ave(dat$mc, dat$yr, FUN=function(x){
+     cut(x, quantile(x, prob=seq(0, 1, .1)), include.lowest=TRUE)
+ })
> # compute the mean for year/quantile
> dat$dec.mean <- ave(dat$ret, dat$yr, dat$qByYr, FUN=mean)
> # mean adjusted return
> dat$mean.adjusted <- dat$ret - dat$dec.mean
> dat
          mc         yr    ret qByYr   dec.mean mean.adjusted
1  32902.233 01/01/1995  0.426    10  0.4605000  -0.034500000
2  15793.691 01/01/1995  0.024     9  0.0680000  -0.044000000
3   2375.868 01/01/1995  0.660     6  0.6455000   0.014500000
4  54586.558 01/01/1996  0.497    10  0.2360000   0.261000000
5  10674.900 01/01/1996  0.405     9  0.3555000   0.049500000
6    859.656 01/01/1996 -0.033     1  0.2516667  -0.284666667
7    770.963 01/01/1995 -1.248     3 -0.1895000  -1.058500000
8    423.480 01/01/1995  0.654     1  0.1983333   0.455666667
9   2135.504 01/01/1995  0.394     5 -0.0940000   0.488000000
10   696.599 01/01/1995 -0.482     2 -0.4045000  -0.077500000
11  5115.476 01/01/1995  0.352     8  0.3010000   0.051000000
12   821.347 01/01/1995  0.869     3 -0.1895000   1.058500000
13 43329.695 01/01/1995  0.495    10  0.4605000   0.034500000
14  7975.151 01/01/1995  0.112     9  0.0680000   0.044000000
15   396.450 01/01/1995  0.956     1  0.1983333   0.757666667
16   843.870 01/01/1995  0.172     4  0.0445000   0.127500000
17  2727.037 01/01/1995 -0.358     7 -0.3045000  -0.053500000
18   114.584 01/01/1995 -1.015     1  0.1983333  -1.213333333
19  1347.327 01/01/1995 -0.083     4  0.0445000  -0.127500000
20  4592.049 01/01/1995 -0.251     7 -0.3045000   0.053500000
21   674.305 01/01/1995 -0.327     2 -0.4045000   0.077500000
22 39424.887 01/01/1996  0.198    10  0.2360000  -0.038000000
23  4447.383 01/01/1996 -0.045     8 -0.1235000   0.078500000
24  1608.540 01/01/1996 -0.109     5  0.1623333  -0.271333333
25   217.151 01/01/1996  0.539     1  0.2516667   0.287333333
26  1813.320 01/01/1996  0.754     5  0.1623333   0.591666667
27   145.170 01/01/1996  0.249     1  0.2516667  -0.002666667
28  3176.298 01/01/1996 -0.202     8 -0.1235000  -0.078500000
29 14379.686 01/01/1996  0.013    10  0.2360000  -0.223000000
30  3009.059 01/01/1996 -0.328     7 -0.0885000  -0.239500000
31  1781.406 01/01/1996 -0.158     5  0.1623333  -0.320333333
32  2576.215 01/01/1996  0.514     6  0.5210000  -0.007000000
33  1236.317 01/01/1996  0.346     2  0.2675000   0.078500000
34  3003.735 01/01/1996  0.151     7 -0.0885000   0.239500000
35  1544.003 01/01/1996  0.482     4  0.5780000  -0.096000000
36  7588.657 01/01/1996  0.306     9  0.3555000  -0.049500000
37  1516.625 01/01/1996  0.183     3  0.0965000   0.086500000
38  1596.098 01/01/1996  0.674     4  0.5780000   0.096000000
39  2792.192 01/01/1996  0.528     6  0.5210000   0.007000000
40  1276.702 01/01/1996  0.010     3  0.0965000  -0.086500000
41   875.716 01/01/1996  0.189     2  0.2675000  -0.078500000
42  4858.450 01/01/1995  0.250     8  0.3010000  -0.051000000
43  2033.623 01/01/1995 -0.582     5 -0.0940000  -0.488000000
44  2164.125 01/01/1995  0.631     6  0.6455000  -0.014500000
>
>
>
>

On 7/11/07, Frank Hansen <hansenfrank at yahoo.com> wrote:
> Hi Jim,
>
> Thanks for getting back on this. I did not see your
> email on the help list. I or you can post this
> solution
>
> You are right I mis-stated about mc. mc is real, it is
> yr that is a factor.
>
> Here is a solution, which works, but it is clunky. I
> thought there might be a better/more R-like less
> for-loop way to do this.
>
> dat <- read.table("test.data", header=TRUE)
>
> if( "new.data" %in% ls()) {
>  rm( new.data)
> }
> yrs <- as.character(unique( dat$yr))
> for (y in yrs) {
>  bool <- as.character(dat$yr) == y
>  tmp.dat <-  dat[ bool,]
>  breaks <- quantile(tmp.dat$mc,
> probs=seq(0,1,0.1),na.rm=TRUE)
>  breaks[1] <- breaks[1]*.9
> # breaks >0, else 1st value not in (a,b] interval
>  cuts <- cut(tmp.dat$mc, breaks)
>  means.by.dec <- by( tmp.dat$ret, cuts, mean)
>  for ( i in seq(1, dim( tmp.dat)[1])) {
>    tmp.dat[i,"dec.mean"] <- means.by.dec[ cuts[i]]
>  }
>  if(! "new.data" %in% ls()) {
>    new.data <- tmp.dat
>  }  else {
>    new.data <- rbind( new.data, tmp.dat)
>  }
> }
>
> Here is some test input data in the file test.data
> ----- test.data -----
>         mc         yr    ret
>  32902.233 01/01/1995  0.426
>  15793.691 01/01/1995  0.024
>   2375.868 01/01/1995  0.660
>  54586.558 01/01/1996  0.497
>  10674.900 01/01/1996  0.405
>    859.656 01/01/1996 -0.033
>    770.963 01/01/1995 -1.248
>    423.480 01/01/1995  0.654
>   2135.504 01/01/1995  0.394
>    696.599 01/01/1995 -0.482
>   5115.476 01/01/1995  0.352
>    821.347 01/01/1995  0.869
>  43329.695 01/01/1995  0.495
>   7975.151 01/01/1995  0.112
>    396.450 01/01/1995  0.956
>    843.870 01/01/1995  0.172
>   2727.037 01/01/1995 -0.358
>    114.584 01/01/1995 -1.015
>   1347.327 01/01/1995 -0.083
>   4592.049 01/01/1995 -0.251
>    674.305 01/01/1995 -0.327
>  39424.887 01/01/1996  0.198
>   4447.383 01/01/1996 -0.045
>   1608.540 01/01/1996 -0.109
>    217.151 01/01/1996  0.539
>   1813.320 01/01/1996  0.754
>    145.170 01/01/1996  0.249
>   3176.298 01/01/1996 -0.202
>  14379.686 01/01/1996  0.013
>   3009.059 01/01/1996 -0.328
>   1781.406 01/01/1996 -0.158
>   2576.215 01/01/1996  0.514
>   1236.317 01/01/1996  0.346
>   3003.735 01/01/1996  0.151
>   1544.003 01/01/1996  0.482
>   7588.657 01/01/1996  0.306
>   1516.625 01/01/1996  0.183
>   1596.098 01/01/1996  0.674
>   2792.192 01/01/1996  0.528
>   1276.702 01/01/1996  0.010
>    875.716 01/01/1996  0.189
>   4858.450 01/01/1995  0.250
>   2033.623 01/01/1995 -0.582
>   2164.125 01/01/1995  0.631
>
> Here is the output which looks ok
>
> > new.data
>          mc         yr    ret   dec.mean
> 1  32902.233 01/01/1995  0.426  0.4605000
> 2   4858.450 01/01/1995  0.250  0.3010000
> 3   2033.623 01/01/1995 -0.582 -0.0940000
> 4   2164.125 01/01/1995  0.631  0.6455000
> 5  15793.691 01/01/1995  0.024  0.0680000
> 6   2375.868 01/01/1995  0.660  0.6455000
> 7    770.963 01/01/1995 -1.248 -0.1895000
> 8    423.480 01/01/1995  0.654  0.1983333
> 9   2135.504 01/01/1995  0.394 -0.0940000
> 10   696.599 01/01/1995 -0.482 -0.4045000
> 11  5115.476 01/01/1995  0.352  0.3010000
> 12   821.347 01/01/1995  0.869 -0.1895000
> 13 43329.695 01/01/1995  0.495  0.4605000
> 14  7975.151 01/01/1995  0.112  0.0680000
> 15   396.450 01/01/1995  0.956  0.1983333
> 16   843.870 01/01/1995  0.172  0.0445000
> 17  2727.037 01/01/1995 -0.358 -0.3045000
> 18   114.584 01/01/1995 -1.015  0.1983333
> 19  1347.327 01/01/1995 -0.083  0.0445000
> 20  4592.049 01/01/1995 -0.251 -0.3045000
> 21   674.305 01/01/1995 -0.327 -0.4045000
> 22 39424.887 01/01/1996  0.198  0.2360000
> 23  4447.383 01/01/1996 -0.045 -0.1235000
> 24  1608.540 01/01/1996 -0.109  0.1623333
> 25   217.151 01/01/1996  0.539  0.2516667
> 26  1813.320 01/01/1996  0.754  0.1623333
> 27   145.170 01/01/1996  0.249  0.2516667
> 28  3176.298 01/01/1996 -0.202 -0.1235000
> 29 14379.686 01/01/1996  0.013  0.2360000
> 30  3009.059 01/01/1996 -0.328 -0.0885000
> 31  1781.406 01/01/1996 -0.158  0.1623333
> 32  2576.215 01/01/1996  0.514  0.5210000
> 33  1236.317 01/01/1996  0.346  0.2675000
> 34  3003.735 01/01/1996  0.151 -0.0885000
> 35  1544.003 01/01/1996  0.482  0.5780000
> 36  7588.657 01/01/1996  0.306  0.3555000
> 37  1516.625 01/01/1996  0.183  0.0965000
> 38 54586.558 01/01/1996  0.497  0.2360000
> 39 10674.900 01/01/1996  0.405  0.3555000
> 40   859.656 01/01/1996 -0.033  0.2516667
> 41  1596.098 01/01/1996  0.674  0.5780000
> 42  2792.192 01/01/1996  0.528  0.5210000
> 43  1276.702 01/01/1996  0.010  0.0965000
> 44   875.716 01/01/1996  0.189  0.2675000
> >
>
> notice that records 1 and 13 fall into the same mc
> decile for the year 1995, and their ret mean is .4605
> and so forth for the other mc deciles in both years.
>
> I'd be interested to know if there is a cleaner way to
> do this. Thanks.
>
> Frank
>
>
>
>
> ____________________________________________________________________________________
> TV dinner still cooling?
> Check out "Tonight's Picks" on Yahoo! TV.
> http://tv.yahoo.com/
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From Greg.Snow at intermountainmail.org  Wed Jul 11 18:38:34 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 11 Jul 2007 10:38:34 -0600
Subject: [R] Power calculation for the time series experiment
In-Reply-To: <831006.67400.qm@web32405.mail.mud.yahoo.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAAD681@LP-EXCHVS07.CO.IHC.COM>


The built in power functions are for the fairly straight forward
situations.  Yours does not appear to fit into any of those.  You need
to think through your problem a bit more before starting to think about
power.

What do you mean by effect size of 1.5 (is that 1.5 standard deviations?
Or raw units? What is the SD?  Is the effect of 1.5 the same at each
time point? Or would it change?)

How do you plan on analyzing the data?  Manova? Lme?

What do you expect the correlation structure to be?

I would suggest creating a dataset that represents the structure that
you expect (includes the time points, treatment group, and any thing
else).  Then fill in the response with random data (rnorm to start,
mvrnorm may be useful for the correlated part).  Now analyze this data
with the tool you plan to use to make sure that it works and gives the
expected output.

Now take the code you used above and create a function or set of lines
such that it is easy to change things like the overall sample size, the
correlation(s), the SD and/or effect size.  Have the result of the
function or code be the p-value of interest.

Now use the replicate function to run this code/function a bunch of
times, the number of times that the p-value is less than your alpha is
your estimate of the power for that set of conditions.  Now change some
conditions (sample size, correlation, ...) and repeate.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of A Ezhil
> Sent: Wednesday, July 11, 2007 8:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Power calculation for the time series experiment
> 
> Hi All,
> 
> We are planning to run an experiment, where samples will be 
> taken at different time points (say, 0, 4, 8, 16, 24). If I 
> am interested in the effect size of 1.5 for a reasonably 
> large samples (say 500), what will be the power? Is it a good 
> idea to use F-test (one-way
> ANOVA) as my test statistics?  How can we include correlation 
> structure among samples in the power analysis, if I use 
> one-way ANOVA design? 
> 
> I am aware of power.anova.test() in R that will help me to do 
> power calculation for one-way ANOVA.  It will be of great 
> help if you send me some related articles or pointers to some 
> useful resources.
> 
> Thanks in advance.
> 
> Kind regards,
> Ezhil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From williams222 at llnl.gov  Wed Jul 11 18:40:24 2007
From: williams222 at llnl.gov (Jonathan Williams)
Date: Wed, 11 Jul 2007 09:40:24 -0700
Subject: [R] Drawing rectangles in multiple panels
Message-ID: <7.0.0.16.2.20070711093348.023e31e8@llnl.gov>

Hi folks,

I'm having some trouble understanding the intricacies of panel 
functions.  I wish to create three side-by-side graphs, each with 
different data-- so far, so good: I rbind() the data, add a column of 
subscripts as a conditioning variable, load up the lattice package, 
specify either a c(3,1) 'layout' or work through 'allow.multiple' and 
'outer' and I'm good to go.

But now I wish to add three rectangles to each plot, which will be in 
different places on each panel, and I'm terribly stuck.  I can guess 
this requires defining a panel function on the fly, but none of my 
attempts are working.  Suggestions?

Thanks much, - Jonathan


From Cody_Hamilton at Edwards.com  Wed Jul 11 18:41:59 2007
From: Cody_Hamilton at Edwards.com (Cody Hamilton)
Date: Wed, 11 Jul 2007 09:41:59 -0700
Subject: [R] survfit for interval censored data
In-Reply-To: <web-74209605@uwindsor.ca>
References: <web-74209605@uwindsor.ca>
Message-ID: <4C7D0185E1C6D64AAFB18A2281B262B805A62EF2B8@EXIRV01.am.edwards.lcl>

Sandra,

As far as I am aware, you will have to use a parametric model (survreg) if your survival times are interval-censored.

Regards,
   -Cody

Cody Hamilton
Edwards Lifesciences

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sandra Ellis
Sent: Wednesday, July 11, 2007 7:11 AM
To: r-help at stat.math.ethz.ch
Subject: [R] survfit for interval censored data

<p>Hello,</p><p>I am a new R-user and would like to use survfit for interval censored data.
Whenever I try I get an error message that states I can only use survfit for right censored or
counting process data.  I was wondering if anyone knows if there is an additional package available
that can calculate KM curves for interval censored data, or another program with this
capability?</p><p>Thank you.</p><p>S. Ellis (student)</p>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From deepayan.sarkar at gmail.com  Wed Jul 11 19:04:11 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Wed, 11 Jul 2007 10:04:11 -0700
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <7.0.0.16.2.20070711093348.023e31e8@llnl.gov>
References: <7.0.0.16.2.20070711093348.023e31e8@llnl.gov>
Message-ID: <eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.com>

On 7/11/07, Jonathan Williams <williams222 at llnl.gov> wrote:
> Hi folks,
>
> I'm having some trouble understanding the intricacies of panel
> functions.  I wish to create three side-by-side graphs, each with
> different data-- so far, so good: I rbind() the data, add a column of
> subscripts as a conditioning variable, load up the lattice package,
> specify either a c(3,1) 'layout' or work through 'allow.multiple' and
> 'outer' and I'm good to go.
>
> But now I wish to add three rectangles to each plot, which will be in
> different places on each panel, and I'm terribly stuck.  I can guess
> this requires defining a panel function on the fly, but none of my
> attempts are working.  Suggestions?

You haven't told us what determines the rectangles (only that they are
different in each panel). If they are completely driven by panel data,
here's an example:

panel.qrect <-
    function(x, y, ...)
{
    xq <- quantile(x, c(0.1, 0.9))
    yq <- quantile(y, c(0.1, 0.9))
    panel.rect(xq[1], yq[1], xq[2], yq[2],
               col = "grey86", border = NA)
    panel.xyplot(x, y, ...)
}

xyplot(Sepal.Length ~ Sepal.Width | Species, iris,
       panel = panel.qrect)

If the rectangles are somehow determined externally, you probably want
to use one of the accessor functions described in help(panel.number).
There are good and bad (i.e. less robust) ways to use these, but we
need to know your use case before recommending one.

-Deepayan


From bjs379 at psu.edu  Wed Jul 11 19:06:06 2007
From: bjs379 at psu.edu (Byran Smucker)
Date: Wed, 11 Jul 2007 13:06:06 -0400
Subject: [R] error using lp function in linux
Message-ID: <1184173565l.1896650l.0l@psu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/4a4f04c9/attachment.pl 

From williams222 at llnl.gov  Wed Jul 11 19:22:06 2007
From: williams222 at llnl.gov (Jonathan Williams)
Date: Wed, 11 Jul 2007 10:22:06 -0700
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.co
 m>
References: <7.0.0.16.2.20070711093348.023e31e8@llnl.gov>
	<eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.com>
Message-ID: <7.0.0.16.2.20070711101948.02381eb8@llnl.gov>

Deepayan,

Thanks for the clarification.  The rectangles are completely external 
to the panel data, and correspond to 90% confidence intervals built 
from training data, to be overlaid on these graphs of the test data.

- Jonathan

At 10:04 AM 7/11/2007, you wrote:
>On 7/11/07, Jonathan Williams <williams222 at llnl.gov> wrote:
>>Hi folks,
>>
>>I'm having some trouble understanding the intricacies of panel
>>functions.  I wish to create three side-by-side graphs, each with
>>different data-- so far, so good: I rbind() the data, add a column of
>>subscripts as a conditioning variable, load up the lattice package,
>>specify either a c(3,1) 'layout' or work through 'allow.multiple' and
>>'outer' and I'm good to go.
>>
>>But now I wish to add three rectangles to each plot, which will be in
>>different places on each panel, and I'm terribly stuck.  I can guess
>>this requires defining a panel function on the fly, but none of my
>>attempts are working.  Suggestions?
>
>You haven't told us what determines the rectangles (only that they are
>different in each panel). If they are completely driven by panel data,
>here's an example:
>
>panel.qrect <-
>    function(x, y, ...)
>{
>    xq <- quantile(x, c(0.1, 0.9))
>    yq <- quantile(y, c(0.1, 0.9))
>    panel.rect(xq[1], yq[1], xq[2], yq[2],
>               col = "grey86", border = NA)
>    panel.xyplot(x, y, ...)
>}
>
>xyplot(Sepal.Length ~ Sepal.Width | Species, iris,
>       panel = panel.qrect)
>
>If the rectangles are somehow determined externally, you probably want
>to use one of the accessor functions described in help(panel.number).
>There are good and bad (i.e. less robust) ways to use these, but we
>need to know your use case before recommending one.
>
>-Deepayan


From amnakhan493 at gmail.com  Wed Jul 11 19:30:59 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Wed, 11 Jul 2007 10:30:59 -0700
Subject: [R] How to load permtest package
Message-ID: <3ffd3bb60707111030q8023c9ax4dcb71ae2854cecc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/cd74c65b/attachment.pl 

From gunter.berton at gene.com  Wed Jul 11 19:30:55 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Wed, 11 Jul 2007 10:30:55 -0700
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.com>
Message-ID: <003d01c7c3e1$3d1babb0$4d908980@gne.windows.gene.com>

Deepayan et. al.:

A question/comment: I have usually found that the subscripts argument is
what I need when passing *external* information into the panel function, for
example, when I wish to add results from a fit done external to the trellis
call. Fits[subscripts] gives me the fits (or whatever) I want to plot for
each panel. It is not clear to me how the panel layout information from
panel.number(), etc. would be helpful here instead. Am I correct? -- or is
there a smarter way to do this that I've missed?

Cheers,

Bert

Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
deepayan.sarkar at gmail.com
Sent: Wednesday, July 11, 2007 10:04 AM
To: Jonathan Williams
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Drawing rectangles in multiple panels

On 7/11/07, Jonathan Williams <williams222 at llnl.gov> wrote:
> Hi folks,
>
> I'm having some trouble understanding the intricacies of panel
> functions.  I wish to create three side-by-side graphs, each with
> different data-- so far, so good: I rbind() the data, add a column of
> subscripts as a conditioning variable, load up the lattice package,
> specify either a c(3,1) 'layout' or work through 'allow.multiple' and
> 'outer' and I'm good to go.
>
> But now I wish to add three rectangles to each plot, which will be in
> different places on each panel, and I'm terribly stuck.  I can guess
> this requires defining a panel function on the fly, but none of my
> attempts are working.  Suggestions?

You haven't told us what determines the rectangles (only that they are
different in each panel). If they are completely driven by panel data,
here's an example:

panel.qrect <-
    function(x, y, ...)
{
    xq <- quantile(x, c(0.1, 0.9))
    yq <- quantile(y, c(0.1, 0.9))
    panel.rect(xq[1], yq[1], xq[2], yq[2],
               col = "grey86", border = NA)
    panel.xyplot(x, y, ...)
}

xyplot(Sepal.Length ~ Sepal.Width | Species, iris,
       panel = panel.qrect)

If the rectangles are somehow determined externally, you probably want
to use one of the accessor functions described in help(panel.number).
There are good and bad (i.e. less robust) ways to use these, but we
need to know your use case before recommending one.

-Deepayan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Leigh.Alexander at Colorado.EDU  Wed Jul 11 19:32:40 2007
From: Leigh.Alexander at Colorado.EDU (Leigh E Alexander)
Date: Wed, 11 Jul 2007 11:32:40 -0600
Subject: [R] aov() question
Message-ID: <95470DF9-A25F-4626-8584-7D3A4CE74F6C@Colorado.EDU>

Hi all,
So I think I have seen some similar questions to mine when I searched  
the archives, but have not seen any concrete answers and was  
wondering if any one could help.
I have been trying to use R's aov() function to analyze my data.  I  
have a 3 x 4 x 2 repeated measures design.  All of the IVs are within  
subjects.    I do also have missing values (unequal N), as I have to  
remove any incorrect trials for each subject.

Here is the code I entered and the error message:

a<-aov(log(rt)~(tran*block*half) + Error (sid/ (tran*block*half)),  
data=mydata2)

Warning message:
Error() model is singular in: aov(log(rt) ~ (tran * block * half) +  
Error(sid/(tran * block *

I then do summary(a) and am able to get an output, but I am not sure  
whether or not I can trust that output since I got the error message.
Any body have any thoughts/solutions for this?

Also, are there any benefits of you aov() vs. use some of the linear  
model functions or vice versa?

Thanks for any help you can offer!!

~Leigh Alexander


From deepayan.sarkar at gmail.com  Wed Jul 11 19:54:23 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Wed, 11 Jul 2007 10:54:23 -0700
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <003d01c7c3e1$3d1babb0$4d908980@gne.windows.gene.com>
References: <eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.com>
	<003d01c7c3e1$3d1babb0$4d908980@gne.windows.gene.com>
Message-ID: <eb555e660707111054q1dbac9e1l920423835cfb89e5@mail.gmail.com>

On 7/11/07, Bert Gunter <gunter.berton at gene.com> wrote:
> Deepayan et. al.:
>
> A question/comment: I have usually found that the subscripts argument is
> what I need when passing *external* information into the panel function, for
> example, when I wish to add results from a fit done external to the trellis
> call. Fits[subscripts] gives me the fits (or whatever) I want to plot for
> each panel. It is not clear to me how the panel layout information from
> panel.number(), etc. would be helpful here instead. Am I correct? -- or is
> there a smarter way to do this that I've missed?

subscripts is absolutely the right thing to use if your auxiliary
information is in the form of vectors than have the same length as the
rest of your data. Examples would include a color for every point in a
xyplot or confidence bounds in a dotplot. However, sometimes your
external information might be a summary; say the parameters defining a
fitted curve for every combination of your conditioning variables (and
the underlying model might have shared information across
combinations, so you wouldn't be able to compute them from the panel
data alone). In that case, which.packet(), which gives you the levels
of the conditioning variables defining the current panel, may be
helpful.

Of course, there are workarounds using subscripts, or in this example,
adding rows to your data frame containing the fitted values. The
accessors are a convenience that sometimes make life simpler.

-Deepayan


From deepayan.sarkar at gmail.com  Wed Jul 11 20:07:25 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Wed, 11 Jul 2007 11:07:25 -0700
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <7.0.0.16.2.20070711101948.02381eb8@llnl.gov>
References: <7.0.0.16.2.20070711093348.023e31e8@llnl.gov>
	<eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.com>
	<7.0.0.16.2.20070711101948.02381eb8@llnl.gov>
Message-ID: <eb555e660707111107s1ea7cd1cuc42e6ef000aad6a1@mail.gmail.com>

On 7/11/07, Jonathan Williams <williams222 at llnl.gov> wrote:
> Deepayan,
>
> Thanks for the clarification.  The rectangles are completely external
> to the panel data, and correspond to 90% confidence intervals built
> from training data, to be overlaid on these graphs of the test data.

Right. So if you have that information in a single object (say a
list), you can specify that as an argument to xyplot (or whatever),
and capture that in the panel function to then index it. E.g.


rectInfo <-
    list(matrix(runif(4), 2, 2),
         matrix(runif(4), 2, 2),
         matrix(runif(4), 2, 2))


panel.qrect <-
    function(x, y, ..., rect.info)
{
    ri <- rect.info[[packet.number()]]
    ## if you have more than one conditioning variable, this might be
    ## something like
    ## ri <- do.call("[", list(rect.info, as.list(which.packet())))[[1]]
    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
               col = "grey86", border = NA)
    panel.xyplot(x, y, ...)
}


xyplot(runif(30) ~ runif(30) | gl(3, 10),
       rect.info = rectInfo,
       panel = panel.qrect)

-Deepayan


From h.wickham at gmail.com  Wed Jul 11 20:08:32 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 11 Jul 2007 20:08:32 +0200
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <003d01c7c3e1$3d1babb0$4d908980@gne.windows.gene.com>
References: <eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.com>
	<003d01c7c3e1$3d1babb0$4d908980@gne.windows.gene.com>
Message-ID: <f8e6ff050707111108i2a1e5587gee1617dae21b20b@mail.gmail.com>

> A question/comment: I have usually found that the subscripts argument is
> what I need when passing *external* information into the panel function, for
> example, when I wish to add results from a fit done external to the trellis
> call. Fits[subscripts] gives me the fits (or whatever) I want to plot for
> each panel. It is not clear to me how the panel layout information from
> panel.number(), etc. would be helpful here instead. Am I correct? -- or is
> there a smarter way to do this that I've missed?

This is one of things that I think ggplot does better - it's much
easier to plot multiple data sources.  I don't have many examples of
this yet, but the final example on
http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.

For the original poster ggplot2 isn't that much more convenient,
because there isn't a built in rectangle geom (although it would be
trivial to add one).  You could use the more general polygon geom,
http://had.co.nz/ggplot2/geom_polygon.html, however it currently
doesn't have a lot of documentation.

Hadley


From albmont at centroin.com.br  Wed Jul 11 20:16:10 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Wed, 11 Jul 2007 16:16:10 -0200
Subject: [R] system: Linux vs. Windows differences
Message-ID: <20070711181342.M88643@centroin.com.br>

[I tried to send this messages two days ago, but I guess I mistyped the To: 
address...]

Why "system" is different in Linux and Windows? Both in R 2.4.1, but in 
Windows there is an option:

  system(something, wait = FALSE)

while on Linux (Fedora Core 4), there is no such option?

Alberto Monteiro


From marc_schwartz at comcast.net  Wed Jul 11 20:28:19 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 11 Jul 2007 13:28:19 -0500
Subject: [R] system: Linux vs. Windows differences
In-Reply-To: <20070711181342.M88643@centroin.com.br>
References: <20070711181342.M88643@centroin.com.br>
Message-ID: <1184178499.27333.7.camel@Bellerophon.localdomain>

On Wed, 2007-07-11 at 16:16 -0200, Alberto Monteiro wrote:
> [I tried to send this messages two days ago, but I guess I mistyped the To: 
> address...]
> 
> Why "system" is different in Linux and Windows? Both in R 2.4.1, but in 
> Windows there is an option:
> 
>   system(something, wait = FALSE)
> 
> while on Linux (Fedora Core 4), there is no such option?
> 
> Alberto Monteiro

From:

  https://svn.r-project.org/R/trunk/NEWS

for changes in R 2.5.0:

o	system() now takes the same set of arguments on all platforms,
	with those which are not applicable being ignored with a
	warning.  Unix-alikes gain 'input' and 'wait', and Windows
	gains 'ignore.stderr'.


Time to upgrade both your R and your FC installation.  

R is at 2.5.1.

FC4 has been EOL (End of Life) for some time and FC5 hit EOL last month.

HTH,

Marc Schwartz


From lutz.breitling at gmail.com  Wed Jul 11 21:06:25 2007
From: lutz.breitling at gmail.com (Lutz Ph. Breitling)
Date: Wed, 11 Jul 2007 21:06:25 +0200
Subject: [R] Stepwise GLM selection by LRT?
Message-ID: <2e38a1c80707111206l499cc328r6fff2d785fc7719e@mail.gmail.com>

Dear List,

having searched the help and archives, I have the impression that
there is no automatic model selection procedure implemented in R that
includes/excludes predictors in logistic regression models based on
LRT P-values. Is that true, or is someone aware of an appropriate
function somewhere in a custom package?

Even if automatic model selection and LRT might not be the most
appropriate methods, I actually would like to use these in order to
simulate someone else's modeling approach...

Many thanks for all comments-
Lutz
-----
Lutz Ph. Breitling
German Cancer Research Center
Heidelberg/Germany


From rvaradhan at jhmi.edu  Wed Jul 11 21:38:34 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 11 Jul 2007 15:38:34 -0400
Subject: [R] Stepwise GLM selection by LRT?
In-Reply-To: <2e38a1c80707111206l499cc328r6fff2d785fc7719e@mail.gmail.com>
References: <2e38a1c80707111206l499cc328r6fff2d785fc7719e@mail.gmail.com>
Message-ID: <002a01c7c3f3$119d01c0$7c94100a@win.ad.jhu.edu>

Check out the stepAIC function in MASS package.  This is a nice tool, where
you can actually implement any penalty even though the function's name has
"AIC" in it because it is the default.  Although this doesn't do an LRT test
based variable selection, you can sort of approximate it by using a penalty
of k = qchisq(1-p, df=1), where p is the p-value for variable selection.
This penalty means that a variable enters/exits an existing model, when the
absolute value of change in log-likelihood is greater than qchisq(1-p,
df=1). For p = 0.1, k = 2.71, and for p=0.05, k = 3.84.  Is this whhant
you'd like to do?
 
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lutz Ph. Breitling
Sent: Wednesday, July 11, 2007 3:06 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Stepwise GLM selection by LRT?

Dear List,

having searched the help and archives, I have the impression that
there is no automatic model selection procedure implemented in R that
includes/excludes predictors in logistic regression models based on
LRT P-values. Is that true, or is someone aware of an appropriate
function somewhere in a custom package?

Even if automatic model selection and LRT might not be the most
appropriate methods, I actually would like to use these in order to
simulate someone else's modeling approach...

Many thanks for all comments-
Lutz
-----
Lutz Ph. Breitling
German Cancer Research Center
Heidelberg/Germany

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From mazatlanmexico at yahoo.com  Wed Jul 11 23:00:15 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Wed, 11 Jul 2007 14:00:15 -0700 (PDT)
Subject: [R] How to get weekly Co-Variance
Message-ID: <945707.87002.qm@web56612.mail.re3.yahoo.com>

Hi: I am trying to migrate from Systat to R but I am facing my first challenge. While I easily can get weekly co-variance for my data, I can't seem to acomplish this with R ( I can't figure out how is done) If interested in looking a sample of my data, please check the Excel attachment. In Systat from Week 28 I get a covariance of 1055 fish.
  Thanks in advance


Felipe D. Carrillo
  Fishery Biologist
  US Fish & Wildlife Service
  Red Bluff, California 96080

       
---------------------------------

From dpowers at mail.la.utexas.edu  Wed Jul 11 23:24:06 2007
From: dpowers at mail.la.utexas.edu (Dan Powers)
Date: Wed, 11 Jul 2007 16:24:06 -0500
Subject: [R] make error R-5.1 on  sun solaris
Message-ID: <000001c7c401$d31a0e20$775d7481@austin.utexas.edu>

I hope this is enough information to determine the problem. Thanks in
advance for any help.

Configure goes ok (I think)

./configure --prefix=$HOME --without-iconv


R is now configured for sparc-sun-solaris2.9

  Source directory:          .
  Installation directory:    /home/dpowers

  C compiler:                gcc  -g -O2
  Fortran 77 compiler:       f95  -g

  C++ compiler:              g++  -g -O2
  Fortran 90/95 compiler:    f95 -g
  Obj-C compiler:             -g -O2

  Interfaces supported:      X11
  External libraries:        readline
  Additional capabilities:   NLS
  Options enabled:           shared BLAS, R profiling, Java

  Recommended packages:      yes

Make ends after the gcc..

make
.
.
.

gcc -I. -I../../src/include -I../../src/include -I/usr/openwin/include
-I/usr/local/include -DHAVE_CONFIG_H   -g -O2 -c system.c -o system.o
system.c: In function `Rf_initialize_R':
system.c:144: parse error before `char'
system.c:216: `localedir' undeclared (first use in this function)
system.c:216: (Each undeclared identifier is reported only once
system.c:216: for each function it appears in.)
*** Error code 1
make: Fatal error: Command failed for target `system.o'
Current working directory /home/dpowers/R-2.5.1/src/unix
*** Error code 1
make: Fatal error: Command failed for target `R'
Current working directory /home/dpowers/R-2.5.1/src/unix
*** Error code 1
make: Fatal error: Command failed for target `R'
Current working directory /home/dpowers/R-2.5.1/src
*** Error code 1
make: Fatal error: Command failed for target `R'


I have tried setting localedir directly in configure options, but get the
same error.

Any ideas?

Thanks,
Dan
=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Daniel A. Powers, Ph.D.
Department of Sociology
University of Texas at Austin
1 University Station A1700
Austin, TX  78712-0118
phone: 512-232-6335
fax:   512-471-1748
dpowers at mail.la.utexas.edu


From ted.harding at nessie.mcc.ac.uk  Wed Jul 11 23:32:58 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 11 Jul 2007 22:32:58 +0100 (BST)
Subject: [R] system: Linux vs. Windows differences
In-Reply-To: <1184178499.27333.7.camel@Bellerophon.localdomain>
Message-ID: <XFMail.070711223258.ted.harding@nessie.mcc.ac.uk>

On 11-Jul-07 18:28:19, Marc Schwartz wrote:
> On Wed, 2007-07-11 at 16:16 -0200, Alberto Monteiro wrote:
>> [I tried to send this messages two days ago, but I guess I mistyped
>> the To: 
>> address...]
>> 
>> Why "system" is different in Linux and Windows? Both in R 2.4.1, but
>> in 
>> Windows there is an option:
>> 
>>   system(something, wait = FALSE)
>> 
>> while on Linux (Fedora Core 4), there is no such option?
>> 
>> Alberto Monteiro
> 
> From:
> 
>   https://svn.r-project.org/R/trunk/NEWS
> 
> for changes in R 2.5.0:
> 
> o     system() now takes the same set of arguments on all platforms,
>       with those which are not applicable being ignored with a
>       warning.  Unix-alikes gain 'input' and 'wait', and Windows
>       gains 'ignore.stderr'.
> 
> 
> Time to upgrade both your R and your FC installation.  
> 
> R is at 2.5.1.
> 
> FC4 has been EOL (End of Life) for some time and FC5 hit EOL last
> month.
> 
> HTH,
> 
> Marc Schwartz

"End of Life" is a Nomenklatura categorisation.

While we loyal Members welcome and applaud President-2.5.1,
we do not forget old Comrades now air-brushed from the photographs
who, now faceless, sturdily labour 24 hours a day in the fields
and saltmines, and even in the dark attics of those who conceal
and protect them still.

There can be life in old dogs, and even strong teeth in some.

Ted.
[emailing from SuSE-5.2 (1997), logged in from Red Hat 9 (2003)]

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 11-Jul-07                                       Time: 22:32:13
------------------------------ XFMail ------------------------------


From p.dalgaard at biostat.ku.dk  Thu Jul 12 00:08:15 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 12 Jul 2007 00:08:15 +0200
Subject: [R] make error R-5.1 on  sun solaris
In-Reply-To: <000001c7c401$d31a0e20$775d7481@austin.utexas.edu>
References: <000001c7c401$d31a0e20$775d7481@austin.utexas.edu>
Message-ID: <469554CF.7020402@biostat.ku.dk>

Dan Powers wrote:
> I hope this is enough information to determine the problem. Thanks in
> advance for any help.
>
> Configure goes ok (I think)
>
> ./configure --prefix=$HOME --without-iconv
>
>
> R is now configured for sparc-sun-solaris2.9
>
>   Source directory:          .
>   Installation directory:    /home/dpowers
>
>   C compiler:                gcc  -g -O2
>   Fortran 77 compiler:       f95  -g
>
>   C++ compiler:              g++  -g -O2
>   Fortran 90/95 compiler:    f95 -g
>   Obj-C compiler:             -g -O2
>
>   Interfaces supported:      X11
>   External libraries:        readline
>   Additional capabilities:   NLS
>   Options enabled:           shared BLAS, R profiling, Java
>
>   Recommended packages:      yes
>
> Make ends after the gcc..
>
> make
> .
> .
> .
>
> gcc -I. -I../../src/include -I../../src/include -I/usr/openwin/include
> -I/usr/local/include -DHAVE_CONFIG_H   -g -O2 -c system.c -o system.o
> system.c: In function `Rf_initialize_R':
> system.c:144: parse error before `char'
> system.c:216: `localedir' undeclared (first use in this function)
> system.c:216: (Each undeclared identifier is reported only once
> system.c:216: for each function it appears in.)
> *** Error code 1
> make: Fatal error: Command failed for target `system.o'
> Current working directory /home/dpowers/R-2.5.1/src/unix
> *** Error code 1
> make: Fatal error: Command failed for target `R'
> Current working directory /home/dpowers/R-2.5.1/src/unix
> *** Error code 1
> make: Fatal error: Command failed for target `R'
> Current working directory /home/dpowers/R-2.5.1/src
> *** Error code 1
> make: Fatal error: Command failed for target `R'
>
>
> I have tried setting localedir directly in configure options, but get the
> same error.
>
> Any ideas?
>
>   
Hmm, which version of gcc is this? The problem seems to be around line 
144 which reads

140     Rstart Rp = &rstart;
141     cmdlines[0] = '\0';
142 
143 #ifdef ENABLE_NLS
144     char localedir[PATH_MAX+20];
145 #endif
146 
147 #if defined(HAVE_SYS_RESOURCE_H) && defined(HAVE_GETRLIMIT)
148 {
149     struct rlimit rlim;


I seem to remember that it used to be non-kosher to mix declarations 
and ordinary code like that, but the current compiler doesn't seem to 
care (I do have #define ENABLE_NLS 1 in Rconfig.h, as I assume you do 
too). Could you perhaps try moving line 141 down below #endif?



> Thanks,
> Dan
> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
> Daniel A. Powers, Ph.D.
> Department of Sociology
> University of Texas at Austin
> 1 University Station A1700
> Austin, TX  78712-0118
> phone: 512-232-6335
> fax:   512-471-1748
> dpowers at mail.la.utexas.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From marc_schwartz at comcast.net  Thu Jul 12 00:33:05 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 11 Jul 2007 17:33:05 -0500
Subject: [R] system: Linux vs. Windows differences
In-Reply-To: <XFMail.070711223258.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070711223258.ted.harding@nessie.mcc.ac.uk>
Message-ID: <1184193185.27333.70.camel@Bellerophon.localdomain>

On Wed, 2007-07-11 at 22:32 +0100, ted.harding at nessie.mcc.ac.uk wrote:
> On 11-Jul-07 18:28:19, Marc Schwartz wrote:
> > On Wed, 2007-07-11 at 16:16 -0200, Alberto Monteiro wrote:
> >> [I tried to send this messages two days ago, but I guess I mistyped
> >> the To: 
> >> address...]
> >> 
> >> Why "system" is different in Linux and Windows? Both in R 2.4.1, but
> >> in 
> >> Windows there is an option:
> >> 
> >>   system(something, wait = FALSE)
> >> 
> >> while on Linux (Fedora Core 4), there is no such option?
> >> 
> >> Alberto Monteiro
> > 
> > From:
> > 
> >   https://svn.r-project.org/R/trunk/NEWS
> > 
> > for changes in R 2.5.0:
> > 
> > o     system() now takes the same set of arguments on all platforms,
> >       with those which are not applicable being ignored with a
> >       warning.  Unix-alikes gain 'input' and 'wait', and Windows
> >       gains 'ignore.stderr'.
> > 
> > 
> > Time to upgrade both your R and your FC installation.  
> > 
> > R is at 2.5.1.
> > 
> > FC4 has been EOL (End of Life) for some time and FC5 hit EOL last
> > month.
> > 
> > HTH,
> > 
> > Marc Schwartz
> 
> "End of Life" is a Nomenklatura categorisation.
> 
> While we loyal Members welcome and applaud President-2.5.1,
> we do not forget old Comrades now air-brushed from the photographs
> who, now faceless, sturdily labour 24 hours a day in the fields
> and saltmines, and even in the dark attics of those who conceal
> and protect them still.
> 
> There can be life in old dogs, and even strong teeth in some.
> 
> Ted.
> [emailing from SuSE-5.2 (1997), logged in from Red Hat 9 (2003)]

Wow...  :-)

I am envisioning the above as the Prologue for a book...I am feeling
suddenly melancholy...   

I whole heartily nominate this as a fortune!

Regards,

Marc


From deepayan.sarkar at gmail.com  Thu Jul 12 00:52:58 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 11 Jul 2007 15:52:58 -0700
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <f8e6ff050707111108i2a1e5587gee1617dae21b20b@mail.gmail.com>
References: <eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.com>
	<003d01c7c3e1$3d1babb0$4d908980@gne.windows.gene.com>
	<f8e6ff050707111108i2a1e5587gee1617dae21b20b@mail.gmail.com>
Message-ID: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>

On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > A question/comment: I have usually found that the subscripts argument is
> > what I need when passing *external* information into the panel function, for
> > example, when I wish to add results from a fit done external to the trellis
> > call. Fits[subscripts] gives me the fits (or whatever) I want to plot for
> > each panel. It is not clear to me how the panel layout information from
> > panel.number(), etc. would be helpful here instead. Am I correct? -- or is
> > there a smarter way to do this that I've missed?
>
> This is one of things that I think ggplot does better - it's much
> easier to plot multiple data sources.  I don't have many examples of
> this yet, but the final example on
> http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.

That's probably true. The Trellis approach is to define a plot by
"data source" + "type of plot", whereas the ggplot approach (if I
understand correctly) is to create a specification for the display
(incrementally?) and then render it. Since the specification can be
very general, the approach is very flexible. The downside is that you
need to learn the language.

On a philosophical note, I think the apparent limitations of Trellis
in some (not all) cases is just due to the artificial importance given
to data frames as the one true container for data. Now that we have
proper multiple dispatch in S4, we can write methods that behave like
traditional Trellis calls but work with more complex data structures.
We have tried this in one bioconductor package (flowViz) with
encouraging results.

-Deepayan


From richard.rowe at jcu.edu.au  Thu Jul 12 01:55:24 2007
From: richard.rowe at jcu.edu.au (Richard Rowe)
Date: Thu, 12 Jul 2007 09:55:24 +1000
Subject: [R] type III ANOVA for a nested linear model
In-Reply-To: <11536509.post@talk.nabble.com>
References: <1184066076.3890.31.camel@Amilo>	<B998A44C8986644EA8029CFE6396A924B68056@exqld2-bne.nexus.csiro.au>	<07E228A5BE53C24CAD490193A7381BBBAAD4EA@LP-EXCHVS07.CO.IHC.COM>	<1184108283.4812.6.camel@sib-sblomber01d.sib.uq.edu.au>
	<11536509.post@talk.nabble.com>
Message-ID: <46956DEC.7030300@jcu.edu.au>

Mark Difford wrote:
> Indeed!  And, apropos of the expression, "to be Ripleyed" (and so be
> condemned to eating cookies for a long, long time), what about being
> "Billasted"?
>
> BestR,
> Mark.
>
>
> Simon Blomberg-4 wrote:
>   
>> I second the nomination!
>>
>> Simon.
>>
>> On Tue, 2007-07-10 at 10:02 -0600, Greg Snow wrote:
>>     
>>> I nominate the following 2 pieces from Bill's reply for fortunes
>>> (probably 2 separate fortunes):
>>>  
>>>
>>>
>>>       
>>>> All this becomes even more glaring if you take the unusal 
>>>> step of plotting the data.
>>>>         
>>> and
>>>
>>>       
>>>> What sort of editor would overlook this clear and 
>>>> demonstrable message leaping out from the data in favour of 
>>>> some arcane argument about "types of sums of squares"?  
>>>> Several answers come to mind: A power freak, a SAS 
>>>> afficianado, an idiot.
>>>>         
More seriously on this topic is the need to educate editors. Few editors 
in the biological field (real biology, biomed etc) appear to have, or if 
they have, to exercise, any sort of near current judgment on statistical 
methods, techniques or interpretations. Referees often have no knowledge 
of analysis and editors blindly back their referee ... (my own pet gripe 
here is being asked for replicates when showing the existence of a 
phenomenon ... The assertion 'all swans are black' is refuted by the 
observation of a white swan ... Referee: how many replicates did the 
researcher have? There appears to be a single sample here; where is the 
confidence interval on the proportion of white swans?).

A little learning is a dangerous thing ... (Pope) ... and most 
biological editors may have a compulsory undergraduate subject in their 
distant background, from which they remember 'Yates' correction' and 'no 
cell with fewer than 5 observations' (sic),

Richard Rowe

-- 
Dr Richard Rowe
Zoology & Tropical Ecology
School of Tropical Biology
James Cook University
Townsville 4811
AUSTRALIA

ph +61 7 47 81 4851
fax +61 7 47 25 1570
JCU has CRICOS Provider Code 00117J


From milton_ruser at yahoo.com.br  Thu Jul 12 02:44:28 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Wed, 11 Jul 2007 17:44:28 -0700 (PDT)
Subject: [R] rgdal memory error for a small map.
Message-ID: <489465.78322.qm@web56612.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/4118c6c7/attachment.pl 

From brown_emu at yahoo.com  Thu Jul 12 03:53:28 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 11 Jul 2007 18:53:28 -0700 (PDT)
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
Message-ID: <305702.50052.qm@web39707.mail.mud.yahoo.com>

Not that Trellis/lattice was entirely easy to learn at first. :)

I've been playing around with ggplot2 and there is a plot()-like wrapper for
building a quick plot [incidentally, called qplot()], but otherwise it's my
understanding that you superpose elements (incrementally) to build up to the
graph you want. Here is the same plot in ggplot2:

rectInfo <-
    list(matrix(runif(4), 2, 2),
         matrix(runif(4), 2, 2),
         matrix(runif(4), 2, 2))

library(ggplot2)
ggopt(grid.fill = "white") # just my preference
## original plot of points
p <-
qplot(x,y,data=data.frame(x=runif(30),y=runif(30),f=gl(3,30)),facets=f~.)
# print(p)

## external data (rectangles) -> in coordinates for geom_polygon 
x <- do.call(rbind,
             mapply(function(.r,.f)
                    data.frame(x=.r[c(1,1,2,2),1],y=.r[c(1,2,2,1),2],f=.f),
                    .r=rectInfo,.f=seq(along=rectInfo),SIMPLIFY=FALSE))
## add rectangle to original plot of points
p+layer(geom="polygon",data=x,mapping=aes(x=x,y=y),facets=f~.)
# will print the graphics on my windows() device

Though lattice does seem to emphasize the 'chart type' approach to graphing,
in a way I see that it provides a similar flexibility - just that the
specifications for each element are contained in functions and objects that
are ultimately invoked by a high-level/higher-order function, instead of
being combined in the linear fashion of ggplot2.

ST

--- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:

> On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > A question/comment: I have usually found that the subscripts argument
> is
> > > what I need when passing *external* information into the panel
> function, for
> > > example, when I wish to add results from a fit done external to the
> trellis
> > > call. Fits[subscripts] gives me the fits (or whatever) I want to plot
> for
> > > each panel. It is not clear to me how the panel layout information from
> > > panel.number(), etc. would be helpful here instead. Am I correct? -- or
> is
> > > there a smarter way to do this that I've missed?
> >
> > This is one of things that I think ggplot does better - it's much
> > easier to plot multiple data sources.  I don't have many examples of
> > this yet, but the final example on
> > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> 
> That's probably true. The Trellis approach is to define a plot by
> "data source" + "type of plot", whereas the ggplot approach (if I
> understand correctly) is to create a specification for the display
> (incrementally?) and then render it. Since the specification can be
> very general, the approach is very flexible. The downside is that you
> need to learn the language.
> 
> On a philosophical note, I think the apparent limitations of Trellis
> in some (not all) cases is just due to the artificial importance given
> to data frames as the one true container for data. Now that we have
> proper multiple dispatch in S4, we can write methods that behave like
> traditional Trellis calls but work with more complex data structures.
> We have tried this in one bioconductor package (flowViz) with
> encouraging results.
> 
> -Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



      ____________________________________________________________________________________


From brown_emu at yahoo.com  Thu Jul 12 04:15:59 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 11 Jul 2007 19:15:59 -0700 (PDT)
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
Message-ID: <348148.61456.qm@web39706.mail.mud.yahoo.com>

In the Trellis approach, another way (I like) to deal with multiple pieces of
external data sources is to 'attach' them to panel functions through lexical
closures. For instance...

rectInfo <-
    list(matrix(runif(4), 2, 2),
         matrix(runif(4), 2, 2),
         matrix(runif(4), 2, 2))

panel.qrect <- function(rect.info) {
  function(x, y, ...) {
    ri <- rect.info[[packet.number()]]
    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
               col = "grey86", border = NA)
    panel.xyplot(x, y, ...)
  }
}

xyplot(runif(30) ~ runif(30) | gl(3, 10),
       panel = panel.qrect(rectInfo))

...which may or may not be more convenient than passing rectInfo (and perhaps
other objects if desired) explicitly as an argument to xyplot().


--- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:

> On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > A question/comment: I have usually found that the subscripts argument
> is
> > > what I need when passing *external* information into the panel
> function, for
> > > example, when I wish to add results from a fit done external to the
> trellis
> > > call. Fits[subscripts] gives me the fits (or whatever) I want to plot
> for
> > > each panel. It is not clear to me how the panel layout information from
> > > panel.number(), etc. would be helpful here instead. Am I correct? -- or
> is
> > > there a smarter way to do this that I've missed?
> >
> > This is one of things that I think ggplot does better - it's much
> > easier to plot multiple data sources.  I don't have many examples of
> > this yet, but the final example on
> > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> 
> That's probably true. The Trellis approach is to define a plot by
> "data source" + "type of plot", whereas the ggplot approach (if I
> understand correctly) is to create a specification for the display
> (incrementally?) and then render it. Since the specification can be
> very general, the approach is very flexible. The downside is that you
> need to learn the language.
> 
> On a philosophical note, I think the apparent limitations of Trellis
> in some (not all) cases is just due to the artificial importance given
> to data frames as the one true container for data. Now that we have
> proper multiple dispatch in S4, we can write methods that behave like
> traditional Trellis calls but work with more complex data structures.
> We have tried this in one bioconductor package (flowViz) with
> encouraging results.
> 
> -Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From deepayan.sarkar at gmail.com  Thu Jul 12 04:37:06 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 11 Jul 2007 19:37:06 -0700
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <348148.61456.qm@web39706.mail.mud.yahoo.com>
References: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
	<348148.61456.qm@web39706.mail.mud.yahoo.com>
Message-ID: <eb555e660707111937o7fa825f4ke3a000e3750138b1@mail.gmail.com>

On 7/11/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> In the Trellis approach, another way (I like) to deal with multiple pieces of
> external data sources is to 'attach' them to panel functions through lexical
> closures. For instance...
>
> rectInfo <-
>     list(matrix(runif(4), 2, 2),
>          matrix(runif(4), 2, 2),
>          matrix(runif(4), 2, 2))
>
> panel.qrect <- function(rect.info) {
>   function(x, y, ...) {
>     ri <- rect.info[[packet.number()]]
>     panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
>                col = "grey86", border = NA)
>     panel.xyplot(x, y, ...)
>   }
> }
>
> xyplot(runif(30) ~ runif(30) | gl(3, 10),
>        panel = panel.qrect(rectInfo))
>
> ...which may or may not be more convenient than passing rectInfo (and perhaps
> other objects if desired) explicitly as an argument to xyplot().

That's an interesting approach. I think the important thing is to make
sure that the data required to reproduce the plot is available as part
of the trellis object (e.g. if you save it and load it in another
session (not that anyone actually ever does that, but it's the
principle of the thing)). This happens transparently if you supply the
external data as arguments to xyplot(). It happens in your example
too, but the data is hidden inside the environment of the panel
function. The two approaches might differ in terms of memory use, but
I'm not sure.

-Deepayan


From ggrothendieck at gmail.com  Thu Jul 12 05:21:56 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 11 Jul 2007 23:21:56 -0400
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <348148.61456.qm@web39706.mail.mud.yahoo.com>
References: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
	<348148.61456.qm@web39706.mail.mud.yahoo.com>
Message-ID: <971536df0707112021wa3d47e7wad8008e9bb2e67d4@mail.gmail.com>

Your approach of using closures is cleaner than that
given below but just for comparison in:

http://tolstoy.newcastle.edu.au/R/devel/06/03/4476.html

there is a createWrapper function which creates a new function based
on the function passed as its first argument by using the components
of the list passed as its second argument to overwrite its formal
arguments.  For example,

createWrapper <- function(FUN, Params) {
   as.function(c(replace(formals(FUN), names(Params), Params), body(FUN)))
}

library(lattice)

rectInfo <-
   list(matrix(runif(4), 2, 2),
        matrix(runif(4), 2, 2),
        matrix(runif(4), 2, 2))


panel.qrect <- function(x, y, ..., rect.info) {
   ri <- rect.info[[packet.number()]]
   panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
              col = "grey86", border = NA)
   panel.xyplot(x, y, ...)
}

xyplot(runif(30) ~ runif(30) | gl(3, 10),
      panel = createWrapper(panel.qrect, list(rect.info = rectInfo)))

The createWrapper approach does have an advantage in the situation
where the function analogous to panel.qrect is existing since using
scoping then involves manipulation of environments in the closure
approach.

On 7/11/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> In the Trellis approach, another way (I like) to deal with multiple pieces of
> external data sources is to 'attach' them to panel functions through lexical
> closures. For instance...
>
> rectInfo <-
>    list(matrix(runif(4), 2, 2),
>         matrix(runif(4), 2, 2),
>         matrix(runif(4), 2, 2))
>
> panel.qrect <- function(rect.info) {
>  function(x, y, ...) {
>    ri <- rect.info[[packet.number()]]
>    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
>               col = "grey86", border = NA)
>    panel.xyplot(x, y, ...)
>  }
> }
>
> xyplot(runif(30) ~ runif(30) | gl(3, 10),
>       panel = panel.qrect(rectInfo))
>
> ...which may or may not be more convenient than passing rectInfo (and perhaps
> other objects if desired) explicitly as an argument to xyplot().
>
>
> --- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
>
> > On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > > A question/comment: I have usually found that the subscripts argument
> > is
> > > > what I need when passing *external* information into the panel
> > function, for
> > > > example, when I wish to add results from a fit done external to the
> > trellis
> > > > call. Fits[subscripts] gives me the fits (or whatever) I want to plot
> > for
> > > > each panel. It is not clear to me how the panel layout information from
> > > > panel.number(), etc. would be helpful here instead. Am I correct? -- or
> > is
> > > > there a smarter way to do this that I've missed?
> > >
> > > This is one of things that I think ggplot does better - it's much
> > > easier to plot multiple data sources.  I don't have many examples of
> > > this yet, but the final example on
> > > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> >
> > That's probably true. The Trellis approach is to define a plot by
> > "data source" + "type of plot", whereas the ggplot approach (if I
> > understand correctly) is to create a specification for the display
> > (incrementally?) and then render it. Since the specification can be
> > very general, the approach is very flexible. The downside is that you
> > need to learn the language.
> >
> > On a philosophical note, I think the apparent limitations of Trellis
> > in some (not all) cases is just due to the artificial importance given
> > to data frames as the one true container for data. Now that we have
> > proper multiple dispatch in S4, we can write methods that behave like
> > traditional Trellis calls but work with more complex data structures.
> > We have tried this in one bioconductor package (flowViz) with
> > encouraging results.
> >
> > -Deepayan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Thu Jul 12 05:41:42 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 11 Jul 2007 20:41:42 -0700 (PDT)
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <305702.50052.qm@web39707.mail.mud.yahoo.com>
Message-ID: <415537.88654.qm@web39704.mail.mud.yahoo.com>

Regarding this, I meant to imply that lattice was similarly flexible in the
sense of handing multiple data sets [IMHO], in regards to other aspects of
the 'grammar of graphics' I have no qualifications to justify comment. But
the idea and intuitiveness of graph construction in ggplot2 is very appealing
- in an hour I picked up enough to do quite a bit, just by going through
examples in the author's book <http://had.co.nz/ggplot2/>. Will be
interesting to see how this package will be received by the community.

Stephen

--- Stephen Tucker <brown_emu at yahoo.com> wrote:

> Not that Trellis/lattice was entirely easy to learn at first. :)
> 
> I've been playing around with ggplot2 and there is a plot()-like wrapper
> for
> building a quick plot [incidentally, called qplot()], but otherwise it's my
> understanding that you superpose elements (incrementally) to build up to
> the
> graph you want. Here is the same plot in ggplot2:
> 
> rectInfo <-
>     list(matrix(runif(4), 2, 2),
>          matrix(runif(4), 2, 2),
>          matrix(runif(4), 2, 2))
> 
> library(ggplot2)
> ggopt(grid.fill = "white") # just my preference
> ## original plot of points
> p <-
> qplot(x,y,data=data.frame(x=runif(30),y=runif(30),f=gl(3,30)),facets=f~.)
> # print(p)
> 
> ## external data (rectangles) -> in coordinates for geom_polygon 
> x <- do.call(rbind,
>              mapply(function(.r,.f)
>                     data.frame(x=.r[c(1,1,2,2),1],y=.r[c(1,2,2,1),2],f=.f),
>                     .r=rectInfo,.f=seq(along=rectInfo),SIMPLIFY=FALSE))
> ## add rectangle to original plot of points
> p+layer(geom="polygon",data=x,mapping=aes(x=x,y=y),facets=f~.)
> # will print the graphics on my windows() device
> 
> Though lattice does seem to emphasize the 'chart type' approach to
> graphing,
> in a way I see that it provides a similar flexibility - just that the
> specifications for each element are contained in functions and objects that
> are ultimately invoked by a high-level/higher-order function, instead of
> being combined in the linear fashion of ggplot2.
> 
> ST
> 
> --- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> 
> > On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > > A question/comment: I have usually found that the subscripts argument
> > is
> > > > what I need when passing *external* information into the panel
> > function, for
> > > > example, when I wish to add results from a fit done external to the
> > trellis
> > > > call. Fits[subscripts] gives me the fits (or whatever) I want to plot
> > for
> > > > each panel. It is not clear to me how the panel layout information
> from
> > > > panel.number(), etc. would be helpful here instead. Am I correct? --
> or
> > is
> > > > there a smarter way to do this that I've missed?
> > >
> > > This is one of things that I think ggplot does better - it's much
> > > easier to plot multiple data sources.  I don't have many examples of
> > > this yet, but the final example on
> > > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> > 
> > That's probably true. The Trellis approach is to define a plot by
> > "data source" + "type of plot", whereas the ggplot approach (if I
> > understand correctly) is to create a specification for the display
> > (incrementally?) and then render it. Since the specification can be
> > very general, the approach is very flexible. The downside is that you
> > need to learn the language.
> > 
> > On a philosophical note, I think the apparent limitations of Trellis
> > in some (not all) cases is just due to the artificial importance given
> > to data frames as the one true container for data. Now that we have
> > proper multiple dispatch in S4, we can write methods that behave like
> > traditional Trellis calls but work with more complex data structures.
> > We have tried this in one bioconductor package (flowViz) with
> > encouraging results.
> > 
> > -Deepayan
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> 
> 
>      
>
____________________________________________________________________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



       
____________________________________________________________________________________
Pinpoint customers who are looking for what you sell.


From h.wickham at gmail.com  Thu Jul 12 06:32:44 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 06:32:44 +0200
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
References: <eb555e660707111004l7d41b32cx4a58b96c35912bf9@mail.gmail.com>
	<003d01c7c3e1$3d1babb0$4d908980@gne.windows.gene.com>
	<f8e6ff050707111108i2a1e5587gee1617dae21b20b@mail.gmail.com>
	<eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
Message-ID: <f8e6ff050707112132k19f07f48p2335b27f8909ff1@mail.gmail.com>

On 7/12/07, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > A question/comment: I have usually found that the subscripts argument is
> > > what I need when passing *external* information into the panel function, for
> > > example, when I wish to add results from a fit done external to the trellis
> > > call. Fits[subscripts] gives me the fits (or whatever) I want to plot for
> > > each panel. It is not clear to me how the panel layout information from
> > > panel.number(), etc. would be helpful here instead. Am I correct? -- or is
> > > there a smarter way to do this that I've missed?
> >
> > This is one of things that I think ggplot does better - it's much
> > easier to plot multiple data sources.  I don't have many examples of
> > this yet, but the final example on
> > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
>
> That's probably true. The Trellis approach is to define a plot by
> "data source" + "type of plot", whereas the ggplot approach (if I
> understand correctly) is to create a specification for the display
> (incrementally?) and then render it. Since the specification can be
> very general, the approach is very flexible. The downside is that you
> need to learn the language.

Yes, that's right.  ggplot basically decomposes "type of plot" into
statistical transformation (stat) + geometric object and allows you to
control each component separately.  ggplot also explicitly includes
the idea of layers (ie. one layer is a scatterplot and another layer
is a loess smooth) and allows you to supply different datasets to
different layers.

> On a philosophical note, I think the apparent limitations of Trellis
> in some (not all) cases is just due to the artificial importance given
> to data frames as the one true container for data. Now that we have
> proper multiple dispatch in S4, we can write methods that behave like
> traditional Trellis calls but work with more complex data structures.
> We have tried this in one bioconductor package (flowViz) with
> encouraging results.

That's one area which I haven't thought much about.  ggplot is very
data.frame centric and it's not yet clear to me how plotting a linear
model (say) would fit into the grammar.

Hadley


From h.wickham at gmail.com  Thu Jul 12 06:39:14 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 06:39:14 +0200
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <305702.50052.qm@web39707.mail.mud.yahoo.com>
References: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
	<305702.50052.qm@web39707.mail.mud.yahoo.com>
Message-ID: <f8e6ff050707112139g7c3633devd39da6da418be850@mail.gmail.com>

On 7/12/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> Not that Trellis/lattice was entirely easy to learn at first. :)
>
> I've been playing around with ggplot2 and there is a plot()-like wrapper for
> building a quick plot [incidentally, called qplot()], but otherwise it's my
> understanding that you superpose elements (incrementally) to build up to the
> graph you want. Here is the same plot in ggplot2:
>
> rectInfo <-
>     list(matrix(runif(4), 2, 2),
>          matrix(runif(4), 2, 2),
>          matrix(runif(4), 2, 2))
>
> library(ggplot2)
> ggopt(grid.fill = "white") # just my preference
> ## original plot of points
> p <-
> qplot(x,y,data=data.frame(x=runif(30),y=runif(30),f=gl(3,30)),facets=f~.)
> # print(p)
>
> ## external data (rectangles) -> in coordinates for geom_polygon
> x <- do.call(rbind,
>              mapply(function(.r,.f)
>                     data.frame(x=.r[c(1,1,2,2),1],y=.r[c(1,2,2,1),2],f=.f),
>                     .r=rectInfo,.f=seq(along=rectInfo),SIMPLIFY=FALSE))
> ## add rectangle to original plot of points
> p+layer(geom="polygon",data=x,mapping=aes(x=x,y=y),facets=f~.)
> # will print the graphics on my windows() device

You should be able to simplify this line to:
p+geom_polygon(data=x)
because all the other information is already contained in the plot object.

> Though lattice does seem to emphasize the 'chart type' approach to graphing,
> in a way I see that it provides a similar flexibility - just that the
> specifications for each element are contained in functions and objects that
> are ultimately invoked by a high-level/higher-order function, instead of
> being combined in the linear fashion of ggplot2.

I tend to think in very data centric approach, where you first
generate the data (in a data frame) and then you plot it.  There is
very little data creation/modification during the plotting itself - I
think this is different to lattice, where you often do more data
manipulation in the panel function itself.  I don't think one is
better or worse, just different.

Hadley


From h.wickham at gmail.com  Thu Jul 12 06:41:45 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 06:41:45 +0200
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <348148.61456.qm@web39706.mail.mud.yahoo.com>
References: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
	<348148.61456.qm@web39706.mail.mud.yahoo.com>
Message-ID: <f8e6ff050707112141m2a565e38s13c7d4c3373c56b5@mail.gmail.com>

On 7/12/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> In the Trellis approach, another way (I like) to deal with multiple pieces of
> external data sources is to 'attach' them to panel functions through lexical
> closures. For instance...
>
> rectInfo <-
>     list(matrix(runif(4), 2, 2),
>          matrix(runif(4), 2, 2),
>          matrix(runif(4), 2, 2))
>
> panel.qrect <- function(rect.info) {
>   function(x, y, ...) {
>     ri <- rect.info[[packet.number()]]
>     panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
>                col = "grey86", border = NA)
>     panel.xyplot(x, y, ...)
>   }
> }
>
> xyplot(runif(30) ~ runif(30) | gl(3, 10),
>        panel = panel.qrect(rectInfo))
>
> ...which may or may not be more convenient than passing rectInfo (and perhaps
> other objects if desired) explicitly as an argument to xyplot().

This is an interesting approach.  The one problem I see with it is
that if you change the trellising specification, you have to change
your rectInfo datastructure.  I guess we're missing the code that
actually generates rectInfo in the first place, so maybe in practice
it's not such a big problem.

Hadley


From cressonim at nhlbi.nih.gov  Thu Jul 12 06:44:44 2007
From: cressonim at nhlbi.nih.gov (Cressoni, Massimo (NIH/NHLBI) [F])
Date: Thu, 12 Jul 2007 00:44:44 -0400
Subject: [R] Subsetting problem
Message-ID: <B0F504209244B14EA9A4C1DFB599B92201852225@NIHCESMLBX6.nih.gov>

I need to perform the Exact Wilcoxon Mann-Whitney on a subset of my database.
Assuming that IPPO is my data frame and IPPOBIS is the subset my variable still
have 3 different levels and the function wilcox_test (package "coin")
does not accept it. 
I do not know how to overcome this problem.

ippo <- c(rep("A",10),rep("B",10),rep("C",10))
ippo2 <- c(rnorm(10,0,1),rnorm(10,10,10),rnorm(10,10,10))
IPPO <- data.frame(ippo,ippo2)

IPPOBIS <- IPPO[IPPO$ippo == "A" | IPPO$ippo == "B",]

wilcox_test(ippo2 ~ ippo,data=IPPOBIS,distribution=exact())
Error in check(itp) : 'object' does not represent a two sample problem
levels(IPPOBIS$ippo)
[1] "A" "B" "C"

Massimo Cressoni


From ripley at stats.ox.ac.uk  Thu Jul 12 07:00:20 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jul 2007 06:00:20 +0100 (BST)
Subject: [R] make error R-5.1 on  sun solaris
In-Reply-To: <469554CF.7020402@biostat.ku.dk>
References: <000001c7c401$d31a0e20$775d7481@austin.utexas.edu>
	<469554CF.7020402@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0707120548380.15630@gannet.stats.ox.ac.uk>

You are asked for a C99 compiler and configure normally finds one: mixing 
declarations and code is valid C99.

Unless something has been done with environment variables (e.g. in 
config.site) this gcc is very old.  configure should come up with 
'gcc -std=gnu99'.  Re-ordering the code will help (but it may need to go 
below the next #ifdef block too), but a gcc update would be a very good 
idea as gcc on Sparc has been buggy up to about 3.4.x.

Note too that --without-iconv is undesirable and should not be necessary 
as libiconv can be installed as a preload on Solaris (8 and 10, so 
presumably also 9).


On Thu, 12 Jul 2007, Peter Dalgaard wrote:

> Dan Powers wrote:
>> I hope this is enough information to determine the problem. Thanks in
>> advance for any help.
>>
>> Configure goes ok (I think)
>>
>> ./configure --prefix=$HOME --without-iconv
>>
>>
>> R is now configured for sparc-sun-solaris2.9
>>
>>   Source directory:          .
>>   Installation directory:    /home/dpowers
>>
>>   C compiler:                gcc  -g -O2
>>   Fortran 77 compiler:       f95  -g
>>
>>   C++ compiler:              g++  -g -O2
>>   Fortran 90/95 compiler:    f95 -g
>>   Obj-C compiler:             -g -O2
>>
>>   Interfaces supported:      X11
>>   External libraries:        readline
>>   Additional capabilities:   NLS
>>   Options enabled:           shared BLAS, R profiling, Java
>>
>>   Recommended packages:      yes
>>
>> Make ends after the gcc..
>>
>> make
>> .
>> .
>> .
>>
>> gcc -I. -I../../src/include -I../../src/include -I/usr/openwin/include
>> -I/usr/local/include -DHAVE_CONFIG_H   -g -O2 -c system.c -o system.o
>> system.c: In function `Rf_initialize_R':
>> system.c:144: parse error before `char'
>> system.c:216: `localedir' undeclared (first use in this function)
>> system.c:216: (Each undeclared identifier is reported only once
>> system.c:216: for each function it appears in.)
>> *** Error code 1
>> make: Fatal error: Command failed for target `system.o'
>> Current working directory /home/dpowers/R-2.5.1/src/unix
>> *** Error code 1
>> make: Fatal error: Command failed for target `R'
>> Current working directory /home/dpowers/R-2.5.1/src/unix
>> *** Error code 1
>> make: Fatal error: Command failed for target `R'
>> Current working directory /home/dpowers/R-2.5.1/src
>> *** Error code 1
>> make: Fatal error: Command failed for target `R'
>>
>>
>> I have tried setting localedir directly in configure options, but get the
>> same error.
>>
>> Any ideas?
>>
>>
> Hmm, which version of gcc is this? The problem seems to be around line
> 144 which reads
>
> 140     Rstart Rp = &rstart;
> 141     cmdlines[0] = '\0';
> 142
> 143 #ifdef ENABLE_NLS
> 144     char localedir[PATH_MAX+20];
> 145 #endif
> 146
> 147 #if defined(HAVE_SYS_RESOURCE_H) && defined(HAVE_GETRLIMIT)
> 148 {
> 149     struct rlimit rlim;
>
>
> I seem to remember that it used to be non-kosher to mix declarations
> and ordinary code like that, but the current compiler doesn't seem to
> care (I do have #define ENABLE_NLS 1 in Rconfig.h, as I assume you do
> too). Could you perhaps try moving line 141 down below #endif?
>
>
>
>> Thanks,
>> Dan
>> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
>> Daniel A. Powers, Ph.D.
>> Department of Sociology
>> University of Texas at Austin
>> 1 University Station A1700
>> Austin, TX  78712-0118
>> phone: 512-232-6335
>> fax:   512-471-1748
>> dpowers at mail.la.utexas.edu
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pete-expires-20070910 at kazmier.com  Thu Jul 12 07:01:21 2007
From: pete-expires-20070910 at kazmier.com (Pete Kazmier)
Date: Thu, 12 Jul 2007 01:01:21 -0400
Subject: [R] ggplot2 / reshape / Question on manipulating data
Message-ID: <87ir8qz04e.fsf@coco.kazmier.com>

I'm an R newbie but recently discovered the ggplot2 and reshape
packages which seem incredibly useful and much easier to use for a
beginner.  Using the data from the IMDB, I'm trying to see how the
average movie rating varies by year.  Here is what my data looks like:

> ratings <- read.delim("groomed.list", header = TRUE, sep = "|", comment.char = "")
> ratings <- subset(ratings, VoteCount > 100)
> head(ratings)
                             Title  Histogram VoteCount VoteMean Year
1                !Huff (2004) (TV) 0000000016       299      8.4 2004
8              'Allo 'Allo! (1982) 0000000125       829      8.6 1982
50              .hack//SIGN (2002) 0000001113       150      7.0 2002
56            1-800-Missing (2003) 0000000103       118      5.4 2003
66  Greatest Artists (2000) (mini) 00..000016       110      7.8 2000
77 00 Scariest Movie (2004) (mini) 00..000115       256      8.6 2004

The above data is not aggregated.  So after playing around with basic
R functionality, I stumbled across the 'aggregate' function and was
able to see the information in the manner I desired (average movie
rating by year).

> byYear <- aggregate(ratings$VoteMean, list(Year = ratings$Year), mean)
> plot(byYear)

Having just discovered gglot2, I wanted to create the same graph but
augment it with a color attribute based on the total number of votes
in a year.  So first I tried to see if I could reproduce the above:

> library(ggplot2)
> qplot(Year, x, byYear)

This did not work as expected because the x-axis contained labels for
each and every year making it impossible to read whereas the plot
created with basic R had nice x-axis labels.  How do I get 'qplot' to
treat the x-axis in a similar manner to 'plot'?

After playing around further, I was able to get 'qplot' to work in a
manner similar to 'plot' with regards to the x-axis labels by using
'melt' and 'cast'.  The 'qplot' now behaves correctly:

> mratings <- melt(ratings, id = c("Title", "Year"), measure = c("VoteCount", "VoteMean"))
> byYear2 <- cast(mratings, Year ~ variable, mean, subset = variable == "VoteMean")
> qplot(Year, VoteMean, data = byYear2)

How do 'byYear' and 'byYear2' differ?  I am trying to use 'typeof' but
both seem to be lists.  However, they are clearly different in some
way because 'qplot' graphs them differently.

Finally, I'd like to use a color attribute to 'qplot' to augment each
point with a color based on the total number of votes for the year.
Using attributes with 'qplot' seems simple, but I'm having a hard time
grooming my data appropriately.  I believe this requires aggregation
by summing the VoteCount column.  Is there a way to cast the data
using different aggregation functions for various columns?  In my
case, I want the mean of the VoteMean column, and the sum of the
VoteCount column.  Then I want to produce a graph showing the average
movie rating per year but with each point colored to reflect the total
number of votes for that year.  Any pointers?

Thanks,
Pete


From ripley at stats.ox.ac.uk  Thu Jul 12 07:07:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jul 2007 06:07:07 +0100 (BST)
Subject: [R] Subsetting problem
In-Reply-To: <B0F504209244B14EA9A4C1DFB599B92201852225@NIHCESMLBX6.nih.gov>
References: <B0F504209244B14EA9A4C1DFB599B92201852225@NIHCESMLBX6.nih.gov>
Message-ID: <Pine.LNX.4.64.0707120603540.15630@gannet.stats.ox.ac.uk>

You have three levels of factor 'ippo' and data on two.  That is not a 
two-sample problem, as the error message says. Try

IPPOBIS$ippo <- IPPOBIS$ippo[drop=TRUE]

And please use an informative subject line (see the posting guide).

On Thu, 12 Jul 2007, Cressoni, Massimo (NIH/NHLBI) [F] wrote:

> I need to perform the Exact Wilcoxon Mann-Whitney on a subset of my database.
> Assuming that IPPO is my data frame and IPPOBIS is the subset my variable still
> have 3 different levels and the function wilcox_test (package "coin")
> does not accept it.
> I do not know how to overcome this problem.
>
> ippo <- c(rep("A",10),rep("B",10),rep("C",10))
> ippo2 <- c(rnorm(10,0,1),rnorm(10,10,10),rnorm(10,10,10))
> IPPO <- data.frame(ippo,ippo2)
>
> IPPOBIS <- IPPO[IPPO$ippo == "A" | IPPO$ippo == "B",]
>
> wilcox_test(ippo2 ~ ippo,data=IPPOBIS,distribution=exact())
> Error in check(itp) : 'object' does not represent a two sample problem
> levels(IPPOBIS$ippo)
> [1] "A" "B" "C"
>
> Massimo Cressoni
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From h.wickham at gmail.com  Thu Jul 12 09:35:27 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 09:35:27 +0200
Subject: [R] ggplot2 / reshape / Question on manipulating data
In-Reply-To: <87ir8qz04e.fsf@coco.kazmier.com>
References: <87ir8qz04e.fsf@coco.kazmier.com>
Message-ID: <f8e6ff050707120035r5668e93bqf963acc3540c4890@mail.gmail.com>

On 7/12/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
> I'm an R newbie but recently discovered the ggplot2 and reshape
> packages which seem incredibly useful and much easier to use for a
> beginner.  Using the data from the IMDB, I'm trying to see how the
> average movie rating varies by year.  Here is what my data looks like:
>
> > ratings <- read.delim("groomed.list", header = TRUE, sep = "|", comment.char = "")
> > ratings <- subset(ratings, VoteCount > 100)
> > head(ratings)
>                              Title  Histogram VoteCount VoteMean Year
> 1                !Huff (2004) (TV) 0000000016       299      8.4 2004
> 8              'Allo 'Allo! (1982) 0000000125       829      8.6 1982
> 50              .hack//SIGN (2002) 0000001113       150      7.0 2002
> 56            1-800-Missing (2003) 0000000103       118      5.4 2003
> 66  Greatest Artists (2000) (mini) 00..000016       110      7.8 2000
> 77 00 Scariest Movie (2004) (mini) 00..000115       256      8.6 2004

Have you tried using the movies dataset included in ggplot?  Or is
there some data that you want that is not in that dataset.

> The above data is not aggregated.  So after playing around with basic
> R functionality, I stumbled across the 'aggregate' function and was
> able to see the information in the manner I desired (average movie
> rating by year).
>
> > byYear <- aggregate(ratings$VoteMean, list(Year = ratings$Year), mean)
> > plot(byYear)
>
> Having just discovered gglot2, I wanted to create the same graph but
> augment it with a color attribute based on the total number of votes
> in a year.  So first I tried to see if I could reproduce the above:
>
> > library(ggplot2)
> > qplot(Year, x, byYear)
>
> This did not work as expected because the x-axis contained labels for
> each and every year making it impossible to read whereas the plot
> created with basic R had nice x-axis labels.  How do I get 'qplot' to
> treat the x-axis in a similar manner to 'plot'?

The problem is probably that Year is a factor - and factors are
labelled on every level (even if they overlap - which is a bug).
There's no terribly easy way to fix this, but the following will work:

qplot(as.numeric(as.character(Year)), x, data=byYear)

> After playing around further, I was able to get 'qplot' to work in a
> manner similar to 'plot' with regards to the x-axis labels by using
> 'melt' and 'cast'.  The 'qplot' now behaves correctly:
>
> > mratings <- melt(ratings, id = c("Title", "Year"), measure = c("VoteCount", "VoteMean"))
> > byYear2 <- cast(mratings, Year ~ variable, mean, subset = variable == "VoteMean")
> > qplot(Year, VoteMean, data = byYear2)
>
> How do 'byYear' and 'byYear2' differ?  I am trying to use 'typeof' but
> both seem to be lists.  However, they are clearly different in some
> way because 'qplot' graphs them differently.

Try using str - it's much more helpful, and you should see the
different quickly.

> Finally, I'd like to use a color attribute to 'qplot' to augment each
> point with a color based on the total number of votes for the year.
> Using attributes with 'qplot' seems simple, but I'm having a hard time
> grooming my data appropriately.  I believe this requires aggregation
> by summing the VoteCount column.  Is there a way to cast the data
> using different aggregation functions for various columns?  In my

Not easily, unfortunately.  However, you could do:

cast(mratings, Year ~ variable, c(mean, sum)), subset = variable %in%
c("VoteMean", "VoteCount"))

which will give you a mean and sum for both.

> case, I want the mean of the VoteMean column, and the sum of the
> VoteCount column.  Then I want to produce a graph showing the average
> movie rating per year but with each point colored to reflect the total
> number of votes for that year.  Any pointers?

Using the built in movies data:

mm <- melt(movies, id=1:2, m=c("rating", "votes"))
msum <- cast(mm, year ~ variable, c(mean, sum))

qplot(year, rating_mean, data=msum, colour=votes_sum)
qplot(year, rating_mean, data=msum, colour=votes_sum, geom="line")

Hadley


From steve at promente.org  Thu Jul 12 09:38:59 2007
From: steve at promente.org (Steve Powell)
Date: Thu, 12 Jul 2007 09:38:59 +0200
Subject: [R] ggplot doesnt work in loops?
Message-ID: <003d01c7c457$b7681900$6501a8c0@STEVE>

Dear list members
I am still a newbie so might be asking a stupid question, but I can't get
ggplot to work in a loop (or a "while" statement for that matter).
 
# to take a minimal example - 
mydata$varc = c(1,2,3)
for (i in 1:1){
        jpeg("test3.jpg")
        plot(mydata$varc)
        #ggplot(mydata, aes(x=mydata$varc)) + geom_bar()
        dev.off()
}

this produces an empty jpeg, whereas the content of the loop produces the
jpeg correctly.
a standard plot() does work inside the loop.
Any ideas? This is with R 2.4.0 and ggplot2
thanks in advance

Steve Powell

proMENTE social research


From h.wickham at gmail.com  Thu Jul 12 09:55:06 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 09:55:06 +0200
Subject: [R] ggplot doesnt work in loops?
In-Reply-To: <003d01c7c457$b7681900$6501a8c0@STEVE>
References: <003d01c7c457$b7681900$6501a8c0@STEVE>
Message-ID: <f8e6ff050707120055r5cba2683ic2de6e4a73e0ada0@mail.gmail.com>

Hi Steve,

You need to explicitly print the ggplot object:
ggplot(mydata, aes(x=mydata$varc)) + geom_bar()

(this is a R-faq for lattice plots, and ggplot works the same way)

In the latest version of ggplot (0.5.4) you can construct the plot
before hand and modify the aesthetics in each instance of the loop:

p <- ggplot(mydata) + geom_bar()
mydata$varc = c(1,2,3)
for (i in 1:1){
        jpeg("test3.jpg")
        p + aes(x = mydata$varc)
        dev.off()
}

(not that this will actually work because you're not using i inside
your loop anywhere)

(and to be strictly correct you should probably use list(x =
as.name(names(mydata)[i]))  instead of the aes call - but I haven't
written any documentation for this yet)

Hadley

On 7/12/07, Steve Powell <steve at promente.org> wrote:
> Dear list members
> I am still a newbie so might be asking a stupid question, but I can't get
> ggplot to work in a loop (or a "while" statement for that matter).
>
> # to take a minimal example -
> mydata$varc = c(1,2,3)
> for (i in 1:1){
>         jpeg("test3.jpg")
>         plot(mydata$varc)
>         #ggplot(mydata, aes(x=mydata$varc)) + geom_bar()
>         dev.off()
> }
>
> this produces an empty jpeg, whereas the content of the loop produces the
> jpeg correctly.
> a standard plot() does work inside the loop.
> Any ideas? This is with R 2.4.0 and ggplot2
> thanks in advance
>
> Steve Powell
>
> proMENTE social research
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mazatlanmexico at yahoo.com  Thu Jul 12 01:08:08 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Wed, 11 Jul 2007 16:08:08 -0700 (PDT)
Subject: [R] How to get weekly Covariance with R
Message-ID: <651837.40861.qm@web56602.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070711/97272a03/attachment.pl 

From benoitchemineau at gmail.com  Thu Jul 12 10:51:59 2007
From: benoitchemineau at gmail.com (Benoit Chemineau)
Date: Thu, 12 Jul 2007 10:51:59 +0200
Subject: [R] how to get the p-values from an lm function ?
Message-ID: <50c8fbc90707120151y21edcf5fo864bf8f8d2af009a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070712/5aca475d/attachment.pl 

From h.wickham at gmail.com  Thu Jul 12 10:57:37 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 10:57:37 +0200
Subject: [R] how to get the p-values from an lm function ?
In-Reply-To: <50c8fbc90707120151y21edcf5fo864bf8f8d2af009a@mail.gmail.com>
References: <50c8fbc90707120151y21edcf5fo864bf8f8d2af009a@mail.gmail.com>
Message-ID: <f8e6ff050707120157h2bd699e7p527b9afa3ec890d6@mail.gmail.com>

On 7/12/07, Benoit Chemineau <benoitchemineau at gmail.com> wrote:
> Hi, dear R-users,
>
> I am computing a liner regression by rating category using the 'by' function
> as stated below:
>
> tmp <- by(projet, rating, function(x) lm(defaults ~ CGDP+CSAVE+SP500, data =
> x))
>
> I would like to get not only the coefficients but also their p-values. I
> can't find the command in the help pages to get them.
>
> Does anyone have a suggestion ?

Hi Benoit,

A general approach to find p-values:

m <- lm(wt ~ mpg, data=mtcars)

First figure out how to display them on screen:
m # nope
coef(m) # nope
summary(m) # got it

# Then use str to look at the components
str(summary(m))

# And pick out the one want
summary(m)$coef
coef(summary(m)) # slighty better style, but won't work in general

# In general, you may also need to try
str(print(summary(m)))
# as sometimes the print method calculates the data you're looking for

Hadley


From christophe at pallier.org  Thu Jul 12 11:05:26 2007
From: christophe at pallier.org (Christophe Pallier)
Date: Thu, 12 Jul 2007 11:05:26 +0200
Subject: [R] aov() question
In-Reply-To: <95470DF9-A25F-4626-8584-7D3A4CE74F6C@Colorado.EDU>
References: <95470DF9-A25F-4626-8584-7D3A4CE74F6C@Colorado.EDU>
Message-ID: <dea6cb960707120205l1d334e76n83d382c081b86c33@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070712/e08c7ff5/attachment.pl 

From dimitris.rizopoulos at med.kuleuven.be  Thu Jul 12 11:17:45 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 12 Jul 2007 11:17:45 +0200
Subject: [R] how to get the p-values from an lm function ?
References: <50c8fbc90707120151y21edcf5fo864bf8f8d2af009a@mail.gmail.com>
Message-ID: <017201c7c465$81da3210$0540210a@www.domain>

try the following:

tmp <- by(projet, rating, function (x) Thursday, 12.July.2007{
    fit <- lm(defaults ~ CGDP + CSAVE + SP500, data = x)
    summary(fit)$coefficients
})


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Benoit Chemineau" <benoitchemineau at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, July 12, 2007 10:51 AM
Subject: [R] how to get the p-values from an lm function ?


> Hi, dear R-users,
>
> I am computing a liner regression by rating category using the 'by' 
> function
> as stated below:
>
> tmp <- by(projet, rating, function(x) lm(defaults ~ 
> CGDP+CSAVE+SP500, data =
> x))
>
> I would like to get not only the coefficients but also their 
> p-values. I
> can't find the command in the help pages to get them.
>
> Does anyone have a suggestion ?
>
> Thank you,
>
> Benoit.
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From carlosguerra at esa.ipvc.pt  Thu Jul 12 11:33:13 2007
From: carlosguerra at esa.ipvc.pt (Carlos Guerra)
Date: Thu, 12 Jul 2007 10:33:13 +0100
Subject: [R] problems with memory in Mac
Message-ID: <4695F559.1050106@esa.ipvc.pt>

Dear friends,

I am having some doubts about the amount of memory that is being used by 
R in my Mac (MacBook Pro, 2Gig). Is there a way to increase the amount 
of memory used?

When I type:

 > mem.limits()

the result is:

nsize vsize
   NA    NA

and I can't change it, tough my computing in R isn't using all the 
memory at it's disposal.

Best regards,
Carlos

-- 
Carlos GUERRA

Gabinete de Sistemas de Informacao Geografica
Escola Superior Agraria de Ponte de Lima
Mosteiro de Refoios do Lima
4990-706 Ponte de Lima

Tlm: +351 91 2407109
Tlf: +351 258 909779

Reclaim your Inbox...!!!
http://www.mozilla.org/products/thunderbird/


From S.Ellison at lgc.co.uk  Thu Jul 12 12:07:38 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Thu, 12 Jul 2007 11:07:38 +0100
Subject: [R] type III ANOVA for a nested linear model
Message-ID: <s6960b94.037@tedmail2.lgc.co.uk>

The aliasing problem arises in alias(), which is what Anova uses to detect aliasing. It is simply the fact that anova more or less blithely ignores the NA's that makes anova behave apparently more 'sensibly' than Anova.

But like Carsten, I found this difficult to understand. Unordered factors are supposed to be arbitrary. I can understand why A:C behaves differently from A:D (where D<-factor(rep(1:3,6)). A:C generates a 27-level factor with only 18 levels populated; A:D has only nine levels. But it is not so simple.

I was goig to suggest using AC<-factor(A:C) instead of A %in% C as a fix, as this generates a 9-level factor with all levels populated - just as A:D does. But 
> alias(lm(resp~A+AC))
generates an alias, where 
> alias(lm(resp~A+A:D))
does not. 

That seemed to me entirely bizarre. table(A,AC) and table(A,A:D) show identical balance and nesting. Something, somewhere, is expanding the two differently. But what?

model.matrix is the 'culprit', I think. The two model matrices differ; model.matrix(resp~A+A:D) has 9 columns, and model.matrix(resp~A+AC) has eleven.

So now I know what has happened. What I don't understand is why, except that 'that is what model.matrix does with this factor level numbering' (refactoring either AC or A:D via as.numeric finally generates identical - and aliased - behaviour)) . I think I am going to invoke the law of unintended consequences and go find a cold compress.

Steve E

>>> Peter Dalgaard <P.Dalgaard at biostat.ku.dk> 11/07/2007 16:03:13 >>>
>A term C %in% A  (or A/C) is not a _specification_ that C is nested in
>A, it is a _directive_ to include the terms A and C:A. Now, C:A involves
>a term for each combination of A and C, of which many are empty if C is
>strictly coarser than A. This may well be what is confusing Anova().

>In fact, with this (c(1:3,6:11)) coding of C, A:C is completely
>equivalent to C, but if you look at summary(lm(....)) you will see a lot
>of NA coefficients in the A:C case. If you use resp ~ A*B+C, then you
>still get a couple of missing coefficients in the C terms because of
>collinearity with the A terms. (Notice that this is one case where the
>order inside the model formula will matter; C+A*B is not the same.)


*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}


From mark_difford at yahoo.co.uk  Thu Jul 12 12:23:06 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Thu, 12 Jul 2007 03:23:06 -0700 (PDT)
Subject: [R] Subsetting problem
In-Reply-To: <Pine.LNX.4.64.0707120603540.15630@gannet.stats.ox.ac.uk>
References: <B0F504209244B14EA9A4C1DFB599B92201852225@NIHCESMLBX6.nih.gov>
	<Pine.LNX.4.64.0707120603540.15630@gannet.stats.ox.ac.uk>
Message-ID: <11557188.post@talk.nabble.com>


Hi Massimo,

Professor Ripley has given you your answer.

It may help you further to know that factor levels aren't automatically
dropped when you subset a data set; you have to do it manually.  Some time
ago I scrounged the following command from Andy Liaw's randomForest package:
it removes all empty factor levels in a subsetted data set.

I subset a great deal, and find it extremely useful.

MyDat[] <- lapply(MyDat, function(x) if (is.factor(x)) x[, drop=T] else x)   
## Liaw's code for doing this

Regards,
Mark.

On Thu, 12 Jul 2007, Cressoni, Massimo (NIH/NHLBI) [F] wrote:

> I need to perform the Exact Wilcoxon Mann-Whitney on a subset of my
> database.
> Assuming that IPPO is my data frame and IPPOBIS is the subset my variable
> still
> have 3 different levels and the function wilcox_test (package "coin")
> does not accept it.
> I do not know how to overcome this problem.
>
> ippo <- c(rep("A",10),rep("B",10),rep("C",10))
> ippo2 <- c(rnorm(10,0,1),rnorm(10,10,10),rnorm(10,10,10))
> IPPO <- data.frame(ippo,ippo2)
>
> IPPOBIS <- IPPO[IPPO$ippo == "A" | IPPO$ippo == "B",]
>
> wilcox_test(ippo2 ~ ippo,data=IPPOBIS,distribution=exact())
> Error in check(itp) : 'object' does not represent a two sample problem
> levels(IPPOBIS$ippo)
> [1] "A" "B" "C"
>
> Massimo Cressoni
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



-- 
View this message in context: http://www.nabble.com/Subsetting-problem-tf4066094.html#a11557188
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Thu Jul 12 12:31:07 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jul 2007 11:31:07 +0100 (BST)
Subject: [R] how to get the p-values from an lm function ?
In-Reply-To: <f8e6ff050707120157h2bd699e7p527b9afa3ec890d6@mail.gmail.com>
References: <50c8fbc90707120151y21edcf5fo864bf8f8d2af009a@mail.gmail.com>
	<f8e6ff050707120157h2bd699e7p527b9afa3ec890d6@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707121125510.11691@gannet.stats.ox.ac.uk>

On Thu, 12 Jul 2007, hadley wickham wrote:

> On 7/12/07, Benoit Chemineau <benoitchemineau at gmail.com> wrote:
>> Hi, dear R-users,
>>
>> I am computing a liner regression by rating category using the 'by' function
>> as stated below:
>>
>> tmp <- by(projet, rating, function(x) lm(defaults ~ CGDP+CSAVE+SP500, data =
>> x))
>>
>> I would like to get not only the coefficients but also their p-values. I
>> can't find the command in the help pages to get them.
>>
>> Does anyone have a suggestion ?
>
> Hi Benoit,
>
> A general approach to find p-values:
>
> m <- lm(wt ~ mpg, data=mtcars)
>
> First figure out how to display them on screen:
> m # nope
> coef(m) # nope
> summary(m) # got it
>
> # Then use str to look at the components
> str(summary(m))
>
> # And pick out the one want
> summary(m)$coef
> coef(summary(m)) # slighty better style, but won't work in general

If x$coef works, coef(x) will almost certainly work at least as well. 
But note that in most cases it is x$coefficients and so x$coef is liable 
to partially match erroneously.

> # In general, you may also need to try
> str(print(summary(m)))
> # as sometimes the print method calculates the data you're looking for

But a print method should always return its input, so

str(summary(m))
str(print(summary(m)))

should be the same.

Reading the help pages would be a very good idea, as they usually not only 
tell you the names of the components of the result but also what they 
mean.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From villegas.ro at gmail.com  Thu Jul 12 12:38:29 2007
From: villegas.ro at gmail.com (Rod)
Date: Thu, 12 Jul 2007 12:38:29 +0200
Subject: [R] problems with memory in Mac
In-Reply-To: <4695F559.1050106@esa.ipvc.pt>
References: <4695F559.1050106@esa.ipvc.pt>
Message-ID: <29cf68350707120338yd6efdedpf86713a31c997e18@mail.gmail.com>

2007/7/12, Carlos Guerra <carlosguerra at esa.ipvc.pt>:
> Dear friends,
>
> I am having some doubts about the amount of memory that is being used by
> R in my Mac (MacBook Pro, 2Gig). Is there a way to increase the amount
> of memory used?
>
> When I type:
>
>  > mem.limits()
>
> the result is:
>
> nsize vsize
>    NA    NA
>
> and I can't change it, tough my computing in R isn't using all the
> memory at it's disposal.
>
> Best regards,
> Carlos
>
> --
> Carlos GUERRA
>
> Gabinete de Sistemas de Informacao Geografica
> Escola Superior Agraria de Ponte de Lima
> Mosteiro de Refoios do Lima
> 4990-706 Ponte de Lima
>
> Tlm: +351 91 2407109
> Tlf: +351 258 909779
>
> Reclaim your Inbox...!!!
> http://www.mozilla.org/products/thunderbird/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

read the help ?memory.limit

- Rod.


From h.wickham at gmail.com  Thu Jul 12 12:46:13 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 12:46:13 +0200
Subject: [R] how to get the p-values from an lm function ?
In-Reply-To: <Pine.LNX.4.64.0707121125510.11691@gannet.stats.ox.ac.uk>
References: <50c8fbc90707120151y21edcf5fo864bf8f8d2af009a@mail.gmail.com>
	<f8e6ff050707120157h2bd699e7p527b9afa3ec890d6@mail.gmail.com>
	<Pine.LNX.4.64.0707121125510.11691@gannet.stats.ox.ac.uk>
Message-ID: <f8e6ff050707120346h14a65cc8t628310f5d6602665@mail.gmail.com>

On 7/12/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Thu, 12 Jul 2007, hadley wickham wrote:
>
> > On 7/12/07, Benoit Chemineau <benoitchemineau at gmail.com> wrote:
> >> Hi, dear R-users,
> >>
> >> I am computing a liner regression by rating category using the 'by' function
> >> as stated below:
> >>
> >> tmp <- by(projet, rating, function(x) lm(defaults ~ CGDP+CSAVE+SP500, data =
> >> x))
> >>
> >> I would like to get not only the coefficients but also their p-values. I
> >> can't find the command in the help pages to get them.
> >>
> >> Does anyone have a suggestion ?
> >
> > Hi Benoit,
> >
> > A general approach to find p-values:
> >
> > m <- lm(wt ~ mpg, data=mtcars)
> >
> > First figure out how to display them on screen:
> > m # nope
> > coef(m) # nope
> > summary(m) # got it
> >
> > # Then use str to look at the components
> > str(summary(m))
> >
> > # And pick out the one want
> > summary(m)$coef
> > coef(summary(m)) # slighty better style, but won't work in general
>
> If x$coef works, coef(x) will almost certainly work at least as well.
> But note that in most cases it is x$coefficients and so x$coef is liable
> to partially match erroneously.

I meant in general that x$y, does not correspond to y(x) - I realised
after I wrote it that I was unclear.

> > # In general, you may also need to try
> > str(print(summary(m)))
> > # as sometimes the print method calculates the data you're looking for
>
> But a print method should always return its input, so
>
> str(summary(m))
> str(print(summary(m)))

Oh yes, I was getting confused with print functions which compute
values and print them but do not return them.

And that comment has made me realise many of my print methods don't
return x.   - something to fix.
Hadley


From william.a.simpson at gmail.com  Thu Jul 12 13:01:46 2007
From: william.a.simpson at gmail.com (William Simpson)
Date: Thu, 12 Jul 2007 12:01:46 +0100
Subject: [R] dose-response on a grid
Message-ID: <21ce39b20707120401h1cd3371btb70d2ff5012060e6@mail.gmail.com>

I have the following problem. I have measured a dose response curve
(binary response, continuous dose) on a grid of x,y positions. I would
like to produce a grey-level plot that shows the LD50 at each (x,y)
position.

I am thinking that I have to do something like
fit<-glm(resp ~ x*y + dose, family = binomial)
Corrections welcome.

But from here I don't know how to get LD50, and certainly not at each
x,y, position.

Thanks very much for any help.

Bill


From strinz at freenet.de  Thu Jul 12 13:18:14 2007
From: strinz at freenet.de (strinz at freenet.de)
Date: Thu, 12 Jul 2007 13:18:14 +0200
Subject: [R] RWeka control parameters classifiers interface
Message-ID: <E1I8wgY-00063M-2F@www15.emo.freenet-rz.de>

Hi,

  many thanks for the answer.
  It ist true, that for example

  m1 <- SMO(Species ~ ., data = iris, control = Weka_control( K = "weka.classifiers.functions.supportVector.PolyKernel"))
  m2 <- SMO(Species ~ ., data = iris, control = Weka_control( K = "weka.classifiers.functions.supportVector.RBFKernel"))
  deliver different results
  
  but
  m3 <- SMO(Species ~ ., data = iris, control = Weka_control( K = "weka.classifiers.functions.supportVector.PolyKernel",E=2))
  m4 <- SMO(Species ~ ., data = iris, control = Weka_control( K = "weka.classifiers.functions.supportVector.RBFKernel"),G=0.2)

  m3 does not differ from m1 (from the point of view of the setup, irrespective of the data!)
  m4 does not differ from m2 (from the point of view of the setup, irrespective of the data!)

  which can be seen, when looking at the results:
  m1 # (Linear Kernel, okay )
  m2 # (RBF Kernel, okay)
  m3  # still uses a linear kernel, but should be a <x,y>^2 kernel
  m4 # G is ignored, resulting in m2


Thx
Bjoern




----- original Nachricht --------

Betreff: Re: [R] RWeka control parameters classifiers interface
Gesendet: Mi 11 Jul 2007 14:42:10 CEST
Von: "Achim Zeileis"<Achim.Zeileis at wu-wien.ac.at>

> On Wed, 11 Jul 2007 strinz at freenet.de wrote:
> 
> >   The problem is, that the functions
> >   result=classifier(formula, data, subset, na.action, control =
> Weka_control(mycontrol))
> >   do not seem to be manipulated by the mycontrol- arguments
> 
> Yes, they are...not all parameter changes have always an effect on the
> specified learner.
> 
> >   Perhaps this should be resepected via the handlers- argument ,
> >   but the documentation in this regard is rather sparse.
> 
> Handlers are not needed here.
> 
> Re: sparse docs. In case you have not seen that paper already, there is a
> technical report on the ideas behind RWeka:
>  
> http://epub.wu-wien.ac.at/dyn/openURL?id=oai:epub.wu-wien.ac.at:epub-wu-01_b
> a6
> 
> Re: SMO. Compare
> 
> m1 <- SMO(Species ~ ., data = iris)
> m2 <- SMO(Species ~ ., data = iris, control = Weka_control(
>   K = "weka.classifiers.functions.supportVector.RBFKernel"))
> 
> which yield different results so the Weka_control() works.
> 
> The same happens if you register the mySMO() interface yourself. I'm not
> sure why the "E = ..." argument has no influence on the SMO, please check
> the Weka docs for this particular learner.
> 
> Best,
> Z
> 
> 
> 

--- original Nachricht Ende ----


From felix at nfrac.org  Thu Jul 12 13:29:16 2007
From: felix at nfrac.org (Felix Andrews)
Date: Thu, 12 Jul 2007 21:29:16 +1000
Subject: [R] time-varying recursive filter - vectorized
Message-ID: <94730b8a0707120429x5154947akdc59bb6ddfa83209@mail.gmail.com>

A question about vectorized operations (avoiding loops, for speed)...

I need to run a simple recursive (autoregressive) filter with a
time-varying coefficient. It is just a one-step recursive filter, so
it would be an exponential decay if the filter was constant.

I just want to do this, where 'x' is the data and 'w' is the weight to
apply to the previous time step:

x <- c(1, 1, 0, 2, 0, 0)
w <- c(NA, 0.1, 0.5, 0.4, 0.3, 0.2)
y <- x
for (i in seq_along(x)[-1]) y[i] <- y[i] + w[i] * y[i-1]
print(y)
[1] 1.0000 1.1000 0.5500 2.2200 0.6660 0.1332

But, since loops are slow, I would like a vectorized method, like
filter(, method="recursive").

Any ideas?

-- 
Felix Andrews / ???
PhD candidate, The Fenner School of Environment and Society
The Australian National University (Building 48A), ACT 0200
Beijing Bag, Locked Bag 40, Kingston ACT 2604
http://www.neurofractal.org/felix/
voice:+86_1051404394 (in China)
mobile:+86_13522529265 (in China)
mobile:+61_410400963 (in Australia)
xmpp:foolish.android at gmail.com
3358 543D AAC6 22C2 D336  80D9 360B 72DD 3E4C F5D8


From yn19832 at msn.com  Thu Jul 12 13:35:24 2007
From: yn19832 at msn.com (livia)
Date: Thu, 12 Jul 2007 04:35:24 -0700 (PDT)
Subject: [R] matrix of scatterplots
Message-ID: <11558049.post@talk.nabble.com>


Hi, I would like to use the function pairs() to plot a matrix of
scatterplots. For each scatterplot, the data are plotted in circles, can I
add some argument to change the circles into dots?

Could anyone give me some advice?Many thanks
-- 
View this message in context: http://www.nabble.com/matrix-of-scatterplots-tf4067527.html#a11558049
Sent from the R help mailing list archive at Nabble.com.


From landronimirc at gmail.com  Thu Jul 12 13:41:15 2007
From: landronimirc at gmail.com (Liviu Andronic)
Date: Thu, 12 Jul 2007 14:41:15 +0300
Subject: [R]  How to activate the R commands in SciViews
In-Reply-To: <68b1e2610707120440k3060c02eh169da200d07d137f@mail.gmail.com>
References: <161728.34547.qm@web56603.mail.re3.yahoo.com>
	<68b1e2610707120440k3060c02eh169da200d07d137f@mail.gmail.com>
Message-ID: <68b1e2610707120441k455fe576l35f7f73f9c858a12@mail.gmail.com>

Hello everybody,

I have a problem similar to that reported by Felipe. I installed R
2.5.0, Rcmdr from CRAN and SciViews-R 0.8-9 (with all the required and
optional components).

When accessing the R Commander menu from within
SciViews, the links cannot be clicked. When pointing at them, the mouse
transforms in a hand (as it normally does, similar to Internet
hyperlinks). However, the links cannot be activated. I had the
impression that in some way SciViews makes use of Internet Explorer in
its GUI, and that the problem was somewhere there. But I could not get
to the bottom of it.

Is there any way to make SciViews correctly use Rcmdr functionality?

Thanks in advance,
Liviu


On 6/20/07, Felipe Carrillo <mazatlanmexico at yahoo.com> wrote:
> Please help, I have SciViews(svGUI) and Rcmdr but when the SciViews Console opens the R commander menu don't work. Any ideas anybody?
>


-- 
Liviu


From tanyalele at gmail.com  Thu Jul 12 14:01:08 2007
From: tanyalele at gmail.com (Tanya Li)
Date: Thu, 12 Jul 2007 13:01:08 +0100
Subject: [R] Please Help
Message-ID: <193316cc0707120501n3acf0915u4a398dfa08e49081@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070712/33eed43c/attachment.pl 

From ramasamy at cancer.org.uk  Thu Jul 12 14:04:19 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 12 Jul 2007 13:04:19 +0100
Subject: [R] matrix of scatterplots
In-Reply-To: <11558049.post@talk.nabble.com>
References: <11558049.post@talk.nabble.com>
Message-ID: <469618C3.6070603@cancer.org.uk>

m <- matrix( rnorm(300), nc=3 )
pairs(m, pch=20)

or pairs(m, pch=".")

See help(par) for more details.


livia wrote:
> Hi, I would like to use the function pairs() to plot a matrix of
> scatterplots. For each scatterplot, the data are plotted in circles, can I
> add some argument to change the circles into dots?
> 
> Could anyone give me some advice?Many thanks


From phgrosjean at sciviews.org  Thu Jul 12 14:09:43 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 12 Jul 2007 14:09:43 +0200
Subject: [R] How to activate the R commands in SciViews
In-Reply-To: <68b1e2610707120441k455fe576l35f7f73f9c858a12@mail.gmail.com>
References: <161728.34547.qm@web56603.mail.re3.yahoo.com>	<68b1e2610707120440k3060c02eh169da200d07d137f@mail.gmail.com>
	<68b1e2610707120441k455fe576l35f7f73f9c858a12@mail.gmail.com>
Message-ID: <46961A07.6040701@sciviews.org>

As explained on the web page from where you downloaded SciViews 0.8-9, 
this version is not compatible with R 2.5.0.
Best,

Philippe Grosjean
..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Liviu Andronic wrote:
> Hello everybody,
> 
> I have a problem similar to that reported by Felipe. I installed R
> 2.5.0, Rcmdr from CRAN and SciViews-R 0.8-9 (with all the required and
> optional components).
> 
> When accessing the R Commander menu from within
> SciViews, the links cannot be clicked. When pointing at them, the mouse
> transforms in a hand (as it normally does, similar to Internet
> hyperlinks). However, the links cannot be activated. I had the
> impression that in some way SciViews makes use of Internet Explorer in
> its GUI, and that the problem was somewhere there. But I could not get
> to the bottom of it.
> 
> Is there any way to make SciViews correctly use Rcmdr functionality?
> 
> Thanks in advance,
> Liviu
> 
> 
> On 6/20/07, Felipe Carrillo <mazatlanmexico at yahoo.com> wrote:
>> Please help, I have SciViews(svGUI) and Rcmdr but when the SciViews Console opens the R commander menu don't work. Any ideas anybody?
>>
> 
>


From Zava.Aydemir at morganstanley.com  Thu Jul 12 14:13:56 2007
From: Zava.Aydemir at morganstanley.com (Aydemir, Zava (FID))
Date: Thu, 12 Jul 2007 08:13:56 -0400
Subject: [R] lead
Message-ID: <755261CA22782948B1C42ACDC83912A104614893@NYWEXMB27.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070712/3ec49adb/attachment.pl 

From jrkrideau at yahoo.ca  Thu Jul 12 14:22:05 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 12 Jul 2007 08:22:05 -0400 (EDT)
Subject: [R] elementary statistics with R (rkward?)
In-Reply-To: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
Message-ID: <773907.86412.qm@web32806.mail.mud.yahoo.com>


--- "Donatas G." <dgvirtual at akl.lt> wrote:

> Hi, I am trying to learn some basic statistics stuff
> but I cannot find any
> elementary statistics exercises using R language.
> Using RKward would be even
> better...
> 
> I need that in analysing sociological data, obtained
> through questionnairres -
> findind corelations between variables, relations
> between different types of
> data, etc.
> 
> Could anyone recommend simple tutorials/exercises,
> available on www for me to
> work on?
> 
> I realize it would be much simple to do this
> introductory stuff with spss, that
> everyone around me is using here in Lithuania, but
> I'd really like to learn to
> do it with R instead...
> 
> -- 
> Donatas G.

http://www.math.ilstu.edu/dhkim/Rstuff/Rtutor.html  is
not a bad place to start.   John Verzani's notes on
Simple R
http://www.math.csi.cuny.edu/Statistics/R/simpleR/ may
also help but his book is better.  

Peter Dalgarrd's book "Introductory Statistics with R"
is also very good.


From ritz at kvl.dk  Thu Jul 12 14:23:11 2007
From: ritz at kvl.dk (Christian Ritz)
Date: Thu, 12 Jul 2007 14:23:11 +0200
Subject: [R] dose-response on a grid
In-Reply-To: <21ce39b20707120401h1cd3371btb70d2ff5012060e6@mail.gmail.com>
References: <21ce39b20707120401h1cd3371btb70d2ff5012060e6@mail.gmail.com>
Message-ID: <46961D2F.8000206@kvl.dk>

Hi Bill,

have a look at the following artificial example:

## Loading the package 'drc' (on CRAN)
library(drc)

## Generating dataset with four dose-response curves
finneyx4 <- rbind(finney71, finney71, finney71, finney71)

## Generating artificial points (x,y)
## different pairs for each of the 4 curves in the above dataset
finneyx4$x <- rep(1, 24)
finneyx4$y <- rep(1:4, c(6, 6, 6, 6))

## Fitting the two-parameter log-logistic model (logistic regression)
m1 <- drm(affected/total ~ dose, as.factor(x):as.factor(y), weights = total,
data = finneyx4, fct = LL.2(), type = "binomial")

## Calculating ED50/LD50 for each location (they are all the same for this dataset)
ED(m1, 50)


You could try the same approach for your data!


Christian


From ramasamy at cancer.org.uk  Thu Jul 12 14:25:39 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 12 Jul 2007 13:25:39 +0100
Subject: [R] Please Help
In-Reply-To: <193316cc0707120501n3acf0915u4a398dfa08e49081@mail.gmail.com>
References: <193316cc0707120501n3acf0915u4a398dfa08e49081@mail.gmail.com>
Message-ID: <46961DC3.80807@cancer.org.uk>

This is the R-help mailing list. See help(BATCH).

You will need to write the required R commands in a separate script, say 
script.R and then execute it as

  R --no-save < script.R > logfile

You may need to augment the code above to include directory paths etc. 
There are other useful documentations at http://www.r-project.org/

Regards, Adai





Tanya Li wrote:
> Hello,
> 
> I got this email address from
> http://tolstoy.newcastle.edu.au/R/e2/help/06/10/2516.html, I got started to
> use R recently, Can I ask you a question ?
> 
> this is what I am using:
> platform       i686-pc-linux-gnu
> arch           i686
> os             linux-gnu
> system         i686, linux-gnu
> status
> major          2
> minor          4.0
> year           2006
> month          10
> day            03
> svn rev        39566
> language       R
> version.string R version 2.4.0 (2006-10-03)
> 
> I wanna to call R in shell( bash ) , write all R commands in the shell
> script and make it a cron job to execute automatically.
> 
> do you know how to do this ?
> 
> Looking forward to hearing from you, thanks a million.
> 
> Tanya Li


From yn19832 at msn.com  Thu Jul 12 14:28:10 2007
From: yn19832 at msn.com (livia)
Date: Thu, 12 Jul 2007 05:28:10 -0700 (PDT)
Subject: [R] matrix of scatterplots
In-Reply-To: <469618C3.6070603@cancer.org.uk>
References: <11558049.post@talk.nabble.com> <469618C3.6070603@cancer.org.uk>
Message-ID: <11558687.post@talk.nabble.com>


Thank you very much for your help.

Adaikalavan Ramasamy wrote:
> 
> m <- matrix( rnorm(300), nc=3 )
> pairs(m, pch=20)
> 
> or pairs(m, pch=".")
> 
> See help(par) for more details.
> 
> 
> livia wrote:
>> Hi, I would like to use the function pairs() to plot a matrix of
>> scatterplots. For each scatterplot, the data are plotted in circles, can
>> I
>> add some argument to change the circles into dots?
>> 
>> Could anyone give me some advice?Many thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/matrix-of-scatterplots-tf4067527.html#a11558687
Sent from the R help mailing list archive at Nabble.com.


From landronimirc at gmail.com  Thu Jul 12 14:34:59 2007
From: landronimirc at gmail.com (Liviu Andronic)
Date: Thu, 12 Jul 2007 15:34:59 +0300
Subject: [R]  How to activate the R commands in SciViews
In-Reply-To: <68b1e2610707120533s29bb9d33r1c55e50be4594fe@mail.gmail.com>
References: <161728.34547.qm@web56603.mail.re3.yahoo.com>
	<68b1e2610707120440k3060c02eh169da200d07d137f@mail.gmail.com>
	<68b1e2610707120441k455fe576l35f7f73f9c858a12@mail.gmail.com>
	<46961A07.6040701@sciviews.org>
	<68b1e2610707120533s29bb9d33r1c55e50be4594fe@mail.gmail.com>
Message-ID: <68b1e2610707120534q54960a9aj2be47f87ad7abc81@mail.gmail.com>

Thank you for the pointer.

On the SciViews official site
(http://www.sciviews.org/SciViews-R/index.html) I could not find any
indications on this incompatibility. At any rate, I would have two
questions related to the future of SciViews-R.

Are there any developments planned in some near future that would make
SciViews compatible with recent versions of R and Rcmdr? At the
Unviversity of Social Sciences of Toulouse, for example, the most
recent version of R is installed. And for statistics introductory
classes, SciViews would be an important advantage. R should be
downgraded to which version for the two to be compatible?

Secondly, is there any chance that SciViews become available on Linux,
again in some not so distant future? That is, would the tcltk2 package
be ported to other OS's and would SciViews be subsequently enhanced to
build on such systems? Last time I checked the net, no interesting
information was available as to such developments. Maybe there is some
beta version of tcltk2 that I don't know of..

Regards,
Liviu

On 7/12/07, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
> As explained on the web page from where you downloaded SciViews 0.8-9,
> this version is not compatible with R 2.5.0.
> Best,
>
> Philippe Grosjean
> ..............................................<?}))><........
>   ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>   ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>   ) ) ) ) )   Mons-Hainaut University, Belgium
> ( ( ( ( (
> ..............................................................


From phgrosjean at sciviews.org  Thu Jul 12 15:04:57 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 12 Jul 2007 15:04:57 +0200
Subject: [R] [Fwd: Re:  How to activate the R commands in SciViews]
Message-ID: <469626F9.9010108@sciviews.org>

Well.. plans are there from a long time to rewrite SciViews completely
and make it platform independent (to work on Linux/Unix and MacOS X, as
well as Windows). I have done some work in this direction when time
permitted, but I am pretty busy with other work. During the holidays, I
will continue to work in this direction. I will try to package a first
running version of SciViews compatible with latest R and Rcmdr for
Windows and Linux for next September. At least, I *have* to do so,
because I *need* it for one of my teachings which will be done in a
different University (where all machines are running with Linux)!

So, my advice would be to check the web site again in September/October
for some news. Unfortunately, I cannot promise more. Sorry for the long
delays. I do my best, and the second programmer (Eric Lecoutre) has left
the project a long time ago... so, I am alone on this!

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Liviu Andronic wrote:
> Thank you for the pointer.
> 
> On the SciViews official site
> (http://www.sciviews.org/SciViews-R/index.html) I could not find any
> indications on this incompatibility. At any rate, I would have two
> questions related to the future of SciViews-R.
> 
> Are there any developments planned in some near future that would make
> SciViews compatible with recent versions of R and Rcmdr? At the
> Unviversity of Social Sciences of Toulouse, for example, the most
> recent version of R is installed. And for statistics introductory
> classes, SciViews would be an important advantage. R should be
> downgraded to which version for the two to be compatible?
> 
> Secondly, is there any chance that SciViews become available on Linux,
> again in some not so distant future? That is, would the tcltk2 package
> be ported to other OS's and would SciViews be subsequently enhanced to
> build on such systems? Last time I checked the net, no interesting
> information was available as to such developments. Maybe there is some
> beta version of tcltk2 that I don't know of..
> 
> Regards,
> Liviu
> 
> On 7/12/07, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
>> As explained on the web page from where you downloaded SciViews 0.8-9,
>> this version is not compatible with R 2.5.0.
>> Best,
>>
>> Philippe Grosjean
>> ..............................................<?}))><........
>>   ) ) ) ) )
>> ( ( ( ( (    Prof. Philippe Grosjean
>>   ) ) ) ) )
>> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>>   ) ) ) ) )   Mons-Hainaut University, Belgium
>> ( ( ( ( (
>> ..............................................................
>>
>> Liviu Andronic wrote:
>> > Hello everybody,
>> >
>> > I have a problem similar to that reported by Felipe. I installed R
>> > 2.5.0, Rcmdr from CRAN and SciViews-R 0.8-9 (with all the required and
>> > optional components).
>> >
>> > When accessing the R Commander menu from within
>> > SciViews, the links cannot be clicked. When pointing at them, the mouse
>> > transforms in a hand (as it normally does, similar to Internet
>> > hyperlinks). However, the links cannot be activated. I had the
>> > impression that in some way SciViews makes use of Internet Explorer in
>> > its GUI, and that the problem was somewhere there. But I could not get
>> > to the bottom of it.
>> >
>> > Is there any way to make SciViews correctly use Rcmdr functionality?
>> >
>> > Thanks in advance,
>> > Liviu
>> >
>> >
>> > On 6/20/07, Felipe Carrillo <mazatlanmexico at yahoo.com> wrote:
>> >> Please help, I have SciViews(svGUI) and Rcmdr but when the SciViews 
>> Console opens the R commander menu don't work. Any ideas anybody?
>> >>
>> >
>> >
>>
> 
> 


-- 
..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................


From thomas.schwander at mvv.de  Thu Jul 12 15:52:56 2007
From: thomas.schwander at mvv.de (thomas.schwander at mvv.de)
Date: Thu, 12 Jul 2007 15:52:56 +0200
Subject: [R] eMail results out of R
Message-ID: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070712/e80a73ef/attachment.pl 

From ramasamy at cancer.org.uk  Thu Jul 12 15:56:30 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 12 Jul 2007 14:56:30 +0100
Subject: [R] lead
In-Reply-To: <755261CA22782948B1C42ACDC83912A104614893@NYWEXMB27.msad.ms.com>
References: <755261CA22782948B1C42ACDC83912A104614893@NYWEXMB27.msad.ms.com>
Message-ID: <4696330E.30406@cancer.org.uk>

How about

  revLag <- function(x, shift=1) rev( Lag(rev(x), shift) )

  x <- 1:5
  revLag(x, shift=2)


As a matter of fact, here is a generalized version of Lag to include 
negative shifts.

myLag <- function (x, shift = 1){

     xLen <- length(x)
     ret <- as.vector(character(xLen), mode = storage.mode(x))
     attrib <- attributes(x)
     if (!is.null(attrib$label))
         atr$label <- paste(attrib$label, "lagged", shift, "observations")

     if (shift == 0) return(x)

     if( xLen <= abs(shift) ) return(ret)

     if (shift < 0) x <- rev(x)
     retrange = 1:abs(shift)
     ret[-retrange] <- x[1:(xLen - abs(shift))]
     if (shift < 0) ret <- rev(ret)

     attributes(ret) <- attrib
     return(ret)
}

and some test examples:

myLag(1:5, shift=2)
  [1] NA NA  1  2  3

myLag(letters[1:4], shift=2)
[1] ""  ""  "a" "b"

myLag(factor(letters[1:4]), shift=2)
  [1] <NA> <NA> a    b
  Levels: a b c d

myLag(1:5, shift=-2)
  [1]  3  4  5 NA NA

myLag(letters[1:4], shift=-2)
  [1] "c" "d" ""  ""

myLag(factor(letters[1:4]), shift=-2)
  [1] c    d    <NA> <NA>
  Levels: a b c d

Regards, Adai




Aydemir, Zava (FID) wrote:
> Hi,
>  
> is there any function in R that shifts elements of a vector to the
> opposite direction of what Lag()  of the Hmisc package does? (something
> like, Lag(x, shift = -1) )
>  
> Thanks
>  
> Zava
> --------------------------------------------------------
> 
> This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From gordon.morrison at hsbcib.com  Thu Jul 12 15:56:17 2007
From: gordon.morrison at hsbcib.com (gordon.morrison at hsbcib.com)
Date: Thu, 12 Jul 2007 14:56:17 +0100
Subject: [R]  XAML
Message-ID: <OF62FFFE6C.A963397E-ON80257316.004C02E8-80257316.004C9081@hsbcib.com>

Hi

I wonder if anyone has had any thoughts about rendering R graphical output
into XAML?

================================================================================

Gordon M. Morrison
HSBC Bank plc
8 Canada Square
London
E14 5HQ

-----------------------------------------
SAVE PAPER - THINK BEFORE YOU PRINT!

This transmission has been issued by a member of the HSBC Group
"HSBC" for the information of the addressee only and should not be
reproduced and/or distributed to any other person. Each page
attached hereto must be read in conjunction with any disclaimer
which forms part of it. Unless otherwise stated, this transmission
is neither an offer nor the solicitation of an offer to sell or
purchase any investment. Its contents are based on information
obtained from sources believed to be reliable but HSBC makes no
representation and accepts no responsibility or liability as to its
completeness or accuracy.


From F.MENDIBURU at CGIAR.ORG  Thu Jul 12 16:08:08 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Thu, 12 Jul 2007 09:08:08 -0500
Subject: [R] type III ANOVA for a nested linear model
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A201406AE3@webmail.cip.cgiar.org>

Dear Carsten

In this test, factor B would be representing to a factor of block or repetition according to as the levels of A, B, and C are in the data. Factor C this nested in A, then the model should include: B, A and C nested in A, the difference it is the error.

Model:
B	1
A	2
C(A)	6
Error	(2+6)*1 = 8
Total    

mydata<-read.table("mydata.txt",header=T)
mydata[,1]<- as.factor(mydata[,1])
mydata[,2]<- as.factor(mydata[,2])
mydata[,3]<- as.factor(mydata[,3])
model <- aov(resp ~ B + A + C/A, mydata)
summary(model)
            Df Sum Sq Mean Sq F value    Pr(>F)    
B            1 915.21  915.21 89.6476 1.274e-05 ***
A            2  33.12   16.56  1.6223   0.25621    
C            6 199.50   33.25  3.2570   0.06316 .  
Residuals    8  81.67   10.21

Best regards,

Felipe de Mendiburu
Statistician



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Carsten Jaeger
Sent: Tuesday, July 10, 2007 6:15 AM
To: R help list
Subject: [R] type III ANOVA for a nested linear model


Hello,

is it possible to obtain type III sums of squares for a nested model as
in the following:

lmod <- lm(resp ~ A * B + (C %in% A), mydata))

I have tried

library(car)
Anova(lmod, type="III")

but this gives me an error (and I also understand from the documentation
of Anova as well as from a previous request
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/64477.html) that it is
not possible to specify nested models with car's Anova).

anova(lmod) works, of course.

My data (given below) is balanced so I expect the results to be similar
for both type I and type III sums of squares. But are they *exactly* the
same? The editor of the journal which I'm sending my manuscript to
requests what he calls "conventional" type III tests and I'm not sure if
can convince him to accept my type I analysis.

R> mydata
      A     B     C  resp
1     1     1      1 34.12
2     1     1      2 32.45
3     1     1      3 44.55
4     1     2      1 20.88
5     1     2      2 22.32
6     1     2      3 27.71
7     2     1      6 38.20
8     2     1      7 31.62
9     2     1      8 38.71
10    2     2      6 18.93
11    2     2      7 20.57
12    2     2      8 31.55
13    3     1      9 40.81
14    3     1     10 42.23
15    3     1     11 41.26
16    3     2      9 28.41
17    3     2     10 24.07
18    3     2     11 21.16

Thanks a lot,

Carsten

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From therneau at mayo.edu  Thu Jul 12 16:33:13 2007
From: therneau at mayo.edu (Terry Therneau)
Date: Thu, 12 Jul 2007 09:33:13 -0500 (CDT)
Subject: [R] p-value from survreg
Message-ID: <200707121433.l6CEXCF18506@hsrnfs-101.mayo.edu>

The question was how to get the p-value from the fit below, as an S object
 
sr<-survreg(s~groups, dist="gaussian")
Coefficients:
(Intercept)      groups
-0.02138485  0.03868351

Scale= 0.01789372

Loglik(model)= 31.1   Loglik(intercept only)= 25.4
        Chisq= 11.39 on 1 degrees of freedom, p= 0.00074
n= 16
 

----
  In general, good places to start are 
    > names(sr)
    > help(survreg.object)  
    > ssr <- summary(sr)
    > names(ssr)
As someone else pointed out, it's also easy to look at the print.survreg
function and see how the value was created -- one of the things I love
about S.
    
Unfortunately, doing the above myself showed that I have let the documentation
page for survreg.object get seriously out of date -- quite embarassing as
that is logically the first place to start.

As to the print function creating things "on the fly": there is an area where
there is no good answer.  Does one make the return object from a fit such
that it contains only minimal data, or add in all of the other computations
that can be derived from these?  The Chambers and Hastie book "Statistical
Models in S", which was the starting point for model objects, leaned towards
the former, and this still influences many functions.  Often the summary
function will "fill in" these derived values, the std and t-tests for
the individual coefficients for instance.

	Terry T.


From h.wickham at gmail.com  Thu Jul 12 16:36:28 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 16:36:28 +0200
Subject: [R] ggplot doesnt work in loops?
In-Reply-To: <f8e6ff050707120055r5cba2683ic2de6e4a73e0ada0@mail.gmail.com>
References: <003d01c7c457$b7681900$6501a8c0@STEVE>
	<f8e6ff050707120055r5cba2683ic2de6e4a73e0ada0@mail.gmail.com>
Message-ID: <f8e6ff050707120736l5778952bx123758477a5108cd@mail.gmail.com>

On 7/12/07, hadley wickham <h.wickham at gmail.com> wrote:
> Hi Steve,
>
> You need to explicitly print the ggplot object:
> ggplot(mydata, aes(x=mydata$varc)) + geom_bar()
>
> (this is a R-faq for lattice plots, and ggplot works the same way)
>
> In the latest version of ggplot (0.5.4) you can construct the plot
> before hand and modify the aesthetics in each instance of the loop:
>
> p <- ggplot(mydata) + geom_bar()
> mydata$varc = c(1,2,3)
> for (i in 1:1){
>         jpeg("test3.jpg")
>         p + aes(x = mydata$varc)
>         dev.off()
> }
>
> (not that this will actually work because you're not using i inside
> your loop anywhere)
>
> (and to be strictly correct you should probably use list(x =
> as.name(names(mydata)[i]))  instead of the aes call - but I haven't
> written any documentation for this yet)

Actually a better solution (will be included in the next version of ggplot) is:

aes_string <- function(...) structure(lapply(list(...), as.name),
class="uneval")

p + aes_string(x = names(mydata)[i])

It converts aes(x = "x", y="y") to aes(x=x, y=y).  The first is easy
to generate programmatically, the second is less to type.

Hadley


From ed_deroiste at yahoo.co.uk  Thu Jul 12 16:38:34 2007
From: ed_deroiste at yahoo.co.uk (TIMMMAY)
Date: Thu, 12 Jul 2007 07:38:34 -0700 (PDT)
Subject: [R] Fitting a Gamma Curve
Message-ID: <11561404.post@talk.nabble.com>


Hi there, I hope someone can help me before I tear all my hair out. I have a
set transition intensities and when plotted the curve looks like a gamma
density. I want to fit a gamma density curve to these intensities. It is
just a curve fitting problem but whats causing the trouble is that I need to
use least squares minimization to calculate the parameters for the gamma
curve. How do I do this??? 

The curve will be a truncated gamma function so it will have 3 paramaters a,
b, c. I tried to do the following 

nls(lograte ~ log(c) + a*log(b) + (a-1)*log(age)+b*(age)-lgamma(a),
start=list(a=1,b=1,c=1)), where lograte, and age are my data. a,b the gamma
parameters and c the parameter we need because we are fitting a truncated
distribution.

I also tried defining 
fn = function(p) sum((log(y)-log(dgamma(x,p[1],p[2])*p[3]))^2)
a residual sum of squares and using nlm to minimise this and find paramaters
but this doesnt work either. Can anyone help me ?? Please :)

-- 
View this message in context: http://www.nabble.com/Fitting-a-Gamma-Curve-tf4068543.html#a11561404
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Thu Jul 12 16:40:47 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 12 Jul 2007 10:40:47 -0400
Subject: [R] lead
In-Reply-To: <755261CA22782948B1C42ACDC83912A104614893@NYWEXMB27.msad.ms.com>
References: <755261CA22782948B1C42ACDC83912A104614893@NYWEXMB27.msad.ms.com>
Message-ID: <971536df0707120740ke318dd6k27b0e5f25dcada2@mail.gmail.com>

The lag.zoo method of lag in the zoo package supports positive, negative
and multiple lags and has an na.pad= argument.  (zoo also has a
lag.zooreg method, not shown, for zooreg objects):

> library(zoo)
> z <- zoo(11:15)
> z
 1  2  3  4  5
11 12 13 14 15
> lag(z, na.pad = TRUE)
 1  2  3  4  5
12 13 14 15 NA
> lag(z, 1, na.pad = TRUE) # same
 1  2  3  4  5
12 13 14 15 NA
>
> # negative lag
> lag(z, -1, na.pad = TRUE)
 1  2  3  4  5
NA 11 12 13 14
>
> # mulitple lags
> lag(z, 1:3, na.pad = TRUE)
  lag1 lag2 lag3
1   12   13   14
2   13   14   15
3   14   15   NA
4   15   NA   NA
> lag(z, -(1:3), na.pad = TRUE)
  lag-1 lag-2 lag-3
2    11    NA    NA
3    12    11    NA
4    13    12    11
5    14    13    12

vignette("zoo") # more info on zoo


On 7/12/07, Aydemir, Zava (FID) <Zava.Aydemir at morganstanley.com> wrote:
> Hi,
>
> is there any function in R that shifts elements of a vector to the
> opposite direction of what Lag()  of the Hmisc package does? (something
> like, Lag(x, shift = -1) )
>
> Thanks
>
> Zava
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sabya23 at gmail.com  Thu Jul 12 16:41:01 2007
From: sabya23 at gmail.com (spime)
Date: Thu, 12 Jul 2007 07:41:01 -0700 (PDT)
Subject: [R] Package for .632 (and .632+) bootstrap and the cross-validation
 of ROC Parameters
Message-ID: <11561405.post@talk.nabble.com>



Hi users,

I need to calculate .632 (and .632+) bootstrap and the cross-validation of
area under curve (AUC) to compare my models. Is there any package for the
same. I know about 'ipred' and using it i can calculate misclassification
errors. 

Please help. It's urgent. 
-- 
View this message in context: http://www.nabble.com/Package-for-.632-%28and-.632%2B%29-bootstrap-and-the-cross-validation-of-ROC-Parameters-tf4068544.html#a11561405
Sent from the R help mailing list archive at Nabble.com.


From pete-expires-20070910 at kazmier.com  Thu Jul 12 16:41:13 2007
From: pete-expires-20070910 at kazmier.com (Pete Kazmier)
Date: Thu, 12 Jul 2007 10:41:13 -0400
Subject: [R] ggplot2 / reshape / Question on manipulating data
References: <87ir8qz04e.fsf@coco.kazmier.com>
	<f8e6ff050707120035r5668e93bqf963acc3540c4890@mail.gmail.com>
Message-ID: <87d4yxznue.fsf@coco.kazmier.com>

"hadley wickham" <h.wickham at gmail.com> writes:

> On 7/12/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
>> I'm an R newbie but recently discovered the ggplot2 and reshape
>> packages which seem incredibly useful and much easier to use for a
>> beginner.  Using the data from the IMDB, I'm trying to see how the
>> average movie rating varies by year.  Here is what my data looks like:
>>
>> > ratings <- read.delim("groomed.list", header = TRUE, sep = "|", comment.char = "")
>> > ratings <- subset(ratings, VoteCount > 100)
>> > head(ratings)
>>                              Title  Histogram VoteCount VoteMean Year
>> 1                !Huff (2004) (TV) 0000000016       299      8.4 2004
>> 8              'Allo 'Allo! (1982) 0000000125       829      8.6 1982
>> 50              .hack//SIGN (2002) 0000001113       150      7.0 2002
>> 56            1-800-Missing (2003) 0000000103       118      5.4 2003
>> 66  Greatest Artists (2000) (mini) 00..000016       110      7.8 2000
>> 77 00 Scariest Movie (2004) (mini) 00..000115       256      8.6 2004
>
> Have you tried using the movies dataset included in ggplot?  Or is
> there some data that you want that is not in that dataset.

It's funny that you mention this because I had intended to write this
email about a month ago but was delayed due to other reasons.  In any
case, when I was typing this up last night, I wanted to recreate my
steps but I could not find the IMDB movie data I had used originally.
I searched everywhere to no avail so I downloaded the data myself and
groomed it.  Only now do I remember that I had used the movies dataset
included in ggplot.

>> How do 'byYear' and 'byYear2' differ?  I am trying to use 'typeof' but
>> both seem to be lists.  However, they are clearly different in some
>> way because 'qplot' graphs them differently.
>
> Try using str - it's much more helpful, and you should see the
> different quickly.

Thanks!  This is the function I've been looking for in my quest to
learn about internal data types of R.  Too bad it has such a terrible
name! 

> Using the built in movies data:
>
> mm <- melt(movies, id=1:2, m=c("rating", "votes"))
> msum <- cast(mm, year ~ variable, c(mean, sum))
>
> qplot(year, rating_mean, data=msum, colour=votes_sum)
> qplot(year, rating_mean, data=msum, colour=votes_sum, geom="line")

Great!  This is exactly what I was looking to do.  By the way, does
any of your documentation use the movie dataset as an example?  I'm
curious what else I can do with the dataset.  For example, how can I
use ggplot's facets to see the same information by type of movie?  I'm
unsure of how to manipulate the binary variables into a single
variable so that it can be treated as levels.

Thanks!
Pete


From h.wickham at gmail.com  Thu Jul 12 16:45:08 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 16:45:08 +0200
Subject: [R] p-value from survreg
In-Reply-To: <200707121433.l6CEXCF18506@hsrnfs-101.mayo.edu>
References: <200707121433.l6CEXCF18506@hsrnfs-101.mayo.edu>
Message-ID: <f8e6ff050707120745o53a6ed4avb44fdcf83946a2d3@mail.gmail.com>

On 7/12/07, Terry Therneau <therneau at mayo.edu> wrote:
> The question was how to get the p-value from the fit below, as an S object
>
> sr<-survreg(s~groups, dist="gaussian")
> Coefficients:
> (Intercept)      groups
> -0.02138485  0.03868351
>
> Scale= 0.01789372
>
> Loglik(model)= 31.1   Loglik(intercept only)= 25.4
>         Chisq= 11.39 on 1 degrees of freedom, p= 0.00074
> n= 16
>
>
> ----
>   In general, good places to start are
>     > names(sr)
>     > help(survreg.object)
>     > ssr <- summary(sr)
>     > names(ssr)
> As someone else pointed out, it's also easy to look at the print.survreg
> function and see how the value was created -- one of the things I love
> about S.
>
> Unfortunately, doing the above myself showed that I have let the documentation
> page for survreg.object get seriously out of date -- quite embarassing as
> that is logically the first place to start.
>
> As to the print function creating things "on the fly": there is an area where
> there is no good answer.  Does one make the return object from a fit such
> that it contains only minimal data, or add in all of the other computations
> that can be derived from these?  The Chambers and Hastie book "Statistical
> Models in S", which was the starting point for model objects, leaned towards
> the former, and this still influences many functions.  Often the summary
> function will "fill in" these derived values, the std and t-tests for
> the individual coefficients for instance.

I think this is where it's nice to have a separate function that does
the filling in - then you can have the best of both worlds.  That's
the role that summary often plays.

Hadley


From John.Scillieri at constellation.com  Thu Jul 12 16:49:03 2007
From: John.Scillieri at constellation.com (Scillieri, John)
Date: Thu, 12 Jul 2007 10:49:03 -0400
Subject: [R] eMail results out of R
In-Reply-To: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>
References: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>
Message-ID: <7B145F644E353D48A1FC22F09D97767702994461@EXM-OMF-22.Ceg.Corp.Net>

We use a program called Blat (www.blat.net) on Windows to email out
results of overnight runs. If you're on Unix/Linux you can definitely do
a similar thing using one of the hundreds of command line utils.

The R code is similar to below:

sendEmail <- function(from, to, subject, body)
{
  BLAT <- "PATH TO BLAT.EXE"
  MAILSERVER <- "your mail server here";
  
  command <- paste(BLAT, "-", "-to", dQuote(to), "-server", 
    MAILSERVER, "-s", dQuote(subject), "-f", dQuote(from))
    
  system(command, input=body)
}

HTH,

John Scillieri


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
thomas.schwander at mvv.de
Sent: Thursday, July 12, 2007 9:53 AM
To: r-help at stat.math.ethz.ch
Subject: [R] eMail results out of R

Hi everyone,

I did my homework and read the posting guideline :-)

I want to eMail the results of a computing automatically. So I get the
results (the parameters of a garch process) and I want to eMail them to
another person. How can I do that?

Thx
>>> This e-mail and any attachments are confidential, may contain legal,
professional or other privileged information, and are intended solely for the
addressee.  If you are not the intended recipient, do not use the information
in this e-mail in any way, delete this e-mail and notify the sender. CEG-IP2


From murdoch at stats.uwo.ca  Thu Jul 12 16:57:16 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 12 Jul 2007 10:57:16 -0400
Subject: [R] eMail results out of R
In-Reply-To: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>
References: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>
Message-ID: <4696414C.6030908@stats.uwo.ca>

On 7/12/2007 9:52 AM, thomas.schwander at mvv.de wrote:
> Hi everyone,
> 
> I did my homework and read the posting guideline :-)
> 
> I want to eMail the results of a computing automatically. So I get the results (the parameters of a garch process) and I want to eMail them to another person. How can I do that?


This will depend on the system you're using.  If the command "emailit" 
would work from the command line on your system, then

system("emailit")

should work from within R.  Writing that command is the hard part, of 
course.

Duncan Murdoch


From rfrancois at mango-solutions.com  Thu Jul 12 17:53:23 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Thu, 12 Jul 2007 16:53:23 +0100
Subject: [R] eMail results out of R
In-Reply-To: <4696414C.6030908@stats.uwo.ca>
References: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>
	<4696414C.6030908@stats.uwo.ca>
Message-ID: <46964E73.6050505@mango-solutions.com>

Hi,

There is a paper in the April 2007 issue of R News that might be of help 
here.
http://##cran mirror##/doc/Rnews/Rnews_2007-1.pdf

Romain

Duncan Murdoch wrote:
> On 7/12/2007 9:52 AM, thomas.schwander at mvv.de wrote:
>   
>> Hi everyone,
>>
>> I did my homework and read the posting guideline :-)
>>
>> I want to eMail the results of a computing automatically. So I get the results (the parameters of a garch process) and I want to eMail them to another person. How can I do that?
>>     
>
>
> This will depend on the system you're using.  If the command "emailit" 
> would work from the command line on your system, then
>
> system("emailit")
>
> should work from within R.  Writing that command is the hard part, of 
> course.
>
> Duncan Murdoch
>   
-- 
Mango Solutions
data analysis that delivers

Tel:  +44(0) 1249 467 467
Fax:  +44(0) 1249 467 468
Mob:  +44(0) 7813 526 123


From dray at biomserv.univ-lyon1.fr  Thu Jul 12 18:10:46 2007
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Thu, 12 Jul 2007 18:10:46 +0200
Subject: [R] eMail results out of R
In-Reply-To: <46964E73.6050505@mango-solutions.com>
References: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>	<4696414C.6030908@stats.uwo.ca>
	<46964E73.6050505@mango-solutions.com>
Message-ID: <46965286.7040904@biomserv.univ-lyon1.fr>

Here is a small function that I used on Debian. It requires exim4 :

send.mail<-function(addr='dray at biomserv.univ-lyon1.fr',subject='A 
message from R',
                    text=paste("I have finished to work 
",Sys.time(),coll="")){
    # send an email
    # it requires the reconfiguration of exim4
    # you have to connect as root and
    # then type dpkg-reconfigure exim4config
   
    mail.cmd<-paste("mail ",
                    "-s \"",subject,"\" ",
                    addr,
                    " << EOT &\n",
                    text,"\n",
                    "EOT",
                    sep="",collapse="")
     system(mail.cmd,intern=FALSE)
  }

Cheers,

Romain Francois wrote:
> Hi,
>
> There is a paper in the April 2007 issue of R News that might be of help 
> here.
> http://##cran mirror##/doc/Rnews/Rnews_2007-1.pdf
>
> Romain
>
> Duncan Murdoch wrote:
>   
>> On 7/12/2007 9:52 AM, thomas.schwander at mvv.de wrote:
>>   
>>     
>>> Hi everyone,
>>>
>>> I did my homework and read the posting guideline :-)
>>>
>>> I want to eMail the results of a computing automatically. So I get the results (the parameters of a garch process) and I want to eMail them to another person. How can I do that?
>>>     
>>>       
>> This will depend on the system you're using.  If the command "emailit" 
>> would work from the command line on your system, then
>>
>> system("emailit")
>>
>> should work from within R.  Writing that command is the hard part, of 
>> course.
>>
>> Duncan Murdoch
>>   
>>     


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/


From lutz.breitling at gmail.com  Thu Jul 12 18:15:02 2007
From: lutz.breitling at gmail.com (Lutz Ph. Breitling)
Date: Thu, 12 Jul 2007 18:15:02 +0200
Subject: [R] Stepwise GLM selection by LRT?
In-Reply-To: <002a01c7c3f3$119d01c0$7c94100a@win.ad.jhu.edu>
References: <2e38a1c80707111206l499cc328r6fff2d785fc7719e@mail.gmail.com>
	<002a01c7c3f3$119d01c0$7c94100a@win.ad.jhu.edu>
Message-ID: <2e38a1c80707120915w3a22c78em5162dd86d2929861@mail.gmail.com>

Thank you very much for the prompt reply. Seems like I had not fully
understood what the k-parameter to stepAIC is doing.
Your suggested approach looks indeed fine to me, actually I do not
quite understand why you say that it's only an approximation to the
LRT?

Best wishes-
Lutz

On 7/11/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> Check out the stepAIC function in MASS package.  This is a nice tool, where
> you can actually implement any penalty even though the function's name has
> "AIC" in it because it is the default.  Although this doesn't do an LRT test
> based variable selection, you can sort of approximate it by using a penalty
> of k = qchisq(1-p, df=1), where p is the p-value for variable selection.
> This penalty means that a variable enters/exits an existing model, when the
> absolute value of change in log-likelihood is greater than qchisq(1-p,
> df=1). For p = 0.1, k = 2.71, and for p=0.05, k = 3.84.  Is this whhant
> you'd like to do?
>
> Ravi.
>
> ----------------------------------------------------------------------------
> -------
>
> Ravi Varadhan, Ph.D.
>
> Assistant Professor, The Center on Aging and Health
>
> Division of Geriatric Medicine and Gerontology
>
> Johns Hopkins University
>
> Ph: (410) 502-2619
>
> Fax: (410) 614-9625
>
> Email: rvaradhan at jhmi.edu
>
> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>
>
>
> ----------------------------------------------------------------------------
> --------
>
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lutz Ph. Breitling
> Sent: Wednesday, July 11, 2007 3:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Stepwise GLM selection by LRT?
>
> Dear List,
>
> having searched the help and archives, I have the impression that
> there is no automatic model selection procedure implemented in R that
> includes/excludes predictors in logistic regression models based on
> LRT P-values. Is that true, or is someone aware of an appropriate
> function somewhere in a custom package?
>
> Even if automatic model selection and LRT might not be the most
> appropriate methods, I actually would like to use these in order to
> simulate someone else's modeling approach...
>
> Many thanks for all comments-
> Lutz
> -----
> Lutz Ph. Breitling
> German Cancer Research Center
> Heidelberg/Germany
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kdestler at u.washington.edu  Thu Jul 12 18:17:24 2007
From: kdestler at u.washington.edu (kdestler)
Date: Thu, 12 Jul 2007 09:17:24 -0700 (PDT)
Subject: [R] Difference in linear regression results for Stata and R
Message-ID: <11563283.post@talk.nabble.com>


Hi
I recently imported data from r into Stata.  I then ran the linear
regression model I've been working on, only to discover that the results are
somewhat (though not dramatically different).  the standard errors vary more
between the two programs than do the coefficients themselves.  Any
suggestions on what I've done that causes this mismatch?

Thanks,
Kate
-- 
View this message in context: http://www.nabble.com/Difference-in-linear-regression-results-for-Stata-and-R-tf4069072.html#a11563283
Sent from the R help mailing list archive at Nabble.com.


From ted.harding at nessie.mcc.ac.uk  Thu Jul 12 18:25:24 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jul 2007 17:25:24 +0100 (BST)
Subject: [R] eMail results out of R
In-Reply-To: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>
Message-ID: <XFMail.070712172524.ted.harding@nessie.mcc.ac.uk>

On 12-Jul-07 13:52:56, thomas.schwander at mvv.de wrote:
> Hi everyone,
> 
> I did my homework and read the posting guideline :-)
> 
> I want to eMail the results of a computing automatically. So I get the
> results (the parameters of a garch process) and I want to eMail them to
> another person. How can I do that?
> 
> Thx

As well as the answers from John Scillieri and Duncan Murdoch:
if it does happen that you're using a Unix/Linux system, then
the appropriate variant of the following illustration will work
(the 'mail' command should always be present on variants of these
systems):

In the little project I'm working on in R at the moment,
I have variables x1 and x2 (each length-50 numeric vectors).

So I can, in R, do:

  sink(file="myoutput")
  cbind(x1,x2)
  sink()
  system("mail ted -s \"Test No 2\" < myoutput")

which has stored the 2-column output from cbind(x1,x2) in the
file "myoutput", and then used the 'mail' command to mail its
contents to 'ted' with subject 'Test No 2' (I used a multi-word
subject to illustrate the quotes the command line will need,
to avoid splitting the subject into separate tokens).

See ?sink for how to use the sink() command.

Then 'ted' duly received an email with contents:

===========================================
Date: Thu, 12 Jul 2007 17:10:44 +0100
From: Ted Harding <ted at compo.fort.knox.uk>
To: ted at compo.fort.knox.uk
Subject: Test No 2

               x1           x2
 [1,]  0.37282844  0.002743146
 [2,]  0.93293155 -0.108009247
 .......
[49,] -0.08681427  0.828313288
[50,] -0.23621908  0.385269729
===========================================

You can of course encapsulate something like the above sequence
of commands into an R function, depending on how you organise the
storage of the results you want to email.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jul-07                                       Time: 17:25:05
------------------------------ XFMail ------------------------------


From ripley at stats.ox.ac.uk  Thu Jul 12 18:44:35 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jul 2007 17:44:35 +0100 (BST)
Subject: [R] eMail results out of R
In-Reply-To: <4696414C.6030908@stats.uwo.ca>
References: <6699922FABDD9145A5C94569C2B438EBBDE549@MA-EXCL02.konzern.mvvcorp.de>
	<4696414C.6030908@stats.uwo.ca>
Message-ID: <Pine.LNX.4.64.0707121708220.24257@gannet.stats.ox.ac.uk>

On Thu, 12 Jul 2007, Duncan Murdoch wrote:

> On 7/12/2007 9:52 AM, thomas.schwander at mvv.de wrote:
>> Hi everyone,
>>
>> I did my homework and read the posting guideline :-)

But failed to follow it, and not telling us the OS makes this very much 
harder to answer adequately.

>> I want to eMail the results of a computing automatically. So I get the 
>> results (the parameters of a garch process) and I want to eMail them to 
>> another person. How can I do that?
>
>
> This will depend on the system you're using.  If the command "emailit"
> would work from the command line on your system, then
>
> system("emailit")
>
> should work from within R.  Writing that command is the hard part, of
> course.

But bug.report() will give you a good start on command-line oriented 
systems which have mailx (the POSIX mail client).  (mailx is more standard 
than mail that a couple of others have referred to.)

The adventurous could use make.socket and friends to talk to the sendmail 
daemon on a Unix-alike.

Windows is somewhat harder: 'blat' provides a command-line mail client 
that talks to a SMTP server elsewhere.  If you use MS Exchange you will 
need to find other ways to talk to it (DCOM?)

Long ago we talked about have a mailer() function in R, but making one 
that was close to universal proved to be far too difficult for its 
utility.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ted.harding at nessie.mcc.ac.uk  Thu Jul 12 19:03:23 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jul 2007 18:03:23 +0100 (BST)
Subject: [R] eMail results out of R
In-Reply-To: <46965286.7040904@biomserv.univ-lyon1.fr>
Message-ID: <XFMail.070712180323.ted.harding@nessie.mcc.ac.uk>

On 12-Jul-07 16:10:46, St?phane Dray wrote:
> Here is a small function that I used on Debian. It requires exim4 :
> 
> send.mail<-function(addr='dray at biomserv.univ-lyon1.fr',subject='A 
> message from R',
>                     text=paste("I have finished to work 
> ",Sys.time(),coll="")){
>     # send an email
>     # it requires the reconfiguration of exim4
>     # you have to connect as root and
>     # then type dpkg-reconfigure exim4config

I'm a bit puzzled by this. On any Unix/Linux system (unless
something has changed very recently which I haven't heard about),
the 'mail' command simply works, for any user (without having
to become root); does not require exim4 (or any particular version
of any particular mail agent--so long as something has to be set up
so that email can be sent at all), and (for the purpose of using
'mail' from R) does not require exim4 or any other mail agent to
be re-configured. The email will be sent "From:" the user who
is running R.

In the example I posted just now, I just used 'mail' in R's
system() command without doing anything special. The mail transfer
agent in my case is 'sendmail', but it's a standard configuration
and nothing special has been done.



>     mail.cmd<-paste("mail ",
>                     "-s \"",subject,"\" ",
>                     addr,
>                     " << EOT &\n",
>                     text,"\n",
>                     "EOT",
>                     sep="",collapse="")
>      system(mail.cmd,intern=FALSE)
>   }
> 
> Cheers,
> 
> Romain Francois wrote:
>> Hi,
>>
>> There is a paper in the April 2007 issue of R News that might be of
>> help 
>> here.
>> http://##cran mirror##/doc/Rnews/Rnews_2007-1.pdf
>>
>> Romain
>>
>> Duncan Murdoch wrote:
>>   
>>> On 7/12/2007 9:52 AM, thomas.schwander at mvv.de wrote:
>>>   
>>>     
>>>> Hi everyone,
>>>>
>>>> I did my homework and read the posting guideline :-)
>>>>
>>>> I want to eMail the results of a computing automatically. So I get
>>>> the results (the parameters of a garch process) and I want to eMail
>>>> them to another person. How can I do that?
>>>>     
>>>>       
>>> This will depend on the system you're using.  If the command
>>> "emailit" 
>>> would work from the command line on your system, then
>>>
>>> system("emailit")
>>>
>>> should work from within R.  Writing that command is the hard part, of
>>> course.
>>>
>>> Duncan Murdoch
>>>   
>>>     
> 
> 
> -- 
> St?phane DRAY (dray at biomserv.univ-lyon1.fr )
> Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
> 43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
> Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
> http://biomserv.univ-lyon1.fr/~dray/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jul-07                                       Time: 18:03:20
------------------------------ XFMail ------------------------------


From furey at cires.colorado.edu  Thu Jul 12 19:06:41 2007
From: furey at cires.colorado.edu (Peter Furey)
Date: Thu, 12 Jul 2007 11:06:41 -0600
Subject: [R] error problem with glht
Message-ID: <20070712165951.M18857@cires.colorado.edu>>


Can anyone help me? 
I'm having problems with the following code where I want to test the null
hypothesis that regression slopes are the same among regressions. Here's the
code I've written with comments that include the final error I get. ...

initial.dir <- getwd()
library(systemfit)
library(multcomp)

basdata <- read.table("data_into7_test.txt", header=TRUE,sep="")

#show basdata 
basdata

#output from above looks like this ...
#     basin Order       lnLcl      lnArea
#1    SFK     6  3.46322000  5.19766000
#2    SFK     6  3.51767000  5.11809000
#3    SFK     6  3.79962000  5.79118000
#4    SFK     6  2.90242000  4.09768000
#5    SFK     4  1.92408000  2.31617000
# ....
#21   NFK     6  3.88342000  5.50927000
#22   NFK     6  3.84522000  5.13474000
#23   NFK     6  3.49130000  5.28586000
# ....

#linear model fit and analysis of variance
fit_area_1 <- lm(lnArea~Order*basin,data=basdata)
anova_area_1 <- anova(fit_area_1)

#get coefficients, covariance matrix, terms
coef(fit_area_1)
vcov(fit_area_1)
terms(fit_area_1)

#perform multiple comparisons to test null hypothesis that
#linear model fits are the same among basins
fit1_mc <- glht(anova_area_1,linfct=mcp(basin="Tukey"))

#the line above gives ...
#Error in terms.default(object) : no terms component
#Error in factor_contrasts(model) : no model.matrix method for model found!


I appreciate any help offered. 
Cheers,   Pete

------

Peter Furey
Research Scientist
Northwest Research Associates / CORA
3380 Mitchell Lane
Boulder, CO  80301

Also affiliated with:
Cooperative Institute for Research in Environmental Sciences (CIRES)
University of Colorado, Boulder


From andrea.storto at met.no  Thu Jul 12 19:15:19 2007
From: andrea.storto at met.no (Andrea Storto)
Date: Thu, 12 Jul 2007 19:15:19 +0200
Subject: [R] contour and filled contour plots
Message-ID: <469661A7.6080705@met.no>

Hello,

I'm trying to overlap a contour and a filled.contour:
as I read in a previous post this can be done
calling contour() in the plot.axes of filled.contour.

When I do it (see example below) the contour is placed
instead of the color bar, while the filled.contour
is not drawn at all, which is the right way?

Thanks

Andrea



postscript("covtd.xy.eps", width=6.5, height=5.5,
             horizontal=FALSE, onefile=FALSE, paper = "special") 
filled.contour(plot.axes={ contour(seq(1,nlev),seq(1,nlev),real2) },
seq(1,nlev),seq(1,nlev),real2,levels=c(-40,-20,-15,-10,-7.5,-5,-2.5,-1.5,-0.5,0.5,1.5,2.5,5,7.5,10,15,20,40),xlab="Model 
levels",ylab="Model levels",
color = colorRampPalette(c("blue", "white", 
"red")),main=expression(paste("Unit: ",10^-6," ",K%.%s^-1)))
dev.off()


From f.harrell at vanderbilt.edu  Thu Jul 12 19:22:24 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 12 Jul 2007 12:22:24 -0500
Subject: [R] Package for .632 (and .632+) bootstrap and the
 cross-validation of ROC Parameters
In-Reply-To: <11561405.post@talk.nabble.com>
References: <11561405.post@talk.nabble.com>
Message-ID: <46966350.4090202@vanderbilt.edu>

spime wrote:
> 
> Hi users,
> 
> I need to calculate .632 (and .632+) bootstrap and the cross-validation of
> area under curve (AUC) to compare my models. Is there any package for the
> same. I know about 'ipred' and using it i can calculate misclassification
> errors. 
> 
> Please help. It's urgent. 

See the validate* functions in the Design package.

Note that some simulations (see http://biostat.mc.vanderbilt.edu/rms) 
indicate that the advantages of .632 and .632+ over the ordinary 
bootstrap are highly dependent on the choice of the accuracy measure 
being validated.  The bootstrap variants seem to have advantages mainly 
if an improper, inefficient, discontinuous scoring rule such as the 
percent classified correct is used.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From sbearer at tnc.org  Thu Jul 12 19:32:03 2007
From: sbearer at tnc.org (Scott Bearer)
Date: Thu, 12 Jul 2007 13:32:03 -0400
Subject: [R] calculating percent error from 2 vectors
In-Reply-To: <FOEPJOMFPIAGCACENKDMKEJGCEAA.sbearer@tnc.org>
Message-ID: <FOEPJOMFPIAGCACENKDMOEKMCEAA.sbearer@tnc.org>

Hello,

I believe this is an easy scripting problem, but one I am stumbling on.

I have a "known" vector of 3 colors with nrow=10:
known<-c("red", "blue", "red", "red", "yellow", "blue", "yellow", "blue",
"blue", "yellow")

and a model output vector:
modelout<-c("red", "red", "red", "blue", "yellow", "blue", "blue", "red",
"blue", "yellow")

I would like to determine the proportion (in)correctly identified for each
color.  In other words:
% correct "red"=
% correct "blue"=
% correct "yellow"=

How would I code this (assuming the actual dataset is more complex)?

Any help would be much appreciated.

Thank you,
Scott


From rosa.clements at somerville.oxon.net  Thu Jul 12 19:38:50 2007
From: rosa.clements at somerville.oxon.net (rosa clements)
Date: Thu, 12 Jul 2007 18:38:50 +0100 (BST)
Subject: [R] Interpreting a string as a variable in a column header
Message-ID: <38039.192.168.0.1.1184261930.squirrel@webmail.secure.aluminati.net>

This must be a very simple question, but I can't find any information
on it elsewhere, sorry. When extracting information from a list using
column headers, how do I get R to interpret something as a variable
rather than a string? For example:

xx$"YAL002"

works, but this doesn't:

gene <- "YAL002"
xx$gene

neither do

xx$parse(gene)
xx$eval(gene)
xx$eval(parse(gene))

or a variety of other constructions I have tried.

Background: I have a table of information about yeast genes, and I
also have a list of yeast genes and their GO terms (xx) from the YEAST
package that I downloaded. I want to go through all the genes in my
table and look up their GO terms in the list from the YEAST package.
They might not contain exactly the same genes (ideally they should,
but I'd be surprised if they do) and I don't think they're in the same
order, so I do want to use the column names, but there are a lot of
them so I'm not typing them all out individually. It might be possible
to turn the GO data into something other than a list, but the help
page recommends using xx <- as.list(YEASTGO) and doesn't make any
other suggestions, so I should probably do as I'm told.

Thanks for any help or suggestions of where to look,

Rosa


From thelozee_x at yahoo.com  Thu Jul 12 15:45:13 2007
From: thelozee_x at yahoo.com (thelozee_x)
Date: Thu, 12 Jul 2007 06:45:13 -0700 (PDT)
Subject: [R] Generating bivariate or multivariate data with known
 parameter values
In-Reply-To: <45293F81.2080607@education.wisc.edu>
References: <45293F81.2080607@education.wisc.edu>
Message-ID: <11560289.post@talk.nabble.com>




David Kaplan-2 wrote:
> 
> Greetings,
> 
> I'm interested in generating data from various bivariate or 
> mulitivariate distributions (e.g. gamma, t, etc), where I can specify 
> the parameter values, including the correlations among the variables.  I 
> haven't been able to dig anything up on the faq, but I probably missed 
> something.  A nudge in the right direction would be appreciated.
> 
> David
> 
> 
> -- 
> ========================================================================
> David Kaplan, Ph.D.
> Professor
> Department of Educational Psychology
> University of Wisconsin - Madison
> Educational Sciences, Room 1061
> 1025 W. Johnson Street
> Madison, WI 53706
> 
> email: dkaplan at education.wisc.edu
> Web:   http://www.education.wisc.edu/edpsych/facstaff/kaplan/kaplan.htm
> Phone: 608-262-0836
> Fax:   608-262-0843
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

i'm interesting to use multivariate gamma distributed gamma data, in
 copula packages, you say that it can generate bivariate gamma
 distribution, i have try this and succes. my question, 
1. are the copula can be used for generating multivariate gamma
 distributed gamma data?
2. when i try to generating bivariate gamma, can i set the correlation?
 for knowing, i have try many combination of parameters, but the result
 is taht correlation value is about 0.5.

thankyou very much for your responses.

dian
-- 
View this message in context: http://www.nabble.com/-R--Generating-bivariate-or-multivariate-data-with-known-parameter-values-tf2405919.html#a11560289
Sent from the R help mailing list archive at Nabble.com.


From ajay at som.iitb.ac.in  Thu Jul 12 18:28:46 2007
From: ajay at som.iitb.ac.in (Ajay Singh)
Date: Thu, 12 Jul 2007 21:58:46 +0530 (IST)
Subject: [R] multiple plots in a graph
Message-ID: <Pine.LNX.4.61.0707122149350.10257@som.iitb.ac.in>

Hi,
I have to generate 10 cdfs in a graph. I need to compare the cdf's 
nature by plotting ten cdfs in a graph. Thus, I need multiple plots in a 
graph.
I would appreciate if you could give some solution to the problem asap.

Thanking you,

Sincerely,

Ajay.

-- 
Ajay Singh
Research Scientist,
SOM, IIT-Bombay, Powai,
MUMBAI-400076, MH (INDIA).


From thchung at tgen.org  Thu Jul 12 20:06:34 2007
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Thu, 12 Jul 2007 11:06:34 -0700
Subject: [R] Error in dyn.load()
Message-ID: <51F1023E-E74E-49F5-952E-1C301A8AAC35@tgen.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070712/4fd86c2b/attachment.pl 

From ken.williams at thomson.com  Thu Jul 12 20:08:39 2007
From: ken.williams at thomson.com (Ken Williams)
Date: Thu, 12 Jul 2007 13:08:39 -0500
Subject: [R] Compute rank within factor groups
Message-ID: <C2BBD857.10BA7%ken.williams@thomson.com>

Hi,

I have a data.frame which is ordered by score, and has a factor column:

  Browse[1]> wc[c("report","score")]
          report score
  9         ADEA  0.96
  8         ADEA  0.90
  11 Asylum_FED9  0.86
  3         ADEA  0.75
  14 Asylum_FED9  0.60
  5         ADEA  0.56
  13 Asylum_FED9  0.51
  16 Asylum_FED9  0.51
  2         ADEA  0.42
  7         ADEA  0.31
  17 Asylum_FED9  0.27
  1         ADEA  0.17
  4         ADEA  0.17
  6         ADEA  0.12
  10        ADEA  0.11
  12 Asylum_FED9  0.10
  15 Asylum_FED9  0.09
  18 Asylum_FED9  0.07
  Browse[1]> 

I need to add a column indicating rank within each factor group, which I
currently accomplish like so:

  wc$rank <- 0
  for(report in as.character(unique(wc$report))) {
    wc[wc$report==report,]$rank <- 1:sum(wc$report==report)
  }

I have to wonder whether there's a better way, something that gets rid of
the for() loop using tapply() or by() or similar.  But I haven't come up
with anything.

I've tried these:

  by(wc, wc$report, FUN=function(pr){pr$rank <- 1:nrow(pr)})

  by(wc, wc$report, FUN=function(pr){wc[wc$report %in% pr$report,]$rank <-
1:nrow(pr)})

But in both cases the effect of the assignment is lost, there's no $rank
column generated for wc.

Any suggestions?

 -Ken


From h.wickham at gmail.com  Thu Jul 12 20:15:30 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 20:15:30 +0200
Subject: [R] ggplot2 / reshape / Question on manipulating data
In-Reply-To: <87d4yxznue.fsf@coco.kazmier.com>
References: <87ir8qz04e.fsf@coco.kazmier.com>
	<f8e6ff050707120035r5668e93bqf963acc3540c4890@mail.gmail.com>
	<87d4yxznue.fsf@coco.kazmier.com>
Message-ID: <f8e6ff050707121115x55b4d957i7475277ef91661ab@mail.gmail.com>

On 7/12/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
> "hadley wickham" <h.wickham at gmail.com> writes:
>
> > On 7/12/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
> >> I'm an R newbie but recently discovered the ggplot2 and reshape
> >> packages which seem incredibly useful and much easier to use for a
> >> beginner.  Using the data from the IMDB, I'm trying to see how the
> >> average movie rating varies by year.  Here is what my data looks like:
> >>
> >> > ratings <- read.delim("groomed.list", header = TRUE, sep = "|", comment.char = "")
> >> > ratings <- subset(ratings, VoteCount > 100)
> >> > head(ratings)
> >>                              Title  Histogram VoteCount VoteMean Year
> >> 1                !Huff (2004) (TV) 0000000016       299      8.4 2004
> >> 8              'Allo 'Allo! (1982) 0000000125       829      8.6 1982
> >> 50              .hack//SIGN (2002) 0000001113       150      7.0 2002
> >> 56            1-800-Missing (2003) 0000000103       118      5.4 2003
> >> 66  Greatest Artists (2000) (mini) 00..000016       110      7.8 2000
> >> 77 00 Scariest Movie (2004) (mini) 00..000115       256      8.6 2004
> >
> > Have you tried using the movies dataset included in ggplot?  Or is
> > there some data that you want that is not in that dataset.
>
> It's funny that you mention this because I had intended to write this
> email about a month ago but was delayed due to other reasons.  In any
> case, when I was typing this up last night, I wanted to recreate my
> steps but I could not find the IMDB movie data I had used originally.
> I searched everywhere to no avail so I downloaded the data myself and
> groomed it.  Only now do I remember that I had used the movies dataset
> included in ggplot.
>
> >> How do 'byYear' and 'byYear2' differ?  I am trying to use 'typeof' but
> >> both seem to be lists.  However, they are clearly different in some
> >> way because 'qplot' graphs them differently.
> >
> > Try using str - it's much more helpful, and you should see the
> > different quickly.
>
> Thanks!  This is the function I've been looking for in my quest to
> learn about internal data types of R.  Too bad it has such a terrible
> name!
>
> > Using the built in movies data:
> >
> > mm <- melt(movies, id=1:2, m=c("rating", "votes"))
> > msum <- cast(mm, year ~ variable, c(mean, sum))
> >
> > qplot(year, rating_mean, data=msum, colour=votes_sum)
> > qplot(year, rating_mean, data=msum, colour=votes_sum, geom="line")
>
> Great!  This is exactly what I was looking to do.  By the way, does
> any of your documentation use the movie dataset as an example?  I'm
> curious what else I can do with the dataset.  For example, how can I
> use ggplot's facets to see the same information by type of movie?  I'm
> unsure of how to manipulate the binary variables into a single
> variable so that it can be treated as levels.

A lot of the examples do use the movies data, but I don't think any of
it is particularly revealing.  You might want to look at the results
for the 2007 infovis visualisation challenge
(http://www.apl.jhu.edu/Misc/Visualization/) which uses similar data.
Submission isn't complete yet, but you can see my teams entry at
http://had.co.nz/infovis-2007/.  There are lots of interesting stories
to pursue.

I think I will update the movies data to include the first genre as
another column.  That will make it easier to facet by genre

Hadley


From pete-expires-20070910 at kazmier.com  Thu Jul 12 20:43:55 2007
From: pete-expires-20070910 at kazmier.com (Pete Kazmier)
Date: Thu, 12 Jul 2007 14:43:55 -0400
Subject: [R] ggplot2 / histogram / y-axis
Message-ID: <878x9lzclw.fsf@coco.kazmier.com>

Is there a way in ggplot to make a histogram with the left-hand y-axis
label as frequency, and a right-hand y-axis label as percentage?

Thanks!
Pete


From Greg.Snow at intermountainmail.org  Thu Jul 12 20:51:22 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 12 Jul 2007 12:51:22 -0600
Subject: [R] Interpreting a string as a variable in a column header
In-Reply-To: <38039.192.168.0.1.1184261930.squirrel@webmail.secure.aluminati.net>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAAD844@LP-EXCHVS07.CO.IHC.COM>

Use xx[[gene]] instead of xx$gene (the $ is a shorthand for [[ with some
extra magic to be more convenient, the magic is getting in your way, so
go back to the [[ syntax (make sure you double the braces)).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of rosa clements
> Sent: Thursday, July 12, 2007 11:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Interpreting a string as a variable in a column header
> 
> This must be a very simple question, but I can't find any 
> information on it elsewhere, sorry. When extracting 
> information from a list using column headers, how do I get R 
> to interpret something as a variable rather than a string? 
> For example:
> 
> xx$"YAL002"
> 
> works, but this doesn't:
> 
> gene <- "YAL002"
> xx$gene
> 
> neither do
> 
> xx$parse(gene)
> xx$eval(gene)
> xx$eval(parse(gene))
> 
> or a variety of other constructions I have tried.
> 
> Background: I have a table of information about yeast genes, 
> and I also have a list of yeast genes and their GO terms (xx) 
> from the YEAST package that I downloaded. I want to go 
> through all the genes in my table and look up their GO terms 
> in the list from the YEAST package.
> They might not contain exactly the same genes (ideally they 
> should, but I'd be surprised if they do) and I don't think 
> they're in the same order, so I do want to use the column 
> names, but there are a lot of them so I'm not typing them all 
> out individually. It might be possible to turn the GO data 
> into something other than a list, but the help page 
> recommends using xx <- as.list(YEASTGO) and doesn't make any 
> other suggestions, so I should probably do as I'm told.
> 
> Thanks for any help or suggestions of where to look,
> 
> Rosa
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Thu Jul 12 20:53:01 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 12 Jul 2007 12:53:01 -0600
Subject: [R] calculating percent error from 2 vectors
In-Reply-To: <FOEPJOMFPIAGCACENKDMOEKMCEAA.sbearer@tnc.org>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAAD845@LP-EXCHVS07.CO.IHC.COM>

Try something like:

> mytable <- table(known, modelout)
> prop.table( mytable, 1 )

Also look at ?addmargins and the CrossTable function in the gmodels
package.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Scott Bearer
> Sent: Thursday, July 12, 2007 11:32 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] calculating percent error from 2 vectors
> 
> Hello,
> 
> I believe this is an easy scripting problem, but one I am 
> stumbling on.
> 
> I have a "known" vector of 3 colors with nrow=10:
> known<-c("red", "blue", "red", "red", "yellow", "blue", 
> "yellow", "blue", "blue", "yellow")
> 
> and a model output vector:
> modelout<-c("red", "red", "red", "blue", "yellow", "blue", 
> "blue", "red", "blue", "yellow")
> 
> I would like to determine the proportion (in)correctly 
> identified for each color.  In other words:
> % correct "red"=
> % correct "blue"=
> % correct "yellow"=
> 
> How would I code this (assuming the actual dataset is more complex)?
> 
> Any help would be much appreciated.
> 
> Thank you,
> Scott
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Thu Jul 12 20:58:40 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 12 Jul 2007 12:58:40 -0600
Subject: [R] Compute rank within factor groups
In-Reply-To: <C2BBD857.10BA7%ken.williams@thomson.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAAD846@LP-EXCHVS07.CO.IHC.COM>

Look at ?ave and try something like:

> wc$rank <- ave( wc$score, wc$report, FUN=rank )

This works even if the dataframe is not pre sorted.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ken Williams
> Sent: Thursday, July 12, 2007 12:09 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Compute rank within factor groups
> 
> Hi,
> 
> I have a data.frame which is ordered by score, and has a 
> factor column:
> 
>   Browse[1]> wc[c("report","score")]
>           report score
>   9         ADEA  0.96
>   8         ADEA  0.90
>   11 Asylum_FED9  0.86
>   3         ADEA  0.75
>   14 Asylum_FED9  0.60
>   5         ADEA  0.56
>   13 Asylum_FED9  0.51
>   16 Asylum_FED9  0.51
>   2         ADEA  0.42
>   7         ADEA  0.31
>   17 Asylum_FED9  0.27
>   1         ADEA  0.17
>   4         ADEA  0.17
>   6         ADEA  0.12
>   10        ADEA  0.11
>   12 Asylum_FED9  0.10
>   15 Asylum_FED9  0.09
>   18 Asylum_FED9  0.07
>   Browse[1]> 
> 
> I need to add a column indicating rank within each factor 
> group, which I currently accomplish like so:
> 
>   wc$rank <- 0
>   for(report in as.character(unique(wc$report))) {
>     wc[wc$report==report,]$rank <- 1:sum(wc$report==report)
>   }
> 
> I have to wonder whether there's a better way, something that 
> gets rid of the for() loop using tapply() or by() or similar. 
>  But I haven't come up with anything.
> 
> I've tried these:
> 
>   by(wc, wc$report, FUN=function(pr){pr$rank <- 1:nrow(pr)})
> 
>   by(wc, wc$report, FUN=function(pr){wc[wc$report %in% 
> pr$report,]$rank <-
> 1:nrow(pr)})
> 
> But in both cases the effect of the assignment is lost, 
> there's no $rank column generated for wc.
> 
> Any suggestions?
> 
>  -Ken
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ted.harding at nessie.mcc.ac.uk  Thu Jul 12 21:15:37 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 12 Jul 2007 20:15:37 +0100 (BST)
Subject: [R] calculating percent error from 2 vectors
In-Reply-To: <FOEPJOMFPIAGCACENKDMOEKMCEAA.sbearer@tnc.org>
Message-ID: <XFMail.070712201537.ted.harding@nessie.mcc.ac.uk>

On 12-Jul-07 17:32:03, Scott Bearer wrote:
> Hello,
> 
> I believe this is an easy scripting problem, but one I am stumbling on.
> 
> I have a "known" vector of 3 colors with nrow=10:
> known<-c("red", "blue", "red", "red", "yellow", "blue", "yellow",
> "blue",
> "blue", "yellow")
> 
> and a model output vector:
> modelout<-c("red", "red", "red", "blue", "yellow", "blue", "blue",
> "red",
> "blue", "yellow")
> 
> I would like to determine the proportion (in)correctly identified for
> each
> color.  In other words:
> % correct "red"=
> % correct "blue"=
> % correct "yellow"=
> 
> How would I code this (assuming the actual dataset is more complex)?

For your example:

> tbl<-table(known,modelout)

> tbl
        modelout
known    blue red yellow
  blue   2    2   0     
  red    1    2   0     
  yellow 1    0   2     

> dim(tbl)
[1] 3 3

> for(i in (1:dim(tbl)[1])){print(sum(tbl[i,-i])/sum(tbl[i,]))}
[1] 0.5
[1] 0.3333333
[1] 0.3333333

and you can modify the "print" command produce a desired format,
e.g. using rownames(tbl)[i] for the successive colour names.

Hoping this helps (as a start),
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 12-Jul-07                                       Time: 20:15:34
------------------------------ XFMail ------------------------------


From thchung at tgen.org  Thu Jul 12 21:31:11 2007
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Thu, 12 Jul 2007 12:31:11 -0700
Subject: [R] Resolved: Error in dyn.load()
Message-ID: <29FC1581-0020-42CC-9147-72924A86B33E@tgen.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070712/97b0aa02/attachment.pl 

From ripley at stats.ox.ac.uk  Thu Jul 12 21:33:00 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Jul 2007 20:33:00 +0100 (BST)
Subject: [R] Stepwise GLM selection by LRT?
In-Reply-To: <2e38a1c80707120915w3a22c78em5162dd86d2929861@mail.gmail.com>
References: <2e38a1c80707111206l499cc328r6fff2d785fc7719e@mail.gmail.com>
	<002a01c7c3f3$119d01c0$7c94100a@win.ad.jhu.edu>
	<2e38a1c80707120915w3a22c78em5162dd86d2929861@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707121731190.24257@gannet.stats.ox.ac.uk>

On Thu, 12 Jul 2007, Lutz Ph. Breitling wrote:

> Thank you very much for the prompt reply. Seems like I had not fully
> understood what the k-parameter to stepAIC is doing.
> Your suggested approach looks indeed fine to me, actually I do not
> quite understand why you say that it's only an approximation to the
> LRT?

So this is computing AIC_k = -2L + kp.  If you compare models with p and 
p+q parameters, this is equvalent to comparing 2 log LR with kq and so for 
q=1 the Wilks' LRT is found for k = qchisq(1-p, df=1) (which is just a 
squared Normal).

However, no one said q would always be one, and stepAIC steps in terms, 
not individual coefficients.  Therein lies one of the approximations 
(another is in the asympototic distribution theory of the test).


> Best wishes-
> Lutz
>
> On 7/11/07, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
>> Check out the stepAIC function in MASS package.  This is a nice tool, where
>> you can actually implement any penalty even though the function's name has
>> "AIC" in it because it is the default.  Although this doesn't do an LRT test
>> based variable selection, you can sort of approximate it by using a penalty
>> of k = qchisq(1-p, df=1), where p is the p-value for variable selection.
>> This penalty means that a variable enters/exits an existing model, when the
>> absolute value of change in log-likelihood is greater than qchisq(1-p,
>> df=1). For p = 0.1, k = 2.71, and for p=0.05, k = 3.84.  Is this whhant
>> you'd like to do?
>>
>> Ravi.
>>
>> ----------------------------------------------------------------------------
>> -------
>>
>> Ravi Varadhan, Ph.D.
>>
>> Assistant Professor, The Center on Aging and Health
>>
>> Division of Geriatric Medicine and Gerontology
>>
>> Johns Hopkins University
>>
>> Ph: (410) 502-2619
>>
>> Fax: (410) 614-9625
>>
>> Email: rvaradhan at jhmi.edu
>>
>> Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
>>
>>
>>
>> ----------------------------------------------------------------------------
>> --------
>>
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lutz Ph. Breitling
>> Sent: Wednesday, July 11, 2007 3:06 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Stepwise GLM selection by LRT?
>>
>> Dear List,
>>
>> having searched the help and archives, I have the impression that
>> there is no automatic model selection procedure implemented in R that
>> includes/excludes predictors in logistic regression models based on
>> LRT P-values. Is that true, or is someone aware of an appropriate
>> function somewhere in a custom package?
>>
>> Even if automatic model selection and LRT might not be the most
>> appropriate methods, I actually would like to use these in order to
>> simulate someone else's modeling approach...
>>
>> Many thanks for all comments-
>> Lutz
>> -----
>> Lutz Ph. Breitling
>> German Cancer Research Center
>> Heidelberg/Germany
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jholtman at gmail.com  Thu Jul 12 21:34:49 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 12 Jul 2007 15:34:49 -0400
Subject: [R] Compute rank within factor groups
In-Reply-To: <C2BBD857.10BA7%ken.williams@thomson.com>
References: <C2BBD857.10BA7%ken.williams@thomson.com>
Message-ID: <644e1f320707121234w498e965are464688cfa3edc51@mail.gmail.com>

Is this what you are looking for:

> x
        report score
9         ADEA  0.96
8         ADEA  0.90
11 Asylum_FED9  0.86
3         ADEA  0.75
14 Asylum_FED9  0.60
5         ADEA  0.56
13 Asylum_FED9  0.51
16 Asylum_FED9  0.51
2         ADEA  0.42
7         ADEA  0.31
17 Asylum_FED9  0.27
1         ADEA  0.17
4         ADEA  0.17
6         ADEA  0.12
10        ADEA  0.11
12 Asylum_FED9  0.10
15 Asylum_FED9  0.09
18 Asylum_FED9  0.07
> x$rank <- ave(x$score, x$report, FUN=rank)
> x
        report score rank
9         ADEA  0.96 10.0
8         ADEA  0.90  9.0
11 Asylum_FED9  0.86  8.0
3         ADEA  0.75  8.0
14 Asylum_FED9  0.60  7.0
5         ADEA  0.56  7.0
13 Asylum_FED9  0.51  5.5
16 Asylum_FED9  0.51  5.5
2         ADEA  0.42  6.0
7         ADEA  0.31  5.0
17 Asylum_FED9  0.27  4.0
1         ADEA  0.17  3.5
4         ADEA  0.17  3.5
6         ADEA  0.12  2.0
10        ADEA  0.11  1.0
12 Asylum_FED9  0.10  3.0
15 Asylum_FED9  0.09  2.0
18 Asylum_FED9  0.07  1.0
>


On 7/12/07, Ken Williams <ken.williams at thomson.com> wrote:
> Hi,
>
> I have a data.frame which is ordered by score, and has a factor column:
>
>  Browse[1]> wc[c("report","score")]
>          report score
>  9         ADEA  0.96
>  8         ADEA  0.90
>  11 Asylum_FED9  0.86
>  3         ADEA  0.75
>  14 Asylum_FED9  0.60
>  5         ADEA  0.56
>  13 Asylum_FED9  0.51
>  16 Asylum_FED9  0.51
>  2         ADEA  0.42
>  7         ADEA  0.31
>  17 Asylum_FED9  0.27
>  1         ADEA  0.17
>  4         ADEA  0.17
>  6         ADEA  0.12
>  10        ADEA  0.11
>  12 Asylum_FED9  0.10
>  15 Asylum_FED9  0.09
>  18 Asylum_FED9  0.07
>  Browse[1]>
>
> I need to add a column indicating rank within each factor group, which I
> currently accomplish like so:
>
>  wc$rank <- 0
>  for(report in as.character(unique(wc$report))) {
>    wc[wc$report==report,]$rank <- 1:sum(wc$report==report)
>  }
>
> I have to wonder whether there's a better way, something that gets rid of
> the for() loop using tapply() or by() or similar.  But I haven't come up
> with anything.
>
> I've tried these:
>
>  by(wc, wc$report, FUN=function(pr){pr$rank <- 1:nrow(pr)})
>
>  by(wc, wc$report, FUN=function(pr){wc[wc$report %in% pr$report,]$rank <-
> 1:nrow(pr)})
>
> But in both cases the effect of the assignment is lost, there's no $rank
> column generated for wc.
>
> Any suggestions?
>
>  -Ken
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From h.wickham at gmail.com  Thu Jul 12 21:43:11 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 12 Jul 2007 21:43:11 +0200
Subject: [R] ggplot2 / histogram / y-axis
In-Reply-To: <878x9lzclw.fsf@coco.kazmier.com>
References: <878x9lzclw.fsf@coco.kazmier.com>
Message-ID: <f8e6ff050707121243g5edc6588k2efaae628ca3f417@mail.gmail.com>

On 7/12/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
> Is there a way in ggplot to make a histogram with the left-hand y-axis
> label as frequency, and a right-hand y-axis label as percentage?

Not currently.  I did a quick exploration to see if it was feasible to
draw another axis on with grid, but it doesn't look like it's
possible:

p <- qplot(rating, data=movies, geom="histogram")

# Map aesthetics to data
data <- p$layers[[1]]$make_aesthetics(p)
# Calculate statistic "by hand" (we'll need this to get the scales right)
binned <- StatBin$calculate(data=data, p$scales)

n <- nrow(movies)

# Manually recreate the y scale
sp <- scale_y_continuous()
sp$train(binned$count)

# rescale the labels
labels <- formatC(sp$breaks() / n, digits=2)

# Have to do without labels because of bug in grid
print(p, pretty=FALSE)
downViewport("panel_1_1")
grid.draw(ggaxis(sp$breaks(), as.list(labels), "right", sp$frange()))

# Why don't labels line up? - I'm not sure
# How could you make space for the extra axis? - Not sure either
# How would this worked for a facetted graphic - not well


Also how were you expecting the axes/gridlines to line up?  Would both
axes be labelled "nicely" (with whole numbers) and the secondary axis
wouldn't have gridlines; or would the second axis match the lines of
the primary, even though the number wouldn't be so attractive?

Hadley


From attenka at utu.fi  Thu Jul 12 21:46:37 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Thu, 12 Jul 2007 22:46:37 +0300
Subject: [R] is.null doesn't work
Message-ID: <f6ea964a8ffa.4696af4d@utu.fi>

Hi,

What's wrong here?:

> v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",9,"X",2)
> i2=16
> v[i2]
[[1]]
NULL

> is.null(v[i2])
[1] FALSE

Is it a bug or have I misunderstood something?

Atte Tenkanen
University of Turku, Finland


From ligges at statistik.uni-dortmund.de  Thu Jul 12 21:54:42 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 12 Jul 2007 21:54:42 +0200
Subject: [R] multiple plots in a graph
In-Reply-To: <Pine.LNX.4.61.0707122149350.10257@som.iitb.ac.in>
References: <Pine.LNX.4.61.0707122149350.10257@som.iitb.ac.in>
Message-ID: <46968702.90701@statistik.uni-dortmund.de>



Ajay Singh wrote:
> Hi,
> I have to generate 10 cdfs in a graph. I need to compare the cdf's 
> nature by plotting ten cdfs in a graph. Thus, I need multiple plots in a 
> graph.
> I would appreciate if you could give some solution to the problem asap.


  plot(ecdf(rnorm(10)))
  plot(ecdf(rnorm(12)), add=TRUE, col.points="red", col.hor="red")

etc...


Uwe Ligges


> Thanking you,
> 
> Sincerely,
> 
> Ajay.
>


From attenka at utu.fi  Thu Jul 12 22:01:22 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Thu, 12 Jul 2007 23:01:22 +0300
Subject: [R] is.null doesn't work
In-Reply-To: <f6ea964a8ffa.4696af4d@utu.fi>
References: <f6ea964a8ffa.4696af4d@utu.fi>
Message-ID: <fd6fb373ed7d.4696b2c2@utu.fi>


Seems to work, if I unlist the argument at first ;-)

Atte

> Hi,
> 
> What's wrong here?:
> 
> > v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",9,"X",2)
> > i2=16
> > v[i2]
> [[1]]
> NULL
> 
> > is.null(v[i2])
> [1] FALSE
> 
> Is it a bug or have I misunderstood something?
> 
> Atte Tenkanen
> University of Turku, Finland
>


From szhan at uoguelph.ca  Thu Jul 12 22:16:28 2007
From: szhan at uoguelph.ca (szhan at uoguelph.ca)
Date: Thu, 12 Jul 2007 16:16:28 -0400
Subject: [R] how to estimate treatment-interaction contrasts
Message-ID: <20070712161628.h4rqf4j48wwks444@webmail.uoguelph.ca>

Hello, R experts,
Sorry for asking this question again again since I really want a help!

I have a two-factor experiment data and like to calculate estimates of
interation contrasts say factor A has levels of a1, a2, and B has
levels of b1, b2, b3, b4, and b5 with 3 replicates. I am not sure the
constrast estimate I got is right using the script below:

score<-c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)

A <- gl(2, 15, labels=c("a1", "a2"))
B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)

contrasts(B)<-cbind(c(-4,rep(1,4)),c(rep(-3,2),rep(2,3)),
+  c(rep(-2,3),rep(3,2)),c(rep(-1,4), rep(4,1)))
fit1 <- aov(score ~ A*B)
summary(fit1, split=list(B=1:4), expand.split = TRUE)
               Df Sum Sq Mean Sq F value    Pr(>F)
A            1 3.2013  3.2013 15.1483 0.0009054 ***
B            4 8.7780  2.1945 10.3841 0.0001019 ***
     B: C1      1 0.0301  0.0301  0.1424 0.7099296
     B: C2      1 2.0335  2.0335  9.6221 0.0056199 **
     B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
     B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
A:B          4 5.3420  1.3355  6.3194 0.0018616 **
     A:B: C1    1 0.7207  0.7207  3.4105 0.0796342 .
     A:B: C2    1 2.6068  2.6068 12.3350 0.0021927 **
     A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
     A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
Residuals   20 4.2267  0.2113
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Now I like to get interaction contrast estimate for b1 and b2 vs b3, b4 and b5
cont <- c(1, -1)[A] * c(-3, -3, 2, 2, 2)[B]

estimat<-sum(cont*score) # value of the contrast estimate for A:B C2

> estimat
[1] -24.1

I am not sure the estimate for A:B C2 contrast  (-24.1) is correct
because the F value given the output above(12.3350) does not equal to
those I calculate below (15.2684):

t.stat <- sum(cont*score)/se.contrast(fit1, as.matrix(cont))
> t.stat^2
Contrast 1
     15.2684

Could you please help me calculate the correct the estimate of
interaction contrast and corresponding F value?
Thanks in advance!
Joshua


From pete-expires-20070910 at kazmier.com  Thu Jul 12 22:36:32 2007
From: pete-expires-20070910 at kazmier.com (Pete Kazmier)
Date: Thu, 12 Jul 2007 16:36:32 -0400
Subject: [R] ggplot2 / histogram / y-axis
References: <878x9lzclw.fsf@coco.kazmier.com>
	<f8e6ff050707121243g5edc6588k2efaae628ca3f417@mail.gmail.com>
Message-ID: <874pk9z7e7.fsf@coco.kazmier.com>

"hadley wickham" <h.wickham at gmail.com> writes:

> On 7/12/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
>> Is there a way in ggplot to make a histogram with the left-hand y-axis
>> label as frequency, and a right-hand y-axis label as percentage?
>
> Not currently.  I did a quick exploration to see if it was feasible to
> draw another axis on with grid, but it doesn't look like it's
> possible:

Thank you for trying.

> Also how were you expecting the axes/gridlines to line up?  Would both
> axes be labelled "nicely" (with whole numbers) and the secondary axis
> wouldn't have gridlines; or would the second axis match the lines of
> the primary, even though the number wouldn't be so attractive?

I hadn't thought that far ahead.  Depending on the audience, I render
histograms differently, and was curious if I could just put both on a
single graph.  However, you bring up some interesting questions in
terms of the presentation.

On another note, and feel free to defer me to the documentation which
I'm still in the process of reading, but will I be able to take
advantage of some of Tufte's recommendations in terms of the typical
histogram and/or scatterplots (pp126-134 in Visual Display of
Quantitative Information)?

For example, with histograms, he would eliminates the use of
coordinate lines in favor of using a white grid to improve the
data/ink ratio.  Likewise in scatterplots, he uses range-frames and
dot-dash-plots.  Will I be able to use ggplot for these types of
enhancements?  

Thanks,
Pete


From Greg.Snow at intermountainmail.org  Thu Jul 12 23:28:25 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 12 Jul 2007 15:28:25 -0600
Subject: [R] Compute rank within factor groups
In-Reply-To: <C2BBFE1D.10BD3%ken.williams@thomson.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAAD88F@LP-EXCHVS07.CO.IHC.COM>

Why are you using order instead of rank?

If the data is pre sorted then they tend to give the same result (unless
there are ties), but if your data is not presorted, then the results
will be different.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: Ken Williams [mailto:ken.williams at thomson.com] 
> Sent: Thursday, July 12, 2007 2:50 PM
> To: Greg Snow; R-help at stat.math.ethz.ch
> Subject: Re: [R] Compute rank within factor groups
> 
> 
> 
> 
> On 7/12/07 3:42 PM, "Ken Williams" <ken.williams at thomson.com> wrote:
> 
> > I ended up using:
> > 
> >  wc$rank <- ave( wc$score, wc$report,
> >                  FUN=function(x) order(x, decreasing=TRUE) )
> > 
> > Which gives me the 1-based rank integers I was looking for.
> 
> Of course, immediately after sending I realized a simpler way:
> 
>   wc$rank <- ave( -wc$score, wc$report, FUN=order )
> 
> And as a newbie I think I get to be blissfully ignorant of 
> which one is faster. =)
> 
> 
> --
> Ken Williams
> Research Scientist
> The Thomson Corporation
> Eagan, MN
> 
>


From chuckanut at gmail.com  Fri Jul 13 01:04:42 2007
From: chuckanut at gmail.com (chuckanut)
Date: Thu, 12 Jul 2007 16:04:42 -0700
Subject: [R] JRI problem on 64bit Linux
Message-ID: <412ad0330707121604l33e247ddh1e55a492868067f1@mail.gmail.com>

Sorry if this isn't the correct list, but I couldn't find any mailing
lists on the JRI site.  I am able to install JRI on a 32 bin linux
machine without any problem, but I am unable to isntall on a 64 bit
machine.  I have R installed with the correct option, R_HOME is set up
and libR.so is in there.  However, I get the following error when
running "./configure", any help is appreciated

checking build system type... x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for java... /usr/java/jdk1.5.0_11/bin/java
checking for javac... /usr/java/jdk1.5.0_11/bin/javac
checking for javah... /usr/java/jdk1.5.0_11/bin/javah
checking for jar... /usr/java/jdk1.5.0_11/bin/jar
checking whether Java interpreter works... yes
checking for Java environment... in /usr/java/jdk1.5.0_11
checking for /usr/java/jdk1.5.0_11/include/jni.h... yes
checking for /usr/java/jdk1.5.0_11/include/./jni_md.h... no
checking for /usr/java/jdk1.5.0_11/include/linux/jni_md.h... yes
checking whether JNI programs can be compiled... configure: error:
Cannot compile a simple JNI program. See config.log for details.


From p.dalgaard at biostat.ku.dk  Fri Jul 13 01:10:41 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 13 Jul 2007 01:10:41 +0200
Subject: [R] Compute rank within factor groups
In-Reply-To: <C2BBD857.10BA7%ken.williams@thomson.com>
References: <C2BBD857.10BA7%ken.williams@thomson.com>
Message-ID: <4696B4F1.1000802@biostat.ku.dk>

Ken Williams wrote:
> Hi,
>
> I have a data.frame which is ordered by score, and has a factor column:
>
>   Browse[1]> wc[c("report","score")]
>           report score
>   9         ADEA  0.96
>   8         ADEA  0.90
>   11 Asylum_FED9  0.86
>   3         ADEA  0.75
>   14 Asylum_FED9  0.60
>   5         ADEA  0.56
>   13 Asylum_FED9  0.51
>   16 Asylum_FED9  0.51
>   2         ADEA  0.42
>   7         ADEA  0.31
>   17 Asylum_FED9  0.27
>   1         ADEA  0.17
>   4         ADEA  0.17
>   6         ADEA  0.12
>   10        ADEA  0.11
>   12 Asylum_FED9  0.10
>   15 Asylum_FED9  0.09
>   18 Asylum_FED9  0.07
>   Browse[1]> 
>
> I need to add a column indicating rank within each factor group, which I
> currently accomplish like so:
>
>   wc$rank <- 0
>   for(report in as.character(unique(wc$report))) {
>     wc[wc$report==report,]$rank <- 1:sum(wc$report==report)
>   }
>
> I have to wonder whether there's a better way, something that gets rid of
> the for() loop using tapply() or by() or similar.  But I haven't come up
> with anything.
>
> I've tried these:
>
>   by(wc, wc$report, FUN=function(pr){pr$rank <- 1:nrow(pr)})
>
>   by(wc, wc$report, FUN=function(pr){wc[wc$report %in% pr$report,]$rank <-
> 1:nrow(pr)})
>
> But in both cases the effect of the assignment is lost, there's no $rank
> column generated for wc.
>
> Any suggestions?
>   
There's a little known and somewhat unfortunately named function called 
ave() which does just that sort of thing.

 > ave(wc$score, wc$report, FUN=rank)
 [1] 10.0  9.0  8.0  8.0  7.0  7.0  5.5  5.5  6.0  5.0  4.0  3.5  3.5  
2.0  1.0
[16]  3.0  2.0  1.0


From michael.drescher at ontario.ca  Fri Jul 13 01:13:16 2007
From: michael.drescher at ontario.ca (Drescher, Michael (MNR))
Date: Thu, 12 Jul 2007 19:13:16 -0400
Subject: [R] (no subject)
Message-ID: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>

Hi All,

I want to automatically generate a number of data frames, each with an
automatically generated name and an automatically generated number of
rows. The number of rows has been calculated before and is different for
all data frames (e.g. c(4,5,2)). The number of columns is known a priori
and the same for all data frames (e.g. c(3,3,3)). The resulting data
frames could look something like this:

> auto.data.1
  X1 X2 X3
1  0  0  0
2  0  0  0
3  0  0  0
4  0  0  0

> auto.data.2
  X1 X2 X3
1  0  0  0
2  0  0  0
3  0  0  0
4  0  0  0
5  0  0  0

> auto.data.3
  X1 X2 X3
1  0  0  0
2  0  0  0

Later, I want to fill the elements of the data frames with values read
from somewhere else, automatically looping through the previously
generated data frames.

I know that I can automatically generate variables with the right number
of elements with something like this:

> auto.length <- c(12,15,6)
> for(i in 1:3) {
+ nam <- paste("auto.data",i, sep=".")
+ assign(nam, 1:auto.length[i])
+ }
> auto.data.1
 [1]  1  2  3  4  5  6  7  8  9 10 11 12
> auto.data.2
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
> auto.data.3
[1]  1 2 3 4 5 6

But how do I turn these variables into data frames or give them any
dimensions? Any commands such as 'as.matrix', 'data.frame', or 'dim' do
not seem to work. I also seem not to be able to access the variables
with something like "auto.data.i" since:

> auto.data.i
Error: object "auto.data.i" not found

Thus, how would I be able to automatically write to the elements of the
data frames later in a loop such as ...

> for(i in 1:3) {
+ for(j in 1:nrow(auto.data.i)) {	### this obviously does not work
since 'Error in nrow(auto.data.i) : object "auto.data.i" not found'
+ for(k in 1:ncol(auto.data.i)) {
+ auto.data.i[j,k] <- 'some value'
+ }}}

Thanks a bunch for all your help.

Best, Michael


Michael Drescher
Ontario Forest Research Institute
Ontario Ministry of Natural Resources
1235 Queen St East
Sault Ste Marie, ON, P6A 2E3
Tel: (705) 946-7406
Fax: (705) 946-2030


From michael.drescher at ontario.ca  Fri Jul 13 01:19:26 2007
From: michael.drescher at ontario.ca (Drescher, Michael (MNR))
Date: Thu, 12 Jul 2007 19:19:26 -0400
Subject: [R] automatically generating and accessing data frames of varying
	dimensions
In-Reply-To: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
References: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <76D2AA307C39054DBA8BD42DE44E71A403140A16@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>

Hi All,

I want to automatically generate a number of data frames, each with an
automatically generated name and an automatically generated number of
rows. The number of rows has been calculated before and is different for
all data frames (e.g. c(4,5,2)). The number of columns is known a priori
and the same for all data frames (e.g. c(3,3,3)). The resulting data
frames could look something like this:

> auto.data.1
  X1 X2 X3
1  0  0  0
2  0  0  0
3  0  0  0
4  0  0  0

> auto.data.2
  X1 X2 X3
1  0  0  0
2  0  0  0
3  0  0  0
4  0  0  0
5  0  0  0

> auto.data.3
  X1 X2 X3
1  0  0  0
2  0  0  0

Later, I want to fill the elements of the data frames with values read
from somewhere else, automatically looping through the previously
generated data frames.

I know that I can automatically generate variables with the right number
of elements with something like this:

> auto.length <- c(12,15,6)
> for(i in 1:3) {
+ nam <- paste("auto.data",i, sep=".")
+ assign(nam, 1:auto.length[i])
+ }
> auto.data.1
 [1]  1  2  3  4  5  6  7  8  9 10 11 12
> auto.data.2
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
> auto.data.3
[1]  1 2 3 4 5 6

But how do I turn these variables into data frames or give them any
dimensions? Any commands such as 'as.matrix', 'data.frame', or 'dim' do
not seem to work. I also seem not to be able to access the variables
with something like "auto.data.i" since:

> auto.data.i
Error: object "auto.data.i" not found

Thus, how would I be able to automatically write to the elements of the
data frames later in a loop such as ...

> for(i in 1:3) {
+ for(j in 1:nrow(auto.data.i)) {	### this obviously does not work
since 'Error in nrow(auto.data.i) : object "auto.data.i" not found'
+ for(k in 1:ncol(auto.data.i)) {
+ auto.data.i[j,k] <- 'some value'
+ }}}

Thanks a bunch for all your help.

Best, Michael


Michael Drescher
Ontario Forest Research Institute
Ontario Ministry of Natural Resources
1235 Queen St East
Sault Ste Marie, ON, P6A 2E3
Tel: (705) 946-7406
Fax: (705) 946-2030


From topkatz at msn.com  Fri Jul 13 01:23:25 2007
From: topkatz at msn.com (Talbot Katz)
Date: Thu, 12 Jul 2007 19:23:25 -0400
Subject: [R] sub-function default arguments
Message-ID: <BAY108-F24DC7A5B1F6E6A01D5042DAAFC0@phx.gbl>

Hi.

I have defined a function, f1, that calls another function, f2.  Inside f1 
an intermediate variable called nm1 is created; it is a matrix.  f2 takes a 
matrix argument, and I defined f2 (schematically) as follows:

f2<-function(nmArg1=nm1,...){nC<-ncol(nmArg1); ... }

so that it expects nm1 as the default value of its argument.  f1 is defined 
(schematically) as:

f1<-function(...){result1<-f2(); ... }

When I ran f1 I got the following error message:

Error in ncol(nmArg1) : object "nm1" not found.

If I redefine f1 schematically as:

f1<-function(...){result1<-f2(nmArg1=nm1); ... }

it runs okay.  If I have nm1 defined outside of f1 and I run "result1<-f2()" 
it also runs okay.  So f2 doesn't seem to pick up the default argument value 
inside the function f1, even when the default argument is defined inside f1. 
  Is there a way to have the subfunction default arguments recognized inside 
of a function (perhaps a better protocol for using defaults than what I 
did?), or do I just have to spell them out explicitly?

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From bates at stat.wisc.edu  Fri Jul 13 01:44:04 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 12 Jul 2007 18:44:04 -0500
Subject: [R] is.null doesn't work
In-Reply-To: <fd6fb373ed7d.4696b2c2@utu.fi>
References: <f6ea964a8ffa.4696af4d@utu.fi> <fd6fb373ed7d.4696b2c2@utu.fi>
Message-ID: <40e66e0b0707121644l29f8a21dx822a63a7d4ab7ecd@mail.gmail.com>

On 7/12/07, Atte Tenkanen <attenka at utu.fi> wrote:

> Seems to work, if I unlist the argument at first ;-)

> Atte

> > Hi,

> > What's wrong here?:

> > > v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",9,"X",2)
> > > i2=16
> > > v[i2]
> > [[1]]
> > NULL

> > > is.null(v[i2])
> > [1] FALSE

> > Is it a bug or have I misunderstood something?

v[2] is a list with a single element which happens to be NULL.
v[[2]], on the other hand, is NULL.

A subset of a list, obtained with "[", is a list.  An element of a
list, obtained with "[[", is the native type of that element.

> > Atte Tenkanen
> > University of Turku, Finland
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jmburgos at u.washington.edu  Fri Jul 13 01:40:36 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Thu, 12 Jul 2007 16:40:36 -0700
Subject: [R] automatically generating and accessing data frames of
 varying dimensions
In-Reply-To: <76D2AA307C39054DBA8BD42DE44E71A403140A16@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
References: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
	<76D2AA307C39054DBA8BD42DE44E71A403140A16@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <4696BBF4.9070802@u.washington.edu>

Replace

assign(nam, 1:auto.length[i])

with


assign(nam,data.frame(matrix(1:auto.length[i], ncol=3)))

Verify if the matrix (and hence the data frame) is filled the way you 
intended.  Otherwise, use the argument byrow=T.

Julian M. Burgos

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science
University of Washington


Drescher, Michael (MNR) wrote:
> Hi All,
>
> I want to automatically generate a number of data frames, each with an
> automatically generated name and an automatically generated number of
> rows. The number of rows has been calculated before and is different for
> all data frames (e.g. c(4,5,2)). The number of columns is known a priori
> and the same for all data frames (e.g. c(3,3,3)). The resulting data
> frames could look something like this:
>
>> auto.data.1
>   X1 X2 X3
> 1  0  0  0
> 2  0  0  0
> 3  0  0  0
> 4  0  0  0
>
>> auto.data.2
>   X1 X2 X3
> 1  0  0  0
> 2  0  0  0
> 3  0  0  0
> 4  0  0  0
> 5  0  0  0
>
>> auto.data.3
>   X1 X2 X3
> 1  0  0  0
> 2  0  0  0
>
> Later, I want to fill the elements of the data frames with values read
> from somewhere else, automatically looping through the previously
> generated data frames.
>
> I know that I can automatically generate variables with the right number
> of elements with something like this:
>
>> auto.length <- c(12,15,6)
>> for(i in 1:3) {
> + nam <- paste("auto.data",i, sep=".")
> + assign(nam, 1:auto.length[i])
> + }
>> auto.data.1
>  [1]  1  2  3  4  5  6  7  8  9 10 11 12
>> auto.data.2
>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
>> auto.data.3
> [1]  1 2 3 4 5 6
>
> But how do I turn these variables into data frames or give them any
> dimensions? Any commands such as 'as.matrix', 'data.frame', or 'dim' do
> not seem to work. I also seem not to be able to access the variables
> with something like "auto.data.i" since:
>
>> auto.data.i
> Error: object "auto.data.i" not found
>
> Thus, how would I be able to automatically write to the elements of the
> data frames later in a loop such as ...
>
>> for(i in 1:3) {
> + for(j in 1:nrow(auto.data.i)) {	### this obviously does not work
> since 'Error in nrow(auto.data.i) : object "auto.data.i" not found'
> + for(k in 1:ncol(auto.data.i)) {
> + auto.data.i[j,k] <- 'some value'
> + }}}
>
> Thanks a bunch for all your help.
>
> Best, Michael
>
>
> Michael Drescher
> Ontario Forest Research Institute
> Ontario Ministry of Natural Resources
> 1235 Queen St East
> Sault Ste Marie, ON, P6A 2E3
> Tel: (705) 946-7406
> Fax: (705) 946-2030
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch at stats.uwo.ca  Fri Jul 13 02:05:50 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 12 Jul 2007 20:05:50 -0400
Subject: [R] sub-function default arguments
In-Reply-To: <BAY108-F24DC7A5B1F6E6A01D5042DAAFC0@phx.gbl>
References: <BAY108-F24DC7A5B1F6E6A01D5042DAAFC0@phx.gbl>
Message-ID: <4696C1DE.7030800@stats.uwo.ca>

On 12/07/2007 7:23 PM, Talbot Katz wrote:
> Hi.
> 
> I have defined a function, f1, that calls another function, f2.  Inside f1 
> an intermediate variable called nm1 is created; it is a matrix.  f2 takes a 
> matrix argument, and I defined f2 (schematically) as follows:
> 
> f2<-function(nmArg1=nm1,...){nC<-ncol(nmArg1); ... }
> 
> so that it expects nm1 as the default value of its argument.  f1 is defined 
> (schematically) as:
> 
> f1<-function(...){result1<-f2(); ... }
> 
> When I ran f1 I got the following error message:
> 
> Error in ncol(nmArg1) : object "nm1" not found.
> 
> If I redefine f1 schematically as:
> 
> f1<-function(...){result1<-f2(nmArg1=nm1); ... }
> 
> it runs okay.  If I have nm1 defined outside of f1 and I run "result1<-f2()" 
> it also runs okay.  So f2 doesn't seem to pick up the default argument value 
> inside the function f1, even when the default argument is defined inside f1. 
>   Is there a way to have the subfunction default arguments recognized inside 
> of a function (perhaps a better protocol for using defaults than what I 
> did?), or do I just have to spell them out explicitly?

Defaults to arguments are evaluated in the evaluation frame of the 
function being called, f2 in your case.  Since nm1 is meaningless within 
f2, you get the error.

If you want nm1 to be meaningful within f2, you could define f2 within 
f1, e.g.

f1<-function(...){
   nm1 <- something
   f2 <- function (...) {}
   result1<-f2(nmArg1=nm1)
   ...
}

(which is the best way to do it), or you could explicitly manipulate the 
environment of f2 (which is an ugly way), or you could store nm1 in some 
place that's visible to both f1 and f2 and use <<- when you set it from 
within f1 (another ugly way, but sometimes less ugly than my second 
suggestion).

Duncan Murdoch


From ggrothendieck at gmail.com  Fri Jul 13 03:28:50 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 12 Jul 2007 21:28:50 -0400
Subject: [R] automatically generating and accessing data frames of
	varying dimensions
In-Reply-To: <76D2AA307C39054DBA8BD42DE44E71A403140A16@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
References: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
	<76D2AA307C39054DBA8BD42DE44E71A403140A16@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <971536df0707121828t60ebaf8dj29f43ccd87215229@mail.gmail.com>

Its desirable to put the data frames into a list so they can be easily
iterated over in the future.  Try this:

auto.length <- 2:4 # replace with desired row lengths
f <- function(names, nr) {
	x <- rep(0, nr)
	data.frame(X1 = x, X2 = x, X3 = x)
}
auto.names <- paste("auto.data", seq_along(auto.length), sep = ".")
mapply(f, auto.names, auto.length, SIMPLIFY = FALSE)


On 7/12/07, Drescher, Michael (MNR) <michael.drescher at ontario.ca> wrote:
> Hi All,
>
> I want to automatically generate a number of data frames, each with an
> automatically generated name and an automatically generated number of
> rows. The number of rows has been calculated before and is different for
> all data frames (e.g. c(4,5,2)). The number of columns is known a priori
> and the same for all data frames (e.g. c(3,3,3)). The resulting data
> frames could look something like this:
>
> > auto.data.1
>  X1 X2 X3
> 1  0  0  0
> 2  0  0  0
> 3  0  0  0
> 4  0  0  0
>
> > auto.data.2
>  X1 X2 X3
> 1  0  0  0
> 2  0  0  0
> 3  0  0  0
> 4  0  0  0
> 5  0  0  0
>
> > auto.data.3
>  X1 X2 X3
> 1  0  0  0
> 2  0  0  0
>
> Later, I want to fill the elements of the data frames with values read
> from somewhere else, automatically looping through the previously
> generated data frames.
>
> I know that I can automatically generate variables with the right number
> of elements with something like this:
>
> > auto.length <- c(12,15,6)
> > for(i in 1:3) {
> + nam <- paste("auto.data",i, sep=".")
> + assign(nam, 1:auto.length[i])
> + }
> > auto.data.1
>  [1]  1  2  3  4  5  6  7  8  9 10 11 12
> > auto.data.2
>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
> > auto.data.3
> [1]  1 2 3 4 5 6
>
> But how do I turn these variables into data frames or give them any
> dimensions? Any commands such as 'as.matrix', 'data.frame', or 'dim' do
> not seem to work. I also seem not to be able to access the variables
> with something like "auto.data.i" since:
>
> > auto.data.i
> Error: object "auto.data.i" not found
>
> Thus, how would I be able to automatically write to the elements of the
> data frames later in a loop such as ...
>
> > for(i in 1:3) {
> + for(j in 1:nrow(auto.data.i)) {       ### this obviously does not work
> since 'Error in nrow(auto.data.i) : object "auto.data.i" not found'
> + for(k in 1:ncol(auto.data.i)) {
> + auto.data.i[j,k] <- 'some value'
> + }}}
>
> Thanks a bunch for all your help.
>
> Best, Michael
>
>
> Michael Drescher
> Ontario Forest Research Institute
> Ontario Ministry of Natural Resources
> 1235 Queen St East
> Sault Ste Marie, ON, P6A 2E3
> Tel: (705) 946-7406
> Fax: (705) 946-2030
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Fri Jul 13 03:47:46 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 12 Jul 2007 21:47:46 -0400
Subject: [R] is.null doesn't work
In-Reply-To: <f6ea964a8ffa.4696af4d@utu.fi>
References: <f6ea964a8ffa.4696af4d@utu.fi>
Message-ID: <644e1f320707121847y166ec8a0q5faebe52ccae390d@mail.gmail.com>

'v' appears to be a list:

>  v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",9,"X",2)
>  i2=16
>  v[i2]
[[1]]
NULL

> str(v)
List of 11
 $ :function (e1, e2)
 $ :function (e1, e2)
 $ : num 1
 $ :function (e1, e2)
 $ :function (e1, e2)
 $ : logi NA
 $ : logi NA
 $ : chr "X"
 $ : num 9
 $ : chr "X"
 $ : num 2

because you used backquotes(`) on the '-'; notice the difference:

> str(c(`-`,1))
List of 2
 $ :function (e1, e2)
 $ : num 1
> str(c('-',1))
 chr [1:2] "-" "1"
>




On 7/12/07, Atte Tenkanen <attenka at utu.fi> wrote:
> Hi,
>
> What's wrong here?:
>
> > v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",9,"X",2)
> > i2=16
> > v[i2]
> [[1]]
> NULL
>
> > is.null(v[i2])
> [1] FALSE
>
> Is it a bug or have I misunderstood something?
>
> Atte Tenkanen
> University of Turku, Finland
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Fri Jul 13 03:55:44 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 12 Jul 2007 21:55:44 -0400
Subject: [R] (no subject)
In-Reply-To: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
References: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <644e1f320707121855p48ebe22fw9a25494f0e835e12@mail.gmail.com>

Is this what you want to do:

> auto.length <- c(12,15,6)
> for(i in 1:3) {
+ nam <- paste("auto.data",i, sep=".")
+ assign(nam, as.data.frame(matrix(1:auto.length[i], ncol=3)))
+ }
> auto.data.1
  V1 V2 V3
1  1  5  9
2  2  6 10
3  3  7 11
4  4  8 12
> auto.data.2
  V1 V2 V3
1  1  6 11
2  2  7 12
3  3  8 13
4  4  9 14
5  5 10 15
> # output the data
> for(i in 1:3){
+     cat(x <- paste('auto.data.', i, sep=''), '\n')
+     print(get(x))
+ }
auto.data.1
  V1 V2 V3
1  1  5  9
2  2  6 10
3  3  7 11
4  4  8 12
auto.data.2
  V1 V2 V3
1  1  6 11
2  2  7 12
3  3  8 13
4  4  9 14
5  5 10 15
auto.data.3
  V1 V2 V3
1  1  3  5
2  2  4  6
>


On 7/12/07, Drescher, Michael (MNR) <michael.drescher at ontario.ca> wrote:
> Hi All,
>
> I want to automatically generate a number of data frames, each with an
> automatically generated name and an automatically generated number of
> rows. The number of rows has been calculated before and is different for
> all data frames (e.g. c(4,5,2)). The number of columns is known a priori
> and the same for all data frames (e.g. c(3,3,3)). The resulting data
> frames could look something like this:
>
> > auto.data.1
>  X1 X2 X3
> 1  0  0  0
> 2  0  0  0
> 3  0  0  0
> 4  0  0  0
>
> > auto.data.2
>  X1 X2 X3
> 1  0  0  0
> 2  0  0  0
> 3  0  0  0
> 4  0  0  0
> 5  0  0  0
>
> > auto.data.3
>  X1 X2 X3
> 1  0  0  0
> 2  0  0  0
>
> Later, I want to fill the elements of the data frames with values read
> from somewhere else, automatically looping through the previously
> generated data frames.
>
> I know that I can automatically generate variables with the right number
> of elements with something like this:
>
> > auto.length <- c(12,15,6)
> > for(i in 1:3) {
> + nam <- paste("auto.data",i, sep=".")
> + assign(nam, 1:auto.length[i])
> + }
> > auto.data.1
>  [1]  1  2  3  4  5  6  7  8  9 10 11 12
> > auto.data.2
>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
> > auto.data.3
> [1]  1 2 3 4 5 6
>
> But how do I turn these variables into data frames or give them any
> dimensions? Any commands such as 'as.matrix', 'data.frame', or 'dim' do
> not seem to work. I also seem not to be able to access the variables
> with something like "auto.data.i" since:
>
> > auto.data.i
> Error: object "auto.data.i" not found
>
> Thus, how would I be able to automatically write to the elements of the
> data frames later in a loop such as ...
>
> > for(i in 1:3) {
> + for(j in 1:nrow(auto.data.i)) {       ### this obviously does not work
> since 'Error in nrow(auto.data.i) : object "auto.data.i" not found'
> + for(k in 1:ncol(auto.data.i)) {
> + auto.data.i[j,k] <- 'some value'
> + }}}
>
> Thanks a bunch for all your help.
>
> Best, Michael
>
>
> Michael Drescher
> Ontario Forest Research Institute
> Ontario Ministry of Natural Resources
> 1235 Queen St East
> Sault Ste Marie, ON, P6A 2E3
> Tel: (705) 946-7406
> Fax: (705) 946-2030
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From edd at debian.org  Fri Jul 13 04:25:15 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 12 Jul 2007 21:25:15 -0500
Subject: [R] R and HTTP get 'has file changed'
Message-ID: <18070.57995.858839.650331@basebud.nulle.part>


Is there a way, maybe using Duncan TL's RCurl, to efficiently test whether
an URL such as 

	http://$CRAN/src/contrib/ 

has changed?  I.e. one way is via a test of a page in that directory as per
(sorry about the long line, and this would be on Linux with links and awk
installed)

   > strptime(system("links -width 160 -dump http://cran.r-project.org/src/contrib/ | awk '/PACKAGES.html/ {print $3,$4}\'", intern=TRUE), "%d-%b-%Y %H:%M")
   [1] "2007-07-12 18:16:00"
   > 

and one can then compare the POSIXt with a cached value --- but requesting
the header would presumably be more efficient.

Is there are way to request the 'has changed' part of the http 1.1 spe
directly in R?

Thanks, Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From sfalcon at fhcrc.org  Fri Jul 13 04:46:59 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 12 Jul 2007 19:46:59 -0700
Subject: [R] R and HTTP get 'has file changed'
In-Reply-To: <18070.57995.858839.650331@basebud.nulle.part> (Dirk
	Eddelbuettel's message of "Thu, 12 Jul 2007 21:25:15 -0500")
References: <18070.57995.858839.650331@basebud.nulle.part>
Message-ID: <m21wfdvx3w.fsf@ziti.fhcrc.org>

Hi Dirk,

Dirk Eddelbuettel <edd at debian.org> writes:
> Is there a way, maybe using Duncan TL's RCurl, to efficiently test whether
> an URL such as 
>
> 	http://$CRAN/src/contrib/ 
>
> has changed?  I.e. one way is via a test of a page in that directory as per
> (sorry about the long line, and this would be on Linux with links and awk
> installed)
>
>    > strptime(system("links -width 160 -dump http://cran.r-project.org/src/contrib/ | awk '/PACKAGES.html/ {print $3,$4}\'", intern=TRUE), "%d-%b-%Y %H:%M")
>    [1] "2007-07-12 18:16:00"
>    > 
>
> and one can then compare the POSIXt with a cached value --- but requesting
> the header would presumably be more efficient.
>
> Is there are way to request the 'has changed' part of the http 1.1 spe
> directly in R?

Here's a way to use RCurl obtain HTTP headers:

        h <- basicTextGatherer()
        junk <- getURI(url, writeheader=h$update, header=TRUE, nobody=TRUE)
        h <- h$value()

If you want to check many URLs, I think you will find the following
much faster as opposed to looping the above:

        h <- multiTextGatherer(urls)
        junk <- getURIAsynchronous(urls, write=h, header=TRUE, nobody=TRUE)
        yourInfo <- sapply(h, function(x) something(x$value()))

I've used this in the pkgDepTools package to retrieve package download
sizes.

Cheers,

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From edd at debian.org  Fri Jul 13 05:04:18 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 12 Jul 2007 22:04:18 -0500
Subject: [R] R and HTTP get 'has file changed'
In-Reply-To: <m21wfdvx3w.fsf@ziti.fhcrc.org>
References: <18070.57995.858839.650331@basebud.nulle.part>
	<m21wfdvx3w.fsf@ziti.fhcrc.org>
Message-ID: <18070.60338.794960.774499@basebud.nulle.part>


On 12 July 2007 at 19:46, Seth Falcon wrote:
| Hi Dirk,
| 
| Dirk Eddelbuettel <edd at debian.org> writes:
| > Is there a way, maybe using Duncan TL's RCurl, to efficiently test whether
| > an URL such as 
| >
| > 	http://$CRAN/src/contrib/ 
| >
| > has changed?  I.e. one way is via a test of a page in that directory as per
| > (sorry about the long line, and this would be on Linux with links and awk
| > installed)
| >
| >    > strptime(system("links -width 160 -dump http://cran.r-project.org/src/contrib/ | awk '/PACKAGES.html/ {print $3,$4}\'", intern=TRUE), "%d-%b-%Y %H:%M")
| >    [1] "2007-07-12 18:16:00"
| >    > 
| >
| > and one can then compare the POSIXt with a cached value --- but requesting
| > the header would presumably be more efficient.
| >
| > Is there are way to request the 'has changed' part of the http 1.1 spe
| > directly in R?
| 
| Here's a way to use RCurl obtain HTTP headers:
| 
|         h <- basicTextGatherer()
|         junk <- getURI(url, writeheader=h$update, header=TRUE, nobody=TRUE)
|         h <- h$value()

Sweet:

> library(RCurl)
> h <- basicTextGatherer()
> junk <- getURI("http://cran.r-project.org/src/contrib/PACKAGES.html", writeheader=h$update, header=TRUE, nobody=TRUE)
> h <- h$value()
> h
[1] "HTTP/1.1 200 OK\r\nDate: Fri, 13 Jul 2007 02:58:03 GMT\r\nServer: Apache/2.2.3 (Debian)\r\nLast-Modified: Thu, 12 Jul 2007 16:16:08 GMT\r\nETag: \"a7c11e-21f34-4fe68200\"\r\nAccept-Ranges: bytes\r\nContent-Length: 139060\r\nContent-Type: text/html\r\n\r\n"
> 

So I can just filter the Date and Last-Modified fields from here, without
having to worry the particular header request. Nice!

| If you want to check many URLs, I think you will find the following

I don't. I just want something 'light and easy' as the script (to feed
CRANberries) may get run a few times from crontan and should stop early if
no new data will be there to be processed.

Thanks!

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From jvverkuilen at gmail.com  Fri Jul 13 05:29:12 2007
From: jvverkuilen at gmail.com (JVVerkuilen)
Date: Thu, 12 Jul 2007 22:29:12 -0500
Subject: [R] imposing constraints on the covariance matrix of random effects
	in lme4?
Message-ID: <9aba10be0707122029h7ffa05a1w57d7ed951997c4ee@mail.gmail.com>

Hello all,

I am using lme4 to fit some mixed logistic regressions. I need to
impose an identification constraint of the following form:

(1        sig12)
(sig12  sig22)

and have not figured out how to do it, i.e., sig11 = 1 but the rest of
the parameters are free to vary. Is this possible and, if so, how?

I've been looking through the archive and help to no avail, but
perhaps I'm just missing something.

Thanks for any help,

Jay
-- 
JVVerkuilen. PhD
jvverkuilen at gmail.com

"If you've been playing poker for half an hour and you still don't
know who the patsy is, you're the patsy." --Warren Buffett


From luka.lezaic at kclj.si  Fri Jul 13 05:49:26 2007
From: luka.lezaic at kclj.si (Luka =?iso-8859-2?Q?Le=BEai=E6?=)
Date: Fri, 13 Jul 2007 05:49:26 +0200 (CEST)
Subject: [R] ICC
Message-ID: <25052.194.165.96.227.1184298566.squirrel@destination.kclj.si>

Hello all.

I'm working on a PhD thesis analyzing reproducibility/repeatability of a
nuclear medicine examination. I have two sets of data (50 patients each)
and 3 observers. In each case (patient), two variables are evaluated,
first with a range of 0..100 and second 1..3. Each case is also evaluated
twice by each observer (for additional intraobserver reproducibility).

I'm using R package psy to calculate ICC (the other method I'm using is
repeatability). My question with ICC is: how do I evaluate if the
difference between observers (and patient groups) is statistically
significant?

I have been searching through the R archives, but I haven't been able to
find an answer.

Thank you,

Luka Lezaic


-- 
Nobody is perfect. My name is nobody.


-------------------------------------------------------------
This mail was scanned by BitDefender
For more informations please visit http://www.bitdefender.com


From Bill.Venables at csiro.au  Fri Jul 13 07:00:53 2007
From: Bill.Venables at csiro.au (Bill.Venables at csiro.au)
Date: Fri, 13 Jul 2007 15:00:53 +1000
Subject: [R] automatically generating and accessing data frames of
	varyingdimensions
References: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
	<76D2AA307C39054DBA8BD42DE44E71A403140A16@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <B998A44C8986644EA8029CFE6396A924B6808D@exqld2-bne.nexus.csiro.au>

I'm not sure why you want to do it this way (it would probably help if
we had a more complete picture of what you were really trying to do, but
here are a few possibilities for the questions you ask.

1. generating data frames.

rw <- c(4,5,2)
cl <- c(3,3,3)

for(i in 1:length(rw))
	assign(paste("auto.data", i, sep="."),
		as.data.frame(array(0, dim=c(rw[i], cl[i]), 
			dimnames = list(NULL, paste("X", 1:cl[i],
sep="")))))

check: 

> auto.data.1
  X1 X2 X3
1  0  0  0
2  0  0  0
3  0  0  0
4  0  0  0
> auto.data.2
  X1 X2 X3
1  0  0  0
2  0  0  0
3  0  0  0
4  0  0  0
5  0  0  0
> auto.data.3
  X1 X2 X3
1  0  0  0
2  0  0  0

2. filling them up (... are you sure you want to do it this way?)

The simplest way is probably through an intermediary

for(nam in paste("auto.data", 1:3, sep=".")) { # loop over the names
  tmp <- get(nam)
  for(i in 1:nrow(tmp))
	for(j in 1:ncol(tmp))
	  tmp[i, j] <- i+j-i*j # 'some value'
  assign(nam, tmp)
  rm(tmp)
}

check:

> auto.data.1
  X1 X2 X3
1  1  1  1
2  1  0 -1
3  1 -1 -3
4  1 -2 -5
> auto.data.2
  X1 X2 X3
1  1  1  1
2  1  0 -1
3  1 -1 -3
4  1 -2 -5
5  1 -3 -7
> auto.data.3
  X1 X2 X3
1  1  1  1
2  1  0 -1
> 

It may work, but I have to say, though, I'm almost sure this is a
mistake.  There has to be a better way using the facilities that R
provides for avoiding heavy loops like this.

Just a hunch...

Bill Venables
CSIRO Laboratories
PO Box 120, Cleveland, 4163
AUSTRALIA
Office Phone (email preferred): +61 7 3826 7251
Fax (if absolutely necessary):  +61 7 3826 7304
Mobile:                         +61 4 8819 4402
Home Phone:                     +61 7 3286 7700
mailto:Bill.Venables at csiro.au
http://www.cmis.csiro.au/bill.venables/ 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Drescher, Michael
(MNR)
Sent: Friday, 13 July 2007 9:19 AM
To: r-help at stat.math.ethz.ch
Subject: [R] automatically generating and accessing data frames of
varyingdimensions

Hi All,

I want to automatically generate a number of data frames, each with an
automatically generated name and an automatically generated number of
rows. The number of rows has been calculated before and is different for
all data frames (e.g. c(4,5,2)). The number of columns is known a priori
and the same for all data frames (e.g. c(3,3,3)). The resulting data
frames could look something like this:

> auto.data.1
  X1 X2 X3
1  0  0  0
2  0  0  0
3  0  0  0
4  0  0  0

> auto.data.2
  X1 X2 X3
1  0  0  0
2  0  0  0
3  0  0  0
4  0  0  0
5  0  0  0

> auto.data.3
  X1 X2 X3
1  0  0  0
2  0  0  0

Later, I want to fill the elements of the data frames with values read
from somewhere else, automatically looping through the previously
generated data frames.

I know that I can automatically generate variables with the right number
of elements with something like this:

> auto.length <- c(12,15,6)
> for(i in 1:3) {
+ nam <- paste("auto.data",i, sep=".")
+ assign(nam, 1:auto.length[i])
+ }
> auto.data.1
 [1]  1  2  3  4  5  6  7  8  9 10 11 12
> auto.data.2
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
> auto.data.3
[1]  1 2 3 4 5 6

But how do I turn these variables into data frames or give them any
dimensions? Any commands such as 'as.matrix', 'data.frame', or 'dim' do
not seem to work. I also seem not to be able to access the variables
with something like "auto.data.i" since:

> auto.data.i
Error: object "auto.data.i" not found

Thus, how would I be able to automatically write to the elements of the
data frames later in a loop such as ...

> for(i in 1:3) {
+ for(j in 1:nrow(auto.data.i)) {	### this obviously does not work
since 'Error in nrow(auto.data.i) : object "auto.data.i" not found'
+ for(k in 1:ncol(auto.data.i)) {
+ auto.data.i[j,k] <- 'some value'
+ }}}

Thanks a bunch for all your help.

Best, Michael


Michael Drescher
Ontario Forest Research Institute
Ontario Ministry of Natural Resources
1235 Queen St East
Sault Ste Marie, ON, P6A 2E3
Tel: (705) 946-7406
Fax: (705) 946-2030

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Fri Jul 13 07:34:55 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jul 2007 06:34:55 +0100 (BST)
Subject: [R] JRI problem on 64bit Linux
In-Reply-To: <412ad0330707121604l33e247ddh1e55a492868067f1@mail.gmail.com>
References: <412ad0330707121604l33e247ddh1e55a492868067f1@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707130628270.1185@gannet.stats.ox.ac.uk>

Our experience (and that of several others) is that you need Java 1.6 on 
x86_64 Linux for any of the JNI-using R packages to work correctly.  E.g. 
the CRAN check machine is using 1.6.

On Thu, 12 Jul 2007, chuckanut wrote:

> Sorry if this isn't the correct list, but I couldn't find any mailing
> lists on the JRI site.

Please see the posting guide: that asks you to ask the maintainer (who 
knows about the problems).

> I am able to install JRI on a 32 bin linux
> machine without any problem, but I am unable to isntall on a 64 bit
> machine.  I have R installed with the correct option, R_HOME is set up
> and libR.so is in there.  However, I get the following error when
> running "./configure", any help is appreciated

You need to read config.log: we can't read it for you as it is on your 
machine.

> checking build system type... x86_64-unknown-linux-gnu
> checking host system type... x86_64-unknown-linux-gnu
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for java... /usr/java/jdk1.5.0_11/bin/java
> checking for javac... /usr/java/jdk1.5.0_11/bin/javac
> checking for javah... /usr/java/jdk1.5.0_11/bin/javah
> checking for jar... /usr/java/jdk1.5.0_11/bin/jar
> checking whether Java interpreter works... yes
> checking for Java environment... in /usr/java/jdk1.5.0_11
> checking for /usr/java/jdk1.5.0_11/include/jni.h... yes
> checking for /usr/java/jdk1.5.0_11/include/./jni_md.h... no
> checking for /usr/java/jdk1.5.0_11/include/linux/jni_md.h... yes
> checking whether JNI programs can be compiled... configure: error:
> Cannot compile a simple JNI program. See config.log for details.
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ajay at som.iitb.ac.in  Fri Jul 13 08:01:00 2007
From: ajay at som.iitb.ac.in (Ajay Singh)
Date: Fri, 13 Jul 2007 11:31:00 +0530 (IST)
Subject: [R] legend and x,y cordinate values
In-Reply-To: <Pine.LNX.4.61.0707122149350.10257@som.iitb.ac.in>
References: <Pine.LNX.4.61.0707122149350.10257@som.iitb.ac.in>
Message-ID: <Pine.LNX.4.61.0707131113440.14489@som.iitb.ac.in>

Hi,

I have two problems in R.

1. I need 10 cdfs on a graph, the graph needs to have legend. Can you let 
me know how to get legend on the graph?

2. In ecdf plot, I need to know the x and y co-ordinates. I have to get 
corresponding y coordinate values to x coordinate value so that I could be 
able to know the particular percentile value to the x-coordinate value. 
Can you let me know how could I be able the corresponding values of x to 
the y coordinates?

Thanking you,
Looking forward to your kind response,
Sincerely,
Ajay.
-- 
Ajay Singh
Research Scientist,
SOM, IIT-Bombay, Powai,
MUMBAI-400076, MH (INDIA).


From h.wickham at gmail.com  Fri Jul 13 08:12:46 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 13 Jul 2007 08:12:46 +0200
Subject: [R] ggplot2 / histogram / y-axis
In-Reply-To: <874pk9z7e7.fsf@coco.kazmier.com>
References: <878x9lzclw.fsf@coco.kazmier.com>
	<f8e6ff050707121243g5edc6588k2efaae628ca3f417@mail.gmail.com>
	<874pk9z7e7.fsf@coco.kazmier.com>
Message-ID: <f8e6ff050707122312m11557fb6h3d8e6d28effdbac9@mail.gmail.com>

On 7/12/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
> "hadley wickham" <h.wickham at gmail.com> writes:
>
> > On 7/12/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
> >> Is there a way in ggplot to make a histogram with the left-hand y-axis
> >> label as frequency, and a right-hand y-axis label as percentage?
> >
> > Not currently.  I did a quick exploration to see if it was feasible to
> > draw another axis on with grid, but it doesn't look like it's
> > possible:
>
> Thank you for trying.
>
> > Also how were you expecting the axes/gridlines to line up?  Would both
> > axes be labelled "nicely" (with whole numbers) and the secondary axis
> > wouldn't have gridlines; or would the second axis match the lines of
> > the primary, even though the number wouldn't be so attractive?
>
> I hadn't thought that far ahead.  Depending on the audience, I render
> histograms differently, and was curious if I could just put both on a
> single graph.  However, you bring up some interesting questions in
> terms of the presentation.
>
> On another note, and feel free to defer me to the documentation which
> I'm still in the process of reading, but will I be able to take
> advantage of some of Tufte's recommendations in terms of the typical
> histogram and/or scatterplots (pp126-134 in Visual Display of
> Quantitative Information)?
>
> For example, with histograms, he would eliminates the use of
> coordinate lines in favor of using a white grid to improve the
> data/ink ratio.  Likewise in scatterplots, he uses range-frames and
> dot-dash-plots.  Will I be able to use ggplot for these types of
> enhancements?

I am familiar with Tufte's suggestions, and while they do increase the
data-ink ratio, I'm not confident they actually make the plot any
better perceptually.  Displaying grid lines on _top_ of data seems
like a bad idea, and throwing away the plot frame is a bad idea
because you loose important visual reference points.  Range frames
also fail to scale to facetted plots.

If you're not already familiar with them, I strongly recommend the
following two papers which tacke similar ideas to Tufte but in a
rigourous scientific framework:

@article{cleveland:1987,
	Author = {Cleveland, William and McGill, Robert},
	Journal = {Journal of the Royal Statistical Society. Series A (General)},
	Number = {3},
	Pages = {192-229},
	Title = {Graphical Perception: The Visual Decoding of Quantitative
Information on Graphical Displays of Data},
	Volume = {150},
	Year = {1987}}

@article{cleveland:1993a,
	Author = {Cleveland, William},
	Journal = {Journal of Computational and Graphical Statistics},
	Pages = {323-364},
	Title = {A model for studying display methods of statistical graphics},
	Volume = {2},
	Year = {1993}}

Hadley


From pgoninstat1 at yahoo.com  Fri Jul 13 08:34:05 2007
From: pgoninstat1 at yahoo.com (Patrick Gonin)
Date: Thu, 12 Jul 2007 23:34:05 -0700 (PDT)
Subject: [R] RODBC Access
Message-ID: <840331.80479.qm@web45002.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070712/407173d0/attachment.pl 

From my2167 at columbia.edu  Fri Jul 13 01:47:39 2007
From: my2167 at columbia.edu (Masanao Yajima)
Date: Thu, 12 Jul 2007 19:47:39 -0400
Subject: [R] mix package causes R to crash
In-Reply-To: <46942DE0.2080904@stat.columbia.edu>
References: <7A1521B8-5747-48DE-85DC-E6CF84847023@duke.edu>
	<46942DE0.2080904@stat.columbia.edu>
Message-ID: <1184284059.4696bd9b9fab8@cubmail.cc.columbia.edu>

Dear Professor Schaefer

I am experiencing a technical difficulty with your mix package.
I would appreciate it if you could help me with this problem.

When I run the following code, R 2.5.1 and R 2.6.0 crashes.
It's been tested on at least 2 windows machine and it is consistent.
Execution code it's self was coped from the help file of imp.mix.
Only thing I supplied was a fake dataset.

################################################################
    library(mix)
    n <-100
    x1<-rnorm(n)
    x2<-rnorm(n,2,1.2)
    x3<-rnorm(n,1,2)
    x4<-floor(rnorm(n)*3)
    y <-rnorm(n,1*x1+2*x2+3*x3+4*x4,2)
    w <-rnorm(n,3,1.2)
    ymis<-y
    ymis[floor(runif(10,1,n))]<-NA
    wmis<-w
    wmis[floor(runif(10,1,n))]<-NA
    dat<-as.data.frame(cbind(wmis,ymis,x1,x2,x3,x4))
     s <- prelim.mix(dat,3)    # do preliminary manipulations
     thetahat <- em.mix(s)   # ML estimate for unrestricted model
     rngseed(1234567)     # set random number generator seed
     newtheta <- da.mix(s,thetahat,steps=100) # data augmentation
     ximp <- imp.mix(s, newtheta, dat)  # impute under newtheta
################################################################

Your mix package is important part of our ongoing research on the
missing data project and we would like to have it working.
If you could point out to me what I am doing wrong or
some technical difficulty that I am not aware of it will be highly
appreciated.

Thank you for your help in advance.

Sincerely

Masanao Yajima
my2167 at columbia.edu


From mathankumar.ssr at gmail.com  Fri Jul 13 08:51:13 2007
From: mathankumar.ssr at gmail.com (mathan kumar)
Date: Fri, 13 Jul 2007 12:21:13 +0530
Subject: [R] errors using the caMassClass package
Message-ID: <2f3725740707122351wf824b81u4d68c8acdb8a49f8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/7c6d9fab/attachment.pl 

From amirhendi at yahoo.com  Fri Jul 13 09:37:07 2007
From: amirhendi at yahoo.com (Amir_17)
Date: Fri, 13 Jul 2007 00:37:07 -0700 (PDT)
Subject: [R] standardization
Message-ID: <575862.8768.qm@web37911.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/b9511c5a/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jul 13 09:37:46 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jul 2007 08:37:46 +0100 (BST)
Subject: [R] RODBC Access
In-Reply-To: <840331.80479.qm@web45002.mail.sp1.yahoo.com>
References: <840331.80479.qm@web45002.mail.sp1.yahoo.com>
Message-ID: <Pine.LNX.4.64.0707130835440.15162@gannet.stats.ox.ac.uk>

This is an ODBC issue, not an R issue.  Use a command-line ODBC client to 
find a suitable SQL invocation to retrieve the data, and run that via
sqlQuery.

(It might be a quoting issue: Access is not using standard SQL.)


On Thu, 12 Jul 2007, Patrick Gonin wrote:

> Dear R users,
>
> I am trying to connect an Access database.
> The code I use is :
> channel<-odbcConnectAccess("Base")
> table.db<-sqlFetch(Base,"table",colnames=T)
>
> When I look at the table in R, the table imported lacks the first 41 
> lines, and the column names are the values of the 42nd record.
>
> Any hints ?
>
> I am working on a WinXP pro 5.1 box, R 2.5-0 compiled from source using MINGW and R tools.
> I searched R archives through Jonathan Baron site...
>
> Thank you very much for your help
>
> Dr P. Gonin
> Institut Gustave Roussy
> 94805 Villejuif
> France
>
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dray at biomserv.univ-lyon1.fr  Fri Jul 13 09:59:17 2007
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Fri, 13 Jul 2007 09:59:17 +0200
Subject: [R] eMail results out of R
In-Reply-To: <XFMail.070712180323.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070712180323.ted.harding@nessie.mcc.ac.uk>
Message-ID: <469730D5.30201@biomserv.univ-lyon1.fr>

I did it a long time ago, so I do not remember why I have to use 
exim4... sorry. mail did not work alone.. that is why I use exim4.  
Perhaps a simpler solution exists.

Cheers

(Ted Harding) wrote:
> On 12-Jul-07 16:10:46, St?phane Dray wrote:
>   
>> Here is a small function that I used on Debian. It requires exim4 :
>>
>> send.mail<-function(addr='dray at biomserv.univ-lyon1.fr',subject='A 
>> message from R',
>>                     text=paste("I have finished to work 
>> ",Sys.time(),coll="")){
>>     # send an email
>>     # it requires the reconfiguration of exim4
>>     # you have to connect as root and
>>     # then type dpkg-reconfigure exim4config
>>     
>
> I'm a bit puzzled by this. On any Unix/Linux system (unless
> something has changed very recently which I haven't heard about),
> the 'mail' command simply works, for any user (without having
> to become root); does not require exim4 (or any particular version
> of any particular mail agent--so long as something has to be set up
> so that email can be sent at all), and (for the purpose of using
> 'mail' from R) does not require exim4 or any other mail agent to
> be re-configured. The email will be sent "From:" the user who
> is running R.
>
> In the example I posted just now, I just used 'mail' in R's
> system() command without doing anything special. The mail transfer
> agent in my case is 'sendmail', but it's a standard configuration
> and nothing special has been done.
>
>
>
>   
>>     mail.cmd<-paste("mail ",
>>                     "-s \"",subject,"\" ",
>>                     addr,
>>                     " << EOT &\n",
>>                     text,"\n",
>>                     "EOT",
>>                     sep="",collapse="")
>>      system(mail.cmd,intern=FALSE)
>>   }
>>
>> Cheers,
>>
>> Romain Francois wrote:
>>     
>>> Hi,
>>>
>>> There is a paper in the April 2007 issue of R News that might be of
>>> help 
>>> here.
>>> http://##cran mirror##/doc/Rnews/Rnews_2007-1.pdf
>>>
>>> Romain
>>>
>>> Duncan Murdoch wrote:
>>>   
>>>       
>>>> On 7/12/2007 9:52 AM, thomas.schwander at mvv.de wrote:
>>>>   
>>>>     
>>>>         
>>>>> Hi everyone,
>>>>>
>>>>> I did my homework and read the posting guideline :-)
>>>>>
>>>>> I want to eMail the results of a computing automatically. So I get
>>>>> the results (the parameters of a garch process) and I want to eMail
>>>>> them to another person. How can I do that?
>>>>>     
>>>>>       
>>>>>           
>>>> This will depend on the system you're using.  If the command
>>>> "emailit" 
>>>> would work from the command line on your system, then
>>>>
>>>> system("emailit")
>>>>
>>>> should work from within R.  Writing that command is the hard part, of
>>>> course.
>>>>
>>>> Duncan Murdoch
>>>>   
>>>>     
>>>>         
>> -- 
>> St?phane DRAY (dray at biomserv.univ-lyon1.fr )
>> Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
>> 43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
>> Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
>> http://biomserv.univ-lyon1.fr/~dray/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>     
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 12-Jul-07                                       Time: 18:03:20
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>   


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/


From pgoninstat1 at yahoo.com  Fri Jul 13 10:32:59 2007
From: pgoninstat1 at yahoo.com (Patrick Gonin)
Date: Fri, 13 Jul 2007 01:32:59 -0700 (PDT)
Subject: [R] RODBC Access
In-Reply-To: <Pine.LNX.4.64.0707130835440.15162@gannet.stats.ox.ac.uk>
Message-ID: <793121.86152.qm@web45010.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/b41bb12a/attachment.pl 

From dgvirtual at akl.lt  Fri Jul 13 10:50:43 2007
From: dgvirtual at akl.lt (Donatas G.)
Date: Fri, 13 Jul 2007 11:50:43 +0300
Subject: [R] charset in graphics
Message-ID: <200707131150.44106.dgvirtual@akl.lt>

How do I make Lithuanian characters display correctly in R graphics? 

Instead of the special characters for Lithuanian language I get question 
marks...

I use Ubuntu Feisty, the locale is utf-8 ...

Do I need to specify somewhere the locale for R, or - default font for the 
graphics?
-- 
Donatas Glodenis
http://dg.lapas.info


From mothsailor at googlemail.com  Fri Jul 13 10:57:51 2007
From: mothsailor at googlemail.com (David Barron)
Date: Fri, 13 Jul 2007 09:57:51 +0100
Subject: [R] standardization
In-Reply-To: <575862.8768.qm@web37911.mail.mud.yahoo.com>
References: <575862.8768.qm@web37911.mail.mud.yahoo.com>
Message-ID: <815b70590707130157m170f950csca843134188bdfeb@mail.gmail.com>

Try having a look at the scale and sweep functions.

David

On 13/07/07, Amir_17 <amirhendi at yahoo.com> wrote:
> Hi
>   I have dataframe which contain 5 columns and 1000 records. I want standard each cell.
>   I want range each column  between 0 and 1 . I think i must use loop?
>   could you help me?
>
>
> ---------------------------------
> Moody friends. Drama queens. Your life? Nope! - their life, your story.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From ted.harding at nessie.mcc.ac.uk  Fri Jul 13 11:11:43 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 13 Jul 2007 10:11:43 +0100 (BST)
Subject: [R] mix package causes R to crash
In-Reply-To: <1184284059.4696bd9b9fab8@cubmail.cc.columbia.edu>
Message-ID: <XFMail.070713095056.ted.harding@nessie.mcc.ac.uk>

On 12-Jul-07 23:47:39, Masanao Yajima wrote:
> Dear Professor Schaefer
> 
> I am experiencing a technical difficulty with your mix package.
> I would appreciate it if you could help me with this problem.
> 
> When I run the following code, R 2.5.1 and R 2.6.0 crashes.
> It's been tested on at least 2 windows machine and it is consistent.
> Execution code it's self was coped from the help file of imp.mix.
> Only thing I supplied was a fake dataset.
> 

There are several things wrong with your approach to this.

1. The mix package is for imputation when the data consist
   of variables of two kinds: one or more continuous variables
   (which will be modelled as normally distributed); and one
   or more discrete (categorical) variables. The levels of
   the latter must be represented in the data as conscutive
   integers starting at 1. In your data, all the variables
   have been generated as normally distributed, and hence
   not integers, except for x4 which has been converted to
   integer from a Normal sample using floor(). However, this
   generates negative integers, which are not acceptable to
   the mix package as levels of categorical variables.

2. The categorical variables must be the first columns in the
   data matrix. Your construction has made x4 (the only
   possible candidate) the last column; in any case your
   first 5 columns are continuous variables.

3. The data presented to mix must be a matrix, not a dataframe.

4. You invoke prelim.mix as prelim.mix(dat,3), which makes it
   treat the first 3 columns as the categorical variables,
   which they are not.

It is your "fake dataset" which is causing the trouble.
All the points above are covered in the documentation for
the functions in the mix package.

Hoping this helps,
Ted.

>################################################################
>     library(mix)
>     n <-100
>     x1<-rnorm(n)
>     x2<-rnorm(n,2,1.2)
>     x3<-rnorm(n,1,2)
>     x4<-floor(rnorm(n)*3)
>     y <-rnorm(n,1*x1+2*x2+3*x3+4*x4,2)
>     w <-rnorm(n,3,1.2)
>     ymis<-y
>     ymis[floor(runif(10,1,n))]<-NA
>     wmis<-w
>     wmis[floor(runif(10,1,n))]<-NA
>     dat<-as.data.frame(cbind(wmis,ymis,x1,x2,x3,x4))
>      s <- prelim.mix(dat,3)    # do preliminary manipulations
>      thetahat <- em.mix(s)   # ML estimate for unrestricted model
>      rngseed(1234567)     # set random number generator seed
>      newtheta <- da.mix(s,thetahat,steps=100) # data augmentation
>      ximp <- imp.mix(s, newtheta, dat)  # impute under newtheta
>################################################################
> 
> Your mix package is important part of our ongoing research on the
> missing data project and we would like to have it working.
> If you could point out to me what I am doing wrong or
> some technical difficulty that I am not aware of it will be highly
> appreciated.
> 
> Thank you for your help in advance.
> 
> Sincerely
> 
> Masanao Yajima
> my2167 at columbia.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jul-07                                       Time: 09:50:52
------------------------------ XFMail ------------------------------

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jul-07                                       Time: 10:11:39
------------------------------ XFMail ------------------------------


From ripley at stats.ox.ac.uk  Fri Jul 13 11:12:55 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jul 2007 10:12:55 +0100 (BST)
Subject: [R] charset in graphics
In-Reply-To: <200707131150.44106.dgvirtual@akl.lt>
References: <200707131150.44106.dgvirtual@akl.lt>
Message-ID: <Pine.LNX.4.64.0707131006430.6873@gannet.stats.ox.ac.uk>

On Fri, 13 Jul 2007, Donatas G. wrote:

> How do I make Lithuanian characters display correctly in R graphics?
>
> Instead of the special characters for Lithuanian language I get question
> marks...
>
> I use Ubuntu Feisty, the locale is utf-8 ...
>
> Do I need to specify somewhere the locale for R, or - default font for the
> graphics?

See ?X11, especially the 'Fonts' section (assuming that is the graphics 
device you are using: you did not actually say but that is the device 
most likely to be giving question marks or empty rectangles for missing 
glyphs).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gustaf.rydevik at gmail.com  Fri Jul 13 11:17:10 2007
From: gustaf.rydevik at gmail.com (Gustaf Rydevik)
Date: Fri, 13 Jul 2007 11:17:10 +0200
Subject: [R] The "$" operator and vectors
Message-ID: <45f568c70707130217l30da100dk305ede3ddfb4ff79@mail.gmail.com>

Hi all,

I've run into a slightly illogical (to me) behaviour with the "$"
subsetting function.

consider:
> Test
  A B
1 1 Q
2 2 R

> Test$A
[1] 1 2

> vector<-"A"
> Test$vector
NULL

> Test$"A"
[1] 1 2

> Test[,vector]
[1] 1 2


Is there a reason for the $ operator not evaluating the vector before executing?

best,

Gustaf


-- 
Gustaf Rydevik, M.Sci.
tel: +46(0)703 051 451 /+46(0)8 760 52 93
address: C/O Rydevik, Syrenv. 12, 17851 Eker?, SE
skype:gustaf_rydevik


From ripley at stats.ox.ac.uk  Fri Jul 13 11:20:10 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jul 2007 10:20:10 +0100 (BST)
Subject: [R] standardization
In-Reply-To: <815b70590707130157m170f950csca843134188bdfeb@mail.gmail.com>
References: <575862.8768.qm@web37911.mail.mud.yahoo.com>
	<815b70590707130157m170f950csca843134188bdfeb@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707131013210.6873@gannet.stats.ox.ac.uk>

On Fri, 13 Jul 2007, David Barron wrote:

> Try having a look at the scale and sweep functions.

sweep applies to arrays, not data frames, and scale converts to a matrix.

For a data frame

df2 <- df1
df2[] <- lapply(df1, function(x) {r <- range(x, na.rm=TRUE);
                                   (x-r[1])/diff(r)})

seems simple enough.

> On 13/07/07, Amir_17 <amirhendi at yahoo.com> wrote:
>> Hi
>>   I have dataframe which contain 5 columns and 1000 records. I want standard each cell.
>>   I want range each column  between 0 and 1 . I think i must use loop?
>>   could you help me?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tkeitt at gmail.com  Fri Jul 13 11:36:18 2007
From: tkeitt at gmail.com (THK)
Date: Fri, 13 Jul 2007 09:36:18 +0000 (UTC)
Subject: [R] The
References: <45f568c70707130217l30da100dk305ede3ddfb4ff79@mail.gmail.com>
Message-ID: <loom.20070713T112558-317@post.gmane.org>

Gustaf Rydevik <gustaf.rydevik <at> gmail.com> writes:
 
> Is there a reason for the $ operator not evaluating
> the vector before executing?

Yes. Consider:

> x <- list(a = 1, b = 'n')
> a <- 'b'
> x[[a]] == 'n'
[1] TRUE
> x[['a']] == 1
[1] TRUE
> x$a == ???

If the result is 'n', then all bare $ references would require
quotes to avoid name collisions.

THK

> 
> best,
> 
> Gustaf
>


From xkneifl at mendelu.cz  Fri Jul 13 11:58:08 2007
From: xkneifl at mendelu.cz (Michal Kneifl)
Date: Fri, 13 Jul 2007 11:58:08 +0200
Subject: [R] RODBC Access
References: <793121.86152.qm@web45010.mail.sp1.yahoo.com>
Message-ID: <000801c7c534$50bd8f40$a64bb2c3@Mistr1>

Please could you write an example of this command: 
table.db<-sqlQuery(channel,paste("select * from table"))
With real names of .mdb file and table?
Thanks

Michael
----- Original Message ----- 
From: "Patrick Gonin" <pgoninstat1 at yahoo.com>
To: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, July 13, 2007 10:32 AM
Subject: Re: [R] RODBC Access


> Thank you Pr Ripley, it worked well with a simple sqlQuery rather than a 
> sqlFetch command:
> I paste it here in case some other users encounter a similar problem:
> table.db<-sqlQuery(channel,paste("select * from table"))
>
> Dr Patrick Gonin
> Institut Gustave Roussy
> 94800 Villejuif
> France
>
> Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote: This is an ODBC issue, 
> not an R issue.  Use a command-line ODBC client to
> find a suitable SQL invocation to retrieve the data, and run that via
> sqlQuery.
>
> (It might be a quoting issue: Access is not using standard SQL.)
>
>
> On Thu, 12 Jul 2007, Patrick Gonin wrote:
>
>> Dear R users,
>>
>> I am trying to connect an Access database.
>> The code I use is :
>> channel<-odbcConnectAccess("Base")
>> table.db<-sqlFetch(Base,"table",colnames=T)
>>
>> When I look at the table in R, the table imported lacks the first 41
>> lines, and the column names are the values of the 42nd record.
>>
>> Any hints ?
>>
>> I am working on a WinXP pro 5.1 box, R 2.5-0 compiled from source using 
>> MINGW and R tools.
>> I searched R archives through Jonathan Baron site...
>>
>> Thank you very much for your help
>>
>> Dr P. Gonin
>> Institut Gustave Roussy
>> 94805 Villejuif
>> France
>>
>>
>> ---------------------------------
>>
>>  [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
>
> ---------------------------------
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kopwaterlinesystemsmyg at waterlinesystems.com  Fri Jul 13 14:17:03 2007
From: kopwaterlinesystemsmyg at waterlinesystems.com (Stevie Leonard)
Date: Fri, 13 Jul 2007 10:17:03 -0200
Subject: [R] Olny this 5 days special price on pharma for you dear customer
Message-ID: <262901102.89392495421152@waterlinesystems.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/474c074c/attachment.pl 

From torma at sztaki.hu  Fri Jul 13 12:19:59 2007
From: torma at sztaki.hu (Balazs Torma)
Date: Fri, 13 Jul 2007 12:19:59 +0200
Subject: [R] filling a list faster
Message-ID: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>

hello,

    first I create a list:

l <- list("1"<-c(1,2,3))

    then I run the following cycle, it takes over a minute(!) to  
complete on a very fast mashine:

for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i)

How can I fill a list faster? (This is just a demo test, the elements  
of the list are calculated iteratively in an algorithm)

Are there any packages and documents on how to use more advanced and  
fast data structures like linked-lists, hash-tables or trees for  
example?

Thank you,
Balazs Torma


From P.Dalgaard at biostat.ku.dk  Fri Jul 13 12:36:38 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 13 Jul 2007 12:36:38 +0200
Subject: [R] charset in graphics
In-Reply-To: <200707131150.44106.dgvirtual@akl.lt>
References: <200707131150.44106.dgvirtual@akl.lt>
Message-ID: <469755B6.9030300@biostat.ku.dk>

Donatas G. wrote:
> How do I make Lithuanian characters display correctly in R graphics? 
>
> Instead of the special characters for Lithuanian language I get question 
> marks...
>
> I use Ubuntu Feisty, the locale is utf-8 ...
>
> Do I need to specify somewhere the locale for R, or - default font for the 
> graphics?
>   
You mean as in

> plot(0,main="\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u017d")
>
plot(0,main=tolower("\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u017d"))

?

This works fine for me on OpenSUSE 10.2, so I don't think the issue is
in R. More likely, this has to do with X11 fonts (Unicode is handled via
a rather complicated mechanism involving virtual fonts). Postscript/PDF
is a bit more difficult. See ?postscript and the reference to
Murrell+Ripley's R News article inside.

The correct incantation seems to be

postscript(font="URWHelvetica", encoding="ISOLatin7")
plot(0,main=tolower("\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u017d"))
dev.off()

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From pgoninstat1 at yahoo.com  Fri Jul 13 12:39:55 2007
From: pgoninstat1 at yahoo.com (Patrick Gonin)
Date: Fri, 13 Jul 2007 03:39:55 -0700 (PDT)
Subject: [R] RODBC Access
In-Reply-To: <000801c7c534$50bd8f40$a64bb2c3@Mistr1>
Message-ID: <262338.87363.qm@web45013.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/ac02559d/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Fri Jul 13 12:41:20 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 13 Jul 2007 12:41:20 +0200
Subject: [R] The "$" operator and vectors
In-Reply-To: <45f568c70707130217l30da100dk305ede3ddfb4ff79@mail.gmail.com>
References: <45f568c70707130217l30da100dk305ede3ddfb4ff79@mail.gmail.com>
Message-ID: <469756D0.3000302@biostat.ku.dk>

Gustaf Rydevik wrote:
> Hi all,
>
> I've run into a slightly illogical (to me) behaviour with the "$"
> subsetting function.
>
> consider:
>   
>> Test
>>     
>   A B
> 1 1 Q
> 2 2 R
>
>   
>> Test$A
>>     
> [1] 1 2
>
>   
>> vector<-"A"
>> Test$vector
>>     
> NULL
>
>   
>> Test$"A"
>>     
> [1] 1 2
>
>   
>> Test[,vector]
>>     
> [1] 1 2
>
>
> Is there a reason for the $ operator not evaluating the vector before executing?
>   
Yes, the evaluation rule for "$" is like that....

Notice that it also didn't go looking for an object called A when you
said test$A.


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jholtman at gmail.com  Fri Jul 13 13:00:21 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 13 Jul 2007 07:00:21 -0400
Subject: [R] filling a list faster
In-Reply-To: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>
References: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>
Message-ID: <644e1f320707130400x158f27dfld1fe82786a14e4b9@mail.gmail.com>

It all depends on what you want to do.  In your example, it is faster
to first fill in a matrix and then convert the matrix to a list.  The
problem with filling in the list is that you are dynamically
allocating space for each iteration which is probably taking at least
an order of magnitude more time than the calculations you are doing.
So I just translated your problem into two steps and it takes about 2
seconds on my system.


> # fill in a matris
> l <- matrix(ncol=3, nrow=10^5)
> system.time(for(i in (1:10^5)) l[i,] <- c(i,i+1,i))
   user  system elapsed
   1.06    0.00    1.10
> # convert to a list
> system.time(l.list <- lapply(1:10^5, function(i) l[i,]))
   user  system elapsed
   0.45    0.00    0.46
> l.list[1:10]
[[1]]
[1] 1 2 1

[[2]]
[1] 2 3 2

[[3]]
[1] 3 4 3

[[4]]
[1] 4 5 4

[[5]]
[1] 5 6 5





On 7/13/07, Balazs Torma <torma at sztaki.hu> wrote:
> hello,
>
>    first I create a list:
>
> l <- list("1"<-c(1,2,3))
>
>    then I run the following cycle, it takes over a minute(!) to
> complete on a very fast mashine:
>
> for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i)
>
> How can I fill a list faster? (This is just a demo test, the elements
> of the list are calculated iteratively in an algorithm)
>
> Are there any packages and documents on how to use more advanced and
> fast data structures like linked-lists, hash-tables or trees for
> example?
>
> Thank you,
> Balazs Torma
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From phgrosjean at sciviews.org  Fri Jul 13 13:03:09 2007
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 13 Jul 2007 13:03:09 +0200
Subject: [R] filling a list faster
In-Reply-To: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>
References: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>
Message-ID: <46975BED.2000005@sciviews.org>

If all the data coming from your iterations are numeric (as in your toy 
example), why not to use a matrix with one row per iteration? Also, do 
preallocate the matrix and do not add row or column names before the end 
of the calculation. Something like:

 > m <- matrix(rep(NA, 3*10^5), ncol = 3)
 > system.time(for(i in (1:10^5)) m[i, ] <- c(i,i+1,i))
    user  system elapsed
   1.362   0.033   1.424

That is, about 1.5sec on my Intel Duo Core 2.33Mhz MacBook Pro, compared to:

 > l <- list("1"<-c(1,2,3))
 > system.time(for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i))
    user  system elapsed
191.629  49.110 248.454

... more than 4 minutes for your code.

By the way, what is your "very fast machine", that is actually four 
times faster than mine (grrrrr!)?

Best,

Philippe Grosjean

..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Belgium
( ( ( ( (
..............................................................

Balazs Torma wrote:
> hello,
> 
>     first I create a list:
> 
> l <- list("1"<-c(1,2,3))
> 
>     then I run the following cycle, it takes over a minute(!) to  
> complete on a very fast mashine:
> 
> for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i)
> 
> How can I fill a list faster? (This is just a demo test, the elements  
> of the list are calculated iteratively in an algorithm)
> 
> Are there any packages and documents on how to use more advanced and  
> fast data structures like linked-lists, hash-tables or trees for  
> example?
> 
> Thank you,
> Balazs Torma
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From info at aghmed.fsnet.co.uk  Fri Jul 13 13:06:36 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Fri, 13 Jul 2007 12:06:36 +0100
Subject: [R] Difference in linear regression results for Stata and R
In-Reply-To: <11563283.post@talk.nabble.com>
References: <11563283.post@talk.nabble.com>
Message-ID: <Zen-1I9Iz0-0004oK-UW@pythagoras.zen.co.uk>

At 17:17 12/07/2007, kdestler wrote:

>Hi
>I recently imported data from r into Stata.  I then ran the linear
>regression model I've been working on, only to discover that the results are
>somewhat (though not dramatically different).  the standard errors vary more
>between the two programs than do the coefficients themselves.  Any
>suggestions on what I've done that causes this mismatch?

You really need to find a small example which exhibits the problem 
and then post that.


>Thanks,
>Kate
>--
>View this message in context: 
>http://www.nabble.com/Difference-in-linear-regression-results-for-Stata-and-R-tf4069072.html#a11563283
>Sent from the R help mailing list archive at Nabble.com.

Michael Dewey
http://www.aghmed.fsnet.co.uk


From jholtman at gmail.com  Fri Jul 13 13:12:12 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 13 Jul 2007 07:12:12 -0400
Subject: [R] filling a list faster
In-Reply-To: <46975BED.2000005@sciviews.org>
References: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>
	<46975BED.2000005@sciviews.org>
Message-ID: <644e1f320707130412v6fb66a25v8143ec2c64c4a00@mail.gmail.com>

Actually if you are really interested in the list, then just do the
lapply and compute your data; it seems to be even faster than the
matrix:

> system.time(l.1 <- lapply(1:10^5, function(i) c(i, i+1, i)))
   user  system elapsed
   0.50    0.00    0.61
> l.1[1:4]
[[1]]
[1] 1 2 1

[[2]]
[1] 2 3 2

[[3]]
[1] 3 4 3

[[4]]
[1] 4 5 4



On 7/13/07, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
> If all the data coming from your iterations are numeric (as in your toy
> example), why not to use a matrix with one row per iteration? Also, do
> preallocate the matrix and do not add row or column names before the end
> of the calculation. Something like:
>
>  > m <- matrix(rep(NA, 3*10^5), ncol = 3)
>  > system.time(for(i in (1:10^5)) m[i, ] <- c(i,i+1,i))
>    user  system elapsed
>   1.362   0.033   1.424
>
> That is, about 1.5sec on my Intel Duo Core 2.33Mhz MacBook Pro, compared to:
>
>  > l <- list("1"<-c(1,2,3))
>  > system.time(for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i))
>    user  system elapsed
> 191.629  49.110 248.454
>
> ... more than 4 minutes for your code.
>
> By the way, what is your "very fast machine", that is actually four
> times faster than mine (grrrrr!)?
>
> Best,
>
> Philippe Grosjean
>
> ..............................................<?}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Belgium
> ( ( ( ( (
> ..............................................................
>
> Balazs Torma wrote:
> > hello,
> >
> >     first I create a list:
> >
> > l <- list("1"<-c(1,2,3))
> >
> >     then I run the following cycle, it takes over a minute(!) to
> > complete on a very fast mashine:
> >
> > for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i)
> >
> > How can I fill a list faster? (This is just a demo test, the elements
> > of the list are calculated iteratively in an algorithm)
> >
> > Are there any packages and documents on how to use more advanced and
> > fast data structures like linked-lists, hash-tables or trees for
> > example?
> >
> > Thank you,
> > Balazs Torma
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From nitish at imtech.res.in  Fri Jul 13 13:10:34 2007
From: nitish at imtech.res.in (Nitish Kumar Mishra)
Date: Fri, 13 Jul 2007 16:40:34 +0530 (IST)
Subject: [R] About eigen value
Message-ID: <44553.59.160.112.42.1184325034.squirrel@webmail.imtech.res.in>

Hi R help group,
Any budy give me tutorial/paper for eigen value calculation and PCA.
Thanking you.


-- 
Nitish Kumar Mishra
Junior Research Fellow
BIC, IMTECH, Chandigarh, India
E-Mail Address:
nitish_km at yahoo.com
nitish at imtech.res.in


From Matthias.Kohl at stamats.de  Fri Jul 13 13:18:54 2007
From: Matthias.Kohl at stamats.de (Matthias.Kohl at stamats.de)
Date: Fri, 13 Jul 2007 13:18:54 +0200 (MEST)
Subject: [R] filling a list faster
Message-ID: <200707131118.l6DBIsss008878@post.webmailer.de>

another solution:
l <- vector(mode = "list", length = 10^5)
system.time(for(i in (1:10^5)) l[[i]] <- c(i,i+1,i))

On my system this version is even slightly faster than the matrix version ...

Best,
Matthias

----- original message --------

Subject: Re: [R] filling a list faster
Sent: Fri, 13 Jul 2007
From: Philippe Grosjean<phgrosjean at sciviews.org>

> If all the data coming from your iterations are numeric (as in your toy 
> example), why not to use a matrix with one row per iteration? Also, do 
> preallocate the matrix and do not add row or column names before the end 
> of the calculation. Something like:
> 
>  > m <- matrix(rep(NA, 3*10^5), ncol = 3)
>  > system.time(for(i in (1:10^5)) m[i, ] <- c(i,i+1,i))
>     user  system elapsed
>    1.362   0.033   1.424
> 
> That is, about 1.5sec on my Intel Duo Core 2.33Mhz MacBook Pro, compared to:
> 
>  > l <- list("1"<-c(1,2,3))
>  > system.time(for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i))
>     user  system elapsed
> 191.629  49.110 248.454
> 
> ... more than 4 minutes for your code.
> 
> By the way, what is your "very fast machine", that is actually four 
> times faster than mine (grrrrr!)?
> 
> Best,
> 
> Philippe Grosjean
> 
> ..............................................<?}))><........
>   ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>   ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>   ) ) ) ) )   Mons-Hainaut University, Belgium
> ( ( ( ( (
> ..............................................................
> 
> Balazs Torma wrote:
> > hello,
> > 
> >     first I create a list:
> > 
> > l <- list("1"<-c(1,2,3))
> > 
> >     then I run the following cycle, it takes over a minute(!) to  
> > complete on a very fast mashine:
> > 
> > for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i)
> > 
> > How can I fill a list faster? (This is just a demo test, the elements  
> > of the list are calculated iteratively in an algorithm)
> > 
> > Are there any packages and documents on how to use more advanced and  
> > fast data structures like linked-lists, hash-tables or trees for  
> > example?
> > 
> > Thank you,
> > Balazs Torma
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

--- original message end ----


From marilu.81 at tiscali.it  Fri Jul 13 13:23:59 2007
From: marilu.81 at tiscali.it (marilu.81 at tiscali.it)
Date: Fri, 13 Jul 2007 13:23:59 +0200 (CEST)
Subject: [R] R file via SSH
Message-ID: <19073010.1184325839696.JavaMail.root@ps21>

Goodmorning everybody,
I need to run an R program via SSH. Usually I open R, I run the 
program and I stay logged-in, waiting for the output. As a matter of 
fact, if I close the connection with SSH I loose the calculations and 
the output of my R program. What command I have to use in order to 
preseve the calculations and the output without staying logged-in a SSH 
connection?
thanks in advance

Marialucia


____________________________________________________________
TISCALI TANDEM FREE
Telefono e Adsl 4 MB da ? 22.95 al mese. Stacchi Telecom!
GRATIS modem, segreteria e seconda linea telefonica!   
http://abbonati.tiscali.it/adsl/prodotti/tc/tandemfree_tel/


From torma at sztaki.hu  Fri Jul 13 13:26:01 2007
From: torma at sztaki.hu (Balazs Torma)
Date: Fri, 13 Jul 2007 13:26:01 +0200
Subject: [R] filling a list faster
In-Reply-To: <644e1f320707130412v6fb66a25v8143ec2c64c4a00@mail.gmail.com>
References: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>
	<46975BED.2000005@sciviews.org>
	<644e1f320707130412v6fb66a25v8143ec2c64c4a00@mail.gmail.com>
Message-ID: <20070713132601.zecf402k544okcgk@webmail.sztaki.hu>

Thank you all for your answers!

The problem is that I don't know the length of the list in advance!  
And hoped for a convinience structure which reallocates once the  
preallocated list (or matrix) becomes full.

>> By the way, what is your "very fast machine", that is actually four
>> times faster than mine (grrrrr!)?
I don't know the exact parameters of this machine, I just know its  
name is "monster", has 32 GB RAM and >10 CPUs :)

Quoting jim holtman <jholtman at gmail.com>:

> Actually if you are really interested in the list, then just do the
> lapply and compute your data; it seems to be even faster than the
> matrix:
>
>> system.time(l.1 <- lapply(1:10^5, function(i) c(i, i+1, i)))
>   user  system elapsed
>   0.50    0.00    0.61
>> l.1[1:4]
> [[1]]
> [1] 1 2 1
>
> [[2]]
> [1] 2 3 2
>
> [[3]]
> [1] 3 4 3
>
> [[4]]
> [1] 4 5 4
>
>
>
> On 7/13/07, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
>> If all the data coming from your iterations are numeric (as in your toy
>> example), why not to use a matrix with one row per iteration? Also, do
>> preallocate the matrix and do not add row or column names before the end
>> of the calculation. Something like:
>>
>> > m <- matrix(rep(NA, 3*10^5), ncol = 3)
>> > system.time(for(i in (1:10^5)) m[i, ] <- c(i,i+1,i))
>>   user  system elapsed
>>  1.362   0.033   1.424
>>
>> That is, about 1.5sec on my Intel Duo Core 2.33Mhz MacBook Pro, compared to:
>>
>> > l <- list("1"<-c(1,2,3))
>> > system.time(for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i))
>>   user  system elapsed
>> 191.629  49.110 248.454
>>
>> ... more than 4 minutes for your code.
>>
>> By the way, what is your "very fast machine", that is actually four
>> times faster than mine (grrrrr!)?
>>
>> Best,
>>
>> Philippe Grosjean
>>
>> ..............................................<?}))><........
>> ) ) ) ) )
>> ( ( ( ( (    Prof. Philippe Grosjean
>> ) ) ) ) )
>> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>> ) ) ) ) )   Mons-Hainaut University, Belgium
>> ( ( ( ( (
>> ..............................................................
>>
>> Balazs Torma wrote:
>>> hello,
>>>
>>>     first I create a list:
>>>
>>> l <- list("1"<-c(1,2,3))
>>>
>>>     then I run the following cycle, it takes over a minute(!) to
>>> complete on a very fast mashine:
>>>
>>> for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i)
>>>
>>> How can I fill a list faster? (This is just a demo test, the elements
>>> of the list are calculated iteratively in an algorithm)
>>>
>>> Are there any packages and documents on how to use more advanced and
>>> fast data structures like linked-lists, hash-tables or trees for
>>> example?
>>>
>>> Thank you,
>>> Balazs Torma
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?


From martin.becker at mx.uni-saarland.de  Fri Jul 13 13:43:14 2007
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Fri, 13 Jul 2007 13:43:14 +0200
Subject: [R] R file via SSH
In-Reply-To: <19073010.1184325839696.JavaMail.root@ps21>
References: <19073010.1184325839696.JavaMail.root@ps21>
Message-ID: <46976552.7030202@mx.uni-saarland.de>

You may find "screen" useful, see e.g.

  http://tolstoy.newcastle.edu.au/R/e2/help/07/02/10824.html

for a short description how to use it with R (reattaching works via 
"screen -r"). (Maybe your sysadmin has to install screen first, it's a 
unix tool.)
KR

  Martin

marilu.81 at tiscali.it wrote:
> Goodmorning everybody,
> I need to run an R program via SSH. Usually I open R, I run the 
> program and I stay logged-in, waiting for the output. As a matter of 
> fact, if I close the connection with SSH I loose the calculations and 
> the output of my R program. What command I have to use in order to 
> preseve the calculations and the output without staying logged-in a SSH 
> connection?
> thanks in advance
>
> Marialucia
>
>
> ____________________________________________________________
> TISCALI TANDEM FREE
> Telefono e Adsl 4 MB da ? 22.95 al mese. Stacchi Telecom!
> GRATIS modem, segreteria e seconda linea telefonica!   
> http://abbonati.tiscali.it/adsl/prodotti/tc/tandemfree_tel/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mark at wardle.org  Fri Jul 13 13:47:34 2007
From: mark at wardle.org (Mark Wardle)
Date: Fri, 13 Jul 2007 12:47:34 +0100
Subject: [R] R file via SSH
In-Reply-To: <19073010.1184325839696.JavaMail.root@ps21>
References: <19073010.1184325839696.JavaMail.root@ps21>
Message-ID: <b59a37130707130447l4a22343ekd559dbe29f34c320@mail.gmail.com>

Try using the unix command "screen"



On 13/07/07, marilu.81 at tiscali.it <marilu.81 at tiscali.it> wrote:
> Goodmorning everybody,
> I need to run an R program via SSH. Usually I open R, I run the
> program and I stay logged-in, waiting for the output. As a matter of
> fact, if I close the connection with SSH I loose the calculations and
> the output of my R program. What command I have to use in order to
> preseve the calculations and the output without staying logged-in a SSH
> connection?
> thanks in advance
>
> Marialucia
>
>
> ____________________________________________________________
> TISCALI TANDEM FREE
> Telefono e Adsl 4 MB da ? 22.95 al mese. Stacchi Telecom!
> GRATIS modem, segreteria e seconda linea telefonica!
> http://abbonati.tiscali.it/adsl/prodotti/tc/tandemfree_tel/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________________________________
> This email has been scanned by the MessageLabs Email Security System.
> For more information please visit http://www.messagelabs.com/email
> ______________________________________________________________________
>


-- 
Dr. Mark Wardle
Clinical research fellow and specialist registrar, Neurology
Cardiff, UK


From h.y.wong at leeds.ac.uk  Fri Jul 13 13:44:56 2007
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Fri, 13 Jul 2007 12:44:56 +0100
Subject: [R] Direction of panel plots in trellis graphics
Message-ID: <8288D1F1-C1E4-4EAC-B1ED-FFC1A17153C2@leeds.ac.uk>

Hi,

Using library(lattice), is there any way to tell xyplot to plot  
panels top to bottom, then left to right (i.e. panels are appended  
vertically, then horizontally). as.table changes the plot direction  
from left-to-right then top-to-bottom, to right-to-left then bottom- 
to-top, but that's not quite what I want to do.

Thanks

Yan


From ripley at stats.ox.ac.uk  Fri Jul 13 13:55:45 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jul 2007 12:55:45 +0100 (BST)
Subject: [R] filling a list faster
In-Reply-To: <46975BED.2000005@sciviews.org>
References: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>
	<46975BED.2000005@sciviews.org>
Message-ID: <Pine.LNX.4.64.0707131242330.17339@gannet.stats.ox.ac.uk>

On Fri, 13 Jul 2007, Philippe Grosjean wrote:

> If all the data coming from your iterations are numeric (as in your toy
> example), why not to use a matrix with one row per iteration? Also, do
> preallocate the matrix and do not add row or column names before the end
> of the calculation. Something like:
>
> > m <- matrix(rep(NA, 3*10^5), ncol = 3)
> > system.time(for(i in (1:10^5)) m[i, ] <- c(i,i+1,i))
>    user  system elapsed
>   1.362   0.033   1.424
>
> That is, about 1.5sec on my Intel Duo Core 2.33Mhz MacBook Pro, compared to:
>
> > l <- list("1"<-c(1,2,3))
> > system.time(for(i in (1:10^5)) l[[length(l)+1]] <- c(i,i+1,i))
>    user  system elapsed
> 191.629  49.110 248.454
>
> ... more than 4 minutes for your code.
>
> By the way, what is your "very fast machine", that is actually four
> times faster than mine (grrrrr!)?

He said 'over a minute', not how long exactly.
But it is not just the machine but also the OS: memory management on MacOS 
(it you are running MacOS) is known to be slow under some circumstances.
It took about 100s under 64-bit Linux on my 2.4Ghz Intel Core 2 Duo.

Preallocating the list helps here

l <- vector("list", 10^5)
system.time(for(i in (1:10^5)) l[[i]] <- c(i,i+1,i))

takes well under a second.

If you don't know the length and know an upper bound, just use that.
The storage allocated is only one pointer per element.  And if you don't 
even know an upper bound, start with a reasonable number and use

length(l) <- 1.5*length(l)

when you need to.

There are some ideas for doing this internally (R vectors have a LENGTH 
and TRUELENGTH field and so could over-allocate).  The problem is that 
they would penalize careful coding to ameliorate the effects of careless 
coding.  (One of the most basic points in S/R programming is that you 
avoid growing vectors.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rfrancois at mango-solutions.com  Fri Jul 13 13:56:35 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Fri, 13 Jul 2007 12:56:35 +0100
Subject: [R] R file via SSH
In-Reply-To: <46976552.7030202@mx.uni-saarland.de>
References: <19073010.1184325839696.JavaMail.root@ps21>
	<46976552.7030202@mx.uni-saarland.de>
Message-ID: <46976873.20809@mango-solutions.com>

Hi,

Again, screen was described in the April issue of R News.
"Augmenting R with Unix Tools" by Andrew Robinson.

@ARTICLE{Rnews:Robinson:2007,
  AUTHOR = {Andrew Robinson},
  TITLE = {Augmenting {R} with {Unix} Tools},
  JOURNAL = {R News},
  YEAR = 2007,
  VOLUME = 7,
  NUMBER = 1,
  PAGES = {30--33},
  MONTH = {April},
  URL = {http://CRAN.R-project.org/doc/Rnews/},
  PDF = {http://CRAN.R-project.org/doc/Rnews/Rnews_2007-1.pdf}
}

Cheers,

Romain

Martin Becker wrote:
> You may find "screen" useful, see e.g.
>
>   http://tolstoy.newcastle.edu.au/R/e2/help/07/02/10824.html
>
> for a short description how to use it with R (reattaching works via 
> "screen -r"). (Maybe your sysadmin has to install screen first, it's a 
> unix tool.)
> KR
>
>   Martin
>
> marilu.81 at tiscali.it wrote:
>   
>> Goodmorning everybody,
>> I need to run an R program via SSH. Usually I open R, I run the 
>> program and I stay logged-in, waiting for the output. As a matter of 
>> fact, if I close the connection with SSH I loose the calculations and 
>> the output of my R program. What command I have to use in order to 
>> preseve the calculations and the output without staying logged-in a SSH 
>> connection?
>> thanks in advance
>>
>> Marialucia
>>
>>
>> ____________________________________________________________
>> TISCALI TANDEM FREE
>> Telefono e Adsl 4 MB da ? 22.95 al mese. Stacchi Telecom!
>> GRATIS modem, segreteria e seconda linea telefonica!   
>> http://abbonati.tiscali.it/adsl/prodotti/tc/tandemfree_tel/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>     
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Mango Solutions
data analysis that delivers

Tel:  +44(0) 1249 467 467
Fax:  +44(0) 1249 467 468
Mob:  +44(0) 7813 526 123


From yn19832 at msn.com  Fri Jul 13 14:00:08 2007
From: yn19832 at msn.com (livia)
Date: Fri, 13 Jul 2007 05:00:08 -0700 (PDT)
Subject: [R] correlation matrix difference
Message-ID: <11578046.post@talk.nabble.com>


Hi, I have got four correlation matrix. They are the same set of variables
under different conditions. Is there a way to test whether the correlation
matrix are significently different among each other? Could 
anyone give me some advice?
-- 
View this message in context: http://www.nabble.com/correlation-matrix-difference-tf4073868.html#a11578046
Sent from the R help mailing list archive at Nabble.com.


From P.Dalgaard at biostat.ku.dk  Fri Jul 13 14:01:55 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 13 Jul 2007 14:01:55 +0200
Subject: [R] filling a list faster
In-Reply-To: <20070713132601.zecf402k544okcgk@webmail.sztaki.hu>
References: <20070713121959.1frq49i2iow8sg84@webmail.sztaki.hu>	<46975BED.2000005@sciviews.org>	<644e1f320707130412v6fb66a25v8143ec2c64c4a00@mail.gmail.com>
	<20070713132601.zecf402k544okcgk@webmail.sztaki.hu>
Message-ID: <469769B3.1000805@biostat.ku.dk>

Balazs Torma wrote:
> Thank you all for your answers!
>
> The problem is that I don't know the length of the list in advance!  
> And hoped for a convinience structure which reallocates once the  
> preallocated list (or matrix) becomes full.
>   
That's not massively hard to do yourself, is it?
As in

if (i > N) {l <- c(l, vector("list",N); N <- N*2}

i.e.
> N<-1; l <- vector("list", N)
> system.time(for(i in (1:1e5)) { if (i > N) {l <- c(l,
vector("list",N)); N <- N*2} ; l[[i]] <- c(i,i+1,i)})
   user  system elapsed
  1.508   0.012   1.520
> l[i+1:N]<-NULL

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From sabya23 at gmail.com  Fri Jul 13 14:06:41 2007
From: sabya23 at gmail.com (spime)
Date: Fri, 13 Jul 2007 05:06:41 -0700 (PDT)
Subject: [R] Package for .632 (and .632+) bootstrap and the
 cross-validation of ROC Parameters
In-Reply-To: <46966350.4090202@vanderbilt.edu>
References: <11561405.post@talk.nabble.com> <46966350.4090202@vanderbilt.edu>
Message-ID: <11578129.post@talk.nabble.com>


Suppose I have

Training data: my.train
Testing data: my.test

I want to calculate bootstrap error rate for logistic model. My wrapper
function for prediction

pred.glm <- function(object, newdata) {
        ret <- as.factor(ifelse(predict.glm(object, newdata,
type='response') < 0.4, 0, 1))
        return(ret)
        }

But i thing i cant understand if i want to calculate misclassification error
for my testing data what will be in my data in the following formula.

errorest(RES ~., data=???, model=glm, estimator="boot", predict=pred.glm, 
       est.para=control.errorest(nboot = 10))

Using my.test got following error,

Error in predict(mymodel, newdata = outbootdata) : 
        unused argument(s) (newdata = list(RES = c(1, 0, 0, 0, 1, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
0), CAT01 = c(4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 1, 2, 2, 4, 4, 4, 1, 1, 2, 2, 1,
4, 1, 4, 1, 4, 2, 4, 1, 4, 2, 3, 1, 1, 3, 3, 4, 2, 4, 2, 1, 2, 2, 1, 1, 
> 

please reply...






Frank E Harrell Jr wrote:
> 
> spime wrote:
>> 
>> Hi users,
>> 
>> I need to calculate .632 (and .632+) bootstrap and the cross-validation
>> of
>> area under curve (AUC) to compare my models. Is there any package for the
>> same. I know about 'ipred' and using it i can calculate misclassification
>> errors. 
>> 
>> Please help. It's urgent. 
> 
> See the validate* functions in the Design package.
> 
> Note that some simulations (see http://biostat.mc.vanderbilt.edu/rms) 
> indicate that the advantages of .632 and .632+ over the ordinary 
> bootstrap are highly dependent on the choice of the accuracy measure 
> being validated.  The bootstrap variants seem to have advantages mainly 
> if an improper, inefficient, discontinuous scoring rule such as the 
> percent classified correct is used.
> 
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Package-for-.632-%28and-.632%2B%29-bootstrap-and-the-cross-validation-of-ROC-Parameters-tf4068544.html#a11578129
Sent from the R help mailing list archive at Nabble.com.


From oehl_list at gmx.de  Fri Jul 13 14:25:54 2007
From: oehl_list at gmx.de (=?iso-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Fri, 13 Jul 2007 14:25:54 +0200
Subject: [R] nearest correlation to polychoric
Message-ID: <20070713122554.32910@gmx.net>

Dear all,

Has someone implemented in R (or any other language)

Knol DL, ten Berge JMF.  Least-squares approximation of an improper correlation matrix by a proper one.  Psychometrika, 1989, 54, 53-61.

or any other similar algorithm?

Best regards


Jens  Oehlschl?gel


Background:

I want to factanal() matrices of polychoric correlations which have negative eigenvalue. I coded

Highham 2002 Computing the nearest correlation matrix - a problem from finance, IMA Journal of Numerical Analysis (2002), 22, 329-343.

which basically works but still leaves very small negative eigenvalues which causes factanal() to fail with 

> factanal(cov=ncor$cor, factors=2)
Fehler in optim(start, FAfn, FAgr, method = "L-BFGS-B", lower = lower,  : 
        L-BFGS-B ben?tigt endliche Werte von 'fn'
Zus?tzlich: Warning message:
NaNs wurden erzeugt in: log(x) 
> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          4.1                         
year           2006                        
month          12                          
day            18                          
svn rev        40228                       
language       R                           
version.string R version 2.4.1 (2006-12-18)

--


From aolinto_r at bignet.com.br  Fri Jul 13 14:26:33 2007
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Fri, 13 Jul 2007 09:26:33 -0300
Subject: [R] weighted nonlinear regression
Message-ID: <1184329593.46976f799b11f@webmail.bignet.com.br>

Hello,

I'm using the weighted nonlinear regression proposed in nls help.

I would like to know which is proper reference to the "Weighted version of 
Michaelis-Menten model". Which is "The White book" cited in nls help?


And, by-the-way, many thanks to Martin Maechler for this function.

Best regards,

Antonio Olinto

Sao Paulo Fisheries Institute
Brazil
www.pesca.sp.gov.br
-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br


From f.harrell at vanderbilt.edu  Fri Jul 13 14:26:48 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 13 Jul 2007 07:26:48 -0500
Subject: [R] Package for .632 (and .632+) bootstrap and the
 cross-validation of ROC Parameters
In-Reply-To: <11578129.post@talk.nabble.com>
References: <11561405.post@talk.nabble.com> <46966350.4090202@vanderbilt.edu>
	<11578129.post@talk.nabble.com>
Message-ID: <46976F88.6070308@vanderbilt.edu>

spime wrote:
> Suppose I have
> 
> Training data: my.train
> Testing data: my.test

The bootstrap does not need split samples.

> 
> I want to calculate bootstrap error rate for logistic model. My wrapper
> function for prediction
> 
> pred.glm <- function(object, newdata) {
>         ret <- as.factor(ifelse(predict.glm(object, newdata,
> type='response') < 0.4, 0, 1))
>         return(ret)
>         }
> 
> But i thing i cant understand if i want to calculate misclassification error
> for my testing data what will be in my data in the following formula.

Misclassification error has many problems because it is not a proper 
scoring rule, i.e., it is optimized by bogus models.

Frank

> 
> errorest(RES ~., data=???, model=glm, estimator="boot", predict=pred.glm, 
>        est.para=control.errorest(nboot = 10))
> 
> Using my.test got following error,
> 
> Error in predict(mymodel, newdata = outbootdata) : 
>         unused argument(s) (newdata = list(RES = c(1, 0, 0, 0, 1, 0, 0, 0,
> 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
> 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
> 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
> 0), CAT01 = c(4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 1, 2, 2, 4, 4, 4, 1, 1, 2, 2, 1,
> 4, 1, 4, 1, 4, 2, 4, 1, 4, 2, 3, 1, 1, 3, 3, 4, 2, 4, 2, 1, 2, 2, 1, 1, 
> 
> please reply...
> 
> 
> 
> 
> 
> 
> Frank E Harrell Jr wrote:
>> spime wrote:
>>> Hi users,
>>>
>>> I need to calculate .632 (and .632+) bootstrap and the cross-validation
>>> of
>>> area under curve (AUC) to compare my models. Is there any package for the
>>> same. I know about 'ipred' and using it i can calculate misclassification
>>> errors. 
>>>
>>> Please help. It's urgent. 
>> See the validate* functions in the Design package.
>>
>> Note that some simulations (see http://biostat.mc.vanderbilt.edu/rms) 
>> indicate that the advantages of .632 and .632+ over the ordinary 
>> bootstrap are highly dependent on the choice of the accuracy measure 
>> being validated.  The bootstrap variants seem to have advantages mainly 
>> if an improper, inefficient, discontinuous scoring rule such as the 
>> percent classified correct is used.
>>
>> -- 
>> Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                       Department of Biostatistics   Vanderbilt University
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From dimitris.rizopoulos at med.kuleuven.be  Fri Jul 13 14:43:08 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 13 Jul 2007 14:43:08 +0200
Subject: [R] nearest correlation to polychoric
References: <20070713122554.32910@gmx.net>
Message-ID: <005201c7c54b$5dcaf670$0540210a@www.domain>

you could also have a look at function posdefify() from package 
`sfsmisc'.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: ""Jens Oehlschl?gel"" <oehl_list at gmx.de>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, July 13, 2007 2:25 PM
Subject: [R] nearest correlation to polychoric


Dear all,

Has someone implemented in R (or any other language)

Knol DL, ten Berge JMF.  Least-squares approximation of an improper 
correlation matrix by a proper one.  Psychometrika, 1989, 54, 53-61.

or any other similar algorithm?

Best regards


Jens  Oehlschl?gel


Background:

I want to factanal() matrices of polychoric correlations which have 
negative eigenvalue. I coded

Highham 2002 Computing the nearest correlation matrix - a problem from 
finance, IMA Journal of Numerical Analysis (2002), 22, 329-343.

which basically works but still leaves very small negative eigenvalues 
which causes factanal() to fail with

> factanal(cov=ncor$cor, factors=2)
Fehler in optim(start, FAfn, FAgr, method = "L-BFGS-B", lower = lower, 
:
        L-BFGS-B ben?tigt endliche Werte von 'fn'
Zus?tzlich: Warning message:
NaNs wurden erzeugt in: log(x)
> version
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          4.1
year           2006
month          12
day            18
svn rev        40228
language       R
version.string R version 2.4.1 (2006-12-18)

--

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ripley at stats.ox.ac.uk  Fri Jul 13 14:43:20 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jul 2007 13:43:20 +0100 (BST)
Subject: [R] charset in graphics
In-Reply-To: <469755B6.9030300@biostat.ku.dk>
References: <200707131150.44106.dgvirtual@akl.lt>
	<469755B6.9030300@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0707131342080.1162@auk.stats>

On Fri, 13 Jul 2007, Peter Dalgaard wrote:

> Donatas G. wrote:
>> How do I make Lithuanian characters display correctly in R graphics?
>>
>> Instead of the special characters for Lithuanian language I get question
>> marks...
>>
>> I use Ubuntu Feisty, the locale is utf-8 ...
>>
>> Do I need to specify somewhere the locale for R, or - default font for the
>> graphics?
>>
> You mean as in
>
>> plot(0,main="\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u017d")
>>
> plot(0,main=tolower("\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u017d"))
>
> ?
>
> This works fine for me on OpenSUSE 10.2, so I don't think the issue is
> in R. More likely, this has to do with X11 fonts (Unicode is handled via
> a rather complicated mechanism involving virtual fonts). Postscript/PDF
> is a bit more difficult. See ?postscript and the reference to
> Murrell+Ripley's R News article inside.
>
> The correct incantation seems to be
>
> postscript(font="URWHelvetica", encoding="ISOLatin7")
> plot(0,main=tolower("\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u017d"))
> dev.off()

The encoding should happen automagically in a Lithuanian UTF-8 locale, and 
does for me.  But suitable fonts (e.g. URW ones) are needed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From tavpritesh at gmail.com  Fri Jul 13 15:05:16 2007
From: tavpritesh at gmail.com (Tavpritesh Sethi)
Date: Fri, 13 Jul 2007 18:35:16 +0530
Subject: [R] how to create matrix in a loop
Message-ID: <33846cd50707130605r5e17b210mf4b7977140518c56@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/34a051a8/attachment.pl 

From bates at stat.wisc.edu  Fri Jul 13 15:20:00 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 13 Jul 2007 08:20:00 -0500
Subject: [R] imposing constraints on the covariance matrix of random
	effects in lme4?
In-Reply-To: <9aba10be0707122029h7ffa05a1w57d7ed951997c4ee@mail.gmail.com>
References: <9aba10be0707122029h7ffa05a1w57d7ed951997c4ee@mail.gmail.com>
Message-ID: <40e66e0b0707130620lb94825cuefed826cc4c69fa3@mail.gmail.com>

On 7/12/07, JVVerkuilen <jvverkuilen at gmail.com> wrote:
> Hello all,

> I am using lme4 to fit some mixed logistic regressions. I need to
> impose an identification constraint of the following form:

> (1        sig12)
> (sig12  sig22)

> and have not figured out how to do it, i.e., sig11 = 1 but the rest of
> the parameters are free to vary. Is this possible and, if so, how?

> I've been looking through the archive and help to no avail, but
> perhaps I'm just missing something.

Current versions of the lme4 package do not allow such a constraint to
be imposed.  It would not be overwhelmingly difficult to do so because
the value that you wish to constrain appears explicitly as one of the
parameters in the optimization of (the approximation to) the
log-likelihood.  However, doing this would involve writing writing
custom versions of some of the functions coded in C. If you are
willing to undertake this contact me off-list and I will describe
exactly what would need to be done.


From rmh at temple.edu  Fri Jul 13 15:26:22 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 13 Jul 2007 09:26:22 -0400 (EDT)
Subject: [R] Direction of panel plots in trellis graphics
Message-ID: <20070713092622.CGJ22729@po-d.temple.edu>

You can control the panel sequence with subscripting and transpose.
Here are several examples.  I think tmp.tr3 is the one you asked for.

library(lattice)

tmp <- data.frame(x=rnorm(24), y=rnorm(24), a=rep(letters[1:6],4), b=rep(LETTERS[1:4],each=6))
tmp.tr <- xyplot(y ~ x | a*b, data=tmp)
tmp.tr
t(tmp.tr)

tmp.tr2 <- xyplot(y ~ x | a, data=tmp)
tmp.tr2
tmp.tr2[c(1,3,5,2,4,6)]

tmp.tr3 <- xyplot(y ~ x | a, data=tmp, as.table=TRUE)
tmp.tr3
tmp.tr3[c(1,3,5,2,4,6)]
 
Rich


From ggrothendieck at gmail.com  Fri Jul 13 15:34:12 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 13 Jul 2007 09:34:12 -0400
Subject: [R] Direction of panel plots in trellis graphics
In-Reply-To: <8288D1F1-C1E4-4EAC-B1ED-FFC1A17153C2@leeds.ac.uk>
References: <8288D1F1-C1E4-4EAC-B1ED-FFC1A17153C2@leeds.ac.uk>
Message-ID: <971536df0707130634i41af487eua31f95b1cc9d1729@mail.gmail.com>

See this:

http://tolstoy.newcastle.edu.au/R/help/06/08/32543.html

and also the other messages in that thread.

On 7/13/07, Yan Wong <h.y.wong at leeds.ac.uk> wrote:
> Hi,
>
> Using library(lattice), is there any way to tell xyplot to plot
> panels top to bottom, then left to right (i.e. panels are appended
> vertically, then horizontally). as.table changes the plot direction
> from left-to-right then top-to-bottom, to right-to-left then bottom-
> to-top, but that's not quite what I want to do.
>
> Thanks
>
> Yan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dpowers at mail.la.utexas.edu  Fri Jul 13 15:38:30 2007
From: dpowers at mail.la.utexas.edu (Dan Powers)
Date: Fri, 13 Jul 2007 08:38:30 -0500
Subject: [R] make error R-5.1 on  sun solaris
In-Reply-To: <Pine.LNX.4.64.0707120548380.15630@gannet.stats.ox.ac.uk>
References: <000001c7c401$d31a0e20$775d7481@austin.utexas.edu>
	<469554CF.7020402@biostat.ku.dk>
	<Pine.LNX.4.64.0707120548380.15630@gannet.stats.ox.ac.uk>
Message-ID: <000001c7c553$1d6abb80$775d7481@austin.utexas.edu>

Thanks. 
I am working (making some progress) on two installations (different sun's).
I am working to get the system admins to install the right libraries and
compilers to get the job done.
Cheers,
Dan


=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Daniel A. Powers, Ph.D.
Department of Sociology
University of Texas at Austin
1 University Station A1700
Austin, TX  78712-0118
phone: 512-232-6335
fax:   512-471-1748
dpowers at mail.la.utexas.edu
-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Thursday, July 12, 2007 12:00 AM
To: Peter Dalgaard
Cc: dpowers at mail.la.utexas.edu; 'R-Help'
Subject: Re: [R] make error R-5.1 on sun solaris

You are asked for a C99 compiler and configure normally finds one: mixing 
declarations and code is valid C99.

Unless something has been done with environment variables (e.g. in 
config.site) this gcc is very old.  configure should come up with 
'gcc -std=gnu99'.  Re-ordering the code will help (but it may need to go 
below the next #ifdef block too), but a gcc update would be a very good 
idea as gcc on Sparc has been buggy up to about 3.4.x.

Note too that --without-iconv is undesirable and should not be necessary 
as libiconv can be installed as a preload on Solaris (8 and 10, so 
presumably also 9).


On Thu, 12 Jul 2007, Peter Dalgaard wrote:

> Dan Powers wrote:
>> I hope this is enough information to determine the problem. Thanks in
>> advance for any help.
>>
>> Configure goes ok (I think)
>>
>> ./configure --prefix=$HOME --without-iconv
>>
>>
>> R is now configured for sparc-sun-solaris2.9
>>
>>   Source directory:          .
>>   Installation directory:    /home/dpowers
>>
>>   C compiler:                gcc  -g -O2
>>   Fortran 77 compiler:       f95  -g
>>
>>   C++ compiler:              g++  -g -O2
>>   Fortran 90/95 compiler:    f95 -g
>>   Obj-C compiler:             -g -O2
>>
>>   Interfaces supported:      X11
>>   External libraries:        readline
>>   Additional capabilities:   NLS
>>   Options enabled:           shared BLAS, R profiling, Java
>>
>>   Recommended packages:      yes
>>
>> Make ends after the gcc..
>>
>> make
>> .
>> .
>> .
>>
>> gcc -I. -I../../src/include -I../../src/include -I/usr/openwin/include
>> -I/usr/local/include -DHAVE_CONFIG_H   -g -O2 -c system.c -o system.o
>> system.c: In function `Rf_initialize_R':
>> system.c:144: parse error before `char'
>> system.c:216: `localedir' undeclared (first use in this function)
>> system.c:216: (Each undeclared identifier is reported only once
>> system.c:216: for each function it appears in.)
>> *** Error code 1
>> make: Fatal error: Command failed for target `system.o'
>> Current working directory /home/dpowers/R-2.5.1/src/unix
>> *** Error code 1
>> make: Fatal error: Command failed for target `R'
>> Current working directory /home/dpowers/R-2.5.1/src/unix
>> *** Error code 1
>> make: Fatal error: Command failed for target `R'
>> Current working directory /home/dpowers/R-2.5.1/src
>> *** Error code 1
>> make: Fatal error: Command failed for target `R'
>>
>>
>> I have tried setting localedir directly in configure options, but get the
>> same error.
>>
>> Any ideas?
>>
>>
> Hmm, which version of gcc is this? The problem seems to be around line
> 144 which reads
>
> 140     Rstart Rp = &rstart;
> 141     cmdlines[0] = '\0';
> 142
> 143 #ifdef ENABLE_NLS
> 144     char localedir[PATH_MAX+20];
> 145 #endif
> 146
> 147 #if defined(HAVE_SYS_RESOURCE_H) && defined(HAVE_GETRLIMIT)
> 148 {
> 149     struct rlimit rlim;
>
>
> I seem to remember that it used to be non-kosher to mix declarations
> and ordinary code like that, but the current compiler doesn't seem to
> care (I do have #define ENABLE_NLS 1 in Rconfig.h, as I assume you do
> too). Could you perhaps try moving line 141 down below #endif?
>
>
>
>> Thanks,
>> Dan
>> =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
>> Daniel A. Powers, Ph.D.
>> Department of Sociology
>> University of Texas at Austin
>> 1 University Station A1700
>> Austin, TX  78712-0118
>> phone: 512-232-6335
>> fax:   512-471-1748
>> dpowers at mail.la.utexas.edu
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From P.Dalgaard at biostat.ku.dk  Fri Jul 13 15:40:46 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 13 Jul 2007 15:40:46 +0200
Subject: [R] charset in graphics
In-Reply-To: <Pine.LNX.4.64.0707131342080.1162@auk.stats>
References: <200707131150.44106.dgvirtual@akl.lt>	<469755B6.9030300@biostat.ku.dk>
	<Pine.LNX.4.64.0707131342080.1162@auk.stats>
Message-ID: <469780DE.30105@biostat.ku.dk>

Prof Brian Ripley wrote:
> On Fri, 13 Jul 2007, Peter Dalgaard wrote:
>   
>> The correct incantation seems to be
>>
>> postscript(font="URWHelvetica", encoding="ISOLatin7")
>> plot(0,main=tolower("\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u017d"))
>> dev.off()
>>     
>
> The encoding should happen automagically in a Lithuanian UTF-8 locale, and 
> does for me.  But suitable fonts (e.g. URW ones) are needed.
>   
OK, I sort of suspected that, although it wasn't entirely clear to me
whether autoconversion would cover cases like en_LT.utf8, if that even
exists. Still, the explicit (portable?) way of doing it is probably
worth knowing too (there could be a few pitfalls with scripts getting
run outside their usual domain).


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jrkrideau at yahoo.ca  Fri Jul 13 15:47:38 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 13 Jul 2007 09:47:38 -0400 (EDT)
Subject: [R] how to create matrix in a loop
In-Reply-To: <33846cd50707130605r5e17b210mf4b7977140518c56@mail.gmail.com>
Message-ID: <58230.82915.qm@web32803.mail.mud.yahoo.com>

You don't need to do a loop.  You can do it with an
apply statement I think. See ?apply

If the matrix is "mat" to do the test by column try:

apply(mat, 2, shapiro.test)


--- Tavpritesh Sethi <tavpritesh at gmail.com> wrote:

> Hi all,
> How does one create a matrix of values in a loop.
> For example, I have 46*70
> matrix with columns giving the values for some blood
> tests performed upon
> the 46 patients(rows). I want to perform a simple
> test of normality
> (shapiro-wilk test) upon each of the biochemical
> tests for the 46
> patients. Can somebody suggest the syntax for such a
> loop. For the data x, I
> wrote the following loop which didnt work:-
> pv<-1:70
> stat<-1:70
> for(i in 1:70)
> {
> pv[i]=shapiro.test(x[,i])$p.value
> stat[i]<-shapiro.test(x[,i])$statistic
> }
> warning msg: items to replace is not a multiple of
> replacement length
> 
> Does it have anything to do with the method of
> importing data as
> read.delimrather than
> read.table?
> Please suggest


From Karl.Hufthammer at math.uib.no  Fri Jul 13 15:48:02 2007
From: Karl.Hufthammer at math.uib.no (Karl Ove Hufthammer)
Date: Fri, 13 Jul 2007 15:48:02 +0200
Subject: [R] Choosing the number of colour breaks in ggplot2
Message-ID: <f77vqi$l9v$1@sea.gmane.org>

A seemingly simple problem has me stumped. Is it possible to choose the
number of colour breaks for a gradient scale in the current version of
ggplot2?

Here is a simple example:

---------------------------------------------
x=-10:10
y=-10:10
dat=expand.grid(x=x,y=y)
dat$z=dat$x^2+dat$y^2-100

ggplot(dat, mapping=aes(x=x, y=y, fill=z)) +
  geom_tile() + scale_fill_gradient2()
---------------------------------------------

The image shows many (61) colours, but only 5 of them are shown in the
legend. How do I change the legend to show, say, 10 colours?

-- 
Karl Ove Hufthammer


From renger at ecoplan.ch  Fri Jul 13 15:57:29 2007
From: renger at ecoplan.ch (Renger van Nieuwkoop)
Date: Fri, 13 Jul 2007 15:57:29 +0200
Subject: [R] Saving workspace data
Message-ID: <7CD806EFDC19B64E82120E3FC0ADF6A31356CE@mx.ecoplan.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/ab559905/attachment.pl 

From h.wickham at gmail.com  Fri Jul 13 16:05:52 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 13 Jul 2007 16:05:52 +0200
Subject: [R] Choosing the number of colour breaks in ggplot2
In-Reply-To: <f77vqi$l9v$1@sea.gmane.org>
References: <f77vqi$l9v$1@sea.gmane.org>
Message-ID: <f8e6ff050707130705x64662b5au3b9e6c2dfc0ee432@mail.gmail.com>

Hi Karl,

There's no official way to do it, but you can "hack" the colour
gradient scale to do what you want:

x=-10:10
y=-10:10
dat=expand.grid(x=x,y=y)
dat$z=dat$x^2+dat$y^2-100

# Create a modified scale
gr <- scale_fill_gradient2()$clone()
gr$breaks <- function(.) seq(-100, 100, by=10)

ggplot(dat, mapping=aes(x=x, y=y, fill=z)) +
 geom_tile() + gr

# Or to use the range of the scale if you don't want to set it by hand
gr$breaks <- function(.) seq(.$frange()[1], .$frange()[2], length=10)

This works because ggplot2 is built on top of the proto library and
has mutable objects.  Most of the time you don't notice this because
the default functions operate with R's copy-on-modify semantics.

Hadley

On 7/13/07, Karl Ove Hufthammer <Karl.Hufthammer at math.uib.no> wrote:
> A seemingly simple problem has me stumped. Is it possible to choose the
> number of colour breaks for a gradient scale in the current version of
> ggplot2?
>
> Here is a simple example:
>
> ---------------------------------------------
> x=-10:10
> y=-10:10
> dat=expand.grid(x=x,y=y)
> dat$z=dat$x^2+dat$y^2-100
>
> ggplot(dat, mapping=aes(x=x, y=y, fill=z)) +
>   geom_tile() + scale_fill_gradient2()
> ---------------------------------------------
>
> The image shows many (61) colours, but only 5 of them are shown in the
> legend. How do I change the legend to show, say, 10 colours?
>
>
> --
> Karl Ove Hufthammer
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From torma at ilab.sztaki.hu  Fri Jul 13 16:11:39 2007
From: torma at ilab.sztaki.hu (Balazs Torma)
Date: Fri, 13 Jul 2007 16:11:39 +0200
Subject: [R] counting occurances of matching rows in a matrix
Message-ID: <E1I9Lrv-0006Z1-W2@monster.ilab.sztaki.hu>

I need help regarding to the following problem:

Consider this matrix:

> M <- matrix(c(1,2, 4,3, 1, 2),3, 2, byrow=TRUE)
> M
     [,1] [,2]
[1,]    1    2
[2,]    4    3
[3,]    1    2

I would like to have a matrix which counts the identical rows and places the counts into its third column. I tried with ftable():

> as.data.frame(ftable(M[,1], M[,2]))
  Var1 Var2 Freq
1    1    2    2
2    4    2    0
3    1    3    0
4    4    3    1

This would be exactly what I want without the 0-freq rows. The problem is that ftable() counts the occurances of all possible factor combinations, which is inproper because the real matrix is very long for which I want to count. (I know that I could filter out 0-frequencies afterwards but I would like ftable not to produce 0-frequencies unnecessarily (it would consume too much space)).

I tried table() too, but I couldn't tell it that it should consider each row as one data point, and not all elements in the matrix separately:

> as.data.frame(table(M))
  M Freq
1 1    2
2 2    2
3 3    1
4 4    1

Any help greatly appreciated!
Balazs Torma


From CaskenetteA at dfo-mpo.gc.ca  Fri Jul 13 16:21:04 2007
From: CaskenetteA at dfo-mpo.gc.ca (Caskenette, Amanda)
Date: Fri, 13 Jul 2007 10:21:04 -0400
Subject: [R] Correlation matrix
Message-ID: <A61708A2D512974BAD5C77CD88B83C08BC1A4D@lauimlex01.lau.dfo-mpo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/2068f236/attachment.pl 

From maechler at stat.math.ethz.ch  Fri Jul 13 16:28:36 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 13 Jul 2007 16:28:36 +0200
Subject: [R] nearest correlation to polychoric
In-Reply-To: <005201c7c54b$5dcaf670$0540210a@www.domain>
References: <20070713122554.32910@gmx.net>
	<005201c7c54b$5dcaf670$0540210a@www.domain>
Message-ID: <18071.35860.790041.559130@stat.math.ethz.ch>

>>>>> "DR" == Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be>
>>>>>     on Fri, 13 Jul 2007 14:43:08 +0200 writes:

    DR> you could also have a look at function posdefify() from
    DR> package `sfsmisc'.

    DR> I hope it helps.

Yes, thanks, Dimitris; note that my posdefify() function uses a
pretty arbitrary "fudge" value for posdefification, namely
      eps.ev = 1e-07

As a matter of fact, earlier this week (in "the first
international R/Rmetrics workshop"),
I've talked to people from finance who also need that (or
something better?).

Jordi Molins Coronado (Madrid) drew my attention to an idea
he found in the book (English re-edition of French as of 1996)
Jean-Philippe Bouchaud (2000)
    Theory of Financial Risk and Derivative Pricing:
    From Statistical Physics to Risk Management

which supposedly uses theory of random matrices and
the entailing distribution of random eigenvalues in order to
find a more sensible cutoff than my 'eps.ev' default of 1e-7.
Unfortunately that book is checked out from our library and I
can't have a look.   Googling and Wikipedia seem to indicate to
me that most of the random matrix theory does not directly apply
here, since I'm really interested in the spectrum of X'X where X
is a de-meaned n x p random matrix.


OTOH, help(posdefify) already mentions more sophisticated
approaches to the problem, the one I (as I vaguely remember)
should be made available being 

  Cheng, Sheung Hun and Higham, Nick (1998)
  A Modified Cholesky Algorithm Based on a Symmetric Indefinite Factorization;
  \emph{SIAM J. Matrix Anal.\ Appl.}, \bold{19}, 1097--1110.


Jens, could you make your code (mentioned below) available to
the community, or even donate to be included as a new method of
posdefify() ?

Regards,
Martin Maechler, ETH Zurich


    DR> Best, Dimitris

    DR> ---- Dimitris Rizopoulos Ph.D. Student Biostatistical
    DR> Centre School of Public Health Catholic University of
    DR> Leuven

    DR> Address: Kapucijnenvoer 35, Leuven, Belgium Tel:
    DR> +32/(0)16/336899 Fax: +32/(0)16/337015 Web:
    DR> http://med.kuleuven.be/biostat/
    DR> http://www.student.kuleuven.be/~m0390867/dimitris.htm


    DR> ----- Original Message ----- From: ""Jens Oehlschl?gel""
    DR> <oehl_list at gmx.de> To: <r-help at stat.math.ethz.ch> Sent:
    DR> Friday, July 13, 2007 2:25 PM Subject: [R] nearest
    DR> correlation to polychoric


    DR> Dear all,

    DR> Has someone implemented in R (or any other language)

    DR> Knol DL, ten Berge JMF.  Least-squares approximation of
    DR> an improper correlation matrix by a proper one.
    DR> Psychometrika, 1989, 54, 53-61.

    DR> or any other similar algorithm?

    DR> Best regards


    DR> Jens Oehlschl?gel


    DR> Background:

    DR> I want to factanal() matrices of polychoric correlations
    DR> which have negative eigenvalue. I coded

    DR> Highham 2002 Computing the nearest correlation matrix -
    DR> a problem from finance, IMA Journal of Numerical
    DR> Analysis (2002), 22, 329-343.

    DR> which basically works but still leaves very small
    DR> negative eigenvalues which causes factanal() to fail
    DR> with

    >> factanal(cov=ncor$cor, factors=2)
    DR> Fehler in optim(start, FAfn, FAgr, method = "L-BFGS-B",
    DR> lower = lower, : L-BFGS-B ben?tigt endliche Werte von
    DR> 'fn' Zus?tzlich: Warning message: NaNs wurden erzeugt
    DR> in: log(x)
    >> version
    DR>                _ platform i386-pc-mingw32 arch i386 os
    DR> mingw32 system i386, mingw32 status major 2 minor 4.1
    DR> year 2006 month 12 day 18 svn rev 40228 language R
    DR> version.string R version 2.4.1 (2006-12-18)


From dimitris.rizopoulos at med.kuleuven.be  Fri Jul 13 16:45:25 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 13 Jul 2007 16:45:25 +0200
Subject: [R] counting occurances of matching rows in a matrix
References: <E1I9Lrv-0006Z1-W2@monster.ilab.sztaki.hu>
Message-ID: <00d701c7c55c$72c7d370$0540210a@www.domain>

one way is the following (maybe there're better):

pats <- do.call(paste, c(as.data.frame(M), sep = "\r"))
pats <- factor(pats, levels = unique(pats))
cbind(unique(M), Freq = as.vector(table(pats)))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Balazs Torma" <torma at ilab.sztaki.hu>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, July 13, 2007 4:11 PM
Subject: [R] counting occurances of matching rows in a matrix


>I need help regarding to the following problem:
>
> Consider this matrix:
>
>> M <- matrix(c(1,2, 4,3, 1, 2),3, 2, byrow=TRUE)
>> M
>     [,1] [,2]
> [1,]    1    2
> [2,]    4    3
> [3,]    1    2
>
> I would like to have a matrix which counts the identical rows and 
> places the counts into its third column. I tried with ftable():
>
>> as.data.frame(ftable(M[,1], M[,2]))
>  Var1 Var2 Freq
> 1    1    2    2
> 2    4    2    0
> 3    1    3    0
> 4    4    3    1
>
> This would be exactly what I want without the 0-freq rows. The 
> problem is that ftable() counts the occurances of all possible 
> factor combinations, which is inproper because the real matrix is 
> very long for which I want to count. (I know that I could filter out 
> 0-frequencies afterwards but I would like ftable not to produce 
> 0-frequencies unnecessarily (it would consume too much space)).
>
> I tried table() too, but I couldn't tell it that it should consider 
> each row as one data point, and not all elements in the matrix 
> separately:
>
>> as.data.frame(table(M))
>  M Freq
> 1 1    2
> 2 2    2
> 3 3    1
> 4 4    1
>
> Any help greatly appreciated!
> Balazs Torma
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From deb37 at columbia.edu  Fri Jul 13 16:47:40 2007
From: deb37 at columbia.edu (Daniel E. Bunker)
Date: Fri, 13 Jul 2007 10:47:40 -0400
Subject: [R] convhulln {geometry} output from .call
Message-ID: <3007904A-C67D-4285-8B12-4BB2EE2EE446@columbia.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/5a14fa09/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Fri Jul 13 16:50:48 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Fri, 13 Jul 2007 16:50:48 +0200
Subject: [R] Correlation matrix
In-Reply-To: <A61708A2D512974BAD5C77CD88B83C08BC1A4D@lauimlex01.lau.dfo-mpo.ca>
References: <A61708A2D512974BAD5C77CD88B83C08BC1A4D@lauimlex01.lau.dfo-mpo.ca>
Message-ID: <46979148.9000404@biostat.ku.dk>

Caskenette, Amanda wrote:
> I have a model with 5 parameters that I am optimising where the (best)
> value of the objective function is negative. I would like to use the
> Hessian matrix (from genoud and/or optim functions)  to construct  the
> covariance and correlation matrices.
>
>   This is the code that I am using:
>
>   est <- out$par                  # Parameter estimates 
>   H <- out$hessian             # Hessian 
>   V <- solve(H)                   # Covariance matrix
>   s <- sqrt(abs(diag(V)))    # Vector of standard deviations 
>   cor <- V/(s%o%s)            # Correlation coefficient matrix 
>   ci <- est+qnorm(0.975)*s%o%c(-1,1) # 95% CI's
>
> However I am getting values that are greater than 1 (1.05, 2.34, etc)
> for the correlation matrix. Might this be due to the fact that the
> out$val is negative?
>
>   

Not by itself (just add a large enough constant to the objective
function and the value becomes positive without changing the Hessian).

More likely, you have not actually found the minimum (Hessian not
positive definite), or there is a code error.

Print out and review the following items:

    H, eigen(H), V, s, s%o%s

and see if that makes you any wiser (why are you taking abs(diag(V))?
Negative elements?)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From av7000 at googlemail.com  Fri Jul 13 16:51:11 2007
From: av7000 at googlemail.com (Alejandro Veen)
Date: Fri, 13 Jul 2007 10:51:11 -0400
Subject: [R] spatstat - Fitting a Strauss model with trend determined by
	kernel density smoother
Message-ID: <8df4d8350707130751v1df6fc7q20339bf3391c5350@mail.gmail.com>

Dear r-help,

I would like to use the 'ppm' function of the 'spatstat' package to
fit a Strauss inhibition model.  I understand that I can specify a
parametric model for the "background" trend, but how would I specify a
trend which is estimated using a Kernel density smoother?

In particular, I would like to use the 'kde' function of the 'ks'
package to estimate the "background" intensity and then use this as
the trend for a Strauss inhibition process.

Thanks already in advance,

Alejandro Veen


From rvaradhan at jhmi.edu  Fri Jul 13 16:52:36 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Fri, 13 Jul 2007 10:52:36 -0400
Subject: [R] nearest correlation to polychoric
In-Reply-To: <18071.35860.790041.559130@stat.math.ethz.ch>
References: <20070713122554.32910@gmx.net>
	<005201c7c54b$5dcaf670$0540210a@www.domain>
	<18071.35860.790041.559130@stat.math.ethz.ch>
Message-ID: <000001c7c55d$73c9e4b0$7c94100a@win.ad.jhu.edu>

Martin,

I sent you the Matlab code for this which I had obtained from Nich Higham. 

Cheng, Sheung Hun and Higham, Nick (1998)
  A Modified Cholesky Algorithm Based on a Symmetric Indefinite
Factorization;
  \emph{SIAM J. Matrix Anal.\ Appl.}, \bold{19}, 1097--1110.

Do you remember?

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Maechler
Sent: Friday, July 13, 2007 10:29 AM
To: Dimitris Rizopoulos
Cc: "Jens Oehlschl?gel"; r-help at stat.math.ethz.ch
Subject: Re: [R] nearest correlation to polychoric

>>>>> "DR" == Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be>
>>>>>     on Fri, 13 Jul 2007 14:43:08 +0200 writes:

    DR> you could also have a look at function posdefify() from
    DR> package `sfsmisc'.

    DR> I hope it helps.

Yes, thanks, Dimitris; note that my posdefify() function uses a
pretty arbitrary "fudge" value for posdefification, namely
      eps.ev = 1e-07

As a matter of fact, earlier this week (in "the first
international R/Rmetrics workshop"),
I've talked to people from finance who also need that (or
something better?).

Jordi Molins Coronado (Madrid) drew my attention to an idea
he found in the book (English re-edition of French as of 1996)
Jean-Philippe Bouchaud (2000)
    Theory of Financial Risk and Derivative Pricing:
    From Statistical Physics to Risk Management

which supposedly uses theory of random matrices and
the entailing distribution of random eigenvalues in order to
find a more sensible cutoff than my 'eps.ev' default of 1e-7.
Unfortunately that book is checked out from our library and I
can't have a look.   Googling and Wikipedia seem to indicate to
me that most of the random matrix theory does not directly apply
here, since I'm really interested in the spectrum of X'X where X
is a de-meaned n x p random matrix.


OTOH, help(posdefify) already mentions more sophisticated
approaches to the problem, the one I (as I vaguely remember)
should be made available being 

  Cheng, Sheung Hun and Higham, Nick (1998)
  A Modified Cholesky Algorithm Based on a Symmetric Indefinite
Factorization;
  \emph{SIAM J. Matrix Anal.\ Appl.}, \bold{19}, 1097--1110.


Jens, could you make your code (mentioned below) available to
the community, or even donate to be included as a new method of
posdefify() ?

Regards,
Martin Maechler, ETH Zurich


    DR> Best, Dimitris

    DR> ---- Dimitris Rizopoulos Ph.D. Student Biostatistical
    DR> Centre School of Public Health Catholic University of
    DR> Leuven

    DR> Address: Kapucijnenvoer 35, Leuven, Belgium Tel:
    DR> +32/(0)16/336899 Fax: +32/(0)16/337015 Web:
    DR> http://med.kuleuven.be/biostat/
    DR> http://www.student.kuleuven.be/~m0390867/dimitris.htm


    DR> ----- Original Message ----- From: ""Jens Oehlschl?gel""
    DR> <oehl_list at gmx.de> To: <r-help at stat.math.ethz.ch> Sent:
    DR> Friday, July 13, 2007 2:25 PM Subject: [R] nearest
    DR> correlation to polychoric


    DR> Dear all,

    DR> Has someone implemented in R (or any other language)

    DR> Knol DL, ten Berge JMF.  Least-squares approximation of
    DR> an improper correlation matrix by a proper one.
    DR> Psychometrika, 1989, 54, 53-61.

    DR> or any other similar algorithm?

    DR> Best regards


    DR> Jens Oehlschl?gel


    DR> Background:

    DR> I want to factanal() matrices of polychoric correlations
    DR> which have negative eigenvalue. I coded

    DR> Highham 2002 Computing the nearest correlation matrix -
    DR> a problem from finance, IMA Journal of Numerical
    DR> Analysis (2002), 22, 329-343.

    DR> which basically works but still leaves very small
    DR> negative eigenvalues which causes factanal() to fail
    DR> with

    >> factanal(cov=ncor$cor, factors=2)
    DR> Fehler in optim(start, FAfn, FAgr, method = "L-BFGS-B",
    DR> lower = lower, : L-BFGS-B ben?tigt endliche Werte von
    DR> 'fn' Zus?tzlich: Warning message: NaNs wurden erzeugt
    DR> in: log(x)
    >> version
    DR>                _ platform i386-pc-mingw32 arch i386 os
    DR> mingw32 system i386, mingw32 status major 2 minor 4.1
    DR> year 2006 month 12 day 18 svn rev 40228 language R
    DR> version.string R version 2.4.1 (2006-12-18)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From johannes_graumann at web.de  Fri Jul 13 16:54:50 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Fri, 13 Jul 2007 16:54:50 +0200
Subject: [R] Algorythmic Question on Array Filtration
Message-ID: <f783nq$2ap$1@sea.gmane.org>

Dear All,

I have a data frame with the columns "Mass" and "Intensity" (this is mass
spectrometry stuff). Each of the mass values gives rise to a mass window of
5 ppm around the individual mass (from mass - mass/1E6*5 to mass +
mass/1E5*5). I need to filter the array such that in case these mass
windows overlap I retain the mass/intensity pair with the highest
intensity.
I apologize for this question, but I have no formal IT education and would
value any nudges toward favorable algorithmic solutions highly.

Thanks for any help,

Joh


From ripley at stats.ox.ac.uk  Fri Jul 13 16:56:45 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Jul 2007 15:56:45 +0100 (BST)
Subject: [R] charset in graphics
In-Reply-To: <469780DE.30105@biostat.ku.dk>
References: <200707131150.44106.dgvirtual@akl.lt>
	<469755B6.9030300@biostat.ku.dk>
	<Pine.LNX.4.64.0707131342080.1162@auk.stats>
	<469780DE.30105@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0707131541080.14348@gannet.stats.ox.ac.uk>

On Fri, 13 Jul 2007, Peter Dalgaard wrote:

> Prof Brian Ripley wrote:
>> On Fri, 13 Jul 2007, Peter Dalgaard wrote:
>>
>>> The correct incantation seems to be
>>>
>>> postscript(font="URWHelvetica", encoding="ISOLatin7")
>>> plot(0,main=tolower("\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u017d"))
>>> dev.off()
>>>
>> The encoding should happen automagically in a Lithuanian UTF-8 locale, and
>> does for me.  But suitable fonts (e.g. URW ones) are needed.
>>
> OK, I sort of suspected that, although it wasn't entirely clear to me
> whether autoconversion would cover cases like en_LT.utf8, if that even
> exists.

The locale would need to be in the Lithuanian language (ISO 639 code lt or 
lit), not some language in Lithuania (ISO 3166 code LT).

> Still, the explicit (portable?) way of doing it is probably
> worth knowing too (there could be a few pitfalls with scripts getting
> run outside their usual domain).

Yes, the appropriate 8-bit encoding can only be a guess, and someone might 
be writing French or Japanese in lt_LT.utf8.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From CaskenetteA at dfo-mpo.gc.ca  Fri Jul 13 17:25:18 2007
From: CaskenetteA at dfo-mpo.gc.ca (Caskenette, Amanda)
Date: Fri, 13 Jul 2007 11:25:18 -0400
Subject: [R] Correlation matrix
In-Reply-To: <46979148.9000404@biostat.ku.dk>
References: <A61708A2D512974BAD5C77CD88B83C08BC1A4D@lauimlex01.lau.dfo-mpo.ca>
	<46979148.9000404@biostat.ku.dk>
Message-ID: <A61708A2D512974BAD5C77CD88B83C08BC1AF5@lauimlex01.lau.dfo-mpo.ca>

Thank you very much for the quick response,

Yes that is correct, some values for V are negative. 
I wish that looking at the H, eigen(H), V, s, s%o%s made me wiser, however I really have no idea what I am looking for. 

The genoud function does not converge if left running for a week so I have a feeling that the real minimum will not be found...however the solutions seem only to differ in the 3rd or 4th decimal place when allowed to run for a week or for a day. Does that mean that the Hessian matrix is useless to me? 

Here is the objective function:

objective= (years*age)/2*log(sigma_s^2) + 1/(2*sigma_s^2) * sum((log(result.data) - log(result.sim))^2)

Where sigma_s, the error component, is one of the parameters estimated by the model, result.data is the data I am fitting, and result.sim is the output of the model.

Amanda


-----Original Message-----
From: Peter Dalgaard [mailto:P.Dalgaard at biostat.ku.dk] 
Sent: July 13, 2007 10:51 AM
To: Caskenette, Amanda
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Correlation matrix

Caskenette, Amanda wrote:
> I have a model with 5 parameters that I am optimising where the (best) 
> value of the objective function is negative. I would like to use the 
> Hessian matrix (from genoud and/or optim functions)  to construct  the 
> covariance and correlation matrices.
>
>   This is the code that I am using:
>
>   est <- out$par                  # Parameter estimates 
>   H <- out$hessian             # Hessian 
>   V <- solve(H)                   # Covariance matrix
>   s <- sqrt(abs(diag(V)))    # Vector of standard deviations 
>   cor <- V/(s%o%s)            # Correlation coefficient matrix 
>   ci <- est+qnorm(0.975)*s%o%c(-1,1) # 95% CI's
>
> However I am getting values that are greater than 1 (1.05, 2.34, etc) 
> for the correlation matrix. Might this be due to the fact that the 
> out$val is negative?
>
>   

Not by itself (just add a large enough constant to the objective function and the value becomes positive without changing the Hessian).

More likely, you have not actually found the minimum (Hessian not positive definite), or there is a code error.

Print out and review the following items:

    H, eigen(H), V, s, s%o%s

and see if that makes you any wiser (why are you taking abs(diag(V))?
Negative elements?)

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From deepayan.sarkar at gmail.com  Fri Jul 13 18:11:23 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Fri, 13 Jul 2007 09:11:23 -0700
Subject: [R] Direction of panel plots in trellis graphics
In-Reply-To: <20070713092622.CGJ22729@po-d.temple.edu>
References: <20070713092622.CGJ22729@po-d.temple.edu>
Message-ID: <eb555e660707130911i1c19755dua7440f9aa4f1a892@mail.gmail.com>

On 7/13/07, Richard M. Heiberger <rmh at temple.edu> wrote:
> You can control the panel sequence with subscripting and transpose.
> Here are several examples.  I think tmp.tr3 is the one you asked for.
>
> library(lattice)
>
> tmp <- data.frame(x=rnorm(24), y=rnorm(24), a=rep(letters[1:6],4),
> b=rep(LETTERS[1:4],each=6))
> tmp.tr <- xyplot(y ~ x | a*b, data=tmp)
> tmp.tr
> t(tmp.tr)
>
> tmp.tr2 <- xyplot(y ~ x | a, data=tmp)
> tmp.tr2
> tmp.tr2[c(1,3,5,2,4,6)]
>
> tmp.tr3 <- xyplot(y ~ x | a, data=tmp, as.table=TRUE)
> tmp.tr3
> tmp.tr3[c(1,3,5,2,4,6)]

Another high level option is to change the rule determining how
packets are chosen for a given panel in the layout.

print(tmp.tr3,
      packet.panel = function(layout, row, column, ...) {
          layout <- layout[c(2, 1, 3)]
          packet.panel.default(layout = layout,
                               row = column,
                               column = row, ...)
      })

This effectively transposes the layout, which (along with
as.table=TRUE) is what you want.

-Deepayan


From ccleland at optonline.net  Fri Jul 13 18:37:05 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 13 Jul 2007 12:37:05 -0400
Subject: [R] how to estimate treatment-interaction contrasts
In-Reply-To: <20070712161628.h4rqf4j48wwks444@webmail.uoguelph.ca>
References: <20070712161628.h4rqf4j48wwks444@webmail.uoguelph.ca>
Message-ID: <4697AA31.403@optonline.net>

szhan at uoguelph.ca wrote:
> Hello, R experts,
> Sorry for asking this question again again since I really want a help!
> 
> I have a two-factor experiment data and like to calculate estimates of
> interation contrasts say factor A has levels of a1, a2, and B has
> levels of b1, b2, b3, b4, and b5 with 3 replicates. I am not sure the
> constrast estimate I got is right using the script below:
> 
> score<-c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
> 7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)
> 
> A <- gl(2, 15, labels=c("a1", "a2"))
> B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)
> 
> contrasts(B)<-cbind(c(-4,rep(1,4)),c(rep(-3,2),rep(2,3)),
> +  c(rep(-2,3),rep(3,2)),c(rep(-1,4), rep(4,1)))
> fit1 <- aov(score ~ A*B)
> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>                Df Sum Sq Mean Sq F value    Pr(>F)
> A            1 3.2013  3.2013 15.1483 0.0009054 ***
> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>      B: C1      1 0.0301  0.0301  0.1424 0.7099296
>      B: C2      1 2.0335  2.0335  9.6221 0.0056199 **
>      B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>      B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>      A:B: C1    1 0.7207  0.7207  3.4105 0.0796342 .
>      A:B: C2    1 2.6068  2.6068 12.3350 0.0021927 **
>      A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>      A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
> Residuals   20 4.2267  0.2113
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Now I like to get interaction contrast estimate for b1 and b2 vs b3, b4 and b5
> cont <- c(1, -1)[A] * c(-3, -3, 2, 2, 2)[B]
> 
> estimat<-sum(cont*score) # value of the contrast estimate for A:B C2
> 
>> estimat
> [1] -24.1
> 
> I am not sure the estimate for A:B C2 contrast  (-24.1) is correct
> because the F value given the output above(12.3350) does not equal to
> those I calculate below (15.2684):
> 
> t.stat <- sum(cont*score)/se.contrast(fit1, as.matrix(cont))
>> t.stat^2
> Contrast 1
>      15.2684
> 
> Could you please help me calculate the correct the estimate of
> interaction contrast and corresponding F value?
> Thanks in advance!
> Joshua

  If the contrasts for B are orthogonal, then you get the result you
expected:

score <- c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
           7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)

A <- gl(2, 15, labels=c("a1", "a2"))
B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)

contrasts(B) <- matrix(c(3, -1,  0,  0,
                         3,  1,  0,  0,
                        -2,  0,  2,  0,
                        -2,  0, -1,  1,
                        -2,  0, -1, -1), ncol=4, byrow=TRUE)

fit1 <- aov(score ~ A*B)

summary(fit1, split=list(B=1:4), expand.split = TRUE)

            Df Sum Sq Mean Sq F value    Pr(>F)
A            1 3.2013  3.2013 15.1483 0.0009054 ***
B            4 8.7780  2.1945 10.3841 0.0001019 ***
  B: C1      1 1.0427  1.0427  4.9340 0.0380408 *
  B: C2      1 1.0208  1.0208  4.8304 0.0399049 *
  B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
  B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
A:B          4 5.3420  1.3355  6.3194 0.0018616 **
  A:B: C1    1 3.2267  3.2267 15.2684 0.0008734 ***
  A:B: C2    1 0.1008  0.1008  0.4771 0.4976647
  A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
  A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
Residuals   20 4.2267  0.2113
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

  Note that I put the contrast of interest for B in the first column of
the contrast matrix.

hope this helps,

Chuck

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ramasamy at cancer.org.uk  Fri Jul 13 18:45:51 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 13 Jul 2007 17:45:51 +0100
Subject: [R] legend and x,y cordinate values
In-Reply-To: <Pine.LNX.4.61.0707131113440.14489@som.iitb.ac.in>
References: <Pine.LNX.4.61.0707122149350.10257@som.iitb.ac.in>
	<Pine.LNX.4.61.0707131113440.14489@som.iitb.ac.in>
Message-ID: <4697AC3F.2070106@cancer.org.uk>

See help(legend) and help(identify).

Ajay Singh wrote:
> Hi,
> 
> I have two problems in R.
> 
> 1. I need 10 cdfs on a graph, the graph needs to have legend. Can you let 
> me know how to get legend on the graph?
> 
> 2. In ecdf plot, I need to know the x and y co-ordinates. I have to get 
> corresponding y coordinate values to x coordinate value so that I could be 
> able to know the particular percentile value to the x-coordinate value. 
> Can you let me know how could I be able the corresponding values of x to 
> the y coordinates?
> 
> Thanking you,
> Looking forward to your kind response,
> Sincerely,
> Ajay.


From ramasamy at cancer.org.uk  Fri Jul 13 18:51:53 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 13 Jul 2007 17:51:53 +0100
Subject: [R] Algorythmic Question on Array Filtration
In-Reply-To: <f783nq$2ap$1@sea.gmane.org>
References: <f783nq$2ap$1@sea.gmane.org>
Message-ID: <4697ADA9.1010101@cancer.org.uk>

Sorry, this sounds like a fairly basic question that can be resolved by 
which() and possible ifelse(). There is no details in your email.

I am afraid you have to learn the basics of R or ask question with more 
details (e.g. example data).

Or ask someone locally.

Regards, Adai



Johannes Graumann wrote:
> Dear All,
> 
> I have a data frame with the columns "Mass" and "Intensity" (this is mass
> spectrometry stuff). Each of the mass values gives rise to a mass window of
> 5 ppm around the individual mass (from mass - mass/1E6*5 to mass +
> mass/1E5*5). I need to filter the array such that in case these mass
> windows overlap I retain the mass/intensity pair with the highest
> intensity.
> I apologize for this question, but I have no formal IT education and would
> value any nudges toward favorable algorithmic solutions highly.
> 
> Thanks for any help,
> 
> Joh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From jrkrideau at yahoo.ca  Fri Jul 13 19:19:13 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 13 Jul 2007 13:19:13 -0400 (EDT)
Subject: [R] Algorythmic Question on Array Filtration
In-Reply-To: <f783nq$2ap$1@sea.gmane.org>
Message-ID: <445456.26729.qm@web32811.mail.mud.yahoo.com>

I think we need a bit more information and perhaps a
small example data set to see what you want.  

I am not familiar with term mass window. Is this a
confidence interval around the mass value? 


--- Johannes Graumann <johannes_graumann at web.de>
wrote:

> Dear All,
> 
> I have a data frame with the columns "Mass" and
> "Intensity" (this is mass
> spectrometry stuff). Each of the mass values gives
> rise to a mass window of
> 5 ppm around the individual mass (from mass -
> mass/1E6*5 to mass +
> mass/1E5*5). I need to filter the array such that in
> case these mass
> windows overlap I retain the mass/intensity pair
> with the highest
> intensity.
> I apologize for this question, but I have no formal
> IT education and would
> value any nudges toward favorable algorithmic
> solutions highly.
> 
> Thanks for any help,
> 
> Joh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From jrkrideau at yahoo.ca  Fri Jul 13 19:22:35 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 13 Jul 2007 13:22:35 -0400 (EDT)
Subject: [R] Saving workspace data
In-Reply-To: <7CD806EFDC19B64E82120E3FC0ADF6A31356CE@mx.ecoplan.local>
Message-ID: <368485.29041.qm@web32807.mail.mud.yahoo.com>


--- Renger van Nieuwkoop <renger at ecoplan.ch> wrote:

> Hi
> 
> I imported a lot of text-files with R and saved them
> by using:
>     save(list=ls(),file="Grunddaten.Rdata")
> 
> After that I wanted to check my saved data and wrote
>     source("Grunddaten.Rdata")
> 
> This gives me an error:
> 	Error in parse(file, n = -1, NULL, "?") : Syntax
> error at 1: 
> 
> Can somebody tell me what I am doing wrong (I am
> using R.2.4.1)?
> 
> Renger

source is to load a script or programme.  Try load()


From opomporay at yahoo.com  Fri Jul 13 19:38:12 2007
From: opomporay at yahoo.com (Shaibu Pompo)
Date: Fri, 13 Jul 2007 10:38:12 -0700 (PDT)
Subject: [R] Lapack Routine Error
Message-ID: <498375.17432.qm@web60016.mail.yahoo.com>

Hello

I am running a simulation study to assess the type I error rate of a likelihood ratio test. This requires me to minimize over restricted spaces under the null and under the alternative hypothesis. My problem is that occassionally I run into an error message that reads: "Error in solve.default(cov, ...) : Lapack routine dgesv: system is exactly singular" when I make a call to "nlminb". The negative log likelihood function being minimized is based on the multivariate normal distribution. Can anyone help me with the source of such an error message? Or better still how to get at the source?

Thanks,
SP: opomporay at yahoo.com




       
____________________________________________________________________________________
Got a little couch potato? 
Check out fun summer activities for kids.


From xli28 at uic.edu  Fri Jul 13 19:46:00 2007
From: xli28 at uic.edu (Li, Xue)
Date: Fri, 13 Jul 2007 12:46:00 -0500 (CDT)
Subject: [R] (no subject)
Message-ID: <1610.128.248.54.123.1184348760.squirrel@webmail.uic.edu>

Hi,

I want to know how to increase the limit size of R object. For example, i
want to store a matrix (10000 rows and 8000 columns)into an object x.
However, it gives me an error message 'can't allocate a vector of size
396.7 Mb'. But there is no problem to store a matrix of 5000*3000 matrix
into x.

Thanks.

xue


From naik.raghu at gmail.com  Fri Jul 13 19:46:12 2007
From: naik.raghu at gmail.com (Raghu Naik)
Date: Fri, 13 Jul 2007 13:46:12 -0400
Subject: [R] THANK YOU: Updating R version
Message-ID: <c98270fd0707131046l662bdf9cm81fe165fdc5fafa0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/04bab734/attachment.pl 

From tom.cohen78 at yahoo.se  Fri Jul 13 20:04:43 2007
From: tom.cohen78 at yahoo.se (Tom Cohen)
Date: Fri, 13 Jul 2007 20:04:43 +0200 (CEST)
Subject: [R] help with handling replicates before reshaping data
Message-ID: <158791.10003.qm@web23005.mail.ird.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/76fda63d/attachment.pl 

From h.y.wong at leeds.ac.uk  Fri Jul 13 20:05:01 2007
From: h.y.wong at leeds.ac.uk (Yan Wong)
Date: Fri, 13 Jul 2007 19:05:01 +0100
Subject: [R] Direction of panel plots in trellis graphics
In-Reply-To: <eb555e660707130911i1c19755dua7440f9aa4f1a892@mail.gmail.com>
References: <20070713092622.CGJ22729@po-d.temple.edu>
	<eb555e660707130911i1c19755dua7440f9aa4f1a892@mail.gmail.com>
Message-ID: <6A727484-C9E5-4099-A4C7-CEC033553405@leeds.ac.uk>


On 13 Jul 2007, at 17:11, deepayan.sarkar at gmail.com wrote:

>
> Another high level option is to change the rule determining how
> packets are chosen for a given panel in the layout.
>
> print(tmp.tr3,
>      packet.panel = function(layout, row, column, ...) {
>          layout <- layout[c(2, 1, 3)]
>          packet.panel.default(layout = layout,
>                               row = column,
>                               column = row, ...)
>      })
>
> This effectively transposes the layout, which (along with
> as.table=TRUE) is what you want.

Thank you Deepayan, that does exactly what I want.

Also thanks for the other suggestions by Gabor and Richard.  
Unfortunately these don't quite work in my case, as I am printing to  
multiple pages (so Richard's suggestion of transposing becomes  
tricky), and my 2 conditioning variables are on the rhs and lhs of  
the formula (i.e. y1 + y2 = x | v) (so I can't easily create a new  
combination factor as in Gabor's situation).

Cheers

Yan


From h.wickham at gmail.com  Fri Jul 13 20:25:54 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 13 Jul 2007 20:25:54 +0200
Subject: [R] help with handling replicates before reshaping data
In-Reply-To: <158791.10003.qm@web23005.mail.ird.yahoo.com>
References: <158791.10003.qm@web23005.mail.ird.yahoo.com>
Message-ID: <f8e6ff050707131125r5353a482hf75c689ae3dbb4fc@mail.gmail.com>

Hi Tom,

>   I have a dataset consists of duplicated sequences within day for each patient (see below data) and I want to reshape the data with patient as time variable. However the reshape function only takes the first sequence of the replicates and ignores the second. How can I 1) average the duplicates and 2) give the duplicated sequences unique names before reshaping the data ?
>
>   > data
>      patient day  seq           y
>   1       10   1 acdf -0.52416066
>   2       10   1 cdsv  0.62551539
>   3       10   1 dlfg -1.54668047
>   4       10   1 acdf  0.82404978
>   5       10   1 cdsv -1.17459914
>   6       10   2 acdf  0.47238216

You mind find that the functions in the reshape package give you a bit
more flexibility.

# The reshape package expects data like to have
# the value variable named "value"
d2 <- rename(data, c("y" = "value"))

# I think this is the format you want, which will average over the reps
cast(d2, day + seq ~ patient, mean)


Hadley


From oehl_list at gmx.de  Fri Jul 13 20:42:14 2007
From: oehl_list at gmx.de (=?iso-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Fri, 13 Jul 2007 20:42:14 +0200
Subject: [R] nearest correlation to polychoric
Message-ID: <20070713184214.82160@gmx.net>

Dimitris,

Thanks a lot for the quick response with the pointer to posdefify. Using its logic as an afterburner to the algorithm of Higham seems to work.

Martin,

> Jens, could you make your code (mentioned below) available to the community, or even donate to be included as a new method of posdefify() ?

Nice opportunity to give-back. Below is the R code for nearcor and .Rd help file. A quite natural place for nearcor would be John Fox' package polycor, what do you think?

John?

Best regards


Jens Oehlschl?gel


# Copyright (2007) Jens Oehlschl?gel
# GPL licence, no warranty, use at your own risk

#! \name{nearcor}
#! \alias{nearcor}
#! \title{ function to find the nearest proper correlation matrix given an improper one }
#! \description{
#!   This function smooths a improper correlation matrix as it can result from \code{\link{cor}} with \code{use="pairwise.complete.obs"} or \code{\link[polycor]{hetcor}}.
#! }
#! \usage{
#! nearcor(R, eig.tol = 1e-06, conv.tol = 1e-07, posd.tol = 1e-08, maxits = 100, verbose = FALSE)
#! }
#! \arguments{
#!   \item{R}{ a square symmetric approximate correlation matrix }
#!   \item{eig.tol}{ defines relative positiveness of eigenvalues compared to largest, default=1.0e-6 }
#!   \item{conv.tol}{ convergence tolerance for algorithm, default=1.0e-7  }
#!   \item{posd.tol}{ tolerance for enforcing positive definiteness, default=1.0e-8 }
#!   \item{maxits}{ maximum number of iterations allowed }
#!   \item{verbose}{ set to TRUE to verbose convergence }
#! }
#! \details{
#!   This implements the algorithm of Higham (2002), then forces symmetry, then forces positive definiteness using code from \code{\link[sfsmisc]{posdefify}}.
#!   This implementation does not make use of direct LAPACK access for tuning purposes as in the MATLAB code of Lucas (2001).
#!   The algorithm of Knol DL and ten Berge (1989) (not implemented here) is more general in (1) that it allows contraints to fix some rows (and columns) of the matrix and (2) to force the smallest eigenvalue to have a certain value.
#! }
#! \value{
#!   A LIST, with components
#!   \item{cor}{resulting correlation matrix}
#!   \item{fnorm}{Froebenius norm of difference of input and output}
#!   \item{iterations}{number of iterations used}
#!   \item{converged}{logical}
#! }
#! \references{
#!        Knol, DL and ten Berge, JMF (1989). Least-squares approximation of an improper correlation matrix by a proper one.  Psychometrika, 54, 53-61.
#!   \cr  Higham (2002). Computing the nearest correlation matrix - a problem from finance, IMA Journal of Numerical Analysis, 22, 329-343.
#!   \cr  Lucas (2001). Computing nearest covariance and correlation matrices. A thesis submitted to the University of Manchester for the degree of Master of Science in the Faculty of Science and Engeneering.
#! }
#! \author{ Jens Oehlschl?gel }
#! \seealso{ \code{\link[polycor]{hetcor}}, \code{\link{eigen}}, \code{\link[sfsmisc]{posdefify}} }
#! \examples{
#!   cat("pr is the example matrix used in Knol DL, ten Berge (1989)\n")
#!   pr <- structure(c(1, 0.477, 0.644, 0.478, 0.651, 0.826, 0.477, 1, 0.516,
#!   0.233, 0.682, 0.75, 0.644, 0.516, 1, 0.599, 0.581, 0.742, 0.478,
#!   0.233, 0.599, 1, 0.741, 0.8, 0.651, 0.682, 0.581, 0.741, 1, 0.798,
#!   0.826, 0.75, 0.742, 0.8, 0.798, 1), .Dim = c(6, 6))
#!
#!   nr <- nearcor(pr)$cor
#!   plot(pr[lower.tri(pr)],nr[lower.tri(nr)])
#!   round(cbind(eigen(pr)$values, eigen(nr)$values), 8)
#!
#!   cat("The following will fail:\n")
#!   try(factanal(cov=pr, factors=2))
#!   cat("and this should work\n")
#!   try(factanal(cov=nr, factors=2))
#!
#!   \dontrun{
#!     library(polycor)
#!
#!     n <- 400
#!     x <- rnorm(n)
#!     y <- rnorm(n)
#!
#!     x1 <- (x + rnorm(n))/2
#!     x2 <- (x + rnorm(n))/2
#!     x3 <- (x + rnorm(n))/2
#!     x4 <- (x + rnorm(n))/2
#!
#!     y1 <- (y + rnorm(n))/2
#!     y2 <- (y + rnorm(n))/2
#!     y3 <- (y + rnorm(n))/2
#!     y4 <- (y + rnorm(n))/2
#!
#!     dat <- data.frame(x1, x2, x3, x4, y1, y2, y3, y4)
#!
#!     x1 <- ordered(as.integer(x1 > 0))
#!     x2 <- ordered(as.integer(x2 > 0))
#!     x3 <- ordered(as.integer(x3 > 1))
#!     x4 <- ordered(as.integer(x4 > -1))
#!
#!     y1 <- ordered(as.integer(y1 > 0))
#!     y2 <- ordered(as.integer(y2 > 0))
#!     y3 <- ordered(as.integer(y3 > 1))
#!     y4 <- ordered(as.integer(y4 > -1))
#!
#!     odat <- data.frame(x1, x2, x3, x4, y1, y2, y3, y4)
#!
#!     xcor <- cor(dat)
#!     pcor <- cor(odat)
#!     hcor <- hetcor(odat, ML=TRUE, std.err=FALSE)$correlations
#!     ncor <- nearcor(hcor)$cor
#!
#!     try(factanal(covmat=xcor, factors=2, n.obs=n))
#!     try(factanal(covmat=pcor, factors=2, n.obs=n))
#!     try(factanal(covmat=hcor, factors=2, n.obs=n))
#!     try(factanal(covmat=ncor, factors=2, n.obs=n))
#!   }
#! }
#! \keyword{algebra}
#! \keyword{array}

nearcor <- function(  # Computes the nearest correlation matrix to an approximate correlation matrix, i.e. not positive semidefinite.
  R                   # n-by-n approx correlation matrix
, eig.tol   = 1.0e-6  # defines relative positiveness of eigenvalues compared to largest
, conv.tol  = 1.0e-7  # convergence tolerance for algorithm
, posd.tol  = 1.0e-8  # tolerance for enforcing positive definiteness
, maxits    = 100     # maximum number of iterations allowed
, verbose   = FALSE   # set to TRUE to verbose convergence

                      # RETURNS list of class nearcor with components cor, iterations, converged
){
  if (!(is.numeric(R) && is.matrix(R) && identical(R,t(R))))
    stop('Error: Input matrix R must be square and symmetric')

  # Inf norm
  inorm <- function(x)max(rowSums(abs(x)))
  # Froebenius norm
  fnorm <- function(x)sqrt(sum(diag(t(x) %*% x)))

  n <- ncol(R)
  U <- matrix(0, n, n)
  Y <- R
  iter <- 0

  while (TRUE){
      T <- Y - U

      # PROJECT ONTO PSD MATRICES
      e <- eigen(Y, symmetric=TRUE)
      Q <- e$vectors
      d <- e$values
      D <- diag(d)

      # create mask from relative positive eigenvalues
      p <- (d>eig.tol*d[1]);

      # use p mask to only compute 'positive' part
      X <- Q[,p,drop=FALSE] %*% D[p,p,drop=FALSE] %*% t(Q[,p,drop=FALSE])

      # UPDATE DYKSTRA'S CORRECTION
      U <- X - T

      # PROJECT ONTO UNIT DIAG MATRICES
      X <- (X + t(X))/2
      diag(X) <- 1

      conv <- inorm(Y-X) / inorm(Y)
      iter <- iter + 1
      if (verbose)
        cat("iter=", iter, "  conv=", conv, "\n", sep="")

      if (conv <= conv.tol){
        converged <- TRUE
        break
      }else if (iter==maxits){
        warning(paste("nearcor did not converge in", iter, "iterations"))
        converged <- FALSE
        break
      }
      Y <- X
  }
  X <- (X + t(X))/2
  # begin from posdefify(sfsmisc)
  e <- eigen(X, symmetric = TRUE)
  d <- e$values
  Eps <- posd.tol * abs(d[1])
  if (d[n] < Eps) {
      d[d < Eps] <- Eps
      Q <- e$vectors
      o.diag <- diag(X)
      X <- Q %*% (d * t(Q))
      D <- sqrt(pmax(Eps, o.diag)/diag(X))
      X[] <- D * X * rep(D, each = n)
  }
  # end from posdefify(sfsmisc)
  # force symmetry
  X <- (X + t(X))/2
  diag(X) <- 1
  ret <- list(cor=X, fnorm=fnorm(R-X), iterations=iter, converged=converged)
  class(ret) <- "nearcor"
  ret
}

-- 
Ist Ihr Browser Vista-kompatibel? Jetzt die neuesten 
Browser-Versionen downloaden: http://www.gmx.net/de/go/browser


From horacio9573 at gmail.com  Fri Jul 13 21:06:37 2007
From: horacio9573 at gmail.com (Horacio Castellini)
Date: Fri, 13 Jul 2007 16:06:37 -0300
Subject: [R] Flow Cytometry Standard, fcs format in R.
Message-ID: <90e13c0707131206td4f430o3069da8e14b656dc@mail.gmail.com>

Hi all.
How do I extract date from fcs format file with R. I.e I'd like
make statistical analysis using R-program, but I don't know if there
are R-packets for fcs format file, and using examples.

Thanks.

Pta: In Linux SO exist any program that transform from fcs format to
ASCII text file?


From colton.smith at skio.usg.edu  Fri Jul 13 21:09:32 2007
From: colton.smith at skio.usg.edu (colton.smith at skio.usg.edu)
Date: Fri, 13 Jul 2007 15:09:32 -0400 (EDT)
Subject: [R] trouble compiling Hmisc package
Message-ID: <46151.168.20.2.120.1184353772.squirrel@mail.skio.usg.edu>

Hi:

  We're trying to install the Hmisc package on a Solaris 9 machine. 
Here's what we get:

R CMD INSTALL /usr/local/srctree/Hmisc_3.4-2.tar.gz
* Installing to library '/usr/local/lib/R/library'
* Installing *source* package 'Hmisc' ...
** libs
g95   -fPIC  -g -O2 -c cidxcn.f -o cidxcn.o
g95   -fPIC  -g -O2 -c cidxcp.f -o cidxcp.o
g95   -fPIC  -g -O2 -c hoeffd.f -o hoeffd.o
g95   -fPIC  -g -O2 -c jacklins.f -o jacklins.o
g95   -fPIC  -g -O2 -c largrec.f -o largrec.o
In file largrec.f:27

      DO xl=xlim(1),xlim(2)-width,xinc
          1
Error: Loop variable at (1) must be a scalar INTEGER
In file largrec.f:28

         DO yl=ylim(1),ylim(2)-height,yinc
             1
Error: Loop variable at (1) must be a scalar INTEGER
In file largrec.f:29

            DO xr=xl+width,xlim(2),xinc
                1
Error: Loop variable at (1) must be a scalar INTEGER
In file largrec.f:30

               DO yu=yl+height,ylim(2),yinc
                   1
Error: Loop variable at (1) must be a scalar INTEGER
make: *** [largrec.o] Error 1
ERROR: compilation failed for package 'Hmisc'
** Removing '/usr/local/lib/R/library/Hmisc'

Can anybody help us?


From cryan at binghamton.edu  Fri Jul 13 22:44:48 2007
From: cryan at binghamton.edu (Christopher W. Ryan)
Date: Fri, 13 Jul 2007 15:44:48 -0500
Subject: [R] THANK YOU: Updating R version
In-Reply-To: <c98270fd0707131046l662bdf9cm81fe165fdc5fafa0@mail.gmail.com>
References: <c98270fd0707131046l662bdf9cm81fe165fdc5fafa0@mail.gmail.com>
Message-ID: <4697E440.5000809@binghamton.edu>

This sounds like a solution I've been looking for.  With this setup now
in place, when you download and install some new packages, where will
they be put?  Into C:\myRlib ?

Thanks.

--Chris
Christopher W. Ryan, MD
SUNY Upstate Medical University Clinical Campus at Binghamton
40 Arch Street, Johnson City, NY  13790
cryanatbinghamtondotedu
PGP public keys available at http://home.stny.rr.com/ryancw/

"If you want to build a ship, don't drum up the men to gather wood,
divide the work and give orders. Instead, teach them to yearn for the
vast and endless sea."  [Antoine de St. Exupery]

Raghu Naik wrote:
> Based on the feedback received, I did the following:
> 
> a) moved my lib sub-directory from the existing installed R version to
> c:\myRLib
> b) installed the updated R version
> c) created .Renviron file in the home directory (C:\R-2.5.1) with the line
> R_LIBS=c:/myRLib
> d) used .libPaths() command to confirm that the new R installation was
> recognizing the myRLib sub-directory
> e) deleted my old R installation
> 
> Things worked fine.
> 
> Thanks you.
> 
> 
> 
> ---------- Forwarded message ----------
> From: Raghu Naik <naik.raghu at gmail.com>
> Date: Jun 2, 2007 5:13 PM
> Subject: Updating R version
> To: r-help at stat.math.ethz.ch
> 
> A quick question.  I am trying to understand how I could move the installed
> packages in my R 2.3 version to the newly installed R 2.5 version, without
> having to install all the packages again. I copied the files under the old
> library subdirectory to the new library subdirectory. But still the newer
> version is not recognizing the packages that were copied over.
> 
> Thanks.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From szhan at uoguelph.ca  Fri Jul 13 21:52:21 2007
From: szhan at uoguelph.ca (szhan at uoguelph.ca)
Date: Fri, 13 Jul 2007 15:52:21 -0400
Subject: [R] how to estimate treatment-interaction contrasts
In-Reply-To: <4697AA31.403@optonline.net>
References: <20070712161628.h4rqf4j48wwks444@webmail.uoguelph.ca>
	<4697AA31.403@optonline.net>
Message-ID: <20070713155221.nya8t2e4cg800kg8@webmail.uoguelph.ca>

Hello, Chuck,
Thank you very much for your help! But the contrasts I want to do  
simutaneously is
  contrasts(B)
    [,1] [,2] [,3] [,4]
b1   -4   -3   -2   -1
b2    1   -3   -2   -1
b3    1    2   -2   -1
b4    1    2    3   -1
b5    1    2    3    4

Could you please show me how to calculate estimates for ALL  
intearaction constrasts using THESE contrasts? Say C2: c(-3, -3, 2, 2,  
2) as an example. I used the ortholognal constrasts as you suggest,  
estimate for interaction contrast C2 is still -24.1.
Joshua

Quoting Chuck Cleland <ccleland at optonline.net>:

> szhan at uoguelph.ca wrote:
>> Hello, R experts,
>> Sorry for asking this question again again since I really want a help!
>>
>> I have a two-factor experiment data and like to calculate estimates of
>> interation contrasts say factor A has levels of a1, a2, and B has
>> levels of b1, b2, b3, b4, and b5 with 3 replicates. I am not sure the
>> constrast estimate I got is right using the script below:
>>
>> score<-c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
>> 7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)
>>
>> A <- gl(2, 15, labels=c("a1", "a2"))
>> B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)
>>
>> contrasts(B)<-cbind(c(-4,rep(1,4)),c(rep(-3,2),rep(2,3)),
>> +  c(rep(-2,3),rep(3,2)),c(rep(-1,4), rep(4,1)))
>> fit1 <- aov(score ~ A*B)
>> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>>                Df Sum Sq Mean Sq F value    Pr(>F)
>> A            1 3.2013  3.2013 15.1483 0.0009054 ***
>> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>>      B: C1      1 0.0301  0.0301  0.1424 0.7099296
>>      B: C2      1 2.0335  2.0335  9.6221 0.0056199 **
>>      B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>>      B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
>> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>>      A:B: C1    1 0.7207  0.7207  3.4105 0.0796342 .
>>      A:B: C2    1 2.6068  2.6068 12.3350 0.0021927 **
>>      A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>>      A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
>> Residuals   20 4.2267  0.2113
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>> Now I like to get interaction contrast estimate for b1 and b2 vs   
>> b3, b4 and b5
>> cont <- c(1, -1)[A] * c(-3, -3, 2, 2, 2)[B]
>>
>> estimat<-sum(cont*score) # value of the contrast estimate for A:B C2
>>
>>> estimat
>> [1] -24.1
>>
>> I am not sure the estimate for A:B C2 contrast  (-24.1) is correct
>> because the F value given the output above(12.3350) does not equal to
>> those I calculate below (15.2684):
>>
>> t.stat <- sum(cont*score)/se.contrast(fit1, as.matrix(cont))
>>> t.stat^2
>> Contrast 1
>>      15.2684
>>
>> Could you please help me calculate the correct the estimate of
>> interaction contrast and corresponding F value?
>> Thanks in advance!
>> Joshua
>
>   If the contrasts for B are orthogonal, then you get the result you
> expected:
>
> score <- c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
>            7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)
>
> A <- gl(2, 15, labels=c("a1", "a2"))
> B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)
>
> contrasts(B) <- matrix(c(3, -1,  0,  0,
>                          3,  1,  0,  0,
>                         -2,  0,  2,  0,
>                         -2,  0, -1,  1,
>                         -2,  0, -1, -1), ncol=4, byrow=TRUE)
>
> fit1 <- aov(score ~ A*B)
>
> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>
>             Df Sum Sq Mean Sq F value    Pr(>F)
> A            1 3.2013  3.2013 15.1483 0.0009054 ***
> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>   B: C1      1 1.0427  1.0427  4.9340 0.0380408 *
>   B: C2      1 1.0208  1.0208  4.8304 0.0399049 *
>   B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>   B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>   A:B: C1    1 3.2267  3.2267 15.2684 0.0008734 ***
>   A:B: C2    1 0.1008  0.1008  0.4771 0.4976647
>   A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>   A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
> Residuals   20 4.2267  0.2113
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
>   Note that I put the contrast of interest for B in the first column of
> the contrast matrix.
>
> hope this helps,
>
> Chuck
>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>


From rgentlem at fhcrc.org  Fri Jul 13 22:34:59 2007
From: rgentlem at fhcrc.org (Robert Gentleman)
Date: Fri, 13 Jul 2007 13:34:59 -0700
Subject: [R] Flow Cytometry Standard, fcs format in R.
In-Reply-To: <90e13c0707131206td4f430o3069da8e14b656dc@mail.gmail.com>
References: <90e13c0707131206td4f430o3069da8e14b656dc@mail.gmail.com>
Message-ID: <4697E1F3.5060306@fhcrc.org>

there is rflowcyt (older and on its way out) and flowCore, which 
contains a newer tool set
available at
www.bioconductor.org


Horacio Castellini wrote:
> Hi all.
> How do I extract date from fcs format file with R. I.e I'd like
> make statistical analysis using R-program, but I don't know if there
> are R-packets for fcs format file, and using examples.
> 
> Thanks.
> 
> Pta: In Linux SO exist any program that transform from fcs format to
> ASCII text file?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Robert Gentleman, PhD
Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M2-B876
PO Box 19024
Seattle, Washington 98109-1024
206-667-7700
rgentlem at fhcrc.org


From Mark.Leeds at morganstanley.com  Fri Jul 13 22:45:29 2007
From: Mark.Leeds at morganstanley.com (Leeds, Mark (IED))
Date: Fri, 13 Jul 2007 16:45:29 -0400
Subject: [R] Question about acception rejection sampling  - NOT R question
Message-ID: <D3AEEDA31E57474B840BEBC25A8A834401957566@NYWEXMB23.msad.ms.com>

This is not related to R but I was hoping that someone could help me. I
am reading the "Understanding the Metropolis Hastings Algorithm"
paper from the American Statistician by Chip and Greenberg, 1995, Vol
49, No 4. Right at the beginning they explain the algorithm for basic
acceptance  rejection sampling in which you want to simulate a density
from f(x) but it's not easy and you are able
to generate from another density called h(x). The assumption is that
there exists some c such that f(x) <= c(h(x) for all x

They clearly explain the steps as follows :

1) generate Z from h(x).

2) generate u from a Uniform(0,1)

3) if u is less than or equal to f(Z)/c(h(Z) then return Z as the
generated RV; otherwise reject it and try again.

I think that, since f(Z)/c(h(z) is U(0,1), then u has the distrbution as
f(Z)/c(h(Z).
 
But, I don't understand why the generated and accepted Z's have the same
density  as f(x) ?

Does someone know where there is a proof of this or if it's reasonably
to explain, please feel free to explain it.
They authors definitely believe it's too trivial because they don't. The
reason I ask is because, if I don't understand this then 
I definitely  won't understand the rest of the paper because it gets
much more complicated.  I willing to track down the proof but I don't
know where to look. Thanks.
--------------------------------------------------------

This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}


From dgvirtual at akl.lt  Fri Jul 13 23:20:15 2007
From: dgvirtual at akl.lt (Donatas G.)
Date: Sat, 14 Jul 2007 00:20:15 +0300
Subject: [R] charset in graphics
In-Reply-To: <Pine.LNX.4.64.0707131541080.14348@gannet.stats.ox.ac.uk>
References: <200707131150.44106.dgvirtual@akl.lt>
	<469780DE.30105@biostat.ku.dk>
	<Pine.LNX.4.64.0707131541080.14348@gannet.stats.ox.ac.uk>
Message-ID: <200707140020.16211.dgvirtual@akl.lt>

On Friday 13 July 2007 17:56:45 Prof Brian Ripley wrote:
> On Fri, 13 Jul 2007, Peter Dalgaard wrote:
> > Prof Brian Ripley wrote:
> >> On Fri, 13 Jul 2007, Peter Dalgaard wrote:
> >>> The correct incantation seems to be
> >>>
> >>> postscript(font="URWHelvetica", encoding="ISOLatin7")
> >>> plot(0,main=tolower("\u104\u116\u0118\u012e\u0172\u016a\u010c\u0160\u01
> >>>7d")) dev.off()
This sequence seems to do the trick, although I do not seem to have a font 
URWHelvetica

Anyway, where do i record these lines to make it permanent?

Maybe the explanation was there in Prof Brian Ripley's first email, but I did 
not get it somehow...

-- 
Donatas Glodenis
http://dg.lapas.info


From frainj at gmail.com  Fri Jul 13 23:31:18 2007
From: frainj at gmail.com (John C Frain)
Date: Fri, 13 Jul 2007 22:31:18 +0100
Subject: [R] Fwd:  THANK YOU: Updating R version
In-Reply-To: <fad888a10707131430obd058b1idc296dd24e1c9a41@mail.gmail.com>
References: <c98270fd0707131046l662bdf9cm81fe165fdc5fafa0@mail.gmail.com>
	<4697E440.5000809@binghamton.edu>
	<fad888a10707131430obd058b1idc296dd24e1c9a41@mail.gmail.com>
Message-ID: <fad888a10707131431v6e76609fm46468a944ea78a42@mail.gmail.com>

---------- Forwarded message ----------
From: John C Frain <frainj at gmail.com>
Date: 13-Jul-2007 22:30
Subject: Re: [R] THANK YOU: Updating R version
To: "Christopher W. Ryan" <cryan at binghamton.edu>


When I update R the following has worked for me (Windows XP)

1. Install the new version to a new directory (say C:\Program Files\R\R-2.5.1).

2 Rename the new library subdirectory  to library2.

3 Copy the entire contents of the old library subdirectory (say
C:\Program Files\R\R-2.4.0\library\ to the new R root to create
C:\Program Files\R\R-2.5.1\library\ .

4 Copy the contents of library2 to library to update your basic library.

5 Now start your new version of R and update packages from the GUI or
from the R console.  (You may need to firs check Rprofile .site to
ensure that no packages have been loaded)

6. On occasion I have got warning messages when I tried to load
packages after this procedure.  This has been cleared by running

update.packages(checkBuilt = TRUE)

This checks that your packages have been built with the latest
version.  When I do this I  agree to install all available updates.

7 You may wish to copy various autoloads etc from your old
Rprofile.site to your new Rprofile.site.  I understand that there are
some compatibility problems with 2.5.1 and SciViews so be careful.

Best Regards

John



On 13/07/07, Christopher W. Ryan <cryan at binghamton.edu> wrote:
> This sounds like a solution I've been looking for.  With this setup now
> in place, when you download and install some new packages, where will
> they be put?  Into C:\myRlib ?
>
> Thanks.
>
> --Chris
> Christopher W. Ryan, MD
> SUNY Upstate Medical University Clinical Campus at Binghamton
> 40 Arch Street, Johnson City, NY  13790
> cryanatbinghamtondotedu
> PGP public keys available at http://home.stny.rr.com/ryancw/
>
> "If you want to build a ship, don't drum up the men to gather wood,
> divide the work and give orders. Instead, teach them to yearn for the
> vast and endless sea."  [Antoine de St. Exupery]
>
> Raghu Naik wrote:
> > Based on the feedback received, I did the following:
> >
> > a) moved my lib sub-directory from the existing installed R version to
> > c:\myRLib
> > b) installed the updated R version
> > c) created .Renviron file in the home directory (C:\R-2.5.1) with the line
> > R_LIBS=c:/myRLib
> > d) used .libPaths() command to confirm that the new R installation was
> > recognizing the myRLib sub-directory
> > e) deleted my old R installation
> >
> > Things worked fine.
> >
> > Thanks you.
> >
> >
> >
> > ---------- Forwarded message ----------
> > From: Raghu Naik <naik.raghu at gmail.com>
> > Date: Jun 2, 2007 5:13 PM
> > Subject: Updating R version
> > To: r-help at stat.math.ethz.ch
> >
> > A quick question.  I am trying to understand how I could move the installed
> > packages in my R 2.3 version to the newly installed R 2.5 version, without
> > having to install all the packages again. I copied the files under the old
> > library subdirectory to the new library subdirectory. But still the newer
> > version is not recognizing the packages that were copied over.
> >
> > Thanks.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


--
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


-- 
John C Frain
Trinity College Dublin
Dublin 2
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com


From sh13 at gmx.ch  Fri Jul 13 23:31:27 2007
From: sh13 at gmx.ch (simone)
Date: Fri, 13 Jul 2007 23:31:27 +0200
Subject: [R] Power analysis for glm-function
Message-ID: <2FAF4DF1-450F-4333-96B5-0087927A666B@gmx.ch>

Hello everybody

I have done a generalized linear model (glm-function) with a binary  
response variable (presence/absence) and two explanatory variables  
(including the interaction term (model<-glm(x~factor1*factor2,  
family=quasibinomial)). The interaction is what we are mainly  
interested in and it is non-significant (F=0, p=1.0). A reviewer on  
the manuscript now asks for a a priori power analysis on the non- 
significant interaction to see what sort of effect size we could  
reasonably have detected with our experimental design. As far as I  
have seen there is no power analysis function implemented for glm in  
R, or am I mistaken? Or does any one have any other suggestions how  
to deal with that comment?

I would really appreciate your help!
Thanks in advance

Simone

Simone Haerri
Institute of Environmental Sciences
University of Zurich
Winterthurerstrasse 190
Ch - 8057 Zurich


From duvvuru.suman at gmail.com  Sat Jul 14 00:04:38 2007
From: duvvuru.suman at gmail.com (suman Duvvuru)
Date: Fri, 13 Jul 2007 18:04:38 -0400
Subject: [R] Subplot.
Message-ID: <bac8a0820707131504qc21dc3dofb90873ec2f7be4f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070713/0c18c87b/attachment.pl 

From lists at revelle.net  Sat Jul 14 00:31:10 2007
From: lists at revelle.net (William Revelle)
Date: Fri, 13 Jul 2007 17:31:10 -0500
Subject: [R] nearest correlation to polychoric
In-Reply-To: <20070713184214.82160@gmx.net>
References: <20070713184214.82160@gmx.net>
Message-ID: <p0624081fc2bda12473fd@[165.124.163.29]>

Jens,
   An alternative solution to the improper matrix 
problem is to do a principal factor solution 
rather than a maximum likelihood  factor analysis 
solution.   In the following discussion, I am 
using the factor.pa  (principal factor) function 
from my psych package.


Using your test set
  pr <- structure(c(1, 0.477, 0.644, 0.478, 0.651, 0.826, 0.477, 1, 0.516,
    0.233, 0.682, 0.75, 0.644, 0.516, 1, 0.599, 0.581, 0.742, 0.478,
   0.233, 0.599, 1, 0.741, 0.8, 0.651, 0.682, 0.581, 0.741, 1, 0.798,
   0.826, 0.75, 0.742, 0.8, 0.798, 1), .Dim = c(6, 6))

I did a factor.pa(pr,2) and got

Loadings:
      PA1  PA2
[1,] 0.52 0.54
[2,] 0.13 0.92
[3,] 0.56 0.49
[4,] 0.97 0.11
[5,] 0.63 0.58
[6,] 0.73 0.70

                  PA1   PA2
SS loadings    2.472 2.217
Proportion Var 0.412 0.369
Cumulative Var 0.412 0.781

This compares to a factanal solution on the nearcor solution
  nr <- nearcor(pr)$cor
f2 <- factanal(cov=nr, factors=2)

Loadings:
      Factor1 Factor2
[1,] 0.735   0.388 
[2,] 0.868   0.123 
[3,] 0.534   0.523 
[4,] 0.151   0.986 
[5,] 0.508   0.660 
[6,] 0.740   0.669 

                Factor1 Factor2
SS loadings      2.407   2.295
Proportion Var   0.401   0.382
Cumulative Var   0.401   0.784


The factor congruence of the two solutions is

  round(factor.congruence(f2,p2),2)
          PA1  PA2
Factor1 0.74 0.99
Factor2 1.00 0.68

However, when I do the same analysis on your 
second demo set, in one run I got a Haywood case 
for the principal factors.
f2 <- factor.pa(hcor,2,n.obs=400)

Loadings:
    PA1   PA2 
x1        0.87
x2        0.65
x3 -0.10  0.64
x4        0.56
y1  0.51     
y2  0.63     
y3  0.78     
y4  1.13     

                 PA1   PA2
SS loadings    2.56 1.917
Proportion Var 0.32 0.240
Cumulative Var 0.32 0.560


but not for the factanal of the nearcor solution for the hcor problem:
ncor <- nearcor(hcor)$cor
m2 <- factanal(covmat=ncor,factors=2,n.obs=400)

Loadings:
      Factor1 Factor2
[1,]          0.899
[2,] -0.168   0.569
[3,]          0.647
[4,]          0.583
[5,]  0.468        
[6,]  0.683        
[7,]  0.877  -0.100
[8,]  0.997        

                Factor1 Factor2
SS loadings      2.486   1.907
Proportion Var   0.311   0.238
Cumulative Var   0.311   0.549

However, once again, the factor congruence coefficients are very good:

round(factor.congruence(m2,f2),2)
           PA1   PA2
Factor1  0.99 -0.09
Factor2 -0.09  1.00


Then, I did a number of runs comparing principal 
axes to the hetcor data (what you call hcor), 
factanal  to the original data ( what you call 
xcor, and factanal solutions to the nearcor 
solution (ncor).  The general pattern is that 
principal axes of the polychoric (hcor) matrix 
tends to match the factanal solutions to the 
original data better than factanal of the nearcor 
matrix does the original matrix.

   round(factor.congruence(f2,n2),2)   #compare 
principal axes to mle of nearcor solution
     Factor1 Factor2
PA1    0.98    0.17
PA2    0.18    0.99
>        round(factor.congruence(f2,x2),2) 
>#compare principal axes to mle of original data
     Factor1 Factor2
PA1    0.92    0.07
PA2    0.10    0.95
>        round(factor.congruence(n2,x2),2) 
>#compare mle of original to mle of nearcor
         Factor1 Factor2
Factor1    0.92    0.02
Factor2    0.03    0.94

Thanks for posting the nearcor algorithm and function.








At 8:42 PM +0200 7/13/07, Jens Oehlschl?gel wrote:
>Dimitris,
>
>Thanks a lot for the quick response with the 
>pointer to posdefify. Using its logic as an 
>afterburner to the algorithm of Higham seems to 
>work.
>
>Martin,
>
>>  Jens, could you make your code (mentioned 
>>below) available to the community, or even 
>>donate to be included as a new method of 
>>posdefify() ?
>
>Nice opportunity to give-back. Below is the R 
>code for nearcor and .Rd help file. A quite 
>natural place for nearcor would be John Fox' 
>package polycor, what do you think?
>
>John?
>
>Best regards
>
>Jens Oehlschl?gel




{nearcor function and help file deleted --- see original posting}


Bill

-- 
William Revelle		http://personality-project.org/revelle.html
Professor			http://personality-project.org/personality.html
Department of Psychology             http://www.wcas.northwestern.edu/psych/
Northwestern University	http://www.northwestern.edu/
Use R for statistics:                          http://personality-project.org/r


From pete-expires-20070910 at kazmier.com  Sat Jul 14 01:14:28 2007
From: pete-expires-20070910 at kazmier.com (Pete Kazmier)
Date: Fri, 13 Jul 2007 19:14:28 -0400
Subject: [R] ggplot usage question
Message-ID: <87zm1zyjzf.fsf@coco.kazmier.com>

Could someone show me how to get a blue line in this plot?

> ggplot(movies, aes(x=rating)) + stat_qq(geom="line",
    quantiles=seq(0,1,0.005), distribution=qunif)

I've tried many permutations but cannot seem to find the right
combination.  I've tried these flavors:

> ggplot(movies, aes(x=rating)) + stat_qq(geom="line", colour="blue",
    quantiles=seq(0,1,0.005), distribution=qunif)

> ggplot(movies, aes(x=rating)) + layer(geom="line", colour="blue", 
    stat="qq", distribution=qunif, quantiles=seq(0,1,0.005))

I also tried to use grid.gedit but could not figure out the
appropriate gpath to use.

Thanks,
Pete


From deepayan.sarkar at gmail.com  Sat Jul 14 01:25:44 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 13 Jul 2007 16:25:44 -0700
Subject: [R] Subplot.
In-Reply-To: <bac8a0820707131504qc21dc3dofb90873ec2f7be4f@mail.gmail.com>
References: <bac8a0820707131504qc21dc3dofb90873ec2f7be4f@mail.gmail.com>
Message-ID: <eb555e660707131625p5bb7521en1e325dd864cb210a@mail.gmail.com>

On 7/13/07, suman Duvvuru <duvvuru.suman at gmail.com> wrote:
> Hello All,
>
> I wanted to do many plots (in my case, wanted to get 6 histograms) on the
> same figure. Is there a  method in R that analogous to 'subplot' in MATLAB?

Here are a few possibilities:


data(singer, package = "lattice")

## using traditional graphics

singer.split <- with(singer, split(height, voice.part))
par(mfrow = c(2, 4))
for (i in names(singer.split))
    hist(singer.split[[i]], main = i, xlab = "height")

## using lattice

library(lattice)
histogram(~height | voice.part, singer)

## using ggplot2

library(ggplot2)
qplot(height, data = singer, geom = "histogram", facets = voice.part ~ .)

-Deepayan


From cberry at tajo.ucsd.edu  Sat Jul 14 01:43:51 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 13 Jul 2007 16:43:51 -0700
Subject: [R] Question about acception rejection sampling - NOT R question
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A834401957566@NYWEXMB23.msad.ms.com>
References: <D3AEEDA31E57474B840BEBC25A8A834401957566@NYWEXMB23.msad.ms.com>
Message-ID: <Pine.LNX.4.64.0707131627520.20322@tajo.ucsd.edu>

On Fri, 13 Jul 2007, Leeds, Mark (IED) wrote:

> This is not related to R but I was hoping that someone could help me. I
> am reading the "Understanding the Metropolis Hastings Algorithm"
> paper from the American Statistician by Chip and Greenberg, 1995, Vol
> 49, No 4. Right at the beginning they explain the algorithm for basic
> acceptance  rejection sampling in which you want to simulate a density
> from f(x) but it's not easy and you are able
> to generate from another density called h(x). The assumption is that
> there exists some c such that f(x) <= c(h(x) for all x
>
> They clearly explain the steps as follows :
>
> 1) generate Z from h(x).
>
> 2) generate u from a Uniform(0,1)
>
> 3) if u is less than or equal to f(Z)/c(h(Z) then return Z as the
> generated RV; otherwise reject it and try again.
>
> I think that, since f(Z)/c(h(z) is U(0,1), then u has the distrbution as
> f(Z)/c(h(Z).
>
> But, I don't understand why the generated and accepted Z's have the same
> density  as f(x) ?
>
> Does someone know where there is a proof of this or if it's reasonably
> to explain, please feel free to explain it.

The original reference to J. von Neumann's work, which no doubt has a 
proof, is in

 	http://en.wikipedia.org/wiki/Rejection_sampling

along with a suggestive graph.

Here is another graph that may give your intuition a boost:

f <- function(x) dnorm(x)/2+dnorm(x,sd=0.1)/2
h <- dnorm
Z <- rnorm(1000000)
u <- runif(1000000)
cee <- f(0)/h(0) # as is obvious
u.lt.f.over.ch <- u < f(Z)/cee/h(Z)
cutpts <- seq(-5,5,by=0.1)
midpts <- head(cutpts,n=-1)/2 + tail(cutpts,n=-1)/2
tab <- table( !u.lt.f.over.ch, cut( Z, cutpts ) )
bp <- barplot( tab )
lines( bp, f(midpts)*sum(u.lt.f.over.ch)*0.1,col='red',lwd=2)

Of course, there is some discreteness in this plot.
If you have a 1280x1024 or wider screen or want to zoom in on a pdf(), 
you might try 0.025 or even 0.0125 in place of 0.1.

Turn this graph on its side and think about what u.lt.f.over.ch is doing 
conditionally on Z, i.e. look at one bar and think about why it is split 
where it is.


HTH,

Chuck

> They authors definitely believe it's too trivial because they don't. The
> reason I ask is because, if I don't understand this then
> I definitely  won't understand the rest of the paper because it gets
> much more complicated.  I willing to track down the proof but I don't
> know where to look. Thanks.
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From cberry at tajo.ucsd.edu  Sat Jul 14 01:51:26 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 13 Jul 2007 16:51:26 -0700
Subject: [R] Power analysis for glm-function
In-Reply-To: <2FAF4DF1-450F-4333-96B5-0087927A666B@gmx.ch>
References: <2FAF4DF1-450F-4333-96B5-0087927A666B@gmx.ch>
Message-ID: <Pine.LNX.4.64.0707131647120.20322@tajo.ucsd.edu>

On Fri, 13 Jul 2007, simone wrote:

> Hello everybody
>
> I have done a generalized linear model (glm-function) with a binary
> response variable (presence/absence) and two explanatory variables
> (including the interaction term (model<-glm(x~factor1*factor2,
> family=quasibinomial)). The interaction is what we are mainly
> interested in and it is non-significant (F=0, p=1.0). A reviewer on
> the manuscript now asks for a a priori power analysis on the non-
> significant interaction to see what sort of effect size we could
> reasonably have detected with our experimental design. As far as I
> have seen there is no power analysis function implemented for glm in
> R, or am I mistaken? Or does any one have any other suggestions how
> to deal with that comment?

Well, by citing

Hoenig, John M. and Heisey, Dennis M. (2001), ``The Abuse of Power: The 
Pervasive Fallacy of Power Calculations for Data Analysis,'' The American 
Statistician, 55, 19-24.

and reviewing its argument for the benefit of the reviewer and the editor.

---

You have the data, you can compute the confidence interval (or region if 
either factor has more than two levels), and then you will know just 
what is ruled out by your non-significant interaction.

---

HTH,

Chuck


>
> I would really appreciate your help!
> Thanks in advance
>
> Simone
>
> Simone Haerri
> Institute of Environmental Sciences
> University of Zurich
> Winterthurerstrasse 190
> Ch - 8057 Zurich
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From dj at davidcjames.com  Sat Jul 14 02:47:40 2007
From: dj at davidcjames.com (David C. James)
Date: Fri, 13 Jul 2007 19:47:40 -0500
Subject: [R] accessing list components with a variable
Message-ID: <2F25148A-2EBE-4BFD-AF0D-83879D7B03D1@davidcjames.com>

Let's say I have a list called the_list consisting of three components:
the_list$component_1
the_list$component_2
the_list$component_3

Now, I want to access it using a variable called comp.
comp <- "component_1"

I'm looking for some function that let's me do this:
unknown_function(the_list, comp)

Which should do the same thing as:
the_list$component_1

Any ideas?  I'd be open to other ways of doing this, too.  I explored  
this way, but it didn't seem to get me anywhere:
the_list$"component_1"

I possibly could have gotten further along if I knew how to do  
evaluation in R.

Thanks,
David


From dj at davidcjames.com  Sat Jul 14 02:52:05 2007
From: dj at davidcjames.com (David C. James)
Date: Fri, 13 Jul 2007 19:52:05 -0500
Subject: [R] learning the R language (for those strong in Ruby)
Message-ID: <4BE9E217-6D88-4B0E-9830-BDD2E2D3FE8A@davidcjames.com>

I feel like I'm not firing at all cylinders with the R language,  
despite reading over the R Language Definition.

Has anyone seen something like an "R for Rubyists" guide (i.e.  
teaching the R language for those who are more familar with Ruby)?

Alternately, a book with lots of examples about how the R language  
itself works would be fantastic.

-David


From regina.verghis at gmail.com  Sat Jul 14 04:27:27 2007
From: regina.verghis at gmail.com (Regina Verghis)
Date: Sat, 14 Jul 2007 07:57:27 +0530
Subject: [R] Installation of a Package
Message-ID: <49f266b90707131927n5b4ee8a7kb99386fc42139773@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/8817ee85/attachment.pl 

From ripley at stats.ox.ac.uk  Sat Jul 14 05:35:32 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 Jul 2007 04:35:32 +0100 (BST)
Subject: [R] Installation of a Package
In-Reply-To: <49f266b90707131927n5b4ee8a7kb99386fc42139773@mail.gmail.com>
References: <49f266b90707131927n5b4ee8a7kb99386fc42139773@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707140422590.5484@gannet.stats.ox.ac.uk>

Do you really have repeated.gz?  Lindsey usually packs his packages with 
the .tgz extension, e.g. at

http://popgen.unimaas.nl/~jlindsey/rcode.html

You need to get repeated.tgz and rmutil.tgz and use

tar zxf rmutil.tgz
tar zxf repeated.tgz
R CMD INSTALL rmutil repeated

Optional (but highly desirable): fix the many warnings you will see, and 
also those from

R CMD check rmutil repeated


On Sat, 14 Jul 2007, Regina Verghis wrote:

> Hi All,
> I want to upload J K Lindsey's "repeated" in a LINUX OS..
>
> I had tried the command..
>
>  [root at localhost Desktop] # R CMD INSTALL repeated.gz
>
>                               " WARNING: invalid package 'repeated' "
>                                *Installing to library
> '/usr/local/lib/R/library'
>                                ERROR: No packages specified.
> Can you tell me the problem? How can I rectify that?
> With Regards,
> Regina M.Verghis
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From felix at nfrac.org  Sat Jul 14 05:37:44 2007
From: felix at nfrac.org (Felix Andrews)
Date: Sat, 14 Jul 2007 13:37:44 +1000
Subject: [R] ts model challenge (transfer function)
Message-ID: <94730b8a0707132037r6e19cd8aj2f548291f2c4138c@mail.gmail.com>

Dear useRs,

I am trying to model a time series with a transfer function. I think
it can be put into the ARMA framework, and estimated with the 'arima'
function (and others have made similar comments on this list). I have
tried to do that, but the results have so far been disappointing.
Maybe I am trying to make 'arima' do something it can't...

The data are time series of river flow 'x' (output) and rainfall 'u'
(exogenous input).

x <- c(0.645, 0.61, 0.594, 0.579, 0.558, 0.62, 3.977, 2.824, 0.985,
0.764, 0.691, 0.659, 0.633, 0.619, 0.613, 0.708, 0.692, 0.617,
0.599, 0.586, 0.574, 0.566, 0.565, 0.558, 0.588, 0.6, 0.566,
0.887, 0.77, 0.692, 0.846, 0.829, 0.65, 0.613, 0.59, 1.256, 0.85,
0.673, 0.644, 0.649, 0.694, 0.663, 0.629, 0.736, 0.692, 0.611,
0.595, 0.604, 0.626, 0.618, 1.501, 1.255, 0.856, 1.742, 1.575,
1.003, 0.912, 0.972, 1.229, 1.812, 4.067, 6.921, 5.866, 3.26,
2.129, 1.744, 1.566, 2.445, 2.362, 1.77, 1.594, 1.713, 1.784,
1.56, 1.41, 1.839, 3.008, 4.697, 10.853, 7.325, 3.853, 2.96,
2.54, 2.315, 3.488, 4.414, 2.782, 3.498, 6.411, 7.902, 8.18,
7.468, 4.437, 3.663, 3.255, 2.972, 2.757, 2.575, 2.443, 2.316,
2.226, 2.223, 2.166, 2.154, 3.346, 4.047, 4.059, 3.134, 2.628,
2.439, 2.315, 2.238, 2.152, 2.116, 2.31, 3.864, 5.727, 5.356,
4.582, 5.626, 5.383, 4.368, 4.063, 3.963, 3.604, 3.591, 3.25,
3.122, 3.585, 3.488, 3.063, 2.865, 2.672, 2.946, 3.415, 3.237,
2.715, 2.831, 2.59, 2.441, 2.368, 2.285, 2.272, 2.633, 2.332,
2.199, 2.135, 2.063, 2.019, 1.998, 1.967, 1.945, 1.986, 2.004,
2.047, 2.735, 2.101, 2.038, 1.978, 1.94, 1.983, 2.442, 2.239,
2.081, 3.18, 3.039, 2.134, 1.969, 1.857, 1.794, 1.771, 1.703,
1.687, 1.658, 1.64, 1.64, 1.589, 1.546, 1.52, 1.515, 1.505, 1.505,
1.484, 1.477, 1.488, 1.709, 1.505, 1.522, 2.309, 1.836, 1.528,
1.453, 1.417, 1.373, 1.331, 1.307, 1.308, 1.338, 1.29, 1.292,
1.291, 1.466, 1.438, 1.358, 1.31, 1.25, 1.224, 1.192, 1.159,
1.125, 1.107, 1.093, 1.113, 1.114, 1.132, 1.125, 1.077, 1.139,
1.156, 1.082, 1.045, 1.021, 1.121, 1.735, 1.326, 1.126, 1.049,
1.111, 1.08, 1.027, 1, 0.978, 0.947, 0.946, 0.97, 0.936, 0.92,
0.904, 0.878, 0.887, 1.024, 0.994, 1.007, 0.915, 0.894, 0.834,
0.82, 0.799, 0.812, 0.814, 0.786)

u <- c(0, 0, 0, 0, 0.003, 0, 32.551, 3.369, 0, 0.634, 0, 0.129, 0,
0, 0.052, 0.959, 0.634, 0, 0, 0, 0, 0, 0, 0.011, 0.266, 0.116,
0.077, 1.976, 0.288, 0.63, 0.514, 0, 0, 0, 4.062, 0.012, 0, 0.143,
0.291, 0.53, 0.018, 0.14, 0.99, 0.143, 0.199, 0, 0.202, 0.167,
0.069, 3.413, 0.731, 1.892, 2.793, 3.787, 0.658, 0.717, 1.63,
2.375, 4.119, 8.937, 17.177, 11.645, 17.018, 0.54, 0.021, 0.354,
0, 9.792, 2.851, 1.541, 2.498, 2.042, 3.078, 0, 0.119, 5.864,
10.555, 17.609, 28.071, 21.461, 0, 0, 0.027, 0.776, 12.466, 6.447,
8.693, 9.736, 27.098, 26.849, 5.95, 0.058, 0, 0, 0.068, 0.015,
0.07, 0.308, 0.066, 0.013, 0.721, 0.606, 0.792, 9.646, 26.848,
2.4, 0.112, 1.749, 0, 0.183, 0.065, 0, 0.256, 0.932, 11.855,
18.217, 19.209, 7.409, 17.594, 16.336, 2.439, 3.926, 3.677, 1.26,
2.85, 0.833, 0.094, 4.78, 5.436, 0, 0, 0.183, 5.006, 3.348, 0.362,
2.042, 1.653, 0.751, 0, 0.023, 0, 0.43, 4.654, 0.012, 0, 0, 0,
0.117, 0, 0.087, 0.005, 0.701, 0, 0.506, 1.257, 2.697, 1.028,
0, 0.078, 0, 0.39, 4.624, 0.186, 1.925, 6.711, 5.125, 0.112,
0.121, 0.006, 0, 0.344, 0, 0.003, 0, 0.572, 0.007, 0, 0, 0, 0,
0.007, 0.03, 0, 0.006, 1.439, 0, 1.007, 3.312, 0.026, 0.094,
0, 0.093, 0, 0, 0, 0.45, 0.011, 0, 0, 0, 0.777, 0.077, 0.184,
0.1, 0, 0.038, 0, 0, 0, 0, 0.009, 0, 0.009, 0.002, 0.087, 0,
0, 0.21, 0.056, 0.046, 0, 0, 0.578, 1.17, 0.426, 0, 0.318, 0.124,
0.096, 0, 0, 0, 0.012, 0, 0, 0.002, 0, 0, 0, 0.053, 0.178, 0.068,
0.126, 0, 0.001, 0.001, 0, 0.001, 0, 0, 0)

matplot(cbind(flow,rain), type="lh")
# Note that 'u' is highly skewed.

# The system can be modelled by this transfer function:
# x[i] <- a_1*x[i-1] + a_2*x[i-2] + b_0*u[i] + b_1*u[i-1]

# I happen to know that a good model is
a_1 <- 1.6545
a_2 <- -0.6580
b_0 <- 0.1149
b_1 <- -0.1115
# (This was estimated by a proprietary program using
#  "Simple Refined Instrumental Variable" algorithm).

# The transfer function differs from the model used by 'arima', as
# there is a scale factor applied to the instantaneous input u[i].
# So need to scale the input by b_0 and also scale b_1:
arOK <- c(a_1, a_2)
maOK <- b_1 / b_0
scaleOK <- b_0

sim.good <- arima.sim(model=list(ar=arOK, ma=maOK), innov=u*scaleOK,
n=length(x))
lines(sim.good, col="blue")
# good fit to data

# Now I try to estimate the same model using 'arima'...
# Can I use xreg=u to estimate the scale factor b_0 ?
cal <- arima(x=x, order=c(2,0,1), xreg=u, include.mean=F)
round(coef(cal),digits=4)
#    ar1    ar2    ma1      u
# 0.4116 0.4965 0.6992 0.0644

model.new <- list(ar=coef(cal)[1:2], ma=coef(cal)[3])
scale.new <- coef(cal)[["u"]]
sim.new <- arima.sim(model=model.new , innov=u*scale.new , n=length(x))
lines(sim.new , col="green")
# poor fit to data

This result is disappointing, considering how good the other parameter
set is. I think there must be something I am not understanding about
'arima' -- probably 'xreg'.

Many thanks for any help.

Felix

-- 
Felix Andrews / ???
PhD candidate, The Fenner School of Environment and Society
The Australian National University (Building 48A), ACT 0200
Beijing Bag, Locked Bag 40, Kingston ACT 2604
http://www.neurofractal.org/felix/
voice:+86_1051404394 (in China)
mobile:+86_13522529265 (in China)
mobile:+61_410400963 (in Australia)
xmpp:foolish.android at gmail.com
3358 543D AAC6 22C2 D336  80D9 360B 72DD 3E4C F5D8


From patrick at pdrechsler.de  Sat Jul 14 06:16:46 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sat, 14 Jul 2007 06:16:46 +0200
Subject: [R] unixtime conversion
Message-ID: <87fy3rlivl.fsf@pdrechsler.de>

Hi,

is there an R function to convert unixtime to one of the R time
formats (using chron or POSIXct)?

Example data:

unixtime       year  month      day     hour 
1183377301     2007  7          2       13

I would like to only use the first column.

If such a function does not exist: What would be a good input format
for R (I could parse the input using AWK or other GNU/Linux tools).

TIA,

Patrick

> version
               _                           
platform       i486-pc-linux-gnu           
arch           i486                        
os             linux-gnu                   
system         i486, linux-gnu             
status                                     
major          2                           
minor          5.1                         
year           2007                        
month          06                          
day            27                          
svn rev        42083                       
language       R                           
version.string R version 2.5.1 (2007-06-27)


From ripley at stats.ox.ac.uk  Sat Jul 14 06:47:22 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 Jul 2007 05:47:22 +0100 (BST)
Subject: [R] unixtime conversion
In-Reply-To: <87fy3rlivl.fsf@pdrechsler.de>
References: <87fy3rlivl.fsf@pdrechsler.de>
Message-ID: <Pine.LNX.4.64.0707140542010.9149@gannet.stats.ox.ac.uk>

On Sat, 14 Jul 2007, Patrick Drechsler wrote:

> Hi,
>
> is there an R function to convert unixtime to one of the R time
> formats (using chron or POSIXct)?
>
> Example data:
>
> unixtime       year  month      day     hour
> 1183377301     2007  7          2       13
>
> I would like to only use the first column.

See ?as.POSIXct, especially its examples.  (Isn't that the very obvious 
place to look?)

It seems you want ISOdatetime(1970,1,1,0,0,0) + unixtime

although depending on the 'unix' in 'unixtime' you might have to wrorry 
about leap seconds (which POSIX-compliant systems ignore).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gallon.li at gmail.com  Sat Jul 14 07:25:27 2007
From: gallon.li at gmail.com (gallon li)
Date: Sat, 14 Jul 2007 13:25:27 +0800
Subject: [R] change default alphabetic order for bwplot
Message-ID: <54f7e7c30707132225x11c876b1g46ae1a35811b1c94@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/f2fd4df2/attachment.pl 

From deepayan.sarkar at gmail.com  Sat Jul 14 07:38:25 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Fri, 13 Jul 2007 22:38:25 -0700
Subject: [R] change default alphabetic order for bwplot
In-Reply-To: <54f7e7c30707132225x11c876b1g46ae1a35811b1c94@mail.gmail.com>
References: <54f7e7c30707132225x11c876b1g46ae1a35811b1c94@mail.gmail.com>
Message-ID: <eb555e660707132238p73d8ef30l616bf0b74304eb86@mail.gmail.com>

On 7/13/07, gallon li <gallon.li at gmail.com> wrote:
> when producing boxplot from bwplot, I have five groups: Nitrogen, Duration,
> Pressure, A, Z. I wish the graphical display is according to the original
> order. But the R-function bwplot seems to automatically adjust the groups
> according to the alphabetic oder and thus creat a graph for A, Duration,
> Nitrogen, Pressure and Z. How can I specify the original order in bwplot?
>
> This also happens to the older function boxplot.

This has to do with how levels for a factor are computed by default.
help(factor) tells you how to specify your own levels.

-Deepayan


From jzhang1982 at gmail.com  Sat Jul 14 08:35:17 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sat, 14 Jul 2007 00:35:17 -0600
Subject: [R] How to remove the quote "" in the data frame?
Message-ID: <3f2938d50707132335i20c7a196wc8e17217248574d8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/93772fdf/attachment.pl 

From christophe at pallier.org  Sat Jul 14 09:02:29 2007
From: christophe at pallier.org (Christophe Pallier)
Date: Sat, 14 Jul 2007 09:02:29 +0200
Subject: [R] How to remove the quote "" in the data frame?
In-Reply-To: <3f2938d50707132335i20c7a196wc8e17217248574d8@mail.gmail.com>
References: <3f2938d50707132335i20c7a196wc8e17217248574d8@mail.gmail.com>
Message-ID: <dea6cb960707140002n79ad805axee8a758cf9011042@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/9cb2b55b/attachment.pl 

From tavpritesh at gmail.com  Sat Jul 14 09:29:03 2007
From: tavpritesh at gmail.com (Tavpritesh Sethi)
Date: Sat, 14 Jul 2007 12:59:03 +0530
Subject: [R] create a matrix from an excel file?
Message-ID: <33846cd50707140029y4bbb3e06n4be5f27d0323abe4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/0b612010/attachment.pl 

From maechler at stat.math.ethz.ch  Sat Jul 14 10:53:43 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 14 Jul 2007 10:53:43 +0200
Subject: [R] nearest correlation to polychoric
In-Reply-To: <000001c7c55d$73c9e4b0$7c94100a@win.ad.jhu.edu>
References: <20070713122554.32910@gmx.net>
	<005201c7c54b$5dcaf670$0540210a@www.domain>
	<18071.35860.790041.559130@stat.math.ethz.ch>
	<000001c7c55d$73c9e4b0$7c94100a@win.ad.jhu.edu>
Message-ID: <18072.36631.292578.791583@stat.math.ethz.ch>

>>>>> "RV" == Ravi Varadhan <rvaradhan at jhmi.edu>
>>>>>     on Fri, 13 Jul 2007 10:52:36 -0400 writes:

    RV> Martin, I sent you the Matlab code for this which I had
    RV> obtained from Nich Higham.

    RV> Cheng, Sheung Hun and Higham, Nick (1998) A Modified
    RV> Cholesky Algorithm Based on a Symmetric Indefinite
    RV> Factorization; \emph{SIAM J. Matrix Anal.\ Appl.},
    RV> \bold{19}, 1097--1110.

    RV> Do you remember?

I now do .. last November.  I'm embarrassed to admit that I hadn't found the
time at the time and had completely forgotten about it.

But now that Jens already has a version of this in R -- thanks
Jens -- this should really find a home in the "R universe"

As a matter of fact, I was thinking of porting the algorithm to
the 'Matrix' package eventually,
the specific "polychor" related case may still better fit to
John Fox' package.

Martin


From johannes_graumann at web.de  Sat Jul 14 11:15:10 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Sat, 14 Jul 2007 11:15:10 +0200
Subject: [R] Algorythmic Question on Array Filtration
References: <f783nq$2ap$1@sea.gmane.org>
	<445456.26729.qm@web32811.mail.mud.yahoo.com>
Message-ID: <f7a4ef$gud$1@sea.gmane.org>

John Kane wrote:
Thanks for your time.

Please find a small example below - the real data is MUCH bigger.
If you look at rows 5 and 6 of this and calculate the mass precision window
I have to deal with (5 ppm), you'll find the following:

Row     Lower 5ppm      Mass            Higher 5ppm     Intensity
5       312.9419        312.9435        312.9451        20236.181
6       312.9422        312.9438        312.9454        14404.502

The precision windows here obviously overlap and I need to get rid of one of
them, which in this case should be row6, since it has the lower intensity
associated with it.

For now I resort to doing an intensity sort and descending into the list
populate a fresh data.frame with entries that do not have any overlap,
skipping those that do. If somebody has any more sound ideas, I'd
appreciate to hear about them.

Thanks, Joh

Mass    Intensity
304.9117 35595.780
305.1726 18760.413
311.0636 24047.307
312.9303 12886.216
312.9435 20236.181
312.9438 14404.502
313.1763 61033.830
313.1766 50788.418
316.9118 5908.166
317.2805 14084.841
317.2833 25603.689
317.2837 22866.578
318.0114 37929.855
318.9274 27883.295
318.9889 4496.716
321.2784 3893.165
326.1166 23745.851
327.2894 5318.226
328.8852 60934.030
329.1517 31985.486
331.0426 14883.231
332.0268 55126.078
332.2798 47364.519
333.2813 11423.807
337.1990 5330.360
339.2144 38450.804
339.2867 4065.709
340.9561 54101.844
340.9770 28172.160
345.0583 17945.025
345.0583 17877.900
347.1742 7359.428
347.2407 204792.999
353.2302 87864.153
353.2302 129691.696
363.0161 20453.771
363.0943 19481.234
363.2142 9238.244
363.2315 23323.527
363.2533 20039.607
363.2534 22068.718
364.8918 16857.488
364.9368 9527.642
366.9029 18174.233
373.2197 7730.009
385.1147 27907.070
385.1148 19383.655
393.2913 11860.719
396.9074 10793.823
400.8792 10750.249
402.8729 12411.966
407.2771 11270.566
442.8689 18101.972
442.8697 10671.199
447.3470 35927.046
449.2347 6959.247
456.9339 50402.820
461.1670 8636.998
461.1670 8151.706
473.2985 13782.291
490.9224 18510.760

> I think we need a bit more information and perhaps a
> small example data set to see what you want.
> 
> I am not familiar with term mass window. Is this a
> confidence interval around the mass value?
> 
> 
> --- Johannes Graumann <johannes_graumann at web.de>
> wrote:
> 
>> Dear All,
>> 
>> I have a data frame with the columns "Mass" and
>> "Intensity" (this is mass
>> spectrometry stuff). Each of the mass values gives
>> rise to a mass window of
>> 5 ppm around the individual mass (from mass -
>> mass/1E6*5 to mass +
>> mass/1E5*5). I need to filter the array such that in
>> case these mass
>> windows overlap I retain the mass/intensity pair
>> with the highest
>> intensity.
>> I apologize for this question, but I have no formal
>> IT education and would
>> value any nudges toward favorable algorithmic
>> solutions highly.
>> 
>> Thanks for any help,
>> 
>> Joh
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained,
>> reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Sat Jul 14 11:29:41 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 14 Jul 2007 11:29:41 +0200
Subject: [R] Extending Matrix class
In-Reply-To: <18071.37707.561227.593268@stat.math.ethz.ch>
References: <469788AF.8010106@wu-wien.ac.at>
	<18071.37707.561227.593268@stat.math.ethz.ch>
Message-ID: <18072.38789.499665.398176@stat.math.ethz.ch>

This is from a private question which I'm given permission to
answer in public:

>>>>> "IF" == Ingo Feinerer <h0125130 at wu-wien.ac.at>
>>>>>     on Fri, 13 Jul 2007 16:14:07 +0200 writes:

    IF> Hello, We tried to derive a class from Matrix but had
    IF> some problems. Maybe you can help us:

    library("Matrix")
    m <- Matrix(c(1:3,rep(0,12),16:20), nrow = 4, ncol = 5)
    setClass("TermDocMatrix", representation(Weighting = "character"),
            contains = ("Matrix"))

    IF> Now we want to do something like:

    IF> new("TermDocMatrix", .Data = m, weighting = "foobar")

    IF> which obviously does not work due to the missing .Data
    IF> slot.  

yes, obviously, indeed.  There is never any .Data slot  in our
matrices. 

    IF>  Note that we do not know in advance what the
    IF> matrix "m" actually is (we only know it is *some*
    IF> Matrix, e.g., we do not know if it is a dgCMatrix or a
    IF> lgCMatrix or ...).  Is there a (simple) solution?

Well, yes, but probably not the one you had wanted:

setClass("TD_Matrix",
         representation(data = "Matrix", Weighting = "character"))

A <- spMatrix(10,20, i = c(1,3:8),
              j = c(2,9,6:10),
              x = 7 * (1:7))
tdr <- new("TD_Matrix", data = A, Weighting = "foobar")
tdr

----------------

Now I understand that and why you had wanted to do this the
original way you did - which cannot work AFAICS.
OTOH, I wonder if other useRs, particularly those who know about
S4 classes (and the "Matrix" classes), have better proposals
maybe along the following "dream" ..

I think what Ingo would want is to say:
let me  extend the full Matrix class hierarchy (or just the
"dsparseMatrix" sub hierarchy) by a new slot 'Weighting'
and hence by default inherit all methods which I don't
explicitly set myself.
I think this can only partially work, even for a future version
of R, since the inherited methods don't know what to do with
"Weighting", but it would already be interesting if all
necessary new methods could be defined semi-automatically:
 1) call the corresponding Matrix method
 2) pass on all my extra slots

Regards,
Martin


From maechler at stat.math.ethz.ch  Sat Jul 14 11:34:31 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 14 Jul 2007 11:34:31 +0200
Subject: [R] create a matrix from an excel file?
In-Reply-To: <33846cd50707140029y4bbb3e06n4be5f27d0323abe4@mail.gmail.com>
References: <33846cd50707140029y4bbb3e06n4be5f27d0323abe4@mail.gmail.com>
Message-ID: <18072.39079.37502.514218@stat.math.ethz.ch>

>>>>> "TS" == Tavpritesh Sethi <tavpritesh at gmail.com>
>>>>>     on Sat, 14 Jul 2007 12:59:03 +0530 writes:

    TS> how do you create a matrix from an excel file read into
    TS> R by the command read.delim?

data.matrix(.)   {is typical a bit more useful than
as.matrix(.) }


From jrkrideau at yahoo.ca  Sat Jul 14 12:17:10 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Sat, 14 Jul 2007 06:17:10 -0400 (EDT)
Subject: [R] learning the R language (for those strong in Ruby)
In-Reply-To: <4BE9E217-6D88-4B0E-9830-BDD2E2D3FE8A@davidcjames.com>
Message-ID: <421889.35570.qm@web32806.mail.mud.yahoo.com>


--- "David C. James" <dj at davidcjames.com> wrote:

> I feel like I'm not firing at all cylinders with the
> R language,  
> despite reading over the R Language Definition.
> 
> Has anyone seen something like an "R for Rubyists"
> guide (i.e.  
> teaching the R language for those who are more
> familar with Ruby)?
> 
> Alternately, a book with lots of examples about how
> the R language  
> itself works would be fantastic.

Peter Dalgaard's book "Introductory Statistics with R"
or John Verzani's book  "Using R for introductory
statistics" both might be good for a start. Dalgaad's
book has an very handy appendix summarizing a lot of
the basic commands.

There are also quite a few excellent online books and
papers available on the R site (lefthand side at
bottom  you will see books and other)


From ccleland at optonline.net  Sat Jul 14 12:37:06 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 14 Jul 2007 06:37:06 -0400
Subject: [R] how to estimate treatment-interaction contrasts
In-Reply-To: <20070713155221.nya8t2e4cg800kg8@webmail.uoguelph.ca>
References: <20070712161628.h4rqf4j48wwks444@webmail.uoguelph.ca>
	<4697AA31.403@optonline.net>
	<20070713155221.nya8t2e4cg800kg8@webmail.uoguelph.ca>
Message-ID: <4698A752.4080201@optonline.net>

szhan at uoguelph.ca wrote:
> Hello, Chuck,
> Thank you very much for your help! But the contrasts I want to do  
> simutaneously is
>   contrasts(B)
>     [,1] [,2] [,3] [,4]
> b1   -4   -3   -2   -1
> b2    1   -3   -2   -1
> b3    1    2   -2   -1
> b4    1    2    3   -1
> b5    1    2    3    4
> 
> Could you please show me how to calculate estimates for ALL  
> intearaction constrasts using THESE contrasts? Say C2: c(-3, -3, 2, 2,  
> 2) as an example. I used the ortholognal constrasts as you suggest,  
> estimate for interaction contrast C2 is still -24.1.
> Joshua

Joshua:
  I use a variation on my contrasts to show how you might get the
estimates.  I then confirm the estimates by calculating the differences
of differences in means "by hand".

score <- c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
           7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)

A <- gl(2, 15, labels=c("a1", "a2"))
B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)

contrasts(B) <- matrix(c(3/5,  1/2,  0,  0,
                         3/5, -1/2,  0,  0,
                        -2/5,  0,  2/3,  0,
                        -2/5,  0, -1/3,  1/2,
                        -2/5,  0, -1/3, -1/2), ncol=4, byrow=TRUE)

fit1 <- aov(score ~ A*B)

summary(fit1, split=list(B=1:4), expand.split = TRUE)
            Df Sum Sq Mean Sq F value    Pr(>F)
A            1 3.2013  3.2013 15.1483 0.0009054 ***
B            4 8.7780  2.1945 10.3841 0.0001019 ***
  B: C1      1 1.0427  1.0427  4.9340 0.0380408 *
  B: C2      1 1.0208  1.0208  4.8304 0.0399049 *
  B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
  B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
A:B          4 5.3420  1.3355  6.3194 0.0018616 **
  A:B: C1    1 3.2267  3.2267 15.2684 0.0008734 ***
  A:B: C2    1 0.1008  0.1008  0.4771 0.4976647
  A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
  A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
Residuals   20 4.2267  0.2113
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

# Estimates with tests that match those above

summary(lm(score ~ A*B))

Call:
lm(formula = score ~ A * B)

Residuals:
     Min       1Q   Median       3Q      Max
-1.16667 -0.29167  0.03333  0.29167  0.73333

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   6.4933     0.1187  54.705  < 2e-16 ***
Aa2           0.6533     0.1679   3.892 0.000905 ***
B1            0.2889     0.2423   1.192 0.247086
B2            0.4000     0.3754   1.066 0.299271
B3            0.1333     0.3251   0.410 0.686038
B4           -1.5333     0.3754  -4.085 0.000577 ***
Aa2:B1       -1.3389     0.3426  -3.907 0.000873 ***
Aa2:B2        0.3667     0.5308   0.691 0.497665
Aa2:B3       -1.3833     0.4597  -3.009 0.006932 **
Aa2:B4        0.3667     0.5308   0.691 0.497665
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4597 on 20 degrees of freedom
Multiple R-Squared: 0.8038,     Adjusted R-squared: 0.7156
F-statistic: 9.107 on 9 and 20 DF,  p-value: 2.324e-05

# Confirm that estimates match differences of differences in means

diff(rowMeans(tapply(score, list(A,B), mean)[,1:2]) -
     rowMeans(tapply(score, list(A,B), mean)[,3:5]))
       a2
-1.338889

diff(tapply(score, list(A,B), mean)[,1] -
     tapply(score, list(A,B), mean)[,2])
       a2
0.3666667

diff(tapply(score, list(A,B), mean)[,3] -
     rowMeans(tapply(score, list(A,B), mean)[,4:5]))
       a2
-1.383333

diff(tapply(score, list(A,B), mean)[,4] -
     tapply(score, list(A,B), mean)[,5])
       a2
0.3666667

  So, applying the same strategy to your contrasts would give the following:

contrasts(B) <- matrix(c(-4/5, -3/5, -2/5, -1/5,
                          1/5, -3/5, -2/5, -1/5,
                          1/5,  2/5, -2/5, -1/5,
                          1/5,  2/5,  3/5, -1/5,
                          1/5,  2/5,  3/5,  4/5), ncol=4, byrow=TRUE)

fit1 <- aov(score ~ A*B)

summary(fit1, split=list(B=1:4), expand.split = TRUE)
            Df Sum Sq Mean Sq F value    Pr(>F)
A            1 3.2013  3.2013 15.1483 0.0009054 ***
B            4 8.7780  2.1945 10.3841 0.0001019 ***
  B: C1      1 0.0301  0.0301  0.1424 0.7099296
  B: C2      1 2.0335  2.0335  9.6221 0.0056199 **
  B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
  B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
A:B          4 5.3420  1.3355  6.3194 0.0018616 **
  A:B: C1    1 0.7207  0.7207  3.4105 0.0796342 .
  A:B: C2    1 2.6068  2.6068 12.3350 0.0021927 **
  A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
  A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
Residuals   20 4.2267  0.2113
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

summary(lm(score ~ A*B))

Call:
lm(formula = score ~ A * B)

Residuals:
     Min       1Q   Median       3Q      Max
-1.16667 -0.29167  0.03333  0.29167  0.73333

Coefficients:
              Estimate Std. Error   t value Pr(>|t|)
(Intercept)  6.493e+00  1.187e-01    54.705  < 2e-16 ***
Aa2          6.533e-01  1.679e-01     3.892 0.000905 ***
B1          -4.000e-01  3.754e-01    -1.066 0.299271
B2          -3.815e-16  3.754e-01 -1.02e-15 1.000000
B3          -9.000e-01  3.754e-01    -2.398 0.026373 *
B4           1.533e+00  3.754e-01     4.085 0.000577 ***
Aa2:B1      -3.667e-01  5.308e-01    -0.691 0.497665
Aa2:B2       6.000e-01  5.308e-01     1.130 0.271719
Aa2:B3       1.567e+00  5.308e-01     2.951 0.007893 **
Aa2:B4      -3.667e-01  5.308e-01    -0.691 0.497665
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4597 on 20 degrees of freedom
Multiple R-Squared: 0.8038,     Adjusted R-squared: 0.7156
F-statistic: 9.107 on 9 and 20 DF,  p-value: 2.324e-05

  Because your contrasts are correlated, the order in which they are
entered makes a difference.  Thus, the two summaries only agree for the
last interaction contrast (Aa2:B4).
  Perhaps more intuitively, you also can see that the tests are
sensitive to order for your contrasts as follows (no output shown):

# Second Contrast First

anova(lm(score ~
         A * I(B %in% c("b1","b2")) +
                 A * I(B %in% "b1") +
    A * I(B %in% c("b1","b2","b3")) +
 A * I(B %in% c("b1","b2","b3","b4"))), test="F")

# Second Contrast Last

anova(lm(score ~ A * I(B %in% "b1") +
    A * I(B %in% c("b1","b2","b3")) +
A * I(B %in% c("b1","b2","b3","b4")) +
          A * I(B %in% c("b1","b2"))), test="F")

  So if you really do want to evaluate your correlated contrasts
simultaneously, what makes the most sense to me is:

summary(lm(score ~ A*B))

  I would be interested in other people's thoughts on this, particularly
how to use something like estimable() in the gmodels package to achieve
these contrasts.

hope this helps,

Chuck

> Quoting Chuck Cleland <ccleland at optonline.net>:
> 
>> szhan at uoguelph.ca wrote:
>>> Hello, R experts,
>>> Sorry for asking this question again again since I really want a help!
>>>
>>> I have a two-factor experiment data and like to calculate estimates of
>>> interation contrasts say factor A has levels of a1, a2, and B has
>>> levels of b1, b2, b3, b4, and b5 with 3 replicates. I am not sure the
>>> constrast estimate I got is right using the script below:
>>>
>>> score<-c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
>>> 7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)
>>>
>>> A <- gl(2, 15, labels=c("a1", "a2"))
>>> B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)
>>>
>>> contrasts(B)<-cbind(c(-4,rep(1,4)),c(rep(-3,2),rep(2,3)),
>>> +  c(rep(-2,3),rep(3,2)),c(rep(-1,4), rep(4,1)))
>>> fit1 <- aov(score ~ A*B)
>>> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>>>                Df Sum Sq Mean Sq F value    Pr(>F)
>>> A            1 3.2013  3.2013 15.1483 0.0009054 ***
>>> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>>>      B: C1      1 0.0301  0.0301  0.1424 0.7099296
>>>      B: C2      1 2.0335  2.0335  9.6221 0.0056199 **
>>>      B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>>>      B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
>>> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>>>      A:B: C1    1 0.7207  0.7207  3.4105 0.0796342 .
>>>      A:B: C2    1 2.6068  2.6068 12.3350 0.0021927 **
>>>      A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>>>      A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
>>> Residuals   20 4.2267  0.2113
>>> ---
>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>
>>> Now I like to get interaction contrast estimate for b1 and b2 vs   
>>> b3, b4 and b5
>>> cont <- c(1, -1)[A] * c(-3, -3, 2, 2, 2)[B]
>>>
>>> estimat<-sum(cont*score) # value of the contrast estimate for A:B C2
>>>
>>>> estimat
>>> [1] -24.1
>>>
>>> I am not sure the estimate for A:B C2 contrast  (-24.1) is correct
>>> because the F value given the output above(12.3350) does not equal to
>>> those I calculate below (15.2684):
>>>
>>> t.stat <- sum(cont*score)/se.contrast(fit1, as.matrix(cont))
>>>> t.stat^2
>>> Contrast 1
>>>      15.2684
>>>
>>> Could you please help me calculate the correct the estimate of
>>> interaction contrast and corresponding F value?
>>> Thanks in advance!
>>> Joshua
>>   If the contrasts for B are orthogonal, then you get the result you
>> expected:
>>
>> score <- c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
>>            7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)
>>
>> A <- gl(2, 15, labels=c("a1", "a2"))
>> B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)
>>
>> contrasts(B) <- matrix(c(3, -1,  0,  0,
>>                          3,  1,  0,  0,
>>                         -2,  0,  2,  0,
>>                         -2,  0, -1,  1,
>>                         -2,  0, -1, -1), ncol=4, byrow=TRUE)
>>
>> fit1 <- aov(score ~ A*B)
>>
>> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>>
>>             Df Sum Sq Mean Sq F value    Pr(>F)
>> A            1 3.2013  3.2013 15.1483 0.0009054 ***
>> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>>   B: C1      1 1.0427  1.0427  4.9340 0.0380408 *
>>   B: C2      1 1.0208  1.0208  4.8304 0.0399049 *
>>   B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>>   B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
>> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>>   A:B: C1    1 3.2267  3.2267 15.2684 0.0008734 ***
>>   A:B: C2    1 0.1008  0.1008  0.4771 0.4976647
>>   A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>>   A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
>> Residuals   20 4.2267  0.2113
>> ---
>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>
>>   Note that I put the contrast of interest for B in the first column of
>> the contrast matrix.
>>
>> hope this helps,
>>
>> Chuck
>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> --
>> Chuck Cleland, Ph.D.
>> NDRI, Inc.
>> 71 West 23rd Street, 8th floor
>> New York, NY 10010
>> tel: (212) 845-4495 (Tu, Th)
>> tel: (732) 512-0171 (M, W, F)
>> fax: (917) 438-0894
>>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ripley at stats.ox.ac.uk  Sat Jul 14 12:59:34 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 Jul 2007 11:59:34 +0100 (BST)
Subject: [R] trouble compiling Hmisc package
In-Reply-To: <46151.168.20.2.120.1184353772.squirrel@mail.skio.usg.edu>
References: <46151.168.20.2.120.1184353772.squirrel@mail.skio.usg.edu>
Message-ID: <Pine.LNX.4.64.0707141133310.7120@gannet.stats.ox.ac.uk>

On Fri, 13 Jul 2007, colton.smith at skio.usg.edu wrote:

> Hi:
>
>  We're trying to install the Hmisc package on a Solaris 9 machine.

But Solaris 9 does not come with a compiler called 'g95', as far as I 
know.  So what is this?

The underlying problem is that Hmisc is not written in standard Fortran.
DO ... END DO statements are not Fortran77 and in Fortran 2003 I believe 
they are required to have integer loop indices. But most compilers cope, 
including the Solaris 'standard' f95.  Your first port of call (see the R 
posting guide) is the package maintainer, Cc:ed here.

Charles: the use of REAL*8 and INTEGER*4 is both non-standard and not what 
R has tested when being built.  Please use DOUBLE PRECSION and INTEGER.
And tabs are not valid: use spaces.

A quick fix is likely to be to declare

 	INTEGER xl,xr,yl,yu

in largrec.f.


> Here's what we get:
>
> R CMD INSTALL /usr/local/srctree/Hmisc_3.4-2.tar.gz
> * Installing to library '/usr/local/lib/R/library'
> * Installing *source* package 'Hmisc' ...
> ** libs
> g95   -fPIC  -g -O2 -c cidxcn.f -o cidxcn.o
> g95   -fPIC  -g -O2 -c cidxcp.f -o cidxcp.o
> g95   -fPIC  -g -O2 -c hoeffd.f -o hoeffd.o
> g95   -fPIC  -g -O2 -c jacklins.f -o jacklins.o
> g95   -fPIC  -g -O2 -c largrec.f -o largrec.o
> In file largrec.f:27
>
>      DO xl=xlim(1),xlim(2)-width,xinc
>          1
> Error: Loop variable at (1) must be a scalar INTEGER
> In file largrec.f:28
>
>         DO yl=ylim(1),ylim(2)-height,yinc
>             1
> Error: Loop variable at (1) must be a scalar INTEGER
> In file largrec.f:29
>
>            DO xr=xl+width,xlim(2),xinc
>                1
> Error: Loop variable at (1) must be a scalar INTEGER
> In file largrec.f:30
>
>               DO yu=yl+height,ylim(2),yinc
>                   1
> Error: Loop variable at (1) must be a scalar INTEGER
> make: *** [largrec.o] Error 1
> ERROR: compilation failed for package 'Hmisc'
> ** Removing '/usr/local/lib/R/library/Hmisc'
>
> Can anybody help us?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From raza_stat at yahoo.com  Sat Jul 14 13:02:23 2007
From: raza_stat at yahoo.com (Ali raza)
Date: Sat, 14 Jul 2007 04:02:23 -0700 (PDT)
Subject: [R] HELP FOR BUGS
Message-ID: <572513.95356.qm@web53204.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/395ec289/attachment.pl 

From neo27 at rakers.de  Sat Jul 14 13:16:54 2007
From: neo27 at rakers.de (Mark Hempelmann)
Date: Sat, 14 Jul 2007 13:16:54 +0200
Subject: [R] return()  in nested functions
Message-ID: <4698B0A6.9090009@rakers.de>

Dear WizaRds,

	After consulting different sources I am still unable to understand the 
correct use of return() in nested functions. To illustrate the problem:

f <- function(x,y,type){

	est1<-function(x,y){
	z=x+y
	out(x,y,z)}

	est2<-function(x,y){
	z=x*y
	out(x,y,z)}

	out<-function(x,y,z)
	return(x,y,z)

if (type=="est1") est1(x,y)
if (type=="est2") est2(x,y)
}

test<-f(1,2,type="est1") # gives Null for test

However, without the second 'if' condition, it works properly:
Warning message:
multi-argument returns are deprecated in: return(x, y, z)
> test
$x
[1] 1
$y
[1] 2
$z
[1] 3

Basically, the function I am working on is of the above structure, be it
more complex. I would like f to return the results of function 'out' to 
the user in the assigned variable, e.g. 'test'. i did consult try() and 
tryCatch(), but it doesn't seem to be what I am looking for.

Thank you for your help and understanding
mark


From murdoch at stats.uwo.ca  Sat Jul 14 13:31:47 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 14 Jul 2007 07:31:47 -0400
Subject: [R] return()  in nested functions
In-Reply-To: <4698B0A6.9090009@rakers.de>
References: <4698B0A6.9090009@rakers.de>
Message-ID: <4698B423.7020607@stats.uwo.ca>

On 14/07/2007 7:16 AM, Mark Hempelmann wrote:
> Dear WizaRds,
> 
> 	After consulting different sources I am still unable to understand the 
> correct use of return() in nested functions. To illustrate the problem:

return() knows nothing about nested functions.  It just returns from the 
current function.

In most respects, nested functions are just like any other functions in 
R.  The only important difference is the attached environment, which 
affects what is visible from within the function, and where <<- stores 
results.

> 
> f <- function(x,y,type){
> 
> 	est1<-function(x,y){
> 	z=x+y
> 	out(x,y,z)}
> 
> 	est2<-function(x,y){
> 	z=x*y
> 	out(x,y,z)}
> 
> 	out<-function(x,y,z)
> 	return(x,y,z)

Don't ignore the warning you saw from this!  Deprecated things 
eventually become defunct. You want return(list(x, y, z))

> 
> if (type=="est1") est1(x,y)
> if (type=="est2") est2(x,y)
> }
> 
> test<-f(1,2,type="est1") # gives Null for test

The result of the first if is the value from the call to est1.  The 
result of the second if is NULL.  That's what your function returned.

If you had wrapped those calls in return() you'd get what I think you 
expected:

if (type=="est1") return(est1(x,y))
if (type=="est2") return(est2(x,y))

because the return() causes the function to exit and return a value.

Duncan Murdoch

> 
> However, without the second 'if' condition, it works properly:
> Warning message:
> multi-argument returns are deprecated in: return(x, y, z)
>> test
> $x
> [1] 1
> $y
> [1] 2
> $z
> [1] 3
> 
> Basically, the function I am working on is of the above structure, be it
> more complex. I would like f to return the results of function 'out' to 
> the user in the assigned variable, e.g. 'test'. i did consult try() and 
> tryCatch(), but it doesn't seem to be what I am looking for.
> 
> Thank you for your help and understanding
> mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rhago at comcast.net  Sat Jul 14 14:38:09 2007
From: rhago at comcast.net (Forest Floor)
Date: Sat, 14 Jul 2007 08:38:09 -0400
Subject: [R] Extracting elements from a list
Message-ID: <4698C3B1.70206@comcast.net>

Hi,

I would love an easy way to extract elements from a list.  

For example, if I want the first element from each of 10 arrays stored 
in a list,  

Lst[[1:10]][1,1]  seems like a logical approach, but gives this error:  
"Error: recursive indexing failed at level 3"

The following workaround is functional but can get annoying/confusing.  

first.element=vector()
for (i in 1:10){ first.element=c(first.element, Lst[[i]][1,1])  }

Is there a better way to do this?   Thanks for any help!

Jeff


From ramasamy at cancer.org.uk  Sat Jul 14 15:02:31 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 14 Jul 2007 14:02:31 +0100
Subject: [R] Extracting elements from a list
In-Reply-To: <4698C3B1.70206@comcast.net>
References: <4698C3B1.70206@comcast.net>
Message-ID: <4698C967.1040209@cancer.org.uk>

Try

  sapply( Lst, function(m) m[1,1] )

Also note that to subset a list, you just need Lst[ 1:10 ] and not
Lst[[ 1:10 ]] (note the double square brackets).

Regards, Adai


Forest Floor wrote:
> Hi,
> 
> I would love an easy way to extract elements from a list.  
> 
> For example, if I want the first element from each of 10 arrays stored 
> in a list,  
> 
> Lst[[1:10]][1,1]  seems like a logical approach, but gives this error:  
> "Error: recursive indexing failed at level 3"
> 
> The following workaround is functional but can get annoying/confusing.  
> 
> first.element=vector()
> for (i in 1:10){ first.element=c(first.element, Lst[[i]][1,1])  }
> 
> Is there a better way to do this?   Thanks for any help!
> 
> Jeff
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From ramasamy at cancer.org.uk  Sat Jul 14 15:03:49 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 14 Jul 2007 14:03:49 +0100
Subject: [R] How to remove the quote "" in the data frame?
In-Reply-To: <dea6cb960707140002n79ad805axee8a758cf9011042@mail.gmail.com>
References: <3f2938d50707132335i20c7a196wc8e17217248574d8@mail.gmail.com>
	<dea6cb960707140002n79ad805axee8a758cf9011042@mail.gmail.com>
Message-ID: <4698C9B5.5070909@cancer.org.uk>

You can achieve this by cbind.data.frame()

Christophe Pallier wrote:
> Beware: you are not working with data.frames but with a vector and a
> matrice.
> (see ?cbind)
> 
> Solution: convert 'res' to data.frame.
> 
> Christophe
> 
> On 7/14/07, Zhang Jian <jzhang1982 at gmail.com> wrote:
>> If I do not add "ress" into the data frame "res", there is no quote in the
>> data frame. However, I add "ress", all column were found the quote.
>> How to remove it?
>> If you can delete the quote in the file "ress", that is better.
>> Thanks.
>>
>>> ress[1:10]
>> [1] "ABHO.ABNE" "ABHO.ACBA" "ABHO.ACGI" "ABHO.ACKO" "ABHO.ACMA" "ABHO.ACMO
>> "
>> [7] "ABHO.ACPS" "ABHO.ACSE" "ABHO.ACTE" "ABHO.ACTR"
>>> res=cbind(obv.value,p.value,mean.sim)
>>> res[1:10,]
>>       obv.value p.value mean.sim
>> [1,]         2     1.0      6.0
>> [2,]         0     1.0      0.0
>> [3,]        66     0.5     49.6
>> [4,]         3     1.0      3.0
>> [5,]         0     1.0     64.7
>> [6,]         0     1.0      0.0
>> [7,]         0     1.0      0.0
>> [8,]        51     0.5     39.8
>> [9,]         0     1.0     47.4
>> [10,]        59     0.7     72.0
>>
>>> ress=cbind(res,ress)
>>> ress[1:10,]
>>       obv.value p.value mean.sim ress
>> [1,] "2"       "1"     "6"      "ABHO.ABNE"
>> [2,] "0"       "1"     "0"      "ABHO.ACBA"
>> [3,] "66"      "0.5"   "49.6"   "ABHO.ACGI"
>> [4,] "3"       "1"     "3"      "ABHO.ACKO"
>> [5,] "0"       "1"     "64.7"   "ABHO.ACMA"
>> [6,] "0"       "1"     "0"      "ABHO.ACMO"
>> [7,] "0"       "1"     "0"      "ABHO.ACPS"
>> [8,] "51"      "0.5"   "39.8"   "ABHO.ACSE"
>> [9,] "0"       "1"     "47.4"   "ABHO.ACTE"
>> [10,] "59"      "0.7"   "72"     "ABHO.ACTR"
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
>


From ggrothendieck at gmail.com  Sat Jul 14 15:04:08 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 14 Jul 2007 09:04:08 -0400
Subject: [R] Extracting elements from a list
In-Reply-To: <4698C3B1.70206@comcast.net>
References: <4698C3B1.70206@comcast.net>
Message-ID: <971536df0707140604o55098b27r5631bbc7cd179d22@mail.gmail.com>

Use lapply or sapply:

> L <- list(a = 1:4, b = 11:15)
> lapply(L, "[[", 1)
$a
[1] 1

$b
[1] 11

> sapply(L, "[[", 1)
 a  b
 1 11

Also please see last line on every r-help message regarding providing
reproducible code.  Lst was not defined in your post.


On 7/14/07, Forest Floor <rhago at comcast.net> wrote:
> Hi,
>
> I would love an easy way to extract elements from a list.
>
> For example, if I want the first element from each of 10 arrays stored
> in a list,
>
> Lst[[1:10]][1,1]  seems like a logical approach, but gives this error:
> "Error: recursive indexing failed at level 3"
>
> The following workaround is functional but can get annoying/confusing.
>
> first.element=vector()
> for (i in 1:10){ first.element=c(first.element, Lst[[i]][1,1])  }
>
> Is there a better way to do this?   Thanks for any help!
>
> Jeff
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From naik.raghu at gmail.com  Sat Jul 14 15:28:02 2007
From: naik.raghu at gmail.com (Raghu Naik)
Date: Sat, 14 Jul 2007 09:28:02 -0400
Subject: [R] THANK YOU: Updating R version
Message-ID: <c98270fd0707140628n56833337y63fc9bd799297197@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/bd7852f7/attachment.pl 

From ripley at stats.ox.ac.uk  Sat Jul 14 15:32:28 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 Jul 2007 14:32:28 +0100 (BST)
Subject: [R] Extracting elements from a list
In-Reply-To: <4698C3B1.70206@comcast.net>
References: <4698C3B1.70206@comcast.net>
Message-ID: <Pine.LNX.4.64.0707141425450.9257@gannet.stats.ox.ac.uk>

The golden rule is that [[ ]] only returns one element:

sapply(Lst, "[", 1, 1)

is probably what you want.

On Sat, 14 Jul 2007, 'Forest Floor' aka 'rhago' aka 'Jeff' aka 'R User 
confused about his identity' wrote:

> Hi,
>
> I would love an easy way to extract elements from a list.
>
> For example, if I want the first element from each of 10 arrays stored
> in a list,
>
> Lst[[1:10]][1,1]  seems like a logical approach, but gives this error:
> "Error: recursive indexing failed at level 3"

It doesn't if you really have a list of 2D arrays.

> The following workaround is functional but can get annoying/confusing.
>
> first.element=vector()
> for (i in 1:10){ first.element=c(first.element, Lst[[i]][1,1])  }
>
> Is there a better way to do this?   Thanks for any help!
>
> Jeff
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

PLEASE DO!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Sat Jul 14 15:54:40 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 14 Jul 2007 09:54:40 -0400
Subject: [R] Fwd: THANK YOU: Updating R version
In-Reply-To: <fad888a10707131431v6e76609fm46468a944ea78a42@mail.gmail.com>
References: <c98270fd0707131046l662bdf9cm81fe165fdc5fafa0@mail.gmail.com>
	<4697E440.5000809@binghamton.edu>
	<fad888a10707131430obd058b1idc296dd24e1c9a41@mail.gmail.com>
	<fad888a10707131431v6e76609fm46468a944ea78a42@mail.gmail.com>
Message-ID: <971536df0707140654u12580e06vf1e6e70621485496@mail.gmail.com>

Note that the batchfiles distribution on CRAN has two batch
programs:

copydir.bat
movedir.bat

that simplify the copying portion of the procedure you
discuss below.  They can copy or move (copying
preserves the old directory but moving is much faster)
each package but will not overwrite existing packages
so the new versions of packages are preserved while the
old ones are added.  Here is an example using copydir.
Replace copydir with movedir below to move rather than copy:

  cd \Program Files\R
  copydir R-2.4.1\library R-2.5.0\library

See the batchfiles' home page at:

  http://code.google.com/p/batchfiles/

The README file contains the instructions given above (and more) and
also a number of alternative procedures that do not involve the use of
the batchfiles.

On 7/13/07, John C Frain <frainj at gmail.com> wrote:
> ---------- Forwarded message ----------
> From: John C Frain <frainj at gmail.com>
> Date: 13-Jul-2007 22:30
> Subject: Re: [R] THANK YOU: Updating R version
> To: "Christopher W. Ryan" <cryan at binghamton.edu>
>
>
> When I update R the following has worked for me (Windows XP)
>
> 1. Install the new version to a new directory (say C:\Program Files\R\R-2.5.1).
>
> 2 Rename the new library subdirectory  to library2.
>
> 3 Copy the entire contents of the old library subdirectory (say
> C:\Program Files\R\R-2.4.0\library\ to the new R root to create
> C:\Program Files\R\R-2.5.1\library\ .
>
> 4 Copy the contents of library2 to library to update your basic library.
>
> 5 Now start your new version of R and update packages from the GUI or
> from the R console.  (You may need to firs check Rprofile .site to
> ensure that no packages have been loaded)
>
> 6. On occasion I have got warning messages when I tried to load
> packages after this procedure.  This has been cleared by running
>
> update.packages(checkBuilt = TRUE)
>
> This checks that your packages have been built with the latest
> version.  When I do this I  agree to install all available updates.
>
> 7 You may wish to copy various autoloads etc from your old
> Rprofile.site to your new Rprofile.site.  I understand that there are
> some compatibility problems with 2.5.1 and SciViews so be careful.
>
> Best Regards
>
> John
>
>
>
> On 13/07/07, Christopher W. Ryan <cryan at binghamton.edu> wrote:
> > This sounds like a solution I've been looking for.  With this setup now
> > in place, when you download and install some new packages, where will
> > they be put?  Into C:\myRlib ?
> >
> > Thanks.
> >
> > --Chris
> > Christopher W. Ryan, MD
> > SUNY Upstate Medical University Clinical Campus at Binghamton
> > 40 Arch Street, Johnson City, NY  13790
> > cryanatbinghamtondotedu
> > PGP public keys available at http://home.stny.rr.com/ryancw/
> >
> > "If you want to build a ship, don't drum up the men to gather wood,
> > divide the work and give orders. Instead, teach them to yearn for the
> > vast and endless sea."  [Antoine de St. Exupery]
> >
> > Raghu Naik wrote:
> > > Based on the feedback received, I did the following:
> > >
> > > a) moved my lib sub-directory from the existing installed R version to
> > > c:\myRLib
> > > b) installed the updated R version
> > > c) created .Renviron file in the home directory (C:\R-2.5.1) with the line
> > > R_LIBS=c:/myRLib
> > > d) used .libPaths() command to confirm that the new R installation was
> > > recognizing the myRLib sub-directory
> > > e) deleted my old R installation
> > >
> > > Things worked fine.
> > >
> > > Thanks you.
> > >
> > >
> > >
> > > ---------- Forwarded message ----------
> > > From: Raghu Naik <naik.raghu at gmail.com>
> > > Date: Jun 2, 2007 5:13 PM
> > > Subject: Updating R version
> > > To: r-help at stat.math.ethz.ch
> > >
> > > A quick question.  I am trying to understand how I could move the installed
> > > packages in my R 2.3 version to the newly installed R 2.5 version, without
> > > having to install all the packages again. I copied the files under the old
> > > library subdirectory to the new library subdirectory. But still the newer
> > > version is not recognizing the packages that were copied over.
> > >
> > > Thanks.
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> John C Frain
> Trinity College Dublin
> Dublin 2
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.html
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
>
>
> --
> John C Frain
> Trinity College Dublin
> Dublin 2
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.html
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Sat Jul 14 17:01:02 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 14 Jul 2007 17:01:02 +0200
Subject: [R] HELP FOR BUGS
In-Reply-To: <572513.95356.qm@web53204.mail.re2.yahoo.com>
References: <572513.95356.qm@web53204.mail.re2.yahoo.com>
Message-ID: <4698E52E.80102@statistik.uni-dortmund.de>



Ali raza wrote:
> Hi Sir
> 
> I am very new user of R for the research project on multilevel
> logistic regression. There is confusion about bugs() function in R

Do you mean bugs() from package "R2WinBUGS"?
Yes, it is related to the software WinBUGS 1.4.x (and OpenBUGS 2.x with 
package "BRugs").

Uwe Ligges





> and BUGS software. Is there any relation between these two? Is there
> any comprehensive package for  Multilevel Logistic modelling in R?
> 
> Please guide in this regard.
> 
> Thank You
> 
> RAZA
> 
> 
> --------------------------------- Boardwalk for $500? In 2007? Ha!
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Sat Jul 14 17:43:58 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 14 Jul 2007 17:43:58 +0200
Subject: [R] ggplot usage question
In-Reply-To: <87zm1zyjzf.fsf@coco.kazmier.com>
References: <87zm1zyjzf.fsf@coco.kazmier.com>
Message-ID: <f8e6ff050707140843u505085x331882c58aa01952@mail.gmail.com>

On 7/14/07, Pete Kazmier <pete-expires-20070910 at kazmier.com> wrote:
> Could someone show me how to get a blue line in this plot?
>
> > ggplot(movies, aes(x=rating)) + stat_qq(geom="line",
>     quantiles=seq(0,1,0.005), distribution=qunif)

It's a bug in ggplot, sorry.  It will be fixed in the next version.

Hadley


From frainj at gmail.com  Sat Jul 14 18:01:20 2007
From: frainj at gmail.com (John C Frain)
Date: Sat, 14 Jul 2007 17:01:20 +0100
Subject: [R] Fwd: THANK YOU: Updating R version
In-Reply-To: <971536df0707140654u12580e06vf1e6e70621485496@mail.gmail.com>
References: <c98270fd0707131046l662bdf9cm81fe165fdc5fafa0@mail.gmail.com>
	<4697E440.5000809@binghamton.edu>
	<fad888a10707131430obd058b1idc296dd24e1c9a41@mail.gmail.com>
	<fad888a10707131431v6e76609fm46468a944ea78a42@mail.gmail.com>
	<971536df0707140654u12580e06vf1e6e70621485496@mail.gmail.com>
Message-ID: <fad888a10707140901v385852a3ve0744a714f3c96d2@mail.gmail.com>

Everyone using R in Windows should look at these batch files.  Some of
them are pure genius and will speed the process.  Thanks

John

On 14/07/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> Note that the batchfiles distribution on CRAN has two batch
> programs:
>
> copydir.bat
> movedir.bat
>
> that simplify the copying portion of the procedure you
> discuss below.  They can copy or move (copying
> preserves the old directory but moving is much faster)
> each package but will not overwrite existing packages
> so the new versions of packages are preserved while the
> old ones are added.  Here is an example using copydir.
> Replace copydir with movedir below to move rather than copy:
>
>  cd \Program Files\R
>  copydir R-2.4.1\library R-2.5.0\library
>
> See the batchfiles' home page at:
>
>  http://code.google.com/p/batchfiles/
>
> The README file contains the instructions given above (and more) and
> also a number of alternative procedures that do not involve the use of
> the batchfiles.
>
> On 7/13/07, John C Frain <frainj at gmail.com> wrote:
> > ---------- Forwarded message ----------
> > From: John C Frain <frainj at gmail.com>
> > Date: 13-Jul-2007 22:30
> > Subject: Re: [R] THANK YOU: Updating R version
> > To: "Christopher W. Ryan" <cryan at binghamton.edu>
> >
> >
> > When I update R the following has worked for me (Windows XP)
> >
> > 1. Install the new version to a new directory (say C:\Program Files\R\R-2.5.1).
> >
> > 2 Rename the new library subdirectory  to library2.
> >
> > 3 Copy the entire contents of the old library subdirectory (say
> > C:\Program Files\R\R-2.4.0\library\ to the new R root to create
> > C:\Program Files\R\R-2.5.1\library\ .
> >
> > 4 Copy the contents of library2 to library to update your basic library.
> >
> > 5 Now start your new version of R and update packages from the GUI or
> > from the R console.  (You may need to firs check Rprofile .site to
> > ensure that no packages have been loaded)
> >
> > 6. On occasion I have got warning messages when I tried to load
> > packages after this procedure.  This has been cleared by running
> >
> > update.packages(checkBuilt = TRUE)
> >
> > This checks that your packages have been built with the latest
> > version.  When I do this I  agree to install all available updates.
> >
> > 7 You may wish to copy various autoloads etc from your old
> > Rprofile.site to your new Rprofile.site.  I understand that there are
> > some compatibility problems with 2.5.1 and SciViews so be careful.
> >


From xli28 at uic.edu  Sat Jul 14 18:20:58 2007
From: xli28 at uic.edu (Li, Xue)
Date: Sat, 14 Jul 2007 11:20:58 -0500 (CDT)
Subject: [R] memory problem
Message-ID: <2286.128.248.54.123.1184430058.squirrel@webmail.uic.edu>

Hi,

My computer has 2GB of ram and I also request 2GB of virtual ram from c
drive, therefore totally I have 4GB of ram. Before I open R workshop, I
also add "C:\Program Files\R\R-2.5.0\bin\Rgui.exe"
--max-mem-size=3000Mb--max-vsize=3000Mb" into the target of R by right
clicking the R icon-properties.

I am running a program (DPglmm), which is built in DPpackage.  The program
is running, but at the final stage, R generates an error message 'Error:
cannot allocate vector of size 309.9 Mb'

My code and output is as follows.

> memory.limit(size=4000)
NULL
> fit0 <- DPglmm(fixed=Bpov~1,random=~1|cluster,data=MyData,
+ family=binomial(logit),n=Total,prior=prior,mcmc=mcmc,state=NULL, +
status=TRUE)
Error: cannot allocate vector of size 309.9 Mb

It is very interesting that If i don't request 2GB of virtual memory from
hard drive, the DPglmm doesn't run at all. After 2 minutes, R generates an
error message 'Error: cannot allocate vector of size 309.9 Mb'. When i
increased the ram, the DPglmm is running and it takes about 1 hr to finish
running. Then it generates an error message 'Error: cannot allocate vector
of size 309.9 Mb'. When I click fit0, i get nothing.

I am very frustrated with this. It is very wired that my computer does
have enough ram, however a vetor of size 309.9 Mb can't be stored.


Thanks very much.

xue


From jzhang1982 at gmail.com  Sat Jul 14 23:16:25 2007
From: jzhang1982 at gmail.com (Zhang Jian)
Date: Sat, 14 Jul 2007 15:16:25 -0600
Subject: [R] How to read many files at one time?
Message-ID: <3f2938d50707141416j46222b9agcd6880884008af9b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/1cd917c4/attachment.pl 

From brown_emu at yahoo.com  Sat Jul 14 23:38:34 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 14 Jul 2007 14:38:34 -0700 (PDT)
Subject: [R] polymorphic functions in ggplot? (WAS Re: Drawing rectangles in
	multiple panels)
In-Reply-To: <f8e6ff050707112132k19f07f48p2335b27f8909ff1@mail.gmail.com>
Message-ID: <471214.50640.qm@web39714.mail.mud.yahoo.com>

Regarding your earlier statement,

"I tend to think in very data centric approach, where you first generate the
data (in a data frame) and then you plot it. There is very little data
creation/modification during the plotting itself..."

Is the data generation and plotting truly separate and sequential? I'm
not entirely clear on this point - as statistical
transformations/operations return objects that require new variables
to be created - and this may be rooted in semantics (the verbal one,
not the computational) of the grammar of graphics - in the online book
draft of 'ggplot' it says (p. 37)

"The explicit transformation stage was dropped because variable
transformations are already so easy in R: they do not need to be part
of the grammar."

In my understanding of what transformations are defined to be, they
involve statistical ones - which perhaps I'm not truly getting because
tranformations are defined (by L. Wilkinson) as a mapping of elements
of one set to elements of the same set, and yet a function like
median() will accept a list (of values) and return a single
value... in any case maybe there is a distinction between a
statistical 'transformation' and a statistical 'operation' that I've
missed, but statistical 'transformations' are included in ggplot's
"stat" functions. L. Wilkinson also seems to include an explicit TRANS
specification at times (for example, in the case of the boxplot on
p.60) and at other times nest it into the ELEMENT specification (for
example, the histogram on p. 47).

In any case, I interpret that the following progression is achieved
through 'data operations' and 'application of algebra' in the language
of L. Wilkinson and through I/O, merge, reshape, and other functions
in R:

source object -> variables -> varset

A statistic might then computed on the varset, which will return
another source object (true in R as well: e.g., class 'histogram' or
'lm') from which variables can again be extracted, varsets
constructed, etc. to yield a list of tuples to be associated with
geometrical and aesthetic attributes. Indeed, in the bootstrap
example, L. Wilkinson begins by extracting variables from a bootstrap
function on another variable that has not explicitly been created from
source (dataset).

So it's not clear to me that the the data creation step is necessarily
distinct from the plotting, as it is more (but not completely) so in
the traditional graphics system:

## DATA specification
variable <- rnorm(100)
## TRANS specification
statsObj <- hist(variable,nclass=20,plot=FALSE)
## Transformed data is plotted (variables extracted implicity and
## associated with default geometry/aesthetic mappings)
plot(statsObj)

Below is an analogous plot in ggplot, where the creation of the
summary object occurs as part of the grammar:

ggplot(data=data.frame(variable),mapping=aes(x=variable)) +
stat_bin(breaks=statsObj$breaks)

Since all statistical transformations/operations aren't handled by
ggplot, it seems that working with non-data-frame objects (for
example, of class 'nls' or 'rlm') require data operations (p.7) (to
extract fitted values, etc.). Of course, R provides these facilities,
but the plotting functions in the traditional graphics system
accommodate a number of object classes through polymorphic
functions. I wonder if in a similar way for ggplot, stat_bin could
accept objects of 'histogram' class [hist() allows the user to specify
'nclass', which will then compute the breaks], or stat_smooth could
accept 'rlm' objects. Of course, in the case of an 'lm' object, plot()
additionally gives diagnostic (residual and Q-Q) plots but that type of
response does not seem to fit in with the expected behavior of ggplot
functions...


--- hadley wickham <h.wickham at gmail.com> wrote:

> On 7/12/07, Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > > A question/comment: I have usually found that the subscripts argument
> is
> > > > what I need when passing *external* information into the panel
> function, for
> > > > example, when I wish to add results from a fit done external to the
> trellis
> > > > call. Fits[subscripts] gives me the fits (or whatever) I want to plot
> for
> > > > each panel. It is not clear to me how the panel layout information
> from
> > > > panel.number(), etc. would be helpful here instead. Am I correct? --
> or is
> > > > there a smarter way to do this that I've missed?
> > >
> > > This is one of things that I think ggplot does better - it's much
> > > easier to plot multiple data sources.  I don't have many examples of
> > > this yet, but the final example on
> > > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> >
> > That's probably true. The Trellis approach is to define a plot by
> > "data source" + "type of plot", whereas the ggplot approach (if I
> > understand correctly) is to create a specification for the display
> > (incrementally?) and then render it. Since the specification can be
> > very general, the approach is very flexible. The downside is that you
> > need to learn the language.
> 
> Yes, that's right.  ggplot basically decomposes "type of plot" into
> statistical transformation (stat) + geometric object and allows you to
> control each component separately.  ggplot also explicitly includes
> the idea of layers (ie. one layer is a scatterplot and another layer
> is a loess smooth) and allows you to supply different datasets to
> different layers.
> 
> > On a philosophical note, I think the apparent limitations of Trellis
> > in some (not all) cases is just due to the artificial importance given
> > to data frames as the one true container for data. Now that we have
> > proper multiple dispatch in S4, we can write methods that behave like
> > traditional Trellis calls but work with more complex data structures.
> > We have tried this in one bioconductor package (flowViz) with
> > encouraging results.
> 
> That's one area which I haven't thought much about.  ggplot is very
> data.frame centric and it's not yet clear to me how plotting a linear
> model (say) would fit into the grammar.
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Sat Jul 14 23:48:18 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 14 Jul 2007 14:48:18 -0700 (PDT)
Subject: [R] scaling of different data sets in ggplot
Message-ID: <683573.91772.qm@web39706.mail.mud.yahoo.com>

Dear list (but probably mostly Hadley):

In ggplot, operations to modify 'guides' are accessed through grid
objects, but I did not find mention of creating new guides or possibly
removing them altogether using ggplot functions. I wonder if this is
something I need to learn grid to learn more about (which I hope to do
eventually).

Also, ggplot()+geom_object() [where 'object' can be point, line, etc.]
or layer() contains specification for the data, mappings and
geoms/stats - but the geoms/stats can be scale-dependent [for
instance, log]. so I wonder how different scalings can be applied to
different data sets.

Below is an example that requires both:

x <- runif(100) y <- exp(x^2) z <- x^2+rnorm(100,0,0.02)

par(mar=c(5,4,2,4)+0.1) plot(x,y,log="y") lines(lowess(x,y,f=1/3))
par(new=TRUE) plot(x,z,col=2,pch=3,yaxt="n",ylab="")
lines(lowess(x,z,f=1/3),col=2) axis(4,col=2,col.axis=2)
mtext("z",4,line=3,col=2)

In ggplot:

## data specification
ggplot(data=data.frame(x,y,z)) +

  ## first set of points geom_point(mapping=aes(x=x,y=y)) +
  ## scale_y_log() +

  ## second set of points geom_point(mapping=aes(x=x,y=z),pch=3) +
  ## layer(mapping=aes(x=x,y=z),stat="smooth",method="loess") +
  ## scale_y_continuous()

scale_y_log() and scale_y_continuous() appear to apply to both mappings at
once, and I can't figure out how to associate them with the intended ones (I
expect this will be a desire for size and color scales as well).

Of course, I can always try to fool the system by (1) applying the scaling a
priori to create a new variable, (2) plotting points from the new variable,
and (3) creating a new axis with custom labels. Which then brings me back to
...how to add new guides? :)

Thanks,

Stephen



      ____________________________________________________________________________________


From Mike.Lawrence at DAL.CA  Sat Jul 14 23:51:59 2007
From: Mike.Lawrence at DAL.CA (Mike Lawrence)
Date: Sat, 14 Jul 2007 18:51:59 -0300
Subject: [R] How to read many files at one time?
In-Reply-To: <3f2938d50707141416j46222b9agcd6880884008af9b@mail.gmail.com>
References: <3f2938d50707141416j46222b9agcd6880884008af9b@mail.gmail.com>
Message-ID: <EC7085C8-7105-4AD8-8257-89EB16115A9F@DAL.CA>


On 14-Jul-07, at 6:16 PM, Zhang Jian wrote:

> I want to load many files in the R. The names of the files are  
> "Sim1.txt", "
> Sim2.txt", "Sim3.txt", "Sim4.txt", "Sim5.txt" and so on.
> Can I read them at one time? What should I do? I can give the same  
> names in
> R.
> Thanks.
>
> For example:
>> tst=paste("Sim",1:20,".txt",sep="") # the file names
>> tst
>  [1] "Sim1.txt"  "Sim2.txt"  "Sim3.txt"  "Sim4.txt"  "Sim5.txt"   
> "Sim6.txt"
>  [7] "Sim7.txt"  "Sim8.txt"  "Sim9.txt"  "Sim10.txt" "Sim11.txt"  
> "Sim12.txt"
> [13] "Sim13.txt" "Sim14.txt" "Sim15.txt" "Sim16.txt" "Sim17.txt"  
> "Sim18.txt"
> [19] "Sim19.txt" "Sim20.txt"
>
>> data.name=paste("Sim",1:20,sep="") # the file names in R
>> data.name
>  [1] "Sim1"  "Sim2"  "Sim3"  "Sim4"  "Sim5"  "Sim6"  "Sim7"   
> "Sim8"  "Sim9"
> [10] "Sim10" "Sim11" "Sim12" "Sim13" "Sim14" "Sim15" "Sim16"  
> "Sim17" "Sim18"
> [19] "Sim19" "Sim20"

you could read each data file as a named element into a single list
data=list(NULL)
for(i in 1:length(tst)){
	data[[i]]=read.table(tst[i])
	names(data)[i]=data.name[i]
}

if you want to refer to Sim1, type
data$"Sim1"


From brown_emu at yahoo.com  Sat Jul 14 23:58:55 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 14 Jul 2007 14:58:55 -0700 (PDT)
Subject: [R] How to read many files at one time?
In-Reply-To: <3f2938d50707141416j46222b9agcd6880884008af9b@mail.gmail.com>
Message-ID: <986041.57215.qm@web39714.mail.mud.yahoo.com>

This should do it:

allData <- sapply(paste("Sim",1:20,sep=""),
                  function(.x) read.table(paste(.x,"txt",sep=".")),
                  simplify=FALSE)

see ?read.table for specification of delimiters, etc.

allData will be a list, and you can access the contents of each file by
any of the following commands:
allData[[2]]
allData[["Sim2"]]
allData$Sim2


--- Zhang Jian <jzhang1982 at gmail.com> wrote:

> I want to load many files in the R. The names of the files are "Sim1.txt",
> "
> Sim2.txt", "Sim3.txt", "Sim4.txt", "Sim5.txt" and so on.
> Can I read them at one time? What should I do? I can give the same names in
> R.
> Thanks.
> 
> For example:
> > tst=paste("Sim",1:20,".txt",sep="") # the file names
> > tst
>  [1] "Sim1.txt"  "Sim2.txt"  "Sim3.txt"  "Sim4.txt"  "Sim5.txt"  "Sim6.txt"
>  [7] "Sim7.txt"  "Sim8.txt"  "Sim9.txt"  "Sim10.txt" "Sim11.txt"
> "Sim12.txt"
> [13] "Sim13.txt" "Sim14.txt" "Sim15.txt" "Sim16.txt" "Sim17.txt"
> "Sim18.txt"
> [19] "Sim19.txt" "Sim20.txt"
> 
> > data.name=paste("Sim",1:20,sep="") # the file names in R
> > data.name
>  [1] "Sim1"  "Sim2"  "Sim3"  "Sim4"  "Sim5"  "Sim6"  "Sim7"  "Sim8"  "Sim9"
> [10] "Sim10" "Sim11" "Sim12" "Sim13" "Sim14" "Sim15" "Sim16" "Sim17"
> "Sim18"
> [19] "Sim19" "Sim20"
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Sun Jul 15 00:04:42 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 14 Jul 2007 15:04:42 -0700 (PDT)
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
Message-ID: <708788.39980.qm@web39701.mail.mud.yahoo.com>


I wonder what kind of objects? Are there large advantages for allowing
lattice functions to operate on objects other than data frames - I
couldn't find any screenshots of flowViz but I imagine those objects
would probably be list of arrays and such? I tend to think of mapply()
[and more recently melt()], etc. could always be applied beforehand,
but I suppose that would undermine the case for having generic
functions to support the rich collection of object classes in R...


--- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:

> On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > A question/comment: I have usually found that the subscripts argument
> is
> > > what I need when passing *external* information into the panel
> function, for
> > > example, when I wish to add results from a fit done external to the
> trellis
> > > call. Fits[subscripts] gives me the fits (or whatever) I want to plot
> for
> > > each panel. It is not clear to me how the panel layout information from
> > > panel.number(), etc. would be helpful here instead. Am I correct? -- or
> is
> > > there a smarter way to do this that I've missed?
> >
> > This is one of things that I think ggplot does better - it's much
> > easier to plot multiple data sources.  I don't have many examples of
> > this yet, but the final example on
> > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> 
> That's probably true. The Trellis approach is to define a plot by
> "data source" + "type of plot", whereas the ggplot approach (if I
> understand correctly) is to create a specification for the display
> (incrementally?) and then render it. Since the specification can be
> very general, the approach is very flexible. The downside is that you
> need to learn the language.
> 
> On a philosophical note, I think the apparent limitations of Trellis
> in some (not all) cases is just due to the artificial importance given
> to data frames as the one true container for data. Now that we have
> proper multiple dispatch in S4, we can write methods that behave like
> traditional Trellis calls but work with more complex data structures.
> We have tried this in one bioconductor package (flowViz) with
> encouraging results.
> 
> -Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Sun Jul 15 00:10:03 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 14 Jul 2007 15:10:03 -0700 (PDT)
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <971536df0707112021wa3d47e7wad8008e9bb2e67d4@mail.gmail.com>
Message-ID: <536011.27267.qm@web39715.mail.mud.yahoo.com>

This is very interesting - but I'm not entirely clear on your last statement
though about how existing functions can cause problems with the scoping that
createWrapper() avoids... (but thanks for the tip).


--- Gabor Grothendieck <ggrothendieck at gmail.com> wrote:

> Your approach of using closures is cleaner than that
> given below but just for comparison in:
> 
> http://tolstoy.newcastle.edu.au/R/devel/06/03/4476.html
> 
> there is a createWrapper function which creates a new function based
> on the function passed as its first argument by using the components
> of the list passed as its second argument to overwrite its formal
> arguments.  For example,
> 
> createWrapper <- function(FUN, Params) {
>    as.function(c(replace(formals(FUN), names(Params), Params), body(FUN)))
> }
> 
> library(lattice)
> 
> rectInfo <-
>    list(matrix(runif(4), 2, 2),
>         matrix(runif(4), 2, 2),
>         matrix(runif(4), 2, 2))
> 
> 
> panel.qrect <- function(x, y, ..., rect.info) {
>    ri <- rect.info[[packet.number()]]
>    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
>               col = "grey86", border = NA)
>    panel.xyplot(x, y, ...)
> }
> 
> xyplot(runif(30) ~ runif(30) | gl(3, 10),
>       panel = createWrapper(panel.qrect, list(rect.info = rectInfo)))
> 
> The createWrapper approach does have an advantage in the situation
> where the function analogous to panel.qrect is existing since using
> scoping then involves manipulation of environments in the closure
> approach.
> 
> On 7/11/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> > In the Trellis approach, another way (I like) to deal with multiple
> pieces of
> > external data sources is to 'attach' them to panel functions through
> lexical
> > closures. For instance...
> >
> > rectInfo <-
> >    list(matrix(runif(4), 2, 2),
> >         matrix(runif(4), 2, 2),
> >         matrix(runif(4), 2, 2))
> >
> > panel.qrect <- function(rect.info) {
> >  function(x, y, ...) {
> >    ri <- rect.info[[packet.number()]]
> >    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
> >               col = "grey86", border = NA)
> >    panel.xyplot(x, y, ...)
> >  }
> > }
> >
> > xyplot(runif(30) ~ runif(30) | gl(3, 10),
> >       panel = panel.qrect(rectInfo))
> >
> > ...which may or may not be more convenient than passing rectInfo (and
> perhaps
> > other objects if desired) explicitly as an argument to xyplot().
> >
> >
> > --- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> >
> > > On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > > > A question/comment: I have usually found that the subscripts
> argument
> > > is
> > > > > what I need when passing *external* information into the panel
> > > function, for
> > > > > example, when I wish to add results from a fit done external to the
> > > trellis
> > > > > call. Fits[subscripts] gives me the fits (or whatever) I want to
> plot
> > > for
> > > > > each panel. It is not clear to me how the panel layout information
> from
> > > > > panel.number(), etc. would be helpful here instead. Am I correct?
> -- or
> > > is
> > > > > there a smarter way to do this that I've missed?
> > > >
> > > > This is one of things that I think ggplot does better - it's much
> > > > easier to plot multiple data sources.  I don't have many examples of
> > > > this yet, but the final example on
> > > > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> > >
> > > That's probably true. The Trellis approach is to define a plot by
> > > "data source" + "type of plot", whereas the ggplot approach (if I
> > > understand correctly) is to create a specification for the display
> > > (incrementally?) and then render it. Since the specification can be
> > > very general, the approach is very flexible. The downside is that you
> > > need to learn the language.
> > >
> > > On a philosophical note, I think the apparent limitations of Trellis
> > > in some (not all) cases is just due to the artificial importance given
> > > to data frames as the one true container for data. Now that we have
> > > proper multiple dispatch in S4, we can write methods that behave like
> > > traditional Trellis calls but work with more complex data structures.
> > > We have tried this in one bioconductor package (flowViz) with
> > > encouraging results.
> > >
> > > -Deepayan
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 



       
____________________________________________________________________________________

that gives answers, not web links.


From thomas_schwander at web.de  Sat Jul 14 21:37:29 2007
From: thomas_schwander at web.de (Thomas Schwander)
Date: Sat, 14 Jul 2007 21:37:29 +0200
Subject: [R] Send SMS out of R?
Message-ID: <000401c7c64e$6a87ec70$6502a8c0@laptop>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070714/c92624ea/attachment.pl 

From snpandit at hotmail.com  Sun Jul 15 02:35:40 2007
From: snpandit at hotmail.com (SN P)
Date: Sun, 15 Jul 2007 00:35:40 +0000
Subject: [R] how to remove effects (distance as a covariate) in linear model
Message-ID: <BAY123-W2092FCEF37D7A40B3FE2FCD4FF0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070715/c1ab105b/attachment.pl 

From edd at debian.org  Sun Jul 15 02:45:29 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 14 Jul 2007 19:45:29 -0500
Subject: [R] Send SMS out of R?
In-Reply-To: <000401c7c64e$6a87ec70$6502a8c0@laptop>
References: <000401c7c64e$6a87ec70$6502a8c0@laptop>
Message-ID: <18073.28201.848916.86468@basebud.nulle.part>


On 14 July 2007 at 21:37, Thomas Schwander wrote:
| I use Windows XP Professional, R 2.5.1 and I have Blat to send eMails out of
| R. Works perfect! Thank you for your help!
| 
|  
| 
| Now I want to send an SMS out of R! Any idea how it could work? Could I send
| an eMail to a mobile phone number?

Obviously the same way: find a windows program that does the actual sending,
and uses system() from R to talk to it.

Dirk

-- 
Hell, there are no rules here - we're trying to accomplish something. 
                                                  -- Thomas A. Edison


From deepayan.sarkar at gmail.com  Sun Jul 15 03:32:25 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 14 Jul 2007 18:32:25 -0700
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <708788.39980.qm@web39701.mail.mud.yahoo.com>
References: <eb555e660707111552t37a93bb6n5b0f7824fde225ec@mail.gmail.com>
	<708788.39980.qm@web39701.mail.mud.yahoo.com>
Message-ID: <eb555e660707141832g7617b654s21e570b1712fcad@mail.gmail.com>

On 7/14/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
>
> I wonder what kind of objects? Are there large advantages for allowing
> lattice functions to operate on objects other than data frames - I
> couldn't find any screenshots of flowViz but I imagine those objects
> would probably be list of arrays and such? I tend to think of mapply()
> [and more recently melt()], etc. could always be applied beforehand,
> but I suppose that would undermine the case for having generic
> functions to support the rich collection of object classes in R...

There's a copy of a presentation at

http://www.ficcs.org/meetings/ficcs3/presentations/DeepayanSarkar-flowviz.pdf

and a (largish - 37M) vignette linked from

http://bioconductor.org/packages/2.1/bioc/html/flowViz.html

Neither of these really talk about the challenge posed by the size of
the data. The data structure, as with most microarray-type
experiments, is like a data frame, except that the response for every
experimental unit is itself a large matrix. If we represented the GvHD
data set (the one used in the examples) as a "long format" data frame
that lattice would understand, it would have 585644 rows and 12
columns (8 measurements that are different for each row, and 4
phenotypic variables that are the same for all rows coming from a
single sample). And this is for a smallish subset of the actual
experiment.

In practice, the data are stored in an environment to prevent
unnecessary copying, and panel functions only access one data matrix
at a time.

-Deepayan


> --- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
>
> > On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > > A question/comment: I have usually found that the subscripts argument
> > is
> > > > what I need when passing *external* information into the panel
> > function, for
> > > > example, when I wish to add results from a fit done external to the
> > trellis
> > > > call. Fits[subscripts] gives me the fits (or whatever) I want to plot
> > for
> > > > each panel. It is not clear to me how the panel layout information from
> > > > panel.number(), etc. would be helpful here instead. Am I correct? -- or
> > is
> > > > there a smarter way to do this that I've missed?
> > >
> > > This is one of things that I think ggplot does better - it's much
> > > easier to plot multiple data sources.  I don't have many examples of
> > > this yet, but the final example on
> > > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> >
> > That's probably true. The Trellis approach is to define a plot by
> > "data source" + "type of plot", whereas the ggplot approach (if I
> > understand correctly) is to create a specification for the display
> > (incrementally?) and then render it. Since the specification can be
> > very general, the approach is very flexible. The downside is that you
> > need to learn the language.
> >
> > On a philosophical note, I think the apparent limitations of Trellis
> > in some (not all) cases is just due to the artificial importance given
> > to data frames as the one true container for data. Now that we have
> > proper multiple dispatch in S4, we can write methods that behave like
> > traditional Trellis calls but work with more complex data structures.
> > We have tried this in one bioconductor package (flowViz) with
> > encouraging results.
> >
> > -Deepayan


From ggrothendieck at gmail.com  Sun Jul 15 06:01:47 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 15 Jul 2007 00:01:47 -0400
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <536011.27267.qm@web39715.mail.mud.yahoo.com>
References: <971536df0707112021wa3d47e7wad8008e9bb2e67d4@mail.gmail.com>
	<536011.27267.qm@web39715.mail.mud.yahoo.com>
Message-ID: <971536df0707142101p52ea02a8gb41f4a0be826f5ee@mail.gmail.com>

Suppose ri were already defined as in the example below.
Then panel.qrect is a bit harder to define although with
work its possible as shown below:

rectInfo <-
   list(matrix(runif(4), 2, 2),
        matrix(runif(4), 2, 2),
        matrix(runif(4), 2, 2))

ri <- function(x, y, ..., rect.info) {
   ri <- rect.info[[packet.number()]]
   panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
      col = "grey86", border = NA)
   panel.xyplot(x, y, ...)
 }

panel.qrect <- function(rect.info) {
	function(x, y, ...) {
		environment(ri) <- environment() ###
		ri(x, y, ..., rect.info = rect.info)
	}
}

xyplot(runif(30) ~ runif(30) | gl(3, 10),
      panel = panel.qrect(rectInfo))



On 7/14/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> This is very interesting - but I'm not entirely clear on your last statement
> though about how existing functions can cause problems with the scoping that
> createWrapper() avoids... (but thanks for the tip).
>
>
> --- Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>
> > Your approach of using closures is cleaner than that
> > given below but just for comparison in:
> >
> > http://tolstoy.newcastle.edu.au/R/devel/06/03/4476.html
> >
> > there is a createWrapper function which creates a new function based
> > on the function passed as its first argument by using the components
> > of the list passed as its second argument to overwrite its formal
> > arguments.  For example,
> >
> > createWrapper <- function(FUN, Params) {
> >    as.function(c(replace(formals(FUN), names(Params), Params), body(FUN)))
> > }
> >
> > library(lattice)
> >
> > rectInfo <-
> >    list(matrix(runif(4), 2, 2),
> >         matrix(runif(4), 2, 2),
> >         matrix(runif(4), 2, 2))
> >
> >
> > panel.qrect <- function(x, y, ..., rect.info) {
> >    ri <- rect.info[[packet.number()]]
> >    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
> >               col = "grey86", border = NA)
> >    panel.xyplot(x, y, ...)
> > }
> >
> > xyplot(runif(30) ~ runif(30) | gl(3, 10),
> >       panel = createWrapper(panel.qrect, list(rect.info = rectInfo)))
> >
> > The createWrapper approach does have an advantage in the situation
> > where the function analogous to panel.qrect is existing since using
> > scoping then involves manipulation of environments in the closure
> > approach.
> >
> > On 7/11/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> > > In the Trellis approach, another way (I like) to deal with multiple
> > pieces of
> > > external data sources is to 'attach' them to panel functions through
> > lexical
> > > closures. For instance...
> > >
> > > rectInfo <-
> > >    list(matrix(runif(4), 2, 2),
> > >         matrix(runif(4), 2, 2),
> > >         matrix(runif(4), 2, 2))
> > >
> > > panel.qrect <- function(rect.info) {
> > >  function(x, y, ...) {
> > >    ri <- rect.info[[packet.number()]]
> > >    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
> > >               col = "grey86", border = NA)
> > >    panel.xyplot(x, y, ...)
> > >  }
> > > }
> > >
> > > xyplot(runif(30) ~ runif(30) | gl(3, 10),
> > >       panel = panel.qrect(rectInfo))
> > >
> > > ...which may or may not be more convenient than passing rectInfo (and
> > perhaps
> > > other objects if desired) explicitly as an argument to xyplot().
> > >
> > >
> > > --- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > >
> > > > On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > > > > A question/comment: I have usually found that the subscripts
> > argument
> > > > is
> > > > > > what I need when passing *external* information into the panel
> > > > function, for
> > > > > > example, when I wish to add results from a fit done external to the
> > > > trellis
> > > > > > call. Fits[subscripts] gives me the fits (or whatever) I want to
> > plot
> > > > for
> > > > > > each panel. It is not clear to me how the panel layout information
> > from
> > > > > > panel.number(), etc. would be helpful here instead. Am I correct?
> > -- or
> > > > is
> > > > > > there a smarter way to do this that I've missed?
> > > > >
> > > > > This is one of things that I think ggplot does better - it's much
> > > > > easier to plot multiple data sources.  I don't have many examples of
> > > > > this yet, but the final example on
> > > > > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> > > >
> > > > That's probably true. The Trellis approach is to define a plot by
> > > > "data source" + "type of plot", whereas the ggplot approach (if I
> > > > understand correctly) is to create a specification for the display
> > > > (incrementally?) and then render it. Since the specification can be
> > > > very general, the approach is very flexible. The downside is that you
> > > > need to learn the language.
> > > >
> > > > On a philosophical note, I think the apparent limitations of Trellis
> > > > in some (not all) cases is just due to the artificial importance given
> > > > to data frames as the one true container for data. Now that we have
> > > > proper multiple dispatch in S4, we can write methods that behave like
> > > > traditional Trellis calls but work with more complex data structures.
> > > > We have tried this in one bioconductor package (flowViz) with
> > > > encouraging results.
> > > >
> > > > -Deepayan
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
>
>
>
>
> ____________________________________________________________________________________
>
> that gives answers, not web links.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Sun Jul 15 07:55:35 2007
From: jholtman at gmail.com (jim holtman)
Date: Sun, 15 Jul 2007 01:55:35 -0400
Subject: [R] Algorythmic Question on Array Filtration
In-Reply-To: <f7a4ef$gud$1@sea.gmane.org>
References: <f783nq$2ap$1@sea.gmane.org>
	<445456.26729.qm@web32811.mail.mud.yahoo.com>
	<f7a4ef$gud$1@sea.gmane.org>
Message-ID: <644e1f320707142255g4db92724g90855a04e2f391f4@mail.gmail.com>

This will determine where the overlaps are and delete them.  You can
add some more code to determine which ones you want to delete.

> # add the 5ppm to the dataframe
> x$lower <- x$Mass * (1 - 5e-6)
> x$upper <- x$Mass * (1 + 5e-6)
> # create a matrix for determining overlap by adding 1 at the lower value of a row
> # and substracting 1 at the upper value.
> overlap <- rbind(
+     cbind(index=seq(nrow(x)), value=x$lower, oper=1),
+     cbind(index=seq(nrow(x)), value=x$upper, oper=-1))
> # sort in 'value' order to determine overlap
> overlap[] <- overlap[order(overlap[,'value'], overlap[, 'oper']),]
> # 'qsize should be 0/1 if no overlap
> overlap <- cbind(overlap, qsize=cumsum(overlap[, 'oper']))
> # find the qsize > 1 indicating overlap and use the index of that one and the one
> # after as the ones to delete.  You could add code to determine which one to keep
> o.index <- which(overlap[,'qsize'] > 1)
> # determine the indices to delete
> i.delete <- unique(c(overlap[o.index,'index'], overlap[o.index+1, 'index']))
> # create the new matrix with overlaps deleted
> new.x <- x[-i.delete,]
>
>
>
> head(new.x,10)
       Mass Intensity    lower    upper
1  304.9117 35595.780 304.9102 304.9132
2  305.1726 18760.413 305.1711 305.1741
3  311.0636 24047.307 311.0620 311.0652
4  312.9303 12886.216 312.9287 312.9319
9  316.9118  5908.166 316.9102 316.9134
13 318.0114 37929.855 318.0098 318.0130
14 318.9274 27883.295 318.9258 318.9290
15 318.9889  4496.716 318.9873 318.9905
16 321.2784  3893.165 321.2768 321.2800
17 326.1166 23745.851 326.1150 326.1182


On 7/14/07, Johannes Graumann <johannes_graumann at web.de> wrote:
> John Kane wrote:
> Thanks for your time.
>
> Please find a small example below - the real data is MUCH bigger.
> If you look at rows 5 and 6 of this and calculate the mass precision window
> I have to deal with (5 ppm), you'll find the following:
>
> Row     Lower 5ppm      Mass            Higher 5ppm     Intensity
> 5       312.9419        312.9435        312.9451        20236.181
> 6       312.9422        312.9438        312.9454        14404.502
>
> The precision windows here obviously overlap and I need to get rid of one of
> them, which in this case should be row6, since it has the lower intensity
> associated with it.
>
> For now I resort to doing an intensity sort and descending into the list
> populate a fresh data.frame with entries that do not have any overlap,
> skipping those that do. If somebody has any more sound ideas, I'd
> appreciate to hear about them.
>
> Thanks, Joh
>
> Mass    Intensity
> 304.9117 35595.780
> 305.1726 18760.413
> 311.0636 24047.307
> 312.9303 12886.216
> 312.9435 20236.181
> 312.9438 14404.502
> 313.1763 61033.830
> 313.1766 50788.418
> 316.9118 5908.166
> 317.2805 14084.841
> 317.2833 25603.689
> 317.2837 22866.578
> 318.0114 37929.855
> 318.9274 27883.295
> 318.9889 4496.716
> 321.2784 3893.165
> 326.1166 23745.851
> 327.2894 5318.226
> 328.8852 60934.030
> 329.1517 31985.486
> 331.0426 14883.231
> 332.0268 55126.078
> 332.2798 47364.519
> 333.2813 11423.807
> 337.1990 5330.360
> 339.2144 38450.804
> 339.2867 4065.709
> 340.9561 54101.844
> 340.9770 28172.160
> 345.0583 17945.025
> 345.0583 17877.900
> 347.1742 7359.428
> 347.2407 204792.999
> 353.2302 87864.153
> 353.2302 129691.696
> 363.0161 20453.771
> 363.0943 19481.234
> 363.2142 9238.244
> 363.2315 23323.527
> 363.2533 20039.607
> 363.2534 22068.718
> 364.8918 16857.488
> 364.9368 9527.642
> 366.9029 18174.233
> 373.2197 7730.009
> 385.1147 27907.070
> 385.1148 19383.655
> 393.2913 11860.719
> 396.9074 10793.823
> 400.8792 10750.249
> 402.8729 12411.966
> 407.2771 11270.566
> 442.8689 18101.972
> 442.8697 10671.199
> 447.3470 35927.046
> 449.2347 6959.247
> 456.9339 50402.820
> 461.1670 8636.998
> 461.1670 8151.706
> 473.2985 13782.291
> 490.9224 18510.760
>
> > I think we need a bit more information and perhaps a
> > small example data set to see what you want.
> >
> > I am not familiar with term mass window. Is this a
> > confidence interval around the mass value?
> >
> >
> > --- Johannes Graumann <johannes_graumann at web.de>
> > wrote:
> >
> >> Dear All,
> >>
> >> I have a data frame with the columns "Mass" and
> >> "Intensity" (this is mass
> >> spectrometry stuff). Each of the mass values gives
> >> rise to a mass window of
> >> 5 ppm around the individual mass (from mass -
> >> mass/1E6*5 to mass +
> >> mass/1E5*5). I need to filter the array such that in
> >> case these mass
> >> windows overlap I retain the mass/intensity pair
> >> with the highest
> >> intensity.
> >> I apologize for this question, but I have no formal
> >> IT education and would
> >> value any nudges toward favorable algorithmic
> >> solutions highly.
> >>
> >> Thanks for any help,
> >>
> >> Joh
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained,
> >> reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From realityrandom at gmail.com  Sun Jul 15 09:39:49 2007
From: realityrandom at gmail.com (Yuchen Luo)
Date: Sun, 15 Jul 2007 00:39:49 -0700
Subject: [R] Looping through a series of (csv) files
Message-ID: <548b8d440707150039q51a2adebo92a5debeefd01006@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070715/9de1cf0a/attachment.pl 

From gchappi at gmail.com  Sun Jul 15 09:58:12 2007
From: gchappi at gmail.com (Hans-Peter)
Date: Sun, 15 Jul 2007 10:58:12 +0300
Subject: [R] Send SMS out of R?
In-Reply-To: <000401c7c64e$6a87ec70$6502a8c0@laptop>
References: <000401c7c64e$6a87ec70$6502a8c0@laptop>
Message-ID: <47fce0650707150058k7d0f9863p5f150e7222ac3aaf@mail.gmail.com>

> Now I want to send an SMS out of R! Any idea how it could work? Could I send
> an eMail to a mobile phone number?

There are "Email to SMS" services you can use. Google gives plenty of
them (also free ones (which I wouldn't use myself...)).

-- 
Regards,
Hans-Peter


From h.wickham at gmail.com  Sun Jul 15 11:02:22 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 15 Jul 2007 11:02:22 +0200
Subject: [R] Send SMS out of R?
In-Reply-To: <000401c7c64e$6a87ec70$6502a8c0@laptop>
References: <000401c7c64e$6a87ec70$6502a8c0@laptop>
Message-ID: <f8e6ff050707150202j7d3e382qa73387546015bf6c@mail.gmail.com>

On 7/14/07, Thomas Schwander <thomas_schwander at web.de> wrote:
> Hi everyone,
>
>
>
> Now I read the posting guidelines again; COMPLETELY! ;-)
>
>
>
> I use Windows XP Professional, R 2.5.1 and I have Blat to send eMails out of
> R. Works perfect! Thank you for your help!
>
>
>
> Now I want to send an SMS out of R! Any idea how it could work? Could I send
> an eMail to a mobile phone number?

This might be a good place to start:
http://en.wikipedia.org/wiki/SMS_gateways

Hadley


From ccleland at optonline.net  Sun Jul 15 12:11:18 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 15 Jul 2007 06:11:18 -0400
Subject: [R] Looping through a series of (csv) files
In-Reply-To: <548b8d440707150039q51a2adebo92a5debeefd01006@mail.gmail.com>
References: <548b8d440707150039q51a2adebo92a5debeefd01006@mail.gmail.com>
Message-ID: <4699F2C6.30507@optonline.net>

Yuchen Luo wrote:
> Dear Colleagues.
> 
> This should be a very common operation and I believe there should be a nice
> way in R to handle it.  I couldn't find it in the manual or by searching
> online. I am wondering if I could ask for some help in this community.
> 
> I have 48 csv files; each stores the data for a specific month. The 48
> corresponding months are consecutively from January, 2001 to December, 2004.
> I name the files A200101, A200102,?.., A200112, A200201, ??,etc.
> 
> I want to process file A2000101 and store the result to a new file named
> B200101, process file A200102 and store the result to a new file named
> B200102? ?etc.
> 
> I do not want to manually change a little bit of the code to read a
> different file and write to a different file every time. I want the program
> to be able to loop through all the files.
> 
> My question is, how to loop through the 48 files?
> 
> Your help will be highly appreciated!
> 
> 
> Best Wishes
> 
> Yuchen Luo

  This will read each of the 48 csv files, add 50 to the values in each
file, and then write each resulting data frame to its own new file:

for(i in 1:48){
 tempdf <- read.csv(paste("c:/myfolder/raw", i, ".csv", sep=""))
 tempdf.new <- tempdf + 50
 write.table(tempdf.new,
             file=paste("c:/myfolder/plus50-", i, ".csv", sep=""),
             sep=",", row.names=FALSE)
}

rm(list=c("tempdf", "tempdf.new", "i"))

  Of course, you can do something more useful than adding 50.

> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From aiisreal at post.tau.ac.il  Sun Jul 15 12:13:42 2007
From: aiisreal at post.tau.ac.il (Alfred Inselberg)
Date: Sun, 15 Jul 2007 13:13:42 +0300 (IDT)
Subject: [R] Mailing List
Message-ID: <Pine.LNX.4.61.0707151312150.13033@soul.cs.tau.ac.il>


Please add me to your mailing list. Thank you

Alfred Inselberg

-- 
Alfred Inselberg, Professor 
School of Mathematical Sciences
Tel Aviv University
Tel Aviv 61390, Israel
Tel +972-3-640 5372
http://www.math.tau.ac.il/~aiisreal/


From patrick at pdrechsler.de  Sun Jul 15 13:43:24 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sun, 15 Jul 2007 13:43:24 +0200
Subject: [R] unixtime conversion
References: <87fy3rlivl.fsf@pdrechsler.de>
	<Pine.LNX.4.64.0707140542010.9149@gannet.stats.ox.ac.uk>
Message-ID: <87fy3p7uzn.fsf@pdrechsler.de>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Sat, 14 Jul 2007, Patrick Drechsler wrote:
>
>> is there an R function to convert unixtime to one of the R time
>> formats (using chron or POSIXct)?
>>
>> Example data:
>>
>> unixtime       year  month      day     hour
>> 1183377301     2007  7          2       13
>>
>> I would like to only use the first column.
>
> See ?as.POSIXct, especially its examples.  (Isn't that the very obvious 
> place to look?)
>
> It seems you want ISOdatetime(1970,1,1,0,0,0) + unixtime
>
> although depending on the 'unix' in 'unixtime' you might have to wrorry 
> about leap seconds (which POSIX-compliant systems ignore).

Thank you very much for your reply! Indeed, ISOdatetime was the
command I was looking for.

Regards

Patrick


From ed_deroiste at yahoo.co.uk  Sun Jul 15 13:44:23 2007
From: ed_deroiste at yahoo.co.uk (TIMMMAY)
Date: Sun, 15 Jul 2007 04:44:23 -0700 (PDT)
Subject: [R] Please Please Help me!!!
Message-ID: <11601686.post@talk.nabble.com>


I posted the message below a few days ago but I have not had any responses. I
keep thinking that there must be some easy way to answer the problem I am
just not familiar enough with regression to answer the problem myself. If
anyone can help me I would be very grateful. I need to fit a gamma curve to
a set of data. ie a scatterplot of the data indicates that the curve looks
like a truncated gamma density function and I would like to estimate the
paramaters so that I can fit a curve to the data points. Its not MLE
paramater estimation just a curve fitting exercise. The problem again is 

I have a set transition intensities and when plotted the curve looks like a
gamma density. I want to fit a gamma density curve to these intensities. It
is just a curve fitting problem but whats causing the trouble is that I need
to use least squares minimization to calculate the parameters for the gamma
curve. How do I do this??? 

The curve will be a truncated gamma function so it will have 3 paramaters a,
b, c. I tried to do the following 

nls(lograte ~ log(c) + a*log(b) + (a-1)*log(age)+b*(age)-lgamma(a),
start=list(a=1,b=1,c=1)), where lograte, and age are my data. a,b the gamma
parameters and c the parameter we need because we are fitting a truncated
distribution. 

I also tried defining 
fn = function(p) sum((log(y)-log(dgamma(x,p[1],p[2])*p[3]))^2) 
a residual sum of squares and using nlm to minimise this and find paramaters
but this doesnt work either. Can anyone help me ?? Please :) 

Original message follows :(
Fitting a Gamma Curve   
by TIMMMAY Jul 12, 2007; 03:38pm :: Rate this Message:    (use ratings to
moderate[?])

Reply | Reply to Author | Show Only this Message 

Hi there, I hope someone can help me before I tear all my hair out. I have a
set transition intensities and when plotted the curve looks like a gamma
density. I want to fit a gamma density curve to these intensities. It is
just a curve fitting problem but whats causing the trouble is that I need to
use least squares minimization to calculate the parameters for the gamma
curve. How do I do this??? 

The curve will be a truncated gamma function so it will have 3 paramaters a,
b, c. I tried to do the following 

nls(lograte ~ log(c) + a*log(b) + (a-1)*log(age)+b*(age)-lgamma(a),
start=list(a=1,b=1,c=1)), where lograte, and age are my data. a,b the gamma
parameters and c the parameter we need because we are fitting a truncated
distribution. 

I also tried defining 
fn = function(p) sum((log(y)-log(dgamma(x,p[1],p[2])*p[3]))^2) 
a residual sum of squares and using nlm to minimise this and find paramaters
but this doesnt work either. Can anyone help me ?? Please :) 




? Return to forum

Start a free forum or mailing list archive on Nabble  Help - Powered by -
Terms of Use - Jobs at Nabble - Nabble Support  

-- 
View this message in context: http://www.nabble.com/Please-Please-Help-me%21%21%21-tf4081887.html#a11601686
Sent from the R help mailing list archive at Nabble.com.


From descall at blueyonder.co.uk  Sun Jul 15 14:41:13 2007
From: descall at blueyonder.co.uk (Des Callaghan)
Date: Sun, 15 Jul 2007 13:41:13 +0100
Subject: [R] Area selection agorithms
Message-ID: <000001c7c6dd$6e70b260$4b521720$@co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070715/7b9f2b24/attachment.pl 

From jsorkin at grecc.umaryland.edu  Sun Jul 15 14:40:25 2007
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 15 Jul 2007 08:40:25 -0400
Subject: [R] Complex surveys,
	properly computed SEs and non-parametric	analyses
Message-ID: <4699DD79020000CB00012C82@MEDICINE.umaryland.edu>

Can someone direct me to an R function that properly computes standard errors of data obtained from a complex survery design, i.e. perform alnalyses similiar to those that can be performed with SUDAAN, particularly for a non-parametric one-way ANOVA, e.g. signed rank test?
Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC,
University of Maryland School of Medicine Claude D. Pepper OAIC,
University of Maryland Clinical Nutrition Research Unit, and
Baltimore VA Center Stroke of Excellence

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)
jsorkin at grecc.umaryland.edu
Confidentiality Statement:
This email message, including any attachments, is for the so...{{dropped}}


From tobias.verbeke at gmail.com  Sun Jul 15 15:04:13 2007
From: tobias.verbeke at gmail.com (Tobias Verbeke)
Date: Sun, 15 Jul 2007 15:04:13 +0200
Subject: [R] Complex surveys,
 properly computed SEs and non-parametric analyses
In-Reply-To: <4699DD79020000CB00012C82@MEDICINE.umaryland.edu>
References: <4699DD79020000CB00012C82@MEDICINE.umaryland.edu>
Message-ID: <469A1B4D.9040100@businessdecision.com>

Dear John,

> Can someone direct me to an R function that properly computes standard errors of data obtained from 
 > a complex survery design i.e. perform alnalyses similiar to those 
that can be performed with SUDAAN,
 > articularly for a non-parametric one-way ANOVA, e.g. signed rank test?

The survey package of Thomas Lumley has very broad functionality for the 
analysis of data from complex sampling designs. Please find below the 
homepage of the package (which is available on CRAN):

http://faculty.washington.edu/tlumley/survey/

I don't think non-parametric one-way ANOVA is implemented, but
quoting

http://faculty.washington.edu/tlumley/survey/survey-wss.pdf

slide 101:

"Many features of the survey package result from requests from
unsatisfied users.

For new methods the most important information is a reference
that gives sufficient detail for implementation. A data set is nice
but not critical."

HTH,
Tobias

-- 

Tobias Verbeke - Consultant
Business & Decision Benelux
Rue de la r?volution 8
1000 Brussels - BELGIUM

+32 499 36 33 15
tobias.verbeke at businessdecision.com


From ggrothendieck at gmail.com  Sun Jul 15 15:16:21 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 15 Jul 2007 09:16:21 -0400
Subject: [R] Please Please Help me!!!
In-Reply-To: <11601686.post@talk.nabble.com>
References: <11601686.post@talk.nabble.com>
Message-ID: <971536df0707150616j7876e1c3x9a8a8a620a63abc1@mail.gmail.com>

The reason no one answered may be that you did not follow the last
line to every r-help message or read and follow the posting guide.  Its
time consuming to develop a test environment and data needed to clarify
and test the answer to a question.  By providing data and reproducible
code you reduce the amount of time it takes responders to answer thereby
making it more likely someone will.

On 7/15/07, TIMMMAY <ed_deroiste at yahoo.co.uk> wrote:
>
> I posted the message below a few days ago but I have not had any responses. I
> keep thinking that there must be some easy way to answer the problem I am
> just not familiar enough with regression to answer the problem myself. If
> anyone can help me I would be very grateful. I need to fit a gamma curve to
> a set of data. ie a scatterplot of the data indicates that the curve looks
> like a truncated gamma density function and I would like to estimate the
> paramaters so that I can fit a curve to the data points. Its not MLE
> paramater estimation just a curve fitting exercise. The problem again is
>
> I have a set transition intensities and when plotted the curve looks like a
> gamma density. I want to fit a gamma density curve to these intensities. It
> is just a curve fitting problem but whats causing the trouble is that I need
> to use least squares minimization to calculate the parameters for the gamma
> curve. How do I do this???
>
> The curve will be a truncated gamma function so it will have 3 paramaters a,
> b, c. I tried to do the following
>
> nls(lograte ~ log(c) + a*log(b) + (a-1)*log(age)+b*(age)-lgamma(a),
> start=list(a=1,b=1,c=1)), where lograte, and age are my data. a,b the gamma
> parameters and c the parameter we need because we are fitting a truncated
> distribution.
>
> I also tried defining
> fn = function(p) sum((log(y)-log(dgamma(x,p[1],p[2])*p[3]))^2)
> a residual sum of squares and using nlm to minimise this and find paramaters
> but this doesnt work either. Can anyone help me ?? Please :)
>
> Original message follows :(
> Fitting a Gamma Curve
> by TIMMMAY Jul 12, 2007; 03:38pm :: Rate this Message:    (use ratings to
> moderate[?])
>
> Reply | Reply to Author | Show Only this Message
>
> Hi there, I hope someone can help me before I tear all my hair out. I have a
> set transition intensities and when plotted the curve looks like a gamma
> density. I want to fit a gamma density curve to these intensities. It is
> just a curve fitting problem but whats causing the trouble is that I need to
> use least squares minimization to calculate the parameters for the gamma
> curve. How do I do this???
>
> The curve will be a truncated gamma function so it will have 3 paramaters a,
> b, c. I tried to do the following
>
> nls(lograte ~ log(c) + a*log(b) + (a-1)*log(age)+b*(age)-lgamma(a),
> start=list(a=1,b=1,c=1)), where lograte, and age are my data. a,b the gamma
> parameters and c the parameter we need because we are fitting a truncated
> distribution.
>
> I also tried defining
> fn = function(p) sum((log(y)-log(dgamma(x,p[1],p[2])*p[3]))^2)
> a residual sum of squares and using nlm to minimise this and find paramaters
> but this doesnt work either. Can anyone help me ?? Please :)
>
>
>
>
> ? Return to forum
>
> Start a free forum or mailing list archive on Nabble  Help - Powered by -
> Terms of Use - Jobs at Nabble - Nabble Support
>
> --
> View this message in context: http://www.nabble.com/Please-Please-Help-me%21%21%21-tf4081887.html#a11601686
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From johannes_graumann at web.de  Sun Jul 15 16:00:41 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Sun, 15 Jul 2007 16:00:41 +0200
Subject: [R] Pairlist of pairlsit assembly howto
Message-ID: <f7d9a9$1sj$1@sea.gmane.org>

Hy guys,

I'm trying something like this

minbins <- list()
for (minute in sequence(3)) {
        minbins[minute] <- list(data="a",variable="b")
}

And it doesn't work ...
Warning messages:
1: number of items to replace is not a multiple of replacement length in:
minbins[minute] <- list(data = "a", variable = "b") 
2: number of items to replace is not a multiple of replacement length in:
minbins[minute] <- list(data = "a", variable = "b") 
3: number of items to replace is not a multiple of replacement length in:
minbins[minute] <- list(data = "a", variable = "b")

What am I doing wrong and how to do this properly?

Thanks, Joh


From attenka at utu.fi  Sun Jul 15 16:06:45 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 15 Jul 2007 17:06:45 +0300
Subject: [R] Break during the recursion?
Message-ID: <f71aec7f1310c.469a5425@utu.fi>

Hi,

Is it possible to break using if-condition during the recursive function?

Here is a function which almost works. It is for inorder-tree-walk. 

iotw<-function(v,i,Stack,Indexes) # input: a vector and the first index (1), Stack=c(), Indexes=c().
{
	print(Indexes)
	# if (sum(i)==0) break # Doesn't work...
	
	if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
		{Stack=c(i,Stack); i=2*i; iotw(v,i,Stack,Indexes)}
	Indexes=c(Indexes,Stack[1])
	Stack=pop.stack(Stack)$vector
	Indexes=c(Indexes,Stack[1])
	i=2*Stack[1]+1
	Stack=pop.stack(Stack)$vector
	iotw(v,i,Stack,Indexes)
}


> v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",3,"X",2)
> Stack=c()
> Indexes=c()

> iotw(v,1,Stack,Indexes)
NULL
NULL
NULL
NULL
NULL
[1] 8 4
[1] 8 4
[1] 8 4 9 2
[1] 8 4 9 2
[1] 8 4 9 2
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
Error in if (is.na(v[i]) == FALSE & is.null(unlist(v[i])) == FALSE) { : 
	argument is of length zero

Regards,

Atte Tenkanen
University of Turku, Finland


From murdoch at stats.uwo.ca  Sun Jul 15 16:08:12 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 15 Jul 2007 10:08:12 -0400
Subject: [R] Pairlist of pairlsit assembly howto
In-Reply-To: <f7d9a9$1sj$1@sea.gmane.org>
References: <f7d9a9$1sj$1@sea.gmane.org>
Message-ID: <469A2A4C.9080604@stats.uwo.ca>


(This isn't important to your question, but those aren't pairlists. 
Pairlists are rarely used in R code, except implicitly in the way R 
stores parsed code.)

On 15/07/2007 10:00 AM, Johannes Graumann wrote:
> Hy guys,
> 
> I'm trying something like this
> 
> minbins <- list()
> for (minute in sequence(3)) {
>         minbins[minute] <- list(data="a",variable="b")
> }

You want to use

minbins[[minute]] <- list(data="a",variable="b")

The difference between [[ ]] and [ ] is that the former works on the 
element, the latter works on a subset.  So your version tried to change 
a subset of length 1 into a subset of length 2, which generates the 
warnings.  You want to assign a list of length 2 as an element of minbins.

Duncan Murdoch
> 
> And it doesn't work ...
> Warning messages:
> 1: number of items to replace is not a multiple of replacement length in:
> minbins[minute] <- list(data = "a", variable = "b") 
> 2: number of items to replace is not a multiple of replacement length in:
> minbins[minute] <- list(data = "a", variable = "b") 
> 3: number of items to replace is not a multiple of replacement length in:
> minbins[minute] <- list(data = "a", variable = "b")
> 
> What am I doing wrong and how to do this properly?
> 
> Thanks, Joh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From attenka at utu.fi  Sun Jul 15 16:13:54 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 15 Jul 2007 17:13:54 +0300
Subject: [R] Break during the recursion?
In-Reply-To: <f71aec7f1310c.469a5425@utu.fi>
References: <f71aec7f1310c.469a5425@utu.fi>
Message-ID: <f71ac11a13c35.469a55d2@utu.fi>

Oh. I forgot one extra-function:

pop.stack<-function(v){
	if(length(v)==0){x=NA}
	if(length(v)==1){x=v[1]; v=c()}
	if(length(v)>1){x=v[1]; v=v[2:length(v)]}
	return(list(vector=v,x=x))
}

Atte

> Hi,
> 
> Is it possible to break using if-condition during the recursive 
> function?
> Here is a function which almost works. It is for inorder-tree-walk. 
> 
> iotw<-function(v,i,Stack,Indexes) # input: a vector and the first 
> index (1), Stack=c(), Indexes=c().
> {
> 	print(Indexes)
> 	# if (sum(i)==0) break # Doesn't work...
> 	
> 	if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
>        	{Stack=c(i,Stack); i=2*i; iotw(v,i,Stack,Indexes)}
> 	Indexes=c(Indexes,Stack[1])
> 	Stack=pop.stack(Stack)$vector
> 	Indexes=c(Indexes,Stack[1])
> 	i=2*Stack[1]+1
> 	Stack=pop.stack(Stack)$vector
> 	iotw(v,i,Stack,Indexes)
> }
> 
> 
> > v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",3,"X",2)
> > Stack=c()
> > Indexes=c()
> 
> > iotw(v,1,Stack,Indexes)
> NULL
> NULL
> NULL
> NULL
> NULL
> [1] 8 4
> [1] 8 4
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> Error in if (is.na(v[i]) == FALSE & is.null(unlist(v[i])) == FALSE) 
> { : 
> 	argument is of length zero
> 
> Regards,
> 
> Atte Tenkanen
> University of Turku, Finland
>


From murdoch at stats.uwo.ca  Sun Jul 15 16:13:56 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 15 Jul 2007 10:13:56 -0400
Subject: [R] Break during the recursion?
In-Reply-To: <f71aec7f1310c.469a5425@utu.fi>
References: <f71aec7f1310c.469a5425@utu.fi>
Message-ID: <469A2BA4.9030301@stats.uwo.ca>

On 15/07/2007 10:06 AM, Atte Tenkanen wrote:
> Hi,
> 
> Is it possible to break using if-condition during the recursive function?

You can do

  if (condition) return(value)

> 
> Here is a function which almost works. It is for inorder-tree-walk. 
> 
> iotw<-function(v,i,Stack,Indexes) # input: a vector and the first index (1), Stack=c(), Indexes=c().
> {
> 	print(Indexes)
> 	# if (sum(i)==0) break # Doesn't work...

	if (sum(i)==0) return(NULL)

should work.

Duncan Murdoch

> 	
> 	if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
> 		{Stack=c(i,Stack); i=2*i; iotw(v,i,Stack,Indexes)}
> 	Indexes=c(Indexes,Stack[1])
> 	Stack=pop.stack(Stack)$vector
> 	Indexes=c(Indexes,Stack[1])
> 	i=2*Stack[1]+1
> 	Stack=pop.stack(Stack)$vector
> 	iotw(v,i,Stack,Indexes)
> }
> 
> 
>> v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",3,"X",2)
>> Stack=c()
>> Indexes=c()
> 
>> iotw(v,1,Stack,Indexes)
> NULL
> NULL
> NULL
> NULL
> NULL
> [1] 8 4
> [1] 8 4
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> Error in if (is.na(v[i]) == FALSE & is.null(unlist(v[i])) == FALSE) { : 
> 	argument is of length zero
> 
> Regards,
> 
> Atte Tenkanen
> University of Turku, Finland
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From attenka at utu.fi  Sun Jul 15 16:33:13 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 15 Jul 2007 17:33:13 +0300
Subject: [R] Break during the recursion?
In-Reply-To: <469A2BA4.9030301@stats.uwo.ca>
References: <f71aec7f1310c.469a5425@utu.fi> <469A2BA4.9030301@stats.uwo.ca>
Message-ID: <f6ecfde4164b3.469a5a59@utu.fi>

> On 15/07/2007 10:06 AM, Atte Tenkanen wrote:
> > Hi,
> > 
> > Is it possible to break using if-condition during the recursive 
> function?
> You can do
> 
>  if (condition) return(value)
> 
> > 
> > Here is a function which almost works. It is for inorder-tree-
> walk. 
> > 
> > iotw<-function(v,i,Stack,Indexes) # input: a vector and the first 
> index (1), Stack=c(), Indexes=c().
> > {
> > 	print(Indexes)
> > 	# if (sum(i)==0) break # Doesn't work...
> 
> 	if (sum(i)==0) return(NULL)
> 
> should work.
> 
> Duncan Murdoch

Hmm - - - I'd like to save the Indexes-vector (in the example c(8,4,9,2,10,5,11,1,3)) and stop, when it is ready. 

The results are enclosed to the end.

Atte

> 
> > 	
> > 	if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
> >         	{Stack=c(i,Stack); i=2*i; iotw(v,i,Stack,Indexes)}
> > 	Indexes=c(Indexes,Stack[1])
> > 	Stack=pop.stack(Stack)$vector
> > 	Indexes=c(Indexes,Stack[1])
> > 	i=2*Stack[1]+1
> > 	Stack=pop.stack(Stack)$vector
> > 	iotw(v,i,Stack,Indexes)
> > }
> > 
> > 
> >> v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",3,"X",2)
> >> Stack=c()
> >> Indexes=c()
> > 
> >> iotw(v,1,Stack,Indexes)
> > NULL
> > NULL
> > NULL
> > NULL
> > NULL
> > [1] 8 4
> > [1] 8 4
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > Error in if (is.na(v[i]) == FALSE & is.null(unlist(v[i])) == 
> FALSE) { : 
> > 	argument is of length zero
> > 
> > Regards,
> > 
> > Atte Tenkanen
> > University of Turku, Finland
> > 


> iotw(v,1,Stack,Indexes)
NULL
NULL
NULL
NULL
NULL
[1] 8 4
[1] 8 4
[1] 8 4 9 2
[1] 8 4 9 2
[1] 8 4 9 2
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1] 8 4 9 2 5 1
[1] 8 4 9 2 5 1
[1] 8 4 9 2 5 1 3
[1] 8 4 9 2 5 1 3
[1] 8 4 9 2
[1] 8 4 9 2
[1] 8 4 9 2
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1] 8 4 9 2 5 1
[1] 8 4 9 2 5 1
[1] 8 4 9 2 5 1 3
[1] 8 4 9 2 5 1 3
[1] 8 4
[1] 8 4
[1] 8 4 9 2
[1] 8 4 9 2
[1] 8 4 9 2
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1] 8 4 9 2 5 1
[1] 8 4 9 2 5 1
[1] 8 4 9 2 5 1 3
[1] 8 4 9 2 5 1 3
[1] 8 4 9 2
[1] 8 4 9 2
[1] 8 4 9 2
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1
[1]  8  4  9  2 10  5 11  1  3
[1]  8  4  9  2 10  5 11  1  3
[1] 8 4 9 2 5 1
[1] 8 4 9 2 5 1
[1] 8 4 9 2 5 1 3
[1] 8 4 9 2 5 1 3
[1] 4 2
[1] 4 2
[1] 4 2
[1]  4  2 10  5
[1]  4  2 10  5
[1]  4  2 10  5 11  1
[1]  4  2 10  5 11  1
[1]  4  2 10  5 11  1  3
[1]  4  2 10  5 11  1  3
[1]  4  2 10  5 11  1
[1]  4  2 10  5 11  1
[1]  4  2 10  5 11  1  3
[1]  4  2 10  5 11  1  3
[1]  4  2 10  5
[1]  4  2 10  5
[1]  4  2 10  5 11  1
[1]  4  2 10  5 11  1
[1]  4  2 10  5 11  1  3
[1]  4  2 10  5 11  1  3
[1]  4  2 10  5 11  1
[1]  4  2 10  5 11  1
[1]  4  2 10  5 11  1  3
[1]  4  2 10  5 11  1  3
[1] 4 2 5 1
[1] 4 2 5 1
[1] 4 2 5 1 3
[1] 4 2 5 1 3
[1] 2 1
[1] 2 1
[1] 2 1 3
[1] 2 1 3
[1] 1
[1] 1


From murdoch at stats.uwo.ca  Sun Jul 15 17:17:45 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 15 Jul 2007 11:17:45 -0400
Subject: [R] Break during the recursion?
In-Reply-To: <f6ecfde4164b3.469a5a59@utu.fi>
References: <f71aec7f1310c.469a5425@utu.fi> <469A2BA4.9030301@stats.uwo.ca>
	<f6ecfde4164b3.469a5a59@utu.fi>
Message-ID: <469A3A99.4050308@stats.uwo.ca>

On 15/07/2007 10:33 AM, Atte Tenkanen wrote:
>> On 15/07/2007 10:06 AM, Atte Tenkanen wrote:
>>> Hi,
>>>
>>> Is it possible to break using if-condition during the recursive 
>> function?
>> You can do
>>
>>  if (condition) return(value)
>>
>>> Here is a function which almost works. It is for inorder-tree-
>> walk. 
>>> iotw<-function(v,i,Stack,Indexes) # input: a vector and the first 
>> index (1), Stack=c(), Indexes=c().
>>> {
>>> 	print(Indexes)
>>> 	# if (sum(i)==0) break # Doesn't work...
>> 	if (sum(i)==0) return(NULL)
>>
>> should work.
>>
>> Duncan Murdoch
> 
> Hmm - - - I'd like to save the Indexes-vector (in the example c(8,4,9,2,10,5,11,1,3)) and stop, when it is ready. 

This seems more like a problem with the design of your function than a 
question about R.  I can't really help you with that, because your 
description of the problem doesn't make sense to me.  What does it mean 
to do an inorder tree walk on something that isn't a tree?

Duncan Murdoch


> 
> The results are enclosed to the end.
> 
> Atte
> 
>>> 	
>>> 	if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
>>>         	{Stack=c(i,Stack); i=2*i; iotw(v,i,Stack,Indexes)}
>>> 	Indexes=c(Indexes,Stack[1])
>>> 	Stack=pop.stack(Stack)$vector
>>> 	Indexes=c(Indexes,Stack[1])
>>> 	i=2*Stack[1]+1
>>> 	Stack=pop.stack(Stack)$vector
>>> 	iotw(v,i,Stack,Indexes)
>>> }
>>>
>>>
>>>> v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",3,"X",2)
>>>> Stack=c()
>>>> Indexes=c()
>>>> iotw(v,1,Stack,Indexes)
>>> NULL
>>> NULL
>>> NULL
>>> NULL
>>> NULL
>>> [1] 8 4
>>> [1] 8 4
>>> [1] 8 4 9 2
>>> [1] 8 4 9 2
>>> [1] 8 4 9 2
>>> [1]  8  4  9  2 10  5
>>> [1]  8  4  9  2 10  5
>>> [1]  8  4  9  2 10  5 11  1
>>> [1]  8  4  9  2 10  5 11  1
>>> [1]  8  4  9  2 10  5 11  1  3
>>> Error in if (is.na(v[i]) == FALSE & is.null(unlist(v[i])) == 
>> FALSE) { : 
>>> 	argument is of length zero
>>>
>>> Regards,
>>>
>>> Atte Tenkanen
>>> University of Turku, Finland
>>>
> 
> 
>> iotw(v,1,Stack,Indexes)
> NULL
> NULL
> NULL
> NULL
> NULL
> [1] 8 4
> [1] 8 4
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1] 8 4 9 2 5 1
> [1] 8 4 9 2 5 1
> [1] 8 4 9 2 5 1 3
> [1] 8 4 9 2 5 1 3
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1] 8 4 9 2 5 1
> [1] 8 4 9 2 5 1
> [1] 8 4 9 2 5 1 3
> [1] 8 4 9 2 5 1 3
> [1] 8 4
> [1] 8 4
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1] 8 4 9 2 5 1
> [1] 8 4 9 2 5 1
> [1] 8 4 9 2 5 1 3
> [1] 8 4 9 2 5 1 3
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1] 8 4 9 2
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1
> [1]  8  4  9  2 10  5 11  1  3
> [1]  8  4  9  2 10  5 11  1  3
> [1] 8 4 9 2 5 1
> [1] 8 4 9 2 5 1
> [1] 8 4 9 2 5 1 3
> [1] 8 4 9 2 5 1 3
> [1] 4 2
> [1] 4 2
> [1] 4 2
> [1]  4  2 10  5
> [1]  4  2 10  5
> [1]  4  2 10  5 11  1
> [1]  4  2 10  5 11  1
> [1]  4  2 10  5 11  1  3
> [1]  4  2 10  5 11  1  3
> [1]  4  2 10  5 11  1
> [1]  4  2 10  5 11  1
> [1]  4  2 10  5 11  1  3
> [1]  4  2 10  5 11  1  3
> [1]  4  2 10  5
> [1]  4  2 10  5
> [1]  4  2 10  5 11  1
> [1]  4  2 10  5 11  1
> [1]  4  2 10  5 11  1  3
> [1]  4  2 10  5 11  1  3
> [1]  4  2 10  5 11  1
> [1]  4  2 10  5 11  1
> [1]  4  2 10  5 11  1  3
> [1]  4  2 10  5 11  1  3
> [1] 4 2 5 1
> [1] 4 2 5 1
> [1] 4 2 5 1 3
> [1] 4 2 5 1 3
> [1] 2 1
> [1] 2 1
> [1] 2 1 3
> [1] 2 1 3
> [1] 1
> [1] 1


From attenka at utu.fi  Sun Jul 15 17:36:56 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 15 Jul 2007 18:36:56 +0300
Subject: [R] Break during the recursion?
In-Reply-To: <469A3A99.4050308@stats.uwo.ca>
References: <f71aec7f1310c.469a5425@utu.fi> <469A2BA4.9030301@stats.uwo.ca>
	<f6ecfde4164b3.469a5a59@utu.fi> <469A3A99.4050308@stats.uwo.ca>
Message-ID: <f6eab7df1679d.469a6948@utu.fi>

> On 15/07/2007 10:33 AM, Atte Tenkanen wrote:
> >> On 15/07/2007 10:06 AM, Atte Tenkanen wrote:
> >>> Hi,
> >>>
> >>> Is it possible to break using if-condition during the recursive 
> >> function?
> >> You can do
> >>
> >>  if (condition) return(value)
> >>
> >>> Here is a function which almost works. It is for inorder-tree-
> >> walk. 
> >>> iotw<-function(v,i,Stack,Indexes) # input: a vector and the 
> first 
> >> index (1), Stack=c(), Indexes=c().
> >>> {
> >>> 	print(Indexes)
> >>> 	# if (sum(i)==0) break # Doesn't work...
> >> 	if (sum(i)==0) return(NULL)
> >>
> >> should work.
> >>
> >> Duncan Murdoch
> > 
> > Hmm - - - I'd like to save the Indexes-vector (in the example 
> c(8,4,9,2,10,5,11,1,3)) and stop, when it is ready. 
> 
> This seems more like a problem with the design of your function 
> than a 
> question about R.  I can't really help you with that, because your 
> description of the problem doesn't make sense to me.  What does it 
> mean 
> to do an inorder tree walk on something that isn't a tree?
> 
> Duncan Murdoch

The symbols in vector v have been originally derived from "tree". See

http://users.utu.fi/attenka/Tree.jpg

But perhaps there's another way to do this, for instance by using loops and if-conditions? 

Atte



> 
> 
> > 
> > The results are enclosed to the end.
> > 
> > Atte
> > 
> >>> 	
> >>> 	if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
> >>>         	{Stack=c(i,Stack); i=2*i; iotw(v,i,Stack,Indexes)}
> >>> 	Indexes=c(Indexes,Stack[1])
> >>> 	Stack=pop.stack(Stack)$vector
> >>> 	Indexes=c(Indexes,Stack[1])
> >>> 	i=2*Stack[1]+1
> >>> 	Stack=pop.stack(Stack)$vector
> >>> 	iotw(v,i,Stack,Indexes)
> >>> }
> >>>
> >>>
> >>>> v=c(`-`,`+`,1,`^`,`^`,NA,NA,"X",3,"X",2)
> >>>> Stack=c()
> >>>> Indexes=c()
> >>>> iotw(v,1,Stack,Indexes)
> >>> NULL
> >>> NULL
> >>> NULL
> >>> NULL
> >>> NULL
> >>> [1] 8 4
> >>> [1] 8 4
> >>> [1] 8 4 9 2
> >>> [1] 8 4 9 2
> >>> [1] 8 4 9 2
> >>> [1]  8  4  9  2 10  5
> >>> [1]  8  4  9  2 10  5
> >>> [1]  8  4  9  2 10  5 11  1
> >>> [1]  8  4  9  2 10  5 11  1
> >>> [1]  8  4  9  2 10  5 11  1  3
> >>> Error in if (is.na(v[i]) == FALSE & is.null(unlist(v[i])) == 
> >> FALSE) { : 
> >>> 	argument is of length zero
> >>>
> >>> Regards,
> >>>
> >>> Atte Tenkanen
> >>> University of Turku, Finland
> >>>
> > 
> > 
> >> iotw(v,1,Stack,Indexes)
> > NULL
> > NULL
> > NULL
> > NULL
> > NULL
> > [1] 8 4
> > [1] 8 4
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1] 8 4 9 2 5 1
> > [1] 8 4 9 2 5 1
> > [1] 8 4 9 2 5 1 3
> > [1] 8 4 9 2 5 1 3
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1] 8 4 9 2 5 1
> > [1] 8 4 9 2 5 1
> > [1] 8 4 9 2 5 1 3
> > [1] 8 4 9 2 5 1 3
> > [1] 8 4
> > [1] 8 4
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1] 8 4 9 2 5 1
> > [1] 8 4 9 2 5 1
> > [1] 8 4 9 2 5 1 3
> > [1] 8 4 9 2 5 1 3
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1] 8 4 9 2
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1
> > [1]  8  4  9  2 10  5 11  1  3
> > [1]  8  4  9  2 10  5 11  1  3
> > [1] 8 4 9 2 5 1
> > [1] 8 4 9 2 5 1
> > [1] 8 4 9 2 5 1 3
> > [1] 8 4 9 2 5 1 3
> > [1] 4 2
> > [1] 4 2
> > [1] 4 2
> > [1]  4  2 10  5
> > [1]  4  2 10  5
> > [1]  4  2 10  5 11  1
> > [1]  4  2 10  5 11  1
> > [1]  4  2 10  5 11  1  3
> > [1]  4  2 10  5 11  1  3
> > [1]  4  2 10  5 11  1
> > [1]  4  2 10  5 11  1
> > [1]  4  2 10  5 11  1  3
> > [1]  4  2 10  5 11  1  3
> > [1]  4  2 10  5
> > [1]  4  2 10  5
> > [1]  4  2 10  5 11  1
> > [1]  4  2 10  5 11  1
> > [1]  4  2 10  5 11  1  3
> > [1]  4  2 10  5 11  1  3
> > [1]  4  2 10  5 11  1
> > [1]  4  2 10  5 11  1
> > [1]  4  2 10  5 11  1  3
> > [1]  4  2 10  5 11  1  3
> > [1] 4 2 5 1
> > [1] 4 2 5 1
> > [1] 4 2 5 1 3
> > [1] 4 2 5 1 3
> > [1] 2 1
> > [1] 2 1
> > [1] 2 1 3
> > [1] 2 1 3
> > [1] 1
> > [1] 1
> 
>


From murdoch at stats.uwo.ca  Sun Jul 15 17:41:25 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 15 Jul 2007 11:41:25 -0400
Subject: [R] Break during the recursion?
In-Reply-To: <f6eab7df1679d.469a6948@utu.fi>
References: <f71aec7f1310c.469a5425@utu.fi>
	<469A2BA4.9030301@stats.uwo.ca>	<f6ecfde4164b3.469a5a59@utu.fi>
	<469A3A99.4050308@stats.uwo.ca> <f6eab7df1679d.469a6948@utu.fi>
Message-ID: <469A4025.7040207@stats.uwo.ca>

On 15/07/2007 11:36 AM, Atte Tenkanen wrote:
>> On 15/07/2007 10:33 AM, Atte Tenkanen wrote:
>>>> On 15/07/2007 10:06 AM, Atte Tenkanen wrote:
>>>>> Hi,
>>>>>
>>>>> Is it possible to break using if-condition during the recursive 
>>>> function?
>>>> You can do
>>>>
>>>>  if (condition) return(value)
>>>>
>>>>> Here is a function which almost works. It is for inorder-tree-
>>>> walk. 
>>>>> iotw<-function(v,i,Stack,Indexes) # input: a vector and the 
>> first 
>>>> index (1), Stack=c(), Indexes=c().
>>>>> {
>>>>> 	print(Indexes)
>>>>> 	# if (sum(i)==0) break # Doesn't work...
>>>> 	if (sum(i)==0) return(NULL)
>>>>
>>>> should work.
>>>>
>>>> Duncan Murdoch
>>> Hmm - - - I'd like to save the Indexes-vector (in the example 
>> c(8,4,9,2,10,5,11,1,3)) and stop, when it is ready. 
>>
>> This seems more like a problem with the design of your function 
>> than a 
>> question about R.  I can't really help you with that, because your 
>> description of the problem doesn't make sense to me.  What does it 
>> mean 
>> to do an inorder tree walk on something that isn't a tree?
>>
>> Duncan Murdoch
> 
> The symbols in vector v have been originally derived from "tree". See
> 
> http://users.utu.fi/attenka/Tree.jpg
> 
> But perhaps there's another way to do this, for instance by using loops and if-conditions? 

Or perhaps by doing the tree walk on the tree, before you collapse it 
into a vector.

Duncan Murdoch


From maja.schroeter at gmx.de  Sun Jul 15 17:58:44 2007
From: maja.schroeter at gmx.de (=?iso-8859-1?Q?=22Maja_Schr=F6ter=22?=)
Date: Sun, 15 Jul 2007 17:58:44 +0200
Subject: [R] Text with differents colors in a plot / Detecting if text exists
Message-ID: <20070715155844.200450@gmx.net>

Hi everybody,

I want to write some text in a plot.

That's simple I know. But I want to make use of different colors.

Eg. text(x,y,paste("Sunderland","high")).

Then Sunderland should be black and "high" red.

Has anyone an idea?

By the way. I'm looking for a function or something similar, that can check whether there is text in some regions on the plot. 
Because I don't want to overwrite an existing text with a new one.

Thank you so much for your help. I appologize in advance if my question is very stupid but I have really no idea.

Yours,

Maja
--


From h.wickham at gmail.com  Sun Jul 15 17:58:44 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 15 Jul 2007 17:58:44 +0200
Subject: [R] Break during the recursion?
In-Reply-To: <469A4025.7040207@stats.uwo.ca>
References: <f71aec7f1310c.469a5425@utu.fi> <469A2BA4.9030301@stats.uwo.ca>
	<f6ecfde4164b3.469a5a59@utu.fi> <469A3A99.4050308@stats.uwo.ca>
	<f6eab7df1679d.469a6948@utu.fi> <469A4025.7040207@stats.uwo.ca>
Message-ID: <f8e6ff050707150858veda4ef2j222e0375036403f1@mail.gmail.com>

On 7/15/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> On 15/07/2007 11:36 AM, Atte Tenkanen wrote:
> >> On 15/07/2007 10:33 AM, Atte Tenkanen wrote:
> >>>> On 15/07/2007 10:06 AM, Atte Tenkanen wrote:
> >>>>> Hi,
> >>>>>
> >>>>> Is it possible to break using if-condition during the recursive
> >>>> function?
> >>>> You can do
> >>>>
> >>>>  if (condition) return(value)
> >>>>
> >>>>> Here is a function which almost works. It is for inorder-tree-
> >>>> walk.
> >>>>> iotw<-function(v,i,Stack,Indexes) # input: a vector and the
> >> first
> >>>> index (1), Stack=c(), Indexes=c().
> >>>>> {
> >>>>>   print(Indexes)
> >>>>>   # if (sum(i)==0) break # Doesn't work...
> >>>>    if (sum(i)==0) return(NULL)
> >>>>
> >>>> should work.
> >>>>
> >>>> Duncan Murdoch
> >>> Hmm - - - I'd like to save the Indexes-vector (in the example
> >> c(8,4,9,2,10,5,11,1,3)) and stop, when it is ready.
> >>
> >> This seems more like a problem with the design of your function
> >> than a
> >> question about R.  I can't really help you with that, because your
> >> description of the problem doesn't make sense to me.  What does it
> >> mean
> >> to do an inorder tree walk on something that isn't a tree?
> >>
> >> Duncan Murdoch
> >
> > The symbols in vector v have been originally derived from "tree". See
> >
> > http://users.utu.fi/attenka/Tree.jpg
> >
> > But perhaps there's another way to do this, for instance by using loops and if-conditions?
>
> Or perhaps by doing the tree walk on the tree, before you collapse it
> into a vector.

If it's a binary tree with n levels, I think you should be able to
generate the positions more directly, depending on how the tree has
been flattened.  Binary heaps work this way, so that might be a good
place to start.  See http://en.wikipedia.org/wiki/Binary_heap,
particularly heap implementation.

Hadley


From tlumley at u.washington.edu  Sun Jul 15 18:07:33 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 15 Jul 2007 09:07:33 -0700 (PDT)
Subject: [R] Complex surveys,
 properly computed SEs and non-parametric analyses
In-Reply-To: <469A1B4D.9040100@businessdecision.com>
References: <4699DD79020000CB00012C82@MEDICINE.umaryland.edu>
	<469A1B4D.9040100@businessdecision.com>
Message-ID: <Pine.LNX.4.64.0707150850150.9077@homer22.u.washington.edu>

On Sun, 15 Jul 2007, Tobias Verbeke wrote:
> The survey package of Thomas Lumley has very broad functionality for the
> analysis of data from complex sampling designs. Please find below the
> homepage of the package (which is available on CRAN):
>
> http://faculty.washington.edu/tlumley/survey/
>
> I don't think non-parametric one-way ANOVA is implemented

No.

> but quoting
> http://faculty.washington.edu/tlumley/survey/survey-wss.pdf
> "Many features of the survey package result from requests from
> unsatisfied users.
>
> For new methods the most important information is a reference
> that gives sufficient detail for implementation. A data set is nice
> but not critical."
>

Yes, and the details are especially non-obvious here.  The test won't be 
small-sample exact, AFAICS, and it isn't clear whether the idea is to add 
weights to the influence function for the signed-rank test or to replace 
it with a design-based estimate of a population quantity. Often these 
approaches are equivalent, but they won't be in this case. It wouldn't 
have occured to me that people would want this. `Non-parametric' isn't 
really a relevant idea since design-based inference assumes a completely 
known model for the sampling indicators and doesn't even treat the data as 
random variables.

All this goes to say that if there is a standard quantity that John wants, 
it will have resulted in part from a set of arbitrary decisions, and it 
won't be possible to reverse-engineer the estimator in the absence of a 
precise description.  This is in contrast to apparently more complicated 
analyses such as calibration estimators for Cox models in case-cohort
designs, which follow just by putting standard pieces together in an 
obvious way.


 	-thomas


From Karl.Hufthammer at math.uib.no  Sun Jul 15 18:14:44 2007
From: Karl.Hufthammer at math.uib.no (Karl Ove Hufthammer)
Date: Sun, 15 Jul 2007 18:14:44 +0200
Subject: [R] Choosing the number of colour breaks in ggplot2
In-Reply-To: <f8e6ff050707130705x64662b5au3b9e6c2dfc0ee432@mail.gmail.com>
References: <f77vqi$l9v$1@sea.gmane.org>
	<f8e6ff050707130705x64662b5au3b9e6c2dfc0ee432@mail.gmail.com>
Message-ID: <200707151814.44921.Karl.Hufthammer@math.uib.no>

Fredag 13. juli 2007 skreiv hadley wickham:

> There's no official way to do it, but you can "hack" the colour
> gradient scale to do what you want:
>
> # Create a modified scale
> gr <- scale_fill_gradient2()$clone()
> gr$breaks <- function(.) seq(-100, 100, by=10)

Thank you so much. It works perfectly.

-- 
Karl Ove Hufthammer


From ap at tellusion.com  Sun Jul 15 18:11:02 2007
From: ap at tellusion.com (Andrew Prendergast)
Date: Mon, 16 Jul 2007 02:11:02 +1000 (EST)
Subject: [R] How to sort a data.frame
Message-ID: <.192.168.0.10.1184515862.squirrel@mail.int.tellusion.com>

I noticed a decent guide on how to sort data.frames is somewhat lacking.

To fill the gap I have written a quick post on the subject, which is here:

http://www.andrewprendergast.com/2007/07/sorting_a_dataframe_in_r.html

Regards,

ap.


From mazatlanmexico at yahoo.com  Sun Jul 15 19:01:08 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Sun, 15 Jul 2007 10:01:08 -0700 (PDT)
Subject: [R] How to load a dataset
Message-ID: <898473.57708.qm@web56607.mail.re3.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070715/9b7cdc3b/attachment.pl 

From attenka at utu.fi  Sun Jul 15 21:54:49 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 15 Jul 2007 22:54:49 +0300
Subject: [R] Break during the recursion?
In-Reply-To: <f8e6ff050707150858veda4ef2j222e0375036403f1@mail.gmail.com>
References: <f71aec7f1310c.469a5425@utu.fi> <469A2BA4.9030301@stats.uwo.ca>
	<f6ecfde4164b3.469a5a59@utu.fi> <469A3A99.4050308@stats.uwo.ca>
	<f6eab7df1679d.469a6948@utu.fi> <469A4025.7040207@stats.uwo.ca>
	<f8e6ff050707150858veda4ef2j222e0375036403f1@mail.gmail.com>
Message-ID: <f6ea95e011cf3.469aa5b9@utu.fi>

Here is now more elegant function for inorder tree walk, but I still can't save the indexes!? This version now prints them ok, but if I use return, I get only the first v[i]. 

leftchild<-function(i){return(2*i)}

rightchild<-function(i){return(2*i+1)}

iotw<-function(v,i)
{
	if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
	{
		iotw(v,leftchild(i))
		print(v[i]) # return doesn't work here
		iotw(v,rightchild(i))
	}
}

> iotw(1:11,1)
[1] 8
[1] 4
[1] 9
[1] 2
[1] 10
[1] 5
[1] 11
[1] 1
[1] 6
[1] 3
[1] 7


Yours faithfully ;-)

Atte





----- Original Message -----
From: hadley wickham <h.wickham at gmail.com>
Date: Sunday, July 15, 2007 6:58 pm
Subject: Re: [R] Break during the recursion?

> On 7/15/07, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> > On 15/07/2007 11:36 AM, Atte Tenkanen wrote:
> > >> On 15/07/2007 10:33 AM, Atte Tenkanen wrote:
> > >>>> On 15/07/2007 10:06 AM, Atte Tenkanen wrote:
> > >>>>> Hi,
> > >>>>>
> > >>>>> Is it possible to break using if-condition during the 
> recursive> >>>> function?
> > >>>> You can do
> > >>>>
> > >>>>  if (condition) return(value)
> > >>>>
> > >>>>> Here is a function which almost works. It is for inorder-
> tree-
> > >>>> walk.
> > >>>>> iotw<-function(v,i,Stack,Indexes) # input: a vector and the
> > >> first
> > >>>> index (1), Stack=c(), Indexes=c().
> > >>>>> {
> > >>>>>   print(Indexes)
> > >>>>>   # if (sum(i)==0) break # Doesn't work...
> > >>>>    if (sum(i)==0) return(NULL)
> > >>>>
> > >>>> should work.
> > >>>>
> > >>>> Duncan Murdoch
> > >>> Hmm - - - I'd like to save the Indexes-vector (in the example
> > >> c(8,4,9,2,10,5,11,1,3)) and stop, when it is ready.
> > >>
> > >> This seems more like a problem with the design of your function
> > >> than a
> > >> question about R.  I can't really help you with that, because 
> your> >> description of the problem doesn't make sense to me.  What 
> does it
> > >> mean
> > >> to do an inorder tree walk on something that isn't a tree?
> > >>
> > >> Duncan Murdoch
> > >
> > > The symbols in vector v have been originally derived from 
> "tree". See
> > >
> > > http://users.utu.fi/attenka/Tree.jpg
> > >
> > > But perhaps there's another way to do this, for instance by 
> using loops and if-conditions?
> >
> > Or perhaps by doing the tree walk on the tree, before you 
> collapse it
> > into a vector.
> 
> If it's a binary tree with n levels, I think you should be able to
> generate the positions more directly, depending on how the tree has
> been flattened.  Binary heaps work this way, so that might be a good
> place to start.  See http://en.wikipedia.org/wiki/Binary_heap,
> particularly heap implementation.
> 
> Hadley
>


From h.wickham at gmail.com  Sun Jul 15 22:04:12 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 15 Jul 2007 22:04:12 +0200
Subject: [R] Break during the recursion?
In-Reply-To: <f6ea95e011cf3.469aa5b9@utu.fi>
References: <f71aec7f1310c.469a5425@utu.fi> <469A2BA4.9030301@stats.uwo.ca>
	<f6ecfde4164b3.469a5a59@utu.fi> <469A3A99.4050308@stats.uwo.ca>
	<f6eab7df1679d.469a6948@utu.fi> <469A4025.7040207@stats.uwo.ca>
	<f8e6ff050707150858veda4ef2j222e0375036403f1@mail.gmail.com>
	<f6ea95e011cf3.469aa5b9@utu.fi>
Message-ID: <f8e6ff050707151304l370df79dk341874298f80735b@mail.gmail.com>

On 7/15/07, Atte Tenkanen <attenka at utu.fi> wrote:
> Here is now more elegant function for inorder tree walk, but I still can't save the indexes!? This version now prints them ok, but if I use return, I get only the first v[i].
>
> leftchild<-function(i){return(2*i)}
>
> rightchild<-function(i){return(2*i+1)}
>
> iotw<-function(v,i)
>
> {
>         if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
>         {
>                 iotw(v,leftchild(i))
>                 print(v[i]) # return doesn't work here
>                 iotw(v,rightchild(i))
>         }
> }

Shouldn't you return:

c(iotw(v, leftchild(i)), v[i], iotw(v, rightchild(i)))

(and rewrite the conditition to return null if the node doesn't exist,
I think it reads clearer that way)

Hadley


From attenka at utu.fi  Sun Jul 15 22:15:42 2007
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 15 Jul 2007 23:15:42 +0300
Subject: [R] Break during the recursion?
In-Reply-To: <f8e6ff050707151304l370df79dk341874298f80735b@mail.gmail.com>
References: <f71aec7f1310c.469a5425@utu.fi> <469A2BA4.9030301@stats.uwo.ca>
	<f6ecfde4164b3.469a5a59@utu.fi> <469A3A99.4050308@stats.uwo.ca>
	<f6eab7df1679d.469a6948@utu.fi> <469A4025.7040207@stats.uwo.ca>
	<f8e6ff050707150858veda4ef2j222e0375036403f1@mail.gmail.com>
	<f6ea95e011cf3.469aa5b9@utu.fi>
	<f8e6ff050707151304l370df79dk341874298f80735b@mail.gmail.com>
Message-ID: <f6e6931514c5f.469aaa9e@utu.fi>

TNX Hadley!

Atte

----- Original Message -----
From: hadley wickham <h.wickham at gmail.com>
Date: Sunday, July 15, 2007 11:04 pm
Subject: Re: [R] Break during the recursion?

> On 7/15/07, Atte Tenkanen <attenka at utu.fi> wrote:
> > Here is now more elegant function for inorder tree walk, but I 
> still can't save the indexes!? This version now prints them ok, but 
> if I use return, I get only the first v[i].
> >
> > leftchild<-function(i){return(2*i)}
> >
> > rightchild<-function(i){return(2*i+1)}
> >
> > iotw<-function(v,i)
> >
> > {
> >         if (is.na(v[i])==FALSE & is.null(unlist(v[i]))==FALSE)
> >         {
> >                 iotw(v,leftchild(i))
> >                 print(v[i]) # return doesn't work here
> >                 iotw(v,rightchild(i))
> >         }
> > }
> 
> Shouldn't you return:
> 
> c(iotw(v, leftchild(i)), v[i], iotw(v, rightchild(i)))
> 
> (and rewrite the conditition to return null if the node doesn't exist,
> I think it reads clearer that way)
> 
> Hadley
>


From mmeredith at wcs.org  Sun Jul 15 22:28:39 2007
From: mmeredith at wcs.org (Mike Meredith)
Date: Sun, 15 Jul 2007 13:28:39 -0700 (PDT)
Subject: [R] HELP FOR BUGS
In-Reply-To: <572513.95356.qm@web53204.mail.re2.yahoo.com>
References: <572513.95356.qm@web53204.mail.re2.yahoo.com>
Message-ID: <11605645.post@talk.nabble.com>



You might find the 'arm' package useful. For a good introduction to
heirarchical modeling, using 'arm' and also WinBUGS and R2WinBUGS, read
Gelman, A; J Hill 2007. Data analysis using regression and
multilevel/hierarchical models. Cambridge University Press.

Cheers,  Mike.


Ali raza-4 wrote:
> 
> Hi Sir
>    
>   I am very new user of R for the research project on multilevel logistic
> regression.
>   There is confusion about bugs() function in R and BUGS software. Is
> there any relation between these two? Is there any comprehensive package
> for  Multilevel Logistic modelling in R?
>    
>   Please guide in this regard.
>    
>   Thank You
>    
>   RAZA
> 
>        
> ---------------------------------
> Boardwalk for $500? In 2007? Ha! 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/HELP-FOR-BUGS-tf4078749.html#a11605645
Sent from the R help mailing list archive at Nabble.com.


From Marlon.dosReis at agresearch.co.nz  Sun Jul 15 22:47:14 2007
From: Marlon.dosReis at agresearch.co.nz (dos Reis, Marlon)
Date: Mon, 16 Jul 2007 08:47:14 +1200
Subject: [R] NNET re-building the model
Message-ID: <F42646D860656644B5DE60C96E3B4FDB0607D503@rmail.agresearch.co.nz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/d70910f1/attachment.pl 

From r.turner at auckland.ac.nz  Sun Jul 15 23:03:43 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 16 Jul 2007 09:03:43 +1200
Subject: [R] spatstat - Fitting a Strauss model with trend determined by
	kernel density smoother
In-Reply-To: <8df4d8350707130751v1df6fc7q20339bf3391c5350@mail.gmail.com>
References: <8df4d8350707130751v1df6fc7q20339bf3391c5350@mail.gmail.com>
Message-ID: <F5F002A4-7367-4D68-BE5E-EF5CEABD3393@auckland.ac.nz>


On 14/07/2007, at 2:51 AM, Alejandro Veen wrote:

> Dear r-help,
>
> I would like to use the 'ppm' function of the 'spatstat' package to
> fit a Strauss inhibition model.  I understand that I can specify a
> parametric model for the "background" trend, but how would I specify a
> trend which is estimated using a Kernel density smoother?
>
> In particular, I would like to use the 'kde' function of the 'ks'
> package to estimate the "background" intensity and then use this as
> the trend for a Strauss inhibition process.


Questions about a specific contributed package should usually be  
directed to the maintainers of
that package rather than to r-help.

To attempt to answer your question:

	You need to convert your estimate of the background trend to an  
***image***; see the
	function im() in the spatstat package.

	Or instead of using kde, you could use the ppp method for density()  
which is provided in
	spatstat; this methop returns an image.  See the help for density.ppp 
().

	Now suppose that your point pattern is ``X'' and your estimate of  
the trend is ``bgim''.

	You can then fit the model you want via

		fit <- ppm(X,~bgim,inter=Strauss(42),covariates=list(bgim=bgim))

	Note that you have to specify the interaction radius for the Strauss  
model (I have specified this
	radius to be 42 in the forgoing example); this radius is an  
``irregular'' parameter --- i.e. it does
	not appear in exponential family form --- and hence is not estimated  
by ppm(), at least not
	directly.

				cheers,

					Rolf Turner

######################################################################
Attention:\ This e-mail message is privileged and confidenti...{{dropped}}


From ed_deroiste at yahoo.co.uk  Sun Jul 15 23:05:47 2007
From: ed_deroiste at yahoo.co.uk (TIMMMAY)
Date: Sun, 15 Jul 2007 14:05:47 -0700 (PDT)
Subject: [R] Please Please Help me!!!
In-Reply-To: <11601686.post@talk.nabble.com>
References: <11601686.post@talk.nabble.com>
Message-ID: <11606008.post@talk.nabble.com>


Followings someones advice on a previous post I am reposting my question
again. Here is is and I would appreciate any help with my problem. This
question is basically a mathematical question, but I am sure there must be
an easy way to achieve the answer to my problem using R as, my problem seems
to me to be quite straight forward and something that people must use quite
a lot. So if there is an easy way to do this I would appreciate the help.

I have a transition intensities at different ages in a markov model. I want
to estimate this intensity by fitting a curve to the known intensities. The
data is as follows

Age Intensity
22 0.0002
27 0.0011
32 0.0074
37 0.0159
42 0.0292
47 0.0428
52 0.0265
57 0.0301
62 0.0270
67 0.0296

When plotted as intensity vs age, the data looks like a gamma curve. I want
to fit a gamma density curve to this data, so I need to estimate the
paramaters for the gamma curve,It is just a curve fitting problem but whats
causing the trouble is that I need to use least squares minimization to
calculate the parameters for the gamma curve. How do I do this??? 

The curve will be a truncated gamma function so it will have 3 paramaters a,
b, c. I tried to do the following 

nls(lograte ~ log(c) + a*log(b) + (a-1)*log(age)+b*(age)-lgamma(a),
start=list(a=1,b=1,c=1)), where lograte, and age are my data. a,b the gamma
parameters and c the parameter we need because we are fitting a truncated
distribution. 

I also tried defining 
fn = function(p) sum((log(y)-log(dgamma(x,p[1],p[2])*p[3]))^2) 
a residual sum of squares and using nlm to minimise this and find paramaters
but I cant get this to work either. Can anyone help me ?? Please :) 

Thanks


TIMMMAY wrote:
> 
> I posted the message below a few days ago but I have not had any
> responses. I keep thinking that there must be some easy way to answer the
> problem I am just not familiar enough with regression to answer the
> problem myself. If anyone can help me I would be very grateful. I need to
> fit a gamma curve to a set of data. ie a scatterplot of the data indicates
> that the curve looks like a truncated gamma density function and I would
> like to estimate the paramaters so that I can fit a curve to the data
> points. Its not MLE paramater estimation just a curve fitting exercise.
> The problem again is 
> 
> I have a set transition intensities and when plotted the curve looks like
> a gamma density. I want to fit a gamma density curve to these intensities.
> It is just a curve fitting problem but whats causing the trouble is that I
> need to use least squares minimization to calculate the parameters for the
> gamma curve. How do I do this??? 
> 
> The curve will be a truncated gamma function so it will have 3 paramaters
> a, b, c. I tried to do the following 
> 
> nls(lograte ~ log(c) + a*log(b) + (a-1)*log(age)+b*(age)-lgamma(a),
> start=list(a=1,b=1,c=1)), where lograte, and age are my data. a,b the
> gamma parameters and c the parameter we need because we are fitting a
> truncated distribution. 
> 
> I also tried defining 
> fn = function(p) sum((log(y)-log(dgamma(x,p[1],p[2])*p[3]))^2) 
> a residual sum of squares and using nlm to minimise this and find
> paramaters but this doesnt work either. Can anyone help me ?? Please :) 
> 
> Original message follows :(
> Fitting a Gamma Curve   
> by TIMMMAY Jul 12, 2007; 03:38pm :: Rate this Message:    (use ratings to
> moderate[?])
> 
> Reply | Reply to Author | Show Only this Message 
> 
> Hi there, I hope someone can help me before I tear all my hair out. I have
> a set transition intensities and when plotted the curve looks like a gamma
> density. I want to fit a gamma density curve to these intensities. It is
> just a curve fitting problem but whats causing the trouble is that I need
> to use least squares minimization to calculate the parameters for the
> gamma curve. How do I do this??? 
> 
> The curve will be a truncated gamma function so it will have 3 paramaters
> a, b, c. I tried to do the following 
> 
> nls(lograte ~ log(c) + a*log(b) + (a-1)*log(age)+b*(age)-lgamma(a),
> start=list(a=1,b=1,c=1)), where lograte, and age are my data. a,b the
> gamma parameters and c the parameter we need because we are fitting a
> truncated distribution. 
> 
> I also tried defining 
> fn = function(p) sum((log(y)-log(dgamma(x,p[1],p[2])*p[3]))^2) 
> a residual sum of squares and using nlm to minimise this and find
> paramaters but this doesnt work either. Can anyone help me ?? Please :) 
> 
> 
> 
> 
> ? Return to forum
> 
> Start a free forum or mailing list archive on Nabble  Help - Powered by -
> Terms of Use - Jobs at Nabble - Nabble Support  
> 
> 

-- 
View this message in context: http://www.nabble.com/Please-Please-Help-me%21%21%21-tf4081887.html#a11606008
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Sun Jul 15 23:26:00 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 15 Jul 2007 22:26:00 +0100 (BST)
Subject: [R] NNET re-building the model
In-Reply-To: <F42646D860656644B5DE60C96E3B4FDB0607D503@rmail.agresearch.co.nz>
References: <F42646D860656644B5DE60C96E3B4FDB0607D503@rmail.agresearch.co.nz>
Message-ID: <Pine.LNX.4.64.0707152224020.2108@gannet.stats.ox.ac.uk>

This is almost unreadable (is your space bar broken?) but you seem to be 
missing the non-linearity of the hidden units.

The definition is in the book nnet (sic) supports, in a layout I cannot 
reproduce in plain text.

On Mon, 16 Jul 2007, dos Reis, Marlon wrote:

> Hello,
> I've been working with "nnet" and  now I'd like to use the weigths, from
> the fitted model, to iterpret some of variables impornatce.
> I used the following command:
> mts <- nnet(y=Y,x=X,size =4, rang = 0.1,
>             decay = 5e-4, maxit = 5000,linout=TRUE)
> X is (m x n) Y is (m x 1)
> And then I get the coeficients by:
> Wts<-coef(mts)
>
> b->h1        i1->h1        i2->h1        i3->h1        i4->h1
> i5->h1        i6->h1        i7->h1 ...
> b->o         h1->o         h2->o         h3->o         h4->o  ...
>
> I understood that I should get the predicted Y (hat(Y)) by:
>
> hat(Y)=
> (b->o)+((b->h1)sum(X[,1:m]*Wts[i1:m-h1]+X[,1:m]*Wts[i1:m-h2]+...+X[,1:m]
> *Wts[i1:m-h4]))*(h1->o)+
> ((b->h2)sum(X[,1:m]*Wts[i1:m-h2]+X[,1:m]*Wts[i1:m-h2]+...+X[,1:m]*Wts[i1
> :m-h4]))*(h2->o)+...)
> Am I missed some point on this definition of NNET.
> Thanks in advance,
> Best Regards,
> Marlon !!!
>
>
> =======================================================================
> Attention: The information contained in this message and/or ...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From daniel at umd.edu  Mon Jul 16 00:21:24 2007
From: daniel at umd.edu (Daniel Malter)
Date: Sun, 15 Jul 2007 18:21:24 -0400
Subject: [R] Restructuring data
Message-ID: <200707152221.CVV52042@md0.mail.umd.edu>

Hi folks,

I am new to the list and relatively new to R. I am trying to unstack data
"arraywise" and could not find a convenient solution yet. I tried to find a
solution for the problem on help archives. I also tried to use the reshape
command in R and the reshape package but could not get result. I will
illustrate the case below, but the real dataset is quite large so that I
would appreciate an easy solution if there is any.

The current data structure (variable names): 

ID, TIME, BUY-A, BUY-B, SELL-A, SELL-B

Achieved structure (with the reshape command or the reshape package)

ID, TIME, BUY-A
ID, TIME, BUY-B
ID, TIME, SELL-A
ID, TIME, SELL-B 

This is regular unstacking with two identifier variables. Nothing special
though. What I am looking for and did not manage is the following structure:

ID, TIME, BUY-A, SELL-A
ID, TIME, BUY-B, SELL-B

I am quite sure it's pretty easy, but I could not find how to do this. 

Thanks a bunch,
Daniel


From rizwanyounis at hotmail.com  Mon Jul 16 01:02:23 2007
From: rizwanyounis at hotmail.com (Rizwan Younis)
Date: Sun, 15 Jul 2007 19:02:23 -0400
Subject: [R] Partial Proportional Odds model using vglm
Message-ID: <BAY116-DAV18F67D0CCC7DD3F841887DC1FF0@phx.gbl>

Hi:
I am trying to fit a PPO model using vglm from VGAM, and get an error while
executing the code. 

Here is the data, code, and error:

Data  = rc13, first row is the column names. a = age, and 1,2,3, 4 and 5 are
condition grades.

  a  1 2 3  4 5
  1  1 0 0  0 0
  2 84 2 7 10 2
  3 16 0 6  6 2
  4 13 0 3  4 0
  5  0 0 0  1 0

rc13<-read.table("icg_rcPivot_group2.txt",header=F)
names(rc13)<-c("a","1","2","3","4","5")

ppo<-vglm(cbind(rc13[,2],rc13[,3],rc13[,4],rc13[,5],rc13[,6])~a,family =
cumulative(link = logit, parallel = F , reverse = F),na.action=na.pass,
data=rc13)
summary(ppo)

I get the following error:

Error in "[<-"(`*tmp*`, , index, value = c(1.13512932539841,
0.533057528200189,  : 
	number of items to replace is not a multiple of replacement length
In addition: Warning messages:
1: NaNs produced in: log(x) 
2: fitted values close to 0 or 1 in: tfun(mu = mu, y = y, w = w, res =
FALSE, eta = eta, extra) 
3: 19 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace = trace,
wzeps = control$wzepsilon)

I will appreciate any help to fix this problem.
Thanks

Reez You
Grad Student
University of Waterloo
Waterloo, ON Canada


From adschai at optonline.net  Mon Jul 16 01:04:04 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Sun, 15 Jul 2007 23:04:04 +0000 (GMT)
Subject: [R] neural networks function in R
Message-ID: <e45185f837a1d.469aa7e4@optonline.net>

Hi all gurus,

I have a few general questions about using neural networks function, i.e. the nnet function. I'm new to this function and still exploring it. So please kindly bear with me.

Here are my questions.

1. Is there anyway that I can specify my own objective or loss function for my neural networks? I see that by arguments that we can pass into the networks, we have either square loss or entropy. I'm trying to create an NN that fits ordinal regression data. So my objective function is the likelihood function but is not exactly the entropy function due to the fact that order of the labels matters in my application. Or I'm better of to write my own function?

2. If I need to write my own function, I could see that I can use constOptim to help me maximize my likelihood. Another question here is how can I pass analytical gradient function of each of my network weight into the function? Specifically, my question is that does my gradient function has to be flat representation of each parameter like the following arbitrary example:

2*w_i1_h1+3*w_h1_o1+4 (where w_i1_h1 is weight of input node 1 to hidden layer node 1, and vice versa)

Or I can do a nest expression like:

sigmoid(node_eval(input_x, j)) 

here my node_eval is a function that takes in input_x feature value vector and produces output to node j in the next layer. 

If expression base evaluation like in case 2 is possible, it will significantly simplify the definition of my gradient function output. 

Any help would be really appreciated on these issues. Thank you.

- adschai


From deepayan.sarkar at gmail.com  Mon Jul 16 01:05:48 2007
From: deepayan.sarkar at gmail.com (deepayan.sarkar at gmail.com)
Date: Sun, 15 Jul 2007 16:05:48 -0700
Subject: [R] Restructuring data
In-Reply-To: <200707152221.CVV52042@md0.mail.umd.edu>
References: <200707152221.CVV52042@md0.mail.umd.edu>
Message-ID: <eb555e660707151605x47b9f3c9nc684e8ccb083f96@mail.gmail.com>

On 7/15/07, Daniel Malter <daniel at umd.edu> wrote:
> Hi folks,
>
> I am new to the list and relatively new to R. I am trying to unstack data
> "arraywise" and could not find a convenient solution yet. I tried to find a
> solution for the problem on help archives. I also tried to use the reshape
> command in R and the reshape package but could not get result. I will
> illustrate the case below, but the real dataset is quite large so that I
> would appreciate an easy solution if there is any.
>
> The current data structure (variable names):
>
> ID, TIME, BUY-A, BUY-B, SELL-A, SELL-B
>
> Achieved structure (with the reshape command or the reshape package)
>
> ID, TIME, BUY-A
> ID, TIME, BUY-B
> ID, TIME, SELL-A
> ID, TIME, SELL-B
>
> This is regular unstacking with two identifier variables. Nothing special
> though. What I am looking for and did not manage is the following structure:
>
> ID, TIME, BUY-A, SELL-A
> ID, TIME, BUY-B, SELL-B
>
> I am quite sure it's pretty easy, but I could not find how to do this.

This seems to work:

> foo <- data.frame(ID = 1:4, TIME=1:4,
+                   "BUY-A" = rnorm(4),
+                   "BUY-B" = rnorm(4),
+                   "SELL-A" = rnorm(4),
+                   "SELL-B" = rnorm(4), check.names = FALSE)
>
>
> foo
  ID TIME       BUY-A      BUY-B     SELL-A      SELL-B
1  1    1  0.47022807 1.09573107  0.1977035 -0.08333043
2  2    2 -0.20672870 0.07397772  1.4959044 -0.98555020
3  3    3  0.05533779 0.25821758  1.3531913  0.16808307
4  4    4 -0.11471772 1.27798740 -0.1101390 -0.36937994
>
> reshape(foo, direction="long",
+         varying = list(c("BUY-A", "BUY-B"), c("SELL-A", "SELL-B")),
+         v.names=c("BUY", "SELL"), idvar="ID",
+         times = c("A", "B"), timevar="which")
    ID TIME which         BUY        SELL
1.A  1    1     A  0.47022807  0.19770349
2.A  2    2     A -0.20672870  1.49590443
3.A  3    3     A  0.05533779  1.35319133
4.A  4    4     A -0.11471772 -0.11013896
1.B  1    1     B  1.09573107 -0.08333043
2.B  2    2     B  0.07397772 -0.98555020
3.B  3    3     B  0.25821758  0.16808307
4.B  4    4     B  1.27798740 -0.36937994

-Deepayan


From ggrothendieck at gmail.com  Mon Jul 16 05:04:14 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 15 Jul 2007 23:04:14 -0400
Subject: [R] How to sort a data.frame
In-Reply-To: <.192.168.0.10.1184515862.squirrel@mail.int.tellusion.com>
References: <.192.168.0.10.1184515862.squirrel@mail.int.tellusion.com>
Message-ID: <971536df0707152004l16e059cdlad557faa508ba99@mail.gmail.com>

The example could be simplified like this:

T <- productSalesByStore
T <- merge(T, rowsum(T$sales, T$store), by.x = 2, by.y = 0)
T[order(T$V1, T$sales, decreasing = TRUE), 1:3]

If you transfer your data to a data base then this SQL statement would
also do it:

select store, product, sales
   from productSalesByStore
   natural join (select store, sum(sales) storesales from
     productSalesByStore group by store)
   order by storesales desc, sales desc

On 7/15/07, Andrew Prendergast <ap at tellusion.com> wrote:
> I noticed a decent guide on how to sort data.frames is somewhat lacking.
>
> To fill the gap I have written a quick post on the subject, which is here:
>
> http://www.andrewprendergast.com/2007/07/sorting_a_dataframe_in_r.html
>
> Regards,
>
> ap.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From josh.quigley at tibra.com.au  Mon Jul 16 05:49:42 2007
From: josh.quigley at tibra.com.au (Josh Quigley)
Date: Mon, 16 Jul 2007 13:49:42 +1000
Subject: [R] Dates in 'persp' plots
Message-ID: <20070716034943.6AE0A54227@bjpub01.tibra.com.au>

Hello,

I would like to have the y axis show dates in a 3D 'persp' plot.

The following example works...

	x <- spot
	y <- as.numeric(dates)    # class(dates) produces output [1] "Date"
	z <- t(price)

	persp(x, y, z, ticktype="detailed")


...however the y axes contains 'meaningless' integers (days since 1970-01-01
is not very intuitive!)

Changing the commented line above to 

	y <- dates

is no good because 'persp' automatically coerces input to numeric types.

I would really like the axis to be labeled with dates. A string, eg
"13-Jul-2007" would be best, but ISO date integer, eg 20070713, would also
be acceptable.

I've checked help for 'persp' and 'par' with no luck - any ideas
appreciated!

Cheers,

Josh.


From ggrothendieck at gmail.com  Mon Jul 16 06:00:00 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 Jul 2007 00:00:00 -0400
Subject: [R] How to sort a data.frame
In-Reply-To: <971536df0707152004l16e059cdlad557faa508ba99@mail.gmail.com>
References: <.192.168.0.10.1184515862.squirrel@mail.int.tellusion.com>
	<971536df0707152004l16e059cdlad557faa508ba99@mail.gmail.com>
Message-ID: <971536df0707152100l3019acdex16b5ac1df8c3f203@mail.gmail.com>

In thinking about this a bit more an even simpler solution based on ave is:

storesales <- ave(T$sales, T$store, FUN = sum)
T[order(storesales, T$sales, decreasing = TRUE), ]


On 7/15/07, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> The example could be simplified like this:
>
> T <- productSalesByStore
> T <- merge(T, rowsum(T$sales, T$store), by.x = 2, by.y = 0)
> T[order(T$V1, T$sales, decreasing = TRUE), 1:3]
>
> If you transfer your data to a data base then this SQL statement would
> also do it:
>
> select store, product, sales
>   from productSalesByStore
>   natural join (select store, sum(sales) storesales from
>     productSalesByStore group by store)
>   order by storesales desc, sales desc
>
> On 7/15/07, Andrew Prendergast <ap at tellusion.com> wrote:
> > I noticed a decent guide on how to sort data.frames is somewhat lacking.
> >
> > To fill the gap I have written a quick post on the subject, which is here:
> >
> > http://www.andrewprendergast.com/2007/07/sorting_a_dataframe_in_r.html
> >
> > Regards,
> >
> > ap.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From h.wickham at gmail.com  Mon Jul 16 07:48:20 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 16 Jul 2007 07:48:20 +0200
Subject: [R] Restructuring data
In-Reply-To: <eb555e660707151605x47b9f3c9nc684e8ccb083f96@mail.gmail.com>
References: <200707152221.CVV52042@md0.mail.umd.edu>
	<eb555e660707151605x47b9f3c9nc684e8ccb083f96@mail.gmail.com>
Message-ID: <f8e6ff050707152248m489d0fd9p42e3a8b239dce835@mail.gmail.com>

On 7/16/07, deepayan.sarkar at gmail.com <deepayan.sarkar at gmail.com> wrote:
> On 7/15/07, Daniel Malter <daniel at umd.edu> wrote:
> > Hi folks,
> >
> > I am new to the list and relatively new to R. I am trying to unstack data
> > "arraywise" and could not find a convenient solution yet. I tried to find a
> > solution for the problem on help archives. I also tried to use the reshape
> > command in R and the reshape package but could not get result. I will
> > illustrate the case below, but the real dataset is quite large so that I
> > would appreciate an easy solution if there is any.
> >
> > The current data structure (variable names):
> >
> > ID, TIME, BUY-A, BUY-B, SELL-A, SELL-B
> >
> > Achieved structure (with the reshape command or the reshape package)
> >
> > ID, TIME, BUY-A
> > ID, TIME, BUY-B
> > ID, TIME, SELL-A
> > ID, TIME, SELL-B
> >
> > This is regular unstacking with two identifier variables. Nothing special
> > though. What I am looking for and did not manage is the following structure:
> >
> > ID, TIME, BUY-A, SELL-A
> > ID, TIME, BUY-B, SELL-B
> >
> > I am quite sure it's pretty easy, but I could not find how to do this.
>
> This seems to work:
>
> > foo <- data.frame(ID = 1:4, TIME=1:4,
> +                   "BUY-A" = rnorm(4),
> +                   "BUY-B" = rnorm(4),
> +                   "SELL-A" = rnorm(4),
> +                   "SELL-B" = rnorm(4), check.names = FALSE)
> >
> >
> > foo
>   ID TIME       BUY-A      BUY-B     SELL-A      SELL-B
> 1  1    1  0.47022807 1.09573107  0.1977035 -0.08333043
> 2  2    2 -0.20672870 0.07397772  1.4959044 -0.98555020
> 3  3    3  0.05533779 0.25821758  1.3531913  0.16808307
> 4  4    4 -0.11471772 1.27798740 -0.1101390 -0.36937994
> >
> > reshape(foo, direction="long",
> +         varying = list(c("BUY-A", "BUY-B"), c("SELL-A", "SELL-B")),
> +         v.names=c("BUY", "SELL"), idvar="ID",
> +         times = c("A", "B"), timevar="which")
>     ID TIME which         BUY        SELL
> 1.A  1    1     A  0.47022807  0.19770349
> 2.A  2    2     A -0.20672870  1.49590443
> 3.A  3    3     A  0.05533779  1.35319133
> 4.A  4    4     A -0.11471772 -0.11013896
> 1.B  1    1     B  1.09573107 -0.08333043
> 2.B  2    2     B  0.07397772 -0.98555020
> 3.B  3    3     B  0.25821758  0.16808307
> 4.B  4    4     B  1.27798740 -0.36937994

It's a little more verbose with the reshape package, but I find it
easier to understand what's going on.

fm <- melt(foo, id=c("ID","TIME"))
fm <- cbind(fm, colsplit(fm$variable, "-", c("direction","type")))
fm$variable <- NULL

cast(fm, ... ~ direction)

There's an example like this in the introduction to reshape manual.

Hadley


From thomas.pujol at yahoo.com  Mon Jul 16 06:13:01 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Sun, 15 Jul 2007 21:13:01 -0700 (PDT)
Subject: [R] can I run/launch an excel VBA macro from wihin R?
Message-ID: <692809.56908.qm@web59304.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070715/9b12352f/attachment.pl 

From dgvirtual at akl.lt  Mon Jul 16 09:06:58 2007
From: dgvirtual at akl.lt (Donatas G.)
Date: Mon, 16 Jul 2007 10:06:58 +0300
Subject: [R] how do I draw such a barplot?
Message-ID: <200707161006.58676.dgvirtual@akl.lt>

Hi, 

I cannot figure out how to draw a certain plot: could someone help me out?

I have this data.frame from a survey
my.data

that looks like something like this:

   col1  col2  col3  col4
1     5     5     4     5
2     3     5     3     1
3     2     3     4     5
4     3     1     1     2
5     5     5     4     5
6     4     2     5     5
....


Each row represents a single questionnaire with someone giving his 
agreement/disagreement with a statement (each column is a statement) that is 
coded from 1 to 5.

I need to draw a barplot giving a visual representation showing differences 
between the five columns: Each bar should represent a single column, and 
should be divided into 5 sections, the thickness of each depending on the 
number of respondents who choose that particular answer.

How do I do that? All I have managed to do so far is to produce a barplot of a 
single column, and that - only with bars side by side...

-- 
Donatas Glodenis
http://dg.lapas.info


From adrian at iprocor.org  Mon Jul 16 09:52:26 2007
From: adrian at iprocor.org (Adrian J. Montero Calvo)
Date: Mon, 16 Jul 2007 09:52:26 +0200
Subject: [R] LSD, HSD,...
Message-ID: <469B23BA.5020309@iprocor.org>

Hi,
    I'm designing a experiment in order to compare the growing of 
several clones of a tree specie. It will be a complete randomized block 
design. How can I decide what model of mean comparision to choose? LSD, 
HSD,TukeyHSD,  Duncan,...?  Thanks in advance


From tamir at imp.univie.ac.at  Mon Jul 16 09:51:02 2007
From: tamir at imp.univie.ac.at (Ido M. Tamir)
Date: Mon, 16 Jul 2007 03:51:02 -0400
Subject: [R] how do I draw such a barplot?
Message-ID: <200707160351.02234.tamir@imp.univie.ac.at>

Hi,

>Each row represents a single questionnaire with someone giving his 
>agreement/disagreement with a statement (each column is a statement) that is 
>coded from 1 to 5.

Maybe I mixed up columns and rows (because I find only
4 columns in you example) but it would go something like this with ggplot2:

library("reshape")
library("ggplot2")

my.data$id=1:nrow(my.data)
resp <- melt(my.data,id=5)
p <- ggplot(resp, aes(x=factor(variable), fill=factor(value))) + 
geom_bar(position="fill")

http://had.co.nz/ggplot2/position_fill.html

best wishes
ido


From ripley at stats.ox.ac.uk  Mon Jul 16 10:14:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jul 2007 09:14:36 +0100 (BST)
Subject: [R] How to load a dataset
In-Reply-To: <898473.57708.qm@web56607.mail.re3.yahoo.com>
References: <898473.57708.qm@web56607.mail.re3.yahoo.com>
Message-ID: <Pine.LNX.4.64.0707160904380.13846@gannet.stats.ox.ac.uk>

See ?Startup.

Rprofile.site (sic) is for site-wide settimgs, not

On Sun, 15 Jul 2007, Felipe Carrillo wrote:

> Hi:
>  Since I didn't get any answers, I'll refresh my question.

What you needed to do was to read the posting guide.  As you are _still_ 
sending HTML mail, you clearly have not yet done so.  It advised you that 
if you do not get any answers you need to improve the quality of your 
posting.

You have not told us your OS, as requested there.

>  I have a dataset called "Chinook Run" saved in Excel and I want it to be loaded everytime R starts, so I can call it with a statement like the one below:
>  qplot(color, Year/Forklength, data = Chinook Run)

R object names cannot contain spaces unless quoted (here `Chinnok Run`). 
It would be a lot easier to use Chinook_Run.

>  I wonder how can I do that.
>  I went to the rprofile.site and copied the path to my dataset there and
>  it seems to work but I am wondering if that's how's done.

Just adding the path to the dataset will not work.  Adding a command 
to load the dataset might, but you have not told us the format of the 
dataset you saved.  The file etc/Rprofile.site (sic) is intended for 
site-wide settings (see ?Startup) and loads into 'base', something you 
should avoid.

The normal way to do this to save the workspace containing the objects you 
want loaded in subsequent sessions.

>  Felipe
>
>
>
>
> Felipe D. Carrillo
>  Fishery Biologist
>  US Fish & Wildlife Service
>  Red Bluff, California 96080
>
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From s.blomberg1 at uq.edu.au  Mon Jul 16 10:19:30 2007
From: s.blomberg1 at uq.edu.au (Simon Blomberg)
Date: Mon, 16 Jul 2007 18:19:30 +1000
Subject: [R] LSD, HSD,...
In-Reply-To: <469B23BA.5020309@iprocor.org>
References: <469B23BA.5020309@iprocor.org>
Message-ID: <1184573970.28656.10.camel@sib-sblomber01d.sib.uq.edu.au>

If you have a priori planned comparisons, you can just test those using
linear contrasts, with no need to correct for multiple testing. If you
do not, and you are relying on looking at the data and analysis to tell
you which treatment means to compare, and you are considering several
tests, then you should consider correcting for multiple testing. There
is a large literature on the properties of the various tests. (Tukey HSD
usually works pretty well for me.)

<rant> Why do people design experiments with a priori hypotheses in
mind, yet test them using post hoc comparison procedures? It's as if
they are afraid to admit that they had hypotheses to begin with! Far
better to test what you had planned to test using the more powerful
methods for planned comparisons, and leave it at that.
</rant>


On Mon, 2007-07-16 at 09:52 +0200, Adrian J. Montero Calvo wrote:
> Hi,
>     I'm designing a experiment in order to compare the growing of 
> several clones of a tree specie. It will be a complete randomized block 
> design. How can I decide what model of mean comparision to choose? LSD, 
> HSD,TukeyHSD,  Duncan,...?  Thanks in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Simon Blomberg, BSc (Hons), PhD, MAppStat. 
Lecturer and Consultant Statistician 
Faculty of Biological and Chemical Sciences 
The University of Queensland 
St. Lucia Queensland 4072 
Australia
Room 320 Goddard Building (8)
T: +61 7 3365 2506 
email: S.Blomberg1_at_uq.edu.au

Policies:
1.  I will NOT analyse your data for you.
2.  Your deadline is your problem.

The combination of some data and an aching desire for 
an answer does not ensure that a reasonable answer can 
be extracted from a given body of data. - John Tukey.


From gmoeser at aol.com  Mon Jul 16 11:28:09 2007
From: gmoeser at aol.com (=?utf-8?Q?Dr._Guido_M=C3=B6ser?=)
Date: Mon, 16 Jul 2007 05:28:09 -0400
Subject: [R] How to write a data.frame into n different csv-files
Message-ID: <8C995A2C24578B3-248-6CF@FWM-D25.sysops.aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/c15d14e8/attachment.pl 

From ccleland at optonline.net  Mon Jul 16 11:35:34 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 16 Jul 2007 05:35:34 -0400
Subject: [R] How to write a data.frame into n different csv-files
In-Reply-To: <8C995A2C24578B3-248-6CF@FWM-D25.sysops.aol.com>
References: <8C995A2C24578B3-248-6CF@FWM-D25.sysops.aol.com>
Message-ID: <469B3BE6.1070009@optonline.net>

Dr. Guido M?ser wrote:
> Hello,
> 
> 
> I want to write a data.frame with 10 different entries x (by index y)? into ten different csv-files. 
> In every output file should be one of the ten numbers of x. 
> 
> 
> I tried this one, but I can't write into ten different files, like test1.csv to test10.csv:
> 
> 
> x <- rnorm(10)
> y <- c(1:10)
> z <- data.frame(y,x)
> 
> n <- nrow(z)
> for (i in 1:n) write.csv(z$x[i], file="test[i].csv")
> 
> 
> Can anyone help me?

  Try this:

x <- rnorm(10)
y <- c(1:10)
z <- data.frame(y,x)

n <- nrow(z)
for (i in 1:n) write.csv(z$x[i], file=paste("test", i, ".csv", sep=""))

> Yours,
> 
> 
> 
> Guido
> 
> 
> 
> ---
> Dr. Guido Moeser
> Diplom Volkswirt
> Diplom Sozialwissenschaftler
> 
> E-Mail: GMoeser at aol.com
> 
> ________________________________________________________________________
> Bei AOL gibt's jetzt kostenlos eMail f?r alle.  Klicken Sie auf AOL.de um heraus zu finden, was es sonst noch kostenlos bei AOL gibt.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From steve at promente.org  Mon Jul 16 09:31:29 2007
From: steve at promente.org (Steve Powell)
Date: Mon, 16 Jul 2007 09:31:29 +0200
Subject: [R] Hmisc variable labels as vector?
In-Reply-To: <200707161006.58676.dgvirtual@akl.lt>
Message-ID: <00d401c7c77b$5570f740$6501a8c0@STEVE>

Dear members
I have imported an SPSS data file using Hmisc. 
So  label(mydata[[1]]) gives me the first variable label
Just wondering how I can access all the variable labels as a vector?
Something like label(mydata[[1:3]]) but that doesn't work.
Thanks in advance


Steve Powell


proMENTE social research
research | evaluation | training & consulting


From sigiml at netvision.net.il  Mon Jul 16 11:34:08 2007
From: sigiml at netvision.net.il (sigalit mangut-leiba)
Date: Mon, 16 Jul 2007 12:34:08 +0300
Subject: [R] table function
Message-ID: <000001c7c78c$768ba2c0$0200000a@fox1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/7b3d6268/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Mon Jul 16 11:48:20 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 16 Jul 2007 11:48:20 +0200
Subject: [R] Hmisc variable labels as vector?
In-Reply-To: <00d401c7c77b$5570f740$6501a8c0@STEVE>
References: <00d401c7c77b$5570f740$6501a8c0@STEVE>
Message-ID: <469B3EE4.9060401@biostat.ku.dk>

Steve Powell wrote:
> Dear members
> I have imported an SPSS data file using Hmisc. 
> So  label(mydata[[1]]) gives me the first variable label
> Just wondering how I can access all the variable labels as a vector?
> Something like label(mydata[[1:3]]) but that doesn't work.
>   
Something like sapply(mydata, label) should work.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From jim at bitwrit.com.au  Mon Jul 16 12:01:46 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Mon, 16 Jul 2007 20:01:46 +1000
Subject: [R] Text with differents colors in a plot / Detecting if text
 exists
In-Reply-To: <20070715155844.200450@gmx.net>
References: <20070715155844.200450@gmx.net>
Message-ID: <469B420A.30904@bitwrit.com.au>

Maja Schr?ter wrote:
> Hi everybody,
> 
> I want to write some text in a plot.
> 
> That's simple I know. But I want to make use of different colors.
> 
> Eg. text(x,y,paste("Sunderland","high")).
> 
> Then Sunderland should be black and "high" red.
> 
> Has anyone an idea?
> 
> By the way. I'm looking for a function or something similar, that can check whether there is text in some regions on the plot. 
> Because I don't want to overwrite an existing text with a new one.
> 
Hi Maja,
For your first question:

text(c(9,9),c(5,5),c("Sunderland","high"),
  pos=c(2,4),offset=0.2,col=c("black","red"))

i.e., don't form a single string so that you can specify different 
colors and positions for the two words.

For the second one, I am about to upload a new version of the plotrix 
package that contains a function named "emptyspace". This tries to find 
the largest rectangle on a plot that has nothing plotted within it.

Jim


From P.Dalgaard at biostat.ku.dk  Mon Jul 16 12:03:13 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 16 Jul 2007 12:03:13 +0200
Subject: [R] table function
In-Reply-To: <000001c7c78c$768ba2c0$0200000a@fox1>
References: <000001c7c78c$768ba2c0$0200000a@fox1>
Message-ID: <469B4261.1090502@biostat.ku.dk>

sigalit mangut-leiba wrote:
> Hello all, 
>
> I want to use the "table" function, but for every id I have a different no.
> of rows (obs.).
>
> If I write:
>
>  
>
> table(x$class, x$infec)
>
>  
>
> I don't get the right frequencies because I should count every id once, if
> id 1 has 20 observations It should count as one.
>
> can I use unique func. here?
>
> Hope it's clear.
>   
Almost. I assume that class and infect are constant over id?  (If people
change infection status during the trial, you have a more complex problem).

You could then use unique() like this

with(unique(x[c("id", "class", "infec")] , table(class, infec)),

but I'd prefer using duplicated() as in

with(subset(x, !duplicated(id)), table(class, infec))

(notice that the latter tabulates the first record for each id, whereas
the former will count ids multiple times if the change class or infec).

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From smangut at gmail.com  Mon Jul 16 12:04:19 2007
From: smangut at gmail.com (sigalit mangut-leiba)
Date: Mon, 16 Jul 2007 13:04:19 +0300
Subject: [R] the table function
Message-ID: <c99f7100707160304w2e4feect4c2647db5a6dd32f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/a66872d5/attachment.pl 

From roger.bos at us.rothschild.com  Mon Jul 16 13:46:18 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Mon, 16 Jul 2007 07:46:18 -0400
Subject: [R] can I run/launch an excel VBA macro from wihin R?
In-Reply-To: <692809.56908.qm@web59304.mail.re1.yahoo.com>
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD347CFEB3@RINNYCSE000.rth.ad.rothschild.com>

I have Excel files that use the auto_open() sub to run automatically and
then I can open the file using R by using the shell() command.  Then
once the file is done running, I can process the file again using R.
Works pretty well, as long as I don't have any problems on the excel
side.


shell("excel {path to filename}")
Excel has to be in your path, or you can give the full path to excel.
You don't need the double quotes if none of your folders or files have
spaces in them.

HTH, Roger

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Pujol
Sent: Monday, July 16, 2007 12:13 AM
To: r-help at stat.math.ethz.ch
Subject: [R] can I run/launch an excel VBA macro from wihin R?

Is there an "easy" or good way to run/launch an Excel VBA macro from
within R?

       
---------------------------------
Pinpoint customers who are looking for what you sell. 
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From rizwanyounis at hotmail.com  Mon Jul 16 14:01:37 2007
From: rizwanyounis at hotmail.com (Rizwan Younis)
Date: Mon, 16 Jul 2007 08:01:37 -0400
Subject: [R] Partial Proportional Odds model using vglm
Message-ID: <BAY116-DAV1675B3ED045F094B91EF31C1F80@phx.gbl>

Hi:
I am trying to fit a PPO model using vglm from VGAM, and get an error while
executing the code. 

Here is the data, code, and error:

Data  = rc13, first row is the column names. a = age, and 1,2,3, 4 and 5 are
condition grades.

  a  1 2 3  4 5
  1  1 0 0  0 0
  2 84 2 7 10 2
  3 16 0 6  6 2
  4 13 0 3  4 0
  5  0 0 0  1 0

Library(VGAM)

rc13<-read.table("icg_rcPivot_group2.txt",header=F)
names(rc13)<-c("a","1","2","3","4","5")

ppo<-vglm(cbind(rc13[,2],rc13[,3],rc13[,4],rc13[,5],rc13[,6])~a,family =
cumulative(link = logit, parallel = F , reverse = F),na.action=na.pass,
data=rc13)
summary(ppo)

I get the following error:

Error in "[<-"(`*tmp*`, , index, value = c(1.13512932539841,
0.533057528200189,  : 
	number of items to replace is not a multiple of replacement length
In addition: Warning messages:
1: NaNs produced in: log(x) 
2: fitted values close to 0 or 1 in: tfun(mu = mu, y = y, w = w, res =
FALSE, eta = eta, extra) 
3: 19 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace = trace,
wzeps = control$wzepsilon)

I will appreciate any help to fix this problem.
Thanks

Reez You
Grad Student
University of Waterloo
Waterloo, ON Canada


From dataanalytics at earthlink.net  Mon Jul 16 14:02:02 2007
From: dataanalytics at earthlink.net (Walter Paczkowski)
Date: Mon, 16 Jul 2007 08:02:02 -0400 (GMT-04:00)
Subject: [R] looking at a function's code
Message-ID: <7461489.1184587322718.JavaMail.root@elwamui-cypress.atl.sa.earthlink.net>

Good morning,

I'd like to look at the code for the R function head.  When I type just the word head, I get back

function(x, ...)
UseMethod("head")
<environment: namespace:utils>


I expected to see several lines of R code.  Any suggestions?

Thanks,

Walt Paczkowski


From Karl.Hufthammer at math.uib.no  Mon Jul 16 14:04:57 2007
From: Karl.Hufthammer at math.uib.no (Karl Ove Hufthammer)
Date: Mon, 16 Jul 2007 14:04:57 +0200
Subject: [R] Different axis limits for each facet in ggplot2
Message-ID: <f7fmt9$ide$1@sea.gmane.org>

Hi!

Is it possible to have different axis limit for each facet in a ggplot2
plot? Here is an example:

--------------------------------------------------------------
library(ggplot2)

x=seq(-10,10,.1)
y=cos(x)
z=sin(x)*10

dat=melt(data.frame(x,y,z), id.var="x")

qplot( x, value, data=dat, facets=variable~., geom="line" )
--------------------------------------------------------------

Both the x and y axes are now the same for both facets. But since the ranges
of the 'y' and the 'z' variable differ so much, I would prefer their y axes
to be different. This is easy to do in 'lattice':

--------------------------------------------------------------
library(lattice)
xyplot( value~x | variable, data=dat, type="l", layout=c(1,2),
scale=list(y="free") )
--------------------------------------------------------------

Is it possible to do this in 'ggplot2' too? I would have thought that
+ scale_y_continuous(limits=c(NA,NA))
would work, but this sets the 'y' axis to cover the range of 'y' and 'z'
*combined* (which is very sensible as a default, but not what I would
prefer here).

-- 
Karl Ove Hufthammer


From meeryana at yahoo.com  Mon Jul 16 12:41:45 2007
From: meeryana at yahoo.com (copula)
Date: Mon, 16 Jul 2007 03:41:45 -0700 (PDT)
Subject: [R] R and Copula
Message-ID: <11612986.post@talk.nabble.com>


hi,
first I want to say that I'm new here, and new with copula and R.

That is the reason why I'm writing, if somebody can help me. 

I have to make an example of Copula. 
On internet I've found this forum and that copula can calculate with R.

Can somebody help me with the thing how can I start and where can read about
these stuffs.

Thank to all who can help!



-- 
View this message in context: http://www.nabble.com/R-and-Copula-tf4085867.html#a11612986
Sent from the R help mailing list archive at Nabble.com.


From Duest_Falko at web.de  Mon Jul 16 13:03:04 2007
From: Duest_Falko at web.de (=?iso-8859-15?Q?Falko_D=FCsterh=F6ft?=)
Date: Mon, 16 Jul 2007 13:03:04 +0200
Subject: [R] harmonic time series
Message-ID: <1998705113@web.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/099adbaf/attachment.pl 

From murdoch at stats.uwo.ca  Mon Jul 16 14:22:49 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 16 Jul 2007 08:22:49 -0400
Subject: [R] looking at a function's code
In-Reply-To: <7461489.1184587322718.JavaMail.root@elwamui-cypress.atl.sa.earthlink.net>
References: <7461489.1184587322718.JavaMail.root@elwamui-cypress.atl.sa.earthlink.net>
Message-ID: <469B6319.7090607@stats.uwo.ca>

On 16/07/2007 8:02 AM, Walter Paczkowski wrote:
> Good morning,
> 
> I'd like to look at the code for the R function head.  When I type just the word head, I get back
> 
> function(x, ...)
> UseMethod("head")
> <environment: namespace:utils>
> 
> 
> I expected to see several lines of R code.  Any suggestions?

Even though it's not very informative, that really is the source for 
that function.  For instructions on how to see the more useful stuff, 
see Uwe Ligges' article in the Oct 2006 R News (available at 
http://cran.r-project.org/doc/Rnews/Rnews_2006-4.pdf).

Duncan Murdoch


From ccleland at optonline.net  Mon Jul 16 14:26:03 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 16 Jul 2007 08:26:03 -0400
Subject: [R] looking at a function's code
In-Reply-To: <7461489.1184587322718.JavaMail.root@elwamui-cypress.atl.sa.earthlink.net>
References: <7461489.1184587322718.JavaMail.root@elwamui-cypress.atl.sa.earthlink.net>
Message-ID: <469B63DB.9080901@optonline.net>

Walter Paczkowski wrote:
> Good morning,
> 
> I'd like to look at the code for the R function head.  When I type just the word head, I get back
> 
> function(x, ...)
> UseMethod("head")
> <environment: namespace:utils>
> 
> 
> I expected to see several lines of R code.  Any suggestions?

  Have a look at:

https://svn.r-project.org/R/trunk/src/library/utils/R/head.R

  Also, ?head shows methods for different types of objects, and you can
see these with

getAnywhere(head.default)

or

utils:::head.default

> Thanks,
> 
> Walt Paczkowski
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Karl.Hufthammer at math.uib.no  Mon Jul 16 14:32:45 2007
From: Karl.Hufthammer at math.uib.no (Karl Ove Hufthammer)
Date: Mon, 16 Jul 2007 14:32:45 +0200
Subject: [R] looking at a function's code
References: <7461489.1184587322718.JavaMail.root@elwamui-cypress.atl.sa.earthlink.net>
Message-ID: <f7fohd$o36$1@sea.gmane.org>

Walter Paczkowski:

> I'd like to look at the code for the R function head. ?When I type just
> the word head, I get back
> 
> function(x, ...)
> UseMethod("head")
> <environment: namespace:utils>

This basically means that 'head' is a generic function that works in
different ways for different classes of objects (data frames, matrices,
tables &c.) Many function, e.g., 'plot' and 'mean', work the same way.

Type ?UseMethod for a better/longer explanation.

Now type

methods(head)

You will get:

[1] head.data.frame* head.default*    head.ftable*     head.function*
[5] head.matrix      head.table*

Now, ordinarily you should be able to write 'head.data.frame' to see the
code for this function, but, since it is starred, '*', this doesn't work.
The easiest way to get hold of it is using 'getAnywhere':

getAnywhere(head.data.frame)

-- 
Karl Ove Hufthammer


From rmh at temple.edu  Mon Jul 16 14:42:14 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 16 Jul 2007 08:42:14 -0400 (EDT)
Subject: [R] can I run/launch an excel VBA macro from wihin R?
Message-ID: <20070716084214.CGO82943@po-d.temple.edu>

Install the R(D)COM server and the RExcel interface.
It provides complete two-way communication between R and Excel.

See 
http://sunsite.univie.ac.at/rcom/
for full information and download.

Rich


From elyakhlifi_mustapha at yahoo.fr  Mon Jul 16 15:01:46 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Mon, 16 Jul 2007 15:01:46 +0200 (CEST)
Subject: [R] extract from anova
Message-ID: <673819.11017.qm@web27506.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/da8c791e/attachment.pl 

From jhallman at frb.gov  Mon Jul 16 15:26:06 2007
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 16 Jul 2007 09:26:06 -0400
Subject: [R] accessing list components with a variable
References: <2F25148A-2EBE-4BFD-AF0D-83879D7B03D1@davidcjames.com>
Message-ID: <xmr7ip0cwep.fsf@mralx1.rsma.frb.gov>

the_list[[comp]]

"David C. James" <dj at davidcjames.com> writes:

> Let's say I have a list called the_list consisting of three components:
> the_list$component_1
> the_list$component_2
> the_list$component_3
> 
> Now, I want to access it using a variable called comp.
> comp <- "component_1"
> 
> I'm looking for some function that let's me do this:
> unknown_function(the_list, comp)
> 
> Which should do the same thing as:
> the_list$component_1
> 
> Any ideas?  I'd be open to other ways of doing this, too.  I explored  
> this way, but it didn't seem to get me anywhere:
> the_list$"component_1"
> 
> I possibly could have gotten further along if I knew how to do  
> evaluation in R.
> 
> Thanks,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Jeff


From jomopo at gmail.com  Mon Jul 16 15:50:41 2007
From: jomopo at gmail.com (=?ISO-8859-1?Q?Josu=E9_Polanco?=)
Date: Mon, 16 Jul 2007 15:50:41 +0200
Subject: [R] question about ar1 time series
Message-ID: <98e024570707160650n2f720dc6p8be014a08ca63e49@mail.gmail.com>

Hello everybody,

I recently wrote a "program" that to generate AR1 time series, here the code:


#By Jomopo. Junio-2007, Leioa, Vizcaya
#This program to create the AR1 syntetic series (one by one)
#Where the mean is zero, but the variance of the serie AR1 and
#the coef. of AR1 are be changed. If var serie AR1 = 1 then is standarized!
#Final version for AR1 time series program
#Mon Jul 16 11:58:03 CEST 2007 Checked again in R-prompt, and it's OK!

#Creating the sintetic AR1 series... where the "white-noise"
#has a mean = 0, and the var = sigmaz_c = stand_dev^2 is whatever value,
#if sigmaz_c = 1 then this "white-noise" is a "Gaussian-noise."
#rho1 (or alpha in another text-books ;-)) < 1 (in fact 0 < rho1 < 1) so that
#the system can be stationary.
#Where var_serie is the variance of the serie

cat("\n Hello, this is creat_AR1_synt_ser.R. \n These are the input
parameters: synt_series(ar1_length, rho1, ...), where rho1 is the
correlat. coef.\n")


ar1 <- function(x, rho1, af)
{
   return(x*rho1 + runif(1, -af, af))
}

#Spin-up for the AR1 series. For this case is enough with this amount
spinup <- function(x0, rho1, af)
{
   xt <- x0
   for (i in 1:100) {
     xtp1 <- ar1(xt, rho1, af)
     xt   <- xtp1
   }
   return(xt)
}

#Wherein "ar1_length" is the number of data in AR1 series
#rho1 is a correlation coef.
#sigmaz_c is optional
synt_series <- function(ar1_length, rho1, var_serie)
{
   if( (var_serie <= 0) || rho1 <= -1 || rho1 >= 1 )
     stop("The variance of the serie should be > 0, or the rho1
parameter should be between (-1, 1) for that the serie can be
stationary, be careful with this, bye. \n")

   syntdata    <- rep(0, ar1_length)
#af = adjustement factor, i.e. for that the var of random numbers =
var of white noise (check the manual of runif)
   af          <- sqrt( 3 * var_serie * (1 - rho1) * (1 + rho1) )
   x0          <- runif(1, -af, af)
   syntdata[1] <- spinup(x0, rho1, af)

   for (i in 2:ar1_length) {
     syntdata[i]  <- ar1(syntdata[i - 1], rho1, af)
   }
   return(syntdata)
}


I would like some suggestions and hints.

Thanks a lot for your help!

-- 
Josu? Mos?s Polanco Mart?nez
Correo-e alternativo jomopo at linuxmail.org
----
It is a wasted day unless you have learned something new and made
someone smile -Mark Weingartz.


From joris.dewolf at cropdesign.com  Mon Jul 16 15:56:31 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Mon, 16 Jul 2007 15:56:31 +0200
Subject: [R] extract from anova
In-Reply-To: <673819.11017.qm@web27506.mail.ukl.yahoo.com>
Message-ID: <OF24D70C3E.256431F4-ONC125731A.004B8F1F-C125731A.004C7DCF@basf-c-s.be>



Mustapha,

You should tell us what fm1 actually is (which class), before we can give
you a correct answer.
It is helpfull to use str() in this case.

str(fm1) will give you the structure of fm1, and with that information you
extract any value you want.

I suppose that this is what you need:

summary(fm1)[[1]]["Residuals","Mean Sq"]








                                                                           
             elyakhlifi                                                    
             mustapha                                                      
             <elyakhlifi_musta                                          To 
             pha at yahoo.fr>             R-help at stat.math.ethz.ch            
             Sent by:                                                   cc 
             r-help-bounces at st                                             
             at.math.ethz.ch                                       Subject 
                                       [R] extract from anova              
                                                                           
             16/07/2007 15:01                                              
                                                                           
                                                                           
                                                                           
                                                                           




Hello,
I would like extract the Mean Sq of the Residuals how should I do  please?
thanks.


> summary(fm1)
            Df  Sum Sq Mean Sq F value    Pr(>F)
groupe      20 300.987  15.049  22.853 3.369e-16 ***
Residuals   41  27.000   0.659
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



_____________________________________________________________________________


             [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From docpat2511 at yahoo.com  Mon Jul 16 16:13:09 2007
From: docpat2511 at yahoo.com (Pat Carroll)
Date: Mon, 16 Jul 2007 07:13:09 -0700 (PDT)
Subject: [R] Errors in data frames from read.table
Message-ID: <849314.73729.qm@web60822.mail.yahoo.com>

Hello, all.

I am working on a project with a large (~350Mb, about 5800 rows) insurance claims dataset. It was supplied in a tilde(~)-delimited format. I imported it into a data frame in R by setting memory.limit to maximum (4Gb) for my computer and using read.table. 

The resulting data frame had 10 bad rows. The errors appear due to read.table missing delimiter characters, with multiple data being imported into the same cell, then the remainder of the row and the next run together and garbled due to the reading frame shift (example: a single cell might contain: <datum>~ ~ <datum> ~<datum>, after which all the cells of the row and the next are wrong). 

To replicate, I tried the same import procedure on a smaller demographics data set from the same supplier- only about 1Mb, and got the same kinds of errors (5 bad rows in about 3500). I also imported as much of the file as Excel would hold and cross-checked, Excel did not produce the same errors but can't handle the entire file. I have used read.table on a number of other formats (mainly csv and tab-delimited) without such problems; so far it appears there's something different about these files that produces the errors but I can't see what it would be.

Does anyone have any thoughts about what is going wrong? And is there a way, short of manual correction, for fixing it?

Thanks for all help,
~Pat.


Pat Carroll. 
what matters most is how well you walk through the fire. 
bukowski.


From bacaro at unisi.it  Mon Jul 16 16:29:25 2007
From: bacaro at unisi.it (giovanni bacaro)
Date: Mon, 16 Jul 2007 16:29:25 +0200
Subject: [R] very complicated nested design
Message-ID: <011501c7c7b5$b5b1b9c0$554a10ac@fisso>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/f1096950/attachment.pl 

From macq at llnl.gov  Mon Jul 16 16:35:08 2007
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 16 Jul 2007 07:35:08 -0700
Subject: [R] How to read many files at one time?
In-Reply-To: <3f2938d50707141416j46222b9agcd6880884008af9b@mail.gmail.com>
References: <3f2938d50707141416j46222b9agcd6880884008af9b@mail.gmail.com>
Message-ID: <p06230900c2c13171617b@[128.115.153.6]>

Storing them as elements of list is fine, but if you want them as 
individual objects, then something like this (not tested):


for (i in seq( length(tst) ) ) {
    tmp <- read.table( tst[i] )    ## or read.delim() or read.csv(), 
as necessary
    assign( data.name[i],  tmp )
}

-Don

At 3:16 PM -0600 7/14/07, Zhang Jian wrote:
>I want to load many files in the R. The names of the files are "Sim1.txt", "
>Sim2.txt", "Sim3.txt", "Sim4.txt", "Sim5.txt" and so on.
>Can I read them at one time? What should I do? I can give the same names in
>R.
>Thanks.
>
>For example:
>>  tst=paste("Sim",1:20,".txt",sep="") # the file names
>>  tst
>  [1] "Sim1.txt"  "Sim2.txt"  "Sim3.txt"  "Sim4.txt"  "Sim5.txt"  "Sim6.txt"
>  [7] "Sim7.txt"  "Sim8.txt"  "Sim9.txt"  "Sim10.txt" "Sim11.txt" "Sim12.txt"
>[13] "Sim13.txt" "Sim14.txt" "Sim15.txt" "Sim16.txt" "Sim17.txt" "Sim18.txt"
>[19] "Sim19.txt" "Sim20.txt"
>
>>  data.name=paste("Sim",1:20,sep="") # the file names in R
>>  data.name
>  [1] "Sim1"  "Sim2"  "Sim3"  "Sim4"  "Sim5"  "Sim6"  "Sim7"  "Sim8"  "Sim9"
>[10] "Sim10" "Sim11" "Sim12" "Sim13" "Sim14" "Sim15" "Sim16" "Sim17" "Sim18"
>[19] "Sim19" "Sim20"
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA
925-423-1062


From ted.harding at nessie.mcc.ac.uk  Mon Jul 16 17:00:26 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 16 Jul 2007 16:00:26 +0100 (BST)
Subject: [R] Errors in data frames from read.table
In-Reply-To: <849314.73729.qm@web60822.mail.yahoo.com>
Message-ID: <XFMail.070716160026.ted.harding@nessie.mcc.ac.uk>

On 16-Jul-07 14:13:09, Pat Carroll wrote:
> Hello, all.
> 
> I am working on a project with a large (~350Mb, about 5800 rows)
> insurance claims dataset. It was supplied in a tilde(~)-delimited
> format. I imported it into a data frame in R by setting memory.limit to
> maximum (4Gb) for my computer and using read.table. 

I had a similar problem put to me some time back, and eventually
solved it by going in with a scalpel. It turned out that there
was a problem with muddling "End-of-Line" with field delimiter
in creating the file. And the file came out of Excel in the first
place ... (did yours?). Quite why excell made this particular
mess of it remains a mystery.

I note that your file size is "350Mb" and "about 5800 rows".
Doing some arithmetic on that:

350 * 1024 * 1024 = 367,001,600 bytes

367001600/5800 = 63276.14 bytes per row.

This (given your "about"s) looks to me dangerously close to
65536 = 64K, and this may be a limit on what Excel can handle?

Just a thought ...
Ted.

> The resulting data frame had 10 bad rows. The errors appear due to
> read.table missing delimiter characters, with multiple data being
> imported into the same cell, then the remainder of the row and the next
> run together and garbled due to the reading frame shift (example: a
> single cell might contain: <datum>~ ~ <datum> ~<datum>, after which all
> the cells of the row and the next are wrong). 
> 
> To replicate, I tried the same import procedure on a smaller
> demographics data set from the same supplier- only about 1Mb, and got
> the same kinds of errors (5 bad rows in about 3500). I also imported as
> much of the file as Excel would hold and cross-checked, Excel did not
> produce the same errors but can't handle the entire file. I have used
> read.table on a number of other formats (mainly csv and tab-delimited)
> without such problems; so far it appears there's something different
> about these files that produce
> s the errors but I can't see what it would be.
> 
> Does anyone have any thoughts about what is going wrong? And is there a
> way, short of manual correction, for fixing it?
> 
> Thanks for all help,
> ~Pat.
> 
> 
> Pat Carroll. 
> what matters most is how well you walk through the fire. 
> bukowski.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 16-Jul-07                                       Time: 15:59:48
------------------------------ XFMail ------------------------------


From macq at llnl.gov  Mon Jul 16 17:09:42 2007
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 16 Jul 2007 08:09:42 -0700
Subject: [R] Errors in data frames from read.table
In-Reply-To: <849314.73729.qm@web60822.mail.yahoo.com>
References: <849314.73729.qm@web60822.mail.yahoo.com>
Message-ID: <p06230901c2c1335fd53c@[128.115.153.6]>

Whenever I've had this kind of problem it has been either:
    the input data file is "corrupt", by which I mean not all lines 
have the same number of fields
    I have miss-specified one of the arguments to read.table() 
(usually comment.char or quote)

Use count.fields() on an offending file to find out if all records 
have the same number of delimiters. If they don't, then look 
carefully at the ones that don't to see how they depart from the 
assumption that all rows have the same number of delimiters. Check 
for "non-standard" characters like control character sequences.

Don't know what you mean by "read.table missing delimiter 
characters". If the delimiters are there, read.table will see them. 
But if they're inside quotes (the 'quote' argument of read.table) or 
after a comment character (the 'comment.char' argument), for example, 
I wouldn't expect them to be interpreted as delimiters.

If you were to edit one of the data files outside of R, changing the 
delimiters from tilde to something else, maybe TAB, and find that it 
reads correctly, then there might be an issue with read.table(). 
Unlikely, though.

If you can find the offending rows, put them into a separate file, 
and import them into Excel, or a text editor that shows everything, 
maybe it will become obvious.

-Don

At 7:13 AM -0700 7/16/07, Pat Carroll wrote:
>Hello, all.
>
>I am working on a project with a large (~350Mb, about 5800 rows) 
>insurance claims dataset. It was supplied in a tilde(~)-delimited 
>format. I imported it into a data frame in R by setting memory.limit 
>to maximum (4Gb) for my computer and using read.table.
>
>The resulting data frame had 10 bad rows. The errors appear due to 
>read.table missing delimiter characters, with multiple data being 
>imported into the same cell, then the remainder of the row and the 
>next run together and garbled due to the reading frame shift 
>(example: a single cell might contain: <datum>~ ~ <datum> ~<datum>, 
>after which all the cells of the row and the next are wrong).
>
>To replicate, I tried the same import procedure on a smaller 
>demographics data set from the same supplier- only about 1Mb, and 
>got the same kinds of errors (5 bad rows in about 3500). I also 
>imported as much of the file as Excel would hold and cross-checked, 
>Excel did not produce the same errors but can't handle the entire 
>file. I have used read.table on a number of other formats (mainly 
>csv and tab-delimited) without such problems; so far it appears 
>there's something different about these files that produces the 
>errors but I can't see what it would be.
>
>Does anyone have any thoughts about what is going wrong? And is 
>there a way, short of manual correction, for fixing it?
>
>Thanks for all help,
>~Pat.
>
>
>Pat Carroll.
>what matters most is how well you walk through the fire.
>bukowski.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA
925-423-1062


From yn19832 at msn.com  Mon Jul 16 17:40:43 2007
From: yn19832 at msn.com (livia)
Date: Mon, 16 Jul 2007 08:40:43 -0700 (PDT)
Subject: [R] Plot time series data
Message-ID: <11620372.post@talk.nabble.com>


Hi all, I have got a list named "data", and data[[1]] is the
Date(dd-mm-yyyy), data[[2]] is the time series data. I would like to plot
the data in the time series format with xlab be the date.I am using the
function in the library(QRMlib).

tfin <- timeSeries(data[[2]])
plot.timeSeriesIts(tfin)

How can I add some argument to make the xlab to be date accordingly?
-- 
View this message in context: http://www.nabble.com/Plot-time-series-data-tf4088285.html#a11620372
Sent from the R help mailing list archive at Nabble.com.


From johannes_graumann at web.de  Mon Jul 16 17:46:01 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Mon, 16 Jul 2007 17:46:01 +0200
Subject: [R] Pairlist of pairlsit assembly howto
In-Reply-To: <469A2A4C.9080604@stats.uwo.ca>
References: <f7d9a9$1sj$1@sea.gmane.org> <469A2A4C.9080604@stats.uwo.ca>
Message-ID: <200707161746.01786.johannes_graumann@web.de>

Thanks for the hints ... there was another issue: "minute" was integers 
starting with 0, which made R a bit upset on top of the subset/element issue.

Joh

On Sunday 15 July 2007 16:08:12 Duncan Murdoch wrote:
> (This isn't important to your question, but those aren't pairlists.
> Pairlists are rarely used in R code, except implicitly in the way R
> stores parsed code.)
>
> On 15/07/2007 10:00 AM, Johannes Graumann wrote:
> > Hy guys,
> >
> > I'm trying something like this
> >
> > minbins <- list()
> > for (minute in sequence(3)) {
> >         minbins[minute] <- list(data="a",variable="b")
> > }
>
> You want to use
>
> minbins[[minute]] <- list(data="a",variable="b")
>
> The difference between [[ ]] and [ ] is that the former works on the
> element, the latter works on a subset.  So your version tried to change
> a subset of length 1 into a subset of length 2, which generates the
> warnings.  You want to assign a list of length 2 as an element of minbins.
>
> Duncan Murdoch
>
> > And it doesn't work ...
> > Warning messages:
> > 1: number of items to replace is not a multiple of replacement length in:
> > minbins[minute] <- list(data = "a", variable = "b")
> > 2: number of items to replace is not a multiple of replacement length in:
> > minbins[minute] <- list(data = "a", variable = "b")
> > 3: number of items to replace is not a multiple of replacement length in:
> > minbins[minute] <- list(data = "a", variable = "b")
> >
> > What am I doing wrong and how to do this properly?
> >
> > Thanks, Joh
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 827 bytes
Desc: This is a digitally signed message part.
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070716/3bd205c8/attachment.bin 

From kirsten-beyer at uiowa.edu  Mon Jul 16 17:48:12 2007
From: kirsten-beyer at uiowa.edu (Kirsten Beyer)
Date: Mon, 16 Jul 2007 10:48:12 -0500
Subject: [R] summary statistics for groups
Message-ID: <e9b46960707160848y3a6052d2v998ff9914f9ff993@mail.gmail.com>

I have a list of ICD9(disease) codes and associated exposure measures.
 I am interested in a script that would calculate the mean exposure
for each ICD9 code and the combined mean for all other codes excluding
that code, then attaching those measures, and a count of how many
numbers were included in each average, to a table of unique ICD9
codes, such that the final table would look like this:

ICD9 code     ave_exposure     ICD9count     otherICD9_ave_exposure
 otherICD9_count

I have been using the gsummary function in the nlme package.

Thanks!


From abr-r-project at xylon.de  Mon Jul 16 18:14:00 2007
From: abr-r-project at xylon.de (Arne Brutschy)
Date: Mon, 16 Jul 2007 18:14:00 +0200
Subject: [R] Problem to sort factor
Message-ID: <439347026.20070716181400@xylon.de>

Hello,

I'm having a problem renaming and sorting the underlying factor of a
ggplot2 based plot. Here's my code:

---8<----------
> delta <- ggplot(subset(data, Model==c("dyn", "dl4", "dl3")), aes(x=Problemsize, y=Fitness)) +
  geom_smooth(size=1, color="black", fill=alpha("blue", 0.2))+
  geom_point(size=0.5, aes(colour=DeltaConfig))+
  scale_colour_gradient2(expression(bold(paste(Delta,"Config"))), limits=c(0,10), midpoint=5, 
                         low="green", mid="yellow", high="red")
> delta <- delta + facet_grid(Model ~ . , margins = TRUE)
> delta
---8<----------

and the data
---8<----------
> data
   Model Problemsize Fitness DeltaConfig
1    dl1           3 81.5271      2.4495
2    dl1           3 83.1999      2.4495
     ...
---8<----------

I want to select a subset of the possible models and display the
resulting three plots in a column. This works fine, but: the displayed
names and order is wrong. Instead of the models "dl3","dl4","dyn"
I want to change dl* to rm* and the order to "dyn","rm3","rm4".

I hope it's understandable what I want. I tried to use factor()
function in a thousand combinations, but I don't seem to get it. Can
someone help me?

Thanks in advance,
Arne


From szhan at uoguelph.ca  Mon Jul 16 18:22:53 2007
From: szhan at uoguelph.ca (szhan at uoguelph.ca)
Date: Mon, 16 Jul 2007 12:22:53 -0400
Subject: [R] how to estimate treatment-interaction contrasts
In-Reply-To: <4698A752.4080201@optonline.net>
References: <20070712161628.h4rqf4j48wwks444@webmail.uoguelph.ca>
	<4697AA31.403@optonline.net>
	<20070713155221.nya8t2e4cg800kg8@webmail.uoguelph.ca>
	<4698A752.4080201@optonline.net>
Message-ID: <20070716122253.16easvl103k4kwcw@webmail.uoguelph.ca>

Hello, R experts,
Could I use the command suggested by Chunk to get the estimates for  
treat-interaction contrasts (non-orthognal) which indicates below:
summary(lm(score ~ A*B))
or is there any other ways to get the estimates?
Again, Chunk, thank you for your help!
Joshua
Quoting Chuck Cleland <ccleland at optonline.net>:

> szhan at uoguelph.ca wrote:
>> Hello, Chuck,
>> Thank you very much for your help! But the contrasts I want to do
>> simutaneously is
>>   contrasts(B)
>>     [,1] [,2] [,3] [,4]
>> b1   -4   -3   -2   -1
>> b2    1   -3   -2   -1
>> b3    1    2   -2   -1
>> b4    1    2    3   -1
>> b5    1    2    3    4
>>
>> Could you please show me how to calculate estimates for ALL
>> intearaction constrasts using THESE contrasts? Say C2: c(-3, -3, 2, 2,
>> 2) as an example. I used the ortholognal constrasts as you suggest,
>> estimate for interaction contrast C2 is still -24.1.
>> Joshua
>
> Joshua:
>   I use a variation on my contrasts to show how you might get the
> estimates.  I then confirm the estimates by calculating the differences
> of differences in means "by hand".
>
> score <- c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
>            7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)
>
> A <- gl(2, 15, labels=c("a1", "a2"))
> B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)
>
> contrasts(B) <- matrix(c(3/5,  1/2,  0,  0,
>                          3/5, -1/2,  0,  0,
>                         -2/5,  0,  2/3,  0,
>                         -2/5,  0, -1/3,  1/2,
>                         -2/5,  0, -1/3, -1/2), ncol=4, byrow=TRUE)
>
> fit1 <- aov(score ~ A*B)
>
> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>             Df Sum Sq Mean Sq F value    Pr(>F)
> A            1 3.2013  3.2013 15.1483 0.0009054 ***
> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>   B: C1      1 1.0427  1.0427  4.9340 0.0380408 *
>   B: C2      1 1.0208  1.0208  4.8304 0.0399049 *
>   B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>   B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>   A:B: C1    1 3.2267  3.2267 15.2684 0.0008734 ***
>   A:B: C2    1 0.1008  0.1008  0.4771 0.4976647
>   A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>   A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
> Residuals   20 4.2267  0.2113
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> # Estimates with tests that match those above
>
> summary(lm(score ~ A*B))
>
> Call:
> lm(formula = score ~ A * B)
>
> Residuals:
>      Min       1Q   Median       3Q      Max
> -1.16667 -0.29167  0.03333  0.29167  0.73333
>
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)   6.4933     0.1187  54.705  < 2e-16 ***
> Aa2           0.6533     0.1679   3.892 0.000905 ***
> B1            0.2889     0.2423   1.192 0.247086
> B2            0.4000     0.3754   1.066 0.299271
> B3            0.1333     0.3251   0.410 0.686038
> B4           -1.5333     0.3754  -4.085 0.000577 ***
> Aa2:B1       -1.3389     0.3426  -3.907 0.000873 ***
> Aa2:B2        0.3667     0.5308   0.691 0.497665
> Aa2:B3       -1.3833     0.4597  -3.009 0.006932 **
> Aa2:B4        0.3667     0.5308   0.691 0.497665
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Residual standard error: 0.4597 on 20 degrees of freedom
> Multiple R-Squared: 0.8038,     Adjusted R-squared: 0.7156
> F-statistic: 9.107 on 9 and 20 DF,  p-value: 2.324e-05
>
> # Confirm that estimates match differences of differences in means
>
> diff(rowMeans(tapply(score, list(A,B), mean)[,1:2]) -
>      rowMeans(tapply(score, list(A,B), mean)[,3:5]))
>        a2
> -1.338889
>
> diff(tapply(score, list(A,B), mean)[,1] -
>      tapply(score, list(A,B), mean)[,2])
>        a2
> 0.3666667
>
> diff(tapply(score, list(A,B), mean)[,3] -
>      rowMeans(tapply(score, list(A,B), mean)[,4:5]))
>        a2
> -1.383333
>
> diff(tapply(score, list(A,B), mean)[,4] -
>      tapply(score, list(A,B), mean)[,5])
>        a2
> 0.3666667
>
>   So, applying the same strategy to your contrasts would give the following:
>
> contrasts(B) <- matrix(c(-4/5, -3/5, -2/5, -1/5,
>                           1/5, -3/5, -2/5, -1/5,
>                           1/5,  2/5, -2/5, -1/5,
>                           1/5,  2/5,  3/5, -1/5,
>                           1/5,  2/5,  3/5,  4/5), ncol=4, byrow=TRUE)
>
> fit1 <- aov(score ~ A*B)
>
> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>             Df Sum Sq Mean Sq F value    Pr(>F)
> A            1 3.2013  3.2013 15.1483 0.0009054 ***
> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>   B: C1      1 0.0301  0.0301  0.1424 0.7099296
>   B: C2      1 2.0335  2.0335  9.6221 0.0056199 **
>   B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>   B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>   A:B: C1    1 0.7207  0.7207  3.4105 0.0796342 .
>   A:B: C2    1 2.6068  2.6068 12.3350 0.0021927 **
>   A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>   A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
> Residuals   20 4.2267  0.2113
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> summary(lm(score ~ A*B))
>
> Call:
> lm(formula = score ~ A * B)
>
> Residuals:
>      Min       1Q   Median       3Q      Max
> -1.16667 -0.29167  0.03333  0.29167  0.73333
>
> Coefficients:
>               Estimate Std. Error   t value Pr(>|t|)
> (Intercept)  6.493e+00  1.187e-01    54.705  < 2e-16 ***
> Aa2          6.533e-01  1.679e-01     3.892 0.000905 ***
> B1          -4.000e-01  3.754e-01    -1.066 0.299271
> B2          -3.815e-16  3.754e-01 -1.02e-15 1.000000
> B3          -9.000e-01  3.754e-01    -2.398 0.026373 *
> B4           1.533e+00  3.754e-01     4.085 0.000577 ***
> Aa2:B1      -3.667e-01  5.308e-01    -0.691 0.497665
> Aa2:B2       6.000e-01  5.308e-01     1.130 0.271719
> Aa2:B3       1.567e+00  5.308e-01     2.951 0.007893 **
> Aa2:B4      -3.667e-01  5.308e-01    -0.691 0.497665
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Residual standard error: 0.4597 on 20 degrees of freedom
> Multiple R-Squared: 0.8038,     Adjusted R-squared: 0.7156
> F-statistic: 9.107 on 9 and 20 DF,  p-value: 2.324e-05
>
>   Because your contrasts are correlated, the order in which they are
> entered makes a difference.  Thus, the two summaries only agree for the
> last interaction contrast (Aa2:B4).
>   Perhaps more intuitively, you also can see that the tests are
> sensitive to order for your contrasts as follows (no output shown):
>
> # Second Contrast First
>
> anova(lm(score ~
>          A * I(B %in% c("b1","b2")) +
>                  A * I(B %in% "b1") +
>     A * I(B %in% c("b1","b2","b3")) +
>  A * I(B %in% c("b1","b2","b3","b4"))), test="F")
>
> # Second Contrast Last
>
> anova(lm(score ~ A * I(B %in% "b1") +
>     A * I(B %in% c("b1","b2","b3")) +
> A * I(B %in% c("b1","b2","b3","b4")) +
>           A * I(B %in% c("b1","b2"))), test="F")
>
>   So if you really do want to evaluate your correlated contrasts
> simultaneously, what makes the most sense to me is:
>
> summary(lm(score ~ A*B))
>
>   I would be interested in other people's thoughts on this, particularly
> how to use something like estimable() in the gmodels package to achieve
> these contrasts.
>
> hope this helps,
>
> Chuck
>
>> Quoting Chuck Cleland <ccleland at optonline.net>:
>>
>>> szhan at uoguelph.ca wrote:
>>>> Hello, R experts,
>>>> Sorry for asking this question again again since I really want a help!
>>>>
>>>> I have a two-factor experiment data and like to calculate estimates of
>>>> interation contrasts say factor A has levels of a1, a2, and B has
>>>> levels of b1, b2, b3, b4, and b5 with 3 replicates. I am not sure the
>>>> constrast estimate I got is right using the script below:
>>>>
>>>> score<-c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
>>>> 7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)
>>>>
>>>> A <- gl(2, 15, labels=c("a1", "a2"))
>>>> B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)
>>>>
>>>> contrasts(B)<-cbind(c(-4,rep(1,4)),c(rep(-3,2),rep(2,3)),
>>>> +  c(rep(-2,3),rep(3,2)),c(rep(-1,4), rep(4,1)))
>>>> fit1 <- aov(score ~ A*B)
>>>> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>>>>                Df Sum Sq Mean Sq F value    Pr(>F)
>>>> A            1 3.2013  3.2013 15.1483 0.0009054 ***
>>>> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>>>>      B: C1      1 0.0301  0.0301  0.1424 0.7099296
>>>>      B: C2      1 2.0335  2.0335  9.6221 0.0056199 **
>>>>      B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>>>>      B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
>>>> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>>>>      A:B: C1    1 0.7207  0.7207  3.4105 0.0796342 .
>>>>      A:B: C2    1 2.6068  2.6068 12.3350 0.0021927 **
>>>>      A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>>>>      A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
>>>> Residuals   20 4.2267  0.2113
>>>> ---
>>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>>
>>>> Now I like to get interaction contrast estimate for b1 and b2 vs
>>>> b3, b4 and b5
>>>> cont <- c(1, -1)[A] * c(-3, -3, 2, 2, 2)[B]
>>>>
>>>> estimat<-sum(cont*score) # value of the contrast estimate for A:B C2
>>>>
>>>>> estimat
>>>> [1] -24.1
>>>>
>>>> I am not sure the estimate for A:B C2 contrast  (-24.1) is correct
>>>> because the F value given the output above(12.3350) does not equal to
>>>> those I calculate below (15.2684):
>>>>
>>>> t.stat <- sum(cont*score)/se.contrast(fit1, as.matrix(cont))
>>>>> t.stat^2
>>>> Contrast 1
>>>>      15.2684
>>>>
>>>> Could you please help me calculate the correct the estimate of
>>>> interaction contrast and corresponding F value?
>>>> Thanks in advance!
>>>> Joshua
>>>   If the contrasts for B are orthogonal, then you get the result you
>>> expected:
>>>
>>> score <- c(7.2,6.5,6.9,6.4,6.9,6.1,6.9,5.3,7.2,5.7,5.1,5.9,7.6,6.9,6.8,
>>>            7.2,6.6,6.9,6.4,6.0,6.0,6.9,6.9,6.4,7.5,7.7,7.0,8.6,8.8,8.3)
>>>
>>> A <- gl(2, 15, labels=c("a1", "a2"))
>>> B <- rep(gl(5, 3, labels=c("b1", "b2", "b3", "b4", "b5")), 2)
>>>
>>> contrasts(B) <- matrix(c(3, -1,  0,  0,
>>>                          3,  1,  0,  0,
>>>                         -2,  0,  2,  0,
>>>                         -2,  0, -1,  1,
>>>                         -2,  0, -1, -1), ncol=4, byrow=TRUE)
>>>
>>> fit1 <- aov(score ~ A*B)
>>>
>>> summary(fit1, split=list(B=1:4), expand.split = TRUE)
>>>
>>>             Df Sum Sq Mean Sq F value    Pr(>F)
>>> A            1 3.2013  3.2013 15.1483 0.0009054 ***
>>> B            4 8.7780  2.1945 10.3841 0.0001019 ***
>>>   B: C1      1 1.0427  1.0427  4.9340 0.0380408 *
>>>   B: C2      1 1.0208  1.0208  4.8304 0.0399049 *
>>>   B: C3      1 1.2469  1.2469  5.9004 0.0246876 *
>>>   B: C4      1 5.4675  5.4675 25.8715 5.637e-05 ***
>>> A:B          4 5.3420  1.3355  6.3194 0.0018616 **
>>>   A:B: C1    1 3.2267  3.2267 15.2684 0.0008734 ***
>>>   A:B: C2    1 0.1008  0.1008  0.4771 0.4976647
>>>   A:B: C3    1 1.9136  1.9136  9.0549 0.0069317 **
>>>   A:B: C4    1 0.1008  0.1008  0.4771 0.4976647
>>> Residuals   20 4.2267  0.2113
>>> ---
>>> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>>>
>>>   Note that I put the contrast of interest for B in the first column of
>>> the contrast matrix.
>>>
>>> hope this helps,
>>>
>>> Chuck
>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide   
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> --
>>> Chuck Cleland, Ph.D.
>>> NDRI, Inc.
>>> 71 West 23rd Street, 8th floor
>>> New York, NY 10010
>>> tel: (212) 845-4495 (Tu, Th)
>>> tel: (732) 512-0171 (M, W, F)
>>> fax: (917) 438-0894
>>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 512-0171 (M, W, F)
> fax: (917) 438-0894
>


From Greg.Snow at intermountainmail.org  Mon Jul 16 18:35:14 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 16 Jul 2007 10:35:14 -0600
Subject: [R] Compute rank within factor groups
In-Reply-To: <C2BC0C30.10BDF%ken.williams@thomson.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAADAB7@LP-EXCHVS07.CO.IHC.COM>

The order and rank functions do something of an inverse of each other.
The rank function tells you the rank of each element of the vector (if
the first element is the 2nd smallest, then the first element of the
return in 2).  The order function tells you what order to put the vector
in to sort it (if the smallest element of the vector is in position 3,
then the first element of the returned vector will be 3).  Doing
something like:

> order(order(x))

Is similar to rank, except in how it deals with ties.

When the data is presorted, then rank and order both give you the same
as seq(along=x) which is just the set of integers from 1 to the length
of the vector.  

Most computations in R will switch between integer and double
automatically, but if you really need a vector to be integer, then use
the as.integer function.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: Ken Williams [mailto:ken.williams at thomson.com] 
> Sent: Thursday, July 12, 2007 3:50 PM
> To: Greg Snow; R-help at stat.math.ethz.ch
> Subject: Re: [R] Compute rank within factor groups
> 
> 
> 
> 
> On 7/12/07 4:28 PM, "Greg Snow" 
> <Greg.Snow at intermountainmail.org> wrote:
> 
> > Why are you using order instead of rank?
> > 
> > If the data is pre sorted then they tend to give the same result 
> > (unless there are ties), but if your data is not presorted, 
> then the 
> > results will be different.
> 
> Indeed, thanks for the catch.  I switched to order because 
> rank was giving me floats instead of integers, which I now 
> see was probably because it defaults to ties.method=average 
> and I wanted ties.method=first.
> 
> My data was indeed pre-sorted so I didn't notice the 
> difference (are they giving inverse permutations or 
> something? Can't quite follow...), but perhaps it won't always be.
> 
> 
> --
> Ken Williams
> Research Scientist
> The Thomson Corporation
> Eagan, MN
> 
>


From yn19832 at msn.com  Mon Jul 16 18:47:51 2007
From: yn19832 at msn.com (livia)
Date: Mon, 16 Jul 2007 09:47:51 -0700 (PDT)
Subject: [R] Time Series Data
Message-ID: <11621776.post@talk.nabble.com>


Hi all, I have got some time series data. Data[[1]] is the data in the format
"1975-12-05 1975-12-12 1975-12-19...", data[[2]] is the time series data. I
would like to generate the time series format as 
1975-12-05  1.5
1975-12-12  2.3etc.

I am thinking about cbind(data[[1]],data[[2]]), but it results in 
      [,1]          [,2]
  [1,]    1     1.5
  [2,]    2     2.3

Could anyone give me some advice?
-- 
View this message in context: http://www.nabble.com/Time-Series-Data-tf4088688.html#a11621776
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Mon Jul 16 18:52:19 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 16 Jul 2007 12:52:19 -0400
Subject: [R] Time Series Data
In-Reply-To: <11621776.post@talk.nabble.com>
References: <11621776.post@talk.nabble.com>
Message-ID: <971536df0707160952v3c8b3603k1e792b457fbd813c@mail.gmail.com>

Check out the zoo package:

Lines <- "1975-12-05  1.5
1975-12-12  2.3
"

library(zoo)
# replace next line with z <- read.zoo("myfile.dat")
z <- read.zoo(textConnection(Lines))
plot(z)
z

vignette("zoo") # gives more info

On 7/16/07, livia <yn19832 at msn.com> wrote:
>
> Hi all, I have got some time series data. Data[[1]] is the data in the format
> "1975-12-05 1975-12-12 1975-12-19...", data[[2]] is the time series data. I
> would like to generate the time series format as
> 1975-12-05  1.5
> 1975-12-12  2.3etc.
>
> I am thinking about cbind(data[[1]],data[[2]]), but it results in
>      [,1]          [,2]
>  [1,]    1     1.5
>  [2,]    2     2.3
>
> Could anyone give me some advice?
> --
> View this message in context: http://www.nabble.com/Time-Series-Data-tf4088688.html#a11621776
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Achim.Zeileis at wu-wien.ac.at  Mon Jul 16 18:59:13 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 16 Jul 2007 18:59:13 +0200 (CEST)
Subject: [R] Time Series Data
In-Reply-To: <11621776.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.44.0707161858070.7607-100000@disco.wu-wien.ac.at>

On Mon, 16 Jul 2007, livia wrote:

>
> Hi all, I have got some time series data. Data[[1]] is the data in the format
> "1975-12-05 1975-12-12 1975-12-19...", data[[2]] is the time series data. I
> would like to generate the time series format as
> 1975-12-05  1.5
> 1975-12-12  2.3etc.
>
> I am thinking about cbind(data[[1]],data[[2]]), but it results in
>       [,1]          [,2]
>   [1,]    1     1.5
>   [2,]    2     2.3
>
> Could anyone give me some advice?

Have a look at the "zoo" package and its vignettes:
  vignette("zoo-quickref", package = "zoo")
  vignette("zoo", package = "zoo")

hth,
Z

> --
> View this message in context: http://www.nabble.com/Time-Series-Data-tf4088688.html#a11621776
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From andrea.johnson at roche.com  Mon Jul 16 19:19:01 2007
From: andrea.johnson at roche.com (Johnson, Andrea)
Date: Mon, 16 Jul 2007 10:19:01 -0700
Subject: [R] FW:  summary statistics for groups
Message-ID: <8697393B889CD74C99B2E7705652378101373B31@rpbmsem01.nala.roche.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/5e24e5f4/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Jul 16 19:23:55 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Jul 2007 18:23:55 +0100 (BST)
Subject: [R] Errors in data frames from read.table
In-Reply-To: <849314.73729.qm@web60822.mail.yahoo.com>
References: <849314.73729.qm@web60822.mail.yahoo.com>
Message-ID: <Pine.LNX.4.64.0707161815530.9119@gannet.stats.ox.ac.uk>

On Mon, 16 Jul 2007, Pat Carroll wrote:

> Hello, all.
>
> I am working on a project with a large (~350Mb, about 5800 rows) 
> insurance claims dataset. It was supplied in a tilde(~)-delimited 
> format. I imported it into a data frame in R by setting memory.limit to 
> maximum (4Gb) for my computer and using read.table.
>
> The resulting data frame had 10 bad rows. The errors appear due to 
> read.table missing delimiter characters, with multiple data being 
> imported into the same cell, then the remainder of the row and the next 
> run together and garbled due to the reading frame shift (example: a 
> single cell might contain: <datum>~ ~ <datum> ~<datum>, after which all 
> the cells of the row and the next are wrong).
>
> To replicate, I tried the same import procedure on a smaller 
> demographics data set from the same supplier- only about 1Mb, and got 
> the same kinds of errors (5 bad rows in about 3500). I also imported as 
> much of the file as Excel would hold and cross-checked, Excel did not 
> produce the same errors but can't handle the entire file. I have used 
> read.table on a number of other formats (mainly csv and tab-delimited) 
> without such problems; so far it appears there's something different 
> about these files that produces the errors but I can't see what it would 
> be.

The usual cause is that the user forgot about quotes and comment 
characters.  Try quote="", comment.char=""

If that does not work, please follow the request in the footer of every 
message on this list.

> Does anyone have any thoughts about what is going wrong? And is there a 
> way, short of manual correction, for fixing it?
>
> Thanks for all help,
> ~Pat.
>
>
> Pat Carroll.
> what matters most is how well you walk through the fire.
> bukowski.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Greg.Snow at intermountainmail.org  Mon Jul 16 19:29:24 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 16 Jul 2007 11:29:24 -0600
Subject: [R] correlation matrix difference
In-Reply-To: <11578046.post@talk.nabble.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAADAF1@LP-EXCHVS07.CO.IHC.COM>

One approach would be to use a permutation test. 

Lets start with a simpler case:  x1 and y1 are 2 variables measured
under condition 1 and x2 and y2 are the same variables measured under
condition 2.  The null hypothesis is that conditions 1 and 2 make no
differences on the measurements (same mean, structure, etc.)

Then the procedure is:

1. Calculate the difference in the 2 observed correlations (I'm using
the difference here, a different statistic could be used).

2. permute the pairs between the 2 groups (keeping an x,y pair together
and the group sizes the same)
3. calculate the difference in the 2 observed correlations between the
permuted groups

4. repeate steps 2 and 3 a bunch of times (999 works nicely).

The p-value is the number of times that the permuted difference in
correlations is larger in absolute value than the difference from step 1

Here is some code that shows a couple of example of doing this (1 has
the null true, 2 has the null false):

library(MASS)

d1 <- mvrnorm(25, mu=c(5,5), Sigma= matrix( c(1,.9,.9,1), 2) )
d2 <- mvrnorm(30, mu=c(5,5), Sigma= matrix( c(1,.9,.9,1), 2) )
d3 <- mvrnorm(30, mu=c(5,5), Sigma= matrix( c(1,.7,.7,1), 2) )

mymat1 <- rbind(d1,d2)
mymat2 <- rbind(d1,d3)

ts1 <- cor(d1[,1],d1[,2]) - cor(d2[,1],d2[,2])
ts2 <- cor(d1[,1],d1[,2]) - cor(d3[,1],d3[,2])

out1 <- replicate(999, {tmp <- mymat1[ sample(55), ];
		cor(tmp[1:25,1],tmp[1:25,2]) -
cor(tmp[26:55,1],tmp[26:55,2])
	})

out2 <- replicate(999, {tmp <- mymat2[ sample(55), ];
		cor(tmp[1:25,1],tmp[1:25,2]) -
cor(tmp[26:55,1],tmp[26:55,2])
	})

out1 <- c(out1,ts1)
out2 <- c(out2,ts2)

hist(out1)
abline(v=ts1)
mean( abs(out1) >= abs(ts1) ) # p-value

hist(out2)
abline(v=ts2)
mean( abs(out2) >= abs(ts2) ) # p-value


With more than 2 variables you have more than 1 correlation per group.
We can expand the above idea, but need to think through how you want to
do it.  A couple of possibilities is to use the average difference in
the correlations, or the maximum absolute value of the differences.
Then do the same as above.

With 4 groups instead of just 2, you can either look at the groups
pairwise, or look at maximum or average differences between the 4
groups.

I don't know how powerful these tests are (or which stat will give the
most power), but they are valid under a null hypothesis of all the
groups being equal.

Hope this helps,




-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of livia
> Sent: Friday, July 13, 2007 6:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] correlation matrix difference
> 
> 
> Hi, I have got four correlation matrix. They are the same set 
> of variables under different conditions. Is there a way to 
> test whether the correlation matrix are significently 
> different among each other? Could anyone give me some advice?
> --
> View this message in context: 
> http://www.nabble.com/correlation-matrix-difference-tf4073868.
> html#a11578046
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pensterfuzzer at yahoo.de  Mon Jul 16 19:38:18 2007
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Mon, 16 Jul 2007 19:38:18 +0200 (CEST)
Subject: [R] Modifying Regression Coefficients
Message-ID: <964299.93151.qm@web23005.mail.ird.yahoo.com>

Hi,

This is probably trivial but I haven't discovered how
it works: 
I have estimated a regression model using glm() and
now I want to modify a coefficient of that model and
use the model object for predictions. How can I modify
such a coefficient?

Many thanks,
  Werner


      __________________________________  Wissenswertes f?r Bastler und Hobby Handwerker. BE A BETTER HEIMWERKER! www.yahoo.de/clever


From chipmaney at hotmail.com  Mon Jul 16 20:39:17 2007
From: chipmaney at hotmail.com (Chip Maney)
Date: Mon, 16 Jul 2007 18:39:17 +0000
Subject: [R] Basic missing values question...
Message-ID: <BAY142-F33B4F2BAA644E96CDE17FBA3F80@phx.gbl>

I am trying to run the power.t.test function to calculate sample size for a 
data frame of variances (i.e, 228 rows x 4 cols).  Many of the fields are 
missing values, which crashes the function.  I have tried na. options in the 
function, but the function doesn't recognize them.  I have also tried an 
if-else loop to skip over fields with missing values.  How can I deal with 
these missing values?

Here's the code:

var.df<-read.table("Transect_variance.csv",sep=",",header=TRUE, fill=TRUE)
n.df<-data.frame(cbind(var.df[,1:2],rep(-1,228),rep(-1,228),rep(-1,228),rep(-1,228)))
	names(n.df)<-c("Strata","Transect","n35","n45","n36","n46")

#var.df[1:2,]
#	Strata 	Transect      	 ID		Var35    	Var45 		Var36    	Var46
#	Herb    	3-1 		Herb 3-1 	2.024264      	NA 		7.182       	NA
#	Herb    	4-1 		Herb 4-1 	2.232552 	2.26 		2.360 		2.393387

for i in 1:6 {
	for j in 1:228 {
			if ( var.df[i,j]=="") {n.df[i,j]==""  } else {
			power.t.test(delta = 2.5, sd = var.df[i,j], sig.level = 0.1, power = 0.9, 
type = "two.sample", alternative = "two.sided")
			}

	}
}


From r.nieuwenhuis at student.ru.nl  Mon Jul 16 20:42:31 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Mon, 16 Jul 2007 20:42:31 +0200
Subject: [R] how do I draw such a barplot?
In-Reply-To: <200707161006.58676.dgvirtual@akl.lt>
References: <200707161006.58676.dgvirtual@akl.lt>
Message-ID: <9F6EBD91-A6E5-44D3-AC12-B5FBFF89652E@student.ru.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/608af60d/attachment.pl 

From drcarbon at gmail.com  Mon Jul 16 20:48:58 2007
From: drcarbon at gmail.com (Dr Carbon)
Date: Mon, 16 Jul 2007 11:48:58 -0700
Subject: [R] Spline - frequency response
Message-ID: <e89bb7ac0707161148v7ec28733uf0d1beef47be3377@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/65be950e/attachment.pl 

From Greg.Snow at intermountainmail.org  Mon Jul 16 20:51:56 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 16 Jul 2007 12:51:56 -0600
Subject: [R] Question about acception rejection sampling - NOT R question
In-Reply-To: <D3AEEDA31E57474B840BEBC25A8A834401957566@NYWEXMB23.msad.ms.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAADB31@LP-EXCHVS07.CO.IHC.COM>

Not a strict proof, but think of it this way:

The liklihood of getting a particular value of x has 2 parts.  1st x has
to be generated from h, the liklihood of this happening is h(x), 2nd the
point has to be accepted with conditional probability f(x)/(c*h(x)).  If
we multiply we get h(x) * f(x)/ ( c* h(x) ) and the 2 h(x)'s cancel out
leaving the liklihood of getting x as f(x)/c.  The /c just indicates
that approximately 1-1/c points will be rejected and thrown out and the
final normalized distribution is f(x), which was the goal.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Leeds, 
> Mark (IED)
> Sent: Friday, July 13, 2007 2:45 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Question about acception rejection sampling - 
> NOT R question
> 
> This is not related to R but I was hoping that someone could 
> help me. I am reading the "Understanding the Metropolis 
> Hastings Algorithm"
> paper from the American Statistician by Chip and Greenberg, 
> 1995, Vol 49, No 4. Right at the beginning they explain the 
> algorithm for basic acceptance  rejection sampling in which 
> you want to simulate a density from f(x) but it's not easy 
> and you are able to generate from another density called 
> h(x). The assumption is that there exists some c such that 
> f(x) <= c(h(x) for all x
> 
> They clearly explain the steps as follows :
> 
> 1) generate Z from h(x).
> 
> 2) generate u from a Uniform(0,1)
> 
> 3) if u is less than or equal to f(Z)/c(h(Z) then return Z as 
> the generated RV; otherwise reject it and try again.
> 
> I think that, since f(Z)/c(h(z) is U(0,1), then u has the 
> distrbution as f(Z)/c(h(Z).
>  
> But, I don't understand why the generated and accepted Z's 
> have the same density  as f(x) ?
> 
> Does someone know where there is a proof of this or if it's 
> reasonably to explain, please feel free to explain it.
> They authors definitely believe it's too trivial because they 
> don't. The reason I ask is because, if I don't understand 
> this then I definitely  won't understand the rest of the 
> paper because it gets much more complicated.  I willing to 
> track down the proof but I don't know where to look. Thanks.
> --------------------------------------------------------
> 
> This is not an offer (or solicitation of an offer) to 
> buy/se...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From h.wickham at gmail.com  Mon Jul 16 21:30:38 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 16 Jul 2007 21:30:38 +0200
Subject: [R] Different axis limits for each facet in ggplot2
In-Reply-To: <f7fmt9$ide$1@sea.gmane.org>
References: <f7fmt9$ide$1@sea.gmane.org>
Message-ID: <f8e6ff050707161230q4d8ede79sd472d1e5b11558af@mail.gmail.com>

On 7/16/07, Karl Ove Hufthammer <Karl.Hufthammer at math.uib.no> wrote:
> Hi!
>
> Is it possible to have different axis limit for each facet in a ggplot2
> plot? Here is an example:

Not yet, although it is on the to do list.

> --------------------------------------------------------------
> library(ggplot2)
>
> x=seq(-10,10,.1)
> y=cos(x)
> z=sin(x)*10

One crude way to get around it is:

df <- data.frame(x,y,z)
df <- rescaler(df)

 - ie. scale all variables to common scales

Hadley


From massimiliano.talarico at poste.it  Mon Jul 16 22:49:32 2007
From: massimiliano.talarico at poste.it (massimiliano.talarico)
Date: Mon, 16 Jul 2007 22:49:32 +0200
Subject: [R] Optimization
Message-ID: <JLAHUK$B02D959B066823E595F53D454A1998AA@poste.it>

Dear all,
I need a suggest to obtain the max of this function:

Max x1*0.021986+x2*0.000964+x3*0.02913

with these conditions:

x1+x2+x3=1;
radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
x1>=0;
x1<=1;
x2>=0;
x2<=1;
x3>=0;
x3<=1;

Any suggests ?

Thanks in advanced,
Massimiliano


From albmont at centroin.com.br  Tue Jul 17 00:11:05 2007
From: albmont at centroin.com.br (Alberto Monteiro)
Date: Mon, 16 Jul 2007 20:11:05 -0200
Subject: [R] Source inside source
Message-ID: <20070716220752.M23227@centroin.com.br>

Is there a way to know where is the source, so as to make a source call 
inside another source smarter?

As an example:

file1.R is in directory /files/dir1/

file2.R is in directory /files/dir1/dir2/

In file1.R, there is this line:

source("dir2/file2.R")

So, if I setwd to /files/dir1/, and then I call source("file1.R"),
it will run correctly. However, if I setwd to /files, then
call source("dir1/file1.R"), it will give an error when 
trying to source file2.R

Alberto Monteiro


From smpowers at wisc.edu  Tue Jul 17 00:11:31 2007
From: smpowers at wisc.edu (Steve Powers)
Date: Mon, 16 Jul 2007 17:11:31 -0500
Subject: [R] print tables to figure in R
Message-ID: <469BED13.50703@wisc.edu>

Doe anyone know of a way to print nice-looking, printer-friendly tables 
directly in the R plot window? This would be very handy for examining 
preliminary results from frequently used routines. I've hunted for this 
with no luck. All anyone seems to do is export text files to excel for 
formatting, or use the print(bla) function which just calls the numbers 
at the command line. There must be a way to do this.---steve


From bcarvalh at jhsph.edu  Tue Jul 17 00:19:29 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 16 Jul 2007 18:19:29 -0400
Subject: [R] Source inside source
In-Reply-To: <20070716220752.M23227@centroin.com.br>
References: <20070716220752.M23227@centroin.com.br>
Message-ID: <D95E794E-4D0E-4D7B-9A7B-A5B65E1473C1@jhsph.edu>

the smarter thing is to write a package, so you don't need source()  
at all.

but your problem will be fixed if you use the full path instead:

source("/home/user/dir1/dir2/file2.R")

(and obviously you could pass the path as an argument to whatever  
function you're using...)

b

On Jul 16, 2007, at 6:11 PM, Alberto Monteiro wrote:

> Is there a way to know where is the source, so as to make a source  
> call
> inside another source smarter?
>
> As an example:
>
> file1.R is in directory /files/dir1/
>
> file2.R is in directory /files/dir1/dir2/
>
> In file1.R, there is this line:
>
> source("dir2/file2.R")
>
> So, if I setwd to /files/dir1/, and then I call source("file1.R"),
> it will run correctly. However, if I setwd to /files, then
> call source("dir1/file1.R"), it will give an error when
> trying to source file2.R
>
> Alberto Monteiro
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From joe_retzer at yahoo.com  Tue Jul 17 00:23:24 2007
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Mon, 16 Jul 2007 15:23:24 -0700 (PDT)
Subject: [R] randomForest impotance problem with combine
In-Reply-To: <f8e6ff050707161230q4d8ede79sd472d1e5b11558af@mail.gmail.com>
Message-ID: <676546.48813.qm@web60314.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/e386bfa3/attachment.pl 

From duncan at wald.ucdavis.edu  Tue Jul 17 00:46:35 2007
From: duncan at wald.ucdavis.edu (Duncan Temple Lang)
Date: Mon, 16 Jul 2007 15:46:35 -0700
Subject: [R] Source inside source
In-Reply-To: <20070716220752.M23227@centroin.com.br>
References: <20070716220752.M23227@centroin.com.br>
Message-ID: <469BF54B.80006@wald.ucdavis.edu>


There is a package available at

 http://www.omegahat.org/Prerelease/RSource_0.1-1.tar.gz

that has an extended source() that maintains a list of directories
in which to search for files and a stack of the files currently
being source()d so that one can determine what is currently going on.

It has extra stuff in there for dealing with encrypted files
and also some experiments on I/O using C code which you may not need or
want installed.

 D.

Alberto Monteiro wrote:
> Is there a way to know where is the source, so as to make a source call 
> inside another source smarter?
> 
> As an example:
> 
> file1.R is in directory /files/dir1/
> 
> file2.R is in directory /files/dir1/dir2/
> 
> In file1.R, there is this line:
> 
> source("dir2/file2.R")
> 
> So, if I setwd to /files/dir1/, and then I call source("file1.R"),
> it will run correctly. However, if I setwd to /files, then
> call source("dir1/file1.R"), it will give an error when 
> trying to source file2.R
> 
> Alberto Monteiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tura at centroin.com.br  Tue Jul 17 01:02:00 2007
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Mon, 16 Jul 2007 20:02:00 -0300
Subject: [R] Flow Cytometry Standard, fcs format in R.
In-Reply-To: <90e13c0707131206td4f430o3069da8e14b656dc@mail.gmail.com>
References: <90e13c0707131206td4f430o3069da8e14b656dc@mail.gmail.com>
Message-ID: <1184626920.32207.13.camel@R3-Thux>

On Fri, 2007-07-13 at 16:06 -0300, Horacio Castellini wrote:
> Hi all.
> How do I extract date from fcs format file with R. I.e I'd like
> make statistical analysis using R-program, but I don't know if there
> are R-packets for fcs format file, and using examples.
> 
> Thanks.

Hi Horacio!

Is possible using rflowcyt or prada available in
http://www.bioconductor.org

In Rnews have article about this:

http://cran.r-project.org/doc/Rnews/Rnews_2006-5.pdf

Bernardo Rangel Tura, MD, Ph.D
National Institute of Cardiology
Rio de Janeiro - Brazil


From joe_retzer at yahoo.com  Tue Jul 17 01:10:43 2007
From: joe_retzer at yahoo.com (Joseph Retzer)
Date: Mon, 16 Jul 2007 16:10:43 -0700 (PDT)
Subject: [R] randomForest importance problem with combine
In-Reply-To: <676546.48813.qm@web60314.mail.yahoo.com>
Message-ID: <615499.60887.qm@web60313.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/66f9b432/attachment.pl 

From avman21 at yahoo.com  Tue Jul 17 01:32:21 2007
From: avman21 at yahoo.com (Joe Swiatek)
Date: Mon, 16 Jul 2007 16:32:21 -0700 (PDT)
Subject: [R] Question about Network library
Message-ID: <866434.44913.qm@web81207.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070716/07ad1e18/attachment.pl 

From vincent.goulet at act.ulaval.ca  Tue Jul 17 01:48:51 2007
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Mon, 16 Jul 2007 19:48:51 -0400
Subject: [R] question about ar1 time series
In-Reply-To: <98e024570707160650n2f720dc6p8be014a08ca63e49@mail.gmail.com>
References: <98e024570707160650n2f720dc6p8be014a08ca63e49@mail.gmail.com>
Message-ID: <F1E95FD2-CF78-49A3-A71F-35750B3E8F44@act.ulaval.ca>

Le 07-07-16 ? 09:50, Josu? Polanco a ?crit :

> Hello everybody,
>
> I recently wrote a "program" that to generate AR1 time series, here  
> the code:
>
>
> #By Jomopo. Junio-2007, Leioa, Vizcaya
> #This program to create the AR1 syntetic series (one by one)
> #Where the mean is zero, but the variance of the serie AR1 and
> #the coef. of AR1 are be changed. If var serie AR1 = 1 then is  
> standarized!
> #Final version for AR1 time series program
> #Mon Jul 16 11:58:03 CEST 2007 Checked again in R-prompt, and it's OK!
>
> #Creating the sintetic AR1 series... where the "white-noise"
> #has a mean = 0, and the var = sigmaz_c = stand_dev^2 is whatever  
> value,
> #if sigmaz_c = 1 then this "white-noise" is a "Gaussian-noise."
> #rho1 (or alpha in another text-books ;-)) < 1 (in fact 0 < rho1 <  
> 1) so that
> #the system can be stationary.
> #Where var_serie is the variance of the serie
>
> cat("\n Hello, this is creat_AR1_synt_ser.R. \n These are the input
> parameters: synt_series(ar1_length, rho1, ...), where rho1 is the
> correlat. coef.\n")
>
>
> ar1 <- function(x, rho1, af)
> {
>    return(x*rho1 + runif(1, -af, af))
> }
>
> #Spin-up for the AR1 series. For this case is enough with this amount
> spinup <- function(x0, rho1, af)
> {
>    xt <- x0
>    for (i in 1:100) {
>      xtp1 <- ar1(xt, rho1, af)
>      xt   <- xtp1
>    }
>    return(xt)
> }
>
> #Wherein "ar1_length" is the number of data in AR1 series
> #rho1 is a correlation coef.
> #sigmaz_c is optional
> synt_series <- function(ar1_length, rho1, var_serie)
> {
>    if( (var_serie <= 0) || rho1 <= -1 || rho1 >= 1 )
>      stop("The variance of the serie should be > 0, or the rho1
> parameter should be between (-1, 1) for that the serie can be
> stationary, be careful with this, bye. \n")
>
>    syntdata    <- rep(0, ar1_length)
> #af = adjustement factor, i.e. for that the var of random numbers =
> var of white noise (check the manual of runif)
>    af          <- sqrt( 3 * var_serie * (1 - rho1) * (1 + rho1) )
>    x0          <- runif(1, -af, af)
>    syntdata[1] <- spinup(x0, rho1, af)
>
>    for (i in 2:ar1_length) {
>      syntdata[i]  <- ar1(syntdata[i - 1], rho1, af)
>    }
>    return(syntdata)
> }
>
>
> I would like some suggestions and hints.

Here's one: look at arima.sim() and ease your life a lot. ;-)

>
> Thanks a lot for your help!
>
> -- 
> Josu? Mos?s Polanco Mart?nez
> Correo-e alternativo jomopo at linuxmail.org
> ----
> It is a wasted day unless you have learned something new and made
> someone smile -Mark Weingartz.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

---
   Vincent Goulet, Associate Professor
   ?cole d'actuariat
   Universit? Laval, Qu?bec
   Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca


From john.maindonald at anu.edu.au  Tue Jul 17 01:49:53 2007
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 17 Jul 2007 09:49:53 +1000
Subject: [R] LSD, HSD,...
In-Reply-To: <mailman.8.1184580003.27115.r-help@stat.math.ethz.ch>
References: <mailman.8.1184580003.27115.r-help@stat.math.ethz.ch>
Message-ID: <99378582-DD71-4033-AE98-1002C7166A65@anu.edu.au>

<follow-on rant> Stepwise regression variable selection
methods make multiple post hoc comparisons.  The
number of comparisons may be very large, vastly more
than the half-dozen post-hoc comparisons that are
common in an experimental design context.

There is a disconnect here. The multiple testing issue is
noted in pretty much every discussion of analysis of
experimental data, but not commonly mentioned (at least
in older texts) in discussions of stepwise regression, best
subsets and related regression approaches. One reason
for this silence may be that there is no ready HSD-like fix.

The SEs and t-statistics that lm() gives for the finally
selected model can be grossly optimistic. Running the
analysis with the same model matrix, but with y-values
that are noise, can give a useful wake-up call.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Mathematics & Its Applications, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 16 Jul 2007, at 8:00 PM, Simon Blomberg wrote:

> If you have a priori planned comparisons, you can just test those  
> using
> linear contrasts, with no need to correct for multiple testing. If you
> do not, and you are relying on looking at the data and analysis to  
> tell
> you which treatment means to compare, and you are considering several
> tests, then you should consider correcting for multiple testing. There
> is a large literature on the properties of the various tests.  
> (Tukey HSD
> usually works pretty well for me.)
>
> <rant> Why do people design experiments with a priori hypotheses in
> mind, yet test them using post hoc comparison procedures? It's as if
> they are afraid to admit that they had hypotheses to begin with! Far
> better to test what you had planned to test using the more powerful
> methods for planned comparisons, and leave it at that.
> </rant>
>
>
> On Mon, 2007-07-16 at 09:52 +0200, Adrian J. Montero Calvo wrote:
>> Hi,
>>     I'm designing a experiment in order to compare the growing of
>> several clones of a tree specie. It will be a complete randomized  
>> block
>> design. How can I decide what model of mean comparision to choose?  
>> LSD,
>> HSD,TukeyHSD,  Duncan,...?  Thanks in advance
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> -- 
> Simon Blomberg, BSc (Hons), PhD, MAppStat.
> Lecturer and Consultant Statistician
> Faculty of Biological and Chemical Sciences
> The University of Queensland
> St. Lucia Queensland 4072
> Australia
> Room 320 Goddard Building (8)
> T: +61 7 3365 2506
> email: S.Blomberg1_at_uq.edu.au


From jacob.etches at utoronto.ca  Tue Jul 17 03:34:15 2007
From: jacob.etches at utoronto.ca (Jacob Etches)
Date: Mon, 16 Jul 2007 21:34:15 -0400
Subject: [R] problem with length()
Message-ID: <8E32AFC4-112B-409C-8B82-966EDB75561D@utoronto.ca>

In the following, can anyone tell me why length(eee) returns 9?  I  
was expecting 15398, and when I try to add this vector to a data  
frame with that many rows, it fails complaining that the vector is of  
length 9.  In what I thought was an identical situation with a  
related dataset, the same code worked as expected.

 > length(fff)
[1] 15398
 > str(fff)
int [1:15398] 20010102 20010102 20010102 20010103 20010103 20010102  
20010102 20010104 20010103 20010102 ...
 > fff[1:12]
[1] 20010102 20010102 20010102 20010103 20010103 20010102 20010102  
20010104 20010103 20010102 20010105 20010103
 > eee <- as.POSIXlt(strptime(fff,"%Y%m%d"))
 > length(eee)
[1] 9
 > eee[1:12]
[1] "2001-01-02" "2001-01-02" "2001-01-02" "2001-01-03" "2001-01-03"  
"2001-01-02" "2001-01-02" "2001-01-04" "2001-01-03" "2001-01-02"  
"2001-01-05" "2001-01-03"
 > str(eee)
'POSIXlt', format: chr [1:15398] "2001-01-02" "2001-01-02"  
"2001-01-02" "2001-01-03" "2001-01-03" "2001-01-02" "2001-01-02"  
"2001-01-04" "2001-01-03" ...



Many thanks in advance,
Jacob Etches


Doctoral candidate, Epidemiology Program
Department of Public Health Sciences, University of Toronto Faculty  
of Medicine

Research Associate
Institute for Work & Health
800-481 University Avenue, Toronto, Ontario, Canada   M5G 2E9
T: 416.927.2027 ext. 2290
F: 416.927.4167
jetches at iwh.on.ca
www.iwh.on.ca





This e-mail may contain confidential information for the sol...{{dropped}}


From jholtman at gmail.com  Tue Jul 17 03:47:07 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 16 Jul 2007 21:47:07 -0400
Subject: [R] problem with length()
In-Reply-To: <8E32AFC4-112B-409C-8B82-966EDB75561D@utoronto.ca>
References: <8E32AFC4-112B-409C-8B82-966EDB75561D@utoronto.ca>
Message-ID: <644e1f320707161847t1fe4d923p4aba11f07ddcf9df@mail.gmail.com>

POSIXlt is a list structure of 9 elements (see ?POSIXlt).  You can see
that in the data below:

> x <- as.POSIXlt(c('2007-01-01','2007-02-01','2007-03-31'))
> length(x)
[1] 9
> unclass(x)
$sec
[1] 0 0 0

$min
[1] 0 0 0

$hour
[1] 0 0 0

$mday
[1]  1  1 31

$mon
[1] 0 1 2

$year
[1] 107 107 107

$wday
[1] 1 4 6

$yday
[1]  0 31 89

$isdst
[1] 0 0 0

attr(,"tzone")
[1] "GMT"
> length(as.POSIXct(x))
[1] 3


What you probably want to do is to use the POSIXct class.

On 7/16/07, Jacob Etches <jacob.etches at utoronto.ca> wrote:
> In the following, can anyone tell me why length(eee) returns 9?  I
> was expecting 15398, and when I try to add this vector to a data
> frame with that many rows, it fails complaining that the vector is of
> length 9.  In what I thought was an identical situation with a
> related dataset, the same code worked as expected.
>
>  > length(fff)
> [1] 15398
>  > str(fff)
> int [1:15398] 20010102 20010102 20010102 20010103 20010103 20010102
> 20010102 20010104 20010103 20010102 ...
>  > fff[1:12]
> [1] 20010102 20010102 20010102 20010103 20010103 20010102 20010102
> 20010104 20010103 20010102 20010105 20010103
>  > eee <- as.POSIXlt(strptime(fff,"%Y%m%d"))
>  > length(eee)
> [1] 9
>  > eee[1:12]
> [1] "2001-01-02" "2001-01-02" "2001-01-02" "2001-01-03" "2001-01-03"
> "2001-01-02" "2001-01-02" "2001-01-04" "2001-01-03" "2001-01-02"
> "2001-01-05" "2001-01-03"
>  > str(eee)
> 'POSIXlt', format: chr [1:15398] "2001-01-02" "2001-01-02"
> "2001-01-02" "2001-01-03" "2001-01-03" "2001-01-02" "2001-01-02"
> "2001-01-04" "2001-01-03" ...
>
>
>
> Many thanks in advance,
> Jacob Etches
>
>
> Doctoral candidate, Epidemiology Program
> Department of Public Health Sciences, University of Toronto Faculty
> of Medicine
>
> Research Associate
> Institute for Work & Health
> 800-481 University Avenue, Toronto, Ontario, Canada   M5G 2E9
> T: 416.927.2027 ext. 2290
> F: 416.927.4167
> jetches at iwh.on.ca
> www.iwh.on.ca
>
>
>
>
>
> This e-mail may contain confidential information for the sol...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From rmh at temple.edu  Tue Jul 17 05:32:50 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 16 Jul 2007 23:32:50 -0400 (EDT)
Subject: [R] print tables to figure in R
Message-ID: <20070716233250.CGT65758@po-d.temple.edu>

look at the latex() function in the Hmisc package.

It formats any R table into a LaTeX tabular environment.
The defaults are good and there are a zillion arguments
to specify your own preferences.

Rich


From bchristo at email.arizona.edu  Tue Jul 17 05:38:32 2007
From: bchristo at email.arizona.edu (Brad Christoffersen)
Date: Mon, 16 Jul 2007 20:38:32 -0700
Subject: [R] data.restore() in R 2.5.1 for Windows 95 and later
Message-ID: <20070716203832.qep6l4aokogw004c@www.email.arizona.edu>

To Whom It May Concern:

I want to read in an S-PLUS data dump and I used

> data.restore("filepath/filename")

(in R 2.5.1 for Windows 95 and later) and I get the message

Error: could not find function "data.restore"

I have also tried read.S() and read.dta() with the same result.  I cannot find
any of these functions in the R Help for package base, although data.restore()
is mentioned in the R Data Import/Export manual.  I have also tried apropos()
and "character(0)" returns.  Additionally, I tried

> getS3method("data","restore")

but get

Error in getS3method("data", "restore") : no function 'data' could be found

In addition, I tried

> dget("filepath/filename")

and it was taking a very long time to import the desired data dump objects which
previously took a shorter time using data.restore in SPlus.

Any help is much appreciated!

Thanks,

Brad Christoffersen
Graduate Student
University of Arizona


From bcarvalh at jhsph.edu  Tue Jul 17 05:43:17 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 16 Jul 2007 23:43:17 -0400
Subject: [R] data.restore() in R 2.5.1 for Windows 95 and later
In-Reply-To: <20070716203832.qep6l4aokogw004c@www.email.arizona.edu>
References: <20070716203832.qep6l4aokogw004c@www.email.arizona.edu>
Message-ID: <65BF9660-33A3-4E56-A6F9-1F7894EF9B1D@jhsph.edu>

The R  Data Import/Export also says that this function is in the  
"foreign" package. :-)

b

On Jul 16, 2007, at 11:38 PM, Brad Christoffersen wrote:

> To Whom It May Concern:
>
> I want to read in an S-PLUS data dump and I used
>
>> data.restore("filepath/filename")
>
> (in R 2.5.1 for Windows 95 and later) and I get the message
>
> Error: could not find function "data.restore"
>
> I have also tried read.S() and read.dta() with the same result.  I  
> cannot find
> any of these functions in the R Help for package base, although  
> data.restore()
> is mentioned in the R Data Import/Export manual.  I have also tried  
> apropos()
> and "character(0)" returns.  Additionally, I tried
>
>> getS3method("data","restore")
>
> but get
>
> Error in getS3method("data", "restore") : no function 'data' could  
> be found
>
> In addition, I tried
>
>> dget("filepath/filename")
>
> and it was taking a very long time to import the desired data dump  
> objects which
> previously took a shorter time using data.restore in SPlus.
>
> Any help is much appreciated!
>
> Thanks,
>
> Brad Christoffersen
> Graduate Student
> University of Arizona
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Tue Jul 17 06:26:08 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 17 Jul 2007 06:26:08 +0200
Subject: [R] how do I draw such a barplot?
In-Reply-To: <200707161006.58676.dgvirtual@akl.lt>
References: <200707161006.58676.dgvirtual@akl.lt>
Message-ID: <f8e6ff050707162126h593a3b4dsc5143e80a626b55a@mail.gmail.com>

On 7/16/07, Donatas G. <dgvirtual at akl.lt> wrote:
> Hi,
>
> I cannot figure out how to draw a certain plot: could someone help me out?
>
> I have this data.frame from a survey
> my.data
>
> that looks like something like this:
>
>    col1  col2  col3  col4
> 1     5     5     4     5
> 2     3     5     3     1
> 3     2     3     4     5
> 4     3     1     1     2
> 5     5     5     4     5
> 6     4     2     5     5
> ....
>
>
> Each row represents a single questionnaire with someone giving his
> agreement/disagreement with a statement (each column is a statement) that is
> coded from 1 to 5.
>
> I need to draw a barplot giving a visual representation showing differences
> between the five columns: Each bar should represent a single column, and
> should be divided into 5 sections, the thickness of each depending on the
> number of respondents who choose that particular answer.
>
> How do I do that? All I have managed to do so far is to produce a barplot of a
> single column, and that - only with bars side by side...

One way would be the use the ggplot2 and reshape packages:

library(ggplot2)
df <- as.data.frame(matrix(sample(1:5, 100, rep=T), ncol=5))

dfm <- melt(df, m=1:5)
qplot(variable, data=dfm, geom="bar", fill=factor(value))
qplot(variable, data=dfm, geom="bar", fill=factor(value), position="dodge")
qplot(variable, data=dfm, geom="bar", fill=factor(value), facets = . ~ value)

Hadley


From h.wickham at gmail.com  Tue Jul 17 06:35:39 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 17 Jul 2007 06:35:39 +0200
Subject: [R] Problem to sort factor
In-Reply-To: <439347026.20070716181400@xylon.de>
References: <439347026.20070716181400@xylon.de>
Message-ID: <f8e6ff050707162135i5ef2a9e8of5867165cb8cd440@mail.gmail.com>

On 7/16/07, Arne Brutschy <abr-r-project at xylon.de> wrote:
> Hello,
>
> I'm having a problem renaming and sorting the underlying factor of a
> ggplot2 based plot. Here's my code:
>
> ---8<----------
> > delta <- ggplot(subset(data, Model==c("dyn", "dl4", "dl3")), aes(x=Problemsize, y=Fitness)) +
>   geom_smooth(size=1, color="black", fill=alpha("blue", 0.2))+
>   geom_point(size=0.5, aes(colour=DeltaConfig))+
>   scale_colour_gradient2(expression(bold(paste(Delta,"Config"))), limits=c(0,10), midpoint=5,
>                          low="green", mid="yellow", high="red")
> > delta <- delta + facet_grid(Model ~ . , margins = TRUE)
> > delta
> ---8<----------
>
> and the data
> ---8<----------
> > data
>    Model Problemsize Fitness DeltaConfig
> 1    dl1           3 81.5271      2.4495
> 2    dl1           3 83.1999      2.4495
>      ...
> ---8<----------
>
> I want to select a subset of the possible models and display the
> resulting three plots in a column. This works fine, but: the displayed
> names and order is wrong. Instead of the models "dl3","dl4","dyn"
> I want to change dl* to rm* and the order to "dyn","rm3","rm4".
>
> I hope it's understandable what I want. I tried to use factor()
> function in a thousand combinations, but I don't seem to get it. Can
> someone help me?

Does this get you started?

x <- factor(c("a", "b", "c"))
factor(x, levels=c("c","b","a"), labels=c("cc","bb", "aa"))

Hadley


From h.wickham at gmail.com  Tue Jul 17 06:48:07 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 17 Jul 2007 06:48:07 +0200
Subject: [R] scaling of different data sets in ggplot
In-Reply-To: <683573.91772.qm@web39706.mail.mud.yahoo.com>
References: <683573.91772.qm@web39706.mail.mud.yahoo.com>
Message-ID: <f8e6ff050707162148j248f7ddw99391d91dc02de81@mail.gmail.com>

Hi Stephen,

You can't do that in ggplot (have two different scales) because I
think it's generally a really bad idea.  The whole point of plotting
the data is so that you can use your visual abilities to gain insight
into the data.  When you have two different scales the positions of
the two groups are essentially arbitrary - the data only have x values
in common, not y values.  You essentially have two almost unrelated
graphs plotted on top of each other.

On the other hand, for this data, I think it would be reasonable to
plot log(z) and y on the same scale - the data is transformed not the
scales.

Hadley

On 7/14/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> Dear list (but probably mostly Hadley):
>
> In ggplot, operations to modify 'guides' are accessed through grid
> objects, but I did not find mention of creating new guides or possibly
> removing them altogether using ggplot functions. I wonder if this is
> something I need to learn grid to learn more about (which I hope to do
> eventually).
>
> Also, ggplot()+geom_object() [where 'object' can be point, line, etc.]
> or layer() contains specification for the data, mappings and
> geoms/stats - but the geoms/stats can be scale-dependent [for
> instance, log]. so I wonder how different scalings can be applied to
> different data sets.
>
> Below is an example that requires both:
>
> x <- runif(100) y <- exp(x^2) z <- x^2+rnorm(100,0,0.02)
>
> par(mar=c(5,4,2,4)+0.1) plot(x,y,log="y") lines(lowess(x,y,f=1/3))
> par(new=TRUE) plot(x,z,col=2,pch=3,yaxt="n",ylab="")
> lines(lowess(x,z,f=1/3),col=2) axis(4,col=2,col.axis=2)
> mtext("z",4,line=3,col=2)
>
> In ggplot:
>
> ## data specification
> ggplot(data=data.frame(x,y,z)) +
>
>   ## first set of points geom_point(mapping=aes(x=x,y=y)) +
>   ## scale_y_log() +
>
>   ## second set of points geom_point(mapping=aes(x=x,y=z),pch=3) +
>   ## layer(mapping=aes(x=x,y=z),stat="smooth",method="loess") +
>   ## scale_y_continuous()
>
> scale_y_log() and scale_y_continuous() appear to apply to both mappings at
> once, and I can't figure out how to associate them with the intended ones (I
> expect this will be a desire for size and color scales as well).
>
> Of course, I can always try to fool the system by (1) applying the scaling a
> priori to create a new variable, (2) plotting points from the new variable,
> and (3) creating a new axis with custom labels. Which then brings me back to
> ...how to add new guides? :)
>
> Thanks,
>
> Stephen
>
>
>
>       ____________________________________________________________________________________
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From phhs80 at gmail.com  Tue Jul 17 08:19:34 2007
From: phhs80 at gmail.com (Paul Smith)
Date: Tue, 17 Jul 2007 07:19:34 +0100
Subject: [R] Optimization
In-Reply-To: <JLAHUK$B02D959B066823E595F53D454A1998AA@poste.it>
References: <JLAHUK$B02D959B066823E595F53D454A1998AA@poste.it>
Message-ID: <6ade6f6c0707162319l3b88458bra2932fd2b17895a@mail.gmail.com>

On 7/16/07, massimiliano.talarico <massimiliano.talarico at poste.it> wrote:
> I need a suggest to obtain the max of this function:
>
> Max x1*0.021986+x2*0.000964+x3*0.02913
>
> with these conditions:
>
> x1+x2+x3=1;
> radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> x1>=0;
> x1<=1;
> x2>=0;
> x2<=1;
> x3>=0;
> x3<=1;
>
> Any suggests ?

What do you mean by 'radq', Massimiliano?

Paul


From delphine.fontaine at genexion.com  Tue Jul 17 08:59:11 2007
From: delphine.fontaine at genexion.com (Delphine Fontaine)
Date: Tue, 17 Jul 2007 08:59:11 +0200
Subject: [R] logical operators priority
Message-ID: <000a01c7c83f$fbc9cdf0$7b010a0a@Genexion.local>

Dear R-users,

I haven?t found rules for logical operators. I need to select data according
the following rule:
Condition A & (Condition B | Condition C) How should I write it?? Is
Condition A & Condition B | Condition C correct or will R execute (Condition
A & Condition B) | Condition C ?

Thanks for your help.

Delphine Fontaine






Delphine Fontaine
Statistician
Statistics Department - Genexion SA
------------------------------------------------------------
29, Quai du Mont-Blanc
Genva, CH-1201
Switzerland
------------------------------------------------------------
Office: +41 22 704 32 44
Fax:??? +41 22 704 32 42
Email: Delphine.Fontaine at genexion.com


From massimiliano.talarico at poste.it  Tue Jul 17 09:18:55 2007
From: massimiliano.talarico at poste.it (massimiliano.talarico)
Date: Tue, 17 Jul 2007 09:18:55 +0200
Subject: [R] Optimization
Message-ID: <JLBAZJ$39C399C584DC82796C77CF86331242D2@poste.it>

I'm sorry the function is 

sqrt((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;

Have you any suggests.

Thanks,
Massimiliano



What is radq?

--- "massimiliano.talarico"
<massimiliano.talarico at poste.it> wrote:

> Dear all,
> I need a suggest to obtain the max of this function:
> 
> Max x1*0.021986+x2*0.000964+x3*0.02913
> 
> with these conditions:
> 
> x1+x2+x3=1;
>
radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> x1>=0;
> x1<=1;
> x2>=0;
> x2<=1;
> x3>=0;
> x3<=1;
> 
> Any suggests ?
> 
> Thanks in advanced,
> Massimiliano
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From i.visser at uva.nl  Tue Jul 17 09:22:38 2007
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 17 Jul 2007 09:22:38 +0200
Subject: [R] logical operators priority
In-Reply-To: <000a01c7c83f$fbc9cdf0$7b010a0a@Genexion.local>
References: <000a01c7c83f$fbc9cdf0$7b010a0a@Genexion.local>
Message-ID: <8EBE76B0-5526-44E9-BE6E-433C8BEA6360@uva.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/ad107eb0/attachment.pl 

From abr-r-project at xylon.de  Tue Jul 17 09:24:02 2007
From: abr-r-project at xylon.de (Arne Brutschy)
Date: Tue, 17 Jul 2007 09:24:02 +0200
Subject: [R] Problem to sort factor
In-Reply-To: <f8e6ff050707162135i5ef2a9e8of5867165cb8cd440@mail.gmail.com>
References: <439347026.20070716181400@xylon.de>
	<f8e6ff050707162135i5ef2a9e8of5867165cb8cd440@mail.gmail.com>
Message-ID: <194406151.20070717092402@xylon.de>

Hi,

hadley wickham wrote:
h> Does this get you started?
h> x <- factor(c("a", "b", "c"))
h> factor(x, levels=c("c","b","a"), labels=c("cc","bb", "aa"))
hmm, not really. I simply don't seem to get the concept of factors
here... How do I combine the above code with my dataframe and pass it
on to facet_grid???

Thanks
Arne


From eric at net2000.ch  Tue Jul 17 09:24:24 2007
From: eric at net2000.ch (eric at net2000.ch)
Date: Tue, 17 Jul 2007 09:24:24 +0200
Subject: [R] [R-sig-DB] RODBC on Oracle DB
Message-ID: <4556.1184657064@net2000.ch>


essai <- odbcConnect("ORESTE_prod",  uid="osis_r",  pwd="12miss15" ,case="oracle")

> sqlTables(essai)$ORESTE

...

1315      <NA>      ORESTE              S_PROFESSIONS_OLD        TABLE    <NA>
1316      <NA>      ORESTE                  S_PROVENANCES        TABLE    <NA>
1317      <NA>      ORESTE                        S_SEXES        TABLE    <NA>
1318      <NA>      ORESTE                 S_SOUS_CLASSES        TABLE    <NA>
1319      <NA>      ORESTE                 S_TYP_COLLEGES        TABLE    <NA>
1320      <NA>      ORESTE             S_TYP_ENSEIGNEMENT        TABLE    <NA>

...

> sqlQuery(essai, "select * from S_TYP_COLLEGES")
[1] "[RODBC] ERROR: Could not SQLExecDirect"                            
[2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"

I have also tried the 
essai2 <- odbcDriverConnect(connection="essai2")
But with no succes. 



On Lun Juil 16 15:32 , Prof Brian Ripley <ripley at stats.ox.ac.uk> sent:

>The problem could be quoting, if Oracle is not standards-compliant.
>See the options in ?odbcConnect.
>
>If sqlQuery(essai, "select * from S_TYP_COLLEGES") works, this is likely 
>to be the problem.
>
>On Mon, 16 Jul 2007, eric at net2000.ch wrote:
>
>>
>>
>>> essai 
>>> odbcGetInfo(essai)
>>       DBMS_Name         DBMS_Ver  Driver_ODBC_Ver
>>        "Oracle"     "09.00.0121"          "03.51"
>> Data_Source_Name      Driver_Name       Driver_Ver
>>   "ORESTE_prod"    "SQORA32.DLL"     "09.00.0101"
>>        ODBC_Ver      Server_Name
>>    "03.52.0000"           "weba"
>>
>>
>>> sqlTables(essai)
>>
>> The result of this function is a liste of tables, one of them is called:
>> S_TYP_COLLEGES.
>>
>>
>>> sqlFetch(essai,"S_TYP_COLLEGES")
>> [1] "[RODBC] ERROR: Could not SQLExecDirect"
>> [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>>
>>> sqlFetch(essai, "S_TYP_COLLEGES", colnames=TRUE, rownames=FALSE)
>> [1] "[RODBC] ERROR: Could not SQLExecDirect"
>> [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>>
>>
>> What could be the problem here ?
>> Any help is welcome
>> Eric R?thlisberger, Neuch?tel
>>
>> _______________________________________________
>> R-sig-DB mailing list -- R Special Interest Group
>> R-sig-DB at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-db
>>
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From meeryana at yahoo.com  Tue Jul 17 09:23:52 2007
From: meeryana at yahoo.com (copula)
Date: Tue, 17 Jul 2007 00:23:52 -0700 (PDT)
Subject: [R] R and Copula
In-Reply-To: <11612986.post@talk.nabble.com>
References: <11612986.post@talk.nabble.com>
Message-ID: <11644534.post@talk.nabble.com>


it would be great when somebody will help me
thanks


copula wrote:
> 
> hi,
> first I want to say that I'm new here, and new with copula and R.
> 
> That is the reason why I'm writing, if somebody can help me. 
> 
> I have to make an example of Copula. 
> On internet I've found this forum and that copula can calculate with R.
> 
> Can somebody help me with the thing how can I start and where can read
> about these stuffs.
> 
> Thank to all who can help!
> 
> 
> 
> 

-- 
View this message in context: http://www.nabble.com/R-and-Copula-tf4085867.html#a11644534
Sent from the R help mailing list archive at Nabble.com.


From e.catchpole at adfa.edu.au  Tue Jul 17 09:31:05 2007
From: e.catchpole at adfa.edu.au (ecatchpole)
Date: Tue, 17 Jul 2007 17:31:05 +1000
Subject: [R] logical operators priority
In-Reply-To: <000a01c7c83f$fbc9cdf0$7b010a0a@Genexion.local>
References: <000a01c7c83f$fbc9cdf0$7b010a0a@Genexion.local>
Message-ID: <469C7039.5090502@adfa.edu.au>

?"&"

will show

See Syntax for the precedence of these operators: unlike many
other languages (including S) the AND and OR operators do not have
the same precedence (the AND operators are higher than the OR
operators).

Ted.


Delphine Fontaine wrote on 07/17/2007 04:59 PM:
> Dear R-users,
>
> I haven?t found rules for logical operators. I need to select data according
> the following rule:
> Condition A & (Condition B | Condition C) How should I write it ? Is
> Condition A & Condition B | Condition C correct or will R execute (Condition
> A & Condition B) | Condition C ?
>
> Thanks for your help.
>
> Delphine Fontaine
>
>
>
>
>
>
> Delphine Fontaine
> Statistician
> Statistics Department - Genexion SA
> ------------------------------------------------------------
> 29, Quai du Mont-Blanc
> Genva, CH-1201
> Switzerland
> ------------------------------------------------------------
> Office: +41 22 704 32 44
> Fax:    +41 22 704 32 42
> Email: Delphine.Fontaine at genexion.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
 Dr E.A. Catchpole  
 Visiting Fellow
 Univ of New South Wales at ADFA, Canberra, Australia
    _	  and University of Kent, Canterbury, England
   'v'	  - www.pems.adfa.edu.au/~ecatchpole          
  /   \	  - fax: +61 2 6268 8786		   
   m m    - ph:  +61 2 6268 8895


From brown_emu at yahoo.com  Tue Jul 17 10:08:53 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 17 Jul 2007 01:08:53 -0700 (PDT)
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <971536df0707142101p52ea02a8gb41f4a0be826f5ee@mail.gmail.com>
Message-ID: <529864.45259.qm@web39702.mail.mud.yahoo.com>

Thanks very much, Gabor - I hadn't considered this possibility. I always
enjoy your posts!

--- Gabor Grothendieck <ggrothendieck at gmail.com> wrote:

> Suppose ri were already defined as in the example below.
> Then panel.qrect is a bit harder to define although with
> work its possible as shown below:
> 
> rectInfo <-
>    list(matrix(runif(4), 2, 2),
>         matrix(runif(4), 2, 2),
>         matrix(runif(4), 2, 2))
> 
> ri <- function(x, y, ..., rect.info) {
>    ri <- rect.info[[packet.number()]]
>    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
>       col = "grey86", border = NA)
>    panel.xyplot(x, y, ...)
>  }
> 
> panel.qrect <- function(rect.info) {
> 	function(x, y, ...) {
> 		environment(ri) <- environment() ###
> 		ri(x, y, ..., rect.info = rect.info)
> 	}
> }
> 
> xyplot(runif(30) ~ runif(30) | gl(3, 10),
>       panel = panel.qrect(rectInfo))
> 
> 
> 
> On 7/14/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> > This is very interesting - but I'm not entirely clear on your last
> statement
> > though about how existing functions can cause problems with the scoping
> that
> > createWrapper() avoids... (but thanks for the tip).
> >
> >
> > --- Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> > > Your approach of using closures is cleaner than that
> > > given below but just for comparison in:
> > >
> > > http://tolstoy.newcastle.edu.au/R/devel/06/03/4476.html
> > >
> > > there is a createWrapper function which creates a new function based
> > > on the function passed as its first argument by using the components
> > > of the list passed as its second argument to overwrite its formal
> > > arguments.  For example,
> > >
> > > createWrapper <- function(FUN, Params) {
> > >    as.function(c(replace(formals(FUN), names(Params), Params),
> body(FUN)))
> > > }
> > >
> > > library(lattice)
> > >
> > > rectInfo <-
> > >    list(matrix(runif(4), 2, 2),
> > >         matrix(runif(4), 2, 2),
> > >         matrix(runif(4), 2, 2))
> > >
> > >
> > > panel.qrect <- function(x, y, ..., rect.info) {
> > >    ri <- rect.info[[packet.number()]]
> > >    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
> > >               col = "grey86", border = NA)
> > >    panel.xyplot(x, y, ...)
> > > }
> > >
> > > xyplot(runif(30) ~ runif(30) | gl(3, 10),
> > >       panel = createWrapper(panel.qrect, list(rect.info = rectInfo)))
> > >
> > > The createWrapper approach does have an advantage in the situation
> > > where the function analogous to panel.qrect is existing since using
> > > scoping then involves manipulation of environments in the closure
> > > approach.
> > >
> > > On 7/11/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> > > > In the Trellis approach, another way (I like) to deal with multiple
> > > pieces of
> > > > external data sources is to 'attach' them to panel functions through
> > > lexical
> > > > closures. For instance...
> > > >
> > > > rectInfo <-
> > > >    list(matrix(runif(4), 2, 2),
> > > >         matrix(runif(4), 2, 2),
> > > >         matrix(runif(4), 2, 2))
> > > >
> > > > panel.qrect <- function(rect.info) {
> > > >  function(x, y, ...) {
> > > >    ri <- rect.info[[packet.number()]]
> > > >    panel.rect(ri[1, 1], ri[1, 2], ri[2, 1], ri[2, 2],
> > > >               col = "grey86", border = NA)
> > > >    panel.xyplot(x, y, ...)
> > > >  }
> > > > }
> > > >
> > > > xyplot(runif(30) ~ runif(30) | gl(3, 10),
> > > >       panel = panel.qrect(rectInfo))
> > > >
> > > > ...which may or may not be more convenient than passing rectInfo (and
> > > perhaps
> > > > other objects if desired) explicitly as an argument to xyplot().
> > > >
> > > >
> > > > --- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> > > >
> > > > > On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > > > > > A question/comment: I have usually found that the subscripts
> > > argument
> > > > > is
> > > > > > > what I need when passing *external* information into the panel
> > > > > function, for
> > > > > > > example, when I wish to add results from a fit done external to
> the
> > > > > trellis
> > > > > > > call. Fits[subscripts] gives me the fits (or whatever) I want
> to
> > > plot
> > > > > for
> > > > > > > each panel. It is not clear to me how the panel layout
> information
> > > from
> > > > > > > panel.number(), etc. would be helpful here instead. Am I
> correct?
> > > -- or
> > > > > is
> > > > > > > there a smarter way to do this that I've missed?
> > > > > >
> > > > > > This is one of things that I think ggplot does better - it's much
> > > > > > easier to plot multiple data sources.  I don't have many examples
> of
> > > > > > this yet, but the final example on
> > > > > > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic
> idea.
> > > > >
> > > > > That's probably true. The Trellis approach is to define a plot by
> > > > > "data source" + "type of plot", whereas the ggplot approach (if I
> > > > > understand correctly) is to create a specification for the display
> > > > > (incrementally?) and then render it. Since the specification can be
> > > > > very general, the approach is very flexible. The downside is that
> you
> > > > > need to learn the language.
> > > > >
> > > > > On a philosophical note, I think the apparent limitations of
> Trellis
> > > > > in some (not all) cases is just due to the artificial importance
> given
> > > > > to data frames as the one true container for data. Now that we have
> > > > > proper multiple dispatch in S4, we can write methods that behave
> like
> > > > > traditional Trellis calls but work with more complex data
> structures.
> > > > > We have tried this in one bioconductor package (flowViz) with
> > > > > encouraging results.
> > > > >
> > > > > -Deepayan
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> >
> >
> >
> >
> >
>
____________________________________________________________________________________
> >
> > that gives answers, not web links.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 



       
____________________________________________________________________________________
Got a little couch potato? 
Check out fun summer activities for kids.


From i.visser at uva.nl  Tue Jul 17 10:17:06 2007
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 17 Jul 2007 10:17:06 +0200
Subject: [R] R and Copula
In-Reply-To: <11644534.post@talk.nabble.com>
References: <11612986.post@talk.nabble.com> <11644534.post@talk.nabble.com>
Message-ID: <64F44ED1-4FD3-42ED-854F-AA92737F152D@uva.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/0582f1d9/attachment.pl 

From brown_emu at yahoo.com  Tue Jul 17 10:22:19 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 17 Jul 2007 01:22:19 -0700 (PDT)
Subject: [R] Drawing rectangles in multiple panels
In-Reply-To: <eb555e660707141832g7617b654s21e570b1712fcad@mail.gmail.com>
Message-ID: <718975.53794.qm@web39710.mail.mud.yahoo.com>

Hi Deepayan, that's very hard-core... for the atmospheric science
applications (which is what I do) that I've encountered, (time-series) data
sets are often pre-aggregated before distribution (to 'average out'
instrument noise) so I haven't had the need for such requirements thus far...
but very good to know (and cool demonstrations btw). Thanks!

Stephen

--- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:

> On 7/14/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> >
> > I wonder what kind of objects? Are there large advantages for allowing
> > lattice functions to operate on objects other than data frames - I
> > couldn't find any screenshots of flowViz but I imagine those objects
> > would probably be list of arrays and such? I tend to think of mapply()
> > [and more recently melt()], etc. could always be applied beforehand,
> > but I suppose that would undermine the case for having generic
> > functions to support the rich collection of object classes in R...
> 
> There's a copy of a presentation at
> 
>
http://www.ficcs.org/meetings/ficcs3/presentations/DeepayanSarkar-flowviz.pdf
> 
> and a (largish - 37M) vignette linked from
> 
> http://bioconductor.org/packages/2.1/bioc/html/flowViz.html
> 
> Neither of these really talk about the challenge posed by the size of
> the data. The data structure, as with most microarray-type
> experiments, is like a data frame, except that the response for every
> experimental unit is itself a large matrix. If we represented the GvHD
> data set (the one used in the examples) as a "long format" data frame
> that lattice would understand, it would have 585644 rows and 12
> columns (8 measurements that are different for each row, and 4
> phenotypic variables that are the same for all rows coming from a
> single sample). And this is for a smallish subset of the actual
> experiment.
> 
> In practice, the data are stored in an environment to prevent
> unnecessary copying, and panel functions only access one data matrix
> at a time.
> 
> -Deepayan
> 
> 
> > --- Deepayan Sarkar <deepayan.sarkar at gmail.com> wrote:
> >
> > > On 7/11/07, hadley wickham <h.wickham at gmail.com> wrote:
> > > > > A question/comment: I have usually found that the subscripts
> argument
> > > is
> > > > > what I need when passing *external* information into the panel
> > > function, for
> > > > > example, when I wish to add results from a fit done external to the
> > > trellis
> > > > > call. Fits[subscripts] gives me the fits (or whatever) I want to
> plot
> > > for
> > > > > each panel. It is not clear to me how the panel layout information
> from
> > > > > panel.number(), etc. would be helpful here instead. Am I correct?
> -- or
> > > is
> > > > > there a smarter way to do this that I've missed?
> > > >
> > > > This is one of things that I think ggplot does better - it's much
> > > > easier to plot multiple data sources.  I don't have many examples of
> > > > this yet, but the final example on
> > > > http://had.co.nz/ggplot2/geom_abline.html illustrates the basic idea.
> > >
> > > That's probably true. The Trellis approach is to define a plot by
> > > "data source" + "type of plot", whereas the ggplot approach (if I
> > > understand correctly) is to create a specification for the display
> > > (incrementally?) and then render it. Since the specification can be
> > > very general, the approach is very flexible. The downside is that you
> > > need to learn the language.
> > >
> > > On a philosophical note, I think the apparent limitations of Trellis
> > > in some (not all) cases is just due to the artificial importance given
> > > to data frames as the one true container for data. Now that we have
> > > proper multiple dispatch in S4, we can write methods that behave like
> > > traditional Trellis calls but work with more complex data structures.
> > > We have tried this in one bioconductor package (flowViz) with
> > > encouraging results.
> > >
> > > -Deepayan
> 



       
____________________________________________________________________________________Ready for the edge of your seat? 
Check out tonight's top picks on Yahoo! TV.


From berwin at maths.uwa.edu.au  Tue Jul 17 10:22:19 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Tue, 17 Jul 2007 16:22:19 +0800
Subject: [R] Optimization
In-Reply-To: <JLAHUK$B02D959B066823E595F53D454A1998AA@poste.it>
References: <JLAHUK$B02D959B066823E595F53D454A1998AA@poste.it>
Message-ID: <20070717162219.7db10e24@berwin5>

G'day Massimiliano,

On Mon, 16 Jul 2007 22:49:32 +0200
"massimiliano.talarico" <massimiliano.talarico at poste.it> wrote:

> Dear all,
> I need a suggest to obtain the max of this function:
> 
> Max x1*0.021986+x2*0.000964+x3*0.02913
> 
> with these conditions:
> 
> x1+x2+x3=1;
> radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> x1>=0;
> x1<=1;
> x2>=0;
> x2<=1;
> x3>=0;
> x3<=1;

Note that given the constraint x1+x2+x3=1 and the positivity constraints on x1,
x2 and x3, the conditions that all of them should be <=1 are redundant.

I would go about as follows:

x1+x2+x3=1 means x1=1-x2-x3

plug this into the next constraint to obtain:

a*(1-x2-x3)^2+b*x2^2+c*x3^2=0.04^2 for suitable a, b, c.

Note that for any value of x3, you can solve this equation for x2.

Hence, take a grid of x3 values between 0 and 1 and calculate the correpsonding
x2 values.  From x3 and x2 calculate x1.  Discard all tuples that do not
fulfill the positivity constraints and calculate the function values for the
remaining ones.  

If the grid of x3 values is fine enough, that should give you an idea what the
solution should be.

Alternatively, you could write a function that takes values of x3 and does all
the computations outline above (return -Inf if the value x3 is not feasible)
and then pass the function to optimise() for numerical optimisation...

Hope this helps.

Cheers,

	Berwin

=========================== Full address =============================
Berwin A Turlach                            Tel.: +65 6515 4416 (secr)        
Dept of Statistics and Applied Probability        +65 6515 6650 (self)
Faculty of Science                          FAX : +65 6872 3919       
National University of Singapore                                      
6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
Singapore 117546                    http://www.stat.nus.edu.sg/~statba


From ted.harding at nessie.mcc.ac.uk  Tue Jul 17 10:23:37 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 17 Jul 2007 09:23:37 +0100 (BST)
Subject: [R] print tables to figure in R
In-Reply-To: <469BED13.50703@wisc.edu>
Message-ID: <XFMail.070717092337.ted.harding@nessie.mcc.ac.uk>

On 16-Jul-07 22:11:31, Steve Powers wrote:
> Doe anyone know of a way to print nice-looking, printer-friendly tables
> directly in the R plot window? This would be very handy for examining 
> preliminary results from frequently used routines. I've hunted for this
> with no luck. All anyone seems to do is export text files to excel for 
> formatting, or use the print(bla) function which just calls the numbers
> at the command line. There must be a way to do this.---steve

It would certainly be possible to write a function include a table
within a plot, since a table is no more than bits of text laid out
positionally in an array, and there is already provision to place
text at assigned positions on a plot -- the function text() does this.

That being said, though, writing the code to layout a particular
table in the particular way you want would not be trivial. I don't
know of an R function which implements a reasonable "default layout"
for arbitrary tables in a plot.

As an example of "doing it with your bare hands", run the following:

x<-0.5*(0:20);y<-(x^2)/10;plot(x,y,pch="+",col="blue",ylim=c(-10,10))
lines(x,y,col="blue")
A<-c(0,2.5,5,7.5,10); B<-(A^2)/10;points(A,B,pch=2,col="red")
Ach<-as.character(round(A,digits=1))
Bch<-as.character(round(B,digits=3))
text(x=c(6.25,8.0),y=c(0,0),labels=c("A","B"),pos=4)
for(i in (1:5)){text(x=6.125,y=(-1-i),labels=Ach[i],pos=4)}
for(i in (1:5)){text(x=7.875,y=(-1-i),labels=Bch[i],pos=4)}

You could augment this with grid lines, etc. according to taste.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 17-Jul-07                                       Time: 09:23:33
------------------------------ XFMail ------------------------------


From Karl.Hufthammer at math.uib.no  Tue Jul 17 10:32:54 2007
From: Karl.Hufthammer at math.uib.no (Karl Ove Hufthammer)
Date: Tue, 17 Jul 2007 10:32:54 +0200
Subject: [R] Different axis limits for each facet in ggplot2
References: <f7fmt9$ide$1@sea.gmane.org>
	<f8e6ff050707161230q4d8ede79sd472d1e5b11558af@mail.gmail.com>
Message-ID: <f7hurm$40r$1@sea.gmane.org>

hadley wickham:

>> Is it possible to have different axis limit for each facet in a ggplot2
>> plot? Here is an example:
> 
> Not yet, although it is on the to do list.

Thanks for the answer. I?ll be looking forward to the next version(s)
then. :)

-- 
Karl Ove Huftammer


From ted.harding at nessie.mcc.ac.uk  Tue Jul 17 10:34:21 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 17 Jul 2007 09:34:21 +0100 (BST)
Subject: [R] logical operators priority
In-Reply-To: <000a01c7c83f$fbc9cdf0$7b010a0a@Genexion.local>
Message-ID: <XFMail.070717093421.ted.harding@nessie.mcc.ac.uk>

On 17-Jul-07 06:59:11, Delphine Fontaine wrote:
> Dear R-users,
> 
> I haven't found rules for logical operators. I need to select data
> according the following rule:
> Condition A & (Condition B | Condition C)
> How should I write it_? Is Condition A & Condition B | Condition C
> correct or will R execute (Condition A & Condition B) | Condition C ?
> 
> Thanks for your help.
> 
> Delphine Fontaine

?Syntax will tell you about the precedence for operators.
In particular you will find:

       '!'                negation
       '&  &&'            and
       '| ||'             or

indicating that "!" takes precedence over "&" and "&&", which take
precedence over "|" and "||". With equal precedence evaluation is
from left to right.

This shows that if you write A & B | C then "A & B" will be evaluated
first, so the expression is equivalent to (A & B) | C.

In any case, there is nothing whatever wrong with using "(... )"
for grouping. It is always accepted, it forces the precedence you want,
and furthermore (most important) you yourself can see exactly what
will happen and will be much less likely to make a mistake.

So write your condition in your code *explicitly* as

  ConditionA & (ConditionB | ConditionC)

You can see what's happening, and R will do exactly what you want.

Best wishes
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 17-Jul-07                                       Time: 09:34:18
------------------------------ XFMail ------------------------------


From gyadav at ccilindia.co.in  Tue Jul 17 10:53:03 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Tue, 17 Jul 2007 14:23:03 +0530
Subject: [R] R and Copula
In-Reply-To: <11644534.post@talk.nabble.com>
Message-ID: <OF3FCBAFFE.844EBC50-ON6525731B.003075E1-6525731B.0030CAFF@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/8f879cee/attachment.pl 

From gyadav at ccilindia.co.in  Tue Jul 17 10:54:11 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Tue, 17 Jul 2007 14:24:11 +0530
Subject: [R] R and Copula
In-Reply-To: <11644534.post@talk.nabble.com>
Message-ID: <OFCA415BBB.C0E2CA8C-ON6525731B.0030DA14-6525731B.0030E546@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/c0376595/attachment.pl 

From r.nieuwenhuis at student.ru.nl  Tue Jul 17 10:57:26 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Tue, 17 Jul 2007 10:57:26 +0200
Subject: [R] elementary statistics with R (rkward?)
In-Reply-To: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
References: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
Message-ID: <CB0F358A-844F-42AA-88A3-A94125FDF63A@student.ru.nl>

Dear Donatas,

you might want to try my yown website, that is still under heavy  
construction (please use the tree-menu-structure):

http://rensenieuwenhuis.nl/r-project/manual


I'd like to hear what you think of it as a beginning R user. I want  
to mention though, that the books mentioned by other mailers are much  
better, complete and concise. The web though gives me the space to  
give many examples.

yours sincerely,

Rense Nieuwenhuis




On Jul 11, 2007, at 15:27 , Donatas G. wrote:

> Hi, I am trying to learn some basic statistics stuff but I cannot  
> find any
> elementary statistics exercises using R language. Using RKward  
> would be even
> better...
>
> I need that in analysing sociological data, obtained through  
> questionnairres -
> findind corelations between variables, relations between different  
> types of
> data, etc.
>
> Could anyone recommend simple tutorials/exercises, available on www  
> for me to
> work on?
>
> I realize it would be much simple to do this introductory stuff  
> with spss, that
> everyone around me is using here in Lithuania, but I'd really like  
> to learn to
> do it with R instead...
>
> -- 
> Donatas G.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Tue Jul 17 11:00:28 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 17 Jul 2007 02:00:28 -0700 (PDT)
Subject: [R] scaling of different data sets in ggplot
In-Reply-To: <f8e6ff050707162148j248f7ddw99391d91dc02de81@mail.gmail.com>
Message-ID: <294098.36195.qm@web39706.mail.mud.yahoo.com>

Hi Hadley,

That was also my initial thought as well, that maybe having different scales
on the same figure would obfuscate the structure and meaning of the data. But
I think in some instances (i.e., publications where page limits are imposed)
I think it's desirable to condense a lot of information onto a single plot
(for instance, if they show the same trend - even if they are not in the same
units), which means having more than one scale in the same plotting window. I
haven't checked what Tufte, Cleveland, and Wilkinson have to say about this,
but in practice I don't think it's all that uncommon.

I agree that log(z) is an operation on the data set, but representing it
graphically can be accomplished either through plotting log(z), or plotting z
on a log scale... in either case having an extra axis showing y and z [and
not log(z)] would be nice I would think.

I haven't tried it in lattice but in the traditional graphics system it is
quite straight-forward. Your claim says that ggplot takes 'tries to take the
good parts of base and lattice graphics and none of the bad parts' - just
trying to hold you to your word :).

Seriously though, I think the idea of ggplot (and implementation) is really
great. Currently R has many graphics systems, of which I know traditional and
lattice - and both are really fantastic (I plan to learn grid sometime in the
future) and I am fanatical about them. But for students and colleagues who
have less programming experience, I think the learning curve for lattice (to
gain proficiency, that is) may be a tad steep... I've been playing around
with ggplot to see if it would be a gentler introduction to conditioning
plots and analysis of multivariate datasets - which, in a way, I think it
could be - so I'm currently trying to test the limits of its flexibility.
It's true that there are some plotting concepts that are generally
discouraged, but it seems to me that the ultimate discretion should lie with
the user, and the plotting system should give him/her the freedom to choose
[to make a bad plot]. Even Lee Wilkinson says in his book that his grammar
will allow someone to make meaningless plots. One example that comes to mind
is the pie chart - I know they are heavily discouraged, but in some
communities, it's commonly used and therefore expected; to communicate to
that particular audience it's sometimes necessary to speak their language...

So, hope you don't mind, but I may ask some more 'can ggplot do this'
questions in the future. But keep up the good work,

Stephen


--- hadley wickham <h.wickham at gmail.com> wrote:

> Hi Stephen,
> 
> You can't do that in ggplot (have two different scales) because I
> think it's generally a really bad idea.  The whole point of plotting
> the data is so that you can use your visual abilities to gain insight
> into the data.  When you have two different scales the positions of
> the two groups are essentially arbitrary - the data only have x values
> in common, not y values.  You essentially have two almost unrelated
> graphs plotted on top of each other.
> 
> On the other hand, for this data, I think it would be reasonable to
> plot log(z) and y on the same scale - the data is transformed not the
> scales.
> 
> Hadley
> 
> On 7/14/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> > Dear list (but probably mostly Hadley):
> >
> > In ggplot, operations to modify 'guides' are accessed through grid
> > objects, but I did not find mention of creating new guides or possibly
> > removing them altogether using ggplot functions. I wonder if this is
> > something I need to learn grid to learn more about (which I hope to do
> > eventually).
> >
> > Also, ggplot()+geom_object() [where 'object' can be point, line, etc.]
> > or layer() contains specification for the data, mappings and
> > geoms/stats - but the geoms/stats can be scale-dependent [for
> > instance, log]. so I wonder how different scalings can be applied to
> > different data sets.
> >
> > Below is an example that requires both:
> >
> > x <- runif(100) y <- exp(x^2) z <- x^2+rnorm(100,0,0.02)
> >
> > par(mar=c(5,4,2,4)+0.1) plot(x,y,log="y") lines(lowess(x,y,f=1/3))
> > par(new=TRUE) plot(x,z,col=2,pch=3,yaxt="n",ylab="")
> > lines(lowess(x,z,f=1/3),col=2) axis(4,col=2,col.axis=2)
> > mtext("z",4,line=3,col=2)
> >
> > In ggplot:
> >
> > ## data specification
> > ggplot(data=data.frame(x,y,z)) +
> >
> >   ## first set of points geom_point(mapping=aes(x=x,y=y)) +
> >   ## scale_y_log() +
> >
> >   ## second set of points geom_point(mapping=aes(x=x,y=z),pch=3) +
> >   ## layer(mapping=aes(x=x,y=z),stat="smooth",method="loess") +
> >   ## scale_y_continuous()
> >
> > scale_y_log() and scale_y_continuous() appear to apply to both mappings
> at
> > once, and I can't figure out how to associate them with the intended ones
> (I
> > expect this will be a desire for size and color scales as well).
> >
> > Of course, I can always try to fool the system by (1) applying the
> scaling a
> > priori to create a new variable, (2) plotting points from the new
> variable,
> > and (3) creating a new axis with custom labels. Which then brings me back
> to
> > ...how to add new guides? :)
> >
> > Thanks,
> >
> > Stephen
> >
> >
> >
> >      
>
____________________________________________________________________________________
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From brown_emu at yahoo.com  Tue Jul 17 11:07:05 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 17 Jul 2007 02:07:05 -0700 (PDT)
Subject: [R] Optimization
In-Reply-To: <JLBAZJ$39C399C584DC82796C77CF86331242D2@poste.it>
Message-ID: <744074.79178.qm@web39708.mail.mud.yahoo.com>

f <- function(x)
  (sqrt((x[1]*0.114434)^2+(x[2]*0.043966)^2+(x[3]*0.100031)^2)-0.04)^2

optim(c(0,0,0),f)

see ?optim for details on arguments, options, etc.

--- "massimiliano.talarico" <massimiliano.talarico at poste.it> wrote:

> I'm sorry the function is 
> 
> sqrt((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> 
> Have you any suggests.
> 
> Thanks,
> Massimiliano
> 
> 
> 
> What is radq?
> 
> --- "massimiliano.talarico"
> <massimiliano.talarico at poste.it> wrote:
> 
> > Dear all,
> > I need a suggest to obtain the max of this function:
> > 
> > Max x1*0.021986+x2*0.000964+x3*0.02913
> > 
> > with these conditions:
> > 
> > x1+x2+x3=1;
> >
> radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> > x1>=0;
> > x1<=1;
> > x2>=0;
> > x2<=1;
> > x3>=0;
> > x3<=1;
> > 
> > Any suggests ?
> > 
> > Thanks in advanced,
> > Massimiliano
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> > reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Tue Jul 17 11:09:25 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 17 Jul 2007 02:09:25 -0700 (PDT)
Subject: [R] Optimization
In-Reply-To: <JLBAZJ$39C399C584DC82796C77CF86331242D2@poste.it>
Message-ID: <525883.80462.qm@web39708.mail.mud.yahoo.com>

My apologies, didn't see the boundary constraints. Try this one...

f <- function(x)
  (sqrt((x[1]*0.114434)^2+(x[2]*0.043966)^2+(x[3]*0.100031)^2)-0.04)^2

optim(par=rep(0,3),f,lower=rep(0,3),upper=rep(1,3),method="L-BFGS-B")

and check ?optim

--- "massimiliano.talarico" <massimiliano.talarico at poste.it> wrote:

> I'm sorry the function is 
> 
> sqrt((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> 
> Have you any suggests.
> 
> Thanks,
> Massimiliano
> 
> 
> 
> What is radq?
> 
> --- "massimiliano.talarico"
> <massimiliano.talarico at poste.it> wrote:
> 
> > Dear all,
> > I need a suggest to obtain the max of this function:
> > 
> > Max x1*0.021986+x2*0.000964+x3*0.02913
> > 
> > with these conditions:
> > 
> > x1+x2+x3=1;
> >
> radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> > x1>=0;
> > x1<=1;
> > x2>=0;
> > x2<=1;
> > x3>=0;
> > x3<=1;
> > 
> > Any suggests ?
> > 
> > Thanks in advanced,
> > Massimiliano
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> > reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



      ____________________________________________________________________________________


From landronimirc at gmail.com  Tue Jul 17 11:26:14 2007
From: landronimirc at gmail.com (Liviu Andronic)
Date: Tue, 17 Jul 2007 12:26:14 +0300
Subject: [R] elementary statistics with R (rkward?)
In-Reply-To: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
References: <20070711162725.itjdigx2ia04kkk8@webmail.akl.lt>
Message-ID: <68b1e2610707170226v4eee36fcua231276416410aac@mail.gmail.com>

On 7/11/07, Donatas G. <dgvirtual at akl.lt> wrote:
> Hi, I am trying to learn some basic statistics stuff but I cannot find any
> elementary statistics exercises using R language. Using RKward would be even
> better...

[..]
> I realize it would be much simple to do this introductory stuff with spss, that
> everyone around me is using here in Lithuania, but I'd really like to learn to
> do it with R instead...


You seem to be looking for a practical GUI allowing you to learn R in
a fairly easy way. Then, you're probably heading towards  the  R
Commander [1] package. SciViews-R [2] is also worth looking at as a
decent alternative to programs like SPSS. However, watch out for the
incompatibility issues of SciViews with recent R _and_ Linux (the
author would fix both problems somewhere towards September/October).
If you want to check other GUIs, look at the link on  available GUIs
[3] (go directly to Overview); you might be interested by anything
starting with Rcmdr and bellow. On a side note, I tried a recent
version of RKward and to me the program felt, just as its name
suggests, awkward.

As for  understanding the basics of R, you would be interested in
checking this document on  differences between SPSS or SAS and R [4].
This pdf reference [5] is also interesting for beginners (like myself,
by the way).

Hope this is of help. Regards,
Liviu

[1] http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/
[2] http://www.sciviews.org/SciViews-R/
[3] http://www.sciviews.org/_rgui/
[4] http://oit.utk.edu/scc/RforSAS&SPSSusers.pdf
[5] http://www.unt.edu/rss/Teaching-with-R.pdf


From bacaro at unisi.it  Tue Jul 17 11:32:20 2007
From: bacaro at unisi.it (giovanni bacaro)
Date: Tue, 17 Jul 2007 11:32:20 +0200
Subject: [R] Diagnostics for Generalized Mixed Model
Message-ID: <000601c7c855$5fa9d710$554a10ac@fisso>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/6bc02d40/attachment.pl 

From abr-r-project at xylon.de  Tue Jul 17 11:53:58 2007
From: abr-r-project at xylon.de (Arne Brutschy)
Date: Tue, 17 Jul 2007 11:53:58 +0200
Subject: [R] Bug in PDF device when using transparent graphics
Message-ID: <43970648.20070717115358@xylon.de>

Hello,

I think I found a bug when creating graphics on a PDF device using
transparency. I'm using ggplots2 like this:

comp <- ggplot(data, aes(x=Problemsize, y=Fitness)) +
        geom_smooth(fill=alpha("blue", 0.2), aes(colour=factor(Clustersize)), size=2)

Which gives me a curve for each "Clustersize" with a transparent blue
"cloud" around it, indication the standard deviation. Of course, using
windows you cannot see this transparent cloud, but when opening a pdf
containing the plot in Adobe Acrobat all is displayed properly.

But: when printing the resulting pdf, only one of the plotted lines
have a transparent stddev, all the others are solid. At first I though
this is a bug of the printer driver... But when importing the pdf file
to CorelDraw, the result is the same. Only one of the blue stddev
markers has a transparency property, the others are simply solid.

BTW, trying to crop the pdf by using "pdfcrop" on linux results in an
error. After fixing the missing transparency in CorelDraw and
exporting the pdf again, this works as well...

I'm not sure if this is a bug or just an improper pdf spec, as Acrobat
displays the the pdf properly. Has anyone encountered something like
this?

Arne


From info at aghmed.fsnet.co.uk  Tue Jul 17 12:28:31 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Tue, 17 Jul 2007 11:28:31 +0100
Subject: [R] Partial Proportional Odds model using vglm
In-Reply-To: <BAY116-DAV1675B3ED045F094B91EF31C1F80@phx.gbl>
References: <BAY116-DAV1675B3ED045F094B91EF31C1F80@phx.gbl>
Message-ID: <Zen-1IAkIH-0003RB-GA@heisenberg.zen.co.uk>

At 13:01 16/07/2007, Rizwan Younis wrote:
>Hi:
>I am trying to fit a PPO model using vglm from VGAM, and get an error while
>executing the code.

You seem to keep posting the same problem.
Since the only person who can tell you what is happening inside VGAM 
is probably the maintainer a more efficient strategy would be to 
email him as the instructions ask you to do.

However if that fails then try simplifying your problem to see if the 
error goes away
1) try merging ages 1 and 2, and ages 4 and 5
2) try merging column 3 "2" with 2 "1" or 4 "3"


>Here is the data, code, and error:
>
>Data  = rc13, first row is the column names. a = age, and 1,2,3, 4 and 5 are
>condition grades.
>
>   a  1 2 3  4 5
>   1  1 0 0  0 0
>   2 84 2 7 10 2
>   3 16 0 6  6 2
>   4 13 0 3  4 0
>   5  0 0 0  1 0
>
>Library(VGAM)
>
>rc13<-read.table("icg_rcPivot_group2.txt",header=F)
>names(rc13)<-c("a","1","2","3","4","5")
>
>ppo<-vglm(cbind(rc13[,2],rc13[,3],rc13[,4],rc13[,5],rc13[,6])~a,family =
>cumulative(link = logit, parallel = F , reverse = F),na.action=na.pass,
>data=rc13)
>summary(ppo)
>
>I get the following error:
>
>Error in "[<-"(`*tmp*`, , index, value = c(1.13512932539841,
>0.533057528200189,  :
>         number of items to replace is not a multiple of replacement length
>In addition: Warning messages:
>1: NaNs produced in: log(x)
>2: fitted values close to 0 or 1 in: tfun(mu = mu, y = y, w = w, res =
>FALSE, eta = eta, extra)
>3: 19 elements replaced by 1.819e-12 in: checkwz(wz, M = M, trace = trace,
>wzeps = control$wzepsilon)
>
>I will appreciate any help to fix this problem.
>Thanks
>
>Reez You
>Grad Student
>University of Waterloo
>Waterloo, ON Canada

Michael Dewey
http://www.aghmed.fsnet.co.uk


From ripley at stats.ox.ac.uk  Tue Jul 17 12:43:15 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jul 2007 11:43:15 +0100 (BST)
Subject: [R] LSD, HSD,...
In-Reply-To: <99378582-DD71-4033-AE98-1002C7166A65@anu.edu.au>
References: <mailman.8.1184580003.27115.r-help@stat.math.ethz.ch>
	<99378582-DD71-4033-AE98-1002C7166A65@anu.edu.au>
Message-ID: <Pine.LNX.4.64.0707170519150.19266@gannet.stats.ox.ac.uk>

On Tue, 17 Jul 2007, John Maindonald wrote:

> <follow-on rant> Stepwise regression variable selection
> methods make multiple post hoc comparisons.  The

Some do, but step() (the only way offered in base R) does not test at all.

> number of comparisons may be very large, vastly more
> than the half-dozen post-hoc comparisons that are
> common in an experimental design context.
>
> There is a disconnect here. The multiple testing issue is
> noted in pretty much every discussion of analysis of
> experimental data, but not commonly mentioned (at least
> in older texts) in discussions of stepwise regression, best
> subsets and related regression approaches. One reason
> for this silence may be that there is no ready HSD-like fix.
>
> The SEs and t-statistics that lm() gives for the finally
> selected model can be grossly optimistic. Running the
> analysis with the same model matrix, but with y-values
> that are noise, can give a useful wake-up call.

Predictions from any single model will also have 'optimistic' standard 
errors.  The major problem is attempting to select a single model and 
there is also a problem with assuming the model to be true (which 
Huber-White so-called 'sandwich' estimators try to avoid, and robust 
fitting does so more comprehensively).  If you really want to assess 
uncertainty you need to take into account that the models are false and 
that several models may capture different aspects of the data and so be 
false in different ways.


> John Maindonald             email: john.maindonald at anu.edu.au
> phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
> Centre for Mathematics & Its Applications, Room 1194,
> John Dedman Mathematical Sciences Building (Building 27)
> Australian National University, Canberra ACT 0200.
>
>
> On 16 Jul 2007, at 8:00 PM, Simon Blomberg wrote:
>
>> If you have a priori planned comparisons, you can just test those
>> using
>> linear contrasts, with no need to correct for multiple testing. If you
>> do not, and you are relying on looking at the data and analysis to
>> tell
>> you which treatment means to compare, and you are considering several
>> tests, then you should consider correcting for multiple testing. There
>> is a large literature on the properties of the various tests.
>> (Tukey HSD
>> usually works pretty well for me.)
>>
>> <rant> Why do people design experiments with a priori hypotheses in
>> mind, yet test them using post hoc comparison procedures? It's as if
>> they are afraid to admit that they had hypotheses to begin with! Far
>> better to test what you had planned to test using the more powerful
>> methods for planned comparisons, and leave it at that.
>> </rant>
>>
>>
>> On Mon, 2007-07-16 at 09:52 +0200, Adrian J. Montero Calvo wrote:
>>> Hi,
>>>     I'm designing a experiment in order to compare the growing of
>>> several clones of a tree specie. It will be a complete randomized
>>> block
>>> design. How can I decide what model of mean comparision to choose?
>>> LSD,
>>> HSD,TukeyHSD,  Duncan,...?  Thanks in advance
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> --
>> Simon Blomberg, BSc (Hons), PhD, MAppStat.
>> Lecturer and Consultant Statistician
>> Faculty of Biological and Chemical Sciences
>> The University of Queensland
>> St. Lucia Queensland 4072
>> Australia
>> Room 320 Goddard Building (8)
>> T: +61 7 3365 2506
>> email: S.Blomberg1_at_uq.edu.au
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From william.a.simpson at gmail.com  Tue Jul 17 12:58:13 2007
From: william.a.simpson at gmail.com (William Simpson)
Date: Tue, 17 Jul 2007 11:58:13 +0100
Subject: [R] fit a nonlinear model using nlm()
Message-ID: <21ce39b20707170358y582a72a6x17ee46f43fafd194@mail.gmail.com>

I am trying to fit a nonlinear model using nlm().
My application of nlm() is a bit complicated.
Here is the story behind the model being fit:

The observer is trying to detect a signal corrupted by noise.
On each trial, the observer gets stim=signal+rnorm().

In the simulation below I have 500 trials. Each row of stim is a new trial.
On each trial, if the cross-correlation between the stim and
the signal is above some criterion level (crit=.5 here), the
observer says "signal" (resp=1), else he says "no signal"
(resp=0).

So the situation is this: I know resp and stim for each trial.
I would like to estimate crit and signal from these.
(You might say that I already know the signal and crit. But the
idea here is that the observer cross-correlates the stim with an
internal template that will not be identical to the actual signal.
I want to estimate this template. Also the observer's crit may
differ from the "correct" one.)

In the code below, please help me get the f() and nlm() bits right.
I want to estimate signal and crit given stim and resp.

Thanks very much for any help!
Bill


x<-1:100
con<-.1
signal<-con*cos(2*pi*3*x/length(x))

crit<-.5
noisesd<-.1

# each row is a new stim (trial). 500 trials
resp<-array(dim=500)
stim<-matrix(nrow=500,ncol=100)
for (i in 1:500)
{
stim[i,]<-signal+rnorm(n=length(signal), mean=0, sd=noisesd)
if (sum(stim[i,]*signal)>crit) (resp[i]<-1) else (resp[i]<-0)
}


f<-function(signalest)
{
r<-array(dim=500)
for (i in 1:500)
{
r[i]<-sum(stim[i,]*signalest)>critest
}
sum((r-resp)^2)
}

fit<-nlm(f, stim[1,])


From d.vanderelst at tue.nl  Tue Jul 17 13:05:53 2007
From: d.vanderelst at tue.nl (Dieter Vanderelst)
Date: Tue, 17 Jul 2007 13:05:53 +0200
Subject: [R] Combine R2HTML and Rcmd BATCH?
Message-ID: <469CA291.7020407@tue.nl>

Hi All,

I have an R script that spawns output in the form of an HTML page. This 
is done by the R2HTML package.

Now I want to run the same script using Rcmd BATCH. However, it seems 
that it is not possible to use R2HTML in this case.

My script ends with this error message:
#########################
Error in dev.print(png, file = AbsGraphFileName, width = Width, height = 
Height,  :

can only print from screen device

Execution halted
#########################

I can not find how to work around this problem in the R2HTML manual or 
the help archives.

Has anybody done a similar thing before? Any suggestions?

Greetings,
Dieter

-- 
Dieter Vanderelst
d.vanderelst at tue.nl
Department of Industrial Design
Designed Intelligence


From knoblauch at lyon.inserm.fr  Tue Jul 17 13:11:02 2007
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Tue, 17 Jul 2007 11:11:02 +0000 (UTC)
Subject: [R] fit a nonlinear model using nlm()
References: <21ce39b20707170358y582a72a6x17ee46f43fafd194@mail.gmail.com>
Message-ID: <loom.20070717T130042-263@post.gmane.org>

William Simpson <william.a.simpson <at> gmail.com> writes:

> 
> I am trying to fit a nonlinear model using nlm().
> The observer is trying to detect a signal corrupted by noise.
> On each trial, the observer gets stim=signal+rnorm().
> 
> In the simulation below I have 500 trials. Each row of stim is a new trial.
> On each trial, if the cross-correlation between the stim and
> the signal is above some criterion level (crit=.5 here), the
> observer says "signal" (resp=1), else he says "no signal"
> (resp=0).
> 
> Thanks very much for any help!
> Bill
> 
> 
It sounds like you are doing a classification image experiment.  
You can use tapply() to get means for each x as a function of the 
observer's classifications and then combine them as a function of 
hits, false alarms, misses, correct rejections using the weights
 1 -1, -1, 1, as in Ahumada's original approach.
You can do this with lm() if you set it up so that the noise is 
the response and the classifications are a 4 level factor that prediccts 
them and the contrasts are set up as above.  
I think that it would be better to set it up as a glm, however,
where the responses of the observer are binary responses, the 
noise and the presence/absence of the signal are predictor variables, 
with a binomial family. 

I have example code that does each of these if you are 
interested and if it is only for simulation, see 

@Article{pmid16277294,
   Author="Thomas, James P and Knoblauch, Kenneth",
   Title="{{F}requency and phase contributions to the detection of 
   temporal luminance modulation}",
   Journal="J Opt Soc Am A Opt Image Sci Vis",
   Year="2005",
   Volume="22",
   Number="10",
   Pages="2257--2261",
   Month="Oct"
}

for which I can send you the code also.

Ken


From johannes_graumann at web.de  Tue Jul 17 13:32:07 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Tue, 17 Jul 2007 13:32:07 +0200
Subject: [R] SLLOOOWWW function ...
Message-ID: <f7i9bn$6e7$1@sea.gmane.org>

Does anybody have any insight into how to make this faster?
I suspect, that the rounding going on may be an issue, as is the stepping
through data frame rows using integers ...

If you have the patience to teach a noob, he will highly appreciate it ;0)

Joh

digit <- 4
for (minute in seq(from=25,to=lrange[2])){
  # Extract all data associtaed with the current time (minute)
  frame <- subset(mylist,mylist[["Time"]] == minute)
  # Sort by Intensity
  frame <- frame[order(frame[["Intensity"]],decreasing = TRUE),]
  # Establish output frame using the most intense candidate
  newframe <- frame[1,]
  # Establish overlap-checking vector using the most intense candidate
  lowppm <- round(newframe[1,][["Mass"]]-newframe[1,
[["Mass"]]/1E6*ppmrange,digits=digit)
  highppm <- round(newframe[1,][["Mass"]]+newframe[1,
[["Mass"]]/1E6*ppmrange,digits=digit)
  presence <- seq(from=lowppm,to=highppm,by=10^(-digit))
  # Walk through the entire original frame and check whether peaks are
overlap-free ... do so until max of 2000 entries
  for (int in seq(from=2,to=nrow(frame))) {
    if(nrow(newframe) < 2000) {
      lowppm <- round(frame[int,][["Mass"]]-frame[int,
[["Mass"]]/1E6*ppmrange,digits=digit)
      highppm <- round(frame[int,][["Mass"]]+frame[int,
[["Mass"]]/1E6*ppmrange,digits=digit)
      windowrange <- seq(from=lowppm,to=highppm,by=10^(-digit))
      if (sum(round(windowrange,digits=digit) %in%
round(presence,digits=digit)) < 1) {
        newframe <- rbind(newframe,frame[int,])
        presence <- c(presence,windowrange)
      }
    } else {
      break()
    }
  }


From Massimiliano.Talarico at unicreditxelion.it  Tue Jul 17 10:00:17 2007
From: Massimiliano.Talarico at unicreditxelion.it (Talarico Massimiliano (UniCredit Xelion Banca))
Date: Tue, 17 Jul 2007 10:00:17 +0200
Subject: [R] Optimization (MAX) with R
Message-ID: <E14EC333CA5C844EBBED6AC9DE19A2D6DFE16F@USEBW104.mailasp.unicredit.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/381d05a1/attachment.pl 

From wills at stats.ox.ac.uk  Tue Jul 17 12:50:33 2007
From: wills at stats.ox.ac.uk (Quin Wills)
Date: Tue, 17 Jul 2007 11:50:33 +0100
Subject: [R] multiple rugs on a single plot
Message-ID: <002801c7c860$533227c0$a45f11ac@simugenqfwills>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/6c8e6569/attachment.pl 

From roger.bos at us.rothschild.com  Tue Jul 17 13:55:54 2007
From: roger.bos at us.rothschild.com (Bos, Roger)
Date: Tue, 17 Jul 2007 07:55:54 -0400
Subject: [R] Combine R2HTML and Rcmd BATCH?
In-Reply-To: <469CA291.7020407@tue.nl>
Message-ID: <D8C95B444AD6EE4AAD638D818A9CFD347CFEF6@RINNYCSE000.rth.ad.rothschild.com>

Dieter,

I use htmlize in library prettyR to produce a HTML report at the end of
a long batch file and this process works fine.  I posted my call to
htmlize below, but it would take too much modification for me to produce
a working example.  My guess is that it is something else that is
messing up the process.  I bet if you created a very simple example it
would work.  And if it didn't work, you could post it here and then
people could try to find out why it doesn't work.  If it does work, you
can add to it gradually until you find the problem.  This is how I got
htmlize to work.

Roger


library(prettyR)
today <- as.Date(Sys.time(), format="%m/%d/%Y")
htmlize(Rfile="//rinnycs0051/research/R_HOME/morningNotesMaster.r",
HTMLbase="morningNotes", HTMLdir="//rinnycs0051/ramdata/morningNotes",
title="RAM Morning Notes " %+% today, echo=FALSE, do.nav=FALSE)


 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dieter Vanderelst
Sent: Tuesday, July 17, 2007 7:06 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Combine R2HTML and Rcmd BATCH?

Hi All,

I have an R script that spawns output in the form of an HTML page. This
is done by the R2HTML package.

Now I want to run the same script using Rcmd BATCH. However, it seems
that it is not possible to use R2HTML in this case.

My script ends with this error message:
#########################
Error in dev.print(png, file = AbsGraphFileName, width = Width, height =
Height,  :

can only print from screen device

Execution halted
#########################

I can not find how to work around this problem in the R2HTML manual or
the help archives.

Has anybody done a similar thing before? Any suggestions?

Greetings,
Dieter

--
Dieter Vanderelst
d.vanderelst at tue.nl
Department of Industrial Design
Designed Intelligence

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

********************************************************************** * 
This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.


From marc_schwartz at comcast.net  Tue Jul 17 14:32:50 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 17 Jul 2007 07:32:50 -0500
Subject: [R] [R-sig-DB] RODBC on Oracle DB
In-Reply-To: <4556.1184657064@net2000.ch>
References: <4556.1184657064@net2000.ch>
Message-ID: <1184675570.6752.11.camel@Bellerophon.localdomain>

Try the sqlQuery() syntax with a semi-colon at the end of it:

  sqlQuery(essai, "select * from S_TYP_COLLEGES;")

Oracle requires the semi-colon at the end of the SQL statement.

If that does not help, try these queries using the Oracle Instant Client
command line application outside of R and see if your queries work
there.  If so, then we can likely isolate the problem to R.  If not,
then there is an ODBC/Oracle configuration issue.

If you are unsure of how to use (or perhaps install) the Oracle Instant
Client, check with one of your SysAdmins.

BTW, unstated is the OS here, but I presume Windows, given the ODBC
driver version and DLL noted previously.

HTH,

Marc Schwartz


On Tue, 2007-07-17 at 09:24 +0200, eric at net2000.ch wrote:
> essai <- odbcConnect("ORESTE_prod",  uid="osis_r",  pwd="12miss15" ,case="oracle")
> 
> > sqlTables(essai)$ORESTE
> 
> ...
> 
> 1315      <NA>      ORESTE              S_PROFESSIONS_OLD        TABLE    <NA>
> 1316      <NA>      ORESTE                  S_PROVENANCES        TABLE    <NA>
> 1317      <NA>      ORESTE                        S_SEXES        TABLE    <NA>
> 1318      <NA>      ORESTE                 S_SOUS_CLASSES        TABLE    <NA>
> 1319      <NA>      ORESTE                 S_TYP_COLLEGES        TABLE    <NA>
> 1320      <NA>      ORESTE             S_TYP_ENSEIGNEMENT        TABLE    <NA>
> 
> ...
> 
> > sqlQuery(essai, "select * from S_TYP_COLLEGES")
> [1] "[RODBC] ERROR: Could not SQLExecDirect"                            
> [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
> 
> I have also tried the 
> essai2 <- odbcDriverConnect(connection="essai2")
> But with no succes. 
> 
> 
> 
> On Lun Juil 16 15:32 , Prof Brian Ripley <ripley at stats.ox.ac.uk> sent:
> 
> >The problem could be quoting, if Oracle is not standards-compliant.
> >See the options in ?odbcConnect.
> >
> >If sqlQuery(essai, "select * from S_TYP_COLLEGES") works, this is likely 
> >to be the problem.
> >
> >On Mon, 16 Jul 2007, eric at net2000.ch wrote:
> >
> >>
> >>
> >>> essai 
> >>> odbcGetInfo(essai)
> >>       DBMS_Name         DBMS_Ver  Driver_ODBC_Ver
> >>        "Oracle"     "09.00.0121"          "03.51"
> >> Data_Source_Name      Driver_Name       Driver_Ver
> >>   "ORESTE_prod"    "SQORA32.DLL"     "09.00.0101"
> >>        ODBC_Ver      Server_Name
> >>    "03.52.0000"           "weba"
> >>
> >>
> >>> sqlTables(essai)
> >>
> >> The result of this function is a liste of tables, one of them is called:
> >> S_TYP_COLLEGES.
> >>
> >>
> >>> sqlFetch(essai,"S_TYP_COLLEGES")
> >> [1] "[RODBC] ERROR: Could not SQLExecDirect"
> >> [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
> >>
> >>> sqlFetch(essai, "S_TYP_COLLEGES", colnames=TRUE, rownames=FALSE)
> >> [1] "[RODBC] ERROR: Could not SQLExecDirect"
> >> [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
> >>
> >>
> >> What could be the problem here ?
> >> Any help is welcome
> >> Eric R?thlisberger, Neuch?tel
> >>
> >> _______________________________________________
> >> R-sig-DB mailing list -- R Special Interest Group
> >> R-sig-DB at stat.math.ethz.ch
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-db
> >>
> >
> >-- 
> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >University of Oxford,             Tel:  +44 1865 272861 (self)
> >1 South Parks Road,                     +44 1865 272866 (PA)
> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From birgit.lemcke at systbot.uzh.ch  Tue Jul 17 14:32:55 2007
From: birgit.lemcke at systbot.uzh.ch (Birgit Lemcke)
Date: Tue, 17 Jul 2007 14:32:55 +0200
Subject: [R] distance function (analogue)
Message-ID: <B49AAAEF-50E8-483B-82EF-0E1BC5B11407@systbot.uzh.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/cf3a84ae/attachment.pl 

From marc_schwartz at comcast.net  Tue Jul 17 14:36:57 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 17 Jul 2007 07:36:57 -0500
Subject: [R] print tables to figure in R
In-Reply-To: <XFMail.070717092337.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070717092337.ted.harding@nessie.mcc.ac.uk>
Message-ID: <1184675817.6752.16.camel@Bellerophon.localdomain>

On Tue, 2007-07-17 at 09:23 +0100, ted.harding at nessie.mcc.ac.uk wrote:
> On 16-Jul-07 22:11:31, Steve Powers wrote:
> > Doe anyone know of a way to print nice-looking, printer-friendly tables
> > directly in the R plot window? This would be very handy for examining 
> > preliminary results from frequently used routines. I've hunted for this
> > with no luck. All anyone seems to do is export text files to excel for 
> > formatting, or use the print(bla) function which just calls the numbers
> > at the command line. There must be a way to do this.---steve
> 
> It would certainly be possible to write a function include a table
> within a plot, since a table is no more than bits of text laid out
> positionally in an array, and there is already provision to place
> text at assigned positions on a plot -- the function text() does this.
> 
> That being said, though, writing the code to layout a particular
> table in the particular way you want would not be trivial. I don't
> know of an R function which implements a reasonable "default layout"
> for arbitrary tables in a plot.
> 
> As an example of "doing it with your bare hands", run the following:
> 
> x<-0.5*(0:20);y<-(x^2)/10;plot(x,y,pch="+",col="blue",ylim=c(-10,10))
> lines(x,y,col="blue")
> A<-c(0,2.5,5,7.5,10); B<-(A^2)/10;points(A,B,pch=2,col="red")
> Ach<-as.character(round(A,digits=1))
> Bch<-as.character(round(B,digits=3))
> text(x=c(6.25,8.0),y=c(0,0),labels=c("A","B"),pos=4)
> for(i in (1:5)){text(x=6.125,y=(-1-i),labels=Ach[i],pos=4)}
> for(i in (1:5)){text(x=7.875,y=(-1-i),labels=Bch[i],pos=4)}
> 
> You could augment this with grid lines, etc. according to taste.
> 
> Best wishes,
> Ted.

Another option is to look at the textplot() function in the 'gplots'
package on CRAN.

Also, run demo(plotmath) to look at some examples of a table/cell
layout. Specifically, the draw.title.cell() and draw.plotmath.cell()
functions used in the example output.

HTH,

Marc Schwartz


From dimitrios.rizopoulos at student.kuleuven.be  Tue Jul 17 14:40:13 2007
From: dimitrios.rizopoulos at student.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 17 Jul 2007 14:40:13 +0200
Subject: [R] multiple rugs on a single plot
References: <002801c7c860$533227c0$a45f11ac@simugenqfwills>
Message-ID: <002a01c7c86f$9eb55f00$0540210a@www.domain>

you could use different colours, e.g.,

x1 <- rnorm(100, -2.5, 1)
x2 <- rnorm(100, 0, 1)
x3 <- rnorm(100, 2.5, 1)
x <- c(x1, x2, x3)

plot(density(x))
rug(x1, col = "red")
rug(x2, col = "black")
rug(x3, col = "blue")

or something like the following:

plot(density(x))
len <- 0.005
ds <- 0.001
segments(x1, -1, x1, 0)
segments(x2, 0 + ds, x2, len)
segments(x3, len + ds, x3, 2*len)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Quin Wills" <wills at stats.ox.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, July 17, 2007 12:50 PM
Subject: [R] multiple rugs on a single plot


> Hi
>
>
>
> I could only find some discussion on this wrt lattice graphics 
> (which I'm
> not using). Apologies if I'm missing something obvious.
>
>
>
> I'd like to produce 3 rug plots under a kernel density plot for a
> population. The population is subdivided into 3 subpopulations, 
> which I'd
> like the rug plots to highlight. Naturally, when I do 3 rug plots, 
> they all
> plot over each other. I'd like 3 parallel rug plots along the 
> x-axis. But
> how.
>
>
>
> Thanks in advance,
>
> Quin
>
>
>
>
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.falciano at cineca.it  Tue Jul 17 14:44:50 2007
From: f.falciano at cineca.it (Francesco Falciano)
Date: Tue, 17 Jul 2007 14:44:50 +0200 (MEST)
Subject: [R] R and Genotyping
Message-ID: <021101c7c870$2d70fba0$9741a8c0@int.cineca.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/52472146/attachment.pl 

From massimiliano.talarico at poste.it  Tue Jul 17 14:57:40 2007
From: massimiliano.talarico at poste.it (massimiliano.talarico)
Date: Tue, 17 Jul 2007 14:57:40 +0200
Subject: [R] Optimization
Message-ID: <JLBQO4$E4CA8A4D5BA34F8500084FF8675385D0@poste.it>

Thanks for your suggests, but I need to obtain the MAX of 
this function:

Max x1*0.021986+x2*0.000964+x3*0.02913

with these conditions:

x1+x2+x3=1;

sqrt((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;

x1>=0;
x2>=0;
x3>=0;


Thanks and again Thanks,
Massimiliano



My apologies, didn't see the boundary constraints. Try this 
one...

f <- function(x)
  (sqrt((x[1]*0.114434)^2+(x[2]*0.043966)^2+(x[3]*0.100031)
^2)-0.04)^2

optim(par=rep(0,3),f,lower=rep(0,3),upper=rep
(1,3),method="L-BFGS-B")

and check ?optim

--- "massimiliano.talarico" 
<massimiliano.talarico at poste.it> wrote:

> I'm sorry the function is 
> 
> sqrt((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)
=0.04;
> 
> Have you any suggests.
> 
> Thanks,
> Massimiliano
> 
> 
> 
> What is radq?
> 
> --- "massimiliano.talarico"
> <massimiliano.talarico at poste.it> wrote:
> 
> > Dear all,
> > I need a suggest to obtain the max of this function:
> > 
> > Max x1*0.021986+x2*0.000964+x3*0.02913
> > 
> > with these conditions:
> > 
> > x1+x2+x3=1;
> >
> radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)
=0.04;
> > x1>=0;
> > x1<=1;
> > x2>=0;
> > x2<=1;
> > x3>=0;
> > x3<=1;
> > 
> > Any suggests ?
> > 
> > Thanks in advanced,
> > Massimiliano
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> > reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, 
reproducible code.
> 



      
____________________________________________________________
________________________
Fussy? Opinionated? Impossible to please? Perfect.  Join 
Yahoo!'s user panel and lay it on us. 
http://surveylink.yahoo.com/gmrs/yahoo_panel_invite.asp?a=7


From ulrich.keller at emacs.lu  Tue Jul 17 14:58:48 2007
From: ulrich.keller at emacs.lu (Ulrich Keller)
Date: Tue, 17 Jul 2007 12:58:48 +0000 (UTC)
Subject: [R] Multiple imputation with plausible values already in the data
Message-ID: <loom.20070717T143238-318@post.gmane.org>

Hello,

this is not really an R-related question, but since the posting guide does not
forbid asking non-R questions (even encourages it to some degree), I though I'd
give it a try.

I am currently doing some secondary analyses of the PISA (http://pisa.oecd.org)
student data. I would like to treat missing values properly, that is using
multiple imputation (with the mix package). But I am not sure how to do the
imputation, since the data set provided by the OECD already contains variables
with plausible values.

Roughly, the situation is like this: for each of the cognitive (achievement)
scales, there are five variables holding plausible values. So for example, there
is not one variable for math achievement, but five, pv1math through pv5math.
There are, of course, no missing values on these variables.

Most other variables show some degree of missing data. For example, some
students did not report their parents' occupation, so there is no information
about the socio-economic background (HISEI). This is the kind of data I want to
impute.

My first thought was splitting the data into five datasets, each holding only
one of the plausible value variables, but all of the "normal" variables. So e.g.
the first data set would include pv1math, pv1read, HISEI, and gender; while the
second would include pv2math, pv2read, HISEI, and gender. I would run mix on the
five data sets independently and end up with five imputed data sets with no
missing values.

But is this a valid approach? There would actually be two imputation runs per
data set: one for the plausible values on the achievement scales (done by the
OECD under an unknown model), and one for the other variables (done by me with
mix). The second run would use data from the first. Would this not lead to an
overestimation of the imputation variance? What alternative approaches are there?

Thank you in advance for you answers,

Uli


From wills at stats.ox.ac.uk  Tue Jul 17 15:19:17 2007
From: wills at stats.ox.ac.uk (Quin Wills)
Date: Tue, 17 Jul 2007 14:19:17 +0100
Subject: [R] multiple rugs on a single plot
In-Reply-To: <002a01c7c86f$9eb55f00$0540210a@www.domain>
Message-ID: <003501c7c875$19c88910$a45f11ac@simugenqfwills>

Excellent! Thanks.

-----Original Message-----
From: Dimitris Rizopoulos [mailto:dimitrios.rizopoulos at student.kuleuven.be] 
Sent: 17 July 2007 13:40
To: Quin Wills
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] multiple rugs on a single plot

you could use different colours, e.g.,

x1 <- rnorm(100, -2.5, 1)
x2 <- rnorm(100, 0, 1)
x3 <- rnorm(100, 2.5, 1)
x <- c(x1, x2, x3)

plot(density(x))
rug(x1, col = "red")
rug(x2, col = "black")
rug(x3, col = "blue")

or something like the following:

plot(density(x))
len <- 0.005
ds <- 0.001
segments(x1, -1, x1, 0)
segments(x2, 0 + ds, x2, len)
segments(x3, len + ds, x3, 2*len)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Quin Wills" <wills at stats.ox.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, July 17, 2007 12:50 PM
Subject: [R] multiple rugs on a single plot


> Hi
>
>
>
> I could only find some discussion on this wrt lattice graphics 
> (which I'm
> not using). Apologies if I'm missing something obvious.
>
>
>
> I'd like to produce 3 rug plots under a kernel density plot for a
> population. The population is subdivided into 3 subpopulations, 
> which I'd
> like the rug plots to highlight. Naturally, when I do 3 rug plots, 
> they all
> plot over each other. I'd like 3 parallel rug plots along the 
> x-axis. But
> how.
>
>
>
> Thanks in advance,
>
> Quin
>
>
>
>
>
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From avilella at gmail.com  Tue Jul 17 15:14:51 2007
From: avilella at gmail.com (Albert Vilella)
Date: Tue, 17 Jul 2007 14:14:51 +0100
Subject: [R] multiple rugs on a single plot
In-Reply-To: <002a01c7c86f$9eb55f00$0540210a@www.domain>
References: <002801c7c860$533227c0$a45f11ac@simugenqfwills>
	<002a01c7c86f$9eb55f00$0540210a@www.domain>
Message-ID: <358f4d650707170614u4ad66a39s1841dcd0dbc1ed46@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/c2eed7d5/attachment.pl 

From jmacdon at med.umich.edu  Tue Jul 17 15:23:53 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 17 Jul 2007 09:23:53 -0400
Subject: [R] R and Genotyping
In-Reply-To: <021101c7c870$2d70fba0$9741a8c0@int.cineca.it>
References: <021101c7c870$2d70fba0$9741a8c0@int.cineca.it>
Message-ID: <469CC2E9.9010203@med.umich.edu>

Hi Francesco,

Francesco Falciano wrote:
> I use dChip and Affymetrix gtype to apply some genotyping functions to some microarrays data.
> Do you know if similar R functions exist?
>   

Yes. Have you seen the affy and oligo packages of the Bioconductor project?

http://www.bioconductor.org

Best,

Jim


> Thank you!
> Francesco
>
> -----------------------------------------------------------
> Francesco Falciano, Ph.D.
> CINECA 
> (High Performance Systems)
> via Magnanelli, 6/3
> 40033 Casalecchio di Reno (BO)-ITALY
> tel: +39-051-6171724
> fax: +39-051-6132198
> e-mail: f.falciano at cineca.it
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623



**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From brown_emu at yahoo.com  Tue Jul 17 15:43:35 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 17 Jul 2007 06:43:35 -0700 (PDT)
Subject: [R] Optimization
In-Reply-To: <JLBQO4$E4CA8A4D5BA34F8500084FF8675385D0@poste.it>
Message-ID: <313888.48676.qm@web39707.mail.mud.yahoo.com>

My apologies, I read the post over too quickly (even the second time). 

It's been a while since I've played around with anything other than box
constraints, but this one is conducive to a brute-force approach (employing
Berwin suggestions). The pseudo-code would look something like this:

delta <- 1e-3   # grid space of x3, the smaller the better
oldvalue <- -Inf # some initial value for objective function
for( x3 in seq(0,1,by=delta) ) {
  ## calculate x1,x2 as per Berwin's response
  ## if all constraints are met, feasible <- TRUE
  ## else feasible <- FALSE
  if( !feasible ) next # if not feasible, go to next x3 value
  ## newvalue <- value of objective function with x1,x2,x3
  if( newvalue > oldvalue ) {
    oldvalue <- newvalue
    max.x1 <- x1; max.x2 <- x2; max.x3 <- x3
  }
}

You should end up with the desired values of max.x1, max.x2, max.x3. Hope
this helps,

ST



--- "massimiliano.talarico" <massimiliano.talarico at poste.it> wrote:

> Thanks for your suggests, but I need to obtain the MAX of
> this function:
> 
> Max x1*0.021986+x2*0.000964+x3*0.02913
> 
> with these conditions:
> 
> x1+x2+x3=1;
> 
> sqrt((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> 
> x1>=0;
> x2>=0;
> x3>=0;
> 
> 
> Thanks and again Thanks,
> Massimiliano
> 
> 
> 
> My apologies, didn't see the boundary constraints. Try this
> one...
> 
> f <- function(x)
>   (sqrt((x[1]*0.114434)^2+(x[2]*0.043966)^2+(x[3]*0.100031)
> ^2)-0.04)^2
> 
> optim(par=rep(0,3),f,lower=rep(0,3),upper=rep
> (1,3),method="L-BFGS-B")
> 
> and check ?optim
> 
> --- "massimiliano.talarico"
> <massimiliano.talarico at poste.it> wrote:
> 
> > I'm sorry the function is
> >
> > sqrt((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)
> =0.04;
> >
> > Have you any suggests.
> >
> > Thanks,
> > Massimiliano
> >
> >
> >
> > What is radq?
> >
> > --- "massimiliano.talarico"
> > <massimiliano.talarico at poste.it> wrote:
> >
> > > Dear all,
> > > I need a suggest to obtain the max of this function:
> > >
> > > Max x1*0.021986+x2*0.000964+x3*0.02913
> > >
> > > with these conditions:
> > >
> > > x1+x2+x3=1;
> > >
> > radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)
> =0.04;
> > > x1>=0;
> > > x1<=1;
> > > x2>=0;
> > > x2<=1;
> > > x3>=0;
> > > x3<=1;
> > >
> > > Any suggests ?
> > >
> > > Thanks in advanced,
> > > Massimiliano
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained,
> > > reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> 
> 
> 
> ____________________________________________________________
> ________________________
> Fussy? Opinionated? Impossible to please? Perfect.  Join
> Yahoo!'s user panel and lay it on us.
> http://surveylink.yahoo.com/gmrs/yahoo_panel_invite.asp?a=7
> 
> 
> 
> 



       
____________________________________________________________________________________

that gives answers, not web links.


From ggrothendieck at gmail.com  Tue Jul 17 15:50:28 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 17 Jul 2007 09:50:28 -0400
Subject: [R] trouble compiling Hmisc package
In-Reply-To: <46151.168.20.2.120.1184353772.squirrel@mail.skio.usg.edu>
References: <46151.168.20.2.120.1184353772.squirrel@mail.skio.usg.edu>
Message-ID: <971536df0707170650p66ebd3d5yabb50aed08a47f53@mail.gmail.com>

I came across this on a 64 bit Fedora machine as well.   I deleted the
src directory in the Hmisc source (and its contents) and commented out
this line in the NAMESPACE file in the Hmisc source:
  useDynLib(Hmisc)
which allowed it to build.  It turned out that I was not using any of the
deleted functionality anyways.

On 7/13/07, colton.smith at skio.usg.edu <colton.smith at skio.usg.edu> wrote:
> Hi:
>
>  We're trying to install the Hmisc package on a Solaris 9 machine.
> Here's what we get:
>
> R CMD INSTALL /usr/local/srctree/Hmisc_3.4-2.tar.gz
> * Installing to library '/usr/local/lib/R/library'
> * Installing *source* package 'Hmisc' ...
> ** libs
> g95   -fPIC  -g -O2 -c cidxcn.f -o cidxcn.o
> g95   -fPIC  -g -O2 -c cidxcp.f -o cidxcp.o
> g95   -fPIC  -g -O2 -c hoeffd.f -o hoeffd.o
> g95   -fPIC  -g -O2 -c jacklins.f -o jacklins.o
> g95   -fPIC  -g -O2 -c largrec.f -o largrec.o
> In file largrec.f:27
>
>      DO xl=xlim(1),xlim(2)-width,xinc
>          1
> Error: Loop variable at (1) must be a scalar INTEGER
> In file largrec.f:28
>
>         DO yl=ylim(1),ylim(2)-height,yinc
>             1
> Error: Loop variable at (1) must be a scalar INTEGER
> In file largrec.f:29
>
>            DO xr=xl+width,xlim(2),xinc
>                1
> Error: Loop variable at (1) must be a scalar INTEGER
> In file largrec.f:30
>
>               DO yu=yl+height,ylim(2),yinc
>                   1
> Error: Loop variable at (1) must be a scalar INTEGER
> make: *** [largrec.o] Error 1
> ERROR: compilation failed for package 'Hmisc'
> ** Removing '/usr/local/lib/R/library/Hmisc'
>
> Can anybody help us?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rvaradhan at jhmi.edu  Tue Jul 17 15:50:31 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 17 Jul 2007 09:50:31 -0400
Subject: [R] Optimization
In-Reply-To: <JLAHUK$B02D959B066823E595F53D454A1998AA@poste.it>
References: <JLAHUK$B02D959B066823E595F53D454A1998AA@poste.it>
Message-ID: <000301c7c879$70f04120$7c94100a@win.ad.jhu.edu>

Hi,

Your problem can be solved analytically.  Eliminate one of the variables,
say x3, from the problem by using the equality x1 + x2 + x3 = 1. Then solve
for the intersection of the circle (in x1 and x2) defined by the radical
constraint, with the straight line defined by the objective function.  There
will be, at most, two intersection points. The extremum has to be one of
these two points, provided they also satisfy the other inequalities (To me,
this sounds an awful lot like a homework problem).  


Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of massimiliano.talarico
Sent: Monday, July 16, 2007 4:50 PM
To: r-help
Subject: [R] Optimization

Dear all,
I need a suggest to obtain the max of this function:

Max x1*0.021986+x2*0.000964+x3*0.02913

with these conditions:

x1+x2+x3=1;
radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
x1>=0;
x1<=1;
x2>=0;
x2<=1;
x3>=0;
x3<=1;

Any suggests ?

Thanks in advanced,
Massimiliano

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From F.MENDIBURU at CGIAR.ORG  Tue Jul 17 15:54:19 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Tue, 17 Jul 2007 08:54:19 -0500
Subject: [R] LSD, HSD,...
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A201406AEB@webmail.cip.cgiar.org>

Dear Adrain,

You can see the library agricolae for the functions LSD.test, HSD.test, and Waller.test for Waller-Duncan. The criterion is that LSD is more used for few treatments and HSD for many treatments (more than 5) the test of Waller is Bayes and minimizes the two types of error (I or II). In experiment with clones, we prefer Waller-Duncan.

Felipe de Mendiburu
Statistician.
International Potato Center Lima-Peru

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Adrian J. Montero
Calvo
Sent: Monday, July 16, 2007 2:52 AM
To: r-help at stat.math.ethz.ch
Subject: [R] LSD, HSD,...


Hi,
    I'm designing a experiment in order to compare the growing of 
several clones of a tree specie. It will be a complete randomized block 
design. How can I decide what model of mean comparision to choose? LSD, 
HSD,TukeyHSD,  Duncan,...?  Thanks in advance

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From quesada at gmail.com  Mon Jul 16 17:58:37 2007
From: quesada at gmail.com (Jose Quesada )
Date: Mon, 16 Jul 2007 08:58:37 -0700
Subject: [R] R equivalent to Matlab's Bayes net toolbox
Message-ID: <op.tvkenzwy4hcap5@delllap.auth.ucla.edu>

Hi,

I'm attending  summer School at UCLA (IPAM) on "probabilistics models of  
cognition". I have been an R-user since v. 1.4.1, but was trained in the  
frequentist tradition (as most psychologists!). I found that all faculty  
here use matlab and Murphy's bayes net toolbox. I have not had the need to  
use matlab before, and would love to stick to R for graphics models and  
bayesian modeling in general (even if it takes me extra time to cross-code  
the examples in matlab into R).

I'm trying to find an R equivalent to Matlab's Bayes net toolbox.

I have found packages 'deal' and 'gR', and played around with:
http://www.ci.tuwien.ac.at/gR/

But I cannot really figure out how all these packages are integrated.  
Also, appendix B of 'bayesian AI' lists gR as "vaporware" (although this  
could well be outdated by now).

Is there any R news article on bayesian networks? It's hard to find,  
because I don't think the content of R-news is indexed in CRAN. I could  
download every issue and search the TOC, but it'd be time-consuming.

Even though the examples in the documentation in package 'deal' are good,  
they fall short. A good tutorial would be great.
What I'd like to know from you is whether R is a sensible choice or  
whether BNT is just easier and more mature.

Right now I could easily chose R or Matlab, since I have made little  
investment in any form of bayesian networks modeling; However, since I  
have a better background in R than in Matlab, I'd love to stay with R.

Any resources (mailing lists, books, tutorials) would be greatly  
appreciated.

Thanks a lot in advance,
-Jose

-- 
Jose Quesada, PhD.
http://www.andrew.cmu.edu/~jquesada


From gavin.simpson at ucl.ac.uk  Tue Jul 17 18:10:50 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 17 Jul 2007 17:10:50 +0100
Subject: [R] distance function (analogue)
In-Reply-To: <B49AAAEF-50E8-483B-82EF-0E1BC5B11407@systbot.uzh.ch>
References: <B49AAAEF-50E8-483B-82EF-0E1BC5B11407@systbot.uzh.ch>
Message-ID: <1184688650.10429.57.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-07-17 at 14:32 +0200, Birgit Lemcke wrote:
> Hello R-Users,
> 
> its again me with a question.
> Im using R 2.5.0 on Mac Power Book running Mac OS X 10.4.10

And again, you should send this to the maintainer (that'd be me), *not*
R-help! Unless you are asking help on a widely used package, it is
unlikely that anyone on R-Help can help you. And I'm not being modest
when I say that analogue is unlikely to fit that description.

> 
> I try to calculate distances betweeen two data tables looking like this
> 
>      V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18  
> V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36
> 1    1  0  0  0  1  1  0  1  0   0   0   1   0   0   0   0   0   1    
> 0   0   1   0   0   0   0   1   1   0   1   1   0   0   0   0   0   0
> 2    1  1  0  0  1  1  0  0  1   0   1   1   0   0   0   0   0   1    
> 0   0   1   0   0   0   0   1   1   0   0   1   0   0   1   1   0   0
> 3    1  1  0  0  1  1  1  1  0   0   1   0   0   0   0   0   0   1    
> 0   0   1   1   0   0   0   0   1   0   1   0   0   1   0   0   0   0
> 4    0  1  0  0  1  1  0  1  0   0   0   1   0   0   0   0   0   1    
> 0   0   1   0   0   1   0   0   1   0   0   1   0   0   1   0   0   0
> 5    0  1  0  0  1  1  0  0  1   0   0   1   1   0   0   0   0   1    
> 0   0   1   0   0   0   0   1   0   0   0   1   0   0   0   0   0   0
> 6    1  1  0  0  1  1  0  1  1   0   1   0   0   0   0   0   1   1    
> 0   0   1   0   0   0   0   0   1   0   0   1   0   1   1   0   0   0

I need to know what R thinks the objects look like, not how you think
they look.

> As i know I did the same 2 weeks ago and it worked properly.
> 
> Here are my codes:
> 
> Table1<-read.table("Alle_DatenFemBeschr4a.csv", sep = ";")
> 
> Table0<-read.table("Alle_DatenMalBeschr4a.csv", sep = ";")
> 
> Dist.Gower<- distance(Table1 ,Table0 ,method ="mixed", weights = c 
> (0.3333333, 0.3333333, 0.3333333, 0.5000000, 0.5000000, 0.5000000,  
> 0.5000000, 0.5000000, 0.5000000, 0.2500000, 0.2500000, 0.2500000,  
> 0.2500000, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571,  
> 0.1428571, 0.1428571, 0.3333333, 0.3333333, 0.3333333, 0.2000000,  
> 0.2000000, 0.2000000, 0.2000000, 0.2000000, 0.1111111, 0.1111111,  
> 0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111,  
> 0.1111111, 0.2000000, 0.2000000, 0.2000000, 0.2000000, 0.2000000,  
> 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571,  
> 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571,  
> 0.1428571, 0.1428571, 0.2000000, 0.2000000, 0.2000000, 0.2000000,  
> 0.2000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000,  
> 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000,  
> 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000,  
> 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000,  
> 0.5000000))
> 
> This produces the following massage:
> 
> Fehler in distance(Table1, Table0, method = "mixed", weights = c 
> (0.3333333,  :
> 	Different columns (species) are coded as factors in 'x' and 'y'

This means that Table1 has columns coded as factors that are not coded
factors in Table0 or vice versa.

Do 

> str(Table1)
> str(Table0)

and compare the output.

Alternatively, send me the two csv files you are reading in and I'll try
to track down the problem for you.

G

Ps: It would be easier to read if you did

my.weights <- c (0.3333333, 0.3333333, 0.3333333, 0.5000000, 0.5000000,
0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.2500000, 0.2500000,
0.2500000, 0.2500000, 0.1428571, 0.1428571, 0.1428571, 0.1428571,
0.1428571, 0.1428571, 0.1428571, 0.3333333, 0.3333333, 0.3333333,
0.2000000, 0.2000000, 0.2000000, 0.2000000, 0.2000000, 0.1111111,
0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111,
0.1111111, 0.1111111, 0.2000000, 0.2000000, 0.2000000, 0.2000000,
0.2000000, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571,
0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571,
0.1428571, 0.1428571, 0.1428571, 0.2000000, 0.2000000, 0.2000000,
0.2000000, 0.2000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000,
0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000,
0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000,
0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000, 0.5000000,
0.5000000, 0.5000000)

Dist.Gower <- distance(Table1, Table0, method ="mixed", weights =
my.weights)

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From Greg.Snow at intermountainmail.org  Tue Jul 17 18:18:05 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 17 Jul 2007 10:18:05 -0600
Subject: [R] multiple rugs on a single plot
In-Reply-To: <002801c7c860$533227c0$a45f11ac@simugenqfwills>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAADC5B@LP-EXCHVS07.CO.IHC.COM>

The rug function will take line and position arguments for the location
of the rug plot, so you can do something like:

> rug( x1, line=0 )
> rug( x2, line=1 )
> rug( x3, line=2 )

Where line=0 means right on the x axis, line=1 means 1 textline height
below the x axis (-1 would be the same distance above the x axis).  Or
pos=y  will plot the rug plot at the specified y value (and you can give
it y values below the y-axis).

You should probably make sure to leave enough room for the rug plots to
not be crowded and look at the other graphical options for how the rug
plot will look.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Quin Wills
> Sent: Tuesday, July 17, 2007 4:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] multiple rugs on a single plot
> 
> Hi
> 
>  
> 
> I could only find some discussion on this wrt lattice 
> graphics (which I'm not using). Apologies if I'm missing 
> something obvious. 
> 
>  
> 
> I'd like to produce 3 rug plots under a kernel density plot 
> for a population. The population is subdivided into 3 
> subpopulations, which I'd like the rug plots to highlight. 
> Naturally, when I do 3 rug plots, they all plot over each 
> other. I'd like 3 parallel rug plots along the x-axis. But how.
> 
>  
> 
> Thanks in advance,
> 
> Quin
> 
>  
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ld7631 at gmail.com  Tue Jul 17 18:47:22 2007
From: ld7631 at gmail.com (Dimitri Liakhovitski)
Date: Tue, 17 Jul 2007 12:47:22 -0400
Subject: [R] (a) R capacity and (b) sorting cases in R
Message-ID: <dae9a2a60707170947k2f668599xffc45337d1fabace@mail.gmail.com>

1. I have a .txt file that has no more than 10 variables and about 3.5
MILLION cases.
Can R handle so many cases or should I not even try?

2. How do I sort cases (e.g., alphabetically) in R and then save the
sorted results in a new file?

Thank you!
Dimitri


From ripley at stats.ox.ac.uk  Tue Jul 17 19:03:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Jul 2007 18:03:13 +0100 (BST)
Subject: [R] (a) R capacity and (b) sorting cases in R
In-Reply-To: <dae9a2a60707170947k2f668599xffc45337d1fabace@mail.gmail.com>
References: <dae9a2a60707170947k2f668599xffc45337d1fabace@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707171800270.857@gannet.stats.ox.ac.uk>

On Tue, 17 Jul 2007, Dimitri Liakhovitski wrote:

> 1. I have a .txt file that has no more than 10 variables and about 3.5
> MILLION cases.
> Can R handle so many cases or should I not even try?

That's only about 300Mb.  On a >=4Gb RAM machine it should be fine, 
especially if you read it in carefully (not just use read.table without 
any options).

> 2. How do I sort cases (e.g., alphabetically) in R and then save the
> sorted results in a new file?

?sort
?write

>
> Thank you!
> Dimitri
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From weller at erdw.ethz.ch  Tue Jul 17 19:33:28 2007
From: weller at erdw.ethz.ch (Andy Weller)
Date: Tue, 17 Jul 2007 19:33:28 +0200
Subject: [R] Speed up computing: looping through data?
In-Reply-To: <mailman.9.1183456807.14106.r-help@stat.math.ethz.ch>
References: <mailman.9.1183456807.14106.r-help@stat.math.ethz.ch>
Message-ID: <469CFD68.6040801@erdw.ethz.ch>

Dear all,

Please excuse my ignorance, but I am having difficulty with this, and am 
unable to find help on the website/Google.

I have a series of explanatory variables that I am aiming to get 
parsimony out of.

For example, if I have 10 variables, a-j, I am initially looking at the 
linear relationships amongst them:

my.lm1 <- lm(a ~ b+c+d+e+f+g+h+i+j, data=my.data)
summary(my.lm1)
my.lm2 <- lm(b ~ a+c+d+e+f+g+h+i+j, data=my.data)
etc

Instead of repeatedly typing this in, is there a way to construct a 
(for) loop of some description to semi-automate this process?

In addition, I have several response variables. Instead of examining 
these individually against the explanatory variables, is there a way to 
'group' all response and explanatory variables and then look at all the 
linear relationship amongst all these variables? I have tried:

my.group1 <- c(a,b,c,d,e,f,g,h,i,j)
my.group2 <- c(k,l,m,n,o,p,q,r,s,t)
my.lm3 <- lm(my.group1 ~ my.group2)

to no avail.

Thanks for your help, Andy


From ld7631 at gmail.com  Tue Jul 17 19:56:00 2007
From: ld7631 at gmail.com (Dimitri Liakhovitski)
Date: Tue, 17 Jul 2007 13:56:00 -0400
Subject: [R] Sorting data frame by a string variable
Message-ID: <dae9a2a60707171056x46674ceau42d3422c5ac769d2@mail.gmail.com>

I have a data frame MyData with 2 variables.
One of the variables (String) contains a string of letters.
How can I resort MyData by MyData$String (alphabetically) and then
save the output as a sorted data file?

I tried:

o<-order(MyData$String)
SortedData<-rbind(MyData$String[o], MyData$Value[o])
write.table(SortedData,file="Sorted.txt",sep="\t",quote=F, row.names=F)


However, all strings get replaced with digits (1 for the first string,
2 for the second string etc.). How can I keep the strings instead of
digits?

Thank you!
Dimitri


From magno_yu at ml.com  Tue Jul 17 20:04:47 2007
From: magno_yu at ml.com (yoooooo)
Date: Tue, 17 Jul 2007 11:04:47 -0700 (PDT)
Subject: [R] GLPK API
Message-ID: <11654799.post@talk.nabble.com>


Hi all, let's say I have:

lp = lpx_create_prob()
lpx_set_obj_dir(lp, LPX_MIN)
lpx_add_cols(lp, 3)
lpx_add_rows(lp, 2)
lpx_set_obj_coef(lp, 1, 100)
lpx_set_obj_coef(lp, 2, 200)
lpx_set_obj_coef(lp, 3, 300)
lpx_set_mat_row(lp, 1, 3, c(1, 2, 3), c(3, 2,1))
lpx_set_row_name(lp, 1, "c1")
lpx_set_row_bnds(lp, 1, LPX_LO, 100, 0)
print(lpx_get_mat_row(lp, 1))
lpx_write_cpxlp(lp, "hello")

> less hello
\* Problem: Unknown *\

Minimize
 obj: + 100 x_1 + 200 x_2 + 300 x_3

Subject To
 c1: + x_3 + 2 x_2 + 3 x_1 >= 100

Bounds
 x_1 = 0
 x_2 = 0
 x_3 = 0

End

But if I do in R, print(lpx_get_mat_row(lp, 1))

$n
[1] 3
$ind
[1] 3 2
$val
[1] 1 2

Is there a reason why I'm missing the 1st index and its constraint value in
get_mat_row? 

Thanks!
-- 
View this message in context: http://www.nabble.com/GLPK-API-tf4098577.html#a11654799
Sent from the R help mailing list archive at Nabble.com.


From p.dalgaard at biostat.ku.dk  Tue Jul 17 20:13:16 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 17 Jul 2007 20:13:16 +0200
Subject: [R] Sorting data frame by a string variable
In-Reply-To: <dae9a2a60707171056x46674ceau42d3422c5ac769d2@mail.gmail.com>
References: <dae9a2a60707171056x46674ceau42d3422c5ac769d2@mail.gmail.com>
Message-ID: <469D06BC.7060003@biostat.ku.dk>

Dimitri Liakhovitski wrote:
> I have a data frame MyData with 2 variables.
> One of the variables (String) contains a string of letters.
> How can I resort MyData by MyData$String (alphabetically) and then
> save the output as a sorted data file?
>
> I tried:
>
> o<-order(MyData$String)
> SortedData<-rbind(MyData$String[o], MyData$Value[o])
> write.table(SortedData,file="Sorted.txt",sep="\t",quote=F, row.names=F)
>
>
> However, all strings get replaced with digits (1 for the first string,
> 2 for the second string etc.). How can I keep the strings instead of
> digits?
>   

Why on earth are you trying to rbind() things together?

Anything wrong with

SortedData <- MyData[o,]
write.table(SortedData,...whatever...)

?

> Thank you!
> Dimitri
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Tue Jul 17 20:23:21 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Tue, 17 Jul 2007 12:23:21 -0600
Subject: [R] Speed up computing: looping through data?
In-Reply-To: <469CFD68.6040801@erdw.ethz.ch>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAADCBF@LP-EXCHVS07.CO.IHC.COM>

There are many options that can help, but which to use depends a lot on
what your research question is.  What are you really looking for in
these relationships?

Here are a couple of tools that may help.

If my.data is a matrix or data.frame with just the variables of interest
then 
look at 1-1/diag(solve(cor(my.data))), it will be a vector of the
R-squared values from the set of regressions you asked about (each
variable regressed on all the others).

Look at ?cancor, cannonical correlation explores relationships between 2
sets of variables.

The lm function can take a matrix as the response variable:

> fit <- lm( cbind(Sepal.Length, Sepal.Width) ~
Petal.Length+Petal.Width, data=iris)
> fit
...
> summary(fit)
...

If you tell us more what you are trying to accomplish, we can be of more
help,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Andy Weller
> Sent: Tuesday, July 17, 2007 11:33 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Speed up computing: looping through data?
> 
> Dear all,
> 
> Please excuse my ignorance, but I am having difficulty with 
> this, and am unable to find help on the website/Google.
> 
> I have a series of explanatory variables that I am aiming to 
> get parsimony out of.
> 
> For example, if I have 10 variables, a-j, I am initially 
> looking at the linear relationships amongst them:
> 
> my.lm1 <- lm(a ~ b+c+d+e+f+g+h+i+j, data=my.data)
> summary(my.lm1)
> my.lm2 <- lm(b ~ a+c+d+e+f+g+h+i+j, data=my.data) etc
> 
> Instead of repeatedly typing this in, is there a way to construct a
> (for) loop of some description to semi-automate this process?
> 
> In addition, I have several response variables. Instead of 
> examining these individually against the explanatory 
> variables, is there a way to 'group' all response and 
> explanatory variables and then look at all the linear 
> relationship amongst all these variables? I have tried:
> 
> my.group1 <- c(a,b,c,d,e,f,g,h,i,j)
> my.group2 <- c(k,l,m,n,o,p,q,r,s,t)
> my.lm3 <- lm(my.group1 ~ my.group2)
> 
> to no avail.
> 
> Thanks for your help, Andy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From kevinvol2002 at yahoo.com  Tue Jul 17 20:38:49 2007
From: kevinvol2002 at yahoo.com (Hai Lin)
Date: Tue, 17 Jul 2007 11:38:49 -0700 (PDT)
Subject: [R] apply a function to loop through
In-Reply-To: <x2fyxu7tmb.fsf@turmalin.kubism.ku.dk>
Message-ID: <681911.97505.qm@web32412.mail.mud.yahoo.com>

Dear R users,

I have a dataset generated as follows,

myDat <- data.frame(matrix(c(rep(LETTERS[1:3],
each=5),
                             rnorm(5,mean=3,sd=0.03),
                             rnorm(5,12,1),
                             rnorm(5,1,0.5)),
                           ncol=2,
                          
dimnames=list(c(letters[1:15]),
                                        
c("tissue","Scores"))))
myDat$Grp <-c("gene1") 

There is one level "gene1" in $Grp in my data step.
I'd like to do pairwise.wilcox.test on $tissue while
going throug $Grp if there are more levels with gene2,
gene3.

I tried to loop through $Grp using apply with an error
message "Error in sort(unique.default(x), na.last =
TRUE) : 
	'x' must be atomic". 

mytry <-  apply(as.matrix(myDat),
                1,
               
function(Grp)pairwise.wilcox.test(Grp$Scores,
                                              
Grp$tissue,
                                              
p.adjust.method = "none")$p.value)     

I could not find any similar stuffs in forum. Could
anyone here give a hand?

Thanks a bunch.

kevin


From francois.gagnon at EC.GC.CA  Tue Jul 17 20:43:18 2007
From: francois.gagnon at EC.GC.CA (Gagnon,Francois [SteFoy])
Date: Tue, 17 Jul 2007 14:43:18 -0400
Subject: [R] Missing value in circ.mean and polar.plot
Message-ID: <41271EA89B1DE246BC0326E0A13288580A33CB@ecqcstfmail2.quebec.int.ec.gc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/2242d454/attachment.pl 

From Soren.Hojsgaard at agrsci.dk  Tue Jul 17 20:49:42 2007
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 17 Jul 2007 20:49:42 +0200
Subject: [R] R equivalent to Matlab's Bayes net toolbox
References: <op.tvkenzwy4hcap5@delllap.auth.ucla.edu>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC038787C2@DJFPOST01.djf.agrsci.dk>

Jose,
I am not entirely sure what Matlabs Bayes net toolbox does, but I guess it implements as propagation algorithm for Bayesian networks. There is no such package on CRAN - yet - but there will be soon: I've created a package called gRbayesnet which implements the "Lauritzen-Spiegelhalter" propagation algorithm. I expect to upload it to CRAN within the next few days. 
Best regards
S?ren

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p? vegne af Jose Quesada 
Sendt: ma 16-07-2007 17:58
Til: r-help at lists.r-project.org
Emne: [R] R equivalent to Matlab's Bayes net toolbox



Hi,

I'm attending  summer School at UCLA (IPAM) on "probabilistics models of 
cognition". I have been an R-user since v. 1.4.1, but was trained in the 
frequentist tradition (as most psychologists!). I found that all faculty 
here use matlab and Murphy's bayes net toolbox. I have not had the need to 
use matlab before, and would love to stick to R for graphics models and 
bayesian modeling in general (even if it takes me extra time to cross-code 
the examples in matlab into R).

I'm trying to find an R equivalent to Matlab's Bayes net toolbox.

I have found packages 'deal' and 'gR', and played around with:
http://www.ci.tuwien.ac.at/gR/

But I cannot really figure out how all these packages are integrated. 
Also, appendix B of 'bayesian AI' lists gR as "vaporware" (although this 
could well be outdated by now).

Is there any R news article on bayesian networks? It's hard to find, 
because I don't think the content of R-news is indexed in CRAN. I could 
download every issue and search the TOC, but it'd be time-consuming.

Even though the examples in the documentation in package 'deal' are good, 
they fall short. A good tutorial would be great.
What I'd like to know from you is whether R is a sensible choice or 
whether BNT is just easier and more mature.

Right now I could easily chose R or Matlab, since I have made little 
investment in any form of bayesian networks modeling; However, since I 
have a better background in R than in Matlab, I'd love to stay with R.

Any resources (mailing lists, books, tutorials) would be greatly 
appreciated.

Thanks a lot in advance,
-Jose

--
Jose Quesada, PhD.
http://www.andrew.cmu.edu/~jquesada

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gangchen at mail.nih.gov  Tue Jul 17 21:36:27 2007
From: gangchen at mail.nih.gov (Gang Chen)
Date: Tue, 17 Jul 2007 15:36:27 -0400
Subject: [R] A simple question
Message-ID: <D0D2E178-6EF9-4B1D-9B3B-74BB9D8A65D1@mail.nih.gov>

Hi all,

I am a newbie of R, and have a very simple question. When I run the  
following t test

result = t.test(weight ~ group)

If I want to get the t value, I can do

tvalue = result$statistic

with tvalue being the following:

t
1.191260

However, if I just want the number 1.191260, not the character "t",  
how can I achieve that?

Thank you very much,
Gang


From murdoch at stats.uwo.ca  Tue Jul 17 21:57:42 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 17 Jul 2007 15:57:42 -0400
Subject: [R] A simple question
In-Reply-To: <D0D2E178-6EF9-4B1D-9B3B-74BB9D8A65D1@mail.nih.gov>
References: <D0D2E178-6EF9-4B1D-9B3B-74BB9D8A65D1@mail.nih.gov>
Message-ID: <469D1F36.8020807@stats.uwo.ca>

On 7/17/2007 3:36 PM, Gang Chen wrote:
> Hi all,
> 
> I am a newbie of R, and have a very simple question. When I run the  
> following t test
> 
> result = t.test(weight ~ group)
> 
> If I want to get the t value, I can do
> 
> tvalue = result$statistic
> 
> with tvalue being the following:
> 
> t
> 1.191260
> 
> However, if I just want the number 1.191260, not the character "t",  
> how can I achieve that?

as.numeric(result$statistic)

will give it to you.  But the value you have is already just as useful: 
  it's just a vector with a named entry.  You can do calculations with 
it without stripping off the name.

Duncan Murdoch


From helprhelp at gmail.com  Tue Jul 17 21:57:09 2007
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 17 Jul 2007 15:57:09 -0400
Subject: [R] apply a function to loop through
In-Reply-To: <681911.97505.qm@web32412.mail.mud.yahoo.com>
References: <x2fyxu7tmb.fsf@turmalin.kubism.ku.dk>
	<681911.97505.qm@web32412.mail.mud.yahoo.com>
Message-ID: <cdf817830707171257s51af361bt106b5880a5e86622@mail.gmail.com>

Hi,

first of all, your Scores are factors instead of numeric, you need to change it;

I made a new myDat for test purpose by

myDat = as.data.frame(rbind(myDat, myDat))
myDat[16:30, 3] <- "gene2"

> myDat
   tissue            Scores   Grp
a       A  3.01494535196933 gene1
b       A  2.99647624484379 gene1
c       A  3.00527533284709 gene1
d       A  3.02321059636917 gene1
e       A  3.04694197334289 gene1
f       B  11.1464974692841 gene1
g       B  12.2372838904939 gene1
h       B  11.7277801841221 gene1
i       B  11.6405683605147 gene1
j       B  11.1631961720026 gene1
k       C  0.67528704974662 gene1
l       C  1.21000950157251 gene1
m       C 0.843722400594721 gene1
n       C 0.881706314004343 gene1
o       C  1.43670211710054 gene1
a1      A  3.01494535196933 gene2
b1      A  2.99647624484379 gene2
c1      A  3.00527533284709 gene2
d1      A  3.02321059636917 gene2
e1      A  3.04694197334289 gene2
f1      B  11.1464974692841 gene2
g1      B  12.2372838904939 gene2
h1      B  11.7277801841221 gene2
i1      B  11.6405683605147 gene2
j1      B  11.1631961720026 gene2
k1      C  0.67528704974662 gene2
l1      C  1.21000950157251 gene2
m1      C 0.843722400594721 gene2
n1      C 0.881706314004343 gene2
o1      C  1.43670211710054 gene2
> mytry <- by(myDat, INDICES=as.factor(myDat[,3]), FUN=function(x) {
+ pairwise.wilcox.test(as.numeric(as.character(x$Scores)),
+
+ x$tissue,
+
+ p.adjust.method = "none")$p.value
+ })
> mytry
as.factor(myDat[, 3]): gene1
            A           B
B 0.007936508          NA
C 0.007936508 0.007936508
------------------------------------------------------------
as.factor(myDat[, 3]): gene2
            A           B
B 0.007936508          NA
C 0.007936508 0.007936508


HTH,

Weiwei

On 7/17/07, Hai Lin <kevinvol2002 at yahoo.com> wrote:
> Dear R users,
>
> I have a dataset generated as follows,
>
> myDat <- data.frame(matrix(c(rep(LETTERS[1:3],
> each=5),
>                              rnorm(5,mean=3,sd=0.03),
>                              rnorm(5,12,1),
>                              rnorm(5,1,0.5)),
>                            ncol=2,
>
> dimnames=list(c(letters[1:15]),
>
> c("tissue","Scores"))))
> myDat$Grp <-c("gene1")
>
> There is one level "gene1" in $Grp in my data step.
> I'd like to do pairwise.wilcox.test on $tissue while
> going throug $Grp if there are more levels with gene2,
> gene3.
>
> I tried to loop through $Grp using apply with an error
> message "Error in sort(unique.default(x), na.last =
> TRUE) :
>         'x' must be atomic".
>
> mytry <-  apply(as.matrix(myDat),
>                 1,
>
> function(Grp)pairwise.wilcox.test(Grp$Scores,
>
> Grp$tissue,
>
> p.adjust.method = "none")$p.value)
>
> I could not find any similar stuffs in forum. Could
> anyone here give a hand?
>
> Thanks a bunch.
>
> kevin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Weiwei Shi, Ph.D
Research Scientist
GeneGO, Inc.

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III


From tobias.verbeke at businessdecision.com  Tue Jul 17 21:59:01 2007
From: tobias.verbeke at businessdecision.com (Tobias Verbeke)
Date: Tue, 17 Jul 2007 21:59:01 +0200
Subject: [R] R equivalent to Matlab's Bayes net toolbox
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC038787C2@DJFPOST01.djf.agrsci.dk>
References: <op.tvkenzwy4hcap5@delllap.auth.ucla.edu>
	<C83C5E3DEEE97E498B74729A33F6EAEC038787C2@DJFPOST01.djf.agrsci.dk>
Message-ID: <469D1F85.7070007@businessdecision.com>

S?ren H?jsgaard wrote:
> Jose,
> I am not entirely sure what Matlabs Bayes net toolbox does, but I guess it implements as propagation algorithm for Bayesian networks. There is no such package on CRAN - yet - but there will be soon: I've created a package called gRbayesnet which implements the "Lauritzen-Spiegelhalter" propagation algorithm. I expect to upload it to CRAN within the next few days. 

An alternative could be to extend the current R-Weka interface
from Kurt Hornik et alii

http://cran.r-project.org/src/contrib/Descriptions/RWeka.html

to include as well an interface to the Weka Bayes net package
described at

http://downloads.sourceforge.net/weka/BayesianNetClassifiers-3.5.6.pdf

library(RWeka)
?list_Weka_interfaces

describes how to register new interfaces.

HTH,
Tobias

-- 

Tobias Verbeke - Consultant
Business & Decision Benelux
Rue de la r?volution 8
1000 Brussels - BELGIUM

+32 499 36 33 15
tobias.verbeke at businessdecision.com


From gangchen at mail.nih.gov  Tue Jul 17 22:37:22 2007
From: gangchen at mail.nih.gov (Gang Chen)
Date: Tue, 17 Jul 2007 16:37:22 -0400
Subject: [R] A simple question
In-Reply-To: <469D1F36.8020807@stats.uwo.ca>
References: <D0D2E178-6EF9-4B1D-9B3B-74BB9D8A65D1@mail.nih.gov>
	<469D1F36.8020807@stats.uwo.ca>
Message-ID: <9C38853E-4AE3-4008-AD18-26736D4E5BFF@mail.nih.gov>

Thank you very much!

Gang

On Jul 17, 2007, at 3:57 PM, Duncan Murdoch wrote:

> On 7/17/2007 3:36 PM, Gang Chen wrote:
>> Hi all,
>> I am a newbie of R, and have a very simple question. When I run  
>> the  following t test
>> result = t.test(weight ~ group)
>> If I want to get the t value, I can do
>> tvalue = result$statistic
>> with tvalue being the following:
>> t
>> 1.191260
>> However, if I just want the number 1.191260, not the character  
>> "t",  how can I achieve that?
>
> as.numeric(result$statistic)
>
> will give it to you.  But the value you have is already just as  
> useful:  it's just a vector with a named entry.  You can do  
> calculations with it without stripping off the name.
>
> Duncan Murdoch


From Manuel.A.Morales at williams.edu  Tue Jul 17 22:39:46 2007
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Tue, 17 Jul 2007 16:39:46 -0400
Subject: [R] Alternative to xyplot()?
Message-ID: <1184704786.20680.1.camel@mrubra.localdomain>

Hello list,

Is anyone aware of a non-lattice-based alternative to xyplot()?

Thanks!

Manuel

-- 
http://mutualism.williams.edu


From nils.ruefenacht at bluewin.ch  Tue Jul 17 22:40:14 2007
From: nils.ruefenacht at bluewin.ch (=?iso-8859-1?Q?Nils_R=FCfenacht?=)
Date: Tue, 17 Jul 2007 22:40:14 +0200
Subject: [R] difficulties with "setMethod"
Message-ID: <000001c7c8b2$adf18910$2401a8c0@NBNILS>

Dear all!

I do definetley have some difficulties. Here is my code:

> setMethod("write",
+             signature(object = "KMatrix", path = "character"),
+             function(object,path){
+             write.table(object at data,path,row.names=FALSE, sep = "\t")
+             }            
+             )
error in match.call(fun, fcall) : unused argument(s) (object =
"KMatrix", path = "character")
>

"KMatrix" is an extension of some data.frame object, i.e. it's a
data.frame (KMatrix at data) with some additional slots (e.g. KMatrix at Size)

What's wrong with my setMethod? 

Kind regards,
nils


From gavin.simpson at ucl.ac.uk  Tue Jul 17 23:18:52 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 17 Jul 2007 22:18:52 +0100
Subject: [R] Alternative to xyplot()?
In-Reply-To: <1184704786.20680.1.camel@mrubra.localdomain>
References: <1184704786.20680.1.camel@mrubra.localdomain>
Message-ID: <1184707132.2875.14.camel@graptoleberis.geog.ucl.ac.uk>

On Tue, 2007-07-17 at 16:39 -0400, Manuel Morales wrote:
> Hello list,
> 
> Is anyone aware of a non-lattice-based alternative to xyplot()?

x <- rnorm(20)
y <- rnorm(20)
plot(x, y) ?

If you mean some specific aspect of xyplot(), you'll have to tell us
what this is.

HTH

G


From ld7631 at gmail.com  Tue Jul 17 23:25:53 2007
From: ld7631 at gmail.com (Dimitri Liakhovitski)
Date: Tue, 17 Jul 2007 17:25:53 -0400
Subject: [R] reading in "!" - delimited file
Message-ID: <dae9a2a60707171425m42184524r4303ec9f210631e5@mail.gmail.com>

I have to read in a file that is "!" delimited.
Would it be correct to use the following syntax:

Data<-read.delim(file.choose(),sep="!")

Or would it be wrong? I am asking rather than trying it just because I
have 3.5 million cases to read in.

Thank you!
Dimitri


From murdoch at stats.uwo.ca  Tue Jul 17 23:48:27 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 17 Jul 2007 17:48:27 -0400
Subject: [R] reading in "!" - delimited file
In-Reply-To: <dae9a2a60707171425m42184524r4303ec9f210631e5@mail.gmail.com>
References: <dae9a2a60707171425m42184524r4303ec9f210631e5@mail.gmail.com>
Message-ID: <469D392B.2010409@stats.uwo.ca>

On 17/07/2007 5:25 PM, Dimitri Liakhovitski wrote:
> I have to read in a file that is "!" delimited.
> Would it be correct to use the following syntax:
> 
> Data<-read.delim(file.choose(),sep="!")
> 
> Or would it be wrong? I am asking rather than trying it just because I
> have 3.5 million cases to read in.

If you have 3.5 million cases to read, you should specify the column 
classes.  I'd recommend setting nrows to a small number for a few tries 
first, until you get it right.

Duncan Murdoch


From Manuel.A.Morales at williams.edu  Tue Jul 17 23:53:51 2007
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Tue, 17 Jul 2007 17:53:51 -0400
Subject: [R] Alternative to xyplot()?
In-Reply-To: <1184707132.2875.14.camel@graptoleberis.geog.ucl.ac.uk>
References: <1184704786.20680.1.camel@mrubra.localdomain>
	<1184707132.2875.14.camel@graptoleberis.geog.ucl.ac.uk>
Message-ID: <1184709231.25735.3.camel@mrubra.localdomain>

On Tue, 2007-07-17 at 22:18 +0100, Gavin Simpson wrote:
> On Tue, 2007-07-17 at 16:39 -0400, Manuel Morales wrote:
> > Hello list,
> > 
> > Is anyone aware of a non-lattice-based alternative to xyplot()?
> 
> x <- rnorm(20)
> y <- rnorm(20)
> plot(x, y) ?
> 
> If you mean some specific aspect of xyplot(), you'll have to tell us
> what this is.
> 
> HTH
> 
> G

Sorry. I was thinking of the "groups" functionality, as illustrated
below:

grps<-rep(c(1:3),10)
x<-rep(c(1:10),3)
y<-x+grps+rnorm(30)
library(lattice)
xyplot(y~x,group=grps, type=c("r","p"))

From Zava.Aydemir at morganstanley.com  Wed Jul 18 00:04:34 2007
From: Zava.Aydemir at morganstanley.com (Aydemir, Zava (FID))
Date: Tue, 17 Jul 2007 18:04:34 -0400
Subject: [R] poor rbind performance
Message-ID: <755261CA22782948B1C42ACDC83912A104614E3D@NYWEXMB27.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/45209444/attachment.pl 

From jholtman at gmail.com  Wed Jul 18 00:26:37 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 17 Jul 2007 18:26:37 -0400
Subject: [R] poor rbind performance
In-Reply-To: <755261CA22782948B1C42ACDC83912A104614E3D@NYWEXMB27.msad.ms.com>
References: <755261CA22782948B1C42ACDC83912A104614E3D@NYWEXMB27.msad.ms.com>
Message-ID: <644e1f320707171526h7df99109vca8ab0b568e8832@mail.gmail.com>

Read the data into a list and then:

do.call('rbind', myList)

at the end so you do it only once.  You are having to reallocate
memory each iteration, so no wonder it is slow.

On 7/17/07, Aydemir, Zava (FID) <Zava.Aydemir at morganstanley.com> wrote:
> Hi
>
> I rbind data frames in a loop in a cumulative way and the performance
> detriorates very quickly.
>
> My code looks like this:
>
> for( k in 1:N)
> {
>    filename <- paste("/tmp/myData_",as.character(k),".txt",sep="")
>    myDataTmp <- read.table(filename,header=TRUE,sep=",")
>    if( k == 1) {
>        myData <- myDataTmp
>    }
>    else{
>        myData <- rbind(myData,myDataTmp)
>    }
> }
>
> Some more details:
> - the size of the stored text files is about 100,000 rows and 50 columns
> each
> - for k=1: rbind takes 0.0004 seconds
> - for k=2: rbind takes 13 seconds
> - for k=3: rbind takes 30 seconds
> - for k=4: rbind takes 36 seconds
> etc
>
> Any suggestions to improve speed?
>
> Thanks
>
> Zava
> --------------------------------------------------------
>
> This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From osman.al.radi at gmail.com  Wed Jul 18 00:53:34 2007
From: osman.al.radi at gmail.com (Osman Al-Radi)
Date: Tue, 17 Jul 2007 18:53:34 -0400
Subject: [R] xyplot for longitudinal data
Message-ID: <c9955100707171553r312ca7c6ra69e076676adb58d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070717/6c491d00/attachment.pl 

From deepayan.sarkar at gmail.com  Wed Jul 18 01:47:59 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 17 Jul 2007 16:47:59 -0700
Subject: [R] xyplot for longitudinal data
In-Reply-To: <c9955100707171553r312ca7c6ra69e076676adb58d@mail.gmail.com>
References: <c9955100707171553r312ca7c6ra69e076676adb58d@mail.gmail.com>
Message-ID: <eb555e660707171647r23e109a4qbe06c232ceaab643@mail.gmail.com>

On 7/17/07, Osman Al-Radi <osman.al.radi at gmail.com> wrote:
> Dear R-help subscribers,
>
> I use xyplot to plot longitudinal data as follows:
>
> score<-runif(100,-4,5)
> group<-sample(1:4,100,rep=T)
> subject<-rep(1:25,4)
> age<-rep(runif(4,1,40),25)
> df<-data.frame(score,group,age,subject)
>
> xyplot(score~age|group, group=subject,
> panel=function(...){
> panel.loess(...,lwd=4)
> panel.superpose(...)}
> ,data=df)
>
> this produced a plot with four panels one for each group, with unique
> plotting parameters for each subject.
>
> How can I create a create a plot with a single panel where all four groups
> are superimposed using different line colors and symbols for each group, but
> preserving the longitudinal nature of the data (i.e. one line per subject).

Create an interaction and specify the colors and symbols you want
explicitly. E.g.,

spcolors <- trellis.par.get("superpose.symbol")$col

df$subject <- factor(df$subject)
df$group <- factor(df$group)

with(df,
     xyplot(score~age, groups = interaction(subject, group),
            type = "l",
            col = rep(spcolors[1:nlevels(group)], each = nlevels(subject))))


Look at ?Rows, which might help in getting code that is a bit more
general (for instance, the code above may not work when groups has
more than 7 levels).

-Deepayan


From m_olshansky at yahoo.com  Wed Jul 18 02:32:52 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 17 Jul 2007 17:32:52 -0700 (PDT)
Subject: [R] Optimization
In-Reply-To: <000301c7c879$70f04120$7c94100a@win.ad.jhu.edu>
Message-ID: <478597.56487.qm@web32201.mail.mud.yahoo.com>

This is partially true since both the function to be
maximized and the constraint are non-linear.  One may
substitute 1-x1-x2 for x3 and use (let say) Lagrange
multipliers to get two non-linear equations with 2
unknowns for which there should be a function solving
them. Then you must find the points where the
constraint function intersects with the triangle
{x1>=0,x2>=0,x1+x2<=1}, which is easier (for each of
the 3 edges you get a non-linear equation in one
variable).
So even though an (almost) analytical solution can be
found it would be much more convenient to use an
optimization function which (hopefully) does all this
for you.

Moshe.

--- Ravi Varadhan <rvaradhan at jhmi.edu> wrote:

> Hi,
> 
> Your problem can be solved analytically.  Eliminate
> one of the variables,
> say x3, from the problem by using the equality x1 +
> x2 + x3 = 1. Then solve
> for the intersection of the circle (in x1 and x2)
> defined by the radical
> constraint, with the straight line defined by the
> objective function.  There
> will be, at most, two intersection points. The
> extremum has to be one of
> these two points, provided they also satisfy the
> other inequalities (To me,
> this sounds an awful lot like a homework problem).  
> 
> 
> Ravi.
> 
>
----------------------------------------------------------------------------
> -------
> 
> Ravi Varadhan, Ph.D.
> 
> Assistant Professor, The Center on Aging and Health
> 
> Division of Geriatric Medicine and Gerontology 
> 
> Johns Hopkins University
> 
> Ph: (410) 502-2619
> 
> Fax: (410) 614-9625
> 
> Email: rvaradhan at jhmi.edu
> 
> Webpage: 
>
http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html
> 
>  
> 
>
----------------------------------------------------------------------------
> --------
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf
> Of massimiliano.talarico
> Sent: Monday, July 16, 2007 4:50 PM
> To: r-help
> Subject: [R] Optimization
> 
> Dear all,
> I need a suggest to obtain the max of this function:
> 
> Max x1*0.021986+x2*0.000964+x3*0.02913
> 
> with these conditions:
> 
> x1+x2+x3=1;
>
radq((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> x1>=0;
> x1<=1;
> x2>=0;
> x2<=1;
> x3>=0;
> x3<=1;
> 
> Any suggests ?
> 
> Thanks in advanced,
> Massimiliano
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From drstrong at ucdavis.edu  Wed Jul 18 03:10:53 2007
From: drstrong at ucdavis.edu (Mr Natural)
Date: Tue, 17 Jul 2007 18:10:53 -0700 (PDT)
Subject: [R] Date problem
In-Reply-To: <44170BCF.3010609@cict.fr>
References: <44170BCF.3010609@cict.fr>
Message-ID: <11660658.post@talk.nabble.com>


Dear Natalia:
I have had the same problem. Solved it by keeping my dates in Excel files
with the rest of my data. When I want to do something in R, highlight the
date column and format it as "general" You will get a 5 digit number that
represents the Julian date (more digits if you have minutes and seconds).
Save the Excel file as a csv file.
Then read it into R making the variables all numeric, with use of
columnClasses,
eg. from a big file  that Im working on now,

ss<-read.csv("six.sites.epn.hist.csv",strip.white=T,na.strings=".",
colClasses=c("numeric","numeric","numeric",
"numeric","numeric","numeric",
"numeric","numeric","numeric",
"numeric","numeric","numeric",
"numeric","numeric","numeric",
"numeric","numeric","numeric",na.rm=T ))

Then you make the Julian dates back into calendar dates that plot so nicely
in R.
In my file, I have one date column for every y variable because each has a
different sequence of dates.
Note that Excel and R have a different Julian "day 1." Excel is Jan 1 1900,
and R is Jan 1 1970.
You set the day one Julian date in R with the origin feature in the dates().

The six interesting "y" variables (MP,C,D,UD,LD,and BS) each have two date
columns, the most interesting of which is the calendar date, which are made
from the Julian dates that I read in.

dmp<-dates(ss$MPdate,origin=c(month = 1, day = 1, year = 1900))
dc<-dates(ss$Cdate,origin=c(month = 1, day = 1, year = 1900))
dd<-dates(ss$Ddate,origin=c(month = 1, day = 1, year = 1900))
dld<-dates(ss$LDdate,origin=c(month = 1, day = 1, year = 1900))
dud<-dates(ss$UDdate,origin=c(month = 1, day = 1, year = 1900))
dbs<-dates(ss$BSdate,origin=c(month = 1, day = 1, year = 1900))

Then, I plot a stack of 6 time series, one on top of each other.
Note that I used "arrow" for error bars.
#stack of plots, one for each site.
attach(ss)

par(mar=c(2,2,1,1), mfrow = c(6,1))
plot(MP~dmp, type="b",xlab="",ylim=c(0,1),xaxt = "n")
arrows(dmp,MP+semp,dmp,MP-semp,length=.02,angle=90,code=3)
plot(C~dc, type="b",xlab="",ylim=c(0,1),xaxt = "n")
arrows(dc,C+sec,dc,C-sec,length=.02,angle=90,code=3)
plot(D~dd, type="b",xlab="",ylim=c(0,1),xaxt = "n")
arrows(dd,D+sed,dd,D-sed,length=.02,angle=90,code=3)
plot(LD~dld, type="b",xlab="",ylim=c(0,1),xaxt = "n")
arrows(dld,LD+seld,dld,LD-seld,length=.02,angle=90,code=3)
plot(UD~dud, type="b",xlab="",ylim=c(0,1),xaxt = "n")
arrows(dud,UD+seud,dud,UD-seud,length=.02,angle=90,code=3)
plot(BS~dbs, type="b",xlab="",ylim=c(0,1))
arrows(dbs,BS+sebs,dbs,BS-sebs,length=.02,angle=90,code=3)



I hope this helps, 
Don










natalia norden wrote:
> 
> Hello,
> I have some "stupid" problems managing "date" data.
> I have a colomn "date", which I converted from a character representation:
> 
> for example:
> a="26/02/06"
> date=strptime(a,format="%d/%m/%y")
> 
> For one part of the analysis, I'm interested only in the month and the 
> year, so I did:
> 
> m.y=strftime(date,format="%m/%y")
> 
> This returns me "02/06", but this is an object of class "character" and 
> I can't convert it into an object of class "Date". Doing 
> strptime(m.y,format="%m/%y") or as.Date(m.y,format="%m/%y") returns me NA
> 
> How can a convert this colomn "m.y" in a Date class?
> 
> Actually, I need it to plot fruiting data against time (month and year).
> Because I have many values of seeds in a month, I used the function
> tapply:
> seeds=tapply(data$seed,data$m.y,sum)
> But plot(x=names(seeds),y=seeds) doesn't work. Does anyone know an 
> easier way?
> 
> Thank you for your time,
> natalia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 

-- 
View this message in context: http://www.nabble.com/Date-problem-tf1280528.html#a11660658
Sent from the R help mailing list archive at Nabble.com.


From bolker at ufl.edu  Wed Jul 18 03:13:51 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 18 Jul 2007 01:13:51 +0000 (UTC)
Subject: [R] Alternative to xyplot()?
References: <1184704786.20680.1.camel@mrubra.localdomain>
	<1184707132.2875.14.camel@graptoleberis.geog.ucl.ac.uk>
	<1184709231.25735.3.camel@mrubra.localdomain>
Message-ID: <loom.20070718T030847-446@post.gmane.org>

Manuel Morales <Manuel.A.Morales <at> williams.edu> writes:

> 
> Sorry. I was thinking of the "groups" functionality, as illustrated
> below:
> 
> grps<-rep(c(1:3),10)
> x<-rep(c(1:10),3)
> y<-x+grps+rnorm(30)
> library(lattice)
> xyplot(y~x,group=grps, type=c("r","p"))

  The points (type "p") are easy, the regression lines (type "r") are a little
harder. How about:


plot(y~x,col=grps)
invisible(mapply(function(z,col) {abline(lm(y~x,data=z),col=col)},
          split(data.frame(x,y),grps),1:3))

  cheers
    Ben Bolker


From m_olshansky at yahoo.com  Wed Jul 18 03:19:28 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 17 Jul 2007 18:19:28 -0700 (PDT)
Subject: [R] Date problem
In-Reply-To: <11660658.post@talk.nabble.com>
Message-ID: <865918.81187.qm@web32212.mail.mud.yahoo.com>

Try the xlsReadWrite package from CRAN.

--- Mr Natural <drstrong at ucdavis.edu> wrote:

> 
> Dear Natalia:
> I have had the same problem. Solved it by keeping my
> dates in Excel files
> with the rest of my data. When I want to do
> something in R, highlight the
> date column and format it as "general" You will get
> a 5 digit number that
> represents the Julian date (more digits if you have
> minutes and seconds).
> Save the Excel file as a csv file.
> Then read it into R making the variables all
> numeric, with use of
> columnClasses,
> eg. from a big file  that Im working on now,
> 
>
ss<-read.csv("six.sites.epn.hist.csv",strip.white=T,na.strings=".",
> colClasses=c("numeric","numeric","numeric",
> "numeric","numeric","numeric",
> "numeric","numeric","numeric",
> "numeric","numeric","numeric",
> "numeric","numeric","numeric",
> "numeric","numeric","numeric",na.rm=T ))
> 
> Then you make the Julian dates back into calendar
> dates that plot so nicely
> in R.
> In my file, I have one date column for every y
> variable because each has a
> different sequence of dates.
> Note that Excel and R have a different Julian "day
> 1." Excel is Jan 1 1900,
> and R is Jan 1 1970.
> You set the day one Julian date in R with the origin
> feature in the dates().
> 
> The six interesting "y" variables (MP,C,D,UD,LD,and
> BS) each have two date
> columns, the most interesting of which is the
> calendar date, which are made
> from the Julian dates that I read in.
> 
> dmp<-dates(ss$MPdate,origin=c(month = 1, day = 1,
> year = 1900))
> dc<-dates(ss$Cdate,origin=c(month = 1, day = 1, year
> = 1900))
> dd<-dates(ss$Ddate,origin=c(month = 1, day = 1, year
> = 1900))
> dld<-dates(ss$LDdate,origin=c(month = 1, day = 1,
> year = 1900))
> dud<-dates(ss$UDdate,origin=c(month = 1, day = 1,
> year = 1900))
> dbs<-dates(ss$BSdate,origin=c(month = 1, day = 1,
> year = 1900))
> 
> Then, I plot a stack of 6 time series, one on top of
> each other.
> Note that I used "arrow" for error bars.
> #stack of plots, one for each site.
> attach(ss)
> 
> par(mar=c(2,2,1,1), mfrow = c(6,1))
> plot(MP~dmp, type="b",xlab="",ylim=c(0,1),xaxt =
> "n")
>
arrows(dmp,MP+semp,dmp,MP-semp,length=.02,angle=90,code=3)
> plot(C~dc, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> arrows(dc,C+sec,dc,C-sec,length=.02,angle=90,code=3)
> plot(D~dd, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> arrows(dd,D+sed,dd,D-sed,length=.02,angle=90,code=3)
> plot(LD~dld, type="b",xlab="",ylim=c(0,1),xaxt =
> "n")
>
arrows(dld,LD+seld,dld,LD-seld,length=.02,angle=90,code=3)
> plot(UD~dud, type="b",xlab="",ylim=c(0,1),xaxt =
> "n")
>
arrows(dud,UD+seud,dud,UD-seud,length=.02,angle=90,code=3)
> plot(BS~dbs, type="b",xlab="",ylim=c(0,1))
>
arrows(dbs,BS+sebs,dbs,BS-sebs,length=.02,angle=90,code=3)
> 
> 
> 
> I hope this helps, 
> Don
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> natalia norden wrote:
> > 
> > Hello,
> > I have some "stupid" problems managing "date"
> data.
> > I have a colomn "date", which I converted from a
> character representation:
> > 
> > for example:
> > a="26/02/06"
> > date=strptime(a,format="%d/%m/%y")
> > 
> > For one part of the analysis, I'm interested only
> in the month and the 
> > year, so I did:
> > 
> > m.y=strftime(date,format="%m/%y")
> > 
> > This returns me "02/06", but this is an object of
> class "character" and 
> > I can't convert it into an object of class "Date".
> Doing 
> > strptime(m.y,format="%m/%y") or
> as.Date(m.y,format="%m/%y") returns me NA
> > 
> > How can a convert this colomn "m.y" in a Date
> class?
> > 
> > Actually, I need it to plot fruiting data against
> time (month and year).
> > Because I have many values of seeds in a month, I
> used the function
> > tapply:
> > seeds=tapply(data$seed,data$m.y,sum)
> > But plot(x=names(seeds),y=seeds) doesn't work.
> Does anyone know an 
> > easier way?
> > 
> > Thank you for your time,
> > natalia
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> -- 
> View this message in context:
>
http://www.nabble.com/Date-problem-tf1280528.html#a11660658
> Sent from the R help mailing list archive at
> Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From berwin at maths.uwa.edu.au  Wed Jul 18 03:26:58 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Wed, 18 Jul 2007 09:26:58 +0800
Subject: [R] Optimization
In-Reply-To: <478597.56487.qm@web32201.mail.mud.yahoo.com>
References: <000301c7c879$70f04120$7c94100a@win.ad.jhu.edu>
	<478597.56487.qm@web32201.mail.mud.yahoo.com>
Message-ID: <20070718092658.3b1351b0@absentia>

G'day Moshe,

On Tue, 17 Jul 2007 17:32:52 -0700 (PDT)
Moshe Olshansky <m_olshansky at yahoo.com> wrote:

> This is partially true since both the function to be
> maximized and the constraint are non-linear. 

I am not sure what your definition of non-linear is, but in my book,
and I believe by most mathematical/statistical definitions, the
objective function is linear.

The only non-linearity comes in through the second constraint.

> One may substitute 1-x1-x2 for x3 and use (let say) Lagrange
> multipliers to get two non-linear equations with 2
> unknowns for which there should be a function solving
> them. 

Why would you want to use Lagrange multipliers?  Isn't that a bit of an
overkill?  Once you substitute 1-x1-x2 for x3 in the second constraint,
you have a quadratic equations in x1 and x2.  So for any given value of
x1 you can solve for x2 (or for any given value of x2 you can solve for
x1).  They still teach how to solve quadratic equations at school,
don't they? ;-)

> Then you must find the points where the
> constraint function intersects with the triangle
> {x1>=0,x2>=0,x1+x2<=1}, which is easier (for each of
> the 3 edges you get a non-linear equation in one
> variable).

Even easier.  Take an x1 between 0 and 1.  If for that x1 the quadratic
equation in x2 has no real solution, then x1 is not feasible.
Otherwise find the values of x2 that solve the equation.  Use each of
these values together with x1 to calculate corresponding values of x3.
Then check these tuples for feasibility.  If they are feasible,
evaluate the objective function and return the tuple with the larger
function value.

All the calculations outlined in the paragraph above are easily
implemented in R, e.g. the function polyroot() returns the roots of a
polynomial.

Cheers,

	Berwin

=========================== Full address =============================
Berwin A Turlach                            Tel.: +65 6515 4416 (secr)
Dept of Statistics and Applied Probability        +65 6515 6650 (self)
Faculty of Science                          FAX : +65 6872 3919       
National University of Singapore     
6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
Singapore 117546                    http://www.stat.nus.edu.sg/~statba


From m_olshansky at yahoo.com  Wed Jul 18 03:38:12 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Tue, 17 Jul 2007 18:38:12 -0700 (PDT)
Subject: [R] Optimization
In-Reply-To: <20070718092658.3b1351b0@absentia>
Message-ID: <462096.25157.qm@web32207.mail.mud.yahoo.com>

You are right!!!

For some strange reason I substituted ^
(exponentiation) for *, so the problem became

Max x1^0.021986+x2^0.000964+x3^0.02913

with these conditions:

x1+x2+x3=1;

sqrt((x1^0.114434)^2+(x2^0.043966)^2+(x3^0.100031)^2)=0.04;

which is clearly non-linear.

--- Berwin A Turlach <berwin at maths.uwa.edu.au> wrote:

> G'day Moshe,
> 
> On Tue, 17 Jul 2007 17:32:52 -0700 (PDT)
> Moshe Olshansky <m_olshansky at yahoo.com> wrote:
> 
> > This is partially true since both the function to
> be
> > maximized and the constraint are non-linear. 
> 
> I am not sure what your definition of non-linear is,
> but in my book,
> and I believe by most mathematical/statistical
> definitions, the
> objective function is linear.
> 
> The only non-linearity comes in through the second
> constraint.
> 
> > One may substitute 1-x1-x2 for x3 and use (let
> say) Lagrange
> > multipliers to get two non-linear equations with 2
> > unknowns for which there should be a function
> solving
> > them. 
> 
> Why would you want to use Lagrange multipliers? 
> Isn't that a bit of an
> overkill?  Once you substitute 1-x1-x2 for x3 in the
> second constraint,
> you have a quadratic equations in x1 and x2.  So for
> any given value of
> x1 you can solve for x2 (or for any given value of
> x2 you can solve for
> x1).  They still teach how to solve quadratic
> equations at school,
> don't they? ;-)
> 
> > Then you must find the points where the
> > constraint function intersects with the triangle
> > {x1>=0,x2>=0,x1+x2<=1}, which is easier (for each
> of
> > the 3 edges you get a non-linear equation in one
> > variable).
> 
> Even easier.  Take an x1 between 0 and 1.  If for
> that x1 the quadratic
> equation in x2 has no real solution, then x1 is not
> feasible.
> Otherwise find the values of x2 that solve the
> equation.  Use each of
> these values together with x1 to calculate
> corresponding values of x3.
> Then check these tuples for feasibility.  If they
> are feasible,
> evaluate the objective function and return the tuple
> with the larger
> function value.
> 
> All the calculations outlined in the paragraph above
> are easily
> implemented in R, e.g. the function polyroot()
> returns the roots of a
> polynomial.
> 
> Cheers,
> 
> 	Berwin
> 
> =========================== Full address
> =============================
> Berwin A Turlach                            Tel.:
> +65 6515 4416 (secr)
> Dept of Statistics and Applied Probability       
> +65 6515 6650 (self)
> Faculty of Science                          FAX :
> +65 6872 3919       
> National University of Singapore     
> 6 Science Drive 2, Blk S16, Level 7          e-mail:
> statba at nus.edu.sg
> Singapore 117546                   
> http://www.stat.nus.edu.sg/~statba
>


From e.catchpole at adfa.edu.au  Wed Jul 18 04:14:11 2007
From: e.catchpole at adfa.edu.au (ecatchpole)
Date: Wed, 18 Jul 2007 12:14:11 +1000
Subject: [R] Optimization (MAX) with R
In-Reply-To: <E14EC333CA5C844EBBED6AC9DE19A2D6DFE16F@USEBW104.mailasp.unicredit.it>
References: <E14EC333CA5C844EBBED6AC9DE19A2D6DFE16F@USEBW104.mailasp.unicredit.it>
Message-ID: <469D7773.5040309@adfa.edu.au>

Talarico Massimiliano (UniCredit Xelion Banca) wrote on 07/17/2007 06:00 
PM:
> Dear all,
> I need a suggest to obtain the max of this function:
>  
> Max x1*0.021986+x2*0.000964+x3*0.02913
>
>
>
> with these conditions:
>  
> x1+x2+x3=1;
> sqrt((x1*0.114434)^2+(x2*0.043966)^2+(x3*0.100031)^2)=0.04;
> x1>=0;
> x1<=1;
> x2>=0;
> x2<=1;
> x3>=0;
> x3<=1;
>  
> Any suggests ?
>   

Use Lagrange multipliers and do it analytically?

Ted.


From brown_emu at yahoo.com  Wed Jul 18 05:49:14 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 17 Jul 2007 20:49:14 -0700 (PDT)
Subject: [R] Alternative to xyplot()?
In-Reply-To: <loom.20070718T030847-446@post.gmane.org>
Message-ID: <23984.22210.qm@web39701.mail.mud.yahoo.com>

What's wrong with lattice? Here's an alternative:

library(ggplot2)
ggplot(data=data.frame(x,y,grps=factor(grps)), 
       mapping=aes(x=x,y=y,colour=grps)) +     # define data
  geom_identity() +                            # points
  geom_smooth(method="lm")                     # regression line



--- Ben Bolker <bolker at ufl.edu> wrote:

> Manuel Morales <Manuel.A.Morales <at> williams.edu> writes:
> 
> > 
> > Sorry. I was thinking of the "groups" functionality, as illustrated
> > below:
> > 
> > grps<-rep(c(1:3),10)
> > x<-rep(c(1:10),3)
> > y<-x+grps+rnorm(30)
> > library(lattice)
> > xyplot(y~x,group=grps, type=c("r","p"))
> 
>   The points (type "p") are easy, the regression lines (type "r") are a
> little
> harder. How about:
> 
> 
> plot(y~x,col=grps)
> invisible(mapply(function(z,col) {abline(lm(y~x,data=z),col=col)},
>           split(data.frame(x,y),grps),1:3))
> 
>   cheers
>     Ben Bolker
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From quesada at gmail.com  Wed Jul 18 05:52:51 2007
From: quesada at gmail.com (Jose)
Date: Wed, 18 Jul 2007 03:52:51 +0000 (UTC)
Subject: [R] R equivalent to Matlab's Bayes net toolbox
References: <op.tvkenzwy4hcap5@delllap.auth.ucla.edu>
	<C83C5E3DEEE97E498B74729A33F6EAEC038787C2@DJFPOST01.djf.agrsci.dk>
Message-ID: <loom.20070718T054251-289@post.gmane.org>

S?ren H?jsgaard <Soren.Hojsgaard <at> agrsci.dk> writes:

> 
> Jose,
> I am not entirely sure what Matlabs Bayes net toolbox does, but I guess it 
implements as propagation
> algorithm for Bayesian networks. There is no such package on CRAN - yet - but 
there will be soon: I've
> created a package called gRbayesnet which implements the "Lauritzen-
Spiegelhalter" propagation
> algorithm. I expect to upload it to CRAN within the next few days. 
> Best regards
> S?ren
> 
> ________________________________
> 
> Fra: r-help-bounces <at> stat.math.ethz.ch p? vegne af Jose Quesada 
> Sendt: ma 16-07-2007 17:58
> Til: r-help <at> lists.r-project.org
> Emne: [R] R equivalent to Matlab's Bayes net toolbox
> 
> Hi,
> 
> I'm attending  summer School at UCLA (IPAM) on "probabilistics models of 
> cognition". I have been an R-user since v. 1.4.1, but was trained in the 
> frequentist tradition (as most psychologists!). I found that all faculty 
> here use matlab and Murphy's bayes net toolbox. I have not had the need to 
> use matlab before, and would love to stick to R for graphics models and 
> bayesian modeling in general (even if it takes me extra time to cross-code 
> the examples in matlab into R).
> 
> I'm trying to find an R equivalent to Matlab's Bayes net toolbox.
> 
> I have found packages 'deal' and 'gR', and played around with:
> http://www.ci.tuwien.ac.at/gR/
> 
> But I cannot really figure out how all these packages are integrated. 
> Also, appendix B of 'bayesian AI' lists gR as "vaporware" (although this 
> could well be outdated by now).
> 
> Is there any R news article on bayesian networks? It's hard to find, 
> because I don't think the content of R-news is indexed in CRAN. I could 
> download every issue and search the TOC, but it'd be time-consuming.
> 
> Even though the examples in the documentation in package 'deal' are good, 
> they fall short. A good tutorial would be great.
> What I'd like to know from you is whether R is a sensible choice or 
> whether BNT is just easier and more mature.
> 
> Right now I could easily chose R or Matlab, since I have made little 
> investment in any form of bayesian networks modeling; However, since I 
> have a better background in R than in Matlab, I'd love to stay with R.
> 
> Any resources (mailing lists, books, tutorials) would be greatly 
> appreciated.
> 
> Thanks a lot in advance,
> -Jose
> 
> --
> Jose Quesada, PhD.
> http://www.andrew.cmu.edu/~jquesada
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 



Hi S?ren,

It looks like bnt implements several algorithms for learning both parameters 
and structure:
http://bnt.sourceforge.net

MCMC for example is implemented. I didn't know about this Lauritzen-
Spiegelhalter" propagation algorithm. 

The thing that I don't understand in the gR page is why there are so many 
different packages and why they are not very integrated:
boa
CoCo
coda
deal
dynamicGraph (core)
ggm
gRbase (core)
mathgraph
mimR
R2WinBUGS
rbugs
SIN
http://cran.r-project.org/src/contrib/Views/gR.html

I tried deal, for example, and it's not really comparable to bnt.

In Matlab, there seems to be a package (well, toolbox) only, and there seems to 
be some of a community around it:
http://tech.groups.yahoo.com/group/BayesNetToolbox/

Is there any kind of community doing graphical models stuff on R?

Thanks,
-Jose


From h.wickham at gmail.com  Wed Jul 18 06:24:23 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 18 Jul 2007 06:24:23 +0200
Subject: [R] xyplot for longitudinal data
In-Reply-To: <c9955100707171553r312ca7c6ra69e076676adb58d@mail.gmail.com>
References: <c9955100707171553r312ca7c6ra69e076676adb58d@mail.gmail.com>
Message-ID: <f8e6ff050707172124n2b85676egcc5977acf9b3da96@mail.gmail.com>

On 7/18/07, Osman Al-Radi <osman.al.radi at gmail.com> wrote:
> Dear R-help subscribers,
>
> I use xyplot to plot longitudinal data as follows:
>
> score<-runif(100,-4,5)
> group<-sample(1:4,100,rep=T)
> subject<-rep(1:25,4)
> age<-rep(runif(4,1,40),25)
> df<-data.frame(score,group,age,subject)
>
> xyplot(score~age|group, group=subject,
> panel=function(...){
> panel.loess(...,lwd=4)
> panel.superpose(...)}
> ,data=df)
>
> this produced a plot with four panels one for each group, with unique
> plotting parameters for each subject.
>
> How can I create a create a plot with a single panel where all four groups
> are superimposed using different line colors and symbols for each group, but
> preserving the longitudinal nature of the data (i.e. one line per subject).
>

Another approach would be to use the ggplot2 package (http://had.co.nz/ggplot2):

library(ggplot2)
qplot(age, score, data=df, group = interaction(subject, group),
geom="line", colour=factor(group)) + geom_smooth(aes(group=group),
enp.target=2, size=4)

# This gives a smooth per group, if you want one over all smooth
# use the following instead
+ geom_smooth(aes(group=1), enp.target=2, size=4)

# You can have both by adding both geom_smooths on

Hadley


From h.wickham at gmail.com  Wed Jul 18 06:28:52 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 18 Jul 2007 06:28:52 +0200
Subject: [R] Alternative to xyplot()?
In-Reply-To: <23984.22210.qm@web39701.mail.mud.yahoo.com>
References: <loom.20070718T030847-446@post.gmane.org>
	<23984.22210.qm@web39701.mail.mud.yahoo.com>
Message-ID: <f8e6ff050707172128w8acf4d0p869a45ace74bde9b@mail.gmail.com>

On 7/18/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> What's wrong with lattice? Here's an alternative:
>
> library(ggplot2)
> ggplot(data=data.frame(x,y,grps=factor(grps)),
>        mapping=aes(x=x,y=y,colour=grps)) +     # define data
>   geom_identity() +                            # points
>   geom_smooth(method="lm")                     # regression line

I think you mean geom_point() not geom_identity()!

Also, if you just want groups, and not colours you can use the group aesthetic.

library(ggplot2)
qplot(x, y, group=grps) + geom_smooth(method=lm)

# You can have different grouping in different layers
qplot(x, y, colour=factor(grps)) + geom_smooth(method=lm)
qplot(x, y, colour=factor(grps)) + geom_smooth(aes(group=1), method=lm)

You can see more examples of ggplot2 in use at http://had.co.nz/ggplot2

Hadley


From ripley at stats.ox.ac.uk  Wed Jul 18 07:44:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jul 2007 06:44:23 +0100 (BST)
Subject: [R] difficulties with "setMethod"
In-Reply-To: <000001c7c8b2$adf18910$2401a8c0@NBNILS>
References: <000001c7c8b2$adf18910$2401a8c0@NBNILS>
Message-ID: <Pine.LNX.4.64.0707180639240.12974@gannet.stats.ox.ac.uk>

On Tue, 17 Jul 2007, Nils R?fenacht wrote:

> Dear all!
>
> I do definetley have some difficulties. Here is my code:
>
>> setMethod("write",
> +             signature(object = "KMatrix", path = "character"),
> +             function(object,path){
> +             write.table(object at data,path,row.names=FALSE, sep = "\t")
> +             }
> +             )
> error in match.call(fun, fcall) : unused argument(s) (object =
> "KMatrix", path = "character")
>>
>
> "KMatrix" is an extension of some data.frame object, i.e. it's a
> data.frame (KMatrix at data) with some additional slots (e.g. KMatrix at Size)
>
> What's wrong with my setMethod?

Take a look at the existing function 'write':
> write
function (x, file = "data", ncolumns = if (is.character(x)) 1 else 5,
     append = FALSE, sep = " ")
cat(x, file = file, sep = c(rep.int(sep, ncolumns - 1), "\n"),
     append = append)

You are trying to set a method for it with completely different arguments.

I am not sure what you are trying to achieve by this: it would look more 
natural to set a write.table() method since no one is going to call 
write() on the data frames you are wishing to 'extend'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From whinev at gmail.com  Wed Jul 18 08:29:09 2007
From: whinev at gmail.com (Ev Whin)
Date: Wed, 18 Jul 2007 14:29:09 +0800
Subject: [R] Check DESCRIPTION meta-information ... ERROR
Message-ID: <dfed1c180707172329t26a16e2bh88c025a1a23cf889@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/742bc3de/attachment.pl 

From whhu at sibs.ac.cn  Wed Jul 18 08:28:18 2007
From: whhu at sibs.ac.cn (whhu)
Date: Wed, 18 Jul 2007 14:28:18 +0800
Subject: [R] hi, about mysql from r
Message-ID: <200707181428180787489@sibs.ac.cn>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/b40db33d/attachment.pl 

From elyakhlifi_mustapha at yahoo.fr  Wed Jul 18 09:03:45 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Wed, 18 Jul 2007 07:03:45 +0000 (GMT)
Subject: [R] list
Message-ID: <64648.5405.qm@web27513.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/6e0683cd/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 18 09:04:16 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Jul 2007 08:04:16 +0100 (BST)
Subject: [R] [R-sig-DB] RODBC on Oracle DB
In-Reply-To: <1184675570.6752.11.camel@Bellerophon.localdomain>
References: <4556.1184657064@net2000.ch>
	<1184675570.6752.11.camel@Bellerophon.localdomain>
Message-ID: <Pine.LNX.4.64.0707171405020.27614@gannet.stats.ox.ac.uk>

On Tue, 17 Jul 2007, Marc Schwartz wrote:

> Try the sqlQuery() syntax with a semi-colon at the end of it:
>
>  sqlQuery(essai, "select * from S_TYP_COLLEGES;")
>
> Oracle requires the semi-colon at the end of the SQL statement.

Over ODBC?  I've never heard of that, and others have used RODBC to 
Oracle successfully when it does not itself add semicolons.  What sqlFetch 
is running is (in a vanilla setup) "SELECT * FROM 'S_TYP_COLLEGES'", and 
we've heard of a few cases where the quotes were a problem, hence my 
suggestion.

The error message is not a syntax error (which is what I would expect 
from a missing terminator), but that the table/view is not found.

> If that does not help, try these queries using the Oracle Instant Client
> command line application outside of R and see if your queries work
> there.  If so, then we can likely isolate the problem to R.  If not,
> then there is an ODBC/Oracle configuration issue.
>
> If you are unsure of how to use (or perhaps install) the Oracle Instant
> Client, check with one of your SysAdmins.
>
> BTW, unstated is the OS here, but I presume Windows, given the ODBC
> driver version and DLL noted previously.
>
> HTH,
>
> Marc Schwartz
>
>
> On Tue, 2007-07-17 at 09:24 +0200, eric at net2000.ch wrote:
>> essai <- odbcConnect("ORESTE_prod",  uid="osis_r",  pwd="12miss15" ,case="oracle")
>>
>>> sqlTables(essai)$ORESTE
>>
>> ...
>>
>> 1315      <NA>      ORESTE              S_PROFESSIONS_OLD        TABLE    <NA>
>> 1316      <NA>      ORESTE                  S_PROVENANCES        TABLE    <NA>
>> 1317      <NA>      ORESTE                        S_SEXES        TABLE    <NA>
>> 1318      <NA>      ORESTE                 S_SOUS_CLASSES        TABLE    <NA>
>> 1319      <NA>      ORESTE                 S_TYP_COLLEGES        TABLE    <NA>
>> 1320      <NA>      ORESTE             S_TYP_ENSEIGNEMENT        TABLE    <NA>
>>
>> ...
>>
>>> sqlQuery(essai, "select * from S_TYP_COLLEGES")
>> [1] "[RODBC] ERROR: Could not SQLExecDirect"
>> [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>>
>> I have also tried the
>> essai2 <- odbcDriverConnect(connection="essai2")
>> But with no succes.
>>
>>
>>
>> On Lun Juil 16 15:32 , Prof Brian Ripley <ripley at stats.ox.ac.uk> sent:
>>
>>> The problem could be quoting, if Oracle is not standards-compliant.
>>> See the options in ?odbcConnect.
>>>
>>> If sqlQuery(essai, "select * from S_TYP_COLLEGES") works, this is likely
>>> to be the problem.
>>>
>>> On Mon, 16 Jul 2007, eric at net2000.ch wrote:
>>>
>>>>
>>>>
>>>>> essai
>>>>> odbcGetInfo(essai)
>>>>       DBMS_Name         DBMS_Ver  Driver_ODBC_Ver
>>>>        "Oracle"     "09.00.0121"          "03.51"
>>>> Data_Source_Name      Driver_Name       Driver_Ver
>>>>   "ORESTE_prod"    "SQORA32.DLL"     "09.00.0101"
>>>>        ODBC_Ver      Server_Name
>>>>    "03.52.0000"           "weba"
>>>>
>>>>
>>>>> sqlTables(essai)
>>>>
>>>> The result of this function is a liste of tables, one of them is called:
>>>> S_TYP_COLLEGES.
>>>>
>>>>
>>>>> sqlFetch(essai,"S_TYP_COLLEGES")
>>>> [1] "[RODBC] ERROR: Could not SQLExecDirect"
>>>> [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>>>>
>>>>> sqlFetch(essai, "S_TYP_COLLEGES", colnames=TRUE, rownames=FALSE)
>>>> [1] "[RODBC] ERROR: Could not SQLExecDirect"
>>>> [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>>>>
>>>>
>>>> What could be the problem here ?
>>>> Any help is welcome
>>>> Eric R?thlisberger, Neuch?tel
>>>>
>>>> _______________________________________________
>>>> R-sig-DB mailing list -- R Special Interest Group
>>>> R-sig-DB at stat.math.ethz.ch
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-db
>>>>
>>>
>>> --
>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From p.dalgaard at biostat.ku.dk  Wed Jul 18 09:07:19 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Wed, 18 Jul 2007 09:07:19 +0200
Subject: [R] R equivalent to Matlab's Bayes net toolbox
In-Reply-To: <loom.20070718T054251-289@post.gmane.org>
References: <op.tvkenzwy4hcap5@delllap.auth.ucla.edu>
	<C83C5E3DEEE97E498B74729A33F6EAEC038787C2@DJFPOST01.djf.agrsci.dk>
	<loom.20070718T054251-289@post.gmane.org>
Message-ID: <1184742440.29989.16.camel@titmouse2.kubism.ku.dk>

On Wed, 2007-07-18 at 03:52 +0000, Jose wrote:

> The thing that I don't understand in the gR page is why there are so many 
> different packages and why they are not very integrated:

You have to understand the gR project for that. It started from a number
of completely separate pieces of software within the general field of
graphical models, and tried to bring people together and make the
existing pieces of software accessible from R. Given that the active
core of the group was really just a handful of people with limited R
programming experience (much of the original code was written in
dialects of Pascal/Delphi), the project must be said to have had some
success. However, the most pronounced effect has been to bring those old
codes out in the open, but seamless integration would be quite far into
the horizon.


From christophe at pallier.org  Wed Jul 18 09:08:39 2007
From: christophe at pallier.org (Christophe Pallier)
Date: Wed, 18 Jul 2007 09:08:39 +0200
Subject: [R] list
In-Reply-To: <64648.5405.qm@web27513.mail.ukl.yahoo.com>
References: <64648.5405.qm@web27513.mail.ukl.yahoo.com>
Message-ID: <dea6cb960707180008g37353071mc22b78ebcdb0220b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/43af4a24/attachment.pl 

From smangut at gmail.com  Wed Jul 18 10:02:13 2007
From: smangut at gmail.com (sigalit mangut-leiba)
Date: Wed, 18 Jul 2007 11:02:13 +0300
Subject: [R] filter out observation by condition
Message-ID: <c99f7100707180102n6b80296bl23f28a6d8c94dec3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/0e73dcfc/attachment.pl 

From gyadav at ccilindia.co.in  Wed Jul 18 10:10:38 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Wed, 18 Jul 2007 13:40:38 +0530
Subject: [R] R and Copula
In-Reply-To: <29082562.1356241184744869893.JavaMail.nabble@isper.nabble.com>
Message-ID: <OFC94AAA62.BB5EBA68-ON6525731C.002B7FA8-6525731C.002CE93A@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/38deb545/attachment.pl 

From ligges at statistik.uni-dortmund.de  Wed Jul 18 10:38:46 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 18 Jul 2007 10:38:46 +0200
Subject: [R] filter out observation by condition
In-Reply-To: <c99f7100707180102n6b80296bl23f28a6d8c94dec3@mail.gmail.com>
References: <c99f7100707180102n6b80296bl23f28a6d8c94dec3@mail.gmail.com>
Message-ID: <469DD196.8060005@statistik.uni-dortmund.de>



sigalit mangut-leiba wrote:
> hello,
> I have a longitudinal data:
> idn     mort30      newinfec
> 1           0               1
> 1           0               1
> 1           0               1
> 1           0               1
> 2           1               1
> 2           1               1
> 2           1               1
> 3           0               0
> 3           0               0
> 3           0               0
> 3           0               0
> 3           0               0
> 
> and i want to filter out those obs. that has mort30==1 (mort30 is constant
> over idn).
> how can i use "if...else" and filter out those unwanted obs.?
> I appriciate the help,
> sigalit.


subset(data, mort30 != 1)

Uwe Ligges



> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From piero.ricchiuto at unimi.it  Wed Jul 18 11:09:28 2007
From: piero.ricchiuto at unimi.it (Piero Ricchiuto)
Date: Wed, 18 Jul 2007 11:09:28 +0200
Subject: [R] Delaunay triangulation
Message-ID: <f697f179ae3d.469df4e8@unimi.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/59a9a136/attachment.pl 

From S.Ellison at lgc.co.uk  Wed Jul 18 11:19:04 2007
From: S.Ellison at lgc.co.uk (S Ellison)
Date: Wed, 18 Jul 2007 10:19:04 +0100
Subject: [R] Sorting data frame by a string variable
Message-ID: <s69de938.017@tedmail2.lgc.co.uk>

Dimitri

If you try 
> order(c("b","a","c"))
[1] 2 1 3

 or
> sort(c("b","a","c"))
[1] "a" "b" "c"
You will see that sort() and order() DO respect character order.

Your problem could be that your data frame variable is not a character but a factor (the default for read.table, for example)

Check the class of the variable. If it is a factor, try order(as.vector(String))

This will also work if String is a character vector; as.vector will just return the character variable. 


>>> "Dimitri Liakhovitski" <ld7631 at gmail.com> 17/07/2007 18:56:00 >>>
I have a data frame MyData with 2 variables.
One of the variables (String) contains a string of letters.
How can I resort MyData by MyData$String (alphabetically) and then
save the output as a sorted data file?

I tried:

o<-order(MyData$String)
SortedData<-rbind(MyData$String[o], MyData$Value[o])
write.table(SortedData,file="Sorted.txt",sep="\t",quote=F, row.names=F)


However, all strings get replaced with digits (1 for the first string,
2 for the second string etc.). How can I keep the strings instead of
digits?

Thank you!
Dimitri

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.

*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}


From johannes_graumann at web.de  Wed Jul 18 11:40:45 2007
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 18 Jul 2007 11:40:45 +0200
Subject: [R] Subsetting Enigma: More rows after dataframe[-list,]?
Message-ID: <f7kn6t$p5a$1@sea.gmane.org>

Hello again,

I'm trying to purge the indexes in i.delete from frame and end up with more
rows!? Please be so kind and let me know where I screw this up ...

Joh

> i.delete
  [1]   40   45  165  212  253  270  280  287  301  352  421  433  463  467 
487
 [16]  517  537  542  573  594  596  612  614  621  635  650  696  699  707 
732
 [31]  738  776  826  891  892  936  937  935  940  976  988  995 1037 1043
1059
 [46] 1081 1111 1123 1128 1132 1140 1153 1155 1165 1176 1179 1200 1281 1289
1300
 [61] 1320 1346 1356 1366 1369 1396 1406 1420 1428 1429 1471 1474 1475 1525
1540
 [76] 1554 1565 1645 1667 1665 1706 1711 1724 1764 1788 1791 1805 1808 1847
1881
 [91]   10   18  137  238  254  260  262  288  292  314  338  349  414  447 
457
[106]  465  470  478  511  530  536  552  582  588  644  655  687  693  701 
724
[121]  739  763  771  836  848  859  888  900  902  919  939  972  979  989
1000
[136] 1002 1015 1020 1026 1029 1032 1055 1060 1073 1088 1104 1117 1124 1130
1135
[151] 1144 1221 1225 1249 1251 1257 1376 1384 1386 1453 1487 1529 1532 1534
1605
[166] 1624 1633 1646 1648 1702 1787 1948 1951

> length(i.delete)
[1] 173

> nrow(frame)
[1] 1975

> nrow(frame[-i.delete,])
[1] 1802


From jim at bitwrit.com.au  Wed Jul 18 11:50:49 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 18 Jul 2007 19:50:49 +1000
Subject: [R] Missing value in circ.mean and polar.plot
In-Reply-To: <41271EA89B1DE246BC0326E0A13288580A33CB@ecqcstfmail2.quebec.int.ec.gc.ca>
References: <41271EA89B1DE246BC0326E0A13288580A33CB@ecqcstfmail2.quebec.int.ec.gc.ca>
Message-ID: <469DE279.1040406@bitwrit.com.au>

Gagnon,Francois [SteFoy] wrote:
> Hi, 
>  
> I try to compute circular means for a matrix with NAs, but the function "circ.mean" return only means for lines with complete values and do not accept "na.omit=T" or "na.rm=T", or "na.action=na.omit", or "na.fail=T". 
> Also, I try to use "polar.plot" of the package poltrix with the same matrix, but angles are not display because of many NAs.
> Does any one know how what should be the right NA action to get results in these two cases ?
>  
Hi Francois,
For one thing, you should be using radial.plot, as the angles are in 
radians, not degrees. However, your data shows up a couple of problems 
with radial.plot that I will fix. I'm currently fixing some minor (I 
hope) problems with plotrix v2.2-3 and this should turn up on CRAN in 
the next few days. I'll see if I can get radial.plot to produce the sort 
of plots you are seeking.

Thanks for finding the problem.

Jim


From gavin.simpson at ucl.ac.uk  Wed Jul 18 12:15:50 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 18 Jul 2007 11:15:50 +0100
Subject: [R] Subsetting Enigma: More rows after dataframe[-list,]?
In-Reply-To: <f7kn6t$p5a$1@sea.gmane.org>
References: <f7kn6t$p5a$1@sea.gmane.org>
Message-ID: <1184753750.8035.5.camel@gsimpson.geog.ucl.ac.uk>

On Wed, 2007-07-18 at 11:40 +0200, Johannes Graumann wrote:
> Hello again,
> 
> I'm trying to purge the indexes in i.delete from frame and end up with more
> rows!? Please be so kind and let me know where I screw this up ...

I think you'll have to explain why you think there are more rows after
using i.delete than before. (1975 - 173 = 1802). By purge, you mean
delete the rows indexed by i.delete? If so, you are doing nothing wrong:

> frame <- data.frame(matrix(runif(1975*10), ncol = 10))
> i.delete <- sample(nrow(frame), 173) # random rows to delete
> nrow(frame)
[1] 1975
> nrow(frame[-i.delete, ])
[1] 1802
> nrow(frame) > nrow(frame[-i.delete, ])
[1] TRUE

G

> 
> Joh
> 
> > i.delete
>   [1]   40   45  165  212  253  270  280  287  301  352  421  433  463  467 
> 487
>  [16]  517  537  542  573  594  596  612  614  621  635  650  696  699  707 
> 732
>  [31]  738  776  826  891  892  936  937  935  940  976  988  995 1037 1043
> 1059
>  [46] 1081 1111 1123 1128 1132 1140 1153 1155 1165 1176 1179 1200 1281 1289
> 1300
>  [61] 1320 1346 1356 1366 1369 1396 1406 1420 1428 1429 1471 1474 1475 1525
> 1540
>  [76] 1554 1565 1645 1667 1665 1706 1711 1724 1764 1788 1791 1805 1808 1847
> 1881
>  [91]   10   18  137  238  254  260  262  288  292  314  338  349  414  447 
> 457
> [106]  465  470  478  511  530  536  552  582  588  644  655  687  693  701 
> 724
> [121]  739  763  771  836  848  859  888  900  902  919  939  972  979  989
> 1000
> [136] 1002 1015 1020 1026 1029 1032 1055 1060 1073 1088 1104 1117 1124 1130
> 1135
> [151] 1144 1221 1225 1249 1251 1257 1376 1384 1386 1453 1487 1529 1532 1534
> 1605
> [166] 1624 1633 1646 1648 1702 1787 1948 1951
> 
> > length(i.delete)
> [1] 173
> 
> > nrow(frame)
> [1] 1975
> 
> > nrow(frame[-i.delete,])
> [1] 1802
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From brassnotdead at googlemail.com  Wed Jul 18 12:30:14 2007
From: brassnotdead at googlemail.com (Patrick Zimmermann)
Date: Wed, 18 Jul 2007 12:30:14 +0200
Subject: [R] how to combine presence only data sets to one presence/absence
	table
Message-ID: <bae7f91d0707180330k1fd7843bicb3df3162e115bad@mail.gmail.com>

Problem: I have a Set of samples each with a list of observed species
(presence only).
Data is stored in a excel spreadsheet and the columns (spl) have
different numbers of observations (spcs).
Now I want to organize the data in a species by sample matrix with
presence/absence style in R.

data style (in excel):

spl_A	spl_B	spl_C
spcs1	spcs1	spcs2
spcs2	spcs3	spcs3
spcs4		spcs5
spcs5

desired style:

	spl_A	spl_B	spl_C
spcs1	1	1	0
spcs2	1	0	1
spcs3	0	1	1
.
.
.

How and in which form do I import the data to R?
(read.table() seems not to be appropriate, as data is not organized as a table)

How can I create the species by sample matrix?

Thanks for any help,
Patrick Zimmermann


From amicogodzilla at bruttocarattere.org  Wed Jul 18 12:34:42 2007
From: amicogodzilla at bruttocarattere.org (Manuele Pesenti)
Date: Wed, 18 Jul 2007 12:34:42 +0200
Subject: [R] hist() Frequancy values
Message-ID: <200707181234.42910.amicogodzilla@bruttocarattere.org>

I have seen that the hist() function plots an histogram of the frequency but I 
cannot find the value of the object hist that contains theese values... how 
is possible to get out them? 

thank you very mutch
best regards

	Manuele


-- 
Manuele Pesenti
	manuele a inventati.org
	amicogodzilla a jabber.linux.it
	http://mpesenti.polito.it


From stef at biostatistics.it  Wed Jul 18 12:49:36 2007
From: stef at biostatistics.it (Stefano Calza)
Date: Wed, 18 Jul 2007 12:49:36 +0200
Subject: [R] hist() Frequancy values
In-Reply-To: <200707181234.42910.amicogodzilla@bruttocarattere.org>
References: <200707181234.42910.amicogodzilla@bruttocarattere.org>
Message-ID: <20070718104936.GD2788@med.unibs.it>

Try 

xx = hist(yy,plot=FALSE) ## plot=FALSE if you don't want to plot it

and then xx$counts

HIH

Stefano

On Wed, Jul 18, 2007 at 12:34:42PM +0200, Manuele Pesenti wrote:
<Manuele>I have seen that the hist() function plots an histogram of the frequency but I 
<Manuele>cannot find the value of the object hist that contains theese values... how 
<Manuele>is possible to get out them? 
<Manuele>
<Manuele>thank you very mutch
<Manuele>best regards
<Manuele>
<Manuele>	Manuele
<Manuele>
<Manuele>
<Manuele>-- 
<Manuele>Manuele Pesenti
<Manuele>	manuele a inventati.org
<Manuele>	amicogodzilla a jabber.linux.it
<Manuele>	http://mpesenti.polito.it
<Manuele>
<Manuele>______________________________________________
<Manuele>R-help a stat.math.ethz.ch mailing list
<Manuele>https://stat.ethz.ch/mailman/listinfo/r-help
<Manuele>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
<Manuele>and provide commented, minimal, self-contained, reproducible code.


From ccleland at optonline.net  Wed Jul 18 13:03:55 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 18 Jul 2007 07:03:55 -0400
Subject: [R] how to combine presence only data sets to one
 presence/absence table
In-Reply-To: <bae7f91d0707180330k1fd7843bicb3df3162e115bad@mail.gmail.com>
References: <bae7f91d0707180330k1fd7843bicb3df3162e115bad@mail.gmail.com>
Message-ID: <469DF39B.2050301@optonline.net>

Patrick Zimmermann wrote:
> Problem: I have a Set of samples each with a list of observed species
> (presence only).
> Data is stored in a excel spreadsheet and the columns (spl) have
> different numbers of observations (spcs).
> Now I want to organize the data in a species by sample matrix with
> presence/absence style in R.
> 
> data style (in excel):
> 
> spl_A	spl_B	spl_C
> spcs1	spcs1	spcs2
> spcs2	spcs3	spcs3
> spcs4		spcs5
> spcs5
> 
> desired style:
> 
> 	spl_A	spl_B	spl_C
> spcs1	1	1	0
> spcs2	1	0	1
> spcs3	0	1	1
> .
> .
> .
> 
> How and in which form do I import the data to R?
> (read.table() seems not to be appropriate, as data is not organized as a table)
> 
> How can I create the species by sample matrix?

  I'm not going to tackle how to read in the Excel data, but assuming
you had several vectors of species names gather together in a list, you
could construct a presence/absence data frame or matrix as follows:

spl_A <- c("spcs1","spcs2","spcs4","spcs5")
spl_B <- c("spcs1","spcs3")
spl_C <- c("spcs2","spcs3","spcs5")

mylist <- list(spl_A = spl_A, spl_B = spl_B, spl_C = spl_C)

mymat <- sapply(mylist,
          function(x){as.numeric(sort(unique(unlist(mylist))) %in% x)})

rownames(mymat) <- sort(unique(unlist(mylist)))

mymat
      spl_A spl_B spl_C
spcs1     1     1     0
spcs2     1     0     1
spcs3     0     1     1
spcs4     1     0     0
spcs5     1     0     1

> Thanks for any help,
> Patrick Zimmermann
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From nshephard at gmail.com  Wed Jul 18 13:05:14 2007
From: nshephard at gmail.com (Neil Shephard)
Date: Wed, 18 Jul 2007 12:05:14 +0100
Subject: [R] Re :  Combine R2HTML and Rcmd BATCH?
Message-ID: <31b34fca0707180405s1503e9a2v6d9302fd58ea6f54@mail.gmail.com>

> I have an R script that spawns output in the form of an HTML page. This
> is done by the R2HTML package.
>
> Now I want to run the same script using Rcmd BATCH. However, it seems
> that it is not possible to use R2HTML in this case.
>
> My script ends with this error message:
> #########################
> Error in dev.print(png, file = AbsGraphFileName, width = Width, height =
> Height,  :
>
> can only print from screen device
>
> Execution halted
> #########################
>
> I can not find how to work around this problem in the R2HTML manual or
> the help archives.
>
> Has anybody done a similar thing before? Any suggestions?

I'm not entierly sure this is what you want, but you may find the GDD
package useful.

I use it to generate graphs when I run scripts remotely or via batches
and don't have a graphical display.

Neil
-- 
"In mathematics you don't understand things. You just get used to
them."  - Johann von Neumann

Email - nshephard at gmail.com / n.shephard at sheffield.ac.uk
Website - http://slack.ser.man.ac.uk/
Photos - http://www.flickr.com/photos/slackline/


From Joao.Fadista at agrsci.dk  Wed Jul 18 13:52:34 2007
From: Joao.Fadista at agrsci.dk (=?iso-8859-1?Q?Jo=E3o_Fadista?=)
Date: Wed, 18 Jul 2007 13:52:34 +0200
Subject: [R] remove columns having a partial match name
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F38@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/0b77892b/attachment.pl 

From wwwhsd at gmail.com  Wed Jul 18 13:58:46 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Wed, 18 Jul 2007 08:58:46 -0300
Subject: [R] remove columns having a partial match name
In-Reply-To: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F38@DJFPOST01.djf.agrsci.dk>
References: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F38@DJFPOST01.djf.agrsci.dk>
Message-ID: <da79af330707180458l2cdabd8cjf4fc119eadcb9cba@mail.gmail.com>

Hi,

 DATA_OK <- DATA[,-match("Start", names(DATA))]

-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


On 18/07/07, Jo?o Fadista <Joao.Fadista at agrsci.dk> wrote:
> Dear all,
>
> I would like to know how can I retrieve a data.frame without the columns that have a partial match name. Let?s say that I have a data.frame with 200 columns and 100 of them have the name "StartX", with X being the unique part for each column name. I want to delete all columns that have the name starting with "Start". I?ve tried to do this but it doesn?t work:
>
> > DATA_OK <- DATA[,-match(("Start*"),names(DATA))]
> > dim(DATA_OK)
> NULL
>
>
> Thanks in advance.
> Best regards
>
> Jo?o Fadista
> Ph.d. student
>
>
>
>          UNIVERSITY OF AARHUS
> Faculty of Agricultural Sciences
> Dept. of Genetics and Biotechnology
> Blichers All? 20, P.O. BOX 50
> DK-8830 Tjele
>
> Phone:   +45 8999 1900
> Direct:  +45 8999 1900
> E-mail:  Joao.Fadista at agrsci.dk <mailto:Joao.Fadista at agrsci.dk>
> Web:     www.agrsci.org <http://www.agrsci.org/>
> ________________________________
>
> News and news media <http://www.agrsci.org/navigation/nyheder_og_presse> .
>
> This email may contain information that is confidential. Any use or publication of this email without written permission from Faculty of Agricultural Sciences is not allowed. If you are not the intended recipient, please notify Faculty of Agricultural Sciences immediately and delete this email.
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From jholtman at gmail.com  Wed Jul 18 14:04:40 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 18 Jul 2007 08:04:40 -0400
Subject: [R] remove columns having a partial match name
In-Reply-To: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F38@DJFPOST01.djf.agrsci.dk>
References: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F38@DJFPOST01.djf.agrsci.dk>
Message-ID: <644e1f320707180504vbe113e7y5a53376c3456f661@mail.gmail.com>

DATA_OK <- DATA[-grep("^Start", names(DATA)),]

On 7/18/07, Jo?o Fadista <Joao.Fadista at agrsci.dk> wrote:
> Dear all,
>
> I would like to know how can I retrieve a data.frame without the columns that have a partial match name. Let?s say that I have a data.frame with 200 columns and 100 of them have the name "StartX", with X being the unique part for each column name. I want to delete all columns that have the name starting with "Start". I?ve tried to do this but it doesn?t work:
>
> > DATA_OK <- DATA[,-match(("Start*"),names(DATA))]
> > dim(DATA_OK)
> NULL
>
>
> Thanks in advance.
> Best regards
>
> Jo?o Fadista
> Ph.d. student
>
>
>
>         UNIVERSITY OF AARHUS
> Faculty of Agricultural Sciences
> Dept. of Genetics and Biotechnology
> Blichers All? 20, P.O. BOX 50
> DK-8830 Tjele
>
> Phone:   +45 8999 1900
> Direct:  +45 8999 1900
> E-mail:  Joao.Fadista at agrsci.dk <mailto:Joao.Fadista at agrsci.dk>
> Web:     www.agrsci.org <http://www.agrsci.org/>
> ________________________________
>
> News and news media <http://www.agrsci.org/navigation/nyheder_og_presse> .
>
> This email may contain information that is confidential. Any use or publication of this email without written permission from Faculty of Agricultural Sciences is not allowed. If you are not the intended recipient, please notify Faculty of Agricultural Sciences immediately and delete this email.
>
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From marc_schwartz at comcast.net  Wed Jul 18 14:23:59 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 18 Jul 2007 07:23:59 -0500
Subject: [R] [R-sig-DB] RODBC on Oracle DB
In-Reply-To: <Pine.LNX.4.64.0707171405020.27614@gannet.stats.ox.ac.uk>
References: <4556.1184657064@net2000.ch>
	<1184675570.6752.11.camel@Bellerophon.localdomain>
	<Pine.LNX.4.64.0707171405020.27614@gannet.stats.ox.ac.uk>
Message-ID: <1184761439.32117.11.camel@Bellerophon.localdomain>

On Wed, 2007-07-18 at 08:04 +0100, Prof Brian Ripley wrote:
> On Tue, 17 Jul 2007, Marc Schwartz wrote:
> 
> > Try the sqlQuery() syntax with a semi-colon at the end of it:
> >
> >  sqlQuery(essai, "select * from S_TYP_COLLEGES;")
> >
> > Oracle requires the semi-colon at the end of the SQL statement.
> 
> Over ODBC?  I've never heard of that, and others have used RODBC to 
> Oracle successfully when it does not itself add semicolons.  What sqlFetch 
> is running is (in a vanilla setup) "SELECT * FROM 'S_TYP_COLLEGES'", and 
> we've heard of a few cases where the quotes were a problem, hence my 
> suggestion.
> 
> The error message is not a syntax error (which is what I would expect 
> from a missing terminator), but that the table/view is not found.

I stand corrected. 

I was going from memory pertaining to some prior errors that I had when
setting up my unixODBC/RODBC connection on Fedora to our Oracle server.
Adding the semi-colon to the query string seemed to have helped to
resolve at least part of the issue, but perhaps it was merely a
red-herring.

I just tried it here from home over our VPN and the queries worked with
and without the semi-colon.

Thanks for pointing that out.

Regards,

Marc


From samay.sar at gmail.com  Wed Jul 18 14:33:30 2007
From: samay.sar at gmail.com (d. sarthi maheshwari)
Date: Wed, 18 Jul 2007 18:03:30 +0530
Subject: [R] How to open an URL using RGtk2
Message-ID: <d4327f7e0707180533g79e6a7besb71d71dc78c8ee34@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/72376c57/attachment.pl 

From elyakhlifi_mustapha at yahoo.fr  Wed Jul 18 14:39:35 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Wed, 18 Jul 2007 12:39:35 +0000 (GMT)
Subject: [R] Neuman-Keuls
Message-ID: <280559.10345.qm@web27510.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/22e8726f/attachment.pl 

From brown_emu at yahoo.com  Wed Jul 18 14:52:43 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Wed, 18 Jul 2007 05:52:43 -0700 (PDT)
Subject: [R] how to combine presence only data sets to one
	presence/absence table
In-Reply-To: <bae7f91d0707180330k1fd7843bicb3df3162e115bad@mail.gmail.com>
Message-ID: <760625.37224.qm@web39704.mail.mud.yahoo.com>

I think you can still read as a table, just use argument fill=TRUE.

Reading from Excel in general: you can save data as 'csv' or tab-delimited
file and then use read.csv or read.delim, respectively, or use one of the
packages listed in the following post (for some reason lines breaks are
messed up but hope you can extract the content):
http://tolstoy.newcastle.edu.au/R/e2/help/07/06/19925.html

## read in data
x <- 
read.table(textConnection(
"spl_A	spl_B	spl_C
spcs1	spcs1	spcs2
spcs2	spcs3	spcs3
spcs4		spcs5
spcs5"
),fill=TRUE,header=TRUE,na.string="")

Then,

## 1. find unique
spcs <- sort(na.omit(unique(unlist(x)))) 
## 2. create matrix of zeros
mat <- matrix(0,ncol=ncol(x),nrow=length(spcs),
              dimnames=list(spcs,names(x))) 
## 3. assign zeros to matches
for( i in 1:ncol(mat) ) mat[match(x[,i],rownames(mat)),i] <- 1

Alternatively,
## find unique
spcs <- sort(na.omit(unique(unlist(x)))) 
## return the matrix you want (combine steps 2 and 3 from above)
sapply(x,function(.x,spcs)
       "names<-"(ifelse(!is.na(match(spcs,.x)),1,0),spcs),spcs)

Hope this helps.

ST

--- Patrick Zimmermann <brassnotdead at googlemail.com> wrote:

> Problem: I have a Set of samples each with a list of observed species
> (presence only).
> Data is stored in a excel spreadsheet and the columns (spl) have
> different numbers of observations (spcs).
> Now I want to organize the data in a species by sample matrix with
> presence/absence style in R.
> 
> data style (in excel):
> 
> spl_A	spl_B	spl_C
> spcs1	spcs1	spcs2
> spcs2	spcs3	spcs3
> spcs4		spcs5
> spcs5
> 
> desired style:
> 
> 	spl_A	spl_B	spl_C
> spcs1	1	1	0
> spcs2	1	0	1
> spcs3	0	1	1
> .
> .
> .
> 
> How and in which form do I import the data to R?
> (read.table() seems not to be appropriate, as data is not organized as a
> table)
> 
> How can I create the species by sample matrix?
> 
> Thanks for any help,
> Patrick Zimmermann
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From senthil.debian at gmail.com  Wed Jul 18 15:06:52 2007
From: senthil.debian at gmail.com (Senthil Kumar M)
Date: Wed, 18 Jul 2007 22:06:52 +0900
Subject: [R] How to open an URL using RGtk2
In-Reply-To: <d4327f7e0707180533g79e6a7besb71d71dc78c8ee34@mail.gmail.com>
References: <d4327f7e0707180533g79e6a7besb71d71dc78c8ee34@mail.gmail.com>
Message-ID: <56b822010707180606je6572a2jd4dee107e36eff06@mail.gmail.com>

On 7/18/07, d. sarthi maheshwari <samay.sar at gmail.com> wrote:
> Hi
>
> I am working on R 2.5.0 on window. I am trying to provide a Hyper-link to
> the user as a result, I have tried using gtkLinkButton to exercise the
> facility, however, i am not able to perform the required task, i.e. when I
> clicked on the LinkButton actually nothing happened.
>
> I have gone through the documentation for the same and found that
> GtkLinkButtonUriFunc is a function which is require to do something with the
> opening of the given URL. Further, I didn't find any other information
> regarding this.

Have you installed GtkLibs:

http://www.omegahat.org/RGtk/Windows/

Senthil


From f.calboli at imperial.ac.uk  Wed Jul 18 15:37:36 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 18 Jul 2007 14:37:36 +0100
Subject: [R] EM unsupervised clustering
Message-ID: <469E17A0.3030209@imperial.ac.uk>

Hi All,

I have a  n x m matrix. The n rows are individuals, the m columns are variables.

The matrix is in itself a collection of 1s (if a variable is observed for an 
individual), and 0s (is there is no observation).

Something like:

      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    0    1    1    0    0
[2,]    1    0    1    1    0    0
[3,]    1    0    1    1    0    0
[4,]    0    1    0    0    0    0
[5,]    1    0    1    1    0    0
[6,]    0    1    0    0    1    0


I use kmeans to find 2 or 3 clusters in this matrix

k2 = kmeans(data, 2, 10000000)
k3 = kmeans(data, 3, 10000000)

but I would like to use something a bit more refined, so I though about a EM 
based clustering. I am using the Mclust() function from the mclust package, but 
I get the following (to me incomprehensible) error message:

plot(Mclust(as.data.frame(data)), as.data.frame(data))
Hit <Return> to see next plot:
Hit <Return> to see next plot:
Hit <Return> to see next plot:
Error in 1:L : NA/NaN argument
In addition: Warning messages:
1: best model occurs at the min or max # of components considered in: 
summary.mclustBIC(Bic, data, G = G, modelNames = modelNames)
2: optimal number of clusters occurs at min choice in: 
Mclust(as.data.frame(anc.st.mat))
3: insufficient input for specified plot in: coordProj(data = data, parameters = 
x$parameters, z = x$z, what = "classification",

That's puzzling because the example given by ?Mclust is something like

plot(Mclust(iris[,-5]), iris[,-5])

which is pretty simple and dumbproof and works flawlessly...

best,

Federico

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From dimitris.rizopoulos at med.kuleuven.be  Wed Jul 18 15:48:07 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 18 Jul 2007 15:48:07 +0200
Subject: [R] EM unsupervised clustering
References: <469E17A0.3030209@imperial.ac.uk>
Message-ID: <008601c7c942$456f1200$0540210a@www.domain>

you could also have a look at function lca() from package `e1071' that 
performs a latent class analysis, e.g.,

fit1 <- lca(data, 2)
fit1

fit2 <- lca(data, 3)
fit2

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Federico Calboli" <f.calboli at imperial.ac.uk>
To: "r-help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, July 18, 2007 3:37 PM
Subject: [R] EM unsupervised clustering


> Hi All,
>
> I have a  n x m matrix. The n rows are individuals, the m columns 
> are variables.
>
> The matrix is in itself a collection of 1s (if a variable is 
> observed for an
> individual), and 0s (is there is no observation).
>
> Something like:
>
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    1    0    1    1    0    0
> [2,]    1    0    1    1    0    0
> [3,]    1    0    1    1    0    0
> [4,]    0    1    0    0    0    0
> [5,]    1    0    1    1    0    0
> [6,]    0    1    0    0    1    0
>
>
> I use kmeans to find 2 or 3 clusters in this matrix
>
> k2 = kmeans(data, 2, 10000000)
> k3 = kmeans(data, 3, 10000000)
>
> but I would like to use something a bit more refined, so I though 
> about a EM
> based clustering. I am using the Mclust() function from the mclust 
> package, but
> I get the following (to me incomprehensible) error message:
>
> plot(Mclust(as.data.frame(data)), as.data.frame(data))
> Hit <Return> to see next plot:
> Hit <Return> to see next plot:
> Hit <Return> to see next plot:
> Error in 1:L : NA/NaN argument
> In addition: Warning messages:
> 1: best model occurs at the min or max # of components considered 
> in:
> summary.mclustBIC(Bic, data, G = G, modelNames = modelNames)
> 2: optimal number of clusters occurs at min choice in:
> Mclust(as.data.frame(anc.st.mat))
> 3: insufficient input for specified plot in: coordProj(data = data, 
> parameters =
> x$parameters, z = x$z, what = "classification",
>
> That's puzzling because the example given by ?Mclust is something 
> like
>
> plot(Mclust(iris[,-5]), iris[,-5])
>
> which is pretty simple and dumbproof and works flawlessly...
>
> best,
>
> Federico
>
> -- 
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Imperial College, St Mary's Campus
> Norfolk Place, London W2 1PG
>
> Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193
>
> f.calboli [.a.t] imperial.ac.uk
> f.calboli [.a.t] gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From f.calboli at imperial.ac.uk  Wed Jul 18 16:05:34 2007
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Wed, 18 Jul 2007 15:05:34 +0100
Subject: [R] EM unsupervised clustering
In-Reply-To: <008601c7c942$456f1200$0540210a@www.domain>
References: <469E17A0.3030209@imperial.ac.uk>
	<008601c7c942$456f1200$0540210a@www.domain>
Message-ID: <469E1E2E.1090409@imperial.ac.uk>

Dimitris Rizopoulos wrote:
> you could also have a look at function lca() from package `e1071' that 
> performs a latent class analysis, e.g.,
> 
> fit1 <- lca(data, 2)

I tried but I got:

 > lca(data, 2)
Error in matrix(0, 2^nvar, nvar) : matrix: invalid 'nrow' value (too large or NA)
In addition: Warning message:
NAs introduced by coercion in: matrix(0, 2^nvar, nvar)

and dim(mat) is 110 and 109.

I am puzzled.

Cheers,

Fede


-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com


From macq at llnl.gov  Wed Jul 18 16:43:22 2007
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 18 Jul 2007 07:43:22 -0700
Subject: [R] [R-sig-DB] RODBC on Oracle DB
In-Reply-To: <4556.1184657064@net2000.ch>
References: <4556.1184657064@net2000.ch>
Message-ID: <p06230901c2c3d4428ce4@[128.115.153.6]>

I believe I have seen that error message from 
Oracle when I tried to query a table for which I 
did not have "select" privileges (and when I knew 
for certain that the table existed). Ask your 
database administrator about the table, and make 
sure that you do have that privilege.

What I am uncertain about is whether Oracle, when 
asked to list tables, returns a list that 
includes tables for which the user does not have 
select privileges.

-Don

At 9:24 AM +0200 7/17/07, eric at net2000.ch wrote:
>essai <- odbcConnect("ORESTE_prod", 
>uid="osis_r",  pwd="12miss15" ,case="oracle")
>
>>  sqlTables(essai)$ORESTE
>
>...
>
>1315      <NA>      ORESTE              S_PROFESSIONS_OLD        TABLE    <NA>
>1316      <NA>      ORESTE                  S_PROVENANCES        TABLE    <NA>
>1317      <NA>      ORESTE                        S_SEXES        TABLE    <NA>
>1318      <NA>      ORESTE                 S_SOUS_CLASSES        TABLE    <NA>
>1319      <NA>      ORESTE                 S_TYP_COLLEGES        TABLE    <NA>
>1320      <NA>      ORESTE             S_TYP_ENSEIGNEMENT        TABLE    <NA>
>
>...
>
>>  sqlQuery(essai, "select * from S_TYP_COLLEGES")
>[1] "[RODBC] ERROR: Could not SQLExecDirect"                           
>[2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>
>I have also tried the
>essai2 <- odbcDriverConnect(connection="essai2")
>But with no succes.
>
>
>
>On Lun Juil 16 15:32 , Prof Brian Ripley <ripley at stats.ox.ac.uk> sent:
>
>>The problem could be quoting, if Oracle is not standards-compliant.
>>See the options in ?odbcConnect.
>>
>>If sqlQuery(essai, "select * from S_TYP_COLLEGES") works, this is likely
>>to be the problem.
>>
>>On Mon, 16 Jul 2007, eric at net2000.ch wrote:
>>
>>>
>>>
>>>>  essai
>>>>  odbcGetInfo(essai)
>>>        DBMS_Name         DBMS_Ver  Driver_ODBC_Ver
>>>         "Oracle"     "09.00.0121"          "03.51"
>>>  Data_Source_Name      Driver_Name       Driver_Ver
>>>    "ORESTE_prod"    "SQORA32.DLL"     "09.00.0101"
>>>         ODBC_Ver      Server_Name
>>>     "03.52.0000"           "weba"
>>>
>>>
>>>>  sqlTables(essai)
>>>
>>>  The result of this function is a liste of tables, one of them is called:
>>>  S_TYP_COLLEGES.
>>>
>>>
>>>>  sqlFetch(essai,"S_TYP_COLLEGES")
>>>  [1] "[RODBC] ERROR: Could not SQLExecDirect"
>>>  [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>>>
>>>>  sqlFetch(essai, "S_TYP_COLLEGES", colnames=TRUE, rownames=FALSE)
>>>  [1] "[RODBC] ERROR: Could not SQLExecDirect"
>>>  [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>  >>
>  >>
>  >> What could be the problem here ?
>  >> Any help is welcome
>  >> Eric R?thlisberger, Neuch?tel
>  >>
>  >> _______________________________________________
>>>  R-sig-DB mailing list -- R Special Interest Group
>>>  R-sig-DB at stat.math.ethz.ch
>>>  https://stat.ethz.ch/mailman/listinfo/r-sig-db
>>>
>>
>>--
>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>1 South Parks Road,                     +44 1865 272866 (PA)
>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA
925-423-1062


From hdazizi at gmail.com  Wed Jul 18 12:46:59 2007
From: hdazizi at gmail.com (Hadi Darzian Azizi)
Date: Wed, 18 Jul 2007 12:46:59 +0200
Subject: [R] random number generation
Message-ID: <db3cad340707180346rd2cc022v258b754c0b0e3af8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/c5d400ba/attachment.pl 

From b3i4old02 at sneakemail.com  Wed Jul 18 13:38:27 2007
From: b3i4old02 at sneakemail.com (Michael Hoffman)
Date: Wed, 18 Jul 2007 12:38:27 +0100
Subject: [R] Forall symbol with plotmath/grid
Message-ID: <f7ku3m$k0a$1@sea.gmane.org>

I am trying to get the forall symbol (upside down "A") as part of the 
label of a lattice plot. Is there an easy way to do this?


From horacio9573 at gmail.com  Wed Jul 18 16:51:30 2007
From: horacio9573 at gmail.com (Horacio Castellini)
Date: Wed, 18 Jul 2007 11:51:30 -0300
Subject: [R] Can any one help me on format file data.
Message-ID: <90e13c0707180751y7d32d633m2930731c6f16db24@mail.gmail.com>

Hi all.
           I'd like know what is the format file saved by  Leica
Microsystems TCS SP2-AOBS equipped with a SP2-FCS2 Leica Microsystems
workstation its datas. Cause it save in *.fcs extention  file but
ins't flow cytometry standart format file...

Tahnks Horacio.


From tobischlot2002 at yahoo.com  Wed Jul 18 14:17:10 2007
From: tobischlot2002 at yahoo.com (Tobias Schlottmann)
Date: Wed, 18 Jul 2007 05:17:10 -0700 (PDT)
Subject: [R] Optimization question
Message-ID: <94179.58263.qm@web45008.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/e31b7c3e/attachment.pl 

From fisk at bowdoin.edu  Wed Jul 18 17:10:37 2007
From: fisk at bowdoin.edu (steve)
Date: Wed, 18 Jul 2007 11:10:37 -0400
Subject: [R] HSAURtable question
Message-ID: <f7lahe$5mn$1@sea.gmane.org>

It appears that HSAURtable only works on two dimensional tables. Is this 
correct?

For example, here is HairEyeColor:

, , Sex = Male

        Eye
Hair    Brown Blue Hazel Green
   Black    32   11    10     3
   Brown    38   50    25    15
   Red      10   10     7     7
   Blond     3   30     5     8

, , Sex = Female

        Eye
Hair    Brown Blue Hazel Green
   Black    36    9     5     2
   Brown    81   34    29    14
   Red      16    7     7     7
   Blond     4   64     5     8

HSAURtable(HairEyeColor) only uses the first matrix above

 > HSAURtable(HairEyeColor)
$xname
[1] "HairEyeColor"

$pkg
NULL

$varnames
[1] "Hair" "Eye"  "Sex"

$data
      [,1]    [,2]    [,3]   [,4]    [,5]
[1,] " "     "Brown" "Blue" "Hazel" "Green"
[2,] "Black" "32"    "11"   "10"    "3"
[3,] "Brown" "38"    "50"   "25"    "15"
[4,] "Red"   "10"    "10"   "7"     "7"
[5,] "Blond" "3"     "30"   "5"     "8"

attr(,"class")
[1] "tabtab"


From gerifalte28 at hotmail.com  Wed Jul 18 17:37:57 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 18 Jul 2007 09:37:57 -0600
Subject: [R] random number generation
In-Reply-To: <db3cad340707180346rd2cc022v258b754c0b0e3af8@mail.gmail.com>
References: <db3cad340707180346rd2cc022v258b754c0b0e3af8@mail.gmail.com>
Message-ID: <469E33D5.1060707@hotmail.com>

Hello Hadi,

See ?rgamma

The Gamma distribution usually takes two parameters, shape and scale, 
not the mean and st. deviation.  If you have data, you can estimate 
those parameters using MLE methods, which are nicely provided in MASS:

library(MASS)
fitdistr(yourdata,"Gamma")

Once you have your parameters you can generate random values using 
rgamma i.e. rgamma(1000,10,1) will generate 1000 random samples from a 
Gamma distribution with shape = 10 and scale = 1.

If you only have the mean and standard deviation you can approximate the 
shape and scale parameters using:

scale=variance/mean
shape=mean^2/var

Remember the variance is equal to sd^2

I hope this helps,


Francisco J. Zagmutt

PS: Please read the posting guide (see the link at the bottom of this 
email). It really helps people trying to help you :-)



Hadi Darzian Azizi wrote:
> Hi there,
> I am relatively new user of R. I need to generate random number following
> Gamma distribution with mean 14 und st.dev 3. I read the help-text but I can
> not understand it well.
> 
> Regards,
> Azizi
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From gerifalte28 at hotmail.com  Wed Jul 18 17:37:57 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 18 Jul 2007 09:37:57 -0600
Subject: [R] random number generation
In-Reply-To: <db3cad340707180346rd2cc022v258b754c0b0e3af8@mail.gmail.com>
References: <db3cad340707180346rd2cc022v258b754c0b0e3af8@mail.gmail.com>
Message-ID: <469E33D5.1060707@hotmail.com>

Hello Hadi,

See ?rgamma

The Gamma distribution usually takes two parameters, shape and scale, 
not the mean and st. deviation.  If you have data, you can estimate 
those parameters using MLE methods, which are nicely provided in MASS:

library(MASS)
fitdistr(yourdata,"Gamma")

Once you have your parameters you can generate random values using 
rgamma i.e. rgamma(1000,10,1) will generate 1000 random samples from a 
Gamma distribution with shape = 10 and scale = 1.

If you only have the mean and standard deviation you can approximate the 
shape and scale parameters using:

scale=variance/mean
shape=mean^2/var

Remember the variance is equal to sd^2

I hope this helps,


Francisco J. Zagmutt

PS: Please read the posting guide (see the link at the bottom of this 
email). It really helps people trying to help you :-)



Hadi Darzian Azizi wrote:
> Hi there,
> I am relatively new user of R. I need to generate random number following
> Gamma distribution with mean 14 und st.dev 3. I read the help-text but I can
> not understand it well.
> 
> Regards,
> Azizi
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Thierry.ONKELINX at inbo.be  Wed Jul 18 17:42:28 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 18 Jul 2007 17:42:28 +0200
Subject: [R] Strange warning in summary.lm
Message-ID: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>

Dear useRs,

Lately I noticed a strange warning in the summary of a lm-object. Any
idea what this warning is about? I'm using R 2.5.1 on Win XP pro.

> x <- rnorm(100)
> y <- rnorm(100)
> summary(lm(y~x))

Call:
lm(formula = y ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-1,77809 -0,68438 -0,04409  0,63891  2,30863 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) -0,00217    0,09244  -0,023    0,981
x            0,01315    0,09628   0,137    0,892

Residual standard error: 0,9236 on 98 degrees of freedom
Multiple R-Squared: 0.0001903,  Adjusted R-squared: -0.01001 
F-statistic: 0.01866 on 1 and 98 DF,  p-value: 0,8916 

Warning message:
NAs introduced by coercion in: as.double.default(Cf[okP]) 

	
Thanks,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


From info at aghmed.fsnet.co.uk  Wed Jul 18 17:43:14 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Wed, 18 Jul 2007 16:43:14 +0100
Subject: [R] SLLOOOWWW function ...
In-Reply-To: <f7i9bn$6e7$1@sea.gmane.org>
References: <f7i9bn$6e7$1@sea.gmane.org>
Message-ID: <Zen-1IBBgU-0004zJ-KA@pythagoras.zen.co.uk>

At 12:32 17/07/2007, Johannes Graumann wrote:
>Does anybody have any insight into how to make this faster?

I am not an expert on R programming by any means but I notice you are 
growing your new data frame row by row. I believe it is normally 
recommended to allocate enough space to start with.

>I suspect, that the rounding going on may be an issue, as is the stepping
>through data frame rows using integers ...
>
>If you have the patience to teach a noob, he will highly appreciate it ;0)
>
>Joh
>
>digit <- 4
>for (minute in seq(from=25,to=lrange[2])){
>   # Extract all data associtaed with the current time (minute)
>   frame <- subset(mylist,mylist[["Time"]] == minute)
>   # Sort by Intensity
>   frame <- frame[order(frame[["Intensity"]],decreasing = TRUE),]
>   # Establish output frame using the most intense candidate
>   newframe <- frame[1,]
>   # Establish overlap-checking vector using the most intense candidate
>   lowppm <- round(newframe[1,][["Mass"]]-newframe[1,
>[["Mass"]]/1E6*ppmrange,digits=digit)
>   highppm <- round(newframe[1,][["Mass"]]+newframe[1,
>[["Mass"]]/1E6*ppmrange,digits=digit)
>   presence <- seq(from=lowppm,to=highppm,by=10^(-digit))
>   # Walk through the entire original frame and check whether peaks are
>overlap-free ... do so until max of 2000 entries
>   for (int in seq(from=2,to=nrow(frame))) {
>     if(nrow(newframe) < 2000) {
>       lowppm <- round(frame[int,][["Mass"]]-frame[int,
>[["Mass"]]/1E6*ppmrange,digits=digit)
>       highppm <- round(frame[int,][["Mass"]]+frame[int,
>[["Mass"]]/1E6*ppmrange,digits=digit)
>       windowrange <- seq(from=lowppm,to=highppm,by=10^(-digit))
>       if (sum(round(windowrange,digits=digit) %in%
>round(presence,digits=digit)) < 1) {
>         newframe <- rbind(newframe,frame[int,])
>         presence <- c(presence,windowrange)
>       }
>     } else {
>       break()
>     }
>   }

Michael Dewey
http://www.aghmed.fsnet.co.uk


From horacio9573 at gmail.com  Wed Jul 18 18:03:02 2007
From: horacio9573 at gmail.com (Horacio Castellini)
Date: Wed, 18 Jul 2007 13:03:02 -0300
Subject: [R] Flow Cytometry Standard, fcs format in R.
In-Reply-To: <1184626920.32207.13.camel@R3-Thux>
References: <90e13c0707131206td4f430o3069da8e14b656dc@mail.gmail.com>
	<1184626920.32207.13.camel@R3-Thux>
Message-ID: <90e13c0707180903w73c41d86u937651cccd9d674e@mail.gmail.com>

2007/7/16, Bernardo Rangel Tura <tura en centroin.com.br>:
> On Fri, 2007-07-13 at 16:06 -0300, Horacio Castellini wrote:
> > Hi all.
> > How do I extract date from fcs format file with R. I.e I'd like
> > make statistical analysis using R-program, but I don't know if there
> > are R-packets for fcs format file, and using examples.
> >
> > Thanks.
>
> Hi Horacio!
>
> Is possible using rflowcyt or prada available in
> http://www.bioconductor.org
>
> In Rnews have article about this:
>
> http://cran.r-project.org/doc/Rnews/Rnews_2006-5.pdf
>
> Bernardo Rangel Tura, MD, Ph.D
> National Institute of Cardiology
> Rio de Janeiro - Brazil
>

Thanks, but the format file wich datas are saving, it isn't flow
cytometry standar.
Leica Microsystems TCS SP2-AOBS equipped with a SP2-FCS2 Leica
Microsystems workstation save its data in *.fcs file extention but I
don't know what format type is it?

>


From mppsrikanth at gmail.com  Wed Jul 18 18:31:36 2007
From: mppsrikanth at gmail.com (Srikanth)
Date: Wed, 18 Jul 2007 11:31:36 -0500
Subject: [R] Ideal number of clusters using the Fanny algorithm
Message-ID: <bf245c000707180931r68be296od3d1d18bccdce5f5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/41e8938a/attachment.pl 

From thomas.pujol at yahoo.com  Wed Jul 18 18:32:18 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Wed, 18 Jul 2007 09:32:18 -0700 (PDT)
Subject: [R] sqlSave, ...colnames=F,
	using "odbcConnectExcel" .... I still get colnames in top row of
	exprted sheet
Message-ID: <678143.37577.qm@web59311.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/70020cb7/attachment.pl 

From thomas.pujol at yahoo.com  Wed Jul 18 18:47:24 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Wed, 18 Jul 2007 09:47:24 -0700 (PDT)
Subject: [R] can I run/launch an excel VBA macro from wihin R?
In-Reply-To: <20070716084214.CGO82943@po-d.temple.edu>
Message-ID: <396420.45796.qm@web59311.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/24b0cbf9/attachment.pl 

From marc_schwartz at comcast.net  Wed Jul 18 18:50:35 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 18 Jul 2007 11:50:35 -0500
Subject: [R] [R-sig-DB] RODBC on Oracle DB
In-Reply-To: <p06230901c2c3d4428ce4@[128.115.153.6]>
References: <4556.1184657064@net2000.ch>
	<p06230901c2c3d4428ce4@[128.115.153.6]>
Message-ID: <1184777435.13375.7.camel@Bellerophon.localdomain>

I think that you are on to something there Don.

I just tried accessing a table from our Oracle server, which I do know
exists, but for which I do not have access permissions.

Using the following query in the Oracle Instant Client:

  select table_name from all_tables;

I can get a list of all tables on the server, which includes a table
called INCOMPATIBLE_USER_AGENTS, for which I do not have access
permissions.

When attempting to query the table in the Instant Client I get:

SQL> select * from INCOMPATIBLE_USER_AGENTS;
select * from INCOMPATIBLE_USER_AGENTS
              *
ERROR at line 1:
ORA-00942: table or view does not exist


When running the same query from R using RODBC I get:

> sqlQuery(db, "select * from INCOMPATIBLE_USER_AGENTS")
[1] "[RODBC] ERROR: Could not SQLExecDirect"                                
[2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: table or view does not exist\n"


So it looks like permission issues may be possible here.  Eric,
definitely confirm with your SysAdmins that you have appropriate
permissions.

HTH,

Marc


On Wed, 2007-07-18 at 07:43 -0700, Don MacQueen wrote:
> I believe I have seen that error message from 
> Oracle when I tried to query a table for which I 
> did not have "select" privileges (and when I knew 
> for certain that the table existed). Ask your 
> database administrator about the table, and make 
> sure that you do have that privilege.
> 
> What I am uncertain about is whether Oracle, when 
> asked to list tables, returns a list that 
> includes tables for which the user does not have 
> select privileges.
> 
> -Don
> 
> At 9:24 AM +0200 7/17/07, eric at net2000.ch wrote:
> >essai <- odbcConnect("ORESTE_prod", 
> >uid="osis_r",  pwd="12miss15" ,case="oracle")
> >
> >>  sqlTables(essai)$ORESTE
> >
> >...
> >
> >1315      <NA>      ORESTE              S_PROFESSIONS_OLD        TABLE    <NA>
> >1316      <NA>      ORESTE                  S_PROVENANCES        TABLE    <NA>
> >1317      <NA>      ORESTE                        S_SEXES        TABLE    <NA>
> >1318      <NA>      ORESTE                 S_SOUS_CLASSES        TABLE    <NA>
> >1319      <NA>      ORESTE                 S_TYP_COLLEGES        TABLE    <NA>
> >1320      <NA>      ORESTE             S_TYP_ENSEIGNEMENT        TABLE    <NA>
> >
> >...
> >
> >>  sqlQuery(essai, "select * from S_TYP_COLLEGES")
> >[1] "[RODBC] ERROR: Could not SQLExecDirect"                           
> >[2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
> >
> >I have also tried the
> >essai2 <- odbcDriverConnect(connection="essai2")
> >But with no succes.
> >
> >
> >
> >On Lun Juil 16 15:32 , Prof Brian Ripley <ripley at stats.ox.ac.uk> sent:
> >
> >>The problem could be quoting, if Oracle is not standards-compliant.
> >>See the options in ?odbcConnect.
> >>
> >>If sqlQuery(essai, "select * from S_TYP_COLLEGES") works, this is likely
> >>to be the problem.
> >>
> >>On Mon, 16 Jul 2007, eric at net2000.ch wrote:
> >>
> >>>
> >>>
> >>>>  essai
> >>>>  odbcGetInfo(essai)
> >>>        DBMS_Name         DBMS_Ver  Driver_ODBC_Ver
> >>>         "Oracle"     "09.00.0121"          "03.51"
> >>>  Data_Source_Name      Driver_Name       Driver_Ver
> >>>    "ORESTE_prod"    "SQORA32.DLL"     "09.00.0101"
> >>>         ODBC_Ver      Server_Name
> >>>     "03.52.0000"           "weba"
> >>>
> >>>
> >>>>  sqlTables(essai)
> >>>
> >>>  The result of this function is a liste of tables, one of them is called:
> >>>  S_TYP_COLLEGES.
> >>>
> >>>
> >>>>  sqlFetch(essai,"S_TYP_COLLEGES")
> >>>  [1] "[RODBC] ERROR: Could not SQLExecDirect"
> >>>  [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
> >>>
> >>>>  sqlFetch(essai, "S_TYP_COLLEGES", colnames=TRUE, rownames=FALSE)
> >>>  [1] "[RODBC] ERROR: Could not SQLExecDirect"
> >>>  [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
> >  >>
> >  >>
> >  >> What could be the problem here ?
> >  >> Any help is welcome
> >  >> Eric R?thlisberger, Neuch?tel
> >  >>


From tobischlot2002 at yahoo.com  Wed Jul 18 19:23:38 2007
From: tobischlot2002 at yahoo.com (Tobias Schlottmann)
Date: Wed, 18 Jul 2007 10:23:38 -0700 (PDT)
Subject: [R] Linear programming question
Message-ID: <135232.54180.qm@web45007.mail.sp1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/0bad89ce/attachment.pl 

From markbulk at gmail.com  Wed Jul 18 19:26:25 2007
From: markbulk at gmail.com (Mark Bulkeley)
Date: Wed, 18 Jul 2007 13:26:25 -0400
Subject: [R] R MySQL Configuration
Message-ID: <f0acec900707181026x394ee07bi2b121abe9c983c9@mail.gmail.com>

Quick question about the configuration files  relative to RMySQL (I've
tried to get feedback directly from the author David James, but his
email address was non-responsive):

Documentation at http://cran.r-project.org/doc/packages/RMySQL.pdf on
page 3 indicates that for windows machines the only place that the
my.cnf file will be looked for is the root (C:\) directory.  Is this
read correct?  When launching R, I've set the HOME directory
elsewhere, but am unsuccessful in getting recognition of my group
definitions in this way (only works when the file is in C:\ )

Does anybody know if there are plans to implement the default.file
option noted on page 17 of the same documentation (noted as
"Currently unused")?  If I set it now, I get "Error in
mysqlNewConnection(drv,  ...): unused argument(s) (default.file ...)"

My goal is to eliminate plain text database passwords from my computer
and at least put it on a secured network location,  but I need to let
RMySQL know how to find it.  Thanks for your  help.

Regards,
Mark


From tplate at acm.org  Wed Jul 18 19:32:55 2007
From: tplate at acm.org (Tony Plate)
Date: Wed, 18 Jul 2007 11:32:55 -0600
Subject: [R] poor rbind performance
In-Reply-To: <755261CA22782948B1C42ACDC83912A104614E3D@NYWEXMB27.msad.ms.com>
References: <755261CA22782948B1C42ACDC83912A104614E3D@NYWEXMB27.msad.ms.com>
Message-ID: <469E4EC7.6050600@acm.org>


As Jim points out, building up a data frame by rbinding in a loop can be 
a slow way to do things in R.

Here's an example of how you can easily read data frames into a list:

 > # Create 3 files
 > invisible(lapply(1:3, function(i) 
write.csv(file=paste("tmp",i,".csv",sep=""), 
data.frame(i=2*i+(1:2),c=letters[2*i+(1:2)]))))
 > # Read the files into a list of data frames
 > list.of.dfs <- lapply(paste("tmp",1:3,".csv",sep=""), read.csv, 
row.names=1)
 > # rbind the data frames
 > myData <- do.call("rbind", list.of.dfs)
 > myData
   i c
1 3 c
2 4 d
3 5 e
4 6 f
5 7 g
6 8 h
 >

(and of course, these last two expressions can be composed into a single 
expression if you want)

-- Tony Plate

Aydemir, Zava (FID) wrote:
> Hi
>  
> I rbind data frames in a loop in a cumulative way and the performance
> detriorates very quickly. 
>  
> My code looks like this:
>  
> for( k in 1:N)
> {
>     filename <- paste("/tmp/myData_",as.character(k),".txt",sep="")
>     myDataTmp <- read.table(filename,header=TRUE,sep=",")
>     if( k == 1) {
>         myData <- myDataTmp
>     }
>     else{
>         myData <- rbind(myData,myDataTmp)
>     }  
> }
>  
> Some more details:
> - the size of the stored text files is about 100,000 rows and 50 columns
> each
> - for k=1: rbind takes 0.0004 seconds
> - for k=2: rbind takes 13 seconds
> - for k=3: rbind takes 30 seconds
> - for k=4: rbind takes 36 seconds
> etc
>  
> Any suggestions to improve speed?
>  
> Thanks
>  
> Zava
> --------------------------------------------------------
> 
> This is not an offer (or solicitation of an offer) to buy/se...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From xkneifl at mendelu.cz  Wed Jul 18 19:36:56 2007
From: xkneifl at mendelu.cz (Ing. Michal Kneifl, Ph.D.)
Date: Wed, 18 Jul 2007 19:36:56 +0200
Subject: [R] Classification
Message-ID: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>

Hi,
I am also a quite new user of R and would like to ask you for help:
I have a data frame where all columns are numeric variables. My aim is  
to convert one columnt in factors.
Example:
MD
0.2
0.1
0.8
0.3
0.7
0.6
0.01
0.2
0.5
1
1


I want to make classes:
0-0.2 A
0.21-0.4 B
0.41-0.6 C
..... and so on

So after classification I wil get:
MD
A
A
D
B
.
.
.
and so on

Please could you give an advice to a newbie?
Thanks a lot in advance..

Michael


From amstat2006 at gmail.com  Wed Jul 18 19:45:04 2007
From: amstat2006 at gmail.com (Am Stat)
Date: Wed, 18 Jul 2007 13:45:04 -0400
Subject: [R] set up automatic running of R
Message-ID: <c8b63a350707181045s3afec59fw92ab8ea2cb900dc8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/59551172/attachment.pl 

From sheck at ucar.edu  Wed Jul 18 19:45:06 2007
From: sheck at ucar.edu (Sherri Heck)
Date: Wed, 18 Jul 2007 11:45:06 -0600 (MDT)
Subject: [R] nested for loop
Message-ID: <20143909.1184780706646.JavaMail.root@squirrel>

Hi, 

I am new to programming and R.  I am reading the manual and R books by Dalgaard and Veranzo to help answer my questions but I am unable to figure out the following:

I have a data file that contains 1080 data points. Here's a snippet of the file:

[241]  0.3603704000  0.1640741000  0.2912963000   NA  0.0159259300  0.0474074100            

I would like to break the file up into 30 consecutive data point segments and then write each segment into a separate data file.  This is one version of code that I've tried.  

mons = c(1:12)

data = scan(paste("C:/R/NWR.txt"))
for (mon in mons)  {
 for (i in c(1:30)) {
  for (j in data)    {

write((data),paste(mon,'NWR dc_dt_zi ppm meters per sec.txt',sep=''),ncol=1)   

} 
 }
  }

I think I'm really close, but no cigar.  Thanks in advance for any help-

S.Heck
Graduate Research Assistant
University of Colorado, Boulder


From HDoran at air.org  Wed Jul 18 19:50:48 2007
From: HDoran at air.org (Doran, Harold)
Date: Wed, 18 Jul 2007 13:50:48 -0400
Subject: [R] Classification
In-Reply-To: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EE57DC4@dc1ex01.air.org>

Michael

Assume your data frame is called "data" and your variable is called
"V1". Converting this to a factor is:

data$V1 <- factor(data$V1) 

Creating the classes can be done using ifelse(). Something like

data$class <- ifelse(data$V1 < .21, A, ifelse(data$V1 < .41, B, C))

Harold


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ing. 
> Michal Kneifl, Ph.D.
> Sent: Wednesday, July 18, 2007 1:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Classification
> 
> Hi,
> I am also a quite new user of R and would like to ask you for help:
> I have a data frame where all columns are numeric variables. 
> My aim is to convert one columnt in factors.
> Example:
> MD
> 0.2
> 0.1
> 0.8
> 0.3
> 0.7
> 0.6
> 0.01
> 0.2
> 0.5
> 1
> 1
> 
> 
> I want to make classes:
> 0-0.2 A
> 0.21-0.4 B
> 0.41-0.6 C
> ..... and so on
> 
> So after classification I wil get:
> MD
> A
> A
> D
> B
> .
> .
> .
> and so on
> 
> Please could you give an advice to a newbie?
> Thanks a lot in advance..
> 
> Michael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rvaradhan at jhmi.edu  Wed Jul 18 19:51:46 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 18 Jul 2007 13:51:46 -0400
Subject: [R] Linear programming question
In-Reply-To: <135232.54180.qm@web45007.mail.sp1.yahoo.com>
References: <135232.54180.qm@web45007.mail.sp1.yahoo.com>
Message-ID: <000501c7c964$4f2fbe30$7c94100a@win.ad.jhu.edu>

Tobias,

Adding the first constraints yields:
S1 + S2 >= -2A

Similarly adding the second set of constraints:
S3 + S4 <= 2B

If A and B are positive (which you didn't specify) then
The minimum of S1+S2 is -2A, and the maximum of S3+S4 is 2B.

Thus, the minimum of S1+S2-S3-S4 is -2(A+B).  

Ravi. 

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tobias Schlottmann
Sent: Wednesday, July 18, 2007 1:24 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Linear programming question

 Hi everybody,    
  consider please an optimization problem:  
   
  minimize   sum S1+S2
   
  Subject to :  y - x =< A + S1
                     x - y =< A + S2
   
  and we want to add two more constraints:
   
                   y - x =< B - S3
                   x - y =< B - S4
   
  where A is a small constant value and B is a large constant value, S1 and
S2 are surplus and S3 and S4 are slack variables.
   
  S3 and S4 have to be maximized in objective function. As objective
function, is this correct?  :
   
  minimize sum S1+ S2 - S3 -S4
   
  where actually we want to minimize S1 and S2; and maximize S3 and S4.
   
  If it is not correct, what to do ?
   
  Thank you for any guide.
   
  Tobias

 
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at yahoo.ca  Wed Jul 18 19:52:05 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 18 Jul 2007 13:52:05 -0400 (EDT)
Subject: [R] Classification
In-Reply-To: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>
Message-ID: <892742.26587.qm@web32806.mail.mud.yahoo.com>

Have a look at the recode function in the car package

library(car)
?recode
 should give you what you need.


--- "Ing. Michal Kneifl, Ph.D." <xkneifl at mendelu.cz>
wrote:

> Hi,
> I am also a quite new user of R and would like to
> ask you for help:
> I have a data frame where all columns are numeric
> variables. My aim is  
> to convert one columnt in factors.
> Example:
> MD
> 0.2
> 0.1
> 0.8
> 0.3
> 0.7
> 0.6
> 0.01
> 0.2
> 0.5
> 1
> 1
> 
> 
> I want to make classes:
> 0-0.2 A
> 0.21-0.4 B
> 0.41-0.6 C
> ..... and so on
> 
> So after classification I wil get:
> MD
> A
> A
> D
> B
> .
> .
> .
> and so on
> 
> Please could you give an advice to a newbie?
> Thanks a lot in advance..
> 
> Michael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From marc_schwartz at comcast.net  Wed Jul 18 19:53:11 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 18 Jul 2007 12:53:11 -0500
Subject: [R] Classification
In-Reply-To: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>
References: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>
Message-ID: <1184781191.13375.26.camel@Bellerophon.localdomain>

On Wed, 2007-07-18 at 19:36 +0200, Ing. Michal Kneifl, Ph.D. wrote:
> Hi,
> I am also a quite new user of R and would like to ask you for help:
> I have a data frame where all columns are numeric variables. My aim is  
> to convert one columnt in factors.
> Example:
> MD
> 0.2
> 0.1
> 0.8
> 0.3
> 0.7
> 0.6
> 0.01
> 0.2
> 0.5
> 1
> 1
> 
> 
> I want to make classes:
> 0-0.2 A
> 0.21-0.4 B
> 0.41-0.6 C
> ..... and so on
> 
> So after classification I wil get:
> MD
> A
> A
> D
> B
> .
> .
> .
> and so on
> 
> Please could you give an advice to a newbie?
> Thanks a lot in advance..
> 
> Michael

See ?cut

You can then do something like:

> DF
     MD
1  0.20
2  0.10
3  0.80
4  0.30
5  0.70
6  0.60
7  0.01
8  0.20
9  0.50
10 1.00
11 1.00


> cut(DF$MD, breaks = c(seq(0, 1, .2)), labels = LETTERS[1:5])
 [1] A A D B D C A A C E E
Levels: A B C D E


HTH,

Marc Schwartz


From bcarvalh at jhsph.edu  Wed Jul 18 19:59:11 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Wed, 18 Jul 2007 13:59:11 -0400
Subject: [R] Classification
In-Reply-To: <2323A6D37908A847A7C32F1E3662C80EE57DC4@dc1ex01.air.org>
References: <2323A6D37908A847A7C32F1E3662C80EE57DC4@dc1ex01.air.org>
Message-ID: <28C314AA-9A33-4CCE-8C1C-1C8D1B55C93A@jhsph.edu>

maybe:

x = c(.2, .1, .8, .3, .7, .6, .01, .2, .5, 1, 1)
breaks = seq(0, 1, .2)
LETTERS[1:(length(breaks)-1)][cut(x, breaks)]

b

On Jul 18, 2007, at 1:50 PM, Doran, Harold wrote:

> Michael
>
> Assume your data frame is called "data" and your variable is called
> "V1". Converting this to a factor is:
>
> data$V1 <- factor(data$V1)
>
> Creating the classes can be done using ifelse(). Something like
>
> data$class <- ifelse(data$V1 < .21, A, ifelse(data$V1 < .41, B, C))
>
> Harold
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ing.
>> Michal Kneifl, Ph.D.
>> Sent: Wednesday, July 18, 2007 1:37 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Classification
>>
>> Hi,
>> I am also a quite new user of R and would like to ask you for help:
>> I have a data frame where all columns are numeric variables.
>> My aim is to convert one columnt in factors.
>> Example:
>> MD
>> 0.2
>> 0.1
>> 0.8
>> 0.3
>> 0.7
>> 0.6
>> 0.01
>> 0.2
>> 0.5
>> 1
>> 1
>>
>>
>> I want to make classes:
>> 0-0.2 A
>> 0.21-0.4 B
>> 0.41-0.6 C
>> ..... and so on
>>
>> So after classification I wil get:
>> MD
>> A
>> A
>> D
>> B
>> .
>> .
>> .
>> and so on
>>
>> Please could you give an advice to a newbie?
>> Thanks a lot in advance..
>>
>> Michael
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Wed Jul 18 20:05:21 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 18 Jul 2007 14:05:21 -0400
Subject: [R] Classification
In-Reply-To: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>
References: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>
Message-ID: <644e1f320707181105v16e4431l321fb6793572b41a@mail.gmail.com>

You can use 'cut':

> x
     MD
1  0.20
2  0.10
3  0.80
4  0.30
5  0.70
6  0.60
7  0.01
8  0.20
9  0.50
10 1.00
11 1.00
> cut(x$MD, breaks=seq(0,1,.2), include.lowest=TRUE, labels=LETTERS[1:5])
 [1] A A D B D C A A C E E
Levels: A B C D E
>


On 7/18/07, Ing. Michal Kneifl, Ph.D. <xkneifl at mendelu.cz> wrote:
> Hi,
> I am also a quite new user of R and would like to ask you for help:
> I have a data frame where all columns are numeric variables. My aim is
> to convert one columnt in factors.
> Example:
> MD
> 0.2
> 0.1
> 0.8
> 0.3
> 0.7
> 0.6
> 0.01
> 0.2
> 0.5
> 1
> 1
>
>
> I want to make classes:
> 0-0.2 A
> 0.21-0.4 B
> 0.41-0.6 C
> ..... and so on
>
> So after classification I wil get:
> MD
> A
> A
> D
> B
> .
> .
> .
> and so on
>
> Please could you give an advice to a newbie?
> Thanks a lot in advance..
>
> Michael
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From J.delasHeras at ed.ac.uk  Wed Jul 18 20:07:20 2007
From: J.delasHeras at ed.ac.uk (J.delasHeras at ed.ac.uk)
Date: Wed, 18 Jul 2007 19:07:20 +0100
Subject: [R] Subsetting Enigma: More rows after dataframe[-list,]?
In-Reply-To: <f7kn6t$p5a$1@sea.gmane.org>
References: <f7kn6t$p5a$1@sea.gmane.org>
Message-ID: <20070718190720.59q4r282ck4o4gck@www.staffmail.ed.ac.uk>

Quoting Johannes Graumann <johannes_graumann at web.de>:

> Hello again,
>
> I'm trying to purge the indexes in i.delete from frame and end up with more
> rows!? Please be so kind and let me know where I screw this up ...
>
> Joh
>
>> i.delete
>   [1]   40   45  165  212  253  270  280  287  301  352  421  433  463  467
> 487
>  [16]  517  537  542  573  594  596  612  614  621  635  650  696  699  707
> 732
>  [31]  738  776  826  891  892  936  937  935  940  976  988  995 1037 1043
> 1059
>  [46] 1081 1111 1123 1128 1132 1140 1153 1155 1165 1176 1179 1200 1281 1289
> 1300
>  [61] 1320 1346 1356 1366 1369 1396 1406 1420 1428 1429 1471 1474 1475 1525
> 1540
>  [76] 1554 1565 1645 1667 1665 1706 1711 1724 1764 1788 1791 1805 1808 1847
> 1881
>  [91]   10   18  137  238  254  260  262  288  292  314  338  349  414  447
> 457
> [106]  465  470  478  511  530  536  552  582  588  644  655  687  693  701
> 724
> [121]  739  763  771  836  848  859  888  900  902  919  939  972  979  989
> 1000
> [136] 1002 1015 1020 1026 1029 1032 1055 1060 1073 1088 1104 1117 1124 1130
> 1135
> [151] 1144 1221 1225 1249 1251 1257 1376 1384 1386 1453 1487 1529 1532 1534
> 1605
> [166] 1624 1633 1646 1648 1702 1787 1948 1951
>
>> length(i.delete)
> [1] 173
>
>> nrow(frame)
> [1] 1975
>
>> nrow(frame[-i.delete,])
> [1] 1802

Hmmm... i.delete is a vector with 173 elements (indices).

frame has originally 1975 rows, then you remove 173 (i.delete) and you  
end up with a new frame with 1802 rows (1975-173).

It works as expected... you get LESS rows afterwards, not more.

(?)

:-)

Jose

-- 
Dr. Jose I. de las Heras                      Email: J.delasHeras at ed.ac.uk
The Wellcome Trust Centre for Cell Biology    Phone: +44 (0)131 6513374
Institute for Cell & Molecular Biology        Fax:   +44 (0)131 6507360
Swann Building, Mayfield Road
University of Edinburgh
Edinburgh EH9 3JR
UK


From rvaradhan at jhmi.edu  Wed Jul 18 20:11:58 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Wed, 18 Jul 2007 14:11:58 -0400
Subject: [R] Linear programming question
In-Reply-To: <000501c7c964$4f2fbe30$7c94100a@win.ad.jhu.edu>
References: <135232.54180.qm@web45007.mail.sp1.yahoo.com>
	<000501c7c964$4f2fbe30$7c94100a@win.ad.jhu.edu>
Message-ID: <000901c7c967$22476ff0$7c94100a@win.ad.jhu.edu>

Tobias,

Just a clarification/correction to my solution: it makes no difference
whether A and B are positive or negative.  The minimum of S1+S2-S3-S4 is
always -2(A+B).

Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ravi Varadhan
Sent: Wednesday, July 18, 2007 1:52 PM
To: 'Tobias Schlottmann'; r-help at stat.math.ethz.ch
Subject: Re: [R] Linear programming question

Tobias,

Adding the first constraints yields:
S1 + S2 >= -2A

Similarly adding the second set of constraints:
S3 + S4 <= 2B

If A and B are positive (which you didn't specify) then
The minimum of S1+S2 is -2A, and the maximum of S3+S4 is 2B.

Thus, the minimum of S1+S2-S3-S4 is -2(A+B).  

Ravi. 

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tobias Schlottmann
Sent: Wednesday, July 18, 2007 1:24 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Linear programming question

 Hi everybody,    
  consider please an optimization problem:  
   
  minimize   sum S1+S2
   
  Subject to :  y - x =< A + S1
                     x - y =< A + S2
   
  and we want to add two more constraints:
   
                   y - x =< B - S3
                   x - y =< B - S4
   
  where A is a small constant value and B is a large constant value, S1 and
S2 are surplus and S3 and S4 are slack variables.
   
  S3 and S4 have to be maximized in objective function. As objective
function, is this correct?  :
   
  minimize sum S1+ S2 - S3 -S4
   
  where actually we want to minimize S1 and S2; and maximize S3 and S4.
   
  If it is not correct, what to do ?
   
  Thank you for any guide.
   
  Tobias

 
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at comcast.net  Wed Jul 18 20:12:37 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Wed, 18 Jul 2007 13:12:37 -0500
Subject: [R] Classification
In-Reply-To: <1184781191.13375.26.camel@Bellerophon.localdomain>
References: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>
	<1184781191.13375.26.camel@Bellerophon.localdomain>
Message-ID: <1184782357.13375.35.camel@Bellerophon.localdomain>

On Wed, 2007-07-18 at 12:53 -0500, Marc Schwartz wrote:
> On Wed, 2007-07-18 at 19:36 +0200, Ing. Michal Kneifl, Ph.D. wrote:
> > Hi,
> > I am also a quite new user of R and would like to ask you for help:
> > I have a data frame where all columns are numeric variables. My aim is  
> > to convert one columnt in factors.
> > Example:
> > MD
> > 0.2
> > 0.1
> > 0.8
> > 0.3
> > 0.7
> > 0.6
> > 0.01
> > 0.2
> > 0.5
> > 1
> > 1
> > 
> > 
> > I want to make classes:
> > 0-0.2 A
> > 0.21-0.4 B
> > 0.41-0.6 C
> > ..... and so on
> > 
> > So after classification I wil get:
> > MD
> > A
> > A
> > D
> > B
> > .
> > .
> > .
> > and so on
> > 
> > Please could you give an advice to a newbie?
> > Thanks a lot in advance..
> > 
> > Michael
> 
> See ?cut
> 
> You can then do something like:
> 
> > DF
>      MD
> 1  0.20
> 2  0.10
> 3  0.80
> 4  0.30
> 5  0.70
> 6  0.60
> 7  0.01
> 8  0.20
> 9  0.50
> 10 1.00
> 11 1.00
> 
> 
> > cut(DF$MD, breaks = c(seq(0, 1, .2)), labels = LETTERS[1:5])
>  [1] A A D B D C A A C E E
> Levels: A B C D E

For precision, let's clean that up as I just realized that I left the
remnants of c() in there from an alternative solution, which is not
needed here:

  cut(DF$MD, breaks = seq(0, 1, .2), labels = LETTERS[1:5])

Marc


From drcarbon at gmail.com  Wed Jul 18 20:14:10 2007
From: drcarbon at gmail.com (Dr Carbon)
Date: Wed, 18 Jul 2007 11:14:10 -0700
Subject: [R] Spline - frequency response (again)
Message-ID: <e89bb7ac0707181114y3fd66eb1g788801ef20a51489@mail.gmail.com>

Is it really possible that nobody can help me with this? Is r-help too
overwhelmed now? Any help appreciated - Jon

On 7/16/07, Dr Carbon <drcarbon at gmail.com> wrote:
> Please preemptively excuse my ignorance.
>
> I'm trying to fit a cubic smoothing spline to a time series according to a method encountered in a paper. The authors state that they fit a spline whose frequency response is 50% at a wavelength of n years where n is 67% of the length of the time series. Is it possible to fit a spline like this in R using the spar parameter in  smooth.spline? Or is there another spline function in R that works with frequency response?
>
> The time series I need to fit is similar to this:
> ts.sim <- arima.sim(list(ar=c(0.5873,0.0873,0.1332,0.0746,-0.0794, 0.0953,0.0313,-0.1393,0.0401,0.2226,0.0024,-0.1030)), n = 350, sd = 0.02)
>
> I believe the spline I want to fit will look not unlike this (could be wrong):
> ts.plot(ts.sim)
> lines(smooth.spline(ts.sim,spar=0.75)$y,col='red',lwd=2)
>
> Can anybody help with this implementation.
>
> TIA, JC
>
>
>


From J.delasHeras at ed.ac.uk  Wed Jul 18 20:14:34 2007
From: J.delasHeras at ed.ac.uk (J.delasHeras at ed.ac.uk)
Date: Wed, 18 Jul 2007 19:14:34 +0100
Subject: [R] hist() Frequancy values
In-Reply-To: <200707181234.42910.amicogodzilla@bruttocarattere.org>
References: <200707181234.42910.amicogodzilla@bruttocarattere.org>
Message-ID: <20070718191434.32rssay1s08wcs8g@www.staffmail.ed.ac.uk>

Quoting Manuele Pesenti <amicogodzilla at bruttocarattere.org>:

> I have seen that the hist() function plots an histogram of the   
> frequency but I
> cannot find the value of the object hist that contains theese values... how
> is possible to get out them?
>
> thank you very mutch
> best regards
>
> 	Manuele

if you add 'plot=FALSE', like this:

hist(rnorm(100),plot=FALSE)

instead of plotting the histogram, you get a list of values. The  
component $counts
contains what you're looking for.

You can give it a name: a<-hist(rnorm(100),plot=FALSE), and treat it  
like any other list.
(if you do "a<-hist(rnorm(100))" then you get both the values and the plot)

check ?hist for details.

Jose

-- 
Dr. Jose I. de las Heras                      Email: J.delasHeras at ed.ac.uk
The Wellcome Trust Centre for Cell Biology    Phone: +44 (0)131 6513374
Institute for Cell & Molecular Biology        Fax:   +44 (0)131 6507360
Swann Building, Mayfield Road
University of Edinburgh
Edinburgh EH9 3JR
UK


From davidr at rhotrading.com  Wed Jul 18 20:19:26 2007
From: davidr at rhotrading.com (davidr at rhotrading.com)
Date: Wed, 18 Jul 2007 13:19:26 -0500
Subject: [R] set up automatic running of R
In-Reply-To: <c8b63a350707181045s3afec59fw92ab8ea2cb900dc8@mail.gmail.com>
References: <c8b63a350707181045s3afec59fw92ab8ea2cb900dc8@mail.gmail.com>
Message-ID: <F9F2A641C593D7408925574C05A7BE773F30ED@rhopost.rhotrading.com>

Just create a batch file, say myproc.bat with the line
C:/pathtoR/bin/R CMD BATCH myscript.R

and use the Windows task scheduler to schedule your job.
(Drag your batch file into C:/WINDOWS/Tasks, change its name if you
like, and right click to Properties to schedule it and set other
attributes.)

HTH,

David L. Reiner
Rho Trading Securities, LLC
550 W. Jackson Blvd #1000
Chicago, IL 60661-5704
 
312-244-4610 direct
312-244-4500 main
312-244-4501 fax
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Am Stat
Sent: Wednesday, July 18, 2007 12:45 PM
To: r-help at stat.math.ethz.ch
Subject: [R] set up automatic running of R

Hi useR,

I am trying to find how to schedule an automatic run of R periodically,
I
have written some scripts to extract data which are updated monthly on
another server, my os is xp. The goal is that my script will run at a
scheduled time every month and record the results to some directories.

Now the scripts are done, only thing I need is to know how to let R run
my
scripts at a certain time, say the first Sunday of each months.

Could anyone give me some clues?

Thanks a million in advance!


Best,

Leon

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Wed Jul 18 20:19:41 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 18 Jul 2007 14:19:41 -0400
Subject: [R] nested for loop
In-Reply-To: <20143909.1184780706646.JavaMail.root@squirrel>
References: <20143909.1184780706646.JavaMail.root@squirrel>
Message-ID: <644e1f320707181119q2f85b5cdu1b6c9b306ef7946b@mail.gmail.com>

This should create your files for you:

x <- 1:1080  # test data
# create a vector of 30 consecutive values for spliting the data
breaks <- rep(1:ceiling(length(x) / 30), each=30)[1:length(x)]
# now partition the data into 30 values and write them
fileNo <- 1  # initialize the file number
invisible(lapply(split(x, breaks), function(.values){
    write(.values, file=sprintf("NWRxx.%03d.txt", fileNo))
    fileNo <<- fileNo + 1   # update the file number
}))


On 7/18/07, Sherri Heck <sheck at ucar.edu> wrote:
> Hi,
>
> I am new to programming and R.  I am reading the manual and R books by Dalgaard and Veranzo to help answer my questions but I am unable to figure out the following:
>
> I have a data file that contains 1080 data points. Here's a snippet of the file:
>
> [241]  0.3603704000  0.1640741000  0.2912963000   NA  0.0159259300  0.0474074100
>
> I would like to break the file up into 30 consecutive data point segments and then write each segment into a separate data file.  This is one version of code that I've tried.
>
> mons = c(1:12)
>
> data = scan(paste("C:/R/NWR.txt"))
> for (mon in mons)  {
>  for (i in c(1:30)) {
>  for (j in data)    {
>
> write((data),paste(mon,'NWR dc_dt_zi ppm meters per sec.txt',sep=''),ncol=1)
>
> }
>  }
>  }
>
> I think I'm really close, but no cigar.  Thanks in advance for any help-
>
> S.Heck
> Graduate Research Assistant
> University of Colorado, Boulder
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From asb at mail.nih.gov  Wed Jul 18 20:23:10 2007
From: asb at mail.nih.gov (Alan S Barnett)
Date: Wed, 18 Jul 2007 14:23:10 -0400
Subject: [R] lattice plot axis scaling
Message-ID: <1184782990.11514.19.camel@papagena>

I want to generate a lattice plot of a multiple linear regression.  I'm
using the code:

xyplot(y ~ x1 + x2 | status, data=datam,
        xlab="Peak separation",ylab="G/W",main="G/W vs Fuzzy peak
separation: Threshold=1.8",
        groups=Fuzzy.gw.t.score>1.8,
	subset=(status %in%  c("control","patient","sibling")),
	panel=function(x,y,groups,subscripts,...){
	  panel.xyplot(x,y,groups=groups,subscripts=subscripts,...)
          panel.abline(tmp<<-lm(y~x),col = "light blue",lwd=2)
          panel.abline(tmp<<-rlm(y~x),col = "blue",lwd=2)
	  good.id<-groups[subscripts]
          fm <- rlm(y[good.id] ~ x[good.id])
          sm <- summary(fm)
          panel.abline(reg = fm)
          panel.abline(tmp1<<-lm(y[good.id]~x[good.id]),col =
"pink",lwd=2)
          panel.abline(tmp1<<-rlm(y[good.id]~x[good.id]),col =
"red",lwd=2)
          slope <- round(coef(fm)[2], 3)
          err <- round(sm$coefficients[4], 3)
          message("Click on desired location")
          panel.text(pos<<-grid.locator("native"), lab = paste("slope
=",slope,"+-",err),col="red")
          })
_______________________________________________________________
The problem is that x1 varies from 1-3, while x2 varies from 20-60.  The
output scales both independent variables the same, so all the data in
the y vs x1 plot are up against the left edge of the plot.  How do I
scale the x-axes separately?


From jholtman at gmail.com  Wed Jul 18 20:29:10 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 18 Jul 2007 14:29:10 -0400
Subject: [R] set up automatic running of R
In-Reply-To: <c8b63a350707181045s3afec59fw92ab8ea2cb900dc8@mail.gmail.com>
References: <c8b63a350707181045s3afec59fw92ab8ea2cb900dc8@mail.gmail.com>
Message-ID: <644e1f320707181129r2e7a004u50846d8b87644c1b@mail.gmail.com>

Create a .bat file with the commands to execute R BATCH and then
create a scheduled task that will run at the desired time to call the
batch file.

On 7/18/07, Am Stat <amstat2006 at gmail.com> wrote:
> Hi useR,
>
> I am trying to find how to schedule an automatic run of R periodically, I
> have written some scripts to extract data which are updated monthly on
> another server, my os is xp. The goal is that my script will run at a
> scheduled time every month and record the results to some directories.
>
> Now the scripts are done, only thing I need is to know how to let R run my
> scripts at a certain time, say the first Sunday of each months.
>
> Could anyone give me some clues?
>
> Thanks a million in advance!
>
>
> Best,
>
> Leon
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From lawremi at iastate.edu  Wed Jul 18 20:36:11 2007
From: lawremi at iastate.edu (Michael Lawrence)
Date: Wed, 18 Jul 2007 13:36:11 -0500
Subject: [R] How to open an URL using RGtk2
In-Reply-To: <d4327f7e0707180533g79e6a7besb71d71dc78c8ee34@mail.gmail.com>
References: <d4327f7e0707180533g79e6a7besb71d71dc78c8ee34@mail.gmail.com>
Message-ID: <509e0620707181136r668d7014hda55e0a517b2a521@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/5ee6d788/attachment.pl 

From rtorgovi at hsph.harvard.edu  Wed Jul 18 21:12:14 2007
From: rtorgovi at hsph.harvard.edu (Roman Torgovitsky)
Date: Wed, 18 Jul 2007 15:12:14 -0400
Subject: [R] derivative estimation using GAM
Message-ID: <469E2DCE0200004F00014A68@hsph.harvard.edu>

Hello,
I have a question about Simon Wood's gam function.

Suppose I have a simple model

n<-100
x <- runif(n, 0, 1);
y<-x*x
data<-data.frame(x,y)
ct<-gam(y~ s(x,k = 5))

how could I estimate first derivative of the smooth. From what I understand,  i could get coefficients of the fit from

ct2<-gam(y~ s(x,k = 5),control =gam.control(absorb.cons = FALSE))
coeff(ct2)

to get unconstrained coefficient estimates for the model (this was suggested in response to a similar question in 2005), but how could I get derivative of the basis?

Thanks!
Roman Torgovitsky
Doctoral Student
Department of Biostatistics
Harvard University


From drstrong at ucdavis.edu  Wed Jul 18 21:14:47 2007
From: drstrong at ucdavis.edu (Mr Natural)
Date: Wed, 18 Jul 2007 12:14:47 -0700 (PDT)
Subject: [R] dates() is a great date function in R
Message-ID: <11675205.post@talk.nabble.com>


Proper calendar dates in R are great for plotting and calculating. 
However for the non-wonks among us, they can be very frustrating.
I have recently discussed the pains that people in my lab have had 
with dates in R. Especially the frustration of bringing date data into R 
from Excel, which we have to do a lot. 

Please find below a simple analgesic for R date importation that I
discovered 
over the last 1.5 days (Learning new stuff in R is calculated in 1/2 days).

The function    dates()    gives the simplest way to get calendar dates into
R from Excel that I can find.
But straight importation of Excel dates, via a csv or txt file, can be a a
huge pain (I'll give details for anyone who cares to know). 

My pain killer is:
Consider that you have Excel columns in month, day, year format. Note that R
hates date data that does not lead with the year. 

a. Load the chron library by typing   library(chron)   in the console.
You know that you need this library from information revealed by 
performing the query,
?dates()"    in the Console window. This gives the R documentation 
help file for this and related time, date functions.  In the upper left 
of the documentation, one sees "dates(chron)". This tells you that you
need the library chron. 

b. Change the format "dates" in Excel to format "general", which gives 
5 digit Julian dates. Import the csv file (I use    read.csv()  with the 
Julian dates and other data of interest.

c.  Now, change the Julian dates that came in with the csv file into 
calendar dates with the    dates() function. Below is my code for performing 
this activity, concerning an R data file called ss,

ss holds the Julian dates, illustrated below from the column MPdate,

>ss$MPdate[1:5]
[1] 34252 34425 34547 34759 34773

The dates() function makes calendar dates from Julian dates,

>dmp<-dates(ss$MPdate,origin=c(month = 1, day = 1, year = 1900))

> dmp[1:5]
[1] 10/12/93 04/03/94 08/03/94 03/03/95 03/17/95

I would appreciate the comments of more sophisticated programmers who
can suggest streamlining or shortcutting this operation.

regards, Don



 
-- 
View this message in context: http://www.nabble.com/dates%28%29-is-a-great-date-function-in-R-tf4105322.html#a11675205
Sent from the R help mailing list archive at Nabble.com.


From seow_ainee at hotmail.com  Wed Jul 18 17:08:50 2007
From: seow_ainee at hotmail.com (rach.s)
Date: Wed, 18 Jul 2007 08:08:50 -0700 (PDT)
Subject: [R] maximum likelihood estimation
Message-ID: <11670424.post@talk.nabble.com>


Hello!

I need to perform maximum likelihood estimation on R, but I am not sure
which command to use. I searched on google, and found an example using the
function mlogl, but I couldn't find the package on R. Is there such
function? Or how should i perform my mle?

Thank you very much. 
-- 
View this message in context: http://www.nabble.com/maximum-likelihood-estimation-tf4103791.html#a11670424
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Wed Jul 18 21:43:53 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 18 Jul 2007 15:43:53 -0400
Subject: [R] dates() is a great date function in R
In-Reply-To: <11675205.post@talk.nabble.com>
References: <11675205.post@talk.nabble.com>
Message-ID: <971536df0707181243y1345c474qdb0eefefd542834@mail.gmail.com>

See the Other Applications section of the R News 4/1 help desk
article on dates.

On 7/18/07, Mr Natural <drstrong at ucdavis.edu> wrote:
>
> Proper calendar dates in R are great for plotting and calculating.
> However for the non-wonks among us, they can be very frustrating.
> I have recently discussed the pains that people in my lab have had
> with dates in R. Especially the frustration of bringing date data into R
> from Excel, which we have to do a lot.
>
> Please find below a simple analgesic for R date importation that I
> discovered
> over the last 1.5 days (Learning new stuff in R is calculated in 1/2 days).
>
> The function    dates()    gives the simplest way to get calendar dates into
> R from Excel that I can find.
> But straight importation of Excel dates, via a csv or txt file, can be a a
> huge pain (I'll give details for anyone who cares to know).
>
> My pain killer is:
> Consider that you have Excel columns in month, day, year format. Note that R
> hates date data that does not lead with the year.
>
> a. Load the chron library by typing   library(chron)   in the console.
> You know that you need this library from information revealed by
> performing the query,
> ?dates()"    in the Console window. This gives the R documentation
> help file for this and related time, date functions.  In the upper left
> of the documentation, one sees "dates(chron)". This tells you that you
> need the library chron.
>
> b. Change the format "dates" in Excel to format "general", which gives
> 5 digit Julian dates. Import the csv file (I use    read.csv()  with the
> Julian dates and other data of interest.
>
> c.  Now, change the Julian dates that came in with the csv file into
> calendar dates with the    dates() function. Below is my code for performing
> this activity, concerning an R data file called ss,
>
> ss holds the Julian dates, illustrated below from the column MPdate,
>
> >ss$MPdate[1:5]
> [1] 34252 34425 34547 34759 34773
>
> The dates() function makes calendar dates from Julian dates,
>
> >dmp<-dates(ss$MPdate,origin=c(month = 1, day = 1, year = 1900))
>
> > dmp[1:5]
> [1] 10/12/93 04/03/94 08/03/94 03/03/95 03/17/95
>
> I would appreciate the comments of more sophisticated programmers who
> can suggest streamlining or shortcutting this operation.
>
> regards, Don
>
>
>
>
> --
> View this message in context: http://www.nabble.com/dates%28%29-is-a-great-date-function-in-R-tf4105322.html#a11675205
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jmburgos at u.washington.edu  Wed Jul 18 21:41:15 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Wed, 18 Jul 2007 12:41:15 -0700
Subject: [R] gamm (package mgcv) with large datasets
In-Reply-To: <509e0620707181136r668d7014hda55e0a517b2a521@mail.gmail.com>
References: <d4327f7e0707180533g79e6a7besb71d71dc78c8ee34@mail.gmail.com>
	<509e0620707181136r668d7014hda55e0a517b2a521@mail.gmail.com>
Message-ID: <469E6CDB.3020205@u.washington.edu>

Dear list,

I am interested in fitting a Generalized Additive Mixed Model with 
spatially correlated errors to a large, spatially indexed, data set 
(~4000 observations).

My initial analysis was a Generalized Additive Model that included a two 
dimensional smooth term to model spatially correlated effect (i.e. 
s(latitude,longitude)).  The problem is that the residuals of this model 
are still spatially correlated, so it seems that I should use a GAMM in 
which the spatial autocorrelation is modeled explicitly.

The problem is that, as stated in the documentation of the mgcv package, 
my dataset is too large for the gamm function.  Is anybody aware of an 
alternative approach to analyze this data?


 
Julian M. Burgos

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science
University of Washington

1122 NE Boat Street
Seattle, WA  98105 

Phone: 206-221-6864


From mark at wardle.org  Wed Jul 18 21:50:30 2007
From: mark at wardle.org (Mark Wardle)
Date: Wed, 18 Jul 2007 20:50:30 +0100
Subject: [R] Neuman-Keuls
In-Reply-To: <280559.10345.qm@web27510.mail.ukl.yahoo.com>
References: <280559.10345.qm@web27510.mail.ukl.yahoo.com>
Message-ID: <b59a37130707181250q4ec20d96i9056101e7d4a90b0@mail.gmail.com>

Hi Elyakhlifi,

I'm not a statistics expert and so can't intuit what your code is
doing easily. I also can't just run the code as supplied, as it uses
variables and data structures you haven't specified.

>From a coding perspective, there are several issues though that will
almost certainly make a difference.

You need to read about SCOPE.

Try isolating functions, and strictly define inputs and outputs. Your
function magically uses variables E, exple, lst, lst1, E, and others.
Don't do this! Try to make your functions a little more generic and
restrict using global variables!

try defining your function  (and I would use a better name too, but
that is personal preference!) to

NK <- function(x, E, exple) {
   lst<- list()
   lst1<-list()
   lst2<-list()

  ...

  and then return(lst)
}

USE the result of the function by doing this:

returned.lst <- NK(whatever... )

and not expecting NK to magically set a variable in the global scope.

It is possible to set variables in the parent scope from a function,
but I would suggest you don't even look up how to do this!

Best wishes,

Mark



On 18/07/07, elyakhlifi mustapha <elyakhlifi_mustapha at yahoo.fr> wrote:
> hello,
> I have programmed this function to calculate the Neuman-Keuls test but I have a problem the function return an empty list and I don't know why.
>
> summary(fm1)
> E <- sqrt((summary(fm1)[[1]]["Residuals","Mean Sq"])/length(LR))
> lst <- list()
> lst1 <- list()
> lst2 <- list()
> NK <- function (x) {
>  if (length(x) == 2) {
>   Tstudent  <- t.test(subset(exple, groupe == names(x)[1])$vd,subset(exple, groupe == names(x)[2])$vd)
>   t <- as.numeric(Tstudent$statistic)
>   if (t >= Tstudent$conf.int[1:2][1] & t <= Tstudent$conf.int[1:2][2]) {
>    lst1[[1]] <- x
>    lst <- c(lst,lst1)
>   } else {
>    lst1[[1]] <- x[1]
>    lst2[[1]] <- x[2]
>    lst <- c(lst,lst1)
>    lst <- c(lst,lst2)
>   }
>  } else {
>   test <- x[c(1,length(x))]
>   Q <- (as.numeric(test[2]) - as.numeric(test[1]))/E
>   qtt <- qtukey(0.95,length(x),df=41)
>   if (Q < qtt) {
>    lst1[[1]] <- range(x)
>    lst <- c(lst,lst1)
>   } else {
>    x1 <- x[-length(x)]
>    x2 <- x[-1]
>    NK(x1)
>    NK(x2)
>   }
>  }
>  return(lst)
> }
>   with
>
> > fm1
> Call:
>    aov(formula = vd ~ groupe, data = exple)
> Terms:
>                   groupe Residuals
> Sum of Squares  300.9871   27.0000
> Deg. of Freedom       20        41
> Residual standard error: 0.8115027
> Estimated effects may be unbalanced
>
>
> Can you help me please?
> thanks.
>
>
>       _____________________________________________________________________________
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> ______________________________________________________________________
> This email has been scanned by the MessageLabs Email Security System.
> For more information please visit http://www.messagelabs.com/email
> ______________________________________________________________________
>


-- 
Dr. Mark Wardle
Clinical research fellow and specialist registrar, Neurology
Cardiff, UK


From deepayan.sarkar at gmail.com  Wed Jul 18 21:53:26 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 18 Jul 2007 12:53:26 -0700
Subject: [R] lattice plot axis scaling
In-Reply-To: <1184782990.11514.19.camel@papagena>
References: <1184782990.11514.19.camel@papagena>
Message-ID: <eb555e660707181253y2bd862d4n4ec829d457fca16c@mail.gmail.com>

On 7/18/07, Alan S Barnett <asb at mail.nih.gov> wrote:
> I want to generate a lattice plot of a multiple linear regression.  I'm
> using the code:
>
> xyplot(y ~ x1 + x2 | status, data=datam,
>         xlab="Peak separation",ylab="G/W",main="G/W vs Fuzzy peak
> separation: Threshold=1.8",
>         groups=Fuzzy.gw.t.score>1.8,
>         subset=(status %in%  c("control","patient","sibling")),
>         panel=function(x,y,groups,subscripts,...){
>           panel.xyplot(x,y,groups=groups,subscripts=subscripts,...)
>           panel.abline(tmp<<-lm(y~x),col = "light blue",lwd=2)
>           panel.abline(tmp<<-rlm(y~x),col = "blue",lwd=2)
>           good.id<-groups[subscripts]
>           fm <- rlm(y[good.id] ~ x[good.id])
>           sm <- summary(fm)
>           panel.abline(reg = fm)
>           panel.abline(tmp1<<-lm(y[good.id]~x[good.id]),col =
> "pink",lwd=2)
>           panel.abline(tmp1<<-rlm(y[good.id]~x[good.id]),col =
> "red",lwd=2)
>           slope <- round(coef(fm)[2], 3)
>           err <- round(sm$coefficients[4], 3)
>           message("Click on desired location")
>           panel.text(pos<<-grid.locator("native"), lab = paste("slope
> =",slope,"+-",err),col="red")
>           })
> _______________________________________________________________
> The problem is that x1 varies from 1-3, while x2 varies from 20-60.  The
> output scales both independent variables the same, so all the data in
> the y vs x1 plot are up against the left edge of the plot.  How do I
> scale the x-axes separately?

Add

scales = list(x = "free")

to your call.

-Deepayan


From csardi at rmki.kfki.hu  Wed Jul 18 22:00:07 2007
From: csardi at rmki.kfki.hu (Gabor Csardi)
Date: Wed, 18 Jul 2007 22:00:07 +0200
Subject: [R] maximum likelihood estimation
In-Reply-To: <11670424.post@talk.nabble.com>
References: <11670424.post@talk.nabble.com>
Message-ID: <20070718200007.GA5734@localdomain>

On Wed, Jul 18, 2007 at 08:08:50AM -0700, rach.s wrote:
> 
> Hello!
> 
> I need to perform maximum likelihood estimation on R, but I am not sure
> which command to use. I searched on google, and found an example using the
> function mlogl, but I couldn't find the package on R. Is there such
> function? Or how should i perform my mle?
                                       ^^^   :)

library(stats4)
?mle

G.

[...]

-- 
Csardi Gabor <csardi at rmki.kfki.hu>    MTA RMKI, ELTE TTK


From Zava.Aydemir at morganstanley.com  Wed Jul 18 22:00:17 2007
From: Zava.Aydemir at morganstanley.com (Aydemir, Zava (FID))
Date: Wed, 18 Jul 2007 16:00:17 -0400
Subject: [R] passing a parameter to a file from command line
Message-ID: <755261CA22782948B1C42ACDC83912A104614F73@NYWEXMB27.msad.ms.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/8bf3da94/attachment.pl 

From gavin.simpson at ucl.ac.uk  Wed Jul 18 22:13:49 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 18 Jul 2007 21:13:49 +0100
Subject: [R] dates() is a great date function in R
In-Reply-To: <11675205.post@talk.nabble.com>
References: <11675205.post@talk.nabble.com>
Message-ID: <1184789629.2879.14.camel@graptoleberis.geog.ucl.ac.uk>

On Wed, 2007-07-18 at 12:14 -0700, Mr Natural wrote: 
> Proper calendar dates in R are great for plotting and calculating. 
> However for the non-wonks among us, they can be very frustrating.
> I have recently discussed the pains that people in my lab have had 
> with dates in R. Especially the frustration of bringing date data into R 
> from Excel, which we have to do a lot. 

I've always found the following reasonably intuitive:

Given the csv file that I've pasted in below, the following reads the
csv file in, formats the dates and class Date and then draws a plot.

I have dates in DD/MM/YYYY format so year is not first - thus attesting
to R not hating dates in this format ;-)

## read in csv data
## as.is = TRUE stops characters being converted to factors
## thus saving us an extra step to convert them back
dat <- read.csv("date_data.csv", as.is = TRUE)

## we convert to class Date
## format tells R how the dates are formatted in our character strings
## see ?strftime for the meaning and available codes
dat$Date <- as.Date(dat$Date, format = "%d/%m/%Y")

## check this worked ok
str(dat$Date)
dat$Date

## see nicely formatted dates and not a drop of R-related hatred 
## but just about the most boring graph I could come up with
plot(Data ~ Date, dat, type = "l")

And you can keep your Excel file formatted as dates as well - bonus!

Oh, and before you get "Martin'd", it is the chron *package*!

HTH

G

CSV file I used, generated in OpenOffice.org, but I presume it stores
Dates in the same way as Excel?:

"Data","Date"
1,01/01/2007
2,02/01/2007
3,03/01/2007
4,04/01/2007
5,05/01/2007
6,06/01/2007
7,07/01/2007
8,08/01/2007
9,09/01/2007
10,10/01/2007
11,11/01/2007
10,12/01/2007
9,13/01/2007
8,14/01/2007
7,15/01/2007
6,16/01/2007
5,17/01/2007
4,18/01/2007
3,19/01/2007
2,20/01/2007
1,21/01/2007
1,22/01/2007
2,23/01/2007
3,24/01/2007

> Please find below a simple analgesic for R date importation that I
> discovered 
> over the last 1.5 days (Learning new stuff in R is calculated in 1/2 days).
> 
> The function    dates()    gives the simplest way to get calendar dates into
> R from Excel that I can find.
> But straight importation of Excel dates, via a csv or txt file, can be a a
> huge pain (I'll give details for anyone who cares to know). 
> 
> My pain killer is:
> Consider that you have Excel columns in month, day, year format. Note that R
> hates date data that does not lead with the year. 
> 
> a. Load the chron library by typing   library(chron)   in the console.
> You know that you need this library from information revealed by 
> performing the query,
> ?dates()"    in the Console window. This gives the R documentation 
> help file for this and related time, date functions.  In the upper left 
> of the documentation, one sees "dates(chron)". This tells you that you
> need the library chron. 
> 
> b. Change the format "dates" in Excel to format "general", which gives 
> 5 digit Julian dates. Import the csv file (I use    read.csv()  with the 
> Julian dates and other data of interest.
> 
> c.  Now, change the Julian dates that came in with the csv file into 
> calendar dates with the    dates() function. Below is my code for performing 
> this activity, concerning an R data file called ss,
> 
> ss holds the Julian dates, illustrated below from the column MPdate,
> 
> >ss$MPdate[1:5]
> [1] 34252 34425 34547 34759 34773
> 
> The dates() function makes calendar dates from Julian dates,
> 
> >dmp<-dates(ss$MPdate,origin=c(month = 1, day = 1, year = 1900))
> 
> > dmp[1:5]
> [1] 10/12/93 04/03/94 08/03/94 03/03/95 03/17/95
> 
> I would appreciate the comments of more sophisticated programmers who
> can suggest streamlining or shortcutting this operation.
> 
> regards, Don
> 
> 
> 
>  
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From Greg.Snow at intermountainmail.org  Wed Jul 18 22:46:39 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Wed, 18 Jul 2007 14:46:39 -0600
Subject: [R] passing a parameter to a file from command line
In-Reply-To: <755261CA22782948B1C42ACDC83912A104614F73@NYWEXMB27.msad.ms.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF227B@LP-EXCHVS07.CO.IHC.COM>

Look at the commandArgs function to see if that does what you want.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Aydemir, Zava (FID)
> Sent: Wednesday, July 18, 2007 2:00 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] passing a parameter to a file from command line
> 
> Hi,
>  
> I have a file fileFoo.R, say that contains these two lines, 
> invoking function foo that is specified in "foo_details.R":
>  
> source("foo_details.R")
> foo(parameter1)
>  
>  
> I want to specify and pass parameter1 in my command line when 
> invoking R
> in linux:   R --no-save <fileFoo.R.
>  
> How can I do that? 
> And how can I retrieve the value of parameter1 in my 
> fileFoo.R function (something analogous to perl: my 
> $parameter1=$ARGV[0])?
>  
> Thank you
>  
> Zava
> --------------------------------------------------------
> 
> This is not an offer (or solicitation of an offer) to 
> buy/se...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rab at nauticom.net  Wed Jul 18 23:02:37 2007
From: rab at nauticom.net (Rick Bilonick)
Date: Wed, 18 Jul 2007 17:02:37 -0400
Subject: [R] odfWeave - How to Insert eps rather than png
Message-ID: <1184792558.3499.18.camel@localhost.localdomain>

If you create plots and use odfWeave to create an odf text document, the
default for figures is png bitmap graphics. The only way I've found to
insert eps graphics is to first create the eps graphic using the
postscript driver and then use odfInsertPlot. I've tried to use
getImageDefs and setImageDefs but I get an empty plot. Could someone
show an example?

Rick B.


From Achim.Zeileis at wu-wien.ac.at  Wed Jul 18 23:16:31 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 18 Jul 2007 23:16:31 +0200 (CEST)
Subject: [R] dates() is a great date function in R
In-Reply-To: <1184789629.2879.14.camel@graptoleberis.geog.ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0707182309510.7607-100000@disco.wu-wien.ac.at>

...just a follow up to reading time series data from CSV files. If you've
got data like Gavin's (only with the dates in the first column)

  Date,Data
  01/01/2007,1
  02/01/2007,2
  03/01/2007,3
  04/01/2007,4
  ...

then you can use read.zoo() in package "zoo":
  x <- read.zoo("mydata.csv", sep = ",", format = "%d/%m/%Y", header = TRUE)
  plot(x)
which produces the time-series plot.

This uses the "Date" class contained in base R rather than "dates" from
chron. Concerning the different time/date classes, see the R News article
Gabor already mentioned. For some more examples of using zoo/read.zoo see
  vignette("zoo-quickref", package = "zoo")

hth,
Z


On Wed, 18 Jul 2007, Gavin Simpson wrote:

> On Wed, 2007-07-18 at 12:14 -0700, Mr Natural wrote:
> > Proper calendar dates in R are great for plotting and calculating.
> > However for the non-wonks among us, they can be very frustrating.
> > I have recently discussed the pains that people in my lab have had
> > with dates in R. Especially the frustration of bringing date data into R
> > from Excel, which we have to do a lot.
>
> I've always found the following reasonably intuitive:
>
> Given the csv file that I've pasted in below, the following reads the
> csv file in, formats the dates and class Date and then draws a plot.
>
> I have dates in DD/MM/YYYY format so year is not first - thus attesting
> to R not hating dates in this format ;-)
>
> ## read in csv data
> ## as.is = TRUE stops characters being converted to factors
> ## thus saving us an extra step to convert them back
> dat <- read.csv("date_data.csv", as.is = TRUE)
>
> ## we convert to class Date
> ## format tells R how the dates are formatted in our character strings
> ## see ?strftime for the meaning and available codes
> dat$Date <- as.Date(dat$Date, format = "%d/%m/%Y")
>
> ## check this worked ok
> str(dat$Date)
> dat$Date
>
> ## see nicely formatted dates and not a drop of R-related hatred
> ## but just about the most boring graph I could come up with
> plot(Data ~ Date, dat, type = "l")
>
> And you can keep your Excel file formatted as dates as well - bonus!
>
> Oh, and before you get "Martin'd", it is the chron *package*!
>
> HTH
>
> G
>
> CSV file I used, generated in OpenOffice.org, but I presume it stores
> Dates in the same way as Excel?:
>
> "Data","Date"
> 1,01/01/2007
> 2,02/01/2007
> 3,03/01/2007
> 4,04/01/2007
> 5,05/01/2007
> 6,06/01/2007
> 7,07/01/2007
> 8,08/01/2007
> 9,09/01/2007
> 10,10/01/2007
> 11,11/01/2007
> 10,12/01/2007
> 9,13/01/2007
> 8,14/01/2007
> 7,15/01/2007
> 6,16/01/2007
> 5,17/01/2007
> 4,18/01/2007
> 3,19/01/2007
> 2,20/01/2007
> 1,21/01/2007
> 1,22/01/2007
> 2,23/01/2007
> 3,24/01/2007
>
> > Please find below a simple analgesic for R date importation that I
> > discovered
> > over the last 1.5 days (Learning new stuff in R is calculated in 1/2 days).
> >
> > The function    dates()    gives the simplest way to get calendar dates into
> > R from Excel that I can find.
> > But straight importation of Excel dates, via a csv or txt file, can be a a
> > huge pain (I'll give details for anyone who cares to know).
> >
> > My pain killer is:
> > Consider that you have Excel columns in month, day, year format. Note that R
> > hates date data that does not lead with the year.
> >
> > a. Load the chron library by typing   library(chron)   in the console.
> > You know that you need this library from information revealed by
> > performing the query,
> > ?dates()"    in the Console window. This gives the R documentation
> > help file for this and related time, date functions.  In the upper left
> > of the documentation, one sees "dates(chron)". This tells you that you
> > need the library chron.
> >
> > b. Change the format "dates" in Excel to format "general", which gives
> > 5 digit Julian dates. Import the csv file (I use    read.csv()  with the
> > Julian dates and other data of interest.
> >
> > c.  Now, change the Julian dates that came in with the csv file into
> > calendar dates with the    dates() function. Below is my code for performing
> > this activity, concerning an R data file called ss,
> >
> > ss holds the Julian dates, illustrated below from the column MPdate,
> >
> > >ss$MPdate[1:5]
> > [1] 34252 34425 34547 34759 34773
> >
> > The dates() function makes calendar dates from Julian dates,
> >
> > >dmp<-dates(ss$MPdate,origin=c(month = 1, day = 1, year = 1900))
> >
> > > dmp[1:5]
> > [1] 10/12/93 04/03/94 08/03/94 03/03/95 03/17/95
> >
> > I would appreciate the comments of more sophisticated programmers who
> > can suggest streamlining or shortcutting this operation.
> >
> > regards, Don
> >
> >
> >
> >
> --
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679 0522
>  ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
>  UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From dan.oshea at dnr.state.mn.us  Wed Jul 18 23:19:24 2007
From: dan.oshea at dnr.state.mn.us (Daniel O'Shea)
Date: Wed, 18 Jul 2007 16:19:24 -0500
Subject: [R] multicollinearity in nlme models
Message-ID: <469E3D88.537F.005A.0@dnr.state.mn.us>

I am working on a nlme model that has multiple fixed effects (linear and nonlinear) with a nonlinear (asymptotic) random effect.

asymporig<-function(x,th1,th2)th1*(1-exp(-exp(th2)*x))
asymporigb<-function(x,th1b,th2b)th1b*(1-exp(-exp(th2b)*x))

mod.vol.nlme<-nlme(fa20~(ah*habdiv+ads*ds+ads2*ds2+at*trout)+asymporig(da.p,th1,th2)+
    asymporigb(vol,th1b,th2b),
    fixed=ah+ads+ads2+at+th1+th2+th1b+th2b~1,
    random=pdBlocked(list(th1~1,th2~1)),
    start=c(ah=.5524,ads=.8,ads2=-.1,at=-1,th1=2.542,th2=-7.117,th1b=2,th2b=-7),
    data=pca1.grouped,verbose=T)

I am looking at potential multicollinearity among the fixed effects, in particular I am concerned about multicollinearity between da.p (drainage area) and vol (volume).  How do I interpret the correlation reported in the summary command for th1 and th1b, which are the asymptotes for fa20~da.p and fa20~vol.  It is -.50, but how is the correlation calculated?  

I have run the above model with out vol and the coefficients for the remaining variables are very similar (within the approx. 95% conf. interv.) to the coefficients in the above model and vol and da.p are significant, both suggesting multicollinearity is not severe??  I am interested in which variables influence fa20 (richness) not necessarily the model with the smallest residual sd.

I do have Pinheiro and Bates, but do not find much reference to this type of information.  Thanks for any suggestions or help.

Dan


####summary
#################
Nonlinear mixed-effects model fit by maximum likelihood
  Model: fa20 ~ (ah * habdiv + ads * ds + ads2 * ds2 + at * trout) + asymporig(da.p,      th1, th2) + asymporigb(vol, th1b, th2b) 
 Data: pca1.grouped 
       AIC      BIC    logLik
  3151.665 3248.518 -1555.832

Random effects:
 Composite Structure: Blocked

 Block 1: th1
 Formula: th1 ~ 1 | bas
              th1
StdDev: 0.8125094

 Block 2: th2
 Formula: th2 ~ 1 | bas
              th2 Residual
StdDev: 0.9468531 1.028757

Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | bas 
 Parameter estimates:
       LS        CD        MS        DM        RN        LM        UM        RD 
1.0000000 0.7884995 1.2107482 1.4159803 1.0463657 1.3982966 1.2195945 1.1978807 
       MN        SC 
1.3858409 1.2006228 
Fixed effects: ah + ads + ads2 + at + th1 + th2 + th1b + th2b ~ 1 
         Value Std.Error  DF    t-value p-value
ah    0.597032 0.1330044 920   4.488812       0
ads   1.283297 0.0874561 920  14.673614       0
ads2 -0.125186 0.0130289 920  -9.608281       0
at   -0.731506 0.1394553 920  -5.245451       0
th1   2.363269 0.3385592 920   6.980373       0
th2  -3.910520 0.3575392 920 -10.937318       0
th1b  1.402536 0.2188125 920   6.409764       0
th2b -6.765038 0.2931669 920 -23.075723       0
 Correlation: 
     ah     ads    ads2   at     th1    th2    th1b  
ads  -0.595                                          
ads2  0.571 -0.974                                   
at   -0.092 -0.104  0.104                            
th1   0.010 -0.153  0.147 -0.020                     
th2  -0.012 -0.139  0.105 -0.015 -0.071              
th1b  0.043 -0.110  0.070  0.084 -0.500  0.163       
th2b -0.038 -0.032 -0.030 -0.016 -0.017 -0.225 -0.056

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.73841391 -0.63008005  0.03189713  0.68903314  3.90583424 

Number of Observations: 937
Number of Groups: 10


From lzhtom at hotmail.com  Wed Jul 18 23:37:14 2007
From: lzhtom at hotmail.com (zhihua li)
Date: Wed, 18 Jul 2007 21:37:14 +0000
Subject: [R] memory error with 64-bit R in linux
Message-ID: <BAY110-F276541B2F797F47D6C741DC7FA0@phx.gbl>

Hi netters,

I'm using the 64-bit R-2.5.0 on a x86-64 cpu, with an RAM of 2 GB.  The 
operating system is SUSE 10.
The system information is:  
-uname -a
Linux someone 2.6.13-15.15-smp #1 SMP Mon Feb 26 14:11:33 UTC 2007 x86_64 
x86_64 x86_64 GNU/Linux

I used heatmap to process a matrix of the dim [16000,100].  After 3 hours 
of desperating waiting, R told me:
cannot allocate vector of size 896 MB.

I know the matrix is very big, but since I have 2 GB of RAM and in a 64-bit 
system, there should be no problem to deal with a vector smaller than 1 GB? 
(I was not running any other applications in my system)

Does anyone know what's going on?  Is there a hardware limit where I have 
to add more RAM, or is there some way to resolve it softwarely? Also is it 
possible to speed up the computing (I don't wanna wait another 3 hours to 
know I get another error message)

Thank you in advance!

_________________________________________________________________
?????????????????????????????? MSN Hotmail??  http://www.hotmail.com


From mazatlanmexico at yahoo.com  Wed Jul 18 23:53:06 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Wed, 18 Jul 2007 14:53:06 -0700 (PDT)
Subject: [R] Saving a dataset permanently in R
Message-ID: <703578.9892.qm@web56613.mail.re3.yahoo.com>

HI:
I'm still struggling with datasets, the more I read
about it the more confussed I get. This is the
scenario... In R console|Edit|Data Editor, I can find
all the datasets available with the different
packages, So to create a new dataset in the R console
I use the following commands to create an empty data
frame.
My_Dataset <- data.frame()
My_Dataset <- edit(My_dataset)

The problem is that I can't copy my data into the
dataframe. Is there any suggestions as of how I can
transfer the data and how it can be saved so everytime
I open R the dataset would be available.?
Thanks

 Felipe D. Carrillo
  Fishery Biologist
  US Fish & Wildlife Service
  Red Bluff, California 96080


From michael.drescher at ontario.ca  Thu Jul 19 00:31:11 2007
From: michael.drescher at ontario.ca (Drescher, Michael (MNR))
Date: Wed, 18 Jul 2007 18:31:11 -0400
Subject: [R] confidence intervals for multinomial
In-Reply-To: <76D2AA307C39054DBA8BD42DE44E71A403140A16@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
References: <76D2AA307C39054DBA8BD42DE44E71A403140A15@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
	<76D2AA307C39054DBA8BD42DE44E71A403140A16@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>
Message-ID: <76D2AA307C39054DBA8BD42DE44E71A403140A31@CTSPITDCEMMVX14.cihs.ad.gov.on.ca>

Hi All,

I want to test an H0 hypothesis about the proportions of observed counts
in k classes. I know that I can do this with the chisq.test.

However, besides of the overall acceptance or rejection of the H0, I
would like to know which of the k classes cause(s) rejection and I would
like to know the observation-based confidence envelopes for the
proportions for the k classes.

My quick-and-dirty approach thus far is to do an initial chisq.test on
the original k classes and then to lump data into two classes (=one of
the original classes and all other original classes lumped into one new
class) and do a binom.test. I interpret the result of the binom.test as
indicating whether the current class might be the reason for the
rejection of the overall H0. Additionally, it gives me a confidence
envelope for this class.

This approach seems fairly straightforward, but I just do not feel
totally comfortable with it. I would feel so much better if there was
something like a multinom.test, but to my knowledge there is none.

Do you have any suggestions what I could rather do? For instance, I
might follow a Monte Carlo-like approach:
I simulate proportions for the k classes based on the proportions of
observed counts with rmultinom. After exclusion of the most extreme
values I construct my confidence envelope based on the remaining
simulated proportions. Based on whether the hypothesized proportions
fall into the observation-based confidence envelopes, I accept or
reject.

Do you think that either of these approaches is better or would you
suggest doing something totally different?

All comments and suggestions are highly appreciated.

Kind regards, Michael

PS: I guess my request parallels that of Matthias Schmidt from Apr 5,
2004, that was answered by Brian Ripley ...


Michael Drescher
Ontario Forest Research Institute
Ontario Ministry of Natural Resources
1235 Queen St East
Sault Ste Marie, ON, P6A 2E3
Tel: (705) 946-7406
Fax: (705) 946-2030


From Tanja.Srebotnjak at Yale.edu  Thu Jul 19 00:42:31 2007
From: Tanja.Srebotnjak at Yale.edu (Tanja Srebotnjak)
Date: Wed, 18 Jul 2007 18:42:31 -0400
Subject: [R] creating a world map of eco-climatic zones
Message-ID: <20070718184231.5bmfbdtfggs08sk4@www.mail.yale.edu>

Hello R users:

I would like to produce a world map with countries colored according to whether
they fall into one of 7 eco-climatic zones. For simplicity, each country is
allocated to exactly 1 eco-climatic zone. For this purpose I have looked at the
map and mapdata packages, which contain world maps composed of polygons (1 for
each country, it seems). Each of the polygons can be referred to by a region
name in the map package.

I am thinking, I would do something like

map(region=c('Germany', 'Switzerland',...), fill=TRUE, col='blue')

for each of the 7 eco-climatic zones.

My question is, what are the names used in the world map dataset for each
country and how can I get them?

Perhaps there's also a smarter way to produce this map, in which case, your
advise is greatly appreciated as well.

Lastly, some countries (mostly large ones such as USA, Russia, Canada, China)
fall into multiple eco-climatic zones. Assume I could get geographical
coordinates for polygons describing the sub-country portion falling into a
single eco-climatic zone, is there a way to improve the above map to show
portions of countries belonging to a specific eco-climatic zone?

Thanks!
Tanja


From efg at stowers-institute.org  Thu Jul 19 00:48:40 2007
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Wed, 18 Jul 2007 17:48:40 -0500
Subject: [R] Can any one help me on format file data.
References: <90e13c0707180751y7d32d633m2930731c6f16db24@mail.gmail.com>
Message-ID: <f7m5cb$4oo$1@sea.gmane.org>


"Horacio Castellini" <horacio9573 at gmail.com> wrote in message 
news:90e13c0707180751y7d32d633m2930731c6f16db24 at mail.gmail.com...
> Hi all.
>           I'd like know what is the format file saved by  Leica
> Microsystems TCS SP2-AOBS equipped with a SP2-FCS2 Leica Microsystems
> workstation its datas. Cause it save in *.fcs extention  file but
> ins't flow cytometry standart format file...

>From a Google search for "SP2-FCS2 Leica Microsystems" I found this paper 
http://jcs.biologists.org/cgi/reprint/118/24/5825.pdf that talked about 
Fluorescence Correlation Spectroscopy (FCS) data.  Is is possible you have 
Fluorescence Correlation Spectroscopy (FCS) data instead of flow cytometry 
(FCS) data?  I've worked some with both types of FCS data.

I wrote a ConfoCor 3 Fluroescence Correlation Spectroscopy FCS viewer in R:
http://research.stowers-institute.org/efg/ScientificSoftware/Utility/FCSViewer/R.htm. 
The software that reads that older Fluroescence Correlation Spectroscopy FCS 
data was in Delphi 
http://research.stowers-institute.org/efg/ScientificSoftware/Utility/FCSViewer/index.htm. 
It would be a bit of a pain to read that older Fluroescence Correlation 
Spectroscopy FCS bit stream in R.   But it'slikely Leica's format is 
different than Zeiss.

I have also worked with some versions of flow cytometery FCS data (e.g., see 
http://research.stowers-institute.org/efg/ScientificSoftware/Utility/FCSExtract/index.htm). 
There are different versions of that FCS standard too.

If you send me a small file, I'll see if I can recognize if it's a format 
I've seen.

efg

Earl F. Glynn
Scientific Programmer
Stowers Institute for Medical Research


From jholtman at gmail.com  Thu Jul 19 00:50:31 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 18 Jul 2007 17:50:31 -0500
Subject: [R] memory error with 64-bit R in linux
In-Reply-To: <BAY110-F276541B2F797F47D6C741DC7FA0@phx.gbl>
References: <BAY110-F276541B2F797F47D6C741DC7FA0@phx.gbl>
Message-ID: <644e1f320707181550y55b8f508gb58994e787d8adf2@mail.gmail.com>

Are you paging?  That might explain the long run times. How much space
are your other objects taking up?  The matrix by itself should only
require about 13MB if it is numeric.  I would guess it is some of the
other objects that you have in your working space.  Put some gc() in
your loop to see how much space is being used.  Run it with a subset
of the data and see how long it takes.  This might give you an
estimate of the time, and space, that might be needed for the entire
dataset.

Do a 'ps' to see how much memory your process is using.  Do one every
couple of minutes to see if it is growing.  You can alway use Rprof()
to get an idea of where time is being spent (use it on a small
subset).

On 7/18/07, zhihua li <lzhtom at hotmail.com> wrote:
> Hi netters,
>
> I'm using the 64-bit R-2.5.0 on a x86-64 cpu, with an RAM of 2 GB.  The
> operating system is SUSE 10.
> The system information is:
> -uname -a
> Linux someone 2.6.13-15.15-smp #1 SMP Mon Feb 26 14:11:33 UTC 2007 x86_64
> x86_64 x86_64 GNU/Linux
>
> I used heatmap to process a matrix of the dim [16000,100].  After 3 hours
> of desperating waiting, R told me:
> cannot allocate vector of size 896 MB.
>
> I know the matrix is very big, but since I have 2 GB of RAM and in a 64-bit
> system, there should be no problem to deal with a vector smaller than 1 GB?
> (I was not running any other applications in my system)
>
> Does anyone know what's going on?  Is there a hardware limit where I have
> to add more RAM, or is there some way to resolve it softwarely? Also is it
> possible to speed up the computing (I don't wanna wait another 3 hours to
> know I get another error message)
>
> Thank you in advance!
>
> _________________________________________________________________
> ?????????????????????????????? MSN Hotmail??  http://www.hotmail.com
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From deepayan.sarkar at gmail.com  Thu Jul 19 01:02:46 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Wed, 18 Jul 2007 16:02:46 -0700
Subject: [R] creating a world map of eco-climatic zones
In-Reply-To: <20070718184231.5bmfbdtfggs08sk4@www.mail.yale.edu>
References: <20070718184231.5bmfbdtfggs08sk4@www.mail.yale.edu>
Message-ID: <eb555e660707181602y7d928421o1adb80a08523018f@mail.gmail.com>

On 7/18/07, Tanja Srebotnjak <Tanja.Srebotnjak at yale.edu> wrote:
> Hello R users:
>
> I would like to produce a world map with countries colored according to whether
> they fall into one of 7 eco-climatic zones. For simplicity, each country is
> allocated to exactly 1 eco-climatic zone. For this purpose I have looked at the
> map and mapdata packages, which contain world maps composed of polygons (1 for
> each country, it seems). Each of the polygons can be referred to by a region
> name in the map package.
>
> I am thinking, I would do something like
>
> map(region=c('Germany', 'Switzerland',...), fill=TRUE, col='blue')
>
> for each of the 7 eco-climatic zones.
>
> My question is, what are the names used in the world map dataset for each
> country and how can I get them?

> library(maps)
> wmap <- map('world', plot = FALSE, fill = TRUE)
> str(wmap)
List of 4
 $ x    : num [1:27636] -133 -132 -132 -132 -130 ...
 $ y    : num [1:27636] 58.4 57.2 57.0 56.7 56.1 ...
 $ range: num [1:4] -180.0  190.3  -85.4   83.6
 $ names: chr [1:2284] "Canada" "South Africa" "Denmark" "Great
Lakes:Superior, Huron, Michigan" ...
 - attr(*, "class")= chr "map"

So wmap$names will give you the names you want.

> Perhaps there's also a smarter way to produce this map, in which case, your
> advise is greatly appreciated as well.

If you construct a suitable color vector (with the same order as
$names), you should be able to do the whole thing in one call.

> Lastly, some countries (mostly large ones such as USA, Russia, Canada, China)
> fall into multiple eco-climatic zones. Assume I could get geographical
> coordinates for polygons describing the sub-country portion falling into a
> single eco-climatic zone, is there a way to improve the above map to show
> portions of countries belonging to a specific eco-climatic zone?

wmap$x and wmap$y are exactly that (coordinates of polygons, with NA's
separating polygons). You should be able to add a few more, add
suitable names, and supply the result as the 'database' argument to
another call to map().

-Deepayan


From jholtman at gmail.com  Thu Jul 19 01:08:09 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 18 Jul 2007 18:08:09 -0500
Subject: [R] Saving a dataset permanently in R
In-Reply-To: <703578.9892.qm@web56613.mail.re3.yahoo.com>
References: <703578.9892.qm@web56613.mail.re3.yahoo.com>
Message-ID: <644e1f320707181608p4ce51f9qb80c930c455fc996@mail.gmail.com>

Where are you trying to copy data from?  I would assume that with that
script you are typing all the data in by hand.  Why don't you put it
in a text file and use read.table?  By default, R will save your
workspace on exit and then reload it on startup.  Is this enough to
save your data?  You can also use the 'save' function to store
explicit objects.

On 7/18/07, Felipe Carrillo <mazatlanmexico at yahoo.com> wrote:
> HI:
> I'm still struggling with datasets, the more I read
> about it the more confussed I get. This is the
> scenario... In R console|Edit|Data Editor, I can find
> all the datasets available with the different
> packages, So to create a new dataset in the R console
> I use the following commands to create an empty data
> frame.
> My_Dataset <- data.frame()
> My_Dataset <- edit(My_dataset)
>
> The problem is that I can't copy my data into the
> dataframe. Is there any suggestions as of how I can
> transfer the data and how it can be saved so everytime
> I open R the dataset would be available.?
> Thanks
>
>  Felipe D. Carrillo
>  Fishery Biologist
>  US Fish & Wildlife Service
>  Red Bluff, California 96080
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From darena at stanford.edu  Thu Jul 19 01:20:53 2007
From: darena at stanford.edu (Dylan Arena)
Date: Wed, 18 Jul 2007 16:20:53 -0700
Subject: [R] Is there a facility in R similar to MatLab "syms" that allows
 using unevaluated numeric symbols in matrices?
Message-ID: <469EA055.40608@stanford.edu>

Hi,


I'm trying to use R to get eigenvalues and eigenvectors of a matrix 
whose elements are of the form (2 * lambda), -(lambda + mu), etc.  I'd 
like R to treat this matrix as a numeric matrix without treating lambda 
and mu as variable names but rather as some sort of atomic quantities 
(and hence give eigenvectors in terms of mu and/or lambda).  MatLab and 
Mathematica both do this, but I'm not sure whether R does.  Does anyone 
have any ideas about how to do this?


Please let me know,
Dylan


From suzanne.j.matthews at gmail.com  Thu Jul 19 02:07:19 2007
From: suzanne.j.matthews at gmail.com (Suzanne Matthews)
Date: Wed, 18 Jul 2007 20:07:19 -0400
Subject: [R] help with heatmap - how to remove annoying "X" before numeric
	values?
Message-ID: <f0fd79140707181707kdf9890fi28147c86dea701ad@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/6fc5dad6/attachment.pl 

From mjfeldman at wsu.edu  Thu Jul 19 02:09:28 2007
From: mjfeldman at wsu.edu (Feldman, Maximilian Jeffrey)
Date: Wed, 18 Jul 2007 17:09:28 -0700
Subject: [R] RAM, swap, Error: cannot allocate vector of size, Linux:
Message-ID: <C69D0659F0206346A12EF75423B90BD012F047@EXCHANGEVS-03.ad.wsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070718/a0ad9fe1/attachment.pl 

From job75 at business.kaist.ac.kr  Thu Jul 19 02:41:31 2007
From: job75 at business.kaist.ac.kr (=?ks_c_5601-1987?B?sejIoyhIbyBLaW0p?=)
Date: Thu, 19 Jul 2007 09:41:31 +0900
Subject: [R] Estimating mixed logit using Maximum simulated likelihood
Message-ID: <67A063C3AE560C42A929FA2DAAF68FC3B1CB10@kgsm.kaistgsm.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/721dc742/attachment.pl 

From jsumner at it.usyd.edu.au  Thu Jul 19 02:55:56 2007
From: jsumner at it.usyd.edu.au (Jeremy Sumner)
Date: Thu, 19 Jul 2007 10:55:56 +1000
Subject: [R] a type of generalized inner product
Message-ID: <f66e589e0707181755r408632aaja3a09a0463b9d9b1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/1a139870/attachment.pl 

From wgavioli at fas.harvard.edu  Thu Jul 19 03:26:58 2007
From: wgavioli at fas.harvard.edu (Wayne Aldo Gavioli)
Date: Wed, 18 Jul 2007 21:26:58 -0400
Subject: [R] 2 Biplot Questions
Message-ID: <1184808418.469ebde2c259b@webmail.fas.harvard.edu>


I have 2 questions about the Biplot function:

Both of the questions refer to the following type of graph, which is a biplot of
a principal component analysis:



biplot(prcomp(dataset))



1. Does anyone know how to change the appearance of data points on the biplot? 
As it is currently, for this type of graph, the label for each data point shows
up on the graph - does anyone know how to get rid of those labels, say, and just
make each piece of data represented by a point, the old-fashioned way?  (Also, I
want to be able to keep the labels for the red arrows on the graph, just not the
data points - any suggestions?  Is this even possible?)

2. What is the effect of scaling the biplot - for example, if I did:

biplot(prcomp(USArrests),  scale = TRUE)


I get a different graph than if I enter the command above.  I looked in the help
pages, and I see that it says that when you scale it, "the observations are
scaled by lamda ^ (1-scale) where lambda are the singular values as computed by
princomp".  Can anyone explain what that means in layman's terms?  Why would you
want to scale the observations by lamda - what does that accomplish?  And what
is being scaled anyway: the principal components or the underlying data points?


If anyone could help, that would be great.


From adschai at optonline.net  Thu Jul 19 03:49:33 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Thu, 19 Jul 2007 01:49:33 +0000 (GMT)
Subject: [R] Any implementation of multiobjective optimization using
 evolutionary approach?
Message-ID: <e29bc2d631812.469ec32d@optonline.net>

Hi

I'm quite new to this area a bit but I'm wondering if there is any implementation of multi-objective optimization using evolutionary approach available in R? Any point to reference would be really appreciated. Thank you.

- adschai


From jmacdon at med.umich.edu  Thu Jul 19 03:51:01 2007
From: jmacdon at med.umich.edu (James MacDonald)
Date: Wed, 18 Jul 2007 21:51:01 -0400
Subject: [R] memory error with 64-bit R in linux
In-Reply-To: <644e1f320707181550y55b8f508gb58994e787d8adf2@mail.gmail.com>
References: <BAY110-F276541B2F797F47D6C741DC7FA0@phx.gbl>
	<644e1f320707181550y55b8f508gb58994e787d8adf2@mail.gmail.com>
Message-ID: <469EC385.7060205@med.umich.edu>

The dist object for the rows of the matrix will be 16000x16000, which if 
there are any copies will easily suck up all of your RAM.

A more pertinent question is what use would a heatmap of that size be? 
How do you plan to visualize 16000 rows? In a pdf? You certainly 
couldn't publish such a thing, nor would it be useful as a picture in a 
presentation.

You would probably be better off filtering down to a more reasonable 
number of rows (say 500 or less), and using that to make your heatmap.

Best,

Jim

jim holtman wrote:
> Are you paging?  That might explain the long run times. How much space
> are your other objects taking up?  The matrix by itself should only
> require about 13MB if it is numeric.  I would guess it is some of the
> other objects that you have in your working space.  Put some gc() in
> your loop to see how much space is being used.  Run it with a subset
> of the data and see how long it takes.  This might give you an
> estimate of the time, and space, that might be needed for the entire
> dataset.
> 
> Do a 'ps' to see how much memory your process is using.  Do one every
> couple of minutes to see if it is growing.  You can alway use Rprof()
> to get an idea of where time is being spent (use it on a small
> subset).
> 
> On 7/18/07, zhihua li <lzhtom at hotmail.com> wrote:
>> Hi netters,
>>
>> I'm using the 64-bit R-2.5.0 on a x86-64 cpu, with an RAM of 2 GB.  The
>> operating system is SUSE 10.
>> The system information is:
>> -uname -a
>> Linux someone 2.6.13-15.15-smp #1 SMP Mon Feb 26 14:11:33 UTC 2007 x86_64
>> x86_64 x86_64 GNU/Linux
>>
>> I used heatmap to process a matrix of the dim [16000,100].  After 3 hours
>> of desperating waiting, R told me:
>> cannot allocate vector of size 896 MB.
>>
>> I know the matrix is very big, but since I have 2 GB of RAM and in a 
>> 64-bit
>> system, there should be no problem to deal with a vector smaller than 
>> 1 GB?
>> (I was not running any other applications in my system)
>>
>> Does anyone know what's going on?  Is there a hardware limit where I have
>> to add more RAM, or is there some way to resolve it softwarely? Also 
>> is it
>> possible to speed up the computing (I don't wanna wait another 3 hours to
>> know I get another error message)
>>
>> Thank you in advance!
>>
>> _________________________________________________________________
>> ??????????????? MSN Hotmail?  http://www.hotmail.com
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
James W. MacDonald, MS
Biostatistician
UMCCC cDNA and Affymetrix Core
University of Michigan
1500 E Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From daj025 at gmail.com  Thu Jul 19 03:53:16 2007
From: daj025 at gmail.com (David James)
Date: Wed, 18 Jul 2007 21:53:16 -0400
Subject: [R] R MySQL Configuration
In-Reply-To: <f0acec900707181026x394ee07bi2b121abe9c983c9@mail.gmail.com>
References: <f0acec900707181026x394ee07bi2b121abe9c983c9@mail.gmail.com>
Message-ID: <74c69e370707181853qbe57fccvcf38e6f698cda93f@mail.gmail.com>

Hi,

You may specify a different "default.file" in the dbConnect() call
to point to a non-default configuration file;  dbConnect() simply passes this
filename to the MySQL API, which does the appropriate thing (in particular
you may want to double check the MySQL documentation regarding
using this under Windows).

For full details on the arguments to the dbConnect method try

    methods?dbConnect

Hope this helps,

--
David



On 7/18/07, Mark Bulkeley <markbulk at gmail.com> wrote:
> Quick question about the configuration files  relative to RMySQL (I've
> tried to get feedback directly from the author David James, but his
> email address was non-responsive):
>
> Documentation at http://cran.r-project.org/doc/packages/RMySQL.pdf on
> page 3 indicates that for windows machines the only place that the
> my.cnf file will be looked for is the root (C:\) directory.  Is this
> read correct?  When launching R, I've set the HOME directory
> elsewhere, but am unsuccessful in getting recognition of my group
> definitions in this way (only works when the file is in C:\ )
>
> Does anybody know if there are plans to implement the default.file
> option noted on page 17 of the same documentation (noted as
> "Currently unused")?  If I set it now, I get "Error in
> mysqlNewConnection(drv,  ...): unused argument(s) (default.file ...)"
>
> My goal is to eliminate plain text database passwords from my computer
> and at least put it on a secured network location,  but I need to let
> RMySQL know how to find it.  Thanks for your  help.
>
> Regards,
> Mark
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Jul 19 04:05:58 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 18 Jul 2007 22:05:58 -0400
Subject: [R] help with heatmap - how to remove annoying "X" before
	numeric values?
In-Reply-To: <f0fd79140707181707kdf9890fi28147c86dea701ad@mail.gmail.com>
References: <f0fd79140707181707kdf9890fi28147c86dea701ad@mail.gmail.com>
Message-ID: <971536df0707181905x79b39690l71da075b428ea6e7@mail.gmail.com>

read.table is doing that, not heatmap.2.  Use
   read.table("temp.txt", header = TRUE, check.names = FALSE)

On 7/18/07, Suzanne Matthews <suzanne.j.matthews at gmail.com> wrote:
> Hello All,
>
> I have a simple question based on how things are labeled on my heat map;
> particularly, there is this annoying "X" that appears before the numeric
> value of all the labels of my columns.
>
> Let's say I have the following silly data, stored in "temp.txt"
>        1905    1910    1950    1992    2011    2020
> Gnat    0.08    0.29    0.29    0.37    0.39    0.43
> Snake   0.16    0.34    0.32    0.40    0.41    0.53
> Bat     0.40    0.54    0.52    0.60    0.60    0.63
> Cat     0.16    0.27    0.29    0.39    0.37    0.41
> Dog     0.43    0.54    0.52    0.61    0.60    0.62
> Lynx    0.50    0.57    0.54    0.59    0.5     0.59
>
> I use the following commands to generate my heatmap:
> heat <- read.table('temp.txt')
> x <- as.matrix(heat)
>
> heatmap.2(x, keysize=1.2, dendrogram="none", trace="none", Colv = FALSE,
> main = "Silly Data", labCol=
> NULL, margin=c(7,8))
>
> This generates a very nice heatmap, but there is one thing I have an issue
> with: How do I get rid of the 'X' that seems to come automatically before my
> numeric column values? I just want those columns to be labeled 1905, 1910,
> 1950, and so on. I cannot find anything in the heatmap.2 documentation that
> suggests how I should do this.
>
> Thank you very much for your time, and patience in reading this!
>
> Sincerely,
> Suzanne
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From m_olshansky at yahoo.com  Thu Jul 19 04:20:50 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 18 Jul 2007 19:20:50 -0700 (PDT)
Subject: [R] Linear programming question
In-Reply-To: <135232.54180.qm@web45007.mail.sp1.yahoo.com>
Message-ID: <729461.16062.qm@web32211.mail.mud.yahoo.com>

Hi Tobias,

Could you please explain the role of x and y - are
they somehow related to S1,S2,S3,S4?  Are they
constant?  Are they additional variable?
What was your original problem (without the slack
variables)?

Regards,

Moshe.

--- Tobias Schlottmann <tobischlot2002 at yahoo.com>
wrote:

>  Hi everybody,    
>   consider please an optimization problem:  
>    
>   minimize   sum S1+S2
>    
>   Subject to :  y - x =< A + S1
>                      x - y =< A + S2
>    
>   and we want to add two more constraints:
>    
>                    y - x =< B - S3
>                    x - y =< B - S4
>    
>   where A is a small constant value and B is a large
> constant value, S1 and S2 are surplus and S3 and S4
> are slack variables.
>    
>   S3 and S4 have to be maximized in objective
> function. As objective function, is this correct?  :
>    
>   minimize sum S1+ S2 - S3 -S4
>    
>   where actually we want to minimize S1 and S2; and
> maximize S3 and S4.
>    
>   If it is not correct, what to do ?
>    
>   Thank you for any guide.
>    
>   Tobias
> 
>  
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From m_olshansky at yahoo.com  Thu Jul 19 04:57:12 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 18 Jul 2007 19:57:12 -0700 (PDT)
Subject: [R] help with heatmap - how to remove annoying "X" before
	numeric values?
In-Reply-To: <f0fd79140707181707kdf9890fi28147c86dea701ad@mail.gmail.com>
Message-ID: <839361.20175.qm@web32207.mail.mud.yahoo.com>

Hi Suzanne,

My solution (which I am sure is not the best) would
be:

> heat <- read.table('temp.txt')
> heat
      X1905 X1910 X1950 X1992 X2011 X2020
Gnat   0.08  0.29  0.29  0.37  0.39  0.43
Snake  0.16  0.34  0.32  0.40  0.41  0.53
Bat    0.40  0.54  0.52  0.60  0.60  0.63
Cat    0.16  0.27  0.29  0.39  0.37  0.41
Dog    0.43  0.54  0.52  0.61  0.60  0.62
Lynx   0.50  0.57  0.54  0.59  0.50  0.59
> a<-names(heat)
> b<-strsplit(a,split="X")
> w<-unlist(b)
> w
 [1] ""     "1905" ""     "1910" ""     "1950" ""    
"1992" ""     "2011" ""     "2020"
> z <- w[seq(2,length(w),by=2)]
> z
[1] "1905" "1910" "1950" "1992" "2011" "2020"
> names(heat) <- z
> heat
      1905 1910 1950 1992 2011 2020
Gnat  0.08 0.29 0.29 0.37 0.39 0.43
Snake 0.16 0.34 0.32 0.40 0.41 0.53
Bat   0.40 0.54 0.52 0.60 0.60 0.63
Cat   0.16 0.27 0.29 0.39 0.37 0.41
Dog   0.43 0.54 0.52 0.61 0.60 0.62
Lynx  0.50 0.57 0.54 0.59 0.50 0.59
> 

Regards,

Moshe.

--- Suzanne Matthews <suzanne.j.matthews at gmail.com>
wrote:

> Hello All,
> 
> I have a simple question based on how things are
> labeled on my heat map;
> particularly, there is this annoying "X" that
> appears before the numeric
> value of all the labels of my columns.
> 
> Let's say I have the following silly data, stored in
> "temp.txt"
>         1905    1910    1950    1992    2011    2020
> Gnat    0.08    0.29    0.29    0.37    0.39    0.43
> Snake   0.16    0.34    0.32    0.40    0.41    0.53
> Bat     0.40    0.54    0.52    0.60    0.60    0.63
> Cat     0.16    0.27    0.29    0.39    0.37    0.41
> Dog     0.43    0.54    0.52    0.61    0.60    0.62
> Lynx    0.50    0.57    0.54    0.59    0.5     0.59
> 
> I use the following commands to generate my heatmap:
> heat <- read.table('temp.txt')
> x <- as.matrix(heat)
> 
> heatmap.2(x, keysize=1.2, dendrogram="none",
> trace="none", Colv = FALSE,
> main = "Silly Data", labCol=
> NULL, margin=c(7,8))
> 
> This generates a very nice heatmap, but there is one
> thing I have an issue
> with: How do I get rid of the 'X' that seems to come
> automatically before my
> numeric column values? I just want those columns to
> be labeled 1905, 1910,
> 1950, and so on. I cannot find anything in the
> heatmap.2 documentation that
> suggests how I should do this.
> 
> Thank you very much for your time, and patience in
> reading this!
> 
> Sincerely,
> Suzanne
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From ripley at stats.ox.ac.uk  Thu Jul 19 05:40:43 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jul 2007 04:40:43 +0100 (BST)
Subject: [R] help with heatmap - how to remove annoying "X" before
 numeric values?
In-Reply-To: <839361.20175.qm@web32207.mail.mud.yahoo.com>
References: <839361.20175.qm@web32207.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0707190430380.30260@gannet.stats.ox.ac.uk>

read.table('temp.txt', check.names = FALSE)

would be easier (and more general, since make.names can do more than 
prepend an 'X').

On Wed, 18 Jul 2007, Moshe Olshansky wrote:

> Hi Suzanne,
>
> My solution (which I am sure is not the best) would
> be:
>
>> heat <- read.table('temp.txt')
>> heat
>      X1905 X1910 X1950 X1992 X2011 X2020
> Gnat   0.08  0.29  0.29  0.37  0.39  0.43
> Snake  0.16  0.34  0.32  0.40  0.41  0.53
> Bat    0.40  0.54  0.52  0.60  0.60  0.63
> Cat    0.16  0.27  0.29  0.39  0.37  0.41
> Dog    0.43  0.54  0.52  0.61  0.60  0.62
> Lynx   0.50  0.57  0.54  0.59  0.50  0.59
>> a<-names(heat)
>> b<-strsplit(a,split="X")
>> w<-unlist(b)
>> w
> [1] ""     "1905" ""     "1910" ""     "1950" ""
> "1992" ""     "2011" ""     "2020"
>> z <- w[seq(2,length(w),by=2)]
>> z
> [1] "1905" "1910" "1950" "1992" "2011" "2020"
>> names(heat) <- z
>> heat
>      1905 1910 1950 1992 2011 2020
> Gnat  0.08 0.29 0.29 0.37 0.39 0.43
> Snake 0.16 0.34 0.32 0.40 0.41 0.53
> Bat   0.40 0.54 0.52 0.60 0.60 0.63
> Cat   0.16 0.27 0.29 0.39 0.37 0.41
> Dog   0.43 0.54 0.52 0.61 0.60 0.62
> Lynx  0.50 0.57 0.54 0.59 0.50 0.59
>>
>
> Regards,
>
> Moshe.
>
> --- Suzanne Matthews <suzanne.j.matthews at gmail.com>
> wrote:
>
>> Hello All,
>>
>> I have a simple question based on how things are
>> labeled on my heat map;
>> particularly, there is this annoying "X" that
>> appears before the numeric
>> value of all the labels of my columns.
>>
>> Let's say I have the following silly data, stored in
>> "temp.txt"
>>         1905    1910    1950    1992    2011    2020
>> Gnat    0.08    0.29    0.29    0.37    0.39    0.43
>> Snake   0.16    0.34    0.32    0.40    0.41    0.53
>> Bat     0.40    0.54    0.52    0.60    0.60    0.63
>> Cat     0.16    0.27    0.29    0.39    0.37    0.41
>> Dog     0.43    0.54    0.52    0.61    0.60    0.62
>> Lynx    0.50    0.57    0.54    0.59    0.5     0.59
>>
>> I use the following commands to generate my heatmap:
>> heat <- read.table('temp.txt')
>> x <- as.matrix(heat)
>>
>> heatmap.2(x, keysize=1.2, dendrogram="none",
>> trace="none", Colv = FALSE,
>> main = "Silly Data", labCol=
>> NULL, margin=c(7,8))
>>
>> This generates a very nice heatmap, but there is one
>> thing I have an issue
>> with: How do I get rid of the 'X' that seems to come
>> automatically before my
>> numeric column values? I just want those columns to
>> be labeled 1905, 1910,
>> 1950, and so on. I cannot find anything in the
>> heatmap.2 documentation that
>> suggests how I should do this.
>>
>> Thank you very much for your time, and patience in
>> reading this!
>>
>> Sincerely,
>> Suzanne
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained,
>> reproducible code.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From lzhtom at hotmail.com  Thu Jul 19 05:49:55 2007
From: lzhtom at hotmail.com (zhihua li)
Date: Thu, 19 Jul 2007 03:49:55 +0000
Subject: [R] memory error with 64-bit R in linux
In-Reply-To: <644e1f320707181550y55b8f508gb58994e787d8adf2@mail.gmail.com>
Message-ID: <BAY110-F18B1BA4A6DB9A464CB1A5BC7FB0@phx.gbl>

Thanks for replying!
i don't think i'm paging. i tried to use a smaller version of my matrix and 
do all the checkings as suggested by jim. The smaller matrix caused another 
problem, for which I've opened another thread. But i've found something 
about memory that I don't understand.
> gc()
          used (Mb) gc trigger  (Mb) max used  (Mb)
Ncells  269577 14.4    5570995 297.6  8919855 476.4
Vcells 3353395 25.6    9493567  72.5 15666095 119.6

Does this mean the maximum memory I can use for variables is only 120 M?
However, when I tried to check the memory limits:
> mem.limits()
nsize vsize
   NA    NA

Here it seems the maximum memory is not limited?

When there is no R function is being executed, I checked the system process 
by:
ps u

PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
7821  0.0  0.1  10048  2336 pts/0    Ss   Jul18   0:00 -bash
8076  2.9 24.5 523088 504004 pts/0   S+   Jul18   2:46 /usr/lib64/R/bi
8918  1.5  0.1   9912  2328 pts/1    Ss   00:44   0:00 -bash
8962  0.0  0.0   3808   868 pts/1    R+   00:45   0:00 ps u

Does this mean R is using 25% of my memory? But my RAM is 2 GB and the 
objects in R only occupy 40 MB from gc().

Did I interpret it wrong?

Thanks a lot!



>From: "jim holtman" <jholtman at gmail.com>
>To: "zhihua li" <lzhtom at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] memory error with 64-bit R in linux
>Date: Wed, 18 Jul 2007 17:50:31 -0500
>
>Are you paging?  That might explain the long run times. How much 
>space
>are your other objects taking up?  The matrix by itself should only
>require about 13MB if it is numeric.  I would guess it is some of 
>the
>other objects that you have in your working space.  Put some gc() in
>your loop to see how much space is being used.  Run it with a subset
>of the data and see how long it takes.  This might give you an
>estimate of the time, and space, that might be needed for the entire
>dataset.
>
>Do a 'ps' to see how much memory your process is using.  Do one 
>every
>couple of minutes to see if it is growing.  You can alway use 
>Rprof()
>to get an idea of where time is being spent (use it on a small
>subset).
>
>On 7/18/07, zhihua li <lzhtom at hotmail.com> wrote:
>>Hi netters,
>>
>>I'm using the 64-bit R-2.5.0 on a x86-64 cpu, with an RAM of 2 GB.  
>>The
>>operating system is SUSE 10.
>>The system information is:
>>-uname -a
>>Linux someone 2.6.13-15.15-smp #1 SMP Mon Feb 26 14:11:33 UTC 2007 
>>x86_64
>>x86_64 x86_64 GNU/Linux
>>
>>I used heatmap to process a matrix of the dim [16000,100].  After 3 
>>hours
>>of desperating waiting, R told me:
>>cannot allocate vector of size 896 MB.
>>
>>I know the matrix is very big, but since I have 2 GB of RAM and in 
>>a 64-bit
>>system, there should be no problem to deal with a vector smaller 
>>than 1 GB?
>>(I was not running any other applications in my system)
>>
>>Does anyone know what's going on?  Is there a hardware limit where 
>>I have
>>to add more RAM, or is there some way to resolve it softwarely? 
>>Also is it
>>possible to speed up the computing (I don't wanna wait another 3 
>>hours to
>>know I get another error message)
>>
>>Thank you in advance!
>>
>>_________________________________________________________________
>>?????????????????????????????? MSN Hotmail??  
>>http://www.hotmail.com
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>
>--
>Jim Holtman
>Cincinnati, OH
>+1 513 646 9390
>
>What is the problem you are trying to solve?


From m_olshansky at yahoo.com  Thu Jul 19 05:53:42 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Wed, 18 Jul 2007 20:53:42 -0700 (PDT)
Subject: [R] help with heatmap - how to remove annoying "X" before
	numeric values?
In-Reply-To: <Pine.LNX.4.64.0707190430380.30260@gannet.stats.ox.ac.uk>
Message-ID: <397533.49001.qm@web32211.mail.mud.yahoo.com>

I was right saying that my solution was not the best
possible!

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> read.table('temp.txt', check.names = FALSE)
> 
> would be easier (and more general, since make.names
> can do more than 
> prepend an 'X').
> 
> On Wed, 18 Jul 2007, Moshe Olshansky wrote:
> 
> > Hi Suzanne,
> >
> > My solution (which I am sure is not the best)
> would
> > be:
> >
> >> heat <- read.table('temp.txt')
> >> heat
> >      X1905 X1910 X1950 X1992 X2011 X2020
> > Gnat   0.08  0.29  0.29  0.37  0.39  0.43
> > Snake  0.16  0.34  0.32  0.40  0.41  0.53
> > Bat    0.40  0.54  0.52  0.60  0.60  0.63
> > Cat    0.16  0.27  0.29  0.39  0.37  0.41
> > Dog    0.43  0.54  0.52  0.61  0.60  0.62
> > Lynx   0.50  0.57  0.54  0.59  0.50  0.59
> >> a<-names(heat)
> >> b<-strsplit(a,split="X")
> >> w<-unlist(b)
> >> w
> > [1] ""     "1905" ""     "1910" ""     "1950" ""
> > "1992" ""     "2011" ""     "2020"
> >> z <- w[seq(2,length(w),by=2)]
> >> z
> > [1] "1905" "1910" "1950" "1992" "2011" "2020"
> >> names(heat) <- z
> >> heat
> >      1905 1910 1950 1992 2011 2020
> > Gnat  0.08 0.29 0.29 0.37 0.39 0.43
> > Snake 0.16 0.34 0.32 0.40 0.41 0.53
> > Bat   0.40 0.54 0.52 0.60 0.60 0.63
> > Cat   0.16 0.27 0.29 0.39 0.37 0.41
> > Dog   0.43 0.54 0.52 0.61 0.60 0.62
> > Lynx  0.50 0.57 0.54 0.59 0.50 0.59
> >>
> >
> > Regards,
> >
> > Moshe.
> >
> > --- Suzanne Matthews
> <suzanne.j.matthews at gmail.com>
> > wrote:
> >
> >> Hello All,
> >>
> >> I have a simple question based on how things are
> >> labeled on my heat map;
> >> particularly, there is this annoying "X" that
> >> appears before the numeric
> >> value of all the labels of my columns.
> >>
> >> Let's say I have the following silly data, stored
> in
> >> "temp.txt"
> >>         1905    1910    1950    1992    2011   
> 2020
> >> Gnat    0.08    0.29    0.29    0.37    0.39   
> 0.43
> >> Snake   0.16    0.34    0.32    0.40    0.41   
> 0.53
> >> Bat     0.40    0.54    0.52    0.60    0.60   
> 0.63
> >> Cat     0.16    0.27    0.29    0.39    0.37   
> 0.41
> >> Dog     0.43    0.54    0.52    0.61    0.60   
> 0.62
> >> Lynx    0.50    0.57    0.54    0.59    0.5    
> 0.59
> >>
> >> I use the following commands to generate my
> heatmap:
> >> heat <- read.table('temp.txt')
> >> x <- as.matrix(heat)
> >>
> >> heatmap.2(x, keysize=1.2, dendrogram="none",
> >> trace="none", Colv = FALSE,
> >> main = "Silly Data", labCol=
> >> NULL, margin=c(7,8))
> >>
> >> This generates a very nice heatmap, but there is
> one
> >> thing I have an issue
> >> with: How do I get rid of the 'X' that seems to
> come
> >> automatically before my
> >> numeric column values? I just want those columns
> to
> >> be labeled 1905, 1910,
> >> 1950, and so on. I cannot find anything in the
> >> heatmap.2 documentation that
> >> suggests how I should do this.
> >>
> >> Thank you very much for your time, and patience
> in
> >> reading this!
> >>
> >> Sincerely,
> >> Suzanne
> >>
> >> 	[[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained,
> >> reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained,
> reproducible code.
> >
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
>


From lzhtom at hotmail.com  Thu Jul 19 05:58:38 2007
From: lzhtom at hotmail.com (zhihua li)
Date: Thu, 19 Jul 2007 03:58:38 +0000
Subject: [R] Error: evaluation nested too deeply when doing heatmap with
	binary distfunction
Message-ID: <BAY110-F26DE1AF7405ACF3A6A47A7C7FB0@phx.gbl>

Hi netters,

I have a matrix X of the size (1000,100). The values are from -3 to +3.  
When I tried

heatmap(X, 
distfun=function(c),dist(c,method="bin"),hclustfun=function(m),hclust(m,method="average"))


I got the error message: 
Error: evaluation nested too deeply: infinite recursion / 
options(expressions=)?

However, if I used default parameters for distfunction:
heatmap(X, hclustfun=function(m),hclust(m,method="average"))
there is no error messages at all.

But the problem is that I have to use binary method in my disfunction. How 
can I resolve the problem?

Thanks a lot!


From jholtman at gmail.com  Thu Jul 19 06:09:45 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 19 Jul 2007 00:09:45 -0400
Subject: [R] memory error with 64-bit R in linux
In-Reply-To: <BAY110-F18B1BA4A6DB9A464CB1A5BC7FB0@phx.gbl>
References: <644e1f320707181550y55b8f508gb58994e787d8adf2@mail.gmail.com>
	<BAY110-F18B1BA4A6DB9A464CB1A5BC7FB0@phx.gbl>
Message-ID: <644e1f320707182109k272a7561webf4c9b6c1e5172e@mail.gmail.com>

The output from gc() indicates that you had a maximum usage of
476MB+119MB=~600MB.  If you look at the output of ps you will notice
that the process size is 523MB (or about 500MB if you want to be
exact).  So you are using about 25% of the 2GB that you have
available.

mem.limit just shows the current value of the parameters, and as the
help file says:

"Value
mem.limits() returns an integer vector giving the current settings of
the maxima, possibly NA."



On 7/18/07, zhihua li <lzhtom at hotmail.com> wrote:
> Thanks for replying!
> i don't think i'm paging. i tried to use a smaller version of my matrix and
> do all the checkings as suggested by jim. The smaller matrix caused another
> problem, for which I've opened another thread. But i've found something
> about memory that I don't understand.
> > gc()
>          used (Mb) gc trigger  (Mb) max used  (Mb)
> Ncells  269577 14.4    5570995 297.6  8919855 476.4
> Vcells 3353395 25.6    9493567  72.5 15666095 119.6
>
> Does this mean the maximum memory I can use for variables is only 120 M?
> However, when I tried to check the memory limits:
> > mem.limits()
> nsize vsize
>   NA    NA
>
> Here it seems the maximum memory is not limited?
>
> When there is no R function is being executed, I checked the system process
> by:
> ps u
>
> PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
> 7821  0.0  0.1  10048  2336 pts/0    Ss   Jul18   0:00 -bash
> 8076  2.9 24.5 523088 504004 pts/0   S+   Jul18   2:46 /usr/lib64/R/bi
> 8918  1.5  0.1   9912  2328 pts/1    Ss   00:44   0:00 -bash
> 8962  0.0  0.0   3808   868 pts/1    R+   00:45   0:00 ps u
>
> Does this mean R is using 25% of my memory? But my RAM is 2 GB and the
> objects in R only occupy 40 MB from gc().
>
> Did I interpret it wrong?
>
> Thanks a lot!
>
>
>
> >From: "jim holtman" <jholtman at gmail.com>
> >To: "zhihua li" <lzhtom at hotmail.com>
> >CC: r-help at stat.math.ethz.ch
> >Subject: Re: [R] memory error with 64-bit R in linux
> >Date: Wed, 18 Jul 2007 17:50:31 -0500
> >
> >Are you paging?  That might explain the long run times. How much
> >space
> >are your other objects taking up?  The matrix by itself should only
> >require about 13MB if it is numeric.  I would guess it is some of
> >the
> >other objects that you have in your working space.  Put some gc() in
> >your loop to see how much space is being used.  Run it with a subset
> >of the data and see how long it takes.  This might give you an
> >estimate of the time, and space, that might be needed for the entire
> >dataset.
> >
> >Do a 'ps' to see how much memory your process is using.  Do one
> >every
> >couple of minutes to see if it is growing.  You can alway use
> >Rprof()
> >to get an idea of where time is being spent (use it on a small
> >subset).
> >
> >On 7/18/07, zhihua li <lzhtom at hotmail.com> wrote:
> >>Hi netters,
> >>
> >>I'm using the 64-bit R-2.5.0 on a x86-64 cpu, with an RAM of 2 GB.
> >>The
> >>operating system is SUSE 10.
> >>The system information is:
> >>-uname -a
> >>Linux someone 2.6.13-15.15-smp #1 SMP Mon Feb 26 14:11:33 UTC 2007
> >>x86_64
> >>x86_64 x86_64 GNU/Linux
> >>
> >>I used heatmap to process a matrix of the dim [16000,100].  After 3
> >>hours
> >>of desperating waiting, R told me:
> >>cannot allocate vector of size 896 MB.
> >>
> >>I know the matrix is very big, but since I have 2 GB of RAM and in
> >>a 64-bit
> >>system, there should be no problem to deal with a vector smaller
> >>than 1 GB?
> >>(I was not running any other applications in my system)
> >>
> >>Does anyone know what's going on?  Is there a hardware limit where
> >>I have
> >>to add more RAM, or is there some way to resolve it softwarely?
> >>Also is it
> >>possible to speed up the computing (I don't wanna wait another 3
> >>hours to
> >>know I get another error message)
> >>
> >>Thank you in advance!
> >>
> >>_________________________________________________________________
> >>?????????????????????????????? MSN Hotmail??
> >>http://www.hotmail.com
> >>
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >
> >
> >--
> >Jim Holtman
> >Cincinnati, OH
> >+1 513 646 9390
> >
> >What is the problem you are trying to solve?
>
> _________________________________________________________________
> ???????????????????????????? MSN Messenger:  http://messenger.msn.com/cn
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From gruen at ci.tuwien.ac.at  Thu Jul 19 08:14:37 2007
From: gruen at ci.tuwien.ac.at (Bettina Gruen)
Date: Thu, 19 Jul 2007 08:14:37 +0200
Subject: [R] EM unsupervised clustering
In-Reply-To: <469E17A0.3030209@imperial.ac.uk>
References: <469E17A0.3030209@imperial.ac.uk>
Message-ID: <469F014D.8090705@ci.tuwien.ac.at>

Federico,

you might also want to have a look at packages "flexclust" or "flexmix", 
so you can take into account that you have binary data. The "mclust" 
package can be used to estimate mixtures of Gaussian distributions. 
"flexclust" implements kmeans-like algorithms, but you can specify a 
distance measure appropriate for binary data. "flexmix" allows latent 
class analysis with binary data using FLXMCmvbinary() for the component 
specific model.

Best,
Bettina


Federico Calboli wrote:
> Hi All,
> 
> I have a  n x m matrix. The n rows are individuals, the m columns are variables.
> 
> The matrix is in itself a collection of 1s (if a variable is observed for an 
> individual), and 0s (is there is no observation).
> 
> Something like:
> 
>       [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    1    0    1    1    0    0
> [2,]    1    0    1    1    0    0
> [3,]    1    0    1    1    0    0
> [4,]    0    1    0    0    0    0
> [5,]    1    0    1    1    0    0
> [6,]    0    1    0    0    1    0
> 
> 
> I use kmeans to find 2 or 3 clusters in this matrix
> 
> k2 = kmeans(data, 2, 10000000)
> k3 = kmeans(data, 3, 10000000)
> 
> but I would like to use something a bit more refined, so I though about a EM 
> based clustering. I am using the Mclust() function from the mclust package, but 
> I get the following (to me incomprehensible) error message:
> 
> plot(Mclust(as.data.frame(data)), as.data.frame(data))
> Hit <Return> to see next plot:
> Hit <Return> to see next plot:
> Hit <Return> to see next plot:
> Error in 1:L : NA/NaN argument
> In addition: Warning messages:
> 1: best model occurs at the min or max # of components considered in: 
> summary.mclustBIC(Bic, data, G = G, modelNames = modelNames)
> 2: optimal number of clusters occurs at min choice in: 
> Mclust(as.data.frame(anc.st.mat))
> 3: insufficient input for specified plot in: coordProj(data = data, parameters = 
> x$parameters, z = x$z, what = "classification",
> 
> That's puzzling because the example given by ?Mclust is something like
> 
> plot(Mclust(iris[,-5]), iris[,-5])
> 
> which is pretty simple and dumbproof and works flawlessly...
> 
> best,
> 
> Federico
>


From rfluss at gmail.com  Thu Jul 19 09:03:37 2007
From: rfluss at gmail.com (Fluss)
Date: Thu, 19 Jul 2007 09:03:37 +0200
Subject: [R] R
Message-ID: <9c946f8e0707190003h6a97df0dhcf3a3a6c378dc876@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/8bfbe4d9/attachment.pl 

From xkneifl at mendelu.cz  Thu Jul 19 10:08:38 2007
From: xkneifl at mendelu.cz (Michal Kneifl)
Date: Thu, 19 Jul 2007 10:08:38 +0200
Subject: [R] Classification
References: <20070718193656.qxcet1zrc0goos8o@mail.mendelu.cz>
	<1184781191.13375.26.camel@Bellerophon.localdomain>
Message-ID: <002801c7c9dc$036c8460$a64bb2c3@Mistr1>

For all who sent help on topic Classification:

Thank you very much folks. 
I have got some inspiration how to solve this task.

Michael

----- Original Message ----- 
From: "Marc Schwartz" <marc_schwartz at comcast.net>
To: "Ing. Michal Kneifl, Ph.D." <xkneifl at mendelu.cz>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, July 18, 2007 7:53 PM
Subject: Re: [R] Classification


> On Wed, 2007-07-18 at 19:36 +0200, Ing. Michal Kneifl, Ph.D. wrote:
>> Hi,
>> I am also a quite new user of R and would like to ask you for help:
>> I have a data frame where all columns are numeric variables. My aim is  
>> to convert one columnt in factors.
>> Example:
>> MD
>> 0.2
>> 0.1
>> 0.8
>> 0.3
>> 0.7
>> 0.6
>> 0.01
>> 0.2
>> 0.5
>> 1
>> 1
>> 
>> 
>> I want to make classes:
>> 0-0.2 A
>> 0.21-0.4 B
>> 0.41-0.6 C
>> ..... and so on
>> 
>> So after classification I wil get:
>> MD
>> A
>> A
>> D
>> B
>> .
>> .
>> .
>> and so on
>> 
>> Please could you give an advice to a newbie?
>> Thanks a lot in advance..
>> 
>> Michael
> 
> See ?cut
> 
> You can then do something like:
> 
>> DF
>     MD
> 1  0.20
> 2  0.10
> 3  0.80
> 4  0.30
> 5  0.70
> 6  0.60
> 7  0.01
> 8  0.20
> 9  0.50
> 10 1.00
> 11 1.00
> 
> 
>> cut(DF$MD, breaks = c(seq(0, 1, .2)), labels = LETTERS[1:5])
> [1] A A D B D C A A C E E
> Levels: A B C D E
> 
> 
> HTH,
> 
> Marc Schwartz
> 
>


From p.dalgaard at biostat.ku.dk  Thu Jul 19 10:19:39 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 19 Jul 2007 10:19:39 +0200
Subject: [R] RAM, swap, Error: cannot allocate vector of size, Linux:
In-Reply-To: <C69D0659F0206346A12EF75423B90BD012F047@EXCHANGEVS-03.ad.wsu.edu>
References: <C69D0659F0206346A12EF75423B90BD012F047@EXCHANGEVS-03.ad.wsu.edu>
Message-ID: <469F1E9B.20802@biostat.ku.dk>

Feldman, Maximilian Jeffrey wrote:
> Dear Community,
>
> I am very new to the world of Linux and R and I have stumbled upon a problem that I cannot seem to resolve on my own. Here is the relevant background:
>
> I am working on a 64-bit Linux Fedora Core 6 OS. I using R version 2.5.1. I have 3.8 Gb of RAM and 1.9 Gb of swap. As I see it, there are no restraints on the amount of memory that R can use imposed by this particular OS build. When I type in the 'ulimit' command at the command line the response is 'unlimited'.
>
> Here is the problem:
>
> I have uploaded and normalized 48 ATH1 microarray slides using the justRMA function.
>
>   
>> library("affy")
>>     
>
>   
>> setwd("/Data/cel")
>>     
>
>   
>> Data<-justRMA()
>>     
>
> The next step in my analysis is to calculate a distance matrix for my dataset using bioDist package. This is where I get my error.
>
>   
>> library("bioDist")
>>     
>
>   
>> x<-cor.dist(exprs(Data))
>>     
>
> Error: cannot allocate vector of size 3.9 Gb
>
> I used the following function to examine my memory limitations:
>
>   
>> mem.limits()
>>     
>
> nsize vsize 
>
> NA NA 
>
> I believe this means there isn't any specified limit to the amount of memory R can allocate to my task. I realize I only have 3.8 Gb of RAM but I would expect that R would use my 1.9 Gb of swap. 
>   
It does, if swap works at all on your machine. However, the error 
message is relates to the object that R fails to create, not the total 
memory usage. I.e. this might very well be the _second_ object of size 
3.9Gb that you are trying to fit into 5.7Gb of memory.  You could try 
increasing the swap space (the expedient, although perhaps not 
efficient, way is to find a file system with a few tens of Gb to spare 
and create a large swapfile on it.)
> Does R not use my swap space? Can I explicitly tell R to use my swap space for large tasks such as this? 
>
> I was not able to find any information regarding this particular issue in the R Linux manual, Linux FAQ, or on previous listserv threads. Many of the users who had similar questions resolved their problems in a different manner.
>
> Thanks to anyone who thinks they can provide assistance!
>
> Max 
>
> Graduate Student
>
> Molecular Plant Sciences
>
> Washington State University
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ligges at statistik.uni-dortmund.de  Thu Jul 19 10:39:01 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jul 2007 10:39:01 +0200
Subject: [R] Strange warning in summary.lm
In-Reply-To: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>
Message-ID: <469F2325.5010205@statistik.uni-dortmund.de>



ONKELINX, Thierry wrote:
> Dear useRs,
> 
> Lately I noticed a strange warning in the summary of a lm-object. Any
> idea what this warning is about? I'm using R 2.5.1 on Win XP pro.
> 
>> x <- rnorm(100)
>> y <- rnorm(100)
>> summary(lm(y~x))
> 
> Call:
> lm(formula = y ~ x)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max 
> -1,77809 -0,68438 -0,04409  0,63891  2,30863 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) -0,00217    0,09244  -0,023    0,981
> x            0,01315    0,09628   0,137    0,892
> 
> Residual standard error: 0,9236 on 98 degrees of freedom
> Multiple R-Squared: 0.0001903,  Adjusted R-squared: -0.01001 
> F-statistic: 0.01866 on 1 and 98 DF,  p-value: 0,8916 
> 
> Warning message:
> NAs introduced by coercion in: as.double.default(Cf[okP]) 

Probably you have an object in your workspace or another attached 
environment in the search path that conflicts with objects that are 
required to call summary(lm(...)). E.g. some lm... oder summary... function?

Best,
Uwe Ligges



> 	
> Thanks,
> 
> Thierry
> 
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be 
> 
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Thierry.ONKELINX at inbo.be  Thu Jul 19 10:52:13 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 19 Jul 2007 10:52:13 +0200
Subject: [R] Strange warning in summary.lm
In-Reply-To: <469F2325.5010205@statistik.uni-dortmund.de>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>
	<469F2325.5010205@statistik.uni-dortmund.de>
Message-ID: <2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>

The problem also exists in a clean workspace. But I've found the
troublemaker. I had set options(OutDec = ","). Resetting this to
options(OutDec = ".") solved the problem.

Thanks,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: Uwe Ligges [mailto:ligges op statistik.uni-dortmund.de] 
> Verzonden: donderdag 19 juli 2007 10:39
> Aan: ONKELINX, Thierry
> CC: r-help op stat.math.ethz.ch
> Onderwerp: Re: [R] Strange warning in summary.lm
> 
> 
> 
> ONKELINX, Thierry wrote:
> > Dear useRs,
> > 
> > Lately I noticed a strange warning in the summary of a 
> lm-object. Any 
> > idea what this warning is about? I'm using R 2.5.1 on Win XP pro.
> > 
> >> x <- rnorm(100)
> >> y <- rnorm(100)
> >> summary(lm(y~x))
> > 
> > Call:
> > lm(formula = y ~ x)
> > 
> > Residuals:
> >      Min       1Q   Median       3Q      Max 
> > -1,77809 -0,68438 -0,04409  0,63891  2,30863
> > 
> > Coefficients:
> >             Estimate Std. Error t value Pr(>|t|)
> > (Intercept) -0,00217    0,09244  -0,023    0,981
> > x            0,01315    0,09628   0,137    0,892
> > 
> > Residual standard error: 0,9236 on 98 degrees of freedom Multiple 
> > R-Squared: 0.0001903,  Adjusted R-squared: -0.01001
> > F-statistic: 0.01866 on 1 and 98 DF,  p-value: 0,8916
> > 
> > Warning message:
> > NAs introduced by coercion in: as.double.default(Cf[okP])
> 
> Probably you have an object in your workspace or another 
> attached environment in the search path that conflicts with 
> objects that are required to call summary(lm(...)). E.g. some 
> lm... oder summary... function?
> 
> Best,
> Uwe Ligges
> 
> 
> 
> > 	
> > Thanks,
> > 
> > Thierry
> > 
> > 
> ----------------------------------------------------------------------
> > --
> > ----
> > ir. Thierry Onkelinx
> > Instituut voor natuur- en bosonderzoek / Research Institute 
> for Nature 
> > and Forest Cel biometrie, methodologie en kwaliteitszorg / Section 
> > biometrics, methodology and quality assurance Gaverstraat 4 9500 
> > Geraardsbergen Belgium tel. + 32 54/436 185 
> Thierry.Onkelinx op inbo.be 
> > www.inbo.be
> > 
> > Do not put your faith in what statistics say until you have 
> carefully 
> > considered what they do not say.  ~William W. Watt A statistical 
> > analysis, properly conducted, is a delicate dissection of 
> > uncertainties, a surgery of suppositions. ~M.J.Moroney
> > 
> > ______________________________________________
> > R-help op stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From smangut at gmail.com  Thu Jul 19 11:00:13 2007
From: smangut at gmail.com (sigalit mangut-leiba)
Date: Thu, 19 Jul 2007 12:00:13 +0300
Subject: [R] tapply
Message-ID: <c99f7100707190200h409ace37p4f848310ea034825@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/83137688/attachment.pl 

From hwborchers at yahoo.de  Thu Jul 19 09:55:12 2007
From: hwborchers at yahoo.de (Hans W. Borchers)
Date: Thu, 19 Jul 2007 07:55:12 +0000 (UTC)
Subject: [R] Is there a facility in R similar to MatLab
References: <469EA055.40608@stanford.edu>
Message-ID: <loom.20070719T094344-115@post.gmane.org>

Dylan Arena <darena <at> stanford.edu> writes:
> 
> Hi,
> 
> I'm trying to use R to get eigenvalues and eigenvectors of a matrix 
> whose elements are of the form (2 * lambda), -(lambda + mu), etc.  I'd 
> like R to treat this matrix as a numeric matrix without treating lambda 
> and mu as variable names but rather as some sort of atomic quantities 
> (and hence give eigenvectors in terms of mu and/or lambda).  MatLab and 
> Mathematica both do this, but I'm not sure whether R does.  Does anyone 
> have any ideas about how to do this?
> 
> Please let me know,
> Dylan
> 

You are asking for a Computer Algebra problem and thus you should apply a
Computer Algebra System (CAS) such as Maple, Mathematica, MuPAD, or the
free software Maxima.
By the way, Matlab does _not_ do this, it utilizes the Maple kernel in its
Symbolic Toolbox.
In R you can try the Ryacas package, but I personally would prefer one of
the more extended systems. Maxima with wxMaxima interface is quite nice,
and for MuPAD there is a non-expensive personal copy available.

Hans Werner Borchers

> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cg.pettersson at vpe.slu.se  Thu Jul 19 11:52:09 2007
From: cg.pettersson at vpe.slu.se (CG Pettersson)
Date: Thu, 19 Jul 2007 11:52:09 +0200 (CEST)
Subject: [R] Subsetting dataframes
Message-ID: <61510.194.17.178.40.1184838729.squirrel@webmail.slu.se>

Dear all!

W2k, R 2.5.1

I am working with an ongoing malting barley variety evaluation within
Sweden. The structure is 25 cultivars tested each year at four sites, in
field trials with three replicates and 'lattice' structure (the replicates
are divided into five sub blocks in a structured way). As we are normally
keeping around 15 varieties from each year to the next, and take in 10 new
for next year, we have tested totally 72 different varieties during five
years.

I store the data in a field trial database, and generate text tables with
the subset of data I want and import the frame to R. I take in all
cultivars in R and use 'subset' to select what I want to look at. Using
lme{nlme} works with no problems to get mean results over the years, but
as I now have a number of years I want to analyse the general site x
cultivar relation. I am testing AMMI{agricolae} for this and it seems to
work except for the subsetting. This is what happens:

If I do the subsetting like this:

x62_samvar <- subset(x62_5, cn %in%
c("Astoria","Barke","Christina","Makof", "Prestige","Publican","Quench"))

A test run with AMMI seems to work in the first part:

> AMMI(site, cn, rep, yield)

ANALYSIS AMMI:  yield
Class level information

ENV:  Hag Klb Bjt Ska
GEN:  Astoria Prestige Makof Christina Publican Quench
REP:  1 2 3

Number of observations:  240

model Y: yield  ~ ENV + REP%in%ENV + GEN + ENV:GEN

Analysis of Variance Table

Response: Y
           Df    Sum Sq   Mean Sq F value    Pr(>F)
ENV         3 120092418  40030806 90.0424 1.665e-06 ***
REP(ENV)    8   3556620    444578  0.5674  0.803923
GEN         5  21376142   4275228  5.4564 9.680e-05 ***
ENV:GEN    15  28799807   1919987  2.4504  0.002555 **
Residuals 208 162973213    783525
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Coeff var       Mean yield
13.08629         6764.098

After this something goes wrong, as AMMI finds a cultivar name not
selected in the subsetting. (The plotting might go wrong anyhow, but I
haven?t got that far yet):

Error in model.frame.default(Terms, newdata, na.action = na.action, xlev =
object$xlevels) :
        factor 'y' has new level(s) Arkadia


Looking at the dataframe using

> edit(x62_samvar)

only shows the selected lines, but using levels() gives another answer as

> levels(x62_samvar$cn)

gives back all 72 cultivar names used during the five years (starting with
Arcadia).

Where do I go wrong and how do I use subset in a proper way?

Thanks
/CG

-- 
CG Pettersson, PhD
Swedish University of Agricultural Sciences (SLU)
Dept. of Crop Production Ecology. Box 7043.
SE-750 07 Uppsala, Sweden
cg.pettersson at vpe.slu.se


From jim at bitwrit.com.au  Thu Jul 19 12:01:16 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 19 Jul 2007 20:01:16 +1000
Subject: [R] mfrow is ignored by some plots
Message-ID: <469F366C.2080702@bitwrit.com.au>

Hi all,
I was just informed that the plots in the radial.plot family in the 
plotrix package do not plot correctly when using mfrow or mfcol to 
subdivide the plot window. I found one related message, an answer from 
Deepayan Sarkar to a question about lattice graphics, but that shed no 
light on this problem.

If I invoke par(mfrow=c(2,2)) and run radial.plot a few times, the plots 
appears in the upper left corner. _Sometimes_ the second plot appears in 
the upper right corner once, but then returns to the upper left. If I 
invoke par(new=TRUE), the plots appear in the lower right corner.

For instance, the following just keeps writing the plots in the upper left.

par(mfrow=c(2,2))
radial.plot(rnorm(7),0:6,line.col="red",lwd=2)
radial.plot(rnorm(7),0:6,line.col="red",lwd=2)
radial.plot(rnorm(7),0:6,line.col="red",lwd=2)
radial.plot(rnorm(7),0:6,line.col="red",lwd=2)

If I then enter:

par(new=TRUE)

and repeat the plots, they all accumulate in the lower right. I think 
this may have something to do with the calls to points or lines or 
symbols within radial.plot, but I could find no information on this in 
the help for plot, par, or several other pages.

Any clues?

Jim


From birgit.lemcke at systbot.uzh.ch  Thu Jul 19 12:43:03 2007
From: birgit.lemcke at systbot.uzh.ch (Birgit Lemcke)
Date: Thu, 19 Jul 2007 12:43:03 +0200
Subject: [R] write.table linebreaks
Message-ID: <7EEB5606-9CD3-4EBA-8207-628ABFAFD7DD@systbot.uzh.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/43f6c397/attachment.pl 

From yn19832 at msn.com  Thu Jul 19 12:49:46 2007
From: yn19832 at msn.com (livia)
Date: Thu, 19 Jul 2007 03:49:46 -0700 (PDT)
Subject: [R] fSeries GARCH(1,1)
Message-ID: <11686281.post@talk.nabble.com>


Hello all, I am trying to use the "garchFit" function in the fSeries Package
to fit a Garch(1,1) Model with t distribution. I am using the following
codes.

fit <- garchFit(~garch(1,1),data,cond.dist="dstd")
fitted(fit)

I was expecting the fitted(fit) would return the fitted volatility, but the
result turns out to be a series of repeated same value. I tried to change
the distribution to normal, and the same thing happened.

Could anyone give me some advice? Many thanks.
-- 
View this message in context: http://www.nabble.com/fSeries-GARCH%281%2C1%29-tf4109574.html#a11686281
Sent from the R help mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Thu Jul 19 12:50:26 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jul 2007 11:50:26 +0100 (BST)
Subject: [R] mfrow is ignored by some plots
In-Reply-To: <469F366C.2080702@bitwrit.com.au>
References: <469F366C.2080702@bitwrit.com.au>
Message-ID: <Pine.LNX.4.64.0707191142260.17359@gannet.stats.ox.ac.uk>

I'd ask the plotrix maintainer to fix his code!

radial.plot saves and restores all the par() values, including mfrow and 
mfg. When you do par(mfrow=c(2,2)) the plot position is reset to the 
bottom right, and the next plot will advance to the top left (but 
par(new=TRUE) negates that).

Please restore only the par values you have changed, not all the 
no.readonly ones.  That values can be set does not make it a good idea to 
do so.

On Thu, 19 Jul 2007, Jim Lemon wrote:

> Hi all,
> I was just informed that the plots in the radial.plot family in the
> plotrix package do not plot correctly when using mfrow or mfcol to
> subdivide the plot window. I found one related message, an answer from
> Deepayan Sarkar to a question about lattice graphics, but that shed no
> light on this problem.
>
> If I invoke par(mfrow=c(2,2)) and run radial.plot a few times, the plots
> appears in the upper left corner. _Sometimes_ the second plot appears in
> the upper right corner once, but then returns to the upper left. If I
> invoke par(new=TRUE), the plots appear in the lower right corner.
>
> For instance, the following just keeps writing the plots in the upper left.
>
> par(mfrow=c(2,2))
> radial.plot(rnorm(7),0:6,line.col="red",lwd=2)
> radial.plot(rnorm(7),0:6,line.col="red",lwd=2)
> radial.plot(rnorm(7),0:6,line.col="red",lwd=2)
> radial.plot(rnorm(7),0:6,line.col="red",lwd=2)
>
> If I then enter:
>
> par(new=TRUE)
>
> and repeat the plots, they all accumulate in the lower right. I think
> this may have something to do with the calls to points or lines or
> symbols within radial.plot, but I could find no information on this in
> the help for plot, par, or several other pages.
>
> Any clues?
>
> Jim
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jim at bitwrit.com.au  Thu Jul 19 13:06:17 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Thu, 19 Jul 2007 21:06:17 +1000
Subject: [R] mfrow is ignored by some plots
In-Reply-To: <Pine.LNX.4.64.0707191142260.17359@gannet.stats.ox.ac.uk>
References: <469F366C.2080702@bitwrit.com.au>
	<Pine.LNX.4.64.0707191142260.17359@gannet.stats.ox.ac.uk>
Message-ID: <469F45A9.2050407@bitwrit.com.au>

Prof Brian Ripley wrote:
> I'd ask the plotrix maintainer to fix his code!
> 
> radial.plot saves and restores all the par() values, including mfrow and 
> mfg. When you do par(mfrow=c(2,2)) the plot position is reset to the 
> bottom right, and the next plot will advance to the top left (but 
> par(new=TRUE) negates that).
> 
> Please restore only the par values you have changed, not all the 
> no.readonly ones.  That values can be set does not make it a good idea 
> to do so.
> 
Thank you Prof. Ripley, an erudite and functional answer as usual.

Jim


From markov at lyon.inserm.fr  Thu Jul 19 11:22:38 2007
From: markov at lyon.inserm.fr (Nikola Markov)
Date: Thu, 19 Jul 2007 11:22:38 +0200
Subject: [R] df manipulation
Message-ID: <469F2D5E.6090900@lyon.inserm.fr>

I have multicolumn data.frames with the first comumn giving ordinal 
observation index ( ex.: 1 4 7 9 11 13 etc). I would like to fill up the 
missing observations (i.e. 2 3 5 6 8 etc) with "NA"s.
Thank you


From meeryana at yahoo.com  Thu Jul 19 09:41:49 2007
From: meeryana at yahoo.com (copula)
Date: Thu, 19 Jul 2007 00:41:49 -0700 (PDT)
Subject: [R] R and Copula
In-Reply-To: <OFC94AAA62.BB5EBA68-ON6525731C.002B7FA8-6525731C.002CE93A@ccilindia.co.in>
References: <11612986.post@talk.nabble.com>
	<OFC94AAA62.BB5EBA68-ON6525731C.002B7FA8-6525731C.002CE93A@ccilindia.co.in>
Message-ID: <11683844.post@talk.nabble.com>


I have installed fast all packages for copula,
I have installed package 'gnml' and the others from the Jim Lindsey's web
site and the other packages like 'repeted' which is necessary for
calculating copula.
 
but when I start with copula it write: 
'gausscop' function is not found. And also 'fitted.gausscop(z)' and
'residuals.gausscop(z)'.

also what I want to say is: when I have installed package 'repeted' from Jim
Lindsey's web site in R project, this what I have received: 
 
"utils:::menuInstallLocal()
updating HTML package descriptions"

what have I to do next to solve this problem and to continue with copula.


Thanks a lot for previous help!


 

  

gyadav wrote:
> 
> 
> hi meeryana,
> 
> may be this time nobnody is responding. but dont worry you will get a lot 
> of help eventually, so always post a copy to the mailing list.
> The reason is there are a lot many newbies, although i am also not so old 
> enough, who have even the simplest questions but are hesitant to ask.
> the objective of the list is to help, and thus feel free to ask, stand and 
> contribute :-) i hope you will understand not today then tomorrow as you 
> will get associated with the mailing list more closely. Personally i have 
> found friends here. Further, never post a mail with a loose subject and 
> secondly try to maintain the thread i.e. reply to the mail which you want 
> to reply(for more details refer to the posting guide) :-) that would 
> really help. 
> 
> Yes now regarding your question :-) 
> 
> Step1 : List of all packages can be found at this link :-)
> http://cran.r-project.org/src/contrib/PACKAGES.html
> Step2: Click on the package you want to install :-) 
> http://cran.r-project.org/src/contrib/Descriptions/copula.html
> Step3: Then download the binary of your Operating system. If windows then 
> download corresponding zip file.
> for copula it is
>  http://cran.r-project.org/bin/windows/contrib/r-release/copula_0.5-3.zip
> save zip file on your system
> Step4 Open your R rpogram
> Step5: Goto Packages -> Install packages from Local Zip file
> Step6: Select your package zip file which you want to install
> Step7: Sit back and relax
> Step8: load the library using library(LibraryName) on R prompt
> 
> There are alternate ways of installing the package directly from R prompt. 
> It didn't worked for me a long time back, so  i always adopt this method.
> Somebody on the list may help you in this regards :-)
> 
> bye and learn
> Join and stand with Open Source and R community
> Cheers and Chiao, Welcome
> -gaurav
> 
> 
> 
> dear Mr. Yadav,
> 
> I want to thank for help, 
> and for that you are only who is willing to help,
> but I have one question:
> because I'm new with R project also, I think I should install a package 
> for copula.
> I have only installed R program.
> How should I install this package? And is it what I have also to do with 
> credit metrics, Value at Risk, matix and the other formulas, I mean 
> install packages.
> 
> I hope that you have a little time for me and my problem, and I hope I'm 
> not disturbing you.
> thank you for all you can do for me 
> 
> and 
> 
> best regards,
> 
> Mirjana
> 
> 
> 
> 
> gyadav wrote:
>> 
>> 
>> hi
>> 
>> see the code below i hope this will make your understanding of copulas 
>> better
>> this code plots two normal distribution and their joint distribution 
>> N[0,2] & N[0,4]
>> 
>> HTH
>> 
>> ######################code################################
>> library("copula")
>> ###################copy in two parts in R#################
>> 
>> ##################PART A##################################
>> ## construct a bivariate distribution whose marginals
>> ## are normal and Normal respectively, coupled
>> ## together via a normal copula
>> op <- par(mfrow = c(2, 2), # 2 x 2 pictures on one plot
>>           pty = "s")       # square plotting region,
>>                            # independent of device size
>> 
>> x <- mvdc(normalCopula(0.75), c("norm", "norm"),
>> list(list(mean = 0, sd =2),list(mean = 0, sd =4)))
>> x.samp <- rmvdc(x, 10000)
>> par(mfrow=c(2,3))
>> hist(x.samp[,1],xlab="Normal")
>> hist(x.samp[,2],xlab="Normal")
>> plot(x.samp[,2],x.samp[,1],pch=21,xlab="Normal",ylab="Normal")
>> 
>> plot(dmvdc(x, x.samp))
>> plot(pmvdc(x, x.samp))
>> 
>> ## At end of plotting, reset to previous settings:
>> 
>> 
>> ###########################PART B#######################
>> par(op)
>> for (i in seq(1:360)){
>> persp(x, dmvdc, xlim = c(-4, 4), ylim=c(0, 1),theta=i)
>> }
>> 
>> 
>> Regards,
>> 
>> Gaurav Yadav
>> +++++++++++
>> Assistant Manager, CCIL, Mumbai (India)
>> Mob: +919821286118 Email: emailtogauravyadav at gmail.com
>> Bhagavad Gita:  Man is made by his Belief, as He believes, so He is
>> 
>> 
>> 
>> copula <meeryana at yahoo.com> 
>> Sent by: r-help-bounces at stat.math.ethz.ch
>> 07/17/2007 12:53 PM
>> 
>> To
>> r-help at stat.math.ethz.ch
>> cc
>> 
>> Subject
>> Re: [R] R and Copula
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> it would be great when somebody will help me
>> thanks
>> 
>> 
>> copula wrote:
>>> 
>>> hi,
>>> first I want to say that I'm new here, and new with copula and R.
>>> 
>>> That is the reason why I'm writing, if somebody can help me. 
>>> 
>>> I have to make an example of Copula. 
>>> On internet I've found this forum and that copula can calculate with R.
>>> 
>>> Can somebody help me with the thing how can I start and where can read
>>> about these stuffs.
>>> 
>>> Thank to all who can help!
>>> 
>>> 
>>> 
>>> 
>> 
>> -- 
>> View this message in context: 
>> http://www.nabble.com/R-and-Copula-tf4085867.html#a11644534
>> Sent from the R help mailing list archive at Nabble.com.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> 
>> 
> ============================================================================================
>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and 
> ...{{dropped}}
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
> Quoted from:  http://www.nabble.com/R-and-Copula-tf4085867.html#a11645614
> 
> 
> 
> 
> ============================================================================================
> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/R-and-Copula-tf4085867.html#a11683844
Sent from the R help mailing list archive at Nabble.com.


From Silvia.Lipski at uni-konstanz.de  Thu Jul 19 13:16:04 2007
From: Silvia.Lipski at uni-konstanz.de (Silvia Lipski)
Date: Thu, 19 Jul 2007 13:16:04 +0200
Subject: [R] Error for TukeyHSD ?
Message-ID: <20070719131604.ateo9fyo044gc00w@webmail.uni-konstanz.de>

Hi,

one question about TukeyHSD in R:

it seems to work only for models without Error terms but not when an  
Error is specified:

Examples:

aov1<-aov(MMN_ind~(GenCond*Lang)+Error(VP),data=cutie_all)
TukeyHSD(aov1,"Lang",ordered=TRUE,conf.level = 0.95)
--> the German error  message tells me its not a method that can be  
used with TukeyHSD:
Fehler in TukeyHSD(aov1, ordered = TRUE, conf.level = 0.95) :
         keine anwendbare Methode f?r "TukeyHSD"

A model like this is no problem:
aov1<-aov(MMN_ind~(GenCond*Lang,data=cutie_all)

I am very grateful for any help!
Silvia

-- 
Dr. Silvia C. Lipski
University of Konstanz, Department of Linguistics, Neurolinguistics
Fach D 185, 78457 Konstanz, Germany
Tel: +49 7531 88-2927
Fax: +49 7531 88-2741
Email: Silvia.Lipski at uni-konstanz.de
www.uni-konstanz.de/FuF/Philo/Sprachwiss/neuroling/Lipski.html


From ripley at stats.ox.ac.uk  Thu Jul 19 13:26:27 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jul 2007 12:26:27 +0100 (BST)
Subject: [R] write.table linebreaks
In-Reply-To: <7EEB5606-9CD3-4EBA-8207-628ABFAFD7DD@systbot.uzh.ch>
References: <7EEB5606-9CD3-4EBA-8207-628ABFAFD7DD@systbot.uzh.ch>
Message-ID: <Pine.LNX.4.64.0707191215510.17995@gannet.stats.ox.ac.uk>

What do you think the 'eol' argument to write.table is for?

I don't have a Mac to hand, but eol='\r' does this on Linux and Windows.

On Thu, 19 Jul 2007, Birgit Lemcke wrote:

> Hello R users,
>
> I am a newby using R  2.5.0 on a Apple Power Book G4 with Mac OS X
> 10.4.10.
>
> when I use the write.table function, I always get the output in Unix
> linebreaks that I have to change to McIntosh linebreaks to be able to
> Import the data in Excel 2004 for Mac.
>
> Is there a possibility to do this automatically in R and respectively
> in the write.table function?
>
> Thanks in advance.
>
> Birgit
>
>
> Birgit Lemcke
> Institut f?r Systematische Botanik
> Zollikerstrasse 107
> CH-8008 Z?rich
> Switzerland
> Ph: +41 (0)44 634 8351
> birgit.lemcke at systbot.uzh.ch
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From P.Dalgaard at biostat.ku.dk  Thu Jul 19 13:37:13 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 19 Jul 2007 13:37:13 +0200
Subject: [R] Strange warning in summary.lm
In-Reply-To: <2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>	<469F2325.5010205@statistik.uni-dortmund.de>
	<2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>
Message-ID: <469F4CE9.2050709@biostat.ku.dk>

ONKELINX, Thierry wrote:
> The problem also exists in a clean workspace. But I've found the
> troublemaker. I had set options(OutDec = ","). Resetting this to
> options(OutDec = ".") solved the problem.
>
> Thanks,
>
> Thierry
>   
Oups. That sounds like there's a bug somewhere. Can you cook up a
minimal example which shows the behaviour?

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From Thierry.ONKELINX at inbo.be  Thu Jul 19 13:55:59 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Thu, 19 Jul 2007 13:55:59 +0200
Subject: [R] Strange warning in summary.lm
In-Reply-To: <469F4CE9.2050709@biostat.ku.dk>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>	<469F2325.5010205@statistik.uni-dortmund.de>
	<2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>
	<469F4CE9.2050709@biostat.ku.dk>
Message-ID: <2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be>

Dear Peter,

Here's an example. Notice the warning in the last two lines of the summary with options(OutDec = ","). It's not present with options(OutDec = ".").

Cheers,

Thierry

> x <- runif(100)
> y <- rnorm(100)
> options(OutDec = ",")
> summary(lm(y~x))

Call:
lm(formula = y ~ x)

Residuals:
      Min        1Q    Median        3Q       Max 
-2,389749 -0,607002  0,006969  0,689535  1,713197 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  0,03397    0,17774   0,191    0,849
x           -0,09219    0,29518  -0,312    0,755

Residual standard error: 0,868 on 98 degrees of freedom
Multiple R-Squared: 0.0009943,  Adjusted R-squared: -0.0092 
F-statistic: 0.09754 on 1 and 98 DF,  p-value: 0,7555 

Warning message:
NAs introduced by coercion in: as.double.default(Cf[okP]) 
> options(OutDec = ".")
> summary(lm(y~x))

Call:
lm(formula = y ~ x)

Residuals:
      Min        1Q    Median        3Q       Max 
-2.389749 -0.607002  0.006969  0.689535  1.713197 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  0.03397    0.17774   0.191    0.849
x           -0.09219    0.29518  -0.312    0.755

Residual standard error: 0.868 on 98 degrees of freedom
Multiple R-Squared: 0.0009943,  Adjusted R-squared: -0.0092 
F-statistic: 0.09754 on 1 and 98 DF,  p-value: 0.7555 


----------------------------------------------------------------------------
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: Peter Dalgaard [mailto:P.Dalgaard op biostat.ku.dk] 
> Verzonden: donderdag 19 juli 2007 13:37
> Aan: ONKELINX, Thierry
> CC: Uwe Ligges; r-help op stat.math.ethz.ch
> Onderwerp: Re: [R] Strange warning in summary.lm
> 
> ONKELINX, Thierry wrote:
> > The problem also exists in a clean workspace. But I've found the 
> > troublemaker. I had set options(OutDec = ","). Resetting this to 
> > options(OutDec = ".") solved the problem.
> >
> > Thanks,
> >
> > Thierry
> >   
> Oups. That sounds like there's a bug somewhere. Can you cook 
> up a minimal example which shows the behaviour?
> 
> -- 
>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard op biostat.ku.dk)                  FAX: 
> (+45) 35327907
> 
> 
>


From ripley at stats.ox.ac.uk  Thu Jul 19 14:13:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jul 2007 13:13:36 +0100 (BST)
Subject: [R] Strange warning in summary.lm
In-Reply-To: <469F4CE9.2050709@biostat.ku.dk>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>
	<469F2325.5010205@statistik.uni-dortmund.de>
	<2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>
	<469F4CE9.2050709@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0707191309440.18477@gannet.stats.ox.ac.uk>

On Thu, 19 Jul 2007, Peter Dalgaard wrote:

> ONKELINX, Thierry wrote:
>> The problem also exists in a clean workspace. But I've found the
>> troublemaker. I had set options(OutDec = ","). Resetting this to
>> options(OutDec = ".") solved the problem.
>>
>> Thanks,
>>
>> Thierry
>>
> Oups. That sounds like there's a bug somewhere. Can you cook up a
> minimal example which shows the behaviour?

Any use of summary.lm will do it (e.g. example(lm)).  The problem is in 
printCoefmat, at

    x0 <- (xm[okP] == 0) != (as.numeric(Cf[okP]) == 0)

and yes, it looks like an infelicity to me.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jrkrideau at yahoo.ca  Thu Jul 19 14:22:29 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Thu, 19 Jul 2007 08:22:29 -0400 (EDT)
Subject: [R] tapply
In-Reply-To: <c99f7100707190200h409ace37p4f848310ea034825@mail.gmail.com>
Message-ID: <756615.62877.qm@web32814.mail.mud.yahoo.com>

I do not understand what you want.  If aps is constant
over each class then the mean for each class is equal
to any value of aps.  

Using your example you can do 

tapply(icu1$aps, icu1$d, mean)

but it does not give you anything new.  Can you
explain the problem a bit more? 


--- sigalit mangut-leiba <smangut at gmail.com> wrote:

> hello,
> i want to compute the mean of a variable ("aps") for
> every class
> (1,2, and 3).
> every id have a few obs., "aps" and class are
> constant over id.
> like this:
> id   aps     class
> 1      11       2
> 1      11       2
> 1      11       2
> 1      11       2
> 1      11       2
> 2       8        3
> 2       8        3
> 2       8        3
> 3      12       2
> 3      12       2
> .
> .
> 
> i tried:
> 
> tapply(icu1$aps_st, icu1$hidclass, function(z)
> mean(unique(z)))
> 
> but it's counting every row and not every id.
> 
> thank you,
> 
> Sigalit.


From rmh at temple.edu  Thu Jul 19 14:24:50 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 19 Jul 2007 08:24:50 -0400 (EDT)
Subject: [R] Error for TukeyHSD ?
Message-ID: <20070719082450.CGZ55135@po-d.temple.edu>

TukeyHSD, and glht in the multcomp package, are designed for
aov objects.  They do not have methods for aovlist objects.

The workaround is to download and install the HH package and look at the
maiz example.

library(HH)
?MMC

In that example I show how to get the same sequential sum of squares
without the Error() term as you would get with the Error term.

Rich


From Joao.Fadista at agrsci.dk  Thu Jul 19 14:31:30 2007
From: Joao.Fadista at agrsci.dk (=?iso-8859-1?Q?Jo=E3o_Fadista?=)
Date: Thu, 19 Jul 2007 14:31:30 +0200
Subject: [R] test about distribution of data in a single population
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F3A@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/b04efa84/attachment.pl 

From dataanalytics at earthlink.net  Thu Jul 19 14:39:10 2007
From: dataanalytics at earthlink.net (Walter Paczkowski)
Date: Thu, 19 Jul 2007 08:39:10 -0400 (GMT-04:00)
Subject: [R] multinomial logit estimation
Message-ID: <20668019.1184848750022.JavaMail.root@elwamui-darkeyed.atl.sa.earthlink.net>

Good morning,

I'd like to estimate a simple multinomial logit model in R (not a McFadden conditional logit).  For instance, I'd like to estimate the probability of someone having one of eight titles in a company with the independent variables being the company characteristics.  A binary logit is well documented.  What about the multinomial?

Thanks,

Walt Paczkowski


From P.Dalgaard at biostat.ku.dk  Thu Jul 19 14:39:54 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 19 Jul 2007 14:39:54 +0200
Subject: [R] Strange warning in summary.lm
In-Reply-To: <Pine.LNX.4.64.0707191309440.18477@gannet.stats.ox.ac.uk>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>	<469F2325.5010205@statistik.uni-dortmund.de>	<2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>	<469F4CE9.2050709@biostat.ku.dk>
	<Pine.LNX.4.64.0707191309440.18477@gannet.stats.ox.ac.uk>
Message-ID: <469F5B9A.2080502@biostat.ku.dk>

Prof Brian Ripley wrote:
> On Thu, 19 Jul 2007, Peter Dalgaard wrote:
>
>   
>> ONKELINX, Thierry wrote:
>>     
>>> The problem also exists in a clean workspace. But I've found the
>>> troublemaker. I had set options(OutDec = ","). Resetting this to
>>> options(OutDec = ".") solved the problem.
>>>
>>> Thanks,
>>>
>>> Thierry
>>>
>>>       
>> Oups. That sounds like there's a bug somewhere. Can you cook up a
>> minimal example which shows the behaviour?
>>     
>
> Any use of summary.lm will do it (e.g. example(lm)).  The problem is in 
> printCoefmat, at
>
>     x0 <- (xm[okP] == 0) != (as.numeric(Cf[okP]) == 0)
>
> and yes, it looks like an infelicity to me.
>
>   
Ick. Any better ideas than

printsAs0 <- scan(con <- textConnection(Cf[okP), dec=options("outDec")) ; close(con)
x0 <- (xm[okP] == 0) != printsAs0 

?

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From wwwhsd at gmail.com  Thu Jul 19 14:46:36 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 19 Jul 2007 09:46:36 -0300
Subject: [R] df manipulation
In-Reply-To: <469F2D5E.6090900@lyon.inserm.fr>
References: <469F2D5E.6090900@lyon.inserm.fr>
Message-ID: <da79af330707190546l289da815u8aada888f8a620b0@mail.gmail.com>

Hi, see below:

> df
  index value
1     1     1
2     4     6
3     7     4
4     9     5
5    11     3
6    13     2

> foo <- function(x){
+     index <- ifelse(x %in% df$index, df$value[which(df$index %in% x)], NA)
+     return(index)
+ }

> df_ok <- data.frame(index=1:13, value=sapply(1:13, foo))
> df_ok
   index value
1      1     1
2      2    NA
3      3    NA
4      4     6
5      5    NA
6      6    NA
7      7     4
8      8    NA
9      9     5
10    10    NA
11    11     3
12    12    NA
13    13     2

-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


On 19/07/07, Nikola Markov <markov at lyon.inserm.fr> wrote:
> I have multicolumn data.frames with the first comumn giving ordinal
> observation index ( ex.: 1 4 7 9 11 13 etc). I would like to fill up the
> missing observations (i.e. 2 3 5 6 8 etc) with "NA"s.
> Thank you
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ccleland at optonline.net  Thu Jul 19 14:49:27 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 19 Jul 2007 08:49:27 -0400
Subject: [R] multinomial logit estimation
In-Reply-To: <20668019.1184848750022.JavaMail.root@elwamui-darkeyed.atl.sa.earthlink.net>
References: <20668019.1184848750022.JavaMail.root@elwamui-darkeyed.atl.sa.earthlink.net>
Message-ID: <469F5DD7.6070208@optonline.net>

Walter Paczkowski wrote:
> Good morning,
> 
> I'd like to estimate a simple multinomial logit model in R (not a McFadden conditional logit).  For instance, I'd like to estimate the probability of someone having one of eight titles in a company with the independent variables being the company characteristics.  A binary logit is well documented.  What about the multinomial?
> 
> Thanks,
> 
> Walt Paczkowski
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

  Have you considered the results of

RSiteSearch("multinomial", restrict="function")

  which point to a number of relevant packages and functions?

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ligges at statistik.uni-dortmund.de  Thu Jul 19 14:53:06 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jul 2007 14:53:06 +0200
Subject: [R] Strange warning in summary.lm
In-Reply-To: <2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>	<469F2325.5010205@statistik.uni-dortmund.de>	<2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>	<469F4CE9.2050709@biostat.ku.dk>
	<2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be>
Message-ID: <469F5EB2.90709@statistik.uni-dortmund.de>

Ah, in  printCoefmat() there is Cf which is at some point:

Cf

             Estimate  Std. Error t value  Pr(>|t|)
(Intercept) " 0,2542" " 0,1875"  " 1,356" ""
x           "-0,5346" " 0,3463"  "-1,544" ""

and

cat(Cf)
    0,2542 -0,5346  0,1875  0,3463  1,356 -1,544

(i.e. values are character) and then
    x0 <- (xm[okP] == 0) != (as.numeric(Cf[okP]) == 0)
includes as.numeric(Cf[okP]) which won't work any more.



I think this design is not the best possible. Nevertheless, the real 
problem is in format(), because it does not use its default 
`decimal.mark = "."' if options(OutDec = ",") has been set, and even 
ignores manual chosen values:


# nice:
  options(OutDec = ".")
  format(0.1, decimal.mark = ":") #  [1] "0:1"

# bug:
  options(OutDec = ",")
  format(0.1, decimal.mark = ":") #  [1] "0,1"

Best,
Uwe







ONKELINX, Thierry wrote:
> Dear Peter,
> 
> Here's an example. Notice the warning in the last two lines of the summary with options(OutDec = ","). It's not present with options(OutDec = ".").
> 
> Cheers,
> 
> Thierry
> 
>> x <- runif(100)
>> y <- rnorm(100)
>> options(OutDec = ",")
>> summary(lm(y~x))
> 
> Call:
> lm(formula = y ~ x)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max 
> -2,389749 -0,607002  0,006969  0,689535  1,713197 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  0,03397    0,17774   0,191    0,849
> x           -0,09219    0,29518  -0,312    0,755
> 
> Residual standard error: 0,868 on 98 degrees of freedom
> Multiple R-Squared: 0.0009943,  Adjusted R-squared: -0.0092 
> F-statistic: 0.09754 on 1 and 98 DF,  p-value: 0,7555 
> 
> Warning message:
> NAs introduced by coercion in: as.double.default(Cf[okP]) 
>> options(OutDec = ".")
>> summary(lm(y~x))
> 
> Call:
> lm(formula = y ~ x)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max 
> -2.389749 -0.607002  0.006969  0.689535  1.713197 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  0.03397    0.17774   0.191    0.849
> x           -0.09219    0.29518  -0.312    0.755
> 
> Residual standard error: 0.868 on 98 degrees of freedom
> Multiple R-Squared: 0.0009943,  Adjusted R-squared: -0.0092 
> F-statistic: 0.09754 on 1 and 98 DF,  p-value: 0.7555 
> 
> 
> ----------------------------------------------------------------------------
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be 
> 
> Do not put your faith in what statistics say until you have carefully considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions. ~M.J.Moroney
> 
>  
> 
>> -----Oorspronkelijk bericht-----
>> Van: Peter Dalgaard [mailto:P.Dalgaard at biostat.ku.dk] 
>> Verzonden: donderdag 19 juli 2007 13:37
>> Aan: ONKELINX, Thierry
>> CC: Uwe Ligges; r-help at stat.math.ethz.ch
>> Onderwerp: Re: [R] Strange warning in summary.lm
>>
>> ONKELINX, Thierry wrote:
>>> The problem also exists in a clean workspace. But I've found the 
>>> troublemaker. I had set options(OutDec = ","). Resetting this to 
>>> options(OutDec = ".") solved the problem.
>>>
>>> Thanks,
>>>
>>> Thierry
>>>   
>> Oups. That sounds like there's a bug somewhere. Can you cook 
>> up a minimal example which shows the behaviour?
>>
>> -- 
>>    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
>>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>>  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
>> (+45) 35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
>> (+45) 35327907
>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Jul 19 15:01:48 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jul 2007 15:01:48 +0200
Subject: [R] Subsetting dataframes
In-Reply-To: <61510.194.17.178.40.1184838729.squirrel@webmail.slu.se>
References: <61510.194.17.178.40.1184838729.squirrel@webmail.slu.se>
Message-ID: <469F60BC.9050603@statistik.uni-dortmund.de>



CG Pettersson wrote:
> Dear all!
> 
> W2k, R 2.5.1
> 
> I am working with an ongoing malting barley variety evaluation within
> Sweden. The structure is 25 cultivars tested each year at four sites, in
> field trials with three replicates and 'lattice' structure (the replicates
> are divided into five sub blocks in a structured way). As we are normally
> keeping around 15 varieties from each year to the next, and take in 10 new
> for next year, we have tested totally 72 different varieties during five
> years.
> 
> I store the data in a field trial database, and generate text tables with
> the subset of data I want and import the frame to R. I take in all
> cultivars in R and use 'subset' to select what I want to look at. Using
> lme{nlme} works with no problems to get mean results over the years, but
> as I now have a number of years I want to analyse the general site x
> cultivar relation. I am testing AMMI{agricolae} for this and it seems to
> work except for the subsetting. This is what happens:
> 
> If I do the subsetting like this:
> 
> x62_samvar <- subset(x62_5, cn %in%
> c("Astoria","Barke","Christina","Makof", "Prestige","Publican","Quench"))
> 
> A test run with AMMI seems to work in the first part:
> 
>> AMMI(site, cn, rep, yield)
> 
> ANALYSIS AMMI:  yield
> Class level information
> 
> ENV:  Hag Klb Bjt Ska
> GEN:  Astoria Prestige Makof Christina Publican Quench
> REP:  1 2 3
> 
> Number of observations:  240
> 
> model Y: yield  ~ ENV + REP%in%ENV + GEN + ENV:GEN
> 
> Analysis of Variance Table
> 
> Response: Y
>            Df    Sum Sq   Mean Sq F value    Pr(>F)
> ENV         3 120092418  40030806 90.0424 1.665e-06 ***
> REP(ENV)    8   3556620    444578  0.5674  0.803923
> GEN         5  21376142   4275228  5.4564 9.680e-05 ***
> ENV:GEN    15  28799807   1919987  2.4504  0.002555 **
> Residuals 208 162973213    783525
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Coeff var       Mean yield
> 13.08629         6764.098
> 
> After this something goes wrong, as AMMI finds a cultivar name not
> selected in the subsetting. (The plotting might go wrong anyhow, but I
> haven?t got that far yet):
> 
> Error in model.frame.default(Terms, newdata, na.action = na.action, xlev =
> object$xlevels) :
>         factor 'y' has new level(s) Arkadia
> 
> 
> Looking at the dataframe using
> 
>> edit(x62_samvar)
> 
> only shows the selected lines, but using levels() gives another answer as
> 
>> levels(x62_samvar$cn)
> 
> gives back all 72 cultivar names used during the five years (starting with
> Arcadia).
> 
> Where do I go wrong and how do I use subset in a proper way?


So you have to drop the levels you are excluding. Example:

   x <- factor(letters[1:4])
   x
   x[1:2]
   x[1:2, drop=TRUE]


Uwe Ligges




> Thanks
> /CG
>


From bady at univ-lyon1.fr  Thu Jul 19 15:05:03 2007
From: bady at univ-lyon1.fr (bady at univ-lyon1.fr)
Date: Thu, 19 Jul 2007 15:05:03 +0200
Subject: [R] multinomial logit estimation
In-Reply-To: <20668019.1184848750022.JavaMail.root@elwamui-darkeyed.atl.sa.earthlink.net>
References: <20668019.1184848750022.JavaMail.root@elwamui-darkeyed.atl.sa.earthlink.net>
Message-ID: <1184850303.469f617fb457c@webmail.univ-lyon1.fr>

Hi all,

You can consult help of the functions multinom (library(MASS))
and vglm (library(VGAM)).

hope this help,

Pierre




Selon Walter Paczkowski <dataanalytics at earthlink.net>:

> Good morning,
>
> I'd like to estimate a simple multinomial logit model in R (not a McFadden
> conditional logit).  For instance, I'd like to estimate the probability of
> someone having one of eight titles in a company with the independent
> variables being the company characteristics.  A binary logit is well
> documented.  What about the multinomial?
>
> Thanks,
>
> Walt Paczkowski
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From ligges at statistik.uni-dortmund.de  Thu Jul 19 15:08:35 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jul 2007 15:08:35 +0200
Subject: [R] df manipulation
In-Reply-To: <469F2D5E.6090900@lyon.inserm.fr>
References: <469F2D5E.6090900@lyon.inserm.fr>
Message-ID: <469F6253.6010805@statistik.uni-dortmund.de>



Nikola Markov wrote:
> I have multicolumn data.frames with the first comumn giving ordinal 
> observation index ( ex.: 1 4 7 9 11 13 etc). I would like to fill up the 
> missing observations (i.e. 2 3 5 6 8 etc) with "NA"s.

Please specify reproducible examples, they make it much easier to help!


If you have:

df <- data.frame(index = c(1, 4, 7), x1 = 1:3, x2 = 3:1)
df

Then you can say:

temp <- data.frame(index = 1:max(df$index))
df <- merge(temp, df, all.x = TRUE)
df

Uwe Ligges





> Thank you
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Jul 19 15:18:29 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Jul 2007 15:18:29 +0200
Subject: [R] Error: evaluation nested too deeply when doing heatmap with
 binary distfunction
In-Reply-To: <BAY110-F26DE1AF7405ACF3A6A47A7C7FB0@phx.gbl>
References: <BAY110-F26DE1AF7405ACF3A6A47A7C7FB0@phx.gbl>
Message-ID: <469F64A5.6030103@statistik.uni-dortmund.de>



zhihua li wrote:
> Hi netters,
> 
> I have a matrix X of the size (1000,100). The values are from -3 to +3.  
> When I tried
> 
> heatmap(X, 
> distfun=function(c),dist(c,method="bin"),hclustfun=function(m),hclust(m,method="average")) 
> 
> 
> 
> I got the error message: Error: evaluation nested too deeply: infinite 
> recursion / options(expressions=)?



So, does it help to increase the thresholds?
If not, please specify a easily reproducible example that helps us to 
investigate your problem.

Best,
Uwe Ligges




> However, if I used default parameters for distfunction:
> heatmap(X, hclustfun=function(m),hclust(m,method="average"))
> there is no error messages at all.
> 
> But the problem is that I have to use binary method in my disfunction. 
> How can I resolve the problem?
> 
> Thanks a lot!
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Thu Jul 19 15:27:54 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Jul 2007 14:27:54 +0100 (BST)
Subject: [R] Strange warning in summary.lm
In-Reply-To: <469F5B9A.2080502@biostat.ku.dk>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>
	<469F2325.5010205@statistik.uni-dortmund.de>
	<2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>
	<469F4CE9.2050709@biostat.ku.dk>
	<Pine.LNX.4.64.0707191309440.18477@gannet.stats.ox.ac.uk>
	<469F5B9A.2080502@biostat.ku.dk>
Message-ID: <Pine.LNX.4.64.0707191400070.19093@gannet.stats.ox.ac.uk>

On Thu, 19 Jul 2007, Peter Dalgaard wrote:

> Prof Brian Ripley wrote:
>> On Thu, 19 Jul 2007, Peter Dalgaard wrote:
>>
>>
>>> ONKELINX, Thierry wrote:
>>>
>>>> The problem also exists in a clean workspace. But I've found the
>>>> troublemaker. I had set options(OutDec = ","). Resetting this to
>>>> options(OutDec = ".") solved the problem.
>>>>
>>>> Thanks,
>>>>
>>>> Thierry
>>>>
>>>>
>>> Oups. That sounds like there's a bug somewhere. Can you cook up a
>>> minimal example which shows the behaviour?
>>>
>>
>> Any use of summary.lm will do it (e.g. example(lm)).  The problem is in
>> printCoefmat, at
>>
>>     x0 <- (xm[okP] == 0) != (as.numeric(Cf[okP]) == 0)
>>
>> and yes, it looks like an infelicity to me.
>>
>>
> Ick. Any better ideas than
>
> printsAs0 <- scan(con <- textConnection(Cf[okP), dec=options("outDec")) ; close(con)
> x0 <- (xm[okP] == 0) != printsAs0
>
> ?

Yes, several.  That assumes that getOption("OutDec") (which is what you 
need) is a single char since scan() does.  If so, chartr() will do the job 
and more generally sub() would.  Alternatively, we could figure out what 
representations of zero are possible and grep for them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ajayshah at mayin.org  Thu Jul 19 15:44:41 2007
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 19 Jul 2007 19:14:41 +0530
Subject: [R] maximum likelihood estimation
Message-ID: <20070719134441.GH210@lubyanka.local>

> I need to perform maximum likelihood estimation on R, but I am not sure
> which command to use. I searched on google, and found an example using the
> function mlogl, but I couldn't find the package on R. Is there such
> function? Or how should i perform my mle?

http://www.mayin.org/ajayshah/KB/R/documents/mle/mle.html might help.

-- 
Ajay Shah                                      http://www.mayin.org/ajayshah  
ajayshah at mayin.org                             http://ajayshahblog.blogspot.com
<*(:-? - wizard who doesn't know the answer.


From wwwhsd at gmail.com  Thu Jul 19 16:04:32 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Thu, 19 Jul 2007 11:04:32 -0300
Subject: [R] tapply
In-Reply-To: <c99f7100707190200h409ace37p4f848310ea034825@mail.gmail.com>
References: <c99f7100707190200h409ace37p4f848310ea034825@mail.gmail.com>
Message-ID: <da79af330707190704m5ef64e40h3b0bcc24ca3e30d7@mail.gmail.com>

I also don't understand, but perhaps:

with(df, tapply(aps, list(class, id), mean))


-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O


On 19/07/07, sigalit mangut-leiba <smangut at gmail.com> wrote:
> hello,
> i want to compute the mean of a variable ("aps") for every class
> (1,2, and 3).
> every id have a few obs., "aps" and class are constant over id.
> like this:
> id   aps     class
> 1      11       2
> 1      11       2
> 1      11       2
> 1      11       2
> 1      11       2
> 2       8        3
> 2       8        3
> 2       8        3
> 3      12       2
> 3      12       2
> .
> .
>
> i tried:
>
> tapply(icu1$aps_st, icu1$hidclass, function(z) mean(unique(z)))
>
> but it's counting every row and not every id.
>
> thank you,
>
> Sigalit.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From epistat at gmail.com  Thu Jul 19 16:07:41 2007
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 19 Jul 2007 22:07:41 +0800
Subject: [R] Can I test if there are statistical significance between
	different rows in R*C table?
Message-ID: <2fc17e30707190707u6856591al7a0b883e6ff7a236@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/a77f3a8a/attachment.pl 

From claudio.is at libero.it  Thu Jul 19 16:12:05 2007
From: claudio.is at libero.it (claudio.is at libero.it)
Date: Thu, 19 Jul 2007 16:12:05 +0200
Subject: [R] plot centered line on barplot
Message-ID: <JLFJG5$2C2CCDF74EEB7C6120E4BDAE688BF15A@libero.it>

Dear R user,

I need plot an histogram for the occurrence of a dataset, and then add a line corresponding to the freuqnecy of another similar dataset. in order to do this i used the function 
> hist_data1=hist(data1, breaks= seq(0,50,5), plot=FALSE)
> hist_data2=hist(data2, breaks= seq(0,50,5), plot=FALSE)

then I plotted the frequency

> barplot(hist_data1$density)
> lines(hist_data1$density)

but the line is shifted in respect to the center of the bars. how can I properly plot the line? another question. this is easy, how can I smooth the curve (not fit with loess of spline)?


tnx

--
Claudio


From tobias.minder at bluewin.ch  Thu Jul 19 16:13:48 2007
From: tobias.minder at bluewin.ch (squall44)
Date: Thu, 19 Jul 2007 07:13:48 -0700 (PDT)
Subject: [R] (R) Using arguments for the empirical cumulative distribution
 function
Message-ID: <11690150.post@talk.nabble.com>


Hi,

I have just started using R. Now I have the following problem:

I want to create an Empirical Cumulative Distribution Function and I only
came so far:

F10 <- ecdf(x)
plot(F10, verticals= TRUE, do.p = TRUE, lwd=3)
x=c(1.6,1.8,2.4,2.7,2.9,3.3,3.4,3.4,4,5.2)

Now I'd like to use arguments such as xlabs and main but I don't know how to
integrate them.

I hope someone can help me, I am really stuck!

-- 
View this message in context: http://www.nabble.com/%28R%29-Using-arguments-for-the-empirical-cumulative-distribution-function-tf4111355.html#a11690150
Sent from the R help mailing list archive at Nabble.com.


From cg.pettersson at vpe.slu.se  Thu Jul 19 16:20:07 2007
From: cg.pettersson at vpe.slu.se (CG Pettersson)
Date: Thu, 19 Jul 2007 16:20:07 +0200 (CEST)
Subject: [R] Subsetting dataframes
In-Reply-To: <469F60BC.9050603@statistik.uni-dortmund.de>
References: <61510.194.17.178.40.1184838729.squirrel@webmail.slu.se>
	<469F60BC.9050603@statistik.uni-dortmund.de>
Message-ID: <58317.194.17.178.40.1184854807.squirrel@webmail.slu.se>

Thanks a lot.
But an ignorant R user, like me, needed the code example from Jim Holtman
posted outside the list earlier today to understand that:

x62_samvar$cn <- x62_samvar$cn[,drop=TRUE]

was the way to code. Thank you both!

/CG


On Thu, July 19, 2007 3:01 pm, Uwe Ligges said:
>
>
> CG Pettersson wrote:
>> Dear all!
>>
>> W2k, R 2.5.1
>>
>> I am working with an ongoing malting barley variety evaluation within
>> Sweden. The structure is 25 cultivars tested each year at four sites, in

/snip

>>
>> Where do I go wrong and how do I use subset in a proper way?
>
>
> So you have to drop the levels you are excluding. Example:
>
>    x <- factor(letters[1:4])
>    x
>    x[1:2]
>    x[1:2, drop=TRUE]
>
>
> Uwe Ligges
>
>
>
>
>> Thanks
>> /CG
>>
>


-- 
CG Pettersson, PhD
Swedish University of Agricultural Sciences (SLU)
Dept. of Crop Production Ecology. Box 7043.
SE-750 07 Uppsala, Sweden
cg.pettersson at vpe.slu.se


From lzhtom at hotmail.com  Thu Jul 19 16:30:58 2007
From: lzhtom at hotmail.com (zhihua li)
Date: Thu, 19 Jul 2007 14:30:58 +0000
Subject: [R] Error: evaluation nested too deeply when doing heatmap with
 binary distfunction
In-Reply-To: <644e1f320707182119w3e98a172w2f861f5f760b1933@mail.gmail.com>
Message-ID: <BAY110-F31F118E59419EC694A3840C7FB0@phx.gbl>

Sorry, that was a typo.  Actually there wasn't a comma after 'function(m)' 
in my expression.
So I'll try to increase the threshould to see if that works.

Thanks a lot!


>From: "jim holtman" <jholtman at gmail.com>
>To: "zhihua li" <lzhtom at hotmail.com>
>Subject: Re: [R] Error: evaluation nested too deeply when doing heatmap 
with binary distfunction
>Date: Thu, 19 Jul 2007 00:19:54 -0400
>
>you seem to have a syntax error in your statement.  There appears to
>be an extra commas after 'function(m),'.  I think is should be:
>
>heatmap(X,
>    
>distfun=function(c)dist(c,method="bin"),hclustfun=function(m)hclust(m,method="average"))

>
>
>On 7/18/07, zhihua li <lzhtom at hotmail.com> wrote:
>>Hi netters,
>>
>>I have a matrix X of the size (1000,100). The values are from -3 to 
>>+3.
>>When I tried
>>
>>heatmap(X,
>>distfun=function(c),dist(c,method="bin"),hclustfun=function(m),hclust(m,method="average"))

>>
>>
>>I got the error message:
>>Error: evaluation nested too deeply: infinite recursion /
>>options(expressions=)?
>>
>>However, if I used default parameters for distfunction:
>>heatmap(X, hclustfun=function(m),hclust(m,method="average"))
>>there is no error messages at all.
>>
>>But the problem is that I have to use binary method in my 
>>disfunction. How
>>can I resolve the problem?
>>
>>Thanks a lot!
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>
>--
>Jim Holtman
>Cincinnati, OH
>+1 513 646 9390
>
>What is the problem you are trying to solve?


From Jung.Bae.Kim at morganstanley.com  Thu Jul 19 16:33:11 2007
From: Jung.Bae.Kim at morganstanley.com (JB Kim)
Date: Thu, 19 Jul 2007 10:33:11 -0400
Subject: [R] dates() is a great date function in R
In-Reply-To: <1184789629.2879.14.camel@graptoleberis.geog.ucl.ac.uk>
References: <11675205.post@talk.nabble.com>
	<1184789629.2879.14.camel@graptoleberis.geog.ucl.ac.uk>
Message-ID: <20070719143311.GK15468@pi941c2n1.ms.com>

This is great.

I haven't seen "as.Date" function before, and was using "as.date" from library(date).
(note the lowercase 'd')

I have an alternative which might or might not be faster...
If the date is formatted "yyyymmdd" (e.g. 20070719)

	library(date)
  formatted <- gsub("^(\\d{4})(\\d{2})(\\d{2})$", "\\2-\\3-\\1", d$yyyymmdd, perl=TRUE)
  d$dates <- as.date(formatted)

Since as.date only accepts certain type of date formats, I had to use gsub to
reshuffle the date substrings around.

as.Date returns the objects of class "Date", whereas as.date returns the objects of class "date".
Not sure what the differences are, but a simple test below shows that as.date conversion is
slightly faster, given a character vector of 10000 date entries.



FYI,

I ran a quick performance comparison test on a 64bit linux machine on 2.6.9 kernel.
The test is very rudimentary, but hopefully useful...

I have two scripts:


######################################  as.date_test.R  ##############################################

library(date)

d <- read.table("/tmp/dates", as.is = TRUE, col.names = c("yyyymmdd"),  colClasses = c("character"))
formatted <- gsub("^(\\d{4})(\\d{2})(\\d{2})$", "\\2-\\3-\\1", d$yyyymmdd, perl=TRUE)
d$dates <- as.date(formatted)

print(nrow(d))
print(d$dates[1:3])

######################################################################################################


######################################  as.Date_test.R  ##############################################

d <- read.table("/tmp/dates", as.is = TRUE, col.names = c("yyyymmdd"), colClasses = c("character"))

d$dates <- as.Date(d$yyyymmdd, format = "%Y%m%d")

print(nrow(d))
print(d$dates[1:3])

######################################################################################################


Both scripts read in the same text file containing 10000 date strings, and then convert them into 
appropriate date objects.


# 10000 date records in a flat file
<jbkim at mymachine>$ wc -l /tmp/dates
   10000 /tmp/dates

# just to illustrate what the dates look like
<jbkim at mymachine>$ head -2 /tmp/dates
19900817
19900820


# Running the test script 5 times each

<jbkim at mymachine>$ for i in 1 2 3 4 5; do time R --vanilla < as.date_test.R > /dev/null; done

real    0m1.29s
user    0m1.23s
sys     0m0.05s

real    0m1.28s
user    0m1.23s
sys     0m0.06s

real    0m1.28s
user    0m1.22s
sys     0m0.06s

real    0m1.29s
user    0m1.22s
sys     0m0.06s

real    0m1.28s
user    0m1.21s
sys     0m0.07s

<jbkim at mymachine>$ for i in 1 2 3 4 5; do time R --vanilla < as.Date_test.R > /dev/null; done

real    0m1.65s
user    0m0.99s
sys     0m0.64s

real    0m1.64s
user    0m0.98s
sys     0m0.66s

real    0m1.63s
user    0m0.98s
sys     0m0.65s

real    0m1.64s
user    0m1.00s
sys     0m0.64s

real    0m1.64s
user    0m0.98s
sys     0m0.65s



Notice that as.date conversion is silghtly faster than as.Date conversion, on average...

Just thought it was interesting to share.
(and thanks Mark Leeds for reference)

Regards,
JB

On 07/18/07 16:13:49, Gavin Simpson wrote:
> On Wed, 2007-07-18 at 12:14 -0700, Mr Natural wrote: 
> > Proper calendar dates in R are great for plotting and calculating. 
> > However for the non-wonks among us, they can be very frustrating.
> > I have recently discussed the pains that people in my lab have had 
> > with dates in R. Especially the frustration of bringing date data into R 
> > from Excel, which we have to do a lot. 
> 
> I've always found the following reasonably intuitive:
> 
> Given the csv file that I've pasted in below, the following reads the
> csv file in, formats the dates and class Date and then draws a plot.
> 
> I have dates in DD/MM/YYYY format so year is not first - thus attesting
> to R not hating dates in this format ;-)
> 
> ## read in csv data
> ## as.is = TRUE stops characters being converted to factors
> ## thus saving us an extra step to convert them back
> dat <- read.csv("date_data.csv", as.is = TRUE)
> 
> ## we convert to class Date
> ## format tells R how the dates are formatted in our character strings
> ## see ?strftime for the meaning and available codes
> dat$Date <- as.Date(dat$Date, format = "%d/%m/%Y")
> 
> ## check this worked ok
> str(dat$Date)
> dat$Date
> 
> ## see nicely formatted dates and not a drop of R-related hatred 
> ## but just about the most boring graph I could come up with
> plot(Data ~ Date, dat, type = "l")
> 
> And you can keep your Excel file formatted as dates as well - bonus!
> 
> Oh, and before you get "Martin'd", it is the chron *package*!
> 
> HTH
> 
> G
> 
> CSV file I used, generated in OpenOffice.org, but I presume it stores
> Dates in the same way as Excel?:
> 
> "Data","Date"
> 1,01/01/2007
> 2,02/01/2007
> 3,03/01/2007
> 4,04/01/2007
> 5,05/01/2007
> 6,06/01/2007
> 7,07/01/2007
> 8,08/01/2007
> 9,09/01/2007
> 10,10/01/2007
> 11,11/01/2007
> 10,12/01/2007
> 9,13/01/2007
> 8,14/01/2007
> 7,15/01/2007
> 6,16/01/2007
> 5,17/01/2007
> 4,18/01/2007
> 3,19/01/2007
> 2,20/01/2007
> 1,21/01/2007
> 1,22/01/2007
> 2,23/01/2007
> 3,24/01/2007
> 
> > Please find below a simple analgesic for R date importation that I
> > discovered 
> > over the last 1.5 days (Learning new stuff in R is calculated in 1/2 days).
> > 
> > The function    dates()    gives the simplest way to get calendar dates into
> > R from Excel that I can find.
> > But straight importation of Excel dates, via a csv or txt file, can be a a
> > huge pain (I'll give details for anyone who cares to know). 
> > 
> > My pain killer is:
> > Consider that you have Excel columns in month, day, year format. Note that R
> > hates date data that does not lead with the year. 
> > 
> > a. Load the chron library by typing   library(chron)   in the console.
> > You know that you need this library from information revealed by 
> > performing the query,
> > ?dates()"    in the Console window. This gives the R documentation 
> > help file for this and related time, date functions.  In the upper left 
> > of the documentation, one sees "dates(chron)". This tells you that you
> > need the library chron. 
> > 
> > b. Change the format "dates" in Excel to format "general", which gives 
> > 5 digit Julian dates. Import the csv file (I use    read.csv()  with the 
> > Julian dates and other data of interest.
> > 
> > c.  Now, change the Julian dates that came in with the csv file into 
> > calendar dates with the    dates() function. Below is my code for performing 
> > this activity, concerning an R data file called ss,
> > 
> > ss holds the Julian dates, illustrated below from the column MPdate,
> > 
> > >ss$MPdate[1:5]
> > [1] 34252 34425 34547 34759 34773
> > 
> > The dates() function makes calendar dates from Julian dates,
> > 
> > >dmp<-dates(ss$MPdate,origin=c(month = 1, day = 1, year = 1900))
> > 
> > > dmp[1:5]
> > [1] 10/12/93 04/03/94 08/03/94 03/03/95 03/17/95
> > 
> > I would appreciate the comments of more sophisticated programmers who
> > can suggest streamlining or shortcutting this operation.
> > 
> > regards, Don
> > 
> > 
> > 
> >  
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679 0522
>  ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
>  UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
JB Kim
Morgan Stanley, METL
1585 - 9th Floor
New York, NY 10036


From lterlemez at anadolu.edu.tr  Thu Jul 19 16:36:41 2007
From: lterlemez at anadolu.edu.tr (Levent TERLEMEZ)
Date: Thu, 19 Jul 2007 17:36:41 +0300
Subject: [R] A small problem with as.timeSeries() function...
Message-ID: <3A7E721C48B04241A913670D21075F4D@anadolu.edu.tr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/cdb0e056/attachment.pl 

From elyakhlifi_mustapha at yahoo.fr  Thu Jul 19 16:52:59 2007
From: elyakhlifi_mustapha at yahoo.fr (elyakhlifi mustapha)
Date: Thu, 19 Jul 2007 14:52:59 +0000 (GMT)
Subject: [R] tukey or Neuman-Keuls
Message-ID: <689695.32939.qm@web27504.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/92572cb8/attachment.pl 

From tlumley at u.washington.edu  Thu Jul 19 16:54:48 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 19 Jul 2007 07:54:48 -0700 (PDT)
Subject: [R] R
In-Reply-To: <9c946f8e0707190003h6a97df0dhcf3a3a6c378dc876@mail.gmail.com>
References: <9c946f8e0707190003h6a97df0dhcf3a3a6c378dc876@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707190740550.4038@homer23.u.washington.edu>

On Thu, 19 Jul 2007, Fluss wrote:
> Hello!
> I am using for logistic regression in survey data the svyglm procedure.
> I wondered how does the strata effect estimates SE (in addition to the
> weights given proportional to population size).
> I know that for simple regression measurements of each strata is assumed to
> have different variance.
> But in a logistic model this is not the case.

It is simpler (and more complicated) than that.  The survey package uses 
the same formula for nearly all designs and estimators, so it doesn't 
have to handle special cases like estimating separate stratum variances 
for stratified models.

For a population total the variance estimator is just the Horvitz-Thompson 
estimator.  Other estimators are defined by the estimating equations they 
solve, so the mean solves
       sum_i w_i(x_i-mu) = 0
and logistic regression solves
       sum_i w_ix_i(y_i-mu_i) = 0

We now compute the Horvitz-Thompson estimator for the sum of the 
estimating functions (V) and also the population total of the derivative 
of the estimating functions (D). The variance of the estimator is
   D^{-1}VD^{-1}


The standard reference for this in the survey literature seems to be
      Binder, David A. (1983).  On the variances of asymptotically
      normal estimators from complex surveys.  International Statistical
      Review, 51, 279-292.
which is in the References section of help(svyrecvar).

 	-thomas


From eric at net2000.ch  Thu Jul 19 17:36:24 2007
From: eric at net2000.ch (eric at net2000.ch)
Date: Thu, 19 Jul 2007 17:36:24 +0200
Subject: [R] [R-sig-DB] RODBC on Oracle DB
Message-ID: <2117.1184859384@net2000.ch>

Yes, it's a quesion of rights. My system administrator just confirme that.
And I also try with a small Oracle client, and same as R, no permission to read
tables. 
On R with RODBC I could list the tables but no permission to read or import tables.
But I get confused because I was using the same Oracle user with an MS Access
client, wich works perfectly even to importe table from Oracle DB.
My system admin told me that this Oracle user is in fact a synomyme or duplicata
from the real Oracle account. But why this very same account can import table via
the MS Access client ? This remains mystery to me.  
As soon as may sysadmin return from vacations, I will clear that out. 
grettings et thanks to everyone.
Eric R. 



On Mer Juil 18 17:50 , Marc Schwartz  sent:

>I think that you are on to something there Don.
>
>I just tried accessing a table from our Oracle server, which I do know
>exists, but for which I do not have access permissions.
>
>Using the following query in the Oracle Instant Client:
>
>  select table_name from all_tables;
>
>I can get a list of all tables on the server, which includes a table
>called INCOMPATIBLE_USER_AGENTS, for which I do not have access
>permissions.
>
>When attempting to query the table in the Instant Client I get:
>
>SQL> select * from INCOMPATIBLE_USER_AGENTS;
>select * from INCOMPATIBLE_USER_AGENTS
>              *
>ERROR at line 1:
>ORA-00942: table or view does not exist
>
>
>When running the same query from R using RODBC I get:
>
>> sqlQuery(db, "select * from INCOMPATIBLE_USER_AGENTS")
>[1] "[RODBC] ERROR: Could not SQLExecDirect"                                
>[2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: table or view does not exist\n"
>
>
>So it looks like permission issues may be possible here.  Eric,
>definitely confirm with your SysAdmins that you have appropriate
>permissions.
>
>HTH,
>
>Marc
>
>
>On Wed, 2007-07-18 at 07:43 -0700, Don MacQueen wrote:
>> I believe I have seen that error message from 
>> Oracle when I tried to query a table for which I 
>> did not have "select" privileges (and when I knew 
>> for certain that the table existed). Ask your 
>> database administrator about the table, and make 
>> sure that you do have that privilege.
>> 
>> What I am uncertain about is whether Oracle, when 
>> asked to list tables, returns a list that 
>> includes tables for which the user does not have 
>> select privileges.
>> 
>> -Don
>> 
>> At 9:24 AM +0200 7/17/07, eric at net2000.ch wrote:
>> >essai 
>> >uid="osis_r",  pwd="12miss15" ,case="oracle")
>> >
>> >>  sqlTables(essai)$ORESTE
>> >
>> >...
>> >
>> >1315            ORESTE              S_PROFESSIONS_OLD        TABLE    
>> >1316            ORESTE                  S_PROVENANCES        TABLE    
>> >1317            ORESTE                        S_SEXES        TABLE    
>> >1318            ORESTE                 S_SOUS_CLASSES        TABLE    
>> >1319            ORESTE                 S_TYP_COLLEGES        TABLE    
>> >1320            ORESTE             S_TYP_ENSEIGNEMENT        TABLE    
>> >
>> >...
>> >
>> >>  sqlQuery(essai, "select * from S_TYP_COLLEGES")
>> >[1] "[RODBC] ERROR: Could not SQLExecDirect"                           
>> >[2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>> >
>> >I have also tried the
>> >essai2 
>> >But with no succes.
>> >
>> >
>> >
>> >On Lun Juil 16 15:32 , Prof Brian Ripley ripley at stats.ox.ac.uk> sent:
>> >
>> >>The problem could be quoting, if Oracle is not standards-compliant.
>> >>See the options in ?odbcConnect.
>> >>
>> >>If sqlQuery(essai, "select * from S_TYP_COLLEGES") works, this is likely
>> >>to be the problem.
>> >>
>> >>On Mon, 16 Jul 2007, eric at net2000.ch wrote:
>> >>
>> >>>
>> >>>
>> >>>>  essai
>> >>>>  odbcGetInfo(essai)
>> >>>        DBMS_Name         DBMS_Ver  Driver_ODBC_Ver
>> >>>         "Oracle"     "09.00.0121"          "03.51"
>> >>>  Data_Source_Name      Driver_Name       Driver_Ver
>> >>>    "ORESTE_prod"    "SQORA32.DLL"     "09.00.0101"
>> >>>         ODBC_Ver      Server_Name
>> >>>     "03.52.0000"           "weba"
>> >>>
>> >>>
>> >>>>  sqlTables(essai)
>> >>>
>> >>>  The result of this function is a liste of tables, one of them is called:
>> >>>  S_TYP_COLLEGES.
>> >>>
>> >>>
>> >>>>  sqlFetch(essai,"S_TYP_COLLEGES")
>> >>>  [1] "[RODBC] ERROR: Could not SQLExecDirect"
>> >>>  [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>> >>>
>> >>>>  sqlFetch(essai, "S_TYP_COLLEGES", colnames=TRUE, rownames=FALSE)
>> >>>  [1] "[RODBC] ERROR: Could not SQLExecDirect"
>> >>>  [2] "42S02 942 [Oracle][ODBC][Ora]ORA-00942: Table ou vue inexistante\n"
>> >  >>
>> >  >>
>> >  >> What could be the problem here ?
>> >  >> Any help is welcome
>> >  >> Eric R?thlisberger, Neuch?tel
>> >  >>
>


From pgilbert at bank-banque-canada.ca  Thu Jul 19 17:51:09 2007
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Thu, 19 Jul 2007 11:51:09 -0400
Subject: [R] memory error with 64-bit R in linux
In-Reply-To: <BAY110-F276541B2F797F47D6C741DC7FA0@phx.gbl>
References: <BAY110-F276541B2F797F47D6C741DC7FA0@phx.gbl>
Message-ID: <469F886D.40309@bank-banque-canada.ca>

You might try running top while R runs, to get a better idea of what is
happening. 64-bit R takes more memory than 32-bit (longer pointers) and
for a large problem I would say that 2GB RAM is a minimum if you want
any speed. Slowness is likely related to needing to use swap space. The
"cannot allocate" error is because you run out of both RAM and swap. If
you are close to finishing your calculation you may resolve things by
increasing swap, but don't expect it to be fast.

There is also a possibility that your user id is restricted, but I'm not
sure how that works anymore. It used to be controlled by ulimit, but
that does not seem to be the case in newer versions of Linux.

If you are still debugging your code, and there is some chance you are
just gobbling up memory endlessly until it runs out, then you can speed
things up (i.e. fail more quickly) by turning swap off. There are
debugging situations where this turns out to be useful.

HTH,
Paul

zhihua li wrote:
> Hi netters,
> 
> I'm using the 64-bit R-2.5.0 on a x86-64 cpu, with an RAM of 2 GB.  The 
> operating system is SUSE 10.
> The system information is:  -uname -a
> Linux someone 2.6.13-15.15-smp #1 SMP Mon Feb 26 14:11:33 UTC 2007 
> x86_64 x86_64 x86_64 GNU/Linux
> 
> I used heatmap to process a matrix of the dim [16000,100].  After 3 
> hours of desperating waiting, R told me:
> cannot allocate vector of size 896 MB.
> 
> I know the matrix is very big, but since I have 2 GB of RAM and in a 
> 64-bit system, there should be no problem to deal with a vector smaller 
> than 1 GB? (I was not running any other applications in my system)
> 
> Does anyone know what's going on?  Is there a hardware limit where I 
> have to add more RAM, or is there some way to resolve it softwarely? 
> Also is it possible to speed up the computing (I don't wanna wait 
> another 3 hours to know I get another error message)
> 
> Thank you in advance!
> 
> _________________________________________________________________
> ?????????????????????????????? MSN Hotmail??  http://www.hotmail.com
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-------------- next part --------------
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential inform...{{dropped}}


From smangut at gmail.com  Thu Jul 19 18:03:40 2007
From: smangut at gmail.com (sigalit mangut-leiba)
Date: Thu, 19 Jul 2007 19:03:40 +0300
Subject: [R] tapply
Message-ID: <c99f7100707190903la91b07fqf1be2d4a374395f0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/a884c8e0/attachment.pl 

From suzanne.j.matthews at gmail.com  Thu Jul 19 18:14:12 2007
From: suzanne.j.matthews at gmail.com (Suzanne Matthews)
Date: Thu, 19 Jul 2007 12:14:12 -0400
Subject: [R] help with heatmap - how to remove annoying "X" before
	numeric values?
In-Reply-To: <f0fd79140707190656m7c2478e6x9978e334b2f324e4@mail.gmail.com>
References: <Pine.LNX.4.64.0707190430380.30260@gannet.stats.ox.ac.uk>
	<397533.49001.qm@web32211.mail.mud.yahoo.com>
	<f0fd79140707190656m7c2478e6x9978e334b2f324e4@mail.gmail.com>
Message-ID: <f0fd79140707190914m2b78dc83u3ee64c301f5c3168@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/58fa3222/attachment.pl 

From birgit.lemcke at systbot.uzh.ch  Thu Jul 19 18:16:57 2007
From: birgit.lemcke at systbot.uzh.ch (Birgit Lemcke)
Date: Thu, 19 Jul 2007 18:16:57 +0200
Subject: [R] plot3d labels
Message-ID: <43F15BE7-FF80-48CC-A8B9-554196A9B16B@systbot.uzh.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/d54675e6/attachment.pl 

From P.Dalgaard at biostat.ku.dk  Thu Jul 19 18:19:30 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Thu, 19 Jul 2007 18:19:30 +0200
Subject: [R] tapply
In-Reply-To: <c99f7100707190903la91b07fqf1be2d4a374395f0@mail.gmail.com>
References: <c99f7100707190903la91b07fqf1be2d4a374395f0@mail.gmail.com>
Message-ID: <469F8F12.8030504@biostat.ku.dk>

sigalit mangut-leiba wrote:
> I'm sorry for the unfocused questions, i'm new here...
> the output should be:
> class    aps_mean
> 1              na
> 2             11.5
> 3               8
>
> the mean aps of every class, when every id count *once*,  for example: class
> 2, mean= (11+12)/2=11.5
> hope it's clearer.
>   
Much... Get the first record for each individual from (e.g.)

icul.redux <- subset(icul, !duplicated(id))

then use tapply as before using variables from icul.redux. Or in one go

with(
  subset(icul, !duplicated(id)),
  tapply(aps, class, mean, na.rm=TRUE)
)


-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From ggrothendieck at gmail.com  Thu Jul 19 18:34:32 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 Jul 2007 12:34:32 -0400
Subject: [R] help with heatmap - how to remove annoying "X" before
	numeric values?
In-Reply-To: <f0fd79140707190914m2b78dc83u3ee64c301f5c3168@mail.gmail.com>
References: <Pine.LNX.4.64.0707190430380.30260@gannet.stats.ox.ac.uk>
	<397533.49001.qm@web32211.mail.mud.yahoo.com>
	<f0fd79140707190656m7c2478e6x9978e334b2f324e4@mail.gmail.com>
	<f0fd79140707190914m2b78dc83u3ee64c301f5c3168@mail.gmail.com>
Message-ID: <971536df0707190934t7c3c7b12ifada0a50bb710376@mail.gmail.com>

Try this.  It makes a copy of heatmap.2 whose scope is changed to
first look within heatmap.3 for functions like mtext.  We redefine mtext
to intercept the text argument and change it appropriately.  Then
we call our copy of heatmap.2.  With this there is no need to change
the source of heatmap.2.

heatmap.3 <- function(...) {
	environment(heatmap.2) <- environment()
	mtext <- function(text, ...) {
		if (text == "Value") text <- "Silly Value"
		graphics::mtext(text, ...)
	}
	heatmap.2(...)
}

heatmap.3(as.matrix(heat),
	keysize=1.2, dendrogram="none", trace="none", Colv = FALSE,
	main = "Silly Data", labCol= NULL, margin=c(7,8))




On 7/19/07, Suzanne Matthews <suzanne.j.matthews at gmail.com> wrote:
> Sorry, I just realized I didn't send this to the list! (See below)
>
> Thanks for all the help! All is working fine now.
>
> If anyone knows of a more straightforward way to change the "Value" string
> for the Key, please let me know (just to satisfy my curiosity).  I got it to
> work by modifying the source code (specifically, "heatmap.2.R " in the
> gplots package).
>
> However, before, I didn't make the call
> source("heatmap.2.R")
>
> before I called
> source("mysillyheatmap.R")
>
> Making the additional call to " heatmap.2.R" fixed everything.
>
> Thanks again for all your help!
>
>
>
> On 7/19/07, Suzanne Matthews <suzanne.j.matthews at gmail.com> wrote:
> >
> > Thank you all for your prompt replies! The check.names=FALSE parameter
> > fixed things entirely.
> >
> > One more question:
> >
> > Is there a straightforward way to modify the the "Values" string that
> > labels the key to a user-defined value? For example, let's say I want to
> > change "Values" to "Silly Values". So far, what I have found that I need to
> > do is actually go and change a static string in the source code. Is there a
> > more direct way?
> >
> > Also, after I make the source code change (in gplots package, file:
> > heatmap.2.R), how do I have R build from that? If I remember correctly, if
> > I put the new heatmap.2.R in my directory, R is supposed to check for
> > functions and such there before it goes and builds it from the main source
> > code base (located at /usr/bin/R). I am a touch confused on which directory
> > is "my" directory, where R will check first. I tried putting the modified
> > heatmap.2.R file in the directory that my script is, and where I initially
> > run R. But that didn't work!
> >
> > Is there anything that I should add to my R script that will force it to
> > read from that from my directory? If not, which directory should I place
> > this in?
> >
> > My OS is OS X, so I think Unix-based instructions will work.
> >
> > Thank you once again for your time and patience!
> >
> > Sincerely,
> > Suzanne
> >
> > On 7/18/07, Moshe Olshansky <m_olshansky at yahoo.com> wrote:
> > >
> > > I was right saying that my solution was not the best
> > > possible!
> > >
> > > --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > >
> > > > read.table('temp.txt', check.names = FALSE)
> > > >
> > > > would be easier (and more general, since make.names
> > > > can do more than
> > > > prepend an 'X').
> > > >
> > > > On Wed, 18 Jul 2007, Moshe Olshansky wrote:
> > > >
> > > > > Hi Suzanne,
> > > > >
> > > > > My solution (which I am sure is not the best)
> > > > would
> > > > > be:
> > > > >
> > > > >> heat <- read.table('temp.txt')
> > > > >> heat
> > > > >      X1905 X1910 X1950 X1992 X2011 X2020
> > > > > Gnat   0.08  0.29  0.29  0.37  0.39  0.43
> > > > > Snake  0.16  0.34  0.32  0.40  0.41  0.53
> > > > > Bat    0.40  0.54  0.52  0.60  0.60  0.63
> > > > > Cat    0.16  0.27  0.29  0.39  0.37  0.41
> > > > > Dog    0.43  0.54  0.52  0.61  0.60  0.62
> > > > > Lynx   0.50  0.57  0.54  0.59  0.50  0.59
> > > > >> a<-names(heat)
> > > > >> b<-strsplit(a,split="X")
> > > > >> w<-unlist(b)
> > > > >> w
> > > > > [1] ""     "1905" ""     "1910" ""     "1950" ""
> > > > > "1992" ""     "2011" ""     "2020"
> > > > >> z <- w[seq(2,length(w),by=2)]
> > > > >> z
> > > > > [1] "1905" "1910" "1950" "1992" "2011" "2020"
> > > > >> names(heat) <- z
> > > > >> heat
> > > > >      1905 1910 1950 1992 2011 2020
> > > > > Gnat  0.08 0.29 0.29 0.37 0.39 0.43
> > > > > Snake 0.16 0.34 0.32 0.40 0.41 0.53
> > > > > Bat   0.40 0.54 0.52 0.60 0.60 0.63
> > > > > Cat   0.16 0.27 0.29 0.39 0.37 0.41
> > > > > Dog   0.43 0.54 0.52 0.61 0.60 0.62
> > > > > Lynx  0.50 0.57 0.54 0.59 0.50 0.59
> > > > >>
> > > > >
> > > > > Regards,
> > > > >
> > > > > Moshe.
> > > > >
> > > > > --- Suzanne Matthews
> > > > <suzanne.j.matthews at gmail.com>
> > > > > wrote:
> > > > >
> > > > >> Hello All,
> > > > >>
> > > > >> I have a simple question based on how things are
> > > > >> labeled on my heat map;
> > > > >> particularly, there is this annoying "X" that
> > > > >> appears before the numeric
> > > > >> value of all the labels of my columns.
> > > > >>
> > > > >> Let's say I have the following silly data, stored
> > > > in
> > > > >> "temp.txt"
> > > > >>         1905    1910    1950    1992    2011
> > > > 2020
> > > > >> Gnat    0.08    0.29    0.29    0.37    0.39
> > > > 0.43
> > > > >> Snake   0.16    0.34    0.32    0.40    0.41
> > > > 0.53
> > > > >> Bat     0.40    0.54    0.52    0.60     0.60
> > > > 0.63
> > > > >> Cat     0.16    0.27    0.29    0.39    0.37
> > > > 0.41
> > > > >> Dog     0.43    0.54    0.52    0.61    0.60
> > > > 0.62
> > > > >> Lynx    0.50    0.57    0.54    0.59     0.5
> > > > 0.59
> > > > >>
> > > > >> I use the following commands to generate my
> > > > heatmap:
> > > > >> heat <- read.table('temp.txt')
> > > > >> x <- as.matrix(heat)
> > > > >>
> > > > >> heatmap.2(x, keysize=1.2, dendrogram="none",
> > > > >> trace="none", Colv = FALSE,
> > > > >> main = "Silly Data", labCol=
> > > > >> NULL, margin=c(7,8))
> > > > >>
> > > > >> This generates a very nice heatmap, but there is
> > > > one
> > > > >> thing I have an issue
> > > > >> with: How do I get rid of the 'X' that seems to
> > > > come
> > > > >> automatically before my
> > > > >> numeric column values? I just want those columns
> > > > to
> > > > >> be labeled 1905, 1910,
> > > > >> 1950, and so on. I cannot find anything in the
> > > > >> heatmap.2 documentation that
> > > > >> suggests how I should do this.
> > > > >>
> > > > >> Thank you very much for your time, and patience
> > > > in
> > > > >> reading this!
> > > > >>
> > > > >> Sincerely,
> > > > >> Suzanne
> > > > >>
> > > > >>    [[alternative HTML version deleted]]
> > > > >>
> > > > >> ______________________________________________
> > > > >> R-help at stat.math.ethz.ch mailing list
> > > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >> PLEASE do read the posting guide
> > > > >> http://www.R-project.org/posting-guide.html
> > > > >> and provide commented, minimal, self-contained,
> > > > >> reproducible code.
> > > > >>
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained,
> > > > reproducible code.
> > > > >
> > > >
> > > > --
> > > > Brian D. Ripley,
> > > > ripley at stats.ox.ac.uk
> > > > Professor of Applied Statistics,
> > > > http://www.stats.ox.ac.uk/~ripley/<http://www.stats.ox.ac.uk/%7Eripley/>
> > > > University of Oxford,             Tel:  +44 1865
> > > > 272861 (self)
> > > > 1 South Parks Road,                     +44 1865
> > > > 272866 (PA)
> > > > Oxford OX1 3TG, UK                Fax:  +44 1865
> > > > 272595
> > > >
> > >
> > >
> >
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Thu Jul 19 18:48:41 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 19 Jul 2007 12:48:41 -0400
Subject: [R] plot3d labels
In-Reply-To: <43F15BE7-FF80-48CC-A8B9-554196A9B16B@systbot.uzh.ch>
References: <43F15BE7-FF80-48CC-A8B9-554196A9B16B@systbot.uzh.ch>
Message-ID: <644e1f320707190948l8c9044cl42477844d7eff7d9@mail.gmail.com>

The documentation has:

text3d(x, y = NULL, z = NULL, texts, adj = 0.5, justify, ...)

Do this do it for you?

On 7/19/07, Birgit Lemcke <birgit.lemcke at systbot.uzh.ch> wrote:
> Hello R users,
>
> I am a newby using R  2.5.0 on a Apple Power Book G4 with Mac OS X
> 10.4.10.
>
> Sorry that I ask again such stupid questions, but I haven?t found how
> to label the points created with plot3d (rgl).
> Hope somebody can help me.
>
> Thanks in advance.
>
> Birgit
>
>
> Birgit Lemcke
> Institut f?r Systematische Botanik
> Zollikerstrasse 107
> CH-8008 Z?rich
> Switzerland
> Ph: +41 (0)44 634 8351
> birgit.lemcke at systbot.uzh.ch
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From hongmei.jia at basf.com  Thu Jul 19 19:18:37 2007
From: hongmei.jia at basf.com (Hongmei Jia)
Date: Thu, 19 Jul 2007 13:18:37 -0400
Subject: [R] Fw: Do GLM by groups
Message-ID: <OF9B77BA62.27587B0A-ON8525731D.005F0D2D-8525731D.005F167F@notes.basf-corp.com>


Dear All,

I'm trying to do 'glm' analysis by groups just like in SAS you use "by
variable".   I don't know how to do it in R, anyone can help with this?
i.e.

group    line  rep  value
1              1       1       0.2
1              1       2       0.3
1     1       3       0.23
1             2       1       0.2
1             2       2       0.3
1             2      3       0.23
2              1       1       0.2
2              1       2       0.3
2     1       3       0.23
2             2       1       0.2
2             2       2       0.3
2             2      3       0.23

in SAS we say:
model value=line rep;
by group;

How can I do this in R?

Thanks,

Hongmei Jia


From bcarvalh at jhsph.edu  Thu Jul 19 19:26:36 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Thu, 19 Jul 2007 13:26:36 -0400
Subject: [R] Fw: Do GLM by groups
In-Reply-To: <OF9B77BA62.27587B0A-ON8525731D.005F0D2D-8525731D.005F167F@notes.basf-corp.com>
References: <OF9B77BA62.27587B0A-ON8525731D.005F0D2D-8525731D.005F167F@notes.basf-corp.com>
Message-ID: <DA63FAA6-FC7C-4349-AC7F-B0DFAEFC7601@jhsph.edu>

Check the following example for by():

      require(stats)
      attach(warpbreaks)
      by(warpbreaks, tension, function(x) lm(breaks ~ wool, data = x))

or just type:
example(by)

b


On Jul 19, 2007, at 1:18 PM, Hongmei Jia wrote:

>
> Dear All,
>
> I'm trying to do 'glm' analysis by groups just like in SAS you use "by
> variable".   I don't know how to do it in R, anyone can help with  
> this?
> i.e.
>
> group    line  rep  value
> 1              1       1       0.2
> 1              1       2       0.3
> 1     1       3       0.23
> 1             2       1       0.2
> 1             2       2       0.3
> 1             2      3       0.23
> 2              1       1       0.2
> 2              1       2       0.3
> 2     1       3       0.23
> 2             2       1       0.2
> 2             2       2       0.3
> 2             2      3       0.23
>
> in SAS we say:
> model value=line rep;
> by group;
>
> How can I do this in R?
>
> Thanks,
>
> Hongmei Jia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marco.r.help at gmail.com  Thu Jul 19 19:59:15 2007
From: marco.r.help at gmail.com (marco.R.help marco.R.help)
Date: Thu, 19 Jul 2007 19:59:15 +0200
Subject: [R] Sweave on mac os x
Message-ID: <605b4120707191059m22f68261j38801b3a2b2dabca@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/3c526d48/attachment.pl 

From Mike.Lawrence at DAL.CA  Thu Jul 19 20:14:45 2007
From: Mike.Lawrence at DAL.CA (Mike Lawrence)
Date: Thu, 19 Jul 2007 15:14:45 -0300
Subject: [R] linear interpolation of multiple random time series
Message-ID: <10E8C474-EAD2-4E31-9E59-913A1B99645B@DAL.CA>

Hi all,

Looking for tips on how I might more optimally solve this. I have  
time series data (samples from a force sensor) that are not  
guaranteed to be sampled at the same time values across trials. ex.

trial	time	x
1	1	1
1	5	4
1	7	9
1	12	20
2	1	0
2	3	5
2	9	10
2	13	14
2	19	22
2	24	32

Within each trial I'd like to use linear interpolation between each  
successive time sample to fill in intermediary timepoints and x- 
values, ex.

trial	time	x
1	1	1
1	2	1.75
1	3	2.5
1	4	3.25
1	5	4
1	6	6.5
1	7	9
1	8	11.2
1	9	13.4
1	10	15.6
1	11	17.8
1	12	20
2	1	0
2	2	2.5
2	3	5
2	4	5.83333333333333
2	5	6.66666666666667
2	6	7.5
2	7	8.33333333333333
2	8	9.16666666666667
2	9	10
2	10	11
2	11	12
2	12	13
2	13	14
2	14	15.3333333333333
2	15	16.6666666666667
2	16	18
2	17	19.3333333333333
2	18	20.6666666666667
2	19	22
2	20	24
2	21	26
2	22	28
2	23	30
2	24	32


The solution I've coded (below) involves going through the original  
data frame line by line and is thus very slow (indeed, I had to  
resort to writing to file as with a large data set I started running  
into memory issues if I tried to create the new data frame in  
memory). Any suggestions on a faster way to achieve what I'm trying  
to do?

#assumes the first data frame above is stored as 'a'
arows = (length(a$x)-1)
write('', 'temp.txt')
for(i in 1:arows){
	if(a$time[i+1] > a$time[i]){
		write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append  
= T)
		x1 = a$time[i]
		x2 = a$time[i+1]
		dx = x2-x1
		if(dx != 1){
			y1 = a$x[i]
			y2 = a$x[i+1]
			dy = y2-y1
			slope = dy/dx
			int = -slope*x1+y1
			temp=a[i,]
			for(j in (x1+1):(x2-1)){
				temp$time = j
				temp$x = slope*j+int
				write.table(temp, 'temp.txt', row.names = F, col.names = F,  
append = T)
			}
		}
	}else{
		write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append  
= T)
	}
}
i=i+1
write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append = T)

b=read.table('temp.txt',skip=1)
names(b)=names(a)


From Thomas.Adams at noaa.gov  Thu Jul 19 20:27:08 2007
From: Thomas.Adams at noaa.gov (Thomas Adams)
Date: Thu, 19 Jul 2007 14:27:08 -0400
Subject: [R] Sweave on mac os x
In-Reply-To: <605b4120707191059m22f68261j38801b3a2b2dabca@mail.gmail.com>
References: <605b4120707191059m22f68261j38801b3a2b2dabca@mail.gmail.com>
Message-ID: <469FACFC.8070802@noaa.gov>

Marco,

Yes, absolutely sweave comes with the binary Mac OS X distribution; it 
works great for me.

Regards,
Tom

marco.R.help marco.R.help wrote:
> Dear all,
>
>  is Sweave working on MAC ?
> I installed R-2.5.1 but seems like Sweave is not coming with the
> distribution as it comes on linux.
> Do I need to install it separately ?
> In that case is there a .dmg for mac ?
>
> Thanks for the help!
>
> Regards
>
> Marco
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
Thomas E Adams
National Weather Service
Ohio River Forecast Center
1901 South State Route 134
Wilmington, OH 45177

EMAIL:	thomas.adams at noaa.gov

VOICE:	937-383-0528
FAX:	937-383-0033


From jholtman at gmail.com  Thu Jul 19 20:44:38 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 19 Jul 2007 14:44:38 -0400
Subject: [R] linear interpolation of multiple random time series
In-Reply-To: <10E8C474-EAD2-4E31-9E59-913A1B99645B@DAL.CA>
References: <10E8C474-EAD2-4E31-9E59-913A1B99645B@DAL.CA>
Message-ID: <644e1f320707191144x24c63167s63a01a90d69fb8f6@mail.gmail.com>

This should do it for you:

> x <- read.table(textConnection("trial   time    x
+ 1       1       1
+ 1       5       4
+ 1       7       9
+ 1       12      20
+ 2       1       0
+ 2       3       5
+ 2       9       10
+ 2       13      14
+ 2       19      22
+ 2       24      32"), header=TRUE)
> # compute for each trial
> trial.list <- lapply(split(x, x$trial), function(set){
+     .xval <- seq(min(set$time), max(set$time))
+     .yval <- approx(set$time, set$x, xout=.xval)$y
+     cbind(trial=set$trial[1], time=.xval, x=.yval)
+ })
> do.call('rbind', trial.list)
      trial time         x
 [1,]     1    1  1.000000
 [2,]     1    2  1.750000
 [3,]     1    3  2.500000
 [4,]     1    4  3.250000
 [5,]     1    5  4.000000
 [6,]     1    6  6.500000
 [7,]     1    7  9.000000
 [8,]     1    8 11.200000
 [9,]     1    9 13.400000
[10,]     1   10 15.600000
[11,]     1   11 17.800000
[12,]     1   12 20.000000
[13,]     2    1  0.000000
[14,]     2    2  2.500000
[15,]     2    3  5.000000
[16,]     2    4  5.833333
[17,]     2    5  6.666667
[18,]     2    6  7.500000
[19,]     2    7  8.333333
[20,]     2    8  9.166667
[21,]     2    9 10.000000
[22,]     2   10 11.000000
[23,]     2   11 12.000000
[24,]     2   12 13.000000
[25,]     2   13 14.000000
[26,]     2   14 15.333333
[27,]     2   15 16.666667
[28,]     2   16 18.000000
[29,]     2   17 19.333333
[30,]     2   18 20.666667
[31,]     2   19 22.000000
[32,]     2   20 24.000000
[33,]     2   21 26.000000
[34,]     2   22 28.000000
[35,]     2   23 30.000000
[36,]     2   24 32.000000
>


On 7/19/07, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
> Hi all,
>
> Looking for tips on how I might more optimally solve this. I have
> time series data (samples from a force sensor) that are not
> guaranteed to be sampled at the same time values across trials. ex.
>
> trial   time    x
> 1       1       1
> 1       5       4
> 1       7       9
> 1       12      20
> 2       1       0
> 2       3       5
> 2       9       10
> 2       13      14
> 2       19      22
> 2       24      32
>
> Within each trial I'd like to use linear interpolation between each
> successive time sample to fill in intermediary timepoints and x-
> values, ex.
>
> trial   time    x
> 1       1       1
> 1       2       1.75
> 1       3       2.5
> 1       4       3.25
> 1       5       4
> 1       6       6.5
> 1       7       9
> 1       8       11.2
> 1       9       13.4
> 1       10      15.6
> 1       11      17.8
> 1       12      20
> 2       1       0
> 2       2       2.5
> 2       3       5
> 2       4       5.83333333333333
> 2       5       6.66666666666667
> 2       6       7.5
> 2       7       8.33333333333333
> 2       8       9.16666666666667
> 2       9       10
> 2       10      11
> 2       11      12
> 2       12      13
> 2       13      14
> 2       14      15.3333333333333
> 2       15      16.6666666666667
> 2       16      18
> 2       17      19.3333333333333
> 2       18      20.6666666666667
> 2       19      22
> 2       20      24
> 2       21      26
> 2       22      28
> 2       23      30
> 2       24      32
>
>
> The solution I've coded (below) involves going through the original
> data frame line by line and is thus very slow (indeed, I had to
> resort to writing to file as with a large data set I started running
> into memory issues if I tried to create the new data frame in
> memory). Any suggestions on a faster way to achieve what I'm trying
> to do?
>
> #assumes the first data frame above is stored as 'a'
> arows = (length(a$x)-1)
> write('', 'temp.txt')
> for(i in 1:arows){
>        if(a$time[i+1] > a$time[i]){
>                write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append
> = T)
>                x1 = a$time[i]
>                x2 = a$time[i+1]
>                dx = x2-x1
>                if(dx != 1){
>                        y1 = a$x[i]
>                        y2 = a$x[i+1]
>                        dy = y2-y1
>                        slope = dy/dx
>                        int = -slope*x1+y1
>                        temp=a[i,]
>                        for(j in (x1+1):(x2-1)){
>                                temp$time = j
>                                temp$x = slope*j+int
>                                write.table(temp, 'temp.txt', row.names = F, col.names = F,
> append = T)
>                        }
>                }
>        }else{
>                write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append
> = T)
>        }
> }
> i=i+1
> write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append = T)
>
> b=read.table('temp.txt',skip=1)
> names(b)=names(a)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Thu Jul 19 21:01:40 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 19 Jul 2007 15:01:40 -0400
Subject: [R] linear interpolation of multiple random time series
In-Reply-To: <10E8C474-EAD2-4E31-9E59-913A1B99645B@DAL.CA>
References: <10E8C474-EAD2-4E31-9E59-913A1B99645B@DAL.CA>
Message-ID: <971536df0707191201le77e57ah412adb032b1e5bc2@mail.gmail.com>

Thsi can be done compactly using the zoo package.

The first statement after library converts the rows for each trial into
a separate zoo object and then uses by to merge these into a
single zoo object with one column per trial.

The second statement converts it from zoo to ts which has
the effect of filling in all missing times.  na.approx
does the actual linear interpolation.

The result is an mts object (or you could use as.zoo to
convert that to a zoo object if you prefer).

library(zoo)
z <- do.call("merge", by(a, a$trial, function(DF) zoo(DF$x, DF$time)))
na.approx(as.ts(z), na.rm = FALSE)


On 7/19/07, Mike Lawrence <Mike.Lawrence at dal.ca> wrote:
> Hi all,
>
> Looking for tips on how I might more optimally solve this. I have
> time series data (samples from a force sensor) that are not
> guaranteed to be sampled at the same time values across trials. ex.
>
> trial   time    x
> 1       1       1
> 1       5       4
> 1       7       9
> 1       12      20
> 2       1       0
> 2       3       5
> 2       9       10
> 2       13      14
> 2       19      22
> 2       24      32
>
> Within each trial I'd like to use linear interpolation between each
> successive time sample to fill in intermediary timepoints and x-
> values, ex.
>
> trial   time    x
> 1       1       1
> 1       2       1.75
> 1       3       2.5
> 1       4       3.25
> 1       5       4
> 1       6       6.5
> 1       7       9
> 1       8       11.2
> 1       9       13.4
> 1       10      15.6
> 1       11      17.8
> 1       12      20
> 2       1       0
> 2       2       2.5
> 2       3       5
> 2       4       5.83333333333333
> 2       5       6.66666666666667
> 2       6       7.5
> 2       7       8.33333333333333
> 2       8       9.16666666666667
> 2       9       10
> 2       10      11
> 2       11      12
> 2       12      13
> 2       13      14
> 2       14      15.3333333333333
> 2       15      16.6666666666667
> 2       16      18
> 2       17      19.3333333333333
> 2       18      20.6666666666667
> 2       19      22
> 2       20      24
> 2       21      26
> 2       22      28
> 2       23      30
> 2       24      32
>
>
> The solution I've coded (below) involves going through the original
> data frame line by line and is thus very slow (indeed, I had to
> resort to writing to file as with a large data set I started running
> into memory issues if I tried to create the new data frame in
> memory). Any suggestions on a faster way to achieve what I'm trying
> to do?
>
> #assumes the first data frame above is stored as 'a'
> arows = (length(a$x)-1)
> write('', 'temp.txt')
> for(i in 1:arows){
>        if(a$time[i+1] > a$time[i]){
>                write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append
> = T)
>                x1 = a$time[i]
>                x2 = a$time[i+1]
>                dx = x2-x1
>                if(dx != 1){
>                        y1 = a$x[i]
>                        y2 = a$x[i+1]
>                        dy = y2-y1
>                        slope = dy/dx
>                        int = -slope*x1+y1
>                        temp=a[i,]
>                        for(j in (x1+1):(x2-1)){
>                                temp$time = j
>                                temp$x = slope*j+int
>                                write.table(temp, 'temp.txt', row.names = F, col.names = F,
> append = T)
>                        }
>                }
>        }else{
>                write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append
> = T)
>        }
> }
> i=i+1
> write.table(a[i,], 'temp.txt', row.names = F, col.names = F, append = T)
>
> b=read.table('temp.txt',skip=1)
> names(b)=names(a)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From alex.park1 at ntlworld.com  Thu Jul 19 21:35:25 2007
From: alex.park1 at ntlworld.com (Alex Park)
Date: Thu, 19 Jul 2007 20:35:25 +0100
Subject: [R] Help with Dates
Message-ID: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>

R 

I am taking an excel dataset and reading it into R using read.table.
(actually I am dumping the data into a .txt file first and then reading data
in to R).

Here is snippet:

> head(data); 
       Date  Price Open.Int. Comm.Long Comm.Short net.comm
1 15-Jan-86 673.25    175645     65910      28425    37485
2 31-Jan-86 677.00    167350     54060      27120    26940
3 14-Feb-86 680.25    157985     37955      25425    12530
4 28-Feb-86 691.75    162775     49760      16030    33730
5 14-Mar-86 706.50    163495     54120      27995    26125
6 31-Mar-86 709.75    164120     54715      30390    24325

The dataset runs from 1986 to 2007.

I want to be able to take subsets of my data based on date e.g. data between
2000 - 2005.

As it stands, I can't work with the dates as they are not in correct format.

I tried successfully converting the dates to just the year using:

transform(data, Yr = format(as.Date(as.character(Date),format = '%d-%b-%y'),
"%y")))

This gives the following format:

       Date  Price Open.Int. Comm.Long Comm.Short net.comm Yr
1 15-Jan-86 673.25    175645     65910      28425    37485 86
2 31-Jan-86 677.00    167350     54060      27120    26940 86
3 14-Feb-86 680.25    157985     37955      25425    12530 86
4 28-Feb-86 691.75    162775     49760      16030    33730 86
5 14-Mar-86 706.50    163495     54120      27995    26125 86
6 31-Mar-86 709.75    164120     54715      30390    24325 86

I can subset for a single year e.g:

head(subset(df, Yr =="00")

But how can I subset for multiple periods e.g 00- 05? The following won't
work:

head(subset(df, Yr =="00" & Yr=="01")

or

head(subset(df, Yr = c("00","01","02","03")

I can't help but feeling that I am missing something and there is a simpler
route.

I leafed through R newletter 4.1 which deals with dates and times but it
seemed that strptime and POSIXct / POSIXlt are not what I need either.

Can anybody help me?

Regards


Alex


From smangut at gmail.com  Thu Jul 19 21:41:28 2007
From: smangut at gmail.com (sigalit mangut-leiba)
Date: Thu, 19 Jul 2007 22:41:28 +0300
Subject: [R] Fwd: tapply
In-Reply-To: <c99f7100707190903la91b07fqf1be2d4a374395f0@mail.gmail.com>
References: <c99f7100707190903la91b07fqf1be2d4a374395f0@mail.gmail.com>
Message-ID: <c99f7100707191241i73af802do3c896d6361633319@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/15948598/attachment.pl 

From jmburgos at u.washington.edu  Thu Jul 19 22:23:33 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Thu, 19 Jul 2007 13:23:33 -0700
Subject: [R] R and Copula
In-Reply-To: <11683844.post@talk.nabble.com>
References: <11612986.post@talk.nabble.com>	<OFC94AAA62.BB5EBA68-ON6525731C.002B7FA8-6525731C.002CE93A@ccilindia.co.in>
	<11683844.post@talk.nabble.com>
Message-ID: <469FC845.2060704@u.washington.edu>

Hi Meeryana,

It seems you have not loaded the package.  To use a package that you 
already installed, do:

 > library(copula)

I recommend you review the R documentation.  There are several good 
references and tutorial on the CRAN site:

http://cran.r-project.org/manuals.html
http://cran.r-project.org/other-docs.html

Any time you spend learning the basics will make your work more 
efficient. :)
Good luck,

Julian

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science

copula wrote:
> I have installed fast all packages for copula,
> I have installed package 'gnml' and the others from the Jim Lindsey's web
> site and the other packages like 'repeted' which is necessary for
> calculating copula.
>  
> but when I start with copula it write: 
> 'gausscop' function is not found. And also 'fitted.gausscop(z)' and
> 'residuals.gausscop(z)'.
>
> also what I want to say is: when I have installed package 'repeted' from Jim
> Lindsey's web site in R project, this what I have received: 
>  
> "utils:::menuInstallLocal()
> updating HTML package descriptions"
>
> what have I to do next to solve this problem and to continue with copula.
>
>
> Thanks a lot for previous help!
>
>
>  
>
>   
>
> gyadav wrote:
>   
>> hi meeryana,
>>
>> may be this time nobnody is responding. but dont worry you will get a lot 
>> of help eventually, so always post a copy to the mailing list.
>> The reason is there are a lot many newbies, although i am also not so old 
>> enough, who have even the simplest questions but are hesitant to ask.
>> the objective of the list is to help, and thus feel free to ask, stand and 
>> contribute :-) i hope you will understand not today then tomorrow as you 
>> will get associated with the mailing list more closely. Personally i have 
>> found friends here. Further, never post a mail with a loose subject and 
>> secondly try to maintain the thread i.e. reply to the mail which you want 
>> to reply(for more details refer to the posting guide) :-) that would 
>> really help. 
>>
>> Yes now regarding your question :-) 
>>
>> Step1 : List of all packages can be found at this link :-)
>> http://cran.r-project.org/src/contrib/PACKAGES.html
>> Step2: Click on the package you want to install :-) 
>> http://cran.r-project.org/src/contrib/Descriptions/copula.html
>> Step3: Then download the binary of your Operating system. If windows then 
>> download corresponding zip file.
>> for copula it is
>>  http://cran.r-project.org/bin/windows/contrib/r-release/copula_0.5-3.zip
>> save zip file on your system
>> Step4 Open your R rpogram
>> Step5: Goto Packages -> Install packages from Local Zip file
>> Step6: Select your package zip file which you want to install
>> Step7: Sit back and relax
>> Step8: load the library using library(LibraryName) on R prompt
>>
>> There are alternate ways of installing the package directly from R prompt. 
>> It didn't worked for me a long time back, so  i always adopt this method.
>> Somebody on the list may help you in this regards :-)
>>
>> bye and learn
>> Join and stand with Open Source and R community
>> Cheers and Chiao, Welcome
>> -gaurav
>>
>>
>>
>> dear Mr. Yadav,
>>
>> I want to thank for help, 
>> and for that you are only who is willing to help,
>> but I have one question:
>> because I'm new with R project also, I think I should install a package 
>> for copula.
>> I have only installed R program.
>> How should I install this package? And is it what I have also to do with 
>> credit metrics, Value at Risk, matix and the other formulas, I mean 
>> install packages.
>>
>> I hope that you have a little time for me and my problem, and I hope I'm 
>> not disturbing you.
>> thank you for all you can do for me 
>>
>> and 
>>
>> best regards,
>>
>> Mirjana
>>
>>
>>
>>
>> gyadav wrote:
>>     
>>> hi
>>>
>>> see the code below i hope this will make your understanding of copulas 
>>> better
>>> this code plots two normal distribution and their joint distribution 
>>> N[0,2] & N[0,4]
>>>
>>> HTH
>>>
>>> ######################code################################
>>> library("copula")
>>> ###################copy in two parts in R#################
>>>
>>> ##################PART A##################################
>>> ## construct a bivariate distribution whose marginals
>>> ## are normal and Normal respectively, coupled
>>> ## together via a normal copula
>>> op <- par(mfrow = c(2, 2), # 2 x 2 pictures on one plot
>>>           pty = "s")       # square plotting region,
>>>                            # independent of device size
>>>
>>> x <- mvdc(normalCopula(0.75), c("norm", "norm"),
>>> list(list(mean = 0, sd =2),list(mean = 0, sd =4)))
>>> x.samp <- rmvdc(x, 10000)
>>> par(mfrow=c(2,3))
>>> hist(x.samp[,1],xlab="Normal")
>>> hist(x.samp[,2],xlab="Normal")
>>> plot(x.samp[,2],x.samp[,1],pch=21,xlab="Normal",ylab="Normal")
>>>
>>> plot(dmvdc(x, x.samp))
>>> plot(pmvdc(x, x.samp))
>>>
>>> ## At end of plotting, reset to previous settings:
>>>
>>>
>>> ###########################PART B#######################
>>> par(op)
>>> for (i in seq(1:360)){
>>> persp(x, dmvdc, xlim = c(-4, 4), ylim=c(0, 1),theta=i)
>>> }
>>>
>>>
>>> Regards,
>>>
>>> Gaurav Yadav
>>> +++++++++++
>>> Assistant Manager, CCIL, Mumbai (India)
>>> Mob: +919821286118 Email: emailtogauravyadav at gmail.com
>>> Bhagavad Gita:  Man is made by his Belief, as He believes, so He is
>>>
>>>
>>>
>>> copula <meeryana at yahoo.com> 
>>> Sent by: r-help-bounces at stat.math.ethz.ch
>>> 07/17/2007 12:53 PM
>>>
>>> To
>>> r-help at stat.math.ethz.ch
>>> cc
>>>
>>> Subject
>>> Re: [R] R and Copula
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> it would be great when somebody will help me
>>> thanks
>>>
>>>
>>> copula wrote:
>>>       
>>>> hi,
>>>> first I want to say that I'm new here, and new with copula and R.
>>>>
>>>> That is the reason why I'm writing, if somebody can help me. 
>>>>
>>>> I have to make an example of Copula. 
>>>> On internet I've found this forum and that copula can calculate with R.
>>>>
>>>> Can somebody help me with the thing how can I start and where can read
>>>> about these stuffs.
>>>>
>>>> Thank to all who can help!
>>>>
>>>>
>>>>
>>>>
>>>>         
>>> -- 
>>> View this message in context: 
>>> http://www.nabble.com/R-and-Copula-tf4085867.html#a11644534
>>> Sent from the R help mailing list archive at Nabble.com.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>>
>>>       
>> ============================================================================================
>>     
>>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and 
>>>       
>> ...{{dropped}}
>>     
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>       
>> Quoted from:  http://www.nabble.com/R-and-Copula-tf4085867.html#a11645614
>>
>>
>>
>>
>> ============================================================================================
>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and ...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>


From Mike.Lawrence at dal.ca  Thu Jul 19 22:44:23 2007
From: Mike.Lawrence at dal.ca (Mike Lawrence)
Date: Thu, 19 Jul 2007 17:44:23 -0300
Subject: [R] linear interpolation of multiple random time series
In-Reply-To: <971536df0707191201le77e57ah412adb032b1e5bc2@mail.gmail.com>
References: <10E8C474-EAD2-4E31-9E59-913A1B99645B@DAL.CA>
	<971536df0707191201le77e57ah412adb032b1e5bc2@mail.gmail.com>
Message-ID: <96E14B23-30CA-4480-99CC-679C1DE55001@dal.ca>

Jim Holtman's solution works great, and will try the zoo method just  
for fun as well.

Thanks to all of you :)

Mike

--
Mike Lawrence
Graduate Student, Department of Psychology, Dalhousie University

Website: http://memetic.ca

Public calendar: http://icalx.com/public/informavore/Public

"The road to wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
	- Piet Hein


From raschaoot at gmail.com  Thu Jul 19 22:53:36 2007
From: raschaoot at gmail.com (Jeroen van der Heide)
Date: Thu, 19 Jul 2007 15:53:36 -0500
Subject: [R] Questions regarding R and fitting GARCH models
Message-ID: <90b8808d0707191353l1ecac861vbed1befc84b5aa24@mail.gmail.com>

Dear all,

I've recently switched from EViews to R with RMetrics/fSeries (newest
version of july 10) for my analysis because of the much bigger
flexibility it offers. So far my experiences had been great -prior I
had already worked extensively with S-Plus so was already kind of
familiar with the language- until I got to the fSeries package.

My problem with the documentation of fSeries is that it is pretty
sparse; therefore I don't whether I am doing something wrong or if my
problem is related to elementary statistics (though I hold an MSc in
Econometrics, it's been quite awhile :o). Next to that I have two
questions related to the syntaxis of the R language itself; I've been
searching for a good couple of hours but couldn't find the answers.
Hope you can help me out.

>From the descriptive statistics of my series, I had determined that my
GARCH error term should follow a student's t distribution, preferrably
skewed; the sstdFit function returns nu=2.002 and xi=1.012. I know
that for this distribution 2 degrees of freedom means the second,
third and fourth moment are not defined; however, the QQ plot looks
good -had to use the skwt package though, because rsstd didn't work-
so I decided to give it a try.

1) That didn't go all to well. garchFit returns alpha1 and beta1 of
0.1 and 0.8 respectively (and beta1 has a standard error of NaN) -
which are not consistent at all with what EViews reported. How is this
possible? Is it because GARCH estimation with a student's t
distribution with 2 degrees of freedom is impossible? What are my
options in this case? Or do I need to use garchSpec, of which I have
absolutely *no* idea what its use is (i.e. the entire idea is to
obtain estimates for a given order of AR-GARCH, right)?

2) R/RGui crashes frequently - also if I "just" want to estimate a
GARCH(1,1) with a student's t distributed error term and 4 or 5
degrees of freedom. I think it just gets into an endless calculation
loop; is there a way to abort this?

3) Is there a way to extract the data from the garchFit objects? So if
"fit" is my object, how can I extract specific data like the
fit$coef[1] I can use with other objects? Ultimately, fSeries comes
with a really nice plotting function (i.e., plot(fit, which=2)) but I
want to extract the series it actually plots from there - is that
possible like the hist function?

Thanks a lot for your time.

Best regards,


Jeroen van der Heide


From dsmith at viciscapital.com  Thu Jul 19 23:07:34 2007
From: dsmith at viciscapital.com (Dale Smith)
Date: Thu, 19 Jul 2007 17:07:34 -0400
Subject: [R] Questions regarding R and fitting GARCH models
In-Reply-To: <90b8808d0707191353l1ecac861vbed1befc84b5aa24@mail.gmail.com>
References: <90b8808d0707191353l1ecac861vbed1befc84b5aa24@mail.gmail.com>
Message-ID: <0E4F0C7EEAAB274F8DC6B1543949F05BEB7180@vicsrv4.viciscapital.com>

As for (3) below, we are also interested in the xi parameter from the
gpdFit function from fExtremes. It seems the fit classes returned by the
associated functions don't have the full properties as finMetrics has
for its classes.

My solution was to modify the appropriate fMetrics code, build a new
package, and install it. I haven't completed that yet. Are there any
other things I can try?

Dale Smith
Vicis Capital, LLC
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeroen van der
Heide
Sent: Thursday, July 19, 2007 4:54 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Questions regarding R and fitting GARCH models

Dear all,

I've recently switched from EViews to R with RMetrics/fSeries (newest
version of july 10) for my analysis because of the much bigger
flexibility it offers. So far my experiences had been great -prior I
had already worked extensively with S-Plus so was already kind of
familiar with the language- until I got to the fSeries package.

My problem with the documentation of fSeries is that it is pretty
sparse; therefore I don't whether I am doing something wrong or if my
problem is related to elementary statistics (though I hold an MSc in
Econometrics, it's been quite awhile :o). Next to that I have two
questions related to the syntaxis of the R language itself; I've been
searching for a good couple of hours but couldn't find the answers.
Hope you can help me out.

>From the descriptive statistics of my series, I had determined that my
GARCH error term should follow a student's t distribution, preferrably
skewed; the sstdFit function returns nu=2.002 and xi=1.012. I know
that for this distribution 2 degrees of freedom means the second,
third and fourth moment are not defined; however, the QQ plot looks
good -had to use the skwt package though, because rsstd didn't work-
so I decided to give it a try.

1) That didn't go all to well. garchFit returns alpha1 and beta1 of
0.1 and 0.8 respectively (and beta1 has a standard error of NaN) -
which are not consistent at all with what EViews reported. How is this
possible? Is it because GARCH estimation with a student's t
distribution with 2 degrees of freedom is impossible? What are my
options in this case? Or do I need to use garchSpec, of which I have
absolutely *no* idea what its use is (i.e. the entire idea is to
obtain estimates for a given order of AR-GARCH, right)?

2) R/RGui crashes frequently - also if I "just" want to estimate a
GARCH(1,1) with a student's t distributed error term and 4 or 5
degrees of freedom. I think it just gets into an endless calculation
loop; is there a way to abort this?

3) Is there a way to extract the data from the garchFit objects? So if
"fit" is my object, how can I extract specific data like the
fit$coef[1] I can use with other objects? Ultimately, fSeries comes
with a really nice plotting function (i.e., plot(fit, which=2)) but I
want to extract the series it actually plots from there - is that
possible like the hist function?

Thanks a lot for your time.

Best regards,


Jeroen van der Heide

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

All e-mail sent to or from this address will be received or otherwise recorded by Vicis Capital, LLC and is subject to archival, monitoring and/or review, by and/or disclosure to, someone other than the recipient.  This message is intended only for the use of the person(s) ("intended recipient") to whom it is addressed.  It may contain information that is privileged and confidential.  If you are not the intended recipient, please contact the sender as soon as possible and delete the message without reading it or making a copy.  Any dissemination, distribution, copying, or other use of this message or any of its content by any person other than the intended recipient is strictly prohibited.  Vicis Capital, LLC only transacts business in states where it is properly registered or notice filed, or excluded or exempted from registration or notice filing requirements.


From asb at mail.nih.gov  Thu Jul 19 23:51:22 2007
From: asb at mail.nih.gov (Alan S Barnett)
Date: Thu, 19 Jul 2007 17:51:22 -0400
Subject: [R] Trend lines on a scatterplot matrix
Message-ID: <1184881883.17296.0.camel@papagena>

I'm using pairs() to generate a scatterplot matrix;

pairs(~ Fuzzy.gray.white.ratio+Fuzzy.gw.t.score+AgeWhenTested+signal_mean.noise,
	data=datam,subset=status=="control",main="Controls",
        labels=c("G/W","Peak Separation","Age","S/N"))


How can I add regression lines to the plots?


From jholtman at gmail.com  Fri Jul 20 01:36:56 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 19 Jul 2007 19:36:56 -0400
Subject: [R] Help with Dates
In-Reply-To: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
References: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
Message-ID: <644e1f320707191636s5dfcaa3bsa55d96e9e1152bbf@mail.gmail.com>

Try some of the following:

head(subset(df, Yr %in% c("00","01","02","03")))

subset(df, (Yr >= '00') & (Yr <= '03'))  # same as above

subset(df, (Yr == '00') | (Yr == '01') | (Yr == '02') |(Yr == '03'))  # same


On 7/19/07, Alex Park <alex.park1 at ntlworld.com> wrote:
> R
>
> I am taking an excel dataset and reading it into R using read.table.
> (actually I am dumping the data into a .txt file first and then reading data
> in to R).
>
> Here is snippet:
>
> > head(data);
>       Date  Price Open.Int. Comm.Long Comm.Short net.comm
> 1 15-Jan-86 673.25    175645     65910      28425    37485
> 2 31-Jan-86 677.00    167350     54060      27120    26940
> 3 14-Feb-86 680.25    157985     37955      25425    12530
> 4 28-Feb-86 691.75    162775     49760      16030    33730
> 5 14-Mar-86 706.50    163495     54120      27995    26125
> 6 31-Mar-86 709.75    164120     54715      30390    24325
>
> The dataset runs from 1986 to 2007.
>
> I want to be able to take subsets of my data based on date e.g. data between
> 2000 - 2005.
>
> As it stands, I can't work with the dates as they are not in correct format.
>
> I tried successfully converting the dates to just the year using:
>
> transform(data, Yr = format(as.Date(as.character(Date),format = '%d-%b-%y'),
> "%y")))
>
> This gives the following format:
>
>       Date  Price Open.Int. Comm.Long Comm.Short net.comm Yr
> 1 15-Jan-86 673.25    175645     65910      28425    37485 86
> 2 31-Jan-86 677.00    167350     54060      27120    26940 86
> 3 14-Feb-86 680.25    157985     37955      25425    12530 86
> 4 28-Feb-86 691.75    162775     49760      16030    33730 86
> 5 14-Mar-86 706.50    163495     54120      27995    26125 86
> 6 31-Mar-86 709.75    164120     54715      30390    24325 86
>
> I can subset for a single year e.g:
>
> head(subset(df, Yr =="00")
>
> But how can I subset for multiple periods e.g 00- 05? The following won't
> work:
>
> head(subset(df, Yr =="00" & Yr=="01")
>
> or
>
> head(subset(df, Yr = c("00","01","02","03")
>
> I can't help but feeling that I am missing something and there is a simpler
> route.
>
> I leafed through R newletter 4.1 which deals with dates and times but it
> seemed that strptime and POSIXct / POSIXlt are not what I need either.
>
> Can anybody help me?
>
> Regards
>
>
> Alex
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From sunnyside500 at gmail.com  Fri Jul 20 01:41:40 2007
From: sunnyside500 at gmail.com (runner)
Date: Thu, 19 Jul 2007 16:41:40 -0700 (PDT)
Subject: [R] can I paste 'newline'?
Message-ID: <11699845.post@talk.nabble.com>


It is ok to bury a reg expression '\n' when using 'cat', but not 'paste'. 
e.g.

cat ('I need to move on to a new line', '\n', 'at here') # change line!
paste ('I need to move on to a new line', '\n', 'at here') # '\n' is just a
character as it is.

Is there a way around pasting '\n' ? Thanks a lot.
-- 
View this message in context: http://www.nabble.com/can-I-paste-%27newline%27--tf4114350.html#a11699845
Sent from the R help mailing list archive at Nabble.com.


From murdoch at stats.uwo.ca  Fri Jul 20 01:51:09 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 19 Jul 2007 19:51:09 -0400
Subject: [R] can I paste 'newline'?
In-Reply-To: <11699845.post@talk.nabble.com>
References: <11699845.post@talk.nabble.com>
Message-ID: <469FF8ED.1010705@stats.uwo.ca>

On 19/07/2007 7:41 PM, runner wrote:
> It is ok to bury a reg expression '\n' when using 'cat', but not 'paste'. 
> e.g.
> 
> cat ('I need to move on to a new line', '\n', 'at here') # change line!
> paste ('I need to move on to a new line', '\n', 'at here') # '\n' is just a
> character as it is.
> 
> Is there a way around pasting '\n' ? Thanks a lot.

What do you want to get?  Do you want a two element vector?  Then use 
c().  Do you want a one element vector that prints on two lines?  Use 
either form, they both work (but you need to use cat() to do the display).

 > x <- paste ('I need to move on to a new line', '\n', 'at here')
 > cat(x)
I need to move on to a new line
  at here>


From jholtman at gmail.com  Fri Jul 20 01:52:10 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 19 Jul 2007 19:52:10 -0400
Subject: [R] can I paste 'newline'?
In-Reply-To: <11699845.post@talk.nabble.com>
References: <11699845.post@talk.nabble.com>
Message-ID: <644e1f320707191652t73ab7246vbf7b847ee5f72580@mail.gmail.com>

Notice the difference:

> cat ('I need to move on to a new line', '\n', 'at here') # change line!
I need to move on to a new line
 at here> paste ('I need to move on to a new line', '\n', 'at here') #
'\n' is just a
[1] "I need to move on to a new line \n at here"
> cat(paste ('I need to move on to a new line', '\n', 'at here'))
I need to move on to a new line
 at here>

> paste("a long string
+ with carriage
+ returns")
[1] "a long string\nwith carriage\nreturns"
>
>
> cat(paste("a long string
+ with carriage
+ returns"))
a long string
with carriage
returns>


paste is showing you the characters in the string; cat is acutally
outputting to a print device where '\n' is a line feed.

On 7/19/07, runner <sunnyside500 at gmail.com> wrote:
>
> It is ok to bury a reg expression '\n' when using 'cat', but not 'paste'.
> e.g.
>
> cat ('I need to move on to a new line', '\n', 'at here') # change line!
> paste ('I need to move on to a new line', '\n', 'at here') # '\n' is just a
> character as it is.
>
> Is there a way around pasting '\n' ? Thanks a lot.
> --
> View this message in context: http://www.nabble.com/can-I-paste-%27newline%27--tf4114350.html#a11699845
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From davamaillist at gmail.com  Fri Jul 20 01:59:48 2007
From: davamaillist at gmail.com (david dav)
Date: Fri, 20 Jul 2007 01:59:48 +0200
Subject: [R] exclude1 in summary.formula from Hmisc
Message-ID: <772cb06e0707191659m5d012eb8o28bb040a2780eaf4@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/714153cb/attachment.pl 

From mvinic at gmail.com  Fri Jul 20 02:11:14 2007
From: mvinic at gmail.com (Marcus Vinicius)
Date: Thu, 19 Jul 2007 21:11:14 -0300
Subject: [R] BOA (Bayesian Output Analysis)
Message-ID: <c0792190707191711i4fc6e523nfb5ba4e5763062a2@mail.gmail.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070719/d3991989/attachment.pl 

From genouellet at gmail.com  Fri Jul 20 02:21:15 2007
From: genouellet at gmail.com (G.)
Date: Thu, 19 Jul 2007 17:21:15 -0700 (PDT)
Subject: [R] how to determine/assign a numeric vector to "Y" in the cor.test
 function for spearman's correlations?
Message-ID: <11700314.post@talk.nabble.com>


Hello to all of you, R-expeRts!
I am trying to compute the cor.test for a matrix that i labelled mydata
according to mydata=read.csv...
then I converted my csv file into a matrix with the 
mydata=as.matrix(mydata)
NOW, I need to get the p-values from the correlations...
I can successfully get the spearman's correlation matrix with:
cor(mydata, method="s", use="pairwise,complete,obs")
but then if I try the 
cor.test(mydata, method="s", use="pairwise.complete.obs")
i always get an error message as follows:
the "y" argument is missing and does not have any default value assigned... 
(excuse my translation)
WHAT SHOULD I DO???
HOW CAN I DEFINE "Y" AS A NUMERIC VECTOR???????????
thanx guys, i just can't express myself as a computer would...
G :)
-- 
View this message in context: http://www.nabble.com/how-to-determine-assign-a-numeric-vector-to-%22Y%22-in-the-cor.test-function-for-spearman%27s-correlations--tf4114486.html#a11700314
Sent from the R help mailing list archive at Nabble.com.


From lzhtom at hotmail.com  Fri Jul 20 05:03:20 2007
From: lzhtom at hotmail.com (zhihua li)
Date: Fri, 20 Jul 2007 03:03:20 +0000
Subject: [R] Error: evaluation nested too deeply when doing heatmap with
 binary distfunction
In-Reply-To: <469F64A5.6030103@statistik.uni-dortmund.de>
Message-ID: <BAY110-F12E6D37B407B7B237BB700C7F40@phx.gbl>

Yes. After I increase the threshould to 10000 it got through. Thanks a lot!


>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>To: zhihua li <lzhtom at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Error: evaluation nested too deeply when doing heatmap 
with binary distfunction
>Date: Thu, 19 Jul 2007 15:18:29 +0200
>
>
>
>zhihua li wrote:
>>Hi netters,
>>
>>I have a matrix X of the size (1000,100). The values are from -3 to 
>>+3.  When I tried
>>
>>heatmap(X, 
>>distfun=function(c),dist(c,method="bin"),hclustfun=function(m),hclust(m,method="average"))

>>
>>
>>
>>I got the error message: Error: evaluation nested too deeply: 
>>infinite recursion / options(expressions=)?
>
>
>
>So, does it help to increase the thresholds?
>If not, please specify a easily reproducible example that helps us 
>to investigate your problem.
>
>Best,
>Uwe Ligges
>
>
>
>
>>However, if I used default parameters for distfunction:
>>heatmap(X, hclustfun=function(m),hclust(m,method="average"))
>>there is no error messages at all.
>>
>>But the problem is that I have to use binary method in my 
>>disfunction. How can I resolve the problem?
>>
>>Thanks a lot!
>>
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.

_________________________________________________________________
?????????????????????????????? MSN Hotmail??  http://www.hotmail.com


From ldimitro at wfubmc.edu  Fri Jul 20 05:51:24 2007
From: ldimitro at wfubmc.edu (Latchezar Dimitrov)
Date: Thu, 19 Jul 2007 23:51:24 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
Message-ID: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>

Hello,

This is a speed question. I have a dataframe genoT: 

> dim(genoT)
[1]   1002 238304

> str(genoT)
'data.frame':   1002 obs. of  238304 variables:
 $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 3 3 3 3 3
...
 $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1 1 1 2 2 2
...
 $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1
...
 $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 3 3 3 3 3
...
 $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2 3 2 3 3 1
...
 $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA 1 NA 2 1 1 2 1
...
 $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 1 1 1 1 2
...
 $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3 3 3 3 3 2
...
 $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 1 1 1 1 2
...
 $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2 1 2 1 1 3
...
 $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 2 2 3 3 3 2 1 2 2 3
...
 $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 3 3 3 3 3 3 3 3 3 3
...
 $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 2 2 2 1 1 1 2 2 2 2
...
 $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
 $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 2 3 2 2 3 2 2 1 1 2
...
 $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 1 1 1
...
 $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 1 1 2 1 1 1 1 1 1 1
...
 $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
 $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 2 1 1 1 2
...
 $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2 2 2 NA 1 NA 2
1 ...
 $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 1 2 2 1 1 1 3 1 1 1
...
 $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 2 2 2 2 2 2 2 ...
 $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1 1 ...
 $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 2 2 1
...

Its columns are factors with different number of levels (from 1 to 3 -
that's what I got from read.table, i.e., it dropped missing levels). I
want to convert it to uniform factors with 3 levels. The 1st 10 rows
above show already converted columns and the rest are not yet converted.
Here's my attempt wich is a complete failure as speed:

> system.time(
+     for(j in 1:(10         )){ #-- this is to try 1st 10 cols and
measure the time, it otherwise is ncol(genoT) instead of 10

+        gt<-genoT[[j]]          #-- this is to avoid 2D indices
+        for(l in 1:length(gt at levels)){
+          levels(gt)[l] <- switch(gt at levels[l],AA="0",AB="1",BB="2")
#-- convert levels to "0","1", or "2"
+          genoT[[j]]<-factor(gt,levels=0:2)   #-- make a 3-level factor
and put it back
+        }
+     }
+ )
[1] 785.085   4.358 789.454   0.000   0.000

789s for 10 columns only!

To me it seems like replacing 10 x 3 levels and then making a factor of
1002 element vector x 10 is a "negligible" amount of operations needed.

So, what's wrong with me? Any idea how to accelerate significantly the
transformation or (to go to the very beginning) to make read.table use a
fixed set of levels ("AA","AB", and "BB") and not to drop any (missing)
level?

R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit

The machine is with 32G RAM and AMD Opteron 285 (2.? GHz) so it's not
it.

Thank you very much for the help,

Latchezar Dimitrov,
Analyst/Programmer IV,
Wake Forest University School of Medicine,
Winston-Salem, North Carolina, USA


From bcarvalh at jhsph.edu  Fri Jul 20 06:25:03 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 20 Jul 2007 00:25:03 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
Message-ID: <DDAAAB60-ECB1-4B25-9A50-C859F3249DF8@jhsph.edu>

it looks like that whatever method you used to genotype the 1002  
samples on the STY array gave you a transposed matrix of genotype  
calls. :-)

i'd use:

genoT = read.table(yourFile, stringsAsFactors = FALSE)

as a starting point... but I don't think that would be efficient (as  
you'd need to fix one column at a time - lapply).

i'd preprocess yourFile before trying to load it:

cat yourFile | sed -e 's/AA/1/g' | sed -e 's/AB/2/g' | sed -e 's/BB/3/ 
g' > outFile

and, now, in R:

genoT = read.table(outFile, header=TRUE)

b

On Jul 19, 2007, at 11:51 PM, Latchezar Dimitrov wrote:

> Hello,
>
> This is a speed question. I have a dataframe genoT:
>
>> dim(genoT)
> [1]   1002 238304
>
>> str(genoT)
> 'data.frame':   1002 obs. of  238304 variables:
>  $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 3 3 3 3 3
> ...
>  $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1 1 1 2 2 2
> ...
>  $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1
> ...
>  $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 3 3 3 3 3
> ...
>  $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2 3 2 3 3 1
> ...
>  $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA 1 NA 2 1 1  
> 2 1
> ...
>  $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 1 1 1 1 2
> ...
>  $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3 3 3 3 3 2
> ...
>  $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 1 1 1 1 2
> ...
>  $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2 1 2 1 1 3
> ...
>  $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 2 2 3 3 3 2 1  
> 2 2 3
> ...
>  $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 3 3 3 3 3 3 3  
> 3 3 3
> ...
>  $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 2 2 2 1 1 1 2  
> 2 2 2
> ...
>  $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1  
> 1 ...
>  $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 2 3 2 2 3 2 2  
> 1 1 2
> ...
>  $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1  
> 1 1 1
> ...
>  $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 1 1 2 1 1 1 1  
> 1 1 1
> ...
>  $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1  
> 1 ...
>  $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 2 1  
> 1 1 2
> ...
>  $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2 2 2 NA 1  
> NA 2
> 1 ...
>  $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 1 2 2 1 1 1 3  
> 1 1 1
> ...
>  $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 2 2 2 2 2 2  
> 2 ...
>  $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1  
> 1 ...
>  $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1  
> 2 2 1
> ...
>
> Its columns are factors with different number of levels (from 1 to 3 -
> that's what I got from read.table, i.e., it dropped missing levels). I
> want to convert it to uniform factors with 3 levels. The 1st 10 rows
> above show already converted columns and the rest are not yet  
> converted.
> Here's my attempt wich is a complete failure as speed:
>
>> system.time(
> +     for(j in 1:(10         )){ #-- this is to try 1st 10 cols and
> measure the time, it otherwise is ncol(genoT) instead of 10
>
> +        gt<-genoT[[j]]          #-- this is to avoid 2D indices
> +        for(l in 1:length(gt at levels)){
> +          levels(gt)[l] <- switch(gt at levels[l],AA="0",AB="1",BB="2")
> #-- convert levels to "0","1", or "2"
> +          genoT[[j]]<-factor(gt,levels=0:2)   #-- make a 3-level  
> factor
> and put it back
> +        }
> +     }
> + )
> [1] 785.085   4.358 789.454   0.000   0.000
>
> 789s for 10 columns only!
>
> To me it seems like replacing 10 x 3 levels and then making a  
> factor of
> 1002 element vector x 10 is a "negligible" amount of operations  
> needed.
>
> So, what's wrong with me? Any idea how to accelerate significantly the
> transformation or (to go to the very beginning) to make read.table  
> use a
> fixed set of levels ("AA","AB", and "BB") and not to drop any  
> (missing)
> level?
>
> R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
>
> The machine is with 32G RAM and AMD Opteron 285 (2.? GHz) so it's not
> it.
>
> Thank you very much for the help,
>
> Latchezar Dimitrov,
> Analyst/Programmer IV,
> Wake Forest University School of Medicine,
> Winston-Salem, North Carolina, USA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Fri Jul 20 06:47:49 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 20 Jul 2007 00:47:49 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
Message-ID: <644e1f320707192147y6db26d06u4ecd5868185390c4@mail.gmail.com>

Is this what you want?  It took 0.01 seconds to convert 20 rows of the
test data:

> # create some data     (20 rows with 1000 columns)
> n <- 20
> result <- list()
> vals <- c("AA", "AB", "BB")
> for (i in 1:n){
+     result[[as.character(i)]] <- sample(vals,1000, replace=TRUE,
prob=c(9000,1,1))
+ }
> result.df <- do.call('data.frame', result)
>
>
> str(result.df)
'data.frame':   1000 obs. of  20 variables:
 $ X1 : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X2 : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X3 : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X4 : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X5 : Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
 $ X6 : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X7 : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X8 : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X9 : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X10: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X11: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X12: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X13: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X14: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X15: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X16: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X17: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X18: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X19: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ X20: Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
>
> # go through each row and convert the factors according to 'vals' above
> system.time({      # time to convert 20 rows
+     x <- lapply(result.df, function(facts){
+         factor(match(as.character(facts), vals) - 1, levels=0:2)
+     })
+     result.df <- do.call('data.frame', x)
+ })
   user  system elapsed
   0.01    0.00    0.01
>
> str(result.df)
'data.frame':   1000 obs. of  20 variables:
 $ X1 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X2 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X3 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X4 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X5 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X6 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X7 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X8 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X9 : Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X10: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X11: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X12: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X13: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X14: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X15: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X16: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X17: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X18: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X19: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
 $ X20: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1 ...
>


On 7/19/07, Latchezar Dimitrov <ldimitro at wfubmc.edu> wrote:
> Hello,
>
> This is a speed question. I have a dataframe genoT:
>
> > dim(genoT)
> [1]   1002 238304
>
> > str(genoT)
> 'data.frame':   1002 obs. of  238304 variables:
>  $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 3 3 3 3 3
> ...
>  $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1 1 1 2 2 2
> ...
>  $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1
> ...
>  $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 3 3 3 3 3
> ...
>  $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2 3 2 3 3 1
> ...
>  $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA 1 NA 2 1 1 2 1
> ...
>  $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 1 1 1 1 2
> ...
>  $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3 3 3 3 3 2
> ...
>  $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 1 1 1 1 2
> ...
>  $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2 1 2 1 1 3
> ...
>  $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 2 2 3 3 3 2 1 2 2 3
> ...
>  $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 3 3 3 3 3 3 3 3 3 3
> ...
>  $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 2 2 2 1 1 1 2 2 2 2
> ...
>  $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
>  $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 2 3 2 2 3 2 2 1 1 2
> ...
>  $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 1 1 1
> ...
>  $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 1 1 2 1 1 1 1 1 1 1
> ...
>  $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
>  $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 2 1 1 1 2
> ...
>  $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2 2 2 NA 1 NA 2
> 1 ...
>  $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 1 2 2 1 1 1 3 1 1 1
> ...
>  $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 2 2 2 2 2 2 2 ...
>  $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1 1 ...
>  $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 2 2 1
> ...
>
> Its columns are factors with different number of levels (from 1 to 3 -
> that's what I got from read.table, i.e., it dropped missing levels). I
> want to convert it to uniform factors with 3 levels. The 1st 10 rows
> above show already converted columns and the rest are not yet converted.
> Here's my attempt wich is a complete failure as speed:
>
> > system.time(
> +     for(j in 1:(10         )){ #-- this is to try 1st 10 cols and
> measure the time, it otherwise is ncol(genoT) instead of 10
>
> +        gt<-genoT[[j]]          #-- this is to avoid 2D indices
> +        for(l in 1:length(gt at levels)){
> +          levels(gt)[l] <- switch(gt at levels[l],AA="0",AB="1",BB="2")
> #-- convert levels to "0","1", or "2"
> +          genoT[[j]]<-factor(gt,levels=0:2)   #-- make a 3-level factor
> and put it back
> +        }
> +     }
> + )
> [1] 785.085   4.358 789.454   0.000   0.000
>
> 789s for 10 columns only!
>
> To me it seems like replacing 10 x 3 levels and then making a factor of
> 1002 element vector x 10 is a "negligible" amount of operations needed.
>
> So, what's wrong with me? Any idea how to accelerate significantly the
> transformation or (to go to the very beginning) to make read.table use a
> fixed set of levels ("AA","AB", and "BB") and not to drop any (missing)
> level?
>
> R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
>
> The machine is with 32G RAM and AMD Opteron 285 (2.? GHz) so it's not
> it.
>
> Thank you very much for the help,
>
> Latchezar Dimitrov,
> Analyst/Programmer IV,
> Wake Forest University School of Medicine,
> Winston-Salem, North Carolina, USA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ldimitro at wfubmc.edu  Fri Jul 20 08:01:27 2007
From: ldimitro at wfubmc.edu (Latchezar Dimitrov)
Date: Fri, 20 Jul 2007 02:01:27 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <DDAAAB60-ECB1-4B25-9A50-C859F3249DF8@jhsph.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
	<DDAAAB60-ECB1-4B25-9A50-C859F3249DF8@jhsph.edu>
Message-ID: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E6@EXCHVS1.medctr.ad.wfubmc.edu>

Hi,

> -----Original Message-----
> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu] 
> Sent: Friday, July 20, 2007 12:25 AM
> To: Latchezar Dimitrov
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Dataframe of factors transform speed?
> 
> it looks like that whatever method you used to genotype the 
> 1002 samples on the STY array gave you a transposed matrix of 
> genotype calls. :-)

It only looks like :-)

Otherwise it is correctly created dataframe of 1002 samples X (big
number) of columns (SNP genotypes). It worked perfectly until I decided
to put together to cohorts independently processed in R already. I got
stuck with my lack of foreseeing. Otherwise I would have put 3 dummy
lines w/ AA,AB, and AB on each one to make sure all 3 genotypes are
present and that's it! Lesson for the future :-)

Maybe I am not using columns and rows appropriately here but the
dataframe is correct (I have not used FORTRAN since FORTRAN IV ;-) - as
str says 1002 observ. of (big number) vars.

> 
> i'd use:
> 
> genoT = read.table(yourFile, stringsAsFactors = FALSE)
> 
> as a starting point... but I don't think that would be 
> efficient (as you'd need to fix one column at a time - lapply).

No it was not efficient at all. 'matter of fact nothing is more
efficient then loading already read data, alas :-(

> 
> i'd preprocess yourFile before trying to load it:
> 
> cat yourFile | sed -e 's/AA/1/g' | sed -e 's/AB/2/g' | sed -e 
> 's/BB/3/ g' > outFile
> 
> and, now, in R:
> 
> genoT = read.table(outFile, header=TRUE)

... Too late ;-) As it must be clear now I have two dataframes I want to
put together with rbind(geno1,geno2). The issue again is
"uniformization" of factor variables w/ missing factors - they ended up
like levels AA,BB on one of the and levels AB,BB on the other which
means as.numeric of AA is 1 on the 1st and as.numeric of AB is 1 on the
second - complete mess. That's why I tried to make both uniform, i.e.
levels "AA","AB", and "BB" for every SNP and then rbind works.

In any case my 1st questions remains: "What's wrong with me?" :-)

Thanks,
Latchezar

> 
> b
> 
> On Jul 19, 2007, at 11:51 PM, Latchezar Dimitrov wrote:
> 
> > Hello,
> >
> > This is a speed question. I have a dataframe genoT:
> >
> >> dim(genoT)
> > [1]   1002 238304
> >
> >> str(genoT)
> > 'data.frame':   1002 obs. of  238304 variables:
> >  $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 
> 3 3 3 3 3 
> > ...
> >  $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1 
> 1 1 2 2 2 
> > ...
> >  $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 
> 1 1 1 1 1 
> > ...
> >  $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 
> 3 3 3 3 3 
> > ...
> >  $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2 
> 3 2 3 3 1 
> > ...
> >  $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA 1 NA 2 1 1
> > 2 1
> > ...
> >  $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 
> 1 1 1 1 2 
> > ...
> >  $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3 
> 3 3 3 3 2 
> > ...
> >  $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 
> 1 1 1 1 2 
> > ...
> >  $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2 
> 1 2 1 1 3 
> > ...
> >  $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 2 2 3 3 3 2 1
> > 2 2 3
> > ...
> >  $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 3 3 3 3 3 3 3
> > 3 3 3
> > ...
> >  $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 2 2 2 1 1 1 2
> > 2 2 2
> > ...
> >  $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1
> > 1 ...
> >  $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 2 3 2 2 3 2 2
> > 1 1 2
> > ...
> >  $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1
> > 1 1 1
> > ...
> >  $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 1 1 2 1 1 1 1
> > 1 1 1
> > ...
> >  $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1
> > 1 ...
> >  $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 2 1
> > 1 1 2
> > ...
> >  $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2 
> 2 2 NA 1 NA 
> > 2
> > 1 ...
> >  $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 1 2 2 1 1 1 3
> > 1 1 1
> > ...
> >  $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 2 2 2 2 2 2
> > 2 ...
> >  $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1
> > 1 ...
> >  $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1
> > 2 2 1
> > ...
> >
> > Its columns are factors with different number of levels 
> (from 1 to 3 - 
> > that's what I got from read.table, i.e., it dropped missing 
> levels). I 
> > want to convert it to uniform factors with 3 levels. The 
> 1st 10 rows 
> > above show already converted columns and the rest are not yet 
> > converted.
> > Here's my attempt wich is a complete failure as speed:
> >
> >> system.time(
> > +     for(j in 1:(10         )){ #-- this is to try 1st 10 cols and
> > measure the time, it otherwise is ncol(genoT) instead of 10
> >
> > +        gt<-genoT[[j]]          #-- this is to avoid 2D indices
> > +        for(l in 1:length(gt at levels)){
> > +          levels(gt)[l] <- 
> switch(gt at levels[l],AA="0",AB="1",BB="2")
> > #-- convert levels to "0","1", or "2"
> > +          genoT[[j]]<-factor(gt,levels=0:2)   #-- make a 3-level  
> > factor
> > and put it back
> > +        }
> > +     }
> > + )
> > [1] 785.085   4.358 789.454   0.000   0.000
> >
> > 789s for 10 columns only!
> >
> > To me it seems like replacing 10 x 3 levels and then making 
> a factor 
> > of
> > 1002 element vector x 10 is a "negligible" amount of operations 
> > needed.
> >
> > So, what's wrong with me? Any idea how to accelerate 
> significantly the 
> > transformation or (to go to the very beginning) to make 
> read.table use 
> > a fixed set of levels ("AA","AB", and "BB") and not to drop any
> > (missing)
> > level?
> >
> > R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
> >
> > The machine is with 32G RAM and AMD Opteron 285 (2.? GHz) 
> so it's not 
> > it.
> >
> > Thank you very much for the help,
> >
> > Latchezar Dimitrov,
> > Analyst/Programmer IV,
> > Wake Forest University School of Medicine, Winston-Salem, North 
> > Carolina, USA
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting- 
> > guide.html and provide commented, minimal, self-contained, 
> > reproducible code.
>


From bcarvalh at jhsph.edu  Fri Jul 20 09:29:52 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 20 Jul 2007 03:29:52 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E6@EXCHVS1.medctr.ad.wfubmc.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
	<DDAAAB60-ECB1-4B25-9A50-C859F3249DF8@jhsph.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E6@EXCHVS1.medctr.ad.wfubmc.edu>
Message-ID: <62B73FC8-3B10-4037-A51E-2AFC6142F96E@jhsph.edu>

set.seed(123)
genoT = lapply(1:240000, function(i) factor(sample(c("AA", "AB",  
"BB"), 1000, prob=sample(c(1, 1000, 1000), 3), rep=T)))
names(genoT) = paste("snp", 1:240000, sep="")
genoT = as.data.frame(genoT)
dim(genoT)
class(genoT)
system.time(out <- lapply(genoT, function(x) match(x, c("AA", "AB",  
"BB"))-1))
##
##
    user  system elapsed
119.288   0.004 119.339

(for all 240K)

best,
b

ps: note that "out" is a list.

On Jul 20, 2007, at 2:01 AM, Latchezar Dimitrov wrote:

> Hi,
>
>> -----Original Message-----
>> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
>> Sent: Friday, July 20, 2007 12:25 AM
>> To: Latchezar Dimitrov
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Dataframe of factors transform speed?
>>
>> it looks like that whatever method you used to genotype the
>> 1002 samples on the STY array gave you a transposed matrix of
>> genotype calls. :-)
>
> It only looks like :-)
>
> Otherwise it is correctly created dataframe of 1002 samples X (big
> number) of columns (SNP genotypes). It worked perfectly until I  
> decided
> to put together to cohorts independently processed in R already. I got
> stuck with my lack of foreseeing. Otherwise I would have put 3 dummy
> lines w/ AA,AB, and AB on each one to make sure all 3 genotypes are
> present and that's it! Lesson for the future :-)
>
> Maybe I am not using columns and rows appropriately here but the
> dataframe is correct (I have not used FORTRAN since FORTRAN IV ;-)  
> - as
> str says 1002 observ. of (big number) vars.
>
>>
>> i'd use:
>>
>> genoT = read.table(yourFile, stringsAsFactors = FALSE)
>>
>> as a starting point... but I don't think that would be
>> efficient (as you'd need to fix one column at a time - lapply).
>
> No it was not efficient at all. 'matter of fact nothing is more
> efficient then loading already read data, alas :-(
>
>>
>> i'd preprocess yourFile before trying to load it:
>>
>> cat yourFile | sed -e 's/AA/1/g' | sed -e 's/AB/2/g' | sed -e
>> 's/BB/3/ g' > outFile
>>
>> and, now, in R:
>>
>> genoT = read.table(outFile, header=TRUE)
>
> ... Too late ;-) As it must be clear now I have two dataframes I  
> want to
> put together with rbind(geno1,geno2). The issue again is
> "uniformization" of factor variables w/ missing factors - they  
> ended up
> like levels AA,BB on one of the and levels AB,BB on the other which
> means as.numeric of AA is 1 on the 1st and as.numeric of AB is 1 on  
> the
> second - complete mess. That's why I tried to make both uniform, i.e.
> levels "AA","AB", and "BB" for every SNP and then rbind works.
>
> In any case my 1st questions remains: "What's wrong with me?" :-)
>
> Thanks,
> Latchezar
>
>>
>> b
>>
>> On Jul 19, 2007, at 11:51 PM, Latchezar Dimitrov wrote:
>>
>>> Hello,
>>>
>>> This is a speed question. I have a dataframe genoT:
>>>
>>>> dim(genoT)
>>> [1]   1002 238304
>>>
>>>> str(genoT)
>>> 'data.frame':   1002 obs. of  238304 variables:
>>>  $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
>> 3 3 3 3 3
>>> ...
>>>  $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1
>> 1 1 2 2 2
>>> ...
>>>  $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1
>> 1 1 1 1 1
>>> ...
>>>  $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
>> 3 3 3 3 3
>>> ...
>>>  $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2
>> 3 2 3 3 1
>>> ...
>>>  $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA 1 NA 2 1 1
>>> 2 1
>>> ...
>>>  $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
>> 1 1 1 1 2
>>> ...
>>>  $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3
>> 3 3 3 3 2
>>> ...
>>>  $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
>> 1 1 1 1 2
>>> ...
>>>  $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2
>> 1 2 1 1 3
>>> ...
>>>  $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 2 2 3 3 3 2 1
>>> 2 2 3
>>> ...
>>>  $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 3 3 3 3 3 3 3
>>> 3 3 3
>>> ...
>>>  $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 2 2 2 1 1 1 2
>>> 2 2 2
>>> ...
>>>  $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1
>>> 1 ...
>>>  $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 2 3 2 2 3 2 2
>>> 1 1 2
>>> ...
>>>  $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1
>>> 1 1 1
>>> ...
>>>  $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 1 1 2 1 1 1 1
>>> 1 1 1
>>> ...
>>>  $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1
>>> 1 ...
>>>  $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 2 1
>>> 1 1 2
>>> ...
>>>  $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2
>> 2 2 NA 1 NA
>>> 2
>>> 1 ...
>>>  $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 1 2 2 1 1 1 3
>>> 1 1 1
>>> ...
>>>  $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 2 2 2 2 2 2
>>> 2 ...
>>>  $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1
>>> 1 ...
>>>  $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1
>>> 2 2 1
>>> ...
>>>
>>> Its columns are factors with different number of levels
>> (from 1 to 3 -
>>> that's what I got from read.table, i.e., it dropped missing
>> levels). I
>>> want to convert it to uniform factors with 3 levels. The
>> 1st 10 rows
>>> above show already converted columns and the rest are not yet
>>> converted.
>>> Here's my attempt wich is a complete failure as speed:
>>>
>>>> system.time(
>>> +     for(j in 1:(10         )){ #-- this is to try 1st 10 cols and
>>> measure the time, it otherwise is ncol(genoT) instead of 10
>>>
>>> +        gt<-genoT[[j]]          #-- this is to avoid 2D indices
>>> +        for(l in 1:length(gt at levels)){
>>> +          levels(gt)[l] <-
>> switch(gt at levels[l],AA="0",AB="1",BB="2")
>>> #-- convert levels to "0","1", or "2"
>>> +          genoT[[j]]<-factor(gt,levels=0:2)   #-- make a 3-level
>>> factor
>>> and put it back
>>> +        }
>>> +     }
>>> + )
>>> [1] 785.085   4.358 789.454   0.000   0.000
>>>
>>> 789s for 10 columns only!
>>>
>>> To me it seems like replacing 10 x 3 levels and then making
>> a factor
>>> of
>>> 1002 element vector x 10 is a "negligible" amount of operations
>>> needed.
>>>
>>> So, what's wrong with me? Any idea how to accelerate
>> significantly the
>>> transformation or (to go to the very beginning) to make
>> read.table use
>>> a fixed set of levels ("AA","AB", and "BB") and not to drop any
>>> (missing)
>>> level?
>>>
>>> R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
>>>
>>> The machine is with 32G RAM and AMD Opteron 285 (2.? GHz)
>> so it's not
>>> it.
>>>
>>> Thank you very much for the help,
>>>
>>> Latchezar Dimitrov,
>>> Analyst/Programmer IV,
>>> Wake Forest University School of Medicine, Winston-Salem, North
>>> Carolina, USA
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html and provide commented, minimal, self-contained,
>>> reproducible code.
>>


From connect.chris at gmail.com  Fri Jul 20 09:34:55 2007
From: connect.chris at gmail.com (Chris Linton)
Date: Fri, 20 Jul 2007 03:34:55 -0400
Subject: [R] GEE code
Message-ID: <b05bf6c40707200034t193fac27qb8f9f2d4c702a247@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/ebcb68f0/attachment.pl 

From ba208 at exeter.ac.uk  Fri Jul 20 09:42:10 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Fri, 20 Jul 2007 08:42:10 +0100
Subject: [R] custom point shapes
Message-ID: <C2A248D1-5752-40AE-A037-C795D519FD78@ex.ac.uk>

Hi,

I'm new to R, but a search through the list didn't quite solve this  
problem: I want to draw a few ellipses (or any custom shape for that  
matter) at given locations in a graph. I know how to plot points from  
my data, set the point type "pch" to any built in value, but I do not  
know how to specify a custom shape.
A search for drawing ellipses gave me functions to plot one at a  
given location, possibly using some dataset but only to display a  
confidence interval or other convex hull. Nothing to use as pch.

Say I have this data:

x<-c(1:10)
y<-x^2

I would like to plot (x,y) with some ellipses of custom aspect ratio  
for every point.

Thanks,

Best regards,

baptiste


From Achim.Zeileis at wu-wien.ac.at  Fri Jul 20 09:59:19 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 20 Jul 2007 09:59:19 +0200 (CEST)
Subject: [R] Help with Dates
In-Reply-To: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
Message-ID: <Pine.LNX.4.44.0707200954200.7607-100000@disco.wu-wien.ac.at>

Alex:

> I am taking an excel dataset and reading it into R using read.table.

This sets up a "data.frame" object. The data you have are probably more
conveniently represented as a time series, storing the date in an
appropriate format, e.g., in class "Date".

> (actually I am dumping the data into a .txt file first and then reading data
> in to R).

Then you can do both steps (calling read.table() and transformation to a
time series) in one go using the function read.zoo() from package "zoo".

If your text file looks like

     Date  Price Open.Int. Comm.Long Comm.Short net.comm
15-Jan-86 673.25    175645     65910	  28425    37485
31-Jan-86 677.00    167350     54060	  27120    26940
14-Feb-86 680.25    157985     37955	  25425    12530
28-Feb-86 691.75    162775     49760	  16030    33730
14-Mar-86 706.50    163495     54120	  27995    26125
31-Mar-86 709.75    164120     54715	  30390    24325

then you can read it in via
  z <- read.zoo("mydata.txt", format = "%d-%b-%y", header = TRUE)

Then you can do all sorts of standard things for time series, such as
  plot(z)
or...

> The dataset runs from 1986 to 2007.
>
> I want to be able to take subsets of my data based on date e.g. data between
> 2000 - 2005.

...subsetting

  z2 <- window(z, start = as.Date("2000-01-01"), end = as.Date("2005-12-31"))

etc. Look at the "zoo" package vignettes for more information
  vignette("zoo-quickref", package = "zoo")
  vignette("zoo", package = "zoo")

hth,
Z

> As it stands, I can't work with the dates as they are not in correct format.
>
> I tried successfully converting the dates to just the year using:
>
> transform(data, Yr = format(as.Date(as.character(Date),format = '%d-%b-%y'),
> "%y")))
>
> This gives the following format:
>
>        Date  Price Open.Int. Comm.Long Comm.Short net.comm Yr
> 1 15-Jan-86 673.25    175645     65910      28425    37485 86
> 2 31-Jan-86 677.00    167350     54060      27120    26940 86
> 3 14-Feb-86 680.25    157985     37955      25425    12530 86
> 4 28-Feb-86 691.75    162775     49760      16030    33730 86
> 5 14-Mar-86 706.50    163495     54120      27995    26125 86
> 6 31-Mar-86 709.75    164120     54715      30390    24325 86
>
> I can subset for a single year e.g:
>
> head(subset(df, Yr =="00")
>
> But how can I subset for multiple periods e.g 00- 05? The following won't
> work:
>
> head(subset(df, Yr =="00" & Yr=="01")
>
> or
>
> head(subset(df, Yr = c("00","01","02","03")
>
> I can't help but feeling that I am missing something and there is a simpler
> route.
>
> I leafed through R newletter 4.1 which deals with dates and times but it
> seemed that strptime and POSIXct / POSIXlt are not what I need either.
>
> Can anybody help me?
>
> Regards
>
>
> Alex
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From dimitris.rizopoulos at med.kuleuven.be  Fri Jul 20 10:19:05 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 20 Jul 2007 10:19:05 +0200
Subject: [R] GEE code
References: <b05bf6c40707200034t193fac27qb8f9f2d4c702a247@mail.gmail.com>
Message-ID: <001401c7caa6$a33b68b0$0540210a@www.domain>

from your description I think you need a random-effects model. Since 
you assume normality the parameter estimates from a random-effects 
model (in your case `GDP.log2') have a marginal interpretation (in 
fact they have both conditional and marginal interpretation). Thus, 
you could also use lmer() from package `lme4' to fit such a model, 
e.g.,

fit1 <- lmer(ineq ~ GDP.log2 + (1 | country), data = data3)
summary(fit1)
# intercept + Emprirical Bayes estimates of
# the random-effect per country
fixef(fit1)[1] + unlist(ranef(fit1))

If you have repeated measurement in time, you could include 
random-slopes (e.g., in lmer()) or an AR1 structure, as you did below. 
For the latter case, you probably want to use functions lme() and 
gls() from package `nlme'.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Chris Linton" <connect.chris at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, July 20, 2007 9:34 AM
Subject: [R] GEE code


> I'm writing a paper aimed at motivating the use of GEE within the 
> field of
> economics.  However, after computing using the geeglm function, I 
> noticed
> there's one intercept in the summary output.  I assume this means 
> the
> function is pooling the data.  That means my code is not what I 
> want.  I
> want a "fixed effects" model, meaning I want the intercept to vary 
> by
> cluster.  Here's my current code.
>
> gfit4<-geeglm(ineq~GDP.log2,family=gaussian(link="identity"),data=data3,id=country,corstr="ar1")
> summary(gfit4)
>
>
> What do I need to include to get the function to compute a model 
> where the
> intercept varies by the country?
>
>
>
> Thanks
>
> -chris linton
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From Joao.Fadista at agrsci.dk  Fri Jul 20 10:32:41 2007
From: Joao.Fadista at agrsci.dk (=?iso-8859-1?Q?Jo=E3o_Fadista?=)
Date: Fri, 20 Jul 2007 10:32:41 +0200
Subject: [R] binned column in a data.frame
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F3C@DJFPOST01.djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/bf95374a/attachment.pl 

From mark_difford at yahoo.co.uk  Fri Jul 20 10:57:40 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Fri, 20 Jul 2007 01:57:40 -0700 (PDT)
Subject: [R] Trend lines on a scatterplot matrix
In-Reply-To: <1184881883.17296.0.camel@papagena>
References: <1184881883.17296.0.camel@papagena>
Message-ID: <11704499.post@talk.nabble.com>


Hi Alan, 

There is a good, ready made, way of doing this, with additional options, in
Professor Fox's car package.  See:---

require(car)
?scatterplot.matrix

Cheers,
Mark.


Alan S Barnett-2 wrote:
> 
> I'm using pairs() to generate a scatterplot matrix;
> 
> pairs(~
> Fuzzy.gray.white.ratio+Fuzzy.gw.t.score+AgeWhenTested+signal_mean.noise,
> 	data=datam,subset=status=="control",main="Controls",
>         labels=c("G/W","Peak Separation","Age","S/N"))
> 
> 
> How can I add regression lines to the plots?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Trend-lines-on-a-scatterplot-matrix-tf4113952.html#a11704499
Sent from the R help mailing list archive at Nabble.com.


From birgit.lemcke at systbot.uzh.ch  Fri Jul 20 11:23:26 2007
From: birgit.lemcke at systbot.uzh.ch (Birgit Lemcke)
Date: Fri, 20 Jul 2007 11:23:26 +0200
Subject: [R] plot3d labels
In-Reply-To: <644e1f320707190948l8c9044cl42477844d7eff7d9@mail.gmail.com>
References: <43F15BE7-FF80-48CC-A8B9-554196A9B16B@systbot.uzh.ch>
	<644e1f320707190948l8c9044cl42477844d7eff7d9@mail.gmail.com>
Message-ID: <C8EED921-6252-4F93-B352-DB6AE9270379@systbot.uzh.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/f462b10e/attachment.pl 

From mmeredith at wcs.org  Fri Jul 20 11:46:12 2007
From: mmeredith at wcs.org (Mike Meredith)
Date: Fri, 20 Jul 2007 02:46:12 -0700 (PDT)
Subject: [R] binned column in a data.frame
In-Reply-To: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F3C@DJFPOST01.djf.agrsci.dk>
References: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F3C@DJFPOST01.djf.agrsci.dk>
Message-ID: <11705240.post@talk.nabble.com>



Try something like this:

x <- c(1,2,6,8,13,0,5,10, runif(10) * 100)
tmp <- x %/% 5 + 1
bin.names <- paste((0:19) * 5, (1:20) * 5, sep="-")
data.frame(Start = x, Binned_Start = bin.names[tmp])

This assumes you want (eg.) 5 in the 5-10 bin, not the 0-5 bin. You may also
want to make Binned_Start into a factor rather than a character vector.

HTH, Mike.



Jo?o Fadista wrote:
> 
> Dear all,
>  
> I would like to know how can I create a binned column in a data.frame. The
> output that I would like is something like this: 
>  
> Start  Binned_Start
> 1        0-5
> 2        0-5
> 6        5-10
> 8        5-10
> 13      10-15
> ...            
>  
>  
>  
> 
> Best regards
> 
> Jo?o Fadista
> Ph.d. student
> 
> 
>  	
>  	 UNIVERSITY OF AARHUS	
> Faculty of Agricultural Sciences	
> Dept. of Genetics and Biotechnology	
> Blichers All? 20, P.O. BOX 50	
> DK-8830 Tjele	
>  	
> Phone:	 +45 8999 1900	
> Direct:	 +45 8999 1900	
> E-mail:	 Joao.Fadista at agrsci.dk <mailto:Joao.Fadista at agrsci.dk> 	
> Web:	 www.agrsci.org <http://www.agrsci.org/> 	
> ________________________________
> 
> News and news media <http://www.agrsci.org/navigation/nyheder_og_presse> .
> 
> This email may contain information that is confidential. Any use or
> publication of this email without written permission from Faculty of
> Agricultural Sciences is not allowed. If you are not the intended
> recipient, please notify Faculty of Agricultural Sciences immediately and
> delete this email.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/binned-column-in-a-data.frame-tf4115787.html#a11705240
Sent from the R help mailing list archive at Nabble.com.


From murdoch at stats.uwo.ca  Fri Jul 20 11:53:50 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 20 Jul 2007 05:53:50 -0400
Subject: [R] plot3d labels
In-Reply-To: <C8EED921-6252-4F93-B352-DB6AE9270379@systbot.uzh.ch>
References: <43F15BE7-FF80-48CC-A8B9-554196A9B16B@systbot.uzh.ch>	<644e1f320707190948l8c9044cl42477844d7eff7d9@mail.gmail.com>
	<C8EED921-6252-4F93-B352-DB6AE9270379@systbot.uzh.ch>
Message-ID: <46A0862E.6000509@stats.uwo.ca>

On 20/07/2007 5:23 AM, Birgit Lemcke wrote:
> Hello Jim,
> 
> thanks for your answer but  I still have problems with the labels.  
> Perhaps you can help me a second time.
> 
> I have the output of cmdscale and it looks like this:
> 
>                        [,1]          [,2]          [,3]
> Anth_cap      -0.011833788 -0.1091355289 -0.0222467839
> Anth_crin     -0.008178993 -0.0219545815 -0.0076695878
> Anth_eck       0.026900827 -0.0422055677 -0.1340542844
> Anth_gram     -0.010860990  0.0080112322  0.0201743866.........
> 
> In the next step I created 3 vectors out of the three columns
> 
> PCoA1<-PCoA[,1]
> PCoA2<-PCoA[,2]
> PCoA3<-PCoA[,3]
> 
> After that I plotted the 3 vectors
> 
> plot3d(PCoA1,PCoA2,PCoA3, type="p", col=rainbow(1000),size=5)

There's no need to extract the 3 columns:  you could just use

plot3d(PCoA, type="p", col=rainbow(1000),size=5)

with the data above.
> 
> and now I try to handle this text3d function and label the points in  
> the graphic with the names in the first column. But I am a beginner  
> and don?t know the syntax for this in the text3d.
> 
> I tried to make vectors out of every row like this:
> 
> Anth_cap<- PCoA[1,]
>  > Anth_crin<- PCoA[2,]
>  > Anth_eck<- PCoA[3,]
>  > Anth_gram<- PCoA[4,]
>  > Anth_insi<- PCoA[5,]
>  > Anth_laxi<- PCoA[6,].........
> 
> and than tried to implement this in the text3d:
> 
> text3d( c(Anth_cap,
> Anth_crin,

I think all you need is

text3d(PCoA, text=Nam)

but you might want to set the adj argument if you want the labels offset 
from the points, and you might want type="n" in the original plot3d call 
if you don't want the points to interfere with the labels.

Duncan Murdoch


From tobias.minder at bluewin.ch  Fri Jul 20 11:57:40 2007
From: tobias.minder at bluewin.ch (squall44)
Date: Fri, 20 Jul 2007 02:57:40 -0700 (PDT)
Subject: [R] (R) Using arguments for the empirical cumulative
 distribution function
In-Reply-To: <11690150.post@talk.nabble.com>
References: <11690150.post@talk.nabble.com>
Message-ID: <11705448.post@talk.nabble.com>


Is there no one who can help me? :,(
-- 
View this message in context: http://www.nabble.com/%28R%29-Using-arguments-for-the-empirical-cumulative-distribution-function-tf4111355.html#a11705448
Sent from the R help mailing list archive at Nabble.com.


From dieter.menne at menne-biomed.de  Fri Jul 20 12:22:59 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 20 Jul 2007 10:22:59 +0000 (UTC)
Subject: [R] custom point shapes
References: <C2A248D1-5752-40AE-A037-C795D519FD78@ex.ac.uk>
Message-ID: <loom.20070720T122055-760@post.gmane.org>

baptiste Augui? <ba208 <at> exeter.ac.uk> writes:

> I'm new to R, but a search through the list didn't quite solve this  
> problem: I want to draw a few ellipses (or any custom shape for that  
> matter) at given locations in a graph. I know how to plot points from  
> my data, set the point type "pch" to any built in value, but I do not  
> know how to specify a custom shape.
> A search for drawing ellipses gave me functions to plot one at a  
> given location, possibly using some dataset but only to display a  
> confidence interval or other convex hull. Nothing to use as pch.
> 
> Say I have this data:
> 
> x<-c(1:10)
> y<-x^2
> 
> I would like to plot (x,y) with some ellipses of custom aspect ratio  
> for every point.

If you want to use pch, you are stuck with the existing symbols, and don't have
much freedom in manipulating these. Looks like you are best off with ellipse in
package car that you probably found already.

Dieter


From Mike.Lawrence at DAL.CA  Fri Jul 20 12:59:48 2007
From: Mike.Lawrence at DAL.CA (Mike Lawrence)
Date: Fri, 20 Jul 2007 07:59:48 -0300
Subject: [R] (R) Using arguments for the empirical cumulative
 distribution function
In-Reply-To: <11705448.post@talk.nabble.com>
References: <11690150.post@talk.nabble.com> <11705448.post@talk.nabble.com>
Message-ID: <D64F088F-ED35-4B62-968F-44A6AD818E9F@DAL.CA>

?plot.ecdf()

On 20-Jul-07, at 6:57 AM, squall44 wrote:

>
> Is there no one who can help me? :,(
> --  
> View this message in context: http://www.nabble.com/%28R%29-Using- 
> arguments-for-the-empirical-cumulative-distribution-function- 
> tf4111355.html#a11705448
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Mike Lawrence
Graduate Student, Department of Psychology, Dalhousie University

Website: http://memetic.ca

Public calendar: http://icalx.com/public/informavore/Public

"The road to wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
	- Piet Hein


From ligges at statistik.uni-dortmund.de  Fri Jul 20 13:01:24 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Jul 2007 13:01:24 +0200
Subject: [R] how to determine/assign a numeric vector to "Y" in the
 cor.test function for spearman's correlations?
In-Reply-To: <11700314.post@talk.nabble.com>
References: <11700314.post@talk.nabble.com>
Message-ID: <46A09604.3010308@statistik.uni-dortmund.de>



G. wrote:
> Hello to all of you, R-expeRts!
> I am trying to compute the cor.test for a matrix that i labelled mydata
> according to mydata=read.csv...
> then I converted my csv file into a matrix with the 
> mydata=as.matrix(mydata)
> NOW, I need to get the p-values from the correlations...
> I can successfully get the spearman's correlation matrix with:
> cor(mydata, method="s", use="pairwise,complete,obs")
> but then if I try the 
> cor.test(mydata, method="s", use="pairwise.complete.obs")
> i always get an error message as follows:
> the "y" argument is missing and does not have any default value assigned... 
> (excuse my translation)
> WHAT SHOULD I DO???
> HOW CAN I DEFINE "Y" AS A NUMERIC VECTOR???????????
> thanx guys, i just can't express myself as a computer would...
> G :)


You have to specify two vectors for the test, e.g.:

   cor.test(mydata[,1], mydata[,2], method = "s",
     use = "pairwise.complete.obs")

See the help page ?cor.test

Best,
Uwe


From ligges at statistik.uni-dortmund.de  Fri Jul 20 13:09:45 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Jul 2007 13:09:45 +0200
Subject: [R] plot centered line on barplot
In-Reply-To: <JLFJG5$2C2CCDF74EEB7C6120E4BDAE688BF15A@libero.it>
References: <JLFJG5$2C2CCDF74EEB7C6120E4BDAE688BF15A@libero.it>
Message-ID: <46A097F9.8070606@statistik.uni-dortmund.de>

claudio.is at libero.it wrote:
> Dear R user,
> 
> I need plot an histogram for the occurrence of a dataset, and then add a line corresponding to the freuqnecy of another similar dataset. in order to do this i used the function 
>> hist_data1=hist(data1, breaks= seq(0,50,5), plot=FALSE)
>> hist_data2=hist(data2, breaks= seq(0,50,5), plot=FALSE)
> 
> then I plotted the frequency
> 
>> barplot(hist_data1$density)
>> lines(hist_data1$density)


barplot() returns the positions on x-axis (these may be non-integers) 
where it draws the plots, hence you can say:

  bp <- barplot(hist_data1$density)
  lines(bp, hist_data1$density)


> but the line is shifted in respect to the center of the bars. how can I properly plot the line? another question. this is easy, how can I smooth the curve (not fit with loess of spline)?

If you do not tell us which kind of smoother you prefer ....

Uwe Ligges



> 
> tnx
> 
> --
> Claudio
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Fri Jul 20 13:11:21 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Jul 2007 13:11:21 +0200
Subject: [R] Can I test if there are statistical significance between
 different rows in R*C table?
In-Reply-To: <2fc17e30707190707u6856591al7a0b883e6ff7a236@mail.gmail.com>
References: <2fc17e30707190707u6856591al7a0b883e6ff7a236@mail.gmail.com>
Message-ID: <46A09859.9030805@statistik.uni-dortmund.de>



zhijie zhang wrote:
> Dear  friends,
>   My R*C table is as follow:
> 
> 
> 
> better
> 
> good
> 
> bad
> 
> Goup1
> 
> 16
> 
> 71
> 
> 37
> 
> Group2
> 
> 0
> 
> 4
> 
> 61
> 
> Group3
> 
> 1
> 
> 6
> 
> 57
> 
>    Can I test if there are statistical significant between Group1 and
> Group2, Group2 and Group3, Group1 and Group2, taking into the multiple
> comparisons?


So what is you hypothesis? Statistical significance of what it to be tested?

Uwe Ligges



> The table can be set up using the following program:
> 
> a<-matrix(data=c(16,71,37,0,4,61,1,6,57),nrow=3,byrow=TRUE)
> Thanks very much.
> 
>


From birgit.lemcke at systbot.uzh.ch  Fri Jul 20 13:33:08 2007
From: birgit.lemcke at systbot.uzh.ch (Birgit Lemcke)
Date: Fri, 20 Jul 2007 13:33:08 +0200
Subject: [R] plot3d labels
In-Reply-To: <46A0862E.6000509@stats.uwo.ca>
References: <43F15BE7-FF80-48CC-A8B9-554196A9B16B@systbot.uzh.ch>	<644e1f320707190948l8c9044cl42477844d7eff7d9@mail.gmail.com>
	<C8EED921-6252-4F93-B352-DB6AE9270379@systbot.uzh.ch>
	<46A0862E.6000509@stats.uwo.ca>
Message-ID: <4303E063-21FA-4739-9A49-75DBA26ECA34@systbot.uzh.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/504db8fd/attachment.pl 

From fuzzyflou at yahoo.fr  Fri Jul 20 13:45:23 2007
From: fuzzyflou at yahoo.fr (Paul)
Date: Fri, 20 Jul 2007 13:45:23 +0200 (CEST)
Subject: [R] RDCOM and R versions
Message-ID: <20070720114524.96858.qmail@web27710.mail.ukl.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070720/5b905723/attachment.pl 

From f.harrell at vanderbilt.edu  Fri Jul 20 14:06:00 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 20 Jul 2007 07:06:00 -0500
Subject: [R] exclude1 in summary.formula from Hmisc
In-Reply-To: <772cb06e0707191659m5d012eb8o28bb040a2780eaf4@mail.gmail.com>
References: <772cb06e0707191659m5d012eb8o28bb040a2780eaf4@mail.gmail.com>
Message-ID: <46A0A528.5020003@vanderbilt.edu>

david dav wrote:
> Dear Users,
> Still having troubles with sharing my results, I'm trying to display a
> contingency table using summary.formula.
> Computing works well but I'd like to display information on redundant
> entries say, males AND females.

Please tell us what output you got.  And it is sometimes helpful to get 
a trivial example of the failure with simulated data.

Also see a related parameter "long".

Frank

> I wonder if this code is correct :
> 
> desqualjum <- summary.formula(varqual1$X ~.,  data = subset( varqual1,
> select = -X), method = "reverse",  overall = T, test = T, exclude1 = FALSE)
> where varqual1 is the data frame containing the variable "sex".
> 
> I'm using R 2.5.1 and Hmisc 3.4-2 on a Mac (OSX.4, intel)  but I had the
> same trouble with  former versions of R and the package.
> 
> Thank you for your help.
> David
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From ripley at stats.ox.ac.uk  Fri Jul 20 14:21:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jul 2007 13:21:40 +0100 (BST)
Subject: [R] RDCOM and R versions
In-Reply-To: <20070720114524.96858.qmail@web27710.mail.ukl.yahoo.com>
References: <20070720114524.96858.qmail@web27710.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.64.0707201307370.20398@gannet.stats.ox.ac.uk>

There is a separate list for support of RDCOM at

http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l

The answer to your question depends a bit how you are making use of RDCOM: 
the R interface is via rproxy.dll, and that recognizes R_HOME.  This is 
also mentioned in the installation instructions for the server.  So I 
would expect setting R_HOME in the appropriate environment to allow you to 
choose which R to use: it used to work some years ago.

On Fri, 20 Jul 2007, Paul wrote:

> Dear R-helpers,
>
>  I have several versions of R installed on my computer, and I cannot do without any of these.
>  However RCDOM seems to authorize only one version installed. Do you know any means to overcome this problem ?
>  Thank you very much for your response.
>
> Paul Poncet
>
>
>
> ---------------------------------
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From davidsonrac at gmail.com  Fri Jul 20 14:54:37 2007
From: davidsonrac at gmail.com (Rachel Davidson)
Date: Fri, 20 Jul 2007 08:54:37 -0400
Subject: [R] cv.glm error function
Message-ID: <651263270707200554q6cc2f6abu11d6dd8683e06b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/3f27de65/attachment.pl 

From jholtman at gmail.com  Fri Jul 20 14:56:28 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 20 Jul 2007 08:56:28 -0400
Subject: [R] binned column in a data.frame
In-Reply-To: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F3C@DJFPOST01.djf.agrsci.dk>
References: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F3C@DJFPOST01.djf.agrsci.dk>
Message-ID: <644e1f320707200556r24eaff7ewff6c2b69bf4fe8b7@mail.gmail.com>

You can also use 'cut' to break the bins:

> x <- c(1,2,6,8,13,0,5,10, runif(10) * 100)
> x.bins <- seq(0, max(x)+5, 5)
> x.cut <- cut(x, breaks=x.bins, include.lowest=TRUE)
> x.names <- paste(head(x.bins, -1), tail(x.bins, -1), sep='-')
> data.frame(x, bins=x.names[x.cut])
          x  bins
1   1.00000   0-5
2   2.00000   0-5
3   6.00000  5-10
4   8.00000  5-10
5  13.00000 10-15
6   0.00000   0-5
7   5.00000   0-5
8  10.00000  5-10
9  75.85256 75-80
10 38.20424 35-40
11 77.30647 75-80
12 62.02278 60-65
13 73.42095 70-75
14 78.69244 75-80
15 66.52972 65-70
16 61.64897 60-65
17 23.99252 20-25
18 42.08632 40-45


On 7/20/07, Jo?o Fadista <Joao.Fadista at agrsci.dk> wrote:
> Dear all,
>
> I would like to know how can I create a binned column in a data.frame. The output that I would like is something like this:
>
> Start  Binned_Start
> 1        0-5
> 2        0-5
> 6        5-10
> 8        5-10
> 13      10-15
> ...
>
>
>
>
> Best regards
>
> Jo?o Fadista
> Ph.d. student
>
>
>
>         UNIVERSITY OF AARHUS
> Faculty of Agricultural Sciences
> Dept. of Genetics and Biotechnology
> Blichers All? 20, P.O. BOX 50
> DK-8830 Tjele
>
> Phone:   +45 8999 1900
> Direct:  +45 8999 1900
> E-mail:  Joao.Fadista at agrsci.dk <mailto:Joao.Fadista at agrsci.dk>
> Web:     www.agrsci.org <http://www.agrsci.org/>
> ________________________________
>
> News and news media <http://www.agrsci.org/navigation/nyheder_og_presse> .
>
> This email may contain information that is confidential. Any use or publication of this email without written permission from Faculty of Agricultural Sciences is not allowed. If you are not the intended recipient, please notify Faculty of Agricultural Sciences immediately and delete this email.
>
>
>        [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From dgvirtual at akl.lt  Fri Jul 20 14:59:18 2007
From: dgvirtual at akl.lt (Donatas G.)
Date: Fri, 20 Jul 2007 15:59:18 +0300
Subject: [R] how do I draw such a barplot?
Message-ID: <200707201559.18964.dgvirtual@akl.lt>

I did not have much success with ggplot2 and reshape libraries, so finally I 
managed to work out the graph in a rather complicated way. The only thing I 
cannot figure out is, how do I place the legend outside the barplot column 
area?

attach(tolerancija.data)
ateist = table(G09_01)
dvasin = table(G09_02)
gamtos = table(G09_03)
kr_klm = table(G09_04)
musulm = table(G09_05)
na_kri = table(G09_06)
pagoni = table(G09_07)
satani = table(G09_08)
ryt?_k = table(G09_08)
satani = table(G09_09)
tradic = table(G09_10)
eilutes=cbind(ateist,dvasin,gamtos,krik??,musulm,kr_klm,na_kri,pagoni,ryt?_k,satani,tradic)
barplot(prop.table(eilutes,2),col=c("red3","red2","cyan","green2","green3"),legend.text=c("visi?kai 
sutinku","nesutinku","abejoju","sutinku","visi?kai sutinku"),main="My title")
dettach(tolerancija.data)

However, this is a lot of writing. How do I make into a function to be 
reusable? 


Donatas

On Jul 16, 2007, at 9:06 , Donatas G. wrote:

> Hi,
>
> I cannot figure out how to draw a certain plot: could someone help ?
> me out?
>
> I have this data.frame from a survey
> my.data
>
> that looks like something like this:
>
> ? ?col1 ?col2 ?col3 ?col4
> 1 ? ? 5 ? ? 5 ? ? 4 ? ? 5
> 2 ? ? 3 ? ? 5 ? ? 3 ? ? 1
> 3 ? ? 2 ? ? 3 ? ? 4 ? ? 5
> 4 ? ? 3 ? ? 1 ? ? 1 ? ? 2
> 5 ? ? 5 ? ? 5 ? ? 4 ? ? 5
> 6 ? ? 4 ? ? 2 ? ? 5 ? ? 5
> ....
>
>
> Each row represents a single questionnaire with someone giving his
> agreement/disagreement with a statement (each column is a ?
> statement) that is
> coded from 1 to 5.
>
> I need to draw a barplot giving a visual representation showing ?
> differences
> between the five columns: Each bar should represent a single ?
> column, and
> should be divided into 5 sections, the thickness of each depending ?
> on the
> number of respondents who choose that particular answer.
>
> How do I do that? All I have managed to do so far is to produce a ?
> barplot of a
> single column, and that - only with bars side by side...


-- 
Donatas Glodenis
http://dg.lapas.info


From guardhousegnk690 at herblip.net  Fri Jul 20 16:11:33 2007
From: guardhousegnk690 at herblip.net (Susan Parham)
Date: Fri, 20 Jul 2007 13:11:33 -0100
Subject: [R] Huh?
Message-ID: <630396225.40375024539373@herblip.net>


   Vipxiltxyagdohhrfhxrhra $1.79 Cnkwqaffsuaialepqyemfris = $3.93

    and many other items for 10% of the price.
    [1]Click to visit the shop 
     _________________________________________________________________

References

   1. file://localhost/tmp/tmpuDywbA.html

From aa2007r at gmail.com  Fri Jul 20 15:17:12 2007
From: aa2007r at gmail.com (AA)
Date: Fri, 20 Jul 2007 09:17:12 -0400
Subject: [R] (R) Using arguments for the empirical cumulative
	distribution function
References: <11690150.post@talk.nabble.com>
Message-ID: <039a01c7cad0$49346f90$3927a8c0@treesdalellc.net>

Hi Tobias,
Maybe I do not understand the issue here but the following line adds main 
and xlab
plot(F10,verticals = TRUE, do.p = TRUE, lwd = 3, main = "myTitle", xlab = 
"myXlab")
You could find all this in the introduction manual. see r-project.org in the 
documentation section.
Good luck
AA.
----- Original Message ----- 
From: "squall44" <tobias.minder at bluewin.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, July 19, 2007 10:13 AM
Subject: [R] (R) Using arguments for the empirical cumulative distribution 
function


>
> Hi,
>
> I have just started using R. Now I have the following problem:
>
> I want to create an Empirical Cumulative Distribution Function and I only
> came so far:
>
> F10 <- ecdf(x)
> plot(F10, verticals= TRUE, do.p = TRUE, lwd=3)
> x=c(1.6,1.8,2.4,2.7,2.9,3.3,3.4,3.4,4,5.2)
>
> Now I'd like to use arguments such as xlabs and main but I don't know how 
> to
> integrate them.
>
> I hope someone can help me, I am really stuck!
>
> -- 
> View this message in context: 
> http://www.nabble.com/%28R%29-Using-arguments-for-the-empirical-cumulative-distribution-function-tf4111355.html#a11690150
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From amnakhan493 at gmail.com  Fri Jul 20 15:20:18 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Fri, 20 Jul 2007 06:20:18 -0700
Subject: [R] Graphical Parameters
Message-ID: <3ffd3bb60707200620k11111d65t56ce4ef644340f3f@mail.gmail.com>

Hi Sir

There is a difficulty in creating a comprehensive graph.
Suppose I have a time series from 1967 to 2006. I have used the following R
code for creating a graph.

>plot(Year,Dir,ylab="Annual Maximum Daily Rainfall (mm)",cex.lab=1.3
,type="o",lab=c(20,20,25),mgp=c(2.5,1,0),tck=1)
The graph is attached with this email.
Sir I want the grid lines as dashed lines. Second thing to be asked is that
the grid lines should make squared blocks not rectangles. Moreover the
boundries are not in good format.

Sir I request you to please guide in this regards.

Thank You.




-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com

From amnakhan493 at gmail.com  Fri Jul 20 15:20:18 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Fri, 20 Jul 2007 06:20:18 -0700
Subject: [R] Graphical Parameters
Message-ID: <3ffd3bb60707200620k11111d65t56ce4ef644340f3f@mail.gmail.com>

Hi Sir

There is a difficulty in creating a comprehensive graph.
Suppose I have a time series from 1967 to 2006. I have used the following R
code for creating a graph.

>plot(Year,Dir,ylab="Annual Maximum Daily Rainfall (mm)",cex.lab=1.3
,type="o",lab=c(20,20,25),mgp=c(2.5,1,0),tck=1)
The graph is attached with this email.
Sir I want the grid lines as dashed lines. Second thing to be asked is that
the grid lines should make squared blocks not rectangles. Moreover the
boundries are not in good format.

Sir I request you to please guide in this regards.

Thank You.




-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com

From alagador at oniduo.pt  Fri Jul 20 15:43:00 2007
From: alagador at oniduo.pt (Diogo Alagador)
Date: Fri, 20 Jul 2007 14:43:00 +0100
Subject: [R] R CMD SHLIB problem [make: *** No rule to make target ]
Message-ID: <E4A1FE58798703479CD69CB9A1F008B19D2C60@RVS1.at.isp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/1199144d/attachment.pl 

From rocchini at unisi.it  Fri Jul 20 16:16:39 2007
From: rocchini at unisi.it (rocchini at unisi.it)
Date: Fri, 20 Jul 2007 16:16:39 +0200
Subject: [R] presence/absence matrix
Message-ID: <46A0C3C7.2060302@unisi.it>

I have a table such that:

sample #   species
1    a
1    b
2    a   
2    c
3    b     

and i would like to build a matrix with species names (i.e. a b and c) 
as field names and samples (i.e. 1,2 and 3) as row names
and 1/0 as inner values
such as:
    a   b   c
1   1   1   0
2   1   0   1
3   0   1   0

I am currently using Rcmdr package for managing datasets but need a 
function about.... I tried to use stack function but only with worst results

Thanks
Duccio


From alagador at oniduo.pt  Fri Jul 20 16:18:41 2007
From: alagador at oniduo.pt (Diogo Alagador)
Date: Fri, 20 Jul 2007 15:18:41 +0100
Subject: [R] R CMD SHLIB problem [make: *** No rule to make target ]
References: <E4A1FE58798703479CD69CB9A1F008B19D2C60@RVS1.at.isp>
Message-ID: <E4A1FE58798703479CD69CB9A1F008B19D2C61@RVS1.at.isp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/2954747a/attachment.pl 

From gerifalte28 at hotmail.com  Fri Jul 20 16:41:46 2007
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 20 Jul 2007 08:41:46 -0600
Subject: [R] presence/absence matrix
In-Reply-To: <46A0C3C7.2060302@unisi.it>
References: <46A0C3C7.2060302@unisi.it>
Message-ID: <46A0C9AA.9050003@hotmail.com>

See ?table  i.e.

 > x
   sample # species
1        1       a
2        1       b
3        2       a
4        2       c
5        3       b

 > table(x)
         species
sample # a b c
        1 1 1 0
        2 1 0 1
        3 0 1 0

Regards,

Francisco

Francisco J. Zagmutt


rocchini at unisi.it wrote:
> I have a table such that:
> 
> sample #   species
> 1    a
> 1    b
> 2    a   
> 2    c
> 3    b     
> 
> and i would like to build a matrix with species names (i.e. a b and c) 
> as field names and samples (i.e. 1,2 and 3) as row names
> and 1/0 as inner values
> such as:
>     a   b   c
> 1   1   1   0
> 2   1   0   1
> 3   0   1   0
> 
> I am currently using Rcmdr package for managing datasets but need a 
> function about.... I tried to use stack function but only with worst results
> 
> Thanks
> Duccio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From stef at biostatistics.it  Fri Jul 20 16:45:18 2007
From: stef at biostatistics.it (Stefano Calza)
Date: Fri, 20 Jul 2007 16:45:18 +0200
Subject: [R] presence/absence matrix
In-Reply-To: <46A0C3C7.2060302@unisi.it>
References: <46A0C3C7.2060302@unisi.it>
Message-ID: <20070720144518.GA9023@med.unibs.it>

What about

table(sample,species)

Stefano


On Fri, Jul 20, 2007 at 04:16:39PM +0200, rocchini a unisi.it wrote:
<rocchini>I have a table such that:
<rocchini>
<rocchini>sample #   species
<rocchini>1    a
<rocchini>1    b
<rocchini>2    a   
<rocchini>2    c
<rocchini>3    b     
<rocchini>
<rocchini>and i would like to build a matrix with species names (i.e. a b and c) 
<rocchini>as field names and samples (i.e. 1,2 and 3) as row names
<rocchini>and 1/0 as inner values
<rocchini>such as:
<rocchini>    a   b   c
<rocchini>1   1   1   0
<rocchini>2   1   0   1
<rocchini>3   0   1   0
<rocchini>
<rocchini>I am currently using Rcmdr package for managing datasets but need a 
<rocchini>function about.... I tried to use stack function but only with worst results
<rocchini>
<rocchini>Thanks
<rocchini>Duccio
<rocchini>
<rocchini>______________________________________________
<rocchini>R-help a stat.math.ethz.ch mailing list
<rocchini>https://stat.ethz.ch/mailman/listinfo/r-help
<rocchini>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
<rocchini>and provide commented, minimal, self-contained, reproducible code.


From fabricemcshort at hotmail.com  Fri Jul 20 16:48:34 2007
From: fabricemcshort at hotmail.com (Fabrice McShort)
Date: Fri, 20 Jul 2007 16:48:34 +0200
Subject: [R] SOS
Message-ID: <BAY129-W41A7140A333B3125732ADACBF40@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/e404d2ec/attachment.pl 

From davamaillist at gmail.com  Fri Jul 20 16:50:26 2007
From: davamaillist at gmail.com (david dav)
Date: Fri, 20 Jul 2007 16:50:26 +0200
Subject: [R] exclude1 in summary.formula from Hmisc
In-Reply-To: <46A0A528.5020003@vanderbilt.edu>
References: <772cb06e0707191659m5d012eb8o28bb040a2780eaf4@mail.gmail.com>
	<46A0A528.5020003@vanderbilt.edu>
Message-ID: <772cb06e0707200750y76773b36l18a837024fc84e87@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/426362d5/attachment.pl 

From ripley at stats.ox.ac.uk  Fri Jul 20 16:53:32 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jul 2007 15:53:32 +0100 (BST)
Subject: [R] cv.glm error function
In-Reply-To: <651263270707200554q6cc2f6abu11d6dd8683e06b@mail.gmail.com>
References: <651263270707200554q6cc2f6abu11d6dd8683e06b@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707201549180.2905@gannet.stats.ox.ac.uk>

On Fri, 20 Jul 2007, Rachel Davidson wrote:

> I have a couple quick questions about the use of cv.glm for
> cross-validation.
>
> 1. If we have a Poisson GLM with counts Y~Poisson(mu) and
> ln(mu)=beta0+beta1*x1+..., is the prediction error (delta) that is output
> from cv.glm provided in terms of the counts (y) or the (mu)?

It is of prediction error as measured by the cost function.  Since y and 
mu are on the same scale I wonder if you meant was it on log scale?  If 
so, it is on the response scale.

The default cost function is probably not appropriate for a log-linear 
model.

> 2. Can cv.glm be used for negative binomial models fit using glm.nb? It
> appears to work, but since NB models aren't strictly GLM's I wanted to
> check.

NB models are strictly glms, but glm.nb estimates parameters that glm does 
not, so no.

>
> Thanks!
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ted.harding at nessie.mcc.ac.uk  Fri Jul 20 17:14:18 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 20 Jul 2007 16:14:18 +0100 (BST)
Subject: [R] presence/absence matrix
In-Reply-To: <46A0C3C7.2060302@unisi.it>
Message-ID: <XFMail.070720161418.ted.harding@nessie.mcc.ac.uk>

On 20-Jul-07 14:16:39, rocchini at unisi.it wrote:
> I have a table such that:
> 
> sample #   species
> 1    a
> 1    b
> 2    a   
> 2    c
> 3    b     
> 
> and i would like to build a matrix with species names (i.e. a b and c) 
> as field names and samples (i.e. 1,2 and 3) as row names
> and 1/0 as inner values
> such as:
>     a   b   c
> 1   1   1   0
> 2   1   0   1
> 3   0   1   0
> 
> I am currently using Rcmdr package for managing datasets but need a 
> function about.... I tried to use stack function but only with worst
> results

The following generates the sort of thing you show above:

## Identities of possible species and samples
  Species<-c("a","b","c","d")
  Samples<-c(1,2,3,4,5,6)

## Generate a set of samples and corresponding species:
  species<-sample(Species,20,replace=TRUE)
  samples=sample(Samples,20,replace=TRUE)
## Have a look:
 cbind(samples,species)
      samples species
 [1,] "5"     "c"    
 [2,] "2"     "c"    
 [3,] "4"     "a"    
 [4,] "5"     "b"    
 [5,] "5"     "d"    
 [6,] "1"     "d"    
 [7,] "2"     "b"    
 [8,] "2"     "b"    
 [9,] "3"     "b"    
[10,] "2"     "d"    
[11,] "4"     "d"    
[12,] "1"     "b"    
[13,] "3"     "b"    
[14,] "2"     "c"    
[15,] "2"     "d"    
[16,] "6"     "b"    
[17,] "5"     "a"    
[18,] "4"     "a"    
[19,] "2"     "b"    
[20,] "5"     "a"    

## Now generate a table:
  T<-table(samples,species)
  T
          species
  samples a b c d
        1 0 1 0 1
        2 0 3 2 2
        3 0 2 0 0
        4 2 0 0 1
        5 2 1 1 1
        6 0 1 0 0

Now this is a "table" structure, and also gives counts of
occurrences and not merely presence/absence. So we need to
get these out as a matrix.

  U<-as.matrix(T)
  U
          species
  samples a b c d
        1 0 1 0 1
        2 0 3 2 2
        3 0 2 0 0
        4 2 0 0 1
        5 2 1 1 1
        6 0 1 0 0

  U<-1*(U>0)
  U
          species
  samples a b c d
        1 0 1 0 1
        2 0 1 1 1
        3 0 1 0 0
        4 1 0 0 1
        5 1 1 1 1
        6 0 1 0 0

Is that what you want?
Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Jul-07                                       Time: 16:14:14
------------------------------ XFMail ------------------------------


From jmburgos at u.washington.edu  Fri Jul 20 17:15:42 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Fri, 20 Jul 2007 08:15:42 -0700
Subject: [R] SOS
In-Reply-To: <BAY129-W41A7140A333B3125732ADACBF40@phx.gbl>
References: <BAY129-W41A7140A333B3125732ADACBF40@phx.gbl>
Message-ID: <46A0D19E.9080800@u.washington.edu>

Multiply by 100?   Add

R=R*100

Fabrice McShort wrote:
> Dear all, I am a new user of R. I would like to know how to get fund's returns in percentage (%). For example, I use: R <- ts(read.xls("FundData"), frequency = 12, start = c(1996, 1)) Whith this program, the returns are like 0.0152699. But, I would like to have 1.52%. Please advise me about the function. Thanks! Fabrice
> _________________________________________________________________
>
> [[trailing spam removed]]
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>   

-- 
Julian M. Burgos

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science
University of Washington

1122 NE Boat Street
Seattle, WA  98105 

Phone: 206-221-6864


From RDing at ETS.ORG  Fri Jul 20 17:34:00 2007
From: RDing at ETS.ORG (Ding, Rebecca)
Date: Fri, 20 Jul 2007 11:34:00 -0400
Subject: [R] automatically jpeg output
Message-ID: <95782C26106D904E814E554F47EACA9C04C98CD6@rosnt115.etslan.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/e6104c59/attachment.pl 

From fabricemcshort at hotmail.com  Fri Jul 20 17:44:40 2007
From: fabricemcshort at hotmail.com (Fabrice McShort)
Date: Fri, 20 Jul 2007 17:44:40 +0200
Subject: [R]  SOS
Message-ID: <BAY129-W2134EDC6E7E998360BADABCBF40@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/4098304f/attachment.pl 

From bcarvalh at jhsph.edu  Fri Jul 20 17:44:46 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 20 Jul 2007 11:44:46 -0400
Subject: [R] automatically jpeg output
In-Reply-To: <95782C26106D904E814E554F47EACA9C04C98CD6@rosnt115.etslan.org>
References: <95782C26106D904E814E554F47EACA9C04C98CD6@rosnt115.etslan.org>
Message-ID: <ACC0328F-8FC1-4A29-8C40-C87F2635E04D@jhsph.edu>

jpeg(...) ##if you have X11
bitmap(..., type="jpeg") ##otherwise

b

On Jul 20, 2007, at 11:34 AM, Ding, Rebecca wrote:

> Dear R users,
>
> I used R to draw many histograms and I would like to automatically  
> save
> them into a jpeg file. I tried the following code since I know .ps  
> file
> could be saved like this way:
>
> postscript("AYA_ELA.jpeg",horizontal=F,onefile=T)
> ......#some funtions inside here
> dev.off()
>
> There was a jpeg file, however, there is no pictures inside. Any
> suggestion?
>
> Thanks.
>
> Rebecca


From tkremund98 at hotmail.com  Fri Jul 20 17:55:34 2007
From: tkremund98 at hotmail.com (Todd Remund)
Date: Fri, 20 Jul 2007 09:55:34 -0600
Subject: [R] mtext formatting
Message-ID: <BAY121-F15EEEC1F45ADDDB97878FAD4F40@phx.gbl>

I am trying to create a formatted title using mtext.  Using the format 
function I define the spacing for a specific part of the text then paste 
these together, when I put this in mtext it does not keep the spacing I 
defined. Here is an example.

x <- format(paste("X=", a), width=8)
y <- format("F=G(x)", width=8)

mtext(text=paste(x, y, sep=""), col="red", font=1)

Does anyone have any suggestions to get the mtext to follow the formatting 
as it does with cat()?


From tlumley at u.washington.edu  Fri Jul 20 18:07:02 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Jul 2007 09:07:02 -0700 (PDT)
Subject: [R] GEE code
In-Reply-To: <001401c7caa6$a33b68b0$0540210a@www.domain>
References: <b05bf6c40707200034t193fac27qb8f9f2d4c702a247@mail.gmail.com>
	<001401c7caa6$a33b68b0$0540210a@www.domain>
Message-ID: <Pine.LNX.4.64.0707200900360.6976@homer23.u.washington.edu>

On Fri, 20 Jul 2007, Dimitris Rizopoulos wrote:

> from your description I think you need a random-effects model.

Since he specifically said "fixed effects" I would have assumed from the 
description that he wanted a fixed-effects model.

>								 Since
> you assume normality the parameter estimates from a random-effects
> model (in your case `GDP.log2') have a marginal interpretation (in
> fact they have both conditional and marginal interpretation).

But they still aren't the fixed effects estimates, and they behave 
quite differently under model misspecification (and under confounding).

A fixed effects linear model with GEE just needs the formula
   ineq~GDP.log2+country.
to specify an indicator variable for each country.

If he had a logistic regression model things would be more complicated, 
but for a linear or log-linear model it is just a matter of adding 
predictors.

Now, I might well use a linear mixed model in this context, but he did 
fairly clearly indicate that wasn't he was looking for.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From ted.harding at nessie.mcc.ac.uk  Fri Jul 20 18:07:38 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 20 Jul 2007 17:07:38 +0100 (BST)
Subject: [R] automatically jpeg output
In-Reply-To: <95782C26106D904E814E554F47EACA9C04C98CD6@rosnt115.etslan.org>
Message-ID: <XFMail.070720170738.ted.harding@nessie.mcc.ac.uk>

On 20-Jul-07 15:34:00, Ding, Rebecca wrote:
> Dear R users,
> 
> I used R to draw many histograms and I would like to automatically save
> them into a jpeg file. I tried the following code since I know .ps file
> could be saved like this way:
> 
> postscript("AYA_ELA.jpeg",horizontal=F,onefile=T)
> ......#some funtions inside here
> dev.off()
> 
> There was a jpeg file, however, there is no pictures inside. Any
> suggestion? 
> 
> Thanks.
> 
> Rebecca

If your "some functions inside here" do draw histograms, then
there will indeed be pictures inside, but they will be in PostScript,
since that is what is created when you use the postscript() device.
The fact that you used ".jpeg" in the fileneam has nothing to
do with it -- it will simply store the output, as PostScript,
in the file whose name you give to it. It trusts you.

You would have seen the PostScript pictures if you had used a
PostScript viewer which reacts to the content of the files,
whereas presumably your system expects a file whose name ends
in ".jpeg" or ".jpg" to be a JPEG file; and therefore will
use the wrong interpreter and fail to recognise the picture
(as you have just proved).

If you want to create JPEG files in a similar way, use the
jpeg() function -- read ?jpeg for details.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 20-Jul-07                                       Time: 17:07:20
------------------------------ XFMail ------------------------------


From ripley at stats.ox.ac.uk  Fri Jul 20 18:08:44 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jul 2007 17:08:44 +0100 (BST)
Subject: [R] R CMD SHLIB problem [make: *** No rule to make target ]
In-Reply-To: <E4A1FE58798703479CD69CB9A1F008B19D2C60@RVS1.at.isp>
References: <E4A1FE58798703479CD69CB9A1F008B19D2C60@RVS1.at.isp>
Message-ID: <Pine.LNX.4.64.0707201544270.2905@gannet.stats.ox.ac.uk>

Please do *not* use paths in your C code, especially those with spaces in. 
(The rw-FAQ did advise you not to install into a path with spaces in if 
you wanted to do development work.)  The path is not needed for system 
include files (you used <>) and is likely the problem.

And please do not post multiple times.

On Fri, 20 Jul 2007, Diogo Alagador wrote:

> Hy all,
>
> I apologize for my ingenuity in regard to interfaces in R, but I do need it for my work. In that respect I took a simple and small example from the net (the "hello world", one) to interface R with C.
> I have a Windows XP OS using R.2.5.0 and in that regard I have installed the Perl and RTools files to my PC. I also wrote a proper path file, as suggested.
>
> The C program is:
>
> #include <C:/Archivos de programa/R-2.5.0/include/R.h>
> void hello(int *n){
> int i;
> for(i=0;i<*n;i++){
> Rprintf("Hello, world!\n");
> }
> }

You don't need R.h, but you do need to declare Rprintf, which is in
<R_ext/Print.h>.  So a more legible version of a correct program would be

% cat holla.c

#include <R_ext/Print.h>

void hello(int *n)
{
     int i;
     for(i = 0; i < *n; i++)
          Rprintf("Hello, world!\n");
}

which you could call by

> dyn.load("holla.dll")
> .C("hello", 10L)

(note the L).


> However when trying to compile the a C file in the command window:
>
> R CMD SHLIB holla.c
>
> I get the following message:
>
> make: *** No rule to make target `holla.d', needed by `makeMakedeps'.  Stop.
>
> Can somebody give me a hand on this,
> Thanks in advance
>
>
> Diogo Andr? Alagador
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Fri Jul 20 18:17:03 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Jul 2007 17:17:03 +0100 (BST)
Subject: [R] mtext formatting
In-Reply-To: <BAY121-F15EEEC1F45ADDDB97878FAD4F40@phx.gbl>
References: <BAY121-F15EEEC1F45ADDDB97878FAD4F40@phx.gbl>
Message-ID: <Pine.LNX.4.64.0707201709130.4288@gannet.stats.ox.ac.uk>

On Fri, 20 Jul 2007, Todd Remund wrote:

> I am trying to create a formatted title using mtext.  Using the format
> function I define the spacing for a specific part of the text then paste
> these together, when I put this in mtext it does not keep the spacing I
> defined. Here is an example.
>
> x <- format(paste("X=", a), width=8)
> y <- format("F=G(x)", width=8)
>
> mtext(text=paste(x, y, sep=""), col="red", font=1)
>
> Does anyone have any suggestions to get the mtext to follow the formatting
> as it does with cat()?

It does for me: you are outputting in a proportionally spaced font which 
may be what is confusing you.  Try family = "mono" to check.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From cberry at tajo.ucsd.edu  Fri Jul 20 18:20:48 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Fri, 20 Jul 2007 09:20:48 -0700
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
Message-ID: <Pine.LNX.4.64.0707200902030.4403@tajo.ucsd.edu>

On Thu, 19 Jul 2007, Latchezar Dimitrov wrote:

> Hello,
>
> This is a speed question. I have a dataframe genoT:
>
>> dim(genoT)
> [1]   1002 238304

It looks like these are all numeric originally. Handling these as a
vector or matrix will speed things up a bit. You can then stitch
together a data.frame:

# simulate: 
#       genoT.names <- scan('data.file, what='a', nlines=1, <etc> ) 
# 	genoT <- scan('data.file',skip=1)
#
>
> genoT <- sample(0:2, 240000*1002, repl=T)
> t1 <- proc.time()
> genoT <- factor(genoT,0:2,c("AA","AB","BB"))
> dim(genoT) <- c(1002,240000)
> genoT.list <- lapply(1:240000, function(x) genoT[,x])
> # simulate: names(genoT.list) <- genoT.names :
> names(genoT.list) <- make.names(1:240000)
> class(genoT.list) <- "data.frame"
> row.names(genoT.list) <- 1:1002
> proc.time()-t1
    user  system elapsed
  20.978   2.036  49.714
>

Most of the _elapsed_ time is due to lags in copy-and-paste-ing in the 
commands.

HTH,

Chuck
>
>> str(genoT)
> 'data.frame':   1002 obs. of  238304 variables:
> $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 3 3 3 3 3
> ...
> $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1 1 1 2 2 2
> ...
> $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1 1 1 1 1 1
> ...
> $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3 3 3 3 3 3
> ...
> $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2 3 2 3 3 1
> ...
> $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA 1 NA 2 1 1 2 1
> ...
> $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 1 1 1 1 2
> ...
> $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3 3 3 3 3 2
> ...
> $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1 1 1 1 1 2
> ...
> $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2 1 2 1 1 3
> ...
> $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 2 2 3 3 3 2 1 2 2 3
> ...
> $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 3 3 3 3 3 3 3 3 3 3
> ...
> $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 2 2 2 1 1 1 2 2 2 2
> ...
> $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
> $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 2 3 2 2 3 2 2 1 1 2
> ...
> $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 1 1 1
> ...
> $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 1 1 2 1 1 1 1 1 1 1
> ...
> $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
> $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 2 1 1 1 2
> ...
> $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2 2 2 NA 1 NA 2
> 1 ...
> $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 1 2 2 1 1 1 3 1 1 1
> ...
> $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 2 2 2 2 2 2 2 ...
> $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1 1 ...
> $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 2 2 1
> ...
>
> Its columns are factors with different number of levels (from 1 to 3 -
> that's what I got from read.table, i.e., it dropped missing levels). I
> want to convert it to uniform factors with 3 levels. The 1st 10 rows
> above show already converted columns and the rest are not yet converted.
> Here's my attempt wich is a complete failure as speed:
>
>> system.time(
> +     for(j in 1:(10         )){ #-- this is to try 1st 10 cols and
> measure the time, it otherwise is ncol(genoT) instead of 10
>
> +        gt<-genoT[[j]]          #-- this is to avoid 2D indices
> +        for(l in 1:length(gt at levels)){
> +          levels(gt)[l] <- switch(gt at levels[l],AA="0",AB="1",BB="2")
> #-- convert levels to "0","1", or "2"
> +          genoT[[j]]<-factor(gt,levels=0:2)   #-- make a 3-level factor
> and put it back
> +        }
> +     }
> + )
> [1] 785.085   4.358 789.454   0.000   0.000
>
> 789s for 10 columns only!
>
> To me it seems like replacing 10 x 3 levels and then making a factor of
> 1002 element vector x 10 is a "negligible" amount of operations needed.
>
> So, what's wrong with me? Any idea how to accelerate significantly the
> transformation or (to go to the very beginning) to make read.table use a
> fixed set of levels ("AA","AB", and "BB") and not to drop any (missing)
> level?
>
> R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
>
> The machine is with 32G RAM and AMD Opteron 285 (2.? GHz) so it's not
> it.
>
> Thank you very much for the help,
>
> Latchezar Dimitrov,
> Analyst/Programmer IV,
> Wake Forest University School of Medicine,
> Winston-Salem, North Carolina, USA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From jmburgos at u.washington.edu  Fri Jul 20 18:19:42 2007
From: jmburgos at u.washington.edu (Julian Burgos)
Date: Fri, 20 Jul 2007 09:19:42 -0700
Subject: [R] Graphical Parameters
In-Reply-To: <3ffd3bb60707200620k11111d65t56ce4ef644340f3f@mail.gmail.com>
References: <3ffd3bb60707200620k11111d65t56ce4ef644340f3f@mail.gmail.com>
Message-ID: <46A0E09E.5010302@u.washington.edu>

Hi Amna,

To have more control on the grid of your plot use the grid function.  
Remove the tck argument from your plot call and add, as following:

plot(Year,Dir,ylab="Annual Maximum Daily Rainfall 
(mm)",cex.lab=1.3,type="o",lab=c(20,20,25),mgp=c(2.5,1,0),asp=1)
grid()

See ?grid to see how to customize the grid lines.

Julian


amna khan wrote:
> Hi Sir
>
> There is a difficulty in creating a comprehensive graph.
> Suppose I have a time series from 1967 to 2006. I have used the 
> following R
> code for creating a graph.
>
>> plot(Year,Dir,ylab="Annual Maximum Daily Rainfall (mm)",cex.lab=1.3
> ,type="o",lab=c(20,20,25),mgp=c(2.5,1,0),tck=1)
> The graph is attached with this email.
> Sir I want the grid lines as dashed lines. Second thing to be asked is 
> that
> the grid lines should make squared blocks not rectangles. Moreover the
> boundries are not in good format.
>
> Sir I request you to please guide in this regards.
>
> Thank You.
>
>
>
>
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   

-- 
Julian M. Burgos

Fisheries Acoustics Research Lab
School of Aquatic and Fishery Science
University of Washington

1122 NE Boat Street
Seattle, WA  98105 

Phone: 206-221-6864


From f.harrell at vanderbilt.edu  Fri Jul 20 18:28:01 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 20 Jul 2007 11:28:01 -0500
Subject: [R] exclude1 in summary.formula from Hmisc
In-Reply-To: <772cb06e0707200750y76773b36l18a837024fc84e87@mail.gmail.com>
References: <772cb06e0707191659m5d012eb8o28bb040a2780eaf4@mail.gmail.com>	
	<46A0A528.5020003@vanderbilt.edu>
	<772cb06e0707200750y76773b36l18a837024fc84e87@mail.gmail.com>
Message-ID: <46A0E291.8010107@vanderbilt.edu>

david dav wrote:
> Here is a peace of the data and code :
> 
> sex <-c(2,2,1,1,1,1,1,1,1,2,1,1,1,1,1,2,2,1,1,2,2)
> AGN <- 
> c("C","C","C","C","C","A","A","C","B","B","C","C","C","C","C","C","B","B","C","C","C") 
> 
> X <- c(2,2,2,2,1,2,2,2,2,2,1,1,1,1,1,1,1,1,1,2,2)
> 
> varqual <- data.frame(sex,AGN,X)
> 
> desqual <- summary.formula(varqual$X ~.,  data = subset( varqual, select 
> = -X), method = "reverse",  overall = T, test = T, long = T, exclude1 = F)
> 
> desqual doesn't show the results for sex ==1 as it is redundant.
> I also tried long =T wich didn't change anything here.

Oh yes.  exclude1 is not an argument to summary.formula but is an 
argument to the print, plot, and latex methods.  So do print(desqual, 
exclude1=FALSE, long=TRUE).  Thanks for the reproducible example.

Note that you say summary( ) and don't need to type out summary.formula

> 
> Bonus question if I may :
> This function is a little bit complex for me and  I couldn't figure out 
> how to get either Yates' continuity correction or Fisher Test instead of 
> Chisquare. Does it ask a lot of program coding ?

Neither of those is recommended so they are not automatically supported. 
  But users can add their own test functions - see the help file for the 
catTest argument to summary.formula.  Fisher's test is conservative. 
The Yates' continuity correction tries to mimic the conservatism of 
Fisher's test.  I don't like to get larger P-values when I can avoid it. 
  And the recommendations about worrying about the chi-square 
approximation when some cell sizes are small are a bit overdone.  Better 
might be to use the likelihood ratio tests, and many other tests are 
available.

Frank

> 
> Regards.
> 
> David


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From RDing at ETS.ORG  Fri Jul 20 19:14:51 2007
From: RDing at ETS.ORG (Ding, Rebecca)
Date: Fri, 20 Jul 2007 13:14:51 -0400
Subject: [R] automatically jpeg output
In-Reply-To: <ACC0328F-8FC1-4A29-8C40-C87F2635E04D@jhsph.edu>
References: <95782C26106D904E814E554F47EACA9C04C98CD6@rosnt115.etslan.org>
	<ACC0328F-8FC1-4A29-8C40-C87F2635E04D@jhsph.edu>
Message-ID: <95782C26106D904E814E554F47EACA9C04C98E38@rosnt115.etslan.org>


Hi R users,

Thanks for the jpeg() function. I do find it. However when I used
par(mfrow=c(6,7)), which can put all my histograms in one page, R gave
me the error message:
Error in plot.new() : figure margins too large. The mfrow=c(6,7)statment
was fine in postscript("AYA_ELA.ps"). 

Is there a way to solve that problem?

Thanks.

-----Original Message-----
From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu] 
Sent: Friday, July 20, 2007 11:45 AM
To: Ding, Rebecca
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] automatically jpeg output

jpeg(...) ##if you have X11
bitmap(..., type="jpeg") ##otherwise

b

On Jul 20, 2007, at 11:34 AM, Ding, Rebecca wrote:

> Dear R users,
>
> I used R to draw many histograms and I would like to automatically 
> save them into a jpeg file. I tried the following code since I know 
> .ps file could be saved like this way:
>
> postscript("AYA_ELA.jpeg",horizontal=F,onefile=T)
> ......#some funtions inside here
> dev.off()
>
> There was a jpeg file, however, there were no pictures inside. Any 
> suggestion?
>
> Thanks.
>
> Rebecca

--------------------------------------------------
This e-mail and any files transmitted with it may contain privileged or confidential information.
It is solely for use by the individual for whom it is intended, even if addressed incorrectly.
If you received this e-mail in error, please notify the sender; do not disclose, copy, distribute,
or take any action in reliance on the contents of this information; and delete it from
your system. Any other use of this e-mail is prohibited.

Thank you for your compliance.


From sunnyside500 at gmail.com  Fri Jul 20 19:42:47 2007
From: sunnyside500 at gmail.com (runner)
Date: Fri, 20 Jul 2007 10:42:47 -0700 (PDT)
Subject: [R] can I paste 'newline'?
In-Reply-To: <469FF8ED.1010705@stats.uwo.ca>
References: <11699845.post@talk.nabble.com> <469FF8ED.1010705@stats.uwo.ca>
Message-ID: <11712720.post@talk.nabble.com>


Thanks for replied from you all. I 've got better understanding of what
cat/paste does. what I am trying is to make FASTA format data. i.e. for each
line starting with '>', add '\n' at both ends while get rid of all '\n's for
the rest. 

e.g. this is original messy data, I need to remove newlines within the text
bodies:

>no1
this is the first entry ('\n')
but here is extra newline('\n')
and also here that we need to remove.
>no2
this is the second entry, which is fine in format.
>no3
....

Of course, perl would be good at this, but I want to try with R for the
other reason.
thanks.



Duncan Murdoch-2 wrote:
> 
> On 19/07/2007 7:41 PM, runner wrote:
>> It is ok to bury a reg expression '\n' when using 'cat', but not 'paste'. 
>> e.g.
>> 
>> cat ('I need to move on to a new line', '\n', 'at here') # change line!
>> paste ('I need to move on to a new line', '\n', 'at here') # '\n' is just
>> a
>> character as it is.
>> 
>> Is there a way around pasting '\n' ? Thanks a lot.
> 
> What do you want to get?  Do you want a two element vector?  Then use 
> c().  Do you want a one element vector that prints on two lines?  Use 
> either form, they both work (but you need to use cat() to do the display).
> 
>  > x <- paste ('I need to move on to a new line', '\n', 'at here')
>  > cat(x)
> I need to move on to a new line
>   at here>
> 
> 

-- 
View this message in context: http://www.nabble.com/can-I-paste-%27newline%27--tf4114350.html#a11712720
Sent from the R help mailing list archive at Nabble.com.


From jholtman at gmail.com  Fri Jul 20 19:44:14 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 20 Jul 2007 13:44:14 -0400
Subject: [R] SOS
In-Reply-To: <BAY129-W2134EDC6E7E998360BADABCBF40@phx.gbl>
References: <BAY129-W2134EDC6E7E998360BADABCBF40@phx.gbl>
Message-ID: <644e1f320707201044s77a47381u741e588c0f8ff748@mail.gmail.com>

You can use sprintf:

> x <- runif(5)
> x
[1] 0.89838968 0.94467527 0.66079779 0.62911404 0.06178627
> cat(sprintf("%.2f%% ", x * 100))
89.84%  94.47%  66.08%  62.91%  6.18% >


On 7/20/07, Fabrice McShort <fabricemcshort at hotmail.com> wrote:
> Hi Julian,
>
> Thank you very much. Please let me know how to get 2 numbers after the decim.
>
> Best regards,
>
> Fabrice
>
>
>
> > Date: Fri, 20 Jul 2007 08:15:42 -0700> From: jmburgos at u.washington.edu> To: fabricemcshort at hotmail.com> CC: r-help at stat.math.ethz.ch> Subject: Re: [R] SOS> > Multiply by 100? Add> > R=R*100> > Fabrice McShort wrote:> > Dear all, I am a new user of R. I would like to know how to get fund's returns in percentage (%). For example, I use: R <- ts(read.xls("FundData"), frequency = 12, start = c(1996, 1)) Whith this program, the returns are like 0.0152699. But, I would like to have 1.52%. Please advise me about the function. Thanks! Fabrice> > _________________________________________________________________> >> > [[trailing spam removed]]> >> > [[alternative HTML version deleted]]> >> > ______________________________________________> > R-help at stat.math.ethz.ch mailing list> > https://stat.ethz.ch/mailman/listinfo/r-help> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html> > and provide commented, minimal, self-contained, reproducible code.> >> > > > !
>  -- > Julian M. Burgos> > Fisheries Acoustics Research Lab> School of Aquatic and Fishery Science> University of Washington> > 1122 NE Boat Street> Seattle, WA 98105 > > Phone: 206-221-6864> >
> _________________________________________________________________
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jrkrideau at yahoo.ca  Fri Jul 20 19:44:33 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Fri, 20 Jul 2007 13:44:33 -0400 (EDT)
Subject: [R] SOS
In-Reply-To: <BAY129-W2134EDC6E7E998360BADABCBF40@phx.gbl>
Message-ID: <313324.32476.qm@web32806.mail.mud.yahoo.com>

?round
x <- 1.2223
round(x,2)
[1] 1.22
--- Fabrice McShort <fabricemcshort at hotmail.com>
wrote:

> Hi Julian,
>  
> Thank you very much. Please let me know how to get 2
> numbers after the decim.
>  
> Best regards,
>  
> Fabrice
> 
> 
> 
> > Date: Fri, 20 Jul 2007 08:15:42 -0700> From:
> jmburgos at u.washington.edu> To:
> fabricemcshort at hotmail.com> CC:
> r-help at stat.math.ethz.ch> Subject: Re: [R] SOS> >
> Multiply by 100? Add> > R=R*100> > Fabrice McShort
> wrote:> > Dear all, I am a new user of R. I would
> like to know how to get fund's returns in percentage
> (%). For example, I use: R <-
> ts(read.xls("FundData"), frequency = 12, start =
> c(1996, 1)) Whith this program, the returns are like
> 0.0152699. But, I would like to have 1.52%. Please
> advise me about the function. Thanks! Fabrice> >
>
_________________________________________________________________>
> >> > [[trailing spam removed]]> >> > [[alternative
> HTML version deleted]]> >> >
> ______________________________________________> >
> R-help at stat.math.ethz.ch mailing list> >
> https://stat.ethz.ch/mailman/listinfo/r-help> >
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html> > and
> provide commented, minimal, self-contained,
> reproducible code.> >> > > > !
>  -- > Julian M. Burgos> > Fisheries Acoustics
> Research Lab> School of Aquatic and Fishery Science>
> University of Washington> > 1122 NE Boat Street>
> Seattle, WA 98105 > > Phone: 206-221-6864> > 
>
_________________________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From lisas at salford-systems.com  Fri Jul 20 20:03:27 2007
From: lisas at salford-systems.com (Lisa Solomon)
Date: Fri, 20 Jul 2007 11:03:27 -0700
Subject: [R] Free Online: Data Mining Intro for Beginners, Vendor-Neutral
Message-ID: <Q1BYWkxDSSFNOTkqIC8pNDM0NTg0NDUz@sspc-lisa>

Intro to Data Mining for Absolute Beginners (no charge)

This one-hour webinar is a perfect place to start if you are new to data mining and have little-to-no background in statistics or machine learning.

-Dates: July 24, August 9, September 7

-Registration: http://salford.webex.com

-Future Webinars:  Multiple timezones and topics are planned.  Let us know if you would like to be notified as we schedule new webinar dates and topics. 

-Abstract:
In the one hour "Intro to Data Mining" webinar, we will discuss:
**Data basics: what kind of data is required for data mining and predictive analytics; In what format must the data be; what steps are necessary to prepare data appropriately
**What kinds of questions can we answer with data mining
**How data mining models work: the inputs, the outputs, and the nature of the predictive mechanism
**Evaluation criteria: how predictive models can be assessed and their value measured
**Specific background knowledge to prepare you to begin a data mining project.

Please circulate to colleagues who might benefit and do not hesitate to contact me if you have any questions.

Sincerely,
Lisa Solomon
lisas at salford-systems.com
Salford Systems, 4740 Murphy Canyon Rd. Ste 200, San Diego, Calif. 92123


From alagador at oniduo.pt  Fri Jul 20 20:11:33 2007
From: alagador at oniduo.pt (Diogo Alagador)
Date: Fri, 20 Jul 2007 19:11:33 +0100
Subject: [R] R CMD SHLIB problem [make: *** No rule to make target ]
Message-ID: <E4A1FE58798703479CD69CB9A1F008B19D2C63@RVS1.at.isp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/d62460ac/attachment.pl 

From alagador at oniduo.pt  Fri Jul 20 20:17:48 2007
From: alagador at oniduo.pt (Diogo Alagador)
Date: Fri, 20 Jul 2007 19:17:48 +0100
Subject: [R] Unrecognising SEXP [C file compilation using .Call]
Message-ID: <E4A1FE58798703479CD69CB9A1F008B19D2C64@RVS1.at.isp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/65dbffca/attachment.pl 

From Min.Zou at nuffield.ox.ac.uk  Fri Jul 20 20:59:04 2007
From: Min.Zou at nuffield.ox.ac.uk (Min Zou)
Date: Fri, 20 Jul 2007 19:59:04 +0100
Subject: [R] Simultaneous logit/probit models
Message-ID: <E4B21B4B40BA264E9731DCB382A875CC22791F@nuff-chive.nuff.ox.ac.uk>

Dear R Users,
 
I'm currently working on simultaneous multinomial models and wonder whether R has any package that can estimate such models. 
 
I've got a survey dataset that contains 2,000 individuals and one of the survey questions asked the respondents to chose two main reasons to work out of eight options (e.g., pay, pepople, to use abilities, etc.) . Now I'd like to model the responses using simultaneous logit/probit models. To my knowledge, LIMDEP (authored by Greene) can handle these models. As I know little about LIMDEP, I wonder if R has any package designed for these models. 
 
I have searched the R archive but haven't got any clue. Any suggestions?
 
Thanks,
 
Min
 
-------------------------
Sociology Dept.
Oxford University


From bbernzwe at bear.com  Fri Jul 20 21:04:31 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Fri, 20 Jul 2007 15:04:31 -0400
Subject: [R] tagging results of "apply"
References: <Q1BYWkxDSSFNOTkqIC8pNDM0NTg0NDUz@sspc-lisa>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B64@whexchmb16.bsna.bsroot.bear.com>

In trying to get a better understanding of vectorization I wrote the
following code:

My objective is to take two sets of time series and calculate the
correlations for each combination of time series.

mat1 <- matrix(sample(1:500, 25), ncol = 5)
mat2 <- matrix(sample(501:1000, 25), ncol = 5)

Scenario 1:
apply(mat1, 1, function(x) cor(mat1, mat2[1,]))

Scenario 2:
apply(mat1, 1, function(x) cor(mat1, mat2))

Using scenario 1, (output below) I can see that correlations are
calculated for just the first row of mat2 against each individual row of
mat1.

Using scenario 2, (output below) I can see that correlations are
calculated for each row of mat2 against each individual row of mat1.  

Q1: The output of scenario2 consists of 25 rows of data.  Are the first
five rows mat1 against mat2[1,], the next five rows mat1 against
mat2[2,], ... last five rows mat1 against mat2[5,]?

Q2: I assign the output of scenario 2 to a new matrix

	matC <- apply(mat1, 1, function(x) cor(mat1, mat2))

    However, I need a way to identify each row in matC as a pairing of
rows from mat1 and mat2.  Is there a parameter I can add to apply to do
this?

Scenario 1:
> apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
           [,1]       [,2]       [,3]       [,4]       [,5]
[1,] -0.4626122 -0.4626122 -0.4626122 -0.4626122 -0.4626122
[2,] -0.9031543 -0.9031543 -0.9031543 -0.9031543 -0.9031543
[3,]  0.0735273  0.0735273  0.0735273  0.0735273  0.0735273
[4,]  0.7401259  0.7401259  0.7401259  0.7401259  0.7401259
[5,] -0.4548582 -0.4548582 -0.4548582 -0.4548582 -0.4548582

Scenario 2:
> apply(mat1, 1, function(x) cor(mat1, mat2))
             [,1]        [,2]        [,3]        [,4]        [,5]
 [1,]  0.19394126  0.19394126  0.19394126  0.19394126  0.19394126
 [2,]  0.26402400  0.26402400  0.26402400  0.26402400  0.26402400
 [3,]  0.12923842  0.12923842  0.12923842  0.12923842  0.12923842
 [4,] -0.74549676 -0.74549676 -0.74549676 -0.74549676 -0.74549676
 [5,]  0.64074122  0.64074122  0.64074122  0.64074122  0.64074122
 [6,]  0.26931986  0.26931986  0.26931986  0.26931986  0.26931986
 [7,]  0.08527921  0.08527921  0.08527921  0.08527921  0.08527921
 [8,] -0.28034079 -0.28034079 -0.28034079 -0.28034079 -0.28034079
 [9,] -0.15251915 -0.15251915 -0.15251915 -0.15251915 -0.15251915
[10,]  0.19542415  0.19542415  0.19542415  0.19542415  0.19542415
[11,]  0.75107032  0.75107032  0.75107032  0.75107032  0.75107032
[12,]  0.53042767  0.53042767  0.53042767  0.53042767  0.53042767
[13,] -0.51163612 -0.51163612 -0.51163612 -0.51163612 -0.51163612
[14,] -0.44396048 -0.44396048 -0.44396048 -0.44396048 -0.44396048
[15,]  0.57018745  0.57018745  0.57018745  0.57018745  0.57018745
[16,]  0.70480284  0.70480284  0.70480284  0.70480284  0.70480284
[17,] -0.36674283 -0.36674283 -0.36674283 -0.36674283 -0.36674283
[18,] -0.81826607 -0.81826607 -0.81826607 -0.81826607 -0.81826607
[19,]  0.53145184  0.53145184  0.53145184  0.53145184  0.53145184
[20,]  0.24568385  0.24568385  0.24568385  0.24568385  0.24568385
[21,] -0.10610402 -0.10610402 -0.10610402 -0.10610402 -0.10610402
[22,] -0.78650748 -0.78650748 -0.78650748 -0.78650748 -0.78650748
[23,]  0.04269423  0.04269423  0.04269423  0.04269423  0.04269423
[24,]  0.14704698  0.14704698  0.14704698  0.14704698  0.14704698
[25,]  0.28340166  0.28340166  0.28340166  0.28340166  0.28340166



**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From milton_ruser at yahoo.com.br  Fri Jul 20 21:16:43 2007
From: milton_ruser at yahoo.com.br (Milton Cezar Ribeiro)
Date: Fri, 20 Jul 2007 12:16:43 -0700 (PDT)
Subject: [R] main title on splited windows.
Message-ID: <701039.79515.qm@web56604.mail.re3.yahoo.com>

Um texto embutido e sem conjunto de caracteres especificado associado...
Nome: n?o dispon?vel
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/44e16f37/attachment.pl 

From gavin.simpson at ucl.ac.uk  Fri Jul 20 21:34:34 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 20 Jul 2007 20:34:34 +0100
Subject: [R] main title on splited windows.
In-Reply-To: <701039.79515.qm@web56604.mail.re3.yahoo.com>
References: <701039.79515.qm@web56604.mail.re3.yahoo.com>
Message-ID: <1184960074.29195.68.camel@gsimpson.geog.ucl.ac.uk>

On Fri, 2007-07-20 at 12:16 -0700, Milton Cezar Ribeiro wrote:
> Dear all,
> 
> How can I put a main title on the top of a windows?
> I would like put a title like "This is my for graphics" :-)

Create an outer margin to the plot and pop the title in there, e.g.:

## your data
v1<-sort(runif(50))
v2<-sin(v1*3.14)

## set up plotting region
## set oma to have a 2 line + a bit margin at the top, 
## 0 lines on other 3 sides
opar <- par(mfrow=c(2,2), oma = c(0, 0, 2.1, 0))
plot(v1,main="Sort V1")
plot(v2,main="Sin(V1)")
hist(v1,main="Histogram of V1")
boxplot(v1,v2, main="Box plot - v1 & v2")
## now add the title - use outer = TRUE to get it in correct place
## we use cex.main to increase the size a bit
title(main = "This is my for graphics", outer = TRUE, cex.main = 1.5)
## reset the plotting parameters
par(opar)

Is this what you wanted?

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From thchung at tgen.org  Fri Jul 20 21:51:19 2007
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Fri, 20 Jul 2007 12:51:19 -0700
Subject: [R] set attribute method question
Message-ID: <73855898-ACEA-437F-ADFC-362894AC96E7@tgen.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/dd7b259e/attachment.pl 

From Joseph.F.Lucke at uth.tmc.edu  Fri Jul 20 22:06:53 2007
From: Joseph.F.Lucke at uth.tmc.edu (Lucke, Joseph F)
Date: Fri, 20 Jul 2007 15:06:53 -0500
Subject: [R] BOA (Bayesian Output Analysis)
In-Reply-To: <c0792190707191711i4fc6e523nfb5ba4e5763062a2@mail.gmail.com>
References: <c0792190707191711i4fc6e523nfb5ba4e5763062a2@mail.gmail.com>
Message-ID: <4677FCB5A35A0441A0E0C99D56B23D910777FEF2@UTHEVS2.mail.uthouston.edu>

>From the website:

"BOA is an R/S-PLUS program for carrying out convergence diagnostics and statistical and graphical analysis of Monte Carlo sampling output. It can be used as an output processor for the BUGS software or for any other program which produces sampling output."

See http://www.public-health.uiowa.edu/boa/



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marcus Vinicius
Sent: Thursday, July 19, 2007 7:11 PM
To: r-help at stat.math.ethz.ch
Subject: [R] BOA (Bayesian Output Analysis)

Dear all,
May anyone help me to use this package for the R software?
My procedure have been:


> dir()
[1] "line1.ind"     "line1.out"     "line1.txt"     "line2.ind"     "
line2.out"     "line2.txt"     "regressao.odc"


> boa.menu()

Bayesian Output Analysis Program (BOA)
Version 1.1.6 for i386, mingw32
Copyright (c) 2007 Brian J. Smith <brian-j-smith at uiowa.edu>

This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License or any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.

For a copy of the GNU General Public License write to the Free Software Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA, or visit their web site at http://www.gnu.org/copyleft/gpl.html

NOTE: if the event of a menu system crash, type "boa.menu(recover = TRUE)" to restart and recover your work.

BOA MAIN MENU
*************

1: File     >>
2: Data     >>
3: Analysis >>
4: Plot     >>
5: Options  >>
6: Window   >>

Sele??o: 1

FILE MENU
=========

1: Back
2: -----------------------+
3: Import Data         >> |
4: Save Session           |
5: Load Session           |
6: Exit BOA               |
7: -----------------------+

Sele??o: 3

IMPORT DATA MENU
----------------

1: Back
2: ---------------------------+
3: CODA Output Files          |
4: Flat ASCII Files           |
5: Data Matrix Objects        |
6: View Format Specifications |
7: Options...                 |
8: ---------------------------+

Sele??o: 3

Enter index filename prefix without the .ind extension [Working Directory:
""]
1: line1
Read 1 item

Enter output filename prefix without the .out extension [Default: "line1"]
1: line1
Read 1 item
*Warning: import failed
 Could not find '/line1.ind' or '/line1.out'.*

**

*What is happening??*

*Thanks a lot.*

*Marcus Vinicius*

	[[alternative HTML version deleted]]


From drstrong at ucdavis.edu  Fri Jul 20 23:14:32 2007
From: drstrong at ucdavis.edu (Mr Natural)
Date: Fri, 20 Jul 2007 14:14:32 -0700 (PDT)
Subject: [R] ?R:  Removing white space betwen multiple plots,
 traditional graphics
Message-ID: <11716176.post@talk.nabble.com>


I would appreciate suggestions for removing the white spaces the graphs in a
stack:

par(mar=c(2,2,1,1), mfrow = c(6,1))
mydates<-dates(1:20,origin=c(month = 1, day = 1, year = 1986))
plot(rnorm(20,0.1,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
plot(rnorm(20,0.2,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
plot(rnorm(20,0.3,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
plot(rnorm(20,0.5,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
plot(rnorm(20,0.7,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
plot(rnorm(20,0.8,0.1)~mydates, type="b",xlab="",ylim=c(0,1) )

Thanx, Don
-- 
View this message in context: http://www.nabble.com/-R%3A--Removing-white-space-betwen-multiple-plots%2C-traditional-graphics-tf4119626.html#a11716176
Sent from the R help mailing list archive at Nabble.com.


From Achim.Zeileis at wu-wien.ac.at  Sat Jul 21 00:20:19 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 21 Jul 2007 00:20:19 +0200 (CEST)
Subject: [R] ?R:  Removing white space betwen multiple plots,
 traditional graphics
In-Reply-To: <11716176.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.44.0707210013010.7607-100000@disco.wu-wien.ac.at>

On Fri, 20 Jul 2007, Mr Natural wrote:

>
> I would appreciate suggestions for removing the white spaces the graphs in a
> stack:
>
> par(mar=c(2,2,1,1), mfrow = c(6,1))
> mydates<-dates(1:20,origin=c(month = 1, day = 1, year = 1986))
> plot(rnorm(20,0.1,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.2,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.3,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.5,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.7,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.8,0.1)~mydates, type="b",xlab="",ylim=c(0,1) )

Really, there is no need to do this all by hand. Please have a look at the
"zoo" package (as recommended earlier this week).

Then you can do
   x <- cbind(rnorm(20,0.1,0.1),
              rnorm(20,0.2,0.1),
              rnorm(20,0.3,0.1),
              rnorm(20,0.5,0.1),
              rnorm(20,0.7,0.1),
              rnorm(20,0.8,0.1))
   z <- zoo(x, mydates)
   plot(z)
   plot(z, nc = 1, type = "b")
and much more.

Please consult
  vignette("zoo-quickref", package = "zoo")
  vignette("zoo", package = "zoo")
as well as the "Date and Time Classes" article by Gabor Grothendieck and
Thomas Petzoldt in R News 4(1), 29-32. (available from your favourite CRAN
mirror).
Z

> Thanx, Don
> --
> View this message in context: http://www.nabble.com/-R%3A--Removing-white-space-betwen-multiple-plots%2C-traditional-graphics-tf4119626.html#a11716176
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From alagador at oniduo.pt  Sat Jul 21 00:17:51 2007
From: alagador at oniduo.pt (Diogo Alagador)
Date: Fri, 20 Jul 2007 23:17:51 +0100
Subject: [R] Column-mean-values for targeted rows
Message-ID: <E4A1FE58798703479CD69CB9A1F008B19D2C67@RVS1.at.isp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/0d16ae81/attachment.pl 

From bcarvalh at jhsph.edu  Sat Jul 21 00:29:44 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 20 Jul 2007 18:29:44 -0400
Subject: [R] Column-mean-values for targeted rows
In-Reply-To: <E4A1FE58798703479CD69CB9A1F008B19D2C67@RVS1.at.isp>
References: <E4A1FE58798703479CD69CB9A1F008B19D2C67@RVS1.at.isp>
Message-ID: <1AE0F02B-81E0-4217-839E-AEA65BA706AA@jhsph.edu>

set.seed(123)
N = 30000
K = 400
theData = matrix(rnorm(N*K), ncol=K)
theData = as.data.frame(theData)
theData = cbind(indicator = sample(0:1, N, rep=T), theData)

 > system.time(results <- colMeans(subset(theData, indicator == 1)))
    user  system elapsed
   2.309   1.319   3.853


b


On Jul 20, 2007, at 6:17 PM, Diogo Alagador wrote:

> Hi all,
>
> I'm handling massive data.frames and matrices in R (30000 x 400).
> In the 1st column, say, I have 0s and 1s indicating rows that  
> matter; other columns have probability values.
> One simple task I would like to do would be to get the column mean  
> values for signaled rows (the ones with 1)
> As a very fresh "programmer" I have build a simple function in R  
> which should not be very efficient indeed! It works well for  
> current-dimension matrices, but it just not goes so well in huge ones.
>
> meanprob<-function(Robj){
> NLINE<-dim(Robj)[1];
> NCOLUMN<-dim(Robj)[2];
> mprob<-c(rep(0,(NCOLUMN-1)));
> for (i in 2:NCOLUMN){
>     sumprob<-0;
>     pa<-0;
>     for (j in 1:NLINE){
>         if(Robj[j,1]!=0){
>             pa<-pa+1;
>             sumprob<-Robj[j,i]+sumprob;
>         }
>     }
>     mprob[i-1]<-sumprob/pa;
> }
> return(mprob);
> }
>
>
> So I "only" see 3 ways to get through the problem:
>
> - to reformulate the function to gain efficiency;
> - to establish a C-routine (for example), where loops are more  
> "speedy", and then interfacing with R;
> - to find some function/ package that already do that.
>
> Can anybody illuminate my way here,
>
> Mush thanks,
>
> Diogo Andre' Alagador
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From david at davidkatzconsulting.com  Sat Jul 21 00:35:31 2007
From: david at davidkatzconsulting.com (David Katz)
Date: Fri, 20 Jul 2007 15:35:31 -0700 (PDT)
Subject: [R] binned column in a data.frame
In-Reply-To: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F3C@DJFPOST01.djf.agrsci.dk>
References: <EA09C4B2B0F16E44B8F3311629493C0D02ED4F3C@DJFPOST01.djf.agrsci.dk>
Message-ID: <11717127.post@talk.nabble.com>


Would "cut" meet your needs?


Jo?o Fadista wrote:
> 
> Dear all,
>  
> I would like to know how can I create a binned column in a data.frame. The
> output that I would like is something like this: 
>  
> Start  Binned_Start
> 1        0-5
> 2        0-5
> 6        5-10
> 8        5-10
> 13      10-15
> ...            
>  
>  
>  
> 
> Best regards
> 
> Jo?o Fadista
> Ph.d. student
> 
> 
>  	
>  	 UNIVERSITY OF AARHUS	
> Faculty of Agricultural Sciences	
> Dept. of Genetics and Biotechnology	
> Blichers All? 20, P.O. BOX 50	
> DK-8830 Tjele	
>  	
> Phone:	 +45 8999 1900	
> Direct:	 +45 8999 1900	
> E-mail:	 Joao.Fadista at agrsci.dk <mailto:Joao.Fadista at agrsci.dk> 	
> Web:	 www.agrsci.org <http://www.agrsci.org/> 	
> ________________________________
> 
> News and news media <http://www.agrsci.org/navigation/nyheder_og_presse> .
> 
> This email may contain information that is confidential. Any use or
> publication of this email without written permission from Faculty of
> Agricultural Sciences is not allowed. If you are not the intended
> recipient, please notify Faculty of Agricultural Sciences immediately and
> delete this email.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/binned-column-in-a-data.frame-tf4115787.html#a11717127
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Sat Jul 21 00:48:29 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 20 Jul 2007 18:48:29 -0400
Subject: [R] RDCOM and R versions
In-Reply-To: <20070720114524.96858.qmail@web27710.mail.ukl.yahoo.com>
References: <20070720114524.96858.qmail@web27710.mail.ukl.yahoo.com>
Message-ID: <971536df0707201548m5fa0042dke341a0cfb5d317f5@mail.gmail.com>

On 7/20/07, Paul <fuzzyflou at yahoo.fr> wrote:
> Dear R-helpers,
>
>  I have several versions of R installed on my computer, and I cannot do without any of these.
>  However RCDOM seems to authorize only one version installed. Do you know any means to overcome this problem ?
>  Thank you very much for your response.
>
> Paul Poncet

Don't know for sure how RDCOM selects the version but if its via the R
registry keys then, for example, running

   cd \Program Files\R\R-2.5.1\bin
   RSetReg.exe

will set R 2.5.1 to be the current version.

If that works then the batchfiles distribution at:

   http://code.google.com/p/batchfiles/

has two programs:

   Rversions.bat
   Rversions.hta

either of which provides an easier way to select among which version
of R you wish to make current based on the registry.  The .bat one is
a command line tool and the .hta one is a gui tool.

See the README file for more info.


From thchung at tgen.org  Sat Jul 21 02:56:13 2007
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Fri, 20 Jul 2007 17:56:13 -0700
Subject: [R] set class attribute method question
Message-ID: <F84AB36F-7E9F-447B-A088-4CAD298A2995@tgen.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070720/6601643f/attachment.pl 

From toby909 at gmail.com  Sat Jul 21 03:17:13 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Fri, 20 Jul 2007 18:17:13 -0700
Subject: [R] R2WinBUGS awkward to use
Message-ID: <f7rmnr$31m$1@sea.gmane.org>

Hi All

Does anyone know if I can avoid to use the write.model() function below? I dont 
want to do this. Can't bugs() do that automatically for me just by specifying 
the 4th argument 'model'? Just I like I am also using the 'inits' object!

If I use 'model' in the same way as I use 'inits' I am getting the error:

 > sim <- bugs(data, inits, parameters, model, n.chains=1, n.iter=5000, 
bugs.directory="c:/Program Files/WinBUGS14", working.directory=NULL, 
clearWD=TRUE, DIC=0)
Error in file.exists(c(...)) : invalid 'file' argument


Thanks Toby







data <- list(.....)



model <- function() {

[omitted]

}

write.model(model, "cwk.txt")

inits <- function() {.....}

parameters <- c("b", "nu", "S1", "S2")

sim <- bugs(data, inits, parameters, "cwk.txt", n.chains=1, n.iter=5000, 
bugs.directory="c:/Program Files/WinBUGS14", working.directory=NULL, 
clearWD=TRUE, DIC=0)

print(sim)
plot(sim)


From toby909 at gmail.com  Sat Jul 21 03:26:07 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Fri, 20 Jul 2007 18:26:07 -0700
Subject: [R] avoiding timconsuming for loop renaming identifiers
Message-ID: <f7rn8h$42b$1@sea.gmane.org>

Hi All

I was wondering if I can avoid a time-consuming for loop on my 600000 obs dataset.

school_id   y
8           9.87
8           8.89
8           7.89
8           8.88
20          6.78
20          9.99
20          8.79
31          10.1
31          11

There are, say, 143 different schools in this 600000 obs dataset.

I need to thave sequential identifiers, 1,2,3,4,5,...,143.

I was using an awkward for look that took 30 minutes to run.
sid = 1
dta$sid[1] = 1
for (i in 2:nrow(dta)) {
if (dta$school_id[i] != dta$school_[i-1]) sid = sid+1
dta$sid[i] = sid
}

Any hints appreciated.

Thanks Toby


From chenxh007 at gmail.com  Sat Jul 21 03:42:09 2007
From: chenxh007 at gmail.com (chenxh007)
Date: Fri, 20 Jul 2007 18:42:09 -0700
Subject: [R] set class attribute method question
In-Reply-To: <F84AB36F-7E9F-447B-A088-4CAD298A2995@tgen.org>
References: <F84AB36F-7E9F-447B-A088-4CAD298A2995@tgen.org>
Message-ID: <46A16471.3070908@gmail.com>

setReplaceMethod("set.version", c("dat", "character"),
                    function(object, value) {
                        object at .version <- value
                        object })

Xiaohui

Tae-Hoon Chung wrote:
> Hi, All;
> 
> Suppose I have a class 'dat' such that setClass('dat', representation 
> (.version='character', x='numeric', y='numeric')).
> If I want to make a method that changes the .version slot of 'dat'  
> class in a way like set.version(d) <- '0.2' when d is an object of  
> 'dat' class, how can I do this? Thanks in advance.
> 
> Tae-Hoon Chung
> 
> Post-Doctoral Researcher
> Computational Biology Division, TGEN
> 445 N 5th St. Phoenix, AZ 85004 USA
> O: 1-602-343-8724
> F: 1-602-343-8840
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bcarvalh at jhsph.edu  Sat Jul 21 03:55:20 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Fri, 20 Jul 2007 21:55:20 -0400
Subject: [R] avoiding timconsuming for loop renaming identifiers
In-Reply-To: <f7rn8h$42b$1@sea.gmane.org>
References: <f7rn8h$42b$1@sea.gmane.org>
Message-ID: <E9D64D3E-DD14-4959-A847-915AD445D22B@jhsph.edu>

as.integer(factor(dta[["school_id"]]))

b

On Jul 20, 2007, at 9:26 PM, toby909 at gmail.com wrote:

> Hi All
>
> I was wondering if I can avoid a time-consuming for loop on my  
> 600000 obs dataset.
>
> school_id   y
> 8           9.87
> 8           8.89
> 8           7.89
> 8           8.88
> 20          6.78
> 20          9.99
> 20          8.79
> 31          10.1
> 31          11
>
> There are, say, 143 different schools in this 600000 obs dataset.
>
> I need to thave sequential identifiers, 1,2,3,4,5,...,143.
>
> I was using an awkward for look that took 30 minutes to run.
> sid = 1
> dta$sid[1] = 1
> for (i in 2:nrow(dta)) {
> if (dta$school_id[i] != dta$school_[i-1]) sid = sid+1
> dta$sid[i] = sid
> }
>
> Any hints appreciated.
>
> Thanks Toby
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pinard at iro.umontreal.ca  Sat Jul 21 04:03:01 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Fri, 20 Jul 2007 22:03:01 -0400
Subject: [R] avoiding timconsuming for loop renaming identifiers
In-Reply-To: <f7rn8h$42b$1@sea.gmane.org>
References: <f7rn8h$42b$1@sea.gmane.org>
Message-ID: <20070721020301.GA10658@phenix.progiciels-bpi.ca>

[toby909 at gmail.com]

>I was wondering if I can avoid a time-consuming for loop on my 600000 
>obs dataset.

>school_id   y
>8           9.87
>8           8.89
>8           7.89
>8           8.88
>20          6.78
>20          9.99
>20          8.79
>31          10.1
>31          11

>There are, say, 143 different schools in this 600000 obs dataset.
>I need to thave sequential identifiers, 1,2,3,4,5,...,143.

Hello, Toby.  Maybe:

   dta$id <- cumsum(c(1, diff(dta$school_id) != 0))

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From mardones.p at gmail.com  Sat Jul 21 06:12:37 2007
From: mardones.p at gmail.com (Pedro Mardones)
Date: Sat, 21 Jul 2007 00:12:37 -0400
Subject: [R] Robust PLS
Message-ID: <83dca7860707202112v2905b3bfr8995ffea97069c92@mail.gmail.com>

Dear all;
Does anyone have written or know where to get a function for fitting
robust partial least squares in R?
Thanks for any ideas
PM


From ldimitro at wfubmc.edu  Sat Jul 21 06:26:36 2007
From: ldimitro at wfubmc.edu (Latchezar Dimitrov)
Date: Sat, 21 Jul 2007 00:26:36 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <62B73FC8-3B10-4037-A51E-2AFC6142F96E@jhsph.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
	<DDAAAB60-ECB1-4B25-9A50-C859F3249DF8@jhsph.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E6@EXCHVS1.medctr.ad.wfubmc.edu>
	<62B73FC8-3B10-4037-A51E-2AFC6142F96E@jhsph.edu>
Message-ID: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E7@EXCHVS1.medctr.ad.wfubmc.edu>

Hi,

Thanks for the help. My 1st question still unanswered though :-) Please
see bellow 

> -----Original Message-----
> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu] 
> Sent: Friday, July 20, 2007 3:30 AM
> To: Latchezar Dimitrov
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Dataframe of factors transform speed?
> 
> set.seed(123)
> genoT = lapply(1:240000, function(i) factor(sample(c("AA", 
> "AB", "BB"), 1000, prob=sample(c(1, 1000, 1000), 3), rep=T)))
> names(genoT) = paste("snp", 1:240000, sep="") genoT = 
> as.data.frame(genoT)

Now this _is the problem. Everything before converting to data.frame
worked almost instantaneously however as.data.frame runs forever.
Obviously there is some scalability memory management issue. When I
tried my own method but creating a new result (instead of modifying the
old) dataframe it worked like a charm for the 1st 100 cols ~ .3s. I
figured 300,000 cols should be ~1000s. Nope! It ran for about 50,000(!)s
to finish about 42,000 cols only. 

BTW, what ver. of R is yours?

Now here's what I "discovered" further.

#-- create a 1-col frame:
    geno   <-
data.frame(c(geno.GASP[[1]],geno.JAG[[1]]),row.names=c(rownames(geno.GAS
P),rownames(geno.JAG)))

#-- main code I repeated it w/ j in 1:1000, 2001:3000, and 3001:4000,
i.e., adding a 1000 of cols to geno each time

system.time(
#   for(j in 1:(ncol(geno.GASP      ))){
    for(j in 3001:(4000              )){
      gt.GASP<-geno.GASP[[j]]
       for(l in 1:length(gt.GASP at levels)){
         levels(gt.GASP)[l] <-
switch(gt.GASP at levels[l],AA="0",AB="1",BB="2")
       }
       gt.JAG <-geno.JAG [[j]]
#      for(l in 1:length(gt.JAG @levels)){
#        levels(gt.JAG )[l] <- switch(gt.JAG
@levels[l],AA="0",AB="1",BB="2")
#      }
       geno[[j]]<-factor(c(as.numeric(factor(gt.GASP,levels=0:2))-1
###               factor(c(as.numeric(factor(gt.GASP,levels=0:2))-1
                          ,as.numeric(factor(gt.JAG, levels=0:2))-1
                          )
                        ,levels=0:2
                        )
    }
)

Times (each one is for a 1000 cols!):
[1] 26.673  0.032 26.705  0.000  0.000
[1] 77.186  0.037 77.225  0.000  0.000
[1] 128.165   0.042 128.209   0.000   0.000
[1] 180.940   0.047 180.989   0.000   0.000

See the big diff and the scaling I mentioned above?

Further more I removed geno[[j]] assignment leaving the operation
though, i.e., replaced it with ### line above. Times:

[1] 0.857 0.008 0.865 0.000 0.000

Huh!? What the heck! That's my second question :-) Any ideas?

I still believe my method is near optimal. Of course I have to somehow
get rid of the assignment bottleneck.

For now the lesson is: "God bless lists"

Here is my final solution:

> system.time({
+     geno.GASP.L<-lapply(geno.GASP
+                        ,function(x){
+                           for(l in 1:length(x at levels)){levels(x)[l] <-
switch(x at levels[l],AA="0",AB="1",BB="2")}
+                           factor(x,levels=0:2)
+                         }
+                  )
+     geno.JAG.L <-lapply(geno.JAG
+                        ,function(x){
+ #                         for(l in 1:length(x at levels)){levels(x)[l] <-
switch(x at levels[l],AA="0",AB="1",BB="2")}
+                           factor(x,levels=0:2)
+                         }
+                  )
+ })
[1] 192.800   1.566 194.413   0.000   0.000   !!!!!!!!! :-)))))
> system.time({
+     class    (geno.GASP.L)<-"data.frame"
+     row.names(geno.GASP.L)<-row.names(geno.GASP)
+     class    (geno.JAG.L )<-"data.frame"
+     row.names(geno.JAG.L )<-row.names(geno.JAG )
+ })
[1] 12.156  0.001 12.155  0.000  0.000
> system.time({
+     geno<-rbind(geno.GASP.L,geno.JAG.L)
+ })
[1] 1542.340    9.072 2066.310    0.000    0.000

I logged my notes here as I was trying various things. Partly the reason
is my two questions:

"What was wrong with me?" and
"What the heck?!" remember above? :-)))

which  still remain unanswered :-(

I would have had a lot of fun if I had not to have this done by ...
Yesterday :-))

Thanks a lot for the help

Latchezar  

> dim(genoT)
> class(genoT)
> system.time(out <- lapply(genoT, function(x) match(x, c("AA", "AB",
> "BB"))-1))
> ##
> ##
>     user  system elapsed
> 119.288   0.004 119.339
> 
> (for all 240K)
> 
> best,
> b
> 
> ps: note that "out" is a list.
> 
> On Jul 20, 2007, at 2:01 AM, Latchezar Dimitrov wrote:
> 
> > Hi,
> >
> >> -----Original Message-----
> >> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> >> Sent: Friday, July 20, 2007 12:25 AM
> >> To: Latchezar Dimitrov
> >> Cc: r-help at stat.math.ethz.ch
> >> Subject: Re: [R] Dataframe of factors transform speed?
> >>
> >> it looks like that whatever method you used to genotype the
> >> 1002 samples on the STY array gave you a transposed matrix of 
> >> genotype calls. :-)
> >
> > It only looks like :-)
> >
> > Otherwise it is correctly created dataframe of 1002 samples X (big
> > number) of columns (SNP genotypes). It worked perfectly until I 
> > decided to put together to cohorts independently processed in R 
> > already. I got stuck with my lack of foreseeing. Otherwise I would 
> > have put 3 dummy lines w/ AA,AB, and AB on each one to make 
> sure all 3 
> > genotypes are present and that's it! Lesson for the future :-)
> >
> > Maybe I am not using columns and rows appropriately here but the 
> > dataframe is correct (I have not used FORTRAN since FORTRAN IV ;-)
> > - as
> > str says 1002 observ. of (big number) vars.
> >
> >>
> >> i'd use:
> >>
> >> genoT = read.table(yourFile, stringsAsFactors = FALSE)
> >>
> >> as a starting point... but I don't think that would be 
> efficient (as 
> >> you'd need to fix one column at a time - lapply).
> >
> > No it was not efficient at all. 'matter of fact nothing is more 
> > efficient then loading already read data, alas :-(
> >
> >>
> >> i'd preprocess yourFile before trying to load it:
> >>
> >> cat yourFile | sed -e 's/AA/1/g' | sed -e 's/AB/2/g' | sed -e 
> >> 's/BB/3/ g' > outFile
> >>
> >> and, now, in R:
> >>
> >> genoT = read.table(outFile, header=TRUE)
> >
> > ... Too late ;-) As it must be clear now I have two 
> dataframes I want 
> > to put together with rbind(geno1,geno2). The issue again is 
> > "uniformization" of factor variables w/ missing factors - 
> they ended 
> > up like levels AA,BB on one of the and levels AB,BB on the 
> other which 
> > means as.numeric of AA is 1 on the 1st and as.numeric of AB is 1 on 
> > the second - complete mess. That's why I tried to make both 
> uniform, 
> > i.e.
> > levels "AA","AB", and "BB" for every SNP and then rbind works.
> >
> > In any case my 1st questions remains: "What's wrong with me?" :-)
> >
> > Thanks,
> > Latchezar
> >
> >>
> >> b
> >>
> >> On Jul 19, 2007, at 11:51 PM, Latchezar Dimitrov wrote:
> >>
> >>> Hello,
> >>>
> >>> This is a speed question. I have a dataframe genoT:
> >>>
> >>>> dim(genoT)
> >>> [1]   1002 238304
> >>>
> >>>> str(genoT)
> >>> 'data.frame':   1002 obs. of  238304 variables:
> >>>  $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
> >> 3 3 3 3 3
> >>> ...
> >>>  $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1
> >> 1 1 2 2 2
> >>> ...
> >>>  $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1
> >> 1 1 1 1 1
> >>> ...
> >>>  $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
> >> 3 3 3 3 3
> >>> ...
> >>>  $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2
> >> 3 2 3 3 1
> >>> ...
> >>>  $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA 
> 1 NA 2 1 1
> >>> 2 1
> >>> ...
> >>>  $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
> >> 1 1 1 1 2
> >>> ...
> >>>  $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3
> >> 3 3 3 3 2
> >>> ...
> >>>  $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
> >> 1 1 1 1 2
> >>> ...
> >>>  $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2
> >> 1 2 1 1 3
> >>> ...
> >>>  $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 2 2 3 3 3 2 1
> >>> 2 2 3
> >>> ...
> >>>  $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 3 3 3 3 3 3 3
> >>> 3 3 3
> >>> ...
> >>>  $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 2 2 2 1 1 1 2
> >>> 2 2 2
> >>> ...
> >>>  $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1
> >>> 1 ...
> >>>  $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 2 3 2 2 3 2 2
> >>> 1 1 2
> >>> ...
> >>>  $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1
> >>> 1 1 1
> >>> ...
> >>>  $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 1 1 2 1 1 1 1
> >>> 1 1 1
> >>> ...
> >>>  $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1
> >>> 1 ...
> >>>  $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 2 1
> >>> 1 1 2
> >>> ...
> >>>  $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2
> >> 2 2 NA 1 NA
> >>> 2
> >>> 1 ...
> >>>  $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 1 2 2 1 1 1 3
> >>> 1 1 1
> >>> ...
> >>>  $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 2 2 2 2 2 2
> >>> 2 ...
> >>>  $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1
> >>> 1 ...
> >>>  $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1
> >>> 2 2 1
> >>> ...
> >>>
> >>> Its columns are factors with different number of levels
> >> (from 1 to 3 -
> >>> that's what I got from read.table, i.e., it dropped missing
> >> levels). I
> >>> want to convert it to uniform factors with 3 levels. The
> >> 1st 10 rows
> >>> above show already converted columns and the rest are not yet 
> >>> converted.
> >>> Here's my attempt wich is a complete failure as speed:
> >>>
> >>>> system.time(
> >>> +     for(j in 1:(10         )){ #-- this is to try 1st 
> 10 cols and
> >>> measure the time, it otherwise is ncol(genoT) instead of 10
> >>>
> >>> +        gt<-genoT[[j]]          #-- this is to avoid 2D indices
> >>> +        for(l in 1:length(gt at levels)){
> >>> +          levels(gt)[l] <-
> >> switch(gt at levels[l],AA="0",AB="1",BB="2")
> >>> #-- convert levels to "0","1", or "2"
> >>> +          genoT[[j]]<-factor(gt,levels=0:2)   #-- make a 3-level
> >>> factor
> >>> and put it back
> >>> +        }
> >>> +     }
> >>> + )
> >>> [1] 785.085   4.358 789.454   0.000   0.000
> >>>
> >>> 789s for 10 columns only!
> >>>
> >>> To me it seems like replacing 10 x 3 levels and then making
> >> a factor
> >>> of
> >>> 1002 element vector x 10 is a "negligible" amount of operations 
> >>> needed.
> >>>
> >>> So, what's wrong with me? Any idea how to accelerate
> >> significantly the
> >>> transformation or (to go to the very beginning) to make
> >> read.table use
> >>> a fixed set of levels ("AA","AB", and "BB") and not to drop any
> >>> (missing)
> >>> level?
> >>>
> >>> R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
> >>>
> >>> The machine is with 32G RAM and AMD Opteron 285 (2.? GHz)
> >> so it's not
> >>> it.
> >>>
> >>> Thank you very much for the help,
> >>>
> >>> Latchezar Dimitrov,
> >>> Analyst/Programmer IV,
> >>> Wake Forest University School of Medicine, Winston-Salem, North 
> >>> Carolina, USA
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list 
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide 
> http://www.R-project.org/posting- 
> >>> guide.html and provide commented, minimal, self-contained, 
> >>> reproducible code.
> >>
>


From bartjoosen at hotmail.com  Thu Jul 19 20:57:25 2007
From: bartjoosen at hotmail.com (Bartjoosen)
Date: Thu, 19 Jul 2007 11:57:25 -0700 (PDT)
Subject: [R] SLLOOOWWW function ...
In-Reply-To: <f7i9bn$6e7$1@sea.gmane.org>
References: <f7i9bn$6e7$1@sea.gmane.org>
Message-ID: <11695601.post@talk.nabble.com>


Hi

If I'm right you are trying to find peaks with about the same mass as the
maximum intensity.
Why not make the difference between the maximum mass and the list of masses
subsetted by intensity.
then subset the difference with a thresholded value.


If I'm not understanding your question clear, please let me know what you
are trying to do.
I've written some scripts for MS spectra analyses, if you're interested,
I'll try to find them.


Good luck

Bart



Johannes Graumann-2 wrote:
> 
> Does anybody have any insight into how to make this faster?
> I suspect, that the rounding going on may be an issue, as is the stepping
> through data frame rows using integers ...
> 
> If you have the patience to teach a noob, he will highly appreciate it ;0)
> 
> Joh
> 
> digit <- 4
> for (minute in seq(from=25,to=lrange[2])){
>   # Extract all data associtaed with the current time (minute)
>   frame <- subset(mylist,mylist[["Time"]] == minute)
>   # Sort by Intensity
>   frame <- frame[order(frame[["Intensity"]],decreasing = TRUE),]
>   # Establish output frame using the most intense candidate
>   newframe <- frame[1,]
>   # Establish overlap-checking vector using the most intense candidate
>   lowppm <- round(newframe[1,][["Mass"]]-newframe[1,
> [["Mass"]]/1E6*ppmrange,digits=digit)
>   highppm <- round(newframe[1,][["Mass"]]+newframe[1,
> [["Mass"]]/1E6*ppmrange,digits=digit)
>   presence <- seq(from=lowppm,to=highppm,by=10^(-digit))
>   # Walk through the entire original frame and check whether peaks are
> overlap-free ... do so until max of 2000 entries
>   for (int in seq(from=2,to=nrow(frame))) {
>     if(nrow(newframe) < 2000) {
>       lowppm <- round(frame[int,][["Mass"]]-frame[int,
> [["Mass"]]/1E6*ppmrange,digits=digit)
>       highppm <- round(frame[int,][["Mass"]]+frame[int,
> [["Mass"]]/1E6*ppmrange,digits=digit)
>       windowrange <- seq(from=lowppm,to=highppm,by=10^(-digit))
>       if (sum(round(windowrange,digits=digit) %in%
> round(presence,digits=digit)) < 1) {
>         newframe <- rbind(newframe,frame[int,])
>         presence <- c(presence,windowrange)
>       }
>     } else {
>       break()
>     }
>   }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/SLLOOOWWW-function-...-tf4096328.html#a11695601
Sent from the R help mailing list archive at Nabble.com.


From epistat at gmail.com  Sat Jul 21 14:46:46 2007
From: epistat at gmail.com (zhijie zhang)
Date: Sat, 21 Jul 2007 20:46:46 +0800
Subject: [R] Can I test if there are statistical significance between
	different rows in R*C table?
In-Reply-To: <46A09859.9030805@statistik.uni-dortmund.de>
References: <2fc17e30707190707u6856591al7a0b883e6ff7a236@mail.gmail.com>
	<46A09859.9030805@statistik.uni-dortmund.de>
Message-ID: <2fc17e30707210546m1de084bdx9824496aa4d8b35f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070721/3b631c59/attachment.pl 

From mark_difford at yahoo.co.uk  Sat Jul 21 14:58:06 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Sat, 21 Jul 2007 05:58:06 -0700 (PDT)
Subject: [R] Effect size, interactions,
 and main effects (Stats Question) [ffmanova]
Message-ID: <11722075.post@talk.nabble.com>


Dear List Members,

I would very much appreciate any pointers you could give me on the following
matter:

Main Question:
To what extent does the "rule" that it is unreasonable to talk about main
effects if there are significant interactions in a model depend upon effect
size [of the significant interaction terms]?  Or is this not an issue?

More practically:  Suppose I were to carry out a so-called Type-II MANOVA
(using ffmanova) and were to find that the interaction term in a 2-way
analysis has borderline significance (say p = 0.045) and a small effect
size, whereas one of the main effects is highly signficant (say p = 6.8e-10)
and has a large effect size.

Would it in this case be reasonable for me to ignore the interaction term,
and talk only about main effects?  And, presuming the main question is fair,
are there general guidlines concerning the relationship between level of
significance and effect size for interaction terms.

Thank you in advance for your help,

Mark.


-- 
View this message in context: http://www.nabble.com/Effect-size%2C-interactions%2C-and-main-effects-%28Stats-Question%29--ffmanova--tf4121771.html#a11722075
Sent from the R help mailing list archive at Nabble.com.


From ted.harding at nessie.mcc.ac.uk  Sat Jul 21 15:51:55 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 21 Jul 2007 14:51:55 +0100 (BST)
Subject: [R] Can I test if there are statistical significance between
In-Reply-To: <2fc17e30707210546m1de084bdx9824496aa4d8b35f@mail.gmail.com>
Message-ID: <XFMail.070721145155.ted.harding@nessie.mcc.ac.uk>

On 21-Jul-07 12:46:46, zhijie zhang wrote:
> Dear Uwe Ligges,
[restructuring the given layout of the data]


        Grp1  Grp2        Grp3
Better   16     0  [  1]    1  |  17
                               |
Good     71     4  [ 10]    6  |  81
                               |
Bad      37    61  [118]   57  | 155
-------------------------------+-----
        124    65  [129]   64  | 253

> My hypothesis is if the three groups,that is group1, group2,and
> group3, have the same distributions on coloumns? If not, which
> one is difference from which one?

It is clear that there is no discernible difference between
Group 2 and Group 3. If they are pooled together (totals in
[...] above), it is also clear that there is a very large
difference between [Group 2 + Group 3], or either separately,
and Group 1.

In summary, and in round figures, Groups 2 and 3 have about
90% "Bad", about 10% "Good" and hardly any "Better".

Group 1 has only about 30% "Bad", about 60% "Good", and 10%
"Better".

Formally:

A<-cbind(c(16,71,37),c(1,10,118))
A
       [,1] [,2]
  [1,]   16    1
  [2,]   71   10
  [3,]   37  118

chisq.test(A)
  Pearson's Chi-squared test
  data:  A
  X-squared = 101.4434, df = 2, p-value = < 2.2e-16

cbind(round(chisq.test(A)$expected,1),A)
       [,1] [,2]    [,3] [,4]
  [1,]  8.3  8.7      16    1
  [2,] 39.7 41.3      71   10
  [3,] 76.0 79.0      37  118


B<-cbind(c(0,4,61),c(1,6,57))
B
       [,1] [,2]
  [1,]    0    1
  [2,]    4    6
  [3,]   61   57

chisq.test(B,simulate.p.value=TRUE)
  Pearson's Chi-squared test with simulated p-value
  (based on 2000 replicates)
  data:  B 
  X-squared = 1.5279, df = NA, p-value = 0.3215

fisher.test(B)
  Fisher's Exact Test for Count Data
  data:  B 
  p-value = 0.4247
  alternative hypothesis: two.sided 

That, with possible refinements for more careful statements
about the proportions in the three outcome classes, is about
all one can say about these data; and it is definite enough!

With the totally non-committal P-value for Group 2 vs Group 3,
and the absolutely decisive P-value for Group 1 vs Groups 2&3,
there is no need whatever to bother with "multiple comparison"
complications.

Best wishes,
Ted.


> On 7/20/07, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
>>
>>
>>
>> zhijie zhang wrote:
>> > Dear  friends,
>> >   My R*C table is as follow:
>> >
>> >
>> >
>> > better
>> >
>> > good
>> >
>> > bad
>> >
>> > Goup1
>> >
>> > 16
>> >
>> > 71
>> >
>> > 37
>> >
>> > Group2
>> >
>> > 0
>> >
>> > 4
>> >
>> > 61
>> >
>> > Group3
>> >
>> > 1
>> >
>> > 6
>> >
>> > 57
>> >
>> >    Can I test if there are statistical significant between Group1
>> >    and
>> > Group2, Group2 and Group3, Group1 and Group2, taking into the
>> > multiple
>> > comparisons?
>>
>>
>> So what is you hypothesis? Statistical significance of what it to be
>> tested?
>>
>> Uwe Ligges
>>
>>
>>
>> > The table can be set up using the following program:
>> >
>> > a<-matrix(data=c(16,71,37,0,4,61,1,6,57),nrow=3,byrow=TRUE)
>> > Thanks very much.
>> >
>> >
>>
> 
> 
> 
> -- 
> With Kind Regards,
> 
> oooO:::::::::
> (..):::::::::
>:\.(:::Oooo::
>::\_)::(..)::
>:::::::)./:::
>::::::(_/::::
>:::::::::::::
> [***********************************************************************
> ]
> Zhi Jie,Zhang ,PHD
> Tel:86-21-54237149
> Dept. of Epidemiology,School of Public Health,Fudan University
> Address:No. 138 Yi Xue Yuan Road,Shanghai,China
> Postcode:200032
> Email:epistat at gmail.com
> Website: www.statABC.com
> [***********************************************************************
> ]
> oooO:::::::::
> (..):::::::::
> :\.(:::Oooo::
> ::\_)::(..)::
> :::::::)./:::
> ::::::(_/::::
> :::::::::::::

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 21-Jul-07                                       Time: 14:51:46
------------------------------ XFMail ------------------------------


From ral at lcfltd.com  Sat Jul 21 16:00:23 2007
From: ral at lcfltd.com (Robert A LaBudde)
Date: Sat, 21 Jul 2007 10:00:23 -0400
Subject: [R] Effect size, interactions,
 and main effects (Stats  Question) [ffmanova]
In-Reply-To: <11722075.post@talk.nabble.com>
References: <11722075.post@talk.nabble.com>
Message-ID: <0JLJ00LYE88O2QLA@vms042.mailsrvcs.net>

Statistical significance is "detectability", and depends upon the 
size of the  sample as well as the effect. A large enough experiment 
will result in statistical detectability of almost every interaction 
action term allowed.

This is why estimation, not testing, has become the consensus 
recommendation in statistics.

As a practical matter, evaluate the combined effect of your model 
terms with and without the interaction term(s) you are worried about. 
Is the reduction in accuracy of physical importance? If so, the 
interaction terms are required for scientific reasons. If not, 
present both results and indicate the acceptability (for 
interpolation) of the simpler model.

You should also make it your first priority to hypothecate why the 
interaction terms are meaningful and expected. If a cause can be 
found, it may suggest an alternate model that will eliminate 
interactions, or satisfy your anxiety. If not, it may support your 
argument to simplify.


At 08:58 AM 7/21/2007, Mark wrote:

>Dear List Members,
>
>I would very much appreciate any pointers you could give me on the following
>matter:
>
>Main Question:
>To what extent does the "rule" that it is unreasonable to talk about main
>effects if there are significant interactions in a model depend upon effect
>size [of the significant interaction terms]?  Or is this not an issue?
>
>More practically:  Suppose I were to carry out a so-called Type-II MANOVA
>(using ffmanova) and were to find that the interaction term in a 2-way
>analysis has borderline significance (say p = 0.045) and a small effect
>size, whereas one of the main effects is highly signficant (say p = 6.8e-10)
>and has a large effect size.
>
>Would it in this case be reasonable for me to ignore the interaction term,
>and talk only about main effects?  And, presuming the main question is fair,
>are there general guidlines concerning the relationship between level of
>significance and effect size for interaction terms.
>
>Thank you in advance for your help,

================================================================
Robert A. LaBudde, PhD, PAS, Dpl. ACAFS  e-mail: ral at lcfltd.com
Least Cost Formulations, Ltd.            URL: http://lcfltd.com/
824 Timberlake Drive                     Tel: 757-467-0954
Virginia Beach, VA 23464-3239            Fax: 757-467-2947

"Vere scire est per causas scire"


From mark_difford at yahoo.co.uk  Sat Jul 21 18:07:42 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Sat, 21 Jul 2007 09:07:42 -0700 (PDT)
Subject: [R] Effect size, interactions,
 and main effects (Stats  Question) [ffmanova]
In-Reply-To: <0JLJ00LYE88O2QLA@vms042.mailsrvcs.net>
References: <11722075.post@talk.nabble.com>
	<0JLJ00LYE88O2QLA@vms042.mailsrvcs.net>
Message-ID: <11723512.post@talk.nabble.com>


Hi Robert,

Thank you for your reply, and for what appears to be some good/sound
practical advice on managing this kind of problem.

Best Regards,
Mark.


Robert A LaBudde wrote:
> 
> Statistical significance is "detectability", and depends upon the 
> size of the  sample as well as the effect. A large enough experiment 
> will result in statistical detectability of almost every interaction 
> action term allowed.
> 
> This is why estimation, not testing, has become the consensus 
> recommendation in statistics.
> 
> As a practical matter, evaluate the combined effect of your model 
> terms with and without the interaction term(s) you are worried about. 
> Is the reduction in accuracy of physical importance? If so, the 
> interaction terms are required for scientific reasons. If not, 
> present both results and indicate the acceptability (for 
> interpolation) of the simpler model.
> 
> You should also make it your first priority to hypothecate why the 
> interaction terms are meaningful and expected. If a cause can be 
> found, it may suggest an alternate model that will eliminate 
> interactions, or satisfy your anxiety. If not, it may support your 
> argument to simplify.
> 
> 
> At 08:58 AM 7/21/2007, Mark wrote:
> 
>>Dear List Members,
>>
>>I would very much appreciate any pointers you could give me on the
following
>>matter:
>>
>>Main Question:
>>To what extent does the "rule" that it is unreasonable to talk about main
>>effects if there are significant interactions in a model depend upon
effect
>>size [of the significant interaction terms]?  Or is this not an issue?
>>
>>More practically:  Suppose I were to carry out a so-called Type-II MANOVA
>>(using ffmanova) and were to find that the interaction term in a 2-way
>>analysis has borderline significance (say p = 0.045) and a small effect
>>size, whereas one of the main effects is highly signficant (say p =
6.8e-10)
>>and has a large effect size.
>>
>>Would it in this case be reasonable for me to ignore the interaction term,
>>and talk only about main effects?  And, presuming the main question is
fair,
>>are there general guidlines concerning the relationship between level of
>>significance and effect size for interaction terms.
>>
>>Thank you in advance for your help,
> 
> ================================================================
> Robert A. LaBudde, PhD, PAS, Dpl. ACAFS  e-mail: ral at lcfltd.com
> Least Cost Formulations, Ltd.            URL: http://lcfltd.com/
> 824 Timberlake Drive                     Tel: 757-467-0954
> Virginia Beach, VA 23464-3239            Fax: 757-467-2947
> 
> "Vere scire est per causas scire"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Effect-size%2C-interactions%2C-and-main-effects-%28Stats-Question%29--ffmanova--tf4121771.html#a11723512
Sent from the R help mailing list archive at Nabble.com.


From mark at wardle.org  Sat Jul 21 22:43:06 2007
From: mark at wardle.org (Mark Wardle)
Date: Sat, 21 Jul 2007 21:43:06 +0100
Subject: [R] Binomial multi-level (hierarchical) modelling [partly stats
	question, not completely R related]
Message-ID: <b59a37130707211343w5d102ec4t728068264f19e550@mail.gmail.com>

Dear all,

This question is partly statistics and partly R and I apologise in
advance for my (usual) verbosity! My data is a little more complicated
that this suggests, but essentially I have proportion data from
different studies (id), each from a specific country and region of the
World. I would like to examine the variables that affect the
proportion, but these factors are hierarchical. In case I am not
explaining the problem correctly, I have created a simple analysis
with simulated data below!

My question is: How does one perform logistic regression using
interdependent (hieraerchical) variables and decide how much each
factor contributes to the variability?

Originally I wanted to look at whether there was only a regional
difference between studies, and simply aggregated the data and
performed a standard contingency analysis. However, I wanted to make
sure I wasn't aggregating and losing valuable covariates (?Simpson's
paradox...). Hence I thought I would be better off using binomial
regression instead, just to check other factors are not possible
confounders:

# At present I have a data frame with nested groups with proportion
data (counts and totals)
# make up some equivalent simulated data:
d1 <- data.frame(
	id    = factor(seq(1,20)),
	count = round(c(rnorm(10, mean=10, sd=4),rnorm(10, mean=20, sd=2)),0),
	total = round(rnorm(20, mean=50, sd=5),0),
	country = c('UK','UK','Fr','Fr','Sp','Sp',rep('Other',4),
				rep('Japan',8),rep('China',2)),
	region = c(rep('Europe',10), rep('Asia',10))
)
d1$remain <- with(d1, total-count)
summary(d1)	# show what we've done

# current analysis - simple aggregate and perform assessment of contigency table
d2 <- aggregate(d1[,2:3], by=list(d1$region), FUN=sum )
d2$remain <- with(d2, total-count)
d2.f <- fisher.test(matrix(c(d2$count, d2$remain), ncol=2))
print(d2.f)		# highly significant difference between europe and asia

# I liked the result of this - highly significant for both simulated
and real data, and easy to interpret for a stats-idiot like me!

# a better (?) analysis?
m1 <- glm(cbind(count, remain) ~ region, "binomial", data=d1)
summary(m1)

# with real data:
#    Call:
#    glm(formula = cbind(count, remain) ~ region, family = "binomial",
#        data = d1)
#
#    Deviance Residuals:
#           Min          1Q      Median          3Q         Max
#    -5.2427766  -2.4016616  -1.0373422  -0.0004486   4.9658610
#
#    Coefficients:
#                         Estimate Std. Error z value Pr(>|z|)
#    (Intercept)           -1.7402     0.1007 -17.287  < 2e-16 ***
#    regionAsia, Americas  -0.6781     0.2128  -3.187  0.00144 **
#    regionEurope          -0.8692     0.1665  -5.219 1.80e-07 ***
#    regionIndia          -16.6492   956.1380  -0.017  0.98611
#    regionOther            0.3107     0.2882   1.078  0.28096
#    ---
#    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
#    (Dispersion parameter for binomial family taken to be 1)
#
#        Null deviance: 187.02  on 18  degrees of freedom
#    Residual deviance: 141.37  on 14  degrees of freedom
#      (2 observations deleted due to missingness)
#    AIC: 209.28
#
#    Number of Fisher Scoring iterations: 14




# *** but how does one include nested parameters (ie. decide how much
of the variability is dependent on individual study, country and
region? especially given that these factors are inter-dependent -
hierarchical.)

m2 <- glm(cbind(count, remain) ~ region + country + id, "binomial", data=d1)

# with the fake data, nothing is significant - how does one decide
which effect to remove?

# and with REAL data:
#         Null deviance: 1.8702e+02  on 18  degrees of freedom
#     Residual deviance: 5.4476e-10  on  0  degrees of freedom
#       (2 observations deleted due to missingness)
#     AIC: 95.914
#
#     Number of Fisher Scoring iterations: 22

# Therefore, take out "id" as this clearly isn't the correct approach!

m3 <- glm(cbind(count, remain) ~ region + country, "binomial", data=d1)
summary(m3)

# and with REAL data:
#      Call:
#      glm(formula = cbind(count, remain) ~ region + country, family =
"binomial",
#          data = d1)
#
#      Deviance Residuals:
#                Basu:2000          Brusco:2004           Filla:2000
Hadjivassiliou:2003         Juvonen:2005           Leggo:1997
Maruyama:2002         Nagaoka:1999         Onodera:2000
Pujana:1999          Saleem:2000          Schols:1997
#               -6.322e-08           -2.978e-01            3.468e-01
         7.383e-02            0.000e+00           -9.928e-02
 3.328e+00           -1.225e+00           -2.354e+00
-1.718e-07           -2.002e-04            4.215e-08
#            Silveira:2002           Soong:2001          Storey:2000
       Takano:1998            Tang:2000      Warrenburg:2002
Watanabe:1998
#                0.000e+00            0.000e+00           -1.421e-07
         0.000e+00           -2.019e-04            0.000e+00
-3.590e+00
#
#      Coefficients: (3 not defined because of singularities)
#                                Estimate Std. Error z value Pr(>|z|)
#      (Intercept)                -2.1102     0.3744  -5.637 1.73e-08 ***
#      regionAsia, Americas       -0.3080     0.4187  -0.736  0.46193
#      regionEurope               -0.8342     0.7007  -1.191  0.23385
#      regionIndia               -18.1705  4285.1131  -0.004  0.99662
#      regionOther                 0.6807     0.4616   1.475  0.14026
#      countryChina              -20.0407  4247.6466  -0.005  0.99624
#      countryFinland             -0.9268     1.1712  -0.791  0.42877
#      countryGermany              1.6833     0.6530   2.578  0.00994 **
#      countryIndia               -1.1087     1.0863  -1.021  0.30747
#      countryItaly               -1.3562     0.7773  -1.745  0.08104 .
#      countryJapan                0.5989     0.3893   1.538  0.12396
#      countryJapan, USA               NA         NA      NA       NA
#      countryNetherlands          1.2081     0.6209   1.946  0.05167 .
#      countryPortugal / Brazil   -1.7095     1.1664  -1.466  0.14273
#      countrySpain               -1.3182     1.1683  -1.128  0.25918
#      countryTaiwan                   NA         NA      NA       NA
#      countryUK                       NA         NA      NA       NA
#      ---
#      Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
#      (Dispersion parameter for binomial family taken to be 1)
#
#          Null deviance: 187.019  on 18  degrees of freedom
#      Residual deviance:  31.233  on  5  degrees of freedom
#        (2 observations deleted due to missingness)
#      AIC: 117.15
#
#      Number of Fisher Scoring iterations: 17

So I have evidence of marked overdispersion (if I read this
correctly!) with my real data.

It seems to me that this analysis needs a mixed model with
hierarchical terms and a binomial link function (?using nlme). Is this
possible? I tried using lme4's lmer() function like this:
library(lme4)
d1$prop <- with(d1, count/total)
me1 <- lmer(prop ~ country + (1|region) + (0+country|region),
family='binomial', weights=total, data=d1)

But I don't think I am using this properly and can't seem to determine
the variance due to the different hierarchical terms.

Can one use the interaction of country*region instead of using a mixed
model (where presumably country would be a fixed effect and region a
random effct?)
Any pointers much appreciated!

Many thanks,

Mark

P.S. while experimenting while creating the line:
> me1 <- lmer(prop ~ country + (1|region) + (0+country|region), family='binomial', weights=total, data=d1)

I accidentally pressed <RETURN> and issued the following command:

me1 <- lmer(prop ~ country + (0|region), family='binomial',
weights=total, data=d1)

which causes R to crash (and the application closes completely).

This is with R version

> sessionInfo()
R version 2.5.1 (2007-06-27)
i386-apple-darwin8.9.1

locale:
en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] "stats"     "graphics"  "grDevices" "datasets"  "utils"
"methods"   "base"

other attached packages:
        lme4       Matrix      lattice        RODBC
 "0.99875-4" "0.999375-0"    "0.15-11"      "1.2-1"


-- 
Dr. Mark Wardle
Clinical research fellow and specialist registrar, Neurology
Cardiff, UK


From adschai at optonline.net  Sat Jul 21 23:29:34 2007
From: adschai at optonline.net (adschai at optonline.net)
Date: Sat, 21 Jul 2007 21:29:34 +0000 (GMT)
Subject: [R] Question about genalg in R
Message-ID: <e4be8b906b6.46a27abe@optonline.net>

Hi

I am first looking at genalg as a starting point for GA implementation in R. I have a couple of questions regarding this package and GA implementation in general. If you could provide some hints on these questions, I would really appreciate. Thank you in advance.

1. I found two methods for performing optimization based on the type of chromosome encoding, i.e. binary and float. I'm wondering if I have a representation that mixes between the two, is there any other methods or functions that I can use? If there is none in genalg, is there any such  implementation in other package?

2. This is a question on multiobjective optimization base on GA. I'm wondering if there is any implementation in R or known package that I can use? Thank you.

- adschai


From jholtman at gmail.com  Sat Jul 21 23:33:18 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 21 Jul 2007 17:33:18 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E7@EXCHVS1.medctr.ad.wfubmc.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
	<DDAAAB60-ECB1-4B25-9A50-C859F3249DF8@jhsph.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E6@EXCHVS1.medctr.ad.wfubmc.edu>
	<62B73FC8-3B10-4037-A51E-2AFC6142F96E@jhsph.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E7@EXCHVS1.medctr.ad.wfubmc.edu>
Message-ID: <644e1f320707211433k6c95e7e2ydb87f240fcdaa1b2@mail.gmail.com>

One of the problems is that you are probably paging on your system
with an object that size (240000 x 1000).  This is about 1GB for a
single object:

> set.seed(123)
> n <- 240000
> system.time({
+ genoT <- lapply(1:n, function(i) factor(sample(c("AA",
+ "AB", "BB"), 1000, prob=c(1000, 1, 1), rep=T)))
+ })
   user  system elapsed
  95.00    0.61  104.71
> names(genoT) = paste("snp", 1:n, sep="")
>
> object.size(genoT)
[1] 1045258752
>

I can create it on my 2GB machine as a list, but have problems
converting it to a dataframe because I don't have enough memory.

So unless you have at least 4GB on your system, it might take a long
time.  Look at your performance measurements on your system and see if
you have run out of physical memory and are paging.

On 7/21/07, Latchezar Dimitrov <ldimitro at wfubmc.edu> wrote:
> Hi,
>
> Thanks for the help. My 1st question still unanswered though :-) Please
> see bellow
>
> > -----Original Message-----
> > From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> > Sent: Friday, July 20, 2007 3:30 AM
> > To: Latchezar Dimitrov
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Dataframe of factors transform speed?
> >
> > set.seed(123)
> > genoT = lapply(1:240000, function(i) factor(sample(c("AA",
> > "AB", "BB"), 1000, prob=sample(c(1, 1000, 1000), 3), rep=T)))
> > names(genoT) = paste("snp", 1:240000, sep="") genoT =
> > as.data.frame(genoT)
>
> Now this _is the problem. Everything before converting to data.frame
> worked almost instantaneously however as.data.frame runs forever.
> Obviously there is some scalability memory management issue. When I
> tried my own method but creating a new result (instead of modifying the
> old) dataframe it worked like a charm for the 1st 100 cols ~ .3s. I
> figured 300,000 cols should be ~1000s. Nope! It ran for about 50,000(!)s
> to finish about 42,000 cols only.
>
> BTW, what ver. of R is yours?
>
> Now here's what I "discovered" further.
>
> #-- create a 1-col frame:
>    geno   <-
> data.frame(c(geno.GASP[[1]],geno.JAG[[1]]),row.names=c(rownames(geno.GAS
> P),rownames(geno.JAG)))
>
> #-- main code I repeated it w/ j in 1:1000, 2001:3000, and 3001:4000,
> i.e., adding a 1000 of cols to geno each time
>
> system.time(
> #   for(j in 1:(ncol(geno.GASP      ))){
>    for(j in 3001:(4000              )){
>      gt.GASP<-geno.GASP[[j]]
>       for(l in 1:length(gt.GASP at levels)){
>         levels(gt.GASP)[l] <-
> switch(gt.GASP at levels[l],AA="0",AB="1",BB="2")
>       }
>       gt.JAG <-geno.JAG [[j]]
> #      for(l in 1:length(gt.JAG @levels)){
> #        levels(gt.JAG )[l] <- switch(gt.JAG
> @levels[l],AA="0",AB="1",BB="2")
> #      }
>       geno[[j]]<-factor(c(as.numeric(factor(gt.GASP,levels=0:2))-1
> ###               factor(c(as.numeric(factor(gt.GASP,levels=0:2))-1
>                          ,as.numeric(factor(gt.JAG, levels=0:2))-1
>                          )
>                        ,levels=0:2
>                        )
>    }
> )
>
> Times (each one is for a 1000 cols!):
> [1] 26.673  0.032 26.705  0.000  0.000
> [1] 77.186  0.037 77.225  0.000  0.000
> [1] 128.165   0.042 128.209   0.000   0.000
> [1] 180.940   0.047 180.989   0.000   0.000
>
> See the big diff and the scaling I mentioned above?
>
> Further more I removed geno[[j]] assignment leaving the operation
> though, i.e., replaced it with ### line above. Times:
>
> [1] 0.857 0.008 0.865 0.000 0.000
>
> Huh!? What the heck! That's my second question :-) Any ideas?
>
> I still believe my method is near optimal. Of course I have to somehow
> get rid of the assignment bottleneck.
>
> For now the lesson is: "God bless lists"
>
> Here is my final solution:
>
> > system.time({
> +     geno.GASP.L<-lapply(geno.GASP
> +                        ,function(x){
> +                           for(l in 1:length(x at levels)){levels(x)[l] <-
> switch(x at levels[l],AA="0",AB="1",BB="2")}
> +                           factor(x,levels=0:2)
> +                         }
> +                  )
> +     geno.JAG.L <-lapply(geno.JAG
> +                        ,function(x){
> + #                         for(l in 1:length(x at levels)){levels(x)[l] <-
> switch(x at levels[l],AA="0",AB="1",BB="2")}
> +                           factor(x,levels=0:2)
> +                         }
> +                  )
> + })
> [1] 192.800   1.566 194.413   0.000   0.000   !!!!!!!!! :-)))))
> > system.time({
> +     class    (geno.GASP.L)<-"data.frame"
> +     row.names(geno.GASP.L)<-row.names(geno.GASP)
> +     class    (geno.JAG.L )<-"data.frame"
> +     row.names(geno.JAG.L )<-row.names(geno.JAG )
> + })
> [1] 12.156  0.001 12.155  0.000  0.000
> > system.time({
> +     geno<-rbind(geno.GASP.L,geno.JAG.L)
> + })
> [1] 1542.340    9.072 2066.310    0.000    0.000
>
> I logged my notes here as I was trying various things. Partly the reason
> is my two questions:
>
> "What was wrong with me?" and
> "What the heck?!" remember above? :-)))
>
> which  still remain unanswered :-(
>
> I would have had a lot of fun if I had not to have this done by ...
> Yesterday :-))
>
> Thanks a lot for the help
>
> Latchezar
>
> > dim(genoT)
> > class(genoT)
> > system.time(out <- lapply(genoT, function(x) match(x, c("AA", "AB",
> > "BB"))-1))
> > ##
> > ##
> >     user  system elapsed
> > 119.288   0.004 119.339
> >
> > (for all 240K)
> >
> > best,
> > b
> >
> > ps: note that "out" is a list.
> >
> > On Jul 20, 2007, at 2:01 AM, Latchezar Dimitrov wrote:
> >
> > > Hi,
> > >
> > >> -----Original Message-----
> > >> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> > >> Sent: Friday, July 20, 2007 12:25 AM
> > >> To: Latchezar Dimitrov
> > >> Cc: r-help at stat.math.ethz.ch
> > >> Subject: Re: [R] Dataframe of factors transform speed?
> > >>
> > >> it looks like that whatever method you used to genotype the
> > >> 1002 samples on the STY array gave you a transposed matrix of
> > >> genotype calls. :-)
> > >
> > > It only looks like :-)
> > >
> > > Otherwise it is correctly created dataframe of 1002 samples X (big
> > > number) of columns (SNP genotypes). It worked perfectly until I
> > > decided to put together to cohorts independently processed in R
> > > already. I got stuck with my lack of foreseeing. Otherwise I would
> > > have put 3 dummy lines w/ AA,AB, and AB on each one to make
> > sure all 3
> > > genotypes are present and that's it! Lesson for the future :-)
> > >
> > > Maybe I am not using columns and rows appropriately here but the
> > > dataframe is correct (I have not used FORTRAN since FORTRAN IV ;-)
> > > - as
> > > str says 1002 observ. of (big number) vars.
> > >
> > >>
> > >> i'd use:
> > >>
> > >> genoT = read.table(yourFile, stringsAsFactors = FALSE)
> > >>
> > >> as a starting point... but I don't think that would be
> > efficient (as
> > >> you'd need to fix one column at a time - lapply).
> > >
> > > No it was not efficient at all. 'matter of fact nothing is more
> > > efficient then loading already read data, alas :-(
> > >
> > >>
> > >> i'd preprocess yourFile before trying to load it:
> > >>
> > >> cat yourFile | sed -e 's/AA/1/g' | sed -e 's/AB/2/g' | sed -e
> > >> 's/BB/3/ g' > outFile
> > >>
> > >> and, now, in R:
> > >>
> > >> genoT = read.table(outFile, header=TRUE)
> > >
> > > ... Too late ;-) As it must be clear now I have two
> > dataframes I want
> > > to put together with rbind(geno1,geno2). The issue again is
> > > "uniformization" of factor variables w/ missing factors -
> > they ended
> > > up like levels AA,BB on one of the and levels AB,BB on the
> > other which
> > > means as.numeric of AA is 1 on the 1st and as.numeric of AB is 1 on
> > > the second - complete mess. That's why I tried to make both
> > uniform,
> > > i.e.
> > > levels "AA","AB", and "BB" for every SNP and then rbind works.
> > >
> > > In any case my 1st questions remains: "What's wrong with me?" :-)
> > >
> > > Thanks,
> > > Latchezar
> > >
> > >>
> > >> b
> > >>
> > >> On Jul 19, 2007, at 11:51 PM, Latchezar Dimitrov wrote:
> > >>
> > >>> Hello,
> > >>>
> > >>> This is a speed question. I have a dataframe genoT:
> > >>>
> > >>>> dim(genoT)
> > >>> [1]   1002 238304
> > >>>
> > >>>> str(genoT)
> > >>> 'data.frame':   1002 obs. of  238304 variables:
> > >>>  $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
> > >> 3 3 3 3 3
> > >>> ...
> > >>>  $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1
> > >> 1 1 2 2 2
> > >>> ...
> > >>>  $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1
> > >> 1 1 1 1 1
> > >>> ...
> > >>>  $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
> > >> 3 3 3 3 3
> > >>> ...
> > >>>  $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2
> > >> 3 2 3 3 1
> > >>> ...
> > >>>  $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA
> > 1 NA 2 1 1
> > >>> 2 1
> > >>> ...
> > >>>  $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
> > >> 1 1 1 1 2
> > >>> ...
> > >>>  $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3
> > >> 3 3 3 3 2
> > >>> ...
> > >>>  $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
> > >> 1 1 1 1 2
> > >>> ...
> > >>>  $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2
> > >> 1 2 1 1 3
> > >>> ...
> > >>>  $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 2 2 3 3 3 2 1
> > >>> 2 2 3
> > >>> ...
> > >>>  $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 3 3 3 3 3 3 3
> > >>> 3 3 3
> > >>> ...
> > >>>  $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 2 2 2 1 1 1 2
> > >>> 2 2 2
> > >>> ...
> > >>>  $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1
> > >>> 1 ...
> > >>>  $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 2 3 2 2 3 2 2
> > >>> 1 1 2
> > >>> ...
> > >>>  $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1
> > >>> 1 1 1
> > >>> ...
> > >>>  $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 1 1 2 1 1 1 1
> > >>> 1 1 1
> > >>> ...
> > >>>  $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1
> > >>> 1 ...
> > >>>  $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 2 1
> > >>> 1 1 2
> > >>> ...
> > >>>  $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2
> > >> 2 2 NA 1 NA
> > >>> 2
> > >>> 1 ...
> > >>>  $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 1 2 2 1 1 1 3
> > >>> 1 1 1
> > >>> ...
> > >>>  $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 2 2 2 2 2 2
> > >>> 2 ...
> > >>>  $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1
> > >>> 1 ...
> > >>>  $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1
> > >>> 2 2 1
> > >>> ...
> > >>>
> > >>> Its columns are factors with different number of levels
> > >> (from 1 to 3 -
> > >>> that's what I got from read.table, i.e., it dropped missing
> > >> levels). I
> > >>> want to convert it to uniform factors with 3 levels. The
> > >> 1st 10 rows
> > >>> above show already converted columns and the rest are not yet
> > >>> converted.
> > >>> Here's my attempt wich is a complete failure as speed:
> > >>>
> > >>>> system.time(
> > >>> +     for(j in 1:(10         )){ #-- this is to try 1st
> > 10 cols and
> > >>> measure the time, it otherwise is ncol(genoT) instead of 10
> > >>>
> > >>> +        gt<-genoT[[j]]          #-- this is to avoid 2D indices
> > >>> +        for(l in 1:length(gt at levels)){
> > >>> +          levels(gt)[l] <-
> > >> switch(gt at levels[l],AA="0",AB="1",BB="2")
> > >>> #-- convert levels to "0","1", or "2"
> > >>> +          genoT[[j]]<-factor(gt,levels=0:2)   #-- make a 3-level
> > >>> factor
> > >>> and put it back
> > >>> +        }
> > >>> +     }
> > >>> + )
> > >>> [1] 785.085   4.358 789.454   0.000   0.000
> > >>>
> > >>> 789s for 10 columns only!
> > >>>
> > >>> To me it seems like replacing 10 x 3 levels and then making
> > >> a factor
> > >>> of
> > >>> 1002 element vector x 10 is a "negligible" amount of operations
> > >>> needed.
> > >>>
> > >>> So, what's wrong with me? Any idea how to accelerate
> > >> significantly the
> > >>> transformation or (to go to the very beginning) to make
> > >> read.table use
> > >>> a fixed set of levels ("AA","AB", and "BB") and not to drop any
> > >>> (missing)
> > >>> level?
> > >>>
> > >>> R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
> > >>>
> > >>> The machine is with 32G RAM and AMD Opteron 285 (2.? GHz)
> > >> so it's not
> > >>> it.
> > >>>
> > >>> Thank you very much for the help,
> > >>>
> > >>> Latchezar Dimitrov,
> > >>> Analyst/Programmer IV,
> > >>> Wake Forest University School of Medicine, Winston-Salem, North
> > >>> Carolina, USA
> > >>>
> > >>> ______________________________________________
> > >>> R-help at stat.math.ethz.ch mailing list
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide
> > http://www.R-project.org/posting-
> > >>> guide.html and provide commented, minimal, self-contained,
> > >>> reproducible code.
> > >>
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ldimitro at wfubmc.edu  Sun Jul 22 00:51:18 2007
From: ldimitro at wfubmc.edu (Latchezar Dimitrov)
Date: Sat, 21 Jul 2007 18:51:18 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <644e1f320707211433k6c95e7e2ydb87f240fcdaa1b2@mail.gmail.com>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
	<DDAAAB60-ECB1-4B25-9A50-C859F3249DF8@jhsph.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E6@EXCHVS1.medctr.ad.wfubmc.edu>
	<62B73FC8-3B10-4037-A51E-2AFC6142F96E@jhsph.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E7@EXCHVS1.medctr.ad.wfubmc.edu>
	<644e1f320707211433k6c95e7e2ydb87f240fcdaa1b2@mail.gmail.com>
Message-ID: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E8@EXCHVS1.medctr.ad.wfubmc.edu>

Jim,

No, this is _not the problem. If you go to my 1st mail I have a monster
(at least was when I purchased it) with 32GB (sic :-) of RAM and 4 dual
core AMD64 285 (the fastest at that time and still pretty fast now :-) 

The machine stats paging when I run 2 copies of R working on two things
like that :-). If you look at my last e-mail I found a solution but
still have no clue why the heck x<-as.data.frame(y) where why is a list
of the same columns take real for ever and this the thing that killed me
before.

Thanks,
Latchezar

> -----Original Message-----
> From: jim holtman [mailto:jholtman at gmail.com] 
> Sent: Saturday, July 21, 2007 5:33 PM
> To: Latchezar Dimitrov
> Cc: Benilton Carvalho; r-help at stat.math.ethz.ch
> Subject: Re: [R] Dataframe of factors transform speed?
> 
> One of the problems is that you are probably paging on your 
> system with an object that size (240000 x 1000).  This is 
> about 1GB for a single object:
> 
> > set.seed(123)
> > n <- 240000
> > system.time({
> + genoT <- lapply(1:n, function(i) factor(sample(c("AA", "AB", "BB"), 
> + 1000, prob=c(1000, 1, 1), rep=T)))
> + })
>    user  system elapsed
>   95.00    0.61  104.71
> > names(genoT) = paste("snp", 1:n, sep="")
> >
> > object.size(genoT)
> [1] 1045258752
> >
> 
> I can create it on my 2GB machine as a list, but have 
> problems converting it to a dataframe because I don't have 
> enough memory.
> 
> So unless you have at least 4GB on your system, it might take 
> a long time.  Look at your performance measurements on your 
> system and see if you have run out of physical memory and are paging.
> 
> On 7/21/07, Latchezar Dimitrov <ldimitro at wfubmc.edu> wrote:
> > Hi,
> >
> > Thanks for the help. My 1st question still unanswered though :-) 
> > Please see bellow
> >
> > > -----Original Message-----
> > > From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> > > Sent: Friday, July 20, 2007 3:30 AM
> > > To: Latchezar Dimitrov
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: Re: [R] Dataframe of factors transform speed?
> > >
> > > set.seed(123)
> > > genoT = lapply(1:240000, function(i) factor(sample(c("AA", "AB", 
> > > "BB"), 1000, prob=sample(c(1, 1000, 1000), 3), rep=T)))
> > > names(genoT) = paste("snp", 1:240000, sep="") genoT =
> > > as.data.frame(genoT)
> >
> > Now this _is the problem. Everything before converting to 
> data.frame 
> > worked almost instantaneously however as.data.frame runs forever.
> > Obviously there is some scalability memory management issue. When I 
> > tried my own method but creating a new result (instead of modifying 
> > the
> > old) dataframe it worked like a charm for the 1st 100 cols ~ .3s. I 
> > figured 300,000 cols should be ~1000s. Nope! It ran for about 
> > 50,000(!)s to finish about 42,000 cols only.
> >
> > BTW, what ver. of R is yours?
> >
> > Now here's what I "discovered" further.
> >
> > #-- create a 1-col frame:
> >    geno   <-
> > 
> data.frame(c(geno.GASP[[1]],geno.JAG[[1]]),row.names=c(rownames(geno.G
> > AS
> > P),rownames(geno.JAG)))
> >
> > #-- main code I repeated it w/ j in 1:1000, 2001:3000, and 
> 3001:4000, 
> > i.e., adding a 1000 of cols to geno each time
> >
> > system.time(
> > #   for(j in 1:(ncol(geno.GASP      ))){
> >    for(j in 3001:(4000              )){
> >      gt.GASP<-geno.GASP[[j]]
> >       for(l in 1:length(gt.GASP at levels)){
> >         levels(gt.GASP)[l] <-
> > switch(gt.GASP at levels[l],AA="0",AB="1",BB="2")
> >       }
> >       gt.JAG <-geno.JAG [[j]]
> > #      for(l in 1:length(gt.JAG @levels)){
> > #        levels(gt.JAG )[l] <- switch(gt.JAG
> > @levels[l],AA="0",AB="1",BB="2")
> > #      }
> >       geno[[j]]<-factor(c(as.numeric(factor(gt.GASP,levels=0:2))-1
> > ###               factor(c(as.numeric(factor(gt.GASP,levels=0:2))-1
> >                          ,as.numeric(factor(gt.JAG, levels=0:2))-1
> >                          )
> >                        ,levels=0:2
> >                        )
> >    }
> > )
> >
> > Times (each one is for a 1000 cols!):
> > [1] 26.673  0.032 26.705  0.000  0.000 [1] 77.186  0.037 
> 77.225  0.000  
> > 0.000
> > [1] 128.165   0.042 128.209   0.000   0.000
> > [1] 180.940   0.047 180.989   0.000   0.000
> >
> > See the big diff and the scaling I mentioned above?
> >
> > Further more I removed geno[[j]] assignment leaving the operation 
> > though, i.e., replaced it with ### line above. Times:
> >
> > [1] 0.857 0.008 0.865 0.000 0.000
> >
> > Huh!? What the heck! That's my second question :-) Any ideas?
> >
> > I still believe my method is near optimal. Of course I have 
> to somehow 
> > get rid of the assignment bottleneck.
> >
> > For now the lesson is: "God bless lists"
> >
> > Here is my final solution:
> >
> > > system.time({
> > +     geno.GASP.L<-lapply(geno.GASP
> > +                        ,function(x){
> > +                           for(l in 
> 1:length(x at levels)){levels(x)[l] 
> > + <-
> > switch(x at levels[l],AA="0",AB="1",BB="2")}
> > +                           factor(x,levels=0:2)
> > +                         }
> > +                  )
> > +     geno.JAG.L <-lapply(geno.JAG
> > +                        ,function(x){
> > + #                         for(l in 
> 1:length(x at levels)){levels(x)[l] <-
> > switch(x at levels[l],AA="0",AB="1",BB="2")}
> > +                           factor(x,levels=0:2)
> > +                         }
> > +                  )
> > + })
> > [1] 192.800   1.566 194.413   0.000   0.000   !!!!!!!!! :-)))))
> > > system.time({
> > +     class    (geno.GASP.L)<-"data.frame"
> > +     row.names(geno.GASP.L)<-row.names(geno.GASP)
> > +     class    (geno.JAG.L )<-"data.frame"
> > +     row.names(geno.JAG.L )<-row.names(geno.JAG )
> > + })
> > [1] 12.156  0.001 12.155  0.000  0.000
> > > system.time({
> > +     geno<-rbind(geno.GASP.L,geno.JAG.L)
> > + })
> > [1] 1542.340    9.072 2066.310    0.000    0.000
> >
> > I logged my notes here as I was trying various things. Partly the 
> > reason is my two questions:
> >
> > "What was wrong with me?" and
> > "What the heck?!" remember above? :-)))
> >
> > which  still remain unanswered :-(
> >
> > I would have had a lot of fun if I had not to have this done by ...
> > Yesterday :-))
> >
> > Thanks a lot for the help
> >
> > Latchezar
> >
> > > dim(genoT)
> > > class(genoT)
> > > system.time(out <- lapply(genoT, function(x) match(x, 
> c("AA", "AB",
> > > "BB"))-1))
> > > ##
> > > ##
> > >     user  system elapsed
> > > 119.288   0.004 119.339
> > >
> > > (for all 240K)
> > >
> > > best,
> > > b
> > >
> > > ps: note that "out" is a list.
> > >
> > > On Jul 20, 2007, at 2:01 AM, Latchezar Dimitrov wrote:
> > >
> > > > Hi,
> > > >
> > > >> -----Original Message-----
> > > >> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> > > >> Sent: Friday, July 20, 2007 12:25 AM
> > > >> To: Latchezar Dimitrov
> > > >> Cc: r-help at stat.math.ethz.ch
> > > >> Subject: Re: [R] Dataframe of factors transform speed?
> > > >>
> > > >> it looks like that whatever method you used to genotype the
> > > >> 1002 samples on the STY array gave you a transposed matrix of 
> > > >> genotype calls. :-)
> > > >
> > > > It only looks like :-)
> > > >
> > > > Otherwise it is correctly created dataframe of 1002 
> samples X (big
> > > > number) of columns (SNP genotypes). It worked perfectly until I 
> > > > decided to put together to cohorts independently processed in R 
> > > > already. I got stuck with my lack of foreseeing. 
> Otherwise I would 
> > > > have put 3 dummy lines w/ AA,AB, and AB on each one to make
> > > sure all 3
> > > > genotypes are present and that's it! Lesson for the future :-)
> > > >
> > > > Maybe I am not using columns and rows appropriately 
> here but the 
> > > > dataframe is correct (I have not used FORTRAN since 
> FORTRAN IV ;-)
> > > > - as
> > > > str says 1002 observ. of (big number) vars.
> > > >
> > > >>
> > > >> i'd use:
> > > >>
> > > >> genoT = read.table(yourFile, stringsAsFactors = FALSE)
> > > >>
> > > >> as a starting point... but I don't think that would be
> > > efficient (as
> > > >> you'd need to fix one column at a time - lapply).
> > > >
> > > > No it was not efficient at all. 'matter of fact nothing is more 
> > > > efficient then loading already read data, alas :-(
> > > >
> > > >>
> > > >> i'd preprocess yourFile before trying to load it:
> > > >>
> > > >> cat yourFile | sed -e 's/AA/1/g' | sed -e 's/AB/2/g' | sed -e 
> > > >> 's/BB/3/ g' > outFile
> > > >>
> > > >> and, now, in R:
> > > >>
> > > >> genoT = read.table(outFile, header=TRUE)
> > > >
> > > > ... Too late ;-) As it must be clear now I have two
> > > dataframes I want
> > > > to put together with rbind(geno1,geno2). The issue again is 
> > > > "uniformization" of factor variables w/ missing factors -
> > > they ended
> > > > up like levels AA,BB on one of the and levels AB,BB on the
> > > other which
> > > > means as.numeric of AA is 1 on the 1st and as.numeric 
> of AB is 1 
> > > > on the second - complete mess. That's why I tried to make both
> > > uniform,
> > > > i.e.
> > > > levels "AA","AB", and "BB" for every SNP and then rbind works.
> > > >
> > > > In any case my 1st questions remains: "What's wrong 
> with me?" :-)
> > > >
> > > > Thanks,
> > > > Latchezar
> > > >
> > > >>
> > > >> b
> > > >>
> > > >> On Jul 19, 2007, at 11:51 PM, Latchezar Dimitrov wrote:
> > > >>
> > > >>> Hello,
> > > >>>
> > > >>> This is a speed question. I have a dataframe genoT:
> > > >>>
> > > >>>> dim(genoT)
> > > >>> [1]   1002 238304
> > > >>>
> > > >>>> str(genoT)
> > > >>> 'data.frame':   1002 obs. of  238304 variables:
> > > >>>  $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
> > > >> 3 3 3 3 3
> > > >>> ...
> > > >>>  $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1
> > > >> 1 1 2 2 2
> > > >>> ...
> > > >>>  $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1
> > > >> 1 1 1 1 1
> > > >>> ...
> > > >>>  $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
> > > >> 3 3 3 3 3
> > > >>> ...
> > > >>>  $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2
> > > >> 3 2 3 3 1
> > > >>> ...
> > > >>>  $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA
> > > 1 NA 2 1 1
> > > >>> 2 1
> > > >>> ...
> > > >>>  $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
> > > >> 1 1 1 1 2
> > > >>> ...
> > > >>>  $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3
> > > >> 3 3 3 3 2
> > > >>> ...
> > > >>>  $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
> > > >> 1 1 1 1 2
> > > >>> ...
> > > >>>  $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2
> > > >> 1 2 1 1 3
> > > >>> ...
> > > >>>  $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB": 
> 2 2 3 3 3 2 
> > > >>> 1
> > > >>> 2 2 3
> > > >>> ...
> > > >>>  $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB": 
> 3 3 3 3 3 3 
> > > >>> 3
> > > >>> 3 3 3
> > > >>> ...
> > > >>>  $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB": 
> 2 2 2 1 1 1 
> > > >>> 2
> > > >>> 2 2 2
> > > >>> ...
> > > >>>  $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1 
> 1 1 1 1 1 
> > > >>> 1
> > > >>> 1 ...
> > > >>>  $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB": 
> 2 3 2 2 3 2 
> > > >>> 2
> > > >>> 1 1 2
> > > >>> ...
> > > >>>  $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB": 
> 1 1 1 1 1 1 
> > > >>> 1
> > > >>> 1 1 1
> > > >>> ...
> > > >>>  $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB": 
> 1 1 2 1 1 1 
> > > >>> 1
> > > >>> 1 1 1
> > > >>> ...
> > > >>>  $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1 
> 1 1 1 1 1 
> > > >>> 1
> > > >>> 1 ...
> > > >>>  $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB": 
> 1 1 1 1 1 2 
> > > >>> 1
> > > >>> 1 1 2
> > > >>> ...
> > > >>>  $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2
> > > >> 2 2 NA 1 NA
> > > >>> 2
> > > >>> 1 ...
> > > >>>  $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB": 
> 1 2 2 1 1 1 
> > > >>> 3
> > > >>> 1 1 1
> > > >>> ...
> > > >>>  $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2 
> 2 2 2 2 2 
> > > >>> 2
> > > >>> 2 ...
> > > >>>  $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1 
> 1 1 1 1 1 
> > > >>> 1
> > > >>> 1 ...
> > > >>>  $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB": 
> 1 1 1 1 1 1 
> > > >>> 1
> > > >>> 2 2 1
> > > >>> ...
> > > >>>
> > > >>> Its columns are factors with different number of levels
> > > >> (from 1 to 3 -
> > > >>> that's what I got from read.table, i.e., it dropped missing
> > > >> levels). I
> > > >>> want to convert it to uniform factors with 3 levels. The
> > > >> 1st 10 rows
> > > >>> above show already converted columns and the rest are not yet 
> > > >>> converted.
> > > >>> Here's my attempt wich is a complete failure as speed:
> > > >>>
> > > >>>> system.time(
> > > >>> +     for(j in 1:(10         )){ #-- this is to try 1st
> > > 10 cols and
> > > >>> measure the time, it otherwise is ncol(genoT) instead of 10
> > > >>>
> > > >>> +        gt<-genoT[[j]]          #-- this is to avoid 
> 2D indices
> > > >>> +        for(l in 1:length(gt at levels)){
> > > >>> +          levels(gt)[l] <-
> > > >> switch(gt at levels[l],AA="0",AB="1",BB="2")
> > > >>> #-- convert levels to "0","1", or "2"
> > > >>> +          genoT[[j]]<-factor(gt,levels=0:2)   #-- 
> make a 3-level
> > > >>> factor
> > > >>> and put it back
> > > >>> +        }
> > > >>> +     }
> > > >>> + )
> > > >>> [1] 785.085   4.358 789.454   0.000   0.000
> > > >>>
> > > >>> 789s for 10 columns only!
> > > >>>
> > > >>> To me it seems like replacing 10 x 3 levels and then making
> > > >> a factor
> > > >>> of
> > > >>> 1002 element vector x 10 is a "negligible" amount of 
> operations 
> > > >>> needed.
> > > >>>
> > > >>> So, what's wrong with me? Any idea how to accelerate
> > > >> significantly the
> > > >>> transformation or (to go to the very beginning) to make
> > > >> read.table use
> > > >>> a fixed set of levels ("AA","AB", and "BB") and not 
> to drop any
> > > >>> (missing)
> > > >>> level?
> > > >>>
> > > >>> R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
> > > >>>
> > > >>> The machine is with 32G RAM and AMD Opteron 285 (2.? GHz)
> > > >> so it's not
> > > >>> it.
> > > >>>
> > > >>> Thank you very much for the help,
> > > >>>
> > > >>> Latchezar Dimitrov,
> > > >>> Analyst/Programmer IV,
> > > >>> Wake Forest University School of Medicine, 
> Winston-Salem, North 
> > > >>> Carolina, USA
> > > >>>
> > > >>> ______________________________________________
> > > >>> R-help at stat.math.ethz.ch mailing list 
> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>> PLEASE do read the posting guide
> > > http://www.R-project.org/posting-
> > > >>> guide.html and provide commented, minimal, self-contained, 
> > > >>> reproducible code.
> > > >>
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
>


From jholtman at gmail.com  Sun Jul 22 04:26:12 2007
From: jholtman at gmail.com (jim holtman)
Date: Sat, 21 Jul 2007 22:26:12 -0400
Subject: [R] Dataframe of factors transform speed?
In-Reply-To: <F160BE32A2E5E04497668BBDFE83FEDF11CF86E8@EXCHVS1.medctr.ad.wfubmc.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E5@EXCHVS1.medctr.ad.wfubmc.edu>
	<DDAAAB60-ECB1-4B25-9A50-C859F3249DF8@jhsph.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E6@EXCHVS1.medctr.ad.wfubmc.edu>
	<62B73FC8-3B10-4037-A51E-2AFC6142F96E@jhsph.edu>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E7@EXCHVS1.medctr.ad.wfubmc.edu>
	<644e1f320707211433k6c95e7e2ydb87f240fcdaa1b2@mail.gmail.com>
	<F160BE32A2E5E04497668BBDFE83FEDF11CF86E8@EXCHVS1.medctr.ad.wfubmc.edu>
Message-ID: <644e1f320707211926q269e44cfy75bf7de1c3350340@mail.gmail.com>

The problem is in the way that 'as.data.frame' works.  Use Rprof on a
small list and you will see where it is spending its time.

Now if you are really sure that all your data is consistent with being
a data frame,
you can create your own dataframe structure your self.  Not that I
would advocate it, but if you look at the output of 'dput' on a
dataframe, you can construct your own.

Here it took 20 seconds to create the test data with a list of 50,000
and only 2 seconds to create the data frame from that.

> set.seed(123)
> n <- 50000
> system.time({
+ genoT <- lapply(1:n, function(i) factor(sample(c("AA",
+ "AB", "BB"), 1000, prob=c(1000, 1, 1), rep=T)))
+ })
   user  system elapsed
  20.85    0.12   22.83
> names(genoT) = paste("snp", 1:n, sep="")
>
> # create your own data frame structure -- if you are real sure of your data
>
> system.time(genoTz <- structure(genoT, .Names=names(genoT),
+     row.names=c(NA, -length(genoT[[1]])), class='data.frame'))
   user  system elapsed
   2.00    0.08    2.11
> str(genoTz)
'data.frame':   1000 obs. of  50000 variables:
 $ snp1    : Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp2    : Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp3    : Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp4    : Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp5    : Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp6    : Factor w/ 2 levels "AA","AB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp7    : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp8    : Factor w/ 2 levels "AA","BB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp9    : Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp10   : Factor w/ 3 levels "AA","AB","BB": 1 1 1 1 1 1 1 1 1 1 ...
 $ snp11   : Factor w/ 1 level "AA": 1 1 1 1 1 1 1 1 1 1 ...
>


On 7/21/07, Latchezar Dimitrov <ldimitro at wfubmc.edu> wrote:
> Jim,
>
> No, this is _not the problem. If you go to my 1st mail I have a monster
> (at least was when I purchased it) with 32GB (sic :-) of RAM and 4 dual
> core AMD64 285 (the fastest at that time and still pretty fast now :-)
>
> The machine stats paging when I run 2 copies of R working on two things
> like that :-). If you look at my last e-mail I found a solution but
> still have no clue why the heck x<-as.data.frame(y) where why is a list
> of the same columns take real for ever and this the thing that killed me
> before.
>
> Thanks,
> Latchezar
>
> > -----Original Message-----
> > From: jim holtman [mailto:jholtman at gmail.com]
> > Sent: Saturday, July 21, 2007 5:33 PM
> > To: Latchezar Dimitrov
> > Cc: Benilton Carvalho; r-help at stat.math.ethz.ch
> > Subject: Re: [R] Dataframe of factors transform speed?
> >
> > One of the problems is that you are probably paging on your
> > system with an object that size (240000 x 1000).  This is
> > about 1GB for a single object:
> >
> > > set.seed(123)
> > > n <- 240000
> > > system.time({
> > + genoT <- lapply(1:n, function(i) factor(sample(c("AA", "AB", "BB"),
> > + 1000, prob=c(1000, 1, 1), rep=T)))
> > + })
> >    user  system elapsed
> >   95.00    0.61  104.71
> > > names(genoT) = paste("snp", 1:n, sep="")
> > >
> > > object.size(genoT)
> > [1] 1045258752
> > >
> >
> > I can create it on my 2GB machine as a list, but have
> > problems converting it to a dataframe because I don't have
> > enough memory.
> >
> > So unless you have at least 4GB on your system, it might take
> > a long time.  Look at your performance measurements on your
> > system and see if you have run out of physical memory and are paging.
> >
> > On 7/21/07, Latchezar Dimitrov <ldimitro at wfubmc.edu> wrote:
> > > Hi,
> > >
> > > Thanks for the help. My 1st question still unanswered though :-)
> > > Please see bellow
> > >
> > > > -----Original Message-----
> > > > From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> > > > Sent: Friday, July 20, 2007 3:30 AM
> > > > To: Latchezar Dimitrov
> > > > Cc: r-help at stat.math.ethz.ch
> > > > Subject: Re: [R] Dataframe of factors transform speed?
> > > >
> > > > set.seed(123)
> > > > genoT = lapply(1:240000, function(i) factor(sample(c("AA", "AB",
> > > > "BB"), 1000, prob=sample(c(1, 1000, 1000), 3), rep=T)))
> > > > names(genoT) = paste("snp", 1:240000, sep="") genoT =
> > > > as.data.frame(genoT)
> > >
> > > Now this _is the problem. Everything before converting to
> > data.frame
> > > worked almost instantaneously however as.data.frame runs forever.
> > > Obviously there is some scalability memory management issue. When I
> > > tried my own method but creating a new result (instead of modifying
> > > the
> > > old) dataframe it worked like a charm for the 1st 100 cols ~ .3s. I
> > > figured 300,000 cols should be ~1000s. Nope! It ran for about
> > > 50,000(!)s to finish about 42,000 cols only.
> > >
> > > BTW, what ver. of R is yours?
> > >
> > > Now here's what I "discovered" further.
> > >
> > > #-- create a 1-col frame:
> > >    geno   <-
> > >
> > data.frame(c(geno.GASP[[1]],geno.JAG[[1]]),row.names=c(rownames(geno.G
> > > AS
> > > P),rownames(geno.JAG)))
> > >
> > > #-- main code I repeated it w/ j in 1:1000, 2001:3000, and
> > 3001:4000,
> > > i.e., adding a 1000 of cols to geno each time
> > >
> > > system.time(
> > > #   for(j in 1:(ncol(geno.GASP      ))){
> > >    for(j in 3001:(4000              )){
> > >      gt.GASP<-geno.GASP[[j]]
> > >       for(l in 1:length(gt.GASP at levels)){
> > >         levels(gt.GASP)[l] <-
> > > switch(gt.GASP at levels[l],AA="0",AB="1",BB="2")
> > >       }
> > >       gt.JAG <-geno.JAG [[j]]
> > > #      for(l in 1:length(gt.JAG @levels)){
> > > #        levels(gt.JAG )[l] <- switch(gt.JAG
> > > @levels[l],AA="0",AB="1",BB="2")
> > > #      }
> > >       geno[[j]]<-factor(c(as.numeric(factor(gt.GASP,levels=0:2))-1
> > > ###               factor(c(as.numeric(factor(gt.GASP,levels=0:2))-1
> > >                          ,as.numeric(factor(gt.JAG, levels=0:2))-1
> > >                          )
> > >                        ,levels=0:2
> > >                        )
> > >    }
> > > )
> > >
> > > Times (each one is for a 1000 cols!):
> > > [1] 26.673  0.032 26.705  0.000  0.000 [1] 77.186  0.037
> > 77.225  0.000
> > > 0.000
> > > [1] 128.165   0.042 128.209   0.000   0.000
> > > [1] 180.940   0.047 180.989   0.000   0.000
> > >
> > > See the big diff and the scaling I mentioned above?
> > >
> > > Further more I removed geno[[j]] assignment leaving the operation
> > > though, i.e., replaced it with ### line above. Times:
> > >
> > > [1] 0.857 0.008 0.865 0.000 0.000
> > >
> > > Huh!? What the heck! That's my second question :-) Any ideas?
> > >
> > > I still believe my method is near optimal. Of course I have
> > to somehow
> > > get rid of the assignment bottleneck.
> > >
> > > For now the lesson is: "God bless lists"
> > >
> > > Here is my final solution:
> > >
> > > > system.time({
> > > +     geno.GASP.L<-lapply(geno.GASP
> > > +                        ,function(x){
> > > +                           for(l in
> > 1:length(x at levels)){levels(x)[l]
> > > + <-
> > > switch(x at levels[l],AA="0",AB="1",BB="2")}
> > > +                           factor(x,levels=0:2)
> > > +                         }
> > > +                  )
> > > +     geno.JAG.L <-lapply(geno.JAG
> > > +                        ,function(x){
> > > + #                         for(l in
> > 1:length(x at levels)){levels(x)[l] <-
> > > switch(x at levels[l],AA="0",AB="1",BB="2")}
> > > +                           factor(x,levels=0:2)
> > > +                         }
> > > +                  )
> > > + })
> > > [1] 192.800   1.566 194.413   0.000   0.000   !!!!!!!!! :-)))))
> > > > system.time({
> > > +     class    (geno.GASP.L)<-"data.frame"
> > > +     row.names(geno.GASP.L)<-row.names(geno.GASP)
> > > +     class    (geno.JAG.L )<-"data.frame"
> > > +     row.names(geno.JAG.L )<-row.names(geno.JAG )
> > > + })
> > > [1] 12.156  0.001 12.155  0.000  0.000
> > > > system.time({
> > > +     geno<-rbind(geno.GASP.L,geno.JAG.L)
> > > + })
> > > [1] 1542.340    9.072 2066.310    0.000    0.000
> > >
> > > I logged my notes here as I was trying various things. Partly the
> > > reason is my two questions:
> > >
> > > "What was wrong with me?" and
> > > "What the heck?!" remember above? :-)))
> > >
> > > which  still remain unanswered :-(
> > >
> > > I would have had a lot of fun if I had not to have this done by ...
> > > Yesterday :-))
> > >
> > > Thanks a lot for the help
> > >
> > > Latchezar
> > >
> > > > dim(genoT)
> > > > class(genoT)
> > > > system.time(out <- lapply(genoT, function(x) match(x,
> > c("AA", "AB",
> > > > "BB"))-1))
> > > > ##
> > > > ##
> > > >     user  system elapsed
> > > > 119.288   0.004 119.339
> > > >
> > > > (for all 240K)
> > > >
> > > > best,
> > > > b
> > > >
> > > > ps: note that "out" is a list.
> > > >
> > > > On Jul 20, 2007, at 2:01 AM, Latchezar Dimitrov wrote:
> > > >
> > > > > Hi,
> > > > >
> > > > >> -----Original Message-----
> > > > >> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> > > > >> Sent: Friday, July 20, 2007 12:25 AM
> > > > >> To: Latchezar Dimitrov
> > > > >> Cc: r-help at stat.math.ethz.ch
> > > > >> Subject: Re: [R] Dataframe of factors transform speed?
> > > > >>
> > > > >> it looks like that whatever method you used to genotype the
> > > > >> 1002 samples on the STY array gave you a transposed matrix of
> > > > >> genotype calls. :-)
> > > > >
> > > > > It only looks like :-)
> > > > >
> > > > > Otherwise it is correctly created dataframe of 1002
> > samples X (big
> > > > > number) of columns (SNP genotypes). It worked perfectly until I
> > > > > decided to put together to cohorts independently processed in R
> > > > > already. I got stuck with my lack of foreseeing.
> > Otherwise I would
> > > > > have put 3 dummy lines w/ AA,AB, and AB on each one to make
> > > > sure all 3
> > > > > genotypes are present and that's it! Lesson for the future :-)
> > > > >
> > > > > Maybe I am not using columns and rows appropriately
> > here but the
> > > > > dataframe is correct (I have not used FORTRAN since
> > FORTRAN IV ;-)
> > > > > - as
> > > > > str says 1002 observ. of (big number) vars.
> > > > >
> > > > >>
> > > > >> i'd use:
> > > > >>
> > > > >> genoT = read.table(yourFile, stringsAsFactors = FALSE)
> > > > >>
> > > > >> as a starting point... but I don't think that would be
> > > > efficient (as
> > > > >> you'd need to fix one column at a time - lapply).
> > > > >
> > > > > No it was not efficient at all. 'matter of fact nothing is more
> > > > > efficient then loading already read data, alas :-(
> > > > >
> > > > >>
> > > > >> i'd preprocess yourFile before trying to load it:
> > > > >>
> > > > >> cat yourFile | sed -e 's/AA/1/g' | sed -e 's/AB/2/g' | sed -e
> > > > >> 's/BB/3/ g' > outFile
> > > > >>
> > > > >> and, now, in R:
> > > > >>
> > > > >> genoT = read.table(outFile, header=TRUE)
> > > > >
> > > > > ... Too late ;-) As it must be clear now I have two
> > > > dataframes I want
> > > > > to put together with rbind(geno1,geno2). The issue again is
> > > > > "uniformization" of factor variables w/ missing factors -
> > > > they ended
> > > > > up like levels AA,BB on one of the and levels AB,BB on the
> > > > other which
> > > > > means as.numeric of AA is 1 on the 1st and as.numeric
> > of AB is 1
> > > > > on the second - complete mess. That's why I tried to make both
> > > > uniform,
> > > > > i.e.
> > > > > levels "AA","AB", and "BB" for every SNP and then rbind works.
> > > > >
> > > > > In any case my 1st questions remains: "What's wrong
> > with me?" :-)
> > > > >
> > > > > Thanks,
> > > > > Latchezar
> > > > >
> > > > >>
> > > > >> b
> > > > >>
> > > > >> On Jul 19, 2007, at 11:51 PM, Latchezar Dimitrov wrote:
> > > > >>
> > > > >>> Hello,
> > > > >>>
> > > > >>> This is a speed question. I have a dataframe genoT:
> > > > >>>
> > > > >>>> dim(genoT)
> > > > >>> [1]   1002 238304
> > > > >>>
> > > > >>>> str(genoT)
> > > > >>> 'data.frame':   1002 obs. of  238304 variables:
> > > > >>>  $ SNP_A.4261647: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
> > > > >> 3 3 3 3 3
> > > > >>> ...
> > > > >>>  $ SNP_A.4261610: Factor w/ 3 levels "0","1","2": 1 1 3 3 1
> > > > >> 1 1 2 2 2
> > > > >>> ...
> > > > >>>  $ SNP_A.4261601: Factor w/ 3 levels "0","1","2": 1 1 1 1 1
> > > > >> 1 1 1 1 1
> > > > >>> ...
> > > > >>>  $ SNP_A.4261704: Factor w/ 3 levels "0","1","2": 3 3 3 3 3
> > > > >> 3 3 3 3 3
> > > > >>> ...
> > > > >>>  $ SNP_A.4261563: Factor w/ 3 levels "0","1","2": 3 1 2 1 2
> > > > >> 3 2 3 3 1
> > > > >>> ...
> > > > >>>  $ SNP_A.4261554: Factor w/ 3 levels "0","1","2": 1 1 NA
> > > > 1 NA 2 1 1
> > > > >>> 2 1
> > > > >>> ...
> > > > >>>  $ SNP_A.4261666: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
> > > > >> 1 1 1 1 2
> > > > >>> ...
> > > > >>>  $ SNP_A.4261634: Factor w/ 3 levels "0","1","2": 3 3 2 3 3
> > > > >> 3 3 3 3 2
> > > > >>> ...
> > > > >>>  $ SNP_A.4261656: Factor w/ 3 levels "0","1","2": 1 1 2 1 1
> > > > >> 1 1 1 1 2
> > > > >>> ...
> > > > >>>  $ SNP_A.4261637: Factor w/ 3 levels "0","1","2": 1 3 2 3 2
> > > > >> 1 2 1 1 3
> > > > >>> ...
> > > > >>>  $ SNP_A.4261597: Factor w/ 3 levels "AA","AB","BB":
> > 2 2 3 3 3 2
> > > > >>> 1
> > > > >>> 2 2 3
> > > > >>> ...
> > > > >>>  $ SNP_A.4261659: Factor w/ 3 levels "AA","AB","BB":
> > 3 3 3 3 3 3
> > > > >>> 3
> > > > >>> 3 3 3
> > > > >>> ...
> > > > >>>  $ SNP_A.4261594: Factor w/ 3 levels "AA","AB","BB":
> > 2 2 2 1 1 1
> > > > >>> 2
> > > > >>> 2 2 2
> > > > >>> ...
> > > > >>>  $ SNP_A.4261698: Factor w/ 2 levels "AA","AB": 1 1 1
> > 1 1 1 1 1
> > > > >>> 1
> > > > >>> 1 ...
> > > > >>>  $ SNP_A.4261538: Factor w/ 3 levels "AA","AB","BB":
> > 2 3 2 2 3 2
> > > > >>> 2
> > > > >>> 1 1 2
> > > > >>> ...
> > > > >>>  $ SNP_A.4261621: Factor w/ 3 levels "AA","AB","BB":
> > 1 1 1 1 1 1
> > > > >>> 1
> > > > >>> 1 1 1
> > > > >>> ...
> > > > >>>  $ SNP_A.4261553: Factor w/ 3 levels "AA","AB","BB":
> > 1 1 2 1 1 1
> > > > >>> 1
> > > > >>> 1 1 1
> > > > >>> ...
> > > > >>>  $ SNP_A.4261528: Factor w/ 2 levels "AA","AB": 1 1 1
> > 1 1 1 1 1
> > > > >>> 1
> > > > >>> 1 ...
> > > > >>>  $ SNP_A.4261579: Factor w/ 3 levels "AA","AB","BB":
> > 1 1 1 1 1 2
> > > > >>> 1
> > > > >>> 1 1 2
> > > > >>> ...
> > > > >>>  $ SNP_A.4261513: Factor w/ 3 levels "AA","AB","BB": 2 1 2
> > > > >> 2 2 NA 1 NA
> > > > >>> 2
> > > > >>> 1 ...
> > > > >>>  $ SNP_A.4261532: Factor w/ 3 levels "AA","AB","BB":
> > 1 2 2 1 1 1
> > > > >>> 3
> > > > >>> 1 1 1
> > > > >>> ...
> > > > >>>  $ SNP_A.4261600: Factor w/ 2 levels "AB","BB": 2 2 2
> > 2 2 2 2 2
> > > > >>> 2
> > > > >>> 2 ...
> > > > >>>  $ SNP_A.4261706: Factor w/ 2 levels "AA","BB": 1 1 1
> > 1 1 1 1 1
> > > > >>> 1
> > > > >>> 1 ...
> > > > >>>  $ SNP_A.4261575: Factor w/ 3 levels "AA","AB","BB":
> > 1 1 1 1 1 1
> > > > >>> 1
> > > > >>> 2 2 1
> > > > >>> ...
> > > > >>>
> > > > >>> Its columns are factors with different number of levels
> > > > >> (from 1 to 3 -
> > > > >>> that's what I got from read.table, i.e., it dropped missing
> > > > >> levels). I
> > > > >>> want to convert it to uniform factors with 3 levels. The
> > > > >> 1st 10 rows
> > > > >>> above show already converted columns and the rest are not yet
> > > > >>> converted.
> > > > >>> Here's my attempt wich is a complete failure as speed:
> > > > >>>
> > > > >>>> system.time(
> > > > >>> +     for(j in 1:(10         )){ #-- this is to try 1st
> > > > 10 cols and
> > > > >>> measure the time, it otherwise is ncol(genoT) instead of 10
> > > > >>>
> > > > >>> +        gt<-genoT[[j]]          #-- this is to avoid
> > 2D indices
> > > > >>> +        for(l in 1:length(gt at levels)){
> > > > >>> +          levels(gt)[l] <-
> > > > >> switch(gt at levels[l],AA="0",AB="1",BB="2")
> > > > >>> #-- convert levels to "0","1", or "2"
> > > > >>> +          genoT[[j]]<-factor(gt,levels=0:2)   #--
> > make a 3-level
> > > > >>> factor
> > > > >>> and put it back
> > > > >>> +        }
> > > > >>> +     }
> > > > >>> + )
> > > > >>> [1] 785.085   4.358 789.454   0.000   0.000
> > > > >>>
> > > > >>> 789s for 10 columns only!
> > > > >>>
> > > > >>> To me it seems like replacing 10 x 3 levels and then making
> > > > >> a factor
> > > > >>> of
> > > > >>> 1002 element vector x 10 is a "negligible" amount of
> > operations
> > > > >>> needed.
> > > > >>>
> > > > >>> So, what's wrong with me? Any idea how to accelerate
> > > > >> significantly the
> > > > >>> transformation or (to go to the very beginning) to make
> > > > >> read.table use
> > > > >>> a fixed set of levels ("AA","AB", and "BB") and not
> > to drop any
> > > > >>> (missing)
> > > > >>> level?
> > > > >>>
> > > > >>> R-devel_2006-08-26, Sun Solaris 10 OS - x86 64-bit
> > > > >>>
> > > > >>> The machine is with 32G RAM and AMD Opteron 285 (2.? GHz)
> > > > >> so it's not
> > > > >>> it.
> > > > >>>
> > > > >>> Thank you very much for the help,
> > > > >>>
> > > > >>> Latchezar Dimitrov,
> > > > >>> Analyst/Programmer IV,
> > > > >>> Wake Forest University School of Medicine,
> > Winston-Salem, North
> > > > >>> Carolina, USA
> > > > >>>
> > > > >>> ______________________________________________
> > > > >>> R-help at stat.math.ethz.ch mailing list
> > > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >>> PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-
> > > > >>> guide.html and provide commented, minimal, self-contained,
> > > > >>> reproducible code.
> > > > >>
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
> >
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From refuse6508 at indiatimes.com  Sun Jul 22 05:39:12 2007
From: refuse6508 at indiatimes.com (refuse6508)
Date: Sun, 22 Jul 2007 09:09:12 +0530
Subject: [R] Fwd: einem schnellen One- Night- Stand- Ficken (
	Entscheidung)
Message-ID: <62h6km$1f2srt@ipc603.indiatimes.com>

N A M E : Leona, A L T E R : 33, B E R U F : Hausfrau

Na, mein Geiler!

Ist dir auch schon so hei? wie mir? Ich habe einfach die Nase voll, dass mein Mann nie zu Hause ist und mir meine Bed?rfnisse nicht befriedigen kann. Jetzt bin ich gezwungen mir einen richtigen Lover zu suchen, der mich wieder mal so richtig durchbumst! Wenn du also mein Lover sein willst, egal ob f?r eine Nacht oder f?r Langzeitbeziehungen, ich bin f?r beides zu haben! 

Hol dir jetzt einfach einen Zugang f?r 5 Euro, du findest mich mit dem Usernamen Leona33-allone, ich habe dort meine Heimadresse und meine Handynummer hinterlassen, melde dich bei mir und besuche mich, oder ich komme zu dir.

http://skt.netfeature.cc/de/index.html

Schau Dich jetzt auf der gr?n, deutschsprachigen Kontaktdatenbank (Deutschland, Osterreich, Die Schweiz) um, denn hier findest Du die Frauen, die genau das gleiche suchen wie Du!

In unserer Datenbank findest Du mehr als 250,000 Profile fickgeiler Schlampen, die es mal wieder so richtig besorgt bekommen wollen.

Als Mitglied hast Du die M?chkeit in diesen Profilen zu st?n and Dir Das Girl, was Dir gefallt heraus zu suchen um mit ihr in Kontakt zu treten.

http://skt.netfeature.cc/de/index.html

..und entscheide Dich f?r mich! Mein Username ist Leona33-allone
bis dann...

 

Deine scharfe LEONA -:) 

 

 



Wenn Du diesen Newsletter nicht mehr erhalten m?est, klicke einfach auf diesen Link:
http://voe.netfeature.cc/revoke.php










( Polizei dass Personen dann wenn vor allem Aufgabe Chef n?chsten Bank Jetzt darauf wohl heutigen
 Geld nichts weit Mio Zahlen Als sonst bei
 bevor vielleicht w?ren Politik Platz trotz Hamburg darin
 jeder Folgen dann Seit k?nnen
 machen Sonntag wurden
 sonst sah statt w?ren dabei jungen erkl?rt weil   pr?sident
 vor allem
 Verein Wochenende acht fast Stunde Sieg gemeinsam
 gerade bezeichnet neuer Freitag Seit Macht Titel machte ihr)


From brown_emu at yahoo.com  Sun Jul 22 11:05:26 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sun, 22 Jul 2007 02:05:26 -0700 (PDT)
Subject: [R] ?R:  Removing white space betwen multiple plots,
	traditional graphics
In-Reply-To: <11716176.post@talk.nabble.com>
Message-ID: <810776.2145.qm@web39706.mail.mud.yahoo.com>

You could try
par(mar=c(0,5,0,2), mfrow = c(6,1), oma=c(5,0,2,0))
##...then, your plots...##


--- Mr Natural <drstrong at ucdavis.edu> wrote:

> 
> I would appreciate suggestions for removing the white spaces the graphs in
> a
> stack:
> 
> par(mar=c(2,2,1,1), mfrow = c(6,1))
> mydates<-dates(1:20,origin=c(month = 1, day = 1, year = 1986))
> plot(rnorm(20,0.1,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.2,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.3,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.5,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.7,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.8,0.1)~mydates, type="b",xlab="",ylim=c(0,1) )
> 
> Thanx, Don
> -- 
> View this message in context:
>
http://www.nabble.com/-R%3A--Removing-white-space-betwen-multiple-plots%2C-traditional-graphics-tf4119626.html#a11716176
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Sun Jul 22 11:40:26 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sun, 22 Jul 2007 02:40:26 -0700 (PDT)
Subject: [R] tagging results of "apply"
In-Reply-To: <CADFD0E28E1E6A46B0C84335BDB994F504989B64@whexchmb16.bsna.bsroot.bear.com>
Message-ID: <471774.47582.qm@web39715.mail.mud.yahoo.com>

Dear Bruce,
In your functions, you need to use your bound variable, 'x' [not mat1] in
your anonymous function [function(x)] as the argument to cor().

For instance, you wrote:
apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
apply(mat1, 1, function(x) cor(mat1, mat2))

They should be
apply(mat1, 1, function(x) cor(x, mat2[1,]))
apply(mat1, 1, function(x) cor(x, mat2))

or
f <- function(x,y) cor(x, y)
apply(mat1, 1, f, y=mat2[1,])
apply(mat1, 1, f, y=mat2)

Then from the ?apply documentation - under section, 'Value' - the following
statement will help you predict its behavior in this case:
"If each call to FUN returns a vector of length n, then apply returns an
array of dimension c(n, dim(X)[MARGIN]) if n > 1."

[each column of your output is the output from cor(mat1[i,],mat2) in Scenario
2]. As for tagging, you can try adding dimension labels [to the object which
is passed as the 'X' argument to apply()]:

mat1 <- matrix(sample(1:500, 25), ncol = 5,
               dimnames=list(paste("row",1:5,sep=""),
                 paste("col",1:5,sep="")))
mat2 <- matrix(sample(501:1000, 25), ncol = 5)

> apply(mat1, 1, function(x,y) cor(x, y), y=mat2)
            row1       row2       row3        row4        row5
[1,]  0.39412464 -0.6241649  0.7423724  0.48391875  0.27085386
[2,] -0.22912466 -0.4123714  0.2857004 -0.52447327  0.06971423
[3,] -0.51027247  0.3256587 -0.6195050 -0.48309737  0.01699978
[4,]  0.26353316 -0.1873564  0.2121154  0.88784766 -0.02257890
[5,] -0.03771225 -0.4250040  0.3795558 -0.03372794 -0.05874675

Hope this helps,

Stephen

--- "Bernzweig, Bruce (Consultant)" <bbernzwe at bear.com> wrote:

> In trying to get a better understanding of vectorization I wrote the
> following code:
> 
> My objective is to take two sets of time series and calculate the
> correlations for each combination of time series.
> 
> mat1 <- matrix(sample(1:500, 25), ncol = 5)
> mat2 <- matrix(sample(501:1000, 25), ncol = 5)
> 
> Scenario 1:
> apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
> 
> Scenario 2:
> apply(mat1, 1, function(x) cor(mat1, mat2))
> 
> Using scenario 1, (output below) I can see that correlations are
> calculated for just the first row of mat2 against each individual row of
> mat1.
> 
> Using scenario 2, (output below) I can see that correlations are
> calculated for each row of mat2 against each individual row of mat1.  
> 
> Q1: The output of scenario2 consists of 25 rows of data.  Are the first
> five rows mat1 against mat2[1,], the next five rows mat1 against
> mat2[2,], ... last five rows mat1 against mat2[5,]?
> 
> Q2: I assign the output of scenario 2 to a new matrix
> 
> 	matC <- apply(mat1, 1, function(x) cor(mat1, mat2))
> 
>     However, I need a way to identify each row in matC as a pairing of
> rows from mat1 and mat2.  Is there a parameter I can add to apply to do
> this?
> 
> Scenario 1:
> > apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
>            [,1]       [,2]       [,3]       [,4]       [,5]
> [1,] -0.4626122 -0.4626122 -0.4626122 -0.4626122 -0.4626122
> [2,] -0.9031543 -0.9031543 -0.9031543 -0.9031543 -0.9031543
> [3,]  0.0735273  0.0735273  0.0735273  0.0735273  0.0735273
> [4,]  0.7401259  0.7401259  0.7401259  0.7401259  0.7401259
> [5,] -0.4548582 -0.4548582 -0.4548582 -0.4548582 -0.4548582
> 
> Scenario 2:
> > apply(mat1, 1, function(x) cor(mat1, mat2))
>              [,1]        [,2]        [,3]        [,4]        [,5]
>  [1,]  0.19394126  0.19394126  0.19394126  0.19394126  0.19394126
>  [2,]  0.26402400  0.26402400  0.26402400  0.26402400  0.26402400
>  [3,]  0.12923842  0.12923842  0.12923842  0.12923842  0.12923842
>  [4,] -0.74549676 -0.74549676 -0.74549676 -0.74549676 -0.74549676
>  [5,]  0.64074122  0.64074122  0.64074122  0.64074122  0.64074122
>  [6,]  0.26931986  0.26931986  0.26931986  0.26931986  0.26931986
>  [7,]  0.08527921  0.08527921  0.08527921  0.08527921  0.08527921
>  [8,] -0.28034079 -0.28034079 -0.28034079 -0.28034079 -0.28034079
>  [9,] -0.15251915 -0.15251915 -0.15251915 -0.15251915 -0.15251915
> [10,]  0.19542415  0.19542415  0.19542415  0.19542415  0.19542415
> [11,]  0.75107032  0.75107032  0.75107032  0.75107032  0.75107032
> [12,]  0.53042767  0.53042767  0.53042767  0.53042767  0.53042767
> [13,] -0.51163612 -0.51163612 -0.51163612 -0.51163612 -0.51163612
> [14,] -0.44396048 -0.44396048 -0.44396048 -0.44396048 -0.44396048
> [15,]  0.57018745  0.57018745  0.57018745  0.57018745  0.57018745
> [16,]  0.70480284  0.70480284  0.70480284  0.70480284  0.70480284
> [17,] -0.36674283 -0.36674283 -0.36674283 -0.36674283 -0.36674283
> [18,] -0.81826607 -0.81826607 -0.81826607 -0.81826607 -0.81826607
> [19,]  0.53145184  0.53145184  0.53145184  0.53145184  0.53145184
> [20,]  0.24568385  0.24568385  0.24568385  0.24568385  0.24568385
> [21,] -0.10610402 -0.10610402 -0.10610402 -0.10610402 -0.10610402
> [22,] -0.78650748 -0.78650748 -0.78650748 -0.78650748 -0.78650748
> [23,]  0.04269423  0.04269423  0.04269423  0.04269423  0.04269423
> [24,]  0.14704698  0.14704698  0.14704698  0.14704698  0.14704698
> [25,]  0.28340166  0.28340166  0.28340166  0.28340166  0.28340166
> 
> 
> 
> **********************************************************************
> Please be aware that, notwithstanding the fact that the pers...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Sun Jul 22 12:08:21 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sun, 22 Jul 2007 03:08:21 -0700 (PDT)
Subject: [R] tagging results of "apply"
In-Reply-To: <471774.47582.qm@web39715.mail.mud.yahoo.com>
Message-ID: <847654.24028.qm@web39706.mail.mud.yahoo.com>

Actually if you want to tag both column and row, this might also help:

## Give dimension labels to both matrices
mat1 <- matrix(sample(1:500, 25), ncol = 5,
               dimnames=list(paste("mat1row",1:5,sep=""),
                 paste("mat1col",1:5,sep="")))
mat2 <- matrix(sample(501:1000, 25), ncol = 5,
               dimnames=list(paste("mat2row",1:5,sep=""),
                 paste("mat2col",1:5,sep="")))

cor(mat1[1,],mat2)
        mat2col1   mat2col2   mat2col3  mat2col4     mat2col5
[1,] -0.06313535 -0.4679927 -0.5147084 -0.797748 -0.001457972

The column labels are there but are lost when returned from apply(), as it
says in ?apply:

"In all cases the result is coerced by as.vector to one of the basic vector
types before the dimensions are set"

> as.vector(cor(mat1[1,],mat2))
[1] -0.063135353 -0.467992672 -0.514708392 -0.797748010 -0.001457972

You lose the dimension labels in this case, so one option is to guard against
this in the following way:

> as.vector(as.data.frame(cor(mat1[1,],mat2)))
     mat2col1   mat2col2   mat2col3  mat2col4     mat2col5
1 -0.06313535 -0.4679927 -0.5147084 -0.797748 -0.001457972

Unfortunately, if you use 'as.data.frame()' in 'function(x)', apply will
return a list - but you can bind the rows of the output:

> f <- function(x,y) as.data.frame(cor(x,y))
> do.call(rbind, apply(mat1,1,f,y=mat2))
            mat2col1   mat2col2    mat2col3   mat2col4     mat2col5
mat1row1 -0.06313535 -0.4679927 -0.51470839 -0.7977480 -0.001457972
mat1row2 -0.28750363  0.1681777  0.14671484  0.8139768  0.039982028
mat1row3 -0.62017387 -0.6932731 -0.72263865 -0.7929604  0.427366680
mat1row4  0.06441894  0.1707946 -0.11444747 -0.8213577  0.526239013
mat1row5 -0.09849051  0.7024540 -0.01997228  0.3712480  0.439037838

The result is a data frame, not a matrix, and note that the columns/rows are
transposed in relation to the output of
  apply(mat1,1,f,y=mat2)

An alternative is to convert each row of mat1 into a list element [by
transposing it with t() and then feeding it to as.data.frame()] and then use
sapply():

> sapply(as.data.frame(t(mat1)),f,y=mat2)
         mat1row1     mat1row2   mat1row3   mat1row4   mat1row5   
mat2col1 -0.06313535  -0.2875036 -0.6201739 0.06441894 -0.0984905 
mat2col2 -0.4679927   0.1681777  -0.6932731 0.1707946  0.702454   
mat2col3 -0.5147084   0.1467148  -0.7226387 -0.1144475 -0.01997228
mat2col4 -0.797748    0.8139768  -0.7929604 -0.8213577 0.371248   
mat2col5 -0.001457972 0.03998203 0.4273667  0.526239   0.4390378



--- Stephen Tucker <brown_emu at yahoo.com> wrote:

> Dear Bruce,
> In your functions, you need to use your bound variable, 'x' [not mat1] in
> your anonymous function [function(x)] as the argument to cor().
> 
> For instance, you wrote:
> apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
> apply(mat1, 1, function(x) cor(mat1, mat2))
> 
> They should be
> apply(mat1, 1, function(x) cor(x, mat2[1,]))
> apply(mat1, 1, function(x) cor(x, mat2))
> 
> or
> f <- function(x,y) cor(x, y)
> apply(mat1, 1, f, y=mat2[1,])
> apply(mat1, 1, f, y=mat2)
> 
> Then from the ?apply documentation - under section, 'Value' - the following
> statement will help you predict its behavior in this case:
> "If each call to FUN returns a vector of length n, then apply returns an
> array of dimension c(n, dim(X)[MARGIN]) if n > 1."
> 
> [each column of your output is the output from cor(mat1[i,],mat2) in
> Scenario
> 2]. As for tagging, you can try adding dimension labels [to the object
> which
> is passed as the 'X' argument to apply()]:
> 
> mat1 <- matrix(sample(1:500, 25), ncol = 5,
>                dimnames=list(paste("row",1:5,sep=""),
>                  paste("col",1:5,sep="")))
> mat2 <- matrix(sample(501:1000, 25), ncol = 5)
> 
> > apply(mat1, 1, function(x,y) cor(x, y), y=mat2)
>             row1       row2       row3        row4        row5
> [1,]  0.39412464 -0.6241649  0.7423724  0.48391875  0.27085386
> [2,] -0.22912466 -0.4123714  0.2857004 -0.52447327  0.06971423
> [3,] -0.51027247  0.3256587 -0.6195050 -0.48309737  0.01699978
> [4,]  0.26353316 -0.1873564  0.2121154  0.88784766 -0.02257890
> [5,] -0.03771225 -0.4250040  0.3795558 -0.03372794 -0.05874675
> 
> Hope this helps,
> 
> Stephen
> 
> --- "Bernzweig, Bruce (Consultant)" <bbernzwe at bear.com> wrote:
> 
> > In trying to get a better understanding of vectorization I wrote the
> > following code:
> > 
> > My objective is to take two sets of time series and calculate the
> > correlations for each combination of time series.
> > 
> > mat1 <- matrix(sample(1:500, 25), ncol = 5)
> > mat2 <- matrix(sample(501:1000, 25), ncol = 5)
> > 
> > Scenario 1:
> > apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
> > 
> > Scenario 2:
> > apply(mat1, 1, function(x) cor(mat1, mat2))
> > 
> > Using scenario 1, (output below) I can see that correlations are
> > calculated for just the first row of mat2 against each individual row of
> > mat1.
> > 
> > Using scenario 2, (output below) I can see that correlations are
> > calculated for each row of mat2 against each individual row of mat1.  
> > 
> > Q1: The output of scenario2 consists of 25 rows of data.  Are the first
> > five rows mat1 against mat2[1,], the next five rows mat1 against
> > mat2[2,], ... last five rows mat1 against mat2[5,]?
> > 
> > Q2: I assign the output of scenario 2 to a new matrix
> > 
> > 	matC <- apply(mat1, 1, function(x) cor(mat1, mat2))
> > 
> >     However, I need a way to identify each row in matC as a pairing of
> > rows from mat1 and mat2.  Is there a parameter I can add to apply to do
> > this?
> > 
> > Scenario 1:
> > > apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
> >            [,1]       [,2]       [,3]       [,4]       [,5]
> > [1,] -0.4626122 -0.4626122 -0.4626122 -0.4626122 -0.4626122
> > [2,] -0.9031543 -0.9031543 -0.9031543 -0.9031543 -0.9031543
> > [3,]  0.0735273  0.0735273  0.0735273  0.0735273  0.0735273
> > [4,]  0.7401259  0.7401259  0.7401259  0.7401259  0.7401259
> > [5,] -0.4548582 -0.4548582 -0.4548582 -0.4548582 -0.4548582
> > 
> > Scenario 2:
> > > apply(mat1, 1, function(x) cor(mat1, mat2))
> >              [,1]        [,2]        [,3]        [,4]        [,5]
> >  [1,]  0.19394126  0.19394126  0.19394126  0.19394126  0.19394126
> >  [2,]  0.26402400  0.26402400  0.26402400  0.26402400  0.26402400
> >  [3,]  0.12923842  0.12923842  0.12923842  0.12923842  0.12923842
> >  [4,] -0.74549676 -0.74549676 -0.74549676 -0.74549676 -0.74549676
> >  [5,]  0.64074122  0.64074122  0.64074122  0.64074122  0.64074122
> >  [6,]  0.26931986  0.26931986  0.26931986  0.26931986  0.26931986
> >  [7,]  0.08527921  0.08527921  0.08527921  0.08527921  0.08527921
> >  [8,] -0.28034079 -0.28034079 -0.28034079 -0.28034079 -0.28034079
> >  [9,] -0.15251915 -0.15251915 -0.15251915 -0.15251915 -0.15251915
> > [10,]  0.19542415  0.19542415  0.19542415  0.19542415  0.19542415
> > [11,]  0.75107032  0.75107032  0.75107032  0.75107032  0.75107032
> > [12,]  0.53042767  0.53042767  0.53042767  0.53042767  0.53042767
> > [13,] -0.51163612 -0.51163612 -0.51163612 -0.51163612 -0.51163612
> > [14,] -0.44396048 -0.44396048 -0.44396048 -0.44396048 -0.44396048
> > [15,]  0.57018745  0.57018745  0.57018745  0.57018745  0.57018745
> > [16,]  0.70480284  0.70480284  0.70480284  0.70480284  0.70480284
> > [17,] -0.36674283 -0.36674283 -0.36674283 -0.36674283 -0.36674283
> > [18,] -0.81826607 -0.81826607 -0.81826607 -0.81826607 -0.81826607
> > [19,]  0.53145184  0.53145184  0.53145184  0.53145184  0.53145184
> > [20,]  0.24568385  0.24568385  0.24568385  0.24568385  0.24568385
> > [21,] -0.10610402 -0.10610402 -0.10610402 -0.10610402 -0.10610402
> > [22,] -0.78650748 -0.78650748 -0.78650748 -0.78650748 -0.78650748
> > [23,]  0.04269423  0.04269423  0.04269423  0.04269423  0.04269423
> > [24,]  0.14704698  0.14704698  0.14704698  0.14704698  0.14704698
> > [25,]  0.28340166  0.28340166  0.28340166  0.28340166  0.28340166
> > 
> > 
> > 
> > **********************************************************************
> > Please be aware that, notwithstanding the fact that the
> pers...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From amnakhan493 at gmail.com  Sun Jul 22 12:25:56 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 22 Jul 2007 03:25:56 -0700
Subject: [R] Data Set
Message-ID: <3ffd3bb60707220325u2a702b3dp22702aa6777501d1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070722/919481a3/attachment.pl 

From amnakhan493 at gmail.com  Sun Jul 22 12:25:56 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 22 Jul 2007 03:25:56 -0700
Subject: [R] Data Set
Message-ID: <3ffd3bb60707220325u2a702b3dp22702aa6777501d1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070722/919481a3/attachment-0001.pl 

From gavin.simpson at ucl.ac.uk  Sun Jul 22 12:49:11 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sun, 22 Jul 2007 11:49:11 +0100
Subject: [R] Data Set
In-Reply-To: <3ffd3bb60707220325u2a702b3dp22702aa6777501d1@mail.gmail.com>
References: <3ffd3bb60707220325u2a702b3dp22702aa6777501d1@mail.gmail.com>
Message-ID: <1185101351.2866.24.camel@graptoleberis.geog.ucl.ac.uk>

On Sun, 2007-07-22 at 03:25 -0700, amna khan wrote:
> Hi Sir
> I have made a data set having 23 stations of rainfall.
> when I use the attach function to approach indevidual stations then
> following error occurr.
> 
> *>attach(data)*
> *>S.Sharif    #S.Sharif is the station  name which has 50 data values*
> *Error: object "S.Sharif" not found*
> Now how to solve this problem.

Then you don't have a column named exactly "S.Sharif" in your object
"data".

What does str(data) and names(data) tell you about the columns in your
data set? If looking at these doesn't help you, post the output from
str(data) and names(data) and someone might be able to help.

You should always check that R has imported the data in the way you
expect; just because you think there is something in there called
S.Sharif doesn't mean R sees it that way.

You also seem to have included the R-Help email address twice in the To:
header of your email - once is sufficient.

G

> Thank You
> Regards
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From jinlo789 at yahoo.com  Sun Jul 22 12:50:58 2007
From: jinlo789 at yahoo.com (Jin Lo)
Date: Sun, 22 Jul 2007 03:50:58 -0700 (PDT)
Subject: [R] Off-topic: simulate time dependent covariates
Message-ID: <440059.55832.qm@web45402.mail.sp1.yahoo.com>

Dear R friends,

this is an off-topic question. Could you please point
me in ways (e.g., references or even R code) for
simulating time varying covariates in a survival
analysis setting.

Thanks in advance for any responses.

yours sincerely,

Jin


From ba208 at exeter.ac.uk  Sun Jul 22 13:09:08 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Sun, 22 Jul 2007 12:09:08 +0100
Subject: [R] create an array with rep
References: <A367E429-2D11-4F90-BA8D-17CA588F6518@ex.ac.uk>
Message-ID: <B1D3DF3D-13C8-44CF-9C83-DA96B8C9D6DE@ex.ac.uk>


Hi,


I want to make the following array of numbers:

-3 -3 -3 -3 -3 -3 -3 -2 -2 -2 -2 -2 -2 -2 ...  3  3  3  3  3  3  3
-3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3 ... -3 -2 -1  0  1  2  3

(3 would be N, a painful example to type number).

Here is my dirty attempt to do it,

> N<-3
>
> x<-c(-N:N)
>
> rj<-rbind(matrix(outer(matrix(1,1,2*N+1),x),nrow=1),rep(x,2*N+1))
>

It sort of works, but I'd like some advice on how to do it properly  
as I'm very new to R and N may be big at some point.

Thanks,

baptiste


From ggrothendieck at gmail.com  Sun Jul 22 13:23:49 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 Jul 2007 07:23:49 -0400
Subject: [R] tagging results of "apply"
In-Reply-To: <CADFD0E28E1E6A46B0C84335BDB994F504989B64@whexchmb16.bsna.bsroot.bear.com>
References: <Q1BYWkxDSSFNOTkqIC8pNDM0NTg0NDUz@sspc-lisa>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B64@whexchmb16.bsna.bsroot.bear.com>
Message-ID: <971536df0707220423p6d4ee73q5d3bdf34ee4964fe@mail.gmail.com>

You don't need apply at all here.  cor can already do that and it
automatically labels the rows and columns too.  Using the builtin
dataset anscombe whose first 4 columns are labelled x1,x2,x3,x4
and whose next 4 columns are labelled y1,y2,y3,y4 we have:

> cor(anscombe[1:4], anscombe[5:8])
           y1         y2         y3         y4
x1  0.8164205  0.8162365  0.8162867 -0.3140467
x2  0.8164205  0.8162365  0.8162867 -0.3140467
x3  0.8164205  0.8162365  0.8162867 -0.3140467
x4 -0.5290927 -0.7184365 -0.3446610  0.8165214

cor works the same with matrices too.


On 7/20/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> In trying to get a better understanding of vectorization I wrote the
> following code:
>
> My objective is to take two sets of time series and calculate the
> correlations for each combination of time series.
>
> mat1 <- matrix(sample(1:500, 25), ncol = 5)
> mat2 <- matrix(sample(501:1000, 25), ncol = 5)
>
> Scenario 1:
> apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
>
> Scenario 2:
> apply(mat1, 1, function(x) cor(mat1, mat2))
>
> Using scenario 1, (output below) I can see that correlations are
> calculated for just the first row of mat2 against each individual row of
> mat1.
>
> Using scenario 2, (output below) I can see that correlations are
> calculated for each row of mat2 against each individual row of mat1.
>
> Q1: The output of scenario2 consists of 25 rows of data.  Are the first
> five rows mat1 against mat2[1,], the next five rows mat1 against
> mat2[2,], ... last five rows mat1 against mat2[5,]?
>
> Q2: I assign the output of scenario 2 to a new matrix
>
>        matC <- apply(mat1, 1, function(x) cor(mat1, mat2))
>
>    However, I need a way to identify each row in matC as a pairing of
> rows from mat1 and mat2.  Is there a parameter I can add to apply to do
> this?
>
> Scenario 1:
> > apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
>           [,1]       [,2]       [,3]       [,4]       [,5]
> [1,] -0.4626122 -0.4626122 -0.4626122 -0.4626122 -0.4626122
> [2,] -0.9031543 -0.9031543 -0.9031543 -0.9031543 -0.9031543
> [3,]  0.0735273  0.0735273  0.0735273  0.0735273  0.0735273
> [4,]  0.7401259  0.7401259  0.7401259  0.7401259  0.7401259
> [5,] -0.4548582 -0.4548582 -0.4548582 -0.4548582 -0.4548582
>
> Scenario 2:
> > apply(mat1, 1, function(x) cor(mat1, mat2))
>             [,1]        [,2]        [,3]        [,4]        [,5]
>  [1,]  0.19394126  0.19394126  0.19394126  0.19394126  0.19394126
>  [2,]  0.26402400  0.26402400  0.26402400  0.26402400  0.26402400
>  [3,]  0.12923842  0.12923842  0.12923842  0.12923842  0.12923842
>  [4,] -0.74549676 -0.74549676 -0.74549676 -0.74549676 -0.74549676
>  [5,]  0.64074122  0.64074122  0.64074122  0.64074122  0.64074122
>  [6,]  0.26931986  0.26931986  0.26931986  0.26931986  0.26931986
>  [7,]  0.08527921  0.08527921  0.08527921  0.08527921  0.08527921
>  [8,] -0.28034079 -0.28034079 -0.28034079 -0.28034079 -0.28034079
>  [9,] -0.15251915 -0.15251915 -0.15251915 -0.15251915 -0.15251915
> [10,]  0.19542415  0.19542415  0.19542415  0.19542415  0.19542415
> [11,]  0.75107032  0.75107032  0.75107032  0.75107032  0.75107032
> [12,]  0.53042767  0.53042767  0.53042767  0.53042767  0.53042767
> [13,] -0.51163612 -0.51163612 -0.51163612 -0.51163612 -0.51163612
> [14,] -0.44396048 -0.44396048 -0.44396048 -0.44396048 -0.44396048
> [15,]  0.57018745  0.57018745  0.57018745  0.57018745  0.57018745
> [16,]  0.70480284  0.70480284  0.70480284  0.70480284  0.70480284
> [17,] -0.36674283 -0.36674283 -0.36674283 -0.36674283 -0.36674283
> [18,] -0.81826607 -0.81826607 -0.81826607 -0.81826607 -0.81826607
> [19,]  0.53145184  0.53145184  0.53145184  0.53145184  0.53145184
> [20,]  0.24568385  0.24568385  0.24568385  0.24568385  0.24568385
> [21,] -0.10610402 -0.10610402 -0.10610402 -0.10610402 -0.10610402
> [22,] -0.78650748 -0.78650748 -0.78650748 -0.78650748 -0.78650748
> [23,]  0.04269423  0.04269423  0.04269423  0.04269423  0.04269423
> [24,]  0.14704698  0.14704698  0.14704698  0.14704698  0.14704698
> [25,]  0.28340166  0.28340166  0.28340166  0.28340166  0.28340166
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the pers...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Dimitris.Rizopoulos at med.kuleuven.be  Sun Jul 22 13:35:09 2007
From: Dimitris.Rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Sun, 22 Jul 2007 13:35:09 +0200
Subject: [R] create an array with rep
In-Reply-To: <B1D3DF3D-13C8-44CF-9C83-DA96B8C9D6DE@ex.ac.uk>
References: <A367E429-2D11-4F90-BA8D-17CA588F6518@ex.ac.uk>
	<B1D3DF3D-13C8-44CF-9C83-DA96B8C9D6DE@ex.ac.uk>
Message-ID: <20070722133509.bnql5dutc3g0s8ck@webmail5.kuleuven.be>

try this:

x <- -3:3
as.matrix(expand.grid(x, x))
# or
t(as.matrix(expand.grid(x, x)))


I hope it helps.

Best,
Dimitris


----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
      http://www.student.kuleuven.be/~m0390867/dimitris.htm


Quoting baptiste Augui? <ba208 at exeter.ac.uk>:

>
> Hi,
>
>
> I want to make the following array of numbers:
>
> -3 -3 -3 -3 -3 -3 -3 -2 -2 -2 -2 -2 -2 -2 ...  3  3  3  3  3  3  3
> -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3 ... -3 -2 -1  0  1  2  3
>
> (3 would be N, a painful example to type number).
>
> Here is my dirty attempt to do it,
>
>> N<-3
>>
>> x<-c(-N:N)
>>
>> rj<-rbind(matrix(outer(matrix(1,1,2*N+1),x),nrow=1),rep(x,2*N+1))
>>
>
> It sort of works, but I'd like some advice on how to do it properly
> as I'm very new to R and N may be big at some point.
>
> Thanks,
>
> baptiste
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From ggrothendieck at gmail.com  Sun Jul 22 13:56:31 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 Jul 2007 07:56:31 -0400
Subject: [R] create an array with rep
In-Reply-To: <20070722133509.bnql5dutc3g0s8ck@webmail5.kuleuven.be>
References: <A367E429-2D11-4F90-BA8D-17CA588F6518@ex.ac.uk>
	<B1D3DF3D-13C8-44CF-9C83-DA96B8C9D6DE@ex.ac.uk>
	<20070722133509.bnql5dutc3g0s8ck@webmail5.kuleuven.be>
Message-ID: <971536df0707220456x7e324063s1363deb95f9e4b79@mail.gmail.com>

On 7/22/07, Dimitris Rizopoulos <Dimitris.Rizopoulos at med.kuleuven.be> wrote:
> try this:
>
> x <- -3:3
> as.matrix(expand.grid(x, x))
> # or
> t(as.matrix(expand.grid(x, x)))

One minor shortening:

t(expand.grid(x, x))


From ba208 at exeter.ac.uk  Sun Jul 22 14:10:09 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Sun, 22 Jul 2007 13:10:09 +0100
Subject: [R] create an array with rep
In-Reply-To: <1185105269.2866.47.camel@graptoleberis.geog.ucl.ac.uk>
References: <A367E429-2D11-4F90-BA8D-17CA588F6518@ex.ac.uk>
	<B1D3DF3D-13C8-44CF-9C83-DA96B8C9D6DE@ex.ac.uk>
	<1185105269.2866.47.camel@graptoleberis.geog.ucl.ac.uk>
Message-ID: <686DADEC-BB57-4DDE-92BA-6B476131600B@ex.ac.uk>

Thank you all!

This looks like the fastest solution, N being big and my machine slow  
I'll go for this one.

Thanks,

baptiste

On 22 Jul 2007, at 12:54, Gavin Simpson wrote:

>>
>> I want to make the following array of numbers:
>>
>> -3 -3 -3 -3 -3 -3 -3 -2 -2 -2 -2 -2 -2 -2 ...  3  3  3  3  3  3  3
>> -3 -2 -1  0  1  2  3 -3 -2 -1  0  1  2  3 ... -3 -2 -1  0  1  2  3
>>
>> (3 would be N, a painful example to type number).
>
> This does what you want:
>
> foo <- function(N) {
>    reps <- 2*N+1
>    matrix(c(rep(-N:N, each = reps), rep(-N:N, times = reps)),
>           nrow = 2, byrow = TRUE)
> }


From f.harrell at vanderbilt.edu  Sun Jul 22 14:56:11 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 22 Jul 2007 07:56:11 -0500
Subject: [R] Off-topic: simulate time dependent covariates
In-Reply-To: <440059.55832.qm@web45402.mail.sp1.yahoo.com>
References: <440059.55832.qm@web45402.mail.sp1.yahoo.com>
Message-ID: <46A353EB.7090707@vanderbilt.edu>

Jin Lo wrote:
> Dear R friends,
> 
> this is an off-topic question. Could you please point
> me in ways (e.g., references or even R code) for
> simulating time varying covariates in a survival
> analysis setting.
> 
> Thanks in advance for any responses.
> 
> yours sincerely,
> 
> Jin

I'm not sure if this article addresses time-dependent covariates but it 
may be worth a look:

@Article{ben05gen,
   author = 		 {Bender, Ralf and Augustin, Thomas and Blettner, Maria},
   title = 		 {Generating survival times to simulate {Cox}
proportional hazards models},
   journal = 	 Stat in Med,
   year = 		 2005,
   volume =		 24,
   pages =		 {1713-1723},
   annote =		 {simulation setup;simulating survival
times;censoring;Cox model;errata 25:1978-1979}
}

The spower function in the Hmisc package can simulate data for a 
non-constant-hazard ratio treatment effect.  The user specifies the 
hazard ratio as a function of time.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From drstrong at ucdavis.edu  Sun Jul 22 17:49:25 2007
From: drstrong at ucdavis.edu (Mr Natural)
Date: Sun, 22 Jul 2007 08:49:25 -0700 (PDT)
Subject: [R] ?R:  Removing white space betwen multiple plots,
 traditional graphics
In-Reply-To: <11716176.post@talk.nabble.com>
References: <11716176.post@talk.nabble.com>
Message-ID: <11732080.post@talk.nabble.com>


Dear List: Thank you for the several helpful comments on my original post of
this title.
Most to the point, 
1. that from z, who is a coauthor of zoo was very kind. I have spent a
morning learning zoo 
and find it a very cool package. With zoo, one can make very neat time
series graphs with
a minimum of code. I had great fun downloading and plotting financial data
from the web.

I still have two problems with zoo for my particular application as per the
original post of this title.
   First, my dates are slightly different for each series. Neither of the
two suggestions for dealing 
different-dates-for-different-series that I have found are appropriate
(na.approx(m), na.locf.(m)).
Is there a way that the NAs can be skipped by each plot, with lines plotted
between the dates that exist,
as in my original code below?
    Second, I have complex standard errors for each point, which are either
read in as a distinct column 
for each y value or calculated in R and plotted with code such as (for the
first two series):

plot(MP~dmp, type="b",xlab="",ylim=c(0,1),xaxt = "n")
arrows(dmp,MP+semp,dmp,MP-semp,length=.02,angle=90,code=3)
plot(C~dc, type="b",xlab="",ylim=c(0,1),xaxt = "n")
arrows(dc,C+sec,dc,C-sec,length=.02,angle=90,code=3)
   I cannot get zoo to do this. 

2. The second helpful comment is from Steven Tucker, and this solves my
problem completely
and elegantly. He suggests changing my parameter statements to:
par(mar=c(0,5,0,2), mfrow = c(6,1), oma=c(5,0,2,0))

Thank you Steven!

Warm regards, Don

ps. This forum is very valuable to the growth of R usage among those of us
who are not programmers.






Mr Natural wrote:
> 
> I would appreciate suggestions for removing the white spaces the graphs in
> a stack:
> 
> par(mar=c(2,2,1,1), mfrow = c(6,1))
> mydates<-dates(1:20,origin=c(month = 1, day = 1, year = 1986))
> plot(rnorm(20,0.1,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.2,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.3,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.5,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.7,0.1)~mydates, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> plot(rnorm(20,0.8,0.1)~mydates, type="b",xlab="",ylim=c(0,1) )
> 
> Thanx, Don
> 

-- 
View this message in context: http://www.nabble.com/-R%3A--Removing-white-space-betwen-multiple-plots%2C-traditional-graphics-tf4119626.html#a11732080
Sent from the R help mailing list archive at Nabble.com.


From ggrothendieck at gmail.com  Sun Jul 22 18:10:43 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 Jul 2007 12:10:43 -0400
Subject: [R] ?R: Removing white space betwen multiple plots,
	traditional graphics
In-Reply-To: <11732080.post@talk.nabble.com>
References: <11716176.post@talk.nabble.com> <11732080.post@talk.nabble.com>
Message-ID: <971536df0707220910i71a5f80akef405f8fc09e2e6b@mail.gmail.com>

On 7/22/07, Mr Natural <drstrong at ucdavis.edu> wrote:
> I still have two problems with zoo for my particular application as per the
> original post of this title.
>   First, my dates are slightly different for each series. Neither of the
> two suggestions for dealing
> different-dates-for-different-series that I have found are appropriate
> (na.approx(m), na.locf.(m)).
> Is there a way that the NAs can be skipped by each plot, with lines plotted
> between the dates that exist,
> as in my original code below?

But that is precisely what na.approx does.  Here is an example:

library(zoo)
z <- as.zoo(ldeaths)
z[c(4, 8, 17)] <- NA
zz <- cbind(z, lag(z, -5))
plot(na.approx(zz), col = 1:2) # classic
library(lattice)
xyplot(na.approx(zz), col = 1:2) # lattice


>    Second, I have complex standard errors for each point, which are either
> read in as a distinct column
> for each y value or calculated in R and plotted with code such as (for the
> first two series):
>
> plot(MP~dmp, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> arrows(dmp,MP+semp,dmp,MP-semp,length=.02,angle=90,code=3)
> plot(C~dc, type="b",xlab="",ylim=c(0,1),xaxt = "n")
> arrows(dc,C+sec,dc,C-sec,length=.02,angle=90,code=3)
>   I cannot get zoo to do this.

Please provide the data.


From twoutopias at gmail.com  Sun Jul 22 18:38:02 2007
From: twoutopias at gmail.com (Seth Roberts)
Date: Sun, 22 Jul 2007 09:38:02 -0700 (PDT)
Subject: [R] using R for a reaction-time experiment
Message-ID: <11732474.post@talk.nabble.com>


I want to use R to run a reaction-time experiment: Something appears on the
screen, I respond by typing something (one keystroke), the system measures
the speed of my response. R would be great for this if only I didn't have to
hit Enter to enter that keystroke. I am doing such experiments now but they
require two actions per trial: hit keystroke, hit Enter.

Is there some way that R can be made to respond to a single keystroke (other
than Enter)?
-- 
View this message in context: http://www.nabble.com/using-R-for-a-reaction-time-experiment-tf4125643.html#a11732474
Sent from the R help mailing list archive at Nabble.com.


From ted.harding at nessie.mcc.ac.uk  Sun Jul 22 19:11:45 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 22 Jul 2007 18:11:45 +0100 (BST)
Subject: [R] using R for a reaction-time experiment
In-Reply-To: <11732474.post@talk.nabble.com>
Message-ID: <XFMail.070722181145.ted.harding@nessie.mcc.ac.uk>

On 22-Jul-07 16:38:02, Seth Roberts wrote:
> 
> I want to use R to run a reaction-time experiment:
> Something appears on the screen, I respond by typing something
> (one keystroke), the system measures the speed of my response.
> R would be great for this if only I didn't have to hit Enter
> to enter that keystroke. I am doing such experiments now but
> they require two actions per trial: hit keystroke, hit Enter.
> 
> Is there some way that R can be made to respond to a single
> keystroke (other than Enter)?

What operating system are you using? If it's Linux, there would
be ways to detect the keystroke (say "A") and immediately send
"A\n" (i.e. "A" followed by "enter") to a FIFO which R could be
watching. Then R would receive your single keystroke as if you
had followed it by pressing "Enter".

If you're using Windows, then unfortunately I haven't a clue.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 22-Jul-07                                       Time: 18:11:43
------------------------------ XFMail ------------------------------


From twoutopias at gmail.com  Sun Jul 22 19:55:27 2007
From: twoutopias at gmail.com (Seth Roberts)
Date: Sun, 22 Jul 2007 10:55:27 -0700 (PDT)
Subject: [R] using R for a reaction-time experiment
In-Reply-To: <XFMail.070722181145.ted.harding@nessie.mcc.ac.uk>
References: <11732474.post@talk.nabble.com>
	<XFMail.070722181145.ted.harding@nessie.mcc.ac.uk>
Message-ID: <11733124.post@talk.nabble.com>


I'm using Windows XP unfortunately.


Ted.Harding wrote:
> 
> On 22-Jul-07 16:38:02, Seth Roberts wrote:
>> 
>> I want to use R to run a reaction-time experiment:
>> Something appears on the screen, I respond by typing something
>> (one keystroke), the system measures the speed of my response.
>> R would be great for this if only I didn't have to hit Enter
>> to enter that keystroke. I am doing such experiments now but
>> they require two actions per trial: hit keystroke, hit Enter.
>> 
>> Is there some way that R can be made to respond to a single
>> keystroke (other than Enter)?
> 
> What operating system are you using? If it's Linux, there would
> be ways to detect the keystroke (say "A") and immediately send
> "A\n" (i.e. "A" followed by "enter") to a FIFO which R could be
> watching. Then R would receive your single keystroke as if you
> had followed it by pressing "Enter".
> 
> If you're using Windows, then unfortunately I haven't a clue.
> 
> Best wishes,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 22-Jul-07                                       Time: 18:11:43
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/using-R-for-a-reaction-time-experiment-tf4125643.html#a11733124
Sent from the R help mailing list archive at Nabble.com.


From wasquith at austin.rr.com  Sun Jul 22 20:29:24 2007
From: wasquith at austin.rr.com (William Asquith)
Date: Sun, 22 Jul 2007 13:29:24 -0500
Subject: [R]  Package design, placement of legacy functions
Message-ID: <4EDAB73D-63B2-42FF-8B52-822605511203@austin.rr.com>

I have a function XOLD() from a nearly verbatim port of legacy  
FORTRAN in a package. I have remplemented this function as XNEW()  
using much cleaner native R and built-in functions of R. I have  
switched the package to the XNEW(), but for historical reasons would  
like to retain the XOLD() somewhere in the package directory  
structure. An assertion through a README or other will point to this  
historical function and the output from the two should be numerically  
equal.

Placement in package/R is not an option as XOLD() no longer  
constitutes a true user level function, would package/inst/legacyR or  
something like that be suitable to the R community?

Thanks for the guidance. . .

William


From amnakhan493 at gmail.com  Sun Jul 22 21:09:03 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Sun, 22 Jul 2007 12:09:03 -0700
Subject: [R] Data Set
In-Reply-To: <1185101351.2866.24.camel@graptoleberis.geog.ucl.ac.uk>
References: <3ffd3bb60707220325u2a702b3dp22702aa6777501d1@mail.gmail.com>
	<1185101351.2866.24.camel@graptoleberis.geog.ucl.ac.uk>
Message-ID: <3ffd3bb60707221209v333a3aadt1634a18975242a6a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070722/b2107807/attachment.pl 

From seow_ainee at hotmail.com  Sat Jul 21 20:36:07 2007
From: seow_ainee at hotmail.com (rach.s)
Date: Sat, 21 Jul 2007 11:36:07 -0700 (PDT)
Subject: [R] Gamma MLE
Message-ID: <11724823.post@talk.nabble.com>


Hello,

I was asked to try the following code on R,

gamma.mles
function (xx,shape0,rate0)
{
n<- length(xx)
xbar<- mean(xx)
logxbar<- mean(log(xx))
theta<-c(shape0,rate0)
repeat {
theta0<- theta
shape<- theta0[1]
rate<- theta0[2]
S<- n*matrix(c(log(rate)-digamma(shape)+logxbar,shape/rate-xbar),ncol=1)
I<- n*matrix(c(trigamma(shape),-1/rate,-1/rate,shape/rate^2),ncol=2)
theta<- theta0 + solve(I) %*% S
if(max(abs(theta-theta0)) < 1e-08)
break
}
list(estimates=theta, infmat=I)
}

However, this appears: Error: object "gamma.mles" not found

I tried looking in the packages for gamma.mles, but I couldn't find it
anywhere. Can someone tell me where can I load it?

Thanks
-- 
View this message in context: http://www.nabble.com/Gamma-MLE-tf4122769.html#a11724823
Sent from the R help mailing list archive at Nabble.com.


From jrg66 at comcast.net  Sun Jul 22 21:39:26 2007
From: jrg66 at comcast.net (jrg66 at comcast.net)
Date: Sun, 22 Jul 2007 19:39:26 +0000
Subject: [R] Write columns from within a list to a matrix?
Message-ID: <072220071939.21999.46A3B26E00098C96000055EF2216566276C0C0099D06@comcast.net>

Hello,

I think I have a mental block when it comes to working with lists.  lapply and sapply appear to do some magical things, but I can't seem to master their usage.

As an example, I would like to convert a column within a list to a matrix, with the list element corresponding to the new matrix column.

#Here is a simplified example: .
test=vector("list", 3)
for (i in 1:3){ test[[i]]=cbind(runif(15), rnorm(15,2)) }  #create example list (I'm sure there is a better way to do this too).

#Now, I wan to get the second column back out, converting it from a list to a matrix.  This works, but gets confusing/inefficient when I have multiple complex lists I am trying to manage.

savecol2=matrix(0,15,0)
for (i in 1:3){
savecol2=cbind(savecol2, test[[i]][,1])
} 

#Something like??:  (of course this doesn't work)
savecol2=sapply(test, "[[", function(x) x[2,])     

Thank you!

Jeff


From brown_emu at yahoo.com  Sun Jul 22 21:45:22 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sun, 22 Jul 2007 12:45:22 -0700 (PDT)
Subject: [R] Data Set
In-Reply-To: <3ffd3bb60707221209v333a3aadt1634a18975242a6a@mail.gmail.com>
Message-ID: <854894.18124.qm@web39703.mail.mud.yahoo.com>

Could you post the output from 

str(data)

?

Perhaps that will give us a clue.

--- amna khan <amnakhan493 at gmail.com> wrote:

> Sir the station name "S.Sharif" exists in the data but still the error is
> ocurring of being not found.
> Please help in this regard.
> 
> 
> On 7/22/07, Gavin Simpson <gavin.simpson at ucl.ac.uk> wrote:
> >
> > On Sun, 2007-07-22 at 03:25 -0700, amna khan wrote:
> > > Hi Sir
> > > I have made a data set having 23 stations of rainfall.
> > > when I use the attach function to approach indevidual stations then
> > > following error occurr.
> > >
> > > *>attach(data)*
> > > *>S.Sharif    #S.Sharif is the station  name which has 50 data values*
> > > *Error: object "S.Sharif" not found*
> > > Now how to solve this problem.
> >
> > Then you don't have a column named exactly "S.Sharif" in your object
> > "data".
> >
> > What does str(data) and names(data) tell you about the columns in your
> > data set? If looking at these doesn't help you, post the output from
> > str(data) and names(data) and someone might be able to help.
> >
> > You should always check that R has imported the data in the way you
> > expect; just because you think there is something in there called
> > S.Sharif doesn't mean R sees it that way.
> >
> > You also seem to have included the R-Help email address twice in the To:
> > header of your email - once is sufficient.
> >
> > G
> >
> > > Thank You
> > > Regards
> > >
> > --
> > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> > Gavin Simpson                 [t] +44 (0)20 7679 0522
> > ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
> > Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
> > Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
> > UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> >
> >
> >
> 
> 
> -- 
> AMINA SHAHZADI
> Department of Statistics
> GC University Lahore, Pakistan.
> Email:
> amnakhan493 at gmail.com
> amna_989 at hotmail.com
> amna_989 at yahoo.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From idimakos at upatras.gr  Sun Jul 22 21:47:46 2007
From: idimakos at upatras.gr (Ioannis Dimakos)
Date: Sun, 22 Jul 2007 22:47:46 +0300 (EEST)
Subject: [R] using R for a reaction-time experiment
In-Reply-To: <11733124.post@talk.nabble.com>
References: <11732474.post@talk.nabble.com>
	<XFMail.070722181145.ted.harding@nessie.mcc.ac.uk>
	<11733124.post@talk.nabble.com>
Message-ID: <45898.91.140.44.87.1185133666.squirrel@mail.upatras.gr>

Have you tried DMDX instead of R? And then feed the reaction times
collected by DMDX to R for manipulation?

IKD

===================
On ???, ??????? 22, 2007 20:55, Seth Roberts wrote:
>
> I'm using Windows XP unfortunately.
>
>

> Ted.Harding wrote:
>>
>> On 22-Jul-07 16:38:02, Seth Roberts wrote:
>>>
>>> I want to use R to run a reaction-time experiment:
>>> Something appears on the screen, I respond by typing something
>>> (one keystroke), the system measures the speed of my response.
>>> R would be great for this if only I didn't have to hit Enter
>>> to enter that keystroke. I am doing such experiments now but
>>> they require two actions per trial: hit keystroke, hit Enter.
>>>
>>> Is there some way that R can be made to respond to a single
>>> keystroke (other than Enter)?
>>
>> What operating system are you using? If it's Linux, there would
>> be ways to detect the keystroke (say "A") and immediately send
>> "A\n" (i.e. "A" followed by "enter") to a FIFO which R could be
>> watching. Then R would receive your single keystroke as if you
>> had followed it by pressing "Enter".
>>
>> If you're using Windows, then unfortunately I haven't a clue.
>>
>> Best wishes,
>> Ted.
>>
>> --------------------------------------------------------------------
>> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
>> Fax-to-email: +44 (0)870 094 0861
>> Date: 22-Jul-07                                       Time: 18:11:43
>> ------------------------------ XFMail ------------------------------
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> --
> View this message in context:
> http://www.nabble.com/using-R-for-a-reaction-time-experiment-tf4125643.html#a11733124
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ioannis C. Dimakos, Ph.D.
University of Patras
Department of Elementary Education
Patras, GR-26500 GREECE
http://www.elemedu.upatras.gr/dimakos/
http://thedailyblahblah.wordpress.com/


From bcarvalh at jhsph.edu  Sun Jul 22 21:50:05 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Sun, 22 Jul 2007 15:50:05 -0400
Subject: [R] Write columns from within a list to a matrix?
In-Reply-To: <072220071939.21999.46A3B26E00098C96000055EF2216566276C0C0099D06@comcast.net>
References: <072220071939.21999.46A3B26E00098C96000055EF2216566276C0C0099D06@comcast.net>
Message-ID: <D6B56BC9-5567-484A-A9CF-8BA71843342A@jhsph.edu>

test <- lapply(1:3, function(i) cbind(runif(15), rnorm(15,2)))
sapply(test, "[", 16:30)

b

On Jul 22, 2007, at 3:39 PM, jrg66 at comcast.net wrote:

> Hello,
>
> I think I have a mental block when it comes to working with lists.   
> lapply and sapply appear to do some magical things, but I can't  
> seem to master their usage.
>
> As an example, I would like to convert a column within a list to a  
> matrix, with the list element corresponding to the new matrix column.
>
> #Here is a simplified example: .
> test=vector("list", 3)
> for (i in 1:3){ test[[i]]=cbind(runif(15), rnorm(15,2)) }  #create  
> example list (I'm sure there is a better way to do this too).
>
> #Now, I wan to get the second column back out, converting it from a  
> list to a matrix.  This works, but gets confusing/inefficient when  
> I have multiple complex lists I am trying to manage.
>
> savecol2=matrix(0,15,0)
> for (i in 1:3){
> savecol2=cbind(savecol2, test[[i]][,1])
> }
>
> #Something like??:  (of course this doesn't work)
> savecol2=sapply(test, "[[", function(x) x[2,])
>
> Thank you!
>
> Jeff
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bcarvalh at jhsph.edu  Sun Jul 22 21:55:17 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Sun, 22 Jul 2007 15:55:17 -0400
Subject: [R] Write columns from within a list to a matrix?
In-Reply-To: <D6B56BC9-5567-484A-A9CF-8BA71843342A@jhsph.edu>
References: <072220071939.21999.46A3B26E00098C96000055EF2216566276C0C0099D06@comcast.net>
	<D6B56BC9-5567-484A-A9CF-8BA71843342A@jhsph.edu>
Message-ID: <E2D31F9C-F8E7-407D-B467-9C6F2D9A7335@jhsph.edu>

oh! and if you want to be less ad-hoc:

sapply(test, function(x) x[,2])

b

On Jul 22, 2007, at 3:50 PM, Benilton Carvalho wrote:

> test <- lapply(1:3, function(i) cbind(runif(15), rnorm(15,2)))
> sapply(test, "[", 16:30)
>
> b
>
> On Jul 22, 2007, at 3:39 PM, jrg66 at comcast.net wrote:
>
>> Hello,
>>
>> I think I have a mental block when it comes to working with  
>> lists.  lapply and sapply appear to do some magical things, but I  
>> can't seem to master their usage.
>>
>> As an example, I would like to convert a column within a list to a  
>> matrix, with the list element corresponding to the new matrix column.
>>
>> #Here is a simplified example: .
>> test=vector("list", 3)
>> for (i in 1:3){ test[[i]]=cbind(runif(15), rnorm(15,2)) }  #create  
>> example list (I'm sure there is a better way to do this too).
>>
>> #Now, I wan to get the second column back out, converting it from  
>> a list to a matrix.  This works, but gets confusing/inefficient  
>> when I have multiple complex lists I am trying to manage.
>>
>> savecol2=matrix(0,15,0)
>> for (i in 1:3){
>> savecol2=cbind(savecol2, test[[i]][,1])
>> }
>>
>> #Something like??:  (of course this doesn't work)
>> savecol2=sapply(test, "[[", function(x) x[2,])
>>
>> Thank you!
>>
>> Jeff
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting- 
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Sun Jul 22 22:10:19 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sun, 22 Jul 2007 13:10:19 -0700 (PDT)
Subject: [R] Write columns from within a list to a matrix?
In-Reply-To: <072220071939.21999.46A3B26E00098C96000055EF2216566276C0C0099D06@comcast.net>
Message-ID: <714260.77017.qm@web39707.mail.mud.yahoo.com>

Very close... Actually it's more like

savecol2=sapply(test, function(x) x[,1])

to get the same matrix as you showed in your for-loop (did you actually want
the first or second column?).

"when I have multiple complex lists I am trying to manage"...
for this, you can try mapply() which goes something like
mapply(function(x,y) #...function body...#,
       x=list1,y=list2)


--- jrg66 at comcast.net wrote:

> Hello,
> 
> I think I have a mental block when it comes to working with lists.  lapply
> and sapply appear to do some magical things, but I can't seem to master
> their usage.
> 
> As an example, I would like to convert a column within a list to a matrix,
> with the list element corresponding to the new matrix column.
> 
> #Here is a simplified example: .
> test=vector("list", 3)
> for (i in 1:3){ test[[i]]=cbind(runif(15), rnorm(15,2)) }  #create example
> list (I'm sure there is a better way to do this too).
> 
> #Now, I wan to get the second column back out, converting it from a list to
> a matrix.  This works, but gets confusing/inefficient when I have multiple
> complex lists I am trying to manage.
> 
> savecol2=matrix(0,15,0)
> for (i in 1:3){
> savecol2=cbind(savecol2, test[[i]][,1])
> } 
> 
> #Something like??:  (of course this doesn't work)
> savecol2=sapply(test, "[[", function(x) x[2,])     
> 
> Thank you!
> 
> Jeff
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cryan at binghamton.edu  Mon Jul 23 00:33:53 2007
From: cryan at binghamton.edu (Christopher W. Ryan)
Date: Sun, 22 Jul 2007 17:33:53 -0500
Subject: [R] summary of linear fixed effects model is different than the
 HSAUR book
Message-ID: <46A3DB51.1080406@binghamton.edu>

Running R 2.5.1 and a newly downloaded lme4 package on WinXP

I'm trying to work my way through Everitt and Hothorn's "Handbook of
Statistical Analyses Using R," c 2006.  (No, it's not homework.)

Chapter 10 discusses linear mixed effects models for longitudinal data.
 I've called my long data frame BtheB.long

Here's the model from the book, which I run.
lmer1 <- lmer(bdi ~ bdi.pre + months + treatment + drug + length + (1 |
subject), data = BtheB.long, method = "ML", na.action = na.omit)

Here is the summary of the model that I see:

> summary(lmer1)
Linear mixed-effects model fit by maximum likelihood
Formula: bdi ~ bdi.pre + months + treatment + drug + length + (1 | subject)
   Data: BtheB.long
  AIC  BIC logLik MLdeviance REMLdeviance
 1885 1910 -935.3       1871         1866
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 48.299   6.9498
 Residual             25.129   5.0128
number of obs: 280, groups: subject, 97

Fixed effects:
               Estimate Std. Error t value
(Intercept)     5.94372    2.24915   2.643
bdi.pre         0.63819    0.07759   8.225
months         -0.71703    0.14606  -4.909
treatmentBtheB -2.37311    1.66368  -1.426
drugYes        -2.79786    1.71993  -1.627
length>6m       0.25639    1.63213   0.157

Correlation of Fixed Effects:
            (Intr) bdi.pr months trtmBB drugYs
bdi.pre     -0.678
months      -0.264  0.023
tretmntBthB -0.389  0.121  0.022
drugYes     -0.071 -0.237 -0.025 -0.323
length>6m   -0.238 -0.242 -0.043  0.002  0.158


But on page 169, summary() is shown to produce additional columns in the
"fixed effects" section, namely degrees of freedom and the P-value (with
significance stars).

How can I produce that output?  Am I doing something wrong?  Has lme4
changed?

Thanks.

-- 
Christopher W. Ryan, MD
SUNY Upstate Medical University Clinical Campus at Binghamton
40 Arch Street, Johnson City, NY  13790
cryanatbinghamtondotedu
PGP public keys available at http://home.stny.rr.com/ryancw/

"If you want to build a ship, don't drum up the men to gather wood,
divide the work and give orders. Instead, teach them to yearn for the
vast and endless sea."  [Antoine de St. Exupery]


From gavin.simpson at ucl.ac.uk  Sun Jul 22 23:46:49 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sun, 22 Jul 2007 22:46:49 +0100
Subject: [R] Data Set
In-Reply-To: <3ffd3bb60707221209v333a3aadt1634a18975242a6a@mail.gmail.com>
References: <3ffd3bb60707220325u2a702b3dp22702aa6777501d1@mail.gmail.com>
	<1185101351.2866.24.camel@graptoleberis.geog.ucl.ac.uk>
	<3ffd3bb60707221209v333a3aadt1634a18975242a6a@mail.gmail.com>
Message-ID: <1185140809.2960.5.camel@graptoleberis.geog.ucl.ac.uk>

On Sun, 2007-07-22 at 12:09 -0700, amna khan wrote:
> Sir the station name "S.Sharif" exists in the data but still the error is
> ocurring of being not found.
> Please help in this regard.

If you take the time to do what I asked and actually post the results of
typing the following into your R session:

str(data)

And send the output to the list, then we will be able to help.

Did you read /all/ of my email? I did ask you to do this.

HTH

G

> 
> 
> On 7/22/07, Gavin Simpson <gavin.simpson at ucl.ac.uk> wrote:
> >
> > On Sun, 2007-07-22 at 03:25 -0700, amna khan wrote:
> > > Hi Sir
> > > I have made a data set having 23 stations of rainfall.
> > > when I use the attach function to approach indevidual stations then
> > > following error occurr.
> > >
> > > *>attach(data)*
> > > *>S.Sharif    #S.Sharif is the station  name which has 50 data values*
> > > *Error: object "S.Sharif" not found*
> > > Now how to solve this problem.
> >
> > Then you don't have a column named exactly "S.Sharif" in your object
> > "data".
> >
> > What does str(data) and names(data) tell you about the columns in your
> > data set? If looking at these doesn't help you, post the output from
> > str(data) and names(data) and someone might be able to help.
> >
> > You should always check that R has imported the data in the way you
> > expect; just because you think there is something in there called
> > S.Sharif doesn't mean R sees it that way.
> >
> > You also seem to have included the R-Help email address twice in the To:
> > header of your email - once is sufficient.
> >
> > G
> >
> > > Thank You
> > > Regards
> > >
> > --
> > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> > Gavin Simpson                 [t] +44 (0)20 7679 0522
> > ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
> > Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
> > Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
> > UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> >
> >
> >
> 
> 
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From p.dalgaard at biostat.ku.dk  Sun Jul 22 23:49:57 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sun, 22 Jul 2007 23:49:57 +0200
Subject: [R] summary of linear fixed effects model is different than the
 HSAUR book
In-Reply-To: <46A3DB51.1080406@binghamton.edu>
References: <46A3DB51.1080406@binghamton.edu>
Message-ID: <46A3D105.9060507@biostat.ku.dk>

Christopher W. Ryan wrote:
> But on page 169, summary() is shown to produce additional columns in the
> "fixed effects" section, namely degrees of freedom and the P-value (with
> significance stars).
>
> How can I produce that output?  Am I doing something wrong?  Has lme4
> changed?
>
>   
The latter.  To make a long story short, the author got so fed up with 
the reliability of the DF heuristics that he decided to remove them 
altogether.


From mscabral at fc.ul.pt  Sun Jul 22 23:51:27 2007
From: mscabral at fc.ul.pt (=?iso-8859-1?Q?Maria_Salom=E9_Esteves_Cabral?=)
Date: Sun, 22 Jul 2007 22:51:27 +0100
Subject: [R] (no subject)
Message-ID: <3AA0B59C9640784C8956888BF8AFC5DD1FF339@fc-mailserver01.ul.pt>

Hi!
 
When I use lmer (lme4 package) it accept the correlation structure like lme or glmmPQL but in the output I have no information about it. Can anyone help me how can I get it.
 
Thanks
 
 
Salom?


From gavin.simpson at ucl.ac.uk  Sun Jul 22 23:52:35 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sun, 22 Jul 2007 22:52:35 +0100
Subject: [R] Gamma MLE
In-Reply-To: <11724823.post@talk.nabble.com>
References: <11724823.post@talk.nabble.com>
Message-ID: <1185141155.2960.10.camel@graptoleberis.geog.ucl.ac.uk>

On Sat, 2007-07-21 at 11:36 -0700, rach.s wrote:
> Hello,
> 
> I was asked to try the following code on R,
> 

I think, if you typed the code below *exactly* as you reproduced it in
your email, that you are missing the assignment operator "<-" between
gamma.mles and function(xx, shape0, rate0), i.e.:

gamma.mles <- function(xx, shape0, rate0)
{
   ##function body here
}

Does this help? Notice where the error occurs - immediately after the
line "gamma.mles", if you paste the code exactly as you reproduced.

G

> gamma.mles
> function (xx,shape0,rate0)
> {
> n<- length(xx)
> xbar<- mean(xx)
> logxbar<- mean(log(xx))
> theta<-c(shape0,rate0)
> repeat {
> theta0<- theta
> shape<- theta0[1]
> rate<- theta0[2]
> S<- n*matrix(c(log(rate)-digamma(shape)+logxbar,shape/rate-xbar),ncol=1)
> I<- n*matrix(c(trigamma(shape),-1/rate,-1/rate,shape/rate^2),ncol=2)
> theta<- theta0 + solve(I) %*% S
> if(max(abs(theta-theta0)) < 1e-08)
> break
> }
> list(estimates=theta, infmat=I)
> }
> 
> However, this appears: Error: object "gamma.mles" not found
> 
> I tried looking in the packages for gamma.mles, but I couldn't find it
> anywhere. Can someone tell me where can I load it?
> 
> Thanks
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From xh.along at gmail.com  Mon Jul 23 04:19:36 2007
From: xh.along at gmail.com (along zeng)
Date: Mon, 23 Jul 2007 10:19:36 +0800
Subject: [R] R and mysql
Message-ID: <21add9100707221919o639139ecpfbaf34d4017ccce4@mail.gmail.com>

Hi all,
     I want to visit MySQL in R use RMySQL package on Windows 2k.I got
RMySQL and DBI packages from a cran,but only the package  RMySQL  is
built on MasOS.I extracted both and put them under the folder of
"library",started R and typed string as ,
>require(RMySQL);

R told me DBI was loaded,but RMySQL  was't loaded because R not found it.


Thank very much!
  along


From ral at lcfltd.com  Mon Jul 23 04:33:54 2007
From: ral at lcfltd.com (Robert A. LaBudde)
Date: Sun, 22 Jul 2007 22:33:54 -0400
Subject: [R] Conditional logistic regression on n:m matched "cohort" data
Message-ID: <0JLM004SN1SJ123G@vms040.mailsrvcs.net>

I am designing an interlaboratory validation study for a 
presence/absence alternative method test kit vs. a presence/absence 
reference method test kit.

There will be 10 laboratories conducting tests using both methods. In 
each laboratory, there will be 5 specimens tested, each of the 5 
specimens twice by both methods (alternative, standard).

The total number of data are 10 x 5 x 4 = 200.

The general structure is as follows:

id: sequential result label, 1 to 200.
lab: a factor which has 10 levels, one for each participating lab.
specimen: 1 to 5 in each lab, no connection between labs.
method: A or R.
result: 0 or 1 (absence or presence).

This experiment appears to be equivalent to a matched cohort study. 
The matching is done on specimens, and there are 2 alternative and 2 
reference results for each specimen.

The sketchy description for clogit() of package "survival" presumes a 
matched case-control study is being analyzed.

I am looking for the correct syntax to use in clogit() for this experiment.

Is

library('survival')
clogit(result ~ lab + strata(specimen), method="exact", data=mydata)

all that is required?

Thanks.
================================================================
Robert A. LaBudde, PhD, PAS, Dpl. ACAFS  e-mail: ral at lcfltd.com
Least Cost Formulations, Ltd.            URL: http://lcfltd.com/
824 Timberlake Drive                     Tel: 757-467-0954
Virginia Beach, VA 23464-3239            Fax: 757-467-2947

"Vere scire est per causas scire"


From ral at lcfltd.com  Mon Jul 23 04:45:10 2007
From: ral at lcfltd.com (Robert A. LaBudde)
Date: Sun, 22 Jul 2007 22:45:10 -0400
Subject: [R] Conditional logistic regression on n:m matched "cohort" data
 [Corrected]
Message-ID: <0JLM00AEA2BBAGZ5@vms042.mailsrvcs.net>

[Corrected the model formula to include "method".]

I am designing an interlaboratory validation study for a 
presence/absence alternative method test kit vs. a presence/absence 
reference method test kit.

There will be 10 laboratories conducting tests using both methods. In 
each laboratory, there will be 5 specimens tested, each of the 5 
specimens twice by both methods (alternative, standard).

The total number of data are 10 x 5 x 4 = 200.

The general structure is as follows:

id: sequential result label, 1 to 200.
lab: a factor which has 10 levels, one for each participating lab.
specimen: 1 to 5 in each lab, no connection between labs.
method: A or R.
result: 0 or 1 (absence or presence).

This experiment appears to be equivalent to a matched cohort study. 
The matching is done on specimens, and there are 2 alternative and 2 
reference results for each specimen.

The sketchy description for clogit() of package "survival" presumes a 
matched case-control study is being analyzed.

I am looking for the correct syntax to use in clogit() for this experiment.

Is

library('survival')
clogit(result ~ lab + method + strata(specimen), method="exact", data=mydata)

all that is required?

Thanks.
================================================================
Robert A. LaBudde, PhD, PAS, Dpl. ACAFS  e-mail: ral at lcfltd.com
Least Cost Formulations, Ltd.            URL: http://lcfltd.com/
824 Timberlake Drive                     Tel: 757-467-0954
Virginia Beach, VA 23464-3239            Fax: 757-467-2947

"Vere scire est per causas scire"


From ggrothendieck at gmail.com  Mon Jul 23 04:54:34 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 22 Jul 2007 22:54:34 -0400
Subject: [R] R and mysql
In-Reply-To: <21add9100707221919o639139ecpfbaf34d4017ccce4@mail.gmail.com>
References: <21add9100707221919o639139ecpfbaf34d4017ccce4@mail.gmail.com>
Message-ID: <971536df0707221954g5df1c378qf4891462087fdb2c@mail.gmail.com>

To run with R 2.5.1 you need the Windows binary of RMySQL 0.6-0 but
I don't think that that is available on either CRAN or BioConductor currently.
however, one does exist and I suggest you contact the
maintainer, David James or the r-sig-db list.  If those don't get it for you
send me an email off list and I will send you mine.

I have only used the Windows version on XP so I can't say whether there
would be any problems on W2k.

On 7/22/07, along zeng <xh.along at gmail.com> wrote:
> Hi all,
>     I want to visit MySQL in R use RMySQL package on Windows 2k.I got
> RMySQL and DBI packages from a cran,but only the package  RMySQL  is
> built on MasOS.I extracted both and put them under the folder of
> "library",started R and typed string as ,
> >require(RMySQL);
>
> R told me DBI was loaded,but RMySQL  was't loaded because R not found it.
>
>
> Thank very much!
>  along
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From magdalena1236 at gmail.com  Mon Jul 23 06:14:11 2007
From: magdalena1236 at gmail.com (Ana Marcela Florez Rueda)
Date: Sun, 22 Jul 2007 23:14:11 -0500
Subject: [R] text labels on a dendrogram
Message-ID: <3e5f4a310707222114y4a0a5f81g550d373ccd0cc2e0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070722/03a2d7e9/attachment.pl 

From brown_emu at yahoo.com  Mon Jul 23 06:51:45 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sun, 22 Jul 2007 21:51:45 -0700 (PDT)
Subject: [R] Data Set
In-Reply-To: <3ffd3bb60707222025u6fe32b3eo13a62bafd901daec@mail.gmail.com>
Message-ID: <669528.69507.qm@web39714.mail.mud.yahoo.com>

It turns out that "-" and " " (space) are not valid variable names. You can
get around that in two ways:

==============
names(Monsoon)[2] <- "S.Sharif"
names(Monsoon)[8] <- "Islamabad.AP"
attach(Monsoon)
S.Sharif
Islamabad.AP
detach(Monsoon)

and do the same for other variable names that contain "-" or " " characters.

=============
The other way is to enclose the names in ``. For instance:
attach(Monsoon)
`S-Sharif`
`Islamabad AP`
detach(Monsoon)

Here is my example in which it works:
> x <- list(1:5,6:8)
> names(x) <- c("S-Sharif","Peshawar")
> str(x)
List of 2
 $ S-Sharif: int [1:5] 1 2 3 4 5
 $ Peshawar: int [1:3] 6 7 8
> attach(x)
> `S-Sharif`
[1] 1 2 3 4 5
> detach(x)



--- amna khan <amnakhan493 at gmail.com> wrote:

> Yes Sir
>  I am sending u the clue for data.
> 
> > str(Monsoon)
> List of 23
>  $ Dir           : num [1:40] 72.4 60.7 52.1.....
>  $ S-Sharif      : num [1:55] 23.6 93.5 36.3  ......
>  $ Peshawar      : num [1:57] 54.4 27.7 .......
>  $ Kakul         : num [1:54]  50.3 116.1 ...
>  $ Balakot       : num [1:47] 218.2  76.5 ...
>  $ Parachinar    : num [1:40] 41.4 37.6 62.2...
>  $ Kohat         : num [1:53] 50.8 93.2 94.5 ...
>  $ Islamabad AP  : num [1:48] 140.2  69.3...
>  $ Murree        : num [1:47] 130.0 131.3  74.4 ...
>  $ Islamabad SRRC: num [1:24] 172.2  82.3 150.1   ...
>  $ Mian Wali     : num [1:48] 80.5 48.5 56.6 43.2  ...
>  $ Jhelum        : num [1:57] 111.8  82.3  53.8  94.7  ...
>  $ Sialkot       : num [1:55]  62.7 126.0  90.7  ...
>  $ D-I Khan      : num [1:57] 24.9 40.6 34.3  ...
>  $ Faisalabad    : num [1:56] 79.2 43.9 55.4 ...
>  $ Lahore        : num [1:60] 32.5 81.5 28.7  ...
> 
> when I attach the data file and access the site "S-Sharif" or D-I Khan" or
> "Mian Wali" then error messages occur.
> 
> Please help in this regard.
> 
> Thank You
> 
> 
> On 7/23/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> >
> > Could you post the output from
> >
> > str(data)
> >
> > ?
> >
> > Perhaps that will give us a clue.
> >
> > --- amna khan <amnakhan493 at gmail.com> wrote:
> >
> > > Sir the station name "S.Sharif" exists in the data but still the error
> > is
> > > ocurring of being not found.
> > > Please help in this regard.
> > >
> > >
> > > On 7/22/07, Gavin Simpson <gavin.simpson at ucl.ac.uk> wrote:
> > > >
> > > > On Sun, 2007-07-22 at 03:25 -0700, amna khan wrote:
> > > > > Hi Sir
> > > > > I have made a data set having 23 stations of rainfall.
> > > > > when I use the attach function to approach indevidual stations then
> > > > > following error occurr.
> > > > >
> > > > > *>attach(data)*
> > > > > *>S.Sharif    #S.Sharif is the station  name which has 50 data
> > values*
> > > > > *Error: object "S.Sharif" not found*
> > > > > Now how to solve this problem.
> > > >
> > > > Then you don't have a column named exactly "S.Sharif" in your object
> > > > "data".
> > > >
> > > > What does str(data) and names(data) tell you about the columns in
> your
> > > > data set? If looking at these doesn't help you, post the output from
> > > > str(data) and names(data) and someone might be able to help.
> > > >
> > > > You should always check that R has imported the data in the way you
> > > > expect; just because you think there is something in there called
> > > > S.Sharif doesn't mean R sees it that way.
> > > >
> > > > You also seem to have included the R-Help email address twice in the
> > To:
> > > > header of your email - once is sufficient.
> > > >
> > > > G
> > > >
> > > > > Thank You
> > > > > Regards
> > > > >
> > > > --
> > > > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> > > > Gavin Simpson                 [t] +44 (0)20 7679 0522
> > > > ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
> > > > Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
> > > > Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
> > > > UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> > > > %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> > > >
> > > >
> > > >
> > >
> > >
> > > --
> > > AMINA SHAHZADI
> > > Department of Statistics
> > > GC University Lahore, Pakistan.
> > > Email:
> > > amnakhan493 at gmail.com
> > > amna_989 at hotmail.com
> > > amna_989 at yahoo.com
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> >
> >
> >
> >
>
____________________________________________________________________________________

> > news, photos & more.
> > http://mobile.yahoo.com/go?refer=1GNXIC
> >
> 
> 
> 
> -- 
> AMINA SHAHZADI
> Department of Statistics
> GC University Lahore, Pakistan.
> Email:
> amnakhan493 at gmail.com
> amna_989 at hotmail.com

> 



       
____________________________________________________________________________________
Pinpoint customers who are looking for what you sell.


From mcpung at gmail.com  Mon Jul 23 07:20:08 2007
From: mcpung at gmail.com (Murray Pung)
Date: Mon, 23 Jul 2007 15:20:08 +1000
Subject: [R] maths characters in labels & ylab padding
Message-ID: <8d6f66050707222220xb6eb1d6kc2f0d8a21a7e46fc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/4e8c38bc/attachment.pl 

From gattuso2 at obs-vlfr.fr  Mon Jul 23 07:41:30 2007
From: gattuso2 at obs-vlfr.fr (Gattuso, Jean-Pierre)
Date: Mon, 23 Jul 2007 07:41:30 +0200
Subject: [R] Error using Rd2dvi on OSX
Message-ID: <46A43F8A.8020005@obs-vlfr.fr>

Hi,

I run R 2.5.1 on Mac OS 10.4.10 and fail to build a package manual. 
Below is the transcript. The problem is a "pdflatex: command not found" 
error but I think that I have a fully working latex install (installed 
with iInstaller) as I use TeXShop with no problem.

Could someone help?

Jean-Pierre

-----------------------------------------------------------------------------
gattuso@[~/Documents/experiments/seacarb_folder/seacarb]: R CMD Rd2dvi 
--pdf man
Converting Rd files to LaTeX ...
man/K1.Rd
man/K1p.Rd
man/K2.Rd
man/K2p.Rd
man/K3p.Rd
man/Kb.Rd
man/Kf.Rd
man/Kh.Rd
man/Ks.Rd
man/Kspa.Rd
man/Kspc.Rd
man/Kw.Rd
man/bor.Rd
man/carb.Rd
man/phinsi.Rd
man/rho.Rd
man/seacarb_test.Rd
ENCS is
Creating pdf output from LaTeX ...
/Library/Frameworks/R.framework/Resources/bin/Rd2dvi: line 333: 
pdflatex: command not found
lstat(./makeindex) failed ...
./makeindex: No such file or directory
Couldn't find input index file Rd2 nor Rd2.idx.
Usage: makeindex [-ilqrcgLT] [-s sty] [-o ind] [-t log] [-p num] [idx0 
idx1 ...]
/Library/Frameworks/R.framework/Resources/bin/Rd2dvi: line 335: 
pdflatex: command not found
/Library/Frameworks/R.framework/Resources/bin/Rd2dvi: line 337: 
pdflatex: command not found
Saving output to 'Rd2.pdf' ...
cp: .Rd2dvi28208/Rd2.pdf: No such file or directory
Done


From ripley at stats.ox.ac.uk  Mon Jul 23 07:51:38 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jul 2007 06:51:38 +0100 (BST)
Subject: [R] Error using Rd2dvi on OSX
In-Reply-To: <46A43F8A.8020005@obs-vlfr.fr>
References: <46A43F8A.8020005@obs-vlfr.fr>
Message-ID: <Pine.LNX.4.64.0707230646470.15393@gannet.stats.ox.ac.uk>

Please use R-sig-mac for questions about MacOS X.

pdflatex is not in your path, if it is installed.  As makeindex appears to 
be in your path, I suspect that pdflatex is not installed.

R CMD Rd2dvi man

will use latex rather than pdflatex, and will help confirm this.

On Mon, 23 Jul 2007, Gattuso, Jean-Pierre wrote:

> Hi,
>
> I run R 2.5.1 on Mac OS 10.4.10 and fail to build a package manual.
> Below is the transcript. The problem is a "pdflatex: command not found"
> error but I think that I have a fully working latex install (installed
> with iInstaller) as I use TeXShop with no problem.
>
> Could someone help?
>
> Jean-Pierre
>
> -----------------------------------------------------------------------------
> gattuso@[~/Documents/experiments/seacarb_folder/seacarb]: R CMD Rd2dvi
> --pdf man
> Converting Rd files to LaTeX ...
> man/K1.Rd
> man/K1p.Rd
> man/K2.Rd
> man/K2p.Rd
> man/K3p.Rd
> man/Kb.Rd
> man/Kf.Rd
> man/Kh.Rd
> man/Ks.Rd
> man/Kspa.Rd
> man/Kspc.Rd
> man/Kw.Rd
> man/bor.Rd
> man/carb.Rd
> man/phinsi.Rd
> man/rho.Rd
> man/seacarb_test.Rd
> ENCS is
> Creating pdf output from LaTeX ...
> /Library/Frameworks/R.framework/Resources/bin/Rd2dvi: line 333:
> pdflatex: command not found
> lstat(./makeindex) failed ...
> ./makeindex: No such file or directory
> Couldn't find input index file Rd2 nor Rd2.idx.
> Usage: makeindex [-ilqrcgLT] [-s sty] [-o ind] [-t log] [-p num] [idx0
> idx1 ...]
> /Library/Frameworks/R.framework/Resources/bin/Rd2dvi: line 335:
> pdflatex: command not found
> /Library/Frameworks/R.framework/Resources/bin/Rd2dvi: line 337:
> pdflatex: command not found
> Saving output to 'Rd2.pdf' ...
> cp: .Rd2dvi28208/Rd2.pdf: No such file or directory
> Done
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From regina.verghis at gmail.com  Mon Jul 23 08:01:32 2007
From: regina.verghis at gmail.com (Regina Verghis)
Date: Mon, 23 Jul 2007 11:31:32 +0530
Subject: [R] Help in HMM
Message-ID: <49f266b90707222301j1c5cc253ka84756382ce5b968@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/231bf6bc/attachment.pl 

From mmeredith at wcs.org  Mon Jul 23 08:56:04 2007
From: mmeredith at wcs.org (Mike Meredith)
Date: Sun, 22 Jul 2007 23:56:04 -0700 (PDT)
Subject: [R] maths characters in labels & ylab padding
In-Reply-To: <8d6f66050707222220xb6eb1d6kc2f0d8a21a7e46fc@mail.gmail.com>
References: <8d6f66050707222220xb6eb1d6kc2f0d8a21a7e46fc@mail.gmail.com>
Message-ID: <11738746.post@talk.nabble.com>


Help on this is in

?plotmath

You can use 

ylab = expression("Y Label   " (m^2))

Use par(mar..) to increase the margin width, and then mtext to place y axis
label:

y <- 1:10 
x <- rnorm(10,50000,2000) 
par(mar = c(5,6,4,2) + 0.1)
plot(x ~ y, 
   ylab = "", 
   las = 1, 
   type = "n", 
   bty = "n" 
) 
mtext(expression("Y Label   " (m^2)), side = 2, line = 4)

If you use this within a function, you'll probably want to reset the margins
on exit:

old.mar <- par(mar = c(5,6,4,2) + 0.1) ; on.exit(par(old.mar))

HTH, Mike.


Murray Pung wrote:
> 
> I have checked out the help files, but cannot find details on how to use
> maths characters in ylab. Instead of m^2, I would like the 2 in
> superscript,
> if possible. I would also like to place more padding on the label so that
> the label is not obscured by the horizontal numbers.
> 
> 
> y <- 1:10
> x <- rnorm(10,50000,2000)
> plot(x ~ y,
>     ylab = 'Y Label (m^2)',
>     las = 1,
>     type = "n",
>     bty = "n"
>     )
> 
> 
> Thanks
> Murray
> 
> -- 
> Murray Pung
> Statistician, Datapharm Australia Pty Ltd
> 0404 273 283
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/maths-characters-in-labels---ylab-padding-tf4127592.html#a11738746
Sent from the R help mailing list archive at Nabble.com.


From ligges at statistik.uni-dortmund.de  Mon Jul 23 09:23:03 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jul 2007 09:23:03 +0200
Subject: [R] R and mysql
In-Reply-To: <971536df0707221954g5df1c378qf4891462087fdb2c@mail.gmail.com>
References: <21add9100707221919o639139ecpfbaf34d4017ccce4@mail.gmail.com>
	<971536df0707221954g5df1c378qf4891462087fdb2c@mail.gmail.com>
Message-ID: <46A45757.8050002@statistik.uni-dortmund.de>



Gabor Grothendieck wrote:
> To run with R 2.5.1 you need the Windows binary of RMySQL 0.6-0 but
> I don't think that that is available on either CRAN or BioConductor currently.

But on the "CRAN extras" repository which is provided by Brian Ripley. 
You can even select it from the menu (and it should be the default).
Hence install.packages("RMySQL") shoudl work.

Uwe Ligges



> however, one does exist and I suggest you contact the
> maintainer, David James or the r-sig-db list.  If those don't get it for you
> send me an email off list and I will send you mine.
> 
> I have only used the Windows version on XP so I can't say whether there
> would be any problems on W2k.
> 
> On 7/22/07, along zeng <xh.along at gmail.com> wrote:
>> Hi all,
>>     I want to visit MySQL in R use RMySQL package on Windows 2k.I got
>> RMySQL and DBI packages from a cran,but only the package  RMySQL  is
>> built on MasOS.I extracted both and put them under the folder of
>> "library",started R and typed string as ,
>>> require(RMySQL);
>> R told me DBI was loaded,but RMySQL  was't loaded because R not found it.
>>
>>
>> Thank very much!
>>  along
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From claudia.d'aniello at banca.mps.it  Mon Jul 23 09:28:02 2007
From: claudia.d'aniello at banca.mps.it (D ANIELLO CLAUDIA (MPS - 05966))
Date: Mon, 23 Jul 2007 09:28:02 +0200
Subject: [R] R and Excel
Message-ID: <B72F490BE496694FA85E92806C2AED55A7B516@se000010010045.servinternet.local>

? stato filtrato un testo allegato il cui set di caratteri non era
indicato...
Nome: non disponibile
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/2d591177/attachment.pl 

From gavin.simpson at ucl.ac.uk  Mon Jul 23 09:32:56 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 23 Jul 2007 08:32:56 +0100
Subject: [R] Data Set
In-Reply-To: <669528.69507.qm@web39714.mail.mud.yahoo.com>
References: <669528.69507.qm@web39714.mail.mud.yahoo.com>
Message-ID: <1185175976.2890.9.camel@graptoleberis.geog.ucl.ac.uk>

On Sun, 2007-07-22 at 21:51 -0700, Stephen Tucker wrote:
> It turns out that "-" and " " (space) are not valid variable names. 

They are valid names, the problem is that they aren't very convenient to
use, as the OP discovered, because they need to be quoted.

Note that if using something like read.csv or read.table, R will correct
these problem variable names for you when you import the data. If you
read this file in for example:

"Mydata","S-sharif","A site"
1,45,34
2,66,45
3,79,56

using read.csv, you get easy to use names

> dat <- read.csv("temp.csv")
> dat
  Mydata S.sharif A.site
1      1       45     34
2      2       66     45
3      3       79     56

You can turn off this safety checking using the argument check.names =
FALSE

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From bonfigli at inmi.it  Mon Jul 23 10:48:47 2007
From: bonfigli at inmi.it (Bonfigli Sandro)
Date: Mon, 23 Jul 2007 10:48:47 +0200
Subject: [R] R and Excel
In-Reply-To: <B72F490BE496694FA85E92806C2AED55A7B516@se000010010045.servinternet.local>
References: <B72F490BE496694FA85E92806C2AED55A7B516@se000010010045.servinternet.local>
Message-ID: <WorldClient-F200707231048.AA48470483@inmi.it>

> Dear all,
> Someone knows what's the command in R corresponding to the "vlookup" in
> Excel?
> Thank you in advance.
> Claudia
> ______________________________________________
> R-help a stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

As far as I discovered from a quick google query the Excel
command you cite selects some values from a table according 
to some conditions.
So if you have a data frame called example you could do 
something like 
example[example$var1>5,] 
Perhaps you need something like
example[example$var1>5,2:5]
or 
example[example$var1>5,]$var2.

Anyway you should follow the advice you'll find at the end of 
the message:
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
... or at least a good example of what you would like to obtain!

You'll find a lot of informations on the manipulation of data frames
in the manuals.
go to http://www.r-project.org/, select "manuals", go to the 
"Contributed Documentation". You'll find various manuals,
also ones in italian.

Ciao
  Sandro


From arigado0315 at yahoo.com.tw  Mon Jul 23 11:17:56 2007
From: arigado0315 at yahoo.com.tw (arigado)
Date: Mon, 23 Jul 2007 02:17:56 -0700 (PDT)
Subject: [R] About infinite value
Message-ID: <11740491.post@talk.nabble.com>


Hi everyone

I have a problem about "infinite".
If I type 10^308, R shows "1e+308"
When I type 10^309, R shows "Inf"
So, we know if a value is large than 1.XXXe+308, R will show "Inf"
How can i do let the value, like "10^400" ,typed in R to show the word
"1e+400" not "Inf"

-- 
View this message in context: http://www.nabble.com/About-infinite-value-tf4128557.html#a11740491
Sent from the R help mailing list archive at Nabble.com.


From ba208 at exeter.ac.uk  Mon Jul 23 11:29:09 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Mon, 23 Jul 2007 10:29:09 +0100
Subject: [R] code optimization tips
Message-ID: <D4E253DF-0B41-4CF5-BDAE-5009EDC88D53@ex.ac.uk>

Hi,

Being new to R I'm asking for some advice on how to optimize the  
performance of the following piece of code:


> alpha_c <- function(lambda=600e-9,alpha_s=1e-14,N=400,spacing=1e-7){
>
> k<-2*pi/lambda
> ri<-c(0,0) # particle at the origin
> x<-c(-N:N)
> positions <- function(N) {
>    reps <- 2*N+1
>    matrix(c(rep(-N:N, each = reps), rep(-N:N, times = reps)),
>           nrow = 2, byrow = TRUE)
> }
> rj<-positions(N)*spacing # all positions in the 2N x 2N array
> rj<-rj[1:2,-((dim(rj)[2]-1)/2+1)] # remove rj=(0,0)
>
> mod<-function(x){sqrt(x[1]^2+x[2]^2)} # modulus
>
> sij <-function(rj){
> rij=mod(rj-ri)
> cos_ij=rj[1]/rij
> sin_ij=rj[2]/rij
>
> A<-(1-1i*k*rij)*(3*cos_ij^2-1)*exp(1i*k*rij)/(rij^3)
> B<-k^2*sin_ij^2*exp(1i*k*rij)/rij
>
> sij<-A+B
> }
>
> s_ij<-apply(rj,2,sij)
> S<-sum(s_ij)
> alpha_s/(1-alpha_s*S)
> }
> alpha_c()


This function is to be called for a few tens of values of lambda in a  
'for' loop, and possibly a couple of different N and spacing (their  
magnitude is typically around the default one).

This can be a bit slow ??? not that I would expect otherwise --- and  
I wonder if there is something I could do to optimize it (vectorize  
with respect to the lambda parameter?, change the units of the  
problem to deal with numbers closer to unity?,...)

Best regards,

baptiste


From mark at wardle.org  Mon Jul 23 11:34:56 2007
From: mark at wardle.org (Mark Wardle)
Date: Mon, 23 Jul 2007 10:34:56 +0100
Subject: [R] R and Excel
In-Reply-To: <B72F490BE496694FA85E92806C2AED55A7B516@se000010010045.servinternet.local>
References: <B72F490BE496694FA85E92806C2AED55A7B516@se000010010045.servinternet.local>
Message-ID: <b59a37130707230234g6e316bb1ocb35ce40a0f27287@mail.gmail.com>

On 23/07/07, D ANIELLO CLAUDIA (MPS - 05966)
<claudia.d'aniello at banca.mps.it> wrote:
> Dear all,
> Someone knows what's the command in R corresponding to the "vlookup" in
> Excel?
> Thank you in advance.
> Claudia
>

Dear Claudia,

Depending on the dataset, it is possible to achieve the same results
by using merge(). It is a different way of thinking, but can
effectively "lookup" corresponding values in one data frame into
another.

If you give a sample of data then I could be a bit more specific!

Best wishes,

Mark

-- 
Dr. Mark Wardle
Clinical research fellow and specialist registrar, Neurology
Cardiff, UK


From P.Dalgaard at biostat.ku.dk  Mon Jul 23 12:06:23 2007
From: P.Dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Mon, 23 Jul 2007 12:06:23 +0200
Subject: [R] About infinite value
In-Reply-To: <11740491.post@talk.nabble.com>
References: <11740491.post@talk.nabble.com>
Message-ID: <46A47D9F.4000309@biostat.ku.dk>

arigado wrote:
> Hi everyone
>
> I have a problem about "infinite".
> If I type 10^308, R shows "1e+308"
> When I type 10^309, R shows "Inf"
> So, we know if a value is large than 1.XXXe+308, R will show "Inf"
> How can i do let the value, like "10^400" ,typed in R to show the word
> "1e+400" not "Inf"
>
>   
1. You can't, due to the computer representation of floating point numbers.

2. Package brobdingnag lets you do it anyway.

-- 
   O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907


From bmeyer at psychologie.hu-berlin.de  Mon Jul 23 12:14:45 2007
From: bmeyer at psychologie.hu-berlin.de (Bertolt Meyer)
Date: Mon, 23 Jul 2007 12:14:45 +0200
Subject: [R] Multilevel package: Obtaining significance for waba
 within-group correlation?
Message-ID: <46A47F95.8010901@psychologie.hu-berlin.de>

Hello everyone,

I am employing the waba method from the multilevel package for obtaining 
a within-group correlation (Description: 
http://bg9.imslab.co.jp/Rhelp/R-2.4.0/src/library/multilevel/man/waba.html). 
Does anybody know a way or a calculation for obtaining a significance 
value for that correlation?

And another question: Does anybody know whether it is possible to save 
individual group-mean-centered values into a new variable?

I am quite new to R so please do apologize if either of these questions 
has an obvious answer that I didn't get.

Example:

 > package(multilevel)
 > data(bh1996)

 > waba(bh1996$HRS,bh1996$WBEING,bh1996$GRP)
[I would like to obtain the significance of the CorrW value]

Thanks for your help,
Bertolt

-- 
Bertolt Meyer
PhD Student
Institute of Psychology & Institute of Information Systems
Humboldt University Berlin

bmeyer at psychologie.hu-berlin.de

phone: +49-30-20939347
mobile: +49-179-4991046

web1: http://ioe-skillmap.hu-berlin.de
web2: http://amor.rz.hu-berlin.de/~h04440am


From brown_emu at yahoo.com  Mon Jul 23 12:56:29 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Mon, 23 Jul 2007 03:56:29 -0700 (PDT)
Subject: [R] Data Set
In-Reply-To: <1185175976.2890.9.camel@graptoleberis.geog.ucl.ac.uk>
Message-ID: <906030.15478.qm@web39713.mail.mud.yahoo.com>

My bad... corrections (semantic and otherwise) always appreciated. I'm still
learning too.

I also forgot the alternative of using make.names() instead of manually
assigning 'more convenient' names.

input <- 
"Mydata,S-sharif,A site
1,45,34
2,66,45
3,79,56"

> dat <- read.csv(textConnection(input),check.names=FALSE)
> dat
  Mydata S-sharif A site
1      1       45     34
2      2       66     45
3      3       79     56
> names(dat)
[1] "Mydata"   "S-sharif" "A site"  
> names(dat) <- make.names(names(dat))
> names(dat)
[1] "Mydata"   "S.sharif" "A.site"  

Which, in the case of the data set, Monsoon, I don't know how it was created
originally but may be convenient to reassign names by

  names(Monsoon) <- make.names(names(Monsoon))



--- Gavin Simpson <gavin.simpson at ucl.ac.uk> wrote:

> On Sun, 2007-07-22 at 21:51 -0700, Stephen Tucker wrote:
> > It turns out that "-" and " " (space) are not valid variable names. 
> 
> They are valid names, the problem is that they aren't very convenient to
> use, as the OP discovered, because they need to be quoted.
> 
> Note that if using something like read.csv or read.table, R will correct
> these problem variable names for you when you import the data. If you
> read this file in for example:
> 
> "Mydata","S-sharif","A site"
> 1,45,34
> 2,66,45
> 3,79,56
> 
> using read.csv, you get easy to use names
> 
> > dat <- read.csv("temp.csv")
> > dat
>   Mydata S.sharif A.site
> 1      1       45     34
> 2      2       66     45
> 3      3       79     56
> 
> You can turn off this safety checking using the argument check.names =
> FALSE
> 
> G
> 
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
>  Gavin Simpson                 [t] +44 (0)20 7679 0522
>  ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
>  Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
>  Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
>  UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
> 
> 
>


From ligges at statistik.uni-dortmund.de  Mon Jul 23 12:59:44 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Jul 2007 12:59:44 +0200
Subject: [R] R2WinBUGS awkward to use
In-Reply-To: <f7rmnr$31m$1@sea.gmane.org>
References: <f7rmnr$31m$1@sea.gmane.org>
Message-ID: <46A48A20.3030301@statistik.uni-dortmund.de>



toby909 at gmail.com wrote:
> Hi All
> 
> Does anyone know if I can avoid to use the write.model() function below? I dont 
> want to do this. Can't bugs() do that automatically for me just by specifying 
> the 4th argument 'model'? Just I like I am also using the 'inits' object!
> 
> If I use 'model' in the same way as I use 'inits' I am getting the error:
> 
>  > sim <- bugs(data, inits, parameters, model, n.chains=1, n.iter=5000, 
> bugs.directory="c:/Program Files/WinBUGS14", working.directory=NULL, 
> clearWD=TRUE, DIC=0)
> Error in file.exists(c(...)) : invalid 'file' argument


The idea is that people in general write their model files more or less 
while interactively using WinBUGS / OpenBUGS and later on start 
automizing with R2WinBUGS or BRugs. Then the file already exists.
Of course, we can add dome fancy stuff that applies write.model if an 
expression is given in the argument "model".

I'll put it on the ToDo list.

Best,
Uwe Ligges


> 
> Thanks Toby
> 
> 
> 
> 
> 
> 
> 
> data <- list(.....)
> 
> 
> 
> model <- function() {
> 
> [omitted]
> 
> }
> 
> write.model(model, "cwk.txt")
> 
> inits <- function() {.....}
> 
> parameters <- c("b", "nu", "S1", "S2")
> 
> sim <- bugs(data, inits, parameters, "cwk.txt", n.chains=1, n.iter=5000, 
> bugs.directory="c:/Program Files/WinBUGS14", working.directory=NULL, 
> clearWD=TRUE, DIC=0)
> 
> print(sim)
> plot(sim)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sunithak at cdac.in  Mon Jul 23 13:54:18 2007
From: sunithak at cdac.in (Sunithak)
Date: Mon, 23 Jul 2007 17:24:18 +0530
Subject: [R] Rmpi installation error
Message-ID: <BCEOIKLKMHDDGHIPNNIMCEBFCBAA.sunithak@cdac.in>

Hello everybody,
I am trying to install Rmpi on 64-bit HP-Proliant server. I am facing the
following error:


* Installing *source* package 'Rmpi' ...
Try to find mpi.h ...
Found in /usr/local/mpich-1.2.7p1/include
Try to find libmpi or libmpich ...
Found libmpich in /usr/local/mpich-1.2.7p1/lib
Try to find liblam ...
checking for main in -llam... no
liblam not found. Probably not LAM-MPI
checking for openpty in -lutil... no
checking for main in -lpthread... no
configure: creating ./config.status
config.status: creating src/Makevars
** libs
gcc -I/usr/local/lib64/R/include -I/usr/local/lib64/R/include -DPACKAGE_NAME
=\"\" -DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -D
PACKAGE_BUGREPORT=\"\"  -I/usr/local/mpich-1.2.7p1/include -DMPI2 -I/usr/loc
al/include   -fpic  -g -O2 -std=gnu99 -c conversion.c -o conversion.o
gcc -I/usr/local/lib64/R/include -I/usr/local/lib64/R/include -DPACKAGE_NAME
=\"\" -DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -D
PACKAGE_BUGREPORT=\"\"  -I/usr/local/mpich-1.2.7p1/include -DMPI2 -I/usr/loc
al/include   -fpic  -g -O2 -std=gnu99 -c internal.c -o internal.o
gcc -I/usr/local/lib64/R/include -I/usr/local/lib64/R/include -DPACKAGE_NAME
=\"\" -DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -D
PACKAGE_BUGREPORT=\"\"  -I/usr/local/mpich-1.2.7p1/include -DMPI2 -I/usr/loc
al/include   -fpic  -g -O2 -std=gnu99 -c RegQuery.c -o RegQuery.o
gcc -I/usr/local/lib64/R/include -I/usr/local/lib64/R/include -DPACKAGE_NAME
=\"\" -DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -D
PACKAGE_BUGREPORT=\"\"  -I/usr/local/mpich-1.2.7p1/include -DMPI2 -I/usr/loc
al/include   -fpic  -g -O2 -std=gnu99 -c Rmpi.c -o Rmpi.o
Rmpi.c: In function `mpi_universe_size':
Rmpi.c:84: warning: implicit declaration of function `MPI_Comm_get_attr'
Rmpi.c:84: error: `MPI_UNIVERSE_SIZE' undeclared (first use in this
function)
Rmpi.c:84: error: (Each undeclared identifier is reported only once
Rmpi.c:84: error: for each function it appears in.)
Rmpi.c: In function `mpi_comm_spawn':
Rmpi.c:873: warning: implicit declaration of function `MPI_Comm_spawn'
Rmpi.c:873: error: `MPI_ARGV_NULL' undeclared (first use in this function)
Rmpi.c: In function `mpi_comm_get_parent':
Rmpi.c:897: warning: implicit declaration of function `MPI_Comm_get_parent'
Rmpi.c: In function `mpi_comm_disconnect':
Rmpi.c:910: warning: implicit declaration of function `MPI_Comm_disconnect'
make: *** [Rmpi.o] Error 1
chmod: cannot access `/usr/local/lib64/R/library/Rmpi/libs/*': No such file
or directory
ERROR: compilation failed for package 'Rmpi'
** Removing '/usr/local/lib64/R/library/Rmpi'


can any of you please suggest as to what needs to be done?

Thanks for your time
regards
Sunitha Manjari
Bioinformatics Team
C-DAC, Pune-411007
Maharashtra
INDIA


From maechler at stat.math.ethz.ch  Mon Jul 23 14:24:01 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 23 Jul 2007 14:24:01 +0200
Subject: [R] Package design, placement of legacy functions
In-Reply-To: <4EDAB73D-63B2-42FF-8B52-822605511203@austin.rr.com>
References: <4EDAB73D-63B2-42FF-8B52-822605511203@austin.rr.com>
Message-ID: <18084.40417.451215.384232@stat.math.ethz.ch>

>>>>> "WA" == William Asquith <wasquith at austin.rr.com>
>>>>>     on Sun, 22 Jul 2007 13:29:24 -0500 writes:

    WA> I have a function XOLD() from a nearly verbatim port of legacy  
    WA> FORTRAN in a package. I have remplemented this function as XNEW()  
    WA> using much cleaner native R and built-in functions of R. I have  
    WA> switched the package to the XNEW(), but for historical reasons would  
    WA> like to retain the XOLD() somewhere in the package directory  
    WA> structure. An assertion through a README or other will point to this  
    WA> historical function and the output from the two should be numerically  
    WA> equal.

    WA> Placement in package/R is not an option as XOLD() no longer  
    WA> constitutes a true user level function, would package/inst/legacyR or  
    WA> something like that be suitable to the R community?

Yes, put it somewhere inside <pkg>/inst/
(and end the filename in *.R).

A user of your **installed** package will be able
to use
	system.file("legacy.R", package = "<pkg")
e.g.,
as	source(system.file("legacy.R", package = "<pkg"))

    WA> Thanks for the guidance. . .

you're welcome.
Martin Maechler, ETH Zurich


From Mike.Lawrence at DAL.CA  Mon Jul 23 14:30:01 2007
From: Mike.Lawrence at DAL.CA (Mike Lawrence)
Date: Mon, 23 Jul 2007 09:30:01 -0300
Subject: [R] using R for a reaction-time experiment
In-Reply-To: <11732474.post@talk.nabble.com>
References: <11732474.post@talk.nabble.com>
Message-ID: <95FAB3DD-3DD4-4947-AE2A-D079D4553CD6@DAL.CA>

Psych grad student here, R user for 3 years. Using R for experiments  
is likely not advisable as it has no fine control of display timing  
(ex synching stimuli to the screen refresh, etc). On recommendation  
of colleagues I'm learning the Python language, which has a module  
called 'pygame' that is fantastic for psych experiments.

Mike


On 22-Jul-07, at 1:38 PM, Seth Roberts wrote:

>
> I want to use R to run a reaction-time experiment: Something  
> appears on the
> screen, I respond by typing something (one keystroke), the system  
> measures
> the speed of my response. R would be great for this if only I  
> didn't have to
> hit Enter to enter that keystroke. I am doing such experiments now  
> but they
> require two actions per trial: hit keystroke, hit Enter.
>
> Is there some way that R can be made to respond to a single  
> keystroke (other
> than Enter)?
> -- 
> View this message in context: http://www.nabble.com/using-R-for-a- 
> reaction-time-experiment-tf4125643.html#a11732474
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Mike Lawrence
Graduate Student, Department of Psychology, Dalhousie University

Website: http://memetic.ca

Public calendar: http://icalx.com/public/informavore/Public

"The road to wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
	- Piet Hein


From bbernzwe at bear.com  Mon Jul 23 14:36:55 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Mon, 23 Jul 2007 08:36:55 -0400
Subject: [R] tagging results of "apply"
References: <Q1BYWkxDSSFNOTkqIC8pNDM0NTg0NDUz@sspc-lisa>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B64@whexchmb16.bsna.bsroot.bear.com>
	<971536df0707220423p6d4ee73q5d3bdf34ee4964fe@mail.gmail.com>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B69@whexchmb16.bsna.bsroot.bear.com>

Thanks!  I'll take a look at this.

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Sunday, July 22, 2007 7:24 AM
To: Bernzweig, Bruce (Consultant)
Cc: r-help
Subject: Re: [R] tagging results of "apply"

You don't need apply at all here.  cor can already do that and it
automatically labels the rows and columns too.  Using the builtin
dataset anscombe whose first 4 columns are labelled x1,x2,x3,x4
and whose next 4 columns are labelled y1,y2,y3,y4 we have:

> cor(anscombe[1:4], anscombe[5:8])
           y1         y2         y3         y4
x1  0.8164205  0.8162365  0.8162867 -0.3140467
x2  0.8164205  0.8162365  0.8162867 -0.3140467
x3  0.8164205  0.8162365  0.8162867 -0.3140467
x4 -0.5290927 -0.7184365 -0.3446610  0.8165214

cor works the same with matrices too.


On 7/20/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> In trying to get a better understanding of vectorization I wrote the
> following code:
>
> My objective is to take two sets of time series and calculate the
> correlations for each combination of time series.
>
> mat1 <- matrix(sample(1:500, 25), ncol = 5)
> mat2 <- matrix(sample(501:1000, 25), ncol = 5)
>
> Scenario 1:
> apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
>
> Scenario 2:
> apply(mat1, 1, function(x) cor(mat1, mat2))
>
> Using scenario 1, (output below) I can see that correlations are
> calculated for just the first row of mat2 against each individual row
of
> mat1.
>
> Using scenario 2, (output below) I can see that correlations are
> calculated for each row of mat2 against each individual row of mat1.
>
> Q1: The output of scenario2 consists of 25 rows of data.  Are the
first
> five rows mat1 against mat2[1,], the next five rows mat1 against
> mat2[2,], ... last five rows mat1 against mat2[5,]?
>
> Q2: I assign the output of scenario 2 to a new matrix
>
>        matC <- apply(mat1, 1, function(x) cor(mat1, mat2))
>
>    However, I need a way to identify each row in matC as a pairing of
> rows from mat1 and mat2.  Is there a parameter I can add to apply to
do
> this?
>
> Scenario 1:
> > apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
>           [,1]       [,2]       [,3]       [,4]       [,5]
> [1,] -0.4626122 -0.4626122 -0.4626122 -0.4626122 -0.4626122
> [2,] -0.9031543 -0.9031543 -0.9031543 -0.9031543 -0.9031543
> [3,]  0.0735273  0.0735273  0.0735273  0.0735273  0.0735273
> [4,]  0.7401259  0.7401259  0.7401259  0.7401259  0.7401259
> [5,] -0.4548582 -0.4548582 -0.4548582 -0.4548582 -0.4548582
>
> Scenario 2:
> > apply(mat1, 1, function(x) cor(mat1, mat2))
>             [,1]        [,2]        [,3]        [,4]        [,5]
>  [1,]  0.19394126  0.19394126  0.19394126  0.19394126  0.19394126
>  [2,]  0.26402400  0.26402400  0.26402400  0.26402400  0.26402400
>  [3,]  0.12923842  0.12923842  0.12923842  0.12923842  0.12923842
>  [4,] -0.74549676 -0.74549676 -0.74549676 -0.74549676 -0.74549676
>  [5,]  0.64074122  0.64074122  0.64074122  0.64074122  0.64074122
>  [6,]  0.26931986  0.26931986  0.26931986  0.26931986  0.26931986
>  [7,]  0.08527921  0.08527921  0.08527921  0.08527921  0.08527921
>  [8,] -0.28034079 -0.28034079 -0.28034079 -0.28034079 -0.28034079
>  [9,] -0.15251915 -0.15251915 -0.15251915 -0.15251915 -0.15251915
> [10,]  0.19542415  0.19542415  0.19542415  0.19542415  0.19542415
> [11,]  0.75107032  0.75107032  0.75107032  0.75107032  0.75107032
> [12,]  0.53042767  0.53042767  0.53042767  0.53042767  0.53042767
> [13,] -0.51163612 -0.51163612 -0.51163612 -0.51163612 -0.51163612
> [14,] -0.44396048 -0.44396048 -0.44396048 -0.44396048 -0.44396048
> [15,]  0.57018745  0.57018745  0.57018745  0.57018745  0.57018745
> [16,]  0.70480284  0.70480284  0.70480284  0.70480284  0.70480284
> [17,] -0.36674283 -0.36674283 -0.36674283 -0.36674283 -0.36674283
> [18,] -0.81826607 -0.81826607 -0.81826607 -0.81826607 -0.81826607
> [19,]  0.53145184  0.53145184  0.53145184  0.53145184  0.53145184
> [20,]  0.24568385  0.24568385  0.24568385  0.24568385  0.24568385
> [21,] -0.10610402 -0.10610402 -0.10610402 -0.10610402 -0.10610402
> [22,] -0.78650748 -0.78650748 -0.78650748 -0.78650748 -0.78650748
> [23,]  0.04269423  0.04269423  0.04269423  0.04269423  0.04269423
> [24,]  0.14704698  0.14704698  0.14704698  0.14704698  0.14704698
> [25,]  0.28340166  0.28340166  0.28340166  0.28340166  0.28340166
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the
pers...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>




**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From bbernzwe at bear.com  Mon Jul 23 14:38:01 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Mon, 23 Jul 2007 08:38:01 -0400
Subject: [R] tagging results of "apply"
References: <847654.24028.qm@web39706.mail.mud.yahoo.com>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B6A@whexchmb16.bsna.bsroot.bear.com>

Thanks for the clarification and help!

-----Original Message-----
From: Stephen Tucker [mailto:brown_emu at yahoo.com] 
Sent: Sunday, July 22, 2007 6:08 AM
To: Bernzweig, Bruce (Consultant); r-help
Subject: Re: [R] tagging results of "apply"

Actually if you want to tag both column and row, this might also help:

## Give dimension labels to both matrices
mat1 <- matrix(sample(1:500, 25), ncol = 5,
               dimnames=list(paste("mat1row",1:5,sep=""),
                 paste("mat1col",1:5,sep="")))
mat2 <- matrix(sample(501:1000, 25), ncol = 5,
               dimnames=list(paste("mat2row",1:5,sep=""),
                 paste("mat2col",1:5,sep="")))

cor(mat1[1,],mat2)
        mat2col1   mat2col2   mat2col3  mat2col4     mat2col5
[1,] -0.06313535 -0.4679927 -0.5147084 -0.797748 -0.001457972

The column labels are there but are lost when returned from apply(), as
it
says in ?apply:

"In all cases the result is coerced by as.vector to one of the basic
vector
types before the dimensions are set"

> as.vector(cor(mat1[1,],mat2))
[1] -0.063135353 -0.467992672 -0.514708392 -0.797748010 -0.001457972

You lose the dimension labels in this case, so one option is to guard
against
this in the following way:

> as.vector(as.data.frame(cor(mat1[1,],mat2)))
     mat2col1   mat2col2   mat2col3  mat2col4     mat2col5
1 -0.06313535 -0.4679927 -0.5147084 -0.797748 -0.001457972

Unfortunately, if you use 'as.data.frame()' in 'function(x)', apply will
return a list - but you can bind the rows of the output:

> f <- function(x,y) as.data.frame(cor(x,y))
> do.call(rbind, apply(mat1,1,f,y=mat2))
            mat2col1   mat2col2    mat2col3   mat2col4     mat2col5
mat1row1 -0.06313535 -0.4679927 -0.51470839 -0.7977480 -0.001457972
mat1row2 -0.28750363  0.1681777  0.14671484  0.8139768  0.039982028
mat1row3 -0.62017387 -0.6932731 -0.72263865 -0.7929604  0.427366680
mat1row4  0.06441894  0.1707946 -0.11444747 -0.8213577  0.526239013
mat1row5 -0.09849051  0.7024540 -0.01997228  0.3712480  0.439037838

The result is a data frame, not a matrix, and note that the columns/rows
are
transposed in relation to the output of
  apply(mat1,1,f,y=mat2)

An alternative is to convert each row of mat1 into a list element [by
transposing it with t() and then feeding it to as.data.frame()] and then
use
sapply():

> sapply(as.data.frame(t(mat1)),f,y=mat2)
         mat1row1     mat1row2   mat1row3   mat1row4   mat1row5   
mat2col1 -0.06313535  -0.2875036 -0.6201739 0.06441894 -0.0984905 
mat2col2 -0.4679927   0.1681777  -0.6932731 0.1707946  0.702454   
mat2col3 -0.5147084   0.1467148  -0.7226387 -0.1144475 -0.01997228
mat2col4 -0.797748    0.8139768  -0.7929604 -0.8213577 0.371248   
mat2col5 -0.001457972 0.03998203 0.4273667  0.526239   0.4390378



--- Stephen Tucker <brown_emu at yahoo.com> wrote:

> Dear Bruce,
> In your functions, you need to use your bound variable, 'x' [not mat1]
in
> your anonymous function [function(x)] as the argument to cor().
> 
> For instance, you wrote:
> apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
> apply(mat1, 1, function(x) cor(mat1, mat2))
> 
> They should be
> apply(mat1, 1, function(x) cor(x, mat2[1,]))
> apply(mat1, 1, function(x) cor(x, mat2))
> 
> or
> f <- function(x,y) cor(x, y)
> apply(mat1, 1, f, y=mat2[1,])
> apply(mat1, 1, f, y=mat2)
> 
> Then from the ?apply documentation - under section, 'Value' - the
following
> statement will help you predict its behavior in this case:
> "If each call to FUN returns a vector of length n, then apply returns
an
> array of dimension c(n, dim(X)[MARGIN]) if n > 1."
> 
> [each column of your output is the output from cor(mat1[i,],mat2) in
> Scenario
> 2]. As for tagging, you can try adding dimension labels [to the object
> which
> is passed as the 'X' argument to apply()]:
> 
> mat1 <- matrix(sample(1:500, 25), ncol = 5,
>                dimnames=list(paste("row",1:5,sep=""),
>                  paste("col",1:5,sep="")))
> mat2 <- matrix(sample(501:1000, 25), ncol = 5)
> 
> > apply(mat1, 1, function(x,y) cor(x, y), y=mat2)
>             row1       row2       row3        row4        row5
> [1,]  0.39412464 -0.6241649  0.7423724  0.48391875  0.27085386
> [2,] -0.22912466 -0.4123714  0.2857004 -0.52447327  0.06971423
> [3,] -0.51027247  0.3256587 -0.6195050 -0.48309737  0.01699978
> [4,]  0.26353316 -0.1873564  0.2121154  0.88784766 -0.02257890
> [5,] -0.03771225 -0.4250040  0.3795558 -0.03372794 -0.05874675
> 
> Hope this helps,
> 
> Stephen
> 
> --- "Bernzweig, Bruce (Consultant)" <bbernzwe at bear.com> wrote:
> 
> > In trying to get a better understanding of vectorization I wrote the
> > following code:
> > 
> > My objective is to take two sets of time series and calculate the
> > correlations for each combination of time series.
> > 
> > mat1 <- matrix(sample(1:500, 25), ncol = 5)
> > mat2 <- matrix(sample(501:1000, 25), ncol = 5)
> > 
> > Scenario 1:
> > apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
> > 
> > Scenario 2:
> > apply(mat1, 1, function(x) cor(mat1, mat2))
> > 
> > Using scenario 1, (output below) I can see that correlations are
> > calculated for just the first row of mat2 against each individual
row of
> > mat1.
> > 
> > Using scenario 2, (output below) I can see that correlations are
> > calculated for each row of mat2 against each individual row of mat1.

> > 
> > Q1: The output of scenario2 consists of 25 rows of data.  Are the
first
> > five rows mat1 against mat2[1,], the next five rows mat1 against
> > mat2[2,], ... last five rows mat1 against mat2[5,]?
> > 
> > Q2: I assign the output of scenario 2 to a new matrix
> > 
> > 	matC <- apply(mat1, 1, function(x) cor(mat1, mat2))
> > 
> >     However, I need a way to identify each row in matC as a pairing
of
> > rows from mat1 and mat2.  Is there a parameter I can add to apply to
do
> > this?
> > 
> > Scenario 1:
> > > apply(mat1, 1, function(x) cor(mat1, mat2[1,]))
> >            [,1]       [,2]       [,3]       [,4]       [,5]
> > [1,] -0.4626122 -0.4626122 -0.4626122 -0.4626122 -0.4626122
> > [2,] -0.9031543 -0.9031543 -0.9031543 -0.9031543 -0.9031543
> > [3,]  0.0735273  0.0735273  0.0735273  0.0735273  0.0735273
> > [4,]  0.7401259  0.7401259  0.7401259  0.7401259  0.7401259
> > [5,] -0.4548582 -0.4548582 -0.4548582 -0.4548582 -0.4548582
> > 
> > Scenario 2:
> > > apply(mat1, 1, function(x) cor(mat1, mat2))
> >              [,1]        [,2]        [,3]        [,4]        [,5]
> >  [1,]  0.19394126  0.19394126  0.19394126  0.19394126  0.19394126
> >  [2,]  0.26402400  0.26402400  0.26402400  0.26402400  0.26402400
> >  [3,]  0.12923842  0.12923842  0.12923842  0.12923842  0.12923842
> >  [4,] -0.74549676 -0.74549676 -0.74549676 -0.74549676 -0.74549676
> >  [5,]  0.64074122  0.64074122  0.64074122  0.64074122  0.64074122
> >  [6,]  0.26931986  0.26931986  0.26931986  0.26931986  0.26931986
> >  [7,]  0.08527921  0.08527921  0.08527921  0.08527921  0.08527921
> >  [8,] -0.28034079 -0.28034079 -0.28034079 -0.28034079 -0.28034079
> >  [9,] -0.15251915 -0.15251915 -0.15251915 -0.15251915 -0.15251915
> > [10,]  0.19542415  0.19542415  0.19542415  0.19542415  0.19542415
> > [11,]  0.75107032  0.75107032  0.75107032  0.75107032  0.75107032
> > [12,]  0.53042767  0.53042767  0.53042767  0.53042767  0.53042767
> > [13,] -0.51163612 -0.51163612 -0.51163612 -0.51163612 -0.51163612
> > [14,] -0.44396048 -0.44396048 -0.44396048 -0.44396048 -0.44396048
> > [15,]  0.57018745  0.57018745  0.57018745  0.57018745  0.57018745
> > [16,]  0.70480284  0.70480284  0.70480284  0.70480284  0.70480284
> > [17,] -0.36674283 -0.36674283 -0.36674283 -0.36674283 -0.36674283
> > [18,] -0.81826607 -0.81826607 -0.81826607 -0.81826607 -0.81826607
> > [19,]  0.53145184  0.53145184  0.53145184  0.53145184  0.53145184
> > [20,]  0.24568385  0.24568385  0.24568385  0.24568385  0.24568385
> > [21,] -0.10610402 -0.10610402 -0.10610402 -0.10610402 -0.10610402
> > [22,] -0.78650748 -0.78650748 -0.78650748 -0.78650748 -0.78650748
> > [23,]  0.04269423  0.04269423  0.04269423  0.04269423  0.04269423
> > [24,]  0.14704698  0.14704698  0.14704698  0.14704698  0.14704698
> > [25,]  0.28340166  0.28340166  0.28340166  0.28340166  0.28340166
> > 
> > 
> > 
> >
**********************************************************************
> > Please be aware that, notwithstanding the fact that the
> pers...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



       
________________________________________________________________________
____________
Get the Yahoo! toolbar and be alerted to new email wherever you're
surfing.
http://new.toolbar.yahoo.com/toolbar/features/mail/index.php




**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From christophe at pallier.org  Mon Jul 23 14:52:18 2007
From: christophe at pallier.org (Christophe Pallier)
Date: Mon, 23 Jul 2007 14:52:18 +0200
Subject: [R] using R for a reaction-time experiment
In-Reply-To: <95FAB3DD-3DD4-4947-AE2A-D079D4553CD6@DAL.CA>
References: <11732474.post@talk.nabble.com>
	<95FAB3DD-3DD4-4947-AE2A-D079D4553CD6@DAL.CA>
Message-ID: <dea6cb960707230552y7d307eb2refabbec92224767b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/c7313be6/attachment.pl 

From christophe at pallier.org  Mon Jul 23 15:02:59 2007
From: christophe at pallier.org (Christophe Pallier)
Date: Mon, 23 Jul 2007 15:02:59 +0200
Subject: [R] Multilevel package: Obtaining significance for waba
	within-group correlation?
In-Reply-To: <46A47F95.8010901@psychologie.hu-berlin.de>
References: <46A47F95.8010901@psychologie.hu-berlin.de>
Message-ID: <dea6cb960707230602o64a928e2wec0db91014a0d92d@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/ea3a9d0b/attachment.pl 

From guowei at stat.osu.edu  Mon Jul 23 15:56:21 2007
From: guowei at stat.osu.edu (guowei at stat.osu.edu)
Date: Mon, 23 Jul 2007 09:56:21 -0400 (EDT)
Subject: [R] (no subject)
Message-ID: <1631.128.146.6.172.1185198981.squirrel@www.stat.osu.edu>


Dear Sir/Madam,

I am running a R program for my SNP data. There are some errors when I run
glm model in Hapassoc software, sometimes it is over the memory and
sometimes the matrix is singular. I want to ingore these errors and excute
the next statement. Otherwise, I have a big big trouble. Do you have some
idea about this problem of ingore errors.

Wish to get your help assp.

thanks.

-- 
Wei Guo
Department of Statistics
The Ohio State University
Columbus, Ohio 43210
Tel: 001-614-292-4713(o)
e-mail: guowei at stat.osu.edu


From brassnotdead at googlemail.com  Mon Jul 23 15:59:46 2007
From: brassnotdead at googlemail.com (Patrick Zimmermann)
Date: Mon, 23 Jul 2007 15:59:46 +0200
Subject: [R] extraction of vector elements to new list
Message-ID: <bae7f91d0707230659y7da4b6b1kdf273078b46b8d75@mail.gmail.com>

Dear R-community,

I have got a list of vectors and would like to extract the first two
elements of each vector to a new list.

My list is of the style:

my.list = list(c("a", "b", "c"), c("d", "e"), c("f", "g", "h", "i"), ...)

#I want:

new.list = list(c("a", "b"), c("d", "e"), c("f", "g"), ...)

# As

my.list[[3]][1:2]

# is [1] "f" "g"

# I thought

my.list[[1:3]][1:2]

# would be

# [[1]]
# [1] "a" "b"

# [[2]]
# [1] "d" "d"

# [[3]]
# [1] "f" "g"

# but is: 'Error: recursive indexing failed at level 2'


I think it should be easy, but none of my tried combinations of '['
and 'c(' worked.
Who can help?

Patrick


From magdalena1236 at gmail.com  Mon Jul 23 16:06:10 2007
From: magdalena1236 at gmail.com (Ana Marcela Florez Rueda)
Date: Mon, 23 Jul 2007 09:06:10 -0500
Subject: [R] change labels in a dendrogram
Message-ID: <3e5f4a310707230706l5e2d39a7v506c10acdb25f79b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/08eff014/attachment.pl 

From magdalena1236 at gmail.com  Mon Jul 23 16:09:01 2007
From: magdalena1236 at gmail.com (Ana Marcela Florez Rueda)
Date: Mon, 23 Jul 2007 09:09:01 -0500
Subject: [R] change text labels on a dendrogram
Message-ID: <3e5f4a310707230709n1cd4e049l9f4a4e95d1794ed1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/913a7f6f/attachment.pl 

From jholtman at gmail.com  Mon Jul 23 16:19:41 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 23 Jul 2007 10:19:41 -0400
Subject: [R] code optimization tips
In-Reply-To: <D4E253DF-0B41-4CF5-BDAE-5009EDC88D53@ex.ac.uk>
References: <D4E253DF-0B41-4CF5-BDAE-5009EDC88D53@ex.ac.uk>
Message-ID: <644e1f320707230719h1bb86c25xe2aace73f34eecdf@mail.gmail.com>

First question is why are you defining the functions within the main
function each time?  Why don't you define them once outside?

On 7/23/07, baptiste Augui? <ba208 at exeter.ac.uk> wrote:
> Hi,
>
> Being new to R I'm asking for some advice on how to optimize the
> performance of the following piece of code:
>
>
> > alpha_c <- function(lambda=600e-9,alpha_s=1e-14,N=400,spacing=1e-7){
> >
> > k<-2*pi/lambda
> > ri<-c(0,0) # particle at the origin
> > x<-c(-N:N)
> > positions <- function(N) {
> >    reps <- 2*N+1
> >    matrix(c(rep(-N:N, each = reps), rep(-N:N, times = reps)),
> >           nrow = 2, byrow = TRUE)
> > }
> > rj<-positions(N)*spacing # all positions in the 2N x 2N array
> > rj<-rj[1:2,-((dim(rj)[2]-1)/2+1)] # remove rj=(0,0)
> >
> > mod<-function(x){sqrt(x[1]^2+x[2]^2)} # modulus
> >
> > sij <-function(rj){
> > rij=mod(rj-ri)
> > cos_ij=rj[1]/rij
> > sin_ij=rj[2]/rij
> >
> > A<-(1-1i*k*rij)*(3*cos_ij^2-1)*exp(1i*k*rij)/(rij^3)
> > B<-k^2*sin_ij^2*exp(1i*k*rij)/rij
> >
> > sij<-A+B
> > }
> >
> > s_ij<-apply(rj,2,sij)
> > S<-sum(s_ij)
> > alpha_s/(1-alpha_s*S)
> > }
> > alpha_c()
>
>
> This function is to be called for a few tens of values of lambda in a
> 'for' loop, and possibly a couple of different N and spacing (their
> magnitude is typically around the default one).
>
> This can be a bit slow ??? not that I would expect otherwise --- and
> I wonder if there is something I could do to optimize it (vectorize
> with respect to the lambda parameter?, change the units of the
> problem to deal with numbers closer to unity?,...)
>
> Best regards,
>
> baptiste
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From F.MENDIBURU at CGIAR.ORG  Mon Jul 23 16:27:59 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Mon, 23 Jul 2007 09:27:59 -0500
Subject: [R] About infinite value
References: <11740491.post@talk.nabble.com>
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A206FD40@webmail.cip.cgiar.org>

I think that the best thing is to work in logarithmic way, to avoid the limitations of the CPU. 
If y = 10^400, to do y=400*log(10), to change all you formulate to the logarithmic way and the final result to apply the antilogarithm. 
 
Felipe de Mendiburu. 
Professor of statistic 
Agrarian National University -La Molina - PERU 

________________________________

De: r-help-bounces at stat.math.ethz.ch en nombre de arigado
Enviado el: lun 23/07/2007 4:17
Para: r-help at stat.math.ethz.ch
Asunto: [R] About infinite value




Hi everyone

I have a problem about "infinite".
If I type 10^308, R shows "1e+308"
When I type 10^309, R shows "Inf"
So, we know if a value is large than 1.XXXe+308, R will show "Inf"
How can i do let the value, like "10^400" ,typed in R to show the word
"1e+400" not "Inf"

--
View this message in context: http://www.nabble.com/About-infinite-value-tf4128557.html#a11740491
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dimitris.rizopoulos at med.kuleuven.be  Mon Jul 23 16:34:40 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 23 Jul 2007 16:34:40 +0200
Subject: [R] extraction of vector elements to new list
References: <bae7f91d0707230659y7da4b6b1kdf273078b46b8d75@mail.gmail.com>
Message-ID: <008001c7cd36$9a8e17c0$0540210a@www.domain>

try this:

new.list <- lapply(my.list, "[", i = 1:2)
new.list


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Patrick Zimmermann" <brassnotdead at googlemail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, July 23, 2007 3:59 PM
Subject: [R] extraction of vector elements to new list


> Dear R-community,
>
> I have got a list of vectors and would like to extract the first two
> elements of each vector to a new list.
>
> My list is of the style:
>
> my.list = list(c("a", "b", "c"), c("d", "e"), c("f", "g", "h", "i"), 
> ...)
>
> #I want:
>
> new.list = list(c("a", "b"), c("d", "e"), c("f", "g"), ...)
>
> # As
>
> my.list[[3]][1:2]
>
> # is [1] "f" "g"
>
> # I thought
>
> my.list[[1:3]][1:2]
>
> # would be
>
> # [[1]]
> # [1] "a" "b"
>
> # [[2]]
> # [1] "d" "d"
>
> # [[3]]
> # [1] "f" "g"
>
> # but is: 'Error: recursive indexing failed at level 2'
>
>
> I think it should be easy, but none of my tried combinations of '['
> and 'c(' worked.
> Who can help?
>
> Patrick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From wwwhsd at gmail.com  Mon Jul 23 16:37:42 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 23 Jul 2007 11:37:42 -0300
Subject: [R] extraction of vector elements to new list
In-Reply-To: <bae7f91d0707230659y7da4b6b1kdf273078b46b8d75@mail.gmail.com>
References: <bae7f91d0707230659y7da4b6b1kdf273078b46b8d75@mail.gmail.com>
Message-ID: <da79af330707230737x6e88edc1ra1244ed640b10d10@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/abdcf577/attachment.pl 

From tlumley at u.washington.edu  Mon Jul 23 16:38:02 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 23 Jul 2007 07:38:02 -0700 (PDT)
Subject: [R] (no subject)
In-Reply-To: <1631.128.146.6.172.1185198981.squirrel@www.stat.osu.edu>
References: <1631.128.146.6.172.1185198981.squirrel@www.stat.osu.edu>
Message-ID: <Pine.LNX.4.64.0707230737160.20974@homer24.u.washington.edu>


It's a FAQ (Question 7.32)

 	-thomas

On Mon, 23 Jul 2007, guowei at stat.osu.edu wrote:

>
> Dear Sir/Madam,
>
> I am running a R program for my SNP data. There are some errors when I run
> glm model in Hapassoc software, sometimes it is over the memory and
> sometimes the matrix is singular. I want to ingore these errors and excute
> the next statement. Otherwise, I have a big big trouble. Do you have some
> idea about this problem of ingore errors.
>
> Wish to get your help assp.
>
> thanks.
>
> -- 
> Wei Guo
> Department of Statistics
> The Ohio State University
> Columbus, Ohio 43210
> Tel: 001-614-292-4713(o)
> e-mail: guowei at stat.osu.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From xkneifl at mendelu.cz  Mon Jul 23 16:40:10 2007
From: xkneifl at mendelu.cz (Ing. Michal Kneifl, Ph.D.)
Date: Mon, 23 Jul 2007 16:40:10 +0200
Subject: [R] Help on "looping problem" needed!
In-Reply-To: <dea6cb960707230552y7d307eb2refabbec92224767b@mail.gmail.com>
References: <11732474.post@talk.nabble.com>
	<95FAB3DD-3DD4-4947-AE2A-D079D4553CD6@DAL.CA>
	<dea6cb960707230552y7d307eb2refabbec92224767b@mail.gmail.com>
Message-ID: <20070723164010.kcss5dfc0ksgwwwo@mail.mendelu.cz>

I am wondering if someone could help me out with following problem:
I have written a for loop which generates a random normal distribution  
let us say 1000 times.
When the restriction is met (mean<0.000001), the loop stops, prints  
the mean value and plots a histogram.

for(i in 1:1000) {
	a<-rnorm(1000,0,.2)
	b<-abs(mean(a))
		if(b>.000001) next else {print(b);hist(a);break}}

How to reshape the loop when I want to find at least 5 distibutions  
that meet my restriction and save them (assign) under
names R1....R5.
Could you help me please?

Michael


From ba208 at exeter.ac.uk  Mon Jul 23 16:45:06 2007
From: ba208 at exeter.ac.uk (=?ISO-8859-1?Q?baptiste_Augui=E9?=)
Date: Mon, 23 Jul 2007 15:45:06 +0100
Subject: [R] code optimization tips
In-Reply-To: <644e1f320707230719h1bb86c25xe2aace73f34eecdf@mail.gmail.com>
References: <D4E253DF-0B41-4CF5-BDAE-5009EDC88D53@ex.ac.uk>
	<644e1f320707230719h1bb86c25xe2aace73f34eecdf@mail.gmail.com>
Message-ID: <77115831-BD7B-48EF-981C-D13DA4289895@ex.ac.uk>

Thanks for your reply,

On 23 Jul 2007, at 15:19, jim holtman wrote:

> First question is why are you defining the functions within the main
> function each time?  Why don't you define them once outside?
>


Fair enough!

As said, I'm new to R and don't know whether it is best to define  
functions outside and pass to them all necessary arguments, or nest  
them and get variables in the scope from parents. In any case, I'd  
agree my positions(), mod() and sij() functions would be better  
outside. Here is a corrected version (untested as something else is  
running),

>
> positions <- function(N) {
>    reps <- 2*N+1
>    matrix(c(rep(-N:N, each = reps), rep(-N:N, times = reps)),
>           nrow = 2, byrow = TRUE)
> }

> mod<-function(x){sqrt(x[1]^2+x[2]^2)} # modulus

> sij <-function(rj,ri,k){
> rij=mod(rj-ri)
> cos_ij=rj[1]/rij
> sin_ij=rj[2]/rij
>
> A<-(1-1i*k*rij)*(3*cos_ij^2-1)*exp(1i*k*rij)/(rij^3)
> B<-k^2*sin_ij^2*exp(1i*k*rij)/rij
>
> sij<-A+B
> }

> alpha_c <- function(lambda=600e-9,alpha_s=1e-14,N=400,spacing=1e-7){
>
> k<-2*pi/lambda
> ri<-c(0,0) # particle at the origin
>
> rj<-positions(N)*spacing # all positions in the 2N x 2N array
> rj<-rj[1:2,-((dim(rj)[2]-1)/2+1)] # remove rj=(0,0)
>
> s_ij<-apply(rj,2,sij)

*** Now, how do I pass k and ri to this function ? ***

> S<-sum(s_ij)
> alpha_s/(1-alpha_s*S)
> }
> alpha_c()
>


>
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?


Wondering whether that's part of the signature?

the problem is related to scattering by arrays of particles, more  
specifically to evaluate the array influence on the effective  
polarizability (alpha) of a particle via dipolar radiative coupling.


From meeryana at yahoo.com  Mon Jul 23 16:50:01 2007
From: meeryana at yahoo.com (copula)
Date: Mon, 23 Jul 2007 07:50:01 -0700 (PDT)
Subject: [R] R and Copula
In-Reply-To: <11741580.post@talk.nabble.com>
References: <11612986.post@talk.nabble.com>
	<OFC94AAA62.BB5EBA68-ON6525731C.002B7FA8-6525731C.002CE93A@ccilindia.co.in>
	<11683844.post@talk.nabble.com> <469FC845.2060704@u.washington.edu>
	<11705736.post@talk.nabble.com> <11741580.post@talk.nabble.com>
Message-ID: <11741607.post@talk.nabble.com>


only mlcbbsel package. i have the first one.
sorry


copula wrote:
> 
> new question:
> 
> Where can I find the packages "mlCopulaSelection" and "mlcbbsel"?
> 
> 
> 
> copula wrote:
>> 
>> and one more time:
>> 
>> thank you a lot for help!!!
>> 
>> 
>> 
>> Julian Burgos wrote:
>>> 
>>> Hi Meeryana,
>>> 
>>> It seems you have not loaded the package.  To use a package that you 
>>> already installed, do:
>>> 
>>>  > library(copula)
>>> 
>>> I recommend you review the R documentation.  There are several good 
>>> references and tutorial on the CRAN site:
>>> 
>>> http://cran.r-project.org/manuals.html
>>> http://cran.r-project.org/other-docs.html
>>> 
>>> Any time you spend learning the basics will make your work more 
>>> efficient. :)
>>> Good luck,
>>> 
>>> Julian
>>> 
>>> Fisheries Acoustics Research Lab
>>> School of Aquatic and Fishery Science
>>> 
>>> copula wrote:
>>>> I have installed fast all packages for copula,
>>>> I have installed package 'gnml' and the others from the Jim Lindsey's
>>>> web
>>>> site and the other packages like 'repeted' which is necessary for
>>>> calculating copula.
>>>>  
>>>> but when I start with copula it write: 
>>>> 'gausscop' function is not found. And also 'fitted.gausscop(z)' and
>>>> 'residuals.gausscop(z)'.
>>>>
>>>> also what I want to say is: when I have installed package 'repeted'
>>>> from Jim
>>>> Lindsey's web site in R project, this what I have received: 
>>>>  
>>>> "utils:::menuInstallLocal()
>>>> updating HTML package descriptions"
>>>>
>>>> what have I to do next to solve this problem and to continue with
>>>> copula.
>>>>
>>>>
>>>> Thanks a lot for previous help!
>>>>
>>>>
>>>>  
>>>>
>>>>   
>>>>
>>>> gyadav wrote:
>>>>   
>>>>> hi meeryana,
>>>>>
>>>>> may be this time nobnody is responding. but dont worry you will get a
>>>>> lot 
>>>>> of help eventually, so always post a copy to the mailing list.
>>>>> The reason is there are a lot many newbies, although i am also not so
>>>>> old 
>>>>> enough, who have even the simplest questions but are hesitant to ask.
>>>>> the objective of the list is to help, and thus feel free to ask, stand
>>>>> and 
>>>>> contribute :-) i hope you will understand not today then tomorrow as
>>>>> you 
>>>>> will get associated with the mailing list more closely. Personally i
>>>>> have 
>>>>> found friends here. Further, never post a mail with a loose subject
>>>>> and 
>>>>> secondly try to maintain the thread i.e. reply to the mail which you
>>>>> want 
>>>>> to reply(for more details refer to the posting guide) :-) that would 
>>>>> really help. 
>>>>>
>>>>> Yes now regarding your question :-) 
>>>>>
>>>>> Step1 : List of all packages can be found at this link :-)
>>>>> http://cran.r-project.org/src/contrib/PACKAGES.html
>>>>> Step2: Click on the package you want to install :-) 
>>>>> http://cran.r-project.org/src/contrib/Descriptions/copula.html
>>>>> Step3: Then download the binary of your Operating system. If windows
>>>>> then 
>>>>> download corresponding zip file.
>>>>> for copula it is
>>>>> 
>>>>> http://cran.r-project.org/bin/windows/contrib/r-release/copula_0.5-3.zip
>>>>> save zip file on your system
>>>>> Step4 Open your R rpogram
>>>>> Step5: Goto Packages -> Install packages from Local Zip file
>>>>> Step6: Select your package zip file which you want to install
>>>>> Step7: Sit back and relax
>>>>> Step8: load the library using library(LibraryName) on R prompt
>>>>>
>>>>> There are alternate ways of installing the package directly from R
>>>>> prompt. 
>>>>> It didn't worked for me a long time back, so  i always adopt this
>>>>> method.
>>>>> Somebody on the list may help you in this regards :-)
>>>>>
>>>>> bye and learn
>>>>> Join and stand with Open Source and R community
>>>>> Cheers and Chiao, Welcome
>>>>> -gaurav
>>>>>
>>>>>
>>>>>
>>>>> dear Mr. Yadav,
>>>>>
>>>>> I want to thank for help, 
>>>>> and for that you are only who is willing to help,
>>>>> but I have one question:
>>>>> because I'm new with R project also, I think I should install a
>>>>> package 
>>>>> for copula.
>>>>> I have only installed R program.
>>>>> How should I install this package? And is it what I have also to do
>>>>> with 
>>>>> credit metrics, Value at Risk, matix and the other formulas, I mean 
>>>>> install packages.
>>>>>
>>>>> I hope that you have a little time for me and my problem, and I hope
>>>>> I'm 
>>>>> not disturbing you.
>>>>> thank you for all you can do for me 
>>>>>
>>>>> and 
>>>>>
>>>>> best regards,
>>>>>
>>>>> Mirjana
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> gyadav wrote:
>>>>>     
>>>>>> hi
>>>>>>
>>>>>> see the code below i hope this will make your understanding of
>>>>>> copulas 
>>>>>> better
>>>>>> this code plots two normal distribution and their joint distribution 
>>>>>> N[0,2] & N[0,4]
>>>>>>
>>>>>> HTH
>>>>>>
>>>>>> ######################code################################
>>>>>> library("copula")
>>>>>> ###################copy in two parts in R#################
>>>>>>
>>>>>> ##################PART A##################################
>>>>>> ## construct a bivariate distribution whose marginals
>>>>>> ## are normal and Normal respectively, coupled
>>>>>> ## together via a normal copula
>>>>>> op <- par(mfrow = c(2, 2), # 2 x 2 pictures on one plot
>>>>>>           pty = "s")       # square plotting region,
>>>>>>                            # independent of device size
>>>>>>
>>>>>> x <- mvdc(normalCopula(0.75), c("norm", "norm"),
>>>>>> list(list(mean = 0, sd =2),list(mean = 0, sd =4)))
>>>>>> x.samp <- rmvdc(x, 10000)
>>>>>> par(mfrow=c(2,3))
>>>>>> hist(x.samp[,1],xlab="Normal")
>>>>>> hist(x.samp[,2],xlab="Normal")
>>>>>> plot(x.samp[,2],x.samp[,1],pch=21,xlab="Normal",ylab="Normal")
>>>>>>
>>>>>> plot(dmvdc(x, x.samp))
>>>>>> plot(pmvdc(x, x.samp))
>>>>>>
>>>>>> ## At end of plotting, reset to previous settings:
>>>>>>
>>>>>>
>>>>>> ###########################PART B#######################
>>>>>> par(op)
>>>>>> for (i in seq(1:360)){
>>>>>> persp(x, dmvdc, xlim = c(-4, 4), ylim=c(0, 1),theta=i)
>>>>>> }
>>>>>>
>>>>>>
>>>>>> Regards,
>>>>>>
>>>>>> Gaurav Yadav
>>>>>> +++++++++++
>>>>>> Assistant Manager, CCIL, Mumbai (India)
>>>>>> Mob: +919821286118 Email: emailtogauravyadav at gmail.com
>>>>>> Bhagavad Gita:  Man is made by his Belief, as He believes, so He is
>>>>>>
>>>>>>
>>>>>>
>>>>>> copula <meeryana at yahoo.com> 
>>>>>> Sent by: r-help-bounces at stat.math.ethz.ch
>>>>>> 07/17/2007 12:53 PM
>>>>>>
>>>>>> To
>>>>>> r-help at stat.math.ethz.ch
>>>>>> cc
>>>>>>
>>>>>> Subject
>>>>>> Re: [R] R and Copula
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> it would be great when somebody will help me
>>>>>> thanks
>>>>>>
>>>>>>
>>>>>> copula wrote:
>>>>>>       
>>>>>>> hi,
>>>>>>> first I want to say that I'm new here, and new with copula and R.
>>>>>>>
>>>>>>> That is the reason why I'm writing, if somebody can help me. 
>>>>>>>
>>>>>>> I have to make an example of Copula. 
>>>>>>> On internet I've found this forum and that copula can calculate with
>>>>>>> R.
>>>>>>>
>>>>>>> Can somebody help me with the thing how can I start and where can
>>>>>>> read
>>>>>>> about these stuffs.
>>>>>>>
>>>>>>> Thank to all who can help!
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>         
>>>>>> -- 
>>>>>> View this message in context: 
>>>>>> http://www.nabble.com/R-and-Copula-tf4085867.html#a11644534
>>>>>> Sent from the R help mailing list archive at Nabble.com.
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide 
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>       
>>>>> ============================================================================================
>>>>>     
>>>>>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and 
>>>>>>       
>>>>> ...{{dropped}}
>>>>>     
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>>
>>>>>>       
>>>>> Quoted from: 
>>>>> http://www.nabble.com/R-and-Copula-tf4085867.html#a11645614
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> ============================================================================================
>>>>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and
>>>>> ...{{dropped}}
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>
>>>>>
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> 
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/R-and-Copula-tf4085867.html#a11741607
Sent from the R help mailing list archive at Nabble.com.


From nshephard at gmail.com  Mon Jul 23 16:50:39 2007
From: nshephard at gmail.com (Neil Shephard)
Date: Mon, 23 Jul 2007 15:50:39 +0100
Subject: [R] problems with character objects and calls to list()
Message-ID: <31b34fca0707230750v29e527efvcdf816a480bd3890@mail.gmail.com>

Hi All,

I have a problem trying to get a set of columns recognised as a list
and can't work out how to do it despite trawling through the mailing
list archives, and docs.

A short example...

to.convert <- NULL
n <- 6
for(x in 1:n){
  to.convert <- paste(to.convert, paste((2 * x) -1, (2 * x), sep=":"), sep=",")
}
to.convert <- gsub("^,", "", to.convert)
typeof(to.convert)
[1] "character"
## This is the problem....
list(to.convert)
[[1]]
[1] "1:2,3:4,5:6"

Really I'd like the call to list() to behave as though the text had
been entered directly so that you get....

> list(1:2, 3:4, 5:6)
[[1]]
[1] 1 2

[[2]]
[1] 3 4

[[3]]
[1] 5 6


But how do I get the quote's that surround to.convert removed, given
that it is an object of type character?

Thanks in advance,

Neil

Background
========

This is a refined example from a larger problem that I'm working on.
Given a data frame of pairs of columns containing genotype data I
would like to convert them to genotype objects using makeGenotypes.
The number of loci (i.e. ncol() / 2) is variable, so I like to be able
to determine how many loci there are and then convert all of them.

Thus an example data set would be...

<-----Start test.dat------>
ID,rs1.1,rs1.2,rs2.1,rs2.2,rs3.1,rs3.2,rs4.1,rs4.2,rs5.1,rs5.2,rs6.1,rs6.2
A0001,1,1,1,2,1,1,2,2,1,2,1,1
A0002,1,2,2,1,1,2,2,1,1,2,1,2
A0003,1,2,2,1,1,1,,,1,2,2,2
A0004,2,2,1,1,2,2,2,2,1,1,2,2
<-----End test.dat----->

I'd then like to read in and convert as follows...

genotype <-read.csv("test.dat")
genotype <- genotype[,-1]    ## Removes ID column
n.loci <- ncol(genotype) / 2  ## Calcualtes how many loci
## Build a list of the pairs of columns
to.convert <-NULL
for(x in 1:n.loci){
  to.convert <- paste(to.convert, paste((2 * x) -1, (2 * x), sep=":"), sep=",")
}
to.convert <- gsub("^,", "", to.convert)
## Then convert to genotype object, but get errors...
genotype2 <- makeGenotypes(genotype, convert=list(to.convert))
Error in makeGenotypes(genotype, convert = list(to.convert)) :
	When convert is a list, each element must be a 2-vector.
## I've also tried giving the column names but the data disappears
genotype2 <- makeGenotypes(genotype, convert=c(colnames(genotype))
  rs1.1 rs1.2 rs2.1 rs2.2 rs3.1 rs3.2 rs4.1 rs4.2 rs5.1 rs5.2 rs6.1 rs6.2
1  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>
2  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>
3  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>
4  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>  <NA>
## What I would like to happen is demonstrated by...
genotype2 <- makeGenotypes(genotype, convert=c(1:2,3:4,5:6,7:8,9:10,11:12))
genotype2
  rs1.1/rs1.2 rs2.1/rs2.2 rs3.1/rs3.2 rs4.1/rs4.2 rs5.1/rs5.2 rs6.1/rs6.2
1         1/1         1/2         1/1         2/2         1/2         1/1
2         2/1         1/2         1/2         2/1         1/2         2/1
3         2/1         1/2         1/1        <NA>         1/2         2/2
4         2/2         1/1         2/2         2/2         1/1         2/2

In the above the data has been converted to genotypes.

-- 
"In mathematics you don't understand things. You just get used to
them."  - Johann von Neumann

Email - nshephard at gmail.com / n.shephard at sheffield.ac.uk
Website - http://slack.ser.man.ac.uk/
Photos - http://www.flickr.com/photos/slackline/


From Nathalie.Peyrard at toulouse.inra.fr  Mon Jul 23 16:58:05 2007
From: Nathalie.Peyrard at toulouse.inra.fr (Nathalie Peyrard)
Date: Mon, 23 Jul 2007 16:58:05 +0200
Subject: [R] persp and greek symbols in the axes labels
Message-ID: <46A4C1FD.3080706@toulouse.inra.fr>

Hello,

I am plotting a 3D function using persp and I would like to use greek 
symbols in the axes labels.
I  have found examples like  this one on the web:

plot(0,0,xlab=expression(kappa[lambda]),ylab=substitute(paste(phi,"=",true,sigma),list(true=5)))

this works well with plot but not with persp:
with the command

persp(M,theta = -20,phi = 
0,xlab=expression(kappa[lambda]),ylab=substitute(paste(phi,"=",true,sigma),list(true=5)),zlab 
= "Z")

I get the labels as in toto.eps

Any suggestion? Thanks!

Nathalie

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   
INRA  Toulouse - Unit? de Biom?trie et  Intelligence Artificielle 
Chemin de Borde-Rouge BP 52627 31326 CASTANET-TOLOSAN cedex FRANCE 
Tel : +33(0)5.61.28.54.39 - Fax : +33(0)5.61.28.53.35
Web :http://mia.toulouse.inra.fr/index.php?id=217
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


-------------- next part --------------
A non-text attachment was scrubbed...
Name: toto.eps
Type: application/postscript
Size: 18184 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070723/d5dc7c6e/attachment.eps 

From ggrothendieck at gmail.com  Mon Jul 23 17:11:24 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 23 Jul 2007 11:11:24 -0400
Subject: [R] extraction of vector elements to new list
In-Reply-To: <008001c7cd36$9a8e17c0$0540210a@www.domain>
References: <bae7f91d0707230659y7da4b6b1kdf273078b46b8d75@mail.gmail.com>
	<008001c7cd36$9a8e17c0$0540210a@www.domain>
Message-ID: <971536df0707230811t10f085b3j3f9dc6f060862b2e@mail.gmail.com>

On 7/23/07, Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.be> wrote:
> try this:
>
> new.list <- lapply(my.list, "[", i = 1:2)
> new.list

You could use the lapply above or

   lapply(my.list, head, 2)


From F.MENDIBURU at CGIAR.ORG  Mon Jul 23 17:22:19 2007
From: F.MENDIBURU at CGIAR.ORG (Mendiburu, Felipe (CIP))
Date: Mon, 23 Jul 2007 10:22:19 -0500
Subject: [R] change text labels on a dendrogram
References: <3e5f4a310707230709n1cd4e049l9f4a4e95d1794ed1@mail.gmail.com>
Message-ID: <B7B34444ECA41A41AC47DABA057CE7A206FD41@webmail.cip.cgiar.org>

Dear Marcela.
 
In the data, if column 1 has the names:
 
rownames(data) <-data [, 1]
data <- date [, - 1] 
 
now, to make a dendrogram
greetings. 
 
Felipe de Mendiburu. 
Professor of statistic 
Agrarian National University -La Molina - PERU 
 

________________________________

De: r-help-bounces at stat.math.ethz.ch en nombre de Ana Marcela Florez Rueda
Enviado el: lun 23/07/2007 9:09
Para: r-help at stat.math.ethz.ch
Asunto: [R] change text labels on a dendrogram



 Hi all:


I already made a dendrogram, I want to change the labels; In my data set
there is one column with text that I want to set as the labels, any one can
tellme how can  I do it?


Thanks,


Ana Marcela

        [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Greg.Snow at intermountainmail.org  Mon Jul 23 17:24:15 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 23 Jul 2007 09:24:15 -0600
Subject: [R] Trend lines on a scatterplot matrix
In-Reply-To: <1184881883.17296.0.camel@papagena>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF25F3@LP-EXCHVS07.CO.IHC.COM>

Start with something like this:

pairs(iris, panel= function(x,y,...){
	points(x,y);
	abline(lm(y~x))})

Then substitute in your data and extra args and any other enhancements
you want.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alan S Barnett
> Sent: Thursday, July 19, 2007 3:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Trend lines on a scatterplot matrix
> 
> I'm using pairs() to generate a scatterplot matrix;
> 
> pairs(~ 
> Fuzzy.gray.white.ratio+Fuzzy.gw.t.score+AgeWhenTested+signal_m
> ean.noise,
> 	data=datam,subset=status=="control",main="Controls",
>         labels=c("G/W","Peak Separation","Age","S/N"))
> 
> 
> How can I add regression lines to the plots?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mister_bluesman at hotmail.com  Mon Jul 23 17:29:01 2007
From: mister_bluesman at hotmail.com (mister_bluesman)
Date: Mon, 23 Jul 2007 08:29:01 -0700 (PDT)
Subject: [R] cmdscale question
Message-ID: <11746748.post@talk.nabble.com>


Hi.

I know matrices that use distances between places works fine when using
cmdscale. However, what about matricies such as:

  A     B   C   D   E
A 0    1   23  12  9
B 1    0   10  12  3
C 23  10   0   23  4
D 12  12  23  0   21
E  9   3    4   21   0 

i.e. matrices which do not represent physical distances between places (as
they would not make sense for real distances such as the one above) but
other statistics instead?

Thanks
-- 
View this message in context: http://www.nabble.com/cmdscale-question-tf4130562.html#a11746748
Sent from the R help mailing list archive at Nabble.com.


From dimitris.rizopoulos at med.kuleuven.be  Mon Jul 23 17:37:38 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 23 Jul 2007 17:37:38 +0200
Subject: [R] Help on "looping problem" needed!
References: <11732474.post@talk.nabble.com><95FAB3DD-3DD4-4947-AE2A-D079D4553CD6@DAL.CA><dea6cb960707230552y7d307eb2refabbec92224767b@mail.gmail.com>
	<20070723164010.kcss5dfc0ksgwwwo@mail.mendelu.cz>
Message-ID: <00b201c7cd3f$666fdfb0$0540210a@www.domain>

try this:

tol <- 0.000001
mat <- matrix(as.numeric(NA), 1000, 5)
k <- 1
while(any(is.na(mat))){
    x <- rnorm(1000, sd = 0.02)
    if (abs(mean(x)) < tol) {
        mat[, k] <- x
        k <- k + 1
    }
}

abs(colMeans(mat))
par(mfrow = c(2, 3))
apply(mat, 2, hist)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ing. Michal Kneifl, Ph.D." <xkneifl at mendelu.cz>
To: "Rhelp" <r-help at stat.math.ethz.ch>
Sent: Monday, July 23, 2007 4:40 PM
Subject: [R] Help on "looping problem" needed!


>I am wondering if someone could help me out with following problem:
> I have written a for loop which generates a random normal 
> distribution
> let us say 1000 times.
> When the restriction is met (mean<0.000001), the loop stops, prints
> the mean value and plots a histogram.
>
> for(i in 1:1000) {
> a<-rnorm(1000,0,.2)
> b<-abs(mean(a))
> if(b>.000001) next else {print(b);hist(a);break}}
>
> How to reshape the loop when I want to find at least 5 distibutions
> that meet my restriction and save them (assign) under
> names R1....R5.
> Could you help me please?
>
> Michael
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From joseclaudio.faria at terra.com.br  Mon Jul 23 17:41:37 2007
From: joseclaudio.faria at terra.com.br (terra)
Date: Mon, 23 Jul 2007 12:41:37 -0300
Subject: [R] doubt about options(graphics.record=T)
Message-ID: <46A4CC31.5080604@terra.com.br>

Hi all,

I've been using R under WindowsXP.

So, where the R stores the graphic archives (don't saved) if I use the option 
options(graphics.record=T) inside of Rprofile.site file?

Regards,
-- 
/////\\\\\/////\\\\\/////\\\\\/////\\\\\
  Jose Claudio Faria
  Brasil/Bahia/UESC/DCET
  Estatistica Experimental/Prof. Titular
    joseclaudio.faria em terra.com.br
    joseclaudio.faria em oi.com.br
    jc_faria em uesc.br
    jc_faria em uol.com.br
  Tels:
    73-3634.2779 (res - Ilheus/BA)
    19-3435.1536 (res - Piracicaba/SP) *
    19-9144.8979 (cel - Piracicaba/SP) *
/////\\\\\/////\\\\\/////\\\\\/////\\\\\


From fabricemcshort at hotmail.com  Mon Jul 23 17:47:30 2007
From: fabricemcshort at hotmail.com (Fabrice McShort)
Date: Mon, 23 Jul 2007 17:47:30 +0200
Subject: [R]   SOS
Message-ID: <BAY129-W36B3FBDD7AB8FF85CD092DCBF70@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/35e94c12/attachment.pl 

From mister_bluesman at hotmail.com  Mon Jul 23 17:49:44 2007
From: mister_bluesman at hotmail.com (mister_bluesman)
Date: Mon, 23 Jul 2007 08:49:44 -0700 (PDT)
Subject: [R] cmdscale question
Message-ID: <11746748.post@talk.nabble.com>


Hi.

I know matrices that use distances between places works fine when using
cmdscale. However, what about matricies such as:

  A     B   C   D   E
A 0    1   23  12  9
B 1    0   10  12  3
C 23  10   0   23  4
D 12  12  23  0   21
E  9   3    4   21   0 

i.e. matrices which do not represent physical distances between places (as
they would not make sense for real distances such as the one above) but
other statistics instead?

Thanks
-- 
View this message in context: http://www.nabble.com/cmdscale-question-tf4130562.html#a11746748
Sent from the R help mailing list archive at Nabble.com.


From jameslondal at gmail.com  Mon Jul 23 17:54:48 2007
From: jameslondal at gmail.com (James_londal)
Date: Mon, 23 Jul 2007 08:54:48 -0700 (PDT)
Subject: [R] Error in .jinit() : Cannot create Java Virtual Machine
Message-ID: <11747295.post@talk.nabble.com>


Hey 

I have seen a few posts asking this question and nobody has replied to them
so I will ask it again. 

I have intalled java 1.6.0_22  on XP and rJava. I get the following msg when
i run .jinit()

"Error in .jinit() : Cannot create Java Virtual Machine". 

can anybody help?

Java is working fine. 



-- 
View this message in context: http://www.nabble.com/Error-in-.jinit%28%29-%3A-Cannot-create-Java-Virtual-Machine-tf4130764.html#a11747295
Sent from the R help mailing list archive at Nabble.com.


From mister_bluesman at hotmail.com  Mon Jul 23 17:59:20 2007
From: mister_bluesman at hotmail.com (mister_bluesman)
Date: Mon, 23 Jul 2007 08:59:20 -0700 (PDT)
Subject: [R] cmdscale question
Message-ID: <11747336.post@talk.nabble.com>


Hi. 

I know matrices that use distances between places works fine when using
cmdscale. However, what about matricies such as: 

  A     B   C   D   E 
A 0    1   23  12  9 
B 1    0   10  12  3 
C 23  10   0   23  4 
D 12  12  23  0   21 
E  9   3    4   21   0 

i.e. matrices which do not represent physical distances between places (as
they would not make sense for real distances such as the one above) but
other statistics instead? 

Thanks

-- 
View this message in context: http://www.nabble.com/cmdscale-question-tf4130774.html#a11747336
Sent from the R help mailing list archive at Nabble.com.


From Greg.Snow at intermountainmail.org  Mon Jul 23 18:06:33 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 23 Jul 2007 10:06:33 -0600
Subject: [R] using R for a reaction-time experiment
In-Reply-To: <11732474.post@talk.nabble.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF2609@LP-EXCHVS07.CO.IHC.COM>

Under windows you could use a graphics window for the stimulus and
capture the keypress using the getGraphicsEvent function (this does not
require hitting enter, but the graphics window needs to be the active
window through the experiment).  The playSudoku function in the sudoku
package shows one example of using getGraphicsEvent to capture
keypresses. 

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Seth Roberts
> Sent: Sunday, July 22, 2007 10:38 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] using R for a reaction-time experiment
> 
> 
> I want to use R to run a reaction-time experiment: Something 
> appears on the screen, I respond by typing something (one 
> keystroke), the system measures the speed of my response. R 
> would be great for this if only I didn't have to hit Enter to 
> enter that keystroke. I am doing such experiments now but 
> they require two actions per trial: hit keystroke, hit Enter.
> 
> Is there some way that R can be made to respond to a single 
> keystroke (other than Enter)?
> --
> View this message in context: 
> http://www.nabble.com/using-R-for-a-reaction-time-experiment-t
> f4125643.html#a11732474
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From young.stat at gmail.com  Mon Jul 23 18:12:23 2007
From: young.stat at gmail.com (Young Cho)
Date: Mon, 23 Jul 2007 09:12:23 -0700
Subject: [R] replacing double for loops with apply's
Message-ID: <b44da9db0707230912u298ee89ei2df55a3fbb742090@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/376d4ed2/attachment.pl 

From A.R.Wood at exeter.ac.uk  Mon Jul 23 18:17:01 2007
From: A.R.Wood at exeter.ac.uk (Andrew Wood)
Date: Mon, 23 Jul 2007 17:17:01 +0100
Subject: [R] Question about cmdscale function
Message-ID: <1ID0b0-00017n-Sf@dot.ex.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/62e1241d/attachment.pl 

From ripley at stats.ox.ac.uk  Mon Jul 23 18:23:11 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jul 2007 17:23:11 +0100 (BST)
Subject: [R] doubt about options(graphics.record=T)
In-Reply-To: <46A4CC31.5080604@terra.com.br>
References: <46A4CC31.5080604@terra.com.br>
Message-ID: <Pine.LNX.4.64.0707231720470.29667@gannet.stats.ox.ac.uk>

On Mon, 23 Jul 2007, terra wrote:

> Hi all,
>
> I've been using R under WindowsXP.
>
> So, where the R stores the graphic archives (don't saved) if I use the option
> options(graphics.record=T) inside of Rprofile.site file?

The relevant help file (?windows) does tell you: please read it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mtmorgan at fhcrc.org  Mon Jul 23 18:33:01 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Mon, 23 Jul 2007 09:33:01 -0700
Subject: [R] Rmpi installation error
In-Reply-To: <BCEOIKLKMHDDGHIPNNIMCEBFCBAA.sunithak@cdac.in>
	(sunithak@cdac.in's message of "Mon, 23 Jul 2007 17:24:18 +0530")
References: <BCEOIKLKMHDDGHIPNNIMCEBFCBAA.sunithak@cdac.in>
Message-ID: <6phabtnaxmq.fsf@gopher4.fhcrc.org>

Sunitha -- I think Rmpi finds an MPICH1 installation, rather than
MPICH2. Martin

"Sunithak" <sunithak at cdac.in> writes:

> Hello everybody,
> I am trying to install Rmpi on 64-bit HP-Proliant server. I am facing the
> following error:
>
>
> * Installing *source* package 'Rmpi' ...
> Try to find mpi.h ...
> Found in /usr/local/mpich-1.2.7p1/include
> Try to find libmpi or libmpich ...
> Found libmpich in /usr/local/mpich-1.2.7p1/lib
> Try to find liblam ...
> checking for main in -llam... no
> liblam not found. Probably not LAM-MPI
> checking for openpty in -lutil... no
> checking for main in -lpthread... no
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> gcc -I/usr/local/lib64/R/include -I/usr/local/lib64/R/include -DPACKAGE_NAME
> =\"\" -DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -D
> PACKAGE_BUGREPORT=\"\"  -I/usr/local/mpich-1.2.7p1/include -DMPI2 -I/usr/loc
> al/include   -fpic  -g -O2 -std=gnu99 -c conversion.c -o conversion.o
> gcc -I/usr/local/lib64/R/include -I/usr/local/lib64/R/include -DPACKAGE_NAME
> =\"\" -DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -D
> PACKAGE_BUGREPORT=\"\"  -I/usr/local/mpich-1.2.7p1/include -DMPI2 -I/usr/loc
> al/include   -fpic  -g -O2 -std=gnu99 -c internal.c -o internal.o
> gcc -I/usr/local/lib64/R/include -I/usr/local/lib64/R/include -DPACKAGE_NAME
> =\"\" -DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -D
> PACKAGE_BUGREPORT=\"\"  -I/usr/local/mpich-1.2.7p1/include -DMPI2 -I/usr/loc
> al/include   -fpic  -g -O2 -std=gnu99 -c RegQuery.c -o RegQuery.o
> gcc -I/usr/local/lib64/R/include -I/usr/local/lib64/R/include -DPACKAGE_NAME
> =\"\" -DPACKAGE_TARNAME=\"\" -DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -D
> PACKAGE_BUGREPORT=\"\"  -I/usr/local/mpich-1.2.7p1/include -DMPI2 -I/usr/loc
> al/include   -fpic  -g -O2 -std=gnu99 -c Rmpi.c -o Rmpi.o
> Rmpi.c: In function `mpi_universe_size':
> Rmpi.c:84: warning: implicit declaration of function `MPI_Comm_get_attr'
> Rmpi.c:84: error: `MPI_UNIVERSE_SIZE' undeclared (first use in this
> function)
> Rmpi.c:84: error: (Each undeclared identifier is reported only once
> Rmpi.c:84: error: for each function it appears in.)
> Rmpi.c: In function `mpi_comm_spawn':
> Rmpi.c:873: warning: implicit declaration of function `MPI_Comm_spawn'
> Rmpi.c:873: error: `MPI_ARGV_NULL' undeclared (first use in this function)
> Rmpi.c: In function `mpi_comm_get_parent':
> Rmpi.c:897: warning: implicit declaration of function `MPI_Comm_get_parent'
> Rmpi.c: In function `mpi_comm_disconnect':
> Rmpi.c:910: warning: implicit declaration of function `MPI_Comm_disconnect'
> make: *** [Rmpi.o] Error 1
> chmod: cannot access `/usr/local/lib64/R/library/Rmpi/libs/*': No such file
> or directory
> ERROR: compilation failed for package 'Rmpi'
> ** Removing '/usr/local/lib64/R/library/Rmpi'
>
>
> can any of you please suggest as to what needs to be done?
>
> Thanks for your time
> regards
> Sunitha Manjari
> Bioinformatics Team
> C-DAC, Pune-411007
> Maharashtra
> INDIA
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Bioconductor / Computational Biology
http://bioconductor.org


From ripley at stats.ox.ac.uk  Mon Jul 23 19:00:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jul 2007 18:00:47 +0100 (BST)
Subject: [R] cmdscale question
In-Reply-To: <11747336.post@talk.nabble.com>
References: <11747336.post@talk.nabble.com>
Message-ID: <Pine.LNX.4.64.0707231757240.11181@gannet.stats.ox.ac.uk>

Why not do your homework instead of sending the same message three times?

The references on the help page (especially Cox & Cox) will explain to you 
how scaling works on dissimilarities.

There are better alternatives for non-Euclidean dissimilarities: see MASS 
(the book) and its supporting software.

On Mon, 23 Jul 2007, mister_bluesman wrote:

>
> Hi.
>
> I know matrices that use distances between places works fine when using
> cmdscale. However, what about matricies such as:
>
>  A     B   C   D   E
> A 0    1   23  12  9
> B 1    0   10  12  3
> C 23  10   0   23  4
> D 12  12  23  0   21
> E  9   3    4   21   0
>
> i.e. matrices which do not represent physical distances between places (as
> they would not make sense for real distances such as the one above) but
> other statistics instead?
>
> Thanks
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From neil at iwr.ru.ac.za  Mon Jul 23 19:03:40 2007
From: neil at iwr.ru.ac.za (Neil Griffin)
Date: Mon, 23 Jul 2007 19:03:40 +0200
Subject: [R] cca and cca.predict in vegan-what sort of prediction is possible
Message-ID: <46A4FB8C.10796.236A576@localhost>

Hi All

I am not clear quite how one could use cca from package vegan and the associated 
predict.cca to predict species abundance from environmental data (or if this is possible 
in a generalised way). In other words, can one derive a cca object based on known 
community data and use that to predict e.g. species abundances in a different number 
of samples based on environmental data? The help notes show that prediction is 
possible, but it seems that the number of samples is constrained to that in the original, 
"training" set. 

If this is possible, a reference or example would be much, much appreciated.

Thanks
Neil


Neil Griffin
Unilever Centre for Environmental Water Quality
Institute for Water Research, Rhodes University
PO Box 94, Grahamstown, 6140, South Africa
Email: neil at iwr.ru.ac.za 
http://www.rhodes.ac.za/institutes/iwr/
Tel: +27 46 622 2428
Fax: +27 46 622 9427


From linlicc at hotmail.com  Mon Jul 23 19:02:05 2007
From: linlicc at hotmail.com (W. Katy)
Date: Mon, 23 Jul 2007 17:02:05 +0000
Subject: [R]  Questions concerning HOPACH
In-Reply-To: <46A4C1FD.3080706@toulouse.inra.fr>
Message-ID: <BAY102-F21804AA6CCEFCA288E4F18A1F70@phx.gbl>

Hi,

I have been tried to apply hopach HOPACH in R to cluster some genes.  My 
data is a binary matrix.  It works well for the whole set of genes (say, 
1000 genes).  Then I selected a subset of the genes (say 300 genes).  These 
genes are selected based on the regression analysis results for each of the 
1000 genes and only those with highly correlations with specific sample 
types got selected.  With the subset of the genes (300), when I run HOPACH, 
the hopach stuck there for ever, and the CPU occupation kePT 100% as well.  


So I am wondering if there is any restrictions in applying HOPACH commend. 
Apparently, this stuck is not because of memory issue based on the CPU 
monitor.  I will really appreicate any share of comments, thoughts or 
suggestions.  

Thanks very much!

Regards, Katy


From meeryana at yahoo.com  Mon Jul 23 16:21:12 2007
From: meeryana at yahoo.com (copula)
Date: Mon, 23 Jul 2007 07:21:12 -0700 (PDT)
Subject: [R] R and Copula
In-Reply-To: <11741580.post@talk.nabble.com>
References: <11612986.post@talk.nabble.com>
	<OFC94AAA62.BB5EBA68-ON6525731C.002B7FA8-6525731C.002CE93A@ccilindia.co.in>
	<11683844.post@talk.nabble.com> <469FC845.2060704@u.washington.edu>
	<11705736.post@talk.nabble.com> <11741580.post@talk.nabble.com>
Message-ID: <11741607.post@talk.nabble.com>


only mlcbbsel package. i have the first one.
sorry


copula wrote:
> 
> new question:
> 
> Where can I find the packages "mlCopulaSelection" and "mlcbbsel"?
> 
> 
> 
> copula wrote:
>> 
>> and one more time:
>> 
>> thank you a lot for help!!!
>> 
>> 
>> 
>> Julian Burgos wrote:
>>> 
>>> Hi Meeryana,
>>> 
>>> It seems you have not loaded the package.  To use a package that you 
>>> already installed, do:
>>> 
>>>  > library(copula)
>>> 
>>> I recommend you review the R documentation.  There are several good 
>>> references and tutorial on the CRAN site:
>>> 
>>> http://cran.r-project.org/manuals.html
>>> http://cran.r-project.org/other-docs.html
>>> 
>>> Any time you spend learning the basics will make your work more 
>>> efficient. :)
>>> Good luck,
>>> 
>>> Julian
>>> 
>>> Fisheries Acoustics Research Lab
>>> School of Aquatic and Fishery Science
>>> 
>>> copula wrote:
>>>> I have installed fast all packages for copula,
>>>> I have installed package 'gnml' and the others from the Jim Lindsey's
>>>> web
>>>> site and the other packages like 'repeted' which is necessary for
>>>> calculating copula.
>>>>  
>>>> but when I start with copula it write: 
>>>> 'gausscop' function is not found. And also 'fitted.gausscop(z)' and
>>>> 'residuals.gausscop(z)'.
>>>>
>>>> also what I want to say is: when I have installed package 'repeted'
>>>> from Jim
>>>> Lindsey's web site in R project, this what I have received: 
>>>>  
>>>> "utils:::menuInstallLocal()
>>>> updating HTML package descriptions"
>>>>
>>>> what have I to do next to solve this problem and to continue with
>>>> copula.
>>>>
>>>>
>>>> Thanks a lot for previous help!
>>>>
>>>>
>>>>  
>>>>
>>>>   
>>>>
>>>> gyadav wrote:
>>>>   
>>>>> hi meeryana,
>>>>>
>>>>> may be this time nobnody is responding. but dont worry you will get a
>>>>> lot 
>>>>> of help eventually, so always post a copy to the mailing list.
>>>>> The reason is there are a lot many newbies, although i am also not so
>>>>> old 
>>>>> enough, who have even the simplest questions but are hesitant to ask.
>>>>> the objective of the list is to help, and thus feel free to ask, stand
>>>>> and 
>>>>> contribute :-) i hope you will understand not today then tomorrow as
>>>>> you 
>>>>> will get associated with the mailing list more closely. Personally i
>>>>> have 
>>>>> found friends here. Further, never post a mail with a loose subject
>>>>> and 
>>>>> secondly try to maintain the thread i.e. reply to the mail which you
>>>>> want 
>>>>> to reply(for more details refer to the posting guide) :-) that would 
>>>>> really help. 
>>>>>
>>>>> Yes now regarding your question :-) 
>>>>>
>>>>> Step1 : List of all packages can be found at this link :-)
>>>>> http://cran.r-project.org/src/contrib/PACKAGES.html
>>>>> Step2: Click on the package you want to install :-) 
>>>>> http://cran.r-project.org/src/contrib/Descriptions/copula.html
>>>>> Step3: Then download the binary of your Operating system. If windows
>>>>> then 
>>>>> download corresponding zip file.
>>>>> for copula it is
>>>>> 
>>>>> http://cran.r-project.org/bin/windows/contrib/r-release/copula_0.5-3.zip
>>>>> save zip file on your system
>>>>> Step4 Open your R rpogram
>>>>> Step5: Goto Packages -> Install packages from Local Zip file
>>>>> Step6: Select your package zip file which you want to install
>>>>> Step7: Sit back and relax
>>>>> Step8: load the library using library(LibraryName) on R prompt
>>>>>
>>>>> There are alternate ways of installing the package directly from R
>>>>> prompt. 
>>>>> It didn't worked for me a long time back, so  i always adopt this
>>>>> method.
>>>>> Somebody on the list may help you in this regards :-)
>>>>>
>>>>> bye and learn
>>>>> Join and stand with Open Source and R community
>>>>> Cheers and Chiao, Welcome
>>>>> -gaurav
>>>>>
>>>>>
>>>>>
>>>>> dear Mr. Yadav,
>>>>>
>>>>> I want to thank for help, 
>>>>> and for that you are only who is willing to help,
>>>>> but I have one question:
>>>>> because I'm new with R project also, I think I should install a
>>>>> package 
>>>>> for copula.
>>>>> I have only installed R program.
>>>>> How should I install this package? And is it what I have also to do
>>>>> with 
>>>>> credit metrics, Value at Risk, matix and the other formulas, I mean 
>>>>> install packages.
>>>>>
>>>>> I hope that you have a little time for me and my problem, and I hope
>>>>> I'm 
>>>>> not disturbing you.
>>>>> thank you for all you can do for me 
>>>>>
>>>>> and 
>>>>>
>>>>> best regards,
>>>>>
>>>>> Mirjana
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> gyadav wrote:
>>>>>     
>>>>>> hi
>>>>>>
>>>>>> see the code below i hope this will make your understanding of
>>>>>> copulas 
>>>>>> better
>>>>>> this code plots two normal distribution and their joint distribution 
>>>>>> N[0,2] & N[0,4]
>>>>>>
>>>>>> HTH
>>>>>>
>>>>>> ######################code################################
>>>>>> library("copula")
>>>>>> ###################copy in two parts in R#################
>>>>>>
>>>>>> ##################PART A##################################
>>>>>> ## construct a bivariate distribution whose marginals
>>>>>> ## are normal and Normal respectively, coupled
>>>>>> ## together via a normal copula
>>>>>> op <- par(mfrow = c(2, 2), # 2 x 2 pictures on one plot
>>>>>>           pty = "s")       # square plotting region,
>>>>>>                            # independent of device size
>>>>>>
>>>>>> x <- mvdc(normalCopula(0.75), c("norm", "norm"),
>>>>>> list(list(mean = 0, sd =2),list(mean = 0, sd =4)))
>>>>>> x.samp <- rmvdc(x, 10000)
>>>>>> par(mfrow=c(2,3))
>>>>>> hist(x.samp[,1],xlab="Normal")
>>>>>> hist(x.samp[,2],xlab="Normal")
>>>>>> plot(x.samp[,2],x.samp[,1],pch=21,xlab="Normal",ylab="Normal")
>>>>>>
>>>>>> plot(dmvdc(x, x.samp))
>>>>>> plot(pmvdc(x, x.samp))
>>>>>>
>>>>>> ## At end of plotting, reset to previous settings:
>>>>>>
>>>>>>
>>>>>> ###########################PART B#######################
>>>>>> par(op)
>>>>>> for (i in seq(1:360)){
>>>>>> persp(x, dmvdc, xlim = c(-4, 4), ylim=c(0, 1),theta=i)
>>>>>> }
>>>>>>
>>>>>>
>>>>>> Regards,
>>>>>>
>>>>>> Gaurav Yadav
>>>>>> +++++++++++
>>>>>> Assistant Manager, CCIL, Mumbai (India)
>>>>>> Mob: +919821286118 Email: emailtogauravyadav at gmail.com
>>>>>> Bhagavad Gita:  Man is made by his Belief, as He believes, so He is
>>>>>>
>>>>>>
>>>>>>
>>>>>> copula <meeryana at yahoo.com> 
>>>>>> Sent by: r-help-bounces at stat.math.ethz.ch
>>>>>> 07/17/2007 12:53 PM
>>>>>>
>>>>>> To
>>>>>> r-help at stat.math.ethz.ch
>>>>>> cc
>>>>>>
>>>>>> Subject
>>>>>> Re: [R] R and Copula
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> it would be great when somebody will help me
>>>>>> thanks
>>>>>>
>>>>>>
>>>>>> copula wrote:
>>>>>>       
>>>>>>> hi,
>>>>>>> first I want to say that I'm new here, and new with copula and R.
>>>>>>>
>>>>>>> That is the reason why I'm writing, if somebody can help me. 
>>>>>>>
>>>>>>> I have to make an example of Copula. 
>>>>>>> On internet I've found this forum and that copula can calculate with
>>>>>>> R.
>>>>>>>
>>>>>>> Can somebody help me with the thing how can I start and where can
>>>>>>> read
>>>>>>> about these stuffs.
>>>>>>>
>>>>>>> Thank to all who can help!
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>         
>>>>>> -- 
>>>>>> View this message in context: 
>>>>>> http://www.nabble.com/R-and-Copula-tf4085867.html#a11644534
>>>>>> Sent from the R help mailing list archive at Nabble.com.
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide 
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>       
>>>>> ============================================================================================
>>>>>     
>>>>>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and 
>>>>>>       
>>>>> ...{{dropped}}
>>>>>     
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>>
>>>>>>       
>>>>> Quoted from: 
>>>>> http://www.nabble.com/R-and-Copula-tf4085867.html#a11645614
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> ============================================================================================
>>>>> DISCLAIMER AND CONFIDENTIALITY CAUTION:\ \ This message and
>>>>> ...{{dropped}}
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>
>>>>>
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> 
>> 
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/R-and-Copula-tf4085867.html#a11741607
Sent from the R help mailing list archive at Nabble.com.


From sbearer at TNC.ORG  Mon Jul 23 19:38:37 2007
From: sbearer at TNC.ORG (Scott Bearer)
Date: Mon, 23 Jul 2007 13:38:37 -0400
Subject: [R] Cluster prediction from factor/numeric datasets
Message-ID: <FOEPJOMFPIAGCACENKDMGEMICEAA.sbearer@tnc.org>

Hi all,

I have a dataset with numeric and factor columns of data which I developed a
Gower Dissimilarity Matrix for (Daisy) and used Agglomerative Nesting
(Agnes) to develop 20 clusters.

I would like to use the 20 clusters to determine cluster membership for a
new dataset (using predict) but cannot find a way to do this (no way to
"predict" in the cluster package).

I know I can use "predict" in cclust, kcca, and flexclust- but these
algorithms do not permit factor data or use a Gower dissimilarity matrix, so
are unusable to me.

Any suggestions?

Thanks in advance,

Scott

Scott Bearer, Ph.D.
Forest Ecologist
The Nature Conservancy
  in Pennsylvania
Community Arts Center
220 West Fourth Street, 3rd Floor
Williamsport, PA  17701


From gchappi at gmail.com  Mon Jul 23 19:38:43 2007
From: gchappi at gmail.com (Hans-Peter)
Date: Mon, 23 Jul 2007 10:38:43 -0700
Subject: [R] Help with Dates
In-Reply-To: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
References: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
Message-ID: <47fce0650707231038k52846526n5d7af575d88ff9f7@mail.gmail.com>

> I am taking an excel dataset and reading it into R using read.table.
> (actually I am dumping the data into a .txt file first and then reading data
> in to R).

If you are on *windows* you could also try my xlsReadWrite package
which contains some datetime functions. Exceldates (e.g. formatted as
dd-mmm-yy) can be read as COleDateTime (floating point) values or as
character strings.  The first one is preferable imo as it avoids a
typecast and it is the type commonly used in OLE automation.

> But how can I subset for multiple periods e.g 00- 05?

Floating point numbers dates can be converted to year-strings with
dateTimeToStr( value, "yy" ) and then subset as shown in a previous
post. The (paid) pro version contains many more date functions, e.g.
yearOf. Details see
http://treetron.googlepages.com/xls.oledatetimeex.html.

-- 
Regards,
Hans-Peter


From cg.pettersson at vpe.slu.se  Mon Jul 23 19:59:31 2007
From: cg.pettersson at vpe.slu.se (CG Pettersson)
Date: Mon, 23 Jul 2007 19:59:31 +0200 (CEST)
Subject: [R] The 'REP' term in AMMI{agricolae}
Message-ID: <63053.212.112.39.227.1185213571.squirrel@webmail.slu.se>

Dear all,

W2k, R 2.5.1

I am trying out the AMMI function in the agricolae package, to analyse the
dependence of environment for a certain cultivar. The function responds to
four basic variables:

ENV Environment
GEN Genotype
REP Replication
Y Response

My question concerns the 'REP' term. When I normally do an analysis of
variance, the replication would refer to repeated observations within the
each field trial. In the case I am analysing right now, I have five years
of observations for each 'ENV', in such a case it feels natural to use
year as the most important replication of the environment- especially as I
am interested in long term trends. This approach also seems to work
allright.

But the field trials are also structured in three main blocks, subdivided
into five 'lattice' blocks, a structure that is powerful in the analysis
of variance. (I use a random call in lme{nlme}).

Is it possible to use the block structure also in the AMMI analysis? If
that is possible, how should I code? I have tried to find out in the
documentation, but if it is stated there I do not understand it.

Thank you
/CG

-- 
CG Pettersson, PhD
Swedish University of Agricultural Sciences (SLU)
Dept. of Crop Production Ecology. Box 7043.
SE-750 07 Uppsala, Sweden
cg.pettersson at vpe.slu.se


From S.O.Nyangoma at amc.uva.nl  Mon Jul 23 20:13:55 2007
From: S.O.Nyangoma at amc.uva.nl (S.O. Nyangoma)
Date: Mon, 23 Jul 2007 19:13:55 +0100
Subject: [R] nnet 10-fold cross-validation
Message-ID: <9f5939ea83.9ea839f593@amc.uva.nl>

Hi
It clear that to do a classification with svm under 10-fold cross 
validation one uses

svm(Xm, newlabs, type = "C-classification", kernel = "linear",cross = 
10)

What corresponds to the nnet?
nnet(.....,cross=10)?

Regards


From joseclaudio.faria at terra.com.br  Mon Jul 23 20:30:12 2007
From: joseclaudio.faria at terra.com.br (terra)
Date: Mon, 23 Jul 2007 15:30:12 -0300
Subject: [R] doubt about options(graphics.record=T)
In-Reply-To: <Pine.LNX.4.64.0707231720470.29667@gannet.stats.ox.ac.uk>
References: <46A4CC31.5080604@terra.com.br>
	<Pine.LNX.4.64.0707231720470.29667@gannet.stats.ox.ac.uk>
Message-ID: <46A4F3B4.8070901@terra.com.br>

>> Hi all,
>>
>> I've been using R under WindowsXP.
>>
>> So, where the R stores the graphic archives (don't saved) if I use the 
>> option
>> options(graphics.record=T) inside of Rprofile.site file?
> 
> The relevant help file (?windows) does tell you: please read it.

Dear Prof. Ripley,

I read the recommended (?windows) end it was not clear enough!

BTW, I just found a discussion from ([R] RGui: windows-record and command history Thomas 
Steiner (23 Mar 2006)) where Duncan wrote:

- The graphics history is stored in your current workspace in memory, and it can get big.

I think it is the answer I was searching. Do you agree?

Regards,

/////\\\\\/////\\\\\/////\\\\\/////\\\\\
  Jose Claudio Faria
  Brasil/Bahia/UESC/DCET
  Estatistica Experimental/Prof. Titular
    joseclaudio.faria em terra.com.br
    joseclaudio.faria em oi.com.br
    jc_faria em uesc.br
    jc_faria em uol.com.br
  Tels:
    73-3634.2779 (res - Ilheus/BA)
    19-3435.1536 (res - Piracicaba/SP) *
    19-9144.8979 (cel - Piracicaba/SP) *
/////\\\\\/////\\\\\/////\\\\\/////\\\\\


From ripley at stats.ox.ac.uk  Mon Jul 23 20:45:09 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jul 2007 19:45:09 +0100 (BST)
Subject: [R] doubt about options(graphics.record=T)
In-Reply-To: <46A4F3B4.8070901@terra.com.br>
References: <46A4CC31.5080604@terra.com.br>
	<Pine.LNX.4.64.0707231720470.29667@gannet.stats.ox.ac.uk>
	<46A4F3B4.8070901@terra.com.br>
Message-ID: <Pine.LNX.4.64.0707231940200.1903@gannet.stats.ox.ac.uk>

On Mon, 23 Jul 2007, terra wrote:

>>> Hi all,
>>> 
>>> I've been using R under WindowsXP.
>>> 
>>> So, where the R stores the graphic archives (don't saved) if I use the 
>>> option
>>> options(graphics.record=T) inside of Rprofile.site file?
>> 
>> The relevant help file (?windows) does tell you: please read it.
>
> Dear Prof. Ripley,
>
> I read the recommended (?windows) end it was not clear enough!
>
> BTW, I just found a discussion from ([R] RGui: windows-record and command 
> history Thomas Steiner (23 Mar 2006)) where Duncan wrote:
>
> - The graphics history is stored in your current workspace in memory, and it 
> can get big.
>
> I think it is the answer I was searching. Do you agree?

I refer you once again to the help page, which says

      The active plot history is stored in variable
      '.SavedPlots' in the workspace.

I don't see any lack of clarity there (unlike elsewhere in this posting).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gunter.berton at gene.com  Mon Jul 23 20:45:33 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 23 Jul 2007 11:45:33 -0700
Subject: [R] doubt about options(graphics.record=T)
In-Reply-To: <46A4F3B4.8070901@terra.com.br>
Message-ID: <009a01c7cd59$a71e9050$4d908980@gne.windows.gene.com>

Below is an explicit excerpt from the Help file. How, please is this "not
clear enough?"

Bert Gunter
Genentech Nonclinical Statistics


Recorded plot histories are of class "SavedPlots". They have a print method,
and a subset method. As the individual plots are of class "recordedplot"
they can be replayed by printing them: see recordPlot.

 <<The active plot history is stored in variable .SavedPlots in the
workspace. >> [emphasis added]



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of terra
Sent: Monday, July 23, 2007 11:30 AM
To: Prof Brian Ripley; r-help at stat.math.ethz.ch
Subject: Re: [R] doubt about options(graphics.record=T)

>> Hi all,
>>
>> I've been using R under WindowsXP.
>>
>> So, where the R stores the graphic archives (don't saved) if I use the 
>> option
>> options(graphics.record=T) inside of Rprofile.site file?
> 
> The relevant help file (?windows) does tell you: please read it.

Dear Prof. Ripley,

I read the recommended (?windows) end it was not clear enough!

BTW, I just found a discussion from ([R] RGui: windows-record and command
history Thomas 
Steiner (23 Mar 2006)) where Duncan wrote:

- The graphics history is stored in your current workspace in memory, and it
can get big.

I think it is the answer I was searching. Do you agree?

Regards,

/////\\\\\/////\\\\\/////\\\\\/////\\\\\
  Jose Claudio Faria
  Brasil/Bahia/UESC/DCET
  Estatistica Experimental/Prof. Titular
    joseclaudio.faria at terra.com.br
    joseclaudio.faria at oi.com.br
    jc_faria at uesc.br
    jc_faria at uol.com.br
  Tels:
    73-3634.2779 (res - Ilheus/BA)
    19-3435.1536 (res - Piracicaba/SP) *
    19-9144.8979 (cel - Piracicaba/SP) *
/////\\\\\/////\\\\\/////\\\\\/////\\\\\

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jmacdon at med.umich.edu  Mon Jul 23 21:00:39 2007
From: jmacdon at med.umich.edu (James MacDonald)
Date: Mon, 23 Jul 2007 15:00:39 -0400
Subject: [R] OT(slightly) - Tracking extended projects
Message-ID: <46A4C298.47B0.00EE.0@med.umich.edu>

Hi all,

Most of the analyses I do are short little once-and-done type things that are easily encapsulated in a .Rnw file. However, I sometimes end up with projects that take an extended amount of time. Usually these projects are not easily encapsulated in an .Rnw file, so I have been using a single .R file with lots of comments.

The problem with this approach is keeping track of what you have done and what the results were. Once the .R file gets to be a certain size, the comments aren't as useful, and I find it easy to get lost. I have to assume that others have encountered this problem and hopefully have come up with something more elegant.

Any suggestions?

Best,

Jim


-- 

James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623



**********************************************************
Electronic Mail is not secure, may not be read every day, and should not be used for urgent or sensitive issues.


From yvonnick.noel at uhb.fr  Mon Jul 23 21:18:07 2007
From: yvonnick.noel at uhb.fr (Yvonnick NOEL)
Date: Mon, 23 Jul 2007 21:18:07 +0200
Subject: [R] Can I test if there are statistical significance between
 different rows in R*C table?
In-Reply-To: <mailman.9.1185184803.2168.r-help@stat.math.ethz.ch>
References: <mailman.9.1185184803.2168.r-help@stat.math.ethz.ch>
Message-ID: <46A4FEEF.3080801@uhb.fr>

> With the totally non-committal P-value for Group 2 vs Group 3,
> and the absolutely decisive P-value for Group 1 vs Groups 2&3,
> there is no need whatever to bother with "multiple comparison"
> complications.

Note that you can test this as the formal comparison between three 
nested multinomial models, the first assuming homogeneity of all three 
response distributions (M0), the second assuming a similar multinomial 
distribution for groups 2 and 3, and a specific one for the first group 
(M1), and the saturated model (M3).

Maximum likelihood estimates of the corresponding multinomial 
probabilities are:

M0
      Resp   Bad Better  Good
Group                       
Grp1       0.613  0.067 0.320
Grp2       0.613  0.067 0.320
Grp3       0.613  0.067 0.320

M1
      Resp   Bad Better  Good
Group                       
Grp1       0.298  0.129 0.573
Grp2       0.915  0.008 0.078
Grp3       0.915  0.008 0.078

M2
      Resp   Bad Better  Good
Group                       
Grp1       0.298  0.129 0.573
Grp2       0.938  0.000 0.062
Grp3       0.891  0.016 0.094


Comparing them with a likelihood ratio, rather than a Pearson 
chi-square, is probably more appropriate, given the additivity property 
of likelihood ratio with nested models. You get:

   res. df. res. dev.       df. lik.ratio P(>chi2)    AIC     BIC
M0    4.000   114.023                             154.959 162.026
M1    2.000     1.917     2.000 112.106 4.533e-25  46.852  60.986
M2    0.000 4.122e-10     2.000   1.917     0.383  48.936  70.136


M1 is clearly the best model on these data. You can view this as an 
equivalent of orthogonal contrasts in ANOVA, and so indeed you do not 
have to worry about correcting your alpha.

HTH

Yvonnick Noel
University of Rennes 2
France


From jacques.wagnor at gmail.com  Mon Jul 23 21:18:44 2007
From: jacques.wagnor at gmail.com (Jacques Wagnor)
Date: Mon, 23 Jul 2007 14:18:44 -0500
Subject: [R] Aggregate daily data into weekly sums
Message-ID: <787911d50707231218l7b49f2cfy7d5d8c78a7080bdf@mail.gmail.com>

Dear Lest,

I have a two-variable data frame as follows (the time peirod of the
actual data set is 10 years):

        Date Amount
1   6/1/2007      1
2   6/1/2007      1
3   6/4/2007      2
4   6/5/2007      2
5  6/11/2007      3
6  6/12/2007      3
7  6/12/2007      3
8  6/13/2007      3
9  6/13/2007      3
10 6/18/2007      4
11 6/18/2007      4
12 6/25/2007      5
13 6/28/2007      5


Basically, I would like to collapse the daily data into weekly sums
such that the result should look like the following:

              Date Amount
1  2007/6/Week1       2
2  2007/6/Week2       4
3  2007/6/Week3       15
4  2007/6/Week4       8
5  2007/6/Week5      10

Does there already exist a function that aggregates the data at
user-defined time frequency?

Any pointers would be greatly appreciated.

Jacques

> version
               _
platform       i386-pc-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status
major          2
minor          5.0
year           2007
month          04
day            23
svn rev        41293
language       R
version.string R version 2.5.0 (2007-04-23)


From mark.lyman at gmail.com  Mon Jul 23 21:22:13 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Mon, 23 Jul 2007 19:22:13 +0000 (UTC)
Subject: [R] problems with character objects and calls to list()
References: <31b34fca0707230750v29e527efvcdf816a480bd3890@mail.gmail.com>
Message-ID: <loom.20070723T212107-164@post.gmane.org>

> Really I'd like the call to list() to behave as though the text had
> been entered directly so that you get....
> 
> > list(1:2, 3:4, 5:6)
> [[1]]
> [1] 1 2
> 
> [[2]]
> [1] 3 4
> 
> [[3]]
> [1] 5 6
>

> eval(parse(text=paste("list(",to.convert,")",sep="")))
[[1]]
[1] 1 2

[[2]]
[1] 3 4

[[3]]
[1] 5 6

[[4]]
[1] 7 8

[[5]]
[1]  9 10

[[6]]
[1] 11 12


From ngottlieb at marinercapital.com  Mon Jul 23 21:35:12 2007
From: ngottlieb at marinercapital.com (ngottlieb at marinercapital.com)
Date: Mon, 23 Jul 2007 15:35:12 -0400
Subject: [R] Cluster prediction from factor/numeric datasets
In-Reply-To: <FOEPJOMFPIAGCACENKDMGEMICEAA.sbearer@tnc.org>
References: <FOEPJOMFPIAGCACENKDMGEMICEAA.sbearer@tnc.org>
Message-ID: <0946E293C7C22A45A0E33BA14FAA8D88F38916@500MAIL.goldbox.com>

Scott:

Suggest you look at using Discrimnant Analysis (don't know which R
package has it). 
Take the Clusters created, using Discrimnant Analysis, Get Fisher Scores
for the clusters.
Then you can take new dataset applying fisher scores to see what which
defined cluster the new dataset
will be classified into.

Neil 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Scott Bearer
Sent: Monday, July 23, 2007 1:39 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Cluster prediction from factor/numeric datasets

Hi all,

I have a dataset with numeric and factor columns of data which I
developed a Gower Dissimilarity Matrix for (Daisy) and used
Agglomerative Nesting
(Agnes) to develop 20 clusters.

I would like to use the 20 clusters to determine cluster membership for
a new dataset (using predict) but cannot find a way to do this (no way
to "predict" in the cluster package).

I know I can use "predict" in cclust, kcca, and flexclust- but these
algorithms do not permit factor data or use a Gower dissimilarity
matrix, so are unusable to me.

Any suggestions?

Thanks in advance,

Scott

Scott Bearer, Ph.D.
Forest Ecologist
The Nature Conservancy
  in Pennsylvania
Community Arts Center
220 West Fourth Street, 3rd Floor
Williamsport, PA  17701

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
--------------------------------------------------------

 
 
This information is being sent at the recipient's request or...{{dropped}}


From liuwensui at gmail.com  Mon Jul 23 21:35:24 2007
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 23 Jul 2007 15:35:24 -0400
Subject: [R] nnet 10-fold cross-validation
In-Reply-To: <9f5939ea83.9ea839f593@amc.uva.nl>
References: <9f5939ea83.9ea839f593@amc.uva.nl>
Message-ID: <1115a2b00707231235k3e04b780lce6b1a96e7b0cc79@mail.gmail.com>

there is no such thing in nnet(), if i understand correctly.
how hard it is to code one though?

On 7/23/07, S.O. Nyangoma <S.O.Nyangoma at amc.uva.nl> wrote:
> Hi
> It clear that to do a classification with svm under 10-fold cross
> validation one uses
>
> svm(Xm, newlabs, type = "C-classification", kernel = "linear",cross =
> 10)
>
> What corresponds to the nnet?
> nnet(.....,cross=10)?
>
> Regards
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
===============================
"I am dying with the help of too many
physicians." - Alexander the Great, on his deathbed
===============================
WenSui Liu
(http://spaces.msn.com/statcompute/blog)


From ripley at stats.ox.ac.uk  Mon Jul 23 21:38:40 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jul 2007 20:38:40 +0100 (BST)
Subject: [R] nnet 10-fold cross-validation
In-Reply-To: <9f5939ea83.9ea839f593@amc.uva.nl>
References: <9f5939ea83.9ea839f593@amc.uva.nl>
Message-ID: <Pine.LNX.4.64.0707232037200.17460@gannet.stats.ox.ac.uk>

Please read the documentation.  nnet() is support software for a book, and 
that has explicit examples of this, which you will find in the scripts of 
the VR bundle (but I don't expect them to be comprehensible without the 
book).

On Mon, 23 Jul 2007, S.O. Nyangoma wrote:

> Hi
> It clear that to do a classification with svm under 10-fold cross
> validation one uses
>
> svm(Xm, newlabs, type = "C-classification", kernel = "linear",cross =
> 10)
>
> What corresponds to the nnet?
> nnet(.....,cross=10)?
>
> Regards
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From wwwhsd at gmail.com  Mon Jul 23 21:52:22 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Mon, 23 Jul 2007 16:52:22 -0300
Subject: [R] Aggregate daily data into weekly sums
In-Reply-To: <787911d50707231218l7b49f2cfy7d5d8c78a7080bdf@mail.gmail.com>
References: <787911d50707231218l7b49f2cfy7d5d8c78a7080bdf@mail.gmail.com>
Message-ID: <da79af330707231252la239ew903aa1eb29a4eb6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/f5adb440/attachment.pl 

From iwona.szydlowska at hanakom.pl  Mon Jul 23 22:06:58 2007
From: iwona.szydlowska at hanakom.pl (=?iso-8859-2?Q?Iwona_Szyd=B3owska?=)
Date: Mon, 23 Jul 2007 22:06:58 +0200
Subject: [R] extension of rnormp package
Message-ID: <003b01c7cd65$06883810$16b2a8c0@ivii>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/7a9480b4/attachment.pl 

From joseclaudio.faria at terra.com.br  Mon Jul 23 22:58:52 2007
From: joseclaudio.faria at terra.com.br (joseclaudio.faria)
Date: Mon, 23 Jul 2007 17:58:52 -0300
Subject: [R] doubt about options(graphics.record=T)
Message-ID: <JLNGY4$7CFB58F04BFCFB40EF329D1919E13254@multidominios>

Dears Gunter an Ripley,

Ok, it is clear now! So, this topic can be closed.

The problem was that the '.SavedPlots' object is generated in the user workspace only after a second plot
inside a graphic device and it is visible only with ls(all.names=T).

PS: obviously, this consumes memory.

Many thanks,

Jose Claudio Faria
Estat?stica Experimental - Prof. Titular
Universidade Estadual de Santa Cruz - UESC
Departamento de Ciencias Exatas e Tecnologicas - DCET
Bahia - Brasil
Tels:
73-3634.2779 (fixo Ilheus)
19-9144.8979 (Celular Piracicaba)


---------- Cabe?alho original -----------
De: "Bert Gunter" gunter.berton em gene.com
Para: "terra" joseclaudio.faria em terra.com.br, r-help em stat.math.ethz.ch
C?pia: "Prof Brian Ripley" ripley em stats.ox.ac.uk
Data: Mon, 23 Jul 2007 11:45:33 -0700
Assunto: RE: [R] doubt about options(graphics.record=T)

> Below is an explicit excerpt from the Help file. How, please is this "not
> clear enough?"
> 
> Bert Gunter
> Genentech Nonclinical Statistics
> 
> 
> Recorded plot histories are of class "SavedPlots". They have a print method,
> and a subset method. As the individual plots are of class "recordedplot"
> they can be replayed by printing them: see recordPlot.
> 
>  <<The active plot history is stored in variable .SavedPlots in the
> workspace. >> [emphasis added]
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces em stat.math.ethz.ch
> [mailto:r-help-bounces em stat.math.ethz.ch] On Behalf Of terra
> Sent: Monday, July 23, 2007 11:30 AM
> To: Prof Brian Ripley; r-help em stat.math.ethz.ch
> Subject: Re: [R] doubt about options(graphics.record=T)
> 
> >> Hi all,
> >>
> >> I've been using R under WindowsXP.
> >>
> >> So, where the R stores the graphic archives (don't saved) if I use the 
> >> option
> >> options(graphics.record=T) inside of Rprofile.site file?
> > 
> > The relevant help file (?windows) does tell you: please read it.
> 
> Dear Prof. Ripley,
> 
> I read the recommended (?windows) end it was not clear enough!
> 
> BTW, I just found a discussion from ([R] RGui: windows-record and command
> history Thomas 
> Steiner (23 Mar 2006)) where Duncan wrote:
> 
> - The graphics history is stored in your current workspace in memory, and it
> can get big.
> 
> I think it is the answer I was searching. Do you agree?
> 
> Regards,
> 
> /////\\\\\/////\\\\\/////\\\\\/////\\\\\
>   Jose Claudio Faria
>   Brasil/Bahia/UESC/DCET
>   Estatistica Experimental/Prof. Titular
>     joseclaudio.faria em terra.com.br
>     joseclaudio.faria em oi.com.br
>     jc_faria em uesc.br
>     jc_faria em uol.com.br
>   Tels:
>     73-3634.2779 (res - Ilheus/BA)
>     19-3435.1536 (res - Piracicaba/SP) *
>     19-9144.8979 (cel - Piracicaba/SP) *
> /////\\\\\/////\\\\\/////\\\\\/////\\\\\
> 
> ______________________________________________
> R-help em stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sabunime at gmail.com  Mon Jul 23 23:22:46 2007
From: sabunime at gmail.com (Saeed Abu Nimeh)
Date: Mon, 23 Jul 2007 16:22:46 -0500
Subject: [R] nnet 10-fold cross-validation
In-Reply-To: <9f5939ea83.9ea839f593@amc.uva.nl>
References: <9f5939ea83.9ea839f593@amc.uva.nl>
Message-ID: <46A51C26.3050208@gmail.com>

do it manually. divide your dataset into 10 parts then train and test
accordingly.
Saeed


S.O. Nyangoma wrote:
> Hi
> It clear that to do a classification with svm under 10-fold cross 
> validation one uses
> 
> svm(Xm, newlabs, type = "C-classification", kernel = "linear",cross = 
> 10)
> 
> What corresponds to the nnet?
> nnet(.....,cross=10)?
> 
> Regards
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From solie.alizadeh at utoronto.ca  Mon Jul 23 23:23:22 2007
From: solie.alizadeh at utoronto.ca (solie.alizadeh at utoronto.ca)
Date: Mon, 23 Jul 2007 17:23:22 -0400
Subject: [R] M value and fold change
Message-ID: <20070723172322.oct5vk9duoc0sc0o@webmail.utoronto.ca>

Hi everyone,

I have a question about M value and how it corresponds to gene  
expression fold change.I was wondering if for example M-value of -1.27  
which means 0.41 in fold change, doesn it mean 0.41 of the control (in  
my experiment wild type? Better say does it mean this gene is  
down-regulated by 41% or 59% (1-41)? I was abit confused about it. I  
appreciate if anyone can help me to figure this out.

Thanks in Advance,
Solmaz Alizadeh


From ripley at stats.ox.ac.uk  Mon Jul 23 23:25:51 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Jul 2007 22:25:51 +0100 (BST)
Subject: [R] Cluster prediction from factor/numeric datasets
In-Reply-To: <0946E293C7C22A45A0E33BA14FAA8D88F38916@500MAIL.goldbox.com>
References: <FOEPJOMFPIAGCACENKDMGEMICEAA.sbearer@tnc.org>
	<0946E293C7C22A45A0E33BA14FAA8D88F38916@500MAIL.goldbox.com>
Message-ID: <Pine.LNX.4.64.0707232214450.15688@gannet.stats.ox.ac.uk>

You can't do Discrimnant Analysis without a quadratic metric in a 
Euclidean space.  'Scott Bearer' explicitly does not want to assume that 
sort of distance measure.

I am not sure how he used Agnes to form 20 clusters: it forms a 
hierarchical clustering, so it really is not possible to predict from the 
results of such a clustering (you probably would not even predict the 
current cluster membership).

With a methods such as kmeans or PAM, there is a chance to predict: you 
allocate new units to the nearest cluster centre.  With PAM you can do 
this easily by computing a matrix of dissimilarities from new points to 
cluster centres and using which.min.

On Mon, 23 Jul 2007, ngottlieb at marinercapital.com wrote:

> Scott:
>
> Suggest you look at using Discrimnant Analysis (don't know which R
> package has it).
> Take the Clusters created, using Discrimnant Analysis, Get Fisher Scores
> for the clusters.

If you mean linear discriminant analysis, package MASS.  But there are 
many other classification techniques, many preferable to LDA and which 
allow non-Euclidean spaces of observations.

> Then you can take new dataset applying fisher scores to see what which
> defined cluster the new dataset
> will be classified into.
>
> Neil
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Scott Bearer
> Sent: Monday, July 23, 2007 1:39 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Cluster prediction from factor/numeric datasets
>
> Hi all,
>
> I have a dataset with numeric and factor columns of data which I
> developed a Gower Dissimilarity Matrix for (Daisy) and used
> Agglomerative Nesting
> (Agnes) to develop 20 clusters.
>
> I would like to use the 20 clusters to determine cluster membership for
> a new dataset (using predict) but cannot find a way to do this (no way
> to "predict" in the cluster package).
>
> I know I can use "predict" in cclust, kcca, and flexclust- but these
> algorithms do not permit factor data or use a Gower dissimilarity
> matrix, so are unusable to me.
>
> Any suggestions?
>
> Thanks in advance,
>
> Scott
>
> Scott Bearer, Ph.D.
> Forest Ecologist
> The Nature Conservancy
>  in Pennsylvania
> Community Arts Center
> 220 West Fourth Street, 3rd Floor
> Williamsport, PA  17701
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> --------------------------------------------------------
>
>
>
> This information is being sent at the recipient's request or...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From solie.alizadeh at utoronto.ca  Mon Jul 23 23:28:05 2007
From: solie.alizadeh at utoronto.ca (solie.alizadeh at utoronto.ca)
Date: Mon, 23 Jul 2007 17:28:05 -0400
Subject: [R] M-value and fold change
Message-ID: <20070723172805.v33ntycrdnolcwsw@webmail.utoronto.ca>

Hi everyone,

I have a question about M value and how it corresponds to gene  
expression fold change.I
was wondering if for example M-value of -1.27 which means 0.41 in fold  
change, doesn it
mean 0.41 of the control (in my experiment wild type? Better say does  
it mean this gene
is down-regulated by 41% or 59% (1-41)? I was abit confused about it.  
I appreciate if
anyone can help me to figure this out.

Thanks in Advance,
Solmaz Alizadeh


From antonio.raju at gmail.com  Mon Jul 23 23:49:13 2007
From: antonio.raju at gmail.com (antonio rodriguez)
Date: Mon, 23 Jul 2007 23:49:13 +0200
Subject: [R] Aggregate daily data into weekly sums
In-Reply-To: <da79af330707231252la239ew903aa1eb29a4eb6@mail.gmail.com>
References: <787911d50707231218l7b49f2cfy7d5d8c78a7080bdf@mail.gmail.com>
	<da79af330707231252la239ew903aa1eb29a4eb6@mail.gmail.com>
Message-ID: <46A52259.8050808@gmail.com>

Or,

z<-mydata #zoo object

new.time <- as.Date(7 * floor(as.numeric(time(z))/7) + 7)
z2 <- aggregate(z, new.time, mean)




Henrique Dallazuanna escribi?:
> Hi,
>
> Perhaps you can try:
>
>   
>> df
>>     
>          Date Amount
> 1  2007-06-01      1
> 2  2007-06-01      1
> 3  2007-06-04      2
> 4  2007-06-05      2
> 5  2007-06-11      3
> 6  2007-06-12      3
> 7  2007-06-12      3
> 8  2007-06-13      3
> 9  2007-06-13      3
> 10 2007-06-18      4
> 11 2007-06-18      4
> 12 2007-06-25      5
> 13 2007-06-28      5
>
> df_ok <- aggregate(df$Amount, by=list(df$Amount), FUN=sum)
> levels(df_ok$Group.1)<- paste("2007/06/Week", 1:5, sep="")
>   
> ------------------------------------------------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>   


-- 
=====
Por favor, si me mandas correos con copia a varias personas, 
pon mi direcci?n de correo en copia oculta (CCO), para evitar 
que acabe en montones de sitios, eliminando mi privacidad, 
favoreciendo la propagaci?n de virus y la proliferaci?n del SPAM. Gracias.
-----
If you send me e-mail which has also been sent to several other people,
kindly mark my address as blind-carbon-copy (or BCC), to avoid its
distribution, which affects my privacy, increases the likelihood of
spreading viruses, and leads to more SPAM. Thanks.
=====
Antes de imprimir este e-mail piense bien si es necesario hacerlo: El medioambiente es cosa de todos.
Before printing this email, assess if it is really needed.


From chrysopa at gmail.com  Tue Jul 24 00:04:55 2007
From: chrysopa at gmail.com (Ronaldo Reis Junior)
Date: Mon, 23 Jul 2007 19:04:55 -0300
Subject: [R] Function to separate effect in AOV
Message-ID: <200707231904.55564.chrysopa@gmail.com>

Hi,

I have a dummy question.

Suppose that I have two explanatory variable, T1 (A, B) and T2 (C, D) and one 
response variable.

> attach(dados)

> tapply(Y,list(T1,T2),mean)
         C        D
A 2.200000 10.20000
B 2.223333 20.26667

In this case, "A" and "B" inside "C" have no difference, but have differences 
inside "D"

I make this model:

> m <- aov(Y~T1*T2)
> 
> summary(m)
            Df Sum Sq Mean Sq F value    Pr(>F)    
T1           1  76.36   76.36  5617.9 1.119e-12 ***
T2           1 508.69  508.69 37426.7 5.704e-16 ***
T1:T2        1  75.65   75.65  5566.0 1.161e-12 ***
Residuals    8   0.11    0.01                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 


This result don't show the reality of the data, because I cant see that "A" 
and "B" inside "C" are the same.

The anova result is the same of an full different levels, like this:

> attach(dados2)
> 
> tapply(Y,list(T1,T2),mean)
         C        D
A 6.100000 10.20000
B 2.223333 20.26667
> 
> m <- aov(Y~T1*T2)
> 
> summary(m)
            Df Sum Sq Mean Sq F value    Pr(>F)    
T1           1  28.74   28.74  2114.3 5.529e-11 ***
T2           1 367.75  367.75 27056.7 2.088e-15 ***
T1:T2        1 145.81  145.81 10728.1 8.433e-14 ***
Residuals    8   0.11    0.01                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

In this case all level are different, C to D and A to B.

The question is:

The only way to find this real difference is:

1) make T1 and T2 like a Treatment variable with 4 levels (AC,BC,AD,BD)?

or

2) make 3 anova:
	a) Anova (A,B) inside C
	b) Anova (A,B) inside D
	c) Full factorial Anova (like this in the e-mail)

or

3) exist any other way to make this in only one analysis, to find all 
differences e interactions? In other words, to find differences in "A" 
and "B" inside "C", "A" and "B" inside "D", "C" and "D" inside "A" and "C" 
and "D" inside "B" 

Thanks
Ronaldo
--
> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/Depto. Biologia Geral/Lab. de Ecologia
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8187 | ronaldo.reis em unimontes.br | chrysopa em gmail.com
| http://www.ppgcb.unimontes.br/ | ICQ#: 5692561 | LinuxUser#: 205366


From Max.Kuhn at pfizer.com  Tue Jul 24 01:22:47 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Mon, 23 Jul 2007 19:22:47 -0400
Subject: [R]  odfWeave - How to Insert eps rather than png
Message-ID: <71257D09F114DA4A8E134DEAC70F25D309007CBF@groamrexm03.amer.pfizer.com>

Rick,

First, can you send details about your versions/OS using sessionInfo()?

> If you create plots and use odfWeave to create an odf text document,
the
> default for figures is png bitmap graphics. The only way I've found to
> insert eps graphics is to first create the eps graphic using the
> postscript driver and then use odfInsertPlot. I've tried to use

Did the code chunk *fail* or did it complete without error? What were
the options in setImageDefs? I'll need more detail to provide you with
any help.

> getImageDefs and setImageDefs but I get an empty plot. Could someone
> show an example?

A quick search of "odfWeave postscript" resulted in

   http://finzi.psych.upenn.edu/R/Rhelp02a/archive/103278.html

   http://finzi.psych.upenn.edu/R/Rhelp02a/archive/103302.html

OpenOffice uses other applications (like ImageMagick's convert) to
generate previews of ps images. If you are on windows, there is an non-R
issue where Microsoft's file utility called convert interferes with the
other convert and no image shows up (or prints). I have the same issue.
It should be fixable by putting ImageMagick earlier in the path, but I
have not tried this yet.

Max

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From jholtman at gmail.com  Tue Jul 24 01:39:23 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 23 Jul 2007 19:39:23 -0400
Subject: [R] code optimization tips
In-Reply-To: <77115831-BD7B-48EF-981C-D13DA4289895@ex.ac.uk>
References: <D4E253DF-0B41-4CF5-BDAE-5009EDC88D53@ex.ac.uk>
	<644e1f320707230719h1bb86c25xe2aace73f34eecdf@mail.gmail.com>
	<77115831-BD7B-48EF-981C-D13DA4289895@ex.ac.uk>
Message-ID: <644e1f320707231639s421bfcd9xfe133fe41b0ec937@mail.gmail.com>

The quote "what is the problem you are trying to solve" is just part
of my signature.  I used to review projects for performance and
architecture and that was the first question I always asked them.

To pass the argument, if you notice the definition of apply:

apply(X, MARGIN, FUN, ...)

the ... are optional argument, so for your function:

sij <-function(rj,ri,k){
    rij=mod(rj-ri)
    cos_ij=rj[1]/rij
    sin_ij=rj[2]/rij
    A<-(1-1i*k*rij)*(3*cos_ij^2-1)*exp(1i*k*rij)/(rij^3)
    B<-k^2*sin_ij^2*exp(1i*k*rij)/rij
    sij<-A+B
}

you would call apply with the following:

s_ij<-apply(rj,2,sij, ri=ri, k=k)


On 7/23/07, baptiste Augui? <ba208 at exeter.ac.uk> wrote:
> Thanks for your reply,
>
> On 23 Jul 2007, at 15:19, jim holtman wrote:
>
> > First question is why are you defining the functions within the main
> > function each time?  Why don't you define them once outside?
> >
>
>
> Fair enough!
>
> As said, I'm new to R and don't know whether it is best to define
> functions outside and pass to them all necessary arguments, or nest
> them and get variables in the scope from parents. In any case, I'd
> agree my positions(), mod() and sij() functions would be better
> outside. Here is a corrected version (untested as something else is
> running),
>
> >
> > positions <- function(N) {
> >    reps <- 2*N+1
> >    matrix(c(rep(-N:N, each = reps), rep(-N:N, times = reps)),
> >           nrow = 2, byrow = TRUE)
> > }
>
> > mod<-function(x){sqrt(x[1]^2+x[2]^2)} # modulus
>
> > sij <-function(rj,ri,k){
> > rij=mod(rj-ri)
> > cos_ij=rj[1]/rij
> > sin_ij=rj[2]/rij
> >
> > A<-(1-1i*k*rij)*(3*cos_ij^2-1)*exp(1i*k*rij)/(rij^3)
> > B<-k^2*sin_ij^2*exp(1i*k*rij)/rij
> >
> > sij<-A+B
> > }
>
> > alpha_c <- function(lambda=600e-9,alpha_s=1e-14,N=400,spacing=1e-7){
> >
> > k<-2*pi/lambda
> > ri<-c(0,0) # particle at the origin
> >
> > rj<-positions(N)*spacing # all positions in the 2N x 2N array
> > rj<-rj[1:2,-((dim(rj)[2]-1)/2+1)] # remove rj=(0,0)
> >
> > s_ij<-apply(rj,2,sij)
>
> *** Now, how do I pass k and ri to this function ? ***
>
> > S<-sum(s_ij)
> > alpha_s/(1-alpha_s*S)
> > }
> > alpha_c()
> >
>
>
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
>
>
> Wondering whether that's part of the signature?
>
> the problem is related to scattering by arrays of particles, more
> specifically to evaluate the array influence on the effective
> polarizability (alpha) of a particle via dipolar radiative coupling.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Tue Jul 24 01:57:57 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 23 Jul 2007 19:57:57 -0400
Subject: [R] replacing double for loops with apply's
In-Reply-To: <b44da9db0707230912u298ee89ei2df55a3fbb742090@mail.gmail.com>
References: <b44da9db0707230912u298ee89ei2df55a3fbb742090@mail.gmail.com>
Message-ID: <971536df0707231657v58d0e54fte7f52439322c9926@mail.gmail.com>

Please make the posting reproduciblea dn self-contained.
I have added library(Hmisc) and set.seed(1) to satisfy that.

Note that wts = wts/sum(wts) is unnecessary and try this:

library(Hmisc)
set.seed(1)
x <- matrix(rnorm(200), 10, 20)
f <- function(i) {
      if (i < 5) { 0 * x[1, ]
      } else apply(x[1:i,], 2, wtd.var, weights = (0.5)^(i-seq(i)),
normwt = TRUE ,
            na.rm = TRUE)
}
t(sapply(1:nrow(x), f))
	

On 7/23/07, Young Cho <young.stat at gmail.com> wrote:
> Hi,
>
> I am doing double for loops to calculate SDs with some weights and wondering
> if I can get rid of the outer for loop as well. I made a simple examples
> which is essentially what I am doing.
>
> Thanks for your help!
>
> -Young
>
> #------------------------------------------------------
>  # wtd.var is Hmisc package
> # you can replace the 3 lines inside for loop as
> # sdx[i,] = apply(x[(i-4):i,],2,var,na.rm=T)
> # -------------------------------------------------------
>
> x = matrix(rnorm(200),10,20)
> sdx = matrix(0,10,20)
> for(i in 5:nrow(x)){
>                wts = ( 0.5 )^(i-c(1:i))
>                wts = wts/sum(wts)
>                sdx[i,] = apply(x[1:i,],2,wtd.var,weights=wts,normwt=T,
> na.rm=T)
> }
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andrew.clegg at gmail.com  Tue Jul 24 03:09:06 2007
From: andrew.clegg at gmail.com (Andrew Clegg)
Date: Tue, 24 Jul 2007 02:09:06 +0100
Subject: [R] Fitting exponential curve to data points
Message-ID: <e507996c0707231809q5a519cd7yac4bc7f1437e3be@mail.gmail.com>

Hi folks,

I've looked through the list archives and online resources, but I
haven't really found an answer to this -- it's pretty basic, but I'm
(very much) not a statistician, and I just want to check that my
solution is statistically sound.

Basically, I have a data file containing two columns of data, call it data.tsv:

year	count
1999	3
2000	5
2001	9
2002	30
2003	62
2004	154
2005	245
2006	321

These look exponential to me, so what I want to do is plot these
points on a graph with linear axes, and add an exponential curve over
the top. I also want to give an R-squared for the fit.

The way I did it was like so:


# Read in the data, make a copy of it, and take logs
data = read.table("data.tsv", header=TRUE)
log.data = data
log.data$count = log(log.data$count)

# Fit a model to the logs of the data
model = lm(log.data$count ~ year, data = log.data)

# Plot the original data points on a graph
plot(data)

# Draw in the exponents of the model's output
lines(data$year, exp(fitted(model)))


Is this the right way to do it? log-ing the data and then exp-ing the
results seems like a bit of a long-winded way to achieve the desired
effect. Is the R-squared given by summary(model) a valid measurement
of the fit of the points to an exponential curve, and should I use
multiple R-squared or adjusted R-squared?

The R-squared I get from this method (0.98 multiple) seems a little
high going by the deviation of the last data point from the curve --
you'll see what I mean if you try it.

Thanks in advance for any help!

Yours gratefully,

Andrew.


From ggrothendieck at gmail.com  Tue Jul 24 03:09:11 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 23 Jul 2007 21:09:11 -0400
Subject: [R] Aggregate daily data into weekly sums
In-Reply-To: <787911d50707231218l7b49f2cfy7d5d8c78a7080bdf@mail.gmail.com>
References: <787911d50707231218l7b49f2cfy7d5d8c78a7080bdf@mail.gmail.com>
Message-ID: <971536df0707231809l589214f4idfb8a0b7cd37227c@mail.gmail.com>

Try this.  I have changed output format to yyy/mm/Weekwwwww so its ordered.

Lines <- "Date Amount
 6/1/2007      1
 6/1/2007      1
 6/4/2007      2
 6/5/2007      2
6/11/2007      3
6/12/2007      3
6/12/2007      3
6/13/2007      3
6/13/2007      3
6/18/2007      4
6/18/2007      4
6/25/2007      5
6/28/2007      5
"

# replace next line with
# DF <- read.table("myfile.dat", header = TRUE)
DF <- read.table(textConnection(Lines), header = TRUE)
DF$Date <- as.Date(DF$Date, "%m/%d/%Y")

# weeks since first Sunday after Epoch
# assumes week starts on Sunday.  Change 3 to 4 for Monday.
fmt <- function(x) {
	weeks <- function(x) as.numeric(x + 3) %/% 7 + 1
	sprintf("%s%05d", format(x, "%Y/%m/Week"), weeks(x) - weeks(x[1]) + 1)
}

aggregate(DF$Amount, list(Date = fmt(DF$Date)), sum)

# alternative to above using zoo.  DF and fmt are from above.
# Returns a zoo object.
library(zoo)
aggregate(zoo(DF$Amount), fmt(DF$Date), sum)

On 7/23/07, Jacques Wagnor <jacques.wagnor at gmail.com> wrote:
> Dear Lest,
>
> I have a two-variable data frame as follows (the time peirod of the
> actual data set is 10 years):
>
>        Date Amount
> 1   6/1/2007      1
> 2   6/1/2007      1
> 3   6/4/2007      2
> 4   6/5/2007      2
> 5  6/11/2007      3
> 6  6/12/2007      3
> 7  6/12/2007      3
> 8  6/13/2007      3
> 9  6/13/2007      3
> 10 6/18/2007      4
> 11 6/18/2007      4
> 12 6/25/2007      5
> 13 6/28/2007      5
>
>
> Basically, I would like to collapse the daily data into weekly sums
> such that the result should look like the following:
>
>              Date Amount
> 1  2007/6/Week1       2
> 2  2007/6/Week2       4
> 3  2007/6/Week3       15
> 4  2007/6/Week4       8
> 5  2007/6/Week5      10
>
> Does there already exist a function that aggregates the data at
> user-defined time frequency?
>
> Any pointers would be greatly appreciated.
>
> Jacques
>
> > version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          5.0
> year           2007
> month          04
> day            23
> svn rev        41293
> language       R
> version.string R version 2.5.0 (2007-04-23)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Alexander.Herr at csiro.au  Tue Jul 24 03:42:04 2007
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Tue, 24 Jul 2007 11:42:04 +1000
Subject: [R] capture numeric(0) with if statement
Message-ID: <80C7911E901E7E4797B3F88D106CB25D1A9448@exqld2-bne.nexus.csiro.au>

Hi List,

I am trying to capture the numeric(0) with an if statement,
unsucessfully:

test<-numeric(0)

if(test == numeric(0)) print("test is", test)

I get: Error in if (test == numeric(0)) print("test is", test) : 
        argument is of length zero

How can I capture the numeric(0) condition?

Any enlightment appreciated
Cheers
Herry


From rmh at temple.edu  Tue Jul 24 03:51:33 2007
From: rmh at temple.edu (Richard M. Heiberger)
Date: Mon, 23 Jul 2007 21:51:33 -0400 (EDT)
Subject: [R] capture numeric(0) with if statement
Message-ID: <20070723215133.CHL24422@po-d.temple.edu>

> test <- numeric(0)
> length(test)
[1] 0
> if (length(test) == 0) print("length=0")
[1] "length=0"


From ggrothendieck at gmail.com  Tue Jul 24 04:34:23 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 23 Jul 2007 22:34:23 -0400
Subject: [R] capture numeric(0) with if statement
In-Reply-To: <80C7911E901E7E4797B3F88D106CB25D1A9448@exqld2-bne.nexus.csiro.au>
References: <80C7911E901E7E4797B3F88D106CB25D1A9448@exqld2-bne.nexus.csiro.au>
Message-ID: <971536df0707231934t7325cf5dm5ce8af8401547d96@mail.gmail.com>

Try:

if (identical(test, numeric(0))) ...

On 7/23/07, Alexander.Herr at csiro.au <Alexander.Herr at csiro.au> wrote:
> Hi List,
>
> I am trying to capture the numeric(0) with an if statement,
> unsucessfully:
>
> test<-numeric(0)
>
> if(test == numeric(0)) print("test is", test)
>
> I get: Error in if (test == numeric(0)) print("test is", test) :
>        argument is of length zero
>
> How can I capture the numeric(0) condition?
>
> Any enlightment appreciated
> Cheers
> Herry
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Tue Jul 24 07:39:05 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jul 2007 06:39:05 +0100 (BST)
Subject: [R] extension of rnormp package
In-Reply-To: <003b01c7cd65$06883810$16b2a8c0@ivii>
References: <003b01c7cd65$06883810$16b2a8c0@ivii>
Message-ID: <Pine.LNX.4.64.0707240628160.20066@gannet.stats.ox.ac.uk>

On Mon, 23 Jul 2007, Iwona Szyd?owska wrote:

> Hello,

> I would like to ask You, how to generate random numbers from an 
> exponential power family with a shape parameter p less than 1(p->0). I 
> found the rnormp package, which can generate numbers from this 
> distribution, but only for parameter less or equal 1.

It seems you mean package 'normalp', and that the package author believes 
that the exponential power distribution is only defined for p >= 1 
(although that is not on the help page). Other authors believe it is 
defined by a relationship to the gamma for all p > 0. So all you need to 
do is to change the condition from p < 1 to p <= 0 in rnormp and friends.

However, the algorithms used are not adequate for large or small p.  We 
know that the distribution tends to uniform for p -> Inf, but pnormp and 
rnormp break down for quite modest values of p.  As p -> 0 it tends to a 
point distribution at 0, but you will see very large values far too often.
So if you want p smaller than say 0.01 you will need to implement a 
different algorithm.

>
> Regards,
>            Iwona Szydlowska
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From yvonnick.noel at uhb.fr  Tue Jul 24 07:49:11 2007
From: yvonnick.noel at uhb.fr (Yvonnick NOEL)
Date: Tue, 24 Jul 2007 07:49:11 +0200
Subject: [R] Can I test if there are statistical significance between
 different rows in R*C table?
In-Reply-To: <2fc17e30707231722n61737c24h65c4607fe157d911@mail.gmail.com>
References: <mailman.9.1185184803.2168.r-help@stat.math.ethz.ch>	
	<46A4FEEF.3080801@uhb.fr>
	<2fc17e30707231722n61737c24h65c4607fe157d911@mail.gmail.com>
Message-ID: <46A592D7.3010208@uhb.fr>

zhijie zhang a ?crit :
> Dear Noel,
>   Your answers should be what i'm looking for.
>   Could u please show me the corresponding programs and the paper/book 
> for the theories?
I usually fit these models as conditional Poisson loglinear models, so 
the glm() function may be used. On the equivalence of binomial (or 
multinomial) and  loglinear models (conditionally to joint counts on the 
independent variables), see chapter 2 of:

Lindsey, J.K (1999). Applying Generalized Linear Models, Springer.

Also take a look at:

https://home.comcast.net/~lthompson221/Splusdiscrete2.pdf

I have a small script that does the job, with an example script that 
should be self-explanatory:

http://yvonnick.noel.free.fr/r2stats/multinom.R
http://yvonnick.noel.free.fr/r2stats/mnm-example.R

Yvonnick Noel
U. of Rennes 2
France


From stanhopkins at comcast.net  Tue Jul 24 05:34:56 2007
From: stanhopkins at comcast.net (Stan Hopkins)
Date: Mon, 23 Jul 2007 22:34:56 -0500
Subject: [R] Redirecting print output
Message-ID: <000f01c7cda3$9bcaa1e0$6405a8c0@MXD32803WB>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070723/477ac91f/attachment.pl 

From dmca at ucla.edu  Tue Jul 24 00:36:44 2007
From: dmca at ucla.edu (D L McArthur)
Date: Mon, 23 Jul 2007 22:36:44 +0000 (UTC)
Subject: [R] OT(slightly) - Tracking extended projects
References: <46A4C298.47B0.00EE.0@med.umich.edu>
Message-ID: <loom.20070724T002926-46@post.gmane.org>

James MacDonald <jmacdon <at> med.umich.edu> writes:

> 
> Hi all,
> 
> Most of the analyses I do are short little once-and-done type things that are 
easily encapsulated in a .Rnw
> file. However, I sometimes end up with projects that take an extended amount 
of time. Usually these
> projects are not easily encapsulated in an .Rnw file, so I have been using a 
single .R file with lots of comments.
> 
> The problem with this approach is keeping track of what you have done and 
what the results were. Once the .R
> file gets to be a certain size, the comments aren't as useful, and I find it 
easy to get lost. I have to assume
> that others have encountered this problem and hopefully have come up with 
something more elegant.
> 
> Any suggestions?
> 
> Best,
> 
> Jim
> 
One possible choice is the intuitively structured approach of "Projects" in 
Tinn-R (see http://www.sciviews.org/Tinn-R/).
 -- D L McArthur  <dmca <at> ucla.edu>


From Matthieu.Dubois at psp.ucl.ac.be  Tue Jul 24 09:24:39 2007
From: Matthieu.Dubois at psp.ucl.ac.be (Matthieu Dubois)
Date: Tue, 24 Jul 2007 09:24:39 +0200
Subject: [R] function optimization: reducing the computing time
Message-ID: <36E6ADF3-7BBF-4B1B-B141-EF35B904114D@psp.ucl.ac.be>

Dear useRs,

I have written a function that implements a Bayesian method to  
compare a patient's score on two tasks with that of a small control  
group, as described in Crawford, J. and Garthwaite, P. (2007).  
Comparison of a single case to a control or normative sample in  
neuropsychology: Development of a bayesian approach. Cognitive  
Neuropsychology, 24(4):343?372.

The function (see below) return the expected results, but the time  
needed to compute is quite long (at least for a test that may be  
routinely used). There is certainly room for  improvement. It would  
really be helpful if some experts of you may have  a  look ...

Thanks a lot.
Regards,

Matthieu


FUNCTION
----------
The function takes the performance on two tasks  and estimate the  
rarity (the p-value) of the difference between the patient's two  
scores, in comparison to the difference i the  controls subjects. A  
standardized and an unstandardized version are provided (controlled  
by the parameter standardized: T vs. F). Also, for congruency with  
the original publication, both the raw data  and  summary statistics  
could be used for the control group.

##################################################
# Bayesian (un)Standardized Difference Test
##################################################

#from Crawford and Garthwaite (2007) Cognitive Neuropsychology
# implemented by Matthieu Dubois, Matthieu.Dubois<at>psp.ucl.ac.be

#PACKAGE MCMCpack REQUIRED

# patient: a vector with the two scores; controls: matrix/data.frame  
with the raw scores (one column per  task)
# mean.c, sd.c, r, n: possibility to enter summaries statistics  
(mean, standard deviation, correlation, group size)
# n.simul: number of simulations
# two-sided (Boolean): two-sided (T) vs. one-sided (F) Bayesian  
Credible interval
# standardized (Boolean): standardized (T) vs. unstandardized (F) test
# values are: $p.value (one_tailed), $confidence.interval

crawford.BSDT <- function(patient, controls, mean.c=0, sd.c=0 , r=0,  
n=0, na.rm=F, n.simul=100000, two.sided=T, standardized=T)
{
	library(MCMCpack)
	
	#if no summaries are entered, they are computed
	if(missing(n))
	{	
		if(!is.data.frame(controls)) controls <- as.data.frame(controls)
		n <- dim(controls)[1]
		mean.c <- mean(controls, na.rm=na.rm)
		sd.c <- sd(controls, na.rm=na.rm)
		
		na.method <- ifelse(na.rm,"complete.obs","all.obs")
		
		r <- cor(controls[,1], controls[,2], na.method)
	}
	
	#variance/covariance matrix
	s.xx <- (sd.c[1]^2) * (n-1)
	s.yy <- (sd.c[2]^2) * (n-1)
	s.xy <- sd.c[1] * sd.c[2] * r * (n-1)
	
	A <- matrix(c(s.xx, s.xy, s.xy, s.yy), ncol=2)
	
	#estimation function
	if(standardized)
	{
		estimation <- function(patient, mean.c, n, A)
		{
			#estimation of a variance/covariance matrix (sigma)
			sigma = riwish(n,A)	#random obs. from an inverse-Wishart distribution
	
			#estimation of the means (mu)
			z <- rnorm(2)
			T <- t(chol(sigma)) #Cholesky decomposition
			mu <- mean.c + T %*% z/sqrt(n)
	
			#standardization
			z.x <- (patient[1]-mu[1]) / sqrt(sigma[1,1])
			z.y <- (patient[2]-mu[2]) / sqrt(sigma[2,2])
			rho.xy <- sigma[2.2] / sqrt(sigma[1,1]*sigma[2,2])
		
			z.star <- (z.x - z.y) / sqrt(2-2*rho.xy)
		
			#conditional p-value
			p <- pnorm(z.star)
			p
		}
	}
	else
	{
		estimation <- function(patient, mean.c, n, A)
		{
			#estimation of a variance/covariance matrix (sigma)
			sigma = riwish(n,A)	#random obs. from an inverse-Wishart distribution
	
			#estimation of the means (mu)
			z <- rnorm(2)
			T <- t(chol(sigma)) #Cholesky decomposition
			mu <- mean.c + T %*% z/sqrt(n)
	
			num <- (patient[1]-mu[1]) - (patient[2] - mu[2])
			denom <- sqrt(sigma[1,1]+sigma[2,2]-(2*sigma[1,2]))
			
			z.star <- num/denom
		
			#conditional p-value
			p <- pnorm(z.star)
			p
		}
	}
	
	#application
	p <- replicate(n.simul, estimation(patient, mean.c, n, A))
		
	#outputs
	pval <- mean(p)
	CI <- if(two.sided) 100*quantile(p,c(0.025,0.975)) else 100*quantile 
(p,c(0.95))
	output <- list(p.value=pval, confidence.interval=CI)
	output	
}



TIME ESTIMATION
--------------
# the values used in these examples are taken from the original paper
# system times are estimated for both the standardized and   
unstandardized versions.

system.time(crawford.BSDT(c(95,105),mean.c=c(100,100),sd.c=c 
(10,10),n=5,r=0.6, standardized=F))

    user  system elapsed
230.709  19.686 316.464

system.time(crawford.BSDT(c(90,110),mean.c=c(100,100),sd.c=c 
(10,10),n=5,r=0.6, standardized=T))
    user  system elapsed
227.618  15.656 293.810


R version
-------
 >sessionInfo()
R version 2.5.1 (2007-06-27)
powerpc-apple-darwin8.9.1

locale:
en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
[6] "methods"   "base"

other attached packages:
MCMCpack     MASS     coda  lattice
  "0.8-2" "7.2-34" "0.11-2" "0.16-2"




Matthieu Dubois
Ph.D. Student

Cognition and Development Lab
Catholic University of Louvain
10, Place Cardinal Mercier
B-1348 Louvain-la-Neuve - Belgium

E-mail: Matthieu.Dubois at psp.ucl.ac.be
Web:  http://www.code.ucl.ac.be/MatthieuDubois/


From joris.dewolf at cropdesign.com  Tue Jul 24 09:35:47 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Tue, 24 Jul 2007 09:35:47 +0200
Subject: [R] The 'REP' term in AMMI{agricolae}
In-Reply-To: <63053.212.112.39.227.1185213571.squirrel@webmail.slu.se>
Message-ID: <OF1BF936E2.13EADFE3-ONC1257322.0028E996-C1257322.0029A13C@basf-c-s.be>



The AMMI senso strictu part starts from the corrected data for genotype and
environment. In most cases where AMMI is applied (maybe also in the
agricolae package, I haven't checked), starts from the interaction effects
obtained through a general linear model anova.
It should be possible to replace the input by blups from your mixed model.
R news 7/1 (p14-19) gave some basic code for AMMI. There you should be able
to get ideas how to modify the code.


Joris








                                                                           
             "CG Pettersson"                                               
             <cg.pettersson at vp                                             
             e.slu.se>                                                  To 
             Sent by:                  r-help at stat.math.ethz.ch            
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] The 'REP' term in               
             23/07/2007 19:59          AMMI{agricolae}                     
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Dear all,

W2k, R 2.5.1

I am trying out the AMMI function in the agricolae package, to analyse the
dependence of environment for a certain cultivar. The function responds to
four basic variables:

ENV Environment
GEN Genotype
REP Replication
Y Response

My question concerns the 'REP' term. When I normally do an analysis of
variance, the replication would refer to repeated observations within the
each field trial. In the case I am analysing right now, I have five years
of observations for each 'ENV', in such a case it feels natural to use
year as the most important replication of the environment- especially as I
am interested in long term trends. This approach also seems to work
allright.

But the field trials are also structured in three main blocks, subdivided
into five 'lattice' blocks, a structure that is powerful in the analysis
of variance. (I use a random call in lme{nlme}).

Is it possible to use the block structure also in the AMMI analysis? If
that is possible, how should I code? I have tried to find out in the
documentation, but if it is stated there I do not understand it.

Thank you
/CG

--
CG Pettersson, PhD
Swedish University of Agricultural Sciences (SLU)
Dept. of Crop Production Ecology. Box 7043.
SE-750 07 Uppsala, Sweden
cg.pettersson at vpe.slu.se

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wl2776 at gmail.com  Tue Jul 24 10:11:50 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 24 Jul 2007 01:11:50 -0700 (PDT)
Subject: [R] Redirecting print output
In-Reply-To: <000f01c7cda3$9bcaa1e0$6405a8c0@MXD32803WB>
References: <000f01c7cda3$9bcaa1e0$6405a8c0@MXD32803WB>
Message-ID: <11758652.post@talk.nabble.com>




Stan Hopkins wrote:
> 
> I see a rich set of graphic device functions to redirect that output.  Are
> there commands to redirect text as well.  I have a set of functions that
> execute many linear regression tests serially and I want to capture this
> in a file for printing.
> 
> Thanks,
> 
> Stan Hopkins
> 

Yes, there are.
?sink

You could also run your functions from a batch mode:
R < your_script.R > output.txt
or
Rscript your_script.R > output.txt

This, however, will give you a single file, while sink() allows creation of
multiple files.

capture.output can store the output in an array of character strings.

-- 
View this message in context: http://www.nabble.com/Redirecting-print-output-tf4134131.html#a11758652
Sent from the R help mailing list archive at Nabble.com.


From tuechler at gmx.at  Tue Jul 24 11:17:54 2007
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 24 Jul 2007 10:17:54 +0100
Subject: [R] OT(slightly) - Tracking extended projects
In-Reply-To: <loom.20070724T002926-46@post.gmane.org>
References: <46A4C298.47B0.00EE.0@med.umich.edu>
Message-ID: <3.0.6.32.20070724101754.00b44ab0@pop.gmx.net>

At 22:36 23.07.2007 +0000, D L McArthur wrote:
>James MacDonald <jmacdon <at> med.umich.edu> writes:
>
>> 
>> Hi all,
>> 
>> Most of the analyses I do are short little once-and-done type things
that are 
>easily encapsulated in a .Rnw
>> file. However, I sometimes end up with projects that take an extended
amount 
>of time. Usually these
>> projects are not easily encapsulated in an .Rnw file, so I have been
using a 
>single .R file with lots of comments.
>> 
>> The problem with this approach is keeping track of what you have done and 
>what the results were. Once the .R
>> file gets to be a certain size, the comments aren't as useful, and I
find it 
>easy to get lost. I have to assume
>> that others have encountered this problem and hopefully have come up with 
>something more elegant.
>> 
>> Any suggestions?
>> 
>> Best,
>> 
>> Jim
>> 
>One possible choice is the intuitively structured approach of "Projects" in 
>Tinn-R (see http://www.sciviews.org/Tinn-R/).
> -- D L McArthur  <dmca <at> ucla.edu>
>

Or, in case you use emacs and ESS you could find some hints at the ESS help
list (ess-help at stat.math.ethz.ch). Look for "[ESS] hide function bodies"
and "[ESS] outline-minor-mode".

Heinz

>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Tue Jul 24 10:18:51 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 24 Jul 2007 01:18:51 -0700 (PDT)
Subject: [R] Redirecting print output
In-Reply-To: <000f01c7cda3$9bcaa1e0$6405a8c0@MXD32803WB>
Message-ID: <159015.11743.qm@web39711.mail.mud.yahoo.com>

Here are two simple ways:

=== method1 ===
cat("line1","\n",file="output.txt")
cat("line2","\n",file="output.txt",append=TRUE)

=== method2 ===
sink("output.txt")
cat("line1","\n")
cat("line2","\n")
out <- lm(y~x,data=data.frame(x=1:10,y=(1:10+rnorm(10,0,0.1))))
print(out)
sink()

And then there is 'Sweave'. Check out, for instance
<http://www.stat.umn.edu/~charlie/Sweave/>

You can embed R code, figures, and output from print methods into your latex
document.

ST
--- Stan Hopkins <stanhopkins at comcast.net> wrote:

> I see a rich set of graphic device functions to redirect that output.  Are
> there commands to redirect text as well.  I have a set of functions that
> execute many linear regression tests serially and I want to capture this in
> a file for printing.
> 
> Thanks,
> 
> Stan Hopkins
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



       
____________________________________________________________________________________Ready for the edge of your seat? 
Check out tonight's top picks on Yahoo! TV.


From ailsa.kerswell at jcu.edu.au  Tue Jul 24 10:43:57 2007
From: ailsa.kerswell at jcu.edu.au (Ailsa Kerswell)
Date: Tue, 24 Jul 2007 18:43:57 +1000 (EST)
Subject: [R] geoR/likfit: variance explained by model
Message-ID: <20070724184357.BLH06935@mirapoint-ms1.jcu.edu.au>

I have been modelling spatial data using function likfit in package geoR.  Now that I have the spatial regression models, I would like to calculate the amount of variance explained by both the trend and the spatial parts of the model.  Any advice on how to do this would be much appreciated.

Cheers,
Ailsa


From brown_emu at yahoo.com  Tue Jul 24 10:55:55 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 24 Jul 2007 01:55:55 -0700 (PDT)
Subject: [R] persp and greek symbols in the axes labels
In-Reply-To: <46A4C1FD.3080706@toulouse.inra.fr>
Message-ID: <809791.74780.qm@web39714.mail.mud.yahoo.com>

I don't know why it doesn't work but I think people generally recommend that
you use wireframe() in lattice rather than persp(), because wireframe is more
customizable (the pdf document referred to in this post is pretty good):
http://tolstoy.newcastle.edu.au/R/e2/help/07/03/12534.html

Here's an example:

library(lattice)
library(reshape)
x <- 1:5
y <- 1:3
z <- matrix(1:15,ncol=3,dimnames=list(NULL,y))
M <- melt(data.frame(x,z,check.names=FALSE),id=1,variable="y")
wireframe(value~x*y,data=M,
          screen=list(z=45,x=-75),
          xlab=expression(kappa[lambda]),
          ylab=as.expression(substitute(paste(phi,"=",true,sigma),
              list(true=5))),
          zlab = "Z")

[you can play around with the 'screen' argument to rotate the view, analogous
to phi and theta in persp()]


--- Nathalie Peyrard <Nathalie.Peyrard at toulouse.inra.fr> wrote:

> Hello,
> 
> I am plotting a 3D function using persp and I would like to use greek 
> symbols in the axes labels.
> I  have found examples like  this one on the web:
> 
>
plot(0,0,xlab=expression(kappa[lambda]),ylab=substitute(paste(phi,"=",true,sigma),list(true=5)))
> 
> this works well with plot but not with persp:
> with the command
> 
> persp(M,theta = -20,phi = 
>
0,xlab=expression(kappa[lambda]),ylab=substitute(paste(phi,"=",true,sigma),list(true=5)),zlab
> 
> = "Z")
> 
> I get the labels as in toto.eps
> 
> Any suggestion? Thanks!
> 
> Nathalie
> 
> -- 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   
> INRA  Toulouse - Unit? de Biom?trie et  Intelligence Artificielle 
> Chemin de Borde-Rouge BP 52627 31326 CASTANET-TOLOSAN cedex FRANCE 
> Tel : +33(0)5.61.28.54.39 - Fax : +33(0)5.61.28.53.35
> Web :http://mia.toulouse.inra.fr/index.php?id=217
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> 
> > ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



       
____________________________________________________________________________________Ready for the edge of your seat? 
Check out tonight's top picks on Yahoo! TV.


From ripley at stats.ox.ac.uk  Tue Jul 24 11:02:30 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jul 2007 10:02:30 +0100 (BST)
Subject: [R] function optimization: reducing the computing time
In-Reply-To: <36E6ADF3-7BBF-4B1B-B141-EF35B904114D@psp.ucl.ac.be>
References: <36E6ADF3-7BBF-4B1B-B141-EF35B904114D@psp.ucl.ac.be>
Message-ID: <Pine.LNX.4.64.0707240930510.26158@gannet.stats.ox.ac.uk>

You need to make use of the profiling methods described in 'Writing R 
Exensions'. My machine is about 4x faster than yours: I get

Each sample represents 0.02 seconds.
Total run time: 62.0800000000041 seconds.

Total seconds: time spent in function and callees.
Self seconds: time spent in function alone.

    %       total       %       self
  total    seconds     self    seconds    name
100.00     62.08      0.00      0.00     "system.time"
  99.94     62.04      0.00      0.00     "crawford.BSDT"
  99.94     62.04      0.00      0.00     "eval"
  99.10     61.52      1.00      0.62     "lapply"
  99.10     61.52      0.00      0.00     "sapply"
  99.00     61.46      0.00      0.00     "replicate"
  98.61     61.22      2.26      1.40     "FUN"
  98.26     61.00      3.32      2.06     "estimation"
  83.92     52.10      0.26      0.16     "riwish"
  83.67     51.94      4.25      2.64     "solve"
  55.57     34.50      7.18      4.46     "solve.default"
  51.68     32.08      3.77      2.34     "rwish"
...

so 84% of the time is being spent in riwish.  Now given that A is fixed, 
you should be able to speed that up by precomputing the constant parts of 
the computation (and you can also precompute your 'T').


On Tue, 24 Jul 2007, Matthieu Dubois wrote:

> Dear useRs,
>
> I have written a function that implements a Bayesian method to
> compare a patient's score on two tasks with that of a small control
> group, as described in Crawford, J. and Garthwaite, P. (2007).
> Comparison of a single case to a control or normative sample in
> neuropsychology: Development of a bayesian approach. Cognitive
> Neuropsychology, 24(4):343??372.
>
> The function (see below) return the expected results, but the time
> needed to compute is quite long (at least for a test that may be
> routinely used). There is certainly room for  improvement. It would
> really be helpful if some experts of you may have  a  look ...
>
> Thanks a lot.
> Regards,
>
> Matthieu
>
>
> FUNCTION
> ----------
> The function takes the performance on two tasks  and estimate the
> rarity (the p-value) of the difference between the patient's two
> scores, in comparison to the difference i the  controls subjects. A
> standardized and an unstandardized version are provided (controlled
> by the parameter standardized: T vs. F). Also, for congruency with
> the original publication, both the raw data  and  summary statistics
> could be used for the control group.
>
> ##################################################
> # Bayesian (un)Standardized Difference Test
> ##################################################
>
> #from Crawford and Garthwaite (2007) Cognitive Neuropsychology
> # implemented by Matthieu Dubois, Matthieu.Dubois<at>psp.ucl.ac.be
>
> #PACKAGE MCMCpack REQUIRED
>
> # patient: a vector with the two scores; controls: matrix/data.frame
> with the raw scores (one column per  task)
> # mean.c, sd.c, r, n: possibility to enter summaries statistics
> (mean, standard deviation, correlation, group size)
> # n.simul: number of simulations
> # two-sided (Boolean): two-sided (T) vs. one-sided (F) Bayesian
> Credible interval
> # standardized (Boolean): standardized (T) vs. unstandardized (F) test
> # values are: $p.value (one_tailed), $confidence.interval
>
> crawford.BSDT <- function(patient, controls, mean.c=0, sd.c=0 , r=0,
> n=0, na.rm=F, n.simul=100000, two.sided=T, standardized=T)
> {
> 	library(MCMCpack)
>
> 	#if no summaries are entered, they are computed
> 	if(missing(n))
> 	{
> 		if(!is.data.frame(controls)) controls <- as.data.frame(controls)
> 		n <- dim(controls)[1]
> 		mean.c <- mean(controls, na.rm=na.rm)
> 		sd.c <- sd(controls, na.rm=na.rm)
>
> 		na.method <- ifelse(na.rm,"complete.obs","all.obs")
>
> 		r <- cor(controls[,1], controls[,2], na.method)
> 	}
>
> 	#variance/covariance matrix
> 	s.xx <- (sd.c[1]^2) * (n-1)
> 	s.yy <- (sd.c[2]^2) * (n-1)
> 	s.xy <- sd.c[1] * sd.c[2] * r * (n-1)
>
> 	A <- matrix(c(s.xx, s.xy, s.xy, s.yy), ncol=2)
>
> 	#estimation function
> 	if(standardized)
> 	{
> 		estimation <- function(patient, mean.c, n, A)
> 		{
> 			#estimation of a variance/covariance matrix (sigma)
> 			sigma = riwish(n,A)	#random obs. from an inverse-Wishart distribution
>
> 			#estimation of the means (mu)
> 			z <- rnorm(2)
> 			T <- t(chol(sigma)) #Cholesky decomposition
> 			mu <- mean.c + T %*% z/sqrt(n)
>
> 			#standardization
> 			z.x <- (patient[1]-mu[1]) / sqrt(sigma[1,1])
> 			z.y <- (patient[2]-mu[2]) / sqrt(sigma[2,2])
> 			rho.xy <- sigma[2.2] / sqrt(sigma[1,1]*sigma[2,2])
>
> 			z.star <- (z.x - z.y) / sqrt(2-2*rho.xy)
>
> 			#conditional p-value
> 			p <- pnorm(z.star)
> 			p
> 		}
> 	}
> 	else
> 	{
> 		estimation <- function(patient, mean.c, n, A)
> 		{
> 			#estimation of a variance/covariance matrix (sigma)
> 			sigma = riwish(n,A)	#random obs. from an inverse-Wishart distribution
>
> 			#estimation of the means (mu)
> 			z <- rnorm(2)
> 			T <- t(chol(sigma)) #Cholesky decomposition
> 			mu <- mean.c + T %*% z/sqrt(n)
>
> 			num <- (patient[1]-mu[1]) - (patient[2] - mu[2])
> 			denom <- sqrt(sigma[1,1]+sigma[2,2]-(2*sigma[1,2]))
>
> 			z.star <- num/denom
>
> 			#conditional p-value
> 			p <- pnorm(z.star)
> 			p
> 		}
> 	}
>
> 	#application
> 	p <- replicate(n.simul, estimation(patient, mean.c, n, A))
>
> 	#outputs
> 	pval <- mean(p)
> 	CI <- if(two.sided) 100*quantile(p,c(0.025,0.975)) else 100*quantile
> (p,c(0.95))
> 	output <- list(p.value=pval, confidence.interval=CI)
> 	output
> }
>
>
>
> TIME ESTIMATION
> --------------
> # the values used in these examples are taken from the original paper
> # system times are estimated for both the standardized and
> unstandardized versions.
>
> system.time(crawford.BSDT(c(95,105),mean.c=c(100,100),sd.c=c
> (10,10),n=5,r=0.6, standardized=F))
>
>    user  system elapsed
> 230.709  19.686 316.464
>
> system.time(crawford.BSDT(c(90,110),mean.c=c(100,100),sd.c=c
> (10,10),n=5,r=0.6, standardized=T))
>    user  system elapsed
> 227.618  15.656 293.810
>
>
> R version
> -------
> >sessionInfo()
> R version 2.5.1 (2007-06-27)
> powerpc-apple-darwin8.9.1
>
> locale:
> en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"
> [6] "methods"   "base"
>
> other attached packages:
> MCMCpack     MASS     coda  lattice
>  "0.8-2" "7.2-34" "0.11-2" "0.16-2"
>
>
>
>
> Matthieu Dubois
> Ph.D. Student
>
> Cognition and Development Lab
> Catholic University of Louvain
> 10, Place Cardinal Mercier
> B-1348 Louvain-la-Neuve - Belgium
>
> E-mail: Matthieu.Dubois at psp.ucl.ac.be
> Web:  http://www.code.ucl.ac.be/MatthieuDubois/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ted.harding at nessie.mcc.ac.uk  Tue Jul 24 11:08:35 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 24 Jul 2007 10:08:35 +0100 (BST)
Subject: [R] Fitting exponential curve to data points
In-Reply-To: <e507996c0707231809q5a519cd7yac4bc7f1437e3be@mail.gmail.com>
Message-ID: <XFMail.070724100835.ted.harding@nessie.mcc.ac.uk>

On 24-Jul-07 01:09:06, Andrew Clegg wrote:
> Hi folks,
> 
> I've looked through the list archives and online resources, but I
> haven't really found an answer to this -- it's pretty basic, but I'm
> (very much) not a statistician, and I just want to check that my
> solution is statistically sound.
> 
> Basically, I have a data file containing two columns of data, call it
> data.tsv:
> 
> year  count
> 1999  3
> 2000  5
> 2001  9
> 2002  30
> 2003  62
> 2004  154
> 2005  245
> 2006  321
> 
> These look exponential to me, so what I want to do is plot these
> points on a graph with linear axes, and add an exponential curve over
> the top. I also want to give an R-squared for the fit.
> 
> The way I did it was like so:
> 
> 
># Read in the data, make a copy of it, and take logs
> data = read.table("data.tsv", header=TRUE)
> log.data = data
> log.data$count = log(log.data$count)
> 
># Fit a model to the logs of the data
> model = lm(log.data$count ~ year, data = log.data)
> 
># Plot the original data points on a graph
> plot(data)
> 
># Draw in the exponents of the model's output
> lines(data$year, exp(fitted(model)))
> 
> 
> Is this the right way to do it? log-ing the data and then exp-ing the
> results seems like a bit of a long-winded way to achieve the desired
> effect. Is the R-squared given by summary(model) a valid measurement
> of the fit of the points to an exponential curve, and should I use
> multiple R-squared or adjusted R-squared?
> 
> The R-squared I get from this method (0.98 multiple) seems a little
> high going by the deviation of the last data point from the curve --
> you'll see what I mean if you try it.

I just did. From the plot of log(count) against year, with the plot
of the linear fit of log(count)~year superimposed, I see indications
of a non-linear relationship.

The departures of the data from the fit follow a rather systematic
pattern. Initially the data increase more slowly than the fit,
and lie below it. Then they increase faster and corss over above it.
Then the data increase less fast than the fit, and the final data
point is below the fit.

There are not enough data to properly identify the non-linearity,
but the overall appearance of the data plot suggests to me that
you should be considering one of the "growth curve" models.

Many such models start of with an increasing rate of growth,
which then slows down, and typically levels off to an asymptote.
The apparent large discrepancy of your final data point could
be compatible with this kind of behaviour.

At this point, knowledge of what kind of thing is represented
by your "count" variable might be helpful. If, for instance,
it is the count of the numbers of individuals of a species in
an area, then independent knowledge of growth mechanisms may
help to narrow down the kind of model you should be tring to fit.

As to your question about "Is this the right way to do it"
(i.e. fitting an exponential curve by doing a linear fit of the
logarithm), generally speaking the answer is "Yes". But of course
you need to be confident that "exponential" is the right curve
to be fitting in the first place. If it's the wrong type of
curve to be considering, then it's not "the right way to do it"!

Hoping this help[s,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 24-Jul-07                                       Time: 10:08:33
------------------------------ XFMail ------------------------------


From gatedefender at abreuadvogados.com  Tue Jul 24 12:10:08 2007
From: gatedefender at abreuadvogados.com (gatedefender at abreuadvogados.com)
Date: Tue, 24 Jul 2007 11:10:08 +0100
Subject: [R] =?utf-8?q?Warning_generated_by_Panda_GateDefender_Integra=2E?=
Message-ID: <COMMSRVVn20lbhsKd6Q00020b58@commsrv.aca.abreucardigos.com>

07/24/2007 11:10:07 [GMT+0100]
 For security reasons certain items found in an email with your address as the sender have not been accepted.

File name: attachment.zip

Filtered by: Malformed messages

Sender: r-help at stat.math.ethz.ch
Recipients: correio at ourem.tc.mj.pt
CC:


From brown_emu at yahoo.com  Tue Jul 24 11:32:27 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 24 Jul 2007 02:32:27 -0700 (PDT)
Subject: [R] Fitting exponential curve to data points
In-Reply-To: <e507996c0707231809q5a519cd7yac4bc7f1437e3be@mail.gmail.com>
Message-ID: <590397.92009.qm@web39710.mail.mud.yahoo.com>

I think your way is probably the easiest (shockingly). For instance, here are
some alternatives - I think in both cases you have to calculate the
coefficient of determination (R^2) manually. My understanding is that
multiple R^2 in your case is the usual R^2 because you only have one
predictor variable, and the adjusted R^2 considers the degrees of freedom and
penalizes for additional predictors. Which is better... depends? (Perhaps
more stats-savvy people can help you on that one. I'm a chemical engineer so
I unjustifiably claim ignorance).

## Data input
input <-
"Year	Count
1999	3
2000	5
2001	9
2002	30
2003	62
2004	154
2005	245
2006	321"

dat <- read.table(textConnection(input),header=TRUE)
dat[,] <- lapply(dat,function(x) x-x[1])
          # shifting in origin; will need to add back in later

## Nonlinear least squares
plot(dat)
out <- nls(Count~b0*exp(b1*Year),data=dat,
           start=list(b0=1,b1=1))
lines(dat[,1],fitted(out),col=2)
out <- nls(Count~b0+b1*Year+b2*Year^2,data=dat, #polynomial
           start=list(b0=0,b1=1,b2=1))
lines(dat[,1],fitted(out),col=3)

## Optim
f <- function(.pars,.dat,.fun) sum((.dat[,2]-.fun(.pars,.dat[,1]))^2)
fitFun <- function(b,x) cbind(1,x,x^2)%*%b
expFun <- function(b,x) b[1]*exp(b[2]*x)

plot(dat)
out <- optim(c(0,1,1),f,.dat=dat,.fun=fitFun)
lines(dat[,1],fitFun(out$par,dat[,1]),col=2)
out <- optim(c(1,1),f,.dat=dat,.fun=expFun)
lines(dat[,1],expFun(out$par,dat[,1]),col=3)


--- Andrew Clegg <andrew.clegg at gmail.com> wrote:

> Hi folks,
> 
> I've looked through the list archives and online resources, but I
> haven't really found an answer to this -- it's pretty basic, but I'm
> (very much) not a statistician, and I just want to check that my
> solution is statistically sound.
> 
> Basically, I have a data file containing two columns of data, call it
> data.tsv:
> 
> year	count
> 1999	3
> 2000	5
> 2001	9
> 2002	30
> 2003	62
> 2004	154
> 2005	245
> 2006	321
> 
> These look exponential to me, so what I want to do is plot these
> points on a graph with linear axes, and add an exponential curve over
> the top. I also want to give an R-squared for the fit.
> 
> The way I did it was like so:
> 
> 
> # Read in the data, make a copy of it, and take logs
> data = read.table("data.tsv", header=TRUE)
> log.data = data
> log.data$count = log(log.data$count)
> 
> # Fit a model to the logs of the data
> model = lm(log.data$count ~ year, data = log.data)
> 
> # Plot the original data points on a graph
> plot(data)
> 
> # Draw in the exponents of the model's output
> lines(data$year, exp(fitted(model)))
> 
> 
> Is this the right way to do it? log-ing the data and then exp-ing the
> results seems like a bit of a long-winded way to achieve the desired
> effect. Is the R-squared given by summary(model) a valid measurement
> of the fit of the points to an exponential curve, and should I use
> multiple R-squared or adjusted R-squared?
> 
> The R-squared I get from this method (0.98 multiple) seems a little
> high going by the deviation of the last data point from the curve --
> you'll see what I mean if you try it.
> 
> Thanks in advance for any help!
> 
> Yours gratefully,
> 
> Andrew.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From brown_emu at yahoo.com  Tue Jul 24 11:37:24 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 24 Jul 2007 02:37:24 -0700 (PDT)
Subject: [R] Fitting exponential curve to data points
In-Reply-To: <XFMail.070724100835.ted.harding@nessie.mcc.ac.uk>
Message-ID: <723775.18076.qm@web39703.mail.mud.yahoo.com>

Well spoken. And since log transformations are nonlinear and 'compresses' the
data, it's not surprising to find that the fit doesn't look so nice while the
fit metrics tell you that a model does a good job.

--- ted.harding at nessie.mcc.ac.uk wrote:

> On 24-Jul-07 01:09:06, Andrew Clegg wrote:
> > Hi folks,
> > 
> > I've looked through the list archives and online resources, but I
> > haven't really found an answer to this -- it's pretty basic, but I'm
> > (very much) not a statistician, and I just want to check that my
> > solution is statistically sound.
> > 
> > Basically, I have a data file containing two columns of data, call it
> > data.tsv:
> > 
> > year  count
> > 1999  3
> > 2000  5
> > 2001  9
> > 2002  30
> > 2003  62
> > 2004  154
> > 2005  245
> > 2006  321
> > 
> > These look exponential to me, so what I want to do is plot these
> > points on a graph with linear axes, and add an exponential curve over
> > the top. I also want to give an R-squared for the fit.
> > 
> > The way I did it was like so:
> > 
> > 
> ># Read in the data, make a copy of it, and take logs
> > data = read.table("data.tsv", header=TRUE)
> > log.data = data
> > log.data$count = log(log.data$count)
> > 
> ># Fit a model to the logs of the data
> > model = lm(log.data$count ~ year, data = log.data)
> > 
> ># Plot the original data points on a graph
> > plot(data)
> > 
> ># Draw in the exponents of the model's output
> > lines(data$year, exp(fitted(model)))
> > 
> > 
> > Is this the right way to do it? log-ing the data and then exp-ing the
> > results seems like a bit of a long-winded way to achieve the desired
> > effect. Is the R-squared given by summary(model) a valid measurement
> > of the fit of the points to an exponential curve, and should I use
> > multiple R-squared or adjusted R-squared?
> > 
> > The R-squared I get from this method (0.98 multiple) seems a little
> > high going by the deviation of the last data point from the curve --
> > you'll see what I mean if you try it.
> 
> I just did. From the plot of log(count) against year, with the plot
> of the linear fit of log(count)~year superimposed, I see indications
> of a non-linear relationship.
> 
> The departures of the data from the fit follow a rather systematic
> pattern. Initially the data increase more slowly than the fit,
> and lie below it. Then they increase faster and corss over above it.
> Then the data increase less fast than the fit, and the final data
> point is below the fit.
> 
> There are not enough data to properly identify the non-linearity,
> but the overall appearance of the data plot suggests to me that
> you should be considering one of the "growth curve" models.
> 
> Many such models start of with an increasing rate of growth,
> which then slows down, and typically levels off to an asymptote.
> The apparent large discrepancy of your final data point could
> be compatible with this kind of behaviour.
> 
> At this point, knowledge of what kind of thing is represented
> by your "count" variable might be helpful. If, for instance,
> it is the count of the numbers of individuals of a species in
> an area, then independent knowledge of growth mechanisms may
> help to narrow down the kind of model you should be tring to fit.
> 
> As to your question about "Is this the right way to do it"
> (i.e. fitting an exponential curve by doing a linear fit of the
> logarithm), generally speaking the answer is "Yes". But of course
> you need to be confident that "exponential" is the right curve
> to be fitting in the first place. If it's the wrong type of
> curve to be considering, then it's not "the right way to do it"!
> 
> Hoping this help[s,
> Ted.
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 24-Jul-07                                       Time: 10:08:33
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From amicogodzilla at bruttocarattere.org  Tue Jul 24 12:02:58 2007
From: amicogodzilla at bruttocarattere.org (Manuele Pesenti)
Date: Tue, 24 Jul 2007 12:02:58 +0200
Subject: [R] values from a linear model
Message-ID: <200707241202.58668.amicogodzilla@bruttocarattere.org>

Dear R users,
how can I extrapolate values listed in the summary of an lm model but not 
directly available between object values such as the the standard errors of 
the calculated parameters?

for example I got a model:

mod <- lm(Crd ~ 1 + Week, data=data)

and its summary:

> summary(mod)

Call:
lm(formula = Crd ~ 1 + Week, data = data, model = TRUE, y = TRUE)

Residuals:
       Min         1Q     Median         3Q        Max 
-4.299e-03 -1.653e-03  2.628e-05  1.291e-03  5.130e-03 

Coefficients:
             Estimate Std. Error  t value Pr(>|t|)    
(Intercept) 1.000e+01  3.962e-04 25238.73   <2e-16 ***
Week        5.038e-04  6.812e-06    73.96   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.001966 on 98 degrees of freedom
Multiple R-Squared: 0.9824,     Adjusted R-squared: 0.9822 
F-statistic:  5469 on 1 and 98 DF,  p-value: < 2.2e-16

I'm interested in values of Std. Error of coefficients...

thank you very much

Best regards
	Manuele

-- 
Manuele Pesenti
	manuele a inventati.org
	amicogodzilla a jabber.linux.it
	http://mpesenti.polito.it


From nshephard at gmail.com  Tue Jul 24 12:12:51 2007
From: nshephard at gmail.com (Neil Shephard)
Date: Tue, 24 Jul 2007 11:12:51 +0100
Subject: [R] problems with character objects and calls to list() [solved]
Message-ID: <31b34fca0707240312x5e736d1bja3b0a4e79d39dad3@mail.gmail.com>

Thanks to Patrick Burns and Mark Lyman for their suggestions in
solving my problem.

Patrick suggested creating the list directly (the solution I opted for)....

On 7/23/07, Patrick Burns <pburns at pburns.seanet.com> wrote:
> Why not make the list directly:
>
> list.to.convert <- vector('list', n)
> for(x in 1:n) list.to.convert[[x]] <- seq((2*x)-1, 2*x)
>
> S Poetry may be of use to you.

Whilst Mark suggested the correct evaluation of the character object....

> > > list(1:2, 3:4, 5:6)
> > [[1]]
> > [1] 1 2
> >
> > [[2]]
> > [1] 3 4
> >
> > [[3]]
> > [1] 5 6
> >
>
> > eval(parse(text=paste("list(",to.convert,")",sep="")))
> [[1]]
> [1] 1 2
>
> [[2]]
> [1] 3 4
>
> [[3]]
> [1] 5 6
>
> [[4]]
> [1] 7 8
>
> [[5]]
> [1]  9 10
>
> [[6]]
> [1] 11 12


-- 
"In mathematics you don't understand things. You just get used to
them."  - Johann von Neumann

Email - nshephard at gmail.com / n.shephard at sheffield.ac.uk
Website - http://slack.ser.man.ac.uk/
Photos - http://www.flickr.com/photos/slackline/


From Thierry.ONKELINX at inbo.be  Tue Jul 24 12:31:40 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 24 Jul 2007 12:31:40 +0200
Subject: [R] values from a linear model
In-Reply-To: <200707241202.58668.amicogodzilla@bruttocarattere.org>
References: <200707241202.58668.amicogodzilla@bruttocarattere.org>
Message-ID: <2E9C414912813E4EB981326983E0A10403585C40@inboexch.inbo.be>

It's not very clear to me but I think summary(mod)$coef[, "Std. Error"]
is wat you need?

Cheers,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Manuele Pesenti
> Verzonden: dinsdag 24 juli 2007 12:03
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] values from a linear model
> 
> Dear R users,
> how can I extrapolate values listed in the summary of an lm 
> model but not directly available between object values such 
> as the the standard errors of the calculated parameters?
> 
> for example I got a model:
> 
> mod <- lm(Crd ~ 1 + Week, data=data)
> 
> and its summary:
> 
> > summary(mod)
> 
> Call:
> lm(formula = Crd ~ 1 + Week, data = data, model = TRUE, y = TRUE)
> 
> Residuals:
>        Min         1Q     Median         3Q        Max 
> -4.299e-03 -1.653e-03  2.628e-05  1.291e-03  5.130e-03 
> 
> Coefficients:
>              Estimate Std. Error  t value Pr(>|t|)    
> (Intercept) 1.000e+01  3.962e-04 25238.73   <2e-16 ***
> Week        5.038e-04  6.812e-06    73.96   <2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Residual standard error: 0.001966 on 98 degrees of freedom
> Multiple R-Squared: 0.9824,     Adjusted R-squared: 0.9822 
> F-statistic:  5469 on 1 and 98 DF,  p-value: < 2.2e-16
> 
> I'm interested in values of Std. Error of coefficients...
> 
> thank you very much
> 
> Best regards
> 	Manuele
> 
> --
> Manuele Pesenti
> 	manuele op inventati.org
> 	amicogodzilla op jabber.linux.it
> 	http://mpesenti.polito.it
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dimitris.rizopoulos at med.kuleuven.be  Tue Jul 24 12:37:11 2007
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 24 Jul 2007 12:37:11 +0200
Subject: [R] values from a linear model
References: <200707241202.58668.amicogodzilla@bruttocarattere.org>
Message-ID: <013801c7cdde$97a427e0$0540210a@www.domain>

try this:

coef(summary(mod))[, "Std. Error"]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/(0)16/336899
Fax: +32/(0)16/337015
Web: http://med.kuleuven.be/biostat/
     http://www.student.kuleuven.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Manuele Pesenti" <amicogodzilla at bruttocarattere.org>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, July 24, 2007 12:02 PM
Subject: [R] values from a linear model


> Dear R users,
> how can I extrapolate values listed in the summary of an lm model 
> but not
> directly available between object values such as the the standard 
> errors of
> the calculated parameters?
>
> for example I got a model:
>
> mod <- lm(Crd ~ 1 + Week, data=data)
>
> and its summary:
>
>> summary(mod)
>
> Call:
> lm(formula = Crd ~ 1 + Week, data = data, model = TRUE, y = TRUE)
>
> Residuals:
>       Min         1Q     Median         3Q        Max
> -4.299e-03 -1.653e-03  2.628e-05  1.291e-03  5.130e-03
>
> Coefficients:
>             Estimate Std. Error  t value Pr(>|t|)
> (Intercept) 1.000e+01  3.962e-04 25238.73   <2e-16 ***
> Week        5.038e-04  6.812e-06    73.96   <2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> Residual standard error: 0.001966 on 98 degrees of freedom
> Multiple R-Squared: 0.9824,     Adjusted R-squared: 0.9822
> F-statistic:  5469 on 1 and 98 DF,  p-value: < 2.2e-16
>
> I'm interested in values of Std. Error of coefficients...
>
> thank you very much
>
> Best regards
> Manuele
>
> -- 
> Manuele Pesenti
> manuele at inventati.org
> amicogodzilla at jabber.linux.it
> http://mpesenti.polito.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From wl2776 at gmail.com  Tue Jul 24 12:40:58 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 24 Jul 2007 03:40:58 -0700 (PDT)
Subject: [R] values from a linear model
In-Reply-To: <200707241202.58668.amicogodzilla@bruttocarattere.org>
References: <200707241202.58668.amicogodzilla@bruttocarattere.org>
Message-ID: <11760459.post@talk.nabble.com>




Manuele Pesenti wrote:
> 
> Dear R users,
> how can I extrapolate values listed in the summary of an lm model but not 
> directly available between object values such as the the standard errors
> of 
> the calculated parameters?
> 
> for example I got a model:
> 
> mod <- lm(Crd ~ 1 + Week, data=data)
> 
> and its summary:
> 
>> summary(mod)
> 
> Call:
> lm(formula = Crd ~ 1 + Week, data = data, model = TRUE, y = TRUE)
> 
> Residuals:
>        Min         1Q     Median         3Q        Max 
> -4.299e-03 -1.653e-03  2.628e-05  1.291e-03  5.130e-03 
> 
> Coefficients:
>              Estimate Std. Error  t value Pr(>|t|)    
> (Intercept) 1.000e+01  3.962e-04 25238.73   <2e-16 ***
> Week        5.038e-04  6.812e-06    73.96   <2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Residual standard error: 0.001966 on 98 degrees of freedom
> Multiple R-Squared: 0.9824,     Adjusted R-squared: 0.9822 
> F-statistic:  5469 on 1 and 98 DF,  p-value: < 2.2e-16
> 
> I'm interested in values of Std. Error of coefficients...
> 
> thank you very much
> 

If you want to assign these values to some other variables, try assigning
the result of the summary() to a variable and working with its components
(the result is a list, use $ or [[]] to get its members)

mod.sum<-summary(mod)
then
coef(mod.sum)[,2]
or
mod.sum$coefficients[,2]

will give you those Std. Errors

-- 
View this message in context: http://www.nabble.com/values-from-a-linear-model-tf4134911.html#a11760459
Sent from the R help mailing list archive at Nabble.com.


From elio.mineo at dssm.unipa.it  Tue Jul 24 13:14:49 2007
From: elio.mineo at dssm.unipa.it (Elio Mineo)
Date: Tue, 24 Jul 2007 13:14:49 +0200
Subject: [R] extension of rnormp package
In-Reply-To: <Pine.LNX.4.64.0707240628160.20066@gannet.stats.ox.ac.uk>
References: <003b01c7cd65$06883810$16b2a8c0@ivii>
	<Pine.LNX.4.64.0707240628160.20066@gannet.stats.ox.ac.uk>
Message-ID: <1185275689.2602.20.camel@localhost.localdomain>

Il giorno mar, 24/07/2007 alle 06.39 +0100, Prof Brian Ripley ha
scritto:
> On Mon, 23 Jul 2007, Iwona Szyd?owska wrote:
> 
> > Hello,
> 
> > I would like to ask You, how to generate random numbers from an 
> > exponential power family with a shape parameter p less than 1(p->0). I 
> > found the rnormp package, which can generate numbers from this 
> > distribution, but only for parameter less or equal 1.
> 
> It seems you mean package 'normalp', and that the package author believes 
> that the exponential power distribution is only defined for p >= 1 
> (although that is not on the help page). Other authors believe it is 
> defined by a relationship to the gamma for all p > 0. So all you need to 
> do is to change the condition from p < 1 to p <= 0 in rnormp and friends.
> 
Well, I know that an exponential power distribution is defined for p>0,
(I think quite all the references I know consider p>0), but for 0<p<1
the algorithms that I have implemented for the estimates of the
distribution parameters and for the regression parameters are really
instable (pratically are not usable at all). Then, I prefered for all
the functions of the normalp package consider only the case p>=1.
All the best,
Angelo Mineo

> However, the algorithms used are not adequate for large or small p.  We 
> know that the distribution tends to uniform for p -> Inf, but pnormp and 
> rnormp break down for quite modest values of p.  As p -> 0 it tends to a 
> point distribution at 0, but you will see very large values far too often.
> So if you want p smaller than say 0.01 you will need to implement a 
> different algorithm.
> 
> >
> > Regards,
> >            Iwona Szydlowska
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From david.meyer at wu-wien.ac.at  Tue Jul 24 13:08:22 2007
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Tue, 24 Jul 2007 13:08:22 +0200
Subject: [R] nnet 10-fold cross-validation
Message-ID: <46A5DDA6.7010508@wu-wien.ac.at>

Hi all,

there is tune() in the e1071 package for doing this in general, and, 
among others, a tune.nnet() wrapper (see ?tune):


 > tmodel = tune.nnet(Species ~ ., data = iris, size = 1:5)
 > summary(tmodel)

Parameter tuning of `nnet':

- sampling method: 10-fold cross validation

- best parameters:
  size
     1

- best performance: 0.01333333

- Detailed performance results:
   size      error dispersion
1    1 0.01333333 0.02810913
2    2 0.02666667 0.04661373
3    3 0.02666667 0.04661373
4    4 0.02000000 0.04499657
5    5 0.02666667 0.04661373

 > plot(tmodel)
 > tmodel$best.model
a 4-1-3 network with 11 weights
inputs: Sepal.Length Sepal.Width Petal.Length Petal.Width
output(s): Species
options were - softmax modelling

etc.

Best
David



On 7/23/07, S.O. Nyangoma <S.O.Nyangoma at amc.uva.nl> wrote:
 > > Hi
 > > It clear that to do a classification with svm under 10-fold cross
 > > validation one uses
 > >
 > > svm(Xm, newlabs, type = "C-classification", kernel = "linear",cross =
 > > 10)
 > >
 > > What corresponds to the nnet?
 > > nnet(.....,cross=10)?


From jmb at mssl.ucl.ac.uk  Tue Jul 24 13:10:27 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Tue, 24 Jul 2007 12:10:27 +0100 (BST)
Subject: [R] Overlaying a single contour from a new data array in levelplot
Message-ID: <200707241110.l6OBARYu011358@msslhb.mssl.ucl.ac.uk>

Dear R-Help community,

I am trying to overlay a single contour line over a correlation plot using 
levelplot in the lattice package. These are the two arrays:

1) a correlation plot over Africa - so each grid square is a different colour 
dependent on correlation - this is in an array: result_cor with dim[465,465]

2) a single contour line from a ***different data source*** - this is from data 
related to the p-values for the above correlation plot - I want to overlay only 
the 95% confidence contour. The p-values are stored in an array: result.p.values 
with same dimensions as above.

I have read about using panel.levelplot and panel.contourplot in the R-help 
mailing list but I don't know the right way to call two different data arrays, 
can anybody help me please? I appreciate your time and help with this question.

Many thanks,

Jenny 



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student: long range drought prediction 
Climate Extremes Group
Department of Space and Climate Physics
University College London
Holmbury St Mary 
Dorking, Surrey, RH5 6NT
Tel: 01483 204149
Mob: 07916 139187
Web: http://climate.mssl.ucl.ac.uk


From andrew.clegg at gmail.com  Tue Jul 24 13:20:53 2007
From: andrew.clegg at gmail.com (Andrew Clegg)
Date: Tue, 24 Jul 2007 12:20:53 +0100
Subject: [R] Fitting exponential curve to data points
In-Reply-To: <XFMail.070724100835.ted.harding@nessie.mcc.ac.uk>
References: <e507996c0707231809q5a519cd7yac4bc7f1437e3be@mail.gmail.com>
	<XFMail.070724100835.ted.harding@nessie.mcc.ac.uk>
Message-ID: <e507996c0707240420g548ccf4br6f2aa23c943cc168@mail.gmail.com>

Stephen, Ted -- thanks for your input. I'm glad to know I was barking
up the right-ish tree at least.

On 7/24/07, Ted Harding <ted.harding at nessie.mcc.ac.uk> wrote:
>
> There are not enough data to properly identify the non-linearity,
> but the overall appearance of the data plot suggests to me that
> you should be considering one of the "growth curve" models.
>
> Many such models start of with an increasing rate of growth,
> which then slows down, and typically levels off to an asymptote.
> The apparent large discrepancy of your final data point could
> be compatible with this kind of behaviour.

You may have hit the nail on the head there. At least I now know that
my method would be reasonable *if* I had a genuine exponential curve.
Bound to come in handy.

> At this point, knowledge of what kind of thing is represented
> by your "count" variable might be helpful. If, for instance,
> it is the count of the numbers of individuals of a species in
> an area, then independent knowledge of growth mechanisms may
> help to narrow down the kind of model you should be tring to fit.

It's the cumulative number of citations in the MEDLINE literature
database about a particular topic (natural language processing in
biomedicine). So indeed, it can't maintain an exponential growth rate
for long, and an initial spurt while the field is novel and trendy,
followed by a levelling-off, is just what we'd expect. There was a
review a year or so ago that showed a very good exponential fit *then*
but if I could show the last point was indicative of a slowdown, that
would be news at least.

Can anyone point me at a better modelling framework than lm(), in that case?

Thanks again,

Andrew.


From lzarauz at pas.azti.es  Tue Jul 24 13:23:00 2007
From: lzarauz at pas.azti.es (Lucia Zarauz)
Date: Tue, 24 Jul 2007 13:23:00 +0200
Subject: [R] plotting gam models
Message-ID: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>


Hi everybody,

I am working with gams and I have found some questions when plotting gams models.

I am using mgcv, and my model looks something like this:

model<- gam(x ~ s(lat,long))

I can plot the output of the model using plot(model) or plot.gam(model) and I get a surface plot. 

That is ok, but what I want to do now is to extract the data used to perform the surface plot. Like that I would be able to superpose them to a geographical map, and to plot these data using other programs.

Thank you very much in advance

Luc?a
_____________________________________________ 
Lucia Zarauz 
AZTI - Tecnalia / Unidad de Investigaci?n Marina
Herrera kaia portualdea z/g
20110 Pasaia (Gipuzkoa)
Tel: 943 004 800 - Fax: 943 004 801
e-mail: lzarauz at pas.azti.es
www.azti.es ; www.tecnalia.info
_____________________________________________ 
? 
**************** LEGE OHARRA **************** AVISO LEGAL **************** DISCLAIMER ****************
Mezu hau pertsonala eta isilpekoa da eta baimenik gabeko erabilera debekatua dago legalki. Jasotzailea ez bazara ezabatu mezua, bidali eta kontserbatu gabe. 
Este mensaje es personal y confidencial y su uso no autorizado est? prohibido legalmente. Si usted no es el destinatario, proceda a borrarlo, sin reenviarlo ni conservarlo.
This message is personal and confidential, unauthorised use is legally prohibited. If you are not the intended recipient, delete it without resending or backing it.


From brown_emu at yahoo.com  Tue Jul 24 13:29:43 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Tue, 24 Jul 2007 04:29:43 -0700 (PDT)
Subject: [R] Fwd: Re:  Fitting exponential curve to data points
Message-ID: <443111.31257.qm@web39701.mail.mud.yahoo.com>

Hope these help for alternatives to lm()? I show the use of a 2nd order
polynomial as an example to generalize a bit.

Sometimes from the subject line two separate responses can appear as reposts
when in fact they are not... (though there are identical reposts too). I
should probably figure a way around that.

--- Stephen Tucker <brown_emu at yahoo.com> wrote:

> ## Data input
> input <-
> "Year	Count
> 1999	3
> 2000	5
> 2001	9
> 2002	30
> 2003	62
> 2004	154
> 2005	245
> 2006	321"
> 
> dat <- read.table(textConnection(input),header=TRUE)
> dat[,] <- lapply(dat,function(x) x-x[1])
>           # shifting in origin; will need to add back in later
> 
> ## Nonlinear least squares
> plot(dat)
> out <- nls(Count~b0*exp(b1*Year),data=dat,
>            start=list(b0=1,b1=1))
> lines(dat[,1],fitted(out),col=2)
> out <- nls(Count~b0+b1*Year+b2*Year^2,data=dat, #polynomial
>            start=list(b0=0,b1=1,b2=1))
> lines(dat[,1],fitted(out),col=3)
> 
> ## Optim
> f <- function(.pars,.dat,.fun) sum((.dat[,2]-.fun(.pars,.dat[,1]))^2)
> fitFun <- function(b,x) cbind(1,x,x^2)%*%b
> expFun <- function(b,x) b[1]*exp(b[2]*x)
> 
> plot(dat)
> out <- optim(c(0,1,1),f,.dat=dat,.fun=fitFun)
> lines(dat[,1],fitFun(out$par,dat[,1]),col=2)
> out <- optim(c(1,1),f,.dat=dat,.fun=expFun)
> lines(dat[,1],expFun(out$par,dat[,1]),col=3)



       
____________________________________________________________________________________
Got a little couch potato? 
Check out fun summer activities for kids.


From wwwhsd at gmail.com  Tue Jul 24 13:36:25 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Tue, 24 Jul 2007 08:36:25 -0300
Subject: [R] plotting gam models
In-Reply-To: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>
References: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>
Message-ID: <da79af330707240436i404b7d0t2572cda2315535b2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070724/d489dcc3/attachment.pl 

From lzarauz at pas.azti.es  Tue Jul 24 13:52:57 2007
From: lzarauz at pas.azti.es (Lucia Zarauz)
Date: Tue, 24 Jul 2007 13:52:57 +0200
Subject: [R] plotting gam models
References: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>
	<da79af330707240436i404b7d0t2572cda2315535b2@mail.gmail.com>
Message-ID: <FCEE957ABD363A449A2483804ED0EDE3784C8E@psrcorreo.azti.local>

Hi Henrique,

Thank you for your suggestion. 

Actually, I have already tried it, but I am confused because the plot I get is not the same as the output of plot(model) or plot.gam(model). The yaxis is different

On the other hand, if I build a more complex model, as for example:

model<- gam(x ~ s(lat,long) + s(temperature))

I would like to extract the information to build the effects for each explanatory factor (one graph for s(lat,long) and another for s(temperature)), as R does when you use 'plot(model)' and you press return for subsequent pages.

My final aim is to plot the influence of s(lat,long) as a contourplot superposed on a geographical map. Maybe there is an easier way to do it...

Thank you very much

luc?a



_____________________________________________ 
Lucia Zarauz 
AZTI - Tecnalia / Unidad de Investigaci?n Marina
Herrera kaia portualdea z/g
20110 Pasaia (Gipuzkoa)
Tel: 943 004 800 - Fax: 943 004 801
e-mail: lzarauz at pas.azti.es
www.azti.es ; www.tecnalia.info
_____________________________________________ 
? 
**************** LEGE OHARRA **************** AVISO LEGAL **************** DISCLAIMER ****************
Mezu hau pertsonala eta isilpekoa da eta baimenik gabeko erabilera debekatua dago legalki. Jasotzailea ez bazara ezabatu mezua, bidali eta kontserbatu gabe. 
Este mensaje es personal y confidencial y su uso no autorizado est? prohibido legalmente. Si usted no es el destinatario, proceda a borrarlo, sin reenviarlo ni conservarlo.
This message is personal and confidential, unauthorised use is legally prohibited. If you are not the intended recipient, delete it without resending or backing it.

________________________________________
De: Henrique Dallazuanna [mailto:wwwhsd at gmail.com] 
Enviado el: martes, 24 de julio de 2007 13:36
Para: Lucia Zarauz
CC: r-help at stat.math.ethz.ch
Asunto: Re: [R] plotting gam models

see ?predict.gam

-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O 
On 24/07/07, Lucia Zarauz <lzarauz at pas.azti.es> wrote:

Hi everybody,

I am working with gams and I have found some questions when plotting gams models.

I am using mgcv, and my model looks something like this:

model<- gam(x ~ s(lat,long))

I can plot the output of the model using plot(model) or plot.gam(model) and I get a surface plot.

That is ok, but what I want to do now is to extract the data used to perform the surface plot. Like that I would be able to superpose them to a geographical map, and to plot these data using other programs. 

Thank you very much in advance

Luc?a
_____________________________________________
Lucia Zarauz
AZTI - Tecnalia / Unidad de Investigaci?n Marina
Herrera kaia portualdea z/g
20110 Pasaia (Gipuzkoa) 
Tel: 943 004 800 - Fax: 943 004 801
e-mail: lzarauz at pas.azti.es
www.azti.es ; www.tecnalia.info
_____________________________________________

**************** LEGE OHARRA **************** AVISO LEGAL **************** DISCLAIMER ****************
Mezu hau pertsonala eta isilpekoa da eta baimenik gabeko erabilera debekatua dago legalki. Jasotzailea ez bazara ezabatu mezua, bidali eta kontserbatu gabe. 
Este mensaje es personal y confidencial y su uso no autorizado est? prohibido legalmente. Si usted no es el destinatario, proceda a borrarlo, sin reenviarlo ni conservarlo.
This message is personal and confidential, unauthorised use is legally prohibited. If you are not the intended recipient, delete it without resending or backing it. 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From villegas.ro at gmail.com  Tue Jul 24 14:08:24 2007
From: villegas.ro at gmail.com (Rod)
Date: Tue, 24 Jul 2007 14:08:24 +0200
Subject: [R] plotting gam models
In-Reply-To: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>
References: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>
Message-ID: <29cf68350707240508h3ad2c769nfb5eeacdca9e19b5@mail.gmail.com>

2007/7/24, Lucia Zarauz <lzarauz at pas.azti.es>:
>
> Hi everybody,
>
> I am working with gams and I have found some questions when plotting gams models.
>
> I am using mgcv, and my model looks something like this:
>
> model<- gam(x ~ s(lat,long))
>
> I can plot the output of the model using plot(model) or plot.gam(model) and I get a surface plot.
>
> That is ok, but what I want to do now is to extract the data used to perform the surface plot. Like that I would be able to superpose them to a geographical map, and to plot these data using other programs.
>
> Thank you very much in advance
>
> Luc?a
> _____________________________________________
> Lucia Zarauz
> AZTI - Tecnalia / Unidad de Investigaci?n Marina
> Herrera kaia portualdea z/g
> 20110 Pasaia (Gipuzkoa)
> Tel: 943 004 800 - Fax: 943 004 801
> e-mail: lzarauz at pas.azti.es
> www.azti.es ; www.tecnalia.info
> _____________________________________________
>
> **************** LEGE OHARRA **************** AVISO LEGAL **************** DISCLAIMER ****************
> Mezu hau pertsonala eta isilpekoa da eta baimenik gabeko erabilera debekatua dago legalki. Jasotzailea ez bazara ezabatu mezua, bidali eta kontserbatu gabe.
> Este mensaje es personal y confidencial y su uso no autorizado est? prohibido legalmente. Si usted no es el destinatario, proceda a borrarlo, sin reenviarlo ni conservarlo.
> This message is personal and confidential, unauthorised use is legally prohibited. If you are not the intended recipient, delete it without resending or backing it.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Also see ?vis.gam.
Produces perspective or contour plot views of gam model predictions.

Rod.


From s.wood at bath.ac.uk  Tue Jul 24 14:02:18 2007
From: s.wood at bath.ac.uk (Simon Wood)
Date: Tue, 24 Jul 2007 13:02:18 +0100
Subject: [R] plotting gam models
In-Reply-To: <FCEE957ABD363A449A2483804ED0EDE3784C8E@psrcorreo.azti.local>
References: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>
	<da79af330707240436i404b7d0t2572cda2315535b2@mail.gmail.com>
	<FCEE957ABD363A449A2483804ED0EDE3784C8E@psrcorreo.azti.local>
Message-ID: <200707241302.18610.s.wood@bath.ac.uk>

>
> Thank you for your suggestion.
>
> Actually, I have already tried it, but I am confused because the plot I get
> is not the same as the output of plot(model) or plot.gam(model). The yaxis
> is different
-- `plot.gam' will always plot the `centered smooth', i.e.  the smooth 
constrained to sum to zero over the data. By default (i.e. using type="link") 
`predict.gam' will include the intercept in the predctions. If you want 
centered terms out of `predict.gam' use the `type="terms" option.

>
> On the other hand, if I build a more complex model, as for example:
>
> model<- gam(x ~ s(lat,long) + s(temperature))
>
> I would like to extract the information to build the effects for each
> explanatory factor (one graph for s(lat,long) and another for
> s(temperature)), as R does when you use 'plot(model)' and you press return
> for subsequent pages.
>
Again, try predict.gam with type="terms"

best,
Simon
-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From ripley at stats.ox.ac.uk  Tue Jul 24 14:36:22 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jul 2007 13:36:22 +0100 (BST)
Subject: [R] persp and greek symbols in the axes labels
In-Reply-To: <809791.74780.qm@web39714.mail.mud.yahoo.com>
References: <809791.74780.qm@web39714.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0707241323340.29110@gannet.stats.ox.ac.uk>

On Tue, 24 Jul 2007, Stephen Tucker wrote:

> I don't know why it doesn't work but I think people generally recommend that

It has never been implemented, and I believe the main reason is that the 
labels are plotted at an angle other than a multiple of 90 degrees.  Not 
all devices can do that, and rotated plotmath text can look quite ugly.

And of course this _is_ documented in ?persp

xlab, ylab, zlab: titles for the axes.  N.B. These must be character
           strings; expressions are not accepted.  Numbers will be
           coerced to character strings.

> you use wireframe() in lattice rather than persp(), because wireframe is more
> customizable (the pdf document referred to in this post is pretty good):
> http://tolstoy.newcastle.edu.au/R/e2/help/07/03/12534.html
>
> Here's an example:
>
> library(lattice)
> library(reshape)
> x <- 1:5
> y <- 1:3
> z <- matrix(1:15,ncol=3,dimnames=list(NULL,y))
> M <- melt(data.frame(x,z,check.names=FALSE),id=1,variable="y")
> wireframe(value~x*y,data=M,
>          screen=list(z=45,x=-75),
>          xlab=expression(kappa[lambda]),
>          ylab=as.expression(substitute(paste(phi,"=",true,sigma),
>              list(true=5))),
>          zlab = "Z")
>
> [you can play around with the 'screen' argument to rotate the view, analogous
> to phi and theta in persp()]

Of course, that does not rotate the labels.   If unrotated labels are 
acceptable, you can easily set up a new coordinate system (by par(usr=)) 
and call text() to put labels where you want on that.  You can even try 
rotating them via srt=.

There would be no harm in implementing this for use on devices where it 
will work: a nice self-contained project for someone who would like to 
learn about R internals.


> --- Nathalie Peyrard <Nathalie.Peyrard at toulouse.inra.fr> wrote:
>
>> Hello,
>>
>> I am plotting a 3D function using persp and I would like to use greek
>> symbols in the axes labels.
>> I  have found examples like  this one on the web:
>>
>>
> plot(0,0,xlab=expression(kappa[lambda]),ylab=substitute(paste(phi,"=",true,sigma),list(true=5)))
>>
>> this works well with plot but not with persp:
>> with the command
>>
>> persp(M,theta = -20,phi =
>>
> 0,xlab=expression(kappa[lambda]),ylab=substitute(paste(phi,"=",true,sigma),list(true=5)),zlab
>>
>> = "Z")
>>
>> I get the labels as in toto.eps
>>
>> Any suggestion? Thanks!
>>
>> Nathalie
>>
>> --
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> INRA  Toulouse - Unit? de Biom?trie et  Intelligence Artificielle
>> Chemin de Borde-Rouge BP 52627 31326 CASTANET-TOLOSAN cedex FRANCE
>> Tel : +33(0)5.61.28.54.39 - Fax : +33(0)5.61.28.53.35
>> Web :http://mia.toulouse.inra.fr/index.php?id=217
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>
>>
>>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
>
>
> ____________________________________________________________________________________Ready for the edge of your seat?
> Check out tonight's top picks on Yahoo! TV.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From bogklug at mimuw.edu.pl  Tue Jul 24 14:46:16 2007
From: bogklug at mimuw.edu.pl (Boguslaw Kluge)
Date: Tue, 24 Jul 2007 14:46:16 +0200 (CEST)
Subject: [R] "recursive" vectorized arithmetics
Message-ID: <Pine.LNX.4.64N.0707241405080.17921@duch.mimuw.edu.pl>

 	Hi,

 	I would like to write something similar to:
for(t in 1:100)
 	v[x[t]] <- v[y[t]] + v[z[t]]
in a vectorized form. The x, y, and z vectors may contain duplicates (so v[x] <- v[y] + v[z] has different semantics). The for loop is not efficient enough for my purposes and I would like to avoid using C/Fortran.

 	This problem occurred to me on several occasions and I feel it is quite general. Does anyone have an idea how to solve it nicely?

 	Thanks,
 	bk


From Nathalie.Peyrard at toulouse.inra.fr  Tue Jul 24 14:57:27 2007
From: Nathalie.Peyrard at toulouse.inra.fr (Nathalie Peyrard)
Date: Tue, 24 Jul 2007 14:57:27 +0200
Subject: [R] persp and greek symbols in the axes labels
In-Reply-To: <Pine.LNX.4.64.0707241323340.29110@gannet.stats.ox.ac.uk>
References: <809791.74780.qm@web39714.mail.mud.yahoo.com>
	<Pine.LNX.4.64.0707241323340.29110@gannet.stats.ox.ac.uk>
Message-ID: <46A5F737.1060601@toulouse.inra.fr>

Thank you for these answers.

I ended up modifying the ps file directly. But next time I will consider 
wireframe.

Nathalie

Prof Brian Ripley wrote:
> On Tue, 24 Jul 2007, Stephen Tucker wrote:
>
>> I don't know why it doesn't work but I think people generally 
>> recommend that
>
> It has never been implemented, and I believe the main reason is that 
> the labels are plotted at an angle other than a multiple of 90 
> degrees.  Not all devices can do that, and rotated plotmath text can 
> look quite ugly.
>
> And of course this _is_ documented in ?persp
>
> xlab, ylab, zlab: titles for the axes.  N.B. These must be character
>           strings; expressions are not accepted.  Numbers will be
>           coerced to character strings.
>
>> you use wireframe() in lattice rather than persp(), because wireframe 
>> is more
>> customizable (the pdf document referred to in this post is pretty good):
>> http://tolstoy.newcastle.edu.au/R/e2/help/07/03/12534.html
>>
>> Here's an example:
>>
>> library(lattice)
>> library(reshape)
>> x <- 1:5
>> y <- 1:3
>> z <- matrix(1:15,ncol=3,dimnames=list(NULL,y))
>> M <- melt(data.frame(x,z,check.names=FALSE),id=1,variable="y")
>> wireframe(value~x*y,data=M,
>>          screen=list(z=45,x=-75),
>>          xlab=expression(kappa[lambda]),
>>          ylab=as.expression(substitute(paste(phi,"=",true,sigma),
>>              list(true=5))),
>>          zlab = "Z")
>>
>> [you can play around with the 'screen' argument to rotate the view, 
>> analogous
>> to phi and theta in persp()]
>
> Of course, that does not rotate the labels.   If unrotated labels are 
> acceptable, you can easily set up a new coordinate system (by 
> par(usr=)) and call text() to put labels where you want on that.  You 
> can even try rotating them via srt=.
>
> There would be no harm in implementing this for use on devices where 
> it will work: a nice self-contained project for someone who would like 
> to learn about R internals.
>
>
>> --- Nathalie Peyrard <Nathalie.Peyrard at toulouse.inra.fr> wrote:
>>
>>> Hello,
>>>
>>> I am plotting a 3D function using persp and I would like to use greek
>>> symbols in the axes labels.
>>> I  have found examples like  this one on the web:
>>>
>>>
>> plot(0,0,xlab=expression(kappa[lambda]),ylab=substitute(paste(phi,"=",true,sigma),list(true=5))) 
>>
>>>
>>> this works well with plot but not with persp:
>>> with the command
>>>
>>> persp(M,theta = -20,phi =
>>>
>> 0,xlab=expression(kappa[lambda]),ylab=substitute(paste(phi,"=",true,sigma),list(true=5)),zlab 
>>
>>>
>>> = "Z")
>>>
>>> I get the labels as in toto.eps
>>>
>>> Any suggestion? Thanks!
>>>
>>> Nathalie
>>>
>>> -- 
>>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>> INRA  Toulouse - Unit? de Biom?trie et  Intelligence Artificielle
>>> Chemin de Borde-Rouge BP 52627 31326 CASTANET-TOLOSAN cedex FRANCE
>>> Tel : +33(0)5.61.28.54.39 - Fax : +33(0)5.61.28.53.35
>>> Web :http://mia.toulouse.inra.fr/index.php?id=217
>>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>>
>>>
>>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>>
>>
>> ____________________________________________________________________________________Ready 
>> for the edge of your seat?
>> Check out tonight's top picks on Yahoo! TV.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From andrew.clegg at gmail.com  Tue Jul 24 15:20:03 2007
From: andrew.clegg at gmail.com (Andrew Clegg)
Date: Tue, 24 Jul 2007 14:20:03 +0100
Subject: [R] Fitting exponential curve to data points
In-Reply-To: <443111.31257.qm@web39701.mail.mud.yahoo.com>
References: <443111.31257.qm@web39701.mail.mud.yahoo.com>
Message-ID: <e507996c0707240620ycfc596fu14f3d76709ce32ea@mail.gmail.com>

On 7/24/07, Stephen Tucker <brown_emu at yahoo.com> wrote:
> Hope these help for alternatives to lm()? I show the use of a 2nd order
> polynomial as an example to generalize a bit.

Great, thanks. If I want to demonstrate that a non-linear curve fits
better than an exponential, what's the best measure for that? Given
that neither of nls() or optim() provide R-squared. Sorry if these are
really silly questions.

> Sometimes from the subject line two separate responses can appear as reposts
> when in fact they are not... (though there are identical reposts too). I
> should probably figure a way around that.

Nope, my fault, I didn't read them properly and thought you were
demonstrating a different way to do exponential curves.

Cheers,

Andrew.


From Karl.Hufthammer at math.uib.no  Tue Jul 24 16:06:34 2007
From: Karl.Hufthammer at math.uib.no (Karl Ove Hufthammer)
Date: Tue, 24 Jul 2007 16:06:34 +0200
Subject: [R] Fitting exponential curve to data points
References: <443111.31257.qm@web39701.mail.mud.yahoo.com>
	<e507996c0707240620ycfc596fu14f3d76709ce32ea@mail.gmail.com>
Message-ID: <f8511a$2ih$1@sea.gmane.org>

Andrew Clegg:

> Great, thanks. If I want to demonstrate that a non-linear curve fits
> better than an exponential, what's the best measure for that? Given
> that neither of nls() or optim() provide R-squared.

You really need to *very* careful when trying to interprete R? (which can
be defined in many nonequivalent ways) in the nonlinear case. Recommended
(and, dare I say, *required* reading):

Anderson-Sprecher R. (1994). ?Model comparisons and R??. The American
  Statistician, volume 48, no. 2, pages 113?117. DOI: 10.2307/2684259

Kv?lseth T.O. (1985). ?Cautionary note about R??. The American Statistician,
  volume 39, no. 4, pages 279?285. DOI: 10.2307/2683704

Scott A. and Wild C. (1991). ?Transformations and R??. The American
  Statistician, volume 45, no. 2, pages 127?129. ISSN 0003-1305.
  DOI: 10.2307/2684375

The Scott & Wild paper has an example that looks very similar to yours, and
that may be instructive.

FYI, in case you?re not used to DOIs: you can resolve the above DOIs to
fulltext URLs using http://dx.doi.org/

-- 
Karl Ove Hufthammer


From uhtrivedi208 at yahoo.co.in  Tue Jul 24 16:10:33 2007
From: uhtrivedi208 at yahoo.co.in (Urmi Trivedi)
Date: Tue, 24 Jul 2007 15:10:33 +0100 (BST)
Subject: [R] Comparing a distance vs. correlation matrices
Message-ID: <865857.58634.qm@web8503.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070724/31488c1a/attachment.pl 

From yn19832 at msn.com  Tue Jul 24 16:42:52 2007
From: yn19832 at msn.com (livia)
Date: Tue, 24 Jul 2007 07:42:52 -0700 (PDT)
Subject: [R] Fit t distribution
Message-ID: <11764680.post@talk.nabble.com>


Hi all, I am trying to fit t distribution using the function "tFit" in the
library(fBasics). 

I am using the code tFit(datac[[2]]) and it returns the following list.

Title:
 Student-t Parameter Estimation 

Call:
 tFit(x = datac[[2]])

Model:
 Student-t Distribution

Estimated Parameter(s):
     df 
78.4428 

I just wonder how can I refer to the estimated parameters. I tried
tFit(datac[[2]]) $df,tFit(datac[[2]])@df, but neither of them work.
Could anyone give me some advice?

-- 
View this message in context: http://www.nabble.com/Fit-t-distribution-tf4136445.html#a11764680
Sent from the R help mailing list archive at Nabble.com.


From wwwhsd at gmail.com  Tue Jul 24 16:49:21 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Tue, 24 Jul 2007 11:49:21 -0300
Subject: [R] Fit t distribution
In-Reply-To: <11764680.post@talk.nabble.com>
References: <11764680.post@talk.nabble.com>
Message-ID: <da79af330707240749t1e80f4d9k1ab564b7d7c28716@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070724/df2972c8/attachment.pl 

From yn19832 at msn.com  Tue Jul 24 17:01:18 2007
From: yn19832 at msn.com (livia)
Date: Tue, 24 Jul 2007 08:01:18 -0700 (PDT)
Subject: [R] Fit t distribution
In-Reply-To: <da79af330707240749t1e80f4d9k1ab564b7d7c28716@mail.gmail.com>
References: <11764680.post@talk.nabble.com>
	<da79af330707240749t1e80f4d9k1ab564b7d7c28716@mail.gmail.com>
Message-ID: <11764994.post@talk.nabble.com>


It works. Many thanks

Henrique Dallazuanna wrote:
> 
> Hi,
> 
> tFit(datac[[2]])@fit$estimate
> 
> 
> -- 
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
> 
> On 24/07/07, livia <yn19832 at msn.com> wrote:
>>
>>
>> Hi all, I am trying to fit t distribution using the function "tFit" in
>> the
>> library(fBasics).
>>
>> I am using the code tFit(datac[[2]]) and it returns the following list.
>>
>> Title:
>> Student-t Parameter Estimation
>>
>> Call:
>> tFit(x = datac[[2]])
>>
>> Model:
>> Student-t Distribution
>>
>> Estimated Parameter(s):
>>      df
>> 78.4428
>>
>> I just wonder how can I refer to the estimated parameters. I tried
>> tFit(datac[[2]]) $df,tFit(datac[[2]])@df, but neither of them work.
>> Could anyone give me some advice?
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Fit-t-distribution-tf4136445.html#a11764680
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Fit-t-distribution-tf4136445.html#a11764994
Sent from the R help mailing list archive at Nabble.com.


From bbernzwe at bear.com  Tue Jul 24 17:05:25 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Tue, 24 Jul 2007 11:05:25 -0400
Subject: [R] apply & incompatible dimensions error
References: <11740491.post@talk.nabble.com> <46A47D9F.4000309@biostat.ku.dk>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>

Hi,

I've created the following two matrices (mat1 and mat2) and a function
(f) to calculate the correlations between the two on a row by row basis.

	mat1 <- matrix(sample(1:500,50), ncol = 5, 
		dimnames=list(paste("row", 1:10, sep=""), 
		paste("col", 1:5, sep="")))

	mat2 <- matrix(sample(501:1000,50), ncol = 5, 
		dimnames=list(paste("row", 1:10, sep=""), 
		paste("col", 1:5, sep="")))

	f <- function(x,y) cor(x,y)

When the matrices are squares (# rows = # columns) I have no problems.

However, when they are not (as in the example above with 5 columns and
10 rows), I get the following error:

> apply(mat1, 1, f, y=mat2)
Error in cor(x, y, na.method, method == "kendall") : 
        incompatible dimensions

Any help would be appreciated.  Thanks!

- Bruce



**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From andy_liaw at merck.com  Tue Jul 24 17:21:52 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 24 Jul 2007 11:21:52 -0400
Subject: [R] randomForest importance problem with combine [Broadcast]
In-Reply-To: <615499.60887.qm@web60313.mail.yahoo.com>
References: <676546.48813.qm@web60314.mail.yahoo.com>
	<615499.60887.qm@web60313.mail.yahoo.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA047AEEC4@usctmx1106.merck.com>

I've been fixing some problems in the combine() function, but that's
only for regression data.  Looks like you are doing classification, and
I don't see the problem:

R> library(randomForest)
randomForest 4.5-19 
Type rfNews() to see new features/changes/bug fixes.
R> set.seed(1)
R> rflist <- replicate(50, randomForest(iris[-5], iris[[5]], ntree=50,
importance=TRUE), simplify=FALSE)
R> rfall <- do.call(combine, rflist)
R> importance(rfall)
                setosa versicolor virginica MeanDecreaseAccuracy
Sepal.Length 0.4457861 0.53883425 0.5580657            0.4120840
Sepal.Width  0.3266790 0.07652383 0.3620240            0.2128450
Petal.Length 1.1950989 1.42014628 1.3220471            0.7989841
Petal.Width  1.1986973 1.40855969 1.3640620            0.7951053
             MeanDecreaseGini
Sepal.Length         9.578580
Sepal.Width          2.301172
Petal.Length        42.935832
Petal.Width         44.409058
R> importance(rflist[[1]])
               setosa  versicolor virginica MeanDecreaseAccuracy
Sepal.Length 0.401714  0.71583422 0.4946420            0.4166555
Sepal.Width  0.000000 -0.03155946 0.6829287            0.2317111
Petal.Length 1.290430  1.47915219 1.3456770            0.8219003
Petal.Width  1.110142  1.44996777 1.3584799            0.7881210
             MeanDecreaseGini
Sepal.Length         6.168439
Sepal.Width          2.240723
Petal.Length        48.821726
Petal.Width         42.059112

Please provide a reproducible example.

Andy
 

From: Joseph Retzer
> 
> My apologies, subject corrected.
> 
> 
> I'm building a RF 50 trees at a time due to memory limitations (I have
>  roughly .5 million observations and around 20 variables). I thought I
>  could combine some or all of my forests later and look at global
>  importance. 
> 
> If I have say 2 forests : tree1 and tree2, they have similar Gini and
>  Raw importances and, additionally, are similar to one another. After
>  combining (using the combine command) the trees into one however, the
>  combined tree Raw importances have changed in rank order 
> rather dramtically
>  (e.g. the top most important becomes least important. It is not
>  however a completely reversed ordering). In addtion, the 
> scale of both the
>  Raw and Gini importances is orders of magnitude smaller for 
> the combined
>  tree.
> 
> Note that the combined tree Gini importance looks roughly similar to
>  the individual tree Gini (and Raw) importance, at least in 
> terms of rank
>  ordering.
> 
> I'm using the non-formula randomForest specification  along  with
>   norm.votes=FALSE to facilitate  large sample  estimation  and  tree
>  combining.
> 
> I'm using R 2.5.0 on a windows XP machine with 2 gig RAM. I'm also
>  using randomForest 4.5-18.
> 
> Any advice is appreciated,
> Many thanks,
> Joe
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From bbernzwe at bear.com  Tue Jul 24 17:24:28 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Tue, 24 Jul 2007 11:24:28 -0400
Subject: [R] cor inside/outside a function has different output
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B8C@whexchmb16.bsna.bsroot.bear.com>

I'm calculating correlations between two matrices 

 

mat1 <- matrix(sample(1:500,25), ncol = 5, 

dimnames=list(paste("mat1row", 1:5, sep=""), 

paste("mat1col", 1:5, sep="")))

 

mat2 <- matrix(sample(501:1000,25), ncol = 5, 

dimnames=list(paste("mat2row", 1:5, sep=""), 

paste("mat2col", 1:5, sep="")))

 

using what would seem to be two similar methods:

 

  Method 1:

 

       > f <- function(x,y) cor(x,y)

       > apply(mat1, 1, f, y=mat2)

 

  Method 2:

 

        > cor(mat1, mat2)

 

However, the results (see blow) are different:

 

> apply(mat1, 1, f, y=mat2)

 

               mat1row1   mat1row2    mat1row3    mat1row4    mat1row5

[1,] -0.27601028 -0.1352143  0.03538690 -0.03084075 -0.60171704

[2,] -0.01595532 -0.3881197 -0.43663982  0.49081806  0.33291995

[3,]  0.35969624 -0.0582948  0.57462169  0.09926796 -0.02948423

[4,] -0.41435920 -0.7164638 -0.21213496 -0.55183934 -0.25341790

[5,]  0.33802803  0.5371508  0.05219095  0.83533575  0.17850291

 

> cor(mat1, mat2)

            mat2col1    mat2col2   mat2col3   mat2col4   mat2col5

mat1col1 -0.84077496 -0.01538414 -0.6078933 -0.2263840 -0.1421335

mat1col2  0.23074421  0.54606286 -0.2354733  0.5214255 -0.2129077

mat1col3 -0.80000528  0.19550100 -0.5920509 -0.8694040  0.6853990

mat1col4  0.08050976 -0.55449840  0.6225666  0.6187971 -0.8971584

mat1col5 -0.10199564 -0.43854767 -0.5803077 -0.5100285  0.2848351

 

Also, for method 2, the calculations are done on a column x column
basis.  Is there any way to do this on a row by row basis.  Looking at
the help page for cor, I don't see any parameters that could be used to
do this.

 

Thanks,

 

- Bruce

-------------- next part --------------


**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From ggrothendieck at gmail.com  Tue Jul 24 17:30:33 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 Jul 2007 11:30:33 -0400
Subject: [R] apply & incompatible dimensions error
In-Reply-To: <CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
References: <11740491.post@talk.nabble.com> <46A47D9F.4000309@biostat.ku.dk>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
Message-ID: <971536df0707240830r14386f43j849596e4ddc23d42@mail.gmail.com>

Your apply is trying to take the correlations of the rows of mat1 with the
columns of mat2 which, of course, does not work if they have different
numbers of columns. I think you mean to take the correlations of the columns
of mat1 with the columns of mat2.  For example, to take the correlations
of the 5 columns of mat1 with the first 4 columns of mat2 try:

> cor(mat1, mat2[,1:4])
            col1       col2       col3       col4
col1 -0.34624254 -0.2669519 -0.2705077  0.2183249
col2 -0.26553255 -0.2687643 -0.0865895  0.1819025
col3  0.19474613 -0.2334986  0.1746522  0.2326915
col4  0.09328338  0.5117784  0.2413143 -0.3374916
col5  0.27519716  0.1605331 -0.4057137  0.3282105


On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> Hi,
>
> I've created the following two matrices (mat1 and mat2) and a function
> (f) to calculate the correlations between the two on a row by row basis.
>
>        mat1 <- matrix(sample(1:500,50), ncol = 5,
>                dimnames=list(paste("row", 1:10, sep=""),
>                paste("col", 1:5, sep="")))
>
>        mat2 <- matrix(sample(501:1000,50), ncol = 5,
>                dimnames=list(paste("row", 1:10, sep=""),
>                paste("col", 1:5, sep="")))
>
>        f <- function(x,y) cor(x,y)
>
> When the matrices are squares (# rows = # columns) I have no problems.
>
> However, when they are not (as in the example above with 5 columns and
> 10 rows), I get the following error:
>
> > apply(mat1, 1, f, y=mat2)
> Error in cor(x, y, na.method, method == "kendall") :
>        incompatible dimensions
>
> Any help would be appreciated.  Thanks!
>
> - Bruce
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the pers...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bcarvalh at jhsph.edu  Tue Jul 24 17:30:57 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 24 Jul 2007 11:30:57 -0400
Subject: [R] apply & incompatible dimensions error
In-Reply-To: <CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
References: <11740491.post@talk.nabble.com> <46A47D9F.4000309@biostat.ku.dk>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
Message-ID: <A40A0054-D186-4686-8E4E-48C3C3A9ECDA@jhsph.edu>

are you positive that your function is doing what you expect it to do?

it looks like you want something like:

sapply(1:10, function(i) cor(mat1[i,], mat2[i,]))

b

On Jul 24, 2007, at 11:05 AM, Bernzweig, Bruce ((Consultant)) wrote:

> Hi,
>
> I've created the following two matrices (mat1 and mat2) and a function
> (f) to calculate the correlations between the two on a row by row  
> basis.
>
> 	mat1 <- matrix(sample(1:500,50), ncol = 5,
> 		dimnames=list(paste("row", 1:10, sep=""),
> 		paste("col", 1:5, sep="")))
>
> 	mat2 <- matrix(sample(501:1000,50), ncol = 5,
> 		dimnames=list(paste("row", 1:10, sep=""),
> 		paste("col", 1:5, sep="")))
>
> 	f <- function(x,y) cor(x,y)
>
> When the matrices are squares (# rows = # columns) I have no problems.
>
> However, when they are not (as in the example above with 5 columns and
> 10 rows), I get the following error:
>
>> apply(mat1, 1, f, y=mat2)
> Error in cor(x, y, na.method, method == "kendall") :
>         incompatible dimensions
>
> Any help would be appreciated.  Thanks!
>
> - Bruce
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the pers... 
> {{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bbernzwe at bear.com  Tue Jul 24 17:33:24 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Tue, 24 Jul 2007 11:33:24 -0400
Subject: [R] apply & incompatible dimensions error
References: <11740491.post@talk.nabble.com> <46A47D9F.4000309@biostat.ku.dk>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
	<971536df0707240830r14386f43j849596e4ddc23d42@mail.gmail.com>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B8E@whexchmb16.bsna.bsroot.bear.com>

Thanks Gabor!

You state that my apply is taking rows of mat1 with columns of mat2.

Is this because I have the y=mat2 parameter?

> apply(mat1, 1, f, y=mat2)

Actually, what I would like is to run the correlations on a row against
row basis:  mat1 row1 x mat2 row1, etc.

Thanks again,

- Bruce


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Tuesday, July 24, 2007 11:31 AM
To: Bernzweig, Bruce (Consultant)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] apply & incompatible dimensions error

Your apply is trying to take the correlations of the rows of mat1 with
the
columns of mat2 which, of course, does not work if they have different
numbers of columns. I think you mean to take the correlations of the
columns
of mat1 with the columns of mat2.  For example, to take the correlations
of the 5 columns of mat1 with the first 4 columns of mat2 try:

> cor(mat1, mat2[,1:4])
            col1       col2       col3       col4
col1 -0.34624254 -0.2669519 -0.2705077  0.2183249
col2 -0.26553255 -0.2687643 -0.0865895  0.1819025
col3  0.19474613 -0.2334986  0.1746522  0.2326915
col4  0.09328338  0.5117784  0.2413143 -0.3374916
col5  0.27519716  0.1605331 -0.4057137  0.3282105


On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> Hi,
>
> I've created the following two matrices (mat1 and mat2) and a function
> (f) to calculate the correlations between the two on a row by row
basis.
>
>        mat1 <- matrix(sample(1:500,50), ncol = 5,
>                dimnames=list(paste("row", 1:10, sep=""),
>                paste("col", 1:5, sep="")))
>
>        mat2 <- matrix(sample(501:1000,50), ncol = 5,
>                dimnames=list(paste("row", 1:10, sep=""),
>                paste("col", 1:5, sep="")))
>
>        f <- function(x,y) cor(x,y)
>
> When the matrices are squares (# rows = # columns) I have no problems.
>
> However, when they are not (as in the example above with 5 columns and
> 10 rows), I get the following error:
>
> > apply(mat1, 1, f, y=mat2)
> Error in cor(x, y, na.method, method == "kendall") :
>        incompatible dimensions
>
> Any help would be appreciated.  Thanks!
>
> - Bruce
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the
pers...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From bbernzwe at bear.com  Tue Jul 24 17:35:57 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Tue, 24 Jul 2007 11:35:57 -0400
Subject: [R] apply & incompatible dimensions error
References: <11740491.post@talk.nabble.com> <46A47D9F.4000309@biostat.ku.dk>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
	<A40A0054-D186-4686-8E4E-48C3C3A9ECDA@jhsph.edu>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B8F@whexchmb16.bsna.bsroot.bear.com>

Thanks Benilton,

I know what I want to do, just not sure how to do it using R.  The help
documentation is not very clear.

What I am trying to do is calculate correlations on a row against row
basis:  mat1 row1 x mat2 row1, mat1 row1 x mat2 row2, ... mat1 row1 x
mat2 row-n, mat1 row-n, mat2 row-n

- Bruce

-----Original Message-----
From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu] 
Sent: Tuesday, July 24, 2007 11:31 AM
To: Bernzweig, Bruce (Consultant)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] apply & incompatible dimensions error

are you positive that your function is doing what you expect it to do?

it looks like you want something like:

sapply(1:10, function(i) cor(mat1[i,], mat2[i,]))

b

On Jul 24, 2007, at 11:05 AM, Bernzweig, Bruce ((Consultant)) wrote:

> Hi,
>
> I've created the following two matrices (mat1 and mat2) and a function
> (f) to calculate the correlations between the two on a row by row  
> basis.
>
> 	mat1 <- matrix(sample(1:500,50), ncol = 5,
> 		dimnames=list(paste("row", 1:10, sep=""),
> 		paste("col", 1:5, sep="")))
>
> 	mat2 <- matrix(sample(501:1000,50), ncol = 5,
> 		dimnames=list(paste("row", 1:10, sep=""),
> 		paste("col", 1:5, sep="")))
>
> 	f <- function(x,y) cor(x,y)
>
> When the matrices are squares (# rows = # columns) I have no problems.
>
> However, when they are not (as in the example above with 5 columns and
> 10 rows), I get the following error:
>
>> apply(mat1, 1, f, y=mat2)
> Error in cor(x, y, na.method, method == "kendall") :
>         incompatible dimensions
>
> Any help would be appreciated.  Thanks!
>
> - Bruce
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the pers... 
> {{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.



**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From bcarvalh at jhsph.edu  Tue Jul 24 17:40:14 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Tue, 24 Jul 2007 11:40:14 -0400
Subject: [R] apply & incompatible dimensions error
In-Reply-To: <CADFD0E28E1E6A46B0C84335BDB994F504989B8F@whexchmb16.bsna.bsroot.bear.com>
References: <11740491.post@talk.nabble.com> <46A47D9F.4000309@biostat.ku.dk>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
	<A40A0054-D186-4686-8E4E-48C3C3A9ECDA@jhsph.edu>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8F@whexchmb16.bsna.bsroot.bear.com>
Message-ID: <4F7D12AB-C086-411C-B1A7-D64F579FECD7@jhsph.edu>

that's garbor's suggestion then.
sorry for the misunderstanding. :-)
b

On Jul 24, 2007, at 11:35 AM, Bernzweig, Bruce ((Consultant)) wrote:

> Thanks Benilton,
>
> I know what I want to do, just not sure how to do it using R.  The  
> help
> documentation is not very clear.
>
> What I am trying to do is calculate correlations on a row against row
> basis:  mat1 row1 x mat2 row1, mat1 row1 x mat2 row2, ... mat1 row1 x
> mat2 row-n, mat1 row-n, mat2 row-n
>
> - Bruce
>
> -----Original Message-----
> From: Benilton Carvalho [mailto:bcarvalh at jhsph.edu]
> Sent: Tuesday, July 24, 2007 11:31 AM
> To: Bernzweig, Bruce (Consultant)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] apply & incompatible dimensions error
>
> are you positive that your function is doing what you expect it to do?
>
> it looks like you want something like:
>
> sapply(1:10, function(i) cor(mat1[i,], mat2[i,]))
>
> b
>
> On Jul 24, 2007, at 11:05 AM, Bernzweig, Bruce ((Consultant)) wrote:
>
>> Hi,
>>
>> I've created the following two matrices (mat1 and mat2) and a  
>> function
>> (f) to calculate the correlations between the two on a row by row
>> basis.
>>
>> 	mat1 <- matrix(sample(1:500,50), ncol = 5,
>> 		dimnames=list(paste("row", 1:10, sep=""),
>> 		paste("col", 1:5, sep="")))
>>
>> 	mat2 <- matrix(sample(501:1000,50), ncol = 5,
>> 		dimnames=list(paste("row", 1:10, sep=""),
>> 		paste("col", 1:5, sep="")))
>>
>> 	f <- function(x,y) cor(x,y)
>>
>> When the matrices are squares (# rows = # columns) I have no  
>> problems.
>>
>> However, when they are not (as in the example above with 5 columns  
>> and
>> 10 rows), I get the following error:
>>
>>> apply(mat1, 1, f, y=mat2)
>> Error in cor(x, y, na.method, method == "kendall") :
>>         incompatible dimensions
>>
>> Any help would be appreciated.  Thanks!
>>
>> - Bruce
>>
>>
>>
>> ********************************************************************* 
>> *
>> Please be aware that, notwithstanding the fact that the pers...
>> {{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the person sending
> this communication has an address in Bear Stearns' e-mail system, this
> person is not an employee, agent or representative of Bear Stearns.
> Accordingly, this person has no power or authority to represent, make
> any recommendation, solicitation, offer or statements or disclose
> information on behalf of or in any way bind Bear Stearns or any of its
> affiliates.
> **********************************************************************


From ggrothendieck at gmail.com  Tue Jul 24 17:43:05 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 Jul 2007 11:43:05 -0400
Subject: [R] apply & incompatible dimensions error
In-Reply-To: <CADFD0E28E1E6A46B0C84335BDB994F504989B8E@whexchmb16.bsna.bsroot.bear.com>
References: <11740491.post@talk.nabble.com> <46A47D9F.4000309@biostat.ku.dk>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
	<971536df0707240830r14386f43j849596e4ddc23d42@mail.gmail.com>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8E@whexchmb16.bsna.bsroot.bear.com>
Message-ID: <971536df0707240843v6456c921raee7ab66f56f9a66@mail.gmail.com>

Then try this:

cor(t(mat1), t(mat2))

Also note

1. the above implies that mat1 and mat2 must have the same
number of columns since if x and y are vectors cor(x,y) only makes
sense if they have the same length.

2. the usual convention is that variables are stored as columns
andt that rows correspond to cases so typically you would have
(in terms of your mat1 and mat2):

Mat1 <- t(mat1)
Mat2 <- t(mat2)

and then use Mat1 and Mat2, e.g. cor(Mat1, Mat2)



On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> Thanks Gabor!
>
> You state that my apply is taking rows of mat1 with columns of mat2.
>
> Is this because I have the y=mat2 parameter?
>
> > apply(mat1, 1, f, y=mat2)
>
> Actually, what I would like is to run the correlations on a row against
> row basis:  mat1 row1 x mat2 row1, etc.
>
> Thanks again,
>
> - Bruce
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Tuesday, July 24, 2007 11:31 AM
> To: Bernzweig, Bruce (Consultant)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] apply & incompatible dimensions error
>
> Your apply is trying to take the correlations of the rows of mat1 with
> the
> columns of mat2 which, of course, does not work if they have different
> numbers of columns. I think you mean to take the correlations of the
> columns
> of mat1 with the columns of mat2.  For example, to take the correlations
> of the 5 columns of mat1 with the first 4 columns of mat2 try:
>
> > cor(mat1, mat2[,1:4])
>            col1       col2       col3       col4
> col1 -0.34624254 -0.2669519 -0.2705077  0.2183249
> col2 -0.26553255 -0.2687643 -0.0865895  0.1819025
> col3  0.19474613 -0.2334986  0.1746522  0.2326915
> col4  0.09328338  0.5117784  0.2413143 -0.3374916
> col5  0.27519716  0.1605331 -0.4057137  0.3282105
>
>
> On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> > Hi,
> >
> > I've created the following two matrices (mat1 and mat2) and a function
> > (f) to calculate the correlations between the two on a row by row
> basis.
> >
> >        mat1 <- matrix(sample(1:500,50), ncol = 5,
> >                dimnames=list(paste("row", 1:10, sep=""),
> >                paste("col", 1:5, sep="")))
> >
> >        mat2 <- matrix(sample(501:1000,50), ncol = 5,
> >                dimnames=list(paste("row", 1:10, sep=""),
> >                paste("col", 1:5, sep="")))
> >
> >        f <- function(x,y) cor(x,y)
> >
> > When the matrices are squares (# rows = # columns) I have no problems.
> >
> > However, when they are not (as in the example above with 5 columns and
> > 10 rows), I get the following error:
> >
> > > apply(mat1, 1, f, y=mat2)
> > Error in cor(x, y, na.method, method == "kendall") :
> >        incompatible dimensions
> >
> > Any help would be appreciated.  Thanks!
> >
> > - Bruce
> >
> >
> >
> > **********************************************************************
> > Please be aware that, notwithstanding the fact that the
> pers...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the person sending
> this communication has an address in Bear Stearns' e-mail system, this
> person is not an employee, agent or representative of Bear Stearns.
> Accordingly, this person has no power or authority to represent, make
> any recommendation, solicitation, offer or statements or disclose
> information on behalf of or in any way bind Bear Stearns or any of its
> affiliates.
> **********************************************************************
>


From gangchen at mail.nih.gov  Tue Jul 24 17:42:19 2007
From: gangchen at mail.nih.gov (Gang Chen)
Date: Tue, 24 Jul 2007 11:42:19 -0400
Subject: [R] Using lmer with huge amount of data
Message-ID: <C205088E-D754-4D66-BA07-65B73573DE6B@mail.nih.gov>

Based on the examples I've seen in using statistical analysis  
packages such as lmer, it seems that people usually tabulate all the  
input data into one file with the first line indicating the variable  
names (or labels), and then read the file inside R. However, in my  
case I can't do that because of the huge amount of imaging data.

Suppose I have a one-way within-subject ANCOVA with one covariate,  
and I would like to use lmer in R package lme4 to analyze the data.  
In the terminology of linear mixed models, I have a fixed factor A  
with 3 levels, a random factor B (subject), and a covariate (age)  
with a model like this

MyResult <- lmer(Response ~ FactorA + Age + (1 | subject), MyData, ...)

My input data are like this: For each subject I have a file (a huge  
matrix) storing the response values of the subject at many locations  
(~30,000 voxels) corresponding to factor A at the 1st level, another  
file for factor A at the 2nd level, and a 3rd file for factor A at  
the 3rd level. Then I have another file storing the age of those  
subjects. The analysis with the linear mixed model above would be  
done at each voxel separately.

It seems impractical to create one gigantic file or matrix to feed  
into the above command line because of the big number of voxels. I'm  
not sure how to proceed in this case. Any suggestions would be highly  
appreciated.

Also if I'm concerned about any potential violation of sphericity  
among the 3 levels of factor A, how can I test sphericity violation  
in lmer? And if violation exists, how can I make corrections in  
contrast testing?

Thank you very much,
Gang


From ggrothendieck at gmail.com  Tue Jul 24 17:47:56 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 Jul 2007 11:47:56 -0400
Subject: [R] cor inside/outside a function has different output
In-Reply-To: <CADFD0E28E1E6A46B0C84335BDB994F504989B8C@whexchmb16.bsna.bsroot.bear.com>
References: <CADFD0E28E1E6A46B0C84335BDB994F504989B8C@whexchmb16.bsna.bsroot.bear.com>
Message-ID: <971536df0707240847v4a9e8f0ag83845bf96694a4c1@mail.gmail.com>

I think this is really answered already in my previous post but just in case
try this:

> res1 <- t(apply(mat1, 1, cor, t(mat2)))
> res2 <- cor(t(mat1), t(mat2))
> all.equal(res1, res2, check.attributes = FALSE)
[1] TRUE


On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> I'm calculating correlations between two matrices
>
>
>
> mat1 <- matrix(sample(1:500,25), ncol = 5,
>
> dimnames=list(paste("mat1row", 1:5, sep=""),
>
> paste("mat1col", 1:5, sep="")))
>
>
>
> mat2 <- matrix(sample(501:1000,25), ncol = 5,
>
> dimnames=list(paste("mat2row", 1:5, sep=""),
>
> paste("mat2col", 1:5, sep="")))
>
>
>
> using what would seem to be two similar methods:
>
>
>
>  Method 1:
>
>
>
>       > f <- function(x,y) cor(x,y)
>
>       > apply(mat1, 1, f, y=mat2)
>
>
>
>  Method 2:
>
>
>
>        > cor(mat1, mat2)
>
>
>
> However, the results (see blow) are different:
>
>
>
> > apply(mat1, 1, f, y=mat2)
>
>
>
>               mat1row1   mat1row2    mat1row3    mat1row4    mat1row5
>
> [1,] -0.27601028 -0.1352143  0.03538690 -0.03084075 -0.60171704
>
> [2,] -0.01595532 -0.3881197 -0.43663982  0.49081806  0.33291995
>
> [3,]  0.35969624 -0.0582948  0.57462169  0.09926796 -0.02948423
>
> [4,] -0.41435920 -0.7164638 -0.21213496 -0.55183934 -0.25341790
>
> [5,]  0.33802803  0.5371508  0.05219095  0.83533575  0.17850291
>
>
>
> > cor(mat1, mat2)
>
>            mat2col1    mat2col2   mat2col3   mat2col4   mat2col5
>
> mat1col1 -0.84077496 -0.01538414 -0.6078933 -0.2263840 -0.1421335
>
> mat1col2  0.23074421  0.54606286 -0.2354733  0.5214255 -0.2129077
>
> mat1col3 -0.80000528  0.19550100 -0.5920509 -0.8694040  0.6853990
>
> mat1col4  0.08050976 -0.55449840  0.6225666  0.6187971 -0.8971584
>
> mat1col5 -0.10199564 -0.43854767 -0.5803077 -0.5100285  0.2848351
>
>
>
> Also, for method 2, the calculations are done on a column x column
> basis.  Is there any way to do this on a row by row basis.  Looking at
> the help page for cor, I don't see any parameters that could be used to
> do this.
>
>
>
> Thanks,
>
>
>
> - Bruce
>
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the pers...{{dropped}}
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From bbernzwe at bear.com  Tue Jul 24 17:47:06 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Tue, 24 Jul 2007 11:47:06 -0400
Subject: [R] apply & incompatible dimensions error
References: <11740491.post@talk.nabble.com> <46A47D9F.4000309@biostat.ku.dk>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8B@whexchmb16.bsna.bsroot.bear.com>
	<971536df0707240830r14386f43j849596e4ddc23d42@mail.gmail.com>
	<CADFD0E28E1E6A46B0C84335BDB994F504989B8E@whexchmb16.bsna.bsroot.bear.com>
	<971536df0707240843v6456c921raee7ab66f56f9a66@mail.gmail.com>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B94@whexchmb16.bsna.bsroot.bear.com>

Thanks for the explanation.

As for the rows/columns thing, the data I receive is given to me in that
way.  I currently read it in using read.csv.  Is there a function I
should look at that can take that and transpose it or should I just
process the data first outside of R?

Thanks,

- Bruce

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Tuesday, July 24, 2007 11:43 AM
To: Bernzweig, Bruce (Consultant)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] apply & incompatible dimensions error

Then try this:

cor(t(mat1), t(mat2))

Also note

1. the above implies that mat1 and mat2 must have the same
number of columns since if x and y are vectors cor(x,y) only makes
sense if they have the same length.

2. the usual convention is that variables are stored as columns
andt that rows correspond to cases so typically you would have
(in terms of your mat1 and mat2):

Mat1 <- t(mat1)
Mat2 <- t(mat2)

and then use Mat1 and Mat2, e.g. cor(Mat1, Mat2)



On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> Thanks Gabor!
>
> You state that my apply is taking rows of mat1 with columns of mat2.
>
> Is this because I have the y=mat2 parameter?
>
> > apply(mat1, 1, f, y=mat2)
>
> Actually, what I would like is to run the correlations on a row
against
> row basis:  mat1 row1 x mat2 row1, etc.
>
> Thanks again,
>
> - Bruce
>
>
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Tuesday, July 24, 2007 11:31 AM
> To: Bernzweig, Bruce (Consultant)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] apply & incompatible dimensions error
>
> Your apply is trying to take the correlations of the rows of mat1 with
> the
> columns of mat2 which, of course, does not work if they have different
> numbers of columns. I think you mean to take the correlations of the
> columns
> of mat1 with the columns of mat2.  For example, to take the
correlations
> of the 5 columns of mat1 with the first 4 columns of mat2 try:
>
> > cor(mat1, mat2[,1:4])
>            col1       col2       col3       col4
> col1 -0.34624254 -0.2669519 -0.2705077  0.2183249
> col2 -0.26553255 -0.2687643 -0.0865895  0.1819025
> col3  0.19474613 -0.2334986  0.1746522  0.2326915
> col4  0.09328338  0.5117784  0.2413143 -0.3374916
> col5  0.27519716  0.1605331 -0.4057137  0.3282105
>
>
> On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> > Hi,
> >
> > I've created the following two matrices (mat1 and mat2) and a
function
> > (f) to calculate the correlations between the two on a row by row
> basis.
> >
> >        mat1 <- matrix(sample(1:500,50), ncol = 5,
> >                dimnames=list(paste("row", 1:10, sep=""),
> >                paste("col", 1:5, sep="")))
> >
> >        mat2 <- matrix(sample(501:1000,50), ncol = 5,
> >                dimnames=list(paste("row", 1:10, sep=""),
> >                paste("col", 1:5, sep="")))
> >
> >        f <- function(x,y) cor(x,y)
> >
> > When the matrices are squares (# rows = # columns) I have no
problems.
> >
> > However, when they are not (as in the example above with 5 columns
and
> > 10 rows), I get the following error:
> >
> > > apply(mat1, 1, f, y=mat2)
> > Error in cor(x, y, na.method, method == "kendall") :
> >        incompatible dimensions
> >
> > Any help would be appreciated.  Thanks!
> >
> > - Bruce
> >
> >
> >
> >
**********************************************************************
> > Please be aware that, notwithstanding the fact that the
> pers...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the person sending
> this communication has an address in Bear Stearns' e-mail system, this
> person is not an employee, agent or representative of Bear Stearns.
> Accordingly, this person has no power or authority to represent, make
> any recommendation, solicitation, offer or statements or disclose
> information on behalf of or in any way bind Bear Stearns or any of its
> affiliates.
> **********************************************************************
>



**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From bbernzwe at bear.com  Tue Jul 24 17:49:17 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Tue, 24 Jul 2007 11:49:17 -0400
Subject: [R] cor inside/outside a function has different output
References: <CADFD0E28E1E6A46B0C84335BDB994F504989B8C@whexchmb16.bsna.bsroot.bear.com>
	<971536df0707240847v4a9e8f0ag83845bf96694a4c1@mail.gmail.com>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B96@whexchmb16.bsna.bsroot.bear.com>

Sorry.  I looked up t after writing the previous email and realized that
was what I was looking for!



-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Tuesday, July 24, 2007 11:48 AM
To: Bernzweig, Bruce (Consultant)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] cor inside/outside a function has different output

I think this is really answered already in my previous post but just in
case
try this:

> res1 <- t(apply(mat1, 1, cor, t(mat2)))
> res2 <- cor(t(mat1), t(mat2))
> all.equal(res1, res2, check.attributes = FALSE)
[1] TRUE


On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> I'm calculating correlations between two matrices
>
>
>
> mat1 <- matrix(sample(1:500,25), ncol = 5,
>
> dimnames=list(paste("mat1row", 1:5, sep=""),
>
> paste("mat1col", 1:5, sep="")))
>
>
>
> mat2 <- matrix(sample(501:1000,25), ncol = 5,
>
> dimnames=list(paste("mat2row", 1:5, sep=""),
>
> paste("mat2col", 1:5, sep="")))
>
>
>
> using what would seem to be two similar methods:
>
>
>
>  Method 1:
>
>
>
>       > f <- function(x,y) cor(x,y)
>
>       > apply(mat1, 1, f, y=mat2)
>
>
>
>  Method 2:
>
>
>
>        > cor(mat1, mat2)
>
>
>
> However, the results (see blow) are different:
>
>
>
> > apply(mat1, 1, f, y=mat2)
>
>
>
>               mat1row1   mat1row2    mat1row3    mat1row4    mat1row5
>
> [1,] -0.27601028 -0.1352143  0.03538690 -0.03084075 -0.60171704
>
> [2,] -0.01595532 -0.3881197 -0.43663982  0.49081806  0.33291995
>
> [3,]  0.35969624 -0.0582948  0.57462169  0.09926796 -0.02948423
>
> [4,] -0.41435920 -0.7164638 -0.21213496 -0.55183934 -0.25341790
>
> [5,]  0.33802803  0.5371508  0.05219095  0.83533575  0.17850291
>
>
>
> > cor(mat1, mat2)
>
>            mat2col1    mat2col2   mat2col3   mat2col4   mat2col5
>
> mat1col1 -0.84077496 -0.01538414 -0.6078933 -0.2263840 -0.1421335
>
> mat1col2  0.23074421  0.54606286 -0.2354733  0.5214255 -0.2129077
>
> mat1col3 -0.80000528  0.19550100 -0.5920509 -0.8694040  0.6853990
>
> mat1col4  0.08050976 -0.55449840  0.6225666  0.6187971 -0.8971584
>
> mat1col5 -0.10199564 -0.43854767 -0.5803077 -0.5100285  0.2848351
>
>
>
> Also, for method 2, the calculations are done on a column x column
> basis.  Is there any way to do this on a row by row basis.  Looking at
> the help page for cor, I don't see any parameters that could be used
to
> do this.
>
>
>
> Thanks,
>
>
>
> - Bruce
>
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the
pers...{{dropped}}
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>



**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From dieter.menne at menne-biomed.de  Tue Jul 24 17:59:29 2007
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Tue, 24 Jul 2007 15:59:29 +0000 (UTC)
Subject: [R] Fitting exponential curve to data points
References: <443111.31257.qm@web39701.mail.mud.yahoo.com>
	<e507996c0707240620ycfc596fu14f3d76709ce32ea@mail.gmail.com>
Message-ID: <loom.20070724T175645-930@post.gmane.org>

Andrew Clegg <andrew.clegg <at> gmail.com> writes:

> 
> ... If I want to demonstrate that a non-linear curve fits
> better than an exponential, what's the best measure for that? Given
> that neither of nls() or optim() provide R-squared. 

To supplement Karl's comment, try Douglas Bates' (author of nls) comments on the
matter

http://www.ens.gu.edu.au/ROBERTK/R/HELP/00B/0399.HTML

Short summary:
    * ... "the lack of automatic ANOVA, R^2 and adj. R^2 from nls is a feature,
not a bug :-)"
    * My best advice regarding R^2 statistics with nonlinear models is, as Nancy
Reagan suggested, "Just say no."

Dieter


From jrg66 at comcast.net  Tue Jul 24 18:07:31 2007
From: jrg66 at comcast.net (Jeff G.)
Date: Tue, 24 Jul 2007 12:07:31 -0400
Subject: [R] plotting a summary.rq object in using pkg quantreg
In-Reply-To: <714260.77017.qm@web39707.mail.mud.yahoo.com>
References: <714260.77017.qm@web39707.mail.mud.yahoo.com>
Message-ID: <46A623C3.7090200@comcast.net>

Hello,

I am having problems adjusting the plot output from the quantreg 
package.  Anyone know what I'm doing wrong?

For example (borrowing from the help files):

plot(summary(rq(foodexp~income,tau = 1:49/50,data=engel)), nrow=1, 
ncol=2,alpha = .4, ols = TRUE, xlab="test")

The "alpha=" parameter seems to have no effect on my output, even when I 
set it to a ridiculous value like 0.4.  Also, though in the help file it 
says |"...| = optional arguments to plot", "xlab" (as an example) seems 
to do nothing.  If the answer is that I should extract the values I need 
and construct the plot I want independently of the rq.process object, 
that it okay I suppose, if inefficient.  Maybe a more fundamental 
question is how do I get in and see how plot is working in this case so 
that I can modify.

Thanks much!

J

P.S.  I've explored using plot.summary.rqs but the problems seem to be 
the same.


From lobry at biomserv.univ-lyon1.fr  Tue Jul 24 18:39:04 2007
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Tue, 24 Jul 2007 18:39:04 +0200
Subject: [R] crimtab related question
Message-ID: <p0600203ac2cbd27de33c@[192.168.1.10]>

Dear all,

the dataset documented under ?crimtab was also used in:

@article{TreloarAE1934,
     title = {The adequacy of "{S}tudent's" criterion of
              deviations in small sample means},
     author = {Treloar, A.E. and Wilder, M.A.},
     journal = {The Annals of Mathematical Statistics},
     volume = {5},
     pages = {324-341},
     year = {1934}
}

The following is from page 335 of the above paper:

"From the table provided by MacDonell (1902) on
the associated variation of stature (to the nearest inch)
and length of the left middle finger (to the nearest
millimeter) in 3000 British criminals, the measusurements
were transferred to 3000 numbered Denison metal-rim
tags from which the cords has been removed. After
thorough checking and mixing of these circular disks,
samples of 5 tags each were drawn at random until the
supply was exhausted. Unfortunately, three of these
samples were erroneously returned to a receiving box
before being copied, and the records of 597 samples only
are available."

Could someone give me a clue about the kind of device
that was used here? Is it a kind of lottery machine?
I don't understand why three samples were lost. What
is this "receiving box"?

Thanks for any hint,

Best,
-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/


From kamauallan at yahoo.com  Tue Jul 24 18:29:24 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Tue, 24 Jul 2007 09:29:24 -0700 (PDT)
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
Message-ID: <494197.22767.qm@web53502.mail.re2.yahoo.com>

Hi all,
If the question below as been answered before I
apologize for the posting.
I would like to get the frequencies of occurrence of
all values in a given variable in a multivariate
dataset. In short for each variable (or field) a
summary of values contained with in a value:frequency
pair, there can be many such pairs for a given
variable. I would like to do the same for several such
variables.
I have used table() but am unable to extract the
individual value and frequency values.
Please advise.

Allan.


From uhtrivedi208 at yahoo.co.in  Tue Jul 24 19:16:42 2007
From: uhtrivedi208 at yahoo.co.in (Urmi Trivedi)
Date: Tue, 24 Jul 2007 18:16:42 +0100 (BST)
Subject: [R] Fitting the best line to the plot of distance vs. correlation
	matrix
Message-ID: <347770.14135.qm@web8506.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070724/08632f4f/attachment.pl 

From HStevens at MUOhio.edu  Tue Jul 24 19:22:26 2007
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Tue, 24 Jul 2007 13:22:26 -0400
Subject: [R] lme or gls prediction intervals
Message-ID: <C5B40E3A-73C3-4663-9497-7A9E344D0A26@MUOhio.edu>

Hi folks,
I am trying to generate 95% confidence intervals for a gls model  
using predict.nlme
with
R version 2.5.1 (2007-06-27)
. nlme: Linear  and Nonlinear Mixed Effects Models. R package version
   3.1-83.

I have looked in help, and I can do it for lm and glm models, and I  
can generate simple predictions for lme models with various levels --  
I am familiar with the basics.

Is there a way to get prediction intervals for gls models? My "best"  
model uses varPower(), so I am reluctant to fall back on lm predictions.

Thank you,

Hank


Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"


From jo.irisson at gmail.com  Tue Jul 24 19:23:33 2007
From: jo.irisson at gmail.com (jiho)
Date: Tue, 24 Jul 2007 19:23:33 +0200
Subject: [R] x,y,z table to matrix with x as rows and y as columns
Message-ID: <EBE7A093-2159-4E4E-87E8-D2D4FE0FB181@gmail.com>

Hello all,

I am sure I am missing something obvious but I cannot find the  
function I am looking for. I have a data frame with three columns: X,  
Y and Z, with X and Y being grid coordinates and Z the value  
associated with these coordinates. I want to transform this data  
frame in a matrix of Z values, on the grid defined by X and Y (and,  
as a plus, fill the X.Y combinations which do no exist in the  
original data frame with NAs in the resulting matrix). I could do  
this manually but I guess the appropriate function should be  
somewhere around. I just can't find it.

Thank you in advance for your help.

JiHO
---
http://jo.irisson.free.fr/


From bbernzwe at bear.com  Tue Jul 24 19:37:41 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Tue, 24 Jul 2007 13:37:41 -0400
Subject: [R] Calculating subsets of row pairs using somthing faster than a
	for loop.
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989B9D@whexchmb16.bsna.bsroot.bear.com>

Hi all,

 

Situation:

 

 - I have two matrices each w/ 4 rows and 20 columns.

 

mat1 <- matrix(sample(1:500,80), ncol = 20, 

            dimnames=list(paste("mat1row", 1:4, sep=""), 

            paste("mat1col", 1:20, sep="")))

 

mat2 <- matrix(sample(501:1000,80), ncol = 20, 

            dimnames=list(paste("mat2row", 1:4, sep=""), 

            paste("mat2col", 1:20, sep="")))

 

 - Each column represents a value in a time series.

 

Q: What do I want:

 

   Calculate moving average correlations for each row x row pair:

 

   For each row x row pair I want 10 values representing moving average

   correlations for 10 sets of time-values:

 

   cor(mat1[1,1:10], mat2[1,1:10])

   cor(mat1[1,2:11], mat2[1,2:11])

   ...

   cor(mat1[1,11:20], mat2[1,11:20])

   cor(mat1[1,1:10], mat2[2,1:10])

   ...

   cor(mat1[4,11:20], mat2[4,11:20])

 

   Result would be a 16 (rows) x 10 (col) matrix matMA

 

      ma1, ma2, ..., ma10 for (mat1 row1) x (mat2 row1)

      ma1, ma2, ..., ma10 for (mat1 row1) x (mat2 row2)

      ...

      ma1, ma2, ..., ma10 for (mat1 row4) x (mat2 row3)

      ma1, ma2, ..., ma10 for (mat1 row4) x (mat2 row4) 

 

   I would like to be able to do this without using a for loop

   due to the slowness of that method.

 

   Is it possible to iterate through subsets w/o using a for loop?

 

Thanks,

 

- Bruce

 

      P

-------------- next part --------------


**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From maechler at stat.math.ethz.ch  Tue Jul 24 19:43:14 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 24 Jul 2007 19:43:14 +0200
Subject: [R] Computing the rank of a matrix.
In-Reply-To: <f4eabc685f8d.4617e568@johnshopkins.edu>
References: <4616316B.6020403@decsai.ugr.es>
	<OF64AC873D.BB57D00C-ON862572B5.0052ED54-862572B5.00530C61@mmm.com>
	<000801c7786a$dad09e00$7c94100a@win.ad.jhu.edu>
	<17943.45399.832117.683546@stat.math.ethz.ch>
	<f4eabc685f8d.4617e568@johnshopkins.edu>
Message-ID: <18086.14898.656256.84710@stat.math.ethz.ch>

>>>>> "RV" == RAVI VARADHAN <rvaradhan at jhmi.edu>
>>>>>     on Sat, 07 Apr 2007 18:39:36 -0400 writes:
	     ^^^^^^^^^^^^^^^^

this is a bit a late reply...  better late than never

    RV> Hi Martin,

Hi Ravi, thanks for your research.


    RV> I played a bit with rankMat.  I will first state the
    RV> conclusions of my numerical experiments and then present
    RV> the results to support them:

    RV> 1.  I don't believe that rankMat, or equivalently
    RV> Matlab's rank computation, is necessarily better than
    RV> qr(.)$rank or (qr., LAPACK=TRUE)$rank.  In fact, for the
    RV> notorious Hilbert matrix, rankMat can give poor
    RV> estimates of rank.

    RV> 2.  There exists no universally optimal (i.e. optimal
    RV> for all A) tol in qr(A, tol)$rank that would be
    RV> optimally close to rankMat.  The discrepancy in the
    RV> ranks computed by qr(A)$rank and rankMat(A) obviously
    RV> depends on the matrix A. This is evident from the tol
    RV> used in rankMat:
    RV>      tol <- max(d) * .Machine$double.eps * abs(singValA[1])
    RV> So, this value of tol in qr will also minimize the discrepancy. 

    RV> 3.  The tol parameter is irrelevant in qr(A, tol,
    RV> LAPACK=TRUE)$rank, i.e. LAPACK doesn't seem to utilize
    RV> the tol parameter when computing the rank.  However,
    RV> qr(A, tol, LAPACK=FALSE)$rank does depend on tol.

Yes, indeed!  The help page of qr() actually says so 
{at least now, don't know about 3 months ago}.

    RV> Now, here are the results:
    RV> 1.
    >> hilbert.rank <- matrix(NA,20,3)
    >> hilbert <- function(n) { i <- 1:n; 1 / outer(i - 1, i, "+") }
    >> for (i in 1:20) {
    RV> + himat <- hilbert(i)
    RV> + hilbert.rank[i,1] <- rankMat(himat)
    RV> + hilbert.rank[i,2] <- qr(himat,tol=1.0e-14)$rank
    RV> + hilbert.rank[i,3] <- qr(himat, tol = .Machine$double.eps, LAPACK = TRUE)$rank
    RV> + }
    >> hilbert.rank
    RV> [,1] [,2] [,3]
    RV> [1,]    1    1    1
    RV> [2,]    2    2    2
    RV> [3,]    3    3    3
    RV> [4,]    4    4    4
    RV> [5,]    5    5    5
    RV> [6,]    6    6    6
    RV> [7,]    7    7    7
    RV> [8,]    8    8    8
    RV> [9,]    9    9    9
    RV> [10,]   10   10   10
    RV> [11,]   10   11   11
    RV> [12,]   11   12   12
    RV> [13,]   11   12   13
    RV> [14,]   11   13   14
    RV> [15,]   12   13   15
    RV> [16,]   12   15   16
    RV> [17,]   12   16   17
    RV> [18,]   12   16   18
    RV> [19,]   13   17   19
    RV> [20,]   13   17   20

    RV> We see that rankMat underestimates the rank for n > 10, but qr(himat, tol = .Machine$double.eps, LAPACK=TRUE)$rank gets it right.

Yes, indeed;  and that's  seems a bit  against the  ``common
knowledge'' that svd() is more reliable than qr()

Hmm.... I'm still baffled a bit..

Note that with the Hilbert matrices, 
one might argue that  hilbert(20) might really not have a
correct "estimated rank" of 20,
but at least for hilbert(13) or so, the rank should be == n.

BTW, there's a nice plot, slightly related to this problem,
using rcond() from the Matrix package :

  library(Matrix)

  hilbert <- function(n) { i <- 1:n; 1 / outer(i - 1, i, "+") }
  rcHilb <- sapply(1:20, function(n) rcond(Matrix(hilbert(n))))

  plot(rcHilb, xlab = "n", log = "y", col = 2, type = "b",
       main = "reciprocal condition numbers of Hilbert(n)")

where one sees that the LAPACK code that underlies
Matrix::rcond() also gets into "problem" when estimating the
condition number for hilbert(n) when n >~ 16 .

I think if we wanted to make real progress here, we'd have to
consult with numerical analyist specialists.
But for me the topic is too remote to be followed up further at
the moment.

One conclusion for me is that to estimate the rank of a matrix
in current versions of R, one should use

  rankMat <- function(x)  qr(x, LAPACK = TRUE)$rank

(as was suggested as one possibility in the original thread)

Regards, and thank you again, Ravi.

Martin Maechler, ETH Zurich


    RV> 2.  
    RV> Here I first, created a random rectangular matrix, and then added a number of rows to it, where these new rows are the same as some of the original rows except for a tiny amount of noise, which I call eps.  So, the degree of rank deficiency is controlled by eps.  I let eps take on 3 values: 1.e-07, 1.e-10, and 1.e-14, and show that the optimal tol (in terms of being close to rankMat) depends on the level of precision (eps) in the matrix.
    >> set.seed(123)
    >> nrow <- 15
    >> ncol <- 20
    >> nsim <- 5000
    >> ndefic <- 4  # number of "nearly" dependent rows
    >> eps <- 1.e-07
    >> rnk <- matrix(NA, nsim, 5)
    >> for (j in 1:nsim) {
    RV> + A <- matrix(rnorm(nrow*ncol),nrow, ncol)
    RV> + newrows <- matrix(NA, ndefic, ncol)
    RV> + for (i in 1:ndefic) {
    RV> + newrows[i,] <- A[nrow-i,] + rnorm(ncol, sd=min(abs(A[nrow-i+1,]))* eps)
    RV> + }
    RV> + 
    RV> + B <- rbind(A,newrows)
    RV> + rnk[j,1] <- rankMat(B)
    RV> + rnk[j,2] <- qr(B, tol = 1.e-07)$rank
    RV> + rnk[j,3] <- qr(B, tol = 1.e-11)$rank
    RV> + rnk[j,4] <- qr(B, tol = 1.0e-14)$rank
    RV> + rnk[j,5] <- qr(B, tol = .Machine$double.eps, LAPACK = TRUE)$rank
    RV> + }
    >> apply(abs(rnk[,1] - rnk[,2:5]),2,sum)
    RV> [1] 19351    53     0     0

    RV> We observe that both qr(B, tol=1.e-14)$rank and qr(B, tol = .Machine$double.eps, LAPACK = TRUE)$rank give exactly the same rank as rankMat.

    RV> Now, we change eps <- 1.e-10 and the results are:
    >> apply(abs(rnk[,1] - rnk[,2:5]),2,sum)
    RV> [1] 19778 14263   166   220
    RV> This means that a tolerance of 1.e-14 works best.

    RV> Now we lower eps further: eps <- 1.e-14
    >> apply(abs(rnk[,1] - rnk[,2:5]),2,sum)
    RV> [1]     0     3   665 20000
    RV> Clearly, the smaller tolerances yield rank estimates that are higher than that given by rankMat.  That is, rankMat underestimates the rank as in the case of Hilbert matrix.

    RV> 3.
    RV> Now we show that qr(., tol, LAPACK=TRUE)$rank is independent of tol:
    >> exp <- -(7:16)
    >> tol <- 10^exp
    >> sapply(tol, A=hilbert(20), function(x,A)qr(A, tol=x, LAPACK=FALSE)$rank)
    RV> [1] 10 12 14 14 15 16 16 17 18 19
    >> sapply(tol, A=hilbert(20), function(x,A)qr(A, tol=x, LAPACK=TRUE)$rank)
    RV> [1] 20 20 20 20 20 20 20 20 20 20

    RV> Looking forward to comments.

    RV> Best,
    RV> Ravi. 

    RV> ----- Original Message -----
    RV> From: Martin Maechler <maechler at stat.math.ethz.ch>
    RV> Date: Saturday, April 7, 2007 10:57 am
    RV> Subject: Re: [R] Computing the rank of a matrix.
    RV> To: Ravi Varadhan <rvaradhan at jhmi.edu>
    RV> Cc: apjaworski at mmm.com, "'Jos? Luis Aznarte M.'" <jlaznarte at decsai.ugr.es>, r-help at stat.math.ethz.ch


    >> >>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
    >> >>>>>     on Fri, 6 Apr 2007 12:44:33 -0400 writes:
    >> 
    Ravi> Hi, qr(A)$rank will work, but just be wary of the
    Ravi> tolerance parameter (default is 1.e-07), since the
    Ravi> rank computation could be sensitive to the tolerance
    Ravi> chosen.
    >> 
    >> Yes, indeed.
    >> 
    >> The point is that   rank(<Matrix>) 
    >> is well defined in pure math (linear algebra), as well as 
    >> a "singular matrix" is.
    >> 
    >> The same typically no longer makes sense as soon as you enter
    >> the real world: A matrix "close to singular" may have to be
    >> treated "as if singular" depending on its "singularity
    >> closeness" {{ learn about the condition number of a matrix }}
    >> and the same issues arise with rank(<matrix>).
    >> 
    >> Of course, the matlab programmers know all this (and much more),
    >> and indeed, matlab's  rank(A) really is  
    >> rank(A, tol = tol.default(A))
    >> 
    >> and is based on the SVD instead of QR decomposition since the
    >> former is said to be more reliable (even though slightly slower).
    >> 
    >> R's equivalent (with quite a bit of fool-proofing) would be the
    >> following function (assuming correct online documentation of matlab):
    >> 
    >> 
    >> rankMat <- function(A, tol = NULL, singValA = svd(A, 0,0)$d)
    >> {
    >> ## Purpose: rank of a matrix ``as Matlab''
    >> ## ----------------------------------------------------------------------
    >> ## Arguments: A: a numerical matrix, maybe non-square
    >> ##        	tol: numerical tolerance (compared to singular values)
    >> ##           singValA: vector of non-increasing singular values 
    >> of A
    >> ##                     (pass as argument if already known)
    >> ## ----------------------------------------------------------------------
    >> ## Author: Martin Maechler, Date:  7 Apr 2007, 16:16
    >> d <- dim(A)
    >> stopifnot(length(d) == 2, length(singValA) == min(d),
    >> diff(singValA) < 0)       # must be sorted decreasingly
    >> if(is.null(tol))
    >> tol <- max(d) * .Machine$double.eps * abs(singValA[1])
    >> else stopifnot(is.numeric(tol), tol >= 0)
    >> sum(singValA >= tol)
    >> }
    >> 
    >> 
    >> A small scale simulation with random matrices,
    >> i.e., things like
    >> 
    >> ## ranks of random matrices; here will have 5 all the time:
    >> table(replicate(1000, rankMat(matrix(rnorm(5*12),5, 12) )))# < 1 sec.
    >> 
    >> indicates that qr(.)$rank  gives the same typically,
    >> where I assume one should really use
    >> 
    >> qr(., tol = .Machine$double.eps, LAPACK = TRUE)$rank
    >> 
    >> to be closer to Matlab's default tolerance.
    >> 
    >> Ok, who has time to investigate further? 
    >> Research exercise:
    >> 
    1> Is there a fixed number, say  t0 <- 1e-15 
    1> for which  qr(A, tol = t0, LAPACK=TRUE)$rank is 
    1> ``optimally close'' to rankMat(A) ?
    >> 
    2> how easily do you get cases showing svd(.) to more reliable
    2> than qr(., LAPACK=TRUE)?
    >> 
    >> To solve this in an interesting way, you should probably
    >> investigate classes of "almost rank-deficient" matrices,
    >> and I'd also be interested if you "randomly" ever get matrices A
    >> with  rank(A) <  min(dim(A)) - 1
    >> (unless you construct some columns/rows exactly from earlier
    >> ones or use all-0 ones)
    >> 
    >> Martin Maechler, ETH Zurich
    >> 
    >> 
    >> 
    >> 
    Ravi> Ravi.
    >> 
    Ravi> -----------------------------------------------------------------
    Ravi> Ravi Varadhan, Ph.D.
    Ravi> Assistant Professor, The Center on Aging and Health
    >> .......
    Ravi> 
    >> 
    >> 
    Ravi> --------------------------------------------------------------------
    >> 
    >> 
    >> 
    >> >> How about
    >> 
    >> >>       qr(A)$rank
    >> 
    >> >> or perhaps
    >> 
    >> >>       qr(A, LAPACK=TRUE)$rank
    >> 
    >> >> Cheers,
    >> 
    >> >> Andy
    >> 
    >> 
    >> >>     Hi! Maybe this is a silly question, but I need the
    >> >> column rank (
    >> >> of a matrix and R function 'rank()' only gives me the
    >> >> ordering of the elements of my matrix.  How can I
    >> >> compute the column rank of a matrix? Is there not an R
    >> >> equivalent to Matlab's 'rank()'?  I've been browsing
    >> >> for a time now and I can't find anything, so any help
    >> >> will be greatly appreciated. Best regards!
    >> 
    >> 
    >> >> -- -- Jose Luis Aznarte M.
    >> >>  Department of Computer
    >> >> Science and Artificial Intelligence Universidad de
    >> >> Granada Tel. +34 - 958 - 24 04 67 GRANADA (Spain) Fax:
    >> >> +34 - 958 - 24 00 79


From jrg66 at comcast.net  Tue Jul 24 19:57:23 2007
From: jrg66 at comcast.net (Jeff G.)
Date: Tue, 24 Jul 2007 13:57:23 -0400
Subject: [R] quantreg behavior changes for N>1000
In-Reply-To: <D6B56BC9-5567-484A-A9CF-8BA71843342A@jhsph.edu>
References: <072220071939.21999.46A3B26E00098C96000055EF2216566276C0C0099D06@comcast.net>
	<D6B56BC9-5567-484A-A9CF-8BA71843342A@jhsph.edu>
Message-ID: <46A63D83.3070405@comcast.net>

Hello again R-experts and novices (like me),

This seems like a bug to me - or maybe it's intentional...can anyone 
confirm?  Up to 1000 reps, summary() of a rq object gives different 
output and subtly different confidence interval estimates.

Thanks....Jeff

testx=runif(1200)
testy=rnorm(1200, 5)

test.rq=summary(rq(testy[1:1000]~testx[1:1000], tau=2:98/100))
test.rq[[1]]
Gives this output:
Call: rq(formula = testy[1:1000] ~ testx[1:1000], tau = 2:98/100)

    tau: [1] 0.02

    Coefficients:
                  coefficients       lower bd   upper bd
    (Intercept)    3.00026         2.45142     3.17098
    testx[1:1000] -0.00870     -0.39817  0.49946

test.rq=summary(rq(testy[1:1001]~testx[1:1001], tau=2:98/100))
test.rq[[1]]

Gives this (different) output:
    Call: rq(formula = testy[1:1001] ~ testx[1:1001], tau = 2:98/100)

    tau: [1] 0.02

    Coefficients:
                  Value    Std. Error t value  Pr(>|t|)
   (Intercept)    3.00026  0.21605   13.88658  0.00000
    testx[1:1001] -0.00870  0.32976   -0.02638  0.97896


plot(test.rq, nrow=2, ncol=2) # The slope estimates appear to be the 
same but there are subtle differences in the confidence intervals, which 
shouldn't be due simply to the inclusion of one more point.


From roger at ysidro.econ.uiuc.edu  Tue Jul 24 20:00:00 2007
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Tue, 24 Jul 2007 13:00:00 -0500
Subject: [R] plotting a summary.rq object in using pkg quantreg
In-Reply-To: <46A623C3.7090200@comcast.net>
References: <714260.77017.qm@web39707.mail.mud.yahoo.com>
	<46A623C3.7090200@comcast.net>
Message-ID: <FFF25D96-D87F-45FE-91E8-62D700C8C7DC@ysidro.econ.uiuc.edu>

Package questions to package maintainers....  please.

The short answer is that your alpha = .4 parameter needs to
be passed to  summary not to plot.  Try this:

> plot(summary(rq(foodexp~income,tau = 1:49/50,data=engel),alpha =. 
> 4), nrow=1,
> ncol=2, ols = TRUE)

A longer answer would involve a boring disquisition about various  
fitting methods
and standard error estimation methods and their historical evolution  
and defaults.
(By default rank-based confidence bands are being used for the engel  
data since
the sample size is relatively small.)

Regarding your more fundamental question:  you can always modify   
functions
such as summary.rq or plot.summary.rqs  -- see for example ?fix.




url:    www.econ.uiuc.edu/~roger                Roger Koenker
email   rkoenker at uiuc.edu                       Department of Economics
vox:    217-333-4558                            University of Illinois
fax:    217-244-6678                            Champaign, IL 61820


On Jul 24, 2007, at 11:07 AM, Jeff G. wrote:

> Hello,
>
> I am having problems adjusting the plot output from the quantreg
> package.  Anyone know what I'm doing wrong?
>
> For example (borrowing from the help files):
>
> plot(summary(rq(foodexp~income,tau = 1:49/50,data=engel)), nrow=1,
> ncol=2,alpha = .4, ols = TRUE, xlab="test")
>
> The "alpha=" parameter seems to have no effect on my output, even  
> when I
> set it to a ridiculous value like 0.4.  Also, though in the help  
> file it
> says |"...| = optional arguments to plot", "xlab" (as an example)  
> seems
> to do nothing.  If the answer is that I should extract the values I  
> need
> and construct the plot I want independently of the rq.process object,
> that it okay I suppose, if inefficient.  Maybe a more fundamental
> question is how do I get in and see how plot is working in this  
> case so
> that I can modify.
>
> Thanks much!
>
> J
>
> P.S.  I've explored using plot.summary.rqs but the problems seem to be
> the same.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From aaron.king at umich.edu  Tue Jul 24 14:05:19 2007
From: aaron.king at umich.edu (Aaron King)
Date: Tue, 24 Jul 2007 08:05:19 -0400
Subject: [R] [R-pkgs] New package: pomp,
	inference for partially-observed Markov processes
Message-ID: <200707240805.19285.aaron.king@umich.edu>

To: cran at r-project.org
Subject: New package: pomp, inference for partially-observed Markov processes

The new package 'pomp' is built around a very general realization of nonlinear 
partially-observed Markov processes (AKA state-space models, nonlinear 
stochastic dynamical systems). The user provides functions specifying the 
model's process and measurement components. The package's algorithms are 
built on top of these functions. 

At the moment, algorithms are provided for particle filtering (AKA sequential 
importance sampling or sequential Monte Carlo) and the likelihood 
maximization by iterated filtering (MIF) method of Ionides, Breto, and King 
(PNAS, 103:18438-18443, 2006). Future support for a variety of other 
algorithms is envisioned. A working group of the National Center for 
Ecological Analysis and Synthesis (NCEAS), "Inference for Mechanistic 
Models", is currently implementing additional methods for this package.

Simple worked examples are provided in the form of a 
vignette, "random_walk_example".

The package is provided under the GPL. Contributions are welcome, as are 
comments, suggestions, examples, and bug reports.

The development of this package has been aided by support from the U.S. N.S.F 
(Grants #EF-0545276, #EF-0430120) and by the "Inference for Mechanistic 
Models" Working Group supported by the National Center for Ecological 
Analysis and Synthesis, a Center funded by NSF (Grant #DEB-0553768), the 
University of California, Santa Barbara, and the State of California.



-- 
Aaron A. King, Ph.D.
Ecology & Evolutionary Biology
University of Michigan
http://www.umich.edu/~kingaa
GPG Public Key: 0x2B00840F

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From rkoenker at uiuc.edu  Tue Jul 24 20:29:13 2007
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 24 Jul 2007 13:29:13 -0500
Subject: [R] quantreg behavior changes for N>1000
In-Reply-To: <46A63D83.3070405@comcast.net>
References: <072220071939.21999.46A3B26E00098C96000055EF2216566276C0C0099D06@comcast.net>
	<D6B56BC9-5567-484A-A9CF-8BA71843342A@jhsph.edu>
	<46A63D83.3070405@comcast.net>
Message-ID: <7E5DA6C0-4272-4742-B2E0-68D713BDC4A0@uiuc.edu>

When in doubt:  RTFM --  Quoting from ?summary.rq

se: specifies the method used to compute standard standard
           errors.  There are currently five available methods:

              1.  '"rank"' which produces confidence intervals for the
                 estimated parameters by inverting a rank test as
                 described in Koenker (1994).  The default option
                 assumes that the errors are iid, while the option iid =
                 FALSE implements the proposal of Koenker Machado
                 (1999).  This is the default method unless the sample
                 size exceeds 1001, or cov = FALSE in which case se =
                 "nid" is used.

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jul 24, 2007, at 12:57 PM, Jeff G. wrote:

> Hello again R-experts and novices (like me),
>
> This seems like a bug to me - or maybe it's intentional...can anyone
> confirm?  Up to 1000 reps, summary() of a rq object gives different
> output and subtly different confidence interval estimates.
>
> Thanks....Jeff
>
> testx=runif(1200)
> testy=rnorm(1200, 5)
>
> test.rq=summary(rq(testy[1:1000]~testx[1:1000], tau=2:98/100))
> test.rq[[1]]
> Gives this output:
> Call: rq(formula = testy[1:1000] ~ testx[1:1000], tau = 2:98/100)
>
>     tau: [1] 0.02
>
>     Coefficients:
>                   coefficients       lower bd   upper bd
>     (Intercept)    3.00026         2.45142     3.17098
>     testx[1:1000] -0.00870     -0.39817  0.49946
>
> test.rq=summary(rq(testy[1:1001]~testx[1:1001], tau=2:98/100))
> test.rq[[1]]
>
> Gives this (different) output:
>     Call: rq(formula = testy[1:1001] ~ testx[1:1001], tau = 2:98/100)
>
>     tau: [1] 0.02
>
>     Coefficients:
>                   Value    Std. Error t value  Pr(>|t|)
>    (Intercept)    3.00026  0.21605   13.88658  0.00000
>     testx[1:1001] -0.00870  0.32976   -0.02638  0.97896
>
>
> plot(test.rq, nrow=2, ncol=2) # The slope estimates appear to be the
> same but there are subtle differences in the confidence intervals,  
> which
> shouldn't be due simply to the inclusion of one more point.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Tue Jul 24 21:18:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jul 2007 20:18:47 +0100 (BST)
Subject: [R] Using lmer with huge amount of data
In-Reply-To: <C205088E-D754-4D66-BA07-65B73573DE6B@mail.nih.gov>
References: <C205088E-D754-4D66-BA07-65B73573DE6B@mail.nih.gov>
Message-ID: <Pine.LNX.4.64.0707242007160.19606@gannet.stats.ox.ac.uk>

I think I am missing something here: how do you make this 'huge' and 
'gigantic'?  You have not told us how many subjects you have, but in 
imaging experiments it is usually no more than 50 and often less.

For each subject you have 3 x 30,000 responses plus an age.  That is under 
1Mb of data per subject, so the problem looks modest unless you have many 
hundreds of subjects.

Nothing says you need to read the data in one go, but it will be helpful 
to have all the data available to R at once (although this could be 
alleviated by using a DBMS interface).

I think the problem is rather going to be running 30,000 lmer fits, which 
in my experience often take seconds each.  Each fit will only need a 
modest amount of data (3 responses and one age per subject).

On Tue, 24 Jul 2007, Gang Chen wrote:

> Based on the examples I've seen in using statistical analysis
> packages such as lmer, it seems that people usually tabulate all the
> input data into one file with the first line indicating the variable
> names (or labels), and then read the file inside R. However, in my
> case I can't do that because of the huge amount of imaging data.
>
> Suppose I have a one-way within-subject ANCOVA with one covariate,
> and I would like to use lmer in R package lme4 to analyze the data.
> In the terminology of linear mixed models, I have a fixed factor A
> with 3 levels, a random factor B (subject), and a covariate (age)
> with a model like this
>
> MyResult <- lmer(Response ~ FactorA + Age + (1 | subject), MyData, ...)
>
> My input data are like this: For each subject I have a file (a huge
> matrix) storing the response values of the subject at many locations
> (~30,000 voxels) corresponding to factor A at the 1st level, another
> file for factor A at the 2nd level, and a 3rd file for factor A at
> the 3rd level. Then I have another file storing the age of those
> subjects. The analysis with the linear mixed model above would be
> done at each voxel separately.
>
> It seems impractical to create one gigantic file or matrix to feed
> into the above command line because of the big number of voxels. I'm
> not sure how to proceed in this case. Any suggestions would be highly
> appreciated.
>
> Also if I'm concerned about any potential violation of sphericity
> among the 3 levels of factor A, how can I test sphericity violation
> in lmer? And if violation exists, how can I make corrections in
> contrast testing?
>
> Thank you very much,
> Gang

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Agustin.Lobo at ija.csic.es  Tue Jul 24 21:22:58 2007
From: Agustin.Lobo at ija.csic.es (Agustin Lobo)
Date: Tue, 24 Jul 2007 21:22:58 +0200
Subject: [R] Renamig a factor
Message-ID: <46A65192.2010700@ija.csic.es>

Which is the proper way to rename a factor?
If I do:
test$Parc[test$Parc=="Ol?rdola"]<-"Ol?rdola"
R complains that
Warning message:
invalid factor level, NAs generated in: `[<-.factor`(`*tmp*`, test$Parc 
== "Ol?rdola", value = "Ol?rdola")

Thanks
Agus
-- 
Dr. Agustin Lobo
Institut de Ciencies de la Terra "Jaume Almera" (CSIC)
LLuis Sole Sabaris s/n
08028 Barcelona
Spain
Tel. 34 934095410
Fax. 34 934110012
email: Agustin.Lobo at ija.csic.es
http://www.ija.csic.es/gt/obster


From bonedu at gmail.com  Tue Jul 24 21:26:12 2007
From: bonedu at gmail.com (Eduard Bonet)
Date: Tue, 24 Jul 2007 21:26:12 +0200
Subject: [R] Latent class analysis with ordinal manifest variables
Message-ID: <3e192e060707241226o683fd53evcfc9ca7e8fc41ba2@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070724/6debfd9f/attachment.pl 

From mark.lyman at gmail.com  Tue Jul 24 21:33:38 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Tue, 24 Jul 2007 19:33:38 +0000 (UTC)
Subject: [R] x,y,z table to matrix with x as rows and y as columns
References: <EBE7A093-2159-4E4E-87E8-D2D4FE0FB181@gmail.com>
Message-ID: <loom.20070724T212330-587@post.gmane.org>

jiho <jo.irisson <at> gmail.com> writes:

> 
> Hello all,
> 
> I am sure I am missing something obvious but I cannot find the  
> function I am looking for. I have a data frame with three columns: X,  
> Y and Z, with X and Y being grid coordinates and Z the value  
> associated with these coordinates. I want to transform this data  
> frame in a matrix of Z values, on the grid defined by X and Y (and,  
> as a plus, fill the X.Y combinations which do no exist in the  
> original data frame with NAs in the resulting matrix). I could do  
> this manually but I guess the appropriate function should be  
> somewhere around. I just can't find it.

I have used xtabs in the past, but xtabs returns 0 instead of NA, which makes 
great for cross-tabulation, the "offending" line is x[is.na(x)] <- 0. So you 
would need to either modify the xtabs function or trust that z is never 0 and 
replace all 0's with NA.

> mydat <- expand.grid(x=1:5, y=1:5)
> mydat <- data.frame(mydat, z=rnorm(25))
> mydat$z[sample(1:25,4)] <- NA
> mytab <- xtabs(z~x+y, mydat)

Mark Lyman


From sparandekar at worldbank.org  Tue Jul 24 21:40:26 2007
From: sparandekar at worldbank.org (sparandekar at worldbank.org)
Date: Tue, 24 Jul 2007 15:40:26 -0400
Subject: [R] How to add circular text for a graph with concentric circles
Message-ID: <OFF657BC2F.A3814195-ON85257322.006B9F78-85257322.006C12B0@worldbank.org>

Dear R experts,

I am plotting the population of students who live in a city, and in 
successive circular bands made of the contiguous districts that surround 
the city. This is a stylized figure, where I specify the area of each 
successive circle based on the cumulative population of students. I want 
to compare two sets of concentric circles across different populations - 
such as 'All students' and 'Private students' (those attending private 
school) by using the same colours and the same dimension of the outer 
circle. I have attached the .pdf file with the output, and the R code to 
generate the first set of circles.

I would appreciate any tips about how to rotate the text label that marks 
each concentric circle (except the central circle) to be curved around the 
circle, located in the middle of each band, thus following the circle 
instead of being horizontal, as I have it now.

Thank you very much,

best regards,
Suhas

# Conurbano1a.R
# R Program to generate circles, radii based on proportion of ALL STUDENTS
# Prepared by Suhas, Tuesday, July 24, 2007

grid.circle(x=0.5, y=0.5, r=3*(.1268), default.units="npc", name=NULL,
                 gp=gpar(fill="olivedrab1",col=NULL), draw=TRUE, vp=NULL)
grid.circle(x=0.5, y=0.5, r=3*(0.095882), default.units="npc", name=NULL,
                 gp=gpar(fill="cornflowerblue",col=NULL), draw=TRUE, 
vp=NULL)
grid.circle(x=0.5, y=0.5, r=3*(0.077894), default.units="npc", name=NULL,
                 gp=gpar(fill="aliceblue",col=NULL), draw=TRUE, vp=NULL)
grid.circle(x=0.5, y=0.5, r=3*(0.061884), default.units="npc", name=NULL,
                 gp=gpar(fill="seagreen",col=NULL), draw=TRUE, vp=NULL)
grid.circle(x=0.5, y=0.5, r=3*(0.050740), default.units="npc", name=NULL,
                 gp=gpar(fill="lightpink2",col=NULL), draw=TRUE, vp=NULL)
grid.circle(x=0.5, y=0.5, r=3*(0.045906), default.units="npc", name=NULL,
                 gp=gpar(fill="navy",col=NULL), draw=TRUE, vp=NULL)

# Now to add the labels ? used trial and error for x and y values

  grid.text("Provincia Interior", x=0.5, y=0.85,
               gp=gpar(fontsize=6, col="black"))

  grid.text("Conurbano 4", x=0.5, y=0.75,
               gp=gpar(fontsize=6, col="black"))

  grid.text("Conurbano 3", x=0.5, y=0.71,
               gp=gpar(fontsize=6, col="black"))

  grid.text("Conurbano 2", x=0.5, y=0.67,
               gp=gpar(fontsize=6, col="white"))

  grid.text("Conurbano 1", x=0.5, y=0.645,
               gp=gpar(fontsize=6, col="black"))
  grid.text("Ciudad de Buenos Aires", x=0.5, y=0.56,
               gp=gpar(fontsize=6, col="white"))

# Title of graph 
grid.text("All Students", x=0.2,y=0.95,gp=gpar(fontsize=20,col="navy"))




**************************************************************************
Suhas D. Parandekar
Senior Education Economist
Latin American and Caribbean Region
Tel: 202 458 7622
e-mail: sparandekar at worldbank.org

From gangchen at mail.nih.gov  Tue Jul 24 21:42:51 2007
From: gangchen at mail.nih.gov (Gang Chen)
Date: Tue, 24 Jul 2007 15:42:51 -0400
Subject: [R] Using lmer with huge amount of data
In-Reply-To: <Pine.LNX.4.64.0707242007160.19606@gannet.stats.ox.ac.uk>
References: <C205088E-D754-4D66-BA07-65B73573DE6B@mail.nih.gov>
	<Pine.LNX.4.64.0707242007160.19606@gannet.stats.ox.ac.uk>
Message-ID: <1B288AB4-FB5F-4AF5-B132-DE905C1B9A60@mail.nih.gov>

Thank you very much for the response, Prof. Ripley.


> I think I am missing something here: how do you make this 'huge'  
> and 'gigantic'?  You have not told us how many subjects you have,  
> but in imaging experiments it is usually no more than 50 and often  
> less.

Usually we have 10-30 subjects.


> For each subject you have 3 x 30,000 responses plus an age.  That  
> is under 1Mb of data per subject, so the problem looks modest  
> unless you have many hundreds of subjects.
>
> Nothing says you need to read the data in one go, but it will be  
> helpful to have all the data available to R at once (although this  
> could be alleviated by using a DBMS interface).


In the hypothetical situation I mentioned in my previous mail  
(suppose we have 12 subjects), all the input data would be stored in  
3 X 12 files each of which contains 30,000 numbers,  plus one more  
file for age. Sure I can read in those 27 files at once, and I'm not  
concerned about the data size at this point, but my question is: do I  
have to reshuffle those 36 files and create 30,000 separate arrays  
(one for each voxel) in R so that I could run lmer voxel-wise?


> I think the problem is rather going to be running 30,000 lmer fits,  
> which in my experience often take seconds each.  Each fit will only  
> need a modest amount of data (3 responses and one age per subject).

Right. What is the most efficient strategy to run such an analysis  
voxel-wise? Write a function, and then use apply()? Or simply do it  
in a loop?

Thanks,
Gang


>
> On Tue, 24 Jul 2007, Gang Chen wrote:
>
>> Based on the examples I've seen in using statistical analysis
>> packages such as lmer, it seems that people usually tabulate all the
>> input data into one file with the first line indicating the variable
>> names (or labels), and then read the file inside R. However, in my
>> case I can't do that because of the huge amount of imaging data.
>>
>> Suppose I have a one-way within-subject ANCOVA with one covariate,
>> and I would like to use lmer in R package lme4 to analyze the data.
>> In the terminology of linear mixed models, I have a fixed factor A
>> with 3 levels, a random factor B (subject), and a covariate (age)
>> with a model like this
>>
>> MyResult <- lmer(Response ~ FactorA + Age + (1 | subject),  
>> MyData, ...)
>>
>> My input data are like this: For each subject I have a file (a huge
>> matrix) storing the response values of the subject at many locations
>> (~30,000 voxels) corresponding to factor A at the 1st level, another
>> file for factor A at the 2nd level, and a 3rd file for factor A at
>> the 3rd level. Then I have another file storing the age of those
>> subjects. The analysis with the linear mixed model above would be
>> done at each voxel separately.
>>
>> It seems impractical to create one gigantic file or matrix to feed
>> into the above command line because of the big number of voxels. I'm
>> not sure how to proceed in this case. Any suggestions would be highly
>> appreciated.
>>
>> Also if I'm concerned about any potential violation of sphericity
>> among the 3 levels of factor A, how can I test sphericity violation
>> in lmer? And if violation exists, how can I make corrections in
>> contrast testing?
>>
>> Thank you very much,
>> Gang
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mark.lyman at gmail.com  Tue Jul 24 21:50:23 2007
From: mark.lyman at gmail.com (Mark Lyman)
Date: Tue, 24 Jul 2007 19:50:23 +0000 (UTC)
Subject: [R] x,y,z table to matrix with x as rows and y as columns
References: <EBE7A093-2159-4E4E-87E8-D2D4FE0FB181@gmail.com>
Message-ID: <loom.20070724T214425-955@post.gmane.org>

> I am sure I am missing something obvious but I cannot find the  
> function I am looking for. I have a data frame with three columns: X,  
> Y and Z, with X and Y being grid coordinates and Z the value  
> associated with these coordinates. I want to transform this data  
> frame in a matrix of Z values, on the grid defined by X and Y (and,  
> as a plus, fill the X.Y combinations which do no exist in the  
> original data frame with NAs in the resulting matrix). I could do  
> this manually but I guess the appropriate function should be  
> somewhere around. I just can't find it.

Immediately after my last post I realized there was a much better solution

> mydat <- expand.grid(x=1:5, y=1:5)
> mydat <- data.frame(mydat, z=rnorm(25))
> mydat$z[sample(1:25,4)] <- NA
> data2mat <- function(x, y, z)
+ {
+ out <- matrix(unlist(split(z, interaction(x,y))), ncol=length(unique(y)))
+ dimnames(out) <- list(unique(x), unique(y))
+ out
+ }
> with(mydat, data2mat(x, y, z))


Mark Lyman


From wwwhsd at gmail.com  Tue Jul 24 21:50:39 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Tue, 24 Jul 2007 16:50:39 -0300
Subject: [R] Renamig a factor
In-Reply-To: <46A65192.2010700@ija.csic.es>
References: <46A65192.2010700@ija.csic.es>
Message-ID: <da79af330707241250x6603af20i2b3f81c26850ef43@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070724/fb457f48/attachment.pl 

From mazatlanmexico at yahoo.com  Tue Jul 24 22:09:59 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Tue, 24 Jul 2007 13:09:59 -0700 (PDT)
Subject: [R] ggplot2 axis color
Message-ID: <158626.58899.qm@web56605.mail.re3.yahoo.com>

Hi:
Does anyone have an idea on how to color the axis and
labels using ggplot2? This is what I got:

library(ggplot2)
 p <- qplot(total_bill, tip, data = tips)
 NewPlot<-  p + geom_abline(slope=c(0.1,0.15,0.2),
colour=c("red","blue","yellow"),size=c(2,5,2))
NewPlot + geom_smooth(colour="green",
size=3,linetype=3)
NewPlot$background.fill<-"cornsilk" 
NewPlot$background.colour <- "blue"
NewPlot$axis.colour<-"red"  ????? it doesn't do it
Thanks

 Felipe D. Carrillo
  Fishery Biologist
  US Fish & Wildlife Service
  Red Bluff, California 96080



      ____________________________________________________________________________________


From ggrothendieck at gmail.com  Tue Jul 24 22:15:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 24 Jul 2007 16:15:41 -0400
Subject: [R] Calculating subsets of row pairs using somthing faster than
	a for loop.
In-Reply-To: <CADFD0E28E1E6A46B0C84335BDB994F504989B9D@whexchmb16.bsna.bsroot.bear.com>
References: <CADFD0E28E1E6A46B0C84335BDB994F504989B9D@whexchmb16.bsna.bsroot.bear.com>
Message-ID: <971536df0707241315v7c38066fnad9bc527a9e9d884@mail.gmail.com>

I doubt its any faster than using a loop but probably less code is:

library(zoo)
z1 <- zoo(t(mat1)); z2 <- zoo(t(mat2))
idx <- 1:ncol(z1)
out <- rollapply(cbind(z1, z2), 11, by.column = FALSE,
    FUN = function(x) cor(x[,idx],x[,-idx]))

will give you a 10 x 16 multivariate zoo series such that:

out[1, ] is c(cor(t(mat1[,1:11]), t(mat2[,1:11])))
out[2, ] is c(cor(t(mat1[,2:12]), t(mat2[,2:12])))
etc.

and t(out) is a matrix in the orientation you asked for.

Try

library(zoo)
vignette("zoo")

for an intro to zoo.


On 7/24/07, Bernzweig, Bruce (Consultant) <bbernzwe at bear.com> wrote:
> Hi all,
>
>
>
> Situation:
>
>
>
>  - I have two matrices each w/ 4 rows and 20 columns.
>
>
>
> mat1 <- matrix(sample(1:500,80), ncol = 20,
>
>            dimnames=list(paste("mat1row", 1:4, sep=""),
>
>            paste("mat1col", 1:20, sep="")))
>
>
>
> mat2 <- matrix(sample(501:1000,80), ncol = 20,
>
>            dimnames=list(paste("mat2row", 1:4, sep=""),
>
>            paste("mat2col", 1:20, sep="")))
>
>
>
>  - Each column represents a value in a time series.
>
>
>
> Q: What do I want:
>
>
>
>   Calculate moving average correlations for each row x row pair:
>
>
>
>   For each row x row pair I want 10 values representing moving average
>
>   correlations for 10 sets of time-values:
>
>
>
>   cor(mat1[1,1:10], mat2[1,1:10])
>
>   cor(mat1[1,2:11], mat2[1,2:11])
>
>   ...
>
>   cor(mat1[1,11:20], mat2[1,11:20])
>
>   cor(mat1[1,1:10], mat2[2,1:10])
>
>   ...
>
>   cor(mat1[4,11:20], mat2[4,11:20])
>
>
>
>   Result would be a 16 (rows) x 10 (col) matrix matMA
>
>
>
>      ma1, ma2, ..., ma10 for (mat1 row1) x (mat2 row1)
>
>      ma1, ma2, ..., ma10 for (mat1 row1) x (mat2 row2)
>
>      ...
>
>      ma1, ma2, ..., ma10 for (mat1 row4) x (mat2 row3)
>
>      ma1, ma2, ..., ma10 for (mat1 row4) x (mat2 row4)
>
>
>
>   I would like to be able to do this without using a for loop
>
>   due to the slowness of that method.
>
>
>
>   Is it possible to iterate through subsets w/o using a for loop?
>
>
>
> Thanks,
>
>
>
> - Bruce
>
>
>
>      P
>
>
>
>
> **********************************************************************
> Please be aware that, notwithstanding the fact that the pers...{{dropped}}
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From ramasamy at cancer.org.uk  Tue Jul 24 22:21:51 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 24 Jul 2007 21:21:51 +0100
Subject: [R] Obtaining summary of frequencies of value occurrences for
 a	variable in a multivariate dataset.
In-Reply-To: <494197.22767.qm@web53502.mail.re2.yahoo.com>
References: <494197.22767.qm@web53502.mail.re2.yahoo.com>
Message-ID: <46A65F5F.3020702@cancer.org.uk>

The name of the table should give you the "value". And if you have a 
matrix, you just need to convert it into a vector first.

 > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ], nc=3 )
 > m
      [,1] [,2] [,3]
[1,] "A"  "C"  "B"
[2,] "B"  "D"  "C"
[3,] "C"  "E"  "D"
 > tb <- table( as.vector(m) )
 > tb

A B C D E
1 2 3 2 1
 > paste( names(tb), ":", tb, sep="" )
[1] "A:1" "B:2" "C:3" "D:2" "E:1"

If this is not what you want, then please give a simple example.

Regards, Adai



Allan Kamau wrote:
> Hi all,
> If the question below as been answered before I
> apologize for the posting.
> I would like to get the frequencies of occurrence of
> all values in a given variable in a multivariate
> dataset. In short for each variable (or field) a
> summary of values contained with in a value:frequency
> pair, there can be many such pairs for a given
> variable. I would like to do the same for several such
> variables.
> I have used table() but am unable to extract the
> individual value and frequency values.
> Please advise.
> 
> Allan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From fmc2+ at pitt.edu  Tue Jul 24 22:12:42 2007
From: fmc2+ at pitt.edu (Fiona Callaghan)
Date: Tue, 24 Jul 2007 16:12:42 -0400 (EDT)
Subject: [R] Regression trees using Goodness-of-Split
Message-ID: <1748.136.142.101.135.1185307962.squirrel@webmail.pitt.edu>

Hi
I have two questions:
1)
I would like to know if there is a package in R that constructs a
regression tree using the 'goodness-of-split' algorithm for survival
analysis proposed by Le Blanc and Crowley (1993) (rather than the usual
CART algorithm that uses within-node difference and impurity functions). 
In other words, is there a tree package based on between-node difference
using a two sample test e.g log rank test.

2)If such a package exists, are there user-defined split functions (can we
substitute other split functions).

Thanks for any help
Fiona


-- 
Fiona Callaghan, MS
A432 Crabtree Hall
Department of Biostatistics
Graduate School of Public Health
University of Pittsburgh
Phone 412 624 3063


From h.wickham at gmail.com  Tue Jul 24 22:30:36 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 24 Jul 2007 22:30:36 +0200
Subject: [R] ggplot2 axis color
In-Reply-To: <158626.58899.qm@web56605.mail.re3.yahoo.com>
References: <158626.58899.qm@web56605.mail.re3.yahoo.com>
Message-ID: <f8e6ff050707241330y6758d67eq71b48caee1a25dce@mail.gmail.com>

Hi Felipe,

Looks like a bug!  I'll try and get it fixed for the next version.  In
the meantime, you can read the last chapter of the ggplot book to see
how to fix it with grid.

Hadley

On 7/24/07, Felipe Carrillo <mazatlanmexico at yahoo.com> wrote:
> Hi:
> Does anyone have an idea on how to color the axis and
> labels using ggplot2? This is what I got:
>
> library(ggplot2)
>  p <- qplot(total_bill, tip, data = tips)
>  NewPlot<-  p + geom_abline(slope=c(0.1,0.15,0.2),
> colour=c("red","blue","yellow"),size=c(2,5,2))
> NewPlot + geom_smooth(colour="green",
> size=3,linetype=3)
> NewPlot$background.fill<-"cornsilk"
> NewPlot$background.colour <- "blue"
> NewPlot$axis.colour<-"red"  ????? it doesn't do it
> Thanks
>
>  Felipe D. Carrillo
>   Fishery Biologist
>   US Fish & Wildlife Service
>   Red Bluff, California 96080
>
>
>
>       ____________________________________________________________________________________
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ripley at stats.ox.ac.uk  Tue Jul 24 23:04:27 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Jul 2007 22:04:27 +0100 (BST)
Subject: [R] Using lmer with huge amount of data
In-Reply-To: <1B288AB4-FB5F-4AF5-B132-DE905C1B9A60@mail.nih.gov>
References: <C205088E-D754-4D66-BA07-65B73573DE6B@mail.nih.gov>
	<Pine.LNX.4.64.0707242007160.19606@gannet.stats.ox.ac.uk>
	<1B288AB4-FB5F-4AF5-B132-DE905C1B9A60@mail.nih.gov>
Message-ID: <Pine.LNX.4.64.0707242201510.11834@gannet.stats.ox.ac.uk>

On Tue, 24 Jul 2007, Gang Chen wrote:

> Thank you very much for the response, Prof. Ripley.
>
>
>> I think I am missing something here: how do you make this 'huge' and 
>> 'gigantic'?  You have not told us how many subjects you have, but in 
>> imaging experiments it is usually no more than 50 and often less.
>
> Usually we have 10-30 subjects.
>
>
>> For each subject you have 3 x 30,000 responses plus an age.  That is under 
>> 1Mb of data per subject, so the problem looks modest unless you have many 
>> hundreds of subjects.
>> 
>> Nothing says you need to read the data in one go, but it will be helpful to 
>> have all the data available to R at once (although this could be alleviated 
>> by using a DBMS interface).
>
>
> In the hypothetical situation I mentioned in my previous mail (suppose we 
> have 12 subjects), all the input data would be stored in 3 X 12 files each of 
> which contains 30,000 numbers,  plus one more file for age. Sure I can read 
> in those 27 files at once, and I'm not concerned about the data size at this 
> point, but my question is: do I have to reshuffle those 36 files and create 
> 30,000 separate arrays (one for each voxel) in R so that I could run lmer 
> voxel-wise?

No. You can index data structures in R.

>> I think the problem is rather going to be running 30,000 lmer fits, which 
>> in my experience often take seconds each.  Each fit will only need a modest 
>> amount of data (3 responses and one age per subject).
>
> Right. What is the most efficient strategy to run such an analysis 
> voxel-wise? Write a function, and then use apply()? Or simply do it in a 
> loop?

apply() is a loop internally.  I would just use a for() loop here, 
probably running groups of voxels in different jobs run simultaneously on 
multi-CPU machines.

>
> Thanks,
> Gang
>
>
>> 
>> On Tue, 24 Jul 2007, Gang Chen wrote:
>> 
>>> Based on the examples I've seen in using statistical analysis
>>> packages such as lmer, it seems that people usually tabulate all the
>>> input data into one file with the first line indicating the variable
>>> names (or labels), and then read the file inside R. However, in my
>>> case I can't do that because of the huge amount of imaging data.
>>> 
>>> Suppose I have a one-way within-subject ANCOVA with one covariate,
>>> and I would like to use lmer in R package lme4 to analyze the data.
>>> In the terminology of linear mixed models, I have a fixed factor A
>>> with 3 levels, a random factor B (subject), and a covariate (age)
>>> with a model like this
>>> 
>>> MyResult <- lmer(Response ~ FactorA + Age + (1 | subject), MyData, ...)
>>> 
>>> My input data are like this: For each subject I have a file (a huge
>>> matrix) storing the response values of the subject at many locations
>>> (~30,000 voxels) corresponding to factor A at the 1st level, another
>>> file for factor A at the 2nd level, and a 3rd file for factor A at
>>> the 3rd level. Then I have another file storing the age of those
>>> subjects. The analysis with the linear mixed model above would be
>>> done at each voxel separately.
>>> 
>>> It seems impractical to create one gigantic file or matrix to feed
>>> into the above command line because of the big number of voxels. I'm
>>> not sure how to proceed in this case. Any suggestions would be highly
>>> appreciated.
>>> 
>>> Also if I'm concerned about any potential violation of sphericity
>>> among the 3 levels of factor A, how can I test sphericity violation
>>> in lmer? And if violation exists, how can I make corrections in
>>> contrast testing?
>>> 
>>> Thank you very much,
>>> Gang
>> 
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From rvaradhan at jhmi.edu  Tue Jul 24 23:35:25 2007
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 24 Jul 2007 17:35:25 -0400
Subject: [R] Computing the rank of a matrix.
In-Reply-To: <18086.14898.656256.84710@stat.math.ethz.ch>
References: <4616316B.6020403@decsai.ugr.es>
	<OF64AC873D.BB57D00C-ON862572B5.0052ED54-862572B5.00530C61@mmm.com>
	<000801c7786a$dad09e00$7c94100a@win.ad.jhu.edu>
	<17943.45399.832117.683546@stat.math.ethz.ch>
	<f4eabc685f8d.4617e568@johnshopkins.edu>
	<18086.14898.656256.84710@stat.math.ethz.ch>
Message-ID: <002501c7ce3a$8c619960$7c94100a@win.ad.jhu.edu>

Hi Martin,

I just realized (courtesy: ?qr) that LAPACK=TRUE always gives full rank, no
matter what the matrix and tolerance are. So, clearly my results using
LAPACK=TRUE should be ignored.  So, the real comparison is only between
rankMat and qr(., LAPACK=FALSE)$rank.  I can?t help but fell that there can
be no "correct" solution to an ill-posed problem.  Furthermore, I haven't
come across a real application where the numerical estimate of a rank is
directly useful.

Best,
Ravi.

----------------------------------------------------------------------------
-------

Ravi Varadhan, Ph.D.

Assistant Professor, The Center on Aging and Health

Division of Geriatric Medicine and Gerontology 

Johns Hopkins University

Ph: (410) 502-2619

Fax: (410) 614-9625

Email: rvaradhan at jhmi.edu

Webpage:  http://www.jhsph.edu/agingandhealth/People/Faculty/Varadhan.html

 

----------------------------------------------------------------------------
--------

-----Original Message-----
From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
Sent: Tuesday, July 24, 2007 1:43 PM
To: RAVI VARADHAN
Cc: Martin Maechler; r-help at stat.math.ethz.ch
Subject: Re: [R] Computing the rank of a matrix.

>>>>> "RV" == RAVI VARADHAN <rvaradhan at jhmi.edu>
>>>>>     on Sat, 07 Apr 2007 18:39:36 -0400 writes:
	     ^^^^^^^^^^^^^^^^

this is a bit a late reply...  better late than never

    RV> Hi Martin,

Hi Ravi, thanks for your research.


    RV> I played a bit with rankMat.  I will first state the
    RV> conclusions of my numerical experiments and then present
    RV> the results to support them:

    RV> 1.  I don't believe that rankMat, or equivalently
    RV> Matlab's rank computation, is necessarily better than
    RV> qr(.)$rank or (qr., LAPACK=TRUE)$rank.  In fact, for the
    RV> notorious Hilbert matrix, rankMat can give poor
    RV> estimates of rank.

    RV> 2.  There exists no universally optimal (i.e. optimal
    RV> for all A) tol in qr(A, tol)$rank that would be
    RV> optimally close to rankMat.  The discrepancy in the
    RV> ranks computed by qr(A)$rank and rankMat(A) obviously
    RV> depends on the matrix A. This is evident from the tol
    RV> used in rankMat:
    RV>      tol <- max(d) * .Machine$double.eps * abs(singValA[1])
    RV> So, this value of tol in qr will also minimize the discrepancy. 

    RV> 3.  The tol parameter is irrelevant in qr(A, tol,
    RV> LAPACK=TRUE)$rank, i.e. LAPACK doesn't seem to utilize
    RV> the tol parameter when computing the rank.  However,
    RV> qr(A, tol, LAPACK=FALSE)$rank does depend on tol.

Yes, indeed!  The help page of qr() actually says so 
{at least now, don't know about 3 months ago}.

    RV> Now, here are the results:
    RV> 1.
    >> hilbert.rank <- matrix(NA,20,3)
    >> hilbert <- function(n) { i <- 1:n; 1 / outer(i - 1, i, "+") }
    >> for (i in 1:20) {
    RV> + himat <- hilbert(i)
    RV> + hilbert.rank[i,1] <- rankMat(himat)
    RV> + hilbert.rank[i,2] <- qr(himat,tol=1.0e-14)$rank
    RV> + hilbert.rank[i,3] <- qr(himat, tol = .Machine$double.eps, LAPACK =
TRUE)$rank
    RV> + }
    >> hilbert.rank
    RV> [,1] [,2] [,3]
    RV> [1,]    1    1    1
    RV> [2,]    2    2    2
    RV> [3,]    3    3    3
    RV> [4,]    4    4    4
    RV> [5,]    5    5    5
    RV> [6,]    6    6    6
    RV> [7,]    7    7    7
    RV> [8,]    8    8    8
    RV> [9,]    9    9    9
    RV> [10,]   10   10   10
    RV> [11,]   10   11   11
    RV> [12,]   11   12   12
    RV> [13,]   11   12   13
    RV> [14,]   11   13   14
    RV> [15,]   12   13   15
    RV> [16,]   12   15   16
    RV> [17,]   12   16   17
    RV> [18,]   12   16   18
    RV> [19,]   13   17   19
    RV> [20,]   13   17   20

    RV> We see that rankMat underestimates the rank for n > 10, but
qr(himat, tol = .Machine$double.eps, LAPACK=TRUE)$rank gets it right.

Yes, indeed;  and that's  seems a bit  against the  ``common
knowledge'' that svd() is more reliable than qr()

Hmm.... I'm still baffled a bit..

Note that with the Hilbert matrices, 
one might argue that  hilbert(20) might really not have a
correct "estimated rank" of 20,
but at least for hilbert(13) or so, the rank should be == n.

BTW, there's a nice plot, slightly related to this problem,
using rcond() from the Matrix package :

  library(Matrix)

  hilbert <- function(n) { i <- 1:n; 1 / outer(i - 1, i, "+") }
  rcHilb <- sapply(1:20, function(n) rcond(Matrix(hilbert(n))))

  plot(rcHilb, xlab = "n", log = "y", col = 2, type = "b",
       main = "reciprocal condition numbers of Hilbert(n)")

where one sees that the LAPACK code that underlies
Matrix::rcond() also gets into "problem" when estimating the
condition number for hilbert(n) when n >~ 16 .

I think if we wanted to make real progress here, we'd have to
consult with numerical analyist specialists.
But for me the topic is too remote to be followed up further at
the moment.

One conclusion for me is that to estimate the rank of a matrix
in current versions of R, one should use

  rankMat <- function(x)  qr(x, LAPACK = TRUE)$rank

(as was suggested as one possibility in the original thread)

Regards, and thank you again, Ravi.

Martin Maechler, ETH Zurich


    RV> 2.  
    RV> Here I first, created a random rectangular matrix, and then added a
number of rows to it, where these new rows are the same as some of the
original rows except for a tiny amount of noise, which I call eps.  So, the
degree of rank deficiency is controlled by eps.  I let eps take on 3 values:
1.e-07, 1.e-10, and 1.e-14, and show that the optimal tol (in terms of being
close to rankMat) depends on the level of precision (eps) in the matrix.
    >> set.seed(123)
    >> nrow <- 15
    >> ncol <- 20
    >> nsim <- 5000
    >> ndefic <- 4  # number of "nearly" dependent rows
    >> eps <- 1.e-07
    >> rnk <- matrix(NA, nsim, 5)
    >> for (j in 1:nsim) {
    RV> + A <- matrix(rnorm(nrow*ncol),nrow, ncol)
    RV> + newrows <- matrix(NA, ndefic, ncol)
    RV> + for (i in 1:ndefic) {
    RV> + newrows[i,] <- A[nrow-i,] + rnorm(ncol, sd=min(abs(A[nrow-i+1,]))*
eps)
    RV> + }
    RV> + 
    RV> + B <- rbind(A,newrows)
    RV> + rnk[j,1] <- rankMat(B)
    RV> + rnk[j,2] <- qr(B, tol = 1.e-07)$rank
    RV> + rnk[j,3] <- qr(B, tol = 1.e-11)$rank
    RV> + rnk[j,4] <- qr(B, tol = 1.0e-14)$rank
    RV> + rnk[j,5] <- qr(B, tol = .Machine$double.eps, LAPACK = TRUE)$rank
    RV> + }
    >> apply(abs(rnk[,1] - rnk[,2:5]),2,sum)
    RV> [1] 19351    53     0     0

    RV> We observe that both qr(B, tol=1.e-14)$rank and qr(B, tol =
.Machine$double.eps, LAPACK = TRUE)$rank give exactly the same rank as
rankMat.

    RV> Now, we change eps <- 1.e-10 and the results are:
    >> apply(abs(rnk[,1] - rnk[,2:5]),2,sum)
    RV> [1] 19778 14263   166   220
    RV> This means that a tolerance of 1.e-14 works best.

    RV> Now we lower eps further: eps <- 1.e-14
    >> apply(abs(rnk[,1] - rnk[,2:5]),2,sum)
    RV> [1]     0     3   665 20000
    RV> Clearly, the smaller tolerances yield rank estimates that are higher
than that given by rankMat.  That is, rankMat underestimates the rank as in
the case of Hilbert matrix.

    RV> 3.
    RV> Now we show that qr(., tol, LAPACK=TRUE)$rank is independent of tol:
    >> exp <- -(7:16)
    >> tol <- 10^exp
    >> sapply(tol, A=hilbert(20), function(x,A)qr(A, tol=x,
LAPACK=FALSE)$rank)
    RV> [1] 10 12 14 14 15 16 16 17 18 19
    >> sapply(tol, A=hilbert(20), function(x,A)qr(A, tol=x,
LAPACK=TRUE)$rank)
    RV> [1] 20 20 20 20 20 20 20 20 20 20

    RV> Looking forward to comments.

    RV> Best,
    RV> Ravi. 

    RV> ----- Original Message -----
    RV> From: Martin Maechler <maechler at stat.math.ethz.ch>
    RV> Date: Saturday, April 7, 2007 10:57 am
    RV> Subject: Re: [R] Computing the rank of a matrix.
    RV> To: Ravi Varadhan <rvaradhan at jhmi.edu>
    RV> Cc: apjaworski at mmm.com, "'Jos? Luis Aznarte M.'"
<jlaznarte at decsai.ugr.es>, r-help at stat.math.ethz.ch


    >> >>>>> "Ravi" == Ravi Varadhan <rvaradhan at jhmi.edu>
    >> >>>>>     on Fri, 6 Apr 2007 12:44:33 -0400 writes:
    >> 
    Ravi> Hi, qr(A)$rank will work, but just be wary of the
    Ravi> tolerance parameter (default is 1.e-07), since the
    Ravi> rank computation could be sensitive to the tolerance
    Ravi> chosen.
    >> 
    >> Yes, indeed.
    >> 
    >> The point is that   rank(<Matrix>) 
    >> is well defined in pure math (linear algebra), as well as 
    >> a "singular matrix" is.
    >> 
    >> The same typically no longer makes sense as soon as you enter
    >> the real world: A matrix "close to singular" may have to be
    >> treated "as if singular" depending on its "singularity
    >> closeness" {{ learn about the condition number of a matrix }}
    >> and the same issues arise with rank(<matrix>).
    >> 
    >> Of course, the matlab programmers know all this (and much more),
    >> and indeed, matlab's  rank(A) really is  
    >> rank(A, tol = tol.default(A))
    >> 
    >> and is based on the SVD instead of QR decomposition since the
    >> former is said to be more reliable (even though slightly slower).
    >> 
    >> R's equivalent (with quite a bit of fool-proofing) would be the
    >> following function (assuming correct online documentation of matlab):
    >> 
    >> 
    >> rankMat <- function(A, tol = NULL, singValA = svd(A, 0,0)$d)
    >> {
    >> ## Purpose: rank of a matrix ``as Matlab''
    >> ##
----------------------------------------------------------------------
    >> ## Arguments: A: a numerical matrix, maybe non-square
    >> ##        	tol: numerical tolerance (compared to singular
values)
    >> ##           singValA: vector of non-increasing singular values 
    >> of A
    >> ##                     (pass as argument if already known)
    >> ##
----------------------------------------------------------------------
    >> ## Author: Martin Maechler, Date:  7 Apr 2007, 16:16
    >> d <- dim(A)
    >> stopifnot(length(d) == 2, length(singValA) == min(d),
    >> diff(singValA) < 0)       # must be sorted decreasingly
    >> if(is.null(tol))
    >> tol <- max(d) * .Machine$double.eps * abs(singValA[1])
    >> else stopifnot(is.numeric(tol), tol >= 0)
    >> sum(singValA >= tol)
    >> }
    >> 
    >> 
    >> A small scale simulation with random matrices,
    >> i.e., things like
    >> 
    >> ## ranks of random matrices; here will have 5 all the time:
    >> table(replicate(1000, rankMat(matrix(rnorm(5*12),5, 12) )))# < 1 sec.
    >> 
    >> indicates that qr(.)$rank  gives the same typically,
    >> where I assume one should really use
    >> 
    >> qr(., tol = .Machine$double.eps, LAPACK = TRUE)$rank
    >> 
    >> to be closer to Matlab's default tolerance.
    >> 
    >> Ok, who has time to investigate further? 
    >> Research exercise:
    >> 
    1> Is there a fixed number, say  t0 <- 1e-15 
    1> for which  qr(A, tol = t0, LAPACK=TRUE)$rank is 
    1> ``optimally close'' to rankMat(A) ?
    >> 
    2> how easily do you get cases showing svd(.) to more reliable
    2> than qr(., LAPACK=TRUE)?
    >> 
    >> To solve this in an interesting way, you should probably
    >> investigate classes of "almost rank-deficient" matrices,
    >> and I'd also be interested if you "randomly" ever get matrices A
    >> with  rank(A) <  min(dim(A)) - 1
    >> (unless you construct some columns/rows exactly from earlier
    >> ones or use all-0 ones)
    >> 
    >> Martin Maechler, ETH Zurich
    >> 
    >> 
    >> 
    >> 
    Ravi> Ravi.
    >> 
    Ravi> -----------------------------------------------------------------
    Ravi> Ravi Varadhan, Ph.D.
    Ravi> Assistant Professor, The Center on Aging and Health
    >> .......
    Ravi> 
    >> 
    >> 
    Ravi>
--------------------------------------------------------------------
    >> 
    >> 
    >> 
    >> >> How about
    >> 
    >> >>       qr(A)$rank
    >> 
    >> >> or perhaps
    >> 
    >> >>       qr(A, LAPACK=TRUE)$rank
    >> 
    >> >> Cheers,
    >> 
    >> >> Andy
    >> 
    >> 
    >> >>     Hi! Maybe this is a silly question, but I need the
    >> >> column rank (
    >> >> of a matrix and R function 'rank()' only gives me the
    >> >> ordering of the elements of my matrix.  How can I
    >> >> compute the column rank of a matrix? Is there not an R
    >> >> equivalent to Matlab's 'rank()'?  I've been browsing
    >> >> for a time now and I can't find anything, so any help
    >> >> will be greatly appreciated. Best regards!
    >> 
    >> 
    >> >> -- -- Jose Luis Aznarte M.
    >> >>  Department of Computer
    >> >> Science and Artificial Intelligence Universidad de
    >> >> Granada Tel. +34 - 958 - 24 04 67 GRANADA (Spain) Fax:
    >> >> +34 - 958 - 24 00 79


From mark_difford at yahoo.co.uk  Tue Jul 24 23:42:12 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Tue, 24 Jul 2007 14:42:12 -0700 (PDT)
Subject: [R] crimtab related question
In-Reply-To: <p0600203ac2cbd27de33c@[192.168.1.10]>
References: <p0600203ac2cbd27de33c@[192.168.1.10]>
Message-ID: <11772414.post@talk.nabble.com>


Hi Jean,

You haven't yet had a reply from an authoratitive source, so here is my
tuppence worth to part of your enquiry.

It's almost certain that the "receiving box" is a receptacle into which tags
were placed after they had been drawn and the inscribed measurement noted
down.  Measurements on three tags were unwittingly not noted before the tags
were transferred to the receiving box.  They lay there with a good many
other tags, so the inscribed measurement/tag couldn't be recovered.

I hope this clarifies some points.

Regards,
Mark.


Jean lobry wrote:
> 
> Dear all,
> 
> the dataset documented under ?crimtab was also used in:
> 
> @article{TreloarAE1934,
>      title = {The adequacy of "{S}tudent's" criterion of
>               deviations in small sample means},
>      author = {Treloar, A.E. and Wilder, M.A.},
>      journal = {The Annals of Mathematical Statistics},
>      volume = {5},
>      pages = {324-341},
>      year = {1934}
> }
> 
> The following is from page 335 of the above paper:
> 
> "From the table provided by MacDonell (1902) on
> the associated variation of stature (to the nearest inch)
> and length of the left middle finger (to the nearest
> millimeter) in 3000 British criminals, the measusurements
> were transferred to 3000 numbered Denison metal-rim
> tags from which the cords has been removed. After
> thorough checking and mixing of these circular disks,
> samples of 5 tags each were drawn at random until the
> supply was exhausted. Unfortunately, three of these
> samples were erroneously returned to a receiving box
> before being copied, and the records of 597 samples only
> are available."
> 
> Could someone give me a clue about the kind of device
> that was used here? Is it a kind of lottery machine?
> I don't understand why three samples were lost. What
> is this "receiving box"?
> 
> Thanks for any hint,
> 
> Best,
> -- 
> Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
> Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
> 43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
> allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
> http://pbil.univ-lyon1.fr/members/lobry/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/crimtab-related-question-tf4137237.html#a11772414
Sent from the R help mailing list archive at Nabble.com.


From rkoenker at uiuc.edu  Wed Jul 25 00:58:39 2007
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 24 Jul 2007 17:58:39 -0500
Subject: [R] crimtab related question
In-Reply-To: <11772414.post@talk.nabble.com>
References: <p0600203ac2cbd27de33c@[192.168.1.10]>
	<11772414.post@talk.nabble.com>
Message-ID: <231268A7-652E-4591-B9EE-8A11C6E14069@uiuc.edu>

While on the subject of mechanical methods of statistical research  I  
can't
resist quoting Doob's (1997) Statistical Science interview:

> My system, complicated by my inaccurate typing, led to retyping  
> material over and over, and for some time I had an electric drill  
> on my desk, provided with an eraser bit which I used to erase  
> typing. I rarely used the system of brushing white fluid over a  
> typed error because I was not patient enough to let the fluid dry  
> before retyping. Long after my first book was done I discovered the  
> tape rolls which cover lines of type. As I typed and retyped my  
> work it became so repugnant to me that I had more and more  
> difficulty even to look at it to check it. This fact accounts for  
> many slips that a careful reading would have discovered. I commonly  
> used a stochastic system of checking, picking a page and then a  
> place on the page at random and reading a few sentences, in order  
> to avoid reading it in context and thereby to avoid reading what  
> was in my mind rather than what I had written. At first I would  
> catch something at almost every trial, and I would continue until  
> several trials would yield nothing. I have tried this system on  
> other authors, betting for example that I would find something to  
> correct on a randomly chosen printed page of text, and  
> nonmathematicans suffering under the delusion that mathematics is  
> errorless would be surprised at how many bets I have won.

The relevance to the present inquiry is confirmed by the misspelling  
of Dennison in the Annals reference
quoted below.  See, for example:

http://www.amazon.com/Avery-Dennison-Metal-Rim-Tags/dp/B000AN376G

On the substance of Jean's question, Mark's interpretation seems very  
plausible.

Thanks to Jean and to Martin Maechler for adding this dataset to R.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jul 24, 2007, at 4:42 PM, Mark Difford wrote:

>
> Hi Jean,
>
> You haven't yet had a reply from an authoratitive source, so here  
> is my
> tuppence worth to part of your enquiry.
>
> It's almost certain that the "receiving box" is a receptacle into  
> which tags
> were placed after they had been drawn and the inscribed measurement  
> noted
> down.  Measurements on three tags were unwittingly not noted before  
> the tags
> were transferred to the receiving box.  They lay there with a good  
> many
> other tags, so the inscribed measurement/tag couldn't be recovered.
>
> I hope this clarifies some points.
>
> Regards,
> Mark.
>
>
> Jean lobry wrote:
>>
>> Dear all,
>>
>> the dataset documented under ?crimtab was also used in:
>>
>> @article{TreloarAE1934,
>>      title = {The adequacy of "{S}tudent's" criterion of
>>               deviations in small sample means},
>>      author = {Treloar, A.E. and Wilder, M.A.},
>>      journal = {The Annals of Mathematical Statistics},
>>      volume = {5},
>>      pages = {324-341},
>>      year = {1934}
>> }
>>
>> The following is from page 335 of the above paper:
>>
>> "From the table provided by MacDonell (1902) on
>> the associated variation of stature (to the nearest inch)
>> and length of the left middle finger (to the nearest
>> millimeter) in 3000 British criminals, the measusurements
>> were transferred to 3000 numbered Denison metal-rim
>> tags from which the cords has been removed. After
>> thorough checking and mixing of these circular disks,
>> samples of 5 tags each were drawn at random until the
>> supply was exhausted. Unfortunately, three of these
>> samples were erroneously returned to a receiving box
>> before being copied, and the records of 597 samples only
>> are available."
>>
>> Could someone give me a clue about the kind of device
>> that was used here? Is it a kind of lottery machine?
>> I don't understand why three samples were lost. What
>> is this "receiving box"?
>>
>> Thanks for any hint,
>>
>> Best,
>> -- 
>> Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
>> Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
>> 43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
>> allo  : +33 472 43 27 56     fax    : +33 472 43 13 88
>> http://pbil.univ-lyon1.fr/members/lobry/
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
> -- 
> View this message in context: http://www.nabble.com/crimtab-related- 
> question-tf4137237.html#a11772414
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From anup_nandialath at yahoo.com  Wed Jul 25 01:04:33 2007
From: anup_nandialath at yahoo.com (Anup Nandialath)
Date: Tue, 24 Jul 2007 16:04:33 -0700 (PDT)
Subject: [R] Passing equations as arguments
Message-ID: <879364.50439.qm@web53308.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070724/571bc63c/attachment.pl 

From deepayan.sarkar at gmail.com  Wed Jul 25 01:12:25 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Tue, 24 Jul 2007 16:12:25 -0700
Subject: [R] Overlaying a single contour from a new data array in
	levelplot
In-Reply-To: <200707241110.l6OBARYu011358@msslhb.mssl.ucl.ac.uk>
References: <200707241110.l6OBARYu011358@msslhb.mssl.ucl.ac.uk>
Message-ID: <eb555e660707241612y1964ac09n5346a9135b0cdd56@mail.gmail.com>

On 7/24/07, Jenny Barnes <jmb at mssl.ucl.ac.uk> wrote:
> Dear R-Help community,
>
> I am trying to overlay a single contour line over a correlation plot using
> levelplot in the lattice package. These are the two arrays:
>
> 1) a correlation plot over Africa - so each grid square is a different colour
> dependent on correlation - this is in an array: result_cor with dim[465,465]
>
> 2) a single contour line from a ***different data source*** - this is from data
> related to the p-values for the above correlation plot - I want to overlay only
> the 95% confidence contour. The p-values are stored in an array: result.p.values
> with same dimensions as above.
>
> I have read about using panel.levelplot and panel.contourplot in the R-help
> mailing list but I don't know the right way to call two different data arrays,
> can anybody help me please? I appreciate your time and help with this question.

I can think of a couple of different ways, but the simplest will
probably be to compute the single contour beforehand and add it after
the standard levelplot using a panel function.  E.g., using the
'volcano' data for both matrices:

## you need the explicit x and y arguments because
## the default is different from levelplot.

vcl <- contourLines(x = seq_len(nrow(volcano)),
                   y = seq_len(ncol(volcano)),
                   z = volcano,
                   levels = c(172, 182))

levelplot(volcano, add.cl = vcl,
          panel = function(..., add.cl) {
              panel.levelplot(...)
              lapply(add.cl, panel.polygon, border = 'red')
          })

-Deepayan


From jholtman at gmail.com  Wed Jul 25 01:39:15 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 24 Jul 2007 19:39:15 -0400
Subject: [R] Passing equations as arguments
In-Reply-To: <879364.50439.qm@web53308.mail.re2.yahoo.com>
References: <879364.50439.qm@web53308.mail.re2.yahoo.com>
Message-ID: <644e1f320707241639w2d890de5x2dd8ba7cb798bcff@mail.gmail.com>

Here is one possible solution:

ifun <-
function(a, b, FUN){
    evala <- FUN(a)
    evalb <- FUN(b)
    if (evala > evalb) return(evala) else return(evalb)
}
ifun(1,2,function(x) (x*x) - 2)



On 7/24/07, Anup Nandialath <anup_nandialath at yahoo.com> wrote:
> Friends,
>
> I'm trying to pass an equation as an argument to a function. The idea is as follows. Let us say i write an independent function
>
> Ideal Situation:
>
> ifunc <- function(x)
> {
> return((x*x)-2)
> }
>
> mainfunc <- function(a,b)
> {
> evala <- ifunc(a)
> evalb <- ifunc(b)
> if (evala>evalb){return(evala)}
> else
> return(evalb)
> }
>
> Now I want to try and write this entire program in a single function with the user specifying the equation as an argument to the function.
>
> myfunc <- function(a, b, eqn)
> {
>    func1 <- function (x) ??????????????????
>        {
>            return(eqn in terms of x)  ??????????????????
>       }
>
> Further arguments to check
>
> The ???????? imply that this does not seem to be correct. The idea is how to assign the equation expression from the main equation into the inner function. Is there anyway to do that within this set up?
>
>
> Thanks in advance
> Regards
>
> Anup
>
>
> ---------------------------------
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From kavalieris at xtra.co.nz  Wed Jul 25 02:12:07 2007
From: kavalieris at xtra.co.nz (laimonis)
Date: Wed, 25 Jul 2007 12:12:07 +1200
Subject: [R] aggregate.ts
Message-ID: <46A69557.3060008@xtra.co.nz>

Consider the following scrap of code:

 > x<- ts(1:50,start=c(1,11),freq=12)
 > y <- aggregate(x,nfreq=4)
 > c(y)
 [1]   6  15  24  33  42  51  60  69  78  87  96 105 114 123 132 141
 > y
Error in rep.int("", start.pad) : invalid number of copies in rep.int()
 > tsp(y)
[1] 1.833333 5.583333 4.000000

So we can aggregate into quarters, but we cannot print it using 
print.ts  Even if print.ts cannot line the series into columns as it 
normally does for quarterly data, we would expect it to behave as it 
does when we  aggregate into thirds.

 > y3 <- aggregate(x,nfreq=3)
 > y3
Time Series:
Start = 1.83333333333333
End = 5.5
Frequency = 3
 [1]  10  26  42  58  74  90 106 122 138 154 170 186

And don't tell me that the aggregating a monthly series into quarters 
makes no sense!! (see response to Bug 9798).

Laimonis Kavalieris


From Achim.Zeileis at wu-wien.ac.at  Wed Jul 25 03:06:12 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 25 Jul 2007 03:06:12 +0200 (CEST)
Subject: [R] aggregate.ts
In-Reply-To: <46A69557.3060008@xtra.co.nz>
Message-ID: <Pine.LNX.4.44.0707250237430.16266-100000@disco.wu-wien.ac.at>

On Wed, 25 Jul 2007, laimonis wrote:

> Consider the following scrap of code:

...slightly modified to
  x1 <- ts(1:24, start = c(2000, 10), freq = 12)
  x2 <- ts(1:24, start = c(2000, 11), freq = 12)

and then
  y1 <- aggregate(x1, nfreq = 4)
gives the desired result while
  y2 <- aggregate(x2, nfreq = 4)
probably does not what you would like it to do. In both cases, the 24
observations are broken into 8 segments of equal length (as documented on
?aggregate.ts) and then aggregated. Therefore
  as.vector(y1)
  as.vector(y2)
are identical (and not matched by quarter...as you would probably like).

> And don't tell me that the aggregating a monthly series into quarters
> makes no sense!! (see response to Bug 9798).

1. Your tone is not appropriate.
2. You're not quoting the reply correctly. It said: "You cannot aggregate
   a time series that does not run over quarters into quarters. The
   speculation is plain wrong."
   The reply means that aggregate.ts() does not do what you think
   it does. As I tried to illustrate with the example above.

One can probably argue about whether it makes sense to aggregate a monthly
time series into quarter when I don't have complete observations in each
quarter. But maybe it might be worth considering a change in
aggregate.ts() so that the old and new frequency are matched even with
incomplete observations?

Currently, the "zoo" implementation allows this: Coercing back and forth
gives:
  library("zoo")
  z1 <- as.ts(aggregate(as.zoo(x1), as.yearqtr, sum))
  z2 <- as.ts(aggregate(as.zoo(x2), as.yearqtr, sum))
where z1 is identical to y1, and z2 is what you probably want.

hth,
Z


From M.J.Bojanowski at uu.nl  Wed Jul 25 03:16:07 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Wed, 25 Jul 2007 03:16:07 +0200
Subject: [R] initalizing and checking validity of S4 classes
Message-ID: <94E133D09AA24D43BF6341B675C01A338DCD7F@uu01msg-exb01.soliscom.uu.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/c4cefc3b/attachment.pl 

From mtmorgan at fhcrc.org  Wed Jul 25 03:39:42 2007
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Tue, 24 Jul 2007 18:39:42 -0700
Subject: [R] initalizing and checking validity of S4 classes
In-Reply-To: <94E133D09AA24D43BF6341B675C01A338DCD7F@uu01msg-exb01.soliscom.uu.nl>
	(M. J. Bojanowski's message of "Wed, 25 Jul 2007 03:16:07 +0200")
References: <94E133D09AA24D43BF6341B675C01A338DCD7F@uu01msg-exb01.soliscom.uu.nl>
Message-ID: <6phodi16z35.fsf@gopher4.fhcrc.org>

Hi Michal --

Add validObject to your initialize method:

> setMethod("initialize", "someclass",
+ function(.Object, v=numeric(0), l=character(0))
+ {
+     # strip the vector names
+     cv <- v
+     cl <- l
+     names(cv) <- NULL
+     names(cl) <- NULL
+     rval <- .Object
+     rval at v <- cv
+     rval at l <- cl
+     validObject(rval)
+     rval
+ } )
[1] "initialize"
> new("someclass", v=1:2, l=letters[1:3])
Error in validObject(rval) : 
  invalid class "someclass" object: lengths dont match

Note that without initialize, we have:

> setClass("A",
+          representation=representation(
+            v="numeric", l="character"))
[1] "A"
> setValidity("A", f)

Slots:
                          
Name:          v         l
Class:   numeric character
> new("A", v=1:2, l=letters[1:3])            
Error in validObject(.Object) : 
  invalid class "A" object: lengths dont match
 
Here are two interpretations of this. (1) using 'initialize' means
that you are taking control of the initialization process, and hence
know when you need to call validObject. (2) Responsibility for object
validity is ambiguous -- does it belong with 'new', 'initialize', or a
'constructor' that the programmer might write? This is particularly
problematic with R's copy semantics, where creating transiently
invalid objects seems to be almost necessary (e.g., callNextMethod()
in 'initialize' might initialize the inherited slots of the object,
but the object itself is of the derived class and could well be
invalid 'invalid' after the base class has finished with initialize).

Martin

"Bojanowski, M.J.  (Michal)" <M.J.Bojanowski at uu.nl> writes:

> Dear useRs and wizaRds,
>
> I am currently developing a set of functions using S4 classes. On the way I encountered the problem exemplified with the code below. For some reason the 'validity' method does not seem to work, i.e. does not check for errors in the specification of the slots of the defined class. Any hints?
>
> My understanding of the whole S4 system was that validity checks are made *after* class initialization. Is that correct?
>
> Thanks a lot in advance!
>
> PS. Session info:
>
> R version 2.5.1 (2007-06-27) 
> i386-pc-mingw32 
>
> locale:
> LC_COLLATE=Polish_Poland.1250;LC_CTYPE=Polish_Poland.1250;LC_MONETARY=Polish_Poland.1250;LC_NUMERIC=C;LC_TIME=Polish_Poland.1250
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"  
> [7] "base"     
>> 
>
>
>
>
> -b-e-g-i-n--r--c-o-d-e-
>
> setClass( "someclass", representation(v="numeric", l="character"),
>     prototype( v=numeric(0),
> 	l=character(0) )
>     )
>
> setMethod("initialize", "someclass",
> function(.Object, v=numeric(0), l=character(0))
> {
>     # strip the vector names
>     cv <- v
>     cl <- l
>     names(cv) <- NULL
>     names(cl) <- NULL
>     rval <- .Object
>     rval at v <- cv
>     rval at l <- cl
>     rval
> } )
>
> # at this point this should be OK
> o <- new("someclass", v=1:2, l=letters[1:3])
> o
>
> # check validity
> f <- function(object)
> {
>     rval <- NULL
>     if( length(object at v) != length(object at l) )
> 	rval <- c( rval, "lengths dont match")
>     if( is.null(rval) ) return(TRUE)
>     else return(rval)
> }
>  
> # this should return error description
> f(o)
>
>
> # define the validity method
> setValidity( "someclass", f)
>
> # this should return an error
> new("someclass", v=1:2, l=letters[1:3])
>
> # but it doesn't...
>
> -e-n-d--r--c-o-d-e-
>
>
>
>
> ____________________________________
> Michal Bojanowski
> ICS / Department of Sociology
> Utrecht University
> Heidelberglaan 2; 3584 CS Utrecht
> Room 1428
> m.j.bojanowski at uu dot nl
> http://www.fss.uu.nl/soc/bojanowski/
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Martin Morgan
Bioconductor / Computational Biology
http://bioconductor.org


From lilycai2007 at gmail.com  Wed Jul 25 06:50:43 2007
From: lilycai2007 at gmail.com (Li Li)
Date: Wed, 25 Jul 2007 00:50:43 -0400
Subject: [R] question on using "gl1ce" from "lasso2" package
Message-ID: <813cce770707242150j79ae5228x1895f599ea723096@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/1ec27a42/attachment.pl 

From arun.kumar.saha at gmail.com  Wed Jul 25 08:57:46 2007
From: arun.kumar.saha at gmail.com (Arun Kumar Saha)
Date: Wed, 25 Jul 2007 12:27:46 +0530
Subject: [R] Regarding Bivariate normal distribution.
Message-ID: <d4c57560707242357hedb8418h2acb01aca29f848b@mail.gmail.com>

Dear all R gurus,

My question is related to statistics rather directly to R. Suppose
(X,Y) has a bivariate normal distrubution. I want to find two values
of X and Y say x, and y respectively, such that:

P[X<x, Y<y] = 0.05

My questions are :

1. Can x and y be uniquely found?
2. If it is, how I can find them using R

Your help will be highly appreciated.

Thanks and regards,


From joris.dewolf at cropdesign.com  Wed Jul 25 09:21:26 2007
From: joris.dewolf at cropdesign.com (joris.dewolf at cropdesign.com)
Date: Wed, 25 Jul 2007 09:21:26 +0200
Subject: [R] lme or gls prediction intervals
In-Reply-To: <C5B40E3A-73C3-4663-9497-7A9E344D0A26@MUOhio.edu>
Message-ID: <OFA652F5A2.4C519201-ONC1257323.002805DD-C1257323.0028506E@basf-c-s.be>



Martin,


Have you checked ?intervals.gls


This intervals are approximate, but this would be the obvious starting
point to me.


Joris










                                                                           
             "Martin Henry H.                                              
             Stevens"                                                      
             <HStevens at muohio.                                          To 
             edu>                      R-Help <r-help at stat.math.ethz.ch>   
             Sent by:                                                   cc 
             r-help-bounces at st                                             
             at.math.ethz.ch                                       Subject 
                                       [R] lme or gls prediction intervals 
                                                                           
             24/07/2007 19:22                                              
                                                                           
                                                                           
                                                                           
                                                                           




Hi folks,
I am trying to generate 95% confidence intervals for a gls model
using predict.nlme
with
R version 2.5.1 (2007-06-27)
. nlme: Linear  and Nonlinear Mixed Effects Models. R package version
   3.1-83.

I have looked in help, and I can do it for lm and glm models, and I
can generate simple predictions for lme models with various levels --
I am familiar with the basics.

Is there a way to get prediction intervals for gls models? My "best"
model uses varPower(), so I am reluctant to fall back on lm predictions.

Thank you,

Hank


Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From amicogodzilla at bruttocarattere.org  Wed Jul 25 09:21:30 2007
From: amicogodzilla at bruttocarattere.org (Manuele Pesenti)
Date: Wed, 25 Jul 2007 09:21:30 +0200
Subject: [R] values from a linear model
In-Reply-To: <200707241202.58668.amicogodzilla@bruttocarattere.org>
References: <200707241202.58668.amicogodzilla@bruttocarattere.org>
Message-ID: <200707250921.30455.amicogodzilla@bruttocarattere.org>

On Tuesday 24 July 2007 12:02:58 Manuele Pesenti wrote:
> Dear R users,
> how can I extrapolate values listed in the summary of an lm model but not
> directly available between object values such as the the standard errors of
> the calculated parameters?

thank you very much for all interesting answer
:)

	Manuele



-- 
Manuele Pesenti
	manuele a inventati.org
	amicogodzilla a jabber.linux.it
	http://mpesenti.polito.it


From willemvervoort at gmail.com  Wed Jul 25 09:30:50 2007
From: willemvervoort at gmail.com (willem vervoort)
Date: Wed, 25 Jul 2007 09:30:50 +0200
Subject: [R] how to use "replace" for efficiency
Message-ID: <dcb0ed000707250030n5c907964he219ec03f77a0277@mail.gmail.com>

Hi

I think I have been struggling to use replace correctly, I usually
work my way around this using a loop, but I think this is in fact
inefficient.

I have a dataset with runoff from three plots and associated rainfall.
However either the datarecording was sloppy, or the rainfall very
patchy. So I am trying to remove data from my dataset for which the
runoff is larger than the rainfall on the same day. (Don't worry, the
rainfall data is in fact the rainfall associated to the three days
around the runoff event). On days that no runoff was recorded the data
includes "NA"

The dataset is called wheat2
I use the following:
for (i in 1:nrow(wheat2)) {
# Insert NA's if runoff data > rainfall data
    if (is.na(wheat2[i,5:7])==FALSE && any(wheat2[i,5:7]>wheat2[i,8])) {
        wheat2[i,5:7] <- NA
        }
    }

I tried this:
wheat1 <- replace(wheat2[,5:7],is.na(wheat2[i,5:7])==FALSE &&
any(wheat2[i,5:7]>wheat2[i,8]),NA)
wheat3 <- cbind(wheat2[1:4],wheat1,wheat2[,8])

> wheat3[5539,] # the culprit row
DateRain      Rain  StartDate  EndDate  RO.A RO.B RO.C filtered.Rain
5539 28-Feb-58        0 27-Feb-58 28-Feb-58        61.5        88.7
      65           0

Not sure what I am doing wrong, any help is appreciated

Willem

Here is a data sample and note the very high value for runoff (RO) and
no rainfall on 28 Feb 1958

DateRain      Rain  StartDate  EndDate  RO.A RO.B RO.C filtered.Rain
14-Feb-58 0   NA NA NA 0
15-Feb-58 0   NA NA NA 0
16-Feb-58 0   NA NA NA 0
17-Feb-58 0   NA NA NA 0
18-Feb-58 0 18-Feb-58 18-Feb-58 0 0 0 23.6
19-Feb-58 23.6   NA NA NA 23.6
20-Feb-58 0   NA NA NA 23.6
21-Feb-58 0   NA NA NA 0
22-Feb-58 0   NA NA NA 0
23-Feb-58 0   NA NA NA 0
24-Feb-58 0   NA NA NA 0
25-Feb-58 0   NA NA NA 0
26-Feb-58 0   NA NA NA 0
27-Feb-58 0   NA NA NA 0
28-Feb-58 0 27-Feb-58 28-Feb-58 61.5 88.7 65 0
01-Mar-58 0   NA NA NA 0
02-Mar-58 0   NA NA NA 0
03-Mar-58 0   NA NA NA 0
04-Mar-58 0   NA NA NA 1.5
05-Mar-58 1.5   NA NA NA 1.5
06-Mar-58 0   NA NA NA 1.5
07-Mar-58 0   NA NA NA 0.5
08-Mar-58 0.5   NA NA NA 0.5
09-Mar-58 0   NA NA NA 7.6
10-Mar-58 7.1   NA NA NA 9.1
11-Mar-58 2   NA NA NA 57.4
12-Mar-58 48.3 09-Mar-58 12-Mar-58 0.1 1.5 0 51.1
13-Mar-58 0.8   NA NA NA 49.9
14-Mar-58 0.8   NA NA NA 1.6


From felix at nfrac.org  Wed Jul 25 10:04:26 2007
From: felix at nfrac.org (Felix Andrews)
Date: Wed, 25 Jul 2007 18:04:26 +1000
Subject: [R] how to use "replace" for efficiency
In-Reply-To: <dcb0ed000707250030n5c907964he219ec03f77a0277@mail.gmail.com>
References: <dcb0ed000707250030n5c907964he219ec03f77a0277@mail.gmail.com>
Message-ID: <94730b8a0707250104y10031e09s63706cf744c5933a@mail.gmail.com>

You don't need to use 'replace', just use indexing in the assignment:

isCrap <- apply((wheat2[,5:7] > wheat2[,8]), 1, any)
wheat2[isCrap,5:7] <- NA

Felix

On 7/25/07, willem vervoort <willemvervoort at gmail.com> wrote:
> Hi
>
> I think I have been struggling to use replace correctly, I usually
> work my way around this using a loop, but I think this is in fact
> inefficient.
>
> I have a dataset with runoff from three plots and associated rainfall.
> However either the datarecording was sloppy, or the rainfall very
> patchy. So I am trying to remove data from my dataset for which the
> runoff is larger than the rainfall on the same day. (Don't worry, the
> rainfall data is in fact the rainfall associated to the three days
> around the runoff event). On days that no runoff was recorded the data
> includes "NA"
>
> The dataset is called wheat2
> I use the following:
> for (i in 1:nrow(wheat2)) {
> # Insert NA's if runoff data > rainfall data
>     if (is.na(wheat2[i,5:7])==FALSE && any(wheat2[i,5:7]>wheat2[i,8])) {
>         wheat2[i,5:7] <- NA
>         }
>     }
>
> I tried this:
> wheat1 <- replace(wheat2[,5:7],is.na(wheat2[i,5:7])==FALSE &&
> any(wheat2[i,5:7]>wheat2[i,8]),NA)
> wheat3 <- cbind(wheat2[1:4],wheat1,wheat2[,8])
>
> > wheat3[5539,] # the culprit row
> DateRain      Rain  StartDate  EndDate  RO.A RO.B RO.C filtered.Rain
> 5539 28-Feb-58        0 27-Feb-58 28-Feb-58        61.5        88.7
>       65           0
>
> Not sure what I am doing wrong, any help is appreciated
>
> Willem
>
> Here is a data sample and note the very high value for runoff (RO) and
> no rainfall on 28 Feb 1958
>
> DateRain      Rain  StartDate  EndDate  RO.A RO.B RO.C filtered.Rain
> 14-Feb-58 0   NA NA NA 0
> 15-Feb-58 0   NA NA NA 0
> 16-Feb-58 0   NA NA NA 0
> 17-Feb-58 0   NA NA NA 0
> 18-Feb-58 0 18-Feb-58 18-Feb-58 0 0 0 23.6
> 19-Feb-58 23.6   NA NA NA 23.6
> 20-Feb-58 0   NA NA NA 23.6
> 21-Feb-58 0   NA NA NA 0
> 22-Feb-58 0   NA NA NA 0
> 23-Feb-58 0   NA NA NA 0
> 24-Feb-58 0   NA NA NA 0
> 25-Feb-58 0   NA NA NA 0
> 26-Feb-58 0   NA NA NA 0
> 27-Feb-58 0   NA NA NA 0
> 28-Feb-58 0 27-Feb-58 28-Feb-58 61.5 88.7 65 0
> 01-Mar-58 0   NA NA NA 0
> 02-Mar-58 0   NA NA NA 0
> 03-Mar-58 0   NA NA NA 0
> 04-Mar-58 0   NA NA NA 1.5
> 05-Mar-58 1.5   NA NA NA 1.5
> 06-Mar-58 0   NA NA NA 1.5
> 07-Mar-58 0   NA NA NA 0.5
> 08-Mar-58 0.5   NA NA NA 0.5
> 09-Mar-58 0   NA NA NA 7.6
> 10-Mar-58 7.1   NA NA NA 9.1
> 11-Mar-58 2   NA NA NA 57.4
> 12-Mar-58 48.3 09-Mar-58 12-Mar-58 0.1 1.5 0 51.1
> 13-Mar-58 0.8   NA NA NA 49.9
> 14-Mar-58 0.8   NA NA NA 1.6
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Felix Andrews / ???
PhD candidate
Integrated Catchment Assessment and Management Centre
The Fenner School of Environment and Society
The Australian National University (Building 48A), ACT 0200
Beijing Bag, Locked Bag 40, Kingston ACT 2604
http://www.neurofractal.org/felix/
voice:+86_1051404394 (in China)
mobile:+86_13522529265 (in China)
mobile:+61_410400963 (in Australia)
xmpp:foolish.android at gmail.com
3358 543D AAC6 22C2 D336  80D9 360B 72DD 3E4C F5D8


From Wolfgang.Viechtbauer at STAT.unimaas.nl  Wed Jul 25 10:12:32 2007
From: Wolfgang.Viechtbauer at STAT.unimaas.nl (Viechtbauer Wolfgang (STAT))
Date: Wed, 25 Jul 2007 10:12:32 +0200
Subject: [R] Regarding Bivariate normal distribution.
In-Reply-To: <d4c57560707242357hedb8418h2acb01aca29f848b@mail.gmail.com>
Message-ID: <329A68716B57D54E8D39FD3F8A4A84DF057D5FD8@um-mail0136.unimaas.nl>

No, x and y are not unique. In fact, there is an infinite number of x and y pairs that are roots to the equation P[X<x, Y<y] = 0.05.

-- 
Wolfgang Viechtbauer 
?Department of Methodology and Statistics 
?University of Maastricht, The Netherlands 
?http://www.wvbauer.com/ 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Arun Kumar Saha
Sent: Wednesday, July 25, 2007 08:58
To: r-help at stat.math.ethz.ch
Subject: [R] Regarding Bivariate normal distribution.


Dear all R gurus,

My question is related to statistics rather directly to R. Suppose
(X,Y) has a bivariate normal distrubution. I want to find two values of X and Y say x, and y respectively, such that:

P[X<x, Y<y] = 0.05

My questions are :

1. Can x and y be uniquely found?
2. If it is, how I can find them using R

Your help will be highly appreciated.

Thanks and regards,

______________________________________________
R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gyadav at ccilindia.co.in  Wed Jul 25 11:04:51 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Wed, 25 Jul 2007 14:34:51 +0530
Subject: [R] Regarding Bivariate normal distribution.
In-Reply-To: <d4c57560707242357hedb8418h2acb01aca29f848b@mail.gmail.com>
Message-ID: <OFD9046B1F.CC73677C-ON65257323.0031CB33-65257323.0031E164@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/4112d3c7/attachment.pl 

From Thierry.ONKELINX at inbo.be  Wed Jul 25 11:04:43 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 25 Jul 2007 11:04:43 +0200
Subject: [R] Strange warning in summary.lm
In-Reply-To: <2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>	<469F2325.5010205@statistik.uni-dortmund.de><2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be><469F4CE9.2050709@biostat.ku.dk>
	<2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be>
Message-ID: <2E9C414912813E4EB981326983E0A10403585E2B@inboexch.inbo.be>

Dear Peter, Uwe and Brian,

I've found some more problems with options(OutDec = ",").

1) as.numeric yields NA where it shouldn't

> z <- c("12", "12,34", "12.34")
> options(OutDec = ",")
> as.numeric(z)
[1] 12,00    NA 12,34
Warning message:
NAs introduced by coercion in: as.double.default(z) 

# should result in c(12, 12.34, NA)

> options(OutDec = ".")
> as.numeric(z)
[1] 12.00    NA 12.34
Warning message:
NAs introduced by coercion in: as.double.default(z) 


2) anova yields the same warning as summary

> x <- runif(100)
> y <- rnorm(100)
> options(OutDec = ",")
> summary(lm(y~x))

Call:
lm(formula = y ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-2,81744 -0,61680  0,02107  0,66309  2,20599 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) -0,073531   0,195880  -0,375    0,708
x            0,007519   0,318159   0,024    0,981

Residual standard error: 0,9795 on 98 degrees of freedom
Multiple R-Squared: 5.699e-06,  Adjusted R-squared: -0.0102 
F-statistic: 0.0005585 on 1 and 98 DF,  p-value: 0,9812 

Warning message:
NAs introduced by coercion in: as.double.default(Cf[okP]) 
> anova(lm(y~x))
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value Pr(>F)
x          1  0,001   0,001   6e-04 0,9812
Residuals 98 94,031   0,960               
Warning message:
NAs introduced by coercion in: as.double.default(Cf[okP]) 

Cheers,

Thierry


----------------------------------------------------------------------------
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens ONKELINX, Thierry
> Verzonden: donderdag 19 juli 2007 13:56
> Aan: Peter Dalgaard
> CC: r-help op stat.math.ethz.ch; Uwe Ligges
> Onderwerp: Re: [R] Strange warning in summary.lm
> 
> Dear Peter,
> 
> Here's an example. Notice the warning in the last two lines 
> of the summary with options(OutDec = ","). It's not present 
> with options(OutDec = ".").
> 
> Cheers,
> 
> Thierry
> 
> > x <- runif(100)
> > y <- rnorm(100)
> > options(OutDec = ",")
> > summary(lm(y~x))
> 
> Call:
> lm(formula = y ~ x)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max 
> -2,389749 -0,607002  0,006969  0,689535  1,713197 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  0,03397    0,17774   0,191    0,849
> x           -0,09219    0,29518  -0,312    0,755
> 
> Residual standard error: 0,868 on 98 degrees of freedom 
> Multiple R-Squared: 0.0009943,  Adjusted R-squared: -0.0092
> F-statistic: 0.09754 on 1 and 98 DF,  p-value: 0,7555 
> 
> Warning message:
> NAs introduced by coercion in: as.double.default(Cf[okP]) 
> > options(OutDec = ".")
> > summary(lm(y~x))
> 
> Call:
> lm(formula = y ~ x)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max 
> -2.389749 -0.607002  0.006969  0.689535  1.713197 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)  0.03397    0.17774   0.191    0.849
> x           -0.09219    0.29518  -0.312    0.755
> 
> Residual standard error: 0.868 on 98 degrees of freedom 
> Multiple R-Squared: 0.0009943,  Adjusted R-squared: -0.0092
> F-statistic: 0.09754 on 1 and 98 DF,  p-value: 0.7555 
> 
> 
> --------------------------------------------------------------
> --------------
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute 
> for Nature and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section 
> biometrics, methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx op inbo.be
> www.inbo.be 
> 
> Do not put your faith in what statistics say until you have 
> carefully considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate 
> dissection of uncertainties, a surgery of suppositions. ~M.J.Moroney
> 
>  
> 
> > -----Oorspronkelijk bericht-----
> > Van: Peter Dalgaard [mailto:P.Dalgaard op biostat.ku.dk] 
> > Verzonden: donderdag 19 juli 2007 13:37
> > Aan: ONKELINX, Thierry
> > CC: Uwe Ligges; r-help op stat.math.ethz.ch
> > Onderwerp: Re: [R] Strange warning in summary.lm
> > 
> > ONKELINX, Thierry wrote:
> > > The problem also exists in a clean workspace. But I've found the 
> > > troublemaker. I had set options(OutDec = ","). Resetting this to 
> > > options(OutDec = ".") solved the problem.
> > >
> > > Thanks,
> > >
> > > Thierry
> > >   
> > Oups. That sounds like there's a bug somewhere. Can you cook 
> > up a minimal example which shows the behaviour?
> > 
> > -- 
> >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
> > (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard op biostat.ku.dk)                  FAX: 
> > (+45) 35327907
> > 
> > 
> >
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From xkneifl at mendelu.cz  Wed Jul 25 11:12:14 2007
From: xkneifl at mendelu.cz (Michal Kneifl)
Date: Wed, 25 Jul 2007 11:12:14 +0200
Subject: [R] A problem with anova()
Message-ID: <000801c7ce9b$e460cb70$a64bb2c3@Mistr1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/089cbc17/attachment.pl 

From Thierry.ONKELINX at inbo.be  Wed Jul 25 11:37:44 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 25 Jul 2007 11:37:44 +0200
Subject: [R] Ggplot2 equivalent of axis and problem with log scale
Message-ID: <2E9C414912813E4EB981326983E0A10403585E3B@inboexch.inbo.be>

Dear useRs,

Recently I've discorved ggplot2 and I must say that I really like it,
although the documentation still is a working in progress.

My first question: How can I change the position of the labels and the
text of the labels? With a basic plot I would use axis(2, at =
position.of.the.ticks, labels = text.at.the.ticks). Could someone
provide me with an example of how to do this with ggplot2?

The second question is probably a little bug. If I plot the y-axis in
log10 scale then geom_errorbar still plot the values in the original
scale. See the example below. The second plot is what I would suspect
when plotting the first graph.

library(ggplot2)
df <- data.frame(x = rep(1:10, 10), y = rnorm(100))
df$y <- 10 ^ (df$x + df$y)
df <- cbind(df, predict(lm(I(log10(y)) ~ x, data = df), interval = "c"))
df[, 3:5] <- 10 ^ df[, 3:5]

ggplot(data = df, aes(x = x, y = y)) + geom_point() + scale_y_log10() +
geom_line(aes(y = fit)) + geom_errorbar(aes(min = lwr, max = upr))

ggplot(data = df, aes(x = x, y = y)) + geom_point() + scale_y_log10() +
geom_line(aes(y = fit)) + geom_errorbar(aes(min = log10(lwr), max =
log10(upr)))

Thanks,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney


From maechler at stat.math.ethz.ch  Wed Jul 25 11:51:02 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 25 Jul 2007 11:51:02 +0200
Subject: [R] options(OutDec) etc {was "Strange warning in summary.lm"}
In-Reply-To: <2E9C414912813E4EB981326983E0A10403585E2B@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>
	<469F2325.5010205@statistik.uni-dortmund.de>
	<2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be>
	<469F4CE9.2050709@biostat.ku.dk>
	<2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be>
	<2E9C414912813E4EB981326983E0A10403585E2B@inboexch.inbo.be>
Message-ID: <18087.7430.390653.134755@stat.math.ethz.ch>

Without going into your details,
I think  options() should NEVER influence what as.numeric() does
(which I think you are indirectly suggesting it should).

In my eyes, using a decimal comma instead of decimal point in
scientific computing is an abomination in itself.

Providing an  option for  *output* is one thing,
but having it influence basic "R engine" functions like
as.numeric(), format(), ...
is an absolute NO!_NO!  for me.
So I am strongly opposed to an 'InDec' option as was mentioned
earlier in this thread.

We could consider adding a 'dec' (or 'decimal.sep')
*argument* to some R functions,
but such an argument must default to "."  rather than yet
another option.

R should remain as *functional* as possible.
==> options() should be used sparingly.

They should only influence printed output at most, not
computations per se. 
Of course I know that this is not strictly possible since the
computations can use textConnection() etc..

Martin Maechler, ETH Zurich
  

>>>>> "OT" == ONKELINX, Thierry <Thierry.ONKELINX at inbo.be>
>>>>>     on Wed, 25 Jul 2007 11:04:43 +0200 writes:

    OT> Dear Peter, Uwe and Brian,
    OT> I've found some more problems with options(OutDec = ",").

    OT> 1) as.numeric yields NA where it shouldn't

    >> z <- c("12", "12,34", "12.34")
    >> options(OutDec = ",")
    >> as.numeric(z)
    OT> [1] 12,00    NA 12,34
    OT> Warning message:
    OT> NAs introduced by coercion in: as.double.default(z) 

    OT> # should result in c(12, 12.34, NA)

    >> options(OutDec = ".")
    >> as.numeric(z)
    OT> [1] 12.00    NA 12.34
    OT> Warning message:
    OT> NAs introduced by coercion in: as.double.default(z) 


    OT> 2) anova yields the same warning as summary

    >> x <- runif(100)
    >> y <- rnorm(100)
    >> options(OutDec = ",")
    >> summary(lm(y~x))

    OT> Call:
    OT> lm(formula = y ~ x)

    OT> Residuals:
    OT> Min       1Q   Median       3Q      Max 
    OT> -2,81744 -0,61680  0,02107  0,66309  2,20599 

    OT> Coefficients:
    OT> Estimate Std. Error t value Pr(>|t|)
    OT> (Intercept) -0,073531   0,195880  -0,375    0,708
    OT> x            0,007519   0,318159   0,024    0,981

    OT> Residual standard error: 0,9795 on 98 degrees of freedom
    OT> Multiple R-Squared: 5.699e-06,  Adjusted R-squared: -0.0102 
    OT> F-statistic: 0.0005585 on 1 and 98 DF,  p-value: 0,9812 

    OT> Warning message:
    OT> NAs introduced by coercion in: as.double.default(Cf[okP]) 
    >> anova(lm(y~x))
    OT> Analysis of Variance Table

    OT> Response: y
    OT> Df Sum Sq Mean Sq F value Pr(>F)
    OT> x          1  0,001   0,001   6e-04 0,9812
    OT> Residuals 98 94,031   0,960               
    OT> Warning message:
    OT> NAs introduced by coercion in: as.double.default(Cf[okP]) 

    OT> Cheers,

    OT> Thierry


    OT> ----------------------------------------------------------------------------
    OT> ir. Thierry Onkelinx
    OT> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
    OT> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics, methodology and quality assurance
    OT> Gaverstraat 4
    OT> 9500 Geraardsbergen
    OT> Belgium
    OT> tel. + 32 54/436 185
    OT> Thierry.Onkelinx at inbo.be
    OT> www.inbo.be 

    OT> Do not put your faith in what statistics say until you have carefully considered what they do not say.  ~William W. Watt
    OT> A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions. ~M.J.Moroney

 

    >> -----Oorspronkelijk bericht-----
    >> Van: r-help-bounces at stat.math.ethz.ch 
    >> [mailto:r-help-bounces at stat.math.ethz.ch] Namens ONKELINX, Thierry
    >> Verzonden: donderdag 19 juli 2007 13:56
    >> Aan: Peter Dalgaard
    >> CC: r-help at stat.math.ethz.ch; Uwe Ligges
    >> Onderwerp: Re: [R] Strange warning in summary.lm
    >> 
    >> Dear Peter,
    >> 
    >> Here's an example. Notice the warning in the last two lines 
    >> of the summary with options(OutDec = ","). It's not present 
    >> with options(OutDec = ".").
    >> 
    >> Cheers,
    >> 
    >> Thierry
    >> 
    >> > x <- runif(100)
    >> > y <- rnorm(100)
    >> > options(OutDec = ",")
    >> > summary(lm(y~x))
    >> 
    >> Call:
    >> lm(formula = y ~ x)
    >> 
    >> Residuals:
    >> Min        1Q    Median        3Q       Max 
    >> -2,389749 -0,607002  0,006969  0,689535  1,713197 
    >> 
    >> Coefficients:
    >> Estimate Std. Error t value Pr(>|t|)
    >> (Intercept)  0,03397    0,17774   0,191    0,849
    >> x           -0,09219    0,29518  -0,312    0,755
    >> 
    >> Residual standard error: 0,868 on 98 degrees of freedom 
    >> Multiple R-Squared: 0.0009943,  Adjusted R-squared: -0.0092
    >> F-statistic: 0.09754 on 1 and 98 DF,  p-value: 0,7555 
    >> 
    >> Warning message:
    >> NAs introduced by coercion in: as.double.default(Cf[okP]) 
    >> > options(OutDec = ".")
    >> > summary(lm(y~x))
    >> 
    >> Call:
    >> lm(formula = y ~ x)
    >> 
    >> Residuals:
    >> Min        1Q    Median        3Q       Max 
    >> -2.389749 -0.607002  0.006969  0.689535  1.713197 
    >> 
    >> Coefficients:
    >> Estimate Std. Error t value Pr(>|t|)
    >> (Intercept)  0.03397    0.17774   0.191    0.849
    >> x           -0.09219    0.29518  -0.312    0.755
    >> 
    >> Residual standard error: 0.868 on 98 degrees of freedom 
    >> Multiple R-Squared: 0.0009943,  Adjusted R-squared: -0.0092
    >> F-statistic: 0.09754 on 1 and 98 DF,  p-value: 0.7555 
    >> 
    >> 
    >> --------------------------------------------------------------
    >> --------------
    >> ir. Thierry Onkelinx
    >> Instituut voor natuur- en bosonderzoek / Research Institute 
    >> for Nature and Forest
    >> Cel biometrie, methodologie en kwaliteitszorg / Section 
    >> biometrics, methodology and quality assurance
    >> Gaverstraat 4
    >> 9500 Geraardsbergen
    >> Belgium
    >> tel. + 32 54/436 185
    >> Thierry.Onkelinx at inbo.be
    >> www.inbo.be 
    >> 
    >> Do not put your faith in what statistics say until you have 
    >> carefully considered what they do not say.  ~William W. Watt
    >> A statistical analysis, properly conducted, is a delicate 
    >> dissection of uncertainties, a surgery of suppositions. ~M.J.Moroney
    >> 
    >> 
    >> 
    >> > -----Oorspronkelijk bericht-----
    >> > Van: Peter Dalgaard [mailto:P.Dalgaard at biostat.ku.dk] 
    >> > Verzonden: donderdag 19 juli 2007 13:37
    >> > Aan: ONKELINX, Thierry
    >> > CC: Uwe Ligges; r-help at stat.math.ethz.ch
    >> > Onderwerp: Re: [R] Strange warning in summary.lm
    >> > 
    >> > ONKELINX, Thierry wrote:
    >> > > The problem also exists in a clean workspace. But I've found the 
    >> > > troublemaker. I had set options(OutDec = ","). Resetting this to 
    >> > > options(OutDec = ".") solved the problem.
    >> > >
    >> > > Thanks,
    >> > >
    >> > > Thierry
    >> > >   
    >> > Oups. That sounds like there's a bug somewhere. Can you cook 
    >> > up a minimal example which shows the behaviour?
    >> > 
    >> > -- 
    >> >    O__  ---- Peter Dalgaard             ?ster Farimagsgade 5, Entr.B
    >> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
    >> >  (*) \(*) -- University of Copenhagen   Denmark          Ph:  
    >> > (+45) 35327918
    >> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
    >> > (+45) 35327907
    >> > 
    >> > 
    >> >
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide 
    >> http://www.R-project.org/posting-guide.html
    >> and provide commented, minimal, self-contained, reproducible code.
    >> 

    OT> ______________________________________________
    OT> R-help at stat.math.ethz.ch mailing list
    OT> https://stat.ethz.ch/mailman/listinfo/r-help
    OT> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    OT> and provide commented, minimal, self-contained, reproducible code.


From amnakhan493 at gmail.com  Wed Jul 25 12:17:54 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Wed, 25 Jul 2007 15:17:54 +0500
Subject: [R] plots
Message-ID: <3ffd3bb60707250317x2b281487k33c45c6cb743942f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/9456d1cf/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 25 12:20:11 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jul 2007 11:20:11 +0100 (BST)
Subject: [R] Strange warning in summary.lm
In-Reply-To: <2E9C414912813E4EB981326983E0A10403585E2B@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be>
	<469F2325.5010205@statistik.uni-dortmund.de><2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be><469F4CE9.2050709@biostat.ku.dk>
	<2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be>
	<2E9C414912813E4EB981326983E0A10403585E2B@inboexch.inbo.be>
Message-ID: <Pine.LNX.4.64.0707251116280.14381@gannet.stats.ox.ac.uk>

On Wed, 25 Jul 2007, ONKELINX, Thierry wrote:

> Dear Peter, Uwe and Brian,
>
> I've found some more problems with options(OutDec = ",").
>
> 1) as.numeric yields NA where it shouldn't

It should: where does it say otherwise?  OutDec affects output, only.

>
>> z <- c("12", "12,34", "12.34")
>> options(OutDec = ",")
>> as.numeric(z)
> [1] 12,00    NA 12,34
> Warning message:
> NAs introduced by coercion in: as.double.default(z)
>
> # should result in c(12, 12.34, NA)
>
>> options(OutDec = ".")
>> as.numeric(z)
> [1] 12.00    NA 12.34
> Warning message:
> NAs introduced by coercion in: as.double.default(z)
>
>
> 2) anova yields the same warning as summary

It does not in current R: try any fairly recent version of R-devel.

Please don't keep reporting a problem we have already fixed, as the FAQ 
and the posting guide explicitly ask of you.

[...]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From kamauallan at yahoo.com  Wed Jul 25 12:30:25 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Wed, 25 Jul 2007 03:30:25 -0700 (PDT)
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
Message-ID: <747597.66177.qm@web53502.mail.re2.yahoo.com>

A subset of the data looks as follows

> df[1:10,14:20]
   PR10 PR11 PR12 PR13 PR14 PR15 PR16
1     V    T    I    K    V    G    D
2     V    S    I    K    V    G    G
3     V    T    I    R    V    G    G
4     V    S    I    K    I    G    G
5     V    S    I    K    V    G    G
6     V    S    I    R    V    G    G
7     V    T    I    K    I    G    G
8     V    S    I    K    V    E    G
9     V    S    I    K    V    G    G
10    V    S    I    K    V    G    G

The result I would like is as follows

PR10        PR11          PR12   ...
[V:10]    [S:7,T:3]    [I:10]

The result can be in a matrix or a vector and each variablename, value and frequency should be accessible so as to be used for comparisons with another dataset later.
The frequency can be a count or a percentage.


Allan.


----- Original Message ----
From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
To: Allan Kamau <kamauallan at yahoo.com>
Cc: r-help at stat.math.ethz.ch
Sent: Tuesday, July 24, 2007 10:21:51 PM
Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.

The name of the table should give you the "value". And if you have a 
matrix, you just need to convert it into a vector first.

 > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ], nc=3 )
 > m
      [,1] [,2] [,3]
[1,] "A"  "C"  "B"
[2,] "B"  "D"  "C"
[3,] "C"  "E"  "D"
 > tb <- table( as.vector(m) )
 > tb

A B C D E
1 2 3 2 1
 > paste( names(tb), ":", tb, sep="" )
[1] "A:1" "B:2" "C:3" "D:2" "E:1"

If this is not what you want, then please give a simple example.

Regards, Adai



Allan Kamau wrote:
> Hi all,
> If the question below as been answered before I
> apologize for the posting.
> I would like to get the frequencies of occurrence of
> all values in a given variable in a multivariate
> dataset. In short for each variable (or field) a
> summary of values contained with in a value:frequency
> pair, there can be many such pairs for a given
> variable. I would like to do the same for several such
> variables.
> I have used table() but am unable to extract the
> individual value and frequency values.
> Please advise.
> 
> Allan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From wl2776 at gmail.com  Wed Jul 25 12:35:32 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Wed, 25 Jul 2007 03:35:32 -0700 (PDT)
Subject: [R] plots
In-Reply-To: <3ffd3bb60707250317x2b281487k33c45c6cb743942f@mail.gmail.com>
References: <3ffd3bb60707250317x2b281487k33c45c6cb743942f@mail.gmail.com>
Message-ID: <11780013.post@talk.nabble.com>



amna khan wrote:
> 
> I did not find any function of graph which plot one variable on x-axis and
> 2
> or more than 2 variables on y-axis.
> 

You can use xyplot() from the package lattice.
library(lattice)
xyplot(y1+y2+y3~x)

I suspect, the problem is, that plot() erases everything that was plotted
earlier and establishes a new coordinate system in the plotting window.

In case of basic graphics, you can set par(new=TRUE) and call plot() several
times. 

points(), lines() and other functions from the basic graphics will add new
curves to the existing plot.

Initially, you must set axis ranges large enough to fit everything you want
to plot.
-- 
View this message in context: http://www.nabble.com/plots-tf4141246.html#a11780013
Sent from the R help mailing list archive at Nabble.com.


From Thierry.ONKELINX at inbo.be  Wed Jul 25 12:37:30 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 25 Jul 2007 12:37:30 +0200
Subject: [R] Strange warning in summary.lm
In-Reply-To: <Pine.LNX.4.64.0707251116280.14381@gannet.stats.ox.ac.uk>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be><469F2325.5010205@statistik.uni-dortmund.de><2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be><469F4CE9.2050709@biostat.ku.dk><2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be><2E9C414912813E4EB981326983E0A10403585E2B@inboexch.inbo.be>
	<Pine.LNX.4.64.0707251116280.14381@gannet.stats.ox.ac.uk>
Message-ID: <2E9C414912813E4EB981326983E0A10403585E5E@inboexch.inbo.be>

> -----Oorspronkelijk bericht-----
> Van: Prof Brian Ripley [mailto:ripley op stats.ox.ac.uk] 
> Verzonden: woensdag 25 juli 2007 12:20
> Aan: ONKELINX, Thierry
> CC: r-help op stat.math.ethz.ch
> Onderwerp: Re: [R] Strange warning in summary.lm
> 
> On Wed, 25 Jul 2007, ONKELINX, Thierry wrote:
> 
> > Dear Peter, Uwe and Brian,
> >
> > I've found some more problems with options(OutDec = ",").
> >
> > 1) as.numeric yields NA where it shouldn't
> 
> It should: where does it say otherwise?  OutDec affects output, only.

I was doing something like

> options(OutDec = ",")
> df <- data.frame(var = rep(1:8/4, 10), x = rnorm(80))
> df.a <- aggregate(df$x, by = list(var = df$var), FUN = sum)
> as.numeric(df.a$var)
[1] NA NA NA  1 NA NA NA  2
Warning message:
NAs introduced by coercion in: as.double.default(df.a$var) 

because I needed df.a$var as numeric again.

> > 2) anova yields the same warning as summary
> 
> It does not in current R: try any fairly recent version of R-devel.
> 
> Please don't keep reporting a problem we have already fixed, 
> as the FAQ and the posting guide explicitly ask of you.

Sorry about that. I wasn't aware that the error was in a fixed
subroutine that is used by summary and anova.
I'm hestating to use a R-devel version because I haven't tried
installing R from a tar.gz (I'm one of those lazy windows users that
prefer the .exe version). For now I think I'll return to the good old
option(OutDec = ".").
> 
> [...]
> 
> 
> -- 
> Brian D. Ripley,                  ripley op stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From lzarauz at pas.azti.es  Wed Jul 25 12:43:01 2007
From: lzarauz at pas.azti.es (Lucia Zarauz)
Date: Wed, 25 Jul 2007 12:43:01 +0200
Subject: [R] plotting gam models
References: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>
	<da79af330707240436i404b7d0t2572cda2315535b2@mail.gmail.com> 
Message-ID: <FCEE957ABD363A449A2483804ED0EDE3784D1A@psrcorreo.azti.local>


Hi again,

I have found a post in R.help archives made from someone who had the same problem when exporting the smooth function estimate got from a GAM, to plot it into another graphic software (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/23136.html)

However, I haven't found any reply to this post. 

She suggested the function preplot() and I have hound that function in gam package, but not in mgcv package. Is it possible to do this in mgcv package or do I have to move to gam package?

Thank you very much

Luc?a zarauz


_____________________________________________ 
AZTI - TecnaliaLucia Zarauz 
AZTI - Tecnalia / Unidad de Investigaci?n Marina
Herrera kaia portualdea z/g
20110 Pasaia (Gipuzkoa)
Tel: 943 004 800 - Fax: 943 004 801
e-mail: lzarauz at pas.azti.es
www.azti.es ; www.tecnalia.info	 

_____________________________________________ 

  

**************** LEGE OHARRA **************** AVISO LEGAL **************** DISCLAIMER ****************


Mezu hau pertsonala eta isilpekoa da eta baimenik gabeko erabilera debekatua dago legalki. Jasotzailea ez bazara ezabatu mezua, bidali eta kontserbatu gabe. 


Este mensaje es personal y confidencial y su uso no autorizado est? prohibido legalmente. Si usted no es el destinatario, proceda a borrarlo, sin reenviarlo ni conservarlo.


This message is personal and confidential, unauthorised use is legally prohibited. If you are not the intended recipient, delete it without resending or backing it.

-----Mensaje original-----
De: Lucia Zarauz 
Enviado el: martes, 24 de julio de 2007 13:53
Para: 'Henrique Dallazuanna'
CC: r-help at stat.math.ethz.ch
Asunto: RE: [R] plotting gam models

Hi Henrique,

Thank you for your suggestion. 

Actually, I have already tried it, but I am confused because the plot I get is not the same as the output of plot(model) or plot.gam(model). The yaxis is different

On the other hand, if I build a more complex model, as for example:

model<- gam(x ~ s(lat,long) + s(temperature))

I would like to extract the information to build the effects for each explanatory factor (one graph for s(lat,long) and another for s(temperature)), as R does when you use 'plot(model)' and you press return for subsequent pages.

My final aim is to plot the influence of s(lat,long) as a contourplot superposed on a geographical map. Maybe there is an easier way to do it...

Thank you very much

luc?a



_____________________________________________ 
Lucia Zarauz 
AZTI - Tecnalia / Unidad de Investigaci?n Marina
Herrera kaia portualdea z/g
20110 Pasaia (Gipuzkoa)
Tel: 943 004 800 - Fax: 943 004 801
e-mail: lzarauz at pas.azti.es
www.azti.es ; www.tecnalia.info
_____________________________________________ 
? 
**************** LEGE OHARRA **************** AVISO LEGAL **************** DISCLAIMER ****************
Mezu hau pertsonala eta isilpekoa da eta baimenik gabeko erabilera debekatua dago legalki. Jasotzailea ez bazara ezabatu mezua, bidali eta kontserbatu gabe. 
Este mensaje es personal y confidencial y su uso no autorizado est? prohibido legalmente. Si usted no es el destinatario, proceda a borrarlo, sin reenviarlo ni conservarlo.
This message is personal and confidential, unauthorised use is legally prohibited. If you are not the intended recipient, delete it without resending or backing it.

________________________________________
De: Henrique Dallazuanna [mailto:wwwhsd at gmail.com] 
Enviado el: martes, 24 de julio de 2007 13:36
Para: Lucia Zarauz
CC: r-help at stat.math.ethz.ch
Asunto: Re: [R] plotting gam models

see ?predict.gam

-- 
Henrique Dallazuanna
Curitiba-Paran?-Brasil
25? 25' 40" S 49? 16' 22" O 
On 24/07/07, Lucia Zarauz <lzarauz at pas.azti.es> wrote:

Hi everybody,

I am working with gams and I have found some questions when plotting gams models.

I am using mgcv, and my model looks something like this:

model<- gam(x ~ s(lat,long))

I can plot the output of the model using plot(model) or plot.gam(model) and I get a surface plot.

That is ok, but what I want to do now is to extract the data used to perform the surface plot. Like that I would be able to superpose them to a geographical map, and to plot these data using other programs. 

Thank you very much in advance

Luc?a
_____________________________________________
Lucia Zarauz
AZTI - Tecnalia / Unidad de Investigaci?n Marina
Herrera kaia portualdea z/g
20110 Pasaia (Gipuzkoa) 
Tel: 943 004 800 - Fax: 943 004 801
e-mail: lzarauz at pas.azti.es
www.azti.es ; www.tecnalia.info
_____________________________________________

**************** LEGE OHARRA **************** AVISO LEGAL **************** DISCLAIMER ****************
Mezu hau pertsonala eta isilpekoa da eta baimenik gabeko erabilera debekatua dago legalki. Jasotzailea ez bazara ezabatu mezua, bidali eta kontserbatu gabe. 
Este mensaje es personal y confidencial y su uso no autorizado est? prohibido legalmente. Si usted no es el destinatario, proceda a borrarlo, sin reenviarlo ni conservarlo.
This message is personal and confidential, unauthorised use is legally prohibited. If you are not the intended recipient, delete it without resending or backing it. 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Wed Jul 25 12:44:59 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Wed, 25 Jul 2007 12:44:59 +0200
Subject: [R] Odp:  plots
In-Reply-To: <3ffd3bb60707250317x2b281487k33c45c6cb743942f@mail.gmail.com>
Message-ID: <OFD288CF02.BD3ED429-ONC1257323.003ACD91-C1257323.003B0C91@precheza.cz>

Hi

r-help-bounces at stat.math.ethz.ch napsal dne 25.07.2007 12:17:54:

> Hi Sir
> 
> I did not find any function of graph which plot one variable on x-axis 
and 2
> or more than 2 variables on y-axis.

?matplot

or you can do

plot(x,y, ylim=range(all.your.y), type="n")

and add lines/points by

lines(x, one.of.your.y)
points(x, other.of.your.y)

Regards

Petr

> Moreover, how can I change the labels of L-moments diagram obtained by
> plotlmrdia(lmrdia())
> 
> Thank you
> 
> -- 
> AMINA SHAHZADI
> Department of Statistics
> GC University Lahore, Pakistan.
> Email:
> amnakhan493 at gmail.com
> amna_989 at hotmail.com
> amna_989 at yahoo.com
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marco.r.help at gmail.com  Wed Jul 25 12:56:06 2007
From: marco.r.help at gmail.com (marco.R.help marco.R.help)
Date: Wed, 25 Jul 2007 12:56:06 +0200
Subject: [R] problem with sub !
Message-ID: <605b4120707250356y4e6c07caid0dcfd66d193dd3@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/d0b08d1f/attachment.pl 

From ripley at stats.ox.ac.uk  Wed Jul 25 12:57:08 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jul 2007 11:57:08 +0100 (BST)
Subject: [R] Strange warning in summary.lm
In-Reply-To: <2E9C414912813E4EB981326983E0A10403585E5E@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A104035854D9@inboexch.inbo.be><469F2325.5010205@statistik.uni-dortmund.de><2E9C414912813E4EB981326983E0A10403585585@inboexch.inbo.be><469F4CE9.2050709@biostat.ku.dk><2E9C414912813E4EB981326983E0A10403585600@inboexch.inbo.be><2E9C414912813E4EB981326983E0A10403585E2B@inboexch.inbo.be>
	<Pine.LNX.4.64.0707251116280.14381@gannet.stats.ox.ac.uk>
	<2E9C414912813E4EB981326983E0A10403585E5E@inboexch.inbo.be>
Message-ID: <Pine.LNX.4.64.0707251156000.16731@auk.stats>

On Wed, 25 Jul 2007, ONKELINX, Thierry wrote:

>> -----Oorspronkelijk bericht-----
>> Van: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>> Verzonden: woensdag 25 juli 2007 12:20
>> Aan: ONKELINX, Thierry
>> CC: r-help at stat.math.ethz.ch
>> Onderwerp: Re: [R] Strange warning in summary.lm
>>
>> On Wed, 25 Jul 2007, ONKELINX, Thierry wrote:
>>
>>> Dear Peter, Uwe and Brian,
>>>
>>> I've found some more problems with options(OutDec = ",").
>>>
>>> 1) as.numeric yields NA where it shouldn't
>>
>> It should: where does it say otherwise?  OutDec affects output, only.
>
> I was doing something like
>
>> options(OutDec = ",")
>> df <- data.frame(var = rep(1:8/4, 10), x = rnorm(80))
>> df.a <- aggregate(df$x, by = list(var = df$var), FUN = sum)
>> as.numeric(df.a$var)
> [1] NA NA NA  1 NA NA NA  2
> Warning message:
> NAs introduced by coercion in: as.double.default(df.a$var)
>
> because I needed df.a$var as numeric again.

And as Martin maechler has pointed out, we don't (and can't because ',' is 
used for other things) support that.

>
>>> 2) anova yields the same warning as summary
>>
>> It does not in current R: try any fairly recent version of R-devel.
>>
>> Please don't keep reporting a problem we have already fixed,
>> as the FAQ and the posting guide explicitly ask of you.
>
> Sorry about that. I wasn't aware that the error was in a fixed
> subroutine that is used by summary and anova.
> I'm hestating to use a R-devel version because I haven't tried
> installing R from a tar.gz (I'm one of those lazy windows users that
> prefer the .exe version). For now I think I'll return to the good old
> option(OutDec = ".").

Binary versions are available on CRAN.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From s.wood at bath.ac.uk  Wed Jul 25 12:40:28 2007
From: s.wood at bath.ac.uk (Simon Wood)
Date: Wed, 25 Jul 2007 11:40:28 +0100
Subject: [R] plotting gam models
In-Reply-To: <FCEE957ABD363A449A2483804ED0EDE3784D1A@psrcorreo.azti.local>
References: <FCEE957ABD363A449A2483804ED0EDE3784C85@psrcorreo.azti.local>
	<da79af330707240436i404b7d0t2572cda2315535b2@mail.gmail.com>
	<FCEE957ABD363A449A2483804ED0EDE3784D1A@psrcorreo.azti.local>
Message-ID: <200707251140.28872.s.wood@bath.ac.uk>

What's wrong with the predict.gam(...,type="terms") suggestion (see 
yesterday)?

Here's example code:
## example data from ?gam
n<-400;sig<-2
x <- runif(n, 0, 1);z <- runif(n, 0, 1)
x2 <- runif(n, 0, 1);x3 <- runif(n, 0, 1)
f0 <- function(x) 2 * sin(pi * x)
f1 <- function(x) exp(2 * x)
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
f3 <- function(x) 0*x
f <- f0(x) + f1(z) + f2(x2)
e <- rnorm(n, 0, sig)
y <- f + e
b<-gam(y~s(x,z)+s(x2)+s(x3))

## make a grid of data over which to predict
zm <- xm <- seq(0,1,length=40)
pd <- data.frame(x=rep(xm,40),z=rep(zm,rep(40,40)),
                 x2=rep(.5,40^2),x3=rep(.5,40^2))
## predict, termwise
pv <- predict(b,newdata=pd,type="terms")
colnames(pv)
## plot result
contour(zm,xm,matrix(pv[,1],40,40))



 

On Wednesday 25 July 2007 11:43, Lucia Zarauz wrote:
> Hi again,
>
> I have found a post in R.help archives made from someone who had the same
> problem when exporting the smooth function estimate got from a GAM, to plot
> it into another graphic software
> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/23136.html)
>
> However, I haven't found any reply to this post.
>
> She suggested the function preplot() and I have hound that function in gam
> package, but not in mgcv package. Is it possible to do this in mgcv package
> or do I have to move to gam package?
>
> Thank you very much
>
> Luc?a zarauz
>
>
> _____________________________________________
> AZTI - TecnaliaLucia Zarauz
> AZTI - Tecnalia / Unidad de Investigaci?n Marina
> Herrera kaia portualdea z/g
> 20110 Pasaia (Gipuzkoa)
> Tel: 943 004 800 - Fax: 943 004 801
> e-mail: lzarauz at pas.azti.es
> www.azti.es ; www.tecnalia.info
>
> _____________________________________________
>
>
>
> **************** LEGE OHARRA **************** AVISO LEGAL ****************
> DISCLAIMER ****************
>
>
> Mezu hau pertsonala eta isilpekoa da eta baimenik gabeko erabilera
> debekatua dago legalki. Jasotzailea ez bazara ezabatu mezua, bidali eta
> kontserbatu gabe.
>
>
> Este mensaje es personal y confidencial y su uso no autorizado est?
> prohibido legalmente. Si usted no es el destinatario, proceda a borrarlo,
> sin reenviarlo ni conservarlo.
>
>
> This message is personal and confidential, unauthorised use is legally
> prohibited. If you are not the intended recipient, delete it without
> resending or backing it.
>
> -----Mensaje original-----
> De: Lucia Zarauz
> Enviado el: martes, 24 de julio de 2007 13:53
> Para: 'Henrique Dallazuanna'
> CC: r-help at stat.math.ethz.ch
> Asunto: RE: [R] plotting gam models
>
> Hi Henrique,
>
> Thank you for your suggestion.
>
> Actually, I have already tried it, but I am confused because the plot I get
> is not the same as the output of plot(model) or plot.gam(model). The yaxis
> is different
>
> On the other hand, if I build a more complex model, as for example:
>
> model<- gam(x ~ s(lat,long) + s(temperature))
>
> I would like to extract the information to build the effects for each
> explanatory factor (one graph for s(lat,long) and another for
> s(temperature)), as R does when you use 'plot(model)' and you press return
> for subsequent pages.
>
> My final aim is to plot the influence of s(lat,long) as a contourplot
> superposed on a geographical map. Maybe there is an easier way to do it...
>
> Thank you very much
>
> luc?a
>
>
>
> _____________________________________________
> Lucia Zarauz
> AZTI - Tecnalia / Unidad de Investigaci?n Marina
> Herrera kaia portualdea z/g
> 20110 Pasaia (Gipuzkoa)
> Tel: 943 004 800 - Fax: 943 004 801
> e-mail: lzarauz at pas.azti.es
> www.azti.es ; www.tecnalia.info
> _____________________________________________
> ?
> **************** LEGE OHARRA **************** AVISO LEGAL ****************
> DISCLAIMER **************** Mezu hau pertsonala eta isilpekoa da eta
> baimenik gabeko erabilera debekatua dago legalki. Jasotzailea ez bazara
> ezabatu mezua, bidali eta kontserbatu gabe. Este mensaje es personal y
> confidencial y su uso no autorizado est? prohibido legalmente. Si usted no
> es el destinatario, proceda a borrarlo, sin reenviarlo ni conservarlo. This
> message is personal and confidential, unauthorised use is legally
> prohibited. If you are not the intended recipient, delete it without
> resending or backing it.
>
> ________________________________________
> De: Henrique Dallazuanna [mailto:wwwhsd at gmail.com]
> Enviado el: martes, 24 de julio de 2007 13:36
> Para: Lucia Zarauz
> CC: r-help at stat.math.ethz.ch
> Asunto: Re: [R] plotting gam models
>
> see ?predict.gam

-- 
> Simon Wood, Mathematical Sciences, University of Bath, Bath, BA2 7AY UK
> +44 1225 386603  www.maths.bath.ac.uk/~sw283


From Geraldine.Lassalle at bordeaux.cemagref.fr  Wed Jul 25 13:16:57 2007
From: Geraldine.Lassalle at bordeaux.cemagref.fr (=?iso-8859-1?Q?Lassalle_G=E9raldine?=)
Date: Wed, 25 Jul 2007 13:16:57 +0200
Subject: [R] Function polr and discrete ordinal scale
Message-ID: <182D8C7FAD4DCE499BF5AD749B3AA064C7A40F@hermes.bordeaux.cemagref.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/b314e2df/attachment.pl 

From uhtrivedi208 at yahoo.co.in  Wed Jul 25 13:31:58 2007
From: uhtrivedi208 at yahoo.co.in (Urmi Trivedi)
Date: Wed, 25 Jul 2007 12:31:58 +0100 (BST)
Subject: [R] Best-fit linear model for the two matrices plotted.
Message-ID: <705269.32915.qm@web8511.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/bb4f847a/attachment.pl 

From jim at bitwrit.com.au  Wed Jul 25 13:38:36 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Wed, 25 Jul 2007 21:38:36 +1000
Subject: [R] How to add circular text for a graph with concentric circles
In-Reply-To: <OFF657BC2F.A3814195-ON85257322.006B9F78-85257322.006C12B0@worldbank.org>
References: <OFF657BC2F.A3814195-ON85257322.006B9F78-85257322.006C12B0@worldbank.org>
Message-ID: <46A7363C.2090601@bitwrit.com.au>

sparandekar at worldbank.org wrote:
> Dear R experts,
> 
> I am plotting the population of students who live in a city, and in 
> successive circular bands made of the contiguous districts that surround 
> the city. This is a stylized figure, where I specify the area of each 
> successive circle based on the cumulative population of students. I want 
> to compare two sets of concentric circles across different populations - 
> such as 'All students' and 'Private students' (those attending private 
> school) by using the same colours and the same dimension of the outer 
> circle. I have attached the .pdf file with the output, and the R code to 
> generate the first set of circles.
> 
> I would appreciate any tips about how to rotate the text label that marks 
> each concentric circle (except the central circle) to be curved around the 
> circle, located in the middle of each band, thus following the circle 
> instead of being horizontal, as I have it now.
> 
Hi Suhas,
This is kind of rough, being rejigged from an old piece of Postscript 
code that I wrote to get text in an arc. You specify the text and 
whatever else is needed as in the following example:

arctext<-function(x,center=c(0,0),radius=1,
  midangle=pi/2,stretch=1.1,cex=1,...) {
  oldcex<-par("cex")
  par(cex=cex)
  xwidth<-strwidth(x)*stretch
  startpos<-midangle+xwidth/(radius*2)
  xvec<-strsplit(x,"")[[1]]
  xwidths<-rep(mean(stretch*strwidth(xvec)),length(xvec))
  arcpos<-startpos-cumsum(xwidths)
  for(i in 1:length(arcpos))
   text(center[1]+radius*cos(arcpos[i]),center[2]+radius*sin(arcpos[i]),
    xvec[i],adj=c(0.5,0.5),srt=(arcpos[i]-midangle)*180/pi)
  par(cex=oldcex)
}

plot((1:5)
arctext("bendy as a bloody piece of spaghetti",center=c(3,3))

radius is obviously the radius of the arc,
midangle is where you want the center of the text,
stretch is how much to stretch the text to make it look nice
and cex is the expansion factor.
You will probably notice that I kludged the spacing by making
it monospaced. I'll have a try at getting proportionally spaced
text to work right when I get a chance.

Jim


From jholtman at gmail.com  Wed Jul 25 13:46:32 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 25 Jul 2007 07:46:32 -0400
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
In-Reply-To: <747597.66177.qm@web53502.mail.re2.yahoo.com>
References: <747597.66177.qm@web53502.mail.re2.yahoo.com>
Message-ID: <644e1f320707250446u3ef4f250h1571afc12253aa42@mail.gmail.com>

Is this what you want:

> x <- read.table(textConnection("  PR10 PR11 PR12 PR13 PR14 PR15 PR16
+ 1     V    T    I    K    V    G    D
+ 2     V    S    I    K    V    G    G
+ 3     V    T    I    R    V    G    G
+ 4     V    S    I    K    I    G    G
+ 5     V    S    I    K    V    G    G
+ 6     V    S    I    R    V    G    G
+ 7     V    T    I    K    I    G    G
+ 8     V    S    I    K    V    E    G
+ 9     V    S    I    K    V    G    G
+ 10    V    S    I    K    V    G    G"), header=TRUE)
> x.t <- apply(x, 2, function(.col){
+     .tab <- table(.col)
+     paste('[', paste(names(.tab), .tab, sep=":", collapse=','), ']', sep='')
+ })
>
>
> x.t
       PR10        PR11        PR12        PR13        PR14        PR15
   "[V:10]" "[S:7,T:3]"    "[I:10]" "[K:8,R:2]" "[I:2,V:8]" "[E:1,G:9]"
       PR16
"[D:1,G:9]"
>


On 7/25/07, Allan Kamau <kamauallan at yahoo.com> wrote:
> A subset of the data looks as follows
>
> > df[1:10,14:20]
>   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> 1     V    T    I    K    V    G    D
> 2     V    S    I    K    V    G    G
> 3     V    T    I    R    V    G    G
> 4     V    S    I    K    I    G    G
> 5     V    S    I    K    V    G    G
> 6     V    S    I    R    V    G    G
> 7     V    T    I    K    I    G    G
> 8     V    S    I    K    V    E    G
> 9     V    S    I    K    V    G    G
> 10    V    S    I    K    V    G    G
>
> The result I would like is as follows
>
> PR10        PR11          PR12   ...
> [V:10]    [S:7,T:3]    [I:10]
>
> The result can be in a matrix or a vector and each variablename, value and frequency should be accessible so as to be used for comparisons with another dataset later.
> The frequency can be a count or a percentage.
>
>
> Allan.
>
>
> ----- Original Message ----
> From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> To: Allan Kamau <kamauallan at yahoo.com>
> Cc: r-help at stat.math.ethz.ch
> Sent: Tuesday, July 24, 2007 10:21:51 PM
> Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
>
> The name of the table should give you the "value". And if you have a
> matrix, you just need to convert it into a vector first.
>
>  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ], nc=3 )
>  > m
>      [,1] [,2] [,3]
> [1,] "A"  "C"  "B"
> [2,] "B"  "D"  "C"
> [3,] "C"  "E"  "D"
>  > tb <- table( as.vector(m) )
>  > tb
>
> A B C D E
> 1 2 3 2 1
>  > paste( names(tb), ":", tb, sep="" )
> [1] "A:1" "B:2" "C:3" "D:2" "E:1"
>
> If this is not what you want, then please give a simple example.
>
> Regards, Adai
>
>
>
> Allan Kamau wrote:
> > Hi all,
> > If the question below as been answered before I
> > apologize for the posting.
> > I would like to get the frequencies of occurrence of
> > all values in a given variable in a multivariate
> > dataset. In short for each variable (or field) a
> > summary of values contained with in a value:frequency
> > pair, there can be many such pairs for a given
> > variable. I would like to do the same for several such
> > variables.
> > I have used table() but am unable to extract the
> > individual value and frequency values.
> > Please advise.
> >
> > Allan.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Wed Jul 25 13:50:55 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 25 Jul 2007 07:50:55 -0400
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
In-Reply-To: <747597.66177.qm@web53502.mail.re2.yahoo.com>
References: <747597.66177.qm@web53502.mail.re2.yahoo.com>
Message-ID: <644e1f320707250450l51f336ftaabc522697b2e4fa@mail.gmail.com>

Also if you want to access the individual values, you can just leave
it as a list:

> x.val <- apply(x, 2, table)
> # access each value
> x.val$PR14["V"]
V
8



On 7/25/07, Allan Kamau <kamauallan at yahoo.com> wrote:
> A subset of the data looks as follows
>
> > df[1:10,14:20]
>   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> 1     V    T    I    K    V    G    D
> 2     V    S    I    K    V    G    G
> 3     V    T    I    R    V    G    G
> 4     V    S    I    K    I    G    G
> 5     V    S    I    K    V    G    G
> 6     V    S    I    R    V    G    G
> 7     V    T    I    K    I    G    G
> 8     V    S    I    K    V    E    G
> 9     V    S    I    K    V    G    G
> 10    V    S    I    K    V    G    G
>
> The result I would like is as follows
>
> PR10        PR11          PR12   ...
> [V:10]    [S:7,T:3]    [I:10]
>
> The result can be in a matrix or a vector and each variablename, value and frequency should be accessible so as to be used for comparisons with another dataset later.
> The frequency can be a count or a percentage.
>
>
> Allan.
>
>
> ----- Original Message ----
> From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> To: Allan Kamau <kamauallan at yahoo.com>
> Cc: r-help at stat.math.ethz.ch
> Sent: Tuesday, July 24, 2007 10:21:51 PM
> Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
>
> The name of the table should give you the "value". And if you have a
> matrix, you just need to convert it into a vector first.
>
>  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ], nc=3 )
>  > m
>      [,1] [,2] [,3]
> [1,] "A"  "C"  "B"
> [2,] "B"  "D"  "C"
> [3,] "C"  "E"  "D"
>  > tb <- table( as.vector(m) )
>  > tb
>
> A B C D E
> 1 2 3 2 1
>  > paste( names(tb), ":", tb, sep="" )
> [1] "A:1" "B:2" "C:3" "D:2" "E:1"
>
> If this is not what you want, then please give a simple example.
>
> Regards, Adai
>
>
>
> Allan Kamau wrote:
> > Hi all,
> > If the question below as been answered before I
> > apologize for the posting.
> > I would like to get the frequencies of occurrence of
> > all values in a given variable in a multivariate
> > dataset. In short for each variable (or field) a
> > summary of values contained with in a value:frequency
> > pair, there can be many such pairs for a given
> > variable. I would like to do the same for several such
> > variables.
> > I have used table() but am unable to extract the
> > individual value and frequency values.
> > Please advise.
> >
> > Allan.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ggrothendieck at gmail.com  Wed Jul 25 13:54:05 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 25 Jul 2007 07:54:05 -0400
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
In-Reply-To: <747597.66177.qm@web53502.mail.re2.yahoo.com>
References: <747597.66177.qm@web53502.mail.re2.yahoo.com>
Message-ID: <971536df0707250454y36dcd45et53557794302920f3@mail.gmail.com>

Try summary:

> summary(x)
 PR10   PR11  PR12   PR13  PR14  PR15  PR16
 V:10   S:7   I:10   K:8   I:2   E:1   D:1
        T:3          R:2   V:8   G:9   G:9

On 7/25/07, Allan Kamau <kamauallan at yahoo.com> wrote:
> A subset of the data looks as follows
>
> > df[1:10,14:20]
>   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> 1     V    T    I    K    V    G    D
> 2     V    S    I    K    V    G    G
> 3     V    T    I    R    V    G    G
> 4     V    S    I    K    I    G    G
> 5     V    S    I    K    V    G    G
> 6     V    S    I    R    V    G    G
> 7     V    T    I    K    I    G    G
> 8     V    S    I    K    V    E    G
> 9     V    S    I    K    V    G    G
> 10    V    S    I    K    V    G    G
>
> The result I would like is as follows
>
> PR10        PR11          PR12   ...
> [V:10]    [S:7,T:3]    [I:10]
>
> The result can be in a matrix or a vector and each variablename, value and frequency should be accessible so as to be used for comparisons with another dataset later.
> The frequency can be a count or a percentage.
>
>
> Allan.
>
>
> ----- Original Message ----
> From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> To: Allan Kamau <kamauallan at yahoo.com>
> Cc: r-help at stat.math.ethz.ch
> Sent: Tuesday, July 24, 2007 10:21:51 PM
> Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
>
> The name of the table should give you the "value". And if you have a
> matrix, you just need to convert it into a vector first.
>
>  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ], nc=3 )
>  > m
>      [,1] [,2] [,3]
> [1,] "A"  "C"  "B"
> [2,] "B"  "D"  "C"
> [3,] "C"  "E"  "D"
>  > tb <- table( as.vector(m) )
>  > tb
>
> A B C D E
> 1 2 3 2 1
>  > paste( names(tb), ":", tb, sep="" )
> [1] "A:1" "B:2" "C:3" "D:2" "E:1"
>
> If this is not what you want, then please give a simple example.
>
> Regards, Adai
>
>
>
> Allan Kamau wrote:
> > Hi all,
> > If the question below as been answered before I
> > apologize for the posting.
> > I would like to get the frequencies of occurrence of
> > all values in a given variable in a multivariate
> > dataset. In short for each variable (or field) a
> > summary of values contained with in a value:frequency
> > pair, there can be many such pairs for a given
> > variable. I would like to do the same for several such
> > variables.
> > I have used table() but am unable to extract the
> > individual value and frequency values.
> > Please advise.
> >
> > Allan.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wl2776 at gmail.com  Wed Jul 25 14:22:21 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Wed, 25 Jul 2007 05:22:21 -0700 (PDT)
Subject: [R] problem with sub !
In-Reply-To: <605b4120707250356y4e6c07caid0dcfd66d193dd3@mail.gmail.com>
References: <605b4120707250356y4e6c07caid0dcfd66d193dd3@mail.gmail.com>
Message-ID: <11781512.post@talk.nabble.com>


Try Sys.setlocale().


marco.R.help marco.R.help wrote:
> 
>  I am trying to use "sub" to replace  patterns  in  a character array that
> contains german names with german special characters. I have the following
> problem:
> 
>>  sub("\\xdf","ss","Wei\xdferitzkreis")
> Error in sub(pattern, replacement, x, ignore.case, extended, fixed,
> useBytes) :
>     input string 1 is invalid in this locale
>>  sub("\xdf","ss","Wei\xdferitzkreis")
> Error in sub(pattern, replacement, x, ignore.case, extended, fixed,
> useBytes) :
>     'pattern' is invalid in this locale
> 
> Does anybody know how to tackle this problem ?
> 

-- 
View this message in context: http://www.nabble.com/problem-with-sub-%21-tf4141431.html#a11781512
Sent from the R help mailing list archive at Nabble.com.


From yann.mauron at genebio.com  Wed Jul 25 14:23:58 2007
From: yann.mauron at genebio.com (Yann Mauon)
Date: Wed, 25 Jul 2007 05:23:58 -0700 (PDT)
Subject: [R] Operator > and <
Message-ID: <11781567.post@talk.nabble.com>


Hello, 

I'am new to the R world and have a lot of question but the first is : How to
deal with <> opertor in table objects? (Or how to deal with <> in
general...) I explain my problem. 

I read a file with the read.table expression. I then obtain a matrix. I read
the first line for example with the commande data[,1]. Then I would like to
select only the element in this line that are greater than 2. Is there an
elegant way to achieve that ?

Thanks by advance...
-- 
View this message in context: http://www.nabble.com/Operator-%3E-and-%3C-tf4141869.html#a11781567
Sent from the R help mailing list archive at Nabble.com.


From ccleland at optonline.net  Wed Jul 25 14:45:39 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 25 Jul 2007 08:45:39 -0400
Subject: [R] Operator > and <
In-Reply-To: <11781567.post@talk.nabble.com>
References: <11781567.post@talk.nabble.com>
Message-ID: <46A745F3.10509@optonline.net>

Yann Mauon wrote:
> Hello, 
> 
> I'am new to the R world and have a lot of question but the first is : How to
> deal with <> opertor in table objects? (Or how to deal with <> in
> general...) I explain my problem. 
> 
> I read a file with the read.table expression. I then obtain a matrix. I read
> the first line for example with the commande data[,1]. Then I would like to
> select only the element in this line that are greater than 2. Is there an
> elegant way to achieve that ?
> 
> Thanks by advance...

  Note that read.table() returns a data frame, not a matrix.  To subset,
try this:

subset(data, data[,1] > 2)

OR

subset(data, data[,1] > 2, select=1)

  Of course, it is always nice if each column in your data frame has a
meaningful name, so it can be referred to by name rather than number.

?subset
?names

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From jrkrideau at yahoo.ca  Wed Jul 25 14:50:10 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 25 Jul 2007 08:50:10 -0400 (EDT)
Subject: [R] Operator > and <
In-Reply-To: <11781567.post@talk.nabble.com>
Message-ID: <172063.44425.qm@web32802.mail.mud.yahoo.com>

mm  <- matrix(1:9, nrow=3) ; mm
subset(mm[,1],mm[,1] <3)  # Note I used 3 not 2 here.

Have a look at some of the introductory documents on 
the R site  ( Contributed documents under OTHER in the
documentation). They should answer a lot of your basic
questions like this.

Documents by Lemon, Maindonald and Verzani are good
places to start.

Also the Introduction to R is useful.


--- Yann Mauon <yann.mauron at genebio.com> wrote:

> 
> Hello, 
> 
> I'am new to the R world and have a lot of question
> but the first is : How to
> deal with <> opertor in table objects? (Or how to
> deal with <> in
> general...) I explain my problem. 
> 
> I read a file with the read.table expression. I then
> obtain a matrix. I read
> the first line for example with the commande
> data[,1]. Then I would like to
> select only the element in this line that are
> greater than 2. Is there an
> elegant way to achieve that ?
> 
> Thanks by advance...
> -- 
> View this message in context:
>
http://www.nabble.com/Operator-%3E-and-%3C-tf4141869.html#a11781567
> Sent from the R help mailing list archive at
> Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From petr.pikal at precheza.cz  Wed Jul 25 14:50:36 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Wed, 25 Jul 2007 14:50:36 +0200
Subject: [R] Odp:  Operator > and <
In-Reply-To: <11781567.post@talk.nabble.com>
Message-ID: <OF7A704F57.3525258A-ONC1257323.0046060F-C1257323.00468CBB@precheza.cz>

Hi

you definitely shall make a quick glance to some documentation which comes 
with R e.g. R intro manual. Or look at CRAN where is quite impressive 
amount of literature from basic stuff to advanced papers.

To your question:

r-help-bounces at stat.math.ethz.ch napsal dne 25.07.2007 14:23:58:

> 
> Hello, 
> 
> I'am new to the R world and have a lot of question but the first is : 
How to
> deal with <> opertor in table objects? (Or how to deal with <> in
> general...) I explain my problem. 
> 
> I read a file with the read.table expression. I then obtain a matrix. I 
read
> the first line for example with the commande data[,1]. Then I would like 
to

You did not select line but column. 

data[,1] > 2 

will give you logical vector which you can use for selection of rows from 
data. 

data[data[,1] > 2,]

Regards.
Petr
 

> select only the element in this line that are greater than 2. Is there 
an
> elegant way to achieve that ?
> 
> Thanks by advance...
> -- 
> View this message in context: 
http://www.nabble.com/Operator-%3E-and-%3C-
> tf4141869.html#a11781567
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at yahoo.ca  Wed Jul 25 14:58:35 2007
From: jrkrideau at yahoo.ca (John Kane)
Date: Wed, 25 Jul 2007 08:58:35 -0400 (EDT)
Subject: [R] plots
In-Reply-To: <3ffd3bb60707250317x2b281487k33c45c6cb743942f@mail.gmail.com>
Message-ID: <396754.51883.qm@web32807.mail.mud.yahoo.com>


--- amna khan <amnakhan493 at gmail.com> wrote:

> Hi Sir
> 
> I did not find any function of graph which plot one
> variable on x-axis and 2
> or more than 2 variables on y-axis.
I think
?points
or 
?lines 
may be what you want.

> Moreover, how can I change the labels of L-moments
> diagram obtained by
> plotlmrdia(lmrdia())

Cannot help here.
> 
> Thank you
> 
> -- 
> AMINA SHAHZADI
> Department of Statistics
> GC University Lahore, Pakistan.
> Email:
> amnakhan493 at gmail.com
> amna_989 at hotmail.com
> amna_989 at yahoo.com


From HStevens at muohio.edu  Wed Jul 25 15:01:39 2007
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Wed, 25 Jul 2007 09:01:39 -0400
Subject: [R] lme or gls prediction intervals
In-Reply-To: <OFA652F5A2.4C519201-ONC1257323.002805DD-C1257323.0028506E@basf-c-s.be>
References: <OFA652F5A2.4C519201-ONC1257323.002805DD-C1257323.0028506E@basf-c-s.be>
Message-ID: <2A7B975B-692B-4F42-9B4B-6A5A63F7AAE9@muohio.edu>

Hi Joris,
Thank you for the reply.
I also realize that I meant confidence intervals, not prediction  
intervals!

I am trying to do something analogous to

predict(model.gls, newdata, interval="confidence")

but predict.gls does not have an interval argument. I am guessing  
that this is because it is subtle and complicated to figure out what  
the appropriate hypothesis is, but I was hoping to get some feedback.

Cheers,
Hank

On Jul 25, 2007, at 3:21 AM, joris.dewolf at cropdesign.com wrote:

>
>
> Martin,
>
>
> Have you checked ?intervals.gls
>
>
> This intervals are approximate, but this would be the obvious starting
> point to me.
>
>
> Joris
>
>
>
>
>
>
>
>
>
>
>
>              "Martin Henry H.
>              Stevens"
>               
> <HStevens at muohio.                                          To
>              edu>                      R-Help <r- 
> help at stat.math.ethz.ch>
>              Sent  
> by:                                                   cc
>              r-help-bounces at st
>              at.math.ethz.ch                                        
> Subject
>                                        [R] lme or gls prediction  
> intervals
>
>              24/07/2007 19:22
>
>
>
>
>
>
>
>
> Hi folks,
> I am trying to generate 95% confidence intervals for a gls model
> using predict.nlme
> with
> R version 2.5.1 (2007-06-27)
> . nlme: Linear  and Nonlinear Mixed Effects Models. R package version
>    3.1-83.
>
> I have looked in help, and I can do it for lm and glm models, and I
> can generate simple predictions for lme models with various levels --
> I am familiar with the basics.
>
> Is there a way to get prediction intervals for gls models? My "best"
> model uses varPower(), so I am reluctant to fall back on lm  
> predictions.
>
> Thank you,
>
> Hank
>
>
> Dr. Hank Stevens, Associate Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
>
> "E Pluribus Unum"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"

If you send an attachment, please try to send it in a format anyone  
can read, such as PDF, text, Open Document Format, HTML, or RTF.  
Please try not to send me MS Word or PowerPoint attachments-
Why? See:  http://www.gnu.org/philosophy/no-word-attachments.html


From jinlo789 at yahoo.com  Wed Jul 25 13:50:20 2007
From: jinlo789 at yahoo.com (Jin Lo)
Date: Wed, 25 Jul 2007 04:50:20 -0700 (PDT)
Subject: [R] Is Rwiki down?
Message-ID: <975763.12760.qm@web45412.mail.sp1.yahoo.com>

Dear R-friends,

is Rwiki down? I've been trying to login for the past
couple of days with no success.

Yours sincerely,

Jin




       
____________________________________________________________________________________

wherever you're surfing.


From mcoyne at boninc.com  Wed Jul 25 14:45:01 2007
From: mcoyne at boninc.com (My Coyne)
Date: Wed, 25 Jul 2007 08:45:01 -0400
Subject: [R] Rgraphviz and R 2.5.1 entry point Rf_allocString could not be
	located
Message-ID: <001d01c7ceb9$9e445b20$dacd1160$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/e514ed8f/attachment.pl 

From labone at gforcecable.com  Wed Jul 25 13:00:26 2007
From: labone at gforcecable.com (Tom La Bone)
Date: Wed, 25 Jul 2007 07:00:26 -0400
Subject: [R] Minitab Parametric Distribution Analysis in R
Message-ID: <001601c7ceab$01f2fe60$6401a8c0@Boozoo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/4c435848/attachment.pl 

From HStevens at muohio.edu  Wed Jul 25 15:28:14 2007
From: HStevens at muohio.edu (Martin Henry H. Stevens)
Date: Wed, 25 Jul 2007 09:28:14 -0400
Subject: [R] Is Rwiki down?
In-Reply-To: <975763.12760.qm@web45412.mail.sp1.yahoo.com>
References: <975763.12760.qm@web45412.mail.sp1.yahoo.com>
Message-ID: <FDD947B9-44F7-400D-A1FF-D1EAE362DC30@muohio.edu>

Me too.
Hank
On Jul 25, 2007, at 7:50 AM, Jin Lo wrote:

> Dear R-friends,
>
> is Rwiki down? I've been trying to login for the past
> couple of days with no success.
>
> Yours sincerely,
>
> Jin
>
>
>
>
>
> ______________________________________________________________________ 
> ______________
>
> wherever you're surfing.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"

If you send an attachment, please try to send it in a format anyone  
can read, such as PDF, text, Open Document Format, HTML, or RTF.  
Please try not to send me MS Word or PowerPoint attachments-
Why? See:  http://www.gnu.org/philosophy/no-word-attachments.html


From ripley at stats.ox.ac.uk  Wed Jul 25 15:51:11 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 25 Jul 2007 14:51:11 +0100 (BST)
Subject: [R] Rgraphviz and R 2.5.1 entry point Rf_allocString could not
 be located
In-Reply-To: <001d01c7ceb9$9e445b20$dacd1160$@com>
References: <001d01c7ceb9$9e445b20$dacd1160$@com>
Message-ID: <Pine.LNX.4.64.0707251446190.27285@gannet.stats.ox.ac.uk>

On Wed, 25 Jul 2007, My Coyne wrote:

> Dear R-Helpers
>
> In R 2.5.1, the command library(Rgraphviz) fails on my Windows (XP SP2)
> system with error popup "The procedure entry point Rf_allocString could not
> be located in the dynamic link library R.dll".
>
> Thanks in advance for any suggestion in solving the error.

Use a version of the package compiled for R 2.5.1.  That currently picked 
up by using the menus in R 2.5.1 works for me.

Also, this is not the right list for questions about Bioconductor 
packages, and please send properly formatted plain text only.

> My D. Coyne
> Imagination is more important than knowledge... (Albert Einstein)
> mcoyne at boninc.com
> BioInformatics Enthusiast
> Office: 301-865-0243
> Cell:    301-399-6351
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From kahm at rheinahrcampus.de  Wed Jul 25 15:55:12 2007
From: kahm at rheinahrcampus.de (MatzeK)
Date: Wed, 25 Jul 2007 06:55:12 -0700 (PDT)
Subject: [R] Open two graphic devices at a time
Message-ID: <11783029.post@talk.nabble.com>


Hello,

I would like to export my plots to harddisk (jpeg or pdf). I know that could
be done with pdf(...) and jpeg(...).
But my problem is that if I open e.g. the pdf device my plot does not appear
on the screen. If I plot to the x11 device and try to export later the
outputfile is damaged.
How can I manage that problem? I want to see my plots on screen and want to
export them. Is there a chance to open two different devices, or is the only
way printing twice. One time to x11() and one time to pdf()?

Thanks a lot for any idea
-- 
View this message in context: http://www.nabble.com/Open-two-graphic-devices-at-a-time-tf4142321.html#a11783029
Sent from the R help mailing list archive at Nabble.com.


From rroa at udec.cl  Wed Jul 25 16:17:19 2007
From: rroa at udec.cl (=?ISO-8859-1?Q?Rub=E9n_Roa-Ureta?=)
Date: Wed, 25 Jul 2007 10:17:19 -0400
Subject: [R] Time when object was created
Message-ID: <46A75B6F.80407@udec.cl>

Dear ComRades,
Last night I left an Intel Core Duo Windows Vista system running an 
extensive mixed glm with lmer. This morning I found that R-lmer had 
finished the job successfully. I would like to know at what time the 
object containing the results (call it lme_1) was finished. I guess 
there is some simple function that when applied to the object would give 
me the time stamp?
Thanks
Rub?n


From petr.pikal at precheza.cz  Wed Jul 25 16:18:24 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Wed, 25 Jul 2007 16:18:24 +0200
Subject: [R] Odp:  Open two graphic devices at a time
In-Reply-To: <11783029.post@talk.nabble.com>
Message-ID: <OFAF229EF5.5EB2DC76-ONC1257323.004E0524-C1257323.004E969B@precheza.cz>

Hi

far from beeing an expert in graphic devices AFAIK you can either see your 
graph on screen (x11 device) or to open other device (pdf, png, ...) and 
to issue plotting commands to that device. Do not forget

dev.off()

after your plot is finished.

Or you can save your plot by menu command File/Save as... (on Windows), 
which you did not specify.

Regards
Petr

r-help-bounces at stat.math.ethz.ch napsal dne 25.07.2007 15:55:12:

> 
> Hello,
> 
> I would like to export my plots to harddisk (jpeg or pdf). I know that 
could
> be done with pdf(...) and jpeg(...).
> But my problem is that if I open e.g. the pdf device my plot does not 
appear
> on the screen. If I plot to the x11 device and try to export later the
> outputfile is damaged.
> How can I manage that problem? I want to see my plots on screen and want 
to
> export them. Is there a chance to open two different devices, or is the 
only
> way printing twice. One time to x11() and one time to pdf()?
> 
> Thanks a lot for any idea
> -- 
> View this message in context: 
http://www.nabble.com/Open-two-graphic-devices-
> at-a-time-tf4142321.html#a11783029
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sparandekar at worldbank.org  Wed Jul 25 16:23:53 2007
From: sparandekar at worldbank.org (sparandekar at worldbank.org)
Date: Wed, 25 Jul 2007 10:23:53 -0400
Subject: [R] How to add circular text for a graph with concentric circles
In-Reply-To: <46A7363C.2090601@bitwrit.com.au>
Message-ID: <OFEB71A92C.1943CC9C-ON85257323.004DF5D5-85257323.004F1734@worldbank.org>

Thanks, Jim. I tried to implement this but the text swings all over the place. I
had thought that there could be a function where I could specify the circle's
origin, radius and angle of the location of the starting letter, then, based on
a specified font size and letter spacing, the arc could be plotted out. But
clearly, the problem is not as small as I thought, and it appears that I need to
spend some time learning more of the basics  before attempting something this
complicated. Thanks for your help anyways.

best regards,
Suhas



                                                                                
             Jim Lemon                                                          
             <jim at bitwrit.co                                                    
             m.au>                                                           To 
                                     sparandekar at worldbank.org                  
             07/25/2007                                                      cc 
             07:38 AM                r-help at stat.math.ethz.ch                   
                                                                        Subject 
                                     Re: [R] How to add circular text for a     
                                     graph with concentric circles              
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                




sparandekar at worldbank.org wrote:
> Dear R experts,
>
> I am plotting the population of students who live in a city, and in
> successive circular bands made of the contiguous districts that surround
> the city. This is a stylized figure, where I specify the area of each
> successive circle based on the cumulative population of students. I want
> to compare two sets of concentric circles across different populations -
> such as 'All students' and 'Private students' (those attending private
> school) by using the same colours and the same dimension of the outer
> circle. I have attached the .pdf file with the output, and the R code to
> generate the first set of circles.
>
> I would appreciate any tips about how to rotate the text label that marks
> each concentric circle (except the central circle) to be curved around the
> circle, located in the middle of each band, thus following the circle
> instead of being horizontal, as I have it now.
>
Hi Suhas,
This is kind of rough, being rejigged from an old piece of Postscript
code that I wrote to get text in an arc. You specify the text and
whatever else is needed as in the following example:

arctext<-function(x,center=c(0,0),radius=1,
  midangle=pi/2,stretch=1.1,cex=1,...) {
  oldcex<-par("cex")
  par(cex=cex)
  xwidth<-strwidth(x)*stretch
  startpos<-midangle+xwidth/(radius*2)
  xvec<-strsplit(x,"")[[1]]
  xwidths<-rep(mean(stretch*strwidth(xvec)),length(xvec))
  arcpos<-startpos-cumsum(xwidths)
  for(i in 1:length(arcpos))
   text(center[1]+radius*cos(arcpos[i]),center[2]+radius*sin(arcpos[i]),
    xvec[i],adj=c(0.5,0.5),srt=(arcpos[i]-midangle)*180/pi)
  par(cex=oldcex)
}

plot((1:5)
arctext("bendy as a bloody piece of spaghetti",center=c(3,3))

radius is obviously the radius of the arc,
midangle is where you want the center of the text,
stretch is how much to stretch the text to make it look nice
and cex is the expansion factor.
You will probably notice that I kludged the spacing by making
it monospaced. I'll have a try at getting proportionally spaced
text to work right when I get a chance.

Jim


From kahm at rheinahrcampus.de  Wed Jul 25 16:41:45 2007
From: kahm at rheinahrcampus.de (MatzeK)
Date: Wed, 25 Jul 2007 07:41:45 -0700 (PDT)
Subject: [R] Odp:  Open two graphic devices at a time
In-Reply-To: <OFAF229EF5.5EB2DC76-ONC1257323.004E0524-C1257323.004E969B@precheza.cz>
References: <11783029.post@talk.nabble.com>
	<OFAF229EF5.5EB2DC76-ONC1257323.004E0524-C1257323.004E969B@precheza.cz>
Message-ID: <11783905.post@talk.nabble.com>


@Petr
Hello,

now I tried it with savePlot(). This is a very easy way. Now I don't need
two devices.

Thanks a lot
MatzeK 



Petr Pikal wrote:
> 
> Hi
> 
> far from beeing an expert in graphic devices AFAIK you can either see your 
> graph on screen (x11 device) or to open other device (pdf, png, ...) and 
> to issue plotting commands to that device. Do not forget
> 
> dev.off()
> 
> after your plot is finished.
> 
> Or you can save your plot by menu command File/Save as... (on Windows), 
> which you did not specify.
> 
> Regards
> Petr
> 
> r-help-bounces at stat.math.ethz.ch napsal dne 25.07.2007 15:55:12:
> 
>> 
>> Hello,
>> 
>> I would like to export my plots to harddisk (jpeg or pdf). I know that 
> could
>> be done with pdf(...) and jpeg(...).
>> But my problem is that if I open e.g. the pdf device my plot does not 
> appear
>> on the screen. If I plot to the x11 device and try to export later the
>> outputfile is damaged.
>> How can I manage that problem? I want to see my plots on screen and want 
> to
>> export them. Is there a chance to open two different devices, or is the 
> only
>> way printing twice. One time to x11() and one time to pdf()?
>> 
>> Thanks a lot for any idea
>> -- 
>> View this message in context: 
> http://www.nabble.com/Open-two-graphic-devices-
>> at-a-time-tf4142321.html#a11783029
>> Sent from the R help mailing list archive at Nabble.com.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Open-two-graphic-devices-at-a-time-tf4142321.html#a11783905
Sent from the R help mailing list archive at Nabble.com.


From johnzabroski at gmail.com  Wed Jul 25 16:43:50 2007
From: johnzabroski at gmail.com (John Zabroski)
Date: Wed, 25 Jul 2007 10:43:50 -0400
Subject: [R] Constructing bar charts with standard error bars
Message-ID: <546067750707250743vc82f86cha68a302351f12297@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/3a92d588/attachment.pl 

From h.wickham at gmail.com  Wed Jul 25 16:55:38 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 25 Jul 2007 16:55:38 +0200
Subject: [R] Ggplot2 equivalent of axis and problem with log scale
In-Reply-To: <2E9C414912813E4EB981326983E0A10403585E3B@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A10403585E3B@inboexch.inbo.be>
Message-ID: <f8e6ff050707250755ybda1dbem84b6ff66f6f36f5b@mail.gmail.com>

On 7/25/07, ONKELINX, Thierry <Thierry.ONKELINX at inbo.be> wrote:
> Dear useRs,
>
> Recently I've discorved ggplot2 and I must say that I really like it,
> although the documentation still is a working in progress.
>
> My first question: How can I change the position of the labels and the
> text of the labels? With a basic plot I would use axis(2, at =
> position.of.the.ticks, labels = text.at.the.ticks). Could someone
> provide me with an example of how to do this with ggplot2?

Have a look at scale_continous - in particular the breaks and labels
arguments (although I haven?t tested them much yet).  You need +
scale_x_continuous() and + scale_y_continuous() as appropriate.

> The second question is probably a little bug. If I plot the y-axis in
> log10 scale then geom_errorbar still plot the values in the original
> scale. See the example below. The second plot is what I would suspect
> when plotting the first graph.

Yes, that's a bug - I`ll try and get it fixed in the next version.

Thanks,

Hadley


From tlumley at u.washington.edu  Wed Jul 25 18:03:09 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 25 Jul 2007 09:03:09 -0700 (PDT)
Subject: [R] Minitab Parametric Distribution Analysis in R
In-Reply-To: <001601c7ceab$01f2fe60$6401a8c0@Boozoo>
Message-ID: <Pine.LNX.4.43.0707250903090.9786@hymn08.u.washington.edu>


The survival package (survreg() function) will fit quite a few parametric models under censoring.

If you aren't doing regression, but just one-sample fitting, you can feed the appropriate censored or truncated likelihood to mle() in the stat4 package.

Both packages should be part of your R distribution.

    -thomas



On Wed, 25 Jul 2007, Tom La Bone wrote:

> Minitab can perform a "Parametric Distribution Analysis - Arbitrary
> Censoring" with one of eight distributions (e.g., weibull), giving the
> maximum likelihood estimates of the parameters in the distribution for a
> given dataset. Does R have a package that provides equivalent functionality?
> Thanks for any advice you can offer.
>
>
>
> Tom La Bone
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From Mauro.Rossi at irpi.cnr.it  Wed Jul 25 18:17:49 2007
From: Mauro.Rossi at irpi.cnr.it (Mauro Rossi)
Date: Wed, 25 Jul 2007 18:17:49 +0200
Subject: [R]  qda(MASS) function error
Message-ID: <46A777AD.6010006@irpi.cnr.it>

Dear R user,
  I'm using qda (quadratic discriminant analysis) function (package 
MASS) to classify 58 explanatory variables (numeric type with different 
ranges) using a grouping variable (factor 2 levels "0" "1"). I'm using 
the qda method for class 'data.frame' (in this way I don't need to 
specify a formula).
Using the function:
result.qda<-qda(explanatory.variables, grouping.variable, method="moment")
I obtain the following error message:
"Error in qda.default(x, grouping, ...) : rank deficiency in group 0"
I run the script excluding some variables and I've individuated 2 
explanatory variables that give problems, but  I don't understand why 
they give them. The two excluded variables are numeric with two possible 
values: 0 and 1, but in the rest of group of  variables, some similar 
variables are considered.

I don't have this problem using lda  function for linear discriminant 
analysis.

What does this error message mean?
What types of variables does qda function consider?

Thank in advance,
Mauro Rossi

-- 

Mauro Rossi

Istituto di Ricerca per la Protezione Idrogeologica

Consiglio Nazionale delle Ricerche

Via della Madonna Alta, 126

06128 Perugia

Italia

Tel. +39 075 5014421

Fax +39 075 5014420


From f.harrell at vanderbilt.edu  Wed Jul 25 18:21:56 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 25 Jul 2007 11:21:56 -0500
Subject: [R] Constructing bar charts with standard error bars
In-Reply-To: <546067750707250743vc82f86cha68a302351f12297@mail.gmail.com>
References: <546067750707250743vc82f86cha68a302351f12297@mail.gmail.com>
Message-ID: <46A778A4.3080201@vanderbilt.edu>

John Zabroski wrote:
> I am new to R.
> 
> I want to graph group data using a "Traditional Bar Chart with Standard
> Error Bar", like the kind shown here:
> http://samiam.colorado.edu/~mcclella/ftep/twoGroups/twoGroupGraphs.html

There are severe problems with dynamite plots such as these.  See 
http://biostat.mc.vanderbilt.edu/DynamitePlots for a list of problems 
and solutions.

Frank

> 
> Is there a simple way to do this?
> 
> So far, I have only figured out how to plot the bars using barplot.
> 
> testdata <- scan(, list(group=0,xbar=0,se=0))
> 400 0.36038 0.02154
> 200 0.35927 0.02167
> 100 0.35925 0.02341
> 50 0.35712 0.01968
> 25 0.35396 0.01931
> 
> barplot(testdata$xbar, names.arg=as.character(testdata$group), main="a=4.0",
> xlab="Group", ylab="xbar")
> xvalues <- c(0.7, 1.9, 3.1, 4.3, 5.5)
> arrows(xvalues, testdata$xbar, xvalues, testdata$xbar+testdata$se, length=
> 0.4, angle=90, code=3)
> 
> 
> The best clue I have so far is Rtips #5.9:
> http://pj.freefaculty.org/R/Rtips.html#5.9 which is what I based my present
> solution off of.
> 
> However, I do not understand how this works.  It seems like there is no
> concrete way to determine the arrow drawing parameters x0 and x1 for a
> barplot.  Moreover, the bars seem to be "cut off".
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From bolker at ufl.edu  Wed Jul 25 18:24:47 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Wed, 25 Jul 2007 16:24:47 +0000 (UTC)
Subject: [R] Constructing bar charts with standard error bars
References: <546067750707250743vc82f86cha68a302351f12297@mail.gmail.com>
Message-ID: <loom.20070725T182046-933@post.gmane.org>

John Zabroski <johnzabroski <at> gmail.com> writes:

> 
> I am new to R.
> 
> I want to graph group data using a "Traditional Bar Chart with Standard
> Error Bar", like the kind shown here:
> http://samiam.colorado.edu/~mcclella/ftep/twoGroups/twoGroupGraphs.html
> 
> Is there a simple way to do this?

   [snip]

> The best clue I have so far is Rtips #5.9:
> http://pj.freefaculty.org/R/Rtips.html#5.9 which is what I based my present
> solution off of.
> 
> However, I do not understand how this works.  It seems like there is no
> concrete way to determine the arrow drawing parameters x0 and x1 for a
> barplot.  Moreover, the bars seem to be "cut off".
> 
  
  barplot() returns the x values you need for x0 and x1.
barplot(...,ylim=c(0,xbar+se)) will set the upper y limit so
the bars don't get cut off.

Here are three ways to create such a barplot (I will add
this to the wiki once it's back on line):  (n.b.: the
with() command is just a shortcut to avoid having
to type testdata$xbar, testdata$group, etc.)

## 1. tweaked version of what you did above

testdata <- data.frame(group=c(400,200,100,50,25),
	xbar= c(0.36038 , 0.35927 , 0.35925 , 0.35712 , 0.35396), 
	se = c(0.02154,0.02167,0.02341,0.01968,	0.01931))
xvals = with(testdata,
     barplot(xbar, names.arg=group, main="a=4.0",
        xlab="Group", ylab="xbar",ylim=c(0,max(xbar+se))))
with(testdata,
     arrows(xvals, xbar, xvals, xbar+se, length=0.4, angle=90, code=3))

## 2. using the plotCI function from plotrix to draw the
##    arrows instead
library(plotrix)
xvals = with(testdata,
     barplot(xbar, names.arg=group, main="a=4.0",
        xlab="Group", ylab="xbar",ylim=c(0,max(xbar+se))))
with(testdata,
     plotCI(xvals, xbar, liw=0,uiw=se,add=TRUE,pch=NA,gap=FALSE))

## 3. the most automatic way, using barplot2() from the
## gplots package

library(gplots)
with(testdata,
     barplot2(xbar,names.arg=group,main="a=4.0",
              xlab="Group",ylab="xbar",plot.ci=TRUE,
              ci.u=xbar+se,ci.l=xbar))

  P.S. I hope you're not hoping to infer a statistically
significant difference among these groups ...

  cheers
   Ben Bolker


From jjroper at gmail.com  Wed Jul 25 19:20:46 2007
From: jjroper at gmail.com (James J. Roper)
Date: Wed, 25 Jul 2007 14:20:46 -0300
Subject: [R] if - else
Message-ID: <2912e6710707251020y3d66b8d5jae1e5383441e629b@mail.gmail.com>

Greetings,

I have some confusion with the use of if - else.  Let's say I have a
four variables as follows:

Condition               DateFound          DateFirstEvent
DateSecondEvent
NA                        10Jan2000          NA                             NA
0                           05Jan2000          07Jan2000
   10Jan2000
1                           07Jan2000          07Jan2000
   08Jan2000
2                           09Jan2000          NA                             NA

Now, what I need to do is make a new variable that is either the
midpoint of the first and second event dates, or the date found (I
will call Start).

I tried an if - else condition as follows:

Start <- if (DateFirstEven < DateSecondEvent)
(DateFirstEvent+DateSecondEvent)/2 else DateFound

I also tried

Start <- if (any(DateFirstEven < DateSecondEvent))
(DateFirstEvent+DateSecondEvent)/2 else DateFound

Only the first half of the expression was ever evaluated.

I hope I have not been to brief, and will certainly appreciate any help.

Thanks,

Jim

-- 
James J. Roper
Population Dynamics and Conservation of
Terrestrial Vertebrates
Caixa Postal 19034
81531-990 Curitiba, Paran?, Brasil
=======================================
E-mail:                               jjroper at gmail.com
Phone/Fone/Tel?fono:     55 41 33611764
celular:                             55 41 99870543
Casa:                               55 41 33857249
=======================================
Ecologia e Conserva??o na UFPR
http://www.bio.ufpr.br/ecologia/
---
http://jjroper.googlepages.com/
http://arsartium.googlepages.com/


From sfalcon at fhcrc.org  Wed Jul 25 19:21:53 2007
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 25 Jul 2007 10:21:53 -0700
Subject: [R] initalizing and checking validity of S4 classes
In-Reply-To: <6phodi16z35.fsf@gopher4.fhcrc.org> (Martin Morgan's message of
	"Tue, 24 Jul 2007 18:39:42 -0700")
References: <94E133D09AA24D43BF6341B675C01A338DCD7F@uu01msg-exb01.soliscom.uu.nl>
	<6phodi16z35.fsf@gopher4.fhcrc.org>
Message-ID: <m2ejiwv1ou.fsf@ziti.local>

Martin Morgan <mtmorgan at fhcrc.org> writes:

> Hi Michal --
>
> Add validObject to your initialize method:

Actually, a call to valid object is part of the default initialization
method which has been masked.  A different solution, which might have
some other benefits is to delegate back to the default method using
callNextMethod.  So you could do:

setMethod("initialize", "someclass",
function(.Object, v=numeric(0), l=character(0))
{
    # strip the vector names

    cv <- v
    cl <- l
    names(cv) <- NULL
    names(cl) <- NULL
    callNextMethod(.Object=.Object, v=cv, l=cl)
} )


> Here are two interpretations of this. (1) using 'initialize' means
> that you are taking control of the initialization process, and hence
> know when you need to call validObject.

Yes.  Anytime you specialize a method you must take responsibility for
any less specific methods.  In this case, the default 'initialize'
does object validation.  So if you want validation, you either need to
do it directly or invoke the default method.

> (2) Responsibility for object
> validity is ambiguous -- does it belong with 'new', 'initialize', or a
> 'constructor' that the programmer might write? This is particularly
> problematic with R's copy semantics, where creating transiently
> invalid objects seems to be almost necessary (e.g., callNextMethod()
> in 'initialize' might initialize the inherited slots of the object,
> but the object itself is of the derived class and could well be
> invalid 'invalid' after the base class has finished with initialize).

This is a good point.  It suggests that, at least, one must initialize
all non-inherited slots to valid values _before_ calling the next
method.

+ seth

-- 
Seth Falcon | Computational Biology | Fred Hutchinson Cancer Research Center
http://bioconductor.org


From dylan.beaudette at gmail.com  Wed Jul 25 20:18:05 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 25 Jul 2007 11:18:05 -0700
Subject: [R] reversing the x-axis terms on a boxplot
Message-ID: <200707251118.05765.dylan.beaudette@gmail.com>

Hi,

I am able to reverse the order of plotting on regular plots (i.e. with the 
plot() function) by manually setting the xlim variable. 

Is there some trick like this which will work for a boxplot?

* for example:

l <- sample(letters, 500, replace=TRUE)
n <- runif(500)
boxplot(n ~ l)


this will produce a plot with the x-axis ranging from a->z ... i know that 
these are labels, associated with an integer index-- but is there someway to 
reverse the plotting order?

Thanks in advance,

Dylan


From dylan.beaudette at gmail.com  Wed Jul 25 20:28:42 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Wed, 25 Jul 2007 11:28:42 -0700
Subject: [R] DF and intercept term meaning for mixed (lme) models
Message-ID: <200707251128.42402.dylan.beaudette@gmail.com>

Hi,

I am using the lme package to fit mixed effects models to a set of data.

I am having a difficult time understanding the *meaning* of the numDF (degrees 
of freedom in the numerator), denDF (DF in the denomenator), as well as the 
Intercept term in the output.

For example:

I have a groupedData object called 'Soil', and am fitting an lme model as 
follows:

## fit a simple model
# errors partitioned among replicates
fit1 <- lme(
? ?log(ksat) ~ log(conc) + ordered(sar) + soil_id ,
? ?random = ~ 1 | rep,
? ?data=Soil
)

## check significance of model terms
anova(fit1)

? ? ? ? ? ? ? numDF denDF ?F-value p-value
(Intercept) ? ? ?1 ?1253 64313.21 ?<.0001
log(conc) ? ? ? ?1 ? 597 ? 173.34 ?<.0001
ordered(sar) ? ? 2 ? 597 ? ?13.87 ?<.0001
soil_id ? ? ? ? 29 ? 597 ? ?54.92 ?<.0001


I am pretty sure that I am interpreting the p-values for the predictor terms 
to mean that these terms contribute significantly to the variation in the 
response variable, (?) . I am not sure what the significance of the Intercept 
term really means. Does it have something to do with the significance of the 
random effects in the model?

Also, from a practical standpoint, how can I best describe / interpret the 
numDF and denDF terms to others... or do they even matter to a person who is 
looking to see if the 'treatment' predictor terms had any effect on the 
response term?

Thanks in advance. 

Dylan


From colleen.doherty at gmail.com  Wed Jul 25 20:55:17 2007
From: colleen.doherty at gmail.com (Colleen Doherty)
Date: Wed, 25 Jul 2007 14:55:17 -0400
Subject: [R] Updating packages in Ubuntu feisty for 2.5
Message-ID: <37b5d0540707251155hf8dc7fcod713f0b435a06361@mail.gmail.com>

Hi,
I would like to run R2.5.1 in Ubuntu feisty on an AMD64 chip.
I am a total newbie to Linux.
I have successfully compiled 2.5 however, I am not able to figure out
how to upgrade the packages.
I am not able to find an R-base-core_2.5.1-1_amd64 or all on the CRAN mirrors.
I can find one on the Ubuntu feisty site, however when I try to
install it I get a warning that Dependency is not stisfiable: libc6
I am pretty convinced I have libc6 installed.
When I reinstall it with apt-get It says the latest version is
currently installed.
I would appreciate any help or advice that y'all have.
If this is more appropriate on another list, please let me know where
and I will post it there.
Thanks!

-- 
Colleen Doherty
Thomashow Lab
310 Plant Biology
Michigan State University
East Lansing, MI 48824


From jholtman at gmail.com  Wed Jul 25 20:58:10 2007
From: jholtman at gmail.com (jim holtman)
Date: Wed, 25 Jul 2007 14:58:10 -0400
Subject: [R] if - else
In-Reply-To: <2912e6710707251020y3d66b8d5jae1e5383441e629b@mail.gmail.com>
References: <2912e6710707251020y3d66b8d5jae1e5383441e629b@mail.gmail.com>
Message-ID: <644e1f320707251158k2aac3041k2c105cf7ef92eee9@mail.gmail.com>

try:

Start <- ifelse (DateFirstEven < DateSecondEvent,
(DateFirstEvent+DateSecondEvent)/2, DateFound)



On 7/25/07, James J. Roper <jjroper at gmail.com> wrote:
> Greetings,
>
> I have some confusion with the use of if - else.  Let's say I have a
> four variables as follows:
>
> Condition               DateFound          DateFirstEvent
> DateSecondEvent
> NA                        10Jan2000          NA                             NA
> 0                           05Jan2000          07Jan2000
>   10Jan2000
> 1                           07Jan2000          07Jan2000
>   08Jan2000
> 2                           09Jan2000          NA                             NA
>
> Now, what I need to do is make a new variable that is either the
> midpoint of the first and second event dates, or the date found (I
> will call Start).
>
> I tried an if - else condition as follows:
>
> Start <- if (DateFirstEven < DateSecondEvent)
> (DateFirstEvent+DateSecondEvent)/2 else DateFound
>
> I also tried
>
> Start <- if (any(DateFirstEven < DateSecondEvent))
> (DateFirstEvent+DateSecondEvent)/2 else DateFound
>
> Only the first half of the expression was ever evaluated.
>
> I hope I have not been to brief, and will certainly appreciate any help.
>
> Thanks,
>
> Jim
>
> --
> James J. Roper
> Population Dynamics and Conservation of
> Terrestrial Vertebrates
> Caixa Postal 19034
> 81531-990 Curitiba, Paran?, Brasil
> =======================================
> E-mail:                               jjroper at gmail.com
> Phone/Fone/Tel?fono:     55 41 33611764
> celular:                             55 41 99870543
> Casa:                               55 41 33857249
> =======================================
> Ecologia e Conserva??o na UFPR
> http://www.bio.ufpr.br/ecologia/
> ---
> http://jjroper.googlepages.com/
> http://arsartium.googlepages.com/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From sundar.dorai-raj at pdf.com  Wed Jul 25 21:05:21 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 25 Jul 2007 12:05:21 -0700
Subject: [R] reversing the x-axis terms on a boxplot
In-Reply-To: <200707251118.05765.dylan.beaudette@gmail.com>
References: <200707251118.05765.dylan.beaudette@gmail.com>
Message-ID: <46A79EF1.6040906@pdf.com>



Dylan Beaudette said the following on 7/25/2007 11:18 AM:
> Hi,
> 
> I am able to reverse the order of plotting on regular plots (i.e. with the 
> plot() function) by manually setting the xlim variable. 
> 
> Is there some trick like this which will work for a boxplot?
> 
> * for example:
> 
> l <- sample(letters, 500, replace=TRUE)
> n <- runif(500)
> boxplot(n ~ l)
> 
> 
> this will produce a plot with the x-axis ranging from a->z ... i know that 
> these are labels, associated with an integer index-- but is there someway to 
> reverse the plotting order?
> 
> Thanks in advance,
> 
> Dylan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Try this:

set.seed(1)
l <- sample(letters, 500, replace=TRUE)
n <- runif(500)
l <- factor(l, levels = rev(letters))
boxplot(n ~ l)

This is explained in the details section of ?boxplot.

      If multiple groups are supplied either as multiple arguments or
      via a formula, parallel boxplots will be plotted, in the order of
      the arguments or the order of the levels of the factor (see
      'factor').

This means you can create any order you want by setting the factor 
levels explicitly.

HTH,

--sundar


From swang2000 at gmail.com  Wed Jul 25 21:17:01 2007
From: swang2000 at gmail.com (swang)
Date: Wed, 25 Jul 2007 15:17:01 -0400
Subject: [R] Color Question about image function in graphics package
Message-ID: <52f607a50707251217p33da1df6of18ea2910b92752e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/a47ac40c/attachment.pl 

From bonenfan at biomserv.univ-lyon1.fr  Wed Jul 25 21:57:18 2007
From: bonenfan at biomserv.univ-lyon1.fr (Christophe Bonenfant)
Date: Wed, 25 Jul 2007 21:57:18 +0200
Subject: [R] Updating packages in Ubuntu feisty for 2.5
In-Reply-To: <37b5d0540707251155hf8dc7fcod713f0b435a06361@mail.gmail.com>
References: <37b5d0540707251155hf8dc7fcod713f0b435a06361@mail.gmail.com>
Message-ID: <46A7AB1E.7080701@biomserv.univ-lyon1.fr>

Hi -  I do have exactly the same problem with the same distribution 
(Ubuntu Feisty amd64) and am unable to upgrade to R 2.5.1. The 
r-base-core is version 2.4.1 in Ubuntu depositories while all others 
r-cran related packages are upgraded to R 2.5.1 (r-base-dev, r-base...). 
So far I have been unsuccessful at upgrading R. I posted on the Ubuntu 
forum but got no answer yet.

Details:

$ uname -a

Linux bronski 2.6.20-16-generic #2 SMP Thu Jun 7 19:00:28 UTC 2007 
x86_64 GNU/Linux

 > R.version
                _
platform       x86_64-pc-linux-gnu
arch           x86_64
os             linux-gnu
system         x86_64, linux-gnu
status
major          2
minor          4.1
year           2006
month          12
day            18
svn rev        40228
language       R
version.string R version 2.4.1 (2006-12-18)

Christophe


Colleen Doherty a ?crit :
> Hi,
> I would like to run R2.5.1 in Ubuntu feisty on an AMD64 chip.
> I am a total newbie to Linux.
> I have successfully compiled 2.5 however, I am not able to figure out
> how to upgrade the packages.
> I am not able to find an R-base-core_2.5.1-1_amd64 or all on the CRAN mirrors.
> I can find one on the Ubuntu feisty site, however when I try to
> install it I get a warning that Dependency is not stisfiable: libc6
> I am pretty convinced I have libc6 installed.
> When I reinstall it with apt-get It says the latest version is
> currently installed.
> I would appreciate any help or advice that y'all have.
> If this is more appropriate on another list, please let me know where
> and I will post it there.
> Thanks!
>


From edd at debian.org  Wed Jul 25 22:05:32 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 25 Jul 2007 20:05:32 +0000
Subject: [R] Updating packages in Ubuntu feisty for 2.5
In-Reply-To: <46A7AB1E.7080701@biomserv.univ-lyon1.fr>
References: <37b5d0540707251155hf8dc7fcod713f0b435a06361@mail.gmail.com>
	<46A7AB1E.7080701@biomserv.univ-lyon1.fr>
Message-ID: <20070725200531.GA17978@master.debian.org>

On Wed, Jul 25, 2007 at 09:57:18PM +0200, Christophe Bonenfant wrote:
> Hi -  I do have exactly the same problem with the same distribution 
> (Ubuntu Feisty amd64) and am unable to upgrade to R 2.5.1. The 
> r-base-core is version 2.4.1 in Ubuntu depositories while all others 
> r-cran related packages are upgraded to R 2.5.1 (r-base-dev, r-base...). 
> So far I have been unsuccessful at upgrading R. I posted on the Ubuntu 
> forum but got no answer yet.

Wrong place. Please subscribe to r-sig-debian, and ask there. Someone
may just walk you through (locally) building the R 2.5.1 package for
Debian.  After all, there *are* R 2.5.1 packages for Ubuntu, though
for 32-bit bit aka x86 intel.

Dirk

> 
> Details:
> 
> $ uname -a
> 
> Linux bronski 2.6.20-16-generic #2 SMP Thu Jun 7 19:00:28 UTC 2007 
> x86_64 GNU/Linux
> 
>  > R.version
>                 _
> platform       x86_64-pc-linux-gnu
> arch           x86_64
> os             linux-gnu
> system         x86_64, linux-gnu
> status
> major          2
> minor          4.1
> year           2006
> month          12
> day            18
> svn rev        40228
> language       R
> version.string R version 2.4.1 (2006-12-18)
> 
> Christophe
> 
> 
> Colleen Doherty a ?crit :
> > Hi,
> > I would like to run R2.5.1 in Ubuntu feisty on an AMD64 chip.
> > I am a total newbie to Linux.
> > I have successfully compiled 2.5 however, I am not able to figure out
> > how to upgrade the packages.
> > I am not able to find an R-base-core_2.5.1-1_amd64 or all on the CRAN mirrors.
> > I can find one on the Ubuntu feisty site, however when I try to
> > install it I get a warning that Dependency is not stisfiable: libc6
> > I am pretty convinced I have libc6 installed.
> > When I reinstall it with apt-get It says the latest version is
> > currently installed.
> > I would appreciate any help or advice that y'all have.
> > If this is more appropriate on another list, please let me know where
> > and I will post it there.
> > Thanks!
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Three out of two people have difficulties with fractions.


From M.J.Bojanowski at uu.nl  Wed Jul 25 22:49:05 2007
From: M.J.Bojanowski at uu.nl (Bojanowski, M.J.  (Michal))
Date: Wed, 25 Jul 2007 22:49:05 +0200
Subject: [R] initalizing and checking validity of S4 classes
Message-ID: <94E133D09AA24D43BF6341B675C01A338DCD82@uu01msg-exb01.soliscom.uu.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070725/64faa724/attachment.pl 

From toedling at ebi.ac.uk  Wed Jul 25 23:14:34 2007
From: toedling at ebi.ac.uk (Joern Toedling)
Date: Wed, 25 Jul 2007 22:14:34 +0100
Subject: [R] [BioC] Color Question about image function in graphics
	package
In-Reply-To: <52f607a50707251217p33da1df6of18ea2910b92752e@mail.gmail.com>
References: <52f607a50707251217p33da1df6of18ea2910b92752e@mail.gmail.com>
Message-ID: <46A7BD3A.1080907@ebi.ac.uk>

Hello Shiliang,

please have a look a the manual page for image (access from R by typing 
"?image" ). You can specify which colors to use and at which values to 
switch colors using the arguments breaks and colors. For example:

image(matrix(rnorm(36),4,9)+20,col=c("red","blue"),breaks=c(-1e5,20,1e5))

basically says: display every cell with a value below 20 in red 
(breaks[1:2] specify the interval for "red") and every value above 20 in 
blue
(breaks[2:3] specify the interval for "blue").

Regards,
Joern


swang wrote:
> Dear Lists:
>
> I had  a matrix which has lrt score for my microarray data. I just wonder
> how to visualize them with score over 20 in particular color such as blue.
> I cannot understand the color relationship with the value in z matrix. For
> example, If I see a red color in my image generated by image function. I
> don't know what is the value of z in that spot.
> Example:
> aaa <- matrix(rnorm(36),4,9)
> image(aaa)
>
> Can I paint the cells at which their values are higher than a certain value
> in image()?
> Or I have to use some other functions,
>
> best
>
> Shiliang
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> Bioconductor mailing list
> Bioconductor at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/bioconductor
> Search the archives: http://news.gmane.org/gmane.science.biology.informatics.conductor
>


From jjsr at sapo.pt  Wed Jul 25 18:59:44 2007
From: jjsr at sapo.pt (Joaquim J. S. Ramalho)
Date: Wed, 25 Jul 2007 17:59:44 +0100
Subject: [R] For loops
Message-ID: <001401c7cedd$336cc890$5a52fea9@FIXO>

Hi,

is there a way of simplifying the following code:

G <- rep(NA,n)

for(i in 1:n)
{
gj <- 0
for(j in 1:n)
{
for(l in 1:n)
{
for(m in 1:n)
{
gj <- gj+G.fun(XB[i]+p[3]*X[j,3]+p[4]*X[l,4]+p[5]*X[m,5],ff)
}
}
}
G[i] <- gj/n^3
}

Thanks.

Joaquim Santos


From Cody_Hamilton at Edwards.com  Thu Jul 26 01:16:30 2007
From: Cody_Hamilton at Edwards.com (Cody Hamilton)
Date: Wed, 25 Jul 2007 16:16:30 -0700
Subject: [R] Subscript out of bounds when using datadist() from Design
	library
Message-ID: <4C7D0185E1C6D64AAFB18A2281B262B805A662873E@EXIRV01.am.edwards.lcl>

I am running R version 2.4.1 on Windows XP.  I have a question regarding the datadist() function from the Design library.  I have a data.frame (call it my.data) with 4 columns.  When I submit the code

datadist(data=my.data)

I get the following error message:

Error in X[[1]] : subscript out of bounds

I suspect there may be something wrong with my data.frame (I'm certain there is nothing wrong with datadist()), but I don't know what.  Has anyone experienced the mistake I seem to be making?

Regards,
Cody Hamilton
Edwards Lifesciences


From f.harrell at vanderbilt.edu  Thu Jul 26 01:22:06 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 25 Jul 2007 18:22:06 -0500
Subject: [R] Subscript out of bounds when using datadist() from Design
 library
In-Reply-To: <4C7D0185E1C6D64AAFB18A2281B262B805A662873E@EXIRV01.am.edwards.lcl>
References: <4C7D0185E1C6D64AAFB18A2281B262B805A662873E@EXIRV01.am.edwards.lcl>
Message-ID: <46A7DB1E.5010001@vanderbilt.edu>

Cody Hamilton wrote:
> I am running R version 2.4.1 on Windows XP.  I have a question regarding the datadist() function from the Design library.  I have a data.frame (call it my.data) with 4 columns.  When I submit the code
> 
> datadist(data=my.data)

You can just use
dd <- datadist(my.data); options(datadist='dd')

If that doesn't fix it, generate a test dataset that fails or do 
save(..., compress=TRUE) and send me your dataset so I can debug.

Frank

> 
> I get the following error message:
> 
> Error in X[[1]] : subscript out of bounds
> 
> I suspect there may be something wrong with my data.frame (I'm certain there is nothing wrong with datadist()), but I don't know what.  Has anyone experienced the mistake I seem to be making?
> 
> Regards,
> Cody Hamilton
> Edwards Lifesciences
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From maitra at iastate.edu  Thu Jul 26 01:30:36 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Wed, 25 Jul 2007 18:30:36 -0500
Subject: [R] using contrasts on matrix regressions (using gmodels, perhaps)
Message-ID: <20070725183036.6bf59e9d@subarnarekha.stat.iastate.edu>

Hi, 

I want to test for a contrast from a regression where I am regressing the columns of a matrix. In short, the following.

X <- matrix(rnorm(50),10,5)
Y <- matrix(rnorm(50),10,5)
lm(Y~X)  

Call:
lm(formula = Y ~ X)

Coefficients:
             [,1]     [,2]     [,3]     [,4]     [,5]   
(Intercept)   0.3350  -0.1989  -0.1932   0.7528   0.0727
X1            0.2007  -0.8505   0.0520   0.1501   0.3248
X2            0.3212   0.7008  -0.0963  -0.2584   0.6711
X3            0.3781  -0.7321   0.1907  -0.1721   0.3073
X4           -0.1778   0.2822  -0.0644  -0.2649  -0.4140
X5           -0.1079  -0.0475   0.6047  -0.8369  -0.5928


I want to test for c'b = 0 where c is (lets say) the contrast (0, 0, 1, 0, -1). Is it possible to do so, in one shot, using gmodels or something else?

Many thanks and best wishes,
Ranjan


From ccleland at optonline.net  Thu Jul 26 01:33:29 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 25 Jul 2007 19:33:29 -0400
Subject: [R] Subscript out of bounds when using datadist() from Design
 library
In-Reply-To: <4C7D0185E1C6D64AAFB18A2281B262B805A662873E@EXIRV01.am.edwards.lcl>
References: <4C7D0185E1C6D64AAFB18A2281B262B805A662873E@EXIRV01.am.edwards.lcl>
Message-ID: <46A7DDC9.60409@optonline.net>

Cody Hamilton wrote:
> I am running R version 2.4.1 on Windows XP.  I have a question regarding the datadist() function from the Design library.  I have a data.frame (call it my.data) with 4 columns.  When I submit the code
> 
> datadist(data=my.data)
> 
> I get the following error message:
> 
> Error in X[[1]] : subscript out of bounds
> 
> I suspect there may be something wrong with my data.frame (I'm certain there is nothing wrong with datadist()), but I don't know what.  Has anyone experienced the mistake I seem to be making?

  If I follow the help page for datadist(), I think you want the following:

datadist(my.data)

  Note the following in the description of the data argument:

"Unless the first argument is a fit object, data must be an integer."

  A data frame is not a fit object, so I think that was the reason it
did not work for you.

> Regards,
> Cody Hamilton
> Edwards Lifesciences
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Cody_Hamilton at Edwards.com  Thu Jul 26 01:58:29 2007
From: Cody_Hamilton at Edwards.com (Cody Hamilton)
Date: Wed, 25 Jul 2007 16:58:29 -0700
Subject: [R] Subscript out of bounds when using datadist() from Design
 library
In-Reply-To: <46A7DDC9.60409@optonline.net>
References: <4C7D0185E1C6D64AAFB18A2281B262B805A662873E@EXIRV01.am.edwards.lcl>
	<46A7DDC9.60409@optonline.net>
Message-ID: <4C7D0185E1C6D64AAFB18A2281B262B805A662881B@EXIRV01.am.edwards.lcl>

Chuck,

Quite right.  Thank you.

Regards, -Cody

-----Original Message-----
From: Chuck Cleland [mailto:ccleland at optonline.net]
Sent: Wednesday, July 25, 2007 4:33 PM
To: Cody Hamilton
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Subscript out of bounds when using datadist() from Design library

Cody Hamilton wrote:
> I am running R version 2.4.1 on Windows XP.  I have a question regarding the datadist() function from the Design library.  I have a data.frame (call it my.data) with 4 columns.  When I submit the code
>
> datadist(data=my.data)
>
> I get the following error message:
>
> Error in X[[1]] : subscript out of bounds
>
> I suspect there may be something wrong with my data.frame (I'm certain there is nothing wrong with datadist()), but I don't know what.  Has anyone experienced the mistake I seem to be making?

  If I follow the help page for datadist(), I think you want the following:

datadist(my.data)

  Note the following in the description of the data argument:

"Unless the first argument is a fit object, data must be an integer."

  A data frame is not a fit object, so I think that was the reason it
did not work for you.

> Regards,
> Cody Hamilton
> Edwards Lifesciences
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From Soren.Hojsgaard at agrsci.dk  Thu Jul 26 02:22:29 2007
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 26 Jul 2007 02:22:29 +0200
Subject: [R] using contrasts on matrix regressions (using gmodels,
	perhaps)
References: <20070725183036.6bf59e9d@subarnarekha.stat.iastate.edu>
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC038787D2@DJFPOST01.djf.agrsci.dk>

The doBy package has an esticon function which allows you to do that. 
Regards
S?ren

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p? vegne af Ranjan Maitra
Sendt: to 26-07-2007 01:30
Til: R-help
Emne: [R] using contrasts on matrix regressions (using gmodels, perhaps)



Hi,

I want to test for a contrast from a regression where I am regressing the columns of a matrix. In short, the following.

X <- matrix(rnorm(50),10,5)
Y <- matrix(rnorm(50),10,5)
lm(Y~X) 

Call:
lm(formula = Y ~ X)

Coefficients:
             [,1]     [,2]     [,3]     [,4]     [,5]  
(Intercept)   0.3350  -0.1989  -0.1932   0.7528   0.0727
X1            0.2007  -0.8505   0.0520   0.1501   0.3248
X2            0.3212   0.7008  -0.0963  -0.2584   0.6711
X3            0.3781  -0.7321   0.1907  -0.1721   0.3073
X4           -0.1778   0.2822  -0.0644  -0.2649  -0.4140
X5           -0.1079  -0.0475   0.6047  -0.8369  -0.5928


I want to test for c'b = 0 where c is (lets say) the contrast (0, 0, 1, 0, -1). Is it possible to do so, in one shot, using gmodels or something else?

Many thanks and best wishes,
Ranjan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From kw.statr at gmail.com  Thu Jul 26 05:12:25 2007
From: kw.statr at gmail.com (Kevin Wright)
Date: Wed, 25 Jul 2007 22:12:25 -0500
Subject: [R] How to auto-scale cex of y-axis labels in lattice dotplot?
Message-ID: <c968588d0707252012g3cd7cb19k3562e50e598c716c@mail.gmail.com>

When I create a dotplot in lattice, I frequently observe overplotting
of the labels along the vertical axis.  On my screen, this illustrates
overplotting of the letters:

windows()
reps=6
dat=data.frame(let=rep(letters,each=reps), grp=rep(1:reps, 26),
  y=runif(26*reps))
dotplot(let~y|grp, dat)

Is there a way to automatically scale the labels so that they are not
over-plotted?

I currently do something like this:
Calculate or guess the number of panel rows: NumPanelRows
cexLab <- min(1, .9*par()$pin[2]/
              (nlevels(dat$let)*NumPanelRows*strheight("A",units="in")))
dotplot(..., scales=list(y=list(cex=cexLab))

Is there an easier way?

Is there a function that I can call which calculates the layout of the
panels that will be used in the dotplot?

Any tips will be appreciated.

K Wright


From berwin at maths.uwa.edu.au  Thu Jul 26 07:20:54 2007
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Thu, 26 Jul 2007 13:20:54 +0800
Subject: [R] question on using "gl1ce" from "lasso2" package
In-Reply-To: <813cce770707242150j79ae5228x1895f599ea723096@mail.gmail.com>
References: <813cce770707242150j79ae5228x1895f599ea723096@mail.gmail.com>
Message-ID: <20070726132054.32f65a7a@berwin5>

G'day Li,

On Wed, 25 Jul 2007 00:50:43 -0400
"Li Li" <lilycai2007 at gmail.com> wrote:

> I tried several settings by using the "family=gaussian"
> in "gl1ce", but none of them works. [...]
> > gl1ce(Petal.Width~Sepal.Length+Sepal.Width+Petal.Length,
> > data=iris,family=gaussian())
> Error in eval(expr, envir, enclos) : object "etastart" not found
> 
> Does anyone have experience with this function?
> Any help will be appreciated,

Actually,

> gl1ce(Petal.Width~Sepal.Length+Sepal.Width+Petal.Length, data=iris,
+ family=gaussian)

should work.  No need to say `family=gaussian()'.

However, omitting the brackets leads to an even more obscure error
message and using traceback() makes me believe that the function
`family()' must have been changed since this code was ported from
S-Plus to R.  

The version with brackets does not seem to work since the function that
tries to determine initial values for the iterative process of
fitting a GLM tries to access an object called `etastart', which exist
in the list of formal parameters of glm() but is not a formal parameter
of gl1ce(). I am not sure whether this problem always existed or is also
new due to changes in glm() and accompanying functions.  (Time to
re-work the whole lasso2 package, I guess.)

A way to solve your problem is to issue the following commands:

> etastart <- NULL
> gl1ce(Petal.Width~Sepal.Length+Sepal.Width+Petal.Length, data=iris,
+ family=gaussian())

However, since you are using the gaussian family, why not use l1ce()
directly?  The following command leads to the same output:

> l1ce(Petal.Width~Sepal.Length+Sepal.Width+Petal.Length, data=iris,
+ absolute=TRUE)

Hope this helps.

Cheers,

	Berwin

=========================== Full address =============================
Berwin A Turlach                            Tel.: +65 6515 4416 (secr)
Dept of Statistics and Applied Probability        +65 6515 6650 (self)
Faculty of Science                          FAX : +65 6872 3919       
National University of Singapore
6 Science Drive 2, Blk S16, Level 7          e-mail: statba at nus.edu.sg
Singapore 117546                    http://www.stat.nus.edu.sg/~statba


From rithesh.m at brickworkindia.com  Thu Jul 26 07:56:46 2007
From: rithesh.m at brickworkindia.com (Rithesh M. Mohan)
Date: Thu, 26 Jul 2007 11:26:46 +0530
Subject: [R] ROC curve in R
Message-ID: <4BC833C82CD3564298A3A24BD299C0C8B219B8@pioneer.brickworkindia.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/dd5ff944/attachment.pl 

From juliane_willert at web.de  Thu Jul 26 01:20:45 2007
From: juliane_willert at web.de (Juliane Willert)
Date: Thu, 26 Jul 2007 01:20:45 +0200
Subject: [R] colored heights in 3D plot (persp)
Message-ID: <46A7DACD.4050008@web.de>

Hello everybody,

I have a matrix with measurement values and plot them with persp.
I want to highlight different heights in different colors. At least 
everything above and under a certain z-level shall have a different 
color to make the differences in height more obvious.
How can I do that or do I have to use another package?

Best regards,
Juliane


From timpanisterwombat at hotmail.com  Thu Jul 26 05:35:00 2007
From: timpanisterwombat at hotmail.com (jenny tan)
Date: Thu, 26 Jul 2007 11:35:00 +0800
Subject: [R] Finding matches in 2 files
Message-ID: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/77a9143d/attachment.pl 

From gyadav at ccilindia.co.in  Thu Jul 26 08:58:38 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Thu, 26 Jul 2007 12:28:38 +0530
Subject: [R] ROC curve in R
In-Reply-To: <4BC833C82CD3564298A3A24BD299C0C8B219B8@pioneer.brickworkindia.
	local>
Message-ID: <OFA090DBBC.D4F33858-ON65257324.00263229-65257324.00265332@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/c6799f67/attachment.pl 

From ligges at statistik.uni-dortmund.de  Thu Jul 26 09:15:21 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Jul 2007 09:15:21 +0200
Subject: [R] qda(MASS) function error
In-Reply-To: <46A777AD.6010006@irpi.cnr.it>
References: <46A777AD.6010006@irpi.cnr.it>
Message-ID: <46A84A09.3050002@statistik.uni-dortmund.de>



Mauro Rossi wrote:
> Dear R user,
>  I'm using qda (quadratic discriminant analysis) function (package MASS) 
> to classify 58 explanatory variables (numeric type with different 
> ranges) using a grouping variable (factor 2 levels "0" "1"). I'm using 
> the qda method for class 'data.frame' (in this way I don't need to 
> specify a formula).
> Using the function:
> result.qda<-qda(explanatory.variables, grouping.variable, method="moment")
> I obtain the following error message:
> "Error in qda.default(x, grouping, ...) : rank deficiency in group 0"
> I run the script excluding some variables and I've individuated 2 
> explanatory variables that give problems, but  I don't understand why 
> they give them. The two excluded variables are numeric with two possible 
> values: 0 and 1, but in the rest of group of  variables, some similar 
> variables are considered.
> 
> I don't have this problem using lda  function for linear discriminant 
> analysis.
> 
> What does this error message mean?
> What types of variables does qda function consider?

Well, qda assumes real values (and not factors) in the explanatory 
variables. If you think it makes sense to ignore this assumption (and I 
doubt it makes sense), then the error message tells you there is a rank 
deficiency, i.e. some variables might be collinear.
Hence at least one of the covariance matrices cannot be inverted.

Uwe Ligges


> Thank in advance,
> Mauro Rossi
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From christophe at pallier.org  Thu Jul 26 09:43:32 2007
From: christophe at pallier.org (Christophe Pallier)
Date: Thu, 26 Jul 2007 09:43:32 +0200
Subject: [R] Finding matches in 2 files
In-Reply-To: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>
References: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>
Message-ID: <dea6cb960707260043t69a3cc65v8667cf893920d793@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/9d17883b/attachment.pl 

From christophe at pallier.org  Thu Jul 26 09:46:38 2007
From: christophe at pallier.org (Christophe Pallier)
Date: Thu, 26 Jul 2007 09:46:38 +0200
Subject: [R] Finding matches in 2 files
In-Reply-To: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>
References: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>
Message-ID: <dea6cb960707260046v3a7b2323h51adf766794bf5ab@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/f7d5b2cb/attachment.pl 

From kamauallan at yahoo.com  Thu Jul 26 10:03:17 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Thu, 26 Jul 2007 01:03:17 -0700 (PDT)
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
Message-ID: <812512.57020.qm@web53504.mail.re2.yahoo.com>

Thanks so much Jim, Andaikalavan, Gabor and others for the help and suggestions.
The solution will result in a matrix containing nested matrices to enable each variable name, each variables distinct value and the count of the distinct value to be accessible individually.
The main matrix will contain the variable names, the first level nested matrices will consist of the variables unique values, and each such variable entry will contain a one element vector to contain the count or occurrence frequency.
This matrix can now be used in comparing other similar datasets for variable values and their frequencies.

Building on the input received so far, a probable solution in building the matrix will include the following.


1)I reading the csv file (containing column headers)
>my_data=read.table("<path/to/my/data.csv>",header=TRUE,sep=",",dec=".",fill=TRUE)

2)I group the values in each variable producing an occurrence count(frequency)
>x.val<-apply(my_data,2,table)

3)I obtain a vector of the names of the variables in the table
>names(x.val)

4)Now I make use of the names (obtained in step 3) to obtain a vector of distinct values in a given variable (in the example below the variable name is $PR14)
>names(v.val$PR14)

5)I obtain a vector (with one element) of the frequency of a value obtained from the step above (in our example the value is "V")
>as.vector(x.val$PR14["V"])

Todo:
Now I will need to place the steps above in a script (consisting of loops) to build the matrix, step 4 and 5 seem tricky to do programatically.

Allan.


----- Original Message ----
From: jim holtman <jholtman at gmail.com>
To: Allan Kamau <kamauallan at yahoo.com>
Cc: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>; r-help at stat.math.ethz.ch
Sent: Wednesday, July 25, 2007 1:50:55 PM
Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.

Also if you want to access the individual values, you can just leave
it as a list:

> x.val <- apply(x, 2, table)
> # access each value
> x.val$PR14["V"]
V
8



On 7/25/07, Allan Kamau <kamauallan at yahoo.com> wrote:
> A subset of the data looks as follows
>
> > df[1:10,14:20]
>   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> 1     V    T    I    K    V    G    D
> 2     V    S    I    K    V    G    G
> 3     V    T    I    R    V    G    G
> 4     V    S    I    K    I    G    G
> 5     V    S    I    K    V    G    G
> 6     V    S    I    R    V    G    G
> 7     V    T    I    K    I    G    G
> 8     V    S    I    K    V    E    G
> 9     V    S    I    K    V    G    G
> 10    V    S    I    K    V    G    G
>
> The result I would like is as follows
>
> PR10        PR11          PR12   ...
> [V:10]    [S:7,T:3]    [I:10]
>
> The result can be in a matrix or a vector and each variablename, value and frequency should be accessible so as to be used for comparisons with another dataset later.
> The frequency can be a count or a percentage.
>
>
> Allan.
>
>
> ----- Original Message ----
> From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> To: Allan Kamau <kamauallan at yahoo.com>
> Cc: r-help at stat.math.ethz.ch
> Sent: Tuesday, July 24, 2007 10:21:51 PM
> Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
>
> The name of the table should give you the "value". And if you have a
> matrix, you just need to convert it into a vector first.
>
>  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ], nc=3 )
>  > m
>      [,1] [,2] [,3]
> [1,] "A"  "C"  "B"
> [2,] "B"  "D"  "C"
> [3,] "C"  "E"  "D"
>  > tb <- table( as.vector(m) )
>  > tb
>
> A B C D E
> 1 2 3 2 1
>  > paste( names(tb), ":", tb, sep="" )
> [1] "A:1" "B:2" "C:3" "D:2" "E:1"
>
> If this is not what you want, then please give a simple example.
>
> Regards, Adai
>
>
>
> Allan Kamau wrote:
> > Hi all,
> > If the question below as been answered before I
> > apologize for the posting.
> > I would like to get the frequencies of occurrence of
> > all values in a given variable in a multivariate
> > dataset. In short for each variable (or field) a
> > summary of values contained with in a value:frequency
> > pair, there can be many such pairs for a given
> > variable. I would like to do the same for several such
> > variables.
> > I have used table() but am unable to extract the
> > individual value and frequency values.
> > Please advise.
> >
> > Allan.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From tobias.sing at gmail.com  Thu Jul 26 10:11:37 2007
From: tobias.sing at gmail.com (Tobias Sing)
Date: Thu, 26 Jul 2007 10:11:37 +0200
Subject: [R] ROC curve in R
In-Reply-To: <4BC833C82CD3564298A3A24BD299C0C8B219B8@pioneer.brickworkindia.local>
References: <4BC833C82CD3564298A3A24BD299C0C8B219B8@pioneer.brickworkindia.local>
Message-ID: <c3ca233a0707260111v538f9d26u21386fe86b5daa3@mail.gmail.com>

You might also want to try the ROCR package (http://rocr.bioinf.mpi-sb.mpg.de/).
Tutorial slides: http://rocr.bioinf.mpi-sb.mpg.de/ROCR_Talk_Tobias_Sing.ppt
Overview paper:
http://bioinformatics.oxfordjournals.org/cgi/content/full/21/20/3940

Good luck,
  Tobias


On 7/26/07, Rithesh M. Mohan <rithesh.m at brickworkindia.com> wrote:
> Hi,
>
>
>
> I need to build ROC curve in R, can you please provide data steps / code
> or guide me through it.
>
>
>
> Thanks and Regards
>
> Rithesh M Mohan
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Tobias Sing
Computational Biology and Applied Algorithmics
Max Planck Institute for Informatics
Saarbrucken, Germany
Phone: +49 681 9325 315
Fax: +49 681 9325 399
http://www.tobiassing.net


From pburns at pburns.seanet.com  Thu Jul 26 10:17:51 2007
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 26 Jul 2007 09:17:51 +0100
Subject: [R] For loops
In-Reply-To: <001401c7cedd$336cc890$5a52fea9@FIXO>
References: <001401c7cedd$336cc890$5a52fea9@FIXO>
Message-ID: <46A858AF.7080401@pburns.seanet.com>

Any time you are calling a function one value at a time,
it is worth asking if you can eliminate a loop (or more).

If 'G.fun' is vectorized in its first argument, then you can
easily get rid of the three inner loops.  Just generate a
vector of all of the values and do:

gj <- sum(G.fun(long.vector, ff))

If 'G.fun' is not vectorized and can't be vectorized, then
you might save some time by still creating a vector of the
first argument first.  Whether that will be a significant
reduction depends on how time consuming 'G.fun' is.

There is a caveat.  If 'n' is large, then you could create a
vector that strains the amount of memory (RAM) that you
have.  If that is the case, then there will be some compromise
between loops and vectorization that will be optimal.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Joaquim J. S. Ramalho wrote:

>Hi,
>
>is there a way of simplifying the following code:
>
>G <- rep(NA,n)
>
>for(i in 1:n)
>{
>gj <- 0
>for(j in 1:n)
>{
>for(l in 1:n)
>{
>for(m in 1:n)
>{
>gj <- gj+G.fun(XB[i]+p[3]*X[j,3]+p[4]*X[l,4]+p[5]*X[m,5],ff)
>}
>}
>}
>G[i] <- gj/n^3
>}
>
>Thanks.
>
>Joaquim Santos
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From bruno.c at inwind.it  Thu Jul 26 10:47:00 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Thu, 26 Jul 2007 10:47:00 +0200
Subject: [R] Submatrices Extraction
Message-ID: <JLS32C$9B67115B10B78A62FC0D09AFC3E8CBB2@libero.it>

Hello,
Given a submatrix containing 0 or 1
I need to extract the indexes of all the diagonal submatrices 
so one of the two diagonals must contains only 1 for each submatrix ...
Any help?

Thanks in advance
Bruno


------------------------------------------------------
Scegli infostrada: ADSL gratis per tutta l?estate e telefoni senza canone Telecom


From john.seers at bbsrc.ac.uk  Thu Jul 26 11:00:41 2007
From: john.seers at bbsrc.ac.uk (john seers (IFR))
Date: Thu, 26 Jul 2007 10:00:41 +0100
Subject: [R] Finding matches in 2 files
In-Reply-To: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>
References: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>
Message-ID: <AAD49F46EAE3F6479E1D46428FAC31CB0181AB7C@NBIE2KSRV1.nbi.bbsrc.ac.uk>

 

Something like:

# Sample data
g1<-c("gene1", "gene2", "gene3", "gene4", "gene5", "gene9", "gene10",
"geneA")
g2<-c("gene6", "gene9", "gene1", "gene2", "gene7", "gene8", "gene9",
"gene1", "gene10")
df1<-cbind(gene=g1, expr=runif(length(g1)))
df2<-cbind(gene=g2, expr=runif(length(g2)))

# Merge
mdf<-merge(df1, df2, by="gene", sort=T)
# Unique list
ug<-unique(mdf[,"gene"])


You may find the "match" command useful and/or the "%in%" opertaor.


JS 




 
---
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jenny tan
Sent: 26 July 2007 04:35
To: r-help at stat.math.ethz.ch
Subject: [R] Finding matches in 2 files



I have 2 files containing data analysed by 2 different methods. I would
like to find out which genes appear in both analyses. Can someone show
me how to do this?
_________________________________________________________________
[[trailing spam removed]]

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From gathibandhe.vaibhav at gmail.com  Thu Jul 26 11:46:22 2007
From: gathibandhe.vaibhav at gmail.com (Vaibhav Gathibandhe)
Date: Thu, 26 Jul 2007 04:46:22 -0500
Subject: [R] Regression with Missing values. na.action?
Message-ID: <d09859290707260246w6017ae90yb652f4eb4ee9cc89@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/866c8be5/attachment.pl 

From mothsailor at googlemail.com  Thu Jul 26 11:57:58 2007
From: mothsailor at googlemail.com (David Barron)
Date: Thu, 26 Jul 2007 10:57:58 +0100
Subject: [R] Regression with Missing values. na.action?
In-Reply-To: <d09859290707260246w6017ae90yb652f4eb4ee9cc89@mail.gmail.com>
References: <d09859290707260246w6017ae90yb652f4eb4ee9cc89@mail.gmail.com>
Message-ID: <815b70590707260257m4f035a49r505b454ce6a12269@mail.gmail.com>

na.exclude should give the same results as na.omit, which is the
default na.action.  Is the number of complete cases the same in these
two regressions?

On 26/07/07, Vaibhav Gathibandhe <gathibandhe.vaibhav at gmail.com> wrote:
> Hi all,
>
> Can you please tell me what is the problem here.
>
> My regression eq is  y = B0 + B1X1 +B2X2 +e
> And i am interested in coefficient B1
>
> I am doing regression with two cases:
>
> 1) reg<-lm(y ~ X1 + X2, sam) where sam is the data
>
> 2) reg<-lm(y ~ X1 + X2, sam, na.action= na.exclude) . I have missing values
> in X1
>
>
> but the values of coefficient is not consistent in two cases.
>
> Actually B1 in case one sould be smaller than B1 in case 2. But sometimes it
> comes greater.
>
> I can't figure it out. Is there some problem with *na.action ? *My sample
> size is 100
>
>
> *Regards,*
> *Vaibhav*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
=================================
David Barron
Said Business School
University of Oxford
Park End Street
Oxford OX1 1HP


From jessen at econinfo.de  Thu Jul 26 12:43:40 2007
From: jessen at econinfo.de (Owe Jessen)
Date: Thu, 26 Jul 2007 12:43:40 +0200
Subject: [R] Download multiple stock quotes in a loop
Message-ID: <46A87ADC.7080506@econinfo.de>

Hi all,

this should be a simple question, but I haven't been able to do it 
right. I am trying to download multiple stock quotes in a loop, so that 
every timeseries is safed with the symbol of the stock. Can anybody help 
me out? Here's the code:

require(tseries)
startd <- "2000-06-01"
stocks <- c("bmw.de", "vow.de", "dte.de")
for(stock in stocks)
stock <- as.timeSeries(get.hist.quote(instrument=stock, start=startd, 
quote="Close", compress="d"))
}

Thanks in advance,
Owe

-- 
Owe Jessen
Diplom-Volkswirt
Hanssenstra?e 17
24106 Kiel

jessen at econinfo.de
http://www.econinfo.de


From wl2776 at gmail.com  Thu Jul 26 12:51:45 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Thu, 26 Jul 2007 03:51:45 -0700 (PDT)
Subject: [R] Download multiple stock quotes in a loop
In-Reply-To: <46A87ADC.7080506@econinfo.de>
References: <46A87ADC.7080506@econinfo.de>
Message-ID: <11808177.post@talk.nabble.com>




Owe Jessen wrote:
> 
> Hi all,
> 
> this should be a simple question, but I haven't been able to do it 
> right. I am trying to download multiple stock quotes in a loop, so that 
> every timeseries is safed with the symbol of the stock. Can anybody help 
> me out? Here's the code:
> 
> require(tseries)
> startd <- "2000-06-01"
> stocks <- c("bmw.de", "vow.de", "dte.de")
> for(stock in stocks)
> stock <- as.timeSeries(get.hist.quote(instrument=stock, start=startd, 
> quote="Close", compress="d"))
> }
> 
> Thanks in advance,
> Owe
> 

The variable stock is assigned values twice in the cycle.
First, it gets the value of "bmw.de", and immediately after that it is
assigned with the result returned by as.timeSeries(  ... )

If you replace the interior of the loop with the

  assign(paste("stock.",stock,sep=""), as.timeSeries(get.hist.quote  [etc]))

you will get three variables, namely, stock.bmw.de, stock.vow.de and
stock.dte.de.
-- 
View this message in context: http://www.nabble.com/Download-multiple-stock-quotes-in-a-loop-tf4150838.html#a11808177
Sent from the R help mailing list archive at Nabble.com.


From amberti at inwind.it  Thu Jul 26 09:26:16 2007
From: amberti at inwind.it (Daniele Amberti)
Date: Thu, 26 Jul 2007 09:26:16 +0200
Subject: [R] multiple graphs
Message-ID: <JLRZBS$887F30CDFDF5C77E80D913136AD02F1C@libero.it>

Does anyone have a simple explanation and example on how to add histograms or barcharts to an other graph like in the example at the R-graph gallery:

http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=109

looking at the code I'not undertand very well how to add graphs in arbitrary/clever position with an adequate scale.

If somebody have a simplier example with explanations it will be highly appreciate.

Best
Daniele


------------------------------------------------------
Scegli infostrada: ADSL gratis per tutta l?estate e telefoni senza canone Telecom


From jholtman at gmail.com  Thu Jul 26 14:20:15 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 26 Jul 2007 08:20:15 -0400
Subject: [R] Finding matches in 2 files
In-Reply-To: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>
References: <BAY117-W22D4B21444E798399C782BBBF20@phx.gbl>
Message-ID: <644e1f320707260520t75326cc1u9b59faf9777ec984@mail.gmail.com>

Is this what you want?

> g1<-c("gene1", "gene2", "gene3", "gene4", "gene5", "gene9", "gene10",
+ "geneA")
> g2<-c("gene6", "gene9", "gene1", "gene2", "gene7", "gene8", "gene9",
+ "gene1", "gene10")
> intersect(g1,g2)
[1] "gene1"  "gene2"  "gene9"  "gene10"


On 7/25/07, jenny tan <timpanisterwombat at hotmail.com> wrote:
>
>
> I have 2 files containing data analysed by 2 different methods. I would like to find out which genes appear in both analyses. Can someone show me how to do this?
> _________________________________________________________________
> [[trailing spam removed]]
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From MMSullivan at ric.edu  Thu Jul 26 14:34:32 2007
From: MMSullivan at ric.edu (Sullivan, Mary M)
Date: Thu, 26 Jul 2007 08:34:32 -0400
Subject: [R] logistic regression
Message-ID: <38329C144184084CA2E572C51002D8FECEF6D1@mailsvr1.RICOL.EDU>

Greetings,
 

I am working on a logistic regression model in R and I am struggling with the code, as it is a relatively new program for me.  In searching Google for 'logistic regression diagnostics' I came Elizabeth Brown's Lecture 14 from her Winter 2004 Biostatistics 515 course  (http://courses.washington.edu/b515/l14.pdf) .  I found most of the code to be very helpful, but I am struggling with the lines on to calculate the observed and expected values in the 10 groups created by the cut function.  I get error messages in trying to create the E and O matrices:  R won't accept assignment of "fi1c==j" and it won't calculate the sum.  

 

I am wondering whether someone might be able to offer me some assistance...my search of the archives was not fruitful.

 

Here is the code that I adapted from the lecture notes:

 

fit <- fitted(glm.lyme)

fitc <- cut(fit, br = c(0, quantile(fit, p = seq(.1, .9, .1)),1))  

t<-table(fitc)

fitc <- cut(fit, br = c(0, quantile(fit, p = seq(.1, .9, .1)), 1), labels = F)

t<-table(fitc)

 

#Calculate observed and expected values in ea group

E <- matrix(0, nrow=10, ncol = 2)

O <- matrix(0, nrow=10, ncol=2)

for (j in 1:10) {

      E[j, 2] = sum(fit[fitc==j])

      E[j, 1] = sum((1- fit)[fitc==j])

      O[j, 2] = sum(pcdata$lymdis[fitc==j])

      O[j, 1] = sum((1-pcdata$lymdis)[fitc==j])

            

}

 

Here is the error message:  Error in Summary.factor(..., na.rm = na.rm) : 
        sum not meaningful for factors

 

 

I understand what it means; I just can't figure out how to get around it or how to get the output printed in table form.  Thank you in advance for any assistance.

 

Mary Sullivan


From f.harrell at vanderbilt.edu  Thu Jul 26 15:01:43 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 26 Jul 2007 08:01:43 -0500
Subject: [R] ROC curve in R
In-Reply-To: <OFA090DBBC.D4F33858-ON65257324.00263229-65257324.00265332@ccilindia.co.in>
References: <OFA090DBBC.D4F33858-ON65257324.00263229-65257324.00265332@ccilindia.co.in>
Message-ID: <46A89B37.9090707@vanderbilt.edu>

Note that even though the ROC curve as a whole is an interesting 
'statistic' (its area is a linear translation of the 
Wilcoxon-Mann-Whitney-Somers-Goodman-Kruskal rank correlation 
statistics), each individual point on it is an improper scoring rule, 
i.e., a rule that is optimized by fitting an inappropriate model.  Using 
curves to select cutoffs is a low-precision and arbitrary operation, and 
the cutoffs do not replicate from study to study.  Probably the worst 
problem with drawing an ROC curve is that it tempts analysts to try to 
find cutoffs where none really exist, and it makes analysts ignore the 
whole field of decision theory.

Frank Harrell


gyadav at ccilindia.co.in wrote:
> http://search.r-project.org/cgi-bin/namazu.cgi?query=ROC&max=20&result=normal&sort=score&idxname=Rhelp02a&idxname=functions&idxname=docs
> 
> there is a lot of help try help.search("ROC curve") gave
> Help files with alias or concept or title matching 'ROC curve' using fuzzy 
> matching:
> 
> 
> 
> granulo(ade4)                             Granulometric Curves
> plot.roc(analogue)                        Plot ROC curves and associated 
> diagnostics
> roc(analogue)                             ROC curve analysis
> colAUC(caTools)                           Column-wise Area Under ROC Curve 
> (AUC)
> DProc(DPpackage)                          Semiparametric Bayesian ROC 
> curve analysis
> cv.enet(elasticnet)                       Computes K-fold cross-validated 
> error curve for elastic net
> ROC(Epi)                                  Function to compute and draw 
> ROC-curves.
> lroc(epicalc)                             ROC curve
> cv.lars(lars)                             Computes K-fold cross-validated 
> error curve for lars
> roc.demo(TeachingDemos)                   Demonstrate ROC curves by 
> interactively building one
> 
> HTH
> see the help and examples those will suffice
> 
> Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.
> 
> 
> 
> Regards,
> 
> Gaurav Yadav
> +++++++++++
> Assistant Manager, CCIL, Mumbai (India)
> Mob: +919821286118 Email: emailtogauravyadav at gmail.com
> Bhagavad Gita:  Man is made by his Belief, as He believes, so He is
> 
> 
> 
> "Rithesh M. Mohan" <rithesh.m at brickworkindia.com> 
> Sent by: r-help-bounces at stat.math.ethz.ch
> 07/26/2007 11:26 AM
> 
> To
> <R-help at stat.math.ethz.ch>
> cc
> 
> Subject
> [R] ROC curve in R
> 
> 
> 
> 
> 
> 
> Hi,
> 
>  
> 
> I need to build ROC curve in R, can you please provide data steps / code
> or guide me through it.
> 
>  
> 
> Thanks and Regards
> 
> Rithesh M Mohan
> 
> 
>                  [[alternative HTML version deleted]]
> 
-
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From Mike.Lawrence at DAL.CA  Thu Jul 26 15:06:59 2007
From: Mike.Lawrence at DAL.CA (Mike Lawrence)
Date: Thu, 26 Jul 2007 10:06:59 -0300
Subject: [R] logistic regression
In-Reply-To: <38329C144184084CA2E572C51002D8FECEF6D1@mailsvr1.RICOL.EDU>
References: <38329C144184084CA2E572C51002D8FECEF6D1@mailsvr1.RICOL.EDU>
Message-ID: <46DAC696-506F-4E83-864B-85AC90AC5D9D@DAL.CA>

Maybe try making sure the data is numeric:

fac.to.num=function(x) as.numeric(as.character(x))


On 26-Jul-07, at 9:34 AM, Sullivan, Mary M wrote:

> Greetings,
>
>
> I am working on a logistic regression model in R and I am  
> struggling with the code, as it is a relatively new program for  
> me.  In searching Google for 'logistic regression diagnostics' I  
> came Elizabeth Brown's Lecture 14 from her Winter 2004  
> Biostatistics 515 course  (http://courses.washington.edu/b515/ 
> l14.pdf) .  I found most of the code to be very helpful, but I am  
> struggling with the lines on to calculate the observed and expected  
> values in the 10 groups created by the cut function.  I get error  
> messages in trying to create the E and O matrices:  R won't accept  
> assignment of "fi1c==j" and it won't calculate the sum.
>
>
>
> I am wondering whether someone might be able to offer me some  
> assistance...my search of the archives was not fruitful.
>
>
>
> Here is the code that I adapted from the lecture notes:
>
>
>
> fit <- fitted(glm.lyme)
>
> fitc <- cut(fit, br = c(0, quantile(fit, p = seq(.1, .9, .1)),1))
>
> t<-table(fitc)
>
> fitc <- cut(fit, br = c(0, quantile(fit, p = seq(.1, .9, .1)), 1),  
> labels = F)
>
> t<-table(fitc)
>
>
>
> #Calculate observed and expected values in ea group
>
> E <- matrix(0, nrow=10, ncol = 2)
>
> O <- matrix(0, nrow=10, ncol=2)
>
> for (j in 1:10) {
>
>       E[j, 2] = sum(fit[fitc==j])
>
>       E[j, 1] = sum((1- fit)[fitc==j])
>
>       O[j, 2] = sum(pcdata$lymdis[fitc==j])
>
>       O[j, 1] = sum((1-pcdata$lymdis)[fitc==j])
>
>
>
> }
>
>
>
> Here is the error message:  Error in Summary.factor(..., na.rm =  
> na.rm) :
>         sum not meaningful for factors
>
>
>
>
>
> I understand what it means; I just can't figure out how to get  
> around it or how to get the output printed in table form.  Thank  
> you in advance for any assistance.
>
>
>
> Mary Sullivan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Mike Lawrence
Graduate Student, Department of Psychology, Dalhousie University

Website: http://memetic.ca

Public calendar: http://icalx.com/public/informavore/Public

"The road to wisdom? Well, it's plain and simple to express:
Err and err and err again, but less and less and less."
	- Piet Hein


From f.harrell at vanderbilt.edu  Thu Jul 26 15:13:10 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 26 Jul 2007 08:13:10 -0500
Subject: [R] logistic regression
In-Reply-To: <38329C144184084CA2E572C51002D8FECEF6D1@mailsvr1.RICOL.EDU>
References: <38329C144184084CA2E572C51002D8FECEF6D1@mailsvr1.RICOL.EDU>
Message-ID: <46A89DE6.7080407@vanderbilt.edu>

Mary,

The 10-group approach results in a low-resolution and fairly arbitrary 
calibration curve.  Also, it is the basis of the original 
Hosmer-Lemeshow goodness of fit statistic which has been superceded by 
the Hosmer et al single degree of freedom GOF test that does not require 
any binning.  The Design package handles both.  Do ?calibrate.lrm, 
?residuals.lrm, ?lrm for details.

Frank Harrell


Sullivan, Mary M wrote:
> Greetings,
>  
> 
> I am working on a logistic regression model in R and I am struggling with the code, as it is a relatively new program for me.  In searching Google for 'logistic regression diagnostics' I came Elizabeth Brown's Lecture 14 from her Winter 2004 Biostatistics 515 course  (http://courses.washington.edu/b515/l14.pdf) .  I found most of the code to be very helpful, but I am struggling with the lines on to calculate the observed and expected values in the 10 groups created by the cut function.  I get error messages in trying to create the E and O matrices:  R won't accept assignment of "fi1c==j" and it won't calculate the sum.  
> 
>  
> 
> I am wondering whether someone might be able to offer me some assistance...my search of the archives was not fruitful.
> 
>  
> 
> Here is the code that I adapted from the lecture notes:
> 
>  
> 
> fit <- fitted(glm.lyme)
> 
> fitc <- cut(fit, br = c(0, quantile(fit, p = seq(.1, .9, .1)),1))  
> 
> t<-table(fitc)
> 
> fitc <- cut(fit, br = c(0, quantile(fit, p = seq(.1, .9, .1)), 1), labels = F)
> 
> t<-table(fitc)
> 
>  
> 
> #Calculate observed and expected values in ea group
> 
> E <- matrix(0, nrow=10, ncol = 2)
> 
> O <- matrix(0, nrow=10, ncol=2)
> 
> for (j in 1:10) {
> 
>       E[j, 2] = sum(fit[fitc==j])
> 
>       E[j, 1] = sum((1- fit)[fitc==j])
> 
>       O[j, 2] = sum(pcdata$lymdis[fitc==j])
> 
>       O[j, 1] = sum((1-pcdata$lymdis)[fitc==j])
> 
>             
> 
> }
> 
>  
> 
> Here is the error message:  Error in Summary.factor(..., na.rm = na.rm) : 
>         sum not meaningful for factors
> 
>  
> 
>  
> 
> I understand what it means; I just can't figure out how to get around it or how to get the output printed in table form.  Thank you in advance for any assistance.
> 
>  
> 
> Mary Sullivan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University


From petr.pikal at precheza.cz  Thu Jul 26 15:40:52 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Thu, 26 Jul 2007 15:40:52 +0200
Subject: [R] Odp:  multiple graphs
In-Reply-To: <JLRZBS$887F30CDFDF5C77E80D913136AD02F1C@libero.it>
Message-ID: <OFC83B7F64.AE625210-ONC1257324.0046F102-C1257324.004B1DD8@precheza.cz>

Hi

this particular graph is a combination of several approaches
see

layout # how to split plot window (or ?split)
par(new=TRUE) # how to plot several times to the same window without 
erasing previous plot

and of course sophisticated use of all other stuff which is available in 
R.

See also

par(fig=...)

plot(1:10)
par(fig=c(0.1,.5,0.1,.5), new=T)
boxplot(rnorm(10))

Petr

r-help-bounces at stat.math.ethz.ch napsal dne 26.07.2007 09:26:16:

> Does anyone have a simple explanation and example on how to add 
histograms or 
> barcharts to an other graph like in the example at the R-graph gallery:
> 
> http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=109
> 
> looking at the code I'not undertand very well how to add graphs in 
> arbitrary/clever position with an adequate scale.
> 
> If somebody have a simplier example with explanations it will be highly 
appreciate.
> 
> Best
> Daniele
> 
> 
> ------------------------------------------------------
> Scegli infostrada: ADSL gratis per tutta l?estate e telefoni senza 
canone Telecom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ottorino-luca.pantani at unifi.it  Thu Jul 26 15:40:40 2007
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Thu, 26 Jul 2007 15:40:40 +0200
Subject: [R] substituting dots in the names of the columns (sub, gsub,
	regexpr)
Message-ID: <46A8A458.9090600@unifi.it>

Dear R users,
I have the following two problems, related to the function sub, grep, 
regexpr and similia.

The header of the file(s) I have to import is like this.

c("y (m)", "BD (g/cm3)", "PR (Mpa)", "Ks (m/s)", "SP g./g.", "P 
(m3/m3)", "theta1 (g/g)", "theta2 (g/g)", "AWC (g/g)")

To get rid of spaces and symbols in the names of the columns,
I use read.table(... check.names=TRUE) and I get:
str <- c("y..m.", "BD..g.cm3.", "PR..Mpa.", "Ks..m.s.", "SP.g..g.", 
"P..m3.m3.", "theta1..g.g.", "theta2..g.g.", "AWC..g.g.")

Now, my problem is to remove the trailing dots, as well as the double 
dots, in order to get the names like the following
c("y.m", "BD.g.cm3", "PR.Mpa", "Ks.m.s", "SP.g.g", "P.m3.m3.", 
"theta1.g.g", "theta2.g.g", "AWC.g.g")

I've searched the help pages for sub, regexpr and similia, and also 
searched the help archives.
I understand that the dot is a peculiar sign since
sub("..", ".", str)
[1] "..m."        "...g.cm3."   "...Mpa."     "...m.s."     "..g..g."   
[6] "..m3.m3."    ".eta1..g.g." ".eta2..g.g." ".C..g.g."  

Therefore I tried
sub("\\..", ".", str)
[1] "y.m."        "BD.g.cm3."   "PR.Mpa."     "Ks.m.s."     "SP...g."   
[6] "P.m3.m3."    "theta1.g.g." "theta2.g.g." "AWC.g.g."  
and I've been surprised by the (to me) strange behaviour in "SP.g..g." 
modified in "SP...g."
An this is the first problem I cannot solve.

Then there's the problem of trailing dot removal.
In
http://tolstoy.newcastle.edu.au/R/e2/help/07/01/8665.html
I've found a somewhat similar problem, but it do not works in this case 
since:
gsub("[.].*", "", str)
[1] "y"      "BD"     "PR"     "Ks"     "SP"     "P"      "theta1" "theta2"
[9] "AWC"   
And this the second problem

Apart this particular problems I would like to know more on regexp, sub 
and so on, since each time
I have strings to manipulate, I must face my ignorance in the topic of 
regular expression and its syntax.

Is there any page with examples, where I can improve my knowledge and 
stop being frustrated each time I have to manipulate strings?

8rino

-- 
Ottorino-Luca Pantani, Universit? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta
P.zle Cascine 28 50144 Firenze Italia
Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273 
OLPantani at unifi.it


From jhallman at frb.gov  Thu Jul 26 15:43:51 2007
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 26 Jul 2007 09:43:51 -0400
Subject: [R] aggregate.ts
References: <46A69557.3060008@xtra.co.nz>
Message-ID: <xmrabtjb7qg.fsf@mralx1.rsma.frb.gov>

Your troubles with 'aggregate' for a ts are one of the reasons I created the
'tis' and 'ti' classes in the fame package.  If you do this:

> x1 <- tis(1:24, start = c(2000, 10), freq = 12)
> x2 <- tis(1:24, start = c(2000, 11), freq = 12)
> y1 <- aggregate(x1, nfreq = 4)
> y2 <- aggregate(x2, nfreq = 4)
> x1
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2000                                       1   2   3
2001   4   5   6   7   8   9  10  11  12  13  14  15
2002  16  17  18  19  20  21  22  23  24            
class: tis
> x2
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2000                                           1   2
2001   3   4   5   6   7   8   9  10  11  12  13  14
2002  15  16  17  18  19  20  21  22  23  24        
class: tis
> y1
     Qtr1 Qtr2 Qtr3 Qtr4
2000                   6
2001   15   24   33   42
2002   51   60   69     
class: tis
> y2
     Qtr1 Qtr2 Qtr3 Qtr4
2001   12   21   30   39
2002   48   57   66     
class: tis

Everything pretty much works as you would expect.  One thing to notice is
that, even using a 'tis' rather than a 'ts', aggregate will only sum up the
monthly observations for a quarter if all three of the months are there.
That's why y2 starts with 2001Q1, rather than 2000Q4.  If you really want the
2000Q4 observation to be the sum of the first two x2 months, the convert()
function in fame can handle that.

> convert(x2, tif = "quarterly", observed = "summed", ignore = T)
          Qtr1      Qtr2      Qtr3      Qtr4
2000                                4.033333
2001 12.000000 21.000000 30.000000 39.000000
2002 48.000000 57.000000 66.000000 71.225806
class: tis

Now back to ts.  If you look deeper into what's happening here:

> y3 <- aggregate(as.ts(x2), nf = 4)
> y3
Error in rep.int("", start.pad) : invalid number of copies in rep.int()

Enter a frame number, or 0 to exit   

1: print(c(6, 15, 24, 33, 42, 51, 60, 69))
2: print.ts(c(6, 15, 24, 33, 42, 51, 60, 69))
3: matrix(c(rep.int("", start.pad), format(x, ...), rep.int("", end.pad)), nc 
4: as.vector(data)
5: rep.int("", start.pad)

Selection: 0
> unclass(y3)
[1]  6 15 24 33 42 51 60 69
attr(,"tsp")
[1] 2000.833 2002.583    4.000

what you see is that aggregate() did indeed create a quarterly series, but the
quarters cover (Nov-Jan, Feb-Apr, May-Jul, Aug-Oct), not the usual (Jan-Mar,
Apr-Jun, Jul-Sep, Oct-Dec).  The author of the print.ts code evidently never
even thought of this possibility.  Not that I blame him.  I work with monthly
and quarterly data all the time, and the behavior of aggregate.ts() is so
counter-intuitive that I wouldn't have imagined it either.

Bottom line: use 'tis' series from the fame package, or 'zoo` stuff from
Gabor's zoo package.  As the author of the fame package, I hope you'll excuse
me for asserting that the 'tis' class is easier to understand and use than the
zoo stuff, which takes a more general approach.  Some day Gabor or I or some
other enterprising soul should try combining the best ideas from zoo and fame
into a package that is better than either one.

Jeff




-- 
Jeff


From felix_bach at gmx.de  Thu Jul 26 15:51:50 2007
From: felix_bach at gmx.de (Senor_Felix)
Date: Thu, 26 Jul 2007 06:51:50 -0700 (PDT)
Subject: [R] dispersion_parameter_GLMM's
In-Reply-To: <815b70590703060534h106f0813wc3daf104f1cc38d0@mail.gmail.com>
References: <45ED394F.3030307@eva.mpg.de>
	<815b70590703060534h106f0813wc3daf104f1cc38d0@mail.gmail.com>
Message-ID: <11810939.post@talk.nabble.com>


I agree with David. A dispersion parameter of 25 suggests that you have
mainly 0's in your data set and your model is not adequate. Perhabs you
should dichotomize your data in 0 and 1's and use a logistic mixed model but
be aware of small numbers of events. 


That amount of overdispersion would make the use of a poisson model
very questionable, and will very likely result in estimated standard
errors that are too low, hence the change in statistical significance
when you switch to quasipoisson.

O
-- 
View this message in context: http://www.nabble.com/dispersion_parameter_GLMM%27s-tf3354683.html#a11810939
Sent from the R help mailing list archive at Nabble.com.


From jhallman at frb.gov  Thu Jul 26 15:55:13 2007
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 26 Jul 2007 09:55:13 -0400
Subject: [R] dates() is a great date function in R
References: <11675205.post@talk.nabble.com>
Message-ID: <xmr6447b77i.fsf@mralx1.rsma.frb.gov>

Mr Natural <drstrong at ucdavis.edu> writes:

Just save the spreadsheet as a csv file and use tisFromCsv() in the fame
package.  One of the arguments tisFromCsv() takes is a dateFormat, so you can
tell it what format the date column is in.  You can also tell it the name of
the date column if it isn't some variation of DATE, Date, or date.

tisFromCsv() looks at the dates coming in and automatically figures out what
frequency the data are (quarterly, monthly, weekly, daily, etc.) and creates a
univariate or multivariate (if the spreadsheet has more than one data column)
'tis' (Time Indexed Series) object. 

Jeff

> Proper calendar dates in R are great for plotting and calculating. 
> However for the non-wonks among us, they can be very frustrating.
> I have recently discussed the pains that people in my lab have had 
> with dates in R. Especially the frustration of bringing date data into R 
> from Excel, which we have to do a lot. 
> 
> Please find below a simple analgesic for R date importation that I
> discovered 
> over the last 1.5 days (Learning new stuff in R is calculated in 1/2 days).
> 
> The function    dates()    gives the simplest way to get calendar dates into
> R from Excel that I can find.
> But straight importation of Excel dates, via a csv or txt file, can be a a
> huge pain (I'll give details for anyone who cares to know). 
> 
> My pain killer is:
> Consider that you have Excel columns in month, day, year format. Note that R
> hates date data that does not lead with the year. 
> 
> a. Load the chron library by typing   library(chron)   in the console.
> You know that you need this library from information revealed by 
> performing the query,
> ?dates()"    in the Console window. This gives the R documentation 
> help file for this and related time, date functions.  In the upper left 
> of the documentation, one sees "dates(chron)". This tells you that you
> need the library chron. 
> 
> b. Change the format "dates" in Excel to format "general", which gives 
> 5 digit Julian dates. Import the csv file (I use    read.csv()  with the 
> Julian dates and other data of interest.
> 
> c.  Now, change the Julian dates that came in with the csv file into 
> calendar dates with the    dates() function. Below is my code for performing 
> this activity, concerning an R data file called ss,
> 
> ss holds the Julian dates, illustrated below from the column MPdate,
> 
> >ss$MPdate[1:5]
> [1] 34252 34425 34547 34759 34773
> 
> The dates() function makes calendar dates from Julian dates,
> 
> >dmp<-dates(ss$MPdate,origin=c(month = 1, day = 1, year = 1900))
> 
> > dmp[1:5]
> [1] 10/12/93 04/03/94 08/03/94 03/03/95 03/17/95
> 
> I would appreciate the comments of more sophisticated programmers who
> can suggest streamlining or shortcutting this operation.
> 
> regards, Don
> 
> 
> 
>  
> -- 
> View this message in context: http://www.nabble.com/dates%28%29-is-a-great-date-function-in-R-tf4105322.html#a11675205
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Jeff


From jhallman at frb.gov  Thu Jul 26 15:59:31 2007
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 26 Jul 2007 09:59:31 -0400
Subject: [R] Help with Dates
References: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
Message-ID: <xmr1wevb70c.fsf@mralx1.rsma.frb.gov>

zoo is nice.  'tisFromCsv()' in the fame package is nicer.

Jeff


From ggrothendieck at gmail.com  Thu Jul 26 16:06:10 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Jul 2007 10:06:10 -0400
Subject: [R] substituting dots in the names of the columns (sub, gsub,
	regexpr)
In-Reply-To: <46A8A458.9090600@unifi.it>
References: <46A8A458.9090600@unifi.it>
Message-ID: <971536df0707260706y208b29a4x2f8365125f3d9548@mail.gmail.com>

Use \\. or [.] with quotes to denote a literal dot (#1)
or can use fixed = TRUE to remove the meaning of dot (#2) or
use a zero-width lookahead assertion (?=[.]) which will be matched
but is not added to the string to be replaced (#3).  Try ?regexpr .
Also the links on the gsubfn home page (http://code.google.com/p/gsubfn/)
point to a number of good resources on regular expressions.

Str <- c("y..m.", "BD..g.cm3.", "PR..Mpa.", "Ks..m.s.", "SP.g..g.",
"P..m3.m3.", "theta1..g.g.", "theta2..g.g.", "AWC..g.g.")

# 1
tmp <- gsub("[.]+", ".", Str)
sub("[.]+$", "", tmp)

# 2
tmp <- gsub("..", ".", Str, fixed = TRUE)
sub("[.]+$", "", tmp)

# 3 - both done at once using zero-width lookahead
gsub("[.]*$|[.]*(?=[.])", "", Str, perl = TRUE)


On 7/26/07, 8rino-Luca Pantani <ottorino-luca.pantani at unifi.it> wrote:
> Dear R users,
> I have the following two problems, related to the function sub, grep,
> regexpr and similia.
>
> The header of the file(s) I have to import is like this.
>
> c("y (m)", "BD (g/cm3)", "PR (Mpa)", "Ks (m/s)", "SP g./g.", "P
> (m3/m3)", "theta1 (g/g)", "theta2 (g/g)", "AWC (g/g)")
>
> To get rid of spaces and symbols in the names of the columns,
> I use read.table(... check.names=TRUE) and I get:
> str <- c("y..m.", "BD..g.cm3.", "PR..Mpa.", "Ks..m.s.", "SP.g..g.",
> "P..m3.m3.", "theta1..g.g.", "theta2..g.g.", "AWC..g.g.")
>
> Now, my problem is to remove the trailing dots, as well as the double
> dots, in order to get the names like the following
> c("y.m", "BD.g.cm3", "PR.Mpa", "Ks.m.s", "SP.g.g", "P.m3.m3.",
> "theta1.g.g", "theta2.g.g", "AWC.g.g")
>
> I've searched the help pages for sub, regexpr and similia, and also
> searched the help archives.
> I understand that the dot is a peculiar sign since
> sub("..", ".", str)
> [1] "..m."        "...g.cm3."   "...Mpa."     "...m.s."     "..g..g."
> [6] "..m3.m3."    ".eta1..g.g." ".eta2..g.g." ".C..g.g."
>
> Therefore I tried
> sub("\\..", ".", str)
> [1] "y.m."        "BD.g.cm3."   "PR.Mpa."     "Ks.m.s."     "SP...g."
> [6] "P.m3.m3."    "theta1.g.g." "theta2.g.g." "AWC.g.g."
> and I've been surprised by the (to me) strange behaviour in "SP.g..g."
> modified in "SP...g."
> An this is the first problem I cannot solve.
>
> Then there's the problem of trailing dot removal.
> In
> http://tolstoy.newcastle.edu.au/R/e2/help/07/01/8665.html
> I've found a somewhat similar problem, but it do not works in this case
> since:
> gsub("[.].*", "", str)
> [1] "y"      "BD"     "PR"     "Ks"     "SP"     "P"      "theta1" "theta2"
> [9] "AWC"
> And this the second problem
>
> Apart this particular problems I would like to know more on regexp, sub
> and so on, since each time
> I have strings to manipulate, I must face my ignorance in the topic of
> regular expression and its syntax.
>
> Is there any page with examples, where I can improve my knowledge and
> stop being frustrated each time I have to manipulate strings?
>
> 8rino
>
> --
> Ottorino-Luca Pantani, Universit? di Firenze
> Dip. Scienza del Suolo e Nutrizione della Pianta
> P.zle Cascine 28 50144 Firenze Italia
> Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273
> OLPantani at unifi.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Thu Jul 26 16:07:12 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Jul 2007 10:07:12 -0400
Subject: [R] substituting dots in the names of the columns (sub, gsub,
	regexpr)
In-Reply-To: <46A8A458.9090600@unifi.it>
References: <46A8A458.9090600@unifi.it>
Message-ID: <971536df0707260707m4f554aa9vacaab02fa6c9a117@mail.gmail.com>

Use \\. or [.] with quotes to denote a literal dot (#1)
or can use fixed = TRUE to remove the meaning of dot (#2) or
use a zero-width lookahead assertion (?=[.]) which will be matched
but is not added to the string to be replaced (#3).  Try ?regexpr .
Also the links on the gsubfn home page (http://code.google.com/p/gsubfn/)
point to a number of good resources on regular expressions.

Str <- c("y..m.", "BD..g.cm3.", "PR..Mpa.", "Ks..m.s.", "SP.g..g.",
"P..m3.m3.", "theta1..g.g.", "theta2..g.g.", "AWC..g.g.")

# 1
tmp <- gsub("[.]+", ".", Str)
sub("[.]+$", "", tmp)

# 2
tmp <- gsub("..", ".", Str, fixed = TRUE)
sub("[.]+$", "", tmp)

# 3 - both done at once using zero-width lookahead
gsub("[.]*$|[.]*(?=[.])", "", Str, perl = TRUE)


On 7/26/07, 8rino-Luca Pantani <ottorino-luca.pantani at unifi.it> wrote:
> Dear R users,
> I have the following two problems, related to the function sub, grep,
> regexpr and similia.
>
> The header of the file(s) I have to import is like this.
>
> c("y (m)", "BD (g/cm3)", "PR (Mpa)", "Ks (m/s)", "SP g./g.", "P
> (m3/m3)", "theta1 (g/g)", "theta2 (g/g)", "AWC (g/g)")
>
> To get rid of spaces and symbols in the names of the columns,
> I use read.table(... check.names=TRUE) and I get:
> str <- c("y..m.", "BD..g.cm3.", "PR..Mpa.", "Ks..m.s.", "SP.g..g.",
> "P..m3.m3.", "theta1..g.g.", "theta2..g.g.", "AWC..g.g.")
>
> Now, my problem is to remove the trailing dots, as well as the double
> dots, in order to get the names like the following
> c("y.m", "BD.g.cm3", "PR.Mpa", "Ks.m.s", "SP.g.g", "P.m3.m3.",
> "theta1.g.g", "theta2.g.g", "AWC.g.g")
>
> I've searched the help pages for sub, regexpr and similia, and also
> searched the help archives.
> I understand that the dot is a peculiar sign since
> sub("..", ".", str)
> [1] "..m."        "...g.cm3."   "...Mpa."     "...m.s."     "..g..g."
> [6] "..m3.m3."    ".eta1..g.g." ".eta2..g.g." ".C..g.g."
>
> Therefore I tried
> sub("\\..", ".", str)
> [1] "y.m."        "BD.g.cm3."   "PR.Mpa."     "Ks.m.s."     "SP...g."
> [6] "P.m3.m3."    "theta1.g.g." "theta2.g.g." "AWC.g.g."
> and I've been surprised by the (to me) strange behaviour in "SP.g..g."
> modified in "SP...g."
> An this is the first problem I cannot solve.
>
> Then there's the problem of trailing dot removal.
> In
> http://tolstoy.newcastle.edu.au/R/e2/help/07/01/8665.html
> I've found a somewhat similar problem, but it do not works in this case
> since:
> gsub("[.].*", "", str)
> [1] "y"      "BD"     "PR"     "Ks"     "SP"     "P"      "theta1" "theta2"
> [9] "AWC"
> And this the second problem
>
> Apart this particular problems I would like to know more on regexp, sub
> and so on, since each time
> I have strings to manipulate, I must face my ignorance in the topic of
> regular expression and its syntax.
>
> Is there any page with examples, where I can improve my knowledge and
> stop being frustrated each time I have to manipulate strings?
>
> 8rino
>
> --
> Ottorino-Luca Pantani, Universit? di Firenze
> Dip. Scienza del Suolo e Nutrizione della Pianta
> P.zle Cascine 28 50144 Firenze Italia
> Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273
> OLPantani at unifi.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From michael at cassin.name  Thu Jul 26 16:09:33 2007
From: michael at cassin.name (Michael Cassin)
Date: Thu, 26 Jul 2007 15:09:33 +0100
Subject: [R] Problem installing tseries package
Message-ID: <b02e8b330707260709o6bb1750cm531bc73e6b315b83@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/26826d27/attachment.pl 

From vince_itchy at yahoo.fr  Thu Jul 26 16:10:17 2007
From: vince_itchy at yahoo.fr (Nok Noy)
Date: Thu, 26 Jul 2007 07:10:17 -0700 (PDT)
Subject: [R] Average plan
Message-ID: <11811324.post@talk.nabble.com>


Hello, 

I'm looking for a method to compute an average plan from 4 or 5 point in an
cartesian space. I'm sure It can be done using a less-square method but
maybe it a function already exist in R system to get this plan. 
Can somebody help me to solve this problem (I'm looking on the net for hours
but didn't find anything realy satisfiying me)
Thanks


-- 
View this message in context: http://www.nabble.com/Average-plan-tf4151900.html#a11811324
Sent from the R help mailing list archive at Nabble.com.


From felix at nfrac.org  Thu Jul 26 16:15:56 2007
From: felix at nfrac.org (Felix Andrews)
Date: Fri, 27 Jul 2007 00:15:56 +1000
Subject: [R] substituting dots in the names of the columns (sub, gsub,
	regexpr)
In-Reply-To: <46A8A458.9090600@unifi.it>
References: <46A8A458.9090600@unifi.it>
Message-ID: <94730b8a0707260715y76b0496ew4531cda27f476b63@mail.gmail.com>

Hi,

A dot in a regular expression matches any character, so you have to
escape each dot with backslash \\ (which itself is escaped in the
string, to confuse things...).
A plus symbol will match one or more of the preceding characters.
A dollar symbol will match the end of a string.

So:

gsub("\\.$", "", gsub("\\.+", ".", str))
[1] "y.m"        "BD.g.cm3"   "PR.Mpa"     "Ks.m.s"     "SP.g.g"
"P.m3.m3"    "theta1.g.g"
[8] "theta2.g.g" "AWC.g.g"

Learn more at ?regexp

Felix


On 7/26/07, 8rino-Luca Pantani <ottorino-luca.pantani at unifi.it> wrote:
> Dear R users,
> I have the following two problems, related to the function sub, grep,
> regexpr and similia.
>
> The header of the file(s) I have to import is like this.
>
> c("y (m)", "BD (g/cm3)", "PR (Mpa)", "Ks (m/s)", "SP g./g.", "P
> (m3/m3)", "theta1 (g/g)", "theta2 (g/g)", "AWC (g/g)")
>
> To get rid of spaces and symbols in the names of the columns,
> I use read.table(... check.names=TRUE) and I get:
> str <- c("y..m.", "BD..g.cm3.", "PR..Mpa.", "Ks..m.s.", "SP.g..g.",
> "P..m3.m3.", "theta1..g.g.", "theta2..g.g.", "AWC..g.g.")
>
> Now, my problem is to remove the trailing dots, as well as the double
> dots, in order to get the names like the following
> c("y.m", "BD.g.cm3", "PR.Mpa", "Ks.m.s", "SP.g.g", "P.m3.m3.",
> "theta1.g.g", "theta2.g.g", "AWC.g.g")
>
> I've searched the help pages for sub, regexpr and similia, and also
> searched the help archives.
> I understand that the dot is a peculiar sign since
> sub("..", ".", str)
> [1] "..m."        "...g.cm3."   "...Mpa."     "...m.s."     "..g..g."
> [6] "..m3.m3."    ".eta1..g.g." ".eta2..g.g." ".C..g.g."
>
> Therefore I tried
> sub("\\..", ".", str)
> [1] "y.m."        "BD.g.cm3."   "PR.Mpa."     "Ks.m.s."     "SP...g."
> [6] "P.m3.m3."    "theta1.g.g." "theta2.g.g." "AWC.g.g."
> and I've been surprised by the (to me) strange behaviour in "SP.g..g."
> modified in "SP...g."
> An this is the first problem I cannot solve.
>
> Then there's the problem of trailing dot removal.
> In
> http://tolstoy.newcastle.edu.au/R/e2/help/07/01/8665.html
> I've found a somewhat similar problem, but it do not works in this case
> since:
> gsub("[.].*", "", str)
> [1] "y"      "BD"     "PR"     "Ks"     "SP"     "P"      "theta1" "theta2"
> [9] "AWC"
> And this the second problem
>
> Apart this particular problems I would like to know more on regexp, sub
> and so on, since each time
> I have strings to manipulate, I must face my ignorance in the topic of
> regular expression and its syntax.
>
> Is there any page with examples, where I can improve my knowledge and
> stop being frustrated each time I have to manipulate strings?
>
> 8rino
>
> --
> Ottorino-Luca Pantani, Universit?? di Firenze
> Dip. Scienza del Suolo e Nutrizione della Pianta
> P.zle Cascine 28 50144 Firenze Italia
> Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273
> OLPantani at unifi.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Felix Andrews / ??????
PhD candidate
Integrated Catchment Assessment and Management Centre
The Fenner School of Environment and Society
The Australian National University (Building 48A), ACT 0200
Beijing Bag, Locked Bag 40, Kingston ACT 2604
http://www.neurofractal.org/felix/
voice:+86_1051404394 (in China)
mobile:+86_13522529265 (in China)
mobile:+61_410400963 (in Australia)
xmpp:foolish.android at gmail.com
3358 543D AAC6 22C2 D336  80D9 360B 72DD 3E4C F5D8


From Achim.Zeileis at wu-wien.ac.at  Thu Jul 26 16:56:58 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 26 Jul 2007 16:56:58 +0200 (CEST)
Subject: [R] aggregate.ts
In-Reply-To: <xmrabtjb7qg.fsf@mralx1.rsma.frb.gov>
Message-ID: <Pine.LNX.4.44.0707261625420.16266-100000@disco.wu-wien.ac.at>

Jeff,

I'm really not a fan of subjective "mine is bigger than yours"
discussions. Just three comments that I try to keep as objective as
possible.

> Bottom line: use 'tis' series from the fame package, or 'zoo` stuff from
> Gabor's zoo package.

The last time I checked
  packageDescription("zoo")$Author
had more than one entry.

> As the author of the fame package, I hope you'll excuse
> me for asserting that the 'tis' class is easier to understand and use than the
> zoo stuff,

That surely depends on the user and the task he has to do...

> which takes a more general approach.  Some day Gabor or I or some
> other enterprising soul should try combining the best ideas from zoo and fame
> into a package that is better than either one.

I think combination should be straightforward: "zoo" is general enough to
allow for time indexes of class "ti". Overall, "ti" seems to be
well-written and only some methods might need to be added/improved to
cooperate fully with zoo. Maybe some of the functionality that is
currently available for "tis" but is not available for all conceivalbe
zoo+arbitrary_index objects might be special cased for zoo+ti or zooreg or
zooreg+ti etc.

Best,
Z


From ggrothendieck at gmail.com  Thu Jul 26 17:01:50 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Jul 2007 11:01:50 -0400
Subject: [R] Help with Dates
In-Reply-To: <xmr1wevb70c.fsf@mralx1.rsma.frb.gov>
References: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
	<xmr1wevb70c.fsf@mralx1.rsma.frb.gov>
Message-ID: <971536df0707260801u57ab405eqdb22876a8b3e089c@mail.gmail.com>

On 26 Jul 2007 09:59:31 -0400, Jeffrey J. Hallman <jhallman at frb.gov> wrote:
> zoo is nice.  'tisFromCsv()' in the fame package is nicer.
>
> Jeff


1. What am I doing wrong here?  I only get one data column.
2. I assume the regularized dates which do not exactly match the input ones
    are intended so as to make this a regularly spaced series.  Is that right?
3. What is the cause of the warning message?
4. Why is a list returned with a single component containing the output?
Thanks.

> library(fame)
> Lines <- " Date  Price Open.Int. Comm.Long Comm.Short net.comm
+ 15-Jan-86 673.25    175645     65910      28425    37485
+ 31-Jan-86 677.00    167350     54060      27120    26940
+ 14-Feb-86 680.25    157985     37955      25425    12530
+ 28-Feb-86 691.75    162775     49760      16030    33730
+ 14-Mar-86 706.50    163495     54120      27995    26125
+ 31-Mar-86 709.75    164120     54715      30390    24325
+ "
> tisFromCsv(textConnection(Lines), dateFormat = "%d-%b-%y", dateCol = "Date", sep = "")
[[1]]
           [,1]
19860119 673.25
19860202 677.00
19860216 680.25
19860302 691.75
19860316 706.50
19860330 709.75
class: tis

Warning message:
number of items to replace is not a multiple of replacement length in:
x[i] <- value


From johnzabroski at gmail.com  Thu Jul 26 17:13:19 2007
From: johnzabroski at gmail.com (John Zabroski)
Date: Thu, 26 Jul 2007 11:13:19 -0400
Subject: [R] Constructing bar charts with standard error bars
In-Reply-To: <loom.20070725T182046-933@post.gmane.org>
References: <546067750707250743vc82f86cha68a302351f12297@mail.gmail.com>
	<loom.20070725T182046-933@post.gmane.org>
Message-ID: <546067750707260813w578590e0he75f8e09ccf63191@mail.gmail.com>

On 7/25/07, Ben Bolker <bolker at ufl.edu> wrote:
> John Zabroski <johnzabroski <at> gmail.com> writes:
>
> > The best clue I have so far is Rtips #5.9:
> > http://pj.freefaculty.org/R/Rtips.html#5.9 which is what I based my present
> > solution off of.
> >
> > However, I do not understand how this works.  It seems like there is no
> > concrete way to determine the arrow drawing parameters x0 and x1 for a
> > barplot.  Moreover, the bars seem to be "cut off".
> >
>
>   barplot() returns the x values you need for x0 and x1.
> barplot(...,ylim=c(0,xbar+se)) will set the upper y limit so
> the bars don't get cut off.
>
>   P.S. I hope you're not hoping to infer a statistically
> significant difference among these groups ...
>
>   cheers
>    Ben Bolker

Thanks a lot!  I tried all three and they all seem very dependable.
Also, I appreciate you rewriting my solution and adding elegance.

Is there a way to extend the tick marks to the ylim values, such that
the yscale ymax tickmark is something like max(xbar+se)?  In the
documentation, I thought par(yaxp=c(y0,y1,n)) would do the trick, but
after trying to use it I am not sure I understand what yaxp even does.

P.S. I am not looking for statistically significant differences.  I am
trying to learn how to leverage R's graphing capabilities.  I also
appreciate Frank Harrell referring me to the link about Dynamite Plots
and associated weaknesses.


From orzack at freshpond.org  Thu Jul 26 17:34:37 2007
From: orzack at freshpond.org (orzack)
Date: Thu, 26 Jul 2007 11:34:37 -0400
Subject: [R] lmer and scale parameters....
Message-ID: <p06230901c2ce6db2097c@[192.168.0.100]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/4549ce4d/attachment.pl 

From jhallman at frb.gov  Thu Jul 26 17:39:41 2007
From: jhallman at frb.gov (Jeffrey J. Hallman)
Date: 26 Jul 2007 11:39:41 -0400
Subject: [R] Help with Dates
References: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
	<xmr1wevb70c.fsf@mralx1.rsma.frb.gov>
	<971536df0707260801u57ab405eqdb22876a8b3e089c@mail.gmail.com>
Message-ID: <xmrsl7b9nsy.fsf@mralx1.rsma.frb.gov>

Are you using the latest version of fame?  1.05 and earlier had a bug in
tisFromCsv that was fixed in 1.08.

Below I show what I get with fame version 1.08.  There is still a problem in
that the "frequency-figuring" logic appears to think the frequency is bwsunday
(biweekly with weeks ending on Sunday) rather than semimonthly, which would
appear to be a better fit.  That's why the 19860330 observation is getting
filled in with NA's.

Jeff

> Lines <- "Date  Price Open.Int. Comm.Long Comm.Short net.comm
15-Jan-86 673.25    175645     65910      28425    37485
31-Jan-86 677.00    167350     54060      27120    26940
14-Feb-86 680.25    157985     37955      25425    12530
28-Feb-86 691.75    162775     49760      16030    33730
14-Mar-86 706.50    163495     54120      27995    26125
31-Mar-86 709.75    164120     54715      30390    24325"

+ + + + + + > 
> boink <- tisFromCsv(textConnection(Lines), dateFormat = "%d-%b-%y", dateCol = "Date", sep = "")
> boink
$Price
           [,1]
19860119 673.25
19860202 677.00
19860216 680.25
19860302 691.75
19860316 706.50
19860330     NA
19860413 709.75
class: tis

$Open.Int.
           [,1]
19860119 175645
19860202 167350
19860216 157985
19860302 162775
19860316 163495
19860330     NA
19860413 164120
class: tis

$Comm.Long
          [,1]
19860119 65910
19860202 54060
19860216 37955
19860302 49760
19860316 54120
19860330    NA
19860413 54715
class: tis

$Comm.Short
          [,1]
19860119 28425
19860202 27120
19860216 25425
19860302 16030
19860316 27995
19860330    NA
19860413 30390
class: tis

$net.comm
          [,1]
19860119 37485
19860202 26940
19860216 12530
19860302 33730
19860316 26125
19860330    NA
19860413 24325
class: tis


"Gabor Grothendieck" <ggrothendieck at gmail.com> writes:

> On 26 Jul 2007 09:59:31 -0400, Jeffrey J. Hallman <jhallman at frb.gov> wrote:
> > zoo is nice.  'tisFromCsv()' in the fame package is nicer.
> >
> > Jeff
> 
> 
> 1. What am I doing wrong here?  I only get one data column.
> 2. I assume the regularized dates which do not exactly match the input ones
>     are intended so as to make this a regularly spaced series.  Is that right?
> 3. What is the cause of the warning message?
> 4. Why is a list returned with a single component containing the output?
> Thanks.
> 
> > library(fame)
> > Lines <- " Date  Price Open.Int. Comm.Long Comm.Short net.comm
> + 15-Jan-86 673.25    175645     65910      28425    37485
> + 31-Jan-86 677.00    167350     54060      27120    26940
> + 14-Feb-86 680.25    157985     37955      25425    12530
> + 28-Feb-86 691.75    162775     49760      16030    33730
> + 14-Mar-86 706.50    163495     54120      27995    26125
> + 31-Mar-86 709.75    164120     54715      30390    24325
> + "
> > tisFromCsv(textConnection(Lines), dateFormat = "%d-%b-%y", dateCol = "Date", sep = "")
> [[1]]
>            [,1]
> 19860119 673.25
> 19860202 677.00
> 19860216 680.25
> 19860302 691.75
> 19860316 706.50
> 19860330 709.75
> class: tis
> 
> Warning message:
> number of items to replace is not a multiple of replacement length in:
> x[i] <- value
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Jeff


From timo.stolz at gerade-deshalb.org  Thu Jul 26 16:12:48 2007
From: timo.stolz at gerade-deshalb.org (Timo Stolz)
Date: Thu, 26 Jul 2007 16:12:48 +0200 (CEST)
Subject: [R] significance test for difference of two correlations
In-Reply-To: <1707739.12291185430231461.JavaMail.root@roerden.de>
Message-ID: <16399234.12331185459168564.JavaMail.root@roerden.de>

Dear R users,

how can I test, whether two correlations differ significantly. (I want to prove, that variables are correlated differently, depending on the group a person is in.)

Greetings from Freiburg im Breisgau (Germany),
Timo Stolz


From ripley at stats.ox.ac.uk  Thu Jul 26 18:09:00 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jul 2007 17:09:00 +0100 (BST)
Subject: [R] Problem installing tseries package
In-Reply-To: <b02e8b330707260709o6bb1750cm531bc73e6b315b83@mail.gmail.com>
References: <b02e8b330707260709o6bb1750cm531bc73e6b315b83@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707261658180.7472@gannet.stats.ox.ac.uk>

On Thu, 26 Jul 2007, Michael Cassin wrote:

> Hi,
>
> I'm running R 2.4.1 on Fedora Core 6 and am unable to install the tseries
> package.  I've resolved a few problems getting to this point, by running a
> yum update, installing the gcc-gfortran dependency, but now I'm stuck.
> Could someone please point me in the right direction?

Please read the posting guide and provide the information you were asked 
for: only then we may be able to help you.

You seem to have a system which installed R in /usr/lib/R but has x86_64 
components on it.  So what architecture is it that you are trying to run?

My guess is that you installed a i386 RPM on a x86_64 OS.  That will 
install and R will run *but* you will not be able to use it to install 
packages.  If you installed the i386 RPM after the x86_64 one, it will 
have overwritten some crucial files including /usr/bin/R.

It is possible to have i386 and x86_64 R coexisting on x86_64 Linux, but 
not by installing RPMs for different architectures.


>
> ========R install.packages output =======
> ==================================
>
>> install.packages("tseries")
>
> trying URL '
> http://www.sourcekeg.co.uk/cran/src/contrib/tseries_0.10-11.tar.gz'
> Content type 'application/x-tar' length 182043 bytes
> opened URL
> ==================================================
> downloaded 177Kb
>
> * Installing *source* package 'tseries' ...
> ** libs
> gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> -fpic  -O3 -g -std=gnu99 -c arma.c -o arma.o
> gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> -fpic  -O3 -g -std=gnu99 -c bdstest.c -o bdstest.o
> gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> -fpic  -O3 -g -std=gnu99 -c boot.c -o boot.o
> gfortran   -fpic  -O2 -g -c dsumsl.f -o dsumsl.o
> In file dsumsl.f:450
>
>      IF (IV(1) - 2) 30, 40, 50
>                                                                       1
> Warning: Obsolete: arithmetic IF statement at (1)
> In file dsumsl.f:3702
>
>   10 ASSIGN 30 TO NEXT
>                                                                       1
> Warning: Obsolete: ASSIGN statement at (1)
> In file dsumsl.f:3707
>
>   20    GO TO NEXT,(30, 50, 70, 110)
>                  1
> Warning: Obsolete: Assigned GOTO statement at (1)
> In file dsumsl.f:3709
>
>      ASSIGN 50 TO NEXT
>                                                                       1
> Warning: Obsolete: ASSIGN statement at (1)
> In file dsumsl.f:3718
>
>      ASSIGN 70 TO NEXT
>                                                                       1
> Warning: Obsolete: ASSIGN statement at (1)
> In file dsumsl.f:3724
>
>      ASSIGN 110 TO NEXT
>                                                                       1
> Warning: Obsolete: ASSIGN statement at (1)
> In file dsumsl.f:4552
>
>      IF (IV(1) - 2) 999, 30, 70
>                                                                       1
> Warning: Obsolete: arithmetic IF statement at (1)
> In file dsumsl.f:4714
>
>      IF (IRC) 140, 100, 210
>                                                                       1
> Warning: Obsolete: arithmetic IF statement at (1)
> gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> -fpic  -O3 -g -std=gnu99 -c garch.c -o garch.o
> gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> -fpic  -O3 -g -std=gnu99 -c ppsum.c -o ppsum.o
> gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> -fpic  -O3 -g -std=gnu99 -c tsutils.c -o tsutils.o
> gcc -shared -Bdirect,--hash-stype=both,-Wl,-O1 -o tseries.so arma.o
> bdstest.o boot.o dsumsl.o garch.o ppsum.o tsutils.o -L/usr/lib/R/lib -lRblas
> -lgfortran -lm -lgcc_s -lgfortran -lm -lgcc_s -L/usr/lib/R/lib -lR
> /usr/bin/ld: skipping incompatible /usr/lib/R/lib/libRblas.so when searching
> for -lRblas
> /usr/bin/ld: skipping incompatible /usr/lib/R/lib/libRblas.so when searching
> for -lRblas
> /usr/bin/ld: cannot find -lRblas
> collect2: ld returned 1 exit status
> make: *** [tseries.so] Error 1
> ERROR: compilation failed for package 'tseries'
> ** Removing '/usr/lib/R/library/tseries'
>
>
> =============================================
> =============================================
>
>
> I presume the priority is addressing the error: "/usr/bin/ld: cannot find
> -lRblas"
>
> I have the libRblas.so file with R 2.4. Do I need to upgrade to R 2.5 - In
> which case I'll be asking how to fix the problems I'm having doing that  ;)
>
> [~]# yum provides libRblas.so
> <snip>
>
> R.x86_64                                 2.5.1-2.fc6            extras
> Matched from:
> /usr/lib64/R/lib/libRblas.so
> libRblas.so()(64bit)
>
> R.x86_64                                 2.5.1-2.fc6            extras
> Matched from:
> /usr/lib64/R/lib/libRblas.so
> libRblas.so()(64bit)
>
> R.i386                                   2:2.4.1-1.fc6          installed
> Matched from:
> /usr/lib/R/lib/libRblas.so
> libRblas.so
>
> R.x86_64                                 2.4.1-4.fc6            installed
> Matched from:
> /usr/lib64/R/lib/libRblas.so
> libRblas.so()(64bit)
>
>
> Regards,
> Mike
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From meyerjp at jmu.edu  Thu Jul 26 18:03:38 2007
From: meyerjp at jmu.edu (meyerjp at jmu.edu)
Date: Thu, 26 Jul 2007 12:03:38 -0400 (EDT)
Subject: [R] error in using R2WinBUGS on Ubuntu 6.10 Linux
Message-ID: <20070726120338.AUQ96591@mpmail1.jmu.edu>

I am trying to run WinBUGS 1.4 from the Ubuntu 6.10 Linux distribution. I am using the R2WinBUGS packages with the  source file listed below. WinBUGS appears to run properly, but I get the following message after WinBUGS starts in WINE. Does anyone know what may be causing this error and what the correction may be?

Thanks

ERROR MESSAGE:

fixme:ole:GetHGlobalFromILockBytes cbSize is 13824
err:ole:CoGetClassObject class {0003000a-0000-0000-c000-000000000046} not registered
err:ole:CoGetClassObject class {0003000a-0000-0000-c000-000000000046} not registered
err:ole:CoGetClassObject no class object {0003000a-0000-0000-c000-000000000046} could be created for context 0x3
fixme:keyboard:RegisterHotKey (0x10032,13,0x00000002,3): stub
fixme:ntdll:RtlNtStatusToDosErrorNoTeb no mapping for 8000000a
err:ole:local_server_thread Failure during ConnectNamedPipe 317



R SOURCE FILE:

rm(list=ls(all=TRUE))

library(R2WinBUGS)

inits<-function(){
	list(alpha0 = 0, alpha1 = 0, alpha2 = 0, alpha12 = 0, sigma = 1)
}

data<-list(r = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10,   8, 10,   8, 23, 0,  3, 22, 15, 32, 3),
n = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51, 7),
x1 = c(0,   0,  0,   0,   0, 0,   0,   0,  0,   0,   0,  1,   1,   1,   1, 1,   1,  1,   1,   1, 1),
x2 = c(0,   0,  0,   0,   0, 1,   1,   1,  1,   1,   1,  0,   0,   0,   0, 0,   1,  1,   1,   1, 1),
N = 21)

test<-bugs(data,inits,

model.file="/home/meyerjp/rasch/test.bug",

parameters=c("alpha0","alpha1","alpha12","alpha2","sigma"),

n.chains=2,n.iter=10000,n.burnin=1000,

bugs.directory="/home/meyerjp/.wine/drive_c/Program Files/WinBUGS14/",
working.directory="/home/meyerjp/rasch/working",

debug=FALSE,
WINEPATH="/usr/bin/winepath",
newWINE=TRUE)


From Wolfgang.Viechtbauer at STAT.unimaas.nl  Thu Jul 26 18:18:36 2007
From: Wolfgang.Viechtbauer at STAT.unimaas.nl (Viechtbauer Wolfgang (STAT))
Date: Thu, 26 Jul 2007 18:18:36 +0200
Subject: [R] significance test for difference of two correlations
In-Reply-To: <16399234.12331185459168564.JavaMail.root@roerden.de>
Message-ID: <329A68716B57D54E8D39FD3F8A4A84DF057D5FDA@um-mail0136.unimaas.nl>

Let r_1 be the correlation between the two variables for the first group with n_1 subjects and let r_2 be the correlation for the second group with n_2 subjects. Then a simple way to test H0: rho_1 = rho_2 is to convert r_1 and r_2 via Fisher's variance stabilizing transformation ( z = 1/2 * ln[ (1+r)/(1-r)] ) and then calculate:

(z_1 - z_2) / sqrt( 1/(n_1 - 3) + 1/(n_2 - 3) )

which is (approximately) N(0,1) under H0. So, using alpha = .05, you can reject H0 if the absolute value of the test statistic above is larger than 1.96.

-- 
Wolfgang Viechtbauer
?Department of Methodology and Statistics
?University of Maastricht, The Netherlands
?http://www.wvbauer.com/



----Original Message----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Timo Stolz Sent:
Thursday, July 26, 2007 16:13 To: r-help at stat.math.ethz.ch
Subject: [R] significance test for difference of two correlations

> Dear R users,
> 
> how can I test, whether two correlations differ significantly. (I
> want to prove, that variables are correlated differently, depending
> on the group a person is in.)  
> 
> Greetings from Freiburg im Breisgau (Germany),
> Timo Stolz


From ggrothendieck at gmail.com  Thu Jul 26 18:25:07 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Jul 2007 12:25:07 -0400
Subject: [R] Help with Dates
In-Reply-To: <xmrsl7b9nsy.fsf@mralx1.rsma.frb.gov>
References: <20070719193534.QJUY29112.aamtaout04-winn.ispmail.ntl.com@Paula>
	<xmr1wevb70c.fsf@mralx1.rsma.frb.gov>
	<971536df0707260801u57ab405eqdb22876a8b3e089c@mail.gmail.com>
	<xmrsl7b9nsy.fsf@mralx1.rsma.frb.gov>
Message-ID: <971536df0707260925m6d70f0bau8fc4223824f7b57b@mail.gmail.com>

Yes, I was using 1.05.  I get the same result as you with 1.08.

On 26 Jul 2007 11:39:41 -0400, Jeffrey J. Hallman <jhallman at frb.gov> wrote:
> Are you using the latest version of fame?  1.05 and earlier had a bug in
> tisFromCsv that was fixed in 1.08.
>
> Below I show what I get with fame version 1.08.  There is still a problem in
> that the "frequency-figuring" logic appears to think the frequency is bwsunday
> (biweekly with weeks ending on Sunday) rather than semimonthly, which would
> appear to be a better fit.  That's why the 19860330 observation is getting
> filled in with NA's.
>
> Jeff
>
> > Lines <- "Date  Price Open.Int. Comm.Long Comm.Short net.comm
> 15-Jan-86 673.25    175645     65910      28425    37485
> 31-Jan-86 677.00    167350     54060      27120    26940
> 14-Feb-86 680.25    157985     37955      25425    12530
> 28-Feb-86 691.75    162775     49760      16030    33730
> 14-Mar-86 706.50    163495     54120      27995    26125
> 31-Mar-86 709.75    164120     54715      30390    24325"
>
> + + + + + + >
> > boink <- tisFromCsv(textConnection(Lines), dateFormat = "%d-%b-%y", dateCol = "Date", sep = "")
> > boink
> $Price
>           [,1]
> 19860119 673.25
> 19860202 677.00
> 19860216 680.25
> 19860302 691.75
> 19860316 706.50
> 19860330     NA
> 19860413 709.75
> class: tis
>
> $Open.Int.
>           [,1]
> 19860119 175645
> 19860202 167350
> 19860216 157985
> 19860302 162775
> 19860316 163495
> 19860330     NA
> 19860413 164120
> class: tis
>
> $Comm.Long
>          [,1]
> 19860119 65910
> 19860202 54060
> 19860216 37955
> 19860302 49760
> 19860316 54120
> 19860330    NA
> 19860413 54715
> class: tis
>
> $Comm.Short
>          [,1]
> 19860119 28425
> 19860202 27120
> 19860216 25425
> 19860302 16030
> 19860316 27995
> 19860330    NA
> 19860413 30390
> class: tis
>
> $net.comm
>          [,1]
> 19860119 37485
> 19860202 26940
> 19860216 12530
> 19860302 33730
> 19860316 26125
> 19860330    NA
> 19860413 24325
> class: tis
>
>
> "Gabor Grothendieck" <ggrothendieck at gmail.com> writes:
>
> > On 26 Jul 2007 09:59:31 -0400, Jeffrey J. Hallman <jhallman at frb.gov> wrote:
> > > zoo is nice.  'tisFromCsv()' in the fame package is nicer.
> > >
> > > Jeff
> >
> >
> > 1. What am I doing wrong here?  I only get one data column.
> > 2. I assume the regularized dates which do not exactly match the input ones
> >     are intended so as to make this a regularly spaced series.  Is that right?
> > 3. What is the cause of the warning message?
> > 4. Why is a list returned with a single component containing the output?
> > Thanks.
> >
> > > library(fame)
> > > Lines <- " Date  Price Open.Int. Comm.Long Comm.Short net.comm
> > + 15-Jan-86 673.25    175645     65910      28425    37485
> > + 31-Jan-86 677.00    167350     54060      27120    26940
> > + 14-Feb-86 680.25    157985     37955      25425    12530
> > + 28-Feb-86 691.75    162775     49760      16030    33730
> > + 14-Mar-86 706.50    163495     54120      27995    26125
> > + 31-Mar-86 709.75    164120     54715      30390    24325
> > + "
> > > tisFromCsv(textConnection(Lines), dateFormat = "%d-%b-%y", dateCol = "Date", sep = "")
> > [[1]]
> >            [,1]
> > 19860119 673.25
> > 19860202 677.00
> > 19860216 680.25
> > 19860302 691.75
> > 19860316 706.50
> > 19860330 709.75
> > class: tis
> >
> > Warning message:
> > number of items to replace is not a multiple of replacement length in:
> > x[i] <- value
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Jeff
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Thu Jul 26 18:30:17 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 26 Jul 2007 10:30:17 -0600
Subject: [R] multiple graphs
In-Reply-To: <JLRZBS$887F30CDFDF5C77E80D913136AD02F1C@libero.it>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF297B@LP-EXCHVS07.CO.IHC.COM>

One of the nice things about the R Graph Gallery is that if you click on
the R logo underneath the graph (may need to scroll down a bit) it will
show you the code used to create that particular graph.

You may also want to look at the subplot function in the TeachingDemos
package for another way to add histograms to a plot:

Here is one possible example of this:

x <- rep(1:10, each=25)
y <- rexp(250, 1/x)

library(TeachingDemos)

tmp1 <- hist(y, plot=FALSE)
r <- range(tmp1$breaks)
w <- diff(tmp1$breaks)
plot(x,y, type='n', xlim=c(0.5,10.5), ylim=r)
for(i in 1:10){
	tmp2 <- hist( y[x==i], breaks=tmp1$breaks, plot=FALSE )
	subplot( barplot(tmp2$counts, ylim=r, width=w,
                       horiz=TRUE, space=0, xaxt='n', yaxs='i'),
               c(i-0.45, i+.45), r
              )
}

points(x,y) # just to compare



Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Daniele Amberti
> Sent: Thursday, July 26, 2007 1:26 AM
> To: r-help
> Subject: [R] multiple graphs
> 
> Does anyone have a simple explanation and example on how to 
> add histograms or barcharts to an other graph like in the 
> example at the R-graph gallery:
> 
> http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=109
> 
> looking at the code I'not undertand very well how to add 
> graphs in arbitrary/clever position with an adequate scale.
> 
> If somebody have a simplier example with explanations it will 
> be highly appreciate.
> 
> Best
> Daniele
> 
> 
> ------------------------------------------------------
> Scegli infostrada: ADSL gratis per tutta l'estate e telefoni 
> senza canone Telecom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bruno.c at inwind.it  Thu Jul 26 18:31:01 2007
From: bruno.c at inwind.it (Bruno C.)
Date: Thu, 26 Jul 2007 18:31:01 +0200
Subject: [R] Diagonal  Submatrices Extraction
Message-ID: <JLSOJP$9598B722B570976F858DC5D6AD87933D@libero.it>

Yes you are right ... an example is mandatory.

So ... I have a matrix of 0 with just a single 1 per row and per column
I need to extract all maximal 'diagonal' submatrices

Let's say I have the following matrix

  A B C D E
a 0 1 0 0 0
b 1 0 0 0 0
c 0 0 1 0 0
d 0 0 0 1 0
e 0 0 0 0 1

well I would like to get, for this example, the two following submatrices
  A B                 C D E
a 0 1               c 1 0 0
b 1 0               d 0 1 0
                    e 0 0 1
Of course some of the extracted submatrices will have in some situations dim=c(1,1) ...

Thanks in advance
Bruno
> Hi :  I think you need to give an example because I don't understand
> below and my guess is that, since noone else replied,
> I don't think they understood it either. I don't mean to be rude. I've
> just noticed  from being on the list
> That, if something is not clear, people won't even tell you. They just
> won't respond. The list is great
> But people don't want to spend time trying to figure out what you want.
> An example and possibly code is really
> helpful in getting responses.
> 
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bruno C.
> Sent: Thursday, July 26, 2007 4:47 AM
> To: R-help
> Subject: [R] Submatrices Extraction
> 
> Hello,
> Given a submatrix containing 0 or 1
> I need to extract the indexes of all the diagonal submatrices so one of
> the two diagonals must contains only 1 for each submatrix ...
> Any help?
> 
> Thanks in advance
> Bruno
> 
> 
> ------------------------------------------------------
> Scegli infostrada: ADSL gratis per tutta l'estate e telefoni senza
> canone Telecom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> --------------------------------------------------------
> 
> This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are market counterparties or intermediate customers (as defined in the UK Financial Services Authority's rules).
> 


------------------------------------------------------
Scegli infostrada: ADSL gratis per tutta l?estate e telefoni senza canone Telecom


From ggrothendieck at gmail.com  Thu Jul 26 18:38:00 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Jul 2007 12:38:00 -0400
Subject: [R] significance test for difference of two correlations
In-Reply-To: <329A68716B57D54E8D39FD3F8A4A84DF057D5FDA@um-mail0136.unimaas.nl>
References: <16399234.12331185459168564.JavaMail.root@roerden.de>
	<329A68716B57D54E8D39FD3F8A4A84DF057D5FDA@um-mail0136.unimaas.nl>
Message-ID: <971536df0707260938w3a9349fatb5df585cfe856458@mail.gmail.com>

There is R code for both the Fisher transform and the corresponding bootstrap
procedure in the vignette for the proto package:
http://cran.r-project.org/doc/vignettes/proto/proto.pdf

On 7/26/07, Viechtbauer Wolfgang (STAT)
<Wolfgang.Viechtbauer at stat.unimaas.nl> wrote:
> Let r_1 be the correlation between the two variables for the first group with n_1 subjects and let r_2 be the correlation for the second group with n_2 subjects. Then a simple way to test H0: rho_1 = rho_2 is to convert r_1 and r_2 via Fisher's variance stabilizing transformation ( z = 1/2 * ln[ (1+r)/(1-r)] ) and then calculate:
>
> (z_1 - z_2) / sqrt( 1/(n_1 - 3) + 1/(n_2 - 3) )
>
> which is (approximately) N(0,1) under H0. So, using alpha = .05, you can reject H0 if the absolute value of the test statistic above is larger than 1.96.
>
> --
> Wolfgang Viechtbauer
> Department of Methodology and Statistics
> University of Maastricht, The Netherlands
> http://www.wvbauer.com/
>
>
>
> ----Original Message----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Timo Stolz Sent:
> Thursday, July 26, 2007 16:13 To: r-help at stat.math.ethz.ch
> Subject: [R] significance test for difference of two correlations
>
> > Dear R users,
> >
> > how can I test, whether two correlations differ significantly. (I
> > want to prove, that variables are correlated differently, depending
> > on the group a person is in.)
> >
> > Greetings from Freiburg im Breisgau (Germany),
> > Timo Stolz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From yn19832 at msn.com  Thu Jul 26 18:39:06 2007
From: yn19832 at msn.com (livia)
Date: Thu, 26 Jul 2007 09:39:06 -0700 (PDT)
Subject: [R] Fit t Copula
Message-ID: <11814432.post@talk.nabble.com>


Hi, I am trying to fit t copula to some data, and I am using the following
function in the library(QRMlib).
Udatac <- apply(datac, 2, edf,adjust=1)
tcopulac <- fit.tcopula.rank(Udatac)

But the error message come out "Error in fit.tcopula.rank(Udatac) : Non
p.s.d. covariance matrix"

Could anyone give me some advice? In fact, I am not sure what the "adjust=1"
is used for.
Many thanks.

-- 
View this message in context: http://www.nabble.com/Fit-t-Copula-tf4152818.html#a11814432
Sent from the R help mailing list archive at Nabble.com.


From dylan.beaudette at gmail.com  Thu Jul 26 18:46:04 2007
From: dylan.beaudette at gmail.com (Dylan Beaudette)
Date: Thu, 26 Jul 2007 09:46:04 -0700
Subject: [R] ROC curve in R
In-Reply-To: <46A89B37.9090707@vanderbilt.edu>
References: <OFA090DBBC.D4F33858-ON65257324.00263229-65257324.00265332@ccilindia.co.in>
	<46A89B37.9090707@vanderbilt.edu>
Message-ID: <200707260946.04256.dylan.beaudette@gmail.com>

On Thursday 26 July 2007 06:01, Frank E Harrell Jr wrote:
> Note that even though the ROC curve as a whole is an interesting
> 'statistic' (its area is a linear translation of the
> Wilcoxon-Mann-Whitney-Somers-Goodman-Kruskal rank correlation
> statistics), each individual point on it is an improper scoring rule,
> i.e., a rule that is optimized by fitting an inappropriate model.  Using
> curves to select cutoffs is a low-precision and arbitrary operation, and
> the cutoffs do not replicate from study to study.  Probably the worst
> problem with drawing an ROC curve is that it tempts analysts to try to
> find cutoffs where none really exist, and it makes analysts ignore the
> whole field of decision theory.
>
> Frank Harrell

Frank,

This thread has caught may attention for a couple reasons, possibly related to 
my novice-level experience. 

1. in a logistic regression study, where i am predicting the probability of 
the response being 1 (for example) - there exists a continuum of probability 
values - and a finite number of {1,0} realities when i either look within the 
original data set, or with a new 'verification' data set. I understand that 
drawing a line through the probabilities returned from the logistic 
regression is a loss of information, but there are times when a 'hard' 
decision requiring prediction of {1,0} is required. I have found that the 
ROCR package (not necessarily the ROC Curve) can be useful in identifying the 
probability cutoff where accuracy is maximized. Is this an unreasonable way 
of using logistic regression as a predictor? 

2. The ROC curve can be a helpful way of communicating false positives / false 
negatives to other users who are less familiar with the output and 
interpretation of logistic regression. 


3. I have been using the area under the ROC Curve, kendall's tau, and cohen's 
kappa to evaluate the accuracy of a logistic regression based prediction, the 
last two statistics based on a some probability cutoff identified before 
hand. 


How does the topic of decision theory relate to some of the circumstances 
described above? Is there a better way to do some of these things?

Cheers,

Dylan



>
> gyadav at ccilindia.co.in wrote:
> > http://search.r-project.org/cgi-bin/namazu.cgi?query=ROC&max=20&result=no
> >rmal&sort=score&idxname=Rhelp02a&idxname=functions&idxname=docs
> >
> > there is a lot of help try help.search("ROC curve") gave
> > Help files with alias or concept or title matching 'ROC curve' using
> > fuzzy matching:
> >
> >
> >
> > granulo(ade4)                             Granulometric Curves
> > plot.roc(analogue)                        Plot ROC curves and associated
> > diagnostics
> > roc(analogue)                             ROC curve analysis
> > colAUC(caTools)                           Column-wise Area Under ROC
> > Curve (AUC)
> > DProc(DPpackage)                          Semiparametric Bayesian ROC
> > curve analysis
> > cv.enet(elasticnet)                       Computes K-fold cross-validated
> > error curve for elastic net
> > ROC(Epi)                                  Function to compute and draw
> > ROC-curves.
> > lroc(epicalc)                             ROC curve
> > cv.lars(lars)                             Computes K-fold cross-validated
> > error curve for lars
> > roc.demo(TeachingDemos)                   Demonstrate ROC curves by
> > interactively building one
> >
> > HTH
> > see the help and examples those will suffice
> >
> > Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.
> >
> >
> >
> > Regards,
> >
> > Gaurav Yadav
> > +++++++++++
> > Assistant Manager, CCIL, Mumbai (India)
> > Mob: +919821286118 Email: emailtogauravyadav at gmail.com
> > Bhagavad Gita:  Man is made by his Belief, as He believes, so He is
> >
> >
> >
> > "Rithesh M. Mohan" <rithesh.m at brickworkindia.com>
> > Sent by: r-help-bounces at stat.math.ethz.ch
> > 07/26/2007 11:26 AM
> >
> > To
> > <R-help at stat.math.ethz.ch>
> > cc
> >
> > Subject
> > [R] ROC curve in R
> >
> >
> >
> >
> >
> >
> > Hi,
> >
> >
> >
> > I need to build ROC curve in R, can you please provide data steps / code
> > or guide me through it.
> >
> >
> >
> > Thanks and Regards
> >
> > Rithesh M Mohan
> >
> >
> >                  [[alternative HTML version deleted]]
>
> -
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Thu Jul 26 18:54:54 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Jul 2007 18:54:54 +0200
Subject: [R] error in using R2WinBUGS on Ubuntu 6.10 Linux
In-Reply-To: <20070726120338.AUQ96591@mpmail1.jmu.edu>
References: <20070726120338.AUQ96591@mpmail1.jmu.edu>
Message-ID: <46A8D1DE.9000607@statistik.uni-dortmund.de>



meyerjp at jmu.edu wrote:
> I am trying to run WinBUGS 1.4 from the Ubuntu 6.10 Linux distribution. I am using the R2WinBUGS packages with the  source file listed below. WinBUGS appears to run properly, but I get the following message after WinBUGS starts in WINE. Does anyone know what may be causing this error and what the correction may be?
> 
> Thanks
> 
> ERROR MESSAGE:
> 
> fixme:ole:GetHGlobalFromILockBytes cbSize is 13824
> err:ole:CoGetClassObject class {0003000a-0000-0000-c000-000000000046} not registered
> err:ole:CoGetClassObject class {0003000a-0000-0000-c000-000000000046} not registered
> err:ole:CoGetClassObject no class object {0003000a-0000-0000-c000-000000000046} could be created for context 0x3
> fixme:keyboard:RegisterHotKey (0x10032,13,0x00000002,3): stub
> fixme:ntdll:RtlNtStatusToDosErrorNoTeb no mapping for 8000000a
> err:ole:local_server_thread Failure during ConnectNamedPipe 317


This is wine, not R2WinBUGS nor WinBUGS nor R, I fear, and the "fixme:" 
sounds promising that things go away in a more recent version of wine...

Uwe Ligges

> 
> 
> R SOURCE FILE:
> 
> rm(list=ls(all=TRUE))
> 
> library(R2WinBUGS)
> 
> inits<-function(){
> 	list(alpha0 = 0, alpha1 = 0, alpha2 = 0, alpha12 = 0, sigma = 1)
> }
> 
> data<-list(r = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10,   8, 10,   8, 23, 0,  3, 22, 15, 32, 3),
> n = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51, 7),
> x1 = c(0,   0,  0,   0,   0, 0,   0,   0,  0,   0,   0,  1,   1,   1,   1, 1,   1,  1,   1,   1, 1),
> x2 = c(0,   0,  0,   0,   0, 1,   1,   1,  1,   1,   1,  0,   0,   0,   0, 0,   1,  1,   1,   1, 1),
> N = 21)
> 
> test<-bugs(data,inits,
> 
> model.file="/home/meyerjp/rasch/test.bug",
> 
> parameters=c("alpha0","alpha1","alpha12","alpha2","sigma"),
> 
> n.chains=2,n.iter=10000,n.burnin=1000,
> 
> bugs.directory="/home/meyerjp/.wine/drive_c/Program Files/WinBUGS14/",
> working.directory="/home/meyerjp/rasch/working",
> 
> debug=FALSE,
> WINEPATH="/usr/bin/winepath",
> newWINE=TRUE)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From davidsonrac at gmail.com  Thu Jul 26 18:55:07 2007
From: davidsonrac at gmail.com (Rachel Davidson)
Date: Thu, 26 Jul 2007 12:55:07 -0400
Subject: [R] zeroinfl() or zicounts() error
Message-ID: <651263270707260955t1da9339dj46302a61dbfab965@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/c97a72bf/attachment.pl 

From Greg.Snow at intermountainmail.org  Thu Jul 26 19:21:29 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 26 Jul 2007 11:21:29 -0600
Subject: [R] Redirecting print output
In-Reply-To: <000f01c7cda3$9bcaa1e0$6405a8c0@MXD32803WB>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF29B4@LP-EXCHVS07.CO.IHC.COM>

You may want to look at the R2HTML package as one approach (others have
already told you about sink and cat).

Another approach is to use the variations on sweave.  Here you set up a
template file with the code you want run as well as any explanitory text
(you can even write an entire report), then process this with sweave and
the output will be included.  The original sweave works with LaTeX,
there is an HTML driver for sweave in the R2HTML package (so the source
and final documents are html) and there is an odfWeave package that lets
you create the template and output in a word processor (uses the
openoffice word processor, but since you can convert from and to Msword
from there, this is not much of a limitation).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stan Hopkins
> Sent: Monday, July 23, 2007 9:35 PM
> To: R help
> Subject: [R] Redirecting print output
> 
> I see a rich set of graphic device functions to redirect that 
> output.  Are there commands to redirect text as well.  I have 
> a set of functions that execute many linear regression tests 
> serially and I want to capture this in a file for printing.
> 
> Thanks,
> 
> Stan Hopkins
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From f.harrell at vanderbilt.edu  Thu Jul 26 19:45:46 2007
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 26 Jul 2007 12:45:46 -0500
Subject: [R] ROC curve in R
In-Reply-To: <200707260946.04256.dylan.beaudette@gmail.com>
References: <OFA090DBBC.D4F33858-ON65257324.00263229-65257324.00265332@ccilindia.co.in>
	<46A89B37.9090707@vanderbilt.edu>
	<200707260946.04256.dylan.beaudette@gmail.com>
Message-ID: <46A8DDCA.2030400@vanderbilt.edu>

Dylan Beaudette wrote:
> On Thursday 26 July 2007 06:01, Frank E Harrell Jr wrote:
>> Note that even though the ROC curve as a whole is an interesting
>> 'statistic' (its area is a linear translation of the
>> Wilcoxon-Mann-Whitney-Somers-Goodman-Kruskal rank correlation
>> statistics), each individual point on it is an improper scoring rule,
>> i.e., a rule that is optimized by fitting an inappropriate model.  Using
>> curves to select cutoffs is a low-precision and arbitrary operation, and
>> the cutoffs do not replicate from study to study.  Probably the worst
>> problem with drawing an ROC curve is that it tempts analysts to try to
>> find cutoffs where none really exist, and it makes analysts ignore the
>> whole field of decision theory.
>>
>> Frank Harrell
> 
> Frank,
> 
> This thread has caught may attention for a couple reasons, possibly related to 
> my novice-level experience. 
> 
> 1. in a logistic regression study, where i am predicting the probability of 
> the response being 1 (for example) - there exists a continuum of probability 
> values - and a finite number of {1,0} realities when i either look within the 
> original data set, or with a new 'verification' data set. I understand that 
> drawing a line through the probabilities returned from the logistic 
> regression is a loss of information, but there are times when a 'hard' 
> decision requiring prediction of {1,0} is required. I have found that the 
> ROCR package (not necessarily the ROC Curve) can be useful in identifying the 
> probability cutoff where accuracy is maximized. Is this an unreasonable way 
> of using logistic regression as a predictor? 

Logistic regression (with suitable attention to not assuming linearity 
and to avoiding overfitting) is a great way to estimate P[Y=1].  Given 
good predicted P[Y=1] and utilities (losses, costs) for incorrect 
positive and negative decisions, an optimal decision is one that 
optimizes expected utility.  The ROC curve does not play a direct role 
in this regard.  If per-subject utilities are not available, the analyst 
may make various assumptions about utilities (including the unreasonable 
but often used assumption that utilities do not vary over subjects) to 
find a cutoff on P[Y=1].  A very nice feature of P[Y=1] is that error 
probabilities are self-contained.  For example if P[Y=1] = .02 for a 
single subject and you predict Y=0, the probability of an error is .02 
by definition.  One doesn't need to compute an overall error probability 
over the whole distribution of subjects' risks.  If the cost of a false 
negative is C, the expected cost is .02*C in this example.

> 
> 2. The ROC curve can be a helpful way of communicating false positives / false 
> negatives to other users who are less familiar with the output and 
> interpretation of logistic regression. 

What is more useful than that is a rigorous calibration curve estimate 
to demonstrate the faithfulness of predicted P[Y=1] and a histogram 
showing the distribution of predicted P[Y=1].  Models that put a lot of 
predictions near 0 or 1 are the most discriminating.  Calibration curves 
and risk distributions are easier to explain than ROC curves.  Too often 
a statistician will solve for a cutoff on P[Y=1], imposing her own 
utility function without querying any subjects.

> 
> 
> 3. I have been using the area under the ROC Curve, kendall's tau, and cohen's 
> kappa to evaluate the accuracy of a logistic regression based prediction, the 
> last two statistics based on a some probability cutoff identified before 
> hand. 

ROC area (equiv. to Wilcoxon-Mann-Whitney and Somers' Dxy rank 
correlation between pred. P[Y=1] and Y) is a measure of pure 
discrimination, not a measure of accuracy per se.  Rank correlation 
(concordance) measures do not require the use of cutoffs.

> 
> 
> How does the topic of decision theory relate to some of the circumstances 
> described above? Is there a better way to do some of these things?

See above re: expected loses/utilities.

Good questions.

Frank
> 
> Cheers,
> 
> Dylan
> 
> 
> 
>> gyadav at ccilindia.co.in wrote:
>>> http://search.r-project.org/cgi-bin/namazu.cgi?query=ROC&max=20&result=no
>>> rmal&sort=score&idxname=Rhelp02a&idxname=functions&idxname=docs
>>>
>>> there is a lot of help try help.search("ROC curve") gave
>>> Help files with alias or concept or title matching 'ROC curve' using
>>> fuzzy matching:
>>>
>>>
>>>
>>> granulo(ade4)                             Granulometric Curves
>>> plot.roc(analogue)                        Plot ROC curves and associated
>>> diagnostics
>>> roc(analogue)                             ROC curve analysis
>>> colAUC(caTools)                           Column-wise Area Under ROC
>>> Curve (AUC)
>>> DProc(DPpackage)                          Semiparametric Bayesian ROC
>>> curve analysis
>>> cv.enet(elasticnet)                       Computes K-fold cross-validated
>>> error curve for elastic net
>>> ROC(Epi)                                  Function to compute and draw
>>> ROC-curves.
>>> lroc(epicalc)                             ROC curve
>>> cv.lars(lars)                             Computes K-fold cross-validated
>>> error curve for lars
>>> roc.demo(TeachingDemos)                   Demonstrate ROC curves by
>>> interactively building one
>>>
>>> HTH
>>> see the help and examples those will suffice
>>>
>>> Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.
>>>
>>>
>>>
>>> Regards,
>>>
>>> Gaurav Yadav
>>> +++++++++++
>>> Assistant Manager, CCIL, Mumbai (India)
>>> Mob: +919821286118 Email: emailtogauravyadav at gmail.com
>>> Bhagavad Gita:  Man is made by his Belief, as He believes, so He is
>>>
>>>
>>>
>>> "Rithesh M. Mohan" <rithesh.m at brickworkindia.com>
>>> Sent by: r-help-bounces at stat.math.ethz.ch
>>> 07/26/2007 11:26 AM
>>>
>>> To
>>> <R-help at stat.math.ethz.ch>
>>> cc
>>>
>>> Subject
>>> [R] ROC curve in R
>>>
>>>
>>>
>>>
>>>
>>>
>>> Hi,
>>>
>>>
>>>
>>> I need to build ROC curve in R, can you please provide data steps / code
>>> or guide me through it.
>>>
>>>
>>>
>>> Thanks and Regards
>>>
>>> Rithesh M Mohan
>>>
>>>
>>>                  [[alternative HTML version deleted]]
>> -
>> Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                       Department of Biostatistics   Vanderbilt University
>>


From fn211 at cam.ac.uk  Thu Jul 26 20:07:26 2007
From: fn211 at cam.ac.uk (Florian Nigsch)
Date: Thu, 26 Jul 2007 19:07:26 +0100
Subject: [R] Large dataset + randomForest
Message-ID: <FA5AA232-978B-48F6-86F8-0B467B9F2B58@cam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/670110c5/attachment.pl 

From Max.Kuhn at pfizer.com  Thu Jul 26 20:26:11 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Thu, 26 Jul 2007 14:26:11 -0400
Subject: [R] Large dataset + randomForest
In-Reply-To: <FA5AA232-978B-48F6-86F8-0B467B9F2B58@cam.ac.uk>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3090A7977@groamrexm03.amer.pfizer.com>

Florian,

The first thing that you should change is how you call randomForest.
Instead of specifying the model via a formula, use the randomForest(x,
y) interface.

When a formula is used, there is a terms object created so that a model
matrix can be created for these and future observations. That terms
object can get big (I think it would be a matrix of size 151 x 150) and
is diagonal. 

That might not solve it, but it should help.

Max

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Florian Nigsch
Sent: Thursday, July 26, 2007 2:07 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Large dataset + randomForest

[Please CC me in any replies as I am not currently subscribed to the  
list. Thanks!]

Dear all,

I did a bit of searching on the question of large datasets but did  
not come to a definite conclusion. What I am trying to do is the  
following: I want to read in a dataset with approx. 100 000 rows and  
approx 150 columns. The file size is ~ 33MB, which one would deem not  
too big a file for R. To speed up the reading in of the file I do not  
use read.table but a loop that does reading with scan() into a buffer  
and some preprocessing and then adds the data into a dataframe.

When I then want to run randomForest() R complains that I cannot  
allocate a vector of size 313.0 MB. I am aware that randomForest  
needs all data in memory, but
1) why should that suddenly be 10 times the size of the data (I  
acknowedge the need for some internal data of R, but 10 times seems a  
bit too much) and
2) there is still physical memory free on the machine (in total 4GB  
available, even though R is limited to 2GB if I correctly remember  
the help pages - still 2GB should be enough!) - it doesn't seem to  
work either with changed settings done via mem.limits(), or run-time  
arguments --min-vsize --max-vsize - what do these have to be set to  
to work in my case??

 > rf <- randomForest(V1 ~ ., data=df[trainindices,], do.trace=5)
Error: cannot allocate vector of size 313.0 Mb
 > object.size(df)/1024/1024
[1] 129.5390


Any help would be greatly appreciated,

Florian

--
Florian Nigsch <fn211 at cam.ac.uk>
Unilever Centre for Molecular Sciences Informatics
Department of Chemistry
University of Cambridge
http://www-mitchell.ch.cam.ac.uk/
Telephone: +44 (0)1223 763 073




	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From tom.olsson at dnbnor.com  Thu Jul 26 20:29:07 2007
From: tom.olsson at dnbnor.com (Tom.O)
Date: Thu, 26 Jul 2007 11:29:07 -0700 (PDT)
Subject: [R] Create Strings of Column Id's
Message-ID: <11816439.post@talk.nabble.com>


Does anyone know how this is don?

I have a large matrix where I extract specific columns into txt files for
further use. To be able to keep track of which txt files contain which
columns I want to name the filenames with the column Id's.

The most basic example would be to use an for() loop together with paste(),
but the result is blank. Not even NULL.

this is the concept of thecode i use:

for example

MyMatrix <- matrix(NA,ncol=4,nrow=1,dimnames=list(NULL,c("E","R","T","Y")))
COL <- c(1,3) # a vector of columns I want to extract,

Filename <- NULL # the starting variable, so I can use paste
Filename <- for(i in colnames(MyMatrix)[COL]) {paste(Filename,"-",i,sep="")}

The result is "-T", but I want it to be "-E-T"

Anyone have a clue?

Thanks Tom


-- 
View this message in context: http://www.nabble.com/Create-Strings-of-Column-Id%27s-tf4153354.html#a11816439
Sent from the R help mailing list archive at Nabble.com.


From maitra at iastate.edu  Thu Jul 26 20:33:29 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Thu, 26 Jul 2007 13:33:29 -0500
Subject: [R] using contrasts on matrix regressions (using gmodels,
 perhaps): 2 Solutions
In-Reply-To: <20070725183036.6bf59e9d@subarnarekha.stat.iastate.edu>
References: <20070725183036.6bf59e9d@subarnarekha.stat.iastate.edu>
Message-ID: <20070726133329.618be9eb@subarnarekha.stat.iastate.edu>

Dear list,

I got two responses to my post. One was from Soren with a follow-up on personal e-mail, and the other I leave anonymous since he contacted me on personal e-mail. Anyway, here we go:

The first (Soren):

library(doBy)

Y <- as.data.frame(Y)
 
lapply(Y,function(y){reg<- lm(y~X); esticon(reg, c(0,0, 0, 1, 0, -1) )})
 
Confidence interval ( WALD ) level = 0.95 
Confidence interval ( WALD ) level = 0.95 
Confidence interval ( WALD ) level = 0.95 
Confidence interval ( WALD ) level = 0.95 
Confidence interval ( WALD ) level = 0.95 
$V1
  beta0  Estimate Std.Error  t.value DF  Pr(>|t|)  Lower.CI Upper.CI
1     0 0.6701771  0.517921 1.293976  4 0.2653302 -0.767802 2.108156
$V2
  beta0   Estimate Std.Error    t.value DF  Pr(>|t|)  Lower.CI Upper.CI
1     0 -0.2789954   0.64481 -0.4326784  4 0.6875555 -2.069275 1.511284
$V3
  beta0   Estimate Std.Error    t.value DF  Pr(>|t|)  Lower.CI Upper.CI
1     0 -0.7677927 0.9219688 -0.8327751  4 0.4518055 -3.327588 1.792003
$V4
  beta0   Estimate Std.Error   t.value DF Pr(>|t|)  Lower.CI  Upper.CI
1     0 -0.6026635 0.4960805 -1.214850  4  0.29123 -1.980004 0.7746768
$V5
  beta0 Estimate Std.Error  t.value DF Pr(>|t|)  Lower.CI Upper.CI
1     0 2.001558  1.004574 1.992444  4 0.117123 -0.787587 4.790703

 

One thing I do not know how to handle is the output "Confidence interval ( WALD ) level = 0.95"  which shows up for every regression. When I do millions of regressions, this seriously slows it all down. Any idea how I can suppress that?



The second solution uses gmodels, with a lucid explanation which I reproduce. Thanks!
 

The second (anon):

For a standard (non-matrix) regression, you could test the hypothesis  
X3=X4 using

	estimable(reg, c("(Intercept)"=0, X1=0, X2=0, X3=1, X4=0, X5=-1) )

but this won't currently work with the mlm object created by a matrix  
regression.

The best way to solve this problem is to write an estimable.mlm()  
function that simply extracts the individual regressions from the mlm  
object and then calls estimable on each of these, pasting the results  
back together appropriately.

Something like this should do the trick:

`estimable.mlm` <-
   function (object, ...)
{
   coef <- coef(object)
   ny <- ncol(coef)
   effects <- object$effects
   resid <- object$residuals
   fitted <- object$fitted
   ynames <- colnames(coef)
   if (is.null(ynames)) {
     lhs <- object$terms[[2]]
     if (mode(lhs) == "call" && lhs[[1]] == "cbind")
       ynames <- as.character(lhs)[-1]
     else ynames <- paste("Y", seq(ny), sep = "")
   }
   value <- vector("list", ny)
   names(value) <- paste("Response", ynames)
   cl <- oldClass(object)
   class(object) <- cl[match("mlm", cl):length(cl)][-1]
   for (i in seq(ny)) {
     object$coefficients <- coef[, i]
     object$residuals <- resid[, i]
     object$fitted.values <- fitted[, i]
     object$effects <- effects[, i]
     object$call$formula[[2]] <- object$terms[[2]] <- as.name(ynames[i])
     value[[i]] <- estimable(object, ...)
   }
   class(value) <- "listof"
   value
}

Now this all works:

 > X <- matrix(rnorm(50),10,5)
 > Y <- matrix(rnorm(50),10,5)
 > reg <- lm(Y~X)
 > estimable(reg, c("(Intercept)"=0, X1=0, X2=0, X3=1, X4=0, X5=-1) )  

Response Y1 :
                  Estimate Std. Error   t value DF  Pr(>|t|)
(0 0 0 1 0 -1) -0.9024065  0.4334235 -2.082043  4 0.1057782

Response Y2 :
                  Estimate Std. Error   t value DF   Pr(>|t|)
(0 0 0 1 0 -1) -0.7017988  0.2199234 -3.191106  4 0.03318115

Response Y3 :
                 Estimate Std. Error  t value DF  Pr(>|t|)
(0 0 0 1 0 -1) 0.5412863  0.2632527 2.056147  4 0.1089276

Response Y4 :
                  Estimate Std. Error    t value DF Pr(>|t|)
(0 0 0 1 0 -1) -0.1028162  0.5973959 -0.1721073  4  0.87171

Response Y5 :
                 Estimate Std. Error  t value DF  Pr(>|t|)
(0 0 0 1 0 -1) 0.2493330  0.2024061 1.231845  4 0.2854716











On Wed, 25 Jul 2007 18:30:36 -0500 Ranjan Maitra <maitra at iastate.edu>
wrote:

> Hi, 
> 
> I want to test for a contrast from a regression where I am regressing the columns of a matrix. In short, the following.
> 
> X <- matrix(rnorm(50),10,5)
> Y <- matrix(rnorm(50),10,5)
> lm(Y~X)  
> 
> Call:
> lm(formula = Y ~ X)
> 
> Coefficients:
>              [,1]     [,2]     [,3]     [,4]     [,5]   
> (Intercept)   0.3350  -0.1989  -0.1932   0.7528   0.0727
> X1            0.2007  -0.8505   0.0520   0.1501   0.3248
> X2            0.3212   0.7008  -0.0963  -0.2584   0.6711
> X3            0.3781  -0.7321   0.1907  -0.1721   0.3073
> X4           -0.1778   0.2822  -0.0644  -0.2649  -0.4140
> X5           -0.1079  -0.0475   0.6047  -0.8369  -0.5928
> 
> 
> I want to test for c'b = 0 where c is (lets say) the contrast (0, 0, 1, 0, -1). Is it possible to do so, in one shot, using gmodels or something else?
> 
> Many thanks and best wishes,
> Ranjan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Greg.Snow at intermountainmail.org  Thu Jul 26 20:36:52 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Thu, 26 Jul 2007 12:36:52 -0600
Subject: [R] Function to separate effect in AOV
In-Reply-To: <200707231904.55564.chrysopa@gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF29F9@LP-EXCHVS07.CO.IHC.COM>

You may want to look at the interaction function (a quick way to make the single factor with 4 levels that you mention).

You can create your own sets of contrasts and set them using the C or contrasts functions, then use the split argument to summary.aov to look at the individual degrees of freedom.

You may also be interested in the multcomp package for looking at the comparisons.

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Ronaldo Reis Junior
> Sent: Monday, July 23, 2007 4:05 PM
> To: R-Help
> Subject: [R] Function to separate effect in AOV
> 
> Hi,
> 
> I have a dummy question.
> 
> Suppose that I have two explanatory variable, T1 (A, B) and 
> T2 (C, D) and one response variable.
> 
> > attach(dados)
> 
> > tapply(Y,list(T1,T2),mean)
>          C        D
> A 2.200000 10.20000
> B 2.223333 20.26667
> 
> In this case, "A" and "B" inside "C" have no difference, but 
> have differences inside "D"
> 
> I make this model:
> 
> > m <- aov(Y~T1*T2)
> > 
> > summary(m)
>             Df Sum Sq Mean Sq F value    Pr(>F)    
> T1           1  76.36   76.36  5617.9 1.119e-12 ***
> T2           1 508.69  508.69 37426.7 5.704e-16 ***
> T1:T2        1  75.65   75.65  5566.0 1.161e-12 ***
> Residuals    8   0.11    0.01                      
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> 
> This result don't show the reality of the data, because I 
> cant see that "A" 
> and "B" inside "C" are the same.
> 
> The anova result is the same of an full different levels, like this:
> 
> > attach(dados2)
> > 
> > tapply(Y,list(T1,T2),mean)
>          C        D
> A 6.100000 10.20000
> B 2.223333 20.26667
> > 
> > m <- aov(Y~T1*T2)
> > 
> > summary(m)
>             Df Sum Sq Mean Sq F value    Pr(>F)    
> T1           1  28.74   28.74  2114.3 5.529e-11 ***
> T2           1 367.75  367.75 27056.7 2.088e-15 ***
> T1:T2        1 145.81  145.81 10728.1 8.433e-14 ***
> Residuals    8   0.11    0.01                      
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> In this case all level are different, C to D and A to B.
> 
> The question is:
> 
> The only way to find this real difference is:
> 
> 1) make T1 and T2 like a Treatment variable with 4 levels 
> (AC,BC,AD,BD)?
> 
> or
> 
> 2) make 3 anova:
> 	a) Anova (A,B) inside C
> 	b) Anova (A,B) inside D
> 	c) Full factorial Anova (like this in the e-mail)
> 
> or
> 
> 3) exist any other way to make this in only one analysis, to 
> find all differences e interactions? In other words, to find 
> differences in "A" 
> and "B" inside "C", "A" and "B" inside "D", "C" and "D" 
> inside "A" and "C" 
> and "D" inside "B" 
> 
> Thanks
> Ronaldo
> --
> > Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/Depto. Biologia Geral/Lab. de Ecologia
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia `. 
> | `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
> |   `- Fone: (38) 3229-8187 | ronaldo.reis at unimontes.br | 
> | chrysopa at gmail.com http://www.ppgcb.unimontes.br/ | ICQ#: 5692561 | 
> | LinuxUser#: 205366
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jholtman at gmail.com  Thu Jul 26 20:52:42 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 26 Jul 2007 14:52:42 -0400
Subject: [R] Create Strings of Column Id's
In-Reply-To: <11816439.post@talk.nabble.com>
References: <11816439.post@talk.nabble.com>
Message-ID: <644e1f320707261152k6db5131bh3a2f5040ffc9f3ea@mail.gmail.com>

Is this what you want:

> paste("-", paste(colnames(MyMatrix)[COL], collapse='-'), sep='')
[1] "-E-T"


On 7/26/07, Tom.O <tom.olsson at dnbnor.com> wrote:
>
> Does anyone know how this is don?
>
> I have a large matrix where I extract specific columns into txt files for
> further use. To be able to keep track of which txt files contain which
> columns I want to name the filenames with the column Id's.
>
> The most basic example would be to use an for() loop together with paste(),
> but the result is blank. Not even NULL.
>
> this is the concept of thecode i use:
>
> for example
>
> MyMatrix <- matrix(NA,ncol=4,nrow=1,dimnames=list(NULL,c("E","R","T","Y")))
> COL <- c(1,3) # a vector of columns I want to extract,
>
> Filename <- NULL # the starting variable, so I can use paste
> Filename <- for(i in colnames(MyMatrix)[COL]) {paste(Filename,"-",i,sep="")}
>
> The result is "-T", but I want it to be "-E-T"
>
> Anyone have a clue?
>
> Thanks Tom
>
>
> --
> View this message in context: http://www.nabble.com/Create-Strings-of-Column-Id%27s-tf4153354.html#a11816439
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From Achim.Zeileis at wu-wien.ac.at  Thu Jul 26 21:02:55 2007
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 26 Jul 2007 21:02:55 +0200 (CEST)
Subject: [R] zeroinfl() or zicounts() error
In-Reply-To: <651263270707260955t1da9339dj46302a61dbfab965@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0707262054150.16266-100000@disco.wu-wien.ac.at>

On Thu, 26 Jul 2007, Rachel Davidson wrote:

> I'm trying to fit a zero-inflated poisson model using zeroinfl() from the
> pscl library. It works fine for most models I try, but when I include either
> of 2 covariates, I get an error.
>
> When I include "PopulationDensity", I get this error: Error in solve.default
> (as.matrix(fit$hessian)) :        system is computationally singular:
> reciprocal condition number = 1.91306e-34
>
> When I include "BuildingArea", I get this error: Error in optim(fn =
> loglikfun, par = c(start$count, start$zero, if (dist ==  :
> non-finite finite-difference value [2]

Might be due to some close to linear dependencies in your regressor
matrix...

>  I tried fitting the models using zicounts in the zicounts library as well
> and had the same difficulty.

If I recall correctly, zicounts() usses a very similar type of
optimization compared to zeroinfl(), hence the similar problems.

>  When I include "PopulationDensity", it runs, but outputs only the parameter
> estimates, not the standard errors or p-values (those have NaN).

This is due to the same problem as above for zeroinfl(), the Hessian
matrix is (close to) singular.

> When I include "BuildingArea", I get this error: Error in
> solve.default(z0$hessian)
> : system is computationally singular: reciprocal condition number =
> 2.58349e-25
>
> Can anyone suggest what it is about these 2 covariates that might be causing
> the problem? I don't see any obvious problems with them. They are both
> nonnegative with smooth probability distributions and no missing ("NA")
> values. The dataset has 3211 observations. It doesn't matter if there are
> other covariates in the models or not. If one of these is included, I get
> the errors.

Even if you include just one covariate and nothing else?
  zeroinfl(y ~ PopulationDensity, data = ...)

Z


From peltier.david at mac.com  Thu Jul 26 20:59:23 2007
From: peltier.david at mac.com (David Peltier)
Date: Thu, 26 Jul 2007 14:59:23 -0400
Subject: [R] R CMD check sh: line 1: make: command not found
Message-ID: <346AB00C-BE66-4925-A452-A14558B61BAC@mac.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/042a14f0/attachment.pl 

From rsb at wsu.edu  Thu Jul 26 21:11:41 2007
From: rsb at wsu.edu (Bricklemyer, Ross S)
Date: Thu, 26 Jul 2007 12:11:41 -0700
Subject: [R] princomp error
Message-ID: <2FC987BC0B90B24786CAF43DD3F5719CC7DA40@CRU105.cahe.ad.wsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/972ef028/attachment.pl 

From bolker at zoo.ufl.edu  Thu Jul 26 21:46:57 2007
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Thu, 26 Jul 2007 15:46:57 -0400
Subject: [R] Constructing bar charts with standard error bars
Message-ID: <46A8FA31.10109@zoo.ufl.edu>


John Zabroski wrote:
> On 7/25/07, Ben Bolker <bolker at ufl.edu> wrote:
>
> Thanks a lot!  I tried all three and they all seem very dependable.
> Also, I appreciate you rewriting my solution and adding elegance.
>
> Is there a way to extend the tick marks to the ylim values, such that
> the yscale ymax tickmark is something like max(xbar+se)?  In the
> documentation, I thought par(yaxp=c(y0,y1,n)) would do the trick, but
> after trying to use it I am not sure I understand what yaxp even does.

 It took me quite a while to figure this out, I'm not surprised you 
didn't ...

The very easiest way to do this is simply to set ylim to (0,0.4) -- since
you probably want to extend the axes upward to a "pretty" number
anyway.

The other standard way to do this is to use barplot with axes=FALSE and
then add the axes yourself, with the ticks specified wherever you want:

barplot(...,ylim=c(0,0.4),axes=FALSE)
axis(side=1)
axis(side=2,at=seq(0,0.4,length=8))

  However, I was wondering what was up with yaxp, and why setting
it didn't seem to do anything.  The answer is lurking in ?par:

##     This parameter is reset when a user coordinate system is set
##           up, for example by starting a new page or by calling
##           'plot.window' or setting 'par("usr")': 'n' is taken from
##           'par("lab")'.  It affects the default behaviour of subsequent
##           calls to 'axis' for sides 1 or 3.

 Thus, when barplot starts up and plots a new set of axes it RESETS
par("yaxp").  Thus

par(yaxp=...)); barplot(...)

doesn't work.

 However,

barplot(...,yaxp=...)  does work.

 It would actually be nice to have an axis style (xaxs,yaxs) that extended
the axis out beyond the range of the data until it found pretty labels that
extended beyond the data range -- for example, set the range according
to xaxs="r", find the pretty axis ticks, and then "add another tick" ...

 cheers
   Ben Bolker


From deepayan.sarkar at gmail.com  Thu Jul 26 21:53:16 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 26 Jul 2007 12:53:16 -0700
Subject: [R] How to auto-scale cex of y-axis labels in lattice dotplot?
In-Reply-To: <c968588d0707252012g3cd7cb19k3562e50e598c716c@mail.gmail.com>
References: <c968588d0707252012g3cd7cb19k3562e50e598c716c@mail.gmail.com>
Message-ID: <eb555e660707261253s5376beebsbf5b89ce07e737a6@mail.gmail.com>

On 7/25/07, Kevin Wright <kw.statr at gmail.com> wrote:
> When I create a dotplot in lattice, I frequently observe overplotting
> of the labels along the vertical axis.  On my screen, this illustrates
> overplotting of the letters:
>
> windows()
> reps=6
> dat=data.frame(let=rep(letters,each=reps), grp=rep(1:reps, 26),
>   y=runif(26*reps))
> dotplot(let~y|grp, dat)
>
> Is there a way to automatically scale the labels so that they are not
> over-plotted?

Not that I can think of.

> I currently do something like this:
> Calculate or guess the number of panel rows: NumPanelRows
> cexLab <- min(1, .9*par()$pin[2]/
>               (nlevels(dat$let)*NumPanelRows*strheight("A",units="in")))
> dotplot(..., scales=list(y=list(cex=cexLab))
>
> Is there an easier way?
>
> Is there a function that I can call which calculates the layout of the
> panels that will be used in the dotplot?

Not really. The eventual layout is calculated inside print.trellis as
follows (where 'x' is the "trellis" object being plotted):


    panel.layout <-
        compute.layout(x$layout, dim(x), skip = x$skip)

    [...]

    if (panel.layout[1] == 0)
    {
        ddim <- par("din")
        device.aspect <- ddim[2] / ddim[1]
        panel.aspect <- panel.height[[1]] / panel.width[[1]]

        plots.per.page <- panel.layout[2]
        m <- max (1, round(sqrt(panel.layout[2] * device.aspect /
panel.aspect)))
        n <- ceiling(plots.per.page/m)
        m <- ceiling(plots.per.page/n)
        panel.layout[1] <- n
        panel.layout[2] <- m
    }

-Deepayan


From mpg33 at drexel.edu  Thu Jul 26 22:03:25 2007
From: mpg33 at drexel.edu (Michael Gormley)
Date: Thu, 26 Jul 2007 16:03:25 -0400
Subject: [R] offset in coxph
Message-ID: <001d01c7cfc0$069afec0$0e161981@BIOINFORMATICS2>

The offset argument used in glm and other functions seems to have been 
removed from the argument list for coxph.  I am wondering if there is a 
reason for this and if there is a possible work-around in order to produce a 
cox-ph object without fitting coefficients?

Thanks,
Mike


From cvdamme at vub.ac.be  Thu Jul 26 22:32:01 2007
From: cvdamme at vub.ac.be (celine)
Date: Thu, 26 Jul 2007 13:32:01 -0700 (PDT)
Subject: [R] Creating a cross table out of a large dataset
Message-ID: <11818590.post@talk.nabble.com>


Dear all,

I want to make a cross table out of a data set which is 2 columns wide and
more than 150000 rows long. When I use the table() function I get an error
message

This is the code I have used:

>Dataset <- read.table("test.txt", header=TRUE, sep=",", na.strings="NA",
dec=".", strip.white=TRUE) 

> .T <-table(Dataset$K1,Dataset$K2) 

This is the error message I have received

>Error in vector("integer", length) : vector size specified is too large 
>In addition: Warning messages: 
>1: NAs introduced by coercion 
>2: NAs introduced by coercion 

Is it possible to make a cross table with the table() function on a large
dataset or should I consider using another function? I have had a look at
the ?table help file but I could find any information on the size of the
dataset.

Thanks very much in advance for any help:-)

Kind regards,
C?line.
-- 
View this message in context: http://www.nabble.com/Creating-a-cross-table-out-of-a-large-dataset-tf4153948.html#a11818590
Sent from the R help mailing list archive at Nabble.com.


From shitao at hotmail.com  Thu Jul 26 22:50:00 2007
From: shitao at hotmail.com (Tao Shi)
Date: Thu, 26 Jul 2007 20:50:00 +0000
Subject: [R] Creating windows binary R package (PowerArchiver vs. zip -r9X)
Message-ID: <BAY120-W72DCFAAC5F2E809F63AAEC7F20@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/b285110d/attachment.pl 

From marc_schwartz at comcast.net  Thu Jul 26 23:06:25 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Thu, 26 Jul 2007 16:06:25 -0500
Subject: [R] Creating a cross table out of a large dataset
In-Reply-To: <11818590.post@talk.nabble.com>
References: <11818590.post@talk.nabble.com>
Message-ID: <1185483985.3703.51.camel@Bellerophon.localdomain>

On Thu, 2007-07-26 at 13:32 -0700, celine wrote:
> Dear all,
> 
> I want to make a cross table out of a data set which is 2 columns wide and
> more than 150000 rows long. When I use the table() function I get an error
> message
> 
> This is the code I have used:
> 
> >Dataset <- read.table("test.txt", header=TRUE, sep=",", na.strings="NA",
> dec=".", strip.white=TRUE) 
> 
> > .T <-table(Dataset$K1,Dataset$K2) 
> 
> This is the error message I have received
> 
> >Error in vector("integer", length) : vector size specified is too large 
> >In addition: Warning messages: 
> >1: NAs introduced by coercion 
> >2: NAs introduced by coercion 
> 
> Is it possible to make a cross table with the table() function on a large
> dataset or should I consider using another function? I have had a look at
> the ?table help file but I could find any information on the size of the
> dataset.
> 
> Thanks very much in advance for any help:-)
> 
> Kind regards,
> C?line.

A wild guess here, but it sounds like your data does not likely contain
a relatively small set of repeated discrete entries.

Thus, your cross-tabulation results in a large number of combinations,
the number of which exceeds the largest representable integer in R,
which is:

> .Machine$integer.max
[1] 2147483647

or

> 2^31 - 1
[1] 2147483647


An R table is a two (or possibly more) dimension matrix with additional
class attributes.  A matrix is in turn, a vector with 'dim' attributes.
A vector is indexed using integers and thus is limited in size to the
above number.

If the above assumptions are correct, I am struggling to think of a
scenario where the visual representation of a cross-tabulation of your
data will be of value, but that may be just do to a severe lack of sleep
of late.

You might want to run:

 length(unique(Dataset$K1))

and 

  length(unique(Dataset$K2))

which will tell you how many unique values are in each of the two
vectors. That will begin to give you some idea as to what you are
dealing with.

HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Thu Jul 26 23:23:18 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jul 2007 22:23:18 +0100 (BST)
Subject: [R] R CMD check sh: line 1: make: command not found
In-Reply-To: <346AB00C-BE66-4925-A452-A14558B61BAC@mac.com>
References: <346AB00C-BE66-4925-A452-A14558B61BAC@mac.com>
Message-ID: <Pine.LNX.4.64.0707262221050.20683@gannet.stats.ox.ac.uk>

On Thu, 26 Jul 2007, David Peltier wrote:

> hello,
>
> I am using R 2.5.0 under OS X.
>
> I am having " sh: line 1: make: command not found" error message when
> I run " R CMD check " :
>
> Any help would be appreciated.

Well, that is easy: 'make' is missing.  It should be there in the OS, so 
you need to talk to your OS support for help in finding/installing it.

BTW, the list for MacOS-specific questions if r-sig-mac.

>
> R CMD check backtest
>
> * checking for working latex ... OK
> * using log directory '/backtest/trunk/backtest.Rcheck'
> * using R version 2.5.0 (2007-04-23)
> * checking for file 'backtest/DESCRIPTION' ... OK
> * checking extension type ... Package
> * this is package 'backtest' version '0.2-0'
> * checking package dependencies ... OK
> * checking if this is a source package ... OK
> * checking whether package 'backtest' can be installed ... OK
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking for sufficient/correct file permissions ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking top-level files ... OK
> * checking index information ... OK
> * checking package subdirectories ... OK
> * checking R files for non-ASCII characters ... OK
> * checking R files for syntax errors ... OK
> * checking whether the package can be loaded ... OK
> * checking whether the package can be loaded with stated
> dependencies ... OK
> * checking whether the name space can be loaded with stated
> dependencies ... OK
> * checking for unstated dependencies in R code ... OK
> * checking S3 generic/method consistency ... OK
> * checking replacement functions ... OK
> * checking foreign function calls ... OK
> * checking R code for possible problems ... OK
> * checking Rd files ... OK
> * checking Rd cross-references ... OK
> * checking for missing documentation entries ... OK
> * checking for code/documentation mismatches ... OK
> * checking Rd \usage sections ... OK
> * checking data for non-ASCII characters ... OK
> * creating backtest-Ex.R ... OK
> * checking examples ... OK
> * checking tests ...
> sh: line 1: make: command not found
> ERROR
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jul 26 23:34:38 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Jul 2007 22:34:38 +0100 (BST)
Subject: [R] offset in coxph
In-Reply-To: <001d01c7cfc0$069afec0$0e161981@BIOINFORMATICS2>
References: <001d01c7cfc0$069afec0$0e161981@BIOINFORMATICS2>
Message-ID: <Pine.LNX.4.64.0707262218210.20683@gannet.stats.ox.ac.uk>

Removed?  That it was ever there is not my recollection and seems very 
unlikely given that survival is ported from S where glm() does not have 
it,

As far as I know it has only ever been in glm() and lm() in R: the way 
which is described in the White Book is to use the offset() function, and 
this is preferred (it works correctly for prediction, for example).  The 
function form is supported for coxph, and used in the test suite.

Please forget you ever knew 'offset' could be an argument, and use 
offset() in your formulae instead.


On Thu, 26 Jul 2007, Michael Gormley wrote:

> The offset argument used in glm and other functions seems to have been
> removed from the argument list for coxph.  I am wondering if there is a
> reason for this and if there is a possible work-around in order to produce a
> cox-ph object without fitting coefficients?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From zhongmiao at gmail.com  Fri Jul 27 01:15:56 2007
From: zhongmiao at gmail.com (zhongmiao wang)
Date: Thu, 26 Jul 2007 18:15:56 -0500
Subject: [R] Survival analysis with 60% random censoring
Message-ID: <1def27350707261615q5fb171f2r66ecd91cc8ac002c@mail.gmail.com>

Hello,
My study is to predict the likelihood an insurance policy holder will
not renew his policy in the coming expiration date.
My data has about 60% censoring and they are random, because customers
buy insurance at different time, however, the study has to be
terminated on a single date.  Any suggestion or reference is greatly
appreciated. Thanks in advance.

Best Regards
Zhongmiao Wang


From bensaylor at fastmail.fm  Fri Jul 27 02:38:34 2007
From: bensaylor at fastmail.fm (Ben Saylor)
Date: Thu, 26 Jul 2007 16:38:34 -0800
Subject: [R] reading stata files: preserving values of variables converted
 to factors
Message-ID: <46A93E8A.7080906@fastmail.fm>

Hi,

I am a Stata user new to R.  I am using read.dta to read a Stata file 
that has variables with value labels.  read.dta converts them to 
factors, but seems to recode them with values from 1 to <number of 
factor levels> (looking at the output of unclass(<varname>)), so the 
original numerical values are lost.  Using convert.factors=FALSE 
preserves the values, but seems to discard the labels.  Is it possible 
to get these variables into R while preserving both the values and the 
labels?

Thanks,
Ben


From Manuel.A.Morales at williams.edu  Fri Jul 27 02:39:28 2007
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Thu, 26 Jul 2007 20:39:28 -0400
Subject: [R] Convert string to list?
Message-ID: <1185496768.7987.3.camel@mrubra.localdomain>

Let's say I have the following string:

str <- "P = 0.0, T = 0.0, Q = 0.0"

I'd like to find a function that generates the following object from
'str'.

list(P = 0.0, T = 0.0, Q = 0.0)

Thanks!

-- 
http://mutualism.williams.edu


From r.darnell at uq.edu.au  Fri Jul 27 02:51:55 2007
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Fri, 27 Jul 2007 10:51:55 +1000
Subject: [R] Convert string to list?
In-Reply-To: <1185496768.7987.3.camel@mrubra.localdomain>
References: <1185496768.7987.3.camel@mrubra.localdomain>
Message-ID: <E4178EE0463C7D40AF637C4DDAC8030E2519A7@UQEXMB2.soe.uq.edu.au>

Is this what your want?


as.vector(unlist(strsplit(str,",")),mode="list")


Ross Darnell

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Manuel Morales
Sent: Friday, 27 July 2007 10:39 AM
To: r-help
Subject: [R] Convert string to list?

Let's say I have the following string:

str <- "P = 0.0, T = 0.0, Q = 0.0"

I'd like to find a function that generates the following object from
'str'.

list(P = 0.0, T = 0.0, Q = 0.0)

Thanks!

-- 
http://mutualism.williams.edu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From labone at gforcecable.com  Fri Jul 27 03:13:32 2007
From: labone at gforcecable.com (Tom La Bone)
Date: Thu, 26 Jul 2007 21:13:32 -0400
Subject: [R] Minitab Parametric Distribution Analysis in R
In-Reply-To: <Pine.LNX.4.43.0707250903090.9786@hymn08.u.washington.edu>
Message-ID: <000c01c7cfeb$59581f00$6401a8c0@Boozoo>

After a bit of coaching I found what I was looking for: the fitdistr()
function in the MASS package. It appears to be a bit easier to use than
mle() for my application. Thanks all.

Tom


-----Original Message-----
From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
Sent: Wednesday, July 25, 2007 12:03 PM
To: Tom La Bone
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Minitab Parametric Distribution Analysis in R


The survival package (survreg() function) will fit quite a few parametric
models under censoring.

If you aren't doing regression, but just one-sample fitting, you can feed
the appropriate censored or truncated likelihood to mle() in the stat4
package.

Both packages should be part of your R distribution.

    -thomas



On Wed, 25 Jul 2007, Tom La Bone wrote:

> Minitab can perform a "Parametric Distribution Analysis - Arbitrary
> Censoring" with one of eight distributions (e.g., weibull), giving the
> maximum likelihood estimates of the parameters in the distribution for a
> given dataset. Does R have a package that provides equivalent
functionality?
> Thanks for any advice you can offer.
>
>
>
> Tom La Bone
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle


From jholtman at gmail.com  Fri Jul 27 03:20:20 2007
From: jholtman at gmail.com (jim holtman)
Date: Thu, 26 Jul 2007 21:20:20 -0400
Subject: [R] Convert string to list?
In-Reply-To: <1185496768.7987.3.camel@mrubra.localdomain>
References: <1185496768.7987.3.camel@mrubra.localdomain>
Message-ID: <644e1f320707261820x3cf0e1deq3440b072a22f36ec@mail.gmail.com>

Is this what you want:

> str <- "P = 0.0, T = 0.0, Q = 0.0"
> x <- eval(parse(text=paste('list(', str, ')')))
> str(x)
List of 3
 $ P: num 0
 $ T: num 0
 $ Q: num 0
>


On 7/26/07, Manuel Morales <Manuel.A.Morales at williams.edu> wrote:
> Let's say I have the following string:
>
> str <- "P = 0.0, T = 0.0, Q = 0.0"
>
> I'd like to find a function that generates the following object from
> 'str'.
>
> list(P = 0.0, T = 0.0, Q = 0.0)
>
> Thanks!
>
> --
> http://mutualism.williams.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From r.darnell at uq.edu.au  Fri Jul 27 03:40:36 2007
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Fri, 27 Jul 2007 11:40:36 +1000
Subject: [R] Convert string to list?
In-Reply-To: <644e1f320707261820x3cf0e1deq3440b072a22f36ec@mail.gmail.com>
References: <1185496768.7987.3.camel@mrubra.localdomain>
	<644e1f320707261820x3cf0e1deq3440b072a22f36ec@mail.gmail.com>
Message-ID: <E4178EE0463C7D40AF637C4DDAC8030E2519F2@UQEXMB2.soe.uq.edu.au>

Manuel

Jim's may be what you want-- a list of numerics with names P, T and Q or
a list of character strings?

> str <- "P = 0.0, T = 0.0, Q = 0.0"

> str(as.vector(unlist(strsplit(str,",")),mode="list"))
List of 3
 $ : chr "P = 0.0"
 $ : chr " T = 0.0"
 $ : chr " Q = 0.0"



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of jim holtman
Sent: Friday, 27 July 2007 11:20 AM
To: Manuel Morales
Cc: r-help
Subject: Re: [R] Convert string to list?

Is this what you want:

> str <- "P = 0.0, T = 0.0, Q = 0.0"
> x <- eval(parse(text=paste('list(', str, ')')))
> str(x)
List of 3
 $ P: num 0
 $ T: num 0
 $ Q: num 0
>


On 7/26/07, Manuel Morales <Manuel.A.Morales at williams.edu> wrote:
> Let's say I have the following string:
>
> str <- "P = 0.0, T = 0.0, Q = 0.0"
>
> I'd like to find a function that generates the following object from
> 'str'.
>
> list(P = 0.0, T = 0.0, Q = 0.0)
>
> Thanks!
>
> --
> http://mutualism.williams.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Fri Jul 27 05:08:16 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 26 Jul 2007 23:08:16 -0400
Subject: [R] Convert string to list?
In-Reply-To: <1185496768.7987.3.camel@mrubra.localdomain>
References: <1185496768.7987.3.camel@mrubra.localdomain>
Message-ID: <971536df0707262008u46110d79y7e19e8bc66207692@mail.gmail.com>

Try this.  It pastes "list(" onto the front and ")" onto the end giving
"list( P = 0.0, T = 0.0, Q = 0.0 )"
and then parses and evaluates that as an R expression.

Str <- "P = 0.0, T = 0.0, Q = 0.0"
eval(parse(text = paste("list(", Str, ")")))


On 7/26/07, Manuel Morales <Manuel.A.Morales at williams.edu> wrote:
> Let's say I have the following string:
>
> str <- "P = 0.0, T = 0.0, Q = 0.0"
>
> I'd like to find a function that generates the following object from
> 'str'.
>
> list(P = 0.0, T = 0.0, Q = 0.0)
>
> Thanks!
>
> --
> http://mutualism.williams.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From filams0704 at yahoo.com  Fri Jul 27 06:24:28 2007
From: filams0704 at yahoo.com (filame uyaco)
Date: Thu, 26 Jul 2007 21:24:28 -0700 (PDT)
Subject: [R] R codes for g-and-h distribution
Message-ID: <687475.90110.qm@web54005.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070726/78006987/attachment.pl 

From ligges at statistik.uni-dortmund.de  Fri Jul 27 09:16:49 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 Jul 2007 09:16:49 +0200
Subject: [R] Creating windows binary R package (PowerArchiver vs. zip
 -r9X)
In-Reply-To: <BAY120-W72DCFAAC5F2E809F63AAEC7F20@phx.gbl>
References: <BAY120-W72DCFAAC5F2E809F63AAEC7F20@phx.gbl>
Message-ID: <46A99BE1.4000100@statistik.uni-dortmund.de>



Tao Shi wrote:
> Hi list,I apologize if you see funny fonts, b/c I'm using the new
> Windows Live Hotmail and don't know how to turn off the "rich text"
> mode.....I have successfully built and installed a R package in
> windowsXP for R-2.5.1.  But when I tried to create a .zip file so I
> can use "Packages/install package(s) from local .zip files..." to
> install it, it seems R only recognizes the .zip file created by "zip
> -r9X" not by PowerArchiver.  Do you know why?  I vaguely remember I
> used WinZip before and it worked fine.The two threads I found on
> R-help and R-devel help me a lot, but don't really answer my
> question.http://tolstoy.newcastle.edu.au/R/help/06/06/29587.htmlhttp://tolstoy.newcastle.edu.au/R/devel/05/12/3336.htmlThanks,...Tao


In order to provide a Windows binary package, type:

R CMD INSTALL --build PackageName_vers.tar.gz
and the zip file will be generated by R in the correct way.

Uwe Ligges


From Alexander.Herr at csiro.au  Fri Jul 27 09:32:02 2007
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Fri, 27 Jul 2007 17:32:02 +1000
Subject: [R] cluster - identify variables contributions to clusters of cases
Message-ID: <80C7911E901E7E4797B3F88D106CB25D1A9620@exqld2-bne.nexus.csiro.au>

Hi List,

How would I go about best identifying the variables contributing most to
the specific clusters?
eg using either aglomerative or partitioning methods, but with mixed
variables (ie including categorical) eg:

factor(as.integer(runif(min=1, max=5,nrow(USArrests))))->t1
as.data.frame(cbind(USArrests,categ=t1))->test
agnes(test,metric="gower", method="ward")->test1
cutree(test1,k=5)->clust


?where to go from here?

Any hints appreciated
Thanx
Herry


From a-6b at abroaddesign.com  Fri Jul 27 13:33:13 2007
From: a-6b at abroaddesign.com (Owen Blair)
Date: Fri, 27 Jul 2007 08:33:13 -0300
Subject: [R] Hi!
Message-ID: <01c7d028$c5a2d280$463ded53@a-6b>

Hello! I am bored this afternoon. I am nice girl that would like to chat with you. Email me at oyd at linkmailmessage.info only, because I am writing not from my personal email. Would you mind if I share some of my pictures with you?


From ramasamy at cancer.org.uk  Fri Jul 27 12:11:23 2007
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 27 Jul 2007 11:11:23 +0100
Subject: [R] princomp error
In-Reply-To: <2FC987BC0B90B24786CAF43DD3F5719CC7DA40@CRU105.cahe.ad.wsu.edu>
References: <2FC987BC0B90B24786CAF43DD3F5719CC7DA40@CRU105.cahe.ad.wsu.edu>
Message-ID: <46A9C4CB.3030109@cancer.org.uk>

You probably got some missing or undefined values.

Either eyeball the data or use sum(is.na(x)), sum(is.nan(x)), 
sum(is.infinite(x)) to find out if you have such data. You may want to 
use which() to find out where they are.

Regards, Adai


Bricklemyer, Ross S wrote:
> I am attempting to run principal components analysis on a dataset of
> spectral reflectance (6 decimal places).  I imported the data using
> read.table and there are both column and row headers.  When I run
> princomp I receive the following error:
> 
>  
> 
> Error in cov.wt(z) : 'x' must contain finite values only
> 
>  
> 
> Where am I going wrong?
> 
>  
> 
> Ross
> 
>  
> 
> *******************************************************************
> Ross Bricklemyer
> Dept. of Crop and Soil Sciences
> Washington State University
> 291D Johnson Hall
> PO Box 646420
> Pullman, WA 99164-6420
> Work: 509.335.3661
> Cell/Home: 406.570.8576
> Fax: 509.335.8674
> Email: rsb at wsu.edu
> 
> 
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>


From kamauallan at yahoo.com  Fri Jul 27 13:00:21 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Fri, 27 Jul 2007 04:00:21 -0700 (PDT)
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
Message-ID: <705029.77508.qm@web53512.mail.re2.yahoo.com>

Hi All,
I am having difficulties finding a way to find a substitute to the command "names(v.val$PR14)" so that I could generate the command on the fly for all PR14 to PR200 (please see the previous discussion below to understand what the object x.val contains) . I have tried the following 

>results=()#character()
>myVariableNames=names(x.val)
>results[length(myVariableNames)]<-NA

>for (i in myVariableNames){
+    results[i]<-names(x.val$i)    # this does not work it returns a NULL (how can i convert this to x.val$"somevalue" ? )
>}

Allan.


----- Original Message ----
From: Allan Kamau <kamauallan at yahoo.com>
To: r-help at stat.math.ethz.ch
Sent: Thursday, July 26, 2007 10:03:17 AM
Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.

Thanks so much Jim, Andaikalavan, Gabor and others for the help and suggestions.
The solution will result in a matrix containing nested matrices to enable each variable name, each variables distinct value and the count of the distinct value to be accessible individually.
The main matrix will contain the variable names, the first level nested matrices will consist of the variables unique values, and each such variable entry will contain a one element vector to contain the count or occurrence frequency.
This matrix can now be used in comparing other similar datasets for variable values and their frequencies.

Building on the input received so far, a probable solution in building the matrix will include the following.


1)I reading the csv file (containing column headers)
>my_data=read.table("<path/to/my/data.csv>",header=TRUE,sep=",",dec=".",fill=TRUE)

2)I group the values in each variable producing an occurrence count(frequency)
>x.val<-apply(my_data,2,table)

3)I obtain a vector of the names of the variables in the table
>names(x.val)

4)Now I make use of the names (obtained in step 3) to obtain a vector of distinct values in a given variable (in the example below the variable name is $PR14)
>names(v.val$PR14)

5)I obtain a vector (with one element) of the frequency of a value obtained from the step above (in our example the value is "V")
>as.vector(x.val$PR14["V"])

Todo:
Now I will need to place the steps above in a script (consisting of loops) to build the matrix, step 4 and 5 seem tricky to do programatically.

Allan.


----- Original Message ----
From: jim holtman <jholtman at gmail.com>
To: Allan Kamau <kamauallan at yahoo.com>
Cc: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>; r-help at stat.math.ethz.ch
Sent: Wednesday, July 25, 2007 1:50:55 PM
Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.

Also if you want to access the individual values, you can just leave
it as a list:

> x.val <- apply(x, 2, table)
> # access each value
> x.val$PR14["V"]
V
8



On 7/25/07, Allan Kamau <kamauallan at yahoo.com> wrote:
> A subset of the data looks as follows
>
> > df[1:10,14:20]
>   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> 1     V    T    I    K    V    G    D
> 2     V    S    I    K    V    G    G
> 3     V    T    I    R    V    G    G
> 4     V    S    I    K    I    G    G
> 5     V    S    I    K    V    G    G
> 6     V    S    I    R    V    G    G
> 7     V    T    I    K    I    G    G
> 8     V    S    I    K    V    E    G
> 9     V    S    I    K    V    G    G
> 10    V    S    I    K    V    G    G
>
> The result I would like is as follows
>
> PR10        PR11          PR12   ...
> [V:10]    [S:7,T:3]    [I:10]
>
> The result can be in a matrix or a vector and each variablename, value and frequency should be accessible so as to be used for comparisons with another dataset later.
> The frequency can be a count or a percentage.
>
>
> Allan.
>
>
> ----- Original Message ----
> From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> To: Allan Kamau <kamauallan at yahoo.com>
> Cc: r-help at stat.math.ethz.ch
> Sent: Tuesday, July 24, 2007 10:21:51 PM
> Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
>
> The name of the table should give you the "value". And if you have a
> matrix, you just need to convert it into a vector first.
>
>  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ], nc=3 )
>  > m
>      [,1] [,2] [,3]
> [1,] "A"  "C"  "B"
> [2,] "B"  "D"  "C"
> [3,] "C"  "E"  "D"
>  > tb <- table( as.vector(m) )
>  > tb
>
> A B C D E
> 1 2 3 2 1
>  > paste( names(tb), ":", tb, sep="" )
> [1] "A:1" "B:2" "C:3" "D:2" "E:1"
>
> If this is not what you want, then please give a simple example.
>
> Regards, Adai
>
>
>
> Allan Kamau wrote:
> > Hi all,
> > If the question below as been answered before I
> > apologize for the posting.
> > I would like to get the frequencies of occurrence of
> > all values in a given variable in a multivariate
> > dataset. In short for each variable (or field) a
> > summary of values contained with in a value:frequency
> > pair, there can be many such pairs for a given
> > variable. I would like to do the same for several such
> > variables.
> > I have used table() but am unable to extract the
> > individual value and frequency values.
> > Please advise.
> >
> > Allan.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Fri Jul 27 14:13:12 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 27 Jul 2007 08:13:12 -0400
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
In-Reply-To: <705029.77508.qm@web53512.mail.re2.yahoo.com>
References: <705029.77508.qm@web53512.mail.re2.yahoo.com>
Message-ID: <644e1f320707270513h5adabb8au3a870079b4bfbf80@mail.gmail.com>

results=()#character()
myVariableNames=names(x.val)
results[length(myVariableNames)]<-NA

for (i in myVariableNames){
    results[i]<-names(x.val[[i]])    # this does not work it returns a
NULL (how can i convert this to x.val$"somevalue" ? )
}



On 7/27/07, Allan Kamau <kamauallan at yahoo.com> wrote:
> Hi All,
> I am having difficulties finding a way to find a substitute to the command "names(v.val$PR14)" so that I could generate the command on the fly for all PR14 to PR200 (please see the previous discussion below to understand what the object x.val contains) . I have tried the following
>
> >results=()#character()
> >myVariableNames=names(x.val)
> >results[length(myVariableNames)]<-NA
>
> >for as.vector(unlist(strsplit(str,",")),mode="list")
> +    results[i]<-names(x.val$i)    # this does not work it returns a NULL (how can i convert this to x.val$"somevalue" ? )
> >}
>
> Allan.
>
>
> ----- Original Message ----
> From: Allan Kamau <kamauallan at yahoo.com>
> To: r-help at stat.math.ethz.ch
> Sent: Thursday, July 26, 2007 10:03:17 AM
> Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
>
> Thanks so much Jim, Andaikalavan, Gabor and others for the help and suggestions.
> The solution will result in a matrix containing nested matrices to enable each variable name, each variables distinct value and the count of the distinct value to be accessible individually.
> The main matrix will contain the variable names, the first level nested matrices will consist of the variables unique values, and each such variable entry will contain a one element vector to contain the count or occurrence frequency.
> This matrix can now be used in comparing other similar datasets for variable values and their frequencies.
>
> Building on the input received so far, a probable solution in building the matrix will include the following.
>
>
> 1)I reading the csv file (containing column headers)
> >my_data=read.table("<path/to/my/data.csv>",header=TRUE,sep=",",dec=".",fill=TRUE)
>
> 2)I group the values in each variable producing an occurrence count(frequency)
> >x.val<-apply(my_data,2,table)
>
> 3)I obtain a vector of the names of the variables in the table
> >names(x.val)
>
> 4)Now I make use of the names (obtained in step 3) to obtain a vector of distinct values in a given variable (in the example below the variable name is $PR14)
> >names(v.val$PR14)
>
> 5)I obtain a vector (with one element) of the frequency of a value obtained from the step above (in our example the value is "V")
> >as.vector(x.val$PR14["V"])
>
> Todo:
> Now I will need to place the steps above in a script (consisting of loops) to build the matrix, step 4 and 5 seem tricky to do programatically.
>
> Allan.
>
>
> ----- Original Message ----
> From: jim holtman <jholtman at gmail.com>
> To: Allan Kamau <kamauallan at yahoo.com>
> Cc: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>; r-help at stat.math.ethz.ch
> Sent: Wednesday, July 25, 2007 1:50:55 PM
> Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
>
> Also if you want to access the individual values, you can just leave
> it as a list:
>
> > x.val <- apply(x, 2, table)
> > # access each value
> > x.val$PR14["V"]
> V
> 8
>
>
>
> On 7/25/07, Allan Kamau <kamauallan at yahoo.com> wrote:
> > A subset of the data looks as follows
> >
> > > df[1:10,14:20]
> >   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> > 1     V    T    I    K    V    G    D
> > 2     V    S    I    K    V    G    G
> > 3     V    T    I    R    V    G    G
> > 4     V    S    I    K    I    G    G
> > 5     V    S    I    K    V    G    G
> > 6     V    S    I    R    V    G    G
> > 7     V    T    I    K    I    G    G
> > 8     V    S    I    K    V    E    G
> > 9     V    S    I    K    V    G    G
> > 10    V    S    I    K    V    G    G
> >
> > The result I would like is as follows
> >
> > PR10        PR11          PR12   ...
> > [V:10]    [S:7,T:3]    [I:10]
> >
> > The result can be in a matrix or a vector and each variablename, value and frequency should be accessible so as to be used for comparisons with another dataset later.
> > The frequency can be a count or a percentage.
> >
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> > To: Allan Kamau <kamauallan at yahoo.com>
> > Cc: r-help at stat.math.ethz.ch
> > Sent: Tuesday, July 24, 2007 10:21:51 PM
> > Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
> >
> > The name of the table should give you the "value". And if you have a
> > matrix, you just need to convert it into a vector first.
> >
> >  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ], nc=3 )
> >  > m
> >      [,1] [,2] [,3]
> > [1,] "A"  "C"  "B"
> > [2,] "B"  "D"  "C"
> > [3,] "C"  "E"  "D"
> >  > tb <- table( as.vector(m) )
> >  > tb
> >
> > A B C D E
> > 1 2 3 2 1
> >  > paste( names(tb), ":", tb, sep="" )
> > [1] "A:1" "B:2" "C:3" "D:2" "E:1"
> >
> > If this is not what you want, then please give a simple example.
> >
> > Regards, Adai
> >
> >
> >
> > Allan Kamau wrote:
> > > Hi all,
> > > If the question below as been answered before I
> > > apologize for the posting.
> > > I would like to get the frequencies of occurrence of
> > > all values in a given variable in a multivariate
> > > dataset. In short for each variable (or field) a
> > > summary of values contained with in a value:frequency
> > > pair, there can be many such pairs for a given
> > > variable. I would like to do the same for several such
> > > variables.
> > > I have used table() but am unable to extract the
> > > individual value and frequency values.
> > > Please advise.
> > >
> > > Allan.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
> --
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
>
> What is the problem you are trying to solve?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From maitra at iastate.edu  Fri Jul 27 14:23:20 2007
From: maitra at iastate.edu (Ranjan Maitra)
Date: Fri, 27 Jul 2007 07:23:20 -0500
Subject: [R] cluster - identify variables contributions to clusters of
 cases
In-Reply-To: <80C7911E901E7E4797B3F88D106CB25D1A9620@exqld2-bne.nexus.csiro.au>
References: <80C7911E901E7E4797B3F88D106CB25D1A9620@exqld2-bne.nexus.csiro.au>
Message-ID: <20070727072320.3ff2bf2e@triveni.stat.iastate.edu>

I am not sure this might help, but you are perhaps lookign at variable
selection. There is a 2006 JASA paper by Raftery and Dean which may
help.

Many thanks,
Ranjan

On Fri, 27 Jul 2007 17:32:02 +1000 <Alexander.Herr at csiro.au> wrote:

> Hi List,
> 
> How would I go about best identifying the variables contributing most
> to the specific clusters?
> eg using either aglomerative or partitioning methods, but with mixed
> variables (ie including categorical) eg:
> 
> factor(as.integer(runif(min=1, max=5,nrow(USArrests))))->t1
> as.data.frame(cbind(USArrests,categ=t1))->test
> agnes(test,metric="gower", method="ward")->test1
> cutree(test1,k=5)->clust
> 
> 
> ?where to go from here?
> 
> Any hints appreciated
> Thanx
> Herry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.
>


From frstefani at rncan.gc.ca  Fri Jul 27 14:40:04 2007
From: frstefani at rncan.gc.ca (Stefani, Franck)
Date: Fri, 27 Jul 2007 08:40:04 -0400
Subject: [R] heatmap and phylogram / dendogram ploting problem
Message-ID: <8C16DEDEF6DE8B418A724B298297A53001B2FF6E@S0-OTT-X3.nrn.nrcan.gc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/ade77d48/attachment.pl 

From Luisr at frs.fo  Fri Jul 27 15:07:18 2007
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Fri, 27 Jul 2007 14:07:18 +0100
Subject: [R] exporting character vector to text files
Message-ID: <s6a9fc1f.074@ffdata.setur.fo>

R-help,

I have a character vector whose elements are the names of matrixes.
Something like this:

> test <- ls(pattern="Oki")
 [1] "aaOki"    "aOki"     "bOki"     "c1Oki"    "c2Oki"    "c3Oki"   
"cOki"     "dOki"     "eOki"     "fOki"     "gprsOki"  "hOki"     "iOki"
   
[14] "jOki"     "kOki"     "lOki"     "mOki"   ......................

An example:

> aaOki
              x        y
 [1,] -6.333333 61.41667
 [2,] -4.833333 61.41667
 [3,] -4.833333 61.25000
 [4,] -5.000000 61.25000
 [5,] -5.000000 61.16667
 [6,] -5.166667 61.16667
 [7,] -5.166667 61.00000
 [8,] -5.333333 61.00000
 [9,] -5.333333 60.91667
[10,] -5.500000 60.91667
[11,] -5.500000 60.83333
[12,] -5.666667 60.83333
[13,] -5.666667 60.75000
[14,] -5.833333 60.75000
[15,] -5.833333 60.66667
[16,] -6.000000 60.66667
[17,] -6.000000 60.50000
[18,] -6.166667 60.50000
[19,] -6.166667 61.00000
[20,] -6.333333 61.00000
[21,] -6.333333 61.41667

..
..

..

What I want to do is to export these objects to text files
by doing:

for(i in test)
write.table(eval(i),file=paste(i),row.names=FALSE,sep="\t")

but it doesn't work.

Thanks in advance


> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          5.1                         
year           2007                        
month          06                          
day            27                          
svn rev        42083                       
language       R                           
version.string R version 2.5.1 (2007-06-27)


From wl2776 at gmail.com  Fri Jul 27 15:36:25 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Fri, 27 Jul 2007 06:36:25 -0700 (PDT)
Subject: [R] exporting character vector to text files
In-Reply-To: <s6a9fc1f.074@ffdata.setur.fo>
References: <s6a9fc1f.074@ffdata.setur.fo>
Message-ID: <11829294.post@talk.nabble.com>


Use get() instead of eval().
And, probably, some arguments are missing in call to paste(). 
Maybe, extension?

That is, 
for(i in test) 
  write.table(get(i),file=paste(i,"txt",sep="."),row.names=FALSE,sep="\t")

If you want file names matching exactly names of your matrices, and being
without extension,
you don't need to call paste() at all:

for(i in test) 
  write.table(get(i),file=i,row.names=FALSE,sep="\t")


Luis Ridao Cruz wrote:
> 
> R-help,
> 
> I have a character vector whose elements are the names of matrixes.
> Something like this:
> 
>> test <- ls(pattern="Oki")
>  [1] "aaOki"    "aOki"     "bOki"     "c1Oki"    "c2Oki"    "c3Oki"   
> "cOki"     "dOki"     "eOki"     "fOki"     "gprsOki"  "hOki"     "iOki"
>    
> [14] "jOki"     "kOki"     "lOki"     "mOki"   ......................
> 
> An example:
> 
>> aaOki
>               x        y
>  [1,] -6.333333 61.41667
>  [2,] -4.833333 61.41667
>  [3,] -4.833333 61.25000
>  [4,] -5.000000 61.25000
>  [5,] -5.000000 61.16667
>  [6,] -5.166667 61.16667
>  [7,] -5.166667 61.00000
>  [8,] -5.333333 61.00000
>  [9,] -5.333333 60.91667
> [10,] -5.500000 60.91667
> [11,] -5.500000 60.83333
> [12,] -5.666667 60.83333
> [13,] -5.666667 60.75000
> [14,] -5.833333 60.75000
> [15,] -5.833333 60.66667
> [16,] -6.000000 60.66667
> [17,] -6.000000 60.50000
> [18,] -6.166667 60.50000
> [19,] -6.166667 61.00000
> [20,] -6.333333 61.00000
> [21,] -6.333333 61.41667
> 
> ..
> ..
> 
> ..
> 
> What I want to do is to export these objects to text files
> by doing:
> 
> for(i in test)
> write.table(eval(i),file=paste(i),row.names=FALSE,sep="\t")
> 
> but it doesn't work.
> 

-- 
View this message in context: http://www.nabble.com/exporting-character-vector-to-text-files-tf4157676.html#a11829294
Sent from the R help mailing list archive at Nabble.com.


From mnair at iusb.edu  Fri Jul 27 15:45:26 2007
From: mnair at iusb.edu (Nair, Murlidharan T)
Date: Fri, 27 Jul 2007 09:45:26 -0400
Subject: [R] manipulating arrays
Message-ID: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/69573e86/attachment.pl 

From wwwhsd at gmail.com  Fri Jul 27 16:12:34 2007
From: wwwhsd at gmail.com (Henrique Dallazuanna)
Date: Fri, 27 Jul 2007 11:12:34 -0300
Subject: [R] manipulating arrays
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/7d4c9b58/attachment.pl 

From guo.dong99 at gmail.com  Fri Jul 27 16:45:15 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Fri, 27 Jul 2007 16:45:15 +0200
Subject: [R] plot
Message-ID: <989527a20707270745h541fed48kb286f9e408a4c522@mail.gmail.com>

Greetings to the group,

I would like to know if some one could help me with plot 3-d column
graph of a matrix (3-d column graph in Excel).

Many thanks in advance.

Regards,
Dong


From fn211 at cam.ac.uk  Fri Jul 27 16:47:18 2007
From: fn211 at cam.ac.uk (Florian Nigsch)
Date: Fri, 27 Jul 2007 15:47:18 +0100
Subject: [R] Large dataset + randomForest
In-Reply-To: <644e1f320707270517m4c64336bs527c921b19230d57@mail.gmail.com>
References: <FA5AA232-978B-48F6-86F8-0B467B9F2B58@cam.ac.uk>
	<644e1f320707261233j560bd366qbdec08ba0fe61b4e@mail.gmail.com>
	<BCB61808-4D21-4E3A-860C-B786304D9B56@cam.ac.uk>
	<644e1f320707270517m4c64336bs527c921b19230d57@mail.gmail.com>
Message-ID: <0AECBA65-4237-4E0D-B31F-BD1D964C58D3@cam.ac.uk>

I compiled the newest R version on a Redhat Linux (uname -a =  
Linux .cam.ac.uk 2.4.21-50.ELsmp #1 SMP Tue May 8 17:18:29 EDT 2007  
i686 i686 i386 GNU/Linux) with 4GB of physical memory. The step when  
the whole script crashed is within the randomForest() routine, I do  
know that because I want to time it thus I have it inside a  
system.time() call. This function exits with the error I posted earlier:

 > rf <- randomForest(V1 ~ ., data=df[trainindices,], do.trace=5)
Error: cannot allocate vector of size 313.0 Mb

When calling gc() directly before I call randomForest() and after I  
get this:

 > gc()
            used  (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb)
Ncells   255416   6.9     899071  24.1    16800.0    818163   21.9
Vcells 17874469 136.4   90854072 693.2     4000.1 269266598 2054.4
 > rf <- randomForest(V1 ~ ., data=df, subset=trainindices, do.trace=5)
Error: cannot allocate vector of size 626.1 Mb
 > gc()
            used  (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb)
Ncells   255441   6.9     899071  24.1    16800.0    818163   21.9
Vcells 17874541 136.4  112037674 854.8     4000.1 269266598 2054.4
 >

So the only real difference is in the "gc trigger" and the "(Mb)"  
column next to it. By the way, I am not running it in GUI mode


On 27 Jul 2007, at 13:17, jim holtman wrote:

> At the max, you had 2GB of memory being used.  What operating system
> are you running on and how much physical memory do you have on your
> system?  For windows, there are parameters on the command line to
> start RGUI that let you define how much memory can be used.  I am not
> sure of Linus/UNIX.  So you are probably hitting the 2GB max and then
> you don't have any more physical memory available.  If the computation
> is a long script, you might put some 'gc()' statements in the code to
> see what section is using the most memory.
>
> Your problem might have to be broken into parts to run.
>
> On 7/27/07, Florian Nigsch <fn211 at cam.ac.uk> wrote:
>> Hi Jim,
>>
>> Here is the output of gc() of the same session of R (that I have
>> still running...)
>>
>>> gc()
>>            used  (Mb) gc trigger  (Mb) limit (Mb)  max used   (Mb)
>> Ncells   255416   6.9     899071  24.1    16800.0    818163   21.9
>> Vcells 17874469 136.4  113567591 866.5     4000.1 269266598 2054.4
>>
>> By increasing the limit of vcells and ncells to 1GB (if that is
>> possible?!), would that perhaps solve my problem?
>>
>> Cheers,
>>
>> Florian


From fn211 at cam.ac.uk  Fri Jul 27 17:03:43 2007
From: fn211 at cam.ac.uk (Florian Nigsch)
Date: Fri, 27 Jul 2007 16:03:43 +0100
Subject: [R] Large dataset + randomForest
In-Reply-To: <71257D09F114DA4A8E134DEAC70F25D3090A7977@groamrexm03.amer.pfizer.com>
References: <71257D09F114DA4A8E134DEAC70F25D3090A7977@groamrexm03.amer.pfizer.com>
Message-ID: <BECA5182-0189-4993-BE9B-6959DD3D41DA@cam.ac.uk>

Thanks Max,

It looks as though that did actually solve the problem! It is a bit  
of a mystery to me because I think that the 151x150 matrix can't be  
that big, unless its elements are in turn huge datastructures. (?)

I am now calling randomForest() like this:
rf <- randomForest(x=df[trainindices,-1],y=df[trainindices,1],xtest=df 
[testindices,-1],ytest=df[testindices,1], do.trace=5, ntree=500)
and it seems to be working just fine.

Thanks to all for your help,

Florian

On 26 Jul 2007, at 19:26, Kuhn, Max wrote:

> Florian,
>
> The first thing that you should change is how you call randomForest.
> Instead of specifying the model via a formula, use the randomForest(x,
> y) interface.
>
> When a formula is used, there is a terms object created so that a  
> model
> matrix can be created for these and future observations. That terms
> object can get big (I think it would be a matrix of size 151 x 150)  
> and
> is diagonal.
>
> That might not solve it, but it should help.
>
> Max
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Florian Nigsch
> Sent: Thursday, July 26, 2007 2:07 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Large dataset + randomForest
>
> [Please CC me in any replies as I am not currently subscribed to the
> list. Thanks!]
>
> Dear all,
>
> I did a bit of searching on the question of large datasets but did
> not come to a definite conclusion. What I am trying to do is the
> following: I want to read in a dataset with approx. 100 000 rows and
> approx 150 columns. The file size is ~ 33MB, which one would deem not
> too big a file for R. To speed up the reading in of the file I do not
> use read.table but a loop that does reading with scan() into a buffer
> and some preprocessing and then adds the data into a dataframe.
>
> When I then want to run randomForest() R complains that I cannot
> allocate a vector of size 313.0 MB. I am aware that randomForest
> needs all data in memory, but
> 1) why should that suddenly be 10 times the size of the data (I
> acknowedge the need for some internal data of R, but 10 times seems a
> bit too much) and
> 2) there is still physical memory free on the machine (in total 4GB
> available, even though R is limited to 2GB if I correctly remember
> the help pages - still 2GB should be enough!) - it doesn't seem to
> work either with changed settings done via mem.limits(), or run-time
> arguments --min-vsize --max-vsize - what do these have to be set to
> to work in my case??
>
>> rf <- randomForest(V1 ~ ., data=df[trainindices,], do.trace=5)
> Error: cannot allocate vector of size 313.0 Mb
>> object.size(df)/1024/1024
> [1] 129.5390
>
>
> Any help would be greatly appreciated,
>
> Florian
>
> --
> Florian Nigsch <fn211 at cam.ac.uk>
> Unilever Centre for Molecular Sciences Informatics
> Department of Chemistry
> University of Cambridge
> http://www-mitchell.ch.cam.ac.uk/
> Telephone: +44 (0)1223 763 073
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ----------------------------------------------------------------------
> LEGAL NOTICE
> Unless expressly stated otherwise, this message is confidential and  
> may be privileged.  It is intended for the addressee(s) only.   
> Access to this E-mail by anyone else is unauthorized.  If you are  
> not an addressee, any disclosure or copying of the contents of this  
> E-mail or any action taken (or not taken) in reliance on it is  
> unauthorized and may be unlawful.  If you are not an addressee,  
> please inform the sender immediately.


From Greg.Snow at intermountainmail.org  Fri Jul 27 17:05:19 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 09:05:19 -0600
Subject: [R] plot
In-Reply-To: <989527a20707270745h541fed48kb286f9e408a4c522@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF2AF1@LP-EXCHVS07.CO.IHC.COM>

Graphs that rely on 3-d effects tend to distort the data rather than enlighten the viewer.  If your goal is to distort the data (which I doubt), then most of us don't want to help.  On the other hand, if you really do want to enlighten the viewer (even if that is just you), then tell us what your data is like and what you want to learn from it, and we will be happy to give you advice on creating useful graphs.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dong GUO ??
> Sent: Friday, July 27, 2007 8:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] plot
> 
> Greetings to the group,
> 
> I would like to know if some one could help me with plot 3-d 
> column graph of a matrix (3-d column graph in Excel).
> 
> Many thanks in advance.
> 
> Regards,
> Dong
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From justin_bem at yahoo.fr  Fri Jul 27 17:25:17 2007
From: justin_bem at yahoo.fr (justin bem)
Date: Fri, 27 Jul 2007 15:25:17 +0000 (GMT)
Subject: [R] Re :  plot
Message-ID: <449694.77642.qm@web23009.mail.ird.yahoo.com>

Un texte encapsul? et encod? dans un jeu de caract?res inconnu a ?t? nettoy?...
Nom : non disponible
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20070727/c02252fc/attachment.pl 

From mo2259 at columbia.edu  Fri Jul 27 17:52:07 2007
From: mo2259 at columbia.edu (Mark Orr)
Date: Fri, 27 Jul 2007 11:52:07 -0400
Subject: [R] get() with complex objects?
Message-ID: <46AA14A7.9070803@columbia.edu>

Hello R-listers,
I'm having trouble accessing "sub" objects ("attributes"?), e.g., 
"x$silinfo$avg.width" using the /get() /command;  I'm using/ get()/ in a 
loop as illustrated in the following code:

#FIRST MAKE CLUSTERS of VARYING  k
/for (i in 1:300){
  assign(paste("x.",i,sep=""),pam(x,i))  #WORKS FINE
}/

#NEXT, TAKE LOOK AT AVE. SILHOUETTE VALUE FOR EACH k

#PART 1, MAKE LIST OF OBJECTS NEEDED
/gen.list <- rep("t",300)
for (i in 1:300){
  assign(gen.list[i],paste("x.",i,"$silinfo$avg.width",sep=""))
}
#WORKS FINE

/#PART 2, USE LIST IN LOOP TO ACCESS OBJECT.
/si//l.collector <- rep(99,300)
for(i in 1:300){
 sil.collector <- get(gen.list[i])
}/
#HERE IS THE ERROR
/Error in get(x, envir, mode, inherits) : variable 
"x.1$silinfo$avg.width" was not found

/So, I get the gist of this error; x.1 is an object findable from get(), 
but the "attribute"  levels are not accessible.  Any suggestions on how 
to get get() to access these levels?  From reading the get()'s help 
page, I don't think it will access the attributes. (my apologies for 
loosely using the term attributes, but I hope it is clear).

Thanks,

Mark Orr

-- 
***********************************************
Mark G. Orr, PhD
Heilbrunn Dept. of Population and Family Health
Columbia University
60 Haven Ave., B-2
New York, NY 10032

Tele: 212-304-7823
Fax:  212-305-7024

www.columbia.edu/~mo2259


From jholtman at gmail.com  Fri Jul 27 19:19:50 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 27 Jul 2007 13:19:50 -0400
Subject: [R] get() with complex objects?
In-Reply-To: <46AA14A7.9070803@columbia.edu>
References: <46AA14A7.9070803@columbia.edu>
Message-ID: <644e1f320707271019r10e9039bk5badc2e2bfa0fa81@mail.gmail.com>

'get' tries to retrieve the object given by the character string.  The
error message says that object can not be found.  You actually have to
'evaluate' the character string.  See the example below:

> x <- data.frame(a=1:10, b=11:20)
> x$a
 [1]  1  2  3  4  5  6  7  8  9 10
> z <- 'x$a'
> get(z)
Error in get(x, envir, mode, inherits) : variable "x$a" was not found
> # parse and evaluate the character string 'x$a'
> eval(parse(text=z))
 [1]  1  2  3  4  5  6  7  8  9 10

Does this make sense?


On 7/27/07, Mark Orr <mo2259 at columbia.edu> wrote:
> Hello R-listers,
> I'm having trouble accessing "sub" objects ("attributes"?), e.g.,
> "x$silinfo$avg.width" using the /get() /command;  I'm using/ get()/ in a
> loop as illustrated in the following code:
>
> #FIRST MAKE CLUSTERS of VARYING  k
> /for (i in 1:300){
>  assign(paste("x.",i,sep=""),pam(x,i))  #WORKS FINE
> }/
>
> #NEXT, TAKE LOOK AT AVE. SILHOUETTE VALUE FOR EACH k
>
> #PART 1, MAKE LIST OF OBJECTS NEEDED
> /gen.list <- rep("t",300)
> for (i in 1:300){
>  assign(gen.list[i],paste("x.",i,"$silinfo$avg.width",sep=""))
> }
> #WORKS FINE
>
> /#PART 2, USE LIST IN LOOP TO ACCESS OBJECT.
> /si//l.collector <- rep(99,300)
> for(i in 1:300){
>  sil.collector <- get(gen.list[i])
> }/
> #HERE IS THE ERROR
> /Error in get(x, envir, mode, inherits) : variable
> "x.1$silinfo$avg.width" was not found
>
> /So, I get the gist of this error; x.1 is an object findable from get(),
> but the "attribute"  levels are not accessible.  Any suggestions on how
> to get get() to access these levels?  From reading the get()'s help
> page, I don't think it will access the attributes. (my apologies for
> loosely using the term attributes, but I hope it is clear).
>
> Thanks,
>
> Mark Orr
>
> --
> ***********************************************
> Mark G. Orr, PhD
> Heilbrunn Dept. of Population and Family Health
> Columbia University
> 60 Haven Ave., B-2
> New York, NY 10032
>
> Tele: 212-304-7823
> Fax:  212-305-7024
>
> www.columbia.edu/~mo2259
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From Greg.Snow at intermountainmail.org  Fri Jul 27 20:57:47 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 12:57:47 -0600
Subject: [R] plot
In-Reply-To: <989527a20707271109w365c85f3qb8b4b775c805099b@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF2B8C@LP-EXCHVS07.CO.IHC.COM>

Can you really see much of the data in a 26*31 3d barplot?  It seems like most info would be hidden behind the first few rows and it would be so cluttered that you would not be able to make out much of anything from it.

Why not try a line plot instead (year as the x axis, each region a different year).  Here is a quick example:

> data(votes.repub, package='cluster')
> matplot( t(votes.repub[1:31, 1:26]), type='l')
> 

Even better would be to group some of the regions together and use xyplot from the lattice package and have a panel for each group of regions (fewer lines per panel should be easier to see detail).

You could also use the image function (or levelplot from lattice) to create a 26*31 grid with colors used for the 3rd dimension (can be good for overall patterns, not so good for looking at detail).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: Dong GUO ???? [mailto:guo.dong99 at gmail.com] 
> Sent: Friday, July 27, 2007 12:09 PM
> To: Greg Snow
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] plot
> 
> Many thanks, Greg and Justin.
> 
> The matrix is just a 26*31 matrix - 26 years, 31 regions. I 
> am know to R, just dont know how to attach the data here yet..
> 
> As I have such matrices for nine indicators for all regions, 
> so i could show some differences by 3D plot, which I did 
> similar things in Excel. I am sure there is a way to do it in R
> 
> On 7/27/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> > Graphs that rely on 3-d effects tend to distort the data 
> rather than enlighten the viewer.  If your goal is to distort 
> the data (which I doubt), then most of us don't want to help. 
>  On the other hand, if you really do want to enlighten the 
> viewer (even if that is just you), then tell us what your 
> data is like and what you want to learn from it, and we will 
> be happy to give you advice on creating useful graphs.
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dong GUO ??
> > > Sent: Friday, July 27, 2007 8:45 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] plot
> > >
> > > Greetings to the group,
> > >
> > > I would like to know if some one could help me with plot 
> 3-d column 
> > > graph of a matrix (3-d column graph in Excel).
> > >
> > > Many thanks in advance.
> > >
> > > Regards,
> > > Dong
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> 



From bolker at ufl.edu  Fri Jul 27 21:28:30 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 27 Jul 2007 19:28:30 +0000 (UTC)
Subject: [R] R codes for g-and-h distribution
References: <687475.90110.qm@web54005.mail.re2.yahoo.com>
Message-ID: <loom.20070727T212305-710@post.gmane.org>

filame uyaco <filams0704 <at> yahoo.com> writes:

> 
> 
> hi!
> 
> I would like to ask help how to generate numbers from g-and-h distribution. 
This distribution is like
> normal distribution  but span more of the kurtosis and skewness plane. Has R
any package on how to generate
> them? 
> 

  Someone else asked for this in 2005, but I didn't see any 
answers.  If the wiki weren't down I would put this up there ...

## http://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/ghpdf.htm

## G(p,g,h) = exp((g*Zp)-1)*exp((h*Zp^2)/2)/g

[n.b. formatting on this page is very confusing, I inserted
 parentheses]

##  with Zp denoting the normal percent point function of p. When g = 0
##  and h = 0, the g-and-h distribution reduces to a standard normal
##  distribution.

## The g-and-h probability density function is computed by taking the
## numerical derivative of the cumulative distribution function (which is
## turn computed by numerically inverting the percent point function
## using a bisection method).

## [from
## http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm:
## The percent point function (ppf) is the inverse of the cumulative
## distribution function. For this reason, the percent point function
## is also commonly referred to as the inverse distribution function.

## thus R's "q" functions are percent point functions

qgh <- function(q,g,h) {
  Zp <- qnorm(q)
  ## not vectorized!
  if (g==0) Zp else (exp(g*Zp)-1)*exp((h*Zp^2/2))/g
}

## since the quantile function is defined, it makes generating
##  random values easy!
rgh <- function(n,g,h) {
  qgh(runif(n),g,h)
}

## eps sets distance from 1 for searching
##  strategy could probably be improved (first approx. based on
##   qnorm??)
pgh <- function(p,g,h,eps=1e-7) {
  uniroot(function(z) { qgh(z,g,h) - p}, interval=c(eps,1-eps))$root
}

dgh <- function(x,g,h,log=FALSE,ndep=1e-3,...) {
  ## crude vectorization in x (not g or h)
  if (length(x)>1) return(sapply(x,dgh,g=g,h=h,log=log,ndep=ndep,...))
  r <- (pgh(x+ndep,g,h)-pgh(x,g,h))/ndep
  if (log) log(r) else r
}
  

## examples ...
set.seed(1001)
## should be approx. normal
r1 = rgh(10000,1e-5,0)
r2 = rgh(10000,1e-5,0)
r3 = rgh(10000,1e-5,0)

## plot 3 different samples
plot(density(r1))
lines(density(r2))
lines(density(r3))
curve(dnorm(x),col=2,add=TRUE)
curve(dgh(x,1e-5,0),col=4,add=TRUE)
## some slight numerical glitches -- e.g. around
## x=-2.6

r4 = rgh(50000,0.4,0.4)
plot(density(r4,n=1024),xlim=c(-10,10))
curve(dnorm(x),add=TRUE,col=2)
curve(dgh(x,0.4,0.4),add=TRUE,col=4)
legend("topleft",c("density","g-h","normal"),
       lty=rep(1,3),col=c(1,4,2))
## note glitch again, this time around
## y=-3.89




From doc.evans at gmail.com  Fri Jul 27 23:52:19 2007
From: doc.evans at gmail.com (D. R. Evans)
Date: Fri, 27 Jul 2007 15:52:19 -0600
Subject: [R] Q: extracting data from lm
Message-ID: <46AA6913.8060801@gmail.com>

Warning: I am a complete newbie to R. I have read ISwR, but I am still
finding myself completely stuck on some simple concepts.

I have tried everything I can think of to solve this one, and finally
decided that enough was enough and I need a pointer to a solution.

I have the following summary from lm():

----

> summary(lm(nu1~nu4))

Call:
lm(formula = nu1 ~ nu4)

Residuals:
     Min       1Q   Median       3Q      Max
-1572.62  -150.38   -21.70   168.57  2187.84

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 29.88739   43.68881   0.684    0.494
nu4          1.00036    0.01025  97.599   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 470.9 on 298 degrees of freedom
Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16

----

But I want to access some of these numbers programmatically. I finally
figured out that to get the estimate of the nu4 coefficient I need to do:

----

> lm(nu1~nu4)$coefficients[2]
     nu4
1.000363

----

which to me as a long-time C++ programmer is close to black magic (I've
been programming since 1972; I have to say that R is unlike anything I've
ever seen, and it's far from trivial to get my head around some of it --
for example, how I could have known a priori that the above is the way to
get the nu4 coefficient is beyond me). Anyway, having figured out how to
get the estimate of the coefficient, I not-unnaturally wanted also to find
a way to access the std. error of the estimate (the value 0.01025 in the
summary). But I am completely mystified as to how to do it :-(

Any help gratefully (VERY gratefully) received, and I apologise if this is
a really, really stupid question and that the answer lies somewhere in some
documentation that I've obviously not properly taken on board.




From marc_schwartz at comcast.net  Sat Jul 28 00:46:52 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 27 Jul 2007 17:46:52 -0500
Subject: [R] Q: extracting data from lm
In-Reply-To: <46AA6913.8060801@gmail.com>
References: <46AA6913.8060801@gmail.com>
Message-ID: <1185576412.3625.18.camel@Bellerophon.localdomain>

On Fri, 2007-07-27 at 15:52 -0600, D. R. Evans wrote:
> Warning: I am a complete newbie to R. I have read ISwR, but I am still
> finding myself completely stuck on some simple concepts.
> 
> I have tried everything I can think of to solve this one, and finally
> decided that enough was enough and I need a pointer to a solution.
> 
> I have the following summary from lm():
> 
> ----
> 
> > summary(lm(nu1~nu4))
> 
> Call:
> lm(formula = nu1 ~ nu4)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -1572.62  -150.38   -21.70   168.57  2187.84
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) 29.88739   43.68881   0.684    0.494
> nu4          1.00036    0.01025  97.599   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 470.9 on 298 degrees of freedom
> Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
> F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16
> 
> ----
> 
> But I want to access some of these numbers programmatically. I finally
> figured out that to get the estimate of the nu4 coefficient I need to do:
> 
> ----
> 
> > lm(nu1~nu4)$coefficients[2]
>      nu4
> 1.000363
> 
> ----
> 
> which to me as a long-time C++ programmer is close to black magic (I've
> been programming since 1972; I have to say that R is unlike anything I've
> ever seen, and it's far from trivial to get my head around some of it --
> for example, how I could have known a priori that the above is the way to
> get the nu4 coefficient is beyond me). Anyway, having figured out how to
> get the estimate of the coefficient, I not-unnaturally wanted also to find
> a way to access the std. error of the estimate (the value 0.01025 in the
> summary). But I am completely mystified as to how to do it :-(
> 
> Any help gratefully (VERY gratefully) received, and I apologise if this is
> a really, really stupid question and that the answer lies somewhere in some
> documentation that I've obviously not properly taken on board.

It looks like Peter references the notion of 'extractor functions' below
middle on page 97, but does not describe them, unless I am going blind
(or just tired).

In either case, you can supplement Peter's great intro book with many of
the of the other free references available via the R web site or An
Introduction to R, available with your installation or from the web
site.

In the latter case go here:

http://cran.r-project.org/doc/manuals/R-intro.html#Statistical-models-in-R

and scroll down to section 11.3.

Take note also of the 'Value' section of ?lm, which describes the
general contents of an lm object.

Another function that is always helpful is str(), which will display the
structure of an R object, enabling you to gain insights into the
components, their names and content. See ?str

In addition, take note of the 'See Also' section of ?lm, which does list
the generic functions coef, effects, residuals, fitted, vcov.

Using coef() will get you the values from the Coefficients part of the
output above, then enabling you to subset them to get the other values
as you require.

Another option relative to seeking assistance, is to review the Posting
Guide, a link to which is on the bottom of every post to the list. There
is a list of resources there that can guide you through how to search
for help, including utilizing the archives of the R help list where
keyword searches will be of tremendous assistance.

HTH,

Marc Schwartz



From stanhopkins at comcast.net  Sat Jul 28 03:10:22 2007
From: stanhopkins at comcast.net (Stan Hopkins)
Date: Fri, 27 Jul 2007 20:10:22 -0500
Subject: [R] Error when using the cat function
Message-ID: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>

Is the following developed in my console output a recognized bug or am I using the cat function incorrectly?

Thanks,

Stan



> ifelse(class(data[[n]])!="factor",{print("yes")},{print("no")})
[1] "yes"
[1] "yes"
> ifelse(class(data[[n]])!="factor",{cat("yes")},{cat("no")})
yesError in ans[test & !nas] <- rep(yes, length.out = length(ans))[test & : 
incompatible types (from NULL to logical) in subassignment type fix


>cat("yes")
yes 
> class(data[[n]])!="factor"
[1] TRUE
> class(data[[n]])
[1] "numeric"
> n
[1] 28
> length(data[[n]])
[1] 955
> class(data)
[1] "data.frame"
> dim(data)
[1] 955 182
> data[[n]]
[1] 2.5 4.9 2.6 3.0 4.7 5.0 3.9 1.5 4.8 3.2 3.6 5.2 6.3
[14] 6.3 5.0 4.6 6.0 4.5 3.9 3.6 5.7 8.5 4.0 5.0 11.8 4.7
[27] 7.9 2.8 4.8 5.1 4.1 4.2 3.7 2.0 2.1 1.1 14.6 7.0 3.4
[40] 3.4 10.1 4.7 4.9 5.2 4.3 2.9 2.8 2.3 1.2 2.0 2.0 3.0
[53] 2.0 1.1 2.0 1.0 2.0 2.0 2.7 1.0 2.0 2.0 2.0 2.0 1.1
[66] 2.0 2.0 1.0 1.1 2.4 2.0 2.0 5.0 0.8 2.0 3.3 2.7 2.2
[79] 2.9 1.4 2.0 1.9 1.0 1.9 2.1 2.2 2.0 2.0 1.3 3.0 1.4
[92] 2.0 1.5 2.1 1.2 1.7 2.1 2.0 2.0 2.3 2.0 1.6 1.5 2.3
[105] 1.1 2.0 2.0 5.0 2.4 2.0 0.8 4.0 0.0 1.7 8.3 2.0 2.0
[118] 2.0 6.1 14.4 8.2 5.2 2.5 1.0 1.0 1.8 1.1 4.9 0.9 2.1
[131] 1.4 1.0 1.0 3.0 2.6 2.0 1.7 1.2 3.3 2.0 1.1 1.7 1.2
[144] 2.7 0.9 2.0 3.2 1.8 1.8 1.1 1.3 2.3 1.1 1.7 1.9 1.0
[157] 2.3 1.1 1.0 1.2 1.5 3.2 2.2 1.6 1.0 1.7 2.5 2.0 2.0
[170] 2.3 1.1 1.5 2.0 1.7 5.1 3.6 2.0 2.0 1.2 1.2 3.1 1.3
[183] 1.3 2.0 1.7 1.1 2.8 2.0 2.0 1.9 2.0 2.8 4.0 8.8 4.0
[196] 3.2 5.0 2.1 3.0 7.4 2.5 3.2 3.0 2.8 1.9 3.0 3.2 3.6
[209] 2.8 3.2 2.1 2.5 2.2 3.0 3.7 3.2 2.3 2.7 3.1 2.5 3.0
[222] 2.4 2.6 0.9 5.4 2.8 3.9 4.7 2.5 2.9 4.4 4.1 4.0 4.0
[235] 2.0 4.5 3.2 3.0 4.5 6.5 7.3 1.1 9.3 5.1 4.0 4.5 4.8
[248] 7.6 6.7 3.0 3.0 6.0 6.0 4.0 5.0 3.0 5.0 1.0 5.0 4.0
[261] 5.0 4.0 3.8 3.0 7.0 3.0 5.0 120.0 4.0 8.0 4.0 6.0 5.0
[274] 4.0 6.0 2.0 2.6 3.2 4.0 4.0 3.0 6.0 3.0 3.0 2.0 2.5
[287] 5.0 5.0 3.0 3.0 4.0 7.3 2.1 6.3 6.6 15.9 3.6 2.0 9.1
[300] 6.9 4.2 7.8 5.7 7.7 5.6 5.8 16.3 4.0 3.0 3.4 0.0 1.0
[313] 1.0 2.7 1.6 1.6 1.0 3.0 2.0 1.0 2.0 1.3 2.0 1.4 1.0
[326] 0.9 1.0 0.8 0.0 0.0 3.1 2.6 1.4 2.0 6.6 2.0 1.2 2.0
[339] 1.0 1.8 1.7 2.3 1.7 0.0 1.3 2.0 3.5 1.1 0.0 1.2 1.2
[352] 1.0 2.0 1.2 NA 1.2 2.2 2.0 2.2 1.5 1.0 2.8 1.0 1.0
[365] 2.1 2.0 1.3 0.0 1.5 1.8 1.4 1.2 1.2 1.1 1.0 1.1 2.0
[378] 2.0 2.4 2.0 2.8 3.1 1.1 1.8 1.3 1.4 0.7 4.0 4.7 1.0
[391] 0.6 3.0 1.0 0.9 2.0 1.7 2.1 2.0 1.0 2.0 16.0 3.0 10.0
[404] 5.0 1.2 0.7 1.2 1.9 1.3 1.7 1.3 2.0 1.6 4.2 3.8 1.4
[417] 1.2 1.3 2.0 2.1 5.8 5.9 1.2 2.8 1.8 3.6 1.8 1.9 1.1
[430] 1.3 0.9 2.0 3.2 1.7 1.7 2.9 1.6 5.0 4.0 1.9 2.2 2.0
[443] 2.7 2.5 1.1 2.0 1.7 1.5 1.9 1.1 1.6 5.2 1.5 1.4 1.0
[456] 1.9 1.4 1.9 2.2 2.3 3.9 1.7 0.8 0.9 1.5 1.7 2.9 1.2
[469] 1.9 1.8 2.6 1.4 2.1 1.6 1.7 1.6 1.4 2.0 2.1 1.0 5.0
[482] 2.3 2.5 1.0 1.0 1.3 2.3 1.1 1.8 0.9 1.5 1.3 1.0 0.8
[495] 1.0 0.7 0.9 0.9 2.0 2.9 2.6 0.6 1.6 2.0 0.9 1.0 1.1
[508] 2.0 0.9 1.1 2.0 4.0 3.0 1.0 2.0 2.0 1.4 3.0 3.0 1.3
[521] 1.0 1.2 0.8 2.0 0.0 0.0 0.7 1.4 1.0 0.8 1.2 1.4 2.1
[534] 1.0 1.0 1.4 1.2 1.1 4.0 1.3 3.0 1.7 2.0 1.0 1.6 2.0
[547] 0.9 6.0 1.7 1.7 1.7 1.0 0.8 0.6 2.0 2.0 1.0 2.0 1.4
[560] 1.0 1.3 1.0 1.0 1.1 1.0 1.1 5.0 4.0 2.0 1.6 3.0 2.1
[573] 1.2 2.0 0.9 1.2 1.0 1.1 1.9 2.1 2.2 1.0 1.5 1.3 3.0
[586] 2.0 3.6 2.0 2.0 1.5 11.4 5.2 4.5 3.4 1.6 2.1 1.2 2.4
[599] 2.1 2.3 1.7 2.0 1.4 0.5 1.6 1.9 2.6 0.4 1.3 1.4 1.2
[612] 1.1 1.4 2.3 1.0 1.7 1.1 3.4 1.4 2.4 1.2 1.0 1.3 1.0
[625] 1.2 0.8 2.1 1.7 2.1 0.9 1.4 1.2 1.9 1.1 2.3 1.5 3.0
[638] 3.0 4.9 5.8 3.0 3.0 4.2 1.1 2.5 4.9 2.0 1.9 1.8 1.2
[651] 2.0 2.2 1.4 1.8 2.0 1.2 3.2 1.5 2.0 3.5 2.0 0.8 1.8
[664] 1.1 2.0 2.2 1.4 1.1 2.0 1.7 1.4 3.8 4.0 1.7 1.5 1.2
[677] 1.1 2.0 3.0 21.0 6.0 20.0 5.0 20.0 13.0 4.0 2.6 2.8 6.1
[690] 2.1 1.8 2.2 1.9 1.5 4.0 2.9 2.6 2.3 2.2 3.3 3.5 1.2
[703] 1.5 3.7 2.3 3.0 1.9 2.5 1.5 1.7 2.5 3.0 2.6 1.8 2.5
[716] 0.9 3.1 1.5 2.1 2.5 0.6 1.9 1.7 3.7 7.4 2.4 3.3 3.2
[729] 1.2 1.3 2.0 1.4 3.4 1.7 3.5 1.7 2.0 1.3 0.8 3.0 1.9
[742] 1.9 20.6 3.8 3.8 1.2 1.5 3.2 6.1 5.8 6.6 4.0 5.7 4.0
[755] 3.0 4.7 6.8 6.9 4.1 1.9 4.5 3.8 2.7 2.3 2.5 2.3 2.6
[768] 3.8 1.8 2.4 1.8 1.9 6.1 5.1 4.0 3.8 2.8 3.4 3.1 2.3
[781] 7.5 3.0 3.0 3.1 2.4 6.0 2.3 5.0 2.8 2.7 2.2 5.0 5.0
[794] 3.0 9.0 7.0 7.0 7.0 9.0 8.0 9.0 2.0 4.0 4.0 3.0 3.0
[807] 2.0 2.0 3.0 4.0 3.0 3.0 7.9 11.0 16.0 4.0 7.7 5.0 6.6
[820] 16.0 9.0 19.0 4.0 4.0 7.5 6.6 22.0 20.0 7.2 7.1 7.6 6.6
[833] 6.2 7.8 6.9 10.9 11.2 6.0 5.0 8.0 7.0 4.0 3.0 6.0 4.0
[846] 2.6 6.0 7.0 4.0 1.4 2.0 6.0 6.0 6.0 6.0 24.0 27.6 15.8
[859] 8.0 7.8 7.3 8.2 5.3 3.4 18.1 31.6 5.8 5.5 4.0 4.1 4.7
[872] 4.9 3.9 0.5 3.9 6.2 3.0 3.0 4.0 2.0 3.0 3.0 2.0 2.0
[885] 2.0 0.0 3.0 2.0 2.0 4.0 1.6 1.7 2.0 2.0 2.0 2.0 26.0
[898] 2.0 2.0 2.0 3.0 2.0 3.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0
[911] 2.0 0.0 2.0 2.0 2.0 2.0 5.0 2.0 11.7 10.4 8.0 4.0 11.1
[924] 13.2 14.6 11.7 13.4 14.3 15.8 25.6 10.0 6.0 9.1 9.7 4.0 10.7
[937] 6.0 5.0 10.9 10.0 10.6 12.9 12.3 11.6 11.8 13.3 15.1 10.7 11.0
[950] 13.5 32.9 12.9 8.4 8.1 12.5

From shitao at hotmail.com  Fri Jul 27 19:25:19 2007
From: shitao at hotmail.com (Tao Shi)
Date: Fri, 27 Jul 2007 17:25:19 +0000
Subject: [R] Creating windows binary R package (PowerArchiver vs. zip
 -r9X)
Message-ID: <BAY120-W28888A6757C258C1DFD63FC7F30@phx.gbl>

Thanks, Uwe!  It worked....Tao> Date: Fri, 27 Jul 2007 09:16:49 +0200> From: ligges at statistik.uni-dortmund.de> To: shitao at hotmail.com> CC: r-help at stat.math.ethz.ch> Subject: Re: [R] Creating windows binary R package (PowerArchiver vs. zip -r9X)> > > > Tao Shi wrote:> > Hi list,I apologize if you see funny fonts, b/c I'm using the new> > Windows Live Hotmail and don't know how to turn off the "rich text"> > mode.....I have successfully built and installed a R package in> > windowsXP for R-2.5.1.  But when I tried to create a .zip file so I> > can use "Packages/install package(s) from local .zip files..." to> > install it, it seems R only recognizes the .zip file created by "zip> > -r9X" not by PowerArchiver.  Do you know why?  I vaguely remember I> > used WinZip before and it worked fine.The two threads I found on> > R-help and R-devel help me a lot, but don't really answer my> > question.http://tolstoy.newcastle.edu.au/R/help/06/06/29587.htmlhttp://tolstoy.newcastle.edu.au/R/devel/05/12/3336.htmlThanks,...Tao> > > In order to provide a Windows binary package, type:> > R CMD INSTALL --build PackageName_vers.tar.gz> and the zip file will be generated by R in the correct way.> > Uwe Ligges> > > > > 
_________________________________________________________________
PC Magazine?s 2007 editors? choice for best web mail?award-winning Windows Live Hotmail.
http://imagine-windowslive.com/hotmail/?locale=en-us&ocid=TXT_TAGHM_migration_HMWL_mini_pcmag_0707

From mmeredith at wcs.org  Sat Jul 28 14:20:05 2007
From: mmeredith at wcs.org (Mike Meredith)
Date: Sat, 28 Jul 2007 05:20:05 -0700 (PDT)
Subject: [R] Error when using the cat function
In-Reply-To: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>
References: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>
Message-ID: <11842347.post@talk.nabble.com>


Your problem is with ifelse, not with cat.

First clue is that 

ifelse(TRUE,{print("yes")},{print("no")})  # results in "yes" being printed
TWICE. Try this:

tmp <- ifelse(TRUE,{print("yes")},{print("no")}) # one "yes"

tmp  # another "yes"

Try:

print(print("yes")) # prints "yes" and returns "yes" invisibly. This
returned value is passed on to/by ifelse.

Now try:

print(cat("yes\n")) # "yes" appears, but cat("yes") returns NULL, which
ifelse can't handle:

ifelse(TRUE, NULL, "whatever") # Gives the error you saw.

What you need is if { } else { } :

if(!inherits(dat[[n]], "factor")) {cat("yes\n")} else {cat("no\n")}

HTH, Mike.


Stan Hopkins wrote:
> 
> Is the following developed in my console output a recognized bug or am I
> using the cat function incorrectly?
> 
> Thanks,
> 
> Stan
> 
> 
> 
>> ifelse(class(data[[n]])!="factor",{print("yes")},{print("no")})
> [1] "yes"
> [1] "yes"
>> ifelse(class(data[[n]])!="factor",{cat("yes")},{cat("no")})
> yesError in ans[test & !nas] <- rep(yes, length.out = length(ans))[test &
> : 
> incompatible types (from NULL to logical) in subassignment type fix
> 
> 
>>cat("yes")
> yes 
>> class(data[[n]])!="factor"
> [1] TRUE
>> class(data[[n]])
> [1] "numeric"
>> n
> [1] 28
>> length(data[[n]])
> [1] 955
>> class(data)
> [1] "data.frame"
>> dim(data)
> [1] 955 182
>> data[[n]]
> [1] 2.5 4.9 2.6 3.0 4.7 5.0 3.9 1.5 4.8 3.2 3.6 5.2 6.3
> [14] 6.3 5.0 4.6 6.0 4.5 3.9 3.6 5.7 8.5 4.0 5.0 11.8 4.7
> [27] 7.9 2.8 4.8 5.1 4.1 4.2 3.7 2.0 2.1 1.1 14.6 7.0 3.4
> [40] 3.4 10.1 4.7 4.9 5.2 4.3 2.9 2.8 2.3 1.2 2.0 2.0 3.0
> [53] 2.0 1.1 2.0 1.0 2.0 2.0 2.7 1.0 2.0 2.0 2.0 2.0 1.1
> [66] 2.0 2.0 1.0 1.1 2.4 2.0 2.0 5.0 0.8 2.0 3.3 2.7 2.2
> [79] 2.9 1.4 2.0 1.9 1.0 1.9 2.1 2.2 2.0 2.0 1.3 3.0 1.4
> [92] 2.0 1.5 2.1 1.2 1.7 2.1 2.0 2.0 2.3 2.0 1.6 1.5 2.3
> [105] 1.1 2.0 2.0 5.0 2.4 2.0 0.8 4.0 0.0 1.7 8.3 2.0 2.0
> [118] 2.0 6.1 14.4 8.2 5.2 2.5 1.0 1.0 1.8 1.1 4.9 0.9 2.1
> [131] 1.4 1.0 1.0 3.0 2.6 2.0 1.7 1.2 3.3 2.0 1.1 1.7 1.2
> [144] 2.7 0.9 2.0 3.2 1.8 1.8 1.1 1.3 2.3 1.1 1.7 1.9 1.0
> [157] 2.3 1.1 1.0 1.2 1.5 3.2 2.2 1.6 1.0 1.7 2.5 2.0 2.0
> [170] 2.3 1.1 1.5 2.0 1.7 5.1 3.6 2.0 2.0 1.2 1.2 3.1 1.3
> [183] 1.3 2.0 1.7 1.1 2.8 2.0 2.0 1.9 2.0 2.8 4.0 8.8 4.0
> [196] 3.2 5.0 2.1 3.0 7.4 2.5 3.2 3.0 2.8 1.9 3.0 3.2 3.6
> [209] 2.8 3.2 2.1 2.5 2.2 3.0 3.7 3.2 2.3 2.7 3.1 2.5 3.0
> [222] 2.4 2.6 0.9 5.4 2.8 3.9 4.7 2.5 2.9 4.4 4.1 4.0 4.0
> [235] 2.0 4.5 3.2 3.0 4.5 6.5 7.3 1.1 9.3 5.1 4.0 4.5 4.8
> [248] 7.6 6.7 3.0 3.0 6.0 6.0 4.0 5.0 3.0 5.0 1.0 5.0 4.0
> [261] 5.0 4.0 3.8 3.0 7.0 3.0 5.0 120.0 4.0 8.0 4.0 6.0 5.0
> [274] 4.0 6.0 2.0 2.6 3.2 4.0 4.0 3.0 6.0 3.0 3.0 2.0 2.5
> [287] 5.0 5.0 3.0 3.0 4.0 7.3 2.1 6.3 6.6 15.9 3.6 2.0 9.1
> [300] 6.9 4.2 7.8 5.7 7.7 5.6 5.8 16.3 4.0 3.0 3.4 0.0 1.0
> [313] 1.0 2.7 1.6 1.6 1.0 3.0 2.0 1.0 2.0 1.3 2.0 1.4 1.0
> [326] 0.9 1.0 0.8 0.0 0.0 3.1 2.6 1.4 2.0 6.6 2.0 1.2 2.0
> [339] 1.0 1.8 1.7 2.3 1.7 0.0 1.3 2.0 3.5 1.1 0.0 1.2 1.2
> [352] 1.0 2.0 1.2 NA 1.2 2.2 2.0 2.2 1.5 1.0 2.8 1.0 1.0
> [365] 2.1 2.0 1.3 0.0 1.5 1.8 1.4 1.2 1.2 1.1 1.0 1.1 2.0
> [378] 2.0 2.4 2.0 2.8 3.1 1.1 1.8 1.3 1.4 0.7 4.0 4.7 1.0
> [391] 0.6 3.0 1.0 0.9 2.0 1.7 2.1 2.0 1.0 2.0 16.0 3.0 10.0
> [404] 5.0 1.2 0.7 1.2 1.9 1.3 1.7 1.3 2.0 1.6 4.2 3.8 1.4
> [417] 1.2 1.3 2.0 2.1 5.8 5.9 1.2 2.8 1.8 3.6 1.8 1.9 1.1
> [430] 1.3 0.9 2.0 3.2 1.7 1.7 2.9 1.6 5.0 4.0 1.9 2.2 2.0
> [443] 2.7 2.5 1.1 2.0 1.7 1.5 1.9 1.1 1.6 5.2 1.5 1.4 1.0
> [456] 1.9 1.4 1.9 2.2 2.3 3.9 1.7 0.8 0.9 1.5 1.7 2.9 1.2
> [469] 1.9 1.8 2.6 1.4 2.1 1.6 1.7 1.6 1.4 2.0 2.1 1.0 5.0
> [482] 2.3 2.5 1.0 1.0 1.3 2.3 1.1 1.8 0.9 1.5 1.3 1.0 0.8
> [495] 1.0 0.7 0.9 0.9 2.0 2.9 2.6 0.6 1.6 2.0 0.9 1.0 1.1
> [508] 2.0 0.9 1.1 2.0 4.0 3.0 1.0 2.0 2.0 1.4 3.0 3.0 1.3
> [521] 1.0 1.2 0.8 2.0 0.0 0.0 0.7 1.4 1.0 0.8 1.2 1.4 2.1
> [534] 1.0 1.0 1.4 1.2 1.1 4.0 1.3 3.0 1.7 2.0 1.0 1.6 2.0
> [547] 0.9 6.0 1.7 1.7 1.7 1.0 0.8 0.6 2.0 2.0 1.0 2.0 1.4
> [560] 1.0 1.3 1.0 1.0 1.1 1.0 1.1 5.0 4.0 2.0 1.6 3.0 2.1
> [573] 1.2 2.0 0.9 1.2 1.0 1.1 1.9 2.1 2.2 1.0 1.5 1.3 3.0
> [586] 2.0 3.6 2.0 2.0 1.5 11.4 5.2 4.5 3.4 1.6 2.1 1.2 2.4
> [599] 2.1 2.3 1.7 2.0 1.4 0.5 1.6 1.9 2.6 0.4 1.3 1.4 1.2
> [612] 1.1 1.4 2.3 1.0 1.7 1.1 3.4 1.4 2.4 1.2 1.0 1.3 1.0
> [625] 1.2 0.8 2.1 1.7 2.1 0.9 1.4 1.2 1.9 1.1 2.3 1.5 3.0
> [638] 3.0 4.9 5.8 3.0 3.0 4.2 1.1 2.5 4.9 2.0 1.9 1.8 1.2
> [651] 2.0 2.2 1.4 1.8 2.0 1.2 3.2 1.5 2.0 3.5 2.0 0.8 1.8
> [664] 1.1 2.0 2.2 1.4 1.1 2.0 1.7 1.4 3.8 4.0 1.7 1.5 1.2
> [677] 1.1 2.0 3.0 21.0 6.0 20.0 5.0 20.0 13.0 4.0 2.6 2.8 6.1
> [690] 2.1 1.8 2.2 1.9 1.5 4.0 2.9 2.6 2.3 2.2 3.3 3.5 1.2
> [703] 1.5 3.7 2.3 3.0 1.9 2.5 1.5 1.7 2.5 3.0 2.6 1.8 2.5
> [716] 0.9 3.1 1.5 2.1 2.5 0.6 1.9 1.7 3.7 7.4 2.4 3.3 3.2
> [729] 1.2 1.3 2.0 1.4 3.4 1.7 3.5 1.7 2.0 1.3 0.8 3.0 1.9
> [742] 1.9 20.6 3.8 3.8 1.2 1.5 3.2 6.1 5.8 6.6 4.0 5.7 4.0
> [755] 3.0 4.7 6.8 6.9 4.1 1.9 4.5 3.8 2.7 2.3 2.5 2.3 2.6
> [768] 3.8 1.8 2.4 1.8 1.9 6.1 5.1 4.0 3.8 2.8 3.4 3.1 2.3
> [781] 7.5 3.0 3.0 3.1 2.4 6.0 2.3 5.0 2.8 2.7 2.2 5.0 5.0
> [794] 3.0 9.0 7.0 7.0 7.0 9.0 8.0 9.0 2.0 4.0 4.0 3.0 3.0
> [807] 2.0 2.0 3.0 4.0 3.0 3.0 7.9 11.0 16.0 4.0 7.7 5.0 6.6
> [820] 16.0 9.0 19.0 4.0 4.0 7.5 6.6 22.0 20.0 7.2 7.1 7.6 6.6
> [833] 6.2 7.8 6.9 10.9 11.2 6.0 5.0 8.0 7.0 4.0 3.0 6.0 4.0
> [846] 2.6 6.0 7.0 4.0 1.4 2.0 6.0 6.0 6.0 6.0 24.0 27.6 15.8
> [859] 8.0 7.8 7.3 8.2 5.3 3.4 18.1 31.6 5.8 5.5 4.0 4.1 4.7
> [872] 4.9 3.9 0.5 3.9 6.2 3.0 3.0 4.0 2.0 3.0 3.0 2.0 2.0
> [885] 2.0 0.0 3.0 2.0 2.0 4.0 1.6 1.7 2.0 2.0 2.0 2.0 26.0
> [898] 2.0 2.0 2.0 3.0 2.0 3.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0
> [911] 2.0 0.0 2.0 2.0 2.0 2.0 5.0 2.0 11.7 10.4 8.0 4.0 11.1
> [924] 13.2 14.6 11.7 13.4 14.3 15.8 25.6 10.0 6.0 9.1 9.7 4.0 10.7
> [937] 6.0 5.0 10.9 10.0 10.6 12.9 12.3 11.6 11.8 13.3 15.1 10.7 11.0
> [950] 13.5 32.9 12.9 8.4 8.1 12.5
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Error-when-using-the-cat-function-tf4161670.html#a11842347
Sent from the R help mailing list archive at Nabble.com.


From kamauallan at yahoo.com  Sat Jul 28 14:48:47 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Sat, 28 Jul 2007 05:48:47 -0700 (PDT)
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
In-Reply-To: <644e1f320707270513h5adabb8au3a870079b4bfbf80@mail.gmail.com>
Message-ID: <657735.44420.qm@web53507.mail.re2.yahoo.com>

Hi Jim,
The problem description.
I am trying to identify mutations in a given gene from
a particular genome (biological genome sequence).
I have two CSV files consisting of sequences. One file
consists of reference (documented,curated accepted as
standard) sequences. The other consists of sample
sequences I am trying to identify mutations within. In
both files the an individual sequence is contained in
a single record, it?s amino acid residues ( the actual
sequence of alphabets each representing a given amino
acid for example ?A? stands for ?Alanine?, ?C? for
Cysteine and so on) are each allocated a single field
in the CSV file.
The sequences in both files have been well aligned,
each contain 115 residues with the first residue is
contained in the field 5. The fields 1 to 4 are
allocated for metadata (name of sequence and so on).
My task is to compile a residue occurrence count for
each residue present in a given field in the reference
sequence dataset and use this information when reading
each sequence in the sample dataset to identify a
mutation. For example for position 9 of the sample
sequence ?bb? a ?P? is found and according to our
reference sequence dataset of summaries, at position 9
?P? may not even exist or may have an occurrence of
10% or so will be classified as mutation, (I could
employ a cut of parameter for mutation
classification).


Allan.

--- jim holtman <jholtman at gmail.com> wrote:

> results=()#character()
> myVariableNames=names(x.val)
> results[length(myVariableNames)]<-NA
> 
> for (i in myVariableNames){
>     results[i]<-names(x.val[[i]])    # this does not
> work it returns a
> NULL (how can i convert this to x.val$"somevalue" ?
> )
> }
> 
> 
> 
> On 7/27/07, Allan Kamau <kamauallan at yahoo.com>
> wrote:
> > Hi All,
> > I am having difficulties finding a way to find a
> substitute to the command "names(v.val$PR14)" so
> that I could generate the command on the fly for all
> PR14 to PR200 (please see the previous discussion
> below to understand what the object x.val contains)
> . I have tried the following
> >
> > >results=()#character()
> > >myVariableNames=names(x.val)
> > >results[length(myVariableNames)]<-NA
> >
> > >for
> as.vector(unlist(strsplit(str,",")),mode="list")
> > +    results[i]<-names(x.val$i)    # this does not
> work it returns a NULL (how can i convert this to
> x.val$"somevalue" ? )
> > >}
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: Allan Kamau <kamauallan at yahoo.com>
> > To: r-help at stat.math.ethz.ch
> > Sent: Thursday, July 26, 2007 10:03:17 AM
> > Subject: Re: [R] Obtaining summary of frequencies
> of value occurrences for a variable in a
> multivariate dataset.
> >
> > Thanks so much Jim, Andaikalavan, Gabor and others
> for the help and suggestions.
> > The solution will result in a matrix containing
> nested matrices to enable each variable name, each
> variables distinct value and the count of the
> distinct value to be accessible individually.
> > The main matrix will contain the variable names,
> the first level nested matrices will consist of the
> variables unique values, and each such variable
> entry will contain a one element vector to contain
> the count or occurrence frequency.
> > This matrix can now be used in comparing other
> similar datasets for variable values and their
> frequencies.
> >
> > Building on the input received so far, a probable
> solution in building the matrix will include the
> following.
> >
> >
> > 1)I reading the csv file (containing column
> headers)
> >
>
>my_data=read.table("<path/to/my/data.csv>",header=TRUE,sep=",",dec=".",fill=TRUE)
> >
> > 2)I group the values in each variable producing an
> occurrence count(frequency)
> > >x.val<-apply(my_data,2,table)
> >
> > 3)I obtain a vector of the names of the variables
> in the table
> > >names(x.val)
> >
> > 4)Now I make use of the names (obtained in step 3)
> to obtain a vector of distinct values in a given
> variable (in the example below the variable name is
> $PR14)
> > >names(v.val$PR14)
> >
> > 5)I obtain a vector (with one element) of the
> frequency of a value obtained from the step above
> (in our example the value is "V")
> > >as.vector(x.val$PR14["V"])
> >
> > Todo:
> > Now I will need to place the steps above in a
> script (consisting of loops) to build the matrix,
> step 4 and 5 seem tricky to do programatically.
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: jim holtman <jholtman at gmail.com>
> > To: Allan Kamau <kamauallan at yahoo.com>
> > Cc: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>;
> r-help at stat.math.ethz.ch
> > Sent: Wednesday, July 25, 2007 1:50:55 PM
> > Subject: Re: [R] Obtaining summary of frequencies
> of value occurrences for a variable in a
> multivariate dataset.
> >
> > Also if you want to access the individual values,
> you can just leave
> > it as a list:
> >
> > > x.val <- apply(x, 2, table)
> > > # access each value
> > > x.val$PR14["V"]
> > V
> > 8
> >
> >
> >
> > On 7/25/07, Allan Kamau <kamauallan at yahoo.com>
> wrote:
> > > A subset of the data looks as follows
> > >
> > > > df[1:10,14:20]
> > >   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> > > 1     V    T    I    K    V    G    D
> > > 2     V    S    I    K    V    G    G
> > > 3     V    T    I    R    V    G    G
> > > 4     V    S    I    K    I    G    G
> > > 5     V    S    I    K    V    G    G
> > > 6     V    S    I    R    V    G    G
> > > 7     V    T    I    K    I    G    G
> > > 8     V    S    I    K    V    E    G
> > > 9     V    S    I    K    V    G    G
> > > 10    V    S    I    K    V    G    G
> > >
> > > The result I would like is as follows
> > >
> > > PR10        PR11          PR12   ...
> > > [V:10]    [S:7,T:3]    [I:10]
> > >
> > > The result can be in a matrix or a vector and
> each variablename, value and frequency should be
> accessible so as to be used for comparisons with
> another dataset later.
> > > The frequency can be a count or a percentage.
> > >
> > >
> > > Allan.
> > >
> > >
> > > ----- Original Message ----
> > > From: Adaikalavan Ramasamy
> <ramasamy at cancer.org.uk>
> > > To: Allan Kamau <kamauallan at yahoo.com>
> > > Cc: r-help at stat.math.ethz.ch
> > > Sent: Tuesday, July 24, 2007 10:21:51 PM
> > > Subject: Re: [R] Obtaining summary of
> frequencies of value occurrences for a variable in a
> multivariate dataset.
> > >
> > > The name of the table should give you the
> "value". And if you have a
> > > matrix, you just need to convert it into a
> vector first.
> > >
> > >  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ],
> nc=3 )
> > >  > m
> > >      [,1] [,2] [,3]
> > > [1,] "A"  "C"  "B"
> > > [2,] "B"  "D"  "C"
> > > [3,] "C"  "E"  "D"
> > >  > tb <- table( as.vector(m) )
> > >  > tb
> > >
> > > A B C D E
> > > 1 2 3 2 1
> > >  > paste( names(tb), ":", tb, sep="" )
> > > [1] "A:1" "B:2" "C:3" "D:2" "E:1"
> > >
> > > If this is not what you want, then please give a
> simple example.
> > >
> > > Regards, Adai
> > >
> > >
> > >
> > > Allan Kamau wrote:
> > > > Hi all,
> > > > If the question below as been answered before
> I
> > > > apologize for the posting.
> > > > I would like to get the frequencies of
> occurrence of
> > > > all values in a given variable in a
> multivariate
> > > > dataset. In short for each variable (or field)
> a
> > > > summary of values contained with in a
> value:frequency
> 
=== message truncated ===



       
____________________________________________________________________________________
Looking for a deal? Find great prices on flights and hotels with Yahoo! FareChase.
http://farechase.yahoo.com/


From Greg.Snow at intermountainmail.org  Sat Jul 28 06:02:06 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 22:02:06 -0600
Subject: [R] Q: extracting data from lm
References: <46AA6913.8060801@gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A163@LP-EXCHVS07.CO.IHC.COM>

Marc gave some good general advice, here are a couple more things that are more specific to your problem.
 
Remember that most R functions return information, sometimes invisibly, but it is good to save the results.
This includes the summary function (all the numbers that get printed out are also returned in an object).
 
Try something like:
 
fit <- lm(nu1~nu4)
coef(fit)[2]
sfit <- summary(fit)
coef(sfit)
se.nu4 <- coef(sfit)[2,2]
 
of course some of us give into the temptation to go for terseness over readability and end up doing this like:
 
se.nu4 <- coef(summary(lm(nu1~nu4)))[2,2]
 
or
 
se.nu4 <- summary(lm(nu1~nu4))$coefficients[2,2]
 
hope this helps,

________________________________

From: r-help-bounces at stat.math.ethz.ch on behalf of D. R. Evans
Sent: Fri 7/27/2007 3:52 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Q: extracting data from lm



Warning: I am a complete newbie to R. I have read ISwR, but I am still
finding myself completely stuck on some simple concepts.

I have tried everything I can think of to solve this one, and finally
decided that enough was enough and I need a pointer to a solution.

I have the following summary from lm():

----

> summary(lm(nu1~nu4))

Call:
lm(formula = nu1 ~ nu4)

Residuals:
     Min       1Q   Median       3Q      Max
-1572.62  -150.38   -21.70   168.57  2187.84

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 29.88739   43.68881   0.684    0.494
nu4          1.00036    0.01025  97.599   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 470.9 on 298 degrees of freedom
Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16

----

But I want to access some of these numbers programmatically. I finally
figured out that to get the estimate of the nu4 coefficient I need to do:

----

> lm(nu1~nu4)$coefficients[2]
     nu4
1.000363

----

which to me as a long-time C++ programmer is close to black magic (I've
been programming since 1972; I have to say that R is unlike anything I've
ever seen, and it's far from trivial to get my head around some of it --
for example, how I could have known a priori that the above is the way to
get the nu4 coefficient is beyond me). Anyway, having figured out how to
get the estimate of the coefficient, I not-unnaturally wanted also to find
a way to access the std. error of the estimate (the value 0.01025 in the
summary). But I am completely mystified as to how to do it :-(

Any help gratefully (VERY gratefully) received, and I apologise if this is
a really, really stupid question and that the answer lies somewhere in some
documentation that I've obviously not properly taken on board.



______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



From ericlecoutre at gmail.com  Sat Jul 28 16:07:49 2007
From: ericlecoutre at gmail.com (Eric Lecoutre)
Date: Sat, 28 Jul 2007 16:07:49 +0200
Subject: [R] Combine R2HTML and Rcmd BATCH?
In-Reply-To: <469CA291.7020407@tue.nl>
References: <469CA291.7020407@tue.nl>
Message-ID: <5d897a2f0707280707p28a76dabv98a58b8581c6f28b@mail.gmail.com>

Hi Dieter,

There are two ways in R2HTML to work woth graphics.
- HTMLplot, that may require the interactive environment depending on how
you use it
- HTMLInsertGraph that is more suitable for batch scripting but requires
some more work.

First method:
HTMLplot and plotFunction (use print for trellis graphics). Not recommended.

Second method;
- Create the graphic in the format you need
- use HTMLInsertGraph to write HTML linking tag

Synopsis:

myGraphic <- file.path(outdir,"graph1.png")
png(file=myGraphic)
hist(rnorm(100))
dev.off()
HTMLInsertGraph("graph1.png",Caption="Graph1")

Do not hesitate to send me your file if more help is required

Best wishes,

Eric




2007/7/17, Dieter Vanderelst <d.vanderelst at tue.nl>:
>
> Hi All,
>
> I have an R script that spawns output in the form of an HTML page. This
> is done by the R2HTML package.
>
> Now I want to run the same script using Rcmd BATCH. However, it seems
> that it is not possible to use R2HTML in this case.
>
> My script ends with this error message:
> #########################
> Error in dev.print(png, file = AbsGraphFileName, width = Width, height =
> Height,  :
>
> can only print from screen device
>
> Execution halted
> #########################
>
> I can not find how to work around this problem in the R2HTML manual or
> the help archives.
>
> Has anybody done a similar thing before? Any suggestions?
>
> Greetings,
> Dieter
>
> --
> Dieter Vanderelst
> d.vanderelst at tue.nl
> Department of Industrial Design
> Designed Intelligence
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Eric Lecoutre
Consultant - Business & Decision
Business Intelligence & Customer Intelligence

From michael at cassin.name  Sun Jul 29 16:56:41 2007
From: michael at cassin.name (Michael Cassin)
Date: Sun, 29 Jul 2007 15:56:41 +0100
Subject: [R] Problem installing tseries package
In-Reply-To: <Pine.LNX.4.64.0707261658180.7472@gannet.stats.ox.ac.uk>
References: <b02e8b330707260709o6bb1750cm531bc73e6b315b83@mail.gmail.com>
	<Pine.LNX.4.64.0707261658180.7472@gannet.stats.ox.ac.uk>
Message-ID: <b02e8b330707290756m1949e3aag26564754caeaa258@mail.gmail.com>

Thanks, you solved it.

For posterity, here's the extra info:

R Session.info():

R version 2.4.1 (2006-12-18)
i686-redhat-linux-gnu

locale:C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

and

#uname -a
Linux stikir.com 2.6.9-023stab043.1-smp #1 SMP Mon Mar 5 16:35:19 MSK 2007
x86_64 x86_64 x86_64 GNU/Linux

So, yes, it was strange that R.i386 was installed.  I removed both and
reinstalled R.x86_64 (2.5.1) using yum. Now tseries seems to have installed
sucessfully, although it still threw many warnings.

Thanks again,

Mike

On 7/26/07, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>
> On Thu, 26 Jul 2007, Michael Cassin wrote:
>
> > Hi,
> >
> > I'm running R 2.4.1 on Fedora Core 6 and am unable to install the
> tseries
> > package.  I've resolved a few problems getting to this point, by running
> a
> > yum update, installing the gcc-gfortran dependency, but now I'm stuck.
> > Could someone please point me in the right direction?
>
> Please read the posting guide and provide the information you were asked
> for: only then we may be able to help you.
>
> You seem to have a system which installed R in /usr/lib/R but has x86_64
> components on it.  So what architecture is it that you are trying to run?
>
> My guess is that you installed a i386 RPM on a x86_64 OS.  That will
> install and R will run *but* you will not be able to use it to install
> packages.  If you installed the i386 RPM after the x86_64 one, it will
> have overwritten some crucial files including /usr/bin/R.
>
> It is possible to have i386 and x86_64 R coexisting on x86_64 Linux, but
> not by installing RPMs for different architectures.
>
>
> >
> > ========R install.packages output =======
> > ==================================
> >
> >> install.packages("tseries")
> >
> > trying URL '
> > http://www.sourcekeg.co.uk/cran/src/contrib/tseries_0.10-11.tar.gz'
> > Content type 'application/x-tar' length 182043 bytes
> > opened URL
> > ==================================================
> > downloaded 177Kb
> >
> > * Installing *source* package 'tseries' ...
> > ** libs
> > gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> > -fpic  -O3 -g -std=gnu99 -c arma.c -o arma.o
> > gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> > -fpic  -O3 -g -std=gnu99 -c bdstest.c -o bdstest.o
> > gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> > -fpic  -O3 -g -std=gnu99 -c boot.c -o boot.o
> > gfortran   -fpic  -O2 -g -c dsumsl.f -o dsumsl.o
> > In file dsumsl.f:450
> >
> >      IF (IV(1) - 2) 30, 40, 50
> >                                                                       1
> > Warning: Obsolete: arithmetic IF statement at (1)
> > In file dsumsl.f:3702
> >
> >   10 ASSIGN 30 TO NEXT
> >                                                                       1
> > Warning: Obsolete: ASSIGN statement at (1)
> > In file dsumsl.f:3707
> >
> >   20    GO TO NEXT,(30, 50, 70, 110)
> >                  1
> > Warning: Obsolete: Assigned GOTO statement at (1)
> > In file dsumsl.f:3709
> >
> >      ASSIGN 50 TO NEXT
> >                                                                       1
> > Warning: Obsolete: ASSIGN statement at (1)
> > In file dsumsl.f:3718
> >
> >      ASSIGN 70 TO NEXT
> >                                                                       1
> > Warning: Obsolete: ASSIGN statement at (1)
> > In file dsumsl.f:3724
> >
> >      ASSIGN 110 TO NEXT
> >                                                                       1
> > Warning: Obsolete: ASSIGN statement at (1)
> > In file dsumsl.f:4552
> >
> >      IF (IV(1) - 2) 999, 30, 70
> >                                                                       1
> > Warning: Obsolete: arithmetic IF statement at (1)
> > In file dsumsl.f:4714
> >
> >      IF (IRC) 140, 100, 210
> >                                                                       1
> > Warning: Obsolete: arithmetic IF statement at (1)
> > gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> > -fpic  -O3 -g -std=gnu99 -c garch.c -o garch.o
> > gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> > -fpic  -O3 -g -std=gnu99 -c ppsum.c -o ppsum.o
> > gcc -I/usr/lib/R/include -I/usr/lib/R/include  -I/usr/local/include
> > -fpic  -O3 -g -std=gnu99 -c tsutils.c -o tsutils.o
> > gcc -shared -Bdirect,--hash-stype=both,-Wl,-O1 -o tseries.so arma.o
> > bdstest.o boot.o dsumsl.o garch.o ppsum.o tsutils.o -L/usr/lib/R/lib
> -lRblas
> > -lgfortran -lm -lgcc_s -lgfortran -lm -lgcc_s -L/usr/lib/R/lib -lR
> > /usr/bin/ld: skipping incompatible /usr/lib/R/lib/libRblas.so when
> searching
> > for -lRblas
> > /usr/bin/ld: skipping incompatible /usr/lib/R/lib/libRblas.so when
> searching
> > for -lRblas
> > /usr/bin/ld: cannot find -lRblas
> > collect2: ld returned 1 exit status
> > make: *** [tseries.so] Error 1
> > ERROR: compilation failed for package 'tseries'
> > ** Removing '/usr/lib/R/library/tseries'
> >
> >
> > =============================================
> > =============================================
> >
> >
> > I presume the priority is addressing the error: "/usr/bin/ld: cannot
> find
> > -lRblas"
> >
> > I have the libRblas.so file with R 2.4. Do I need to upgrade to R 2.5 -
> In
> > which case I'll be asking how to fix the problems I'm having doing
> that  ;)
> >
> > [~]# yum provides libRblas.so
> > <snip>
> >
> > R.x86_64                                 2.5.1-2.fc6            extras
> > Matched from:
> > /usr/lib64/R/lib/libRblas.so
> > libRblas.so()(64bit)
> >
> > R.x86_64                                 2.5.1-2.fc6            extras
> > Matched from:
> > /usr/lib64/R/lib/libRblas.so
> > libRblas.so()(64bit)
> >
> > R.i386                                   2:2.4.1-1.fc6
>           installed
> > Matched from:
> > /usr/lib/R/lib/libRblas.so
> > libRblas.so
> >
> > R.x86_64                                 2.4.1-4.fc6
>             installed
> > Matched from:
> > /usr/lib64/R/lib/libRblas.so
> > libRblas.so()(64bit)
> >
> >
> > Regards,
> > Mike
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

From fqiu at gatech.edu  Sun Jul 29 22:35:51 2007
From: fqiu at gatech.edu (Feng Qiu)
Date: Sun, 29 Jul 2007 16:35:51 -0400
Subject: [R] Call R program from C++ code
Message-ID: <000001c7d220$0f4735e0$2dd5a1a0$@edu>

Hi All:

               I'm developing an application program using C++. From my C++
code, I would call some R program I have written. I' wondering if R provide
some compiler that can compile R program into executable program. I searched
R-help, there are a lot of posts talking about writing C++ code in R
program, but few about calling R from C++. 

               I might be wrong that R doesn't have complier. What I'm
trying to do is to call R program from C++ code. Any help is highly
appreciated!

 

Best regards,

 

Feng


From fqiu at gatech.edu  Mon Jul 30 01:15:21 2007
From: fqiu at gatech.edu (Feng Qiu)
Date: Sun, 29 Jul 2007 19:15:21 -0400
Subject: [R] Call R program from C++ code
In-Reply-To: <20070729221727.GA28548@master.debian.org>
References: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
	<20070729221727.GA28548@master.debian.org>
Message-ID: <000a01c7d236$56856c40$039044c0$@edu>

Hi Dirk:
	Thanks a lot. 
>> That does not exist to the best of my knowledge.
That's sad :(

>>using the system() call -- but it is also the most tedious way as you
When using System() to call R program, do I need to call some R script
program or my R function directly? 

>> A more advanced method would to use Rserve to run a 'headless' R	
Here you mean Com? I don't know much about this com frame, so ....

>> In any event, you may also want to consider the RcppTemplate package
I'll read this package, hope I can find a simple way that works in my case.

	Thanks again!

Best,

Feng

-----Original Message-----
From: Dirk Eddelbuettel [mailto:edd at master.debian.org] On Behalf Of Dirk
Eddelbuettel
Sent: 2007??7??29?? 18:17
To: Feng Qiu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Call R program from C++ code

On Sun, Jul 29, 2007 at 04:35:51PM -0400, Feng Qiu wrote:
>                I'm developing an application program using C++. From my
C++
> code, I would call some R program I have written. I' wondering if R
provide
> some compiler that can compile R program into executable program. I
searched

That does not exist to the best of my knowledge.

>                I might be wrong that R doesn't have complier. What I'm
> trying to do is to call R program from C++ code. Any help is highly
> appreciated!

As you probably know, C++ can 'call' other object code that is linked
to it.  As for 'calling R', the easiest way is to call an R script
using the system() call -- but it is also the most tedious way as you
to write the inout data to file, and then read the result data back
in.  But it is a start, and it may be easiest to debug.

A more advanced method would to use Rserve to run a 'headless' R	
service to which your C++ program can connect over the network. But
there you need to be already somewhat familiar with the underlying
C/C++ representation of R object. Rserve has simple examples.

Next, you can actually embed R inside your C++ application, but that
is more advanced.

In any event, you may also want to consider the RcppTemplate package
which has a host of examples about how to get R and C++ to work better
together (without forcing you to use C).

The 'Extending R' manual from your R installation is a good starting
point for most of this.

Hope this helps, Dirk

-- 
Three out of two people have difficulties with fractions.



From zhaodj at ioz.ac.cn  Mon Jul 30 08:18:37 2007
From: zhaodj at ioz.ac.cn (De-Jian,ZHAO)
Date: Mon, 30 Jul 2007 14:18:37 +0800 (CST)
Subject: [R] problems in limma
Message-ID: <54194.159.226.67.49.1185776317.squirrel@mail.ioz.ac.cn>

Dear list members,

I am analysing my microarray data using limma package. Now I encounter
several problems. Looking forward to your suggestions!

Question 1:
During the process of background correction using method="normexp", four
warning messages appeared as "NaNs produced in: log(x)" (as you can see in
the program posted below). What does that mean? How will it effect the
final result? How could it be settled?

Question 2:
On my microarray, every probe has two replicates.During the process of
duplicateCorrelation, two warnings appear as "Too much damping -
convergence tolerance not achievable" (as you also can see in the program
posted below). What does it mean? Is there anything wrong with my data?

Question 3:
How to construct the design matrix is a puzzle to me. Here I constructed
the design matrix using the function modelMatrix and the object targets.
However, I am not sure whether it is constructed appropriately. Looking
forward to your suggestions.
(Additional info about my experimental design. Uppercase and lowercase
words in the R object targets (see below in the posted program) have
different meanings. The locusts on the plain [PLAIN] was treated [plain]
in a simulated plateau environment while the locusts on the plateau
[PLATEAU] was treated [plateau] in a simulated plain environment. They
experienced different treatments. I think it is not a complete factorial
design. Therefore I did not choose the design matrix for factorial
designs. However, I do not know whether what I chose is appropriate.)

Question 4:
All in all, I wonder whether the differentially expressed genes produced
via the posted program are convincing. Will the above-mentioned warnings
affect the reliability of the final result? Can I continue to the next
step?

Thanks!

Dejian Zhao

++++++++++++++++++ Program Starts +++++++++++++++++++++

> library(limma)
> library(statmod) #duplicateCorrelation requires this package.
> targets<-readTargets()
> targets
       Cy3     Cy5       FileName      Date
1    PLAIN PLATEAU Locust 186.gpr 2006-5-31
2    PLAIN PLATEAU Locust 187.gpr 2006-5-31
3    PLAIN PLATEAU Locust 188.gpr 2006-5-31
4    PLAIN PLATEAU Locust 189.gpr 2006-5-31
5    PLAIN PLATEAU Locust 190.gpr 2006-5-31
6    PLAIN PLATEAU Locust 191.gpr 2006-5-31
7    plain   PLAIN Locust 192.gpr  2006-6-6
8    plain   PLAIN Locust 193.gpr  2006-6-6
9    plain   PLAIN Locust 194.gpr  2006-6-6
10   plain   PLAIN Locust 195.gpr  2006-6-6
11   plain   PLAIN Locust 196.gpr  2006-6-6
12   plain   PLAIN Locust 197.gpr  2006-6-6
13 plateau PLATEAU Locust 198.gpr  2006-6-8
14 plateau PLATEAU Locust 199.gpr  2006-6-8
15 plateau PLATEAU Locust 200.gpr  2006-6-8
16 plateau PLATEAU Locust 201.gpr  2006-6-8
17 plateau PLATEAU Locust 202.gpr  2006-6-8
18 plateau PLATEAU Locust 203.gpr  2006-6-8
> RG<-read.maimages(targets,source="genepix",wt.fun=wtflags(0.1))
Read Locust 186.gpr
Read Locust 187.gpr
Read Locust 188.gpr
Read Locust 189.gpr
Read Locust 190.gpr
Read Locust 191.gpr
Read Locust 192.gpr
Read Locust 193.gpr
Read Locust 194.gpr
Read Locust 195.gpr
Read Locust 196.gpr
Read Locust 197.gpr
Read Locust 198.gpr
Read Locust 199.gpr
Read Locust 200.gpr
Read Locust 201.gpr
Read Locust 202.gpr
Read Locust 203.gpr
> RG$genes<-readGAL()
> spottypes<-readSpotTypes()
> spottypes
    SpotType     ID Name  Color
1       gene      *    *  black
2      blank  Blank    *  brown
3     buffer    *sc    *   blue
4       rice Os026*    *  green
5 beta-actin  Beta*    *    red
6        18S   18S*    * yellow
7      GAPDH GAPDH*    * purple
> RG$genes$Status<-controlStatus(spottypes,RG)
Matching patterns for: ID Name
Found 19200 gene
Found 96 blank
Found 220 buffer
Found 192 rice
Found 192 beta-actin
Found 96 18S
Found 96 GAPDH
Setting attributes: values Color
> RG.b<-backgroundCorrect(RG,method="normexp",offset=0)
Corrected array 1
Corrected array 2
Corrected array 3
Corrected array 4
Corrected array 5
Corrected array 6
Corrected array 7
Corrected array 8
Corrected array 9
Corrected array 10
Corrected array 11
Corrected array 12
Corrected array 13
Corrected array 14
Corrected array 15
Corrected array 16
Corrected array 17
Corrected array 18
Warning messages:
1: NaNs produced in: log(x)
2: NaNs produced in: log(x)
3: NaNs produced in: log(x)
4: NaNs produced in: log(x)
> w<-modifyWeights(RG$weights,RG$genes$Status,c("rice","beta-actin","18S","GAPDH"),c(0.1,2,2,2))
MA.p<-normalizeWithinArrays(RG.b,weights=w,iterations=6)
> design<-modelMatrix(targets,ref="PLAIN")
Found unique target names:
 plain PLAIN plateau PLATEAU
> design
      plain plateau PLATEAU
 [1,]     0       0       1
 [2,]     0       0       1
 [3,]     0       0       1
 [4,]     0       0       1
 [5,]     0       0       1
 [6,]     0       0       1
 [7,]    -1       0       0
 [8,]    -1       0       0
 [9,]    -1       0       0
[10,]    -1       0       0
[11,]    -1       0       0
[12,]    -1       0       0
[13,]     0      -1       1
[14,]     0      -1       1
[15,]     0      -1       1
[16,]     0      -1       1
[17,]     0      -1       1
[18,]     0      -1       1
> i<-MA.p$genes$Status=="gene"
> corMA.pi<-duplicateCorrelation(MA.p[i,],design,ndups=2)
Warning messages:
1: Too much damping - convergence tolerance not achievable in:
glmgam.fit(dx, dy, start = start, tol = tol, maxit = maxit, trace = trace)

2: Too much damping - convergence tolerance not achievable in:
glmgam.fit(dx, dy, start = start, tol = tol, maxit = maxit, trace = trace)

> fitMA.pi<-lmFit(MA.p[i,],design,ndups=2,correlation=corMA.pi$consensus.correlation)
contrast.matrix<-makeContrasts(plain,plateau-PLATEAU,PLATEAU,levels=design)
contrast.matrix
         Contrasts
Levels    plain plateau - PLATEAU PLATEAU
  plain       1                 0       0
  plateau     0                 1       0
  PLATEAU     0                -1       1
> colnames(contrast.matrix)<-cbind("plain-PLAIN","plateau-PLATEAU","PLATEAU-PLAIN")
contrast.matrix
         Contrasts
Levels    plain-PLAIN plateau-PLATEAU PLATEAU-PLAIN
  plain             1               0             0
  plateau           0               1             0
  PLATEAU           0              -1             1
> fit2MA.pi<-contrasts.fit(fitMA.pi,contrast.matrix)
> fit2MA.pi<-eBayes(fit2MA.pi)
> resultsi.001.fc2=decideTests(fit2MA.pi,method="nestedF",adjust.method="BH",p.value=0.001,lfc=log2(2))
write.fit(fit2MA.pi,
results=decideTests(fit2MA.pi,method="nestedF",adjust.method="BH",p.value=0.001,lfc=1),
"fit2MA.pi.nestedF.adj_BH.P_001.FC_2.csv", digits=4, adjust="BH",
sep=",")
> save.image("result.RData")

++++++++++++++++++ Program Ends +++++++++++++++++++++








-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: program.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/71513898/attachment.txt 

From Nathalie.Cornileau at csiro.au  Mon Jul 30 08:12:24 2007
From: Nathalie.Cornileau at csiro.au (Nathalie.Cornileau at csiro.au)
Date: Mon, 30 Jul 2007 16:12:24 +1000
Subject: [R] correlation and matrix
Message-ID: <9C2327A8CA66F3498C699BBE46F82FE524B772@exnswn2-syd.nexus.csiro.au>

Dear everyone,
 
I am new in R and I've got difficulties in realizing the following
tasks:
-I have variables (factors) with different numbers of levels, either 1,
2 or 3.
-I have a matrix containing these 204 factors and I have to correlate
them by groups of 4 variables.
-I have to delete the factors just having one level ( because when
correlating one-level factors, the output is NA)
 
here is my code:
lst<-seq(1, 204, by=12) % there are 12 factors for 17 natural resources
for (n in lst)
{ 
Mx<- matrix(0, byrow = F, ncol = 4, nrow=nrow(dta)) % I extract the 4
factors I have to correlate and I'd like to do it for each n
{if (nlevels(dta[,n+4])!=1) 
Mx[,1]<-dta[,n+4]
else
Mx[,1]<-NA}
{if (nlevels(dta[,n+5])!=1) 
Mx[,2]<-dta[,n+5]
else
Mx[,2]<-NA}
{if (nlevels(dta[,n+7])!=1) 
Mx[,3]<-dta[,n+7]
else
Mx[,3]<-NA}
{if (nlevels(dta[,n+8])!=1) 
Mx[,4]<-dta[,n+8]
else
Mx[,4]<-NA}
p<-0                % I compute the number of non - NA columns and I'd
like to delete the Na columns from that matrix
 
for (i in 1:4)
{
if(!is.na(sum(Mx[,i])>0)) p<-p+1   
}
print(p)
{if (p==0 | p==1) stop("computation impossible")
  else {
  r<-0
  for (i in 1:4)
{
if(is.na(sum(Mx[,i])>0))  r<-i
}
print(r)
print(cor((as.matrix(Mx[,-r])), use="complete.obs", method="spearman"))
}
}
} %The problem is the last step doesn't work for p==2.
 In fact, it seems the loop for doesn't work either.
 
I hope it is clear enough and I thank you in advance for your help.
Nathalie

From niederlein-rstat at yahoo.de  Mon Jul 30 10:00:25 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Mon, 30 Jul 2007 10:00:25 +0200
Subject: [R] how to combine data of several csv-files
Message-ID: <46AD9A99.2010303@yahoo.de>

Hello,

I'm looking for a solution for the following problem:

1) I have a folder with several csv files; each contains a set of 
measurement values
2) The measurements of each file belong to a position in a two 
dimensional matrix (lets say "B02.csv" belongs to position 2,2
3) The size of the matrix is fix
4) I cannot assure to have a csv file for each position
5) Each position belongs to one category; This information is available 
in a file (means 2,2 and 2,3 may belong to category "c1"; 3,2 and 3,3 
may belong to category "c2")

Now, I process each available file and get a vector of 6 values or NA back.

The aim is to calculate mean and sd for vectors (element wise) coming 
from the same category (means if vec1 <- c(1,2,3,4,5,6) and vec2 <- 
c(6,7,8,9,10,11) belong to the same category, I would like to get mean 
<- c(3.5, 4.5, 5.5, 6.5, 7.5, 8.5))

... but I'm not sure how to proceed. I end up with a list containing 
these vectors for each processed file and I don't know how to combine 
them easily...

Does anybody have a suggestion for me?

What I've got so far:

folder <- choose.dir(getwd(), "Choose folder containing csv files")
setwd(folder)

rowString <- LETTERS[1:8]; cols <- 12

mat <- outer(rowString, formatC(seq(2,length=cols), flag = "0", width = 
2), paste, sep = "")
mat <- paste(mat, ".csv", sep = "_")

layoutfilename <- file.choose()
layoutfile <- read.csv(layoutfilename, sep=";", header=F, na.strings = "")

classmatrix <- sapply(layoutfile,as.character)
classes <- factor(classmatrix)

colnames(classmatrix) <- c(1:cols)
rownames(classmatrix) <- rowString

ret <- sapply(mat, calcHist)


From tom.olsson at dnbnor.com  Mon Jul 30 10:01:36 2007
From: tom.olsson at dnbnor.com (Tom.O)
Date: Mon, 30 Jul 2007 01:01:36 -0700 (PDT)
Subject: [R] Create Strings of Column Id's
In-Reply-To: <644e1f320707261152k6db5131bh3a2f5040ffc9f3ea@mail.gmail.com>
References: <11816439.post@talk.nabble.com>
	<644e1f320707261152k6db5131bh3a2f5040ffc9f3ea@mail.gmail.com>
Message-ID: <11859539.post@talk.nabble.com>


Great, That did the trick,thanks.

regards Tom


jholtman wrote:
> 
> Is this what you want:
> 
>> paste("-", paste(colnames(MyMatrix)[COL], collapse='-'), sep='')
> [1] "-E-T"
> 
> 
> On 7/26/07, Tom.O <tom.olsson at dnbnor.com> wrote:
>>
>> Does anyone know how this is don?
>>
>> I have a large matrix where I extract specific columns into txt files for
>> further use. To be able to keep track of which txt files contain which
>> columns I want to name the filenames with the column Id's.
>>
>> The most basic example would be to use an for() loop together with
>> paste(),
>> but the result is blank. Not even NULL.
>>
>> this is the concept of thecode i use:
>>
>> for example
>>
>> MyMatrix <-
>> matrix(NA,ncol=4,nrow=1,dimnames=list(NULL,c("E","R","T","Y")))
>> COL <- c(1,3) # a vector of columns I want to extract,
>>
>> Filename <- NULL # the starting variable, so I can use paste
>> Filename <- for(i in colnames(MyMatrix)[COL])
>> {paste(Filename,"-",i,sep="")}
>>
>> The result is "-T", but I want it to be "-E-T"
>>
>> Anyone have a clue?
>>
>> Thanks Tom
>>
>>
>> --
>> View this message in context:
>> http://www.nabble.com/Create-Strings-of-Column-Id%27s-tf4153354.html#a11816439
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 
> -- 
> Jim Holtman
> Cincinnati, OH
> +1 513 646 9390
> 
> What is the problem you are trying to solve?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Create-Strings-of-Column-Id%27s-tf4153354.html#a11859539
Sent from the R help mailing list archive at Nabble.com.


From Jacques.Veslot at avignon.inra.fr  Mon Jul 30 10:16:57 2007
From: Jacques.Veslot at avignon.inra.fr (Jacques VESLOT)
Date: Mon, 30 Jul 2007 10:16:57 +0200
Subject: [R] correlation and matrix
In-Reply-To: <9C2327A8CA66F3498C699BBE46F82FE524B772@exnswn2-syd.nexus.csiro.au>
References: <9C2327A8CA66F3498C699BBE46F82FE524B772@exnswn2-syd.nexus.csiro.au>
Message-ID: <46AD9E79.9010605@avignon.inra.fr>

it should be smth like that:

apply(sapply(seq(1, 204, by=12), seq, length=4), 2, function(x)
 {
 M <- dta[,x]
 z <- sapply(M, nlevels) # if dta is a dataframe
 if (sum(z==1)<3) cor(as.matrix(M[,z!=0]), use="comp", method="spear") 
else NA
 })  

Jacques VESLOT

INRA - Biostatistique & Processus Spatiaux
Site Agroparc 84914 Avignon Cedex 9, France

Tel: +33 (0) 4 32 72 21 58
Fax: +33 (0) 4 32 72 21 84



Nathalie.Cornileau at csiro.au a ?crit :
> Dear everyone,
>  
> I am new in R and I've got difficulties in realizing the following
> tasks:
> -I have variables (factors) with different numbers of levels, either 1,
> 2 or 3.
> -I have a matrix containing these 204 factors and I have to correlate
> them by groups of 4 variables.
> -I have to delete the factors just having one level ( because when
> correlating one-level factors, the output is NA)
>  
> here is my code:
> lst<-seq(1, 204, by=12) % there are 12 factors for 17 natural resources
> for (n in lst)
> { 
> Mx<- matrix(0, byrow = F, ncol = 4, nrow=nrow(dta)) % I extract the 4
> factors I have to correlate and I'd like to do it for each n
> {if (nlevels(dta[,n+4])!=1) 
> Mx[,1]<-dta[,n+4]
> else
> Mx[,1]<-NA}
> {if (nlevels(dta[,n+5])!=1) 
> Mx[,2]<-dta[,n+5]
> else
> Mx[,2]<-NA}
> {if (nlevels(dta[,n+7])!=1) 
> Mx[,3]<-dta[,n+7]
> else
> Mx[,3]<-NA}
> {if (nlevels(dta[,n+8])!=1) 
> Mx[,4]<-dta[,n+8]
> else
> Mx[,4]<-NA}
> p<-0                % I compute the number of non - NA columns and I'd
> like to delete the Na columns from that matrix
>  
> for (i in 1:4)
> {
> if(!is.na(sum(Mx[,i])>0)) p<-p+1   
> }
> print(p)
> {if (p==0 | p==1) stop("computation impossible")
>   else {
>   r<-0
>   for (i in 1:4)
> {
> if(is.na(sum(Mx[,i])>0))  r<-i
> }
> print(r)
> print(cor((as.matrix(Mx[,-r])), use="complete.obs", method="spearman"))
> }
> }
> } %The problem is the last step doesn't work for p==2.
>  In fact, it seems the loop for doesn't work either.
>  
> I hope it is clear enough and I thank you in advance for your help.
> Nathalie
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jo.irisson at gmail.com  Mon Jul 30 10:17:53 2007
From: jo.irisson at gmail.com (jiho)
Date: Mon, 30 Jul 2007 10:17:53 +0200
Subject: [R] apply, lapply and data.frame in R 2.5
Message-ID: <793E7A25-1B4A-4ED0-872E-9CFC9C6D3831@gmail.com>

Hello everyone,

A recent (in 2.5 I suspect) change in R is giving me trouble. I want  
to apply a function (tolower) to all the columns of a data.frame and  
get a data.frame in return.
Currently, on a data.frame, both apply (for arrays) and lapply (for  
lists) work, but each returns its native class (resp. matrix and list):

apply(mydat,2,tolower) 	# gives a matrix
lapply(mydat,tolower)	# gives a list
and
sapply(mydat,tolower)	# gives a matrix

If I remember well, apply did not used to work on data.frames and  
lapply returned a data.frame when it was provided with one, with the  
same properties (columns classes etc). At least this is what my code  
written with R 2.4.* suggests.

The solution would be:
as.data.frame(apply(mydat,2,tolower))
or
as.data.frame(lapply(mydat,tolower))

But this does not keep columns attributes (all columns are  
reinterpreted, for example strings are converted to factors etc). For  
my particular use stringsAsFactors=FALSE does what I need, but I am  
wondering wether there is a more general solution to apply a function  
on all elements of a data.frame and get a similar data.frame in  
return. Indeed data.frames are probably the most common object in R  
and applying a function to each of its columns/variables appears to  
me as something one would want to do quite often.

Thank you in advance.

JiHO
---
http://jo.irisson.free.fr/


From j.logsdon at quantex-research.com  Mon Jul 30 10:28:15 2007
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Mon, 30 Jul 2007 09:28:15 +0100
Subject: [R] Slightly OT - use of R
Message-ID: <200707300928.15405.j.logsdon@quantex-research.com>

I am trying to get a measure of how R compares in usage as a statistical 
platform compared to other software.  I would guess it is the most widely 
used among statisticians at least by virtue of it being open source.

But is there any study to which I can refer?  By asking this list I am not 
exactly adopting a rigorous approach!  

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


From wl2776 at gmail.com  Mon Jul 30 11:15:38 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 30 Jul 2007 02:15:38 -0700 (PDT)
Subject: [R] Call R program from C++ code
In-Reply-To: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
References: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
Message-ID: <11860280.post@talk.nabble.com>


"Writing R Extensions" manual contains chapters dedicated to parsing and
evaluating of the R extensions from C.
Also, I vaguely remember I've seen something like "Embedding R" somewhere in
manuals.
R can be compiled as a shared library object, that you can dynamically load
from your application and use its functions.
R doesn't have a compiler, it's the interpreted language. 
However, it can parse a character string representing an expresstion and
transform it into the internal form, ready for evaluation.


Feng Qiu wrote:
> 
> Hi All:
> 
>                I'm developing an application program using C++. From my
> C++
> code, I would call some R program I have written. I' wondering if R
> provide
> some compiler that can compile R program into executable program. I
> searched
> R-help, there are a lot of posts talking about writing C++ code in R
> program, but few about calling R from C++. 
> 
>                I might be wrong that R doesn't have complier. What I'm
> trying to do is to call R program from C++ code. Any help is highly
> appreciated!
> 

-- 
View this message in context: http://www.nabble.com/Call-R-program-from-C%2B%2B-code-tf4167083.html#a11860280
Sent from the R help mailing list archive at Nabble.com.


From ted.harding at nessie.mcc.ac.uk  Mon Jul 30 11:18:42 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 Jul 2007 10:18:42 +0100 (BST)
Subject: [R] Slightly OT - use of R
In-Reply-To: <200707300928.15405.j.logsdon@quantex-research.com>
Message-ID: <XFMail.070730101842.ted.harding@nessie.mcc.ac.uk>

On 30-Jul-07 08:28:15, John Logsdon wrote:
> I am trying to get a measure of how R compares in usage as a
> statistical platform compared to other software. I would guess
> it is the most widely used among statisticians at least by
> virtue of it being open source.
> 
> But is there any study to which I can refer? By asking this
> list I am not exactly adopting a rigorous approach!

I don't know about that -- my own expectation would be that
serious users of R are likely to be subscribers to the list.

So maybe a good answer to your question would be the number
of subscribers (which I'm sure Martin Maechler can find out).
Of course, some people will have subscribed under more than
one email address, so that would somewhat over-estimate the
number of people who subscribe. But it can be traded off
(to a somewhat unknown extent) against R users who do not
subscribe.

More to the point, though, is what you mean by "usage".
If you simply mean "people who use", that's a matter of
counting (one way or another). But there's "use" and "use".

There's a lot of what I call "SatNav Statistics" being done,
and I would guess that "SatNav statisticians" tend to go
for the commercial products, since these have bigger and
brighter displays, and the more mellifluous and reassuring
voice-overs. (And never mind that the voice instructs you
to turn left, at the level-crossing, onto the railway line).

Most serious R users, I tend to think, are more likely to
pull into a layby and unfold large-scale maps. And, when
the need arises, they will get out and push.

So, in "widely used among statisticians", it depends on
what you mean by "statisticians".

Where you will will probably get extra value from the R list
is that many of our people will have extensive and very
professional experience, not only with R, but with many of
the other available packages, and be best placed to provide
serious and thoughtful comparisons.

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at manchester.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-Jul-07                                       Time: 10:18:21
------------------------------ XFMail ------------------------------


From xnyang at seu.edu.cn  Mon Jul 30 11:45:40 2007
From: xnyang at seu.edu.cn (xinan yang)
Date: Mon, 30 Jul 2007 17:45:40 +0800
Subject: [R] g++ verfsion
In-Reply-To: <385786448.24199@eyou.net>
References: <46AD601F.7040909@seu.edu.cn> <385786448.24199@eyou.net>
Message-ID: <46ADB344.8000806@seu.edu.cn>

Hi, Li,

Thanks for reply.

It is strange that I used RBGL_1.6.0 successfully on the same linux 
system with R 2.3.1 before!

After trying to install R 2.5.1, the RBGL_1.12.0 failed to be installed.

Then, I uninstalled R.   Delete the directory R/lib/.
Installed R.2.3.1 again. And run getBioC1.8.5 for R 2.3.1 to download 
the three packages
graph_1.10.6, RBGL_1.12.0 and Ruuid_1.10.0.

But still failed as:
......
   boost::vec_adj_list_vertex_id_map<boost::property<boost::vertex_color_t,
   boost::default_color_type, boost::no_property>, size_t>&)'
make: *** [interfaces.o] Error 1
chmod: failed to get attributes of 
`/usr/local/lib/R/library/RBGL/libs/*': No such file or directory
ERROR: compilation failed for package 'RBGL'
** Removing '/usr/local/lib/R/library/RBGL'


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Is it maybe because I use redhat9 with libxml2-2.5.4-1, which doesn't 
neet the not meets the requirment of RBGL_1.12.0 ?

But I do not know how to updata them, even after downloading the  file 
libxml2-2.5.4-3.rh9.i386.rpm,
I failed to install it:

[root at yxnlinux software]# rpm -i libxml2-2.5.4-3.rh9.i386.rpm
        file /usr/bin/xmlcatalog from install of libxml2-2.5.4-3.rh9 
conflicts with file from package libxml2-2.5.4-1
        file /usr/bin/xmllint from install of libxml2-2.5.4-3.rh9 
conflicts with file from package libxml2-2.5.4-1
        file /usr/lib/libxml2.so.2.5.4 from install of 
libxml2-2.5.4-3.rh9 conflicts with file from package libxml2-2.5.4-1
        file /usr/share/man/man1/xmlcatalog.1.gz from install of 
libxml2-2.5.4-3.rh9 conflicts with file from package libxml2-2.5.4-1
        file /usr/share/man/man1/xmllint.1.gz from install of 
libxml2-2.5.4-3.rh9 conflicts with file from package libxml2-2.5.4-1
        file /usr/share/man/man3/libxml.3.gz from install of 
libxml2-2.5.4-3.rh9 conflicts with file from package libxml2-2.5.4-1

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
FOr redhat9, Is it possible to updat gcc?

Thanks,


Xinan




Li.Long at isb-sib.ch wrote:

>This gcc is VERY old, we saw some problems in template handling with
>different version of g++.
>
>I would suggest to try a newer version of g++ and see what happens.
>
>Li
>
>
>  
>
>>and,
>>
>>[root at yxnlinux getBioC2.5-R2.5.1]# g++ --version
>>g++ (GCC) 3.2.2 20030222 (Red Hat Linux 3.2.2-5)
>>Copyright (C) 2002 Free Software Foundation, Inc.
>>
>>tks,
>>
>>xinan
>>
>>    
>>
>
>
>
>  
>


From kamauallan at yahoo.com  Mon Jul 30 11:32:21 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Mon, 30 Jul 2007 02:32:21 -0700 (PDT)
Subject: [R] Matrix nesting (was Re: Obtaining summary of frequencies of
	value occurrences for a variable in a multivariate dataset.)
Message-ID: <901618.79783.qm@web53510.mail.re2.yahoo.com>

Hi

	
	
	
	
	
	
	<!--
		@page { size: 21cm 29.7cm; margin: 2cm }
		P { margin-bottom: 0.21cm }
	-->
	

I would like to nest matrices, is there
a way of doing so, I am getting ?number of items to replace is not
a multiple of replacement length? errors (probably R is trying to
flatten the matrix into a vector and complains if the vector is
larger than 1 element during the insert)

I have a matrix (see below) in which I
would like to place one other matrices in to each k[2,i] position
(where i is value between 1 to 4)

Why ? each value in k[1,i] may
represent several (1or more) key-value results which I would like to
capture in the corresponding k[2,i] element.





>k

                [,1]   [,2]   [,3]  
[,4]

myVariableNames "PR10" "PR11"
"PR12" "PR13"

x2              "0"    "0"
   "0"    "0"

>

>





Allan.



----- Original Message ----
From: Allan Kamau <kamauallan at yahoo.com>
To: jim holtman <jholtman at gmail.com>
Cc: r-help at stat.math.ethz.ch
Sent: Saturday, July 28, 2007 2:48:47 PM
Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.

Hi Jim,
The problem description.
I am trying to identify mutations in a given gene from
a particular genome (biological genome sequence).
I have two CSV files consisting of sequences. One file
consists of reference (documented,curated accepted as
standard) sequences. The other consists of sample
sequences I am trying to identify mutations within. In
both files the an individual sequence is contained in
a single record, it?s amino acid residues ( the actual
sequence of alphabets each representing a given amino
acid for example ?A? stands for ?Alanine?, ?C? for
Cysteine and so on) are each allocated a single field
in the CSV file.
The sequences in both files have been well aligned,
each contain 115 residues with the first residue is
contained in the field 5. The fields 1 to 4 are
allocated for metadata (name of sequence and so on).
My task is to compile a residue occurrence count for
each residue present in a given field in the reference
sequence dataset and use this information when reading
each sequence in the sample dataset to identify a
mutation. For example for position 9 of the sample
sequence ?bb? a ?P? is found and according to our
reference sequence dataset of summaries, at position 9
?P? may not even exist or may have an occurrence of
10% or so will be classified as mutation, (I could
employ a cut of parameter for mutation
classification).


Allan.

--- jim holtman <jholtman at gmail.com> wrote:

> results=()#character()
> myVariableNames=names(x.val)
> results[length(myVariableNames)]<-NA
> 
> for (i in myVariableNames){
>     results[i]<-names(x.val[[i]])    # this does not
> work it returns a
> NULL (how can i convert this to x.val$"somevalue" ?
> )
> }
> 
> 
> 
> On 7/27/07, Allan Kamau <kamauallan at yahoo.com>
> wrote:
> > Hi All,
> > I am having difficulties finding a way to find a
> substitute to the command "names(v.val$PR14)" so
> that I could generate the command on the fly for all
> PR14 to PR200 (please see the previous discussion
> below to understand what the object x.val contains)
> . I have tried the following
> >
> > >results=()#character()
> > >myVariableNames=names(x.val)
> > >results[length(myVariableNames)]<-NA
> >
> > >for
> as.vector(unlist(strsplit(str,",")),mode="list")
> > +    results[i]<-names(x.val$i)    # this does not
> work it returns a NULL (how can i convert this to
> x.val$"somevalue" ? )
> > >}
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: Allan Kamau <kamauallan at yahoo.com>
> > To: r-help at stat.math.ethz.ch
> > Sent: Thursday, July 26, 2007 10:03:17 AM
> > Subject: Re: [R] Obtaining summary of frequencies
> of value occurrences for a variable in a
> multivariate dataset.
> >
> > Thanks so much Jim, Andaikalavan, Gabor and others
> for the help and suggestions.
> > The solution will result in a matrix containing
> nested matrices to enable each variable name, each
> variables distinct value and the count of the
> distinct value to be accessible individually.
> > The main matrix will contain the variable names,
> the first level nested matrices will consist of the
> variables unique values, and each such variable
> entry will contain a one element vector to contain
> the count or occurrence frequency.
> > This matrix can now be used in comparing other
> similar datasets for variable values and their
> frequencies.
> >
> > Building on the input received so far, a probable
> solution in building the matrix will include the
> following.
> >
> >
> > 1)I reading the csv file (containing column
> headers)
> >
>
>my_data=read.table("<path/to/my/data.csv>",header=TRUE,sep=",",dec=".",fill=TRUE)
> >
> > 2)I group the values in each variable producing an
> occurrence count(frequency)
> > >x.val<-apply(my_data,2,table)
> >
> > 3)I obtain a vector of the names of the variables
> in the table
> > >names(x.val)
> >
> > 4)Now I make use of the names (obtained in step 3)
> to obtain a vector of distinct values in a given
> variable (in the example below the variable name is
> $PR14)
> > >names(v.val$PR14)
> >
> > 5)I obtain a vector (with one element) of the
> frequency of a value obtained from the step above
> (in our example the value is "V")
> > >as.vector(x.val$PR14["V"])
> >
> > Todo:
> > Now I will need to place the steps above in a
> script (consisting of loops) to build the matrix,
> step 4 and 5 seem tricky to do programatically.
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: jim holtman <jholtman at gmail.com>
> > To: Allan Kamau <kamauallan at yahoo.com>
> > Cc: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>;
> r-help at stat.math.ethz.ch
> > Sent: Wednesday, July 25, 2007 1:50:55 PM
> > Subject: Re: [R] Obtaining summary of frequencies
> of value occurrences for a variable in a
> multivariate dataset.
> >
> > Also if you want to access the individual values,
> you can just leave
> > it as a list:
> >
> > > x.val <- apply(x, 2, table)
> > > # access each value
> > > x.val$PR14["V"]
> > V
> > 8
> >
> >
> >
> > On 7/25/07, Allan Kamau <kamauallan at yahoo.com>
> wrote:
> > > A subset of the data looks as follows
> > >
> > > > df[1:10,14:20]
> > >   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> > > 1     V    T    I    K    V    G    D
> > > 2     V    S    I    K    V    G    G
> > > 3     V    T    I    R    V    G    G
> > > 4     V    S    I    K    I    G    G
> > > 5     V    S    I    K    V    G    G
> > > 6     V    S    I    R    V    G    G
> > > 7     V    T    I    K    I    G    G
> > > 8     V    S    I    K    V    E    G
> > > 9     V    S    I    K    V    G    G
> > > 10    V    S    I    K    V    G    G
> > >
> > > The result I would like is as follows
> > >
> > > PR10        PR11          PR12   ...
> > > [V:10]    [S:7,T:3]    [I:10]
> > >
> > > The result can be in a matrix or a vector and
> each variablename, value and frequency should be
> accessible so as to be used for comparisons with
> another dataset later.
> > > The frequency can be a count or a percentage.
> > >
> > >
> > > Allan.
> > >
> > >
> > > ----- Original Message ----
> > > From: Adaikalavan Ramasamy
> <ramasamy at cancer.org.uk>
> > > To: Allan Kamau <kamauallan at yahoo.com>
> > > Cc: r-help at stat.math.ethz.ch
> > > Sent: Tuesday, July 24, 2007 10:21:51 PM
> > > Subject: Re: [R] Obtaining summary of
> frequencies of value occurrences for a variable in a
> multivariate dataset.
> > >
> > > The name of the table should give you the
> "value". And if you have a
> > > matrix, you just need to convert it into a
> vector first.
> > >
> > >  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ],
> nc=3 )
> > >  > m
> > >      [,1] [,2] [,3]
> > > [1,] "A"  "C"  "B"
> > > [2,] "B"  "D"  "C"
> > > [3,] "C"  "E"  "D"
> > >  > tb <- table( as.vector(m) )
> > >  > tb
> > >
> > > A B C D E
> > > 1 2 3 2 1
> > >  > paste( names(tb), ":", tb, sep="" )
> > > [1] "A:1" "B:2" "C:3" "D:2" "E:1"
> > >
> > > If this is not what you want, then please give a
> simple example.
> > >
> > > Regards, Adai
> > >
> > >
> > >
> > > Allan Kamau wrote:
> > > > Hi all,
> > > > If the question below as been answered before
> I
> > > > apologize for the posting.
> > > > I would like to get the frequencies of
> occurrence of
> > > > all values in a given variable in a
> multivariate
> > > > dataset. In short for each variable (or field)
> a
> > > > summary of values contained with in a
> value:frequency
> 
=== message truncated ===



       
____________________________________________________________________________________



______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rithesh.m at brickworkindia.com  Mon Jul 30 09:36:07 2007
From: rithesh.m at brickworkindia.com (Rithesh M. Mohan)
Date: Mon, 30 Jul 2007 13:06:07 +0530
Subject: [R] help with ROC curve
Message-ID: <4BC833C82CD3564298A3A24BD299C0C8B8510B@pioneer.brickworkindia.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/d5c7d92c/attachment.pl 

From Jiqiu.Cheng at biw.kuleuven.be  Mon Jul 30 11:42:44 2007
From: Jiqiu.Cheng at biw.kuleuven.be (Jiqiu Cheng)
Date: Mon, 30 Jul 2007 11:42:44 +0200
Subject: [R] random number generator in batch jobs
Message-ID: <20070730114244.ktwuangus9x4w8s4@webmail3.kuleuven.be>

Dear sir,
   I want to submit R batch jobs (e.g. 5) under the linux cluster by  
the script file "do_mul".
The script file "do_mul"
"
#!/bin/bash
export var
for var in $(seq 1 5)
do
   qsub -v var do_test
done
exit 0
"
Through "do_mul", 5 "do_test" script files are submitted to the cluster.
The script file "do_test":
"
#!/bin/bash -l
#PBS -l ncpus=1
#PBS -l walltime=0:05:00
cd $PBS_O_WORKDIR
mkdir test$var
cd test$var
module load R/2.5.0
R --vanilla< test
exit 0
"
The content in R file "test" is :
"rm(list=ls(all=TRUE))
sample(10)
"
I expect to have different samples each time. However, for these 5  
replications, the first 3 jobs giving me the same samples and the last  
2 are the same. I'm confused because I already used "R --vanilla" to  
avoid loading same workspace each time and "rm(list=ls(all=TRUE))" to  
remove the same random seed each time. Why do same samples still  
happen among 5 replications? Does anybody have some ideas to solve  
this problem? Looking forward to your reply, thanks.

Regards,
Jiqiu

Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm


From andza at osi.lv  Mon Jul 30 12:09:59 2007
From: andza at osi.lv (Andris Jankevics)
Date: Mon, 30 Jul 2007 13:09:59 +0300
Subject: [R] Bind together two vectors of different length...
Message-ID: <200707301309.59325.andza@osi.lv>

Dear everyone,

I've got difficulties in realizing the following
task:

I have two vectors:
A <- c(1:10)
B<- seq(1,10,2)

Now I want to make a table form vectors A and B as rows, and if a value of A 
isn't present B, then I want to put a N/A symbol in it:

Output should look like this:

1 2 3 4 5 6 7 8 9 10 
1 0 3 0 5 0 7 0 9 0

How can I do this in R?

Thank you.

-- 
Andris Jankevics
Assistant
Department of Medicinal Chemistry
Latvian Institute of Organic Synthesis
Aizkraukles 21, LV-1006, Riga, Latvia


From ripley at stats.ox.ac.uk  Mon Jul 30 12:13:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jul 2007 11:13:47 +0100 (BST)
Subject: [R] Slightly OT - use of R
In-Reply-To: <XFMail.070730101842.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070730101842.ted.harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.64.0707301104530.14090@gannet.stats.ox.ac.uk>

On Mon, 30 Jul 2007, ted.harding at nessie.mcc.ac.uk wrote:

> On 30-Jul-07 08:28:15, John Logsdon wrote:
>> I am trying to get a measure of how R compares in usage as a
>> statistical platform compared to other software. I would guess
>> it is the most widely used among statisticians at least by
>> virtue of it being open source.

I don't think that is the main reason.  Most of the R users I know 
migrated from commercial statistical software for reasons other than cost.
(Cross-platform availability has been one major reason.)

>> But is there any study to which I can refer? By asking this
>> list I am not exactly adopting a rigorous approach!
>
> I don't know about that -- my own expectation would be that
> serious users of R are likely to be subscribers to the list.
>
> So maybe a good answer to your question would be the number
> of subscribers (which I'm sure Martin Maechler can find out).
> Of course, some people will have subscribed under more than
> one email address, so that would somewhat over-estimate the
> number of people who subscribe. But it can be traded off
> (to a somewhat unknown extent) against R users who do not
> subscribe.

I think it would be a seriously biased estimate.
Few of our hundreds of student users will be subscribed to R-help 
(since their first port of call for help is local).
Also, we get quite a lot of postings via the gmane and nabble gateways.

> More to the point, though, is what you mean by "usage".
> If you simply mean "people who use", that's a matter of
> counting (one way or another). But there's "use" and "use".

Indeed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Mon Jul 30 12:20:23 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jul 2007 11:20:23 +0100 (BST)
Subject: [R] apply, lapply and data.frame in R 2.5
In-Reply-To: <793E7A25-1B4A-4ED0-872E-9CFC9C6D3831@gmail.com>
References: <793E7A25-1B4A-4ED0-872E-9CFC9C6D3831@gmail.com>
Message-ID: <Pine.LNX.4.64.0707301115010.14090@gannet.stats.ox.ac.uk>

On Mon, 30 Jul 2007, jiho wrote:

> Hello everyone,
>
> A recent (in 2.5 I suspect) change in R is giving me trouble. I want
> to apply a function (tolower) to all the columns of a data.frame and
> get a data.frame in return.
> Currently, on a data.frame, both apply (for arrays) and lapply (for
> lists) work, but each returns its native class (resp. matrix and list):
>
> apply(mydat,2,tolower) 	# gives a matrix
> lapply(mydat,tolower)	# gives a list
> and
> sapply(mydat,tolower)	# gives a matrix

which is exactly what R 2.0.0 did, so no recent(ish) change at all.

> If I remember well, apply did not used to work on data.frames and
> lapply returned a data.frame when it was provided with one, with the
> same properties (columns classes etc). At least this is what my code
> written with R 2.4.* suggests.

apply has coerced data frames for many years and lapply always returned a 
list.  The solution has always been

mydat[] <- lapply(mydat,tolower)


> The solution would be:
> as.data.frame(apply(mydat,2,tolower))
> or
> as.data.frame(lapply(mydat,tolower))
>
> But this does not keep columns attributes (all columns are
> reinterpreted, for example strings are converted to factors etc). For
> my particular use stringsAsFactors=FALSE does what I need, but I am
> wondering wether there is a more general solution to apply a function
> on all elements of a data.frame and get a similar data.frame in
> return. Indeed data.frames are probably the most common object in R
> and applying a function to each of its columns/variables appears to
> me as something one would want to do quite often.
>
> Thank you in advance.
>
> JiHO
> ---
> http://jo.irisson.free.fr/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From petr.pikal at precheza.cz  Mon Jul 30 12:33:42 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Mon, 30 Jul 2007 12:33:42 +0200
Subject: [R] Odp:  Bind together two vectors of different length...
In-Reply-To: <200707301309.59325.andza@osi.lv>
Message-ID: <OFE4719186.BCEC16DD-ONC1257328.00395E6D-C1257328.0039F534@precheza.cz>

Hi
r-help-bounces at stat.math.ethz.ch napsal dne 30.07.2007 12:09:59:

> Dear everyone,
> 
> I've got difficulties in realizing the following
> task:
> 
> I have two vectors:
> A <- c(1:10)
> B<- seq(1,10,2)
> 
> Now I want to make a table form vectors A and B as rows, and if a value 
of A 
> isn't present B, then I want to put a N/A symbol in it:
> 
> Output should look like this:
> 
> 1 2 3 4 5 6 7 8 9 10 
> 1 0 3 0 5 0 7 0 9 0
> 
> How can I do this in R?

in your particular case

rbind(A,A*(A %in% B))

will give you such output, but 0 is not NA thereofore 

> AO<-A*(A %in% B)
> AO[!(A %in% B)]<-NA
> rbind(A, AO)
   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
A     1    2    3    4    5    6    7    8    9    10
AO    1   NA    3   NA    5   NA    7   NA    9    NA

gives you such output but with NA values instead of zeroes

Regards
Petr

> 
> Thank you.
> 
> -- 
> Andris Jankevics
> Assistant
> Department of Medicinal Chemistry
> Latvian Institute of Organic Synthesis
> Aizkraukles 21, LV-1006, Riga, Latvia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Mon Jul 30 12:27:31 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jul 2007 11:27:31 +0100 (BST)
Subject: [R] random number generator in batch jobs
In-Reply-To: <20070730114244.ktwuangus9x4w8s4@webmail3.kuleuven.be>
References: <20070730114244.ktwuangus9x4w8s4@webmail3.kuleuven.be>
Message-ID: <Pine.LNX.4.64.0707301121480.14090@gannet.stats.ox.ac.uk>

Have you read the help page?

      Initially, there is no seed;  a new one is created from the
      current time when one is required.  Hence, different sessions will
      give different simulation results, by default.

Thus if you choose to launch processes on different machines at the 
same time you will get the same random number stream.

Running random number streams for parallel computation is a (very) 
specialized topic and you need to be aware of the literature.  I will 
point out packages rsprng and accuracy (function runifS).

On Mon, 30 Jul 2007, Jiqiu Cheng wrote:

> Dear sir,
>   I want to submit R batch jobs (e.g. 5) under the linux cluster by
> the script file "do_mul".
> The script file "do_mul"
> "
> #!/bin/bash
> export var
> for var in $(seq 1 5)
> do
>   qsub -v var do_test
> done
> exit 0
> "
> Through "do_mul", 5 "do_test" script files are submitted to the cluster.
> The script file "do_test":
> "
> #!/bin/bash -l
> #PBS -l ncpus=1
> #PBS -l walltime=0:05:00
> cd $PBS_O_WORKDIR
> mkdir test$var
> cd test$var
> module load R/2.5.0
> R --vanilla< test
> exit 0
> "
> The content in R file "test" is :
> "rm(list=ls(all=TRUE))
> sample(10)
> "
> I expect to have different samples each time. However, for these 5
> replications, the first 3 jobs giving me the same samples and the last
> 2 are the same. I'm confused because I already used "R --vanilla" to
> avoid loading same workspace each time and "rm(list=ls(all=TRUE))" to
> remove the same random seed each time. Why do same samples still
> happen among 5 replications? Does anybody have some ideas to solve
> this problem? Looking forward to your reply, thanks.
>
> Regards,
> Jiqiu
>
> Disclaimer: http://www.kuleuven.be/cwis/email_disclaimer.htm
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From nshephard at gmail.com  Mon Jul 30 12:35:36 2007
From: nshephard at gmail.com (Neil Shephard)
Date: Mon, 30 Jul 2007 10:35:36 +0000 (UTC)
Subject: [R] Slightly OT - use of R
References: <200707300928.15405.j.logsdon@quantex-research.com>
Message-ID: <loom.20070730T123103-411@post.gmane.org>

John Logsdon <j.logsdon <at> quantex-research.com> writes:

> 
> I am trying to get a measure of how R compares in usage as a statistical 
> platform compared to other software.  I would guess it is the most widely 
> used among statisticians at least by virtue of it being open source.
> 
> But is there any study to which I can refer?  By asking this list I am not 
> exactly adopting a rigorous approach!  
> 

Not sure what your definition of usage is in this instance (user-base v's
usability v's reliability/accuracy) but the following may be of interest...

Kellie B. Keeling and Robert J. Pavur, A comparative study of the reliability of
nine statistical software packages,
Computational Statistics & Data Analysis, Volume 51, Issue 8, 1 May 2007, Pages
3811-3831.
(http://www.sciencedirect.com/science/article/B6V8V-4JHMGWJ-1/2/77a29a95c2071997f13fcca7267711d1)

There is also some discussion in the R-help archive, and a small amount
scattered around in the statalist archives (the two statistical software mailing
lists to which I subscribe).

Search the R-help list at http://search.r-project.org/nmz.html and statalist
archives at http://www.stata.com/statalist/archvies/

HTH's

Neil

"In mathematics you don't understand things. You just get used to
them."  - Johann von Neumann

Email - nshephard at gmail.com / n.shephard at sheffield.ac.uk
Website - http://slack.ser.man.ac.uk/
Photos - http://www.flickr.com/photos/slackline/


From pinard at iro.umontreal.ca  Mon Jul 30 12:44:29 2007
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Mon, 30 Jul 2007 06:44:29 -0400
Subject: [R] Bind together two vectors of different length...
In-Reply-To: <200707301309.59325.andza@osi.lv>
References: <200707301309.59325.andza@osi.lv>
Message-ID: <20070730104429.GA5272@phenix.progiciels-bpi.ca>

[Andris Jankevics]

>I have two vectors:
>A <- c(1:10)
>B<- seq(1,10,2)

>Now I want to make a table form vectors A and B as rows, and if a value of A 
>isn't present B, then I want to put a N/A symbol in it:

>Output should look like this:

>1 2 3 4 5 6 7 8 9 10 
>1 0 3 0 5 0 7 0 9 0

>How can I do this in R?

Either of:

  A[!A %in% B] <- NA
  A[!A %in% B] <- 0

depending on what you want your N/A symbol to be.

-- 
Fran?ois Pinard   http://pinard.progiciels-bpi.ca


From gyadav at ccilindia.co.in  Mon Jul 30 12:58:17 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Mon, 30 Jul 2007 16:28:17 +0530
Subject: [R] ROC curve in R
In-Reply-To: <4BC833C82CD3564298A3A24BD299C0C8B8511F@pioneer.brickworkindia.
	local>
Message-ID: <OF1FFBA804.570639AB-ON65257328.0038C969-65257328.003C44FD@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/8d4e038b/attachment.pl 

From niederlein-rstat at yahoo.de  Mon Jul 30 13:09:34 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Mon, 30 Jul 2007 13:09:34 +0200
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46ADB142.9090502@unifi.it>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
Message-ID: <46ADC6EE.4090900@yahoo.de>

Hello,

sorry for this confusion but I don't know a better way to explain...
I have no problems to read in the files and to process them. I end up 
with a list of results like this:

 > ret
$A02.csv
[1] NA

$B02.csv
[1] 89.130435  8.695652  2.173913  0.000000  0.000000  0.000000  9.892473

$C02.csv
[1] 86.842105 10.526316  2.631579  0.000000  0.000000  0.000000 10.026385

$D02.csv
[1] 85.000000 10.000000  5.000000  0.000000  0.000000  0.000000  4.474273

$E02.csv
[1] 70.786517 13.483146  7.865169  5.617978  2.247191  0.000000 12.125341

$F02.csv
[1] 70.83333 14.16667 10.00000  2.50000  2.50000  0.00000 17.26619

$G02.csv
[1] 64.772727 13.636364  7.954545 11.363636  2.272727  0.000000 12.735166

$H02.csv
[1] NA

$A03.csv
[1] NA

and I have a matrix with categories like this:


 > classmatrix
   1  2
A NA NA
B NA "cat1"
C NA "cat1"
D NA "cat1"
E NA "cat2"
F NA "cat2"
G NA "cat2"
H NA NA


Now, I'm looking for a way to calculate the mean element wise for all 
results coming from the same category:

in this case the mean of the elements:

$B02.csv
$C02.csv
$D02.csv

(belonging to "cat1")

I just don't know, how to combine the result list with the categories...

Does it become clearer? Probably, I try to provide a simple example but 
this will take some time to prepare...

Thanks anyway!

Antje




8rino-Luca Pantani schrieb:
> I'm unclear to what it is your problem.
> Import files into data frame?
> Combine them in one dataframe?
> Some (written) examples of the files would help people to help you out.
> 
> An example on how to get help better and faster
>  >>>>>>>>>>>>
> I have several csv files in the following form
> V1 V2
> 1   4
> 0.3   56
> ................
> V1   V2
> 2.5   25
> 4.5  45
> .....................
> 
> I would like to import them in only one dataframe, and then recode a 
> column in order to get
> V1 V2 V3
> 1   4   file1
> 0.3   56   file1
> 2.5   25   file2
> 4.5  45   file2
> .....................
>  >>>>>>>>>>>>
> Antje ha scritto:
>> Hello,
>>
>> I'm looking for a solution for the following problem:
>>
>> 1) I have a folder with several csv files; each contains a set of 
>> measurement values
>>   
>


From Peter.Ruckdeschel at uni-bayreuth.de  Mon Jul 30 13:35:15 2007
From: Peter.Ruckdeschel at uni-bayreuth.de (Peter Ruckdeschel)
Date: Mon, 30 Jul 2007 13:35:15 +0200
Subject: [R] [R-pkgs] New versions for the distr-family of packages and
	of	package startupmsg
Message-ID: <46ADCCF3.4010207@uni-bayreuth.de>

We would like to announce the availability on CRAN (with possibly a minor delay until on
every mirror) of new versions of our packages in the "distrXXX"-family (version 1.9),
i.e.; "distr", "distrEx", "distrSim", "distrTEst", and "distrDoc"
      as well as of package for managing startup messages, "startupmsg" (0.5).
[all of them require R >= 2.2.0]
-----------------------------------------------------------------------------------------
********************************* Changes ***********************************************
of "distr" (1.9), "distrEx" (1.9), "distrSim" (1.9), "distrTEst" (1.9), "distrDoc" (1.9)
*****************************************************************************************
-----------------------------------------------------------------------------------------
There are major changes in "distr" and "distrEx" from this release on;
the more important ones can be inspected at

http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR

and the pages linked to on this page.

Special thanks go to Spencer Graves for spotting some errors in 1.8 (which
should be fixed by now) and to G.Jay Kerns for detecting some further bugs
and providing code for exact kurtosis and skewness functionals.

After package installation you may also have a look at NEWS("<pkg-name>") for each of the
packages mentioned in this mail.
-----------------------------------------------------------------------------------------
********************************* Changes ***********************************************
of "startupmsg" (0.5)
*****************************************************************************************
-----------------------------------------------------------------------------------------
This may be interesting to those annoyed by our "chatty" startup messages ;-)

-> From this version on, you may use suppressPackageStartupMessages() to suppress the
   startup-messages issued by our packages---

compare http://tolstoy.newcastle.edu.au/R/e2/devel/07/04/3039.html
-----------------------------------------------------------------------------------------
Short Descriptions
-----------------------------------------------------------------------------------------
************ "distr":
"distr" is to provide a conceptual treatment of random variables
(r.v.'s) by means of S4--classes. A virtual mother class "Distribution"
is introduced.
All distributions of the "stats" package are implemented as subclasses of
either "AbscontDistribution" or "DiscreteDistribution".

Using these classes, we also provide (default) methods to automatically
generate the image distributions under unary mathematical operations as
well as a general convolution algorithm.
-----------------------------------------------------------------------------------------
************ "distrSim":
Classes and methods are provided for a standardized treatment of
simulations (also under contaminations) .
-----------------------------------------------------------------------------------------
************ "distrTEst":
Classes and methods are provided for a standardized treatment of
the evaluation of statistical procedures (up to now only estimators)
at data/simulations
-----------------------------------------------------------------------------------------
************ "distrEx":
This package provides some extensions to package "distr" like:
* extreme value distribution classes,
* expectations
+in the form E(X) for the expectation of X where X is some
distribution or
+in the form E(X,f) for the expectation of f(X) where X is
some distribution and f some function in X,
* further functionals: var, sd, IQR, mad, median, kurtosis, skewness
* truncated moments
* distances between distributions
(Hellinger, Kolmogorov, total variation, "convex contamination")
* conditional distributions in factorized form
* conditional expectations in factorized form
-----------------------------------------------------------------------------------------
************ "distrDoc":
"distrDoc" provides a common vignette to the distrXXX family
-----------------------------------------------------------------------------------------
************ "startupmsg":
provides utilities for start-up messages for packages
-----------------------------------------------------------------------------------------

We look forward to receiving questions, comments and suggestions

Peter Ruckdeschel
Matthias Kohl
Thomas Stabla
Florian Camphausen

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From rithesh.m at brickworkindia.com  Mon Jul 30 13:41:54 2007
From: rithesh.m at brickworkindia.com (Rithesh M. Mohan)
Date: Mon, 30 Jul 2007 17:11:54 +0530
Subject: [R] ROC curve in R
Message-ID: <4BC833C82CD3564298A3A24BD299C0C8B85227@pioneer.brickworkindia.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/a0b35893/attachment.pl 

From jo.irisson at gmail.com  Mon Jul 30 13:54:38 2007
From: jo.irisson at gmail.com (jiho)
Date: Mon, 30 Jul 2007 13:54:38 +0200
Subject: [R] apply, lapply and data.frame in R 2.5
In-Reply-To: <Pine.LNX.4.64.0707301115010.14090@gannet.stats.ox.ac.uk>
References: <793E7A25-1B4A-4ED0-872E-9CFC9C6D3831@gmail.com>
	<Pine.LNX.4.64.0707301115010.14090@gannet.stats.ox.ac.uk>
Message-ID: <953911C4-B04F-40B3-8174-18A8C4427BF5@gmail.com>


On 2007-July-30  , at 12:20 , Prof Brian Ripley wrote:
> On Mon, 30 Jul 2007, jiho wrote:
>> A recent (in 2.5 I suspect) change in R is giving me trouble. I want
>> to apply a function (tolower) to all the columns of a data.frame and
>> get a data.frame in return.
>> Currently, on a data.frame, both apply (for arrays) and lapply (for
>> lists) work, but each returns its native class (resp. matrix and  
>> list):
>>
>> apply(mydat,2,tolower) 	# gives a matrix
>> lapply(mydat,tolower)	# gives a list
>> and
>> sapply(mydat,tolower)	# gives a matrix
>
> which is exactly what R 2.0.0 did, so no recent(ish) change at all.
>
>> If I remember well, apply did not used to work on data.frames and
>> lapply returned a data.frame when it was provided with one, with the
>> same properties (columns classes etc). At least this is what my code
>> written with R 2.4.* suggests.
>
> apply has coerced data frames for many years and lapply always  
> returned a list.  The solution has always been
>
> mydat[] <- lapply(mydat,tolower)

sorry about that, my previous code was misleading and indeed your  
code above does exactly what I need. I should have tested this a bit  
further before posting. I was just afraid to install two different R  
versions I guess.
thank you again.

JiHO
---
http://jo.irisson.free.fr/


From D.GOUACHE at arvalisinstitutduvegetal.fr  Mon Jul 30 13:58:38 2007
From: D.GOUACHE at arvalisinstitutduvegetal.fr (GOUACHE David)
Date: Mon, 30 Jul 2007 13:58:38 +0200
Subject: [R] regular expressions : extracting numbers
Message-ID: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>

Hello all,

I have a vector of character strings, in which I have letters, numbers, and symbols. What I wish to do is obtain a vector of the same length with just the numbers.
A quick example -

extract of the original vector :
"lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb" "rb" "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"

and the type of thing I wish to end up with :
"2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""

or, instead of "", NA would be acceptable (actually it would almost be better for me)

Anyways, I've been battling with gsub() and things of the sort, but I'm drowning in the regular expressions, despite a few hours of looking at Perl tutorials...
So if anyone can help me out, it would be greatly appreciated!!

In advance, thanks very much.

David Gouache
Arvalis - Institut du V?g?tal
Station de La Mini?re
78280 Guyancourt
Tel: 01.30.12.96.22 / Port: 06.86.08.94.32


From Horace.Tso at pgn.com  Fri Jul 27 18:52:06 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 27 Jul 2007 09:52:06 -0700
Subject: [R] getting the name of variables passed to a function
Message-ID: <46A9C046020000650000777F@pgn.com>

Folks,

I've entered into an R programming territory I'm not very familiar with, thus this probably very elementary question concerning the mechanic of a function call.

I want to know from within a function the name of the variables I pass down. The function makes use of the "..." to allow for multiple unknown arguments,

myfun = function(...) { do something }

In the body I put,

{    
nm <- names(list(...))
nm
}

When the function is called with two vectors x, and y

myfun(x, y)

It returns NULL. However, when the call made is,

>myfun(x=x, y=y)

The result is
[1] "x" "y"

Question : how do i get the names of the unknown variables without explicitly saying x=x...

Thanks in advance.

Horace


From jholtman at gmail.com  Fri Jul 27 19:19:50 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 27 Jul 2007 13:19:50 -0400
Subject: [R] get() with complex objects?
In-Reply-To: <46AA14A7.9070803@columbia.edu>
References: <46AA14A7.9070803@columbia.edu>
Message-ID: <644e1f320707271019r10e9039bk5badc2e2bfa0fa81@mail.gmail.com>

'get' tries to retrieve the object given by the character string.  The
error message says that object can not be found.  You actually have to
'evaluate' the character string.  See the example below:

> x <- data.frame(a=1:10, b=11:20)
> x$a
 [1]  1  2  3  4  5  6  7  8  9 10
> z <- 'x$a'
> get(z)
Error in get(x, envir, mode, inherits) : variable "x$a" was not found
> # parse and evaluate the character string 'x$a'
> eval(parse(text=z))
 [1]  1  2  3  4  5  6  7  8  9 10

Does this make sense?


On 7/27/07, Mark Orr <mo2259 at columbia.edu> wrote:
> Hello R-listers,
> I'm having trouble accessing "sub" objects ("attributes"?), e.g.,
> "x$silinfo$avg.width" using the /get() /command;  I'm using/ get()/ in a
> loop as illustrated in the following code:
>
> #FIRST MAKE CLUSTERS of VARYING  k
> /for (i in 1:300){
>  assign(paste("x.",i,sep=""),pam(x,i))  #WORKS FINE
> }/
>
> #NEXT, TAKE LOOK AT AVE. SILHOUETTE VALUE FOR EACH k
>
> #PART 1, MAKE LIST OF OBJECTS NEEDED
> /gen.list <- rep("t",300)
> for (i in 1:300){
>  assign(gen.list[i],paste("x.",i,"$silinfo$avg.width",sep=""))
> }
> #WORKS FINE
>
> /#PART 2, USE LIST IN LOOP TO ACCESS OBJECT.
> /si//l.collector <- rep(99,300)
> for(i in 1:300){
>  sil.collector <- get(gen.list[i])
> }/
> #HERE IS THE ERROR
> /Error in get(x, envir, mode, inherits) : variable
> "x.1$silinfo$avg.width" was not found
>
> /So, I get the gist of this error; x.1 is an object findable from get(),
> but the "attribute"  levels are not accessible.  Any suggestions on how
> to get get() to access these levels?  From reading the get()'s help
> page, I don't think it will access the attributes. (my apologies for
> loosely using the term attributes, but I hope it is clear).
>
> Thanks,
>
> Mark Orr
>
> --
> ***********************************************
> Mark G. Orr, PhD
> Heilbrunn Dept. of Population and Family Health
> Columbia University
> 60 Haven Ave., B-2
> New York, NY 10032
>
> Tele: 212-304-7823
> Fax:  212-305-7024
>
> www.columbia.edu/~mo2259
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From tlumley at u.washington.edu  Fri Jul 27 18:45:58 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 27 Jul 2007 09:45:58 -0700 (PDT)
Subject: [R] reading stata files: preserving values of variables
 converted to factors
In-Reply-To: <46A93E8A.7080906@fastmail.fm>
References: <46A93E8A.7080906@fastmail.fm>
Message-ID: <Pine.LNX.4.64.0707270943230.19542@homer22.u.washington.edu>

On Thu, 26 Jul 2007, Ben Saylor wrote:

> Hi,
>
> I am a Stata user new to R.  I am using read.dta to read a Stata file
> that has variables with value labels.  read.dta converts them to
> factors, but seems to recode them with values from 1 to <number of
> factor levels> (looking at the output of unclass(<varname>)), so the
> original numerical values are lost.

Yes. The R factor type should not be used if you want the original levels. 
It is not a 'labelled numeric' type and the numbers are an implementation 
detail.

>  Using convert.factors=FALSE
> preserves the values, but seems to discard the labels.

It doesn't discard the labels. They are kept in the attributes of the data 
frame.

 	-thomas


From amnakhan493 at gmail.com  Fri Jul 27 19:00:37 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Fri, 27 Jul 2007 10:00:37 -0700
Subject: [R] How to edit L-moment Ratio Diagram
Message-ID: <3ffd3bb60707271000q758a352bid1e5950198c8b24b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/1e9fb63e/attachment.pl 

From Greg.Snow at intermountainmail.org  Fri Jul 27 20:57:47 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 12:57:47 -0600
Subject: [R] plot
In-Reply-To: <989527a20707271109w365c85f3qb8b4b775c805099b@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF2B8C@LP-EXCHVS07.CO.IHC.COM>

Can you really see much of the data in a 26*31 3d barplot?  It seems like most info would be hidden behind the first few rows and it would be so cluttered that you would not be able to make out much of anything from it.

Why not try a line plot instead (year as the x axis, each region a different year).  Here is a quick example:

> data(votes.repub, package='cluster')
> matplot( t(votes.repub[1:31, 1:26]), type='l')
> 

Even better would be to group some of the regions together and use xyplot from the lattice package and have a panel for each group of regions (fewer lines per panel should be easier to see detail).

You could also use the image function (or levelplot from lattice) to create a 26*31 grid with colors used for the 3rd dimension (can be good for overall patterns, not so good for looking at detail).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: Dong GUO ???? [mailto:guo.dong99 at gmail.com] 
> Sent: Friday, July 27, 2007 12:09 PM
> To: Greg Snow
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] plot
> 
> Many thanks, Greg and Justin.
> 
> The matrix is just a 26*31 matrix - 26 years, 31 regions. I 
> am know to R, just dont know how to attach the data here yet..
> 
> As I have such matrices for nine indicators for all regions, 
> so i could show some differences by 3D plot, which I did 
> similar things in Excel. I am sure there is a way to do it in R
> 
> On 7/27/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> > Graphs that rely on 3-d effects tend to distort the data 
> rather than enlighten the viewer.  If your goal is to distort 
> the data (which I doubt), then most of us don't want to help. 
>  On the other hand, if you really do want to enlighten the 
> viewer (even if that is just you), then tell us what your 
> data is like and what you want to learn from it, and we will 
> be happy to give you advice on creating useful graphs.
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dong GUO ??
> > > Sent: Friday, July 27, 2007 8:45 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] plot
> > >
> > > Greetings to the group,
> > >
> > > I would like to know if some one could help me with plot 
> 3-d column 
> > > graph of a matrix (3-d column graph in Excel).
> > >
> > > Many thanks in advance.
> > >
> > > Regards,
> > > Dong
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
>


From shitao at hotmail.com  Fri Jul 27 19:25:19 2007
From: shitao at hotmail.com (Tao Shi)
Date: Fri, 27 Jul 2007 17:25:19 +0000
Subject: [R] Creating windows binary R package (PowerArchiver vs. zip
 -r9X)
Message-ID: <BAY120-W28888A6757C258C1DFD63FC7F30@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/5b02ed23/attachment.pl 

From guo.dong99 at gmail.com  Fri Jul 27 20:09:29 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Fri, 27 Jul 2007 20:09:29 +0200
Subject: [R] plot
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBAF2AF1@LP-EXCHVS07.CO.IHC.COM>
References: <989527a20707270745h541fed48kb286f9e408a4c522@mail.gmail.com>
	<07E228A5BE53C24CAD490193A7381BBBAF2AF1@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <989527a20707271109w365c85f3qb8b4b775c805099b@mail.gmail.com>

Many thanks, Greg and Justin.

The matrix is just a 26*31 matrix - 26 years, 31 regions. I am know to
R, just dont know how to attach the data here yet..

As I have such matrices for nine indicators for all regions, so i
could show some differences by 3D plot, which I did similar things in
Excel. I am sure there is a way to do it in R

On 7/27/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> Graphs that rely on 3-d effects tend to distort the data rather than enlighten the viewer.  If your goal is to distort the data (which I doubt), then most of us don't want to help.  On the other hand, if you really do want to enlighten the viewer (even if that is just you), then tell us what your data is like and what you want to learn from it, and we will be happy to give you advice on creating useful graphs.
>
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dong GUO ??
> > Sent: Friday, July 27, 2007 8:45 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] plot
> >
> > Greetings to the group,
> >
> > I would like to know if some one could help me with plot 3-d
> > column graph of a matrix (3-d column graph in Excel).
> >
> > Many thanks in advance.
> >
> > Regards,
> > Dong
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>


From ld7631 at gmail.com  Fri Jul 27 20:35:39 2007
From: ld7631 at gmail.com (Dimitri Liakhovitski)
Date: Fri, 27 Jul 2007 14:35:39 -0400
Subject: [R] Looping through all possible combinations of cases
Message-ID: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>

Hello!

I have a regular data frame (DATA) with 10 people and 1 column
('variable'). Its cases are people with names ('a', 'b', 'c', 'd',
'e', 'f', etc.). I would like to write a function that would sum up
the values on 'variable' of all possible combinations of people, i.e.

1. I would like to write a loop - in such a way that it loops through
each possible pair of cases (i.e., ab, ac, ad, etc.) and sums up their
respective values on 'variable'

2. I would like to write a loop - in such a way that it loops through
each possible trio of cases (i.e., abc, abd, abe, etc.) and sums up
their respective values on 'variable'.

3.  I would like to write a loop - in such a way that it loops through
each possible quartet of cases (i.e., abcd, abce, abcf, etc.) and sums
up their respective values on 'variable'.

etc.

Then, at the end I want to capture all possible combinations that were
considered (i.e., what elements were combined in it) and get the value
of the sum for each combination.

How should I do it?
Thanks a lot!
Dimitri


From bensaylor at fastmail.fm  Fri Jul 27 21:03:51 2007
From: bensaylor at fastmail.fm (Ben Saylor)
Date: Fri, 27 Jul 2007 11:03:51 -0800
Subject: [R] reading stata files: preserving values of variables
 converted to factors
In-Reply-To: <Pine.LNX.4.64.0707270943230.19542@homer22.u.washington.edu>
References: <46A93E8A.7080906@fastmail.fm>
	<Pine.LNX.4.64.0707270943230.19542@homer22.u.washington.edu>
Message-ID: <46AA4197.8020208@fastmail.fm>

Thanks for the clarification.  I found the labels & values in

attributes(<dataframe>)$label.table$<varname>

which looks the same whether convert.factors is TRUE or FALSE.

Ben

Thomas Lumley wrote:
> On Thu, 26 Jul 2007, Ben Saylor wrote:
> 
>> Hi,
>>
>> I am a Stata user new to R.  I am using read.dta to read a Stata file
>> that has variables with value labels.  read.dta converts them to
>> factors, but seems to recode them with values from 1 to <number of
>> factor levels> (looking at the output of unclass(<varname>)), so the
>> original numerical values are lost.
> 
> Yes. The R factor type should not be used if you want the original 
> levels. It is not a 'labelled numeric' type and the numbers are an 
> implementation detail.
> 
>>  Using convert.factors=FALSE
>> preserves the values, but seems to discard the labels.
> 
> It doesn't discard the labels. They are kept in the attributes of the 
> data frame.
> 
>     -thomas


From bolker at ufl.edu  Fri Jul 27 21:28:30 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 27 Jul 2007 19:28:30 +0000 (UTC)
Subject: [R] R codes for g-and-h distribution
References: <687475.90110.qm@web54005.mail.re2.yahoo.com>
Message-ID: <loom.20070727T212305-710@post.gmane.org>

filame uyaco <filams0704 <at> yahoo.com> writes:

> 
> 
> hi!
> 
> I would like to ask help how to generate numbers from g-and-h distribution. 
This distribution is like
> normal distribution  but span more of the kurtosis and skewness plane. Has R
any package on how to generate
> them? 
> 

  Someone else asked for this in 2005, but I didn't see any 
answers.  If the wiki weren't down I would put this up there ...

## http://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/ghpdf.htm

## G(p,g,h) = exp((g*Zp)-1)*exp((h*Zp^2)/2)/g

[n.b. formatting on this page is very confusing, I inserted
 parentheses]

##  with Zp denoting the normal percent point function of p. When g = 0
##  and h = 0, the g-and-h distribution reduces to a standard normal
##  distribution.

## The g-and-h probability density function is computed by taking the
## numerical derivative of the cumulative distribution function (which is
## turn computed by numerically inverting the percent point function
## using a bisection method).

## [from
## http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm:
## The percent point function (ppf) is the inverse of the cumulative
## distribution function. For this reason, the percent point function
## is also commonly referred to as the inverse distribution function.

## thus R's "q" functions are percent point functions

qgh <- function(q,g,h) {
  Zp <- qnorm(q)
  ## not vectorized!
  if (g==0) Zp else (exp(g*Zp)-1)*exp((h*Zp^2/2))/g
}

## since the quantile function is defined, it makes generating
##  random values easy!
rgh <- function(n,g,h) {
  qgh(runif(n),g,h)
}

## eps sets distance from 1 for searching
##  strategy could probably be improved (first approx. based on
##   qnorm??)
pgh <- function(p,g,h,eps=1e-7) {
  uniroot(function(z) { qgh(z,g,h) - p}, interval=c(eps,1-eps))$root
}

dgh <- function(x,g,h,log=FALSE,ndep=1e-3,...) {
  ## crude vectorization in x (not g or h)
  if (length(x)>1) return(sapply(x,dgh,g=g,h=h,log=log,ndep=ndep,...))
  r <- (pgh(x+ndep,g,h)-pgh(x,g,h))/ndep
  if (log) log(r) else r
}
  

## examples ...
set.seed(1001)
## should be approx. normal
r1 = rgh(10000,1e-5,0)
r2 = rgh(10000,1e-5,0)
r3 = rgh(10000,1e-5,0)

## plot 3 different samples
plot(density(r1))
lines(density(r2))
lines(density(r3))
curve(dnorm(x),col=2,add=TRUE)
curve(dgh(x,1e-5,0),col=4,add=TRUE)
## some slight numerical glitches -- e.g. around
## x=-2.6

r4 = rgh(50000,0.4,0.4)
plot(density(r4,n=1024),xlim=c(-10,10))
curve(dnorm(x),add=TRUE,col=2)
curve(dgh(x,0.4,0.4),add=TRUE,col=4)
legend("topleft",c("density","g-h","normal"),
       lty=rep(1,3),col=c(1,4,2))
## note glitch again, this time around
## y=-3.89


From gonzalezmorales at gmail.com  Fri Jul 27 21:28:11 2007
From: gonzalezmorales at gmail.com (Luis Gerardo Gonzalez Morales)
Date: Fri, 27 Jul 2007 15:28:11 -0400
Subject: [R] Calling R functions from a Microsoft SQL Server stored procedure
Message-ID: <c4c1e9860707271228n45dfbde5kde4a1dbb69290e01@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/f64ae863/attachment.pl 

From thomas.harte at yahoo.com  Fri Jul 27 22:44:40 2007
From: thomas.harte at yahoo.com (Thomas Harte)
Date: Fri, 27 Jul 2007 13:44:40 -0700 (PDT)
Subject: [R]  error in using R2WinBUGS on Ubuntu 6.10 Linux
Message-ID: <97753.85074.qm@web43137.mail.sp1.yahoo.com>

what version of Wine are you running? and is this the patched (1.4.1) version of 
WinBUGS that you're trying to run?

cheers,

thomas.

>  Date: Thu, 26 Jul 2007 12:03:38 -0400 (EDT)
>  From: <meyerjp at jmu.edu>
>  Subject: [R] error in using R2WinBUGS on Ubuntu 6.10 Linux
>  To: r-help at stat.math.ethz.ch
>  Message-ID: <20070726120338.AUQ96591 at mpmail1.jmu.edu>
>  Content-Type: text/plain; charset=us-ascii

>  I am trying to run WinBUGS 1.4 from the Ubuntu 6.10 Linux distribution. I am using the
>   R2WinBUGS packages with the  source file listed below. WinBUGS appears to run
properly, but
>   I get the following message after WinBUGS starts in WINE. Does anyone know what may
be
>   causing this error and what the correction may be?

>  Thanks

>  ERROR MESSAGE:

>  fixme:ole:GetHGlobalFromILockBytes cbSize is 13824
>  err:ole:CoGetClassObject class {0003000a-0000-0000-c000-000000000046} not registered
>  err:ole:CoGetClassObject class {0003000a-0000-0000-c000-000000000046} not registered
>  err:ole:CoGetClassObject no class object {0003000a-0000-0000-c000-000000000046} could
be
>   created for context 0x3
>  fixme:keyboard:RegisterHotKey (0x10032,13,0x00000002,3): stub
>  fixme:ntdll:RtlNtStatusToDosErrorNoTeb no mapping for 8000000a
>  err:ole:local_server_thread Failure during ConnectNamedPipe 317



>  R SOURCE FILE:

>  rm(list=ls(all=TRUE))

>  library(R2WinBUGS)

>  inits<-function(){
>  	list(alpha0 = 0, alpha1 = 0, alpha2 = 0, alpha12 = 0, sigma = 1)
>  }

>  data<-list(r = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10,   8, 10,   8, 23, 0,  3,
22,
>   15, 32, 3),
>  n = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51,
7),
>  x1 = c(0,   0,  0,   0,   0, 0,   0,   0,  0,   0,   0,  1,   1,   1,   1, 1,   1,  1,
 
>   1,   1, 1),
>  x2 = c(0,   0,  0,   0,   0, 1,   1,   1,  1,   1,   1,  0,   0,   0,   0, 0,   1,  1,
 
>   1,   1, 1),
>  N = 21)

>  test<-bugs(data,inits,

>  model.file="/home/meyerjp/rasch/test.bug",

>  parameters=c("alpha0","alpha1","alpha12","alpha2","sigma"),

>  n.chains=2,n.iter=10000,n.burnin=1000,

>  bugs.directory="/home/meyerjp/.wine/drive_c/Program Files/WinBUGS14/",
>  working.directory="/home/meyerjp/rasch/working",

>  debug=FALSE,
>  WINEPATH="/usr/bin/winepath",
>  newWINE=TRUE)


From mjanis1 at yahoo.com  Fri Jul 27 23:51:12 2007
From: mjanis1 at yahoo.com (Michael Janis)
Date: Fri, 27 Jul 2007 14:51:12 -0700
Subject: [R] 2nd R Console
Message-ID: <004601c7d098$3fcf11d0$22cf7ca6@heron>

Hi,

I was reading a thread: [R] "2nd R console" and had a similar question
regarding having more than one R console open at a time.  However, my
question differs from that of the thread:

Is it possible, or is there a wrapper that will allow one, to open an
arbitrary number of R consoles which access the same R session (all objects
in that session, etc.).  This would be R on linux accessed through a shell -
kind of like using GNU screen multi-user such that people could work
collaboratively on a given session.  The problem with screen is that all
commands are interleaved in the same terminal, which is confusing and does
not allow access to the command prompt at the same time, rather it would be
sequential.  I know there will be "why" questions but it is useful in an
academic environment.  Basically we have a memory machine for large genomic
analysis - and we could set that up as an Rserver, but this placing R into a
multi-user engine is better suited for our immediate needs.  Does anybody
have thoughts on this?

Thanks for considering,

Michael Janis
UCLA Bioinformatics


From dsp0718 at googlemail.com  Sat Jul 28 01:06:43 2007
From: dsp0718 at googlemail.com (David Pain)
Date: Sat, 28 Jul 2007 00:06:43 +0100
Subject: [R] Package manual examples - 'unexpected$undefined' errors
Message-ID: <acb54ab40707271606k114d8651jd7338c6d6a24e288@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070728/269c0b5c/attachment.pl 

From doc.evans at gmail.com  Sat Jul 28 00:19:15 2007
From: doc.evans at gmail.com (D. R. Evans)
Date: Fri, 27 Jul 2007 16:19:15 -0600
Subject: [R] Q: extracting data from lm
In-Reply-To: <46AA6B2A.8010408@optonline.net>
References: <46AA6913.8060801@gmail.com> <46AA6B2A.8010408@optonline.net>
Message-ID: <256f4e900707271519t189988d2nfee9f1cdb0b01ca1@mail.gmail.com>

On 27/07/07, Chuck Cleland <ccleland at optonline.net> wrote:

>
> coef(summary(lm(nu1 ~ nu2)))[,2]
>
>   Also, try the following which is often useful:
>
> str(summary(lm(nu1 ~ nu2)))
>

Oh, wow! Thank you.

Incidentally, just in case anyone got the wrong end of the stick, I'm
not at all complaining about R. It's good at my age to be faced with
something so different. And from an architectural standpoint I
appreciate its elegance and innate power. It's just the logistics of
knowing exactly what to type that causes me to feel overwhelmed, and
although I've become very used in the past couple of days to typing
?<something> I'm not much good yet at finding out how to help myself
if that doesn't tell me what I want to know.


From marc_schwartz at comcast.net  Sat Jul 28 00:46:52 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 27 Jul 2007 17:46:52 -0500
Subject: [R] Q: extracting data from lm
In-Reply-To: <46AA6913.8060801@gmail.com>
References: <46AA6913.8060801@gmail.com>
Message-ID: <1185576412.3625.18.camel@Bellerophon.localdomain>

On Fri, 2007-07-27 at 15:52 -0600, D. R. Evans wrote:
> Warning: I am a complete newbie to R. I have read ISwR, but I am still
> finding myself completely stuck on some simple concepts.
> 
> I have tried everything I can think of to solve this one, and finally
> decided that enough was enough and I need a pointer to a solution.
> 
> I have the following summary from lm():
> 
> ----
> 
> > summary(lm(nu1~nu4))
> 
> Call:
> lm(formula = nu1 ~ nu4)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -1572.62  -150.38   -21.70   168.57  2187.84
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) 29.88739   43.68881   0.684    0.494
> nu4          1.00036    0.01025  97.599   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 470.9 on 298 degrees of freedom
> Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
> F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16
> 
> ----
> 
> But I want to access some of these numbers programmatically. I finally
> figured out that to get the estimate of the nu4 coefficient I need to do:
> 
> ----
> 
> > lm(nu1~nu4)$coefficients[2]
>      nu4
> 1.000363
> 
> ----
> 
> which to me as a long-time C++ programmer is close to black magic (I've
> been programming since 1972; I have to say that R is unlike anything I've
> ever seen, and it's far from trivial to get my head around some of it --
> for example, how I could have known a priori that the above is the way to
> get the nu4 coefficient is beyond me). Anyway, having figured out how to
> get the estimate of the coefficient, I not-unnaturally wanted also to find
> a way to access the std. error of the estimate (the value 0.01025 in the
> summary). But I am completely mystified as to how to do it :-(
> 
> Any help gratefully (VERY gratefully) received, and I apologise if this is
> a really, really stupid question and that the answer lies somewhere in some
> documentation that I've obviously not properly taken on board.

It looks like Peter references the notion of 'extractor functions' below
middle on page 97, but does not describe them, unless I am going blind
(or just tired).

In either case, you can supplement Peter's great intro book with many of
the of the other free references available via the R web site or An
Introduction to R, available with your installation or from the web
site.

In the latter case go here:

http://cran.r-project.org/doc/manuals/R-intro.html#Statistical-models-in-R

and scroll down to section 11.3.

Take note also of the 'Value' section of ?lm, which describes the
general contents of an lm object.

Another function that is always helpful is str(), which will display the
structure of an R object, enabling you to gain insights into the
components, their names and content. See ?str

In addition, take note of the 'See Also' section of ?lm, which does list
the generic functions coef, effects, residuals, fitted, vcov.

Using coef() will get you the values from the Coefficients part of the
output above, then enabling you to subset them to get the other values
as you require.

Another option relative to seeking assistance, is to review the Posting
Guide, a link to which is on the bottom of every post to the list. There
is a list of resources there that can guide you through how to search
for help, including utilizing the archives of the R help list where
keyword searches will be of tremendous assistance.

HTH,

Marc Schwartz


From doc.evans at gmail.com  Fri Jul 27 23:52:19 2007
From: doc.evans at gmail.com (D. R. Evans)
Date: Fri, 27 Jul 2007 15:52:19 -0600
Subject: [R] Q: extracting data from lm
Message-ID: <46AA6913.8060801@gmail.com>

Warning: I am a complete newbie to R. I have read ISwR, but I am still
finding myself completely stuck on some simple concepts.

I have tried everything I can think of to solve this one, and finally
decided that enough was enough and I need a pointer to a solution.

I have the following summary from lm():

----

> summary(lm(nu1~nu4))

Call:
lm(formula = nu1 ~ nu4)

Residuals:
     Min       1Q   Median       3Q      Max
-1572.62  -150.38   -21.70   168.57  2187.84

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 29.88739   43.68881   0.684    0.494
nu4          1.00036    0.01025  97.599   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 470.9 on 298 degrees of freedom
Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16

----

But I want to access some of these numbers programmatically. I finally
figured out that to get the estimate of the nu4 coefficient I need to do:

----

> lm(nu1~nu4)$coefficients[2]
     nu4
1.000363

----

which to me as a long-time C++ programmer is close to black magic (I've
been programming since 1972; I have to say that R is unlike anything I've
ever seen, and it's far from trivial to get my head around some of it --
for example, how I could have known a priori that the above is the way to
get the nu4 coefficient is beyond me). Anyway, having figured out how to
get the estimate of the coefficient, I not-unnaturally wanted also to find
a way to access the std. error of the estimate (the value 0.01025 in the
summary). But I am completely mystified as to how to do it :-(

Any help gratefully (VERY gratefully) received, and I apologise if this is
a really, really stupid question and that the answer lies somewhere in some
documentation that I've obviously not properly taken on board.


From r.turner at auckland.ac.nz  Sat Jul 28 00:34:48 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 28 Jul 2007 10:34:48 +1200
Subject: [R] manipulating arrays
In-Reply-To: <da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
References: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
	<da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
Message-ID: <CBCD0FA3-B5B2-43E2-A5DE-4854C286725D@auckland.ac.nz>

?append

	cheers,

		Rolf Turner

On 28/07/2007, at 2:12 AM, Henrique Dallazuanna wrote:

> Hi, I don't know if is the more elegant way, but:
>
> X<-c(1,2,3,4,5)
> X <- c(X[1], 0, X[2:5])
>
>
> --  
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
>
> On 27/07/07, Nair, Murlidharan T <mnair at iusb.edu> wrote:
>>
>> Can I insert an element in an array at a particular position without
>> destroying the already existing element?
>>
>>
>>
>> X<-c(1,2,3,4,5)
>>
>>
>>
>> I want to insert an element between 1 and 2.
>>
>>
>>
>> Thanks ../Murli
>>
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

######################################################################
Attention:\ This e-mail message is privileged and confidenti...{{dropped}}


From ggrothendieck at gmail.com  Sat Jul 28 02:49:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 27 Jul 2007 20:49:15 -0400
Subject: [R] manipulating arrays
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0707271749h711b01e8ge143d612f2d88622@mail.gmail.com>

Try this:

> x <- 11:15
> append(x, values = 99, after = 1)
[1] 11 99 12 13 14 15


On 7/27/07, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> Can I insert an element in an array at a particular position without
> destroying the already existing element?
>
>
>
> X<-c(1,2,3,4,5)
>
>
>
> I want to insert an element between 1 and 2.
>
>
>
> Thanks ../Murli
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mazatlanmexico at yahoo.com  Fri Jul 27 21:39:57 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Fri, 27 Jul 2007 12:39:57 -0700 (PDT)
Subject: [R] Creating an instance of R from MS Access?
Message-ID: <428638.87453.qm@web56609.mail.re3.yahoo.com>

Hi all:
Does anyone know if it's at all possible to create a
connection to R from MS access? For example, if I have
a table and want to export it to R,generate a graph in
R and import it back to MS access. I can do this with
sigmaPlot and other graphic programs but just
wondering if it can be done with R. Thanks



 Felipe D. Carrillo
  Fishery Biologist
  US Fish & Wildlife Service
  Red Bluff, California 96080



      ____________________________________________________________________________________


From ccleland at optonline.net  Sat Jul 28 00:01:14 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 27 Jul 2007 18:01:14 -0400
Subject: [R] Q: extracting data from lm
In-Reply-To: <46AA6913.8060801@gmail.com>
References: <46AA6913.8060801@gmail.com>
Message-ID: <46AA6B2A.8010408@optonline.net>

D. R. Evans wrote:
> Warning: I am a complete newbie to R. I have read ISwR, but I am still
> finding myself completely stuck on some simple concepts.
> 
> I have tried everything I can think of to solve this one, and finally
> decided that enough was enough and I need a pointer to a solution.
> 
> I have the following summary from lm():
> 
> ----
> 
>> summary(lm(nu1~nu4))
> 
> Call:
> lm(formula = nu1 ~ nu4)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -1572.62  -150.38   -21.70   168.57  2187.84
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) 29.88739   43.68881   0.684    0.494
> nu4          1.00036    0.01025  97.599   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 470.9 on 298 degrees of freedom
> Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
> F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16
> 
> ----
> 
> But I want to access some of these numbers programmatically. I finally
> figured out that to get the estimate of the nu4 coefficient I need to do:
> 
> ----
> 
>> lm(nu1~nu4)$coefficients[2]
>      nu4
> 1.000363
> 
> ----
> 
> which to me as a long-time C++ programmer is close to black magic (I've
> been programming since 1972; I have to say that R is unlike anything I've
> ever seen, and it's far from trivial to get my head around some of it --
> for example, how I could have known a priori that the above is the way to
> get the nu4 coefficient is beyond me). Anyway, having figured out how to
> get the estimate of the coefficient, I not-unnaturally wanted also to find
> a way to access the std. error of the estimate (the value 0.01025 in the
> summary). But I am completely mystified as to how to do it :-(
> 
> Any help gratefully (VERY gratefully) received, and I apologise if this is
> a really, really stupid question and that the answer lies somewhere in some
> documentation that I've obviously not properly taken on board.

coef(summary(lm(nu1 ~ nu2)))[,2]

  Also, try the following which is often useful:

str(summary(lm(nu1 ~ nu2)))

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From topkatz at msn.com  Sat Jul 28 00:30:41 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 27 Jul 2007 18:30:41 -0400
Subject: [R] Matrix Multiplication, Floating-Point, etc.
Message-ID: <BAY108-F24AABC3636FC394FB1D4A8AAF30@phx.gbl>

Hi.

I recently tried the following in R 2.5.1 on Windows XP:

>ev2<-c(0.8,-0.6)
>ev1<-c(0.6,0.8)
>ev1%*%ev2
              [,1]
[1,] -2.664427e-17
>sum(ev1*ev2)
[1] 0
>

(I got the same result with R 2.4.1 on a different Windows XP machine.)

I expect this issue is very familiar and probably has been discussed in this 
forum before.  Can someone please point me to some documentation or 
discussion about this?  Is there some standard way to get the "correct" 
answer from %*%?

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From amstat2006 at gmail.com  Sat Jul 28 02:18:33 2007
From: amstat2006 at gmail.com (Am Stat)
Date: Fri, 27 Jul 2007 20:18:33 -0400
Subject: [R] data order by different level of variables
Message-ID: <c8b63a350707271718u2b539363r71c244b11832fae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/66ce4ebc/attachment.pl 

From gregory_gentlemen at yahoo.ca  Sat Jul 28 05:28:25 2007
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Fri, 27 Jul 2007 23:28:25 -0400 (EDT)
Subject: [R] generating symmetric matrices
Message-ID: <541365.65996.qm@web63504.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/9bf3b4bc/attachment.pl 

From stanhopkins at comcast.net  Sat Jul 28 03:10:22 2007
From: stanhopkins at comcast.net (Stan Hopkins)
Date: Fri, 27 Jul 2007 20:10:22 -0500
Subject: [R] Error when using the cat function
Message-ID: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/f238859b/attachment.pl 

From brown_emu at yahoo.com  Sat Jul 28 11:46:27 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 28 Jul 2007 02:46:27 -0700 (PDT)
Subject: [R] Fitting exponential curve to data points
In-Reply-To: <loom.20070724T175645-930@post.gmane.org>
Message-ID: <316800.41764.qm@web39714.mail.mud.yahoo.com>

Sorry, just got back into town.

I wonder if AIC, BIC, or cross-validation scoring couldn't also be used as
criteria for model selection - I've seen it mostly in the context of variable
selection rather than 'form' selection but in principle might apply here?


--- Dieter Menne <dieter.menne at menne-biomed.de> wrote:

> Andrew Clegg <andrew.clegg <at> gmail.com> writes:
> 
> > 
> > ... If I want to demonstrate that a non-linear curve fits
> > better than an exponential, what's the best measure for that? Given
> > that neither of nls() or optim() provide R-squared. 
> 
> To supplement Karl's comment, try Douglas Bates' (author of nls) comments
> on the
> matter
> 
> http://www.ens.gu.edu.au/ROBERTK/R/HELP/00B/0399.HTML
> 
> Short summary:
>     * ... "the lack of automatic ANOVA, R^2 and adj. R^2 from nls is a
> feature,
> not a bug :-)"
>     * My best advice regarding R^2 statistics with nonlinear models is, as
> Nancy
> Reagan suggested, "Just say no."
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



      ____________________________________________________________________________________


From jim at bitwrit.com.au  Sat Jul 28 14:02:55 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 28 Jul 2007 22:02:55 +1000
Subject: [R] Average plan
In-Reply-To: <11811324.post@talk.nabble.com>
References: <11811324.post@talk.nabble.com>
Message-ID: <46AB306F.3040703@bitwrit.com.au>

Nok Noy wrote:
> Hello, 
> 
> I'm looking for a method to compute an average plan from 4 or 5 point in an
> cartesian space. I'm sure It can be done using a less-square method but
> maybe it a function already exist in R system to get this plan. 
> Can somebody help me to solve this problem (I'm looking on the net for hours
> but didn't find anything realy satisfiying me)

Hi Nok,
I haven't seen any answers to your question, and this may be due to the 
fact that no one knows what you are asking. Do you mean that you want to 
define a _plane_ that is the best fit to more than three points in a 
three dimensional system?

Jim


From brown_emu at yahoo.com  Sat Jul 28 11:57:49 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 28 Jul 2007 02:57:49 -0700 (PDT)
Subject: [R] manipulating arrays
In-Reply-To: <da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
Message-ID: <400719.24564.qm@web39711.mail.mud.yahoo.com>

I think you are looking for append(), though it won't modify the object
in-place like Python [I believe that is a product of R's 'functional
programming' philosophy].

might want to check this entertaining thread:
http://tolstoy.newcastle.edu.au/R/help/04/11/7727.html

in this example it would be like

> c(X[1], 0, X[2:5])
[1] 1 0 2 3 4 5
> append(X,0,1)
[1] 1 0 2 3 4 5


--- Henrique Dallazuanna <wwwhsd at gmail.com> wrote:

> Hi, I don't know if is the more elegant way, but:
> 
> X<-c(1,2,3,4,5)
> X <- c(X[1], 0, X[2:5])
> 
> 
> -- 
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
> 
> On 27/07/07, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> >
> > Can I insert an element in an array at a particular position without
> > destroying the already existing element?
> >
> >
> >
> > X<-c(1,2,3,4,5)
> >
> >
> >
> > I want to insert an element between 1 and 2.
> >
> >
> >
> > Thanks ../Murli
> >
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> > ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



       
____________________________________________________________________________________

Comedy with an Edge to see what's on, when.


From dsp0718 at googlemail.com  Sat Jul 28 12:03:08 2007
From: dsp0718 at googlemail.com (David Pain)
Date: Sat, 28 Jul 2007 11:03:08 +0100
Subject: [R] Package manual examples - 'unexpected$undefined' errors
Message-ID: <acb54ab40707280303g4fc79bbyaf475eba094f8eb8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070728/f9ad16e5/attachment.pl 

From Greg.Snow at intermountainmail.org  Sat Jul 28 06:02:06 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 22:02:06 -0600
Subject: [R] Q: extracting data from lm
References: <46AA6913.8060801@gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A163@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/d1840d99/attachment.pl 

From mmeredith at wcs.org  Sat Jul 28 14:20:05 2007
From: mmeredith at wcs.org (Mike Meredith)
Date: Sat, 28 Jul 2007 05:20:05 -0700 (PDT)
Subject: [R] Error when using the cat function
In-Reply-To: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>
References: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>
Message-ID: <11842347.post@talk.nabble.com>


Your problem is with ifelse, not with cat.

First clue is that 

ifelse(TRUE,{print("yes")},{print("no")})  # results in "yes" being printed
TWICE. Try this:

tmp <- ifelse(TRUE,{print("yes")},{print("no")}) # one "yes"

tmp  # another "yes"

Try:

print(print("yes")) # prints "yes" and returns "yes" invisibly. This
returned value is passed on to/by ifelse.

Now try:

print(cat("yes\n")) # "yes" appears, but cat("yes") returns NULL, which
ifelse can't handle:

ifelse(TRUE, NULL, "whatever") # Gives the error you saw.

What you need is if { } else { } :

if(!inherits(dat[[n]], "factor")) {cat("yes\n")} else {cat("no\n")}

HTH, Mike.


Stan Hopkins wrote:
> 
> Is the following developed in my console output a recognized bug or am I
> using the cat function incorrectly?
> 
> Thanks,
> 
> Stan
> 
> 
> 
>> ifelse(class(data[[n]])!="factor",{print("yes")},{print("no")})
> [1] "yes"
> [1] "yes"
>> ifelse(class(data[[n]])!="factor",{cat("yes")},{cat("no")})
> yesError in ans[test & !nas] <- rep(yes, length.out = length(ans))[test &
> : 
> incompatible types (from NULL to logical) in subassignment type fix
> 
> 
>>cat("yes")
> yes 
>> class(data[[n]])!="factor"
> [1] TRUE
>> class(data[[n]])
> [1] "numeric"
>> n
> [1] 28
>> length(data[[n]])
> [1] 955
>> class(data)
> [1] "data.frame"
>> dim(data)
> [1] 955 182
>> data[[n]]
> [1] 2.5 4.9 2.6 3.0 4.7 5.0 3.9 1.5 4.8 3.2 3.6 5.2 6.3
> [14] 6.3 5.0 4.6 6.0 4.5 3.9 3.6 5.7 8.5 4.0 5.0 11.8 4.7
> [27] 7.9 2.8 4.8 5.1 4.1 4.2 3.7 2.0 2.1 1.1 14.6 7.0 3.4
> [40] 3.4 10.1 4.7 4.9 5.2 4.3 2.9 2.8 2.3 1.2 2.0 2.0 3.0
> [53] 2.0 1.1 2.0 1.0 2.0 2.0 2.7 1.0 2.0 2.0 2.0 2.0 1.1
> [66] 2.0 2.0 1.0 1.1 2.4 2.0 2.0 5.0 0.8 2.0 3.3 2.7 2.2
> [79] 2.9 1.4 2.0 1.9 1.0 1.9 2.1 2.2 2.0 2.0 1.3 3.0 1.4
> [92] 2.0 1.5 2.1 1.2 1.7 2.1 2.0 2.0 2.3 2.0 1.6 1.5 2.3
> [105] 1.1 2.0 2.0 5.0 2.4 2.0 0.8 4.0 0.0 1.7 8.3 2.0 2.0
> [118] 2.0 6.1 14.4 8.2 5.2 2.5 1.0 1.0 1.8 1.1 4.9 0.9 2.1
> [131] 1.4 1.0 1.0 3.0 2.6 2.0 1.7 1.2 3.3 2.0 1.1 1.7 1.2
> [144] 2.7 0.9 2.0 3.2 1.8 1.8 1.1 1.3 2.3 1.1 1.7 1.9 1.0
> [157] 2.3 1.1 1.0 1.2 1.5 3.2 2.2 1.6 1.0 1.7 2.5 2.0 2.0
> [170] 2.3 1.1 1.5 2.0 1.7 5.1 3.6 2.0 2.0 1.2 1.2 3.1 1.3
> [183] 1.3 2.0 1.7 1.1 2.8 2.0 2.0 1.9 2.0 2.8 4.0 8.8 4.0
> [196] 3.2 5.0 2.1 3.0 7.4 2.5 3.2 3.0 2.8 1.9 3.0 3.2 3.6
> [209] 2.8 3.2 2.1 2.5 2.2 3.0 3.7 3.2 2.3 2.7 3.1 2.5 3.0
> [222] 2.4 2.6 0.9 5.4 2.8 3.9 4.7 2.5 2.9 4.4 4.1 4.0 4.0
> [235] 2.0 4.5 3.2 3.0 4.5 6.5 7.3 1.1 9.3 5.1 4.0 4.5 4.8
> [248] 7.6 6.7 3.0 3.0 6.0 6.0 4.0 5.0 3.0 5.0 1.0 5.0 4.0
> [261] 5.0 4.0 3.8 3.0 7.0 3.0 5.0 120.0 4.0 8.0 4.0 6.0 5.0
> [274] 4.0 6.0 2.0 2.6 3.2 4.0 4.0 3.0 6.0 3.0 3.0 2.0 2.5
> [287] 5.0 5.0 3.0 3.0 4.0 7.3 2.1 6.3 6.6 15.9 3.6 2.0 9.1
> [300] 6.9 4.2 7.8 5.7 7.7 5.6 5.8 16.3 4.0 3.0 3.4 0.0 1.0
> [313] 1.0 2.7 1.6 1.6 1.0 3.0 2.0 1.0 2.0 1.3 2.0 1.4 1.0
> [326] 0.9 1.0 0.8 0.0 0.0 3.1 2.6 1.4 2.0 6.6 2.0 1.2 2.0
> [339] 1.0 1.8 1.7 2.3 1.7 0.0 1.3 2.0 3.5 1.1 0.0 1.2 1.2
> [352] 1.0 2.0 1.2 NA 1.2 2.2 2.0 2.2 1.5 1.0 2.8 1.0 1.0
> [365] 2.1 2.0 1.3 0.0 1.5 1.8 1.4 1.2 1.2 1.1 1.0 1.1 2.0
> [378] 2.0 2.4 2.0 2.8 3.1 1.1 1.8 1.3 1.4 0.7 4.0 4.7 1.0
> [391] 0.6 3.0 1.0 0.9 2.0 1.7 2.1 2.0 1.0 2.0 16.0 3.0 10.0
> [404] 5.0 1.2 0.7 1.2 1.9 1.3 1.7 1.3 2.0 1.6 4.2 3.8 1.4
> [417] 1.2 1.3 2.0 2.1 5.8 5.9 1.2 2.8 1.8 3.6 1.8 1.9 1.1
> [430] 1.3 0.9 2.0 3.2 1.7 1.7 2.9 1.6 5.0 4.0 1.9 2.2 2.0
> [443] 2.7 2.5 1.1 2.0 1.7 1.5 1.9 1.1 1.6 5.2 1.5 1.4 1.0
> [456] 1.9 1.4 1.9 2.2 2.3 3.9 1.7 0.8 0.9 1.5 1.7 2.9 1.2
> [469] 1.9 1.8 2.6 1.4 2.1 1.6 1.7 1.6 1.4 2.0 2.1 1.0 5.0
> [482] 2.3 2.5 1.0 1.0 1.3 2.3 1.1 1.8 0.9 1.5 1.3 1.0 0.8
> [495] 1.0 0.7 0.9 0.9 2.0 2.9 2.6 0.6 1.6 2.0 0.9 1.0 1.1
> [508] 2.0 0.9 1.1 2.0 4.0 3.0 1.0 2.0 2.0 1.4 3.0 3.0 1.3
> [521] 1.0 1.2 0.8 2.0 0.0 0.0 0.7 1.4 1.0 0.8 1.2 1.4 2.1
> [534] 1.0 1.0 1.4 1.2 1.1 4.0 1.3 3.0 1.7 2.0 1.0 1.6 2.0
> [547] 0.9 6.0 1.7 1.7 1.7 1.0 0.8 0.6 2.0 2.0 1.0 2.0 1.4
> [560] 1.0 1.3 1.0 1.0 1.1 1.0 1.1 5.0 4.0 2.0 1.6 3.0 2.1
> [573] 1.2 2.0 0.9 1.2 1.0 1.1 1.9 2.1 2.2 1.0 1.5 1.3 3.0
> [586] 2.0 3.6 2.0 2.0 1.5 11.4 5.2 4.5 3.4 1.6 2.1 1.2 2.4
> [599] 2.1 2.3 1.7 2.0 1.4 0.5 1.6 1.9 2.6 0.4 1.3 1.4 1.2
> [612] 1.1 1.4 2.3 1.0 1.7 1.1 3.4 1.4 2.4 1.2 1.0 1.3 1.0
> [625] 1.2 0.8 2.1 1.7 2.1 0.9 1.4 1.2 1.9 1.1 2.3 1.5 3.0
> [638] 3.0 4.9 5.8 3.0 3.0 4.2 1.1 2.5 4.9 2.0 1.9 1.8 1.2
> [651] 2.0 2.2 1.4 1.8 2.0 1.2 3.2 1.5 2.0 3.5 2.0 0.8 1.8
> [664] 1.1 2.0 2.2 1.4 1.1 2.0 1.7 1.4 3.8 4.0 1.7 1.5 1.2
> [677] 1.1 2.0 3.0 21.0 6.0 20.0 5.0 20.0 13.0 4.0 2.6 2.8 6.1
> [690] 2.1 1.8 2.2 1.9 1.5 4.0 2.9 2.6 2.3 2.2 3.3 3.5 1.2
> [703] 1.5 3.7 2.3 3.0 1.9 2.5 1.5 1.7 2.5 3.0 2.6 1.8 2.5
> [716] 0.9 3.1 1.5 2.1 2.5 0.6 1.9 1.7 3.7 7.4 2.4 3.3 3.2
> [729] 1.2 1.3 2.0 1.4 3.4 1.7 3.5 1.7 2.0 1.3 0.8 3.0 1.9
> [742] 1.9 20.6 3.8 3.8 1.2 1.5 3.2 6.1 5.8 6.6 4.0 5.7 4.0
> [755] 3.0 4.7 6.8 6.9 4.1 1.9 4.5 3.8 2.7 2.3 2.5 2.3 2.6
> [768] 3.8 1.8 2.4 1.8 1.9 6.1 5.1 4.0 3.8 2.8 3.4 3.1 2.3
> [781] 7.5 3.0 3.0 3.1 2.4 6.0 2.3 5.0 2.8 2.7 2.2 5.0 5.0
> [794] 3.0 9.0 7.0 7.0 7.0 9.0 8.0 9.0 2.0 4.0 4.0 3.0 3.0
> [807] 2.0 2.0 3.0 4.0 3.0 3.0 7.9 11.0 16.0 4.0 7.7 5.0 6.6
> [820] 16.0 9.0 19.0 4.0 4.0 7.5 6.6 22.0 20.0 7.2 7.1 7.6 6.6
> [833] 6.2 7.8 6.9 10.9 11.2 6.0 5.0 8.0 7.0 4.0 3.0 6.0 4.0
> [846] 2.6 6.0 7.0 4.0 1.4 2.0 6.0 6.0 6.0 6.0 24.0 27.6 15.8
> [859] 8.0 7.8 7.3 8.2 5.3 3.4 18.1 31.6 5.8 5.5 4.0 4.1 4.7
> [872] 4.9 3.9 0.5 3.9 6.2 3.0 3.0 4.0 2.0 3.0 3.0 2.0 2.0
> [885] 2.0 0.0 3.0 2.0 2.0 4.0 1.6 1.7 2.0 2.0 2.0 2.0 26.0
> [898] 2.0 2.0 2.0 3.0 2.0 3.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0
> [911] 2.0 0.0 2.0 2.0 2.0 2.0 5.0 2.0 11.7 10.4 8.0 4.0 11.1
> [924] 13.2 14.6 11.7 13.4 14.3 15.8 25.6 10.0 6.0 9.1 9.7 4.0 10.7
> [937] 6.0 5.0 10.9 10.0 10.6 12.9 12.3 11.6 11.8 13.3 15.1 10.7 11.0
> [950] 13.5 32.9 12.9 8.4 8.1 12.5
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Error-when-using-the-cat-function-tf4161670.html#a11842347
Sent from the R help mailing list archive at Nabble.com.


From Greg.Snow at intermountainmail.org  Sat Jul 28 06:02:06 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 22:02:06 -0600
Subject: [R] Q: extracting data from lm
References: <46AA6913.8060801@gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A163@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/d1840d99/attachment-0001.pl 

From kamauallan at yahoo.com  Sat Jul 28 14:48:47 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Sat, 28 Jul 2007 05:48:47 -0700 (PDT)
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
In-Reply-To: <644e1f320707270513h5adabb8au3a870079b4bfbf80@mail.gmail.com>
Message-ID: <657735.44420.qm@web53507.mail.re2.yahoo.com>

Hi Jim,
The problem description.
I am trying to identify mutations in a given gene from
a particular genome (biological genome sequence).
I have two CSV files consisting of sequences. One file
consists of reference (documented,curated accepted as
standard) sequences. The other consists of sample
sequences I am trying to identify mutations within. In
both files the an individual sequence is contained in
a single record, it?s amino acid residues ( the actual
sequence of alphabets each representing a given amino
acid for example ?A? stands for ?Alanine?, ?C? for
Cysteine and so on) are each allocated a single field
in the CSV file.
The sequences in both files have been well aligned,
each contain 115 residues with the first residue is
contained in the field 5. The fields 1 to 4 are
allocated for metadata (name of sequence and so on).
My task is to compile a residue occurrence count for
each residue present in a given field in the reference
sequence dataset and use this information when reading
each sequence in the sample dataset to identify a
mutation. For example for position 9 of the sample
sequence ?bb? a ?P? is found and according to our
reference sequence dataset of summaries, at position 9
?P? may not even exist or may have an occurrence of
10% or so will be classified as mutation, (I could
employ a cut of parameter for mutation
classification).


Allan.

--- jim holtman <jholtman at gmail.com> wrote:

> results=()#character()
> myVariableNames=names(x.val)
> results[length(myVariableNames)]<-NA
> 
> for (i in myVariableNames){
>     results[i]<-names(x.val[[i]])    # this does not
> work it returns a
> NULL (how can i convert this to x.val$"somevalue" ?
> )
> }
> 
> 
> 
> On 7/27/07, Allan Kamau <kamauallan at yahoo.com>
> wrote:
> > Hi All,
> > I am having difficulties finding a way to find a
> substitute to the command "names(v.val$PR14)" so
> that I could generate the command on the fly for all
> PR14 to PR200 (please see the previous discussion
> below to understand what the object x.val contains)
> . I have tried the following
> >
> > >results=()#character()
> > >myVariableNames=names(x.val)
> > >results[length(myVariableNames)]<-NA
> >
> > >for
> as.vector(unlist(strsplit(str,",")),mode="list")
> > +    results[i]<-names(x.val$i)    # this does not
> work it returns a NULL (how can i convert this to
> x.val$"somevalue" ? )
> > >}
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: Allan Kamau <kamauallan at yahoo.com>
> > To: r-help at stat.math.ethz.ch
> > Sent: Thursday, July 26, 2007 10:03:17 AM
> > Subject: Re: [R] Obtaining summary of frequencies
> of value occurrences for a variable in a
> multivariate dataset.
> >
> > Thanks so much Jim, Andaikalavan, Gabor and others
> for the help and suggestions.
> > The solution will result in a matrix containing
> nested matrices to enable each variable name, each
> variables distinct value and the count of the
> distinct value to be accessible individually.
> > The main matrix will contain the variable names,
> the first level nested matrices will consist of the
> variables unique values, and each such variable
> entry will contain a one element vector to contain
> the count or occurrence frequency.
> > This matrix can now be used in comparing other
> similar datasets for variable values and their
> frequencies.
> >
> > Building on the input received so far, a probable
> solution in building the matrix will include the
> following.
> >
> >
> > 1)I reading the csv file (containing column
> headers)
> >
>
>my_data=read.table("<path/to/my/data.csv>",header=TRUE,sep=",",dec=".",fill=TRUE)
> >
> > 2)I group the values in each variable producing an
> occurrence count(frequency)
> > >x.val<-apply(my_data,2,table)
> >
> > 3)I obtain a vector of the names of the variables
> in the table
> > >names(x.val)
> >
> > 4)Now I make use of the names (obtained in step 3)
> to obtain a vector of distinct values in a given
> variable (in the example below the variable name is
> $PR14)
> > >names(v.val$PR14)
> >
> > 5)I obtain a vector (with one element) of the
> frequency of a value obtained from the step above
> (in our example the value is "V")
> > >as.vector(x.val$PR14["V"])
> >
> > Todo:
> > Now I will need to place the steps above in a
> script (consisting of loops) to build the matrix,
> step 4 and 5 seem tricky to do programatically.
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: jim holtman <jholtman at gmail.com>
> > To: Allan Kamau <kamauallan at yahoo.com>
> > Cc: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>;
> r-help at stat.math.ethz.ch
> > Sent: Wednesday, July 25, 2007 1:50:55 PM
> > Subject: Re: [R] Obtaining summary of frequencies
> of value occurrences for a variable in a
> multivariate dataset.
> >
> > Also if you want to access the individual values,
> you can just leave
> > it as a list:
> >
> > > x.val <- apply(x, 2, table)
> > > # access each value
> > > x.val$PR14["V"]
> > V
> > 8
> >
> >
> >
> > On 7/25/07, Allan Kamau <kamauallan at yahoo.com>
> wrote:
> > > A subset of the data looks as follows
> > >
> > > > df[1:10,14:20]
> > >   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> > > 1     V    T    I    K    V    G    D
> > > 2     V    S    I    K    V    G    G
> > > 3     V    T    I    R    V    G    G
> > > 4     V    S    I    K    I    G    G
> > > 5     V    S    I    K    V    G    G
> > > 6     V    S    I    R    V    G    G
> > > 7     V    T    I    K    I    G    G
> > > 8     V    S    I    K    V    E    G
> > > 9     V    S    I    K    V    G    G
> > > 10    V    S    I    K    V    G    G
> > >
> > > The result I would like is as follows
> > >
> > > PR10        PR11          PR12   ...
> > > [V:10]    [S:7,T:3]    [I:10]
> > >
> > > The result can be in a matrix or a vector and
> each variablename, value and frequency should be
> accessible so as to be used for comparisons with
> another dataset later.
> > > The frequency can be a count or a percentage.
> > >
> > >
> > > Allan.
> > >
> > >
> > > ----- Original Message ----
> > > From: Adaikalavan Ramasamy
> <ramasamy at cancer.org.uk>
> > > To: Allan Kamau <kamauallan at yahoo.com>
> > > Cc: r-help at stat.math.ethz.ch
> > > Sent: Tuesday, July 24, 2007 10:21:51 PM
> > > Subject: Re: [R] Obtaining summary of
> frequencies of value occurrences for a variable in a
> multivariate dataset.
> > >
> > > The name of the table should give you the
> "value". And if you have a
> > > matrix, you just need to convert it into a
> vector first.
> > >
> > >  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ],
> nc=3 )
> > >  > m
> > >      [,1] [,2] [,3]
> > > [1,] "A"  "C"  "B"
> > > [2,] "B"  "D"  "C"
> > > [3,] "C"  "E"  "D"
> > >  > tb <- table( as.vector(m) )
> > >  > tb
> > >
> > > A B C D E
> > > 1 2 3 2 1
> > >  > paste( names(tb), ":", tb, sep="" )
> > > [1] "A:1" "B:2" "C:3" "D:2" "E:1"
> > >
> > > If this is not what you want, then please give a
> simple example.
> > >
> > > Regards, Adai
> > >
> > >
> > >
> > > Allan Kamau wrote:
> > > > Hi all,
> > > > If the question below as been answered before
> I
> > > > apologize for the posting.
> > > > I would like to get the frequencies of
> occurrence of
> > > > all values in a given variable in a
> multivariate
> > > > dataset. In short for each variable (or field)
> a
> > > > summary of values contained with in a
> value:frequency
> 
=== message truncated ===


From p.dalgaard at biostat.ku.dk  Sat Jul 28 15:25:41 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 28 Jul 2007 15:25:41 +0200
Subject: [R] manipulating arrays
In-Reply-To: <da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
References: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
	<da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
Message-ID: <46AB43D5.8030204@biostat.ku.dk>

Henrique Dallazuanna wrote:
> Hi, I don't know if is the more elegant way, but:
>
> X<-c(1,2,3,4,5)
> X <- c(X[1], 0, X[2:5])
>   
append(X, 0, 1)


From e0425331 at stud4.tuwien.ac.at  Sat Jul 28 18:39:29 2007
From: e0425331 at stud4.tuwien.ac.at (Stefan Nachtnebel)
Date: Sat, 28 Jul 2007 18:39:29 +0200
Subject: [R] xtable with vector
In-Reply-To: <mailman.11.1185616805.19480.r-help@stat.math.ethz.ch>
References: <mailman.11.1185616805.19480.r-help@stat.math.ethz.ch>
Message-ID: <20070728183929.zujhqnbysowww8go@webmail.tuwien.ac.at>

Hello,

Is there a possibility to use xtable with a vector to generate a latex 
table? I always get an error, that no applicable method is available.

For example:

b<-1:12
dim(b)<-c(2,6)
dimnames(b)[[2]]<-paste("col",1:6)
xtable(b)

works fine and does not raise an error, but

a<-1:6
names(a)<-paste(col,1:6)
xtable(b)

does not work.

Regards, Stefan


From mjanis at chem.ucla.edu  Sat Jul 28 10:19:10 2007
From: mjanis at chem.ucla.edu (Michael Janis)
Date: Sat, 28 Jul 2007 01:19:10 -0700
Subject: [R] 2nd R Console
Message-ID: <001a01c7d0ef$fb0d9140$13321b82@heron>

Hi,

I was reading a thread: [R] "2nd R console" and had a similar question
regarding having more than one R console open at a time.  However, my
question differs from that of the thread:

Is it possible, or is there a wrapper that will allow one, to open an
arbitrary number of R consoles which access the same R session (all objects
in that session, etc.).  This would be R on linux accessed through a shell -
kind of like using GNU screen multi-user such that people could work
collaboratively on a given session.  The problem with screen is that all
commands are interleaved in the same terminal, which is confusing and does
not allow access to the command prompt at the same time, rather it would be
sequential.  I know there will be "why" questions but it is useful in an
academic environment.  Basically we have a memory machine for large genomic
analysis - and we could set that up as an Rserver, but this placing R into a
multi-user engine is better suited for our immediate needs.  Does anybody
have thoughts on this?

Thanks for considering,

Michael Janis
UCLA Bioinformatics


From ericlecoutre at gmail.com  Sat Jul 28 16:07:49 2007
From: ericlecoutre at gmail.com (Eric Lecoutre)
Date: Sat, 28 Jul 2007 16:07:49 +0200
Subject: [R] Combine R2HTML and Rcmd BATCH?
In-Reply-To: <469CA291.7020407@tue.nl>
References: <469CA291.7020407@tue.nl>
Message-ID: <5d897a2f0707280707p28a76dabv98a58b8581c6f28b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070728/225dc78c/attachment.pl 

From edoviak at earthlink.net  Sat Jul 28 20:07:33 2007
From: edoviak at earthlink.net (Eric Doviak)
Date: Sat, 28 Jul 2007 14:07:33 -0400 (GMT-04:00)
Subject: [R] the large dataset problem
Message-ID: <9413086.1185646053804.JavaMail.root@elwamui-royal.atl.sa.earthlink.net>

Dear useRs,

I recently began a job at a very large and heavily bureaucratic organization. We're setting up a research office and statistical analysis will form the backbone of our work. We'll be working with large datasets such the SIPP as well as our own administrative data.

Due to the bureaucracy, it will take some time to get the licenses for proprietary software like Stata. Right now, R is the only statistical software package on my computer. 

This, of course, is a huge limitation because R loads data directly into RAM making it difficult (if not impossible) to work with large datasets. My computer only has 1000 MB of RAM, of which Microsucks Winblows devours 400 MB. To make my memory issues even worse, my computer has a virus scanner that runs everyday and I do not have the administrative rights to turn the damn thing off. 

I need to find some way to overcome these constraints and work with large datasets. Does anyone have any suggestions?

I've read that I should "carefully vectorize my code." What does that mean ??? !!!

The "Introduction to R" manual suggests modifying input files with Perl. Any tips on how to get started? Would Perl Data Language (PDL) be a good choice?  http://pdl.perl.org/index_en.html

I wrote a script which loads large datasets a few lines at a time, writes the dozen or so variables of interest to a CSV file, removes the loaded data and then (via a "for" loop) loads the next few lines .... I managed to get it to work with one of the SIPP core files, but it's SLOOOOW. Worse, if I discover later that I omitted a relevant variable, then I'll have to run the whole script all over again.

Any suggestions?

Thanks,
- Eric


From arigado0315 at yahoo.com.tw  Sat Jul 28 18:16:43 2007
From: arigado0315 at yahoo.com.tw (arigado)
Date: Sat, 28 Jul 2007 09:16:43 -0700 (PDT)
Subject: [R] About infinite value
In-Reply-To: <B7B34444ECA41A41AC47DABA057CE7A206FD40@webmail.cip.cgiar.org>
References: <11740491.post@talk.nabble.com>
	<B7B34444ECA41A41AC47DABA057CE7A206FD40@webmail.cip.cgiar.org>
Message-ID: <11844202.post@talk.nabble.com>




I think that the best thing is to work in logarithmic way, to avoid the
limitations of the CPU. 
If y = 10^400, to do y=400*log(10), to change all you formulate to the
logarithmic way and the final result to apply the antilogarithm. 
 
Felipe de Mendiburu. 
Professor of statistic 
Agrarian National University -La Molina - PERU 

Thank you.
Thanks your method.

-- 
View this message in context: http://www.nabble.com/About-infinite-value-tf4128557.html#a11844202
Sent from the R help mailing list archive at Nabble.com.


From dataanalytics at earthlink.net  Sat Jul 28 18:09:03 2007
From: dataanalytics at earthlink.net (Walter R. Paczkowski)
Date: Sat, 28 Jul 2007 12:09:03 -0400
Subject: [R] beta regressions in R
Message-ID: <E1IEonx-0005oh-J8@elasmtp-junco.atl.sa.earthlink.net>


   Good morning,
   Does anyone know of a package or function to do a beta regression?
   Thanks,
   Walt Paczkowski

   _________________________________
   Walter R. Paczkowski, Ph.D.
   Data Analytics Corp.
   44 Hamilton Lane
   Plainsboro, NJ  08536
   (V) 609-936-8999
   (F) 609-936-3733

From patrick at pdrechsler.de  Sat Jul 28 23:49:53 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sat, 28 Jul 2007 23:49:53 +0200
Subject: [R] lattice grayscale "theme"
Message-ID: <87lkd08agu.fsf@pdrechsler.de>

Hi,

is there a grayscale setting for lattice plots?

I like the default color settings. I also like the settings that are
available for setting black and white with something like this:

--8<---------------cut here---------------start------------->8---
ltheme <- canonical.theme(color = FALSE)      ## in-built B&W theme
ltheme$strip.background$col <- "transparent" ## change strip bg
lattice.options(default.theme = ltheme)      ## set as default
--8<---------------cut here---------------end--------------->8---

Is there a simple way of achieving something in between these settings
(using grayscales for the default colors)?

Thankful for any pointers,

Patrick


From strinz at freenet.de  Sat Jul 28 21:19:48 2007
From: strinz at freenet.de (strinz at freenet.de)
Date: Sat, 28 Jul 2007 21:19:48 +0200
Subject: [R] text() and vector arguments like adj
Message-ID: <E1IErpM-0003Ci-Nu@www12.emo.freenet-rz.de>

Hello,

I remarked that the function
## Default S3 method:
text (x, y = NULL, labels = seq(along = x), adj = NULL,pos = NULL, offset = 0.5, vfont = NULL,cex = 1, col = NULL, font = NULL, ...)

accepts vectors of arguments (of the same length) except for the parameter adj.
When passing a vector of information for adjusting the labels, only the first value
is taken.  

Any special reason for this ?

btw: could a rotating argument like the 'srt' argument in mtext() be incorporated ?

best
Bjoern


From adik at ilovebacon.org  Sun Jul 29 04:50:11 2007
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Sat, 28 Jul 2007 19:50:11 -0700 (PDT)
Subject: [R] PCA with missing data?
Message-ID: <Pine.LNX.4.64.0707281949510.19803@parser.ilovebacon.org>

Hello,

 	I'm interested in running a PCA on a data set with lots of missing
data. There're a few techniques online which suggest how I could go about
doing this, but before I committed to coding them into R, I wanted to see
whether anyone could recommend a currently existing package with such a
procedure.

 	Google searching for such a procedure has'nt turned much up...but I
blame that on the difficulty of searcching for single letters, and the
rampant use of R as a variable name. But I did attempt to do my homework
before posting to the list!

Cordially,

--
Adam D. I. Kramer
Ph.D. Student, Social Psychology
University of Oregon
adik at uoregon.edu


From niederlein-rstat at yahoo.de  Mon Jul 30 14:09:21 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Mon, 30 Jul 2007 14:09:21 +0200
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46ADB142.9090502@unifi.it>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
Message-ID: <46ADD4F1.8090901@yahoo.de>

okay, I played a bit around and now I have some kind of testcase for you:

v1 <- NA
v2 <- rnorm(6)
v3 <- rnorm(6)
v4 <- rnorm(6)
v5 <- rnorm(6)
v6 <- rnorm(6)
v7 <- rnorm(6)
v8 <- rnorm(6)
v8 <- NA

list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)

 > list
[[1]]
[1] NA

[[2]]
[1] -0.6442149 -0.2047012 -1.1986041 -0.2097442 -0.7343465 -1.3888750

[[3]]
[1]  0.02354036 -1.36186952 -0.42197792  1.50445971 -1.76763996  0.53722404

[[4]]
[1] -1.40362589  0.13045724 -0.84651458  1.57005071  0.06961015  0.25269771

[[5]]
[1] -1.1829260  2.1411553 -0.1327081 -0.1053442 -0.8179396 -1.2342698

[[6]]
[1]  1.17099178  0.49248118 -0.18690065  1.50050976 -0.65552410 -0.01243247

[[7]]
[1] -0.046778203 -0.233788840  0.443908897 -1.649740180  0.003991354 -0.228020092

[[8]]
[1] NA

now, I need the means (and sd) of element 1 of list[2],list[3],list[4] (because they belong to "cat1") and

= mean(-0.6442149, 0.02354036, -1.40362589)

the same for element 2 up to element 6 (--> I would the get a vector containing the means for "cat1")
the same for the vectors belonging to "cat2".

does anybody now understand what I mean?

Antje


From rfrancois at mango-solutions.com  Mon Jul 30 14:05:19 2007
From: rfrancois at mango-solutions.com (Romain Francois)
Date: Mon, 30 Jul 2007 13:05:19 +0100
Subject: [R] regular expressions : extracting numbers
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
Message-ID: <46ADD3FF.4090408@mango-solutions.com>

Bonjour David,

What about one of these :

R> gsub( "[^[:digit:]]", "", x )

or using perl regular expressions:

R> gsub( "\\D", "", x, perl = T )

Cheers,

Romain

GOUACHE David wrote:
> Hello all,
>
> I have a vector of character strings, in which I have letters, numbers, and symbols. What I wish to do is obtain a vector of the same length with just the numbers.
> A quick example -
>
> extract of the original vector :
> "lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb" "rb" "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"
>
> and the type of thing I wish to end up with :
> "2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""
>
> or, instead of "", NA would be acceptable (actually it would almost be better for me)
>
> Anyways, I've been battling with gsub() and things of the sort, but I'm drowning in the regular expressions, despite a few hours of looking at Perl tutorials...
> So if anyone can help me out, it would be greatly appreciated!!
>
> In advance, thanks very much.
>
> David Gouache
> Arvalis - Institut du V?g?tal
> Station de La Mini?re
> 78280 Guyancourt
> Tel: 01.30.12.96.22 / Port: 06.86.08.94.3


-- 
Mango Solutions
data analysis that delivers

Tel:  +44(0) 1249 467 467
Fax:  +44(0) 1249 467 468
Mob:  +44(0) 7813 526 123


From whorfin at pixar.com  Sun Jul 29 02:41:45 2007
From: whorfin at pixar.com (Rick Sayre)
Date: Sat, 28 Jul 2007 17:41:45 -0700
Subject: [R] Bug in TAB handling for Win32 rTerm and rGUI in 2.5.1?
Message-ID: <46ABE249.1010500@pixar.com>

Greetings.  This seemed like a bug to me, but I wanted to see if this 
was in fact intended before reporting.

Before I start, i want to first extend thanks for the big improvements
in integration of command completion for the windows version.  Really
nice to have now.  But i believe there are some issues.

In getline/getline.c, the "tab" case of the charater handling switch 
statement
in getline() simply "break;"s to the end if tab completion is not 
enabled, thus
eating the tab.  Thus, if tab completion is disabled, a tab no
longer serves as a tab; it disappears.

likewise, in console.c, if k == TABKEY, a return is done without adding 
the key to kbuf, thus TAB is always discarded, even if completion is 
disabled.

It seems to me that this is wrong.

This new TAB behavior now makes it impossible for me to copy/paste text
from a text file of R expressions which use TABs.  Copy paste behavior
which worked in 2.4.x for rTerm now does not, since the discarded TABs
mean that keyword separators may disappear, changing the meaning of
pasted text.  rGUI thankfully still works, since the completion/TAB
processing code is bypassed when activating the "paste" command.

I'd like to request the ability to have both --- TAB as a working
separator, and the ability to configure the completion key to something
other than TAB.  This way one can both enjoy completion and successfully
copy/paste text containing tabs.

Cheers

	--Rick


From jholtman at gmail.com  Mon Jul 30 14:06:49 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 08:06:49 -0400
Subject: [R] regular expressions : extracting numbers
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
Message-ID: <644e1f320707300506r7f2fd074g61f16e2302acbf9e@mail.gmail.com>

Is this what you want:

> x
 [1] "lema, rb 2%"   "rb 2%"         "rb 3%"         "rb 4%"
"rb 3%"         "rb 2%,mineuse"
 [7] "rb"            "rb"            "rb 12"         "rb"
"rj 30%"        "rb"
[13] "rb"            "rb 25%"        "rb"            "rb"
"rb"            "rj, rb"
> gsub("[^0-9]*([0-9]*)[^0-9]*", "\\1", x)
 [1] "2"  "2"  "3"  "4"  "3"  "2"  ""   ""   "12" ""   "30" ""   ""
"25" ""   ""   ""   ""
>


On 7/30/07, GOUACHE David <D.GOUACHE at arvalisinstitutduvegetal.fr> wrote:
> Hello all,
>
> I have a vector of character strings, in which I have letters, numbers, and symbols. What I wish to do is obtain a vector of the same length with just the numbers.
> A quick example -
>
> extract of the original vector :
> "lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb" "rb" "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"
>
> and the type of thing I wish to end up with :
> "2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""
>
> or, instead of "", NA would be acceptable (actually it would almost be better for me)
>
> Anyways, I've been battling with gsub() and things of the sort, but I'm drowning in the regular expressions, despite a few hours of looking at Perl tutorials...
> So if anyone can help me out, it would be greatly appreciated!!
>
> In advance, thanks very much.
>
> David Gouache
> Arvalis - Institut du V?g?tal
> Station de La Mini?re
> 78280 Guyancourt
> Tel: 01.30.12.96.22 / Port: 06.86.08.94.32
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From tlumley at u.washington.edu  Fri Jul 27 18:45:58 2007
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 27 Jul 2007 09:45:58 -0700 (PDT)
Subject: [R] reading stata files: preserving values of variables
 converted to factors
In-Reply-To: <46A93E8A.7080906@fastmail.fm>
References: <46A93E8A.7080906@fastmail.fm>
Message-ID: <Pine.LNX.4.64.0707270943230.19542@homer22.u.washington.edu>

On Thu, 26 Jul 2007, Ben Saylor wrote:

> Hi,
>
> I am a Stata user new to R.  I am using read.dta to read a Stata file
> that has variables with value labels.  read.dta converts them to
> factors, but seems to recode them with values from 1 to <number of
> factor levels> (looking at the output of unclass(<varname>)), so the
> original numerical values are lost.

Yes. The R factor type should not be used if you want the original levels. 
It is not a 'labelled numeric' type and the numbers are an implementation 
detail.

>  Using convert.factors=FALSE
> preserves the values, but seems to discard the labels.

It doesn't discard the labels. They are kept in the attributes of the data 
frame.

 	-thomas


From jholtman at gmail.com  Fri Jul 27 19:19:50 2007
From: jholtman at gmail.com (jim holtman)
Date: Fri, 27 Jul 2007 13:19:50 -0400
Subject: [R] get() with complex objects?
In-Reply-To: <46AA14A7.9070803@columbia.edu>
References: <46AA14A7.9070803@columbia.edu>
Message-ID: <644e1f320707271019r10e9039bk5badc2e2bfa0fa81@mail.gmail.com>

'get' tries to retrieve the object given by the character string.  The
error message says that object can not be found.  You actually have to
'evaluate' the character string.  See the example below:

> x <- data.frame(a=1:10, b=11:20)
> x$a
 [1]  1  2  3  4  5  6  7  8  9 10
> z <- 'x$a'
> get(z)
Error in get(x, envir, mode, inherits) : variable "x$a" was not found
> # parse and evaluate the character string 'x$a'
> eval(parse(text=z))
 [1]  1  2  3  4  5  6  7  8  9 10

Does this make sense?


On 7/27/07, Mark Orr <mo2259 at columbia.edu> wrote:
> Hello R-listers,
> I'm having trouble accessing "sub" objects ("attributes"?), e.g.,
> "x$silinfo$avg.width" using the /get() /command;  I'm using/ get()/ in a
> loop as illustrated in the following code:
>
> #FIRST MAKE CLUSTERS of VARYING  k
> /for (i in 1:300){
>  assign(paste("x.",i,sep=""),pam(x,i))  #WORKS FINE
> }/
>
> #NEXT, TAKE LOOK AT AVE. SILHOUETTE VALUE FOR EACH k
>
> #PART 1, MAKE LIST OF OBJECTS NEEDED
> /gen.list <- rep("t",300)
> for (i in 1:300){
>  assign(gen.list[i],paste("x.",i,"$silinfo$avg.width",sep=""))
> }
> #WORKS FINE
>
> /#PART 2, USE LIST IN LOOP TO ACCESS OBJECT.
> /si//l.collector <- rep(99,300)
> for(i in 1:300){
>  sil.collector <- get(gen.list[i])
> }/
> #HERE IS THE ERROR
> /Error in get(x, envir, mode, inherits) : variable
> "x.1$silinfo$avg.width" was not found
>
> /So, I get the gist of this error; x.1 is an object findable from get(),
> but the "attribute"  levels are not accessible.  Any suggestions on how
> to get get() to access these levels?  From reading the get()'s help
> page, I don't think it will access the attributes. (my apologies for
> loosely using the term attributes, but I hope it is clear).
>
> Thanks,
>
> Mark Orr
>
> --
> ***********************************************
> Mark G. Orr, PhD
> Heilbrunn Dept. of Population and Family Health
> Columbia University
> 60 Haven Ave., B-2
> New York, NY 10032
>
> Tele: 212-304-7823
> Fax:  212-305-7024
>
> www.columbia.edu/~mo2259
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From amnakhan493 at gmail.com  Fri Jul 27 19:00:37 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Fri, 27 Jul 2007 10:00:37 -0700
Subject: [R] How to edit L-moment Ratio Diagram
Message-ID: <3ffd3bb60707271000q758a352bid1e5950198c8b24b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/1e9fb63e/attachment-0001.pl 

From shitao at hotmail.com  Fri Jul 27 19:25:19 2007
From: shitao at hotmail.com (Tao Shi)
Date: Fri, 27 Jul 2007 17:25:19 +0000
Subject: [R] Creating windows binary R package (PowerArchiver vs. zip
 -r9X)
Message-ID: <BAY120-W28888A6757C258C1DFD63FC7F30@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/5b02ed23/attachment-0001.pl 

From guo.dong99 at gmail.com  Fri Jul 27 20:09:29 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Fri, 27 Jul 2007 20:09:29 +0200
Subject: [R] plot
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBAF2AF1@LP-EXCHVS07.CO.IHC.COM>
References: <989527a20707270745h541fed48kb286f9e408a4c522@mail.gmail.com>
	<07E228A5BE53C24CAD490193A7381BBBAF2AF1@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <989527a20707271109w365c85f3qb8b4b775c805099b@mail.gmail.com>

Many thanks, Greg and Justin.

The matrix is just a 26*31 matrix - 26 years, 31 regions. I am know to
R, just dont know how to attach the data here yet..

As I have such matrices for nine indicators for all regions, so i
could show some differences by 3D plot, which I did similar things in
Excel. I am sure there is a way to do it in R

On 7/27/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> Graphs that rely on 3-d effects tend to distort the data rather than enlighten the viewer.  If your goal is to distort the data (which I doubt), then most of us don't want to help.  On the other hand, if you really do want to enlighten the viewer (even if that is just you), then tell us what your data is like and what you want to learn from it, and we will be happy to give you advice on creating useful graphs.
>
> --
> Gregory (Greg) L. Snow Ph.D.
> Statistical Data Center
> Intermountain Healthcare
> greg.snow at intermountainmail.org
> (801) 408-8111
>
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dong GUO ??
> > Sent: Friday, July 27, 2007 8:45 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] plot
> >
> > Greetings to the group,
> >
> > I would like to know if some one could help me with plot 3-d
> > column graph of a matrix (3-d column graph in Excel).
> >
> > Many thanks in advance.
> >
> > Regards,
> > Dong
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>


From ld7631 at gmail.com  Fri Jul 27 20:35:39 2007
From: ld7631 at gmail.com (Dimitri Liakhovitski)
Date: Fri, 27 Jul 2007 14:35:39 -0400
Subject: [R] Looping through all possible combinations of cases
Message-ID: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>

Hello!

I have a regular data frame (DATA) with 10 people and 1 column
('variable'). Its cases are people with names ('a', 'b', 'c', 'd',
'e', 'f', etc.). I would like to write a function that would sum up
the values on 'variable' of all possible combinations of people, i.e.

1. I would like to write a loop - in such a way that it loops through
each possible pair of cases (i.e., ab, ac, ad, etc.) and sums up their
respective values on 'variable'

2. I would like to write a loop - in such a way that it loops through
each possible trio of cases (i.e., abc, abd, abe, etc.) and sums up
their respective values on 'variable'.

3.  I would like to write a loop - in such a way that it loops through
each possible quartet of cases (i.e., abcd, abce, abcf, etc.) and sums
up their respective values on 'variable'.

etc.

Then, at the end I want to capture all possible combinations that were
considered (i.e., what elements were combined in it) and get the value
of the sum for each combination.

How should I do it?
Thanks a lot!
Dimitri


From Greg.Snow at intermountainmail.org  Fri Jul 27 20:57:47 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 12:57:47 -0600
Subject: [R] plot
In-Reply-To: <989527a20707271109w365c85f3qb8b4b775c805099b@mail.gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBBAF2B8C@LP-EXCHVS07.CO.IHC.COM>

Can you really see much of the data in a 26*31 3d barplot?  It seems like most info would be hidden behind the first few rows and it would be so cluttered that you would not be able to make out much of anything from it.

Why not try a line plot instead (year as the x axis, each region a different year).  Here is a quick example:

> data(votes.repub, package='cluster')
> matplot( t(votes.repub[1:31, 1:26]), type='l')
> 

Even better would be to group some of the regions together and use xyplot from the lattice package and have a panel for each group of regions (fewer lines per panel should be easier to see detail).

You could also use the image function (or levelplot from lattice) to create a 26*31 grid with colors used for the 3rd dimension (can be good for overall patterns, not so good for looking at detail).

Hope this helps,

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at intermountainmail.org
(801) 408-8111
 
 

> -----Original Message-----
> From: Dong GUO ???? [mailto:guo.dong99 at gmail.com] 
> Sent: Friday, July 27, 2007 12:09 PM
> To: Greg Snow
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] plot
> 
> Many thanks, Greg and Justin.
> 
> The matrix is just a 26*31 matrix - 26 years, 31 regions. I 
> am know to R, just dont know how to attach the data here yet..
> 
> As I have such matrices for nine indicators for all regions, 
> so i could show some differences by 3D plot, which I did 
> similar things in Excel. I am sure there is a way to do it in R
> 
> On 7/27/07, Greg Snow <Greg.Snow at intermountainmail.org> wrote:
> > Graphs that rely on 3-d effects tend to distort the data 
> rather than enlighten the viewer.  If your goal is to distort 
> the data (which I doubt), then most of us don't want to help. 
>  On the other hand, if you really do want to enlighten the 
> viewer (even if that is just you), then tell us what your 
> data is like and what you want to learn from it, and we will 
> be happy to give you advice on creating useful graphs.
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > Statistical Data Center
> > Intermountain Healthcare
> > greg.snow at intermountainmail.org
> > (801) 408-8111
> >
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dong GUO ??
> > > Sent: Friday, July 27, 2007 8:45 AM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] plot
> > >
> > > Greetings to the group,
> > >
> > > I would like to know if some one could help me with plot 
> 3-d column 
> > > graph of a matrix (3-d column graph in Excel).
> > >
> > > Many thanks in advance.
> > >
> > > Regards,
> > > Dong
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
>


From bensaylor at fastmail.fm  Fri Jul 27 21:03:51 2007
From: bensaylor at fastmail.fm (Ben Saylor)
Date: Fri, 27 Jul 2007 11:03:51 -0800
Subject: [R] reading stata files: preserving values of variables
 converted to factors
In-Reply-To: <Pine.LNX.4.64.0707270943230.19542@homer22.u.washington.edu>
References: <46A93E8A.7080906@fastmail.fm>
	<Pine.LNX.4.64.0707270943230.19542@homer22.u.washington.edu>
Message-ID: <46AA4197.8020208@fastmail.fm>

Thanks for the clarification.  I found the labels & values in

attributes(<dataframe>)$label.table$<varname>

which looks the same whether convert.factors is TRUE or FALSE.

Ben

Thomas Lumley wrote:
> On Thu, 26 Jul 2007, Ben Saylor wrote:
> 
>> Hi,
>>
>> I am a Stata user new to R.  I am using read.dta to read a Stata file
>> that has variables with value labels.  read.dta converts them to
>> factors, but seems to recode them with values from 1 to <number of
>> factor levels> (looking at the output of unclass(<varname>)), so the
>> original numerical values are lost.
> 
> Yes. The R factor type should not be used if you want the original 
> levels. It is not a 'labelled numeric' type and the numbers are an 
> implementation detail.
> 
>>  Using convert.factors=FALSE
>> preserves the values, but seems to discard the labels.
> 
> It doesn't discard the labels. They are kept in the attributes of the 
> data frame.
> 
>     -thomas


From gonzalezmorales at gmail.com  Fri Jul 27 21:28:11 2007
From: gonzalezmorales at gmail.com (Luis Gerardo Gonzalez Morales)
Date: Fri, 27 Jul 2007 15:28:11 -0400
Subject: [R] Calling R functions from a Microsoft SQL Server stored procedure
Message-ID: <c4c1e9860707271228n45dfbde5kde4a1dbb69290e01@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/f64ae863/attachment-0001.pl 

From thomas.harte at yahoo.com  Fri Jul 27 22:44:40 2007
From: thomas.harte at yahoo.com (Thomas Harte)
Date: Fri, 27 Jul 2007 13:44:40 -0700 (PDT)
Subject: [R]  error in using R2WinBUGS on Ubuntu 6.10 Linux
Message-ID: <97753.85074.qm@web43137.mail.sp1.yahoo.com>

what version of Wine are you running? and is this the patched (1.4.1) version of 
WinBUGS that you're trying to run?

cheers,

thomas.

>  Date: Thu, 26 Jul 2007 12:03:38 -0400 (EDT)
>  From: <meyerjp at jmu.edu>
>  Subject: [R] error in using R2WinBUGS on Ubuntu 6.10 Linux
>  To: r-help at stat.math.ethz.ch
>  Message-ID: <20070726120338.AUQ96591 at mpmail1.jmu.edu>
>  Content-Type: text/plain; charset=us-ascii

>  I am trying to run WinBUGS 1.4 from the Ubuntu 6.10 Linux distribution. I am using the
>   R2WinBUGS packages with the  source file listed below. WinBUGS appears to run
properly, but
>   I get the following message after WinBUGS starts in WINE. Does anyone know what may
be
>   causing this error and what the correction may be?

>  Thanks

>  ERROR MESSAGE:

>  fixme:ole:GetHGlobalFromILockBytes cbSize is 13824
>  err:ole:CoGetClassObject class {0003000a-0000-0000-c000-000000000046} not registered
>  err:ole:CoGetClassObject class {0003000a-0000-0000-c000-000000000046} not registered
>  err:ole:CoGetClassObject no class object {0003000a-0000-0000-c000-000000000046} could
be
>   created for context 0x3
>  fixme:keyboard:RegisterHotKey (0x10032,13,0x00000002,3): stub
>  fixme:ntdll:RtlNtStatusToDosErrorNoTeb no mapping for 8000000a
>  err:ole:local_server_thread Failure during ConnectNamedPipe 317



>  R SOURCE FILE:

>  rm(list=ls(all=TRUE))

>  library(R2WinBUGS)

>  inits<-function(){
>  	list(alpha0 = 0, alpha1 = 0, alpha2 = 0, alpha12 = 0, sigma = 1)
>  }

>  data<-list(r = c(10, 23, 23, 26, 17, 5, 53, 55, 32, 46, 10,   8, 10,   8, 23, 0,  3,
22,
>   15, 32, 3),
>  n = c(39, 62, 81, 51, 39, 6, 74, 72, 51, 79, 13, 16, 30, 28, 45, 4, 12, 41, 30, 51,
7),
>  x1 = c(0,   0,  0,   0,   0, 0,   0,   0,  0,   0,   0,  1,   1,   1,   1, 1,   1,  1,
 
>   1,   1, 1),
>  x2 = c(0,   0,  0,   0,   0, 1,   1,   1,  1,   1,   1,  0,   0,   0,   0, 0,   1,  1,
 
>   1,   1, 1),
>  N = 21)

>  test<-bugs(data,inits,

>  model.file="/home/meyerjp/rasch/test.bug",

>  parameters=c("alpha0","alpha1","alpha12","alpha2","sigma"),

>  n.chains=2,n.iter=10000,n.burnin=1000,

>  bugs.directory="/home/meyerjp/.wine/drive_c/Program Files/WinBUGS14/",
>  working.directory="/home/meyerjp/rasch/working",

>  debug=FALSE,
>  WINEPATH="/usr/bin/winepath",
>  newWINE=TRUE)


From mazatlanmexico at yahoo.com  Fri Jul 27 21:39:57 2007
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Fri, 27 Jul 2007 12:39:57 -0700 (PDT)
Subject: [R] Creating an instance of R from MS Access?
Message-ID: <428638.87453.qm@web56609.mail.re3.yahoo.com>

Hi all:
Does anyone know if it's at all possible to create a
connection to R from MS access? For example, if I have
a table and want to export it to R,generate a graph in
R and import it back to MS access. I can do this with
sigmaPlot and other graphic programs but just
wondering if it can be done with R. Thanks



 Felipe D. Carrillo
  Fishery Biologist
  US Fish & Wildlife Service
  Red Bluff, California 96080



      ____________________________________________________________________________________


From topkatz at msn.com  Sat Jul 28 00:30:41 2007
From: topkatz at msn.com (Talbot Katz)
Date: Fri, 27 Jul 2007 18:30:41 -0400
Subject: [R] Matrix Multiplication, Floating-Point, etc.
Message-ID: <BAY108-F24AABC3636FC394FB1D4A8AAF30@phx.gbl>

Hi.

I recently tried the following in R 2.5.1 on Windows XP:

>ev2<-c(0.8,-0.6)
>ev1<-c(0.6,0.8)
>ev1%*%ev2
              [,1]
[1,] -2.664427e-17
>sum(ev1*ev2)
[1] 0
>

(I got the same result with R 2.4.1 on a different Windows XP machine.)

I expect this issue is very familiar and probably has been discussed in this 
forum before.  Can someone please point me to some documentation or 
discussion about this?  Is there some standard way to get the "correct" 
answer from %*%?

Thanks!

--  TMK  --
212-460-5430	home
917-656-5351	cell


From r.turner at auckland.ac.nz  Sat Jul 28 00:34:48 2007
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 28 Jul 2007 10:34:48 +1200
Subject: [R] manipulating arrays
In-Reply-To: <da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
References: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
	<da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
Message-ID: <CBCD0FA3-B5B2-43E2-A5DE-4854C286725D@auckland.ac.nz>

?append

	cheers,

		Rolf Turner

On 28/07/2007, at 2:12 AM, Henrique Dallazuanna wrote:

> Hi, I don't know if is the more elegant way, but:
>
> X<-c(1,2,3,4,5)
> X <- c(X[1], 0, X[2:5])
>
>
> --  
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
>
> On 27/07/07, Nair, Murlidharan T <mnair at iusb.edu> wrote:
>>
>> Can I insert an element in an array at a particular position without
>> destroying the already existing element?
>>
>>
>>
>> X<-c(1,2,3,4,5)
>>
>>
>>
>> I want to insert an element between 1 and 2.
>>
>>
>>
>> Thanks ../Murli
>>
>>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

######################################################################
Attention:\ This e-mail message is privileged and confidenti...{{dropped}}


From mjanis1 at yahoo.com  Fri Jul 27 23:51:12 2007
From: mjanis1 at yahoo.com (Michael Janis)
Date: Fri, 27 Jul 2007 14:51:12 -0700
Subject: [R] 2nd R Console
Message-ID: <004601c7d098$3fcf11d0$22cf7ca6@heron>

Hi,

I was reading a thread: [R] "2nd R console" and had a similar question
regarding having more than one R console open at a time.  However, my
question differs from that of the thread:

Is it possible, or is there a wrapper that will allow one, to open an
arbitrary number of R consoles which access the same R session (all objects
in that session, etc.).  This would be R on linux accessed through a shell -
kind of like using GNU screen multi-user such that people could work
collaboratively on a given session.  The problem with screen is that all
commands are interleaved in the same terminal, which is confusing and does
not allow access to the command prompt at the same time, rather it would be
sequential.  I know there will be "why" questions but it is useful in an
academic environment.  Basically we have a memory machine for large genomic
analysis - and we could set that up as an Rserver, but this placing R into a
multi-user engine is better suited for our immediate needs.  Does anybody
have thoughts on this?

Thanks for considering,

Michael Janis
UCLA Bioinformatics


From ccleland at optonline.net  Sat Jul 28 00:01:14 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 27 Jul 2007 18:01:14 -0400
Subject: [R] Q: extracting data from lm
In-Reply-To: <46AA6913.8060801@gmail.com>
References: <46AA6913.8060801@gmail.com>
Message-ID: <46AA6B2A.8010408@optonline.net>

D. R. Evans wrote:
> Warning: I am a complete newbie to R. I have read ISwR, but I am still
> finding myself completely stuck on some simple concepts.
> 
> I have tried everything I can think of to solve this one, and finally
> decided that enough was enough and I need a pointer to a solution.
> 
> I have the following summary from lm():
> 
> ----
> 
>> summary(lm(nu1~nu4))
> 
> Call:
> lm(formula = nu1 ~ nu4)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -1572.62  -150.38   -21.70   168.57  2187.84
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) 29.88739   43.68881   0.684    0.494
> nu4          1.00036    0.01025  97.599   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 470.9 on 298 degrees of freedom
> Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
> F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16
> 
> ----
> 
> But I want to access some of these numbers programmatically. I finally
> figured out that to get the estimate of the nu4 coefficient I need to do:
> 
> ----
> 
>> lm(nu1~nu4)$coefficients[2]
>      nu4
> 1.000363
> 
> ----
> 
> which to me as a long-time C++ programmer is close to black magic (I've
> been programming since 1972; I have to say that R is unlike anything I've
> ever seen, and it's far from trivial to get my head around some of it --
> for example, how I could have known a priori that the above is the way to
> get the nu4 coefficient is beyond me). Anyway, having figured out how to
> get the estimate of the coefficient, I not-unnaturally wanted also to find
> a way to access the std. error of the estimate (the value 0.01025 in the
> summary). But I am completely mystified as to how to do it :-(
> 
> Any help gratefully (VERY gratefully) received, and I apologise if this is
> a really, really stupid question and that the answer lies somewhere in some
> documentation that I've obviously not properly taken on board.

coef(summary(lm(nu1 ~ nu2)))[,2]

  Also, try the following which is often useful:

str(summary(lm(nu1 ~ nu2)))

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From dsp0718 at googlemail.com  Sat Jul 28 01:06:43 2007
From: dsp0718 at googlemail.com (David Pain)
Date: Sat, 28 Jul 2007 00:06:43 +0100
Subject: [R] Package manual examples - 'unexpected$undefined' errors
Message-ID: <acb54ab40707271606k114d8651jd7338c6d6a24e288@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070728/269c0b5c/attachment-0001.pl 

From marc_schwartz at comcast.net  Sat Jul 28 00:46:52 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Fri, 27 Jul 2007 17:46:52 -0500
Subject: [R] Q: extracting data from lm
In-Reply-To: <46AA6913.8060801@gmail.com>
References: <46AA6913.8060801@gmail.com>
Message-ID: <1185576412.3625.18.camel@Bellerophon.localdomain>

On Fri, 2007-07-27 at 15:52 -0600, D. R. Evans wrote:
> Warning: I am a complete newbie to R. I have read ISwR, but I am still
> finding myself completely stuck on some simple concepts.
> 
> I have tried everything I can think of to solve this one, and finally
> decided that enough was enough and I need a pointer to a solution.
> 
> I have the following summary from lm():
> 
> ----
> 
> > summary(lm(nu1~nu4))
> 
> Call:
> lm(formula = nu1 ~ nu4)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -1572.62  -150.38   -21.70   168.57  2187.84
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) 29.88739   43.68881   0.684    0.494
> nu4          1.00036    0.01025  97.599   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 470.9 on 298 degrees of freedom
> Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
> F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16
> 
> ----
> 
> But I want to access some of these numbers programmatically. I finally
> figured out that to get the estimate of the nu4 coefficient I need to do:
> 
> ----
> 
> > lm(nu1~nu4)$coefficients[2]
>      nu4
> 1.000363
> 
> ----
> 
> which to me as a long-time C++ programmer is close to black magic (I've
> been programming since 1972; I have to say that R is unlike anything I've
> ever seen, and it's far from trivial to get my head around some of it --
> for example, how I could have known a priori that the above is the way to
> get the nu4 coefficient is beyond me). Anyway, having figured out how to
> get the estimate of the coefficient, I not-unnaturally wanted also to find
> a way to access the std. error of the estimate (the value 0.01025 in the
> summary). But I am completely mystified as to how to do it :-(
> 
> Any help gratefully (VERY gratefully) received, and I apologise if this is
> a really, really stupid question and that the answer lies somewhere in some
> documentation that I've obviously not properly taken on board.

It looks like Peter references the notion of 'extractor functions' below
middle on page 97, but does not describe them, unless I am going blind
(or just tired).

In either case, you can supplement Peter's great intro book with many of
the of the other free references available via the R web site or An
Introduction to R, available with your installation or from the web
site.

In the latter case go here:

http://cran.r-project.org/doc/manuals/R-intro.html#Statistical-models-in-R

and scroll down to section 11.3.

Take note also of the 'Value' section of ?lm, which describes the
general contents of an lm object.

Another function that is always helpful is str(), which will display the
structure of an R object, enabling you to gain insights into the
components, their names and content. See ?str

In addition, take note of the 'See Also' section of ?lm, which does list
the generic functions coef, effects, residuals, fitted, vcov.

Using coef() will get you the values from the Coefficients part of the
output above, then enabling you to subset them to get the other values
as you require.

Another option relative to seeking assistance, is to review the Posting
Guide, a link to which is on the bottom of every post to the list. There
is a list of resources there that can guide you through how to search
for help, including utilizing the archives of the R help list where
keyword searches will be of tremendous assistance.

HTH,

Marc Schwartz


From doc.evans at gmail.com  Fri Jul 27 23:52:19 2007
From: doc.evans at gmail.com (D. R. Evans)
Date: Fri, 27 Jul 2007 15:52:19 -0600
Subject: [R] Q: extracting data from lm
Message-ID: <46AA6913.8060801@gmail.com>

Warning: I am a complete newbie to R. I have read ISwR, but I am still
finding myself completely stuck on some simple concepts.

I have tried everything I can think of to solve this one, and finally
decided that enough was enough and I need a pointer to a solution.

I have the following summary from lm():

----

> summary(lm(nu1~nu4))

Call:
lm(formula = nu1 ~ nu4)

Residuals:
     Min       1Q   Median       3Q      Max
-1572.62  -150.38   -21.70   168.57  2187.84

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 29.88739   43.68881   0.684    0.494
nu4          1.00036    0.01025  97.599   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 470.9 on 298 degrees of freedom
Multiple R-Squared: 0.9697,     Adjusted R-squared: 0.9696
F-statistic:  9526 on 1 and 298 DF,  p-value: < 2.2e-16

----

But I want to access some of these numbers programmatically. I finally
figured out that to get the estimate of the nu4 coefficient I need to do:

----

> lm(nu1~nu4)$coefficients[2]
     nu4
1.000363

----

which to me as a long-time C++ programmer is close to black magic (I've
been programming since 1972; I have to say that R is unlike anything I've
ever seen, and it's far from trivial to get my head around some of it --
for example, how I could have known a priori that the above is the way to
get the nu4 coefficient is beyond me). Anyway, having figured out how to
get the estimate of the coefficient, I not-unnaturally wanted also to find
a way to access the std. error of the estimate (the value 0.01025 in the
summary). But I am completely mystified as to how to do it :-(

Any help gratefully (VERY gratefully) received, and I apologise if this is
a really, really stupid question and that the answer lies somewhere in some
documentation that I've obviously not properly taken on board.


From doc.evans at gmail.com  Sat Jul 28 00:19:15 2007
From: doc.evans at gmail.com (D. R. Evans)
Date: Fri, 27 Jul 2007 16:19:15 -0600
Subject: [R] Q: extracting data from lm
In-Reply-To: <46AA6B2A.8010408@optonline.net>
References: <46AA6913.8060801@gmail.com> <46AA6B2A.8010408@optonline.net>
Message-ID: <256f4e900707271519t189988d2nfee9f1cdb0b01ca1@mail.gmail.com>

On 27/07/07, Chuck Cleland <ccleland at optonline.net> wrote:

>
> coef(summary(lm(nu1 ~ nu2)))[,2]
>
>   Also, try the following which is often useful:
>
> str(summary(lm(nu1 ~ nu2)))
>

Oh, wow! Thank you.

Incidentally, just in case anyone got the wrong end of the stick, I'm
not at all complaining about R. It's good at my age to be faced with
something so different. And from an architectural standpoint I
appreciate its elegance and innate power. It's just the logistics of
knowing exactly what to type that causes me to feel overwhelmed, and
although I've become very used in the past couple of days to typing
?<something> I'm not much good yet at finding out how to help myself
if that doesn't tell me what I want to know.


From stanhopkins at comcast.net  Sat Jul 28 03:10:22 2007
From: stanhopkins at comcast.net (Stan Hopkins)
Date: Fri, 27 Jul 2007 20:10:22 -0500
Subject: [R] Error when using the cat function
Message-ID: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/f238859b/attachment-0001.pl 

From amstat2006 at gmail.com  Sat Jul 28 02:18:33 2007
From: amstat2006 at gmail.com (Am Stat)
Date: Fri, 27 Jul 2007 20:18:33 -0400
Subject: [R] data order by different level of variables
Message-ID: <c8b63a350707271718u2b539363r71c244b11832fae@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/66ce4ebc/attachment-0001.pl 

From gregory_gentlemen at yahoo.ca  Sat Jul 28 05:28:25 2007
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Fri, 27 Jul 2007 23:28:25 -0400 (EDT)
Subject: [R] generating symmetric matrices
Message-ID: <541365.65996.qm@web63504.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/9bf3b4bc/attachment-0001.pl 

From ggrothendieck at gmail.com  Sat Jul 28 02:49:15 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 27 Jul 2007 20:49:15 -0400
Subject: [R] manipulating arrays
In-Reply-To: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
References: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
Message-ID: <971536df0707271749h711b01e8ge143d612f2d88622@mail.gmail.com>

Try this:

> x <- 11:15
> append(x, values = 99, after = 1)
[1] 11 99 12 13 14 15


On 7/27/07, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> Can I insert an element in an array at a particular position without
> destroying the already existing element?
>
>
>
> X<-c(1,2,3,4,5)
>
>
>
> I want to insert an element between 1 and 2.
>
>
>
> Thanks ../Murli
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Horace.Tso at pgn.com  Fri Jul 27 18:52:06 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Fri, 27 Jul 2007 09:52:06 -0700
Subject: [R] getting the name of variables passed to a function
Message-ID: <46A9C046020000650000777F@pgn.com>

Folks,

I've entered into an R programming territory I'm not very familiar with, thus this probably very elementary question concerning the mechanic of a function call.

I want to know from within a function the name of the variables I pass down. The function makes use of the "..." to allow for multiple unknown arguments,

myfun = function(...) { do something }

In the body I put,

{    
nm <- names(list(...))
nm
}

When the function is called with two vectors x, and y

myfun(x, y)

It returns NULL. However, when the call made is,

>myfun(x=x, y=y)

The result is
[1] "x" "y"

Question : how do i get the names of the unknown variables without explicitly saying x=x...

Thanks in advance.

Horace


From bolker at ufl.edu  Fri Jul 27 21:28:30 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Fri, 27 Jul 2007 19:28:30 +0000 (UTC)
Subject: [R] R codes for g-and-h distribution
References: <687475.90110.qm@web54005.mail.re2.yahoo.com>
Message-ID: <loom.20070727T212305-710@post.gmane.org>

filame uyaco <filams0704 <at> yahoo.com> writes:

> 
> 
> hi!
> 
> I would like to ask help how to generate numbers from g-and-h distribution. 
This distribution is like
> normal distribution  but span more of the kurtosis and skewness plane. Has R
any package on how to generate
> them? 
> 

  Someone else asked for this in 2005, but I didn't see any 
answers.  If the wiki weren't down I would put this up there ...

## http://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/ghpdf.htm

## G(p,g,h) = exp((g*Zp)-1)*exp((h*Zp^2)/2)/g

[n.b. formatting on this page is very confusing, I inserted
 parentheses]

##  with Zp denoting the normal percent point function of p. When g = 0
##  and h = 0, the g-and-h distribution reduces to a standard normal
##  distribution.

## The g-and-h probability density function is computed by taking the
## numerical derivative of the cumulative distribution function (which is
## turn computed by numerically inverting the percent point function
## using a bisection method).

## [from
## http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm:
## The percent point function (ppf) is the inverse of the cumulative
## distribution function. For this reason, the percent point function
## is also commonly referred to as the inverse distribution function.

## thus R's "q" functions are percent point functions

qgh <- function(q,g,h) {
  Zp <- qnorm(q)
  ## not vectorized!
  if (g==0) Zp else (exp(g*Zp)-1)*exp((h*Zp^2/2))/g
}

## since the quantile function is defined, it makes generating
##  random values easy!
rgh <- function(n,g,h) {
  qgh(runif(n),g,h)
}

## eps sets distance from 1 for searching
##  strategy could probably be improved (first approx. based on
##   qnorm??)
pgh <- function(p,g,h,eps=1e-7) {
  uniroot(function(z) { qgh(z,g,h) - p}, interval=c(eps,1-eps))$root
}

dgh <- function(x,g,h,log=FALSE,ndep=1e-3,...) {
  ## crude vectorization in x (not g or h)
  if (length(x)>1) return(sapply(x,dgh,g=g,h=h,log=log,ndep=ndep,...))
  r <- (pgh(x+ndep,g,h)-pgh(x,g,h))/ndep
  if (log) log(r) else r
}
  

## examples ...
set.seed(1001)
## should be approx. normal
r1 = rgh(10000,1e-5,0)
r2 = rgh(10000,1e-5,0)
r3 = rgh(10000,1e-5,0)

## plot 3 different samples
plot(density(r1))
lines(density(r2))
lines(density(r3))
curve(dnorm(x),col=2,add=TRUE)
curve(dgh(x,1e-5,0),col=4,add=TRUE)
## some slight numerical glitches -- e.g. around
## x=-2.6

r4 = rgh(50000,0.4,0.4)
plot(density(r4,n=1024),xlim=c(-10,10))
curve(dnorm(x),add=TRUE,col=2)
curve(dgh(x,0.4,0.4),add=TRUE,col=4)
legend("topleft",c("density","g-h","normal"),
       lty=rep(1,3),col=c(1,4,2))
## note glitch again, this time around
## y=-3.89


From brown_emu at yahoo.com  Sat Jul 28 11:46:27 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 28 Jul 2007 02:46:27 -0700 (PDT)
Subject: [R] Fitting exponential curve to data points
In-Reply-To: <loom.20070724T175645-930@post.gmane.org>
Message-ID: <316800.41764.qm@web39714.mail.mud.yahoo.com>

Sorry, just got back into town.

I wonder if AIC, BIC, or cross-validation scoring couldn't also be used as
criteria for model selection - I've seen it mostly in the context of variable
selection rather than 'form' selection but in principle might apply here?


--- Dieter Menne <dieter.menne at menne-biomed.de> wrote:

> Andrew Clegg <andrew.clegg <at> gmail.com> writes:
> 
> > 
> > ... If I want to demonstrate that a non-linear curve fits
> > better than an exponential, what's the best measure for that? Given
> > that neither of nls() or optim() provide R-squared. 
> 
> To supplement Karl's comment, try Douglas Bates' (author of nls) comments
> on the
> matter
> 
> http://www.ens.gu.edu.au/ROBERTK/R/HELP/00B/0399.HTML
> 
> Short summary:
>     * ... "the lack of automatic ANOVA, R^2 and adj. R^2 from nls is a
> feature,
> not a bug :-)"
>     * My best advice regarding R^2 statistics with nonlinear models is, as
> Nancy
> Reagan suggested, "Just say no."
> 
> Dieter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



      ____________________________________________________________________________________


From dsp0718 at googlemail.com  Sat Jul 28 12:03:08 2007
From: dsp0718 at googlemail.com (David Pain)
Date: Sat, 28 Jul 2007 11:03:08 +0100
Subject: [R] Package manual examples - 'unexpected$undefined' errors
Message-ID: <acb54ab40707280303g4fc79bbyaf475eba094f8eb8@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070728/f9ad16e5/attachment-0001.pl 

From Greg.Snow at intermountainmail.org  Sat Jul 28 06:02:06 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 22:02:06 -0600
Subject: [R] Q: extracting data from lm
References: <46AA6913.8060801@gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A163@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/d1840d99/attachment-0002.pl 

From brown_emu at yahoo.com  Sat Jul 28 11:57:49 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sat, 28 Jul 2007 02:57:49 -0700 (PDT)
Subject: [R] manipulating arrays
In-Reply-To: <da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
Message-ID: <400719.24564.qm@web39711.mail.mud.yahoo.com>

I think you are looking for append(), though it won't modify the object
in-place like Python [I believe that is a product of R's 'functional
programming' philosophy].

might want to check this entertaining thread:
http://tolstoy.newcastle.edu.au/R/help/04/11/7727.html

in this example it would be like

> c(X[1], 0, X[2:5])
[1] 1 0 2 3 4 5
> append(X,0,1)
[1] 1 0 2 3 4 5


--- Henrique Dallazuanna <wwwhsd at gmail.com> wrote:

> Hi, I don't know if is the more elegant way, but:
> 
> X<-c(1,2,3,4,5)
> X <- c(X[1], 0, X[2:5])
> 
> 
> -- 
> Henrique Dallazuanna
> Curitiba-Paran?-Brasil
> 25? 25' 40" S 49? 16' 22" O
> 
> On 27/07/07, Nair, Murlidharan T <mnair at iusb.edu> wrote:
> >
> > Can I insert an element in an array at a particular position without
> > destroying the already existing element?
> >
> >
> >
> > X<-c(1,2,3,4,5)
> >
> >
> >
> > I want to insert an element between 1 and 2.
> >
> >
> >
> > Thanks ../Murli
> >
> >
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> > ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 



       
____________________________________________________________________________________

Comedy with an Edge to see what's on, when.


From mjanis at chem.ucla.edu  Sat Jul 28 10:19:10 2007
From: mjanis at chem.ucla.edu (Michael Janis)
Date: Sat, 28 Jul 2007 01:19:10 -0700
Subject: [R] 2nd R Console
Message-ID: <001a01c7d0ef$fb0d9140$13321b82@heron>

Hi,

I was reading a thread: [R] "2nd R console" and had a similar question
regarding having more than one R console open at a time.  However, my
question differs from that of the thread:

Is it possible, or is there a wrapper that will allow one, to open an
arbitrary number of R consoles which access the same R session (all objects
in that session, etc.).  This would be R on linux accessed through a shell -
kind of like using GNU screen multi-user such that people could work
collaboratively on a given session.  The problem with screen is that all
commands are interleaved in the same terminal, which is confusing and does
not allow access to the command prompt at the same time, rather it would be
sequential.  I know there will be "why" questions but it is useful in an
academic environment.  Basically we have a memory machine for large genomic
analysis - and we could set that up as an Rserver, but this placing R into a
multi-user engine is better suited for our immediate needs.  Does anybody
have thoughts on this?

Thanks for considering,

Michael Janis
UCLA Bioinformatics


From jim at bitwrit.com.au  Sat Jul 28 14:02:55 2007
From: jim at bitwrit.com.au (Jim Lemon)
Date: Sat, 28 Jul 2007 22:02:55 +1000
Subject: [R] Average plan
In-Reply-To: <11811324.post@talk.nabble.com>
References: <11811324.post@talk.nabble.com>
Message-ID: <46AB306F.3040703@bitwrit.com.au>

Nok Noy wrote:
> Hello, 
> 
> I'm looking for a method to compute an average plan from 4 or 5 point in an
> cartesian space. I'm sure It can be done using a less-square method but
> maybe it a function already exist in R system to get this plan. 
> Can somebody help me to solve this problem (I'm looking on the net for hours
> but didn't find anything realy satisfiying me)

Hi Nok,
I haven't seen any answers to your question, and this may be due to the 
fact that no one knows what you are asking. Do you mean that you want to 
define a _plane_ that is the best fit to more than three points in a 
three dimensional system?

Jim


From kamauallan at yahoo.com  Sat Jul 28 14:48:47 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Sat, 28 Jul 2007 05:48:47 -0700 (PDT)
Subject: [R] Obtaining summary of frequencies of value occurrences for a
	variable in a multivariate dataset.
In-Reply-To: <644e1f320707270513h5adabb8au3a870079b4bfbf80@mail.gmail.com>
Message-ID: <657735.44420.qm@web53507.mail.re2.yahoo.com>

Hi Jim,
The problem description.
I am trying to identify mutations in a given gene from
a particular genome (biological genome sequence).
I have two CSV files consisting of sequences. One file
consists of reference (documented,curated accepted as
standard) sequences. The other consists of sample
sequences I am trying to identify mutations within. In
both files the an individual sequence is contained in
a single record, it?s amino acid residues ( the actual
sequence of alphabets each representing a given amino
acid for example ?A? stands for ?Alanine?, ?C? for
Cysteine and so on) are each allocated a single field
in the CSV file.
The sequences in both files have been well aligned,
each contain 115 residues with the first residue is
contained in the field 5. The fields 1 to 4 are
allocated for metadata (name of sequence and so on).
My task is to compile a residue occurrence count for
each residue present in a given field in the reference
sequence dataset and use this information when reading
each sequence in the sample dataset to identify a
mutation. For example for position 9 of the sample
sequence ?bb? a ?P? is found and according to our
reference sequence dataset of summaries, at position 9
?P? may not even exist or may have an occurrence of
10% or so will be classified as mutation, (I could
employ a cut of parameter for mutation
classification).


Allan.

--- jim holtman <jholtman at gmail.com> wrote:

> results=()#character()
> myVariableNames=names(x.val)
> results[length(myVariableNames)]<-NA
> 
> for (i in myVariableNames){
>     results[i]<-names(x.val[[i]])    # this does not
> work it returns a
> NULL (how can i convert this to x.val$"somevalue" ?
> )
> }
> 
> 
> 
> On 7/27/07, Allan Kamau <kamauallan at yahoo.com>
> wrote:
> > Hi All,
> > I am having difficulties finding a way to find a
> substitute to the command "names(v.val$PR14)" so
> that I could generate the command on the fly for all
> PR14 to PR200 (please see the previous discussion
> below to understand what the object x.val contains)
> . I have tried the following
> >
> > >results=()#character()
> > >myVariableNames=names(x.val)
> > >results[length(myVariableNames)]<-NA
> >
> > >for
> as.vector(unlist(strsplit(str,",")),mode="list")
> > +    results[i]<-names(x.val$i)    # this does not
> work it returns a NULL (how can i convert this to
> x.val$"somevalue" ? )
> > >}
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: Allan Kamau <kamauallan at yahoo.com>
> > To: r-help at stat.math.ethz.ch
> > Sent: Thursday, July 26, 2007 10:03:17 AM
> > Subject: Re: [R] Obtaining summary of frequencies
> of value occurrences for a variable in a
> multivariate dataset.
> >
> > Thanks so much Jim, Andaikalavan, Gabor and others
> for the help and suggestions.
> > The solution will result in a matrix containing
> nested matrices to enable each variable name, each
> variables distinct value and the count of the
> distinct value to be accessible individually.
> > The main matrix will contain the variable names,
> the first level nested matrices will consist of the
> variables unique values, and each such variable
> entry will contain a one element vector to contain
> the count or occurrence frequency.
> > This matrix can now be used in comparing other
> similar datasets for variable values and their
> frequencies.
> >
> > Building on the input received so far, a probable
> solution in building the matrix will include the
> following.
> >
> >
> > 1)I reading the csv file (containing column
> headers)
> >
>
>my_data=read.table("<path/to/my/data.csv>",header=TRUE,sep=",",dec=".",fill=TRUE)
> >
> > 2)I group the values in each variable producing an
> occurrence count(frequency)
> > >x.val<-apply(my_data,2,table)
> >
> > 3)I obtain a vector of the names of the variables
> in the table
> > >names(x.val)
> >
> > 4)Now I make use of the names (obtained in step 3)
> to obtain a vector of distinct values in a given
> variable (in the example below the variable name is
> $PR14)
> > >names(v.val$PR14)
> >
> > 5)I obtain a vector (with one element) of the
> frequency of a value obtained from the step above
> (in our example the value is "V")
> > >as.vector(x.val$PR14["V"])
> >
> > Todo:
> > Now I will need to place the steps above in a
> script (consisting of loops) to build the matrix,
> step 4 and 5 seem tricky to do programatically.
> >
> > Allan.
> >
> >
> > ----- Original Message ----
> > From: jim holtman <jholtman at gmail.com>
> > To: Allan Kamau <kamauallan at yahoo.com>
> > Cc: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>;
> r-help at stat.math.ethz.ch
> > Sent: Wednesday, July 25, 2007 1:50:55 PM
> > Subject: Re: [R] Obtaining summary of frequencies
> of value occurrences for a variable in a
> multivariate dataset.
> >
> > Also if you want to access the individual values,
> you can just leave
> > it as a list:
> >
> > > x.val <- apply(x, 2, table)
> > > # access each value
> > > x.val$PR14["V"]
> > V
> > 8
> >
> >
> >
> > On 7/25/07, Allan Kamau <kamauallan at yahoo.com>
> wrote:
> > > A subset of the data looks as follows
> > >
> > > > df[1:10,14:20]
> > >   PR10 PR11 PR12 PR13 PR14 PR15 PR16
> > > 1     V    T    I    K    V    G    D
> > > 2     V    S    I    K    V    G    G
> > > 3     V    T    I    R    V    G    G
> > > 4     V    S    I    K    I    G    G
> > > 5     V    S    I    K    V    G    G
> > > 6     V    S    I    R    V    G    G
> > > 7     V    T    I    K    I    G    G
> > > 8     V    S    I    K    V    E    G
> > > 9     V    S    I    K    V    G    G
> > > 10    V    S    I    K    V    G    G
> > >
> > > The result I would like is as follows
> > >
> > > PR10        PR11          PR12   ...
> > > [V:10]    [S:7,T:3]    [I:10]
> > >
> > > The result can be in a matrix or a vector and
> each variablename, value and frequency should be
> accessible so as to be used for comparisons with
> another dataset later.
> > > The frequency can be a count or a percentage.
> > >
> > >
> > > Allan.
> > >
> > >
> > > ----- Original Message ----
> > > From: Adaikalavan Ramasamy
> <ramasamy at cancer.org.uk>
> > > To: Allan Kamau <kamauallan at yahoo.com>
> > > Cc: r-help at stat.math.ethz.ch
> > > Sent: Tuesday, July 24, 2007 10:21:51 PM
> > > Subject: Re: [R] Obtaining summary of
> frequencies of value occurrences for a variable in a
> multivariate dataset.
> > >
> > > The name of the table should give you the
> "value". And if you have a
> > > matrix, you just need to convert it into a
> vector first.
> > >
> > >  > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ],
> nc=3 )
> > >  > m
> > >      [,1] [,2] [,3]
> > > [1,] "A"  "C"  "B"
> > > [2,] "B"  "D"  "C"
> > > [3,] "C"  "E"  "D"
> > >  > tb <- table( as.vector(m) )
> > >  > tb
> > >
> > > A B C D E
> > > 1 2 3 2 1
> > >  > paste( names(tb), ":", tb, sep="" )
> > > [1] "A:1" "B:2" "C:3" "D:2" "E:1"
> > >
> > > If this is not what you want, then please give a
> simple example.
> > >
> > > Regards, Adai
> > >
> > >
> > >
> > > Allan Kamau wrote:
> > > > Hi all,
> > > > If the question below as been answered before
> I
> > > > apologize for the posting.
> > > > I would like to get the frequencies of
> occurrence of
> > > > all values in a given variable in a
> multivariate
> > > > dataset. In short for each variable (or field)
> a
> > > > summary of values contained with in a
> value:frequency
> 
=== message truncated ===


From p.dalgaard at biostat.ku.dk  Sat Jul 28 15:25:41 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Sat, 28 Jul 2007 15:25:41 +0200
Subject: [R] manipulating arrays
In-Reply-To: <da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
References: <A32055BDEA88C34BB3DBBCD229380778013A359D@iu-mssg-mbx109.ads.iu.edu>
	<da79af330707270712k3b28e3a8u4e28894328c7a65c@mail.gmail.com>
Message-ID: <46AB43D5.8030204@biostat.ku.dk>

Henrique Dallazuanna wrote:
> Hi, I don't know if is the more elegant way, but:
>
> X<-c(1,2,3,4,5)
> X <- c(X[1], 0, X[2:5])
>   
append(X, 0, 1)


From wl2776 at gmail.com  Mon Jul 30 14:14:25 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Mon, 30 Jul 2007 05:14:25 -0700 (PDT)
Subject: [R] regular expressions : extracting numbers
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
Message-ID: <11862597.post@talk.nabble.com>




GOUACHE David wrote:
> 
> Hello all,
> 
> I have a vector of character strings, in which I have letters, numbers,
> and symbols. What I wish to do is obtain a vector of the same length with
> just the numbers.
> A quick example -
> 
> extract of the original vector :
> "lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb" "rb"
> "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"
> 
> and the type of thing I wish to end up with :
> "2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""
> 
> or, instead of "", NA would be acceptable (actually it would almost be
> better for me)
> 

> chv<-scan(what="character",sep=" ") #then copy the text from your message
> to the clipboard and paste it to the R console
> chv
 [1] "lema, rb 2%"   "rb 2%"         "rb 3%"         "rb 4%"        
 [5] "rb 3%"         "rb 2%,mineuse" "rb"            "rb"           
 [9] "rb 12"         "rb"            "rj 30%"        "rb"           
[13] "rb"            "rb 25%"        "rb"            "rb"           
[17] "rb"            "rj, rb"       

# actual replacements :

# replace non-digits with nothing
> chv.digits<-gsub("[^0-9]","",chv)
> chv.digits
 [1] "2"  "2"  "3"  "4"  "3"  "2"  ""   ""   "12" ""   "30" ""   ""   "25"
""  
[16] ""   ""   "" 

# replace empty strings with NA
> chv.digits[chv.digits==""]<-NA
> chv.digits
 [1] "2"  "2"  "3"  "4"  "3"  "2"  NA   NA   "12" NA   "30" NA   NA   "25"
NA  
[16] NA   NA   NA  

 
-- 
View this message in context: http://www.nabble.com/regular-expressions-%3A-extracting-numbers-tf4169660.html#a11862597
Sent from the R help mailing list archive at Nabble.com.


From Greg.Snow at intermountainmail.org  Sat Jul 28 06:02:06 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Fri, 27 Jul 2007 22:02:06 -0600
Subject: [R] Q: extracting data from lm
References: <46AA6913.8060801@gmail.com>
Message-ID: <07E228A5BE53C24CAD490193A7381BBB12A163@LP-EXCHVS07.CO.IHC.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070727/d1840d99/attachment-0003.pl 

From mmeredith at wcs.org  Sat Jul 28 14:20:05 2007
From: mmeredith at wcs.org (Mike Meredith)
Date: Sat, 28 Jul 2007 05:20:05 -0700 (PDT)
Subject: [R] Error when using the cat function
In-Reply-To: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>
References: <00c101c7d0b4$12aa9b90$6405a8c0@MXD32803WB>
Message-ID: <11842347.post@talk.nabble.com>


Your problem is with ifelse, not with cat.

First clue is that 

ifelse(TRUE,{print("yes")},{print("no")})  # results in "yes" being printed
TWICE. Try this:

tmp <- ifelse(TRUE,{print("yes")},{print("no")}) # one "yes"

tmp  # another "yes"

Try:

print(print("yes")) # prints "yes" and returns "yes" invisibly. This
returned value is passed on to/by ifelse.

Now try:

print(cat("yes\n")) # "yes" appears, but cat("yes") returns NULL, which
ifelse can't handle:

ifelse(TRUE, NULL, "whatever") # Gives the error you saw.

What you need is if { } else { } :

if(!inherits(dat[[n]], "factor")) {cat("yes\n")} else {cat("no\n")}

HTH, Mike.


Stan Hopkins wrote:
> 
> Is the following developed in my console output a recognized bug or am I
> using the cat function incorrectly?
> 
> Thanks,
> 
> Stan
> 
> 
> 
>> ifelse(class(data[[n]])!="factor",{print("yes")},{print("no")})
> [1] "yes"
> [1] "yes"
>> ifelse(class(data[[n]])!="factor",{cat("yes")},{cat("no")})
> yesError in ans[test & !nas] <- rep(yes, length.out = length(ans))[test &
> : 
> incompatible types (from NULL to logical) in subassignment type fix
> 
> 
>>cat("yes")
> yes 
>> class(data[[n]])!="factor"
> [1] TRUE
>> class(data[[n]])
> [1] "numeric"
>> n
> [1] 28
>> length(data[[n]])
> [1] 955
>> class(data)
> [1] "data.frame"
>> dim(data)
> [1] 955 182
>> data[[n]]
> [1] 2.5 4.9 2.6 3.0 4.7 5.0 3.9 1.5 4.8 3.2 3.6 5.2 6.3
> [14] 6.3 5.0 4.6 6.0 4.5 3.9 3.6 5.7 8.5 4.0 5.0 11.8 4.7
> [27] 7.9 2.8 4.8 5.1 4.1 4.2 3.7 2.0 2.1 1.1 14.6 7.0 3.4
> [40] 3.4 10.1 4.7 4.9 5.2 4.3 2.9 2.8 2.3 1.2 2.0 2.0 3.0
> [53] 2.0 1.1 2.0 1.0 2.0 2.0 2.7 1.0 2.0 2.0 2.0 2.0 1.1
> [66] 2.0 2.0 1.0 1.1 2.4 2.0 2.0 5.0 0.8 2.0 3.3 2.7 2.2
> [79] 2.9 1.4 2.0 1.9 1.0 1.9 2.1 2.2 2.0 2.0 1.3 3.0 1.4
> [92] 2.0 1.5 2.1 1.2 1.7 2.1 2.0 2.0 2.3 2.0 1.6 1.5 2.3
> [105] 1.1 2.0 2.0 5.0 2.4 2.0 0.8 4.0 0.0 1.7 8.3 2.0 2.0
> [118] 2.0 6.1 14.4 8.2 5.2 2.5 1.0 1.0 1.8 1.1 4.9 0.9 2.1
> [131] 1.4 1.0 1.0 3.0 2.6 2.0 1.7 1.2 3.3 2.0 1.1 1.7 1.2
> [144] 2.7 0.9 2.0 3.2 1.8 1.8 1.1 1.3 2.3 1.1 1.7 1.9 1.0
> [157] 2.3 1.1 1.0 1.2 1.5 3.2 2.2 1.6 1.0 1.7 2.5 2.0 2.0
> [170] 2.3 1.1 1.5 2.0 1.7 5.1 3.6 2.0 2.0 1.2 1.2 3.1 1.3
> [183] 1.3 2.0 1.7 1.1 2.8 2.0 2.0 1.9 2.0 2.8 4.0 8.8 4.0
> [196] 3.2 5.0 2.1 3.0 7.4 2.5 3.2 3.0 2.8 1.9 3.0 3.2 3.6
> [209] 2.8 3.2 2.1 2.5 2.2 3.0 3.7 3.2 2.3 2.7 3.1 2.5 3.0
> [222] 2.4 2.6 0.9 5.4 2.8 3.9 4.7 2.5 2.9 4.4 4.1 4.0 4.0
> [235] 2.0 4.5 3.2 3.0 4.5 6.5 7.3 1.1 9.3 5.1 4.0 4.5 4.8
> [248] 7.6 6.7 3.0 3.0 6.0 6.0 4.0 5.0 3.0 5.0 1.0 5.0 4.0
> [261] 5.0 4.0 3.8 3.0 7.0 3.0 5.0 120.0 4.0 8.0 4.0 6.0 5.0
> [274] 4.0 6.0 2.0 2.6 3.2 4.0 4.0 3.0 6.0 3.0 3.0 2.0 2.5
> [287] 5.0 5.0 3.0 3.0 4.0 7.3 2.1 6.3 6.6 15.9 3.6 2.0 9.1
> [300] 6.9 4.2 7.8 5.7 7.7 5.6 5.8 16.3 4.0 3.0 3.4 0.0 1.0
> [313] 1.0 2.7 1.6 1.6 1.0 3.0 2.0 1.0 2.0 1.3 2.0 1.4 1.0
> [326] 0.9 1.0 0.8 0.0 0.0 3.1 2.6 1.4 2.0 6.6 2.0 1.2 2.0
> [339] 1.0 1.8 1.7 2.3 1.7 0.0 1.3 2.0 3.5 1.1 0.0 1.2 1.2
> [352] 1.0 2.0 1.2 NA 1.2 2.2 2.0 2.2 1.5 1.0 2.8 1.0 1.0
> [365] 2.1 2.0 1.3 0.0 1.5 1.8 1.4 1.2 1.2 1.1 1.0 1.1 2.0
> [378] 2.0 2.4 2.0 2.8 3.1 1.1 1.8 1.3 1.4 0.7 4.0 4.7 1.0
> [391] 0.6 3.0 1.0 0.9 2.0 1.7 2.1 2.0 1.0 2.0 16.0 3.0 10.0
> [404] 5.0 1.2 0.7 1.2 1.9 1.3 1.7 1.3 2.0 1.6 4.2 3.8 1.4
> [417] 1.2 1.3 2.0 2.1 5.8 5.9 1.2 2.8 1.8 3.6 1.8 1.9 1.1
> [430] 1.3 0.9 2.0 3.2 1.7 1.7 2.9 1.6 5.0 4.0 1.9 2.2 2.0
> [443] 2.7 2.5 1.1 2.0 1.7 1.5 1.9 1.1 1.6 5.2 1.5 1.4 1.0
> [456] 1.9 1.4 1.9 2.2 2.3 3.9 1.7 0.8 0.9 1.5 1.7 2.9 1.2
> [469] 1.9 1.8 2.6 1.4 2.1 1.6 1.7 1.6 1.4 2.0 2.1 1.0 5.0
> [482] 2.3 2.5 1.0 1.0 1.3 2.3 1.1 1.8 0.9 1.5 1.3 1.0 0.8
> [495] 1.0 0.7 0.9 0.9 2.0 2.9 2.6 0.6 1.6 2.0 0.9 1.0 1.1
> [508] 2.0 0.9 1.1 2.0 4.0 3.0 1.0 2.0 2.0 1.4 3.0 3.0 1.3
> [521] 1.0 1.2 0.8 2.0 0.0 0.0 0.7 1.4 1.0 0.8 1.2 1.4 2.1
> [534] 1.0 1.0 1.4 1.2 1.1 4.0 1.3 3.0 1.7 2.0 1.0 1.6 2.0
> [547] 0.9 6.0 1.7 1.7 1.7 1.0 0.8 0.6 2.0 2.0 1.0 2.0 1.4
> [560] 1.0 1.3 1.0 1.0 1.1 1.0 1.1 5.0 4.0 2.0 1.6 3.0 2.1
> [573] 1.2 2.0 0.9 1.2 1.0 1.1 1.9 2.1 2.2 1.0 1.5 1.3 3.0
> [586] 2.0 3.6 2.0 2.0 1.5 11.4 5.2 4.5 3.4 1.6 2.1 1.2 2.4
> [599] 2.1 2.3 1.7 2.0 1.4 0.5 1.6 1.9 2.6 0.4 1.3 1.4 1.2
> [612] 1.1 1.4 2.3 1.0 1.7 1.1 3.4 1.4 2.4 1.2 1.0 1.3 1.0
> [625] 1.2 0.8 2.1 1.7 2.1 0.9 1.4 1.2 1.9 1.1 2.3 1.5 3.0
> [638] 3.0 4.9 5.8 3.0 3.0 4.2 1.1 2.5 4.9 2.0 1.9 1.8 1.2
> [651] 2.0 2.2 1.4 1.8 2.0 1.2 3.2 1.5 2.0 3.5 2.0 0.8 1.8
> [664] 1.1 2.0 2.2 1.4 1.1 2.0 1.7 1.4 3.8 4.0 1.7 1.5 1.2
> [677] 1.1 2.0 3.0 21.0 6.0 20.0 5.0 20.0 13.0 4.0 2.6 2.8 6.1
> [690] 2.1 1.8 2.2 1.9 1.5 4.0 2.9 2.6 2.3 2.2 3.3 3.5 1.2
> [703] 1.5 3.7 2.3 3.0 1.9 2.5 1.5 1.7 2.5 3.0 2.6 1.8 2.5
> [716] 0.9 3.1 1.5 2.1 2.5 0.6 1.9 1.7 3.7 7.4 2.4 3.3 3.2
> [729] 1.2 1.3 2.0 1.4 3.4 1.7 3.5 1.7 2.0 1.3 0.8 3.0 1.9
> [742] 1.9 20.6 3.8 3.8 1.2 1.5 3.2 6.1 5.8 6.6 4.0 5.7 4.0
> [755] 3.0 4.7 6.8 6.9 4.1 1.9 4.5 3.8 2.7 2.3 2.5 2.3 2.6
> [768] 3.8 1.8 2.4 1.8 1.9 6.1 5.1 4.0 3.8 2.8 3.4 3.1 2.3
> [781] 7.5 3.0 3.0 3.1 2.4 6.0 2.3 5.0 2.8 2.7 2.2 5.0 5.0
> [794] 3.0 9.0 7.0 7.0 7.0 9.0 8.0 9.0 2.0 4.0 4.0 3.0 3.0
> [807] 2.0 2.0 3.0 4.0 3.0 3.0 7.9 11.0 16.0 4.0 7.7 5.0 6.6
> [820] 16.0 9.0 19.0 4.0 4.0 7.5 6.6 22.0 20.0 7.2 7.1 7.6 6.6
> [833] 6.2 7.8 6.9 10.9 11.2 6.0 5.0 8.0 7.0 4.0 3.0 6.0 4.0
> [846] 2.6 6.0 7.0 4.0 1.4 2.0 6.0 6.0 6.0 6.0 24.0 27.6 15.8
> [859] 8.0 7.8 7.3 8.2 5.3 3.4 18.1 31.6 5.8 5.5 4.0 4.1 4.7
> [872] 4.9 3.9 0.5 3.9 6.2 3.0 3.0 4.0 2.0 3.0 3.0 2.0 2.0
> [885] 2.0 0.0 3.0 2.0 2.0 4.0 1.6 1.7 2.0 2.0 2.0 2.0 26.0
> [898] 2.0 2.0 2.0 3.0 2.0 3.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0
> [911] 2.0 0.0 2.0 2.0 2.0 2.0 5.0 2.0 11.7 10.4 8.0 4.0 11.1
> [924] 13.2 14.6 11.7 13.4 14.3 15.8 25.6 10.0 6.0 9.1 9.7 4.0 10.7
> [937] 6.0 5.0 10.9 10.0 10.6 12.9 12.3 11.6 11.8 13.3 15.1 10.7 11.0
> [950] 13.5 32.9 12.9 8.4 8.1 12.5
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Error-when-using-the-cat-function-tf4161670.html#a11842347
Sent from the R help mailing list archive at Nabble.com.


From ericlecoutre at gmail.com  Sat Jul 28 16:07:49 2007
From: ericlecoutre at gmail.com (Eric Lecoutre)
Date: Sat, 28 Jul 2007 16:07:49 +0200
Subject: [R] Combine R2HTML and Rcmd BATCH?
In-Reply-To: <469CA291.7020407@tue.nl>
References: <469CA291.7020407@tue.nl>
Message-ID: <5d897a2f0707280707p28a76dabv98a58b8581c6f28b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070728/225dc78c/attachment-0001.pl 

From arigado0315 at yahoo.com.tw  Sat Jul 28 18:16:43 2007
From: arigado0315 at yahoo.com.tw (arigado)
Date: Sat, 28 Jul 2007 09:16:43 -0700 (PDT)
Subject: [R] About infinite value
In-Reply-To: <B7B34444ECA41A41AC47DABA057CE7A206FD40@webmail.cip.cgiar.org>
References: <11740491.post@talk.nabble.com>
	<B7B34444ECA41A41AC47DABA057CE7A206FD40@webmail.cip.cgiar.org>
Message-ID: <11844202.post@talk.nabble.com>




I think that the best thing is to work in logarithmic way, to avoid the
limitations of the CPU. 
If y = 10^400, to do y=400*log(10), to change all you formulate to the
logarithmic way and the final result to apply the antilogarithm. 
 
Felipe de Mendiburu. 
Professor of statistic 
Agrarian National University -La Molina - PERU 

Thank you.
Thanks your method.

-- 
View this message in context: http://www.nabble.com/About-infinite-value-tf4128557.html#a11844202
Sent from the R help mailing list archive at Nabble.com.


From dataanalytics at earthlink.net  Sat Jul 28 18:09:03 2007
From: dataanalytics at earthlink.net (Walter R. Paczkowski)
Date: Sat, 28 Jul 2007 12:09:03 -0400
Subject: [R] beta regressions in R
Message-ID: <E1IEonx-0005oh-J8@elasmtp-junco.atl.sa.earthlink.net>


   Good morning,
   Does anyone know of a package or function to do a beta regression?
   Thanks,
   Walt Paczkowski

   _________________________________
   Walter R. Paczkowski, Ph.D.
   Data Analytics Corp.
   44 Hamilton Lane
   Plainsboro, NJ  08536
   (V) 609-936-8999
   (F) 609-936-3733

From e0425331 at stud4.tuwien.ac.at  Sat Jul 28 18:39:29 2007
From: e0425331 at stud4.tuwien.ac.at (Stefan Nachtnebel)
Date: Sat, 28 Jul 2007 18:39:29 +0200
Subject: [R] xtable with vector
In-Reply-To: <mailman.11.1185616805.19480.r-help@stat.math.ethz.ch>
References: <mailman.11.1185616805.19480.r-help@stat.math.ethz.ch>
Message-ID: <20070728183929.zujhqnbysowww8go@webmail.tuwien.ac.at>

Hello,

Is there a possibility to use xtable with a vector to generate a latex 
table? I always get an error, that no applicable method is available.

For example:

b<-1:12
dim(b)<-c(2,6)
dimnames(b)[[2]]<-paste("col",1:6)
xtable(b)

works fine and does not raise an error, but

a<-1:6
names(a)<-paste(col,1:6)
xtable(b)

does not work.

Regards, Stefan


From strinz at freenet.de  Sat Jul 28 21:19:48 2007
From: strinz at freenet.de (strinz at freenet.de)
Date: Sat, 28 Jul 2007 21:19:48 +0200
Subject: [R] text() and vector arguments like adj
Message-ID: <E1IErpM-0003Ci-Nu@www12.emo.freenet-rz.de>

Hello,

I remarked that the function
## Default S3 method:
text (x, y = NULL, labels = seq(along = x), adj = NULL,pos = NULL, offset = 0.5, vfont = NULL,cex = 1, col = NULL, font = NULL, ...)

accepts vectors of arguments (of the same length) except for the parameter adj.
When passing a vector of information for adjusting the labels, only the first value
is taken.  

Any special reason for this ?

btw: could a rotating argument like the 'srt' argument in mtext() be incorporated ?

best
Bjoern


From edoviak at earthlink.net  Sat Jul 28 20:07:33 2007
From: edoviak at earthlink.net (Eric Doviak)
Date: Sat, 28 Jul 2007 14:07:33 -0400 (GMT-04:00)
Subject: [R] the large dataset problem
Message-ID: <9413086.1185646053804.JavaMail.root@elwamui-royal.atl.sa.earthlink.net>

Dear useRs,

I recently began a job at a very large and heavily bureaucratic organization. We're setting up a research office and statistical analysis will form the backbone of our work. We'll be working with large datasets such the SIPP as well as our own administrative data.

Due to the bureaucracy, it will take some time to get the licenses for proprietary software like Stata. Right now, R is the only statistical software package on my computer. 

This, of course, is a huge limitation because R loads data directly into RAM making it difficult (if not impossible) to work with large datasets. My computer only has 1000 MB of RAM, of which Microsucks Winblows devours 400 MB. To make my memory issues even worse, my computer has a virus scanner that runs everyday and I do not have the administrative rights to turn the damn thing off. 

I need to find some way to overcome these constraints and work with large datasets. Does anyone have any suggestions?

I've read that I should "carefully vectorize my code." What does that mean ??? !!!

The "Introduction to R" manual suggests modifying input files with Perl. Any tips on how to get started? Would Perl Data Language (PDL) be a good choice?  http://pdl.perl.org/index_en.html

I wrote a script which loads large datasets a few lines at a time, writes the dozen or so variables of interest to a CSV file, removes the loaded data and then (via a "for" loop) loads the next few lines .... I managed to get it to work with one of the SIPP core files, but it's SLOOOOW. Worse, if I discover later that I omitted a relevant variable, then I'll have to run the whole script all over again.

Any suggestions?

Thanks,
- Eric


From adik at ilovebacon.org  Sun Jul 29 04:50:11 2007
From: adik at ilovebacon.org (Adam D. I. Kramer)
Date: Sat, 28 Jul 2007 19:50:11 -0700 (PDT)
Subject: [R] PCA with missing data?
Message-ID: <Pine.LNX.4.64.0707281949510.19803@parser.ilovebacon.org>

Hello,

 	I'm interested in running a PCA on a data set with lots of missing
data. There're a few techniques online which suggest how I could go about
doing this, but before I committed to coding them into R, I wanted to see
whether anyone could recommend a currently existing package with such a
procedure.

 	Google searching for such a procedure has'nt turned much up...but I
blame that on the difficulty of searcching for single letters, and the
rampant use of R as a variable name. But I did attempt to do my homework
before posting to the list!

Cordially,

--
Adam D. I. Kramer
Ph.D. Student, Social Psychology
University of Oregon
adik at uoregon.edu


From guo.dong99 at gmail.com  Sun Jul 29 06:19:17 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Sun, 29 Jul 2007 06:19:17 +0200
Subject: [R] plot
In-Reply-To: <07E228A5BE53C24CAD490193A7381BBBAF2B8C@LP-EXCHVS07.CO.IHC.COM>
References: <989527a20707271109w365c85f3qb8b4b775c805099b@mail.gmail.com>
	<07E228A5BE53C24CAD490193A7381BBBAF2B8C@LP-EXCHVS07.CO.IHC.COM>
Message-ID: <989527a20707282119v395d4db7x65e7ca9a43015b07@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/f668ad7f/attachment.pl 

From patrick at pdrechsler.de  Sat Jul 28 23:49:53 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Sat, 28 Jul 2007 23:49:53 +0200
Subject: [R] lattice grayscale "theme"
Message-ID: <87lkd08agu.fsf@pdrechsler.de>

Hi,

is there a grayscale setting for lattice plots?

I like the default color settings. I also like the settings that are
available for setting black and white with something like this:

--8<---------------cut here---------------start------------->8---
ltheme <- canonical.theme(color = FALSE)      ## in-built B&W theme
ltheme$strip.background$col <- "transparent" ## change strip bg
lattice.options(default.theme = ltheme)      ## set as default
--8<---------------cut here---------------end--------------->8---

Is there a simple way of achieving something in between these settings
(using grayscales for the default colors)?

Thankful for any pointers,

Patrick


From whorfin at pixar.com  Sun Jul 29 02:41:45 2007
From: whorfin at pixar.com (Rick Sayre)
Date: Sat, 28 Jul 2007 17:41:45 -0700
Subject: [R] Bug in TAB handling for Win32 rTerm and rGUI in 2.5.1?
Message-ID: <46ABE249.1010500@pixar.com>

Greetings.  This seemed like a bug to me, but I wanted to see if this 
was in fact intended before reporting.

Before I start, i want to first extend thanks for the big improvements
in integration of command completion for the windows version.  Really
nice to have now.  But i believe there are some issues.

In getline/getline.c, the "tab" case of the charater handling switch 
statement
in getline() simply "break;"s to the end if tab completion is not 
enabled, thus
eating the tab.  Thus, if tab completion is disabled, a tab no
longer serves as a tab; it disappears.

likewise, in console.c, if k == TABKEY, a return is done without adding 
the key to kbuf, thus TAB is always discarded, even if completion is 
disabled.

It seems to me that this is wrong.

This new TAB behavior now makes it impossible for me to copy/paste text
from a text file of R expressions which use TABs.  Copy paste behavior
which worked in 2.4.x for rTerm now does not, since the discarded TABs
mean that keyword separators may disappear, changing the meaning of
pasted text.  rGUI thankfully still works, since the completion/TAB
processing code is bypassed when activating the "paste" command.

I'd like to request the ability to have both --- TAB as a working
separator, and the ability to configure the completion key to something
other than TAB.  This way one can both enjoy completion and successfully
copy/paste text containing tabs.

Cheers

	--Rick


From toby909 at gmail.com  Sun Jul 29 05:50:35 2007
From: toby909 at gmail.com (toby909 at gmail.com)
Date: Sat, 28 Jul 2007 20:50:35 -0700
Subject: [R] R2WinBUGS more updates after model did not converge
Message-ID: <f8h2n1$4dt$1@sea.gmane.org>

After running a model for a while and seeing that it did not converge yet, how 
can I continue to run, ie not starting anew, the model?

I know if I manually/interactively use winbugs, this is possible anytime, but 
how can I do this in r2winbugs, so that my existing sim$sims.array and other 
stuff in the object that bugs() returns gets extended?

Thanks Toby


From guo.dong99 at gmail.com  Sun Jul 29 06:16:55 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Sun, 29 Jul 2007 06:16:55 +0200
Subject: [R] matrix output in R, and file name creating
Message-ID: <989527a20707282116q1af4452al8c498177b1e4b5a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/8a8f1d3e/attachment.pl 

From guo.dong99 at gmail.com  Sun Jul 29 10:07:03 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Sun, 29 Jul 2007 10:07:03 +0200
Subject: [R] array writing and their filenames
Message-ID: <989527a20707290107t69c1da28ja42ffc7625d0c567@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/009b19bd/attachment.pl 

From amstat2006 at gmail.com  Sun Jul 29 15:01:52 2007
From: amstat2006 at gmail.com (Am Stat)
Date: Sun, 29 Jul 2007 09:01:52 -0400
Subject: [R] Order by the columns
Message-ID: <c8b63a350707290601t53190c48nf8b6a19a3b4ad7ec@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/699c795e/attachment.pl 

From mister_bluesman at hotmail.com  Sun Jul 29 13:06:58 2007
From: mister_bluesman at hotmail.com (mister_bluesman)
Date: Sun, 29 Jul 2007 04:06:58 -0700 (PDT)
Subject: [R] Mapping data with unknown dimensions?
Message-ID: <11850581.post@talk.nabble.com>


Hi. Does anyone know of a function in R which will allow me to map data
points, where the number of dimensions in the data is unknown?

Thanks
-- 
View this message in context: http://www.nabble.com/Mapping-data-with-unknown-dimensions--tf4165241.html#a11850581
Sent from the R help mailing list archive at Nabble.com.


From ritz at kvl.dk  Mon Jul 30 14:18:19 2007
From: ritz at kvl.dk (Christian Ritz)
Date: Mon, 30 Jul 2007 14:18:19 +0200
Subject: [R] regular expressions : extracting numbers
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
Message-ID: <46ADD70B.6020502@kvl.dk>

Dear David,

does the following work for you?


sVec <- c("lema, rb 2%", "rb 2%", "rb 3%", "rb 4%", "rb 3%", "rb 2%,mineuse", "rb", "rb", 
"rb 12", "rb", "rj 30%", "rb", "rb", "rb 25%", "rb", "rb", "rb", "rj, rb")

reVec <- regexpr("[[:digit:]]+", sVec)
# see ?regex for details on '[:digit:]' and '+'

substr(sVec ,start = reVec, stop=reVec + attr(reVec, "match.length") - 1)
# see ?substr for details



Christian


From brown_emu at yahoo.com  Sun Jul 29 18:18:50 2007
From: brown_emu at yahoo.com (Stephen Tucker)
Date: Sun, 29 Jul 2007 09:18:50 -0700 (PDT)
Subject: [R] line widths of plotting symbols in the lattice
Message-ID: <576227.63887.qm@web39701.mail.mud.yahoo.com>

Dear List,

Sorry, this is very simple but I can't seem to find any information regarding
line widths of plotting symbols in the lattice package.

For instance, in traditional graphics:

> plot(1:10,lwd=3)
> points(10:1,lwd=2,col=3)

'lwd' allows control of plotting symbol line widths.

I've tried looking through the documentation for xyplot, panel.points,
trellis.par.set, and the R-help archives. Maybe it goes by another name?

Thanks in advance,

Stephen


From Ralf.Finne at syh.fi  Sun Jul 29 16:22:15 2007
From: Ralf.Finne at syh.fi (Ralf Finne)
Date: Sun, 29 Jul 2007 17:22:15 +0300
Subject: [R] Prompt comes too late
Message-ID: <46ACCCC7020000EE00003F9B@valhall.syh.fi>

Hi R fans

I am trying to make a program to ask the user to choose data file:

print("Choose data file please !")

matr=read.table(file.choose(),dec=".",header=TRUE)

The problem is that the prompt 
Choose data file please !   
comes after I have chosen the file.

What  am doing wrong?

Thanks in advance
Ralf Finne


From michael at cassin.name  Sun Jul 29 16:56:41 2007
From: michael at cassin.name (Michael Cassin)
Date: Sun, 29 Jul 2007 15:56:41 +0100
Subject: [R] Problem installing tseries package
In-Reply-To: <Pine.LNX.4.64.0707261658180.7472@gannet.stats.ox.ac.uk>
References: <b02e8b330707260709o6bb1750cm531bc73e6b315b83@mail.gmail.com>
	<Pine.LNX.4.64.0707261658180.7472@gannet.stats.ox.ac.uk>
Message-ID: <b02e8b330707290756m1949e3aag26564754caeaa258@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/610c6f68/attachment.pl 

From francogrex at mail.com  Sun Jul 29 19:49:09 2007
From: francogrex at mail.com (francogrex)
Date: Sun, 29 Jul 2007 10:49:09 -0700 (PDT)
Subject: [R] deriv; loop
Message-ID: <11853456.post@talk.nabble.com>


Hi, 2 questions:

Question 1: example of what I currently do:

for(i in 1:6){sink("temp.txt",append=TRUE)
dput(i+0)
sink()}
x=scan(file="temp.txt")
print(prod(x))
file.remove("C:/R-2.5.0/temp.txt")

But how to convert the output of the loop to a vector that I can manipulate
(by prod or sum etc), without having to write and append to a file?

Question 2:

> deriv(~gamma(x),"x")

expression({
    .expr1 <- gamma(x)
    .value <- .expr1
    .grad <- array(0, c(length(.value), 1), list(NULL, c("x")))
    .grad[, "x"] <- .expr1 * psigamma(x)
    attr(.value, "gradient") <- .grad
    .value
})

BUT

> deriv3(~gamma(x),"x")
Error in deriv3.formula(~gamma(x), "x") : Function 'psigamma' is not in the
derivatives table

What I want is the expression for the second derivative (which I believe is
trigamma(x), or psigamma(x,1)), how can I obtain that?

Thanks
-- 
View this message in context: http://www.nabble.com/deriv--loop-tf4166283.html#a11853456
Sent from the R help mailing list archive at Nabble.com.


From guo.dong99 at gmail.com  Sun Jul 29 19:41:58 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Sun, 29 Jul 2007 19:41:58 +0200
Subject: [R] write.csv
Message-ID: <989527a20707291041n75301c1ha99e57b94a73a8ba@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/1af9925d/attachment.pl 

From marc_schwartz at comcast.net  Mon Jul 30 14:17:43 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 30 Jul 2007 07:17:43 -0500
Subject: [R] regular expressions : extracting numbers
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
Message-ID: <1185797863.3966.12.camel@Bellerophon.localdomain>

On Mon, 2007-07-30 at 13:58 +0200, GOUACHE David wrote:
> Hello all,
> 
> I have a vector of character strings, in which I have letters,
> numbers, and symbols. What I wish to do is obtain a vector of the same
> length with just the numbers.
> A quick example -
> 
> extract of the original vector :
> "lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb"
> "rb" "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"
> 
> and the type of thing I wish to end up with :
> "2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""
> 
> or, instead of "", NA would be acceptable (actually it would almost be
> better for me)
> 
> Anyways, I've been battling with gsub() and things of the sort, but
> I'm drowning in the regular expressions, despite a few hours of
> looking at Perl tutorials...
> So if anyone can help me out, it would be greatly appreciated!!
> 
> In advance, thanks very much.

Try this:

> Vec
 [1] "lema, rb 2%"   "rb 2%"         "rb 3%"         "rb 4%"        
 [5] "rb 3%"         "rb 2%,mineuse" "rb"            "rb"           
 [9] "rb 12"         "rb"            "rj 30%"        "rb"           
[13] "rb"            "rb 25%"        "rb"            "rb"           
[17] "rb"            "rj, rb" 

> gsub("[^0-9]", "", Vec)
 [1] "2"  "2"  "3"  "4"  "3"  "2"  ""   ""   "12" ""   "30" ""   ""  
[14] "25" ""   ""   ""   ""  


The search pattern regex here is [^0-9] which says to replace anything
that is not (^) in the character range of 0 through 9.

See ?regex and/or http://www.regular-expressions.info/

HTH,

Marc Schwartz


From fqiu at gatech.edu  Sun Jul 29 22:35:51 2007
From: fqiu at gatech.edu (Feng Qiu)
Date: Sun, 29 Jul 2007 16:35:51 -0400
Subject: [R] Call R program from C++ code
Message-ID: <000001c7d220$0f4735e0$2dd5a1a0$@edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/5969e25d/attachment.pl 

From dsp0718 at googlemail.com  Sun Jul 29 22:47:32 2007
From: dsp0718 at googlemail.com (David Pain)
Date: Sun, 29 Jul 2007 21:47:32 +0100
Subject: [R] Fwd: Package manual examples - 'unexpected$undefined' errors
In-Reply-To: <acb54ab40707280303g4fc79bbyaf475eba094f8eb8@mail.gmail.com>
References: <acb54ab40707280303g4fc79bbyaf475eba094f8eb8@mail.gmail.com>
Message-ID: <acb54ab40707291347g35ad691ftabc52671ffb382ec@mail.gmail.com>

Bounced first time!

---------- Forwarded message ----------
From: David Pain <dsp0718 at googlemail.com>
Date: 28-Jul-2007 11:03
Subject: Package manual examples - 'unexpected$undefined' errors
To: r-help at stat.math.ethz.ch


Trying out an unfamiliar package, the natural thing is to use the
examples given in the package's manual - hopefully, the writers of the
package wouldn't include examples which didn't work!

Recently, though, I've been getting 'unexpected$undefined' error
messages when doing this, despite having copy/pasted the text from the
manual (taking out hard breaks on the way).

Moreover, I've had error messages for commands which I've previously
had work fine.

For instance, this from Zelig

z.out <? zelig(vote ~ race + educate, model = "logit", data = turnout)

has at different times worked fine and thrown up the error message.

Any help gratefully received.


From hellokisas at gmail.com  Sun Jul 29 22:38:53 2007
From: hellokisas at gmail.com (Kisas)
Date: Sun, 29 Jul 2007 16:38:53 -0400
Subject: [R] Call R program from C++ code
Message-ID: <000501c7d220$7a368bd0$6ea3a370$@com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/d3354029/attachment.pl 

From shawnmel at gmail.com  Sun Jul 29 22:54:21 2007
From: shawnmel at gmail.com (Shawndelle Noble)
Date: Sun, 29 Jul 2007 16:54:21 -0400
Subject: [R] Problem with code
Message-ID: <7dacf7220707291354o1fd50389t9aac87d5669c1e95@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/533f0249/attachment.pl 

From edd at debian.org  Mon Jul 30 00:17:27 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 29 Jul 2007 22:17:27 +0000
Subject: [R] Call R program from C++ code
In-Reply-To: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
References: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
Message-ID: <20070729221727.GA28548@master.debian.org>

On Sun, Jul 29, 2007 at 04:35:51PM -0400, Feng Qiu wrote:
>                I'm developing an application program using C++. From my C++
> code, I would call some R program I have written. I' wondering if R provide
> some compiler that can compile R program into executable program. I searched

That does not exist to the best of my knowledge.

>                I might be wrong that R doesn't have complier. What I'm
> trying to do is to call R program from C++ code. Any help is highly
> appreciated!

As you probably know, C++ can 'call' other object code that is linked
to it.  As for 'calling R', the easiest way is to call an R script
using the system() call -- but it is also the most tedious way as you
to write the inout data to file, and then read the result data back
in.  But it is a start, and it may be easiest to debug.

A more advanced method would to use Rserve to run a 'headless' R
service to which your C++ program can connect over the network. But
there you need to be already somewhat familiar with the underlying
C/C++ representation of R object. Rserve has simple examples.

Next, you can actually embed R inside your C++ application, but that
is more advanced.

In any event, you may also want to consider the RcppTemplate package
which has a host of examples about how to get R and C++ to work better
together (without forcing you to use C).

The 'Extending R' manual from your R installation is a good starting
point for most of this.

Hope this helps, Dirk

-- 
Three out of two people have difficulties with fractions.


From jebyrnes at ucdavis.edu  Sun Jul 29 23:22:05 2007
From: jebyrnes at ucdavis.edu (Jarrett Byrnes)
Date: Sun, 29 Jul 2007 14:22:05 -0700
Subject: [R] Piecewise Regression with a known slope
Message-ID: <B609F230-7969-4C9B-84DF-6ABC7E3151CC@ucdavis.edu>

Hey, all.  I'm working on a data set with a broken stick linear  
regression where I know one of the two slopes.  It is a negative  
linear function until the line intersects with the x-axis, at which  
point it becomes 0.  It is not a nonlinear asymptotic function, and,  
indeed, using negative exponential or logistic types of fits as an  
approximation has tended to lead to an under or overestimation of  
values.  I am also very interested to know just what the breakpoint  
in the data is.

Essentially

if x<psi y = a + bx + error, where b is negative
else y=0+error

and I want to know the value of psi, as well as a and b.  The error  
is also most likely gamma distributed, as values<0 are not possible  
(nutrient concentrations).

I have attempted to use the segmented package, specifying something  
close to the visually estimated breakpoint as the value of psi, but  
it continues to return fits with a breakpoint that occurs somewhere  
in the middle of the linear portion of the line, and the slope and  
intercept of the second half of the regression is not 0.

Is there either a package that exists that will allow me to estimate  
such a model, or a function that I can use for optim or nlm (I admit,  
I am a rank novice at coding such functions).

Thanks so much!

-Jarrett


From edd at debian.org  Mon Jul 30 01:28:12 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 29 Jul 2007 23:28:12 +0000
Subject: [R] Call R program from C++ code
In-Reply-To: <000a01c7d236$56856c40$039044c0$@edu>
References: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
	<20070729221727.GA28548@master.debian.org>
	<000a01c7d236$56856c40$039044c0$@edu>
Message-ID: <20070729232811.GA21932@master.debian.org>

On Sun, Jul 29, 2007 at 07:15:21PM -0400, Feng Qiu wrote:
> >>using the system() call -- but it is also the most tedious way as you
> When using System() to call R program, do I need to call some R script
> program or my R function directly? 

It's system() with lower-case s, and yes, you would call some script
to process the input you prepared from C++. And the script would
prepare output that your C++ program would need.

> >> A more advanced method would to use Rserve to run a 'headless' R	
> Here you mean Com? I don't know much about this com frame, so ....

No, I did not mean Com. 

Rserve uses tcp/ip networking and is agnostic to the operating system
of the caller, ie your (say, Windoze) program can call and exchange
(binary) data with a program on a Linux box or Mac that runs Rserve.
Could also be Windows, could be the same computer, or could be a
different one. Rserve is quite flexible that way.

Dirk


-- 
Three out of two people have difficulties with fractions.


From fqiu at gatech.edu  Mon Jul 30 01:15:21 2007
From: fqiu at gatech.edu (Feng Qiu)
Date: Sun, 29 Jul 2007 19:15:21 -0400
Subject: [R] Call R program from C++ code
In-Reply-To: <20070729221727.GA28548@master.debian.org>
References: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
	<20070729221727.GA28548@master.debian.org>
Message-ID: <000a01c7d236$56856c40$039044c0$@edu>

Hi Dirk:
	Thanks a lot. 
>> That does not exist to the best of my knowledge.
That's sad :(

>>using the system() call -- but it is also the most tedious way as you
When using System() to call R program, do I need to call some R script
program or my R function directly? 

>> A more advanced method would to use Rserve to run a 'headless' R	
Here you mean Com? I don't know much about this com frame, so ....

>> In any event, you may also want to consider the RcppTemplate package
I'll read this package, hope I can find a simple way that works in my case.

	Thanks again!

Best,

Feng

-----Original Message-----
From: Dirk Eddelbuettel [mailto:edd at master.debian.org] On Behalf Of Dirk
Eddelbuettel
Sent: 2007??7??29?? 18:17
To: Feng Qiu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Call R program from C++ code

On Sun, Jul 29, 2007 at 04:35:51PM -0400, Feng Qiu wrote:
>                I'm developing an application program using C++. From my
C++
> code, I would call some R program I have written. I' wondering if R
provide
> some compiler that can compile R program into executable program. I
searched

That does not exist to the best of my knowledge.

>                I might be wrong that R doesn't have complier. What I'm
> trying to do is to call R program from C++ code. Any help is highly
> appreciated!

As you probably know, C++ can 'call' other object code that is linked
to it.  As for 'calling R', the easiest way is to call an R script
using the system() call -- but it is also the most tedious way as you
to write the inout data to file, and then read the result data back
in.  But it is a start, and it may be easiest to debug.

A more advanced method would to use Rserve to run a 'headless' R	
service to which your C++ program can connect over the network. But
there you need to be already somewhat familiar with the underlying
C/C++ representation of R object. Rserve has simple examples.

Next, you can actually embed R inside your C++ application, but that
is more advanced.

In any event, you may also want to consider the RcppTemplate package
which has a host of examples about how to get R and C++ to work better
together (without forcing you to use C).

The 'Extending R' manual from your R installation is a good starting
point for most of this.

Hope this helps, Dirk

-- 
Three out of two people have difficulties with fractions.


From xnyang at seu.edu.cn  Mon Jul 30 05:16:35 2007
From: xnyang at seu.edu.cn (xinan yang)
Date: Mon, 30 Jul 2007 11:16:35 +0800
Subject: [R] install error for RBGL_1.12.0 on linux
Message-ID: <46AD5813.80700@seu.edu.cn>

Dear all,

I meet error when installing the newest Bioconductor packages 2.0 for R 
2.5.1.

the enviroments are as bellow:

 > sessionInfo()
R version 2.5.1 (2007-06-27)
i686-pc-linux-gnu

locale:
LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"
 >

The error messages are partly as:
.....
 boost::vec_adj_list_vertex_id_map<boost::property<boost::vertex_color_t,
   boost::default_color_type, boost::no_property>, size_t>&)'
make: *** [interfaces.o] Error 1
chmod: failed to get attributes of 
`/usr/local/lib/R/library/RBGL/libs/*': No such file or directory
ERROR: compilation failed for package 'RBGL'


but, there exists such directory " /usr/local/lib/R/library/RBGL/libs/" 
in my computer:
[root at yxnlinux getBioC2.5-R2.5.1]# ls -al 
/usr/local/lib/R/library/RBGL/libs/
total 17364
drwxr-xr-x    2 root     root         4096 Dec 23  2005 .
drwxr-xr-x   17 root     root         4096 Dec 23  2005 ..
-rwxr-xr-x    1 root     root     17743886 Dec 23  2005 RBGL.so


Please help me to fix it.


THanks,


xinan


From gregory_gentlemen at yahoo.ca  Mon Jul 30 04:31:36 2007
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Sun, 29 Jul 2007 22:31:36 -0400 (EDT)
Subject: [R] Constructing correlation matrices
Message-ID: <277647.32862.qm@web63514.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070729/b1560484/attachment.pl 

From xnyang at seu.edu.cn  Mon Jul 30 06:50:31 2007
From: xnyang at seu.edu.cn (xinan yang)
Date: Mon, 30 Jul 2007 12:50:31 +0800
Subject: [R] install error for RBGL_1.12.0 on linux ---2
Message-ID: <46AD6E17.7020103@seu.edu.cn>

Hi, Li and all lists,

More, I have searched the web, and found one solution as described in:
https://stat.ethz.ch/pipermail/bioc-devel/2007-June/001168.html


But, My condition is some how different:
1) my g++ version is: g++ (GCC) 3.2.2 20030222 (Red Hat Linux 3.2.2-5)

2)  the SHLIB file is the same, but some difference in 
/usr/local/lib/R/etc/Makeconf:

SHELL = /bin/sh
SHLIB_CFLAGS =
SHLIB_CXXFLAGS =
SHLIB_CXXLD = g++
SHLIB_CXXLDFLAGS = -shared
SHLIB_EXT = .so
SHLIB_FCLD = g77
SHLIB_FCLDFLAGS = -shared
SHLIB_FFLAGS =
SHLIB_LD = gcc -std=gnu99
SHLIB_LDFLAGS = -shared
SHLIB_LIBADD =
SHLIB_LINK = $(SHLIB_LD) $(SHLIB_LDFLAGS) $(LDFLAGS)


what should I do?

thanks,

xinan


From rithesh.m at brickworkindia.com  Mon Jul 30 09:36:07 2007
From: rithesh.m at brickworkindia.com (Rithesh M. Mohan)
Date: Mon, 30 Jul 2007 13:06:07 +0530
Subject: [R] help with ROC curve
Message-ID: <4BC833C82CD3564298A3A24BD299C0C8B8510B@pioneer.brickworkindia.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/d5c7d92c/attachment-0001.pl 

From rense.nieuwenhuis at gmail.com  Mon Jul 30 14:19:41 2007
From: rense.nieuwenhuis at gmail.com (Rense Nieuwenhuis)
Date: Mon, 30 Jul 2007 14:19:41 +0200
Subject: [R] Extract random part of summary nlme
Message-ID: <9341526C-182D-42CC-874A-E61099E21C26@student.ru.nl>

Dear helpers,

I'm estimating multilevel regression models, using the lme-function  
from the nlme-package. Let's say that I estimated a model and stored  
it inside the object named 'model'. The summary of that model is  
shown below:

Using summary(model)$tTable , I receive the following output:

 > summary(model)$tTable
                     Value  Std.Error   DF     t-value       p-value
(Intercept)    0.23268607 0.09350662 3990   2.4884449  1.287080e-02
sexM          -0.15338225 0.03169762 3990  -4.8389206  1.354802e-06
standLRT       0.38593558 0.01677195 3990  23.0107762 4.005182e-110
vrmid 50%      0.07606394 0.09389376   61   0.8101064  4.210281e-01
vrtop 25%      0.24561327 0.10483374   61   2.3428838  2.241317e-02
intakemid 50% -0.41469716 0.03177240 3990 -13.0521199  3.698344e-38
intaketop 25% -0.75920783 0.05357980 3990 -14.1696648  1.666780e-44
typeSngl       0.15680532 0.07173835   61   2.1857949  3.267903e-02


All looks fine to me. The output above is simply  a section from the  
full summary shown below. Now, I want to extract from the summary (or  
the full model) the part stating the random parameters. More  
specifically, I want to extract from the summary the following:

(Intercept) 0.2869401 (Intr)
typeSngl    0.2791040 -0.617
Residual    0.7302233

How could this be done?

Thanks for the effort,

Rense Nieuwenhuis








Linear mixed-effects model fit by REML
  Data: Exam
       AIC      BIC   logLik
   9158.56 9234.241 -4567.28

Random effects:
  Formula: ~type | school
  Structure: General positive-definite, Log-Cholesky parametrization
             StdDev    Corr
(Intercept) 0.2869401 (Intr)
typeSngl    0.2791040 -0.617
Residual    0.7302233

Fixed effects: normexam ~ sex + standLRT + vr + intake + type
                    Value  Std.Error   DF    t-value p-value
(Intercept)    0.2326861 0.09350662 3990   2.488445  0.0129
sexM          -0.1533822 0.03169762 3990  -4.838921  0.0000
standLRT       0.3859356 0.01677195 3990  23.010776  0.0000
vrmid 50%      0.0760639 0.09389376   61   0.810106  0.4210
vrtop 25%      0.2456133 0.10483374   61   2.342884  0.0224
intakemid 50% -0.4146972 0.03177240 3990 -13.052120  0.0000
intaketop 25% -0.7592078 0.05357980 3990 -14.169665  0.0000
typeSngl       0.1568053 0.07173835   61   2.185795  0.0327
  Correlation:
               (Intr) sexM   stnLRT vrm50% vrt25% int50% int25%
sexM          -0.201
standLRT      -0.125  0.028
vrmid 50%     -0.742  0.028 -0.035
vrtop 25%     -0.652  0.051 -0.065  0.649
intakemid 50% -0.246 -0.011  0.541 -0.002  0.007
intaketop 25% -0.218 -0.018  0.676  0.014  0.013  0.660
typeSngl      -0.421  0.080  0.007  0.033 -0.027 -0.001  0.001

Standardized Within-Group Residuals:
         Min          Q1         Med          Q3         Max
-3.59074329 -0.63776965  0.03829878  0.67303837  3.33952680

Number of Observations: 4059
Number of Groups: 65


From fisher at plessthan.com  Mon Jul 30 14:25:44 2007
From: fisher at plessthan.com (Dennis Fisher)
Date: Mon, 30 Jul 2007 05:25:44 -0700
Subject: [R] Tabs in PDF documents
Message-ID: <9A0F2613-FB53-4C70-8852-80C7E375ED71@plessthan.com>

Colleagues,

I am using R 2.5.1 on an Intel Mac (OS 10) to create PDF outputs  
using pdf(); same problem exists in Linux (RedHat 9)

While adding text to the document with text() and mtext(), I  
encounter the following problem:

In order to align the text, I have embedded tabs ("\t") in some of  
the text.  Each time I do so, I get the following error messages:
	Warning: font metrics unknown for character 0x9
	Warning: font width unknown for character 0x9
and the tabs are ignored.  I have tied par() with and without  
family="mono".

Is there a work-around available for this?

Dennis

COMMANDS:
	pdf("junk.pdf")
	par(family="mono")
	plot(1,1)
	text(1,1, "\txx")
	mtext("\txx")
	dev.off()


From gyadav at ccilindia.co.in  Mon Jul 30 14:30:44 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Mon, 30 Jul 2007 18:00:44 +0530
Subject: [R] ROC curve in R
In-Reply-To: <4BC833C82CD3564298A3A24BD299C0C8B85227@pioneer.brickworkindia.
	local>
Message-ID: <OFFBD9E994.75B02C97-ON65257328.00428226-65257328.0044BB9F@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/30cf218d/attachment.pl 

From rithesh.m at brickworkindia.com  Mon Jul 30 14:34:59 2007
From: rithesh.m at brickworkindia.com (Rithesh M. Mohan)
Date: Mon, 30 Jul 2007 18:04:59 +0530
Subject: [R] ROC curve in R
Message-ID: <4BC833C82CD3564298A3A24BD299C0C8B8526B@pioneer.brickworkindia.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/7948b526/attachment.pl 

From niederlein-rstat at yahoo.de  Mon Jul 30 14:39:22 2007
From: niederlein-rstat at yahoo.de (niederlein-rstat at yahoo.de)
Date: Mon, 30 Jul 2007 14:39:22 +0200 (CEST)
Subject: [R] how to combine data of several csv-files
Message-ID: <62066.18758.qm@web27114.mail.ukl.yahoo.com>

Ein eingebundener Text mit undefiniertem Zeichensatz wurde abgetrennt.
Name: nicht verf?gbar
URL: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/60e9c4b2/attachment.pl 

From rense.nieuwenhuis at gmail.com  Mon Jul 30 14:39:26 2007
From: rense.nieuwenhuis at gmail.com (Rense Nieuwenhuis)
Date: Mon, 30 Jul 2007 14:39:26 +0200
Subject: [R] Extracting random parameters from summary lme
Message-ID: <EFBE9EE1-9ED4-4095-9A85-8CE938AD5C2A@student.ru.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/3e451568/attachment.pl 

From marc_schwartz at comcast.net  Mon Jul 30 14:39:42 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Mon, 30 Jul 2007 07:39:42 -0500
Subject: [R] getting the name of variables passed to a function
In-Reply-To: <46A9C046020000650000777F@pgn.com>
References: <46A9C046020000650000777F@pgn.com>
Message-ID: <1185799182.3966.23.camel@Bellerophon.localdomain>

On Fri, 2007-07-27 at 09:52 -0700, Horace Tso wrote:
> Folks,
> 
> I've entered into an R programming territory I'm not very familiar
> with, thus this probably very elementary question concerning the
> mechanic of a function call.
> 
> I want to know from within a function the name of the variables I pass
> down. The function makes use of the "..." to allow for multiple
> unknown arguments,
> 
> myfun = function(...) { do something }
> 
> In the body I put,
> 
> {    
> nm <- names(list(...))
> nm
> }
> 
> When the function is called with two vectors x, and y
> 
> myfun(x, y)
> 
> It returns NULL. However, when the call made is,
> 
> >myfun(x=x, y=y)
> 
> The result is
> [1] "x" "y"
> 
> Question : how do i get the names of the unknown variables without
> explicitly saying x=x...
> 
> Thanks in advance.
> 
> Horace


See ?match.call and take note of the 'expand.dots' argument, which
defaults to TRUE.

  DotsFun <- function(...) as.character(match.call())[-1]
  
  x <- 1:10
  y <- 5:6

  > DotsFun(x, y)
  [1] "x" "y"

match.call() returns the full function call. In the above, we take that
result, coerce it to a character vector and remove the first element,
which is the function being called, thus leaving the arguments.

HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Mon Jul 30 14:39:53 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jul 2007 13:39:53 +0100 (BST)
Subject: [R] getting the name of variables passed to a function
In-Reply-To: <46A9C046020000650000777F@pgn.com>
References: <46A9C046020000650000777F@pgn.com>
Message-ID: <Pine.LNX.4.64.0707301338240.19795@gannet.stats.ox.ac.uk>

I would start from match.call(expand.dots=TRUE) which has done the hard 
work for you.

On Fri, 27 Jul 2007, Horace Tso wrote:

> Folks,
>
> I've entered into an R programming territory I'm not very familiar with, 
> thus this probably very elementary question concerning the mechanic of a 
> function call.
>
> I want to know from within a function the name of the variables I pass 
> down. The function makes use of the "..." to allow for multiple unknown 
> arguments,
>
> myfun = function(...) { do something }
>
> In the body I put,
>
> {
> nm <- names(list(...))
> nm
> }
>
> When the function is called with two vectors x, and y
>
> myfun(x, y)
>
> It returns NULL. However, when the call made is,
>
>> myfun(x=x, y=y)
>
> The result is
> [1] "x" "y"
>
> Question : how do i get the names of the unknown variables without 
> explicitly saying x=x...
>
> Thanks in advance.
>
> Horace
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Jacques.Veslot at avignon.inra.fr  Mon Jul 30 14:57:55 2007
From: Jacques.Veslot at avignon.inra.fr (Jacques VESLOT)
Date: Mon, 30 Jul 2007 14:57:55 +0200
Subject: [R] regular expressions : extracting numbers
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
Message-ID: <46ADE053.5000008@avignon.inra.fr>

 > gsub(" ", "", gsub("%", "", gsub("[a-z]", "", c("tr3","jh40%qs  dqd"))))
[1] "3"  "40"


Jacques VESLOT

INRA - Biostatistique & Processus Spatiaux
Site Agroparc 84914 Avignon Cedex 9, France

Tel: +33 (0) 4 32 72 21 58
Fax: +33 (0) 4 32 72 21 84



GOUACHE David a ?crit :
> Hello all,
>
> I have a vector of character strings, in which I have letters, numbers, and symbols. What I wish to do is obtain a vector of the same length with just the numbers.
> A quick example -
>
> extract of the original vector :
> "lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb" "rb" "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"
>
> and the type of thing I wish to end up with :
> "2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""
>
> or, instead of "", NA would be acceptable (actually it would almost be better for me)
>
> Anyways, I've been battling with gsub() and things of the sort, but I'm drowning in the regular expressions, despite a few hours of looking at Perl tutorials...
> So if anyone can help me out, it would be greatly appreciated!!
>
> In advance, thanks very much.
>
> David Gouache
> Arvalis - Institut du V?g?tal
> Station de La Mini?re
> 78280 Guyancourt
> Tel: 01.30.12.96.22 / Port: 06.86.08.94.32
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ggrothendieck at gmail.com  Mon Jul 30 15:04:46 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jul 2007 09:04:46 -0400
Subject: [R] regular expressions : extracting numbers
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
Message-ID: <971536df0707300604v5a3dea26sb628035ac20c82f2@mail.gmail.com>

I assume if you want the "" components to be NA then you really intend
the result to be a numeric vector.  The following replaces all non-digits
with "" (thereby removing them) and then uses as.numeric to convert the
result to numeric.  Just omit the conversion if you want a character
vector result:

s <- c("lema, rb 2%", "rb 2%", "rb 3%", "rb 4%", "rb 3%", "rb 2%,mineuse",
   "rb", "rb", "rb 12", "rb", "rj 30%", "rb", "rb", "rb 25%", "rb", "rb",
   "rb", "rj, rb")

as.numeric(gsub("[^[:digit:]]+", "", s))

On 7/30/07, GOUACHE David <D.GOUACHE at arvalisinstitutduvegetal.fr> wrote:
> Hello all,
>
> I have a vector of character strings, in which I have letters, numbers, and symbols. What I wish to do is obtain a vector of the same length with just the numbers.
> A quick example -
>
> extract of the original vector :
> "lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb" "rb" "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"
>
> and the type of thing I wish to end up with :
> "2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""
>
> or, instead of "", NA would be acceptable (actually it would almost be better for me)
>
> Anyways, I've been battling with gsub() and things of the sort, but I'm drowning in the regular expressions, despite a few hours of looking at Perl tutorials...
> So if anyone can help me out, it would be greatly appreciated!!
>
> In advance, thanks very much.
>
> David Gouache
> Arvalis - Institut du V?g?tal
> Station de La Mini?re
> 78280 Guyancourt
> Tel: 01.30.12.96.22 / Port: 06.86.08.94.32
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wills at stats.ox.ac.uk  Mon Jul 30 15:05:05 2007
From: wills at stats.ox.ac.uk (Quin Wills)
Date: Mon, 30 Jul 2007 14:05:05 +0100
Subject: [R] problems saving and loading (PLMset) objects
Message-ID: <001701c7d2aa$45d5a0d0$4f3911ac@simugenqfwills>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/05a4fd86/attachment.pl 

From Max.Kuhn at pfizer.com  Mon Jul 30 15:07:12 2007
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Mon, 30 Jul 2007 09:07:12 -0400
Subject: [R] regular expressions : extracting numbers
In-Reply-To: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
Message-ID: <71257D09F114DA4A8E134DEAC70F25D3090F61A9@groamrexm03.amer.pfizer.com>

This might work:

> numOnly <- function(x) gsub("[^0-9]", "", x)
> numOnly("lema, rb 2%")
[1] "2"
> numOnly("rb")
[1] ""

Max

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of GOUACHE David
Sent: Monday, July 30, 2007 7:59 AM
To: r-help at stat.math.ethz.ch
Subject: [R] regular expressions : extracting numbers

Hello all,

I have a vector of character strings, in which I have letters, numbers, and symbols. What I wish to do is obtain a vector of the same length with just the numbers.
A quick example -

extract of the original vector :
"lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb" "rb" "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"

and the type of thing I wish to end up with :
"2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""

or, instead of "", NA would be acceptable (actually it would almost be better for me)

Anyways, I've been battling with gsub() and things of the sort, but I'm drowning in the regular expressions, despite a few hours of looking at Perl tutorials...
So if anyone can help me out, it would be greatly appreciated!!

In advance, thanks very much.

David Gouache
Arvalis - Institut du V?g?tal
Station de La Mini?re
78280 Guyancourt
Tel: 01.30.12.96.22 / Port: 06.86.08.94.32

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

----------------------------------------------------------------------
LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}


From edoviak at earthlink.net  Mon Jul 30 13:40:47 2007
From: edoviak at earthlink.net (Eric Doviak)
Date: Mon, 30 Jul 2007 07:40:47 -0400 (GMT-04:00)
Subject: [R] the large dataset problem
Message-ID: <25934031.1185795647582.JavaMail.root@elwamui-mouette.atl.sa.earthlink.net>

Dear useRs,

I recently began a job at a very large and heavily bureaucratic organization. We're setting up a research office and statistical analysis will form the backbone of our work. We'll be working with large datasets such the SIPP as well as our own administrative data.

Due to the bureaucracy, it will take some time to get the licenses for proprietary software like Stata. Right now, R is the only statistical software package on my computer. 

This, of course, is a huge limitation because R loads data directly into RAM making it difficult (if not impossible) to work with large datasets. My computer only has 1000 MB of RAM, of which Microsucks Winblows devours 400 MB. To make my memory issues even worse, my computer has a virus scanner that runs everyday and I do not have the administrative rights to turn the damn thing off. 

I need to find some way to overcome these constraints and work with large datasets. Does anyone have any suggestions?

I've read that I should "carefully vectorize my code." What does that mean ??? !!!

The "Introduction to R" manual suggests modifying input files with Perl. Any tips on how to get started? Would Perl Data Language (PDL) be a good choice?  http://pdl.perl.org/index_en.html

I wrote a script which loads large datasets a few lines at a time, writes the dozen or so variables of interest to a CSV file, removes the loaded data and then (via a "for" loop) loads the next few lines .... I managed to get it to work with one of the SIPP core files, but it's SLOOOOW. Worse, if I discover later that I omitted a relevant variable, then I'll have to run the whole script all over again.

Any suggestions?

Thanks,
- Eric


From michal_sa at hotmail.com  Mon Jul 30 05:05:27 2007
From: michal_sa at hotmail.com (michal33)
Date: Sun, 29 Jul 2007 20:05:27 -0700 (PDT)
Subject: [R] A simple question about summary.glm
Message-ID: <11857514.post@talk.nabble.com>


Hello,

I am new to R and have tried to search similar questions but could not find
exactly what I am looking for, but I apologize if the question was already
asked.

I have 10 different treatments and want to know whether they affect the sex
ratios of insect emergence. After running the glms I got this table:

      Df Deviance Resid. Df Resid. Dev      F   Pr(>F)   
NULL                    133     9250.3                   
sex    1    481.5       132     8768.9 7.7212 0.006314 **
trt    9   1099.1       123     7669.7 1.9585 0.049780 * 

But now I would like to know WHICH of the treatments was significant. I
tried to use Tukey test but for some reason it does not work. 
My question is:
I used the following function: 
>summary(file.name, corr=F)
and got the following table:
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-14.118   -4.808   -1.466    2.033   33.882  
Coefficients:
              Estimate Std. Error   t value Pr(>|t|)    
(Intercept)  8.696e+00  1.893e+00     4.594 1.06e-05 ***
sexm        -3.791e+00  1.364e+00    -2.779  0.00631 ** 
trtccc      -1.050e+00  4.325e+00    -0.243  0.80859    
trtcga3      2.450e+00  4.325e+00     0.566  0.57211    
trtcga4     -2.300e+00  4.325e+00    -0.532  0.59584    
trtg         1.550e+00  2.497e+00     0.621  0.53593    
trtga4      -5.550e+00  4.325e+00    -1.283  0.20183    
trtp         5.422e+00  2.566e+00     2.113  0.03658 *  
trtpg       -1.850e+00  2.497e+00    -0.741  0.46019    
trtw        -3.634e-17  2.497e+00 -1.46e-17  1.00000    
trtwg       -3.750e+00  2.497e+00    -1.502  0.13573    

What do the stars  mean? Is it the same as Tukey test that tells me which
treatment is different from which? i.e. is trtp (with *) significantly
different to the control (which, by the way do not appear in this list and I
do not know why)? 

Thanks
Michal


-- 
View this message in context: http://www.nabble.com/A-simple-question-about-summary.glm-tf4167757.html#a11857514
Sent from the R help mailing list archive at Nabble.com.


From maechler at stat.math.ethz.ch  Mon Jul 30 15:38:12 2007
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 30 Jul 2007 15:38:12 +0200
Subject: [R] Slightly OT - use of R
In-Reply-To: <Pine.LNX.4.64.0707301104530.14090@gannet.stats.ox.ac.uk>
References: <XFMail.070730101842.ted.harding@nessie.mcc.ac.uk>
	<Pine.LNX.4.64.0707301104530.14090@gannet.stats.ox.ac.uk>
Message-ID: <18093.59844.742194.99366@stat.math.ethz.ch>

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>>>     on Mon, 30 Jul 2007 11:13:47 +0100 (BST) writes:

    BDR> On Mon, 30 Jul 2007, ted.harding at nessie.mcc.ac.uk wrote:
    >> On 30-Jul-07 08:28:15, John Logsdon wrote:
    >>> I am trying to get a measure of how R compares in usage as a
    >>> statistical platform compared to other software. I would guess
    >>> it is the most widely used among statisticians at least by
    >>> virtue of it being open source.

    BDR> I don't think that is the main reason.  Most of the R users I know 
    BDR> migrated from commercial statistical software for reasons other than cost.
    BDR> (Cross-platform availability has been one major reason.)

much of this is true here (Switzerland) as well.
{And some have *not* migrated because R is Free Software, but
 that's really another story}

Note however that the (non-PhD-graduate) students we teach here
would not be urged to using R if it was not the combination of
its quality and its Free Software state.
And I have had several acquaintances who have only started using
R because they could get it so easily and quickly, and they have
changed to using R as their main computational/statistical
software tool.

    >>> But is there any study to which I can refer? By asking this
    >>> list I am not exactly adopting a rigorous approach!
    >> 
    >> I don't know about that -- my own expectation would be that
    >> serious users of R are likely to be subscribers to the list.
    >> 
    >> So maybe a good answer to your question would be the number
    >> of subscribers (which I'm sure Martin Maechler can find out).
    >> Of course, some people will have subscribed under more than
    >> one email address, so that would somewhat over-estimate the
    >> number of people who subscribe. But it can be traded off
    >> (to a somewhat unknown extent) against R users who do not
    >> subscribe.

    BDR> I think it would be a seriously biased estimate.
    BDR> Few of our hundreds of student users will be subscribed to R-help 
    BDR> (since their first port of call for help is local).
    BDR> Also, we get quite a lot of postings via the gmane and nabble gateways.

Yes, yes, yes.
The exact same situation here and I'd believe in many places.

And the problem with the bias ('factor' rather than 'offset' I'd say)
is that it has been changing over time - I'd guess increasing pretty
dramatically.

My very wild subjective guess would be that 

   #{statisticians seriously using R} / 
   #{R-help subscribers} =  
    =  N_t / n_t

is nowadays well over 20, maybe even over 100,
of course depending on the definition of the numerator N_t.

I could construct a very accurate time-series for n_t,
but since I agree with Brian,
I haven't done so for several years.

Note that  n_{t = 2007-07-30, 07:00} = 5559

    >> More to the point, though, is what you mean by "usage".
    >> If you simply mean "people who use", that's a matter of
    >> counting (one way or another). But there's "use" and "use".

    BDR> Indeed.

"amen" - Martin


From petr.pikal at precheza.cz  Mon Jul 30 15:49:26 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Mon, 30 Jul 2007 15:49:26 +0200
Subject: [R] Odp:  data order by different level of variables
In-Reply-To: <c8b63a350707271718u2b539363r71c244b11832fae@mail.gmail.com>
Message-ID: <OF6B50738E.CEABF27B-ONC1257328.004BCCDB-C1257328.004BE0BC@precheza.cz>

Hi

r-help-bounces at stat.math.ethz.ch napsal dne 28.07.2007 02:18:33:

> Dear useR,
> 
> I have a data matrix, it has n columns, each column is a two-level 
variable
> with entires -1 and +1. They are randomly generated, now I want to order
> them like (for example, 5 columns case)
> -    -    -   -   -
> -    -   -   -    -
> .................
> (first several rows are the samples with all variables in low level)
> 
> +   -   -    -   -
> +   -   -    -    -
> .............................
> 
> 
> -   +   -    -   -
> 
> 
> +  +   -    -   -
> 
> 
> 
> + + + + +
> 
> Is there any function in R that could let me do this order by Var1 then
> order by Var2 then...order by Var n

Did you try

?order

Regards
Petr


> 
> 
> Thanks very much in advance!
> 
> 
> Best,
> 
> Leon
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bcarvalh at jhsph.edu  Mon Jul 30 15:55:22 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 30 Jul 2007 09:55:22 -0400
Subject: [R] generating symmetric matrices
In-Reply-To: <541365.65996.qm@web63504.mail.re1.yahoo.com>
References: <541365.65996.qm@web63504.mail.re1.yahoo.com>
Message-ID: <7B159BE9-3C4E-4FEC-9992-13F2EA9402F6@jhsph.edu>

after dat.cor use:

Rmat[lower.tri(Rmat)] <- dat.cor
Rmat <- t(Rmat)
Rmat[lower.tri(Rmat)] <- dat.cor

b

On Jul 27, 2007, at 11:28 PM, Gregory Gentlemen wrote:

> Greetings,
>
> I have a seemingly simple task which I have not been able to solve  
> today. I want to construct a symmetric matrix of arbtriray size w/o  
> using loops. The following I thought would do it:
>
> p <- 6
> Rmat <- diag(p)
> dat.cor <- rnorm(p*(p-1)/2)
> Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor
>
> However, the problem is that the matrix is filled by column and so  
> the resulting matrix is not symmetric.
>
> I'd be grateful for any adive and/or solutions.
>
> Gregory
>
>
>
>
> ---------------------------------
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Mon Jul 30 16:02:16 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Mon, 30 Jul 2007 16:02:16 +0200
Subject: [R] Odp:  Looping through all possible combinations of cases
In-Reply-To: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>
Message-ID: <OFDCF2138D.C9E7ECC2-ONC1257328.004C88E7-C1257328.004D0D8E@precheza.cz>

Hi

I am not sure about using loops but something like

colSums(combn(DATA[,1],2))
gives you sums of all pairs and

colSums(combn(DATA[,1],3))
sums of all triplets
etc.

if you want to know which ones they are just use combn on names.
e.g.

> y
  X1.5
a    1
b    2
c    3
d    4
e    5

> colSums(combn(y[,1],2))
 [1] 3 4 5 6 5 6 7 7 8 9

> combn(row.names(y),2)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,] "a"  "a"  "a"  "a"  "b"  "b"  "b"  "c"  "c"  "d" 
[2,] "b"  "c"  "d"  "e"  "c"  "d"  "e"  "d"  "e"  "e" 

Regards

Petr

r-help-bounces at stat.math.ethz.ch napsal dne 27.07.2007 20:35:39:

> Hello!
> 
> I have a regular data frame (DATA) with 10 people and 1 column
> ('variable'). Its cases are people with names ('a', 'b', 'c', 'd',
> 'e', 'f', etc.). I would like to write a function that would sum up
> the values on 'variable' of all possible combinations of people, i.e.
> 
> 1. I would like to write a loop - in such a way that it loops through
> each possible pair of cases (i.e., ab, ac, ad, etc.) and sums up their
> respective values on 'variable'
> 
> 2. I would like to write a loop - in such a way that it loops through
> each possible trio of cases (i.e., abc, abd, abe, etc.) and sums up
> their respective values on 'variable'.
> 
> 3.  I would like to write a loop - in such a way that it loops through
> each possible quartet of cases (i.e., abcd, abce, abcf, etc.) and sums
> up their respective values on 'variable'.
> 
> etc.
> 
> Then, at the end I want to capture all possible combinations that were
> considered (i.e., what elements were combined in it) and get the value
> of the sum for each combination.
> 
> How should I do it?
> Thanks a lot!
> Dimitri
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j.logsdon at quantex-research.com  Mon Jul 30 16:05:50 2007
From: j.logsdon at quantex-research.com (John Logsdon)
Date: Mon, 30 Jul 2007 15:05:50 +0100
Subject: [R] generating symmetric matrices
In-Reply-To: <541365.65996.qm@web63504.mail.re1.yahoo.com>
References: <541365.65996.qm@web63504.mail.re1.yahoo.com>
Message-ID: <200707301505.50440.j.logsdon@quantex-research.com>

Since a symmetric matrix must be square, the following code does it:

> X<-5
> MAT<-matrix(rnorm(X*X),X,X)
> MAT<-MAT+t(MAT)
> MAT
           [,1]       [,2]       [,3]       [,4]       [,5]
[1,]  0.1084372 -1.7867366 -0.9620313 -1.0925719 -0.5902326
[2,] -1.7867366 -0.0462097 -1.2707656  0.6112664  1.8673785
[3,] -0.9620313 -1.2707656 -0.3248162  1.5458446  0.7641865
[4,] -1.0925719  0.6112664  1.5458446 -0.1621192 -1.1381366
[5,] -0.5902326  1.8673785  0.7641865 -1.1381366 -2.8668220

If you want to rescale it, for example to make a correlation matrix with the 
diagonal=1, just use cor():

> cor(MAT)
            [,1]       [,2]        [,3]       [,4]       [,5]
[1,]  1.00000000 -0.2941807  0.03784626 -0.6431474 -0.5743853
[2,] -0.29418072  1.0000000  0.65284419 -0.3410521 -0.5921743
[3,]  0.03784626  0.6528442  1.00000000 -0.2502129 -0.7101451
[4,] -0.64314741 -0.3410521 -0.25021285  1.0000000  0.7593021
[5,] -0.57438527 -0.5921743 -0.71014507  0.7593021  1.0000000

The essential trick is to add matrix to transpose.

On Saturday 28 July 2007 04:28:25 Gregory Gentlemen wrote:
> Greetings,
>
> I have a seemingly simple task which I have not been able to solve today. I
> want to construct a symmetric matrix of arbtriray size w/o using loops. The
> following I thought would do it:
>
> p <- 6
> Rmat <- diag(p)
> dat.cor <- rnorm(p*(p-1)/2)
> Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor
>
> However, the problem is that the matrix is filled by column and so the
> resulting matrix is not symmetric.
>
> I'd be grateful for any adive and/or solutions.
>
> Gregory
>
>
>
>
> ---------------------------------
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented, minimal,
> self-contained, reproducible code.



-- 
Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com


From niederlein-rstat at yahoo.de  Mon Jul 30 16:18:44 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Mon, 30 Jul 2007 16:18:44 +0200
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46ADE7E6.4050900@unifi.it>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
	<46ADD4F1.8090901@yahoo.de> <46ADE7E6.4050900@unifi.it>
Message-ID: <46ADF344.3050700@yahoo.de>

Hello,

thank you for your help. But I guess, it's still not what I want... printing df.my gives me

df.my
   v1         v2          v3          v4         v5          v6           v7 v8
1 NA -0.6442149  0.02354036 -1.40362589 -1.1829260  1.17099178 -0.046778203 NA
2 NA -0.2047012 -1.36186952  0.13045724  2.1411553  0.49248118 -0.233788840 NA
3 NA -1.1986041 -0.42197792 -0.84651458 -0.1327081 -0.18690065  0.443908897 NA
4 NA -0.2097442  1.50445971  1.57005071 -0.1053442  1.50050976 -1.649740180 NA
5 NA -0.7343465 -1.76763996  0.06961015 -0.8179396 -0.65552410  0.003991354 NA
6 NA -1.3888750  0.53722404  0.25269771 -1.2342698 -0.01243247 -0.228020092 NA

now, I have to combine like this:

   v1         v2          v3          v4         v5          v6           v7     v8
   NA	     cat1	 cat1	     cat1       cat2        cat2         cat2   NA

-->

mean(df.my$v2[1],df.my$v3[1],df.my$v4[1])
mean(df.my$v2[2],df.my$v3[2],df.my$v4[2])
mean(df.my$v2[3],df.my$v3[3],df.my$v4[3])
mean(df.my$v2[4],df.my$v3[4],df.my$v4[4])
mean(df.my$v2[5],df.my$v3[5],df.my$v4[5])
mean(df.my$v2[6],df.my$v3[6],df.my$v4[6])

the same for v5, v6 and v7

further, I'm not sure how to avoid the list, because this is the result of the processing I did before...

Ciao,
Antje


8rino-Luca Pantani schrieb:
> I hope I see.
> 
> Why not try the following, and avoid lists, which I'm not still able to 
> manage properly ;-)
> v1 <- NA
> v2 <- rnorm(6)
> v3 <- rnorm(6)
> v4 <- rnorm(6)
> v5 <- rnorm(6)
> v6 <- rnorm(6)
> v7 <- rnorm(6)
> v8 <- rnorm(6)
> v8 <- NA
> (df.my <- cbind.data.frame(v1, v2, v3, v4, v5, v6, v7, v8))
> (df.my2 <- reshape(df.my,
>                  varying=list(c("v1","v2","v3", "v4","v5","v6","v7","v8")),
>                  idvar="sequential",
>                  timevar="cat",
>                  direction="long"
>        ))
> aggregate(df.my2$v1, by=list(category=df.my2$cat), mean)
> aggregate(df.my2$v1, by=list(category=df.my2$cat), function(x){sd(x, 
> na.rm = TRUE)})
> 
> 
> Antje ha scritto:
>> okay, I played a bit around and now I have some kind of testcase for you:
>>
>> v1 <- NA
>> v2 <- rnorm(6)
>> v3 <- rnorm(6)
>> v4 <- rnorm(6)
>> v5 <- rnorm(6)
>> v6 <- rnorm(6)
>> v7 <- rnorm(6)
>> v8 <- rnorm(6)
>> v8 <- NA
>>
>> list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
>> categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)
>>
>> > list
>> [[1]]
>> [1] NA
>>
>> [[2]]
>> [1] -0.6442149 -0.2047012 -1.1986041 -0.2097442 -0.7343465 -1.3888750
>>
>> [[3]]
>> [1]  0.02354036 -1.36186952 -0.42197792  1.50445971 -1.76763996  
>> 0.53722404
>>
>> [[4]]
>> [1] -1.40362589  0.13045724 -0.84651458  1.57005071  0.06961015  
>> 0.25269771
>>
>> [[5]]
>> [1] -1.1829260  2.1411553 -0.1327081 -0.1053442 -0.8179396 -1.2342698
>>
>> [[6]]
>> [1]  1.17099178  0.49248118 -0.18690065  1.50050976 -0.65552410 
>> -0.01243247
>>
>> [[7]]
>> [1] -0.046778203 -0.233788840  0.443908897 -1.649740180  0.003991354 
>> -0.228020092
>>
>> [[8]]
>> [1] NA
>>
>> now, I need the means (and sd) of element 1 of list[2],list[3],list[4] 
>> (because they belong to "cat1") and
>>
>> = mean(-0.6442149, 0.02354036, -1.40362589)
>>
>> the same for element 2 up to element 6 (--> I would the get a vector 
>> containing the means for "cat1")
>> the same for the vectors belonging to "cat2".
>>
>> does anybody now understand what I mean?
>>
>> Antje
>>
>>
>>
>


From kamauallan at yahoo.com  Mon Jul 30 16:22:30 2007
From: kamauallan at yahoo.com (Allan Kamau)
Date: Mon, 30 Jul 2007 07:22:30 -0700 (PDT)
Subject: [R] Matrix nesting (was Re: Obtaining summary of frequencies of
	value occurrences for a variable in a multivariate dataset.)
Message-ID: <360884.31834.qm@web53504.mail.re2.yahoo.com>

Success, thanks Patrick. Below is the final matrix construction code.

x=list()
x[length(myVariableNames)]<-NA
names(x)<-names(x.val)
for (i in myVariableNames){
    residues=names(x.val[[i]])
    residuesFrequencies=as.vector(x.val[[i]])
    someList=list()
    names(residuesFrequencies)=residues
    
    someList<-list(frequency=residuesFrequencies)
    x[i]<-someList
}

#The output

> x[16:18]
$PR12
 I
10

$PR13
K R
8 2

$PR14
I V
2 8

>



----- Original Message ----
From: Patrick Burns <pburns at pburns.seanet.com>
To: Allan Kamau <kamauallan at yahoo.com>
Sent: Monday, July 30, 2007 12:01:32 PM
Subject: Re: [R] Matrix nesting (was Re: Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.)

I think you want your main matrix to be of mode
list.  S Poetry talks about this some.

Patrick Burns
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Allan Kamau wrote:

>Hi
>
>    
>    
>    
>    
>    
>    
>    <!--
>        @page { size: 21cm 29.7cm; margin: 2cm }
>        P { margin-bottom: 0.21cm }
>    -->
>    
>
>I would like to nest matrices, is there
>a way of doing so, I am getting ?number of items to replace is not
>a multiple of replacement length? errors (probably R is trying to
>flatten the matrix into a vector and complains if the vector is
>larger than 1 element during the insert)
>
>I have a matrix (see below) in which I
>would like to place one other matrices in to each k[2,i] position
>(where i is value between 1 to 4)
>
>Why ? each value in k[1,i] may
>represent several (1or more) key-value results which I would like to
>capture in the corresponding k[2,i] element.
>
>
>
>
>
>  
>
>>k
>>    
>>
>
>                [,1]   [,2]   [,3]  
>[,4]
>
>myVariableNames "PR10" "PR11"
>"PR12" "PR13"
>
>x2              "0"    "0"
>   "0"    "0"
>
>  
>
>
>  
>
>
>
>
>
>
>Allan.
>
>
>
>----- Original Message ----
>From: Allan Kamau <kamauallan at yahoo.com>
>To: jim holtman <jholtman at gmail.com>
>Cc: r-help at stat.math.ethz.ch
>Sent: Saturday, July 28, 2007 2:48:47 PM
>Subject: Re: [R] Obtaining summary of frequencies of value occurrences for a variable in a multivariate dataset.
>
>Hi Jim,
>The problem description.
>I am trying to identify mutations in a given gene from
>a particular genome (biological genome sequence).
>I have two CSV files consisting of sequences. One file
>consists of reference (documented,curated accepted as
>standard) sequences. The other consists of sample
>sequences I am trying to identify mutations within. In
>both files the an individual sequence is contained in
>a single record, it?s amino acid residues ( the actual
>sequence of alphabets each representing a given amino
>acid for example ?A? stands for ?Alanine?, ?C? for
>Cysteine and so on) are each allocated a single field
>in the CSV file.
>The sequences in both files have been well aligned,
>each contain 115 residues with the first residue is
>contained in the field 5. The fields 1 to 4 are
>allocated for metadata (name of sequence and so on).
>My task is to compile a residue occurrence count for
>each residue present in a given field in the reference
>sequence dataset and use this information when reading
>each sequence in the sample dataset to identify a
>mutation. For example for position 9 of the sample
>sequence ?bb? a ?P? is found and according to our
>reference sequence dataset of summaries, at position 9
>?P? may not even exist or may have an occurrence of
>10% or so will be classified as mutation, (I could
>employ a cut of parameter for mutation
>classification).
>
>
>Allan.
>
>--- jim holtman <jholtman at gmail.com> wrote:
>
>  
>
>>results=()#character()
>>myVariableNames=names(x.val)
>>results[length(myVariableNames)]<-NA
>>
>>for (i in myVariableNames){
>>    results[i]<-names(x.val[[i]])    # this does not
>>work it returns a
>>NULL (how can i convert this to x.val$"somevalue" ?
>>)
>>}
>>
>>
>>
>>On 7/27/07, Allan Kamau <kamauallan at yahoo.com>
>>wrote:
>>    
>>
>>>Hi All,
>>>I am having difficulties finding a way to find a
>>>      
>>>
>>substitute to the command "names(v.val$PR14)" so
>>that I could generate the command on the fly for all
>>PR14 to PR200 (please see the previous discussion
>>below to understand what the object x.val contains)
>>. I have tried the following
>>    
>>
>>>>results=()#character()
>>>>myVariableNames=names(x.val)
>>>>results[length(myVariableNames)]<-NA
>>>>        
>>>>
>>>>for
>>>>        
>>>>
>>as.vector(unlist(strsplit(str,",")),mode="list")
>>    
>>
>>>+    results[i]<-names(x.val$i)    # this does not
>>>      
>>>
>>work it returns a NULL (how can i convert this to
>>x.val$"somevalue" ? )
>>    
>>
>>>>}
>>>>        
>>>>
>>>Allan.
>>>
>>>
>>>----- Original Message ----
>>>From: Allan Kamau <kamauallan at yahoo.com>
>>>To: r-help at stat.math.ethz.ch
>>>Sent: Thursday, July 26, 2007 10:03:17 AM
>>>Subject: Re: [R] Obtaining summary of frequencies
>>>      
>>>
>>of value occurrences for a variable in a
>>multivariate dataset.
>>    
>>
>>>Thanks so much Jim, Andaikalavan, Gabor and others
>>>      
>>>
>>for the help and suggestions.
>>    
>>
>>>The solution will result in a matrix containing
>>>      
>>>
>>nested matrices to enable each variable name, each
>>variables distinct value and the count of the
>>distinct value to be accessible individually.
>>    
>>
>>>The main matrix will contain the variable names,
>>>      
>>>
>>the first level nested matrices will consist of the
>>variables unique values, and each such variable
>>entry will contain a one element vector to contain
>>the count or occurrence frequency.
>>    
>>
>>>This matrix can now be used in comparing other
>>>      
>>>
>>similar datasets for variable values and their
>>frequencies.
>>    
>>
>>>Building on the input received so far, a probable
>>>      
>>>
>>solution in building the matrix will include the
>>following.
>>    
>>
>>>1)I reading the csv file (containing column
>>>      
>>>
>>headers)
>>    
>>
>>my_data=read.table("<path/to/my/data.csv>",header=TRUE,sep=",",dec=".",fill=TRUE)
>>    
>>
>>>2)I group the values in each variable producing an
>>>      
>>>
>>occurrence count(frequency)
>>    
>>
>>>>x.val<-apply(my_data,2,table)
>>>>        
>>>>
>>>3)I obtain a vector of the names of the variables
>>>      
>>>
>>in the table
>>    
>>
>>>>names(x.val)
>>>>        
>>>>
>>>4)Now I make use of the names (obtained in step 3)
>>>      
>>>
>>to obtain a vector of distinct values in a given
>>variable (in the example below the variable name is
>>$PR14)
>>    
>>
>>>>names(v.val$PR14)
>>>>        
>>>>
>>>5)I obtain a vector (with one element) of the
>>>      
>>>
>>frequency of a value obtained from the step above
>>(in our example the value is "V")
>>    
>>
>>>>as.vector(x.val$PR14["V"])
>>>>        
>>>>
>>>Todo:
>>>Now I will need to place the steps above in a
>>>      
>>>
>>script (consisting of loops) to build the matrix,
>>step 4 and 5 seem tricky to do programatically.
>>    
>>
>>>Allan.
>>>
>>>
>>>----- Original Message ----
>>>From: jim holtman <jholtman at gmail.com>
>>>To: Allan Kamau <kamauallan at yahoo.com>
>>>Cc: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>;
>>>      
>>>
>>r-help at stat.math.ethz.ch
>>    
>>
>>>Sent: Wednesday, July 25, 2007 1:50:55 PM
>>>Subject: Re: [R] Obtaining summary of frequencies
>>>      
>>>
>>of value occurrences for a variable in a
>>multivariate dataset.
>>    
>>
>>>Also if you want to access the individual values,
>>>      
>>>
>>you can just leave
>>    
>>
>>>it as a list:
>>>
>>>      
>>>
>>>>x.val <- apply(x, 2, table)
>>>># access each value
>>>>x.val$PR14["V"]
>>>>        
>>>>
>>>V
>>>8
>>>
>>>
>>>
>>>On 7/25/07, Allan Kamau <kamauallan at yahoo.com>
>>>      
>>>
>>wrote:
>>    
>>
>>>>A subset of the data looks as follows
>>>>
>>>>        
>>>>
>>>>>df[1:10,14:20]
>>>>>          
>>>>>
>>>>  PR10 PR11 PR12 PR13 PR14 PR15 PR16
>>>>1     V    T    I    K    V    G    D
>>>>2     V    S    I    K    V    G    G
>>>>3     V    T    I    R    V    G    G
>>>>4     V    S    I    K    I    G    G
>>>>5     V    S    I    K    V    G    G
>>>>6     V    S    I    R    V    G    G
>>>>7     V    T    I    K    I    G    G
>>>>8     V    S    I    K    V    E    G
>>>>9     V    S    I    K    V    G    G
>>>>10    V    S    I    K    V    G    G
>>>>
>>>>The result I would like is as follows
>>>>
>>>>PR10        PR11          PR12   ...
>>>>[V:10]    [S:7,T:3]    [I:10]
>>>>
>>>>The result can be in a matrix or a vector and
>>>>        
>>>>
>>each variablename, value and frequency should be
>>accessible so as to be used for comparisons with
>>another dataset later.
>>    
>>
>>>>The frequency can be a count or a percentage.
>>>>
>>>>
>>>>Allan.
>>>>
>>>>
>>>>----- Original Message ----
>>>>From: Adaikalavan Ramasamy
>>>>        
>>>>
>><ramasamy at cancer.org.uk>
>>    
>>
>>>>To: Allan Kamau <kamauallan at yahoo.com>
>>>>Cc: r-help at stat.math.ethz.ch
>>>>Sent: Tuesday, July 24, 2007 10:21:51 PM
>>>>Subject: Re: [R] Obtaining summary of
>>>>        
>>>>
>>frequencies of value occurrences for a variable in a
>>multivariate dataset.
>>    
>>
>>>>The name of the table should give you the
>>>>        
>>>>
>>"value". And if you have a
>>    
>>
>>>>matrix, you just need to convert it into a
>>>>        
>>>>
>>vector first.
>>    
>>
>>>> > m <- matrix( LETTERS[ c(1:3, 3:5, 2:4) ],
>>>>        
>>>>
>>nc=3 )
>>    
>>
>>>> > m
>>>>     [,1] [,2] [,3]
>>>>[1,] "A"  "C"  "B"
>>>>[2,] "B"  "D"  "C"
>>>>[3,] "C"  "E"  "D"
>>>> > tb <- table( as.vector(m) )
>>>> > tb
>>>>
>>>>A B C D E
>>>>1 2 3 2 1
>>>> > paste( names(tb), ":", tb, sep="" )
>>>>[1] "A:1" "B:2" "C:3" "D:2" "E:1"
>>>>
>>>>If this is not what you want, then please give a
>>>>        
>>>>
>>simple example.
>>    
>>
>>>>Regards, Adai
>>>>
>>>>
>>>>
>>>>Allan Kamau wrote:
>>>>        
>>>>
>>>>>Hi all,
>>>>>If the question below as been answered before
>>>>>          
>>>>>
>>I
>>    
>>
>>>>>apologize for the posting.
>>>>>I would like to get the frequencies of
>>>>>          
>>>>>
>>occurrence of
>>    
>>
>>>>>all values in a given variable in a
>>>>>          
>>>>>
>>multivariate
>>    
>>
>>>>>dataset. In short for each variable (or field)
>>>>>          
>>>>>
>>a
>>    
>>
>>>>>summary of values contained with in a
>>>>>          
>>>>>
>>value:frequency
>>
>>    
>>
>=== message truncated ===
>
>
>
>       
>____________________________________________________________________________________
>
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>  
>


From murdoch at stats.uwo.ca  Mon Jul 30 16:35:49 2007
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 30 Jul 2007 10:35:49 -0400
Subject: [R] Package manual examples - 'unexpected$undefined' errors
In-Reply-To: <acb54ab40707280303g4fc79bbyaf475eba094f8eb8@mail.gmail.com>
References: <acb54ab40707280303g4fc79bbyaf475eba094f8eb8@mail.gmail.com>
Message-ID: <46ADF745.2050308@stats.uwo.ca>

David Pain wrote:
> Trying out an unfamiliar package, the natural thing is to use the examples
> given in the package's manual - hopefully, the writers of the package
> wouldn't include examples which didn't work!
>
> Recently, though, I've been getting 'unexpected$undefined' error messages
> when doing this, despite having copy/pasted the text from the manual (taking
> out hard breaks on the way).
>
> Moreover, I've had error messages for commands which I've previously had
> work fine.
>
> For instance, this from Zelig
>
> z.out <? zelig(vote ~ race + educate, model = "logit", data = turnout)
>
> has at different times worked fine and thrown up the error message.
>
> Any help gratefully received.
>
> 	[[alternative HTML version deleted]]
It's hard to say exactly what's going wrong, but a guess is that in your 
workspace you have an object which is somehow conflicting with an object 
in the package.  Try starting R with the --vanilla command line option
and if the errors go away, that's why.

I don't know whether this applies to the Zelig package, but packages 
that don't define namespaces are fragile in that their internal 
functions can be masked by same-named functions in your workspace.  Even 
if the package does have a namespace, you can mask functions from it 
that you call:  for example, if you had a function called zelig (perhaps 
because you used fix(zelig) to make a small change to the existing one), 
your line above would call yours, not the original.

Duncan Murdoch


From HDoran at air.org  Mon Jul 30 16:43:59 2007
From: HDoran at air.org (Doran, Harold)
Date: Mon, 30 Jul 2007 10:43:59 -0400
Subject: [R] Matrix Multiplication, Floating-Point, etc.
In-Reply-To: <BAY108-F24AABC3636FC394FB1D4A8AAF30@phx.gbl>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EE58107@dc1ex01.air.org>

This is giving you exactly what you are asking for. The operator * does
element by element multiplication. So, .48 + -.48 =0, right?  Is there
another mathematical possibility you were expecting?



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talbot Katz
> Sent: Friday, July 27, 2007 6:31 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Matrix Multiplication, Floating-Point, etc.
> 
> Hi.
> 
> I recently tried the following in R 2.5.1 on Windows XP:
> 
> >ev2<-c(0.8,-0.6)
> >ev1<-c(0.6,0.8)
> >ev1%*%ev2
>               [,1]
> [1,] -2.664427e-17
> >sum(ev1*ev2)
> [1] 0
> >
> 
> (I got the same result with R 2.4.1 on a different Windows XP 
> machine.)
> 
> I expect this issue is very familiar and probably has been 
> discussed in this forum before.  Can someone please point me 
> to some documentation or discussion about this?  Is there 
> some standard way to get the "correct" 
> answer from %*%?
> 
> Thanks!
> 
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bates at stat.wisc.edu  Mon Jul 30 17:00:16 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 Jul 2007 10:00:16 -0500
Subject: [R] generating symmetric matrices
In-Reply-To: <541365.65996.qm@web63504.mail.re1.yahoo.com>
References: <541365.65996.qm@web63504.mail.re1.yahoo.com>
Message-ID: <40e66e0b0707300800w766f6eb7q36569596d1544254@mail.gmail.com>

On 7/27/07, Gregory Gentlemen <gregory_gentlemen at yahoo.ca> wrote:
> Greetings,

> I have a seemingly simple task which I have not been able to solve today. I want to construct a symmetric matrix of arbtriray size w/o using loops. The following I thought would do it:

> p <- 6
> Rmat <- diag(p)
> dat.cor <- rnorm(p*(p-1)/2)
> Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor

> However, the problem is that the matrix is filled by column and so the resulting matrix is not symmetric.

Could you provide more detail on the properties of the symmetric
matrices that you would like to generate?  It seems that you are
trying to generate correlation matrices.  Is that the case?  Do you
wish the matrices to be a random sample from a specific distribution.
If so, what distribution?


From ottorino-luca.pantani at unifi.it  Mon Jul 30 17:24:17 2007
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Mon, 30 Jul 2007 17:24:17 +0200
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46ADF344.3050700@yahoo.de>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
	<46ADD4F1.8090901@yahoo.de> <46ADE7E6.4050900@unifi.it>
	<46ADF344.3050700@yahoo.de>
Message-ID: <46AE02A1.4050206@unifi.it>

Ok. I missed the grouping factor
Try this.
You can modify "my factor" to fit your needs.
As to avoid list, I cannot help, sorry
I use them only when I have to collect different classes of objects.

v1 <- NA
v2 <- rnorm(6)
v3 <- rnorm(6)
v4 <- rnorm(6)
v5 <- rnorm(6)
v6 <- rnorm(6)
v7 <- rnorm(6)
v8 <- rnorm(6)
v8 <- NA

df.my <- cbind.data.frame(v1, v2, v3, v4, v5, v6, v7, v8)
(df.my2 <- reshape(df.my,
                  varying=list(c("v1","v2","v3", "v4","v5","v6","v7","v8")),
                  idvar="sequential",
                  timevar="cat",
                  direction="long"
        ))
my.factor <- factor(
                    ifelse(is.na(df.my2$v1), "not.considered",
                           ifelse(df.my2$cat %in% 2:4, "cat1", "cat2")
                           )
                    )
df.my3 <- cbind(df.my2, Correct.Cat =my.factor)
aggregate(df.my2$v1, by=list(category=df.my3$Correct.Cat), mean)
aggregate(df.my2$v1, by=list(category=df.my3$Correct.Cat), 
function(x){sd(x, na.rm = TRUE)})

Antje ha scritto:
> Hello,
>
> thank you for your help. But I guess, it's still not what I want... 
> printing df.my gives me
>
> df.my
>   v1         v2          v3          v4         v5          
> v6           v7 v8
> 1 NA -0.6442149  0.02354036 -1.40362589 -1.1829260  1.17099178 
> -0.046778203 NA
> 2 NA -0.2047012 -1.36186952  0.13045724  2.1411553  0.49248118 
> -0.233788840 NA
> 3 NA -1.1986041 -0.42197792 -0.84651458 -0.1327081 -0.18690065  
> 0.443908897 NA
> 4 NA -0.2097442  1.50445971  1.57005071 -0.1053442  1.50050976 
> -1.649740180 NA
> 5 NA -0.7343465 -1.76763996  0.06961015 -0.8179396 -0.65552410  
> 0.003991354 NA
> 6 NA -1.3888750  0.53722404  0.25269771 -1.2342698 -0.01243247 
> -0.228020092 NA
>
> now, I have to combine like this:
>
>   v1         v2          v3          v4         v5          
> v6           v7     v8
>   NA         cat1     cat1         cat1       cat2        cat2         
> cat2   NA
>
> -->
>
> mean(df.my$v2[1],df.my$v3[1],df.my$v4[1])
> mean(df.my$v2[2],df.my$v3[2],df.my$v4[2])
> mean(df.my$v2[3],df.my$v3[3],df.my$v4[3])
> mean(df.my$v2[4],df.my$v3[4],df.my$v4[4])
> mean(df.my$v2[5],df.my$v3[5],df.my$v4[5])
> mean(df.my$v2[6],df.my$v3[6],df.my$v4[6])
>
> the same for v5, v6 and v7
>
> further, I'm not sure how to avoid the list, because this is the 
> result of the processing I did before...
>
> Ciao,
> Antje
>
>
> 8rino-Luca Pantani schrieb:
>> I hope I see.
>>
>> Why not try the following, and avoid lists, which I'm not still able 
>> to manage properly ;-)
>> v1 <- NA
>> v2 <- rnorm(6)
>> v3 <- rnorm(6)
>> v4 <- rnorm(6)
>> v5 <- rnorm(6)
>> v6 <- rnorm(6)
>> v7 <- rnorm(6)
>> v8 <- rnorm(6)
>> v8 <- NA
>> (df.my <- cbind.data.frame(v1, v2, v3, v4, v5, v6, v7, v8))
>> (df.my2 <- reshape(df.my,
>>                  varying=list(c("v1","v2","v3", 
>> "v4","v5","v6","v7","v8")),
>>                  idvar="sequential",
>>                  timevar="cat",
>>                  direction="long"
>>        ))
>> aggregate(df.my2$v1, by=list(category=df.my2$cat), mean)
>> aggregate(df.my2$v1, by=list(category=df.my2$cat), function(x){sd(x, 
>> na.rm = TRUE)})
>>
>>
>> Antje ha scritto:
>>> okay, I played a bit around and now I have some kind of testcase for 
>>> you:
>>>
>>> v1 <- NA
>>> v2 <- rnorm(6)
>>> v3 <- rnorm(6)
>>> v4 <- rnorm(6)
>>> v5 <- rnorm(6)
>>> v6 <- rnorm(6)
>>> v7 <- rnorm(6)
>>> v8 <- rnorm(6)
>>> v8 <- NA
>>>
>>> list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
>>> categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)
>>>
>>> > list
>>> [[1]]
>>> [1] NA
>>>
>>> [[2]]
>>> [1] -0.6442149 -0.2047012 -1.1986041 -0.2097442 -0.7343465 -1.3888750
>>>
>>> [[3]]
>>> [1]  0.02354036 -1.36186952 -0.42197792  1.50445971 -1.76763996  
>>> 0.53722404
>>>
>>> [[4]]
>>> [1] -1.40362589  0.13045724 -0.84651458  1.57005071  0.06961015  
>>> 0.25269771
>>>
>>> [[5]]
>>> [1] -1.1829260  2.1411553 -0.1327081 -0.1053442 -0.8179396 -1.2342698
>>>
>>> [[6]]
>>> [1]  1.17099178  0.49248118 -0.18690065  1.50050976 -0.65552410 
>>> -0.01243247
>>>
>>> [[7]]
>>> [1] -0.046778203 -0.233788840  0.443908897 -1.649740180  0.003991354 
>>> -0.228020092
>>>
>>> [[8]]
>>> [1] NA
>>>
>>> now, I need the means (and sd) of element 1 of 
>>> list[2],list[3],list[4] (because they belong to "cat1") and
>>>
>>> = mean(-0.6442149, 0.02354036, -1.40362589)
>>>
>>> the same for element 2 up to element 6 (--> I would the get a vector 
>>> containing the means for "cat1")
>>> the same for the vectors belonging to "cat2".
>>>
>>> does anybody now understand what I mean?
>>>
>>> Antje
>>>
>>>
>>>
>>
>
>

-- 
Ottorino-Luca Pantani, Universit? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta
P.zle Cascine 28 50144 Firenze Italia
Tel 39 055 3288 202 (348 lab) Fax 39 055 333 273 
OLPantani at unifi.it


From bcarvalh at jhsph.edu  Mon Jul 30 17:27:11 2007
From: bcarvalh at jhsph.edu (Benilton Carvalho)
Date: Mon, 30 Jul 2007 11:27:11 -0400
Subject: [R] xtable with vector
In-Reply-To: <20070728183929.zujhqnbysowww8go@webmail.tuwien.ac.at>
References: <mailman.11.1185616805.19480.r-help@stat.math.ethz.ch>
	<20070728183929.zujhqnbysowww8go@webmail.tuwien.ac.at>
Message-ID: <B103F94D-F807-4C92-AF48-7EA69701A0E1@jhsph.edu>

a <- matrix(1:6, nr=1)
colnames(a) <- paste("col", 1:6)
xtable(a)


On Jul 28, 2007, at 12:39 PM, Stefan Nachtnebel wrote:

> Hello,
>
> Is there a possibility to use xtable with a vector to generate a latex
> table? I always get an error, that no applicable method is available.
>
> For example:
>
> b<-1:12
> dim(b)<-c(2,6)
> dimnames(b)[[2]]<-paste("col",1:6)
> xtable(b)
>
> works fine and does not raise an error, but
>
> a<-1:6
> names(a)<-paste(col,1:6)
> xtable(b)
>
> does not work.
>
> Regards, Stefan


From gregory_gentlemen at yahoo.ca  Mon Jul 30 17:29:29 2007
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Mon, 30 Jul 2007 11:29:29 -0400 (EDT)
Subject: [R] generating symmetric matrices
In-Reply-To: <40e66e0b0707300800w766f6eb7q36569596d1544254@mail.gmail.com>
Message-ID: <155199.55586.qm@web63505.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/f406fb88/attachment.pl 

From ggrothendieck at gmail.com  Mon Jul 30 17:33:02 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jul 2007 11:33:02 -0400
Subject: [R] Bind together two vectors of different length...
In-Reply-To: <200707301309.59325.andza@osi.lv>
References: <200707301309.59325.andza@osi.lv>
Message-ID: <971536df0707300833v2d3d54f0ke0510d05dba603f@mail.gmail.com>

Using the zoo package create zoo objects and merge them.  This
code assumes that A and B each have unique and ascending values:

> library(zoo)
> A <- seq(1, 10, 1)
> B <- seq(1, 10, 2)
> t(merge(A = zoo(A,A), B = zoo(B,B), fill = 0))
  1 2 3 4 5 6 7 8 9 10
A 1 2 3 4 5 6 7 8 9 10
B 1 0 3 0 5 0 7 0 9  0

Omit fill = 0 to fill with NAs (which is the default).  Note that if
we define A <- 1:10 (rather than as we did it above) then A will be
integer while B will be numeric so we will get a warning (which we
can eliminate with the suppressWarnings warnings) but it still
works.

A variation of this without zoo is:

> t(merge(cbind(A), cbind(B, B), by = 1, all.x = TRUE))
  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
A    1    2    3    4    5    6    7    8    9    10
B    1   NA    3   NA    5   NA    7   NA    9    NA

however, R's merge does not support fill= so you will have
to replace the NAs with 0's yourself if you want those instead
of NA's.

> out <- t(merge(cbind(A), cbind(B, B), by = 1, all.x = TRUE))
> out[is.na(out)] <- 0  # replace NA's with 0
> out
  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
A    1    2    3    4    5    6    7    8    9    10
B    1    0    3    0    5    0    7    0    9     0



On 7/30/07, Andris Jankevics <andza at osi.lv> wrote:
> Dear everyone,
>
> I've got difficulties in realizing the following
> task:
>
> I have two vectors:
> A <- c(1:10)
> B<- seq(1,10,2)
>
> Now I want to make a table form vectors A and B as rows, and if a value of A
> isn't present B, then I want to put a N/A symbol in it:
>
> Output should look like this:
>
> 1 2 3 4 5 6 7 8 9 10
> 1 0 3 0 5 0 7 0 9 0
>
> How can I do this in R?
>
> Thank you.
>
> --
> Andris Jankevics
> Assistant
> Department of Medicinal Chemistry
> Latvian Institute of Organic Synthesis
> Aizkraukles 21, LV-1006, Riga, Latvia
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ronggui.huang at gmail.com  Mon Jul 30 17:34:30 2007
From: ronggui.huang at gmail.com (ronggui)
Date: Mon, 30 Jul 2007 23:34:30 +0800
Subject: [R] beta regressions in R
In-Reply-To: <E1IEonx-0005oh-J8@elasmtp-junco.atl.sa.earthlink.net>
References: <E1IEonx-0005oh-J8@elasmtp-junco.atl.sa.earthlink.net>
Message-ID: <38b9f0350707300834t757d64ady93136fdcd7e02999@mail.gmail.com>

see the package "betareg" in CRAN.

2007/7/29, Walter R. Paczkowski <dataanalytics at earthlink.net>:
>
>    Good morning,
>    Does anyone know of a package or function to do a beta regression?
>    Thanks,
>    Walt Paczkowski
>
>    _________________________________
>    Walter R. Paczkowski, Ph.D.
>    Data Analytics Corp.
>    44 Hamilton Lane
>    Plainsboro, NJ  08536
>    (V) 609-936-8999
>    (F) 609-936-3733
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ronggui Huang
Department of Sociology
Fudan University, Shanghai, China


From francogrex at mail.com  Mon Jul 30 17:36:48 2007
From: francogrex at mail.com (francogrex)
Date: Mon, 30 Jul 2007 08:36:48 -0700 (PDT)
Subject: [R] deriv, loop
Message-ID: <11853456.post@talk.nabble.com>


Hi, 2 questions:

Question 1: example of what I currently do:

for(i in 1:6){sink("temp.txt",append=TRUE)
dput(i+0)
sink()}
x=scan(file="temp.txt")
print(prod(x))
file.remove("C:/R-2.5.0/temp.txt")

But how to convert the output of the loop to a vector that I can manipulate
(by prod or sum etc), without having to write and append to a file?

Question 2:

> deriv(~gamma(x),"x")

expression({
    .expr1 <- gamma(x)
    .value <- .expr1
    .grad <- array(0, c(length(.value), 1), list(NULL, c("x")))
    .grad[, "x"] <- .expr1 * psigamma(x)
    attr(.value, "gradient") <- .grad
    .value
})

BUT

> deriv3(~gamma(x),"x")
Error in deriv3.formula(~gamma(x), "x") : Function 'psigamma' is not in the
derivatives table

What I want is the expression for the second derivative (which I believe is
trigamma(x), or psigamma(x,1)), how can I obtain that?

Thanks
-- 
View this message in context: http://www.nabble.com/deriv%2C-loop-tf4166283.html#a11853456
Sent from the R help mailing list archive at Nabble.com.


From meyerjp at jmu.edu  Mon Jul 30 17:41:53 2007
From: meyerjp at jmu.edu (J. Patrick Meyer)
Date: Mon, 30 Jul 2007 11:41:53 -0400
Subject: [R] error in using R2WinBUGS on Ubuntu 6.10 Linux
In-Reply-To: <97753.85074.qm@web43137.mail.sp1.yahoo.com>
References: <97753.85074.qm@web43137.mail.sp1.yahoo.com>
Message-ID: <46AE06C1.5030400@jmu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/1a109fa2/attachment.pl 

From bbernzwe at bear.com  Mon Jul 30 17:46:10 2007
From: bbernzwe at bear.com (Bernzweig, Bruce (Consultant))
Date: Mon, 30 Jul 2007 11:46:10 -0400
Subject: [R] the large dataset problem
References: <9413086.1185646053804.JavaMail.root@elwamui-royal.atl.sa.earthlink.net>
Message-ID: <CADFD0E28E1E6A46B0C84335BDB994F504989BFE@whexchmb16.bsna.bsroot.bear.com>

Hi Eric,

I'm facing a similar problem.

Looking over the list of packages I came across:

 	R.huge: Methods for accessing huge amounts of data 
 	http://cran.r-project.org/src/contrib/Descriptions/R.huge.html

I haven't installed it yet so I don't know how well it works.  I
probably won't have time until next week at the earliest to look at it.

Would be interested in hearing your feedback if you do try it.

- Bruce

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eric Doviak
Sent: Saturday, July 28, 2007 2:08 PM
To: r-help at stat.math.ethz.ch
Subject: [R] the large dataset problem

Dear useRs,

I recently began a job at a very large and heavily bureaucratic
organization. We're setting up a research office and statistical
analysis will form the backbone of our work. We'll be working with large
datasets such the SIPP as well as our own administrative data.

Due to the bureaucracy, it will take some time to get the licenses for
proprietary software like Stata. Right now, R is the only statistical
software package on my computer. 

This, of course, is a huge limitation because R loads data directly into
RAM making it difficult (if not impossible) to work with large datasets.
My computer only has 1000 MB of RAM, of which Microsucks Winblows
devours 400 MB. To make my memory issues even worse, my computer has a
virus scanner that runs everyday and I do not have the administrative
rights to turn the damn thing off. 

I need to find some way to overcome these constraints and work with
large datasets. Does anyone have any suggestions?

I've read that I should "carefully vectorize my code." What does that
mean ??? !!!

The "Introduction to R" manual suggests modifying input files with Perl.
Any tips on how to get started? Would Perl Data Language (PDL) be a good
choice?  http://pdl.perl.org/index_en.html

I wrote a script which loads large datasets a few lines at a time,
writes the dozen or so variables of interest to a CSV file, removes the
loaded data and then (via a "for" loop) loads the next few lines .... I
managed to get it to work with one of the SIPP core files, but it's
SLOOOOW. Worse, if I discover later that I omitted a relevant variable,
then I'll have to run the whole script all over again.

Any suggestions?

Thanks,
- Eric

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



**********************************************************************
Please be aware that, notwithstanding the fact that the pers...{{dropped}}


From apjaworski at mmm.com  Mon Jul 30 17:48:41 2007
From: apjaworski at mmm.com (apjaworski at mmm.com)
Date: Mon, 30 Jul 2007 10:48:41 -0500
Subject: [R] beta regressions in R
In-Reply-To: <E1IEonx-0005oh-J8@elasmtp-junco.atl.sa.earthlink.net>
Message-ID: <OF7FB4B2BA.F80CC3EB-ON86257328.00569F27-86257328.0056DAED@mmm.com>

Walter,

There is a betareg package in CRAN.

Cheers,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             "Walter R.                                                    
             Paczkowski"                                                   
             <dataanalytics at ea                                          To 
             rthlink.net>              r-help at stat.math.ethz.ch            
             Sent by:                                                   cc 
             r-help-bounces at st                                             
             at.math.ethz.ch                                       Subject 
                                       [R] beta regressions in R           
                                                                           
             07/28/2007 11:09                                              
             AM                                                            
                                                                           
                                                                           
                                                                           





   Good morning,
   Does anyone know of a package or function to do a beta regression?
   Thanks,
   Walt Paczkowski

   _________________________________
   Walter R. Paczkowski, Ph.D.
   Data Analytics Corp.
   44 Hamilton Lane
   Plainsboro, NJ  08536
   (V) 609-936-8999
   (F) 609-936-3733
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Mon Jul 30 17:50:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Jul 2007 16:50:13 +0100 (BST)
Subject: [R] text() and vector arguments like adj
In-Reply-To: <E1IErpM-0003Ci-Nu@www12.emo.freenet-rz.de>
References: <E1IErpM-0003Ci-Nu@www12.emo.freenet-rz.de>
Message-ID: <Pine.LNX.4.64.0707301645300.16211@gannet.stats.ox.ac.uk>

On Sat, 28 Jul 2007, strinz at freenet.de wrote:

> Hello,
>
> I remarked that the function
> ## Default S3 method:
> text (x, y = NULL, labels = seq(along = x), adj = NULL,pos = NULL, offset = 0.5, vfont = NULL,cex = 1, col = NULL, font = NULL, ...)
>
> accepts vectors of arguments (of the same length) except for the 
> parameter adj.

Not true, e.g. for vfont.

> When passing a vector of information for adjusting the labels, only the 
> first value is taken.

You mean supplying a vector value of length > 1 for 'adj', I guess.

> Any special reason for this ?

Yes, please DO read the help page:

      adj: one or two values in [0,1] which specify the x (and
           optionally y) adjustment of the labels.  On most devices
           values outside that interval will also work.

Note, 'one or two'.

> btw: could a rotating argument like the 'srt' argument in mtext() be 
> incorporated ?

Yes, *parameter* 'srt' works exactly as in mtext: see the '...' part of 
the help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From cberry at tajo.ucsd.edu  Mon Jul 30 18:23:48 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Mon, 30 Jul 2007 09:23:48 -0700
Subject: [R] Looping through all possible combinations of cases
In-Reply-To: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>
References: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707300909180.28546@tajo.ucsd.edu>


Lots of ways. Here is a simpler one.

start by reading

 	?combn
 	?subset
 	?Subscript
 	?is.element
 	?apply
 	?paste - note the 'collapse' arg
 	?names - note the 'names(x) <- value' usage
 	?list
 	?unlist

then write a function that calc's the sum of variable given a vector of 
names,

then figure out how to use apply on the result of combn() to feed a vector 
of names to that function,

then figure out how to use paste() to turn a vector into a single string,

then figure out how to use apply()  with paste() to turn the vectors 
of names into labels (like 'a:b' ) and 'names<-' to attach them to the 
result of the earlier apply,

and finally wrap it all into a loop (for (i in 2:9) {...} saving the 
results as res[[i]] <- value at teh end of each loop.

At the end 'unlist(res)' will produce a named vector of the sums with each 
name indicating the people who contributed to it.

If you get stuck along the way report back to the list AFTER following the 
suggestions in the POSTING GUIDE mentioned at the bottom of this email.

On Fri, 27 Jul 2007, Dimitri Liakhovitski wrote:

> Hello!
>
> I have a regular data frame (DATA) with 10 people and 1 column
> ('variable'). Its cases are people with names ('a', 'b', 'c', 'd',
> 'e', 'f', etc.). I would like to write a function that would sum up
> the values on 'variable' of all possible combinations of people, i.e.
>
> 1. I would like to write a loop - in such a way that it loops through
> each possible pair of cases (i.e., ab, ac, ad, etc.) and sums up their
> respective values on 'variable'
>
> 2. I would like to write a loop - in such a way that it loops through
> each possible trio of cases (i.e., abc, abd, abe, etc.) and sums up
> their respective values on 'variable'.
>
> 3.  I would like to write a loop - in such a way that it loops through
> each possible quartet of cases (i.e., abcd, abce, abcf, etc.) and sums
> up their respective values on 'variable'.
>
> etc.
>
> Then, at the end I want to capture all possible combinations that were
> considered (i.e., what elements were combined in it) and get the value
> of the sum for each combination.
>
> How should I do it?
> Thanks a lot!
> Dimitri
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From cberry at tajo.ucsd.edu  Mon Jul 30 18:27:42 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Mon, 30 Jul 2007 09:27:42 -0700
Subject: [R] Matrix Multiplication, Floating-Point, etc.
In-Reply-To: <BAY108-F24AABC3636FC394FB1D4A8AAF30@phx.gbl>
References: <BAY108-F24AABC3636FC394FB1D4A8AAF30@phx.gbl>
Message-ID: <Pine.LNX.4.64.0707300927250.28546@tajo.ucsd.edu>



7.31 Why doesn't R think these numbers are equal?

On Fri, 27 Jul 2007, Talbot Katz wrote:

> Hi.
>
> I recently tried the following in R 2.5.1 on Windows XP:
>
>> ev2<-c(0.8,-0.6)
>> ev1<-c(0.6,0.8)
>> ev1%*%ev2
>              [,1]
> [1,] -2.664427e-17
>> sum(ev1*ev2)
> [1] 0
>>
>
> (I got the same result with R 2.4.1 on a different Windows XP machine.)
>
> I expect this issue is very familiar and probably has been discussed in this
> forum before.  Can someone please point me to some documentation or
> discussion about this?  Is there some standard way to get the "correct"
> answer from %*%?
>
> Thanks!
>
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From cberry at tajo.ucsd.edu  Mon Jul 30 18:39:29 2007
From: cberry at tajo.ucsd.edu (Charles C. Berry)
Date: Mon, 30 Jul 2007 09:39:29 -0700
Subject: [R] generating symmetric matrices
In-Reply-To: <541365.65996.qm@web63504.mail.re1.yahoo.com>
References: <541365.65996.qm@web63504.mail.re1.yahoo.com>
Message-ID: <Pine.LNX.4.64.0707300934350.28546@tajo.ucsd.edu>

On Fri, 27 Jul 2007, Gregory Gentlemen wrote:

> Greetings,
>
> I have a seemingly simple task which I have not been able to solve 
> today. I want to construct a symmetric matrix of arbtriray size w/o 
> using loops. The following I thought would do it:
>
> p <- 6
> Rmat <- diag(p)
> dat.cor <- rnorm(p*(p-1)/2)
> Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor
>
> However, the problem is that the matrix is filled by column and so the resulting matrix is not symmetric.
>
> I'd be grateful for any adive and/or solutions.

Depends on the order of elements in dat.cor. Either

 	 Rmat <- diag(p)
 	 Rmat[lower.tri(Rmat)] <- dat.cor
 	 Rmat[upper.tri(Rmat)] <- t( Rmat )[upper.tri(Rmat)]

or swap 'upper' for 'lower'.


>
> Gregory
>
>
>
>
> ---------------------------------
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

Charles C. Berry                            (858) 534-2098
                                             Dept of Family/Preventive Medicine
E mailto:cberry at tajo.ucsd.edu	            UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From researchjj at gmail.com  Mon Jul 30 18:39:45 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Tue, 31 Jul 2007 00:39:45 +0800
Subject: [R] how to install rattle() in R-GUI2.5.1
Message-ID: <b4485c4c0707300939u1f1221b5ha58aac3404749b41@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/a4689c3b/attachment.pl 

From bolker at ufl.edu  Mon Jul 30 18:42:59 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 30 Jul 2007 16:42:59 +0000 (UTC)
Subject: [R] the large dataset problem
References: <9413086.1185646053804.JavaMail.root@elwamui-royal.atl.sa.earthlink.net>
Message-ID: <loom.20070730T183450-440@post.gmane.org>

Eric Doviak <edoviak <at> earthlink.net> writes:

> 
> Dear useRs,
> 
> I recently began a job at a very large and heavily bureaucratic organization.
We're setting up a research
> office and statistical analysis will form the backbone of our work. We'll be
working with large datasets
> such the SIPP as well as our own administrative data.
  
  We need to know more about what you need to do with those
large data sets in order to help -- giving some specific
examples would be useful.  In many situations you can set up a database
connection or use Perl to select carefully and only load the
observations/variables you need into R, but it's hard to make
completely general suggestions.  

  I'm not sure what the purpose of your code to read a few
lines of a data file and write it to a CSV file is ... ?

  "Vectorizing" your code is figuring out a way to tell R
how to do what you want as a single 'vector' operation -- for
example to remove NAs from a vector you could do this:

newvec = numeric(0)
for (i in seq(along=oldvec)) {
  if (!is.na(oldvec[i])) newvec = c(newvec,oldvec[i])
}

but this would be incredibly slow --

newvec = oldvec[!is.na(oldvec)]

or

newvec = na.omit(oldvec)

would be far faster.


From researchjj at gmail.com  Mon Jul 30 18:45:00 2007
From: researchjj at gmail.com (j.joshua thomas)
Date: Tue, 31 Jul 2007 00:45:00 +0800
Subject: [R] how to install rattle() with R-GUI2.5.1 ?
Message-ID: <b4485c4c0707300945w4b43329br462e670775bafa48@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/afca7d2a/attachment.pl 

From bates at stat.wisc.edu  Mon Jul 30 18:48:18 2007
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 30 Jul 2007 11:48:18 -0500
Subject: [R] generating symmetric matrices
In-Reply-To: <155199.55586.qm@web63505.mail.re1.yahoo.com>
References: <40e66e0b0707300800w766f6eb7q36569596d1544254@mail.gmail.com>
	<155199.55586.qm@web63505.mail.re1.yahoo.com>
Message-ID: <40e66e0b0707300948y5eac6c21r51d82c08741cd113@mail.gmail.com>

On 7/30/07, Gregory Gentlemen <gregory_gentlemen at yahoo.ca> wrote:
>
>
> Douglas Bates <bates at stat.wisc.edu> wrote:
>  On 7/27/07, Gregory Gentlemen wrote:
> > Greetings,
>
> > I have a seemingly simple task which I have not been able to solve today.
> I want to construct a symmetric matrix of arbtriray size w/o using loops.
> The following I thought would do it:
>
> > p <- 6
> > Rmat <- diag(p)
> > dat.cor <- rnorm(p*(p-1)/2)
> > Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor
>
> > However, the problem is that the matrix is filled by column and so the
> resulting matrix is not symmetric.
>
> Could you provide more detail on the properties of the symmetric
> matrices that you would like to generate? It seems that you are
> trying to generate correlation matrices. Is that the case? Do you
> wish the matrices to be a random sample from a specific distribution.
> If so, what distribution?
>
> Yes, my goal is to generate correlation matrices whose entries have been
> sampled independently from a normal with a specified mean and variance.

I think that will be difficult.  For one thing correlations are
constrained to be in [-1,1].  Also, when you get into dimensions
greater than 2 the set of allowable correlation matrices is
constrained with difficult constraints.

Maybe you should reconsider what you are trying to do.


From bolker at ufl.edu  Mon Jul 30 18:50:20 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 30 Jul 2007 16:50:20 +0000 (UTC)
Subject: [R] beta regressions in R
References: <E1IEonx-0005oh-J8@elasmtp-junco.atl.sa.earthlink.net>
Message-ID: <loom.20070730T184340-403@post.gmane.org>

Walter R. Paczkowski <dataanalytics <at> earthlink.net> writes:

>    Does anyone know of a package or function to do a beta regression?

RSiteSearch("\\\"beta regression\\\"")

(all those extra \\\\s seem necessary to get the equivalent
of "beta AND regression" ...)

betareg package on CRAN [install.packages("betareg")]
http://psychology.anu.edu.au/people/smithson/details/betareg/betareg.html


From alex at transitive.com  Mon Jul 30 18:51:17 2007
From: alex at transitive.com (Alex Brown)
Date: Mon, 30 Jul 2007 17:51:17 +0100
Subject: [R] duplicate DATE at in lattice scale handled differently from
	base graphics OR lattice numeric scales
Message-ID: <E7ABF739-B796-4851-B64C-F452E3FAB6C4@transitive.com>

When at, label chains contain duplicate at values, axis ticks are  
dropped.  I believe this is handled incorrectly for Date ats in  
lattice 0.15-4 when compared to how it is handled for numeric, or for  
dates in base plot.  This can result in mis-labelled axes in some  
circumstances.  I have included executable examples.

R: 2.5.0

numeric base plot:

plot(1:10, axes=F); axis(1, at=c(2,4,4,6), labels=letters[1:4])

shows labels a, b, d

date scale in base plot:

plot((as.Date("2007-01-01") + 1:10), 1:10, axes=F); axis(1, at= 
(as.Date("2007-01-01")+c(2,4,4,6)), labels=letters[1:4])

shows labels a, b, d

numeric scale in lattice plot

xyplot(1:10 ~ 1:10, scales=list(x=list(relation="free", at=c 
(2,4,4,6), labels=list(letters[1:4]))))

shows labels a, b, d

date scale in lattice plot

xyplot(1:10 ~ (as.Date("2007-01-01") + 1:10) , scales=list(x=list 
(relation="free", at=list(as.Date("2007-01-01")+c(2,4,4,6)),  
labels=list(letters[1:4]))))

shows labels a, b, c

Since this results in label c being placed at position 6, not 4, I  
feel this is a bug.

-Alex Brown


From marcel.austenfeld at uni-bielefeld.de  Mon Jul 30 18:53:58 2007
From: marcel.austenfeld at uni-bielefeld.de (Bio7)
Date: Mon, 30 Jul 2007 09:53:58 -0700 (PDT)
Subject: [R] Release of an Ecological Modelling Software using R (with
 Rserve) and ImageJ
Message-ID: <11866782.post@talk.nabble.com>


Dear R users,

With the hope that this application can also be useful for some R users,
I like to announce the first release of Bio7. This application  is an
Integrated Development Environment for ecological modelling with a main
focus on individual based modelling and spatially explicit models based on a
Rich Client Platform (Eclipse).

Bio7 embedds several tools for:

- Creation and analysis of spatial explicit simulation models.
- Statistical analysis (R).
- Spatial statistics (possibility to send values from a specialized panel to
R).
- Image Analysis (embedded ImageJ version 1.35s).
- Fast communication between R and Java (with Rserve) and
  the possibilty to use R methods inside Java.
- Interpretation of Java and script creation (with BeanShell).
- Direct compilation of Java (Janino).
- Creation of methods for Java, BeanShell and R
  (integrated editors for Java, R, BeanShell). 
- Sensitivity analysis with an embedded flowchart editor in which
  scripts, macros and compiled code can be dragged and executed.

The application itself is an Open Source application and is licensed under
the GPL and the EPL
(Eclipse Puplic License).
A complete manual, lot of examples, Flash tutorials and a small Java API is
also available.

A Linux version (beta) will be available soon!

Please follow:

http://www.uni-bielefeld.de/biologie/Oekosystembiologie/bio7app/


With kind regards 

M. Austenfeld
-- 
View this message in context: http://www.nabble.com/Release-of-an-Ecological-Modelling-Software-using-R-%28with-Rserve%29-and-ImageJ-tf4171202.html#a11866782
Sent from the R help mailing list archive at Nabble.com.


From andy_liaw at merck.com  Mon Jul 30 18:57:47 2007
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Jul 2007 12:57:47 -0400
Subject: [R] getting the name of variables passed to a function
In-Reply-To: <46A9C046020000650000777F@pgn.com>
References: <46A9C046020000650000777F@pgn.com>
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA047EDFF4@usctmx1106.merck.com>

Here's one possibility:

R> f <- function(...) { call <- match.call(); sapply(as.list(call[-1]),
deparse) }
R> f(x, y)
[1] "x" "y"
R> f(x=x, y=y)
  x   y 
"x" "y" 

You basically need to know how to manipulate call objects.  The relevant
section in the R Language Definition should help.

Andy

 
From: Horace Tso
> 
> Folks,
> 
> I've entered into an R programming territory I'm not very 
> familiar with, thus this probably very elementary question 
> concerning the mechanic of a function call.
> 
> I want to know from within a function the name of the 
> variables I pass down. The function makes use of the "..." to 
> allow for multiple unknown arguments,
> 
> myfun = function(...) { do something }
> 
> In the body I put,
> 
> {    
> nm <- names(list(...))
> nm
> }
> 
> When the function is called with two vectors x, and y
> 
> myfun(x, y)
> 
> It returns NULL. However, when the call made is,
> 
> >myfun(x=x, y=y)
> 
> The result is
> [1] "x" "y"
> 
> Question : how do i get the names of the unknown variables 
> without explicitly saying x=x...
> 
> Thanks in advance.
> 
> Horace
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From dae-jin.lee at uc3m.es  Mon Jul 30 18:59:44 2007
From: dae-jin.lee at uc3m.es (Dae-Jin Lee)
Date: Mon, 30 Jul 2007 18:59:44 +0200
Subject: [R] stop criteria when "L-BFGS-B needs finite values of 'fn' " in
	optim
Message-ID: <cbca975a0707300959m65e5e54ife0d77cb148a5b9f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/e00ffa73/attachment.pl 

From patrick at pdrechsler.de  Mon Jul 30 19:07:51 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Mon, 30 Jul 2007 19:07:51 +0200
Subject: [R] lattice grayscale "theme"
References: <87lkd08agu.fsf@pdrechsler.de>
Message-ID: <87abtdyg48.fsf@pdrechsler.de>

Patrick Drechsler <patrick at pdrechsler.de> writes:

> is there a grayscale setting for lattice plots?

Solved using

  ## Set background color of strips to grayscales:
  strip.background <- trellis.par.get("strip.background")
  trellis.par.set(strip.background = list(col = grey(7:1/8)))
  ## Set color of plot symbols to grayscale:
  plot.symbol <- trellis.par.get("plot.symbol")
  trellis.par.set(plot.symbol = list(col = grey(5/8)))

Patrick


From gunter.berton at gene.com  Mon Jul 30 19:12:14 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 30 Jul 2007 10:12:14 -0700
Subject: [R] Slightly OT - use of R
In-Reply-To: <200707300928.15405.j.logsdon@quantex-research.com>
Message-ID: <002a01c7d2cc$c6a59450$4d908980@gne.windows.gene.com>

Why? You might receive more useful replies from a relevant subset of users
if you specify the purpose you have in mind.


Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Logsdon
Sent: Monday, July 30, 2007 1:28 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Slightly OT - use of R

I am trying to get a measure of how R compares in usage as a statistical 
platform compared to other software.  I would guess it is the most widely 
used among statisticians at least by virtue of it being open source.

But is there any study to which I can refer?  By asking this list I am not 
exactly adopting a rigorous approach!  

Best wishes

John

John Logsdon                               "Try to make things as simple
Quantex Research Ltd, Manchester UK         as possible but not simpler"
j.logsdon at quantex-research.com              a.einstein at relativity.org
+44(0)161 445 4951/G:+44(0)7717758675       www.quantex-research.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ted.harding at nessie.mcc.ac.uk  Mon Jul 30 19:24:45 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 Jul 2007 18:24:45 +0100 (BST)
Subject: [R] the large dataset problem
In-Reply-To: <25934031.1185795647582.JavaMail.root@elwamui-mouette.atl.sa.earthlink.net>
Message-ID: <XFMail.070730182445.ted.harding@nessie.mcc.ac.uk>

On 30-Jul-07 11:40:47, Eric Doviak wrote:
> [...]

Sympathies for the constraints you are operating in!

> The "Introduction to R" manual suggests modifying input files with
> Perl. Any tips on how to get started? Would Perl Data Language (PDL) be
> a good choice?  http://pdl.perl.org/index_en.html

I've not used SIPP files, but itseems that they are available in
"delimited" format, including CSV.

For extracting a subset of fields (especially when large datasets may
stretch RAM resources) I would use awk rather than perl, since it
is a much lighter program, transparent to code for, efficient, and
it will do that job.

On a Linux/Unix system (see below), say I wanted to extract fields
1, 1000, 1275, .... , 5678 from a CSV file. Then the 'awk' line
that would do it would look like

awk '
 BEGIN{FS=","}{print $(1) "," $(1000) "," $(1275) "," ... $(5678)
' < sippfile.csv > newdata.csv

Awk reads one line at a tine, and does with it what you tell it to do.
It will not be overcome by a file with an enormous number of lines.
Perl would be similar. So long as one line fits comfortably into RAM,
you would not be limited by file size (unless you're running out
of disk space), and operation will be quick, even for very long
lines (as an experiment, I just set up a file with 10,000 fields
and 35 lines; awk output 6 selected fields from all 35 lines in
about 1 second, on the 366MHz 128MB RAM machine I'm on at the
moment. After transferring it to a 733MHz 512MB RAM machine, it was
too quick to estimate; so I duplicated the lines to get a 363-line
file, and now got those same fields out in a bit less than 1 second.
So that's over 300 lines/second, 200,000 lines a minute, a million
lines in 5 minutes; and all on rather puny hardware.).

In practice, you might want to write a separate script which woould
automatically create the necessary awk script (say if you supply
the filed names, haing already coded the filed positions corresponding
to filed names). You could exploit R's system() command to run the
scripts from within R, and then load in the filtered data.

> I wrote a script which loads large datasets a few lines at a time,
> writes the dozen or so variables of interest to a CSV file, removes
> the loaded data and then (via a "for" loop) loads the next few lines
> .... I managed to get it to work with one of the SIPP core files,
> but it's SLOOOOW.

See above ...

> Worse, if I discover later that I omitted a relevant variable,
> then I'll have to run the whole script all over again.

If the script worked quickly (as with awk), presumably you
wouldn't mind so much?

Regarding Linux/Unix versus Windows. It is general experience
that Linux/Unix works faster, more cleanly and efficiently, and
often more reliably, for similar tasks; and cam do so on low grade
hardware. Also, these systems come with dozens of file-processing
utilities (including perl and awk; also many others), each of which
has been written to be efficient at precisely the repertoire of
tasks it was designed for. A lot of Windows sotware carries a huge
overhead of either cosmetic dross, or a pantechnicon of functionality
of which you are only going to need 0.01% at any one time.

The Unix utilities have been ported to Windows, long since, but
I have no experience of using them in that environment. Others,
who have, can advise! But I'd seriously suggest getting hold of them.

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-Jul-07                                       Time: 18:24:41
------------------------------ XFMail ------------------------------


From gunter.berton at gene.com  Mon Jul 30 19:27:38 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 30 Jul 2007 10:27:38 -0700
Subject: [R] generating symmetric matrices
In-Reply-To: <541365.65996.qm@web63504.mail.re1.yahoo.com>
Message-ID: <003401c7d2ce$ed481a90$4d908980@gne.windows.gene.com>

See ?dist for an object oriented approach that may be better.

Directly, you can do something like (see  ?row ?col):

x <- matrix(NA, 10,10)
## Lower triangular :
x[row(x) >= col(x) ] <- rnorm(55) 
x[row(x) < col(x)] <- x[row(x) > col(x)]
## or you could have saved the random vector and re-used it.


Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gregory Gentlemen
Sent: Friday, July 27, 2007 8:28 PM
To: r-help at stat.math.ethz.ch
Subject: [R] generating symmetric matrices

Greetings,

I have a seemingly simple task which I have not been able to solve today. I
want to construct a symmetric matrix of arbtriray size w/o using loops. The
following I thought would do it:

p <- 6
Rmat <- diag(p)
dat.cor <- rnorm(p*(p-1)/2)
Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor

However, the problem is that the matrix is filled by column and so the
resulting matrix is not symmetric.

I'd be grateful for any adive and/or solutions.

Gregory 

       
 
              
---------------------------------
    
       


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From topkatz at msn.com  Mon Jul 30 19:54:54 2007
From: topkatz at msn.com (Talbot Katz)
Date: Mon, 30 Jul 2007 13:54:54 -0400
Subject: [R] Matrix Multiplication, Floating-Point, etc.
In-Reply-To: <Pine.LNX.4.64.0707300927250.28546@tajo.ucsd.edu>
Message-ID: <BAY108-F372CBDB1B7D1A023C189ACAAEE0@phx.gbl>

Thank you for responding!

I realize that floating point operations are often inexact, and indeed, the 
difference between the two answers is within the all.equal tolerance, as 
mentioned in FAQ 7.31 (cited by Charles):

>(as.numeric(ev1%*%ev2))==(sum(ev1*ev2))
[1] FALSE
>all.equal((as.numeric(ev1%*%ev2)),(sum(ev1*ev2)))
[1] TRUE
>

I suppose that's good enough for numerical computation.  But I was still 
surprised to see that matrix multiplication (ev1%*%ev2) doesn't give the 
exact right answer, whereas sum(ev1*ev2) does give the exact answer.  I 
would've expected them to perform the same two multiplications and one 
addition.  But I guess that's not the case.

However, I did find that if I multiplied the two vectors by 10, making the 
entries integers (although the class was still "numeric" rather than 
"integer"), both computations gave equal answers of 0:

>xf1<-10*ev1
>xf2<-10*ev2
>(as.numeric(xf1%*%xf2))==(sum(xf1*xf2))
[1] TRUE
>

Perhaps the moral of the story is that one should exercise caution and keep 
track of significant digits.

--  TMK  --
212-460-5430	home
917-656-5351	cell



>From: "Charles C. Berry" <cberry at tajo.ucsd.edu>
>To: Talbot Katz <topkatz at msn.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] Matrix Multiplication, Floating-Point, etc.
>Date: Mon, 30 Jul 2007 09:27:42 -0700
>
>
>
>7.31 Why doesn't R think these numbers are equal?
>
>On Fri, 27 Jul 2007, Talbot Katz wrote:
>
>>Hi.
>>
>>I recently tried the following in R 2.5.1 on Windows XP:
>>
>>>ev2<-c(0.8,-0.6)
>>>ev1<-c(0.6,0.8)
>>>ev1%*%ev2
>>              [,1]
>>[1,] -2.664427e-17
>>>sum(ev1*ev2)
>>[1] 0
>>>
>>
>>(I got the same result with R 2.4.1 on a different Windows XP machine.)
>>
>>I expect this issue is very familiar and probably has been discussed in 
>>this
>>forum before.  Can someone please point me to some documentation or
>>discussion about this?  Is there some standard way to get the "correct"
>>answer from %*%?
>>
>>Thanks!
>>
>>--  TMK  --
>>212-460-5430	home
>>917-656-5351	cell
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>>
>
>Charles C. Berry                            (858) 534-2098
>                                             Dept of Family/Preventive 
>Medicine
>E mailto:cberry at tajo.ucsd.edu	            UC San Diego
>http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901
>
>


From HDoran at air.org  Mon Jul 30 19:58:59 2007
From: HDoran at air.org (Doran, Harold)
Date: Mon, 30 Jul 2007 13:58:59 -0400
Subject: [R] Matrix Multiplication, Floating-Point, etc.
In-Reply-To: <BAY108-F372CBDB1B7D1A023C189ACAAEE0@phx.gbl>
Message-ID: <2323A6D37908A847A7C32F1E3662C80EE5812F@dc1ex01.air.org>

Talbot

The general advice on this list is to read the following

http://docs.sun.com/source/806-3568/ncg_goldberg.html

 

> -----Original Message-----
> From: Talbot Katz [mailto:topkatz at msn.com] 
> Sent: Monday, July 30, 2007 1:55 PM
> To: cberry at tajo.ucsd.edu
> Cc: r-help at stat.math.ethz.ch; Doran, Harold
> Subject: Re: [R] Matrix Multiplication, Floating-Point, etc.
> 
> Thank you for responding!
> 
> I realize that floating point operations are often inexact, 
> and indeed, the difference between the two answers is within 
> the all.equal tolerance, as mentioned in FAQ 7.31 (cited by Charles):
> 
> >(as.numeric(ev1%*%ev2))==(sum(ev1*ev2))
> [1] FALSE
> >all.equal((as.numeric(ev1%*%ev2)),(sum(ev1*ev2)))
> [1] TRUE
> >
> 
> I suppose that's good enough for numerical computation.  But 
> I was still surprised to see that matrix multiplication 
> (ev1%*%ev2) doesn't give the exact right answer, whereas 
> sum(ev1*ev2) does give the exact answer.  I would've expected 
> them to perform the same two multiplications and one 
> addition.  But I guess that's not the case.
> 
> However, I did find that if I multiplied the two vectors by 
> 10, making the entries integers (although the class was still 
> "numeric" rather than "integer"), both computations gave 
> equal answers of 0:
> 
> >xf1<-10*ev1
> >xf2<-10*ev2
> >(as.numeric(xf1%*%xf2))==(sum(xf1*xf2))
> [1] TRUE
> >
> 
> Perhaps the moral of the story is that one should exercise 
> caution and keep track of significant digits.
> 
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
> 
> 
> 
> >From: "Charles C. Berry" <cberry at tajo.ucsd.edu>
> >To: Talbot Katz <topkatz at msn.com>
> >CC: r-help at stat.math.ethz.ch
> >Subject: Re: [R] Matrix Multiplication, Floating-Point, etc.
> >Date: Mon, 30 Jul 2007 09:27:42 -0700
> >
> >
> >
> >7.31 Why doesn't R think these numbers are equal?
> >
> >On Fri, 27 Jul 2007, Talbot Katz wrote:
> >
> >>Hi.
> >>
> >>I recently tried the following in R 2.5.1 on Windows XP:
> >>
> >>>ev2<-c(0.8,-0.6)
> >>>ev1<-c(0.6,0.8)
> >>>ev1%*%ev2
> >>              [,1]
> >>[1,] -2.664427e-17
> >>>sum(ev1*ev2)
> >>[1] 0
> >>>
> >>
> >>(I got the same result with R 2.4.1 on a different Windows XP 
> >>machine.)
> >>
> >>I expect this issue is very familiar and probably has been 
> discussed 
> >>in this forum before.  Can someone please point me to some 
> >>documentation or discussion about this?  Is there some 
> standard way to 
> >>get the "correct"
> >>answer from %*%?
> >>
> >>Thanks!
> >>
> >>--  TMK  --
> >>212-460-5430	home
> >>917-656-5351	cell
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >Charles C. Berry                            (858) 534-2098
> >                                             Dept of 
> Family/Preventive 
> >Medicine
> >E mailto:cberry at tajo.ucsd.edu	            UC San Diego
> >http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 
> >92093-0901
> >
> >
> 
> 
>


From hellokisas at gmail.com  Mon Jul 30 20:25:16 2007
From: hellokisas at gmail.com (Kisas)
Date: Mon, 30 Jul 2007 14:25:16 -0400
Subject: [R] Call R program from C++ code
In-Reply-To: <11860280.post@talk.nabble.com>
References: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
	<11860280.post@talk.nabble.com>
Message-ID: <002b01c7d2d6$fa965920$efc30b60$@com>

Hi Vladimir:
	Thank you very much! I'm very interested in this sentence: " R can
be compiled as a shared library object, that you can dynamically load from
your application and use its functions." That's would be great if R programs
can be "compiled" as shared library object(.lib? or .dll ? ) Can you please
give more details about that? 

Best,
Feng

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vladimir Eremeev
Sent: 2007??7??30?? 5:16
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Call R program from C++ code


"Writing R Extensions" manual contains chapters dedicated to parsing and
evaluating of the R extensions from C.
Also, I vaguely remember I've seen something like "Embedding R" somewhere in
manuals.
R can be compiled as a shared library object, that you can dynamically load
from your application and use its functions.
R doesn't have a compiler, it's the interpreted language. 
However, it can parse a character string representing an expresstion and
transform it into the internal form, ready for evaluation.


Feng Qiu wrote:
> 
> Hi All:
> 
>                I'm developing an application program using C++. From my
> C++
> code, I would call some R program I have written. I' wondering if R
> provide
> some compiler that can compile R program into executable program. I
> searched
> R-help, there are a lot of posts talking about writing C++ code in R
> program, but few about calling R from C++. 
> 
>                I might be wrong that R doesn't have complier. What I'm
> trying to do is to call R program from C++ code. Any help is highly
> appreciated!
> 

-- 
View this message in context:
http://www.nabble.com/Call-R-program-from-C%2B%2B-code-tf4167083.html#a11860
280
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ted.harding at nessie.mcc.ac.uk  Mon Jul 30 20:27:54 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 Jul 2007 19:27:54 +0100 (BST)
Subject: [R] 2nd R Console
In-Reply-To: <001a01c7d0ef$fb0d9140$13321b82@heron>
Message-ID: <XFMail.070730192754.ted.harding@nessie.mcc.ac.uk>

On 28-Jul-07 08:19:10, Michael Janis wrote:
> Hi,
> 
> I was reading a thread: [R] "2nd R console" and had a similar
> question regarding having more than one R console open at a time. 
> However, my question differs from that of the thread:
> 
> Is it possible, or is there a wrapper that will allow one, to open
> an arbitrary number of R consoles which access the same R session
> (all objects in that session, etc.).  This would be R on linux
> accessed through a shell - kind of like using GNU screen multi-user
> such that people could work collaboratively on a given session.
> The problem with screen is that all commands are interleaved in
> the same terminal, which is confusing and does not allow access
> to the command prompt at the same time, rather it would be sequential.
> I know there will be "why" questions but it is useful in an
> academic environment.  Basically we have a memory machine for large
> genomic analysis - and we could set that up as an Rserver, but this
> placing R into a multi-user engine is better suited for our immediate
> needs.  Does anybody have thoughts on this?

You could have a look at XMX (X-protocol Multiplexer). This is now
rather old, and I think it has not been further developed for many
years.

http://www.cs.brown.edu/software/xmx/README.patch7

Basically, you would start XMX from one machine (A) networked
to others (B,C,...), all running X-windows. in the startup, you
designate which machines are to share the session, and what rights
they will have.

On machine A, and all designated machines B, C, ... , appears
a window. This in fact acts like a screen emulator, with its
own X session inside it. The A user then starts up a program
(say R) in this window, and what A sees is mirrored on the other
machines.

If A has granted input privileges to the other machines, then
users of B, C, ... can do things themselves, and the effects of
their actions are mirrored to all the other machines.

Thus, for instance, it would be possible for different users to
take turns at entering commands into the R session, and so on,
from their own machines. This is a much better way of arranging
things, than having all the people queue up to take turns at
sitting in the one and only chair!

It is certainly useful in a classroom situation, and even in
a one-to-one tutorial. For instance, I've used it in the past
to teach programming and system management, sitting more or
less beside the other person but at different machines. We
would both, for instance, be editing the same program file,
and either could initiate a program run with the current file.
(Of course, what's technically called a "race condition" can
develop here ... ). I've even envisaged the scenario where two
people, one in England and one in the US, could simultaneously
be editing a joint paper (you'd also need an independent
communication channel as well, but that's no problem using a
"chat" program).

The one constraint with XMX (at least in the past, I'm not
sure to what extent it may have been relaxed) which can limit
its use is that it depends on fairly close compatibility
between the X resources of all the different machines. It's
best of they're identical (same screen resolution e.g. 1024x768,
same colour depths on all screens, ... ). Otherwise it's liable
to not establish the necessary connections, and only some
machines can join in.

However, in a computing lab environment, it may well be that
all machines are compatible.

Suck it and see!

Hoping this is useful,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-Jul-07                                       Time: 19:27:48
------------------------------ XFMail ------------------------------


From D.GOUACHE at arvalisinstitutduvegetal.fr  Mon Jul 30 15:39:02 2007
From: D.GOUACHE at arvalisinstitutduvegetal.fr (GOUACHE David)
Date: Mon, 30 Jul 2007 15:39:02 +0200
Subject: [R] RE :  regular expressions : extracting numbers
In-Reply-To: <971536df0707300604v5a3dea26sb628035ac20c82f2@mail.gmail.com>
Message-ID: <1DF7DB4AB44EFB41A60A889186D43359120A36@srv-laminiere.arvalis-fr.com>

and I was afraid everyone would be on holidays...
thanks to all 7 of you who answered !!

David Gouache
Arvalis - Institut du V?g?tal
Station de La Mini?re
78280 Guyancourt
Tel: 01.30.12.96.22 / Port: 06.86.08.94.32


-----Message d'origine-----
De?: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Envoy??: lundi 30 juillet 2007 15:05
??: GOUACHE David
Cc?: r-help at stat.math.ethz.ch
Objet?: Re: [R] regular expressions : extracting numbers

I assume if you want the "" components to be NA then you really intend
the result to be a numeric vector.  The following replaces all non-digits
with "" (thereby removing them) and then uses as.numeric to convert the
result to numeric.  Just omit the conversion if you want a character
vector result:

s <- c("lema, rb 2%", "rb 2%", "rb 3%", "rb 4%", "rb 3%", "rb 2%,mineuse",
   "rb", "rb", "rb 12", "rb", "rj 30%", "rb", "rb", "rb 25%", "rb", "rb",
   "rb", "rj, rb")

as.numeric(gsub("[^[:digit:]]+", "", s))

On 7/30/07, GOUACHE David <D.GOUACHE at arvalisinstitutduvegetal.fr> wrote:
> Hello all,
>
> I have a vector of character strings, in which I have letters, numbers, and symbols. What I wish to do is obtain a vector of the same length with just the numbers.
> A quick example -
>
> extract of the original vector :
> "lema, rb 2%" "rb 2%" "rb 3%" "rb 4%" "rb 3%" "rb 2%,mineuse" "rb" "rb" "rb 12" "rb" "rj 30%" "rb" "rb" "rb 25%" "rb" "rb" "rb" "rj, rb"
>
> and the type of thing I wish to end up with :
> "2" "2" "3" "4" "3" "2" "" "" "12" "" "30" "" "" "25" "" "" "" ""
>
> or, instead of "", NA would be acceptable (actually it would almost be better for me)
>
> Anyways, I've been battling with gsub() and things of the sort, but I'm drowning in the regular expressions, despite a few hours of looking at Perl tutorials...
> So if anyone can help me out, it would be greatly appreciated!!
>
> In advance, thanks very much.
>
> David Gouache
> Arvalis - Institut du V?g?tal
> Station de La Mini?re
> 78280 Guyancourt
> Tel: 01.30.12.96.22 / Port: 06.86.08.94.32
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From klebyn at yahoo.com.br  Mon Jul 30 20:46:04 2007
From: klebyn at yahoo.com.br (Cleber Borges)
Date: Mon, 30 Jul 2007 15:46:04 -0300
Subject: [R] generating symmetric matrices
In-Reply-To: <541365.65996.qm@web63504.mail.re1.yahoo.com>
References: <541365.65996.qm@web63504.mail.re1.yahoo.com>
Message-ID: <46AE31EC.2010809@yahoo.com.br>


hello


try

?upper.tri

############# example

 p <- 6
 Rmat <- diag(p)
 dat.cor <- rnorm(p*(p-1)/2)
Rmat[upper.tri(Rmat)]<- dat.cor
 Rmat[lower.tri(Rmat)]<- dat.cor


Cleber Borges



> Greetings,
>
> I have a seemingly simple task which I have not been able to solve today. I want to construct a symmetric matrix of arbtriray size w/o using loops. The following I thought would do it:
>
> p <- 6
> Rmat <- diag(p)
> dat.cor <- rnorm(p*(p-1)/2)
> Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor
>
> However, the problem is that the matrix is filled by column and so the resulting matrix is not symmetric.
>
> I'd be grateful for any adive and/or solutions.
>
> Gregory 
>
>   


	

	
		
_______________________________________________________ 

Experimente j? e veja as novidades.


From ted.harding at nessie.mcc.ac.uk  Mon Jul 30 21:01:00 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 Jul 2007 20:01:00 +0100 (BST)
Subject: [R] generating symmetric matrices
In-Reply-To: <541365.65996.qm@web63504.mail.re1.yahoo.com>
Message-ID: <XFMail.070730200100.ted.harding@nessie.mcc.ac.uk>

On 28-Jul-07 03:28:25, Gregory Gentlemen wrote:
> Greetings,
> 
> I have a seemingly simple task which I have not been able to solve
> today. I want to construct a symmetric matrix of arbtriray size w/o
> using loops. The following I thought would do it:
> 
> p <- 6
> Rmat <- diag(p)
> dat.cor <- rnorm(p*(p-1)/2)
> Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor
> 
> However, the problem is that the matrix is filled by column and so the
> resulting matrix is not symmetric.
> 
> I'd be grateful for any adive and/or solutions.
> 
> Gregory 

Would the fact that A + t(A) is symmetric be useful here?

E.g.

p <- 6
A <- matrix(rnorm(p^2),ncol=p)
A <- (A + t(A))/sqrt(2)
diag(A) <- rep(1,p)
round(A,digits=2)
      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]
[1,]  1.00  0.53 -0.20  1.27  0.34  0.83
[2,]  0.53  1.00 -0.99 -0.72  0.68 -1.21
[3,] -0.20 -0.99  1.00 -0.62 -0.36 -0.87
[4,]  1.27 -0.72 -0.62  1.00  2.40  0.33
[5,]  0.34  0.68 -0.36  2.40  1.00  0.20
[6,]  0.83 -1.21 -0.87  0.33  0.20  1.00

(Here, because each off-diagonal element of A is the sum of
2 independent N(0,1)s, divided by sqrt(2), the result is
also N(0,1)).

However, whether this is reallyu seful for you depends on
what you want the elements of A to be!

Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-Jul-07                                       Time: 20:00:55
------------------------------ XFMail ------------------------------


From edna.bell01 at gmail.com  Mon Jul 30 21:08:04 2007
From: edna.bell01 at gmail.com (Edna Bell)
Date: Mon, 30 Jul 2007 14:08:04 -0500
Subject: [R]  zoo, ts and data.frames
Message-ID: <2d1ebb110707301208y399e3a14ga4f208ed853a55ba@mail.gmail.com>

Hi R gurus:

I have some zoo objects that I have put into a data.frame.

However, when I try to plot the objects from the data frame, the x
axis is now Index rather than time

Is there a sort of zoo data frame or a multiple zoo object, please?

Thanks for any help!

Sincerely,
Edna
edna.bell01 at gmail.com


From tobias.verbeke at gmail.com  Mon Jul 30 21:08:59 2007
From: tobias.verbeke at gmail.com (Tobias Verbeke)
Date: Mon, 30 Jul 2007 21:08:59 +0200
Subject: [R] beta regressions in R
In-Reply-To: <E1IEonx-0005oh-J8@elasmtp-junco.atl.sa.earthlink.net>
References: <E1IEonx-0005oh-J8@elasmtp-junco.atl.sa.earthlink.net>
Message-ID: <46AE374B.1020700@businessdecision.com>

Walter R. Paczkowski wrote:

>    Good morning,
>    Does anyone know of a package or function to do a beta regression?

http://cran.r-project.org/src/contrib/Descriptions/betareg.html

HTH,
Tobias

-- 

Tobias Verbeke - Consultant
Business & Decision Benelux
Rue de la r?volution 8
1000 Brussels - BELGIUM

+32 499 36 33 15
tobias.verbeke at businessdecision.com


From jholtman at gmail.com  Mon Jul 30 21:14:21 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 15:14:21 -0400
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46ADD4F1.8090901@yahoo.de>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
	<46ADD4F1.8090901@yahoo.de>
Message-ID: <644e1f320707301214y437b31dfyaf241cafba9a32de@mail.gmail.com>

This should do it:

> v1 <- NA
> v2 <- rnorm(6)
> v3 <- rnorm(6)
> v4 <- rnorm(6)
> v5 <- rnorm(6)
> v6 <- rnorm(6)
> v7 <- rnorm(6)
> v8 <- rnorm(6)
> v8 <- NA
>
> list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
> categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)
>
> # create partitioned list
> list.cat <- split(list, categ)
> # combine each partition into a matrix
> list.mat <- lapply(list.cat, function(x) do.call('rbind', x))
> # now take the means of each column
> lapply(list.mat, colMeans)
$cat1
[1] -0.5699080  0.3855693  1.1051809  0.2379324  0.6684713  0.3240003

$cat2
[1]  0.38160462 -0.10559496 -0.40963090 -0.09507354  0.95021406 -0.31491450

>


On 7/30/07, Antje <niederlein-rstat at yahoo.de> wrote:
> okay, I played a bit around and now I have some kind of testcase for you:
>
> v1 <- NA
> v2 <- rnorm(6)
> v3 <- rnorm(6)
> v4 <- rnorm(6)
> v5 <- rnorm(6)
> v6 <- rnorm(6)
> v7 <- rnorm(6)
> v8 <- rnorm(6)
> v8 <- NA
>
> list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
> categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)
>
>  > list
> [[1]]
> [1] NA
>
> [[2]]
> [1] -0.6442149 -0.2047012 -1.1986041 -0.2097442 -0.7343465 -1.3888750
>
> [[3]]
> [1]  0.02354036 -1.36186952 -0.42197792  1.50445971 -1.76763996  0.53722404
>
> [[4]]
> [1] -1.40362589  0.13045724 -0.84651458  1.57005071  0.06961015  0.25269771
>
> [[5]]
> [1] -1.1829260  2.1411553 -0.1327081 -0.1053442 -0.8179396 -1.2342698
>
> [[6]]
> [1]  1.17099178  0.49248118 -0.18690065  1.50050976 -0.65552410 -0.01243247
>
> [[7]]
> [1] -0.046778203 -0.233788840  0.443908897 -1.649740180  0.003991354 -0.228020092
>
> [[8]]
> [1] NA
>
> now, I need the means (and sd) of element 1 of list[2],list[3],list[4] (because they belong to "cat1") and
>
> = mean(-0.6442149, 0.02354036, -1.40362589)
>
> the same for element 2 up to element 6 (--> I would the get a vector containing the means for "cat1")
> the same for the vectors belonging to "cat2".
>
> does anybody now understand what I mean?
>
> Antje
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From edna.bell01 at gmail.com  Mon Jul 30 21:17:17 2007
From: edna.bell01 at gmail.com (Edna Bell)
Date: Mon, 30 Jul 2007 14:17:17 -0500
Subject: [R]  zoo, ts and data frames - SOLVED
Message-ID: <2d1ebb110707301217w5b2e5c3x8b48afe393a8e849@mail.gmail.com>

Sorry...there is a merge command that solves this.


From roland.rproject at gmail.com  Mon Jul 30 21:34:30 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Mon, 30 Jul 2007 15:34:30 -0400
Subject: [R] the large dataset problem
In-Reply-To: <25934031.1185795647582.JavaMail.root@elwamui-mouette.atl.sa.earthlink.net>
References: <25934031.1185795647582.JavaMail.root@elwamui-mouette.atl.sa.earthlink.net>
Message-ID: <46AE3D46.5020409@gmail.com>

Eric Doviak wrote:
> 
> I need to find some way to overcome these constraints and work with large datasets. Does anyone have any suggestions?
I might be not the most authoritative person on this subject but I put 
all my large datasets[1] into an SQLite database and extract/summarize 
data from it with R using the RSQLite package. If your data come in 
ASCII format, it is rather easy to read them into an SQLite DB.

> 
> I've read that I should "carefully vectorize my code." What does that mean ??? !!!
The book "S Programming" by Venables & Ripley has a sub-chapter on this.
If you happen to have John Chamber's "Programming with Data" book, there 
are a few pages on "The Whole-Object View".

> 
> I wrote a script which loads large datasets a few lines at a time, writes the dozen or so variables of interest to a CSV file, removes the loaded data and then (via a "for" loop) loads the next few lines .... I managed to get it to work with one of the SIPP core files, but it's SLOOOOW. Worse, if I discover later that I omitted a relevant variable, then I'll have to run the whole script all over again.
> 
That means you have huge datasets but you never need the whole dataset? 
Just a selected number of variables and then the files are of managable 
size?
If this is the case, using RSQLite (or any other DB package, also RODBC 
is very easy to use, if you have, for example, an MS Access DB) is a 
good option. Alternatively, are you familiar with some old-fashioned 
Unix-Tools? Ports for MS Windows also exist and the program 'cut' could 
help you considerably.


Please note:
- I am only a causal user of the DB interfaces. So there might be better 
solutions and people with more detailed knowledge about it.
- All the tools I mentioned here are licensed under the same or similar 
free software licenses as R, so you should have no problems 
obtaining/installing them.
- A good source of information is the R Data Import/Export Manual -- 
shipped with every R distribution and available online at 
http://cran.at.r-project.org/doc/manuals/R-data.html

I hope this helps,
Roland


[1] The largest one is 1GB -- so probably not really large.


From jholtman at gmail.com  Mon Jul 30 21:36:59 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 15:36:59 -0400
Subject: [R] Looping through all possible combinations of cases
In-Reply-To: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>
References: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>
Message-ID: <644e1f320707301236l1edc8cdcs2a6aafcace2f0924@mail.gmail.com>

Here is how to do it for 2; you can extend it:

> # test data
> n <- 100
> x <- data.frame(id=sample(letters[1:4], n, TRUE), values=runif(n))
> # get combinations of 2 at a time
> comb.2 <- combn(unique(as.character(x$id)), 2)
> for (i in 1:ncol(comb.2)){
+     cat(sprintf("%s:%s %f\n",comb.2[1,i], comb.2[2,i],
+         sum(x$value[x$id %in% comb.2[,i]])))
+ }
c:d 25.259988
c:b 21.268737
c:a 21.250933
d:b 26.013253
d:a 25.995450
b:a 22.004198


On 7/27/07, Dimitri Liakhovitski <ld7631 at gmail.com> wrote:
> Hello!
>
> I have a regular data frame (DATA) with 10 people and 1 column
> ('variable'). Its cases are people with names ('a', 'b', 'c', 'd',
> 'e', 'f', etc.). I would like to write a function that would sum up
> the values on 'variable' of all possible combinations of people, i.e.
>
> 1. I would like to write a loop - in such a way that it loops through
> each possible pair of cases (i.e., ab, ac, ad, etc.) and sums up their
> respective values on 'variable'
>
> 2. I would like to write a loop - in such a way that it loops through
> each possible trio of cases (i.e., abc, abd, abe, etc.) and sums up
> their respective values on 'variable'.
>
> 3.  I would like to write a loop - in such a way that it loops through
> each possible quartet of cases (i.e., abcd, abce, abcf, etc.) and sums
> up their respective values on 'variable'.
>
> etc.
>
> Then, at the end I want to capture all possible combinations that were
> considered (i.e., what elements were combined in it) and get the value
> of the sum for each combination.
>
> How should I do it?
> Thanks a lot!
> Dimitri
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From roland.rproject at gmail.com  Mon Jul 30 21:52:49 2007
From: roland.rproject at gmail.com (Roland Rau)
Date: Mon, 30 Jul 2007 15:52:49 -0400
Subject: [R] array writing and their filenames
In-Reply-To: <989527a20707290107t69c1da28ja42ffc7625d0c567@mail.gmail.com>
References: <989527a20707290107t69c1da28ja42ffc7625d0c567@mail.gmail.com>
Message-ID: <46AE4191.5090908@gmail.com>

Dong GUO ?? wrote:
> Hi,
> 
> I want to save a array (say, array[6,7,8]) write a cvs file. How can I do
> that??? can I write in one file?
For array[6,7,8], you don't need a csv(!) file since it is only a 
scalar. If this is what you want, check
?write.table

But what you probably meant is how to write a three-dimensional array to 
disk. Have a look at this code:
##########
roland <- array(1:(6*7*8), dim=c(6,7,8))
roland
dump(list="roland", file = "H:\\dumpdata.R")

ls()
rm(list=ls())
ls()
source("H:\\dumpdata.R")
ls()
roland
##########

Does this help?
Roland


From guo.dong99 at gmail.com  Mon Jul 30 21:58:27 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Mon, 30 Jul 2007 21:58:27 +0200
Subject: [R] filenames
Message-ID: <989527a20707301258k43a492d9lb3bf9dd2723bb7c9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070730/ebb7cd7d/attachment.pl 

From dmca at ucla.edu  Mon Jul 30 22:14:36 2007
From: dmca at ucla.edu (D L McArthur)
Date: Mon, 30 Jul 2007 20:14:36 +0000 (UTC)
Subject: [R] Creating an instance of R from MS Access?
References: <428638.87453.qm@web56609.mail.re3.yahoo.com>
Message-ID: <loom.20070730T221111-30@post.gmane.org>

Felipe Carrillo <mazatlanmexico <at> yahoo.com> writes:
> Hi all:
> Does anyone know if it's at all possible to create a
> connection to R from MS access? ...
>       
See function "odbcConnectAccess" in package RODBC.  
--D L McArthur <dmca <at> ucla.edu


From bolker at ufl.edu  Mon Jul 30 22:15:46 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Mon, 30 Jul 2007 20:15:46 +0000 (UTC)
Subject: [R] generating symmetric matrices
References: <541365.65996.qm@web63504.mail.re1.yahoo.com>
Message-ID: <loom.20070730T221334-649@post.gmane.org>

Gregory Gentlemen <gregory_gentlemen <at> yahoo.ca> writes:

> 
> Greetings,
> 
> I have a seemingly simple task which I have not been able to solve today. I
want to construct a symmetric
> matrix of arbtriray size w/o using loops. The following I thought would do it:
>
 
 [snip]

p <- 6
Rmat <- diag(p)
vals <- rnorm(p*(p-1)/2)
Rmat[lower.tri(Rmat)] <- vals
Rmat[upper.tri(Rmat)] <- t(Rmat)[upper.tri(Rmat)]

  appears to work.


From shitao at hotmail.com  Mon Jul 30 22:32:27 2007
From: shitao at hotmail.com (Tao Shi)
Date: Mon, 30 Jul 2007 20:32:27 +0000
Subject: [R] 'non-standard' folder names in R package
Message-ID: <BAY120-F1672E803D429C7BA726EC4C7EE0@phx.gbl>

Hi list,

I use a .xml file for a function's demo in the R package I'm creating.  
Since it doesn't belong to any of the 'standard' folders, i.e. those 
mentioned in the 'Writing R Extension', I put it in a folder call "myXML", 
much like the 'iris.xl' file in 'xls' folder from 'gdata' package, for 
example.  After running R CMD build, I could see the .xml file is in the 
..tar.gz file.  However, after running R CMD INSTALL -build, the file and the 
folder disappeared in both the .zip file and the installed package.  (R CMD 
CHECK, of course, failed too before that, as "myXML" can't be installed.)  
Could anybody tell me what's the tricks to keep those folders in my 
installation?  I'm using R-2.5.1 under WinXP.

Thank you very much!

....Tao

_________________________________________________________________
Need a brain boost? Recharge with a stimulating game. Play now!?


From Horace.Tso at pgn.com  Mon Jul 30 22:48:22 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Mon, 30 Jul 2007 13:48:22 -0700
Subject: [R] getting the name of variables passed to a function
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA047EDFF4@usctmx1106.merck.com>
References: <46A9C046020000650000777F@pgn.com>
	<39B6DDB9048D0F4DAD42CB26AAFF0AFA047EDFF4@usctmx1106.merck.com>
Message-ID: <46ADEC26020000650000787A@pgn.com>

Thanks to Prof. Brian Ripley, Marc, and Andy. match.call() is what i need. After spending a weekend away from email, I stumbled on a thread on exactly the same topic this morning, 

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/101445.html 

Horace

>>> "Liaw, Andy" <andy_liaw at merck.com> 7/30/2007 9:57:47 AM >>>
Here's one possibility:

R> f <- function(...) { call <- match.call(); sapply(as.list(call[-1]),
deparse) }
R> f(x, y)
[1] "x" "y"
R> f(x=x, y=y)
  x   y 
"x" "y" 

You basically need to know how to manipulate call objects.  The relevant
section in the R Language Definition should help.

Andy

 
From: Horace Tso
> 
> Folks,
> 
> I've entered into an R programming territory I'm not very 
> familiar with, thus this probably very elementary question 
> concerning the mechanic of a function call.
> 
> I want to know from within a function the name of the 
> variables I pass down. The function makes use of the "..." to 
> allow for multiple unknown arguments,
> 
> myfun = function(...) { do something }
> 
> In the body I put,
> 
> {    
> nm <- names(list(...))
> nm
> }
> 
> When the function is called with two vectors x, and y
> 
> myfun(x, y)
> 
> It returns NULL. However, when the call made is,
> 
> >myfun(x=x, y=y)
> 
> The result is
> [1] "x" "y"
> 
> Question : how do i get the names of the unknown variables 
> without explicitly saying x=x...
> 
> Thanks in advance.
> 
> Horace
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}


From jmb at mssl.ucl.ac.uk  Mon Jul 30 11:54:33 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Mon, 30 Jul 2007 10:54:33 +0100 (BST)
Subject: [R] Overlaying a single contour from a new data array in levelplot
Message-ID: <200707300954.l6U9sXTx013757@msslhb.mssl.ucl.ac.uk>

Dear Deepayan

Thank you for your response - it has proved very very helpful, I can't thank you 
enough! 

I have another question for you if you have time to reply. I know you have been 
asked about the colour of the polygon outline before (27 April 2007) and you 
replied that is a bug and the border can only be black or transparent...

I was wondering if you have found a way to change the colour of the outline 
since this correspondence? If not please can you tell me how to get around this 
myself? You mentioned writing a replacement to lpolygon - I do not know how to 
do this - would it be possible for you to guide me further? 

I would really benefit from having the border of the polygon in white as it goes 
over the "sea" which is also white and would therefore only be seen over the 
"land", much neater!

Many thanks,

Jenny

 

On 7/24/07, Jenny Barnes <jmb_at_mssl.ucl.ac.uk> wrote:
> Dear R-Help community,
>
> I am trying to overlay a single contour line over a correlation plot using
> levelplot in the lattice package. These are the two arrays:
>
> 1) a correlation plot over Africa - so each grid square is a different colour
> dependent on correlation - this is in an array: result_cor with dim[465,465]
>
> 2) a single contour line from a ***different data source*** - this is from 
data
> related to the p-values for the above correlation plot - I want to overlay 
only
> the 95% confidence contour. The p-values are stored in an array: 
result.p.values
> with same dimensions as above.
>
> I have read about using panel.levelplot and panel.contourplot in the R-help
> mailing list but I don't know the right way to call two different data arrays,
> can anybody help me please? I appreciate your time and help with this 
question.

I can think of a couple of different ways, but the simplest will probably be to 
compute the single contour beforehand and add it after the standard levelplot 
using a panel function. E.g., using the 'volcano' data for both matrices:

## you need the explicit x and y arguments because ## the default is different 
from levelplot.

vcl <- contourLines(x = seq_len(nrow(volcano)),

                   y = seq_len(ncol(volcano)),
                   z = volcano,
                   levels = c(172, 182))

levelplot(volcano, add.cl = vcl,
          panel = function(..., add.cl) {
              panel.levelplot(...)
              lapply(add.cl, panel.polygon, border = 'red')
          })


-Deepayan ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student: long range drought prediction 
Climate Extremes Group
Department of Space and Climate Physics
University College London
Holmbury St Mary 
Dorking, Surrey, RH5 6NT
Web: http://climate.mssl.ucl.ac.uk


From gunter.berton at gene.com  Mon Jul 30 23:12:51 2007
From: gunter.berton at gene.com (Bert Gunter)
Date: Mon, 30 Jul 2007 14:12:51 -0700
Subject: [R] Constructing correlation matrices
In-Reply-To: <277647.32862.qm@web63514.mail.re1.yahoo.com>
Message-ID: <004901c7d2ee$63a11920$4d908980@gne.windows.gene.com>

See ?dist for an object oriented approach that may be better.

Directly, you can do something like (see  ?row ?col):

x <- matrix(NA, 10,10)
## Lower triangular :
x[row(x) >= col(x) ] <- rnorm(55) 
x[row(x) < col(x)] <- x[row(x) > col(x)]
## or you could have saved the random vector and re-used it.


Bert Gunter
Genentech Nonclinical Statistics


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gregory Gentlemen
Sent: Sunday, July 29, 2007 7:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Constructing correlation matrices

Greetings,

I have a seemingly simple task which I have not been able to solve today and
I checked all of the help archives on this and have been unable to find
anything useful. I want to construct a symmetric matrix of arbtriray size
w/o using loops. The following I thought would do it:

p <- 6
Rmat <- diag(p)
dat.cor <- rnorm(p*(p-1)/2)
Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor

However, the problem is that the matrix is filled by column and so the
resulting matrix is not symmetric.

I'd be grateful for any adive and/or solutions.

Gregory 
       
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ted.harding at nessie.mcc.ac.uk  Mon Jul 30 23:24:16 2007
From: ted.harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 Jul 2007 22:24:16 +0100 (BST)
Subject: [R] write.csv
In-Reply-To: <989527a20707291041n75301c1ha99e57b94a73a8ba@mail.gmail.com>
Message-ID: <XFMail.070730222416.ted.harding@nessie.mcc.ac.uk>

On 29-Jul-07 17:41:58, Dong GUO ???? wrote:
> Hi,
> 
> I want to save an array(say, array[6,7,8]) write a cvs file.
> How can I do that??? can I write in one file?
> 
> if I could not write in one file, i want to use a loop to save
> in different files (in the array[6,7,8], should be 8 csv files),
> such as the filename structure should be:
> file ="filename" +str(i)+"." +"csv"
> 
> Many thanks.

The following (illustrated on a smaller array) may help:

A<-array((1:60),dim=c(3,4,5))
D<-dim(A)

write.table(t(dim(A)),file="A.csv",sep=",",
            quote=FALSE,row.names=FALSE,col.names=FALSE)

Z<-as.vector(A)
write.table(t(Z),file="A.csv",append=TRUE,sep=",",
            quote=FALSE,row.names=FALSE,col.names=FALSE)

Then the file A.csv contains two rows:

3,4,5
1,2,3,4,5,6,7,8,9,10,11,12, ... ,55,56,57,58,59,60

of which the first gives the dimension, the second the
data for the array.

You can then reconstruct another array, say B, as:

dimB<-scan(file="A.csv",sep=",",nlines=1)
dataB<-scan(file="A.csv",sep=",",skip=1,nlines=1)
B<-array(dataB,dimB)

That's a hard way to do things, perhaps, but since you said
you wanted the array as a CSV file, this is one way to do that.

Since a CSV text file is essentially a two-dimensional object
(fields in rows, by rows), to store a higher-dimensional object
in such a format you have to include the "meta-data" about the
structure -- in this case the list of dimensions.

Note, however, that, although it is a CSV file,

read.csv("A.csv",header=FALSE)

will not work nicely, since it will give you two rows of equal
length, the first one padded out with (in this case 57) NAs,
which you will then have to clean up; which you can do, but by
the time you've done it you might as well have done it the above
way!

Hoping this helps,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 30-Jul-07                                       Time: 22:24:12
------------------------------ XFMail ------------------------------


From patrick at pdrechsler.de  Mon Jul 30 23:24:49 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Mon, 30 Jul 2007 23:24:49 +0200
Subject: [R] add custom strip to lattice plot
Message-ID: <87ejipa8ke.fsf@pdrechsler.de>

Hi,

what is the recommended way of adding a strip to a lattice plot?

In the example below I would like to add the value of mean(y) to a new
strip.:

--8<---------------cut here---------------start------------->8---
library(lattice)

## Small sample data set:
p0 <- xyplot(uptake ~ Type | Treatment, data = CO2)

p1 <- update(p0,
             panel = function(x, y, ...) {
               panel.xyplot(x, y)
               panel.abline(h = mean(y),
                            col = "red"
                            )
             }
             )

plot(p1)
--8<---------------cut here---------------end--------------->8---

TIA

Patrick


From kirsten-beyer at uiowa.edu  Mon Jul 30 23:31:02 2007
From: kirsten-beyer at uiowa.edu (Kirsten Beyer)
Date: Mon, 30 Jul 2007 16:31:02 -0500
Subject: [R] simple coding question
Message-ID: <e9b46960707301431k2cf369efh6018bf4da92ba257@mail.gmail.com>

I have a list of ICD9 (disease) codes with various formats - 3 digit,
4 digit, 5 digit.  The first three digits of these codes are what I am
most interested in.  I would like to either add zeros to the 3 and 4
digit codes to make them 5 digit codes or add decimal points to put
them all in the format ###.##.  I did not see a function that allows
me to do this in the formatting command.  This seems simple - can
someone help?

Thanks,
K.Beyer


From sundar.dorai-raj at pdf.com  Mon Jul 30 23:36:25 2007
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 30 Jul 2007 15:36:25 -0600
Subject: [R] Tabs in PDF documents
In-Reply-To: <9A0F2613-FB53-4C70-8852-80C7E375ED71@plessthan.com>
References: <9A0F2613-FB53-4C70-8852-80C7E375ED71@plessthan.com>
Message-ID: <46AE59D9.4080207@pdf.com>



Dennis Fisher said the following on 7/30/2007 6:25 AM:
> Colleagues,
> 
> I am using R 2.5.1 on an Intel Mac (OS 10) to create PDF outputs  
> using pdf(); same problem exists in Linux (RedHat 9)
> 
> While adding text to the document with text() and mtext(), I  
> encounter the following problem:
> 
> In order to align the text, I have embedded tabs ("\t") in some of  
> the text.  Each time I do so, I get the following error messages:
> 	Warning: font metrics unknown for character 0x9
> 	Warning: font width unknown for character 0x9
> and the tabs are ignored.  I have tied par() with and without  
> family="mono".
> 
> Is there a work-around available for this?
> 
> Dennis
> 
> COMMANDS:
> 	pdf("junk.pdf")
> 	par(family="mono")
> 	plot(1,1)
> 	text(1,1, "\txx")
> 	mtext("\txx")
> 	dev.off()
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Hi, Dennis,

See this thread:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/96028.html

HTH,

--sundar


From A.Robinson at ms.unimelb.edu.au  Mon Jul 30 23:35:45 2007
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 31 Jul 2007 07:35:45 +1000
Subject: [R] Extract random part of summary nlme
In-Reply-To: <9341526C-182D-42CC-874A-E61099E21C26@student.ru.nl>
References: <9341526C-182D-42CC-874A-E61099E21C26@student.ru.nl>
Message-ID: <20070730213545.GM35312@ms.unimelb.edu.au>

Hi Rense,

Try:


> fm1 <- lme(distance ~ age, data = Orthodont)
> VarCorr(fm1)
Subject = pdSymm(age) 
            Variance   StdDev    Corr  
(Intercept) 5.41508675 2.3270339 (Intr)
age         0.05126947 0.2264276 -0.609
Residual    1.71620460 1.3100399       



I hope that this helps,

Andrew






On Mon, Jul 30, 2007 at 02:19:41PM +0200, Rense Nieuwenhuis wrote:
> Dear helpers,
> 
> I'm estimating multilevel regression models, using the lme-function  
> from the nlme-package. Let's say that I estimated a model and stored  
> it inside the object named 'model'. The summary of that model is  
> shown below:
> 
> Using summary(model)$tTable , I receive the following output:
> 
>  > summary(model)$tTable
>                      Value  Std.Error   DF     t-value       p-value
> (Intercept)    0.23268607 0.09350662 3990   2.4884449  1.287080e-02
> sexM          -0.15338225 0.03169762 3990  -4.8389206  1.354802e-06
> standLRT       0.38593558 0.01677195 3990  23.0107762 4.005182e-110
> vrmid 50%      0.07606394 0.09389376   61   0.8101064  4.210281e-01
> vrtop 25%      0.24561327 0.10483374   61   2.3428838  2.241317e-02
> intakemid 50% -0.41469716 0.03177240 3990 -13.0521199  3.698344e-38
> intaketop 25% -0.75920783 0.05357980 3990 -14.1696648  1.666780e-44
> typeSngl       0.15680532 0.07173835   61   2.1857949  3.267903e-02
> 
> 
> All looks fine to me. The output above is simply  a section from the  
> full summary shown below. Now, I want to extract from the summary (or  
> the full model) the part stating the random parameters. More  
> specifically, I want to extract from the summary the following:
> 
> (Intercept) 0.2869401 (Intr)
> typeSngl    0.2791040 -0.617
> Residual    0.7302233
> 
> How could this be done?
> 
> Thanks for the effort,
> 
> Rense Nieuwenhuis
> 
> 
> 
> 
> 
> 
> 
> 
> Linear mixed-effects model fit by REML
>   Data: Exam
>        AIC      BIC   logLik
>    9158.56 9234.241 -4567.28
> 
> Random effects:
>   Formula: ~type | school
>   Structure: General positive-definite, Log-Cholesky parametrization
>              StdDev    Corr
> (Intercept) 0.2869401 (Intr)
> typeSngl    0.2791040 -0.617
> Residual    0.7302233
> 
> Fixed effects: normexam ~ sex + standLRT + vr + intake + type
>                     Value  Std.Error   DF    t-value p-value
> (Intercept)    0.2326861 0.09350662 3990   2.488445  0.0129
> sexM          -0.1533822 0.03169762 3990  -4.838921  0.0000
> standLRT       0.3859356 0.01677195 3990  23.010776  0.0000
> vrmid 50%      0.0760639 0.09389376   61   0.810106  0.4210
> vrtop 25%      0.2456133 0.10483374   61   2.342884  0.0224
> intakemid 50% -0.4146972 0.03177240 3990 -13.052120  0.0000
> intaketop 25% -0.7592078 0.05357980 3990 -14.169665  0.0000
> typeSngl       0.1568053 0.07173835   61   2.185795  0.0327
>   Correlation:
>                (Intr) sexM   stnLRT vrm50% vrt25% int50% int25%
> sexM          -0.201
> standLRT      -0.125  0.028
> vrmid 50%     -0.742  0.028 -0.035
> vrtop 25%     -0.652  0.051 -0.065  0.649
> intakemid 50% -0.246 -0.011  0.541 -0.002  0.007
> intaketop 25% -0.218 -0.018  0.676  0.014  0.013  0.660
> typeSngl      -0.421  0.080  0.007  0.033 -0.027 -0.001  0.001
> 
> Standardized Within-Group Residuals:
>          Min          Q1         Med          Q3         Max
> -3.59074329 -0.63776965  0.03829878  0.67303837  3.33952680
> 
> Number of Observations: 4059
> Number of Groups: 65
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Andrew Robinson  
Department of Mathematics and Statistics            Tel: +61-3-8344-9763
University of Melbourne, VIC 3010 Australia         Fax: +61-3-8344-4599
http://www.ms.unimelb.edu.au/~andrewpr
http://blogs.mbs.edu/fishing-in-the-bay/


From Horace.Tso at pgn.com  Mon Jul 30 23:41:12 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Mon, 30 Jul 2007 14:41:12 -0700
Subject: [R] Constructing correlation matrices
In-Reply-To: <277647.32862.qm@web63514.mail.re1.yahoo.com>
References: <277647.32862.qm@web63514.mail.re1.yahoo.com>
Message-ID: <46ADF88802000065000078A3@pgn.com>

Greg,

I take it that you're trying to generate a random correlation matrix, so first create a covariance matrix,

p = 6
v = matrix(rnorm(p*p), ncol=p)
cov = t(v) %*% v

Then convert it to a correlation matrix,

cov2cor(cov)

HTH.

Horace


>>> Gregory Gentlemen <gregory_gentlemen at yahoo.ca> 7/29/2007 7:31:36 PM >>>
Greetings,

I have a seemingly simple task which I have not been able to solve today and I checked all of the help archives on this and have been unable to find anything useful. I want to construct a symmetric matrix of arbtriray size w/o using loops. The following I thought would do it:

p <- 6
Rmat <- diag(p)
dat.cor <- rnorm(p*(p-1)/2)
Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor

However, the problem is that the matrix is filled by column and so the resulting matrix is not symmetric.

I'd be grateful for any adive and/or solutions.

Gregory 
       
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From guo.dong99 at gmail.com  Tue Jul 31 00:02:28 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Tue, 31 Jul 2007 00:02:28 +0200
Subject: [R] array writing and their filenames
In-Reply-To: <2d1ebb110707301305k29e4f9c8g4a4e3b702bc92f7d@mail.gmail.com>
References: <989527a20707290107t69c1da28ja42ffc7625d0c567@mail.gmail.com>
	<2d1ebb110707301305k29e4f9c8g4a4e3b702bc92f7d@mail.gmail.com>
Message-ID: <989527a20707301502x2651389ch81456ed64a8d084b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/d52039c2/attachment.pl 

From guo.dong99 at gmail.com  Tue Jul 31 00:11:00 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Tue, 31 Jul 2007 00:11:00 +0200
Subject: [R] array loop
Message-ID: <989527a20707301511g71820657v7afbc59b5f01e22f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/d60a65f3/attachment.pl 

From guo.dong99 at gmail.com  Tue Jul 31 00:13:51 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Tue, 31 Jul 2007 00:13:51 +0200
Subject: [R] write.csv
In-Reply-To: <XFMail.070730222416.ted.harding@nessie.mcc.ac.uk>
References: <989527a20707291041n75301c1ha99e57b94a73a8ba@mail.gmail.com>
	<XFMail.070730222416.ted.harding@nessie.mcc.ac.uk>
Message-ID: <989527a20707301513j125e3155wcd06d66c50d39282@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/090fb9e0/attachment.pl 

From Thierry.ONKELINX at inbo.be  Tue Jul 31 00:12:33 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 31 Jul 2007 00:12:33 +0200
Subject: [R] Tabs in PDF documents
References: <9A0F2613-FB53-4C70-8852-80C7E375ED71@plessthan.com>
Message-ID: <2E9C414912813E4EB981326983E0A1042E49E0@inboexch.inbo.be>

I think you need to escape the '\' character in your string by adding an extra '\' in front of it. So try

	pdf("junk.pdf")
	par(family="mono")
	plot(1,1)
	text(1,1, "\\txx")
	mtext("\\txx")
	dev.off()

HTH,

Thierry

-----Oorspronkelijk bericht-----
Van: r-help-bounces op stat.math.ethz.ch namens Dennis Fisher
Verzonden: ma 30-7-2007 14:25
Aan: r-help op stat.math.ethz.ch
Onderwerp: [R] Tabs in PDF documents
 
Colleagues,

I am using R 2.5.1 on an Intel Mac (OS 10) to create PDF outputs  
using pdf(); same problem exists in Linux (RedHat 9)

While adding text to the document with text() and mtext(), I  
encounter the following problem:

In order to align the text, I have embedded tabs ("\t") in some of  
the text.  Each time I do so, I get the following error messages:
	Warning: font metrics unknown for character 0x9
	Warning: font width unknown for character 0x9
and the tabs are ignored.  I have tied par() with and without  
family="mono".

Is there a work-around available for this?

Dennis

COMMANDS:
	pdf("junk.pdf")
	par(family="mono")
	plot(1,1)
	text(1,1, "\txx")
	mtext("\txx")
	dev.off()

______________________________________________
R-help op stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Tue Jul 31 00:25:20 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 18:25:20 -0400
Subject: [R] Order by the columns
In-Reply-To: <c8b63a350707290601t53190c48nf8b6a19a3b4ad7ec@mail.gmail.com>
References: <c8b63a350707290601t53190c48nf8b6a19a3b4ad7ec@mail.gmail.com>
Message-ID: <644e1f320707301525y246f0824i791216759639387e@mail.gmail.com>

?order

You could do something like:

mat[order(mat[,1], mat[,2], mat[,3]),]

On 7/29/07, Am Stat <amstat2006 at gmail.com> wrote:
> Dear useR,
>
> I have a data matrix, it has n columns, each column is a two-level variable
> with entires -1 and +1. They are randomly generated, now I want to order
> them like (for example, 5 columns case)
> -    -    -   -   -
> -    -   -   -    -
> .................
> (first several rows are the samples with all variables in low level)
>
> +   -   -    -   -
> +   -   -    -    -
> .............................
>
>
> -   +   -    -   -
>
>
> +  +   -    -   -
>
>
>
> + + + + +
>
> Is there any function in R that could let me do this order by Var1 then
> order by Var2 then...order by Var n
>
>
> Thanks very much in advance!
>
>
> Best,
>
> Leon
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From guo.dong99 at gmail.com  Tue Jul 31 00:27:35 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Tue, 31 Jul 2007 00:27:35 +0200
Subject: [R] write.csv
In-Reply-To: <644e1f320707301522j77e2f669g1ab19b890236a686@mail.gmail.com>
References: <989527a20707291041n75301c1ha99e57b94a73a8ba@mail.gmail.com>
	<644e1f320707301522j77e2f669g1ab19b890236a686@mail.gmail.com>
Message-ID: <989527a20707301527j3a74e347jc9482e92d09859fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/09cbd271/attachment.pl 

From jholtman at gmail.com  Tue Jul 31 00:33:37 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 18:33:37 -0400
Subject: [R] write.csv
In-Reply-To: <989527a20707301527j3a74e347jc9482e92d09859fd@mail.gmail.com>
References: <989527a20707291041n75301c1ha99e57b94a73a8ba@mail.gmail.com>
	<644e1f320707301522j77e2f669g1ab19b890236a686@mail.gmail.com>
	<989527a20707301527j3a74e347jc9482e92d09859fd@mail.gmail.com>
Message-ID: <644e1f320707301533n71f9b8c8h68e50023d5eceffc@mail.gmail.com>

Then you can just write a 'for' loop to write out each submatrix:

for (i in 1:dim(x)[3]){
    write.csv(x[,,i], paste("x", i, ".csv", sep=""))
}


On 7/30/07, Dong GUO ???? <guo.dong99 at gmail.com> wrote:
> the dim of my results is (26,31,8) -(years, regions and variables). so, if i
> save each (years, regions) in 8 csv files, later, I could connect the
> (26,31) to dbf file in ArcGIS to show in a map. This is what I intend to do.
>
> I dont know a better way to do it directly in R...
>
>
> On 7/31/07, jim holtman <jholtman at gmail.com> wrote:
> > It really depends on how you want it output.  You can use 'write.csv'
> > to write an array out and it will be a 2-dimentional image that you
> > could then reconstruct it from if you know what the dimensions were.
> > What do you want to do with the data?  If you are just going to read
> > it back into R, then use save/load.
> >
> > On 7/29/07, Dong GUO ???? < guo.dong99 at gmail.com> wrote:
> > > Hi,
> > >
> > > I want to save an array(say, array[6,7,8]) write a cvs file. How can I
> do
> > > that??? can I write in one file?
> > >
> > > if I could not write in one file, i want to use a loop to save in
> different
> > > files (in the array[6,7,8], should be 8 csv files), such as the filename
> > > structure should be: file ="filename" +str(i) +"." +"csv"
> > >
> > > Many thanks.
> > >
> > >        [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >
> > --
> > Jim Holtman
> > Cincinnati, OH
> > +1 513 646 9390
> >
> > What is the problem you are trying to solve?
> >
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From guo.dong99 at gmail.com  Tue Jul 31 00:43:11 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Tue, 31 Jul 2007 00:43:11 +0200
Subject: [R] write.csv
In-Reply-To: <644e1f320707301533n71f9b8c8h68e50023d5eceffc@mail.gmail.com>
References: <989527a20707291041n75301c1ha99e57b94a73a8ba@mail.gmail.com>
	<644e1f320707301522j77e2f669g1ab19b890236a686@mail.gmail.com>
	<989527a20707301527j3a74e347jc9482e92d09859fd@mail.gmail.com>
	<644e1f320707301533n71f9b8c8h68e50023d5eceffc@mail.gmail.com>
Message-ID: <989527a20707301543g6f0eea1cp8f9db7553bf96960@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/933d0420/attachment.pl 

From Greg.Snow at intermountainmail.org  Tue Jul 31 00:43:34 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 30 Jul 2007 16:43:34 -0600
Subject: [R] 2nd R Console
Message-ID: <0de001c7d2fb$0c9b3d34$2e80320a@CO.IHC.COM>

Have you looked at the nws package?

It allows for a common workspace that multiple R sessions can all access.

Hope this helps,

-----Original Message-----
From: "Michael Janis" <mjanis at chem.ucla.edu>
To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
Sent: 7/30/07 7:49 AM
Subject: [R] 2nd R Console

Hi,

I was reading a thread: [R] "2nd R console" and had a similar question
regarding having more than one R console open at a time.  However, my
question differs from that of the thread:

Is it possible, or is there a wrapper that will allow one, to open an
arbitrary number of R consoles which access the same R session (all objects
in that session, etc.).  This would be R on linux accessed through a shell -
kind of like using GNU screen multi-user such that people could work
collaboratively on a given session.  The problem with screen is that all
commands are interleaved in the same terminal, which is confusing and does
not allow access to the command prompt at the same time, rather it would be
sequential.  I know there will be "why" questions but it is useful in an
academic environment.  Basically we have a memory machine for large genomic
analysis - and we could set that up as an Rserver, but this placing R into a
multi-user engine is better suited for our immediate needs.  Does anybody
have thoughts on this?

Thanks for considering,

Michael Janis
UCLA Bioinformatics

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From Greg.Snow at intermountainmail.org  Tue Jul 31 00:43:35 2007
From: Greg.Snow at intermountainmail.org (Greg Snow)
Date: Mon, 30 Jul 2007 16:43:35 -0600
Subject: [R] the large dataset problem
Message-ID: <0de101c7d2fb$0d173c28$2e80320a@CO.IHC.COM>

Check out the biglm package for some tools that may be useful.

-----Original Message-----
From: "Eric Doviak" <edoviak at earthlink.net>
To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
Sent: 7/30/07 9:54 AM
Subject: [R] the large dataset problem

Dear useRs,

I recently began a job at a very large and heavily bureaucratic organization. We're setting up a research office and statistical analysis will form the backbone of our work. We'll be working with large datasets such the SIPP as well as our own administrative data.

Due to the bureaucracy, it will take some time to get the licenses for proprietary software like Stata. Right now, R is the only statistical software package on my computer. 

This, of course, is a huge limitation because R loads data directly into RAM making it difficult (if not impossible) to work with large datasets. My computer only has 1000 MB of RAM, of which Microsucks Winblows devours 400 MB. To make my memory issues even worse, my computer has a virus scanner that runs everyday and I do not have the administrative rights to turn the damn thing off. 

I need to find some way to overcome these constraints and work with large datasets. Does anyone have any suggestions?

I've read that I should "carefully vectorize my code." What does that mean ??? !!!

The "Introduction to R" manual suggests modifying input files with Perl. Any tips on how to get started? Would Perl Data Language (PDL) be a good choice?  http://pdl.perl.org/index_en.html

I wrote a script which loads large datasets a few lines at a time, writes the dozen or so variables of interest to a CSV file, removes the loaded data and then (via a "for" loop) loads the next few lines .... I managed to get it to work with one of the SIPP core files, but it's SLOOOOW. Worse, if I discover later that I omitted a relevant variable, then I'll have to run the whole script all over again.

Any suggestions?

Thanks,
- Eric

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From deepayan.sarkar at gmail.com  Tue Jul 31 00:44:19 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 30 Jul 2007 15:44:19 -0700
Subject: [R] Overlaying a single contour from a new data array in
	levelplot
In-Reply-To: <200707300954.l6U9sXTx013757@msslhb.mssl.ucl.ac.uk>
References: <200707300954.l6U9sXTx013757@msslhb.mssl.ucl.ac.uk>
Message-ID: <eb555e660707301544m2d78f50ao6ccae3798f4a7ed9@mail.gmail.com>

On 7/30/07, Jenny Barnes <jmb at mssl.ucl.ac.uk> wrote:
> Dear Deepayan
>
> Thank you for your response - it has proved very very helpful, I can't thank you
> enough!
>
> I have another question for you if you have time to reply. I know you have been
> asked about the colour of the polygon outline before (27 April 2007) and you
> replied that is a bug and the border can only be black or transparent...
>
> I was wondering if you have found a way to change the colour of the outline
> since this correspondence? If not please can you tell me how to get around this
> myself? You mentioned writing a replacement to lpolygon - I do not know how to
> do this - would it be possible for you to guide me further?

That should have been fixed by now. Is there anything that's not
working as you expect? My code had:

   lapply(add.cl, panel.polygon, border = 'red')

which should have made the borders red. If it doesn't, you probably
need to upgrade to a recent version of R/lattice. If it does, changing
it to border='white' should suffice. If that doesn't work, please
provide a reproducible example.

-Deepayan

> I would really benefit from having the border of the polygon in white as it goes
> over the "sea" which is also white and would therefore only be seen over the
> "land", much neater!
>
> Many thanks,
>
> Jenny
>
>
>
> On 7/24/07, Jenny Barnes <jmb_at_mssl.ucl.ac.uk> wrote:
> > Dear R-Help community,
> >
> > I am trying to overlay a single contour line over a correlation plot using
> > levelplot in the lattice package. These are the two arrays:
> >
> > 1) a correlation plot over Africa - so each grid square is a different colour
> > dependent on correlation - this is in an array: result_cor with dim[465,465]
> >
> > 2) a single contour line from a ***different data source*** - this is from
> data
> > related to the p-values for the above correlation plot - I want to overlay
> only
> > the 95% confidence contour. The p-values are stored in an array:
> result.p.values
> > with same dimensions as above.
> >
> > I have read about using panel.levelplot and panel.contourplot in the R-help
> > mailing list but I don't know the right way to call two different data arrays,
> > can anybody help me please? I appreciate your time and help with this
> question.
>
> I can think of a couple of different ways, but the simplest will probably be to
> compute the single contour beforehand and add it after the standard levelplot
> using a panel function. E.g., using the 'volcano' data for both matrices:
>
> ## you need the explicit x and y arguments because ## the default is different
> from levelplot.
>
> vcl <- contourLines(x = seq_len(nrow(volcano)),
>
>                    y = seq_len(ncol(volcano)),
>                    z = volcano,
>                    levels = c(172, 182))
>
> levelplot(volcano, add.cl = vcl,
>           panel = function(..., add.cl) {
>               panel.levelplot(...)
>               lapply(add.cl, panel.polygon, border = 'red')
>           })
>
>
> -Deepayan ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Jennifer Barnes
> PhD student: long range drought prediction
> Climate Extremes Group
> Department of Space and Climate Physics
> University College London
> Holmbury St Mary
> Dorking, Surrey, RH5 6NT
> Web: http://climate.mssl.ucl.ac.uk
>
>


From jholtman at gmail.com  Tue Jul 31 00:51:03 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 18:51:03 -0400
Subject: [R] problems saving and loading (PLMset) objects
In-Reply-To: <001701c7d2aa$45d5a0d0$4f3911ac@simugenqfwills>
References: <001701c7d2aa$45d5a0d0$4f3911ac@simugenqfwills>
Message-ID: <644e1f320707301551u4b2e821bq44f1468e5bbfaa38@mail.gmail.com>

you just need to say:

load("expr.RData")

You should not be assigning it to 'expr' since it is already 'load'ed

On 7/30/07, Quin Wills <wills at stats.ox.ac.uk> wrote:
> Hi
>
>
>
> I'm running the latest R on a presumably up to date Linux server.
>
>
>
> 'Doing something silly I'm sure, but can't see why my saved PLMset objects
> come out all wrong. To use an example:
>
>
>
> Setting up an example PLMset (I have the same problem no matter what example
> I use)
>
> > library(affyPLM)
>
> > data(Dilution) # affybatch object
>
> > Dilution = updateObject(Dilution)
>
> > options(width=36)
>
> > expr <- fitPLM(Dilution)
>
>
>
>
>
> This works, and I'm able to get the probeset coefficients with coefs(expr).
> until I save and try reloading:
>
> > save(expr, file="expr.RData")
>
> > rm(expr) # just to be sure
>
> > expr <- load(expr.RData)
>
>
>
>
>
> Now, running coefs(expr) says:
>
> > Error in function (classes, fdef, mtable) : unable to find an inherited
> method for function "coefs", for signature "character"
>
>
>
>
>
> Trying str(exp) just gives the following:
>
> > chr "exp"
>
>
>
> expr.Rdata appears to save properly (in that there is an actual file with
> notable size in my working directory).
>
>
>
> Thanks in advance,
>
> Quin
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From gangchen at mail.nih.gov  Tue Jul 31 00:52:10 2007
From: gangchen at mail.nih.gov (Gang Chen)
Date: Mon, 30 Jul 2007 18:52:10 -0400
Subject: [R] Error message in lmer
In-Reply-To: <46ADD70B.6020502@kvl.dk>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
	<46ADD70B.6020502@kvl.dk>
Message-ID: <3D3D2D2C-2D9E-4FF5-A897-266A980EC0C1@mail.nih.gov>

I'm trying to run a simple one-way ANCOVA with the lmer function in R  
package lme4, but have encountered some conceptual problem. The data  
file MyData.txt is like this:

Group Subj Cov Resp
A       1   3.90   4.05
A	 2   4.05	4.25
A	 3   4.25	3.60
A	 4   3.60	4.20
A	 5   4.20	4.05
A	 6   4.05	3.85
B	 7   3.85	4.15
B	 8   4.15	4.60
B	 9   4.60	4.15
B	 10  4.15	4.40
B	 11  4.40	3.35
B	 12  3.35	3.80
B	 13  3.80   3.90

Since I would like to treat subject (Subj) as a random factor, my one- 
way ANCOVA (with covariate, Cov) is:

TestData=read.table("MyData.txt", header=T);
m1 <- lmer(Resp ~ Group * Cov + (1 | Subj), TestData)

but I got the following error message:

Error in lmerFactorList(formula, mf, fltype) :
         number of levels in grouping factor(s) 'Subj' is too large

Do I have some misconception about the model?

Thank you very much.

Gang


From deepayan.sarkar at gmail.com  Tue Jul 31 00:59:58 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 30 Jul 2007 15:59:58 -0700
Subject: [R] lattice grayscale "theme"
In-Reply-To: <87lkd08agu.fsf@pdrechsler.de>
References: <87lkd08agu.fsf@pdrechsler.de>
Message-ID: <eb555e660707301559s56368283wa6cd700cb024edb7@mail.gmail.com>

On 7/28/07, Patrick Drechsler <patrick at pdrechsler.de> wrote:
> Hi,
>
> is there a grayscale setting for lattice plots?
>
> I like the default color settings. I also like the settings that are
> available for setting black and white with something like this:
>
> --8<---------------cut here---------------start------------->8---
> ltheme <- canonical.theme(color = FALSE)      ## in-built B&W theme
> ltheme$strip.background$col <- "transparent" ## change strip bg
> lattice.options(default.theme = ltheme)      ## set as default
> --8<---------------cut here---------------end--------------->8---
>
> Is there a simple way of achieving something in between these settings
> (using grayscales for the default colors)?

Possibly, but you would have to define what you mean by 'simple' and
'something in between' more precisely. help(trellis.par.set) tells you
how to change settings, and you can use it to change all the
components you want (and the colors can of course be greyscale).

-Deepayan


From jholtman at gmail.com  Tue Jul 31 01:08:48 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 19:08:48 -0400
Subject: [R] deriv; loop
In-Reply-To: <11853456.post@talk.nabble.com>
References: <11853456.post@talk.nabble.com>
Message-ID: <644e1f320707301608ifc1b6b8labdbf1f2cf423245@mail.gmail.com>

for question 1, is this what you want (BTW allocate 'result' to the
size you want - the example keeps extending it which is OK for small
numbers, but for larger size preallocate):

> result <- numeric(0)
> for (i in 1:6) result[i] <- i
> result
[1] 1 2 3 4 5 6
> prod(result)
[1] 720


On 7/29/07, francogrex <francogrex at mail.com> wrote:
>
> Hi, 2 questions:
>
> Question 1: example of what I currently do:
>
> for(i in 1:6){sink("temp.txt",append=TRUE)
> dput(i+0)
> sink()}
> x=scan(file="temp.txt")
> print(prod(x))
> file.remove("C:/R-2.5.0/temp.txt")
>
> But how to convert the output of the loop to a vector that I can manipulate
> (by prod or sum etc), without having to write and append to a file?
>
> Question 2:
>
> > deriv(~gamma(x),"x")
>
> expression({
>    .expr1 <- gamma(x)
>    .value <- .expr1
>    .grad <- array(0, c(length(.value), 1), list(NULL, c("x")))
>    .grad[, "x"] <- .expr1 * psigamma(x)
>    attr(.value, "gradient") <- .grad
>    .value
> })
>
> BUT
>
> > deriv3(~gamma(x),"x")
> Error in deriv3.formula(~gamma(x), "x") : Function 'psigamma' is not in the
> derivatives table
>
> What I want is the expression for the second derivative (which I believe is
> trigamma(x), or psigamma(x,1)), how can I obtain that?
>
> Thanks
> --
> View this message in context: http://www.nabble.com/deriv--loop-tf4166283.html#a11853456
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From Horace.Tso at pgn.com  Tue Jul 31 01:12:38 2007
From: Horace.Tso at pgn.com (Horace Tso)
Date: Mon, 30 Jul 2007 16:12:38 -0700
Subject: [R] Constructing correlation matrices (follow up)
In-Reply-To: <277647.32862.qm@web63514.mail.re1.yahoo.com>
References: <277647.32862.qm@web63514.mail.re1.yahoo.com>
Message-ID: <46AE0DF602000065000078A9@pgn.com>

Greg, in light of Doug Bates' question, what i have suggested a little early in response to your question is known as a Wishart matrix with n degree of freedom, which is guarenteed to be positive definite. If this is not what you want, you have to be more specific about the property of this correlation matrix you want to simulate.

H.




=====================================
Greg,

I take it that you're trying to generate a random correlation matrix, so first create a covariance matrix,

p = 6
v = matrix(rnorm(p*p), ncol=p)
cov = t(v) %*% v

Then convert it to a correlation matrix,

cov2cor(cov)

HTH.

Horace


>>> Gregory Gentlemen <gregory_gentlemen at yahoo.ca> 7/29/2007 7:31:36 PM >>>
Greetings,

I have a seemingly simple task which I have not been able to solve today and I checked all of the help archives on this and have been unable to find anything useful. I want to construct a symmetric matrix of arbtriray size w/o using loops. The following I thought would do it:

p <- 6
Rmat <- diag(p)
dat.cor <- rnorm(p*(p-1)/2)
Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor

However, the problem is that the matrix is filled by column and so the resulting matrix is not symmetric.

I'd be grateful for any adive and/or solutions.

Gregory 
       
---------------------------------

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code.


From wills at stats.ox.ac.uk  Tue Jul 31 01:18:31 2007
From: wills at stats.ox.ac.uk (Quin Wills)
Date: Tue, 31 Jul 2007 00:18:31 +0100
Subject: [R] problems saving and loading (PLMset) objects
In-Reply-To: <644e1f320707301551u4b2e821bq44f1468e5bbfaa38@mail.gmail.com>
Message-ID: <000301c7d2ff$f5596b40$0301a8c0@simugenqfwills>

Ah, didn't realize that I couldn't re-assign in one step... I was trying to
load in various data (on separate occasions), using a common object name to
run through some template code.

Many thanks!
 

-----Original Message-----
From: jim holtman [mailto:jholtman at gmail.com] 
Sent: 30 July 2007 23:51
To: Quin Wills
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] problems saving and loading (PLMset) objects

you just need to say:

load("expr.RData")

You should not be assigning it to 'expr' since it is already 'load'ed

On 7/30/07, Quin Wills <wills at stats.ox.ac.uk> wrote:
> Hi
>
>
>
> I'm running the latest R on a presumably up to date Linux server.
>
>
>
> 'Doing something silly I'm sure, but can't see why my saved PLMset objects
> come out all wrong. To use an example:
>
>
>
> Setting up an example PLMset (I have the same problem no matter what
example
> I use)
>
> > library(affyPLM)
>
> > data(Dilution) # affybatch object
>
> > Dilution = updateObject(Dilution)
>
> > options(width=36)
>
> > expr <- fitPLM(Dilution)
>
>
>
>
>
> This works, and I'm able to get the probeset coefficients with
coefs(expr).
> until I save and try reloading:
>
> > save(expr, file="expr.RData")
>
> > rm(expr) # just to be sure
>
> > expr <- load(expr.RData)
>
>
>
>
>
> Now, running coefs(expr) says:
>
> > Error in function (classes, fdef, mtable) : unable to find an inherited
> method for function "coefs", for signature "character"
>
>
>
>
>
> Trying str(exp) just gives the following:
>
> > chr "exp"
>
>
>
> expr.Rdata appears to save properly (in that there is an actual file with
> notable size in my working directory).
>
>
>
> Thanks in advance,
>
> Quin
>
>
>
>
>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From patrick at pdrechsler.de  Tue Jul 31 01:28:21 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Tue, 31 Jul 2007 01:28:21 +0200
Subject: [R] lattice grayscale "theme"
In-Reply-To: <eb555e660707301559s56368283wa6cd700cb024edb7@mail.gmail.com>
	(Deepayan Sarkar's message of "Mon\,
	30 Jul 2007 15\:59\:58 -0700")
References: <87lkd08agu.fsf@pdrechsler.de>
	<eb555e660707301559s56368283wa6cd700cb024edb7@mail.gmail.com>
Message-ID: <87abtdwjxm.fsf@pdrechsler.de>

The Gmane interface seems to have some lag at the moment...

"Deepayan Sarkar" <deepayan.sarkar at gmail.com> writes:

> On 7/28/07, Patrick Drechsler <patrick at pdrechsler.de> wrote:
>>
>> is there a grayscale setting for lattice plots?
>>
>> I like the default color settings. I also like the settings that are
>> available for setting black and white with something like this:
>>
>> --8<---------------cut here---------------start------------->8---
>> ltheme <- canonical.theme(color = FALSE)      ## in-built B&W theme
>> ltheme$strip.background$col <- "transparent" ## change strip bg
>> lattice.options(default.theme = ltheme)      ## set as default
>> --8<---------------cut here---------------end--------------->8---
>>
>> Is there a simple way of achieving something in between these settings
>> (using grayscales for the default colors)?
>
> Possibly, but you would have to define what you mean by 'simple' and
> 'something in between' more precisely. 

Here is an example of 'in between':

--8<---------------cut here---------------start------------->8---
  ## Set background color of strips to grayscales:
  strip.background <- trellis.par.get("strip.background")
  trellis.par.set(strip.background = list(col = grey(7:1/8)))
  ## Set color of plot symbols to grayscale:
  plot.symbol <- trellis.par.get("plot.symbol")
  trellis.par.set(plot.symbol = list(col = grey(5/8)))
--8<---------------cut here---------------end--------------->8---

I think it would be nice to have a few default "themes": A single
switch between "default color", "grayscale" and "black and white".


What do you think?

Cheers,

Patrick 
-- 
We are sorry, but the number you have dialed is imaginary.
Please rotate your phone 90 degrees and try again.


From jholtman at gmail.com  Tue Jul 31 01:36:30 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 19:36:30 -0400
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46ADF344.3050700@yahoo.de>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
	<46ADD4F1.8090901@yahoo.de> <46ADE7E6.4050900@unifi.it>
	<46ADF344.3050700@yahoo.de>
Message-ID: <644e1f320707301636l617fa27chfd6ff1eaa0146f1c@mail.gmail.com>

Is this what you want:

> x <- read.table(textConnection("  v1         v2          v3          v4         v5          v6           v7 v8
+ 1 NA -0.6442149  0.02354036 -1.40362589 -1.1829260  1.17099178 -0.046778203 NA
+ 2 NA -0.2047012 -1.36186952  0.13045724  2.1411553  0.49248118 -0.233788840 NA
+ 3 NA -1.1986041 -0.42197792 -0.84651458 -0.1327081 -0.18690065  0.443908897 NA
+ 4 NA -0.2097442  1.50445971  1.57005071 -0.1053442  1.50050976 -1.649740180 NA
+ 5 NA -0.7343465 -1.76763996  0.06961015 -0.8179396 -0.65552410  0.003991354 NA
+ 6 NA -1.3888750  0.53722404  0.25269771 -1.2342698 -0.01243247
-0.228020092 NA"), header=TRUE)
>
> categ <- scan(textConnection("NA        cat1        cat1        cat1       cat2        cat2         cat2   NA"), what='')
Read 8 items
> cat.col <- split(1:ncol(x), categ)
> lapply(cat.col, function(.cat){
+     rowMeans(x[, .cat])
+ })
$cat1
                  1                   2                   3
       4                   5
-0.6747668100000001 -0.4787044933333333 -0.8223655333333334
0.9549220733333333 -0.8107921033333333
                  6
-0.1996510833333333

$cat2
                   1                    2                    3
           4                    5
-0.01957080766666663  0.79994921333333324  0.04143338233333334
-0.08485820666666670 -0.48982411533333337
                   6
-0.49157412066666667



On 7/30/07, Antje <niederlein-rstat at yahoo.de> wrote:
> Hello,
>
> thank you for your help. But I guess, it's still not what I want... printing df.my gives me
>
> df.my
>   v1         v2          v3          v4         v5          v6           v7 v8
> 1 NA -0.6442149  0.02354036 -1.40362589 -1.1829260  1.17099178 -0.046778203 NA
> 2 NA -0.2047012 -1.36186952  0.13045724  2.1411553  0.49248118 -0.233788840 NA
> 3 NA -1.1986041 -0.42197792 -0.84651458 -0.1327081 -0.18690065  0.443908897 NA
> 4 NA -0.2097442  1.50445971  1.57005071 -0.1053442  1.50050976 -1.649740180 NA
> 5 NA -0.7343465 -1.76763996  0.06961015 -0.8179396 -0.65552410  0.003991354 NA
> 6 NA -1.3888750  0.53722404  0.25269771 -1.2342698 -0.01243247 -0.228020092 NA
>
> now, I have to combine like this:
>
>   v1         v2          v3          v4         v5          v6           v7     v8
>   NA        cat1        cat1        cat1       cat2        cat2         cat2   NA
>
> -->
>
> mean(df.my$v2[1],df.my$v3[1],df.my$v4[1])
> mean(df.my$v2[2],df.my$v3[2],df.my$v4[2])
> mean(df.my$v2[3],df.my$v3[3],df.my$v4[3])
> mean(df.my$v2[4],df.my$v3[4],df.my$v4[4])
> mean(df.my$v2[5],df.my$v3[5],df.my$v4[5])
> mean(df.my$v2[6],df.my$v3[6],df.my$v4[6])
>
> the same for v5, v6 and v7
>
> further, I'm not sure how to avoid the list, because this is the result of the processing I did before...
>
> Ciao,
> Antje
>
>
> 8rino-Luca Pantani schrieb:
> > I hope I see.
> >
> > Why not try the following, and avoid lists, which I'm not still able to
> > manage properly ;-)
> > v1 <- NA
> > v2 <- rnorm(6)
> > v3 <- rnorm(6)
> > v4 <- rnorm(6)
> > v5 <- rnorm(6)
> > v6 <- rnorm(6)
> > v7 <- rnorm(6)
> > v8 <- rnorm(6)
> > v8 <- NA
> > (df.my <- cbind.data.frame(v1, v2, v3, v4, v5, v6, v7, v8))
> > (df.my2 <- reshape(df.my,
> >                  varying=list(c("v1","v2","v3", "v4","v5","v6","v7","v8")),
> >                  idvar="sequential",
> >                  timevar="cat",
> >                  direction="long"
> >        ))
> > aggregate(df.my2$v1, by=list(category=df.my2$cat), mean)
> > aggregate(df.my2$v1, by=list(category=df.my2$cat), function(x){sd(x,
> > na.rm = TRUE)})
> >
> >
> > Antje ha scritto:
> >> okay, I played a bit around and now I have some kind of testcase for you:
> >>
> >> v1 <- NA
> >> v2 <- rnorm(6)
> >> v3 <- rnorm(6)
> >> v4 <- rnorm(6)
> >> v5 <- rnorm(6)
> >> v6 <- rnorm(6)
> >> v7 <- rnorm(6)
> >> v8 <- rnorm(6)
> >> v8 <- NA
> >>
> >> list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
> >> categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)
> >>
> >> > list
> >> [[1]]
> >> [1] NA
> >>
> >> [[2]]
> >> [1] -0.6442149 -0.2047012 -1.1986041 -0.2097442 -0.7343465 -1.3888750
> >>
> >> [[3]]
> >> [1]  0.02354036 -1.36186952 -0.42197792  1.50445971 -1.76763996
> >> 0.53722404
> >>
> >> [[4]]
> >> [1] -1.40362589  0.13045724 -0.84651458  1.57005071  0.06961015
> >> 0.25269771
> >>
> >> [[5]]
> >> [1] -1.1829260  2.1411553 -0.1327081 -0.1053442 -0.8179396 -1.2342698
> >>
> >> [[6]]
> >> [1]  1.17099178  0.49248118 -0.18690065  1.50050976 -0.65552410
> >> -0.01243247
> >>
> >> [[7]]
> >> [1] -0.046778203 -0.233788840  0.443908897 -1.649740180  0.003991354
> >> -0.228020092
> >>
> >> [[8]]
> >> [1] NA
> >>
> >> now, I need the means (and sd) of element 1 of list[2],list[3],list[4]
> >> (because they belong to "cat1") and
> >>
> >> = mean(-0.6442149, 0.02354036, -1.40362589)
> >>
> >> the same for element 2 up to element 6 (--> I would the get a vector
> >> containing the means for "cat1")
> >> the same for the vectors belonging to "cat2".
> >>
> >> does anybody now understand what I mean?
> >>
> >> Antje
> >>
> >>
> >>
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From deepayan.sarkar at gmail.com  Tue Jul 31 01:52:07 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 30 Jul 2007 16:52:07 -0700
Subject: [R] lattice grayscale "theme"
In-Reply-To: <87abtdwjxm.fsf@pdrechsler.de>
References: <87lkd08agu.fsf@pdrechsler.de>
	<eb555e660707301559s56368283wa6cd700cb024edb7@mail.gmail.com>
	<87abtdwjxm.fsf@pdrechsler.de>
Message-ID: <eb555e660707301652w76b68610m620d9aed3c36d28a@mail.gmail.com>

On 7/30/07, Patrick Drechsler <patrick at pdrechsler.de> wrote:
> The Gmane interface seems to have some lag at the moment...
>
> "Deepayan Sarkar" <deepayan.sarkar at gmail.com> writes:
>
> > On 7/28/07, Patrick Drechsler <patrick at pdrechsler.de> wrote:
> >>
> >> is there a grayscale setting for lattice plots?
> >>
> >> I like the default color settings. I also like the settings that are
> >> available for setting black and white with something like this:
> >>
> >> --8<---------------cut here---------------start------------->8---
> >> ltheme <- canonical.theme(color = FALSE)      ## in-built B&W theme
> >> ltheme$strip.background$col <- "transparent" ## change strip bg
> >> lattice.options(default.theme = ltheme)      ## set as default
> >> --8<---------------cut here---------------end--------------->8---
> >>
> >> Is there a simple way of achieving something in between these settings
> >> (using grayscales for the default colors)?
> >
> > Possibly, but you would have to define what you mean by 'simple' and
> > 'something in between' more precisely.
>
> Here is an example of 'in between':
>
> --8<---------------cut here---------------start------------->8---
>   ## Set background color of strips to grayscales:
>   strip.background <- trellis.par.get("strip.background")
>   trellis.par.set(strip.background = list(col = grey(7:1/8)))
>   ## Set color of plot symbols to grayscale:
>   plot.symbol <- trellis.par.get("plot.symbol")
>   trellis.par.set(plot.symbol = list(col = grey(5/8)))
> --8<---------------cut here---------------end--------------->8---

Well, there you go. You have a new theme.

> I think it would be nice to have a few default "themes": A single
> switch between "default color", "grayscale" and "black and white".

I'm still not sure what qualities you are looking for in the new
theme. An upcoming version of the latticeExtra package will have a
function that can be used to create a theme based on some given
colors; you might find it useful. It uses colors from RColorBrewer by
default, and looks like:


brewer.theme <-
    function(symbol = brewer.pal(n = 8, name = "Dark2"),
             fill = brewer.pal(n = 12, name = "Set3"),
             region = brewer.pal(n = 11, name = "Spectral"),
             reference = "#e8e8e8",
             bg = "transparent",
             fg = "black")
{
    theme <-
        list(plot.polygon      = list(col = fill[1], border = fg[1]),
             box.rectangle     = list(col= symbol[1]),
             box.umbrella      = list(col= symbol[1]),
             dot.line          = list(col = reference),
             dot.symbol        = list(col = symbol[1]),
             plot.line         = list(col = symbol[1]),
             plot.symbol       = list(col= symbol[1]),
             regions           = list(col = colorRampPalette(region)(100)),
             reference.line    = list(col = reference),
             superpose.line    = list(col = symbol),
             superpose.symbol  = list(col = symbol),
             superpose.polygon = list(col = fill, border = fg),

             background        = list(col = bg),
             add.line          = list(col = fg),
             add.text          = list(col = fg),
             box.dot           = list(col = fg),
             axis.line         = list(col = fg),
             axis.text         = list(col = fg),
             strip.border      = list(col = fg),
             box.3d            = list(col = fg),
             par.xlab.text     = list(col = fg),
             par.ylab.text     = list(col = fg),
             par.zlab.text     = list(col = fg),
             par.main.text     = list(col = fg),
             par.sub.text      = list(col = fg))
    modifyList(standard.theme("pdf"), theme)
}

-Deepayan


From mjanis at chem.ucla.edu  Tue Jul 31 02:05:51 2007
From: mjanis at chem.ucla.edu (Michael Janis)
Date: Mon, 30 Jul 2007 17:05:51 -0700
Subject: [R] 2nd R Console
In-Reply-To: <0de001c7d2fb$0c9b3d34$2e80320a@CO.IHC.COM>
References: <0de001c7d2fb$0c9b3d34$2e80320a@CO.IHC.COM>
Message-ID: <03a401c7d306$8f1103f0$13321b82@heron>

Oh now that *is* something interesting!  Thank you Greg, I'll have to give
this a try.

-----Original Message-----
From: Greg Snow [mailto:Greg.Snow at intermountainmail.org]
Sent: Monday, July 30, 2007 3:44 PM
To: mjanis at chem.ucla.edu; r-help at stat.math.ethz.ch
Subject: RE: [R] 2nd R Console

Have you looked at the nws package?

It allows for a common workspace that multiple R sessions can all access.

Hope this helps,

-----Original Message-----
From: "Michael Janis" <mjanis at chem.ucla.edu>
To: "r-help at stat.math.ethz.ch" <r-help at stat.math.ethz.ch>
Sent: 7/30/07 7:49 AM
Subject: [R] 2nd R Console

Hi,

I was reading a thread: [R] "2nd R console" and had a similar question
regarding having more than one R console open at a time.  However, my
question differs from that of the thread:

Is it possible, or is there a wrapper that will allow one, to open an
arbitrary number of R consoles which access the same R session (all objects
in that session, etc.).  This would be R on linux accessed through a shell -
kind of like using GNU screen multi-user such that people could work
collaboratively on a given session.  The problem with screen is that all
commands are interleaved in the same terminal, which is confusing and does
not allow access to the command prompt at the same time, rather it would be
sequential.  I know there will be "why" questions but it is useful in an
academic environment.  Basically we have a memory machine for large genomic
analysis - and we could set that up as an Rserver, but this placing R into a
multi-user engine is better suited for our immediate needs.  Does anybody
have thoughts on this?

Thanks for considering,

Michael Janis
UCLA Bioinformatics

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From patrick at pdrechsler.de  Tue Jul 31 03:15:55 2007
From: patrick at pdrechsler.de (Patrick Drechsler)
Date: Tue, 31 Jul 2007 03:15:55 +0200
Subject: [R] lattice grayscale "theme"
In-Reply-To: <eb555e660707301652w76b68610m620d9aed3c36d28a@mail.gmail.com>
	(Deepayan Sarkar's message of "Mon\,
	30 Jul 2007 16\:52\:07 -0700")
References: <87lkd08agu.fsf@pdrechsler.de>
	<eb555e660707301559s56368283wa6cd700cb024edb7@mail.gmail.com>
	<87abtdwjxm.fsf@pdrechsler.de>
	<eb555e660707301652w76b68610m620d9aed3c36d28a@mail.gmail.com>
Message-ID: <87ps292x10.fsf@pdrechsler.de>

"Deepayan Sarkar" <deepayan.sarkar at gmail.com> writes:
> On 7/30/07, Patrick Drechsler <patrick at pdrechsler.de> wrote:
>> The Gmane interface seems to have some lag at the moment...
>>
>> "Deepayan Sarkar" <deepayan.sarkar at gmail.com> writes:
>>
>> > On 7/28/07, Patrick Drechsler <patrick at pdrechsler.de> wrote:
>> >>
>> >> is there a grayscale setting for lattice plots?
[...]
>> >> Is there a simple way of achieving something in between these
>> >> settings (using grayscales for the default colors)?
>> >
>> > Possibly, but you would have to define what you mean by 'simple'
>> > and 'something in between' more precisely.
>>
>> Here is an example of 'in between':
>>
>> --8<---------------cut here---------------start------------->8---
>>   ## Set background color of strips to grayscales:
>>   strip.background <- trellis.par.get("strip.background")
>>   trellis.par.set(strip.background = list(col = grey(7:1/8)))
>>   ## Set color of plot symbols to grayscale:
>>   plot.symbol <- trellis.par.get("plot.symbol")
>>   trellis.par.set(plot.symbol = list(col = grey(5/8)))
>> --8<---------------cut here---------------end--------------->8---
>
> Well, there you go. You have a new theme.

Thanks for confirming that I am on the right track!

>> I think it would be nice to have a few default "themes": A single
>> switch between "default color", "grayscale" and "black and white".
>
> I'm still not sure what qualities you are looking for in the new
> theme. 

Goal: 

Lattice should have good default settings for color, grayscale and
b&w.

The user should not be bothered with the details (similar to using
LaTeX: Many users do not have any idea of DEK's typesetting in the
background). 

Lattice functions should provide reasonable default settings.

The default colors for the lattice suite were chosen for a reason
(very good choice for color display).

IMO the default grayscale colors should be wrapped into a similar
default-suite after discussion of which grayscales are best suited for
printing with different common resolutions using b/w printers.

After this has been decided upon, it might be nice to have something
along the lines of:

lattice.options(default.theme = [color|gray|bw])

Have I described my concern elaborately enough?

> An upcoming version of the latticeExtra package will have a function
> that can be used to create a theme based on some given colors; you
> might find it useful. It uses colors from RColorBrewer by default,
> and looks like:

This looks very interesting, thanks for keeping the development going!

Cheers,

Patrick


From deepayan.sarkar at gmail.com  Tue Jul 31 03:27:52 2007
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Mon, 30 Jul 2007 18:27:52 -0700
Subject: [R] duplicate DATE at in lattice scale handled differently from
	base graphics OR lattice numeric scales
In-Reply-To: <E7ABF739-B796-4851-B64C-F452E3FAB6C4@transitive.com>
References: <E7ABF739-B796-4851-B64C-F452E3FAB6C4@transitive.com>
Message-ID: <eb555e660707301827ka89a651m8aa5d82b875e00af@mail.gmail.com>

On 7/30/07, Alex Brown <alex at transitive.com> wrote:
> When at, label chains contain duplicate at values, axis ticks are
> dropped.  I believe this is handled incorrectly for Date ats in
> lattice 0.15-4 when compared to how it is handled for numeric, or for
> dates in base plot.  This can result in mis-labelled axes in some
> circumstances.  I have included executable examples.
>
> R: 2.5.0
>
> numeric base plot:
>
> plot(1:10, axes=F); axis(1, at=c(2,4,4,6), labels=letters[1:4])
>
> shows labels a, b, d
>
> date scale in base plot:
>
> plot((as.Date("2007-01-01") + 1:10), 1:10, axes=F); axis(1, at=
> (as.Date("2007-01-01")+c(2,4,4,6)), labels=letters[1:4])
>
> shows labels a, b, d

However,

> foo = as.Date("2007-01-01")
> plot(foo + 1:10, 1:10, axes = FALSE)
> axis.Date(1, at = foo + c(2,4,4,6), labels = letters[1:4])
Error in axis(side, at = z, labels = labels, ...) :
  'at' and 'labels' lengths differ, 3 != 4

And this code is used almost unchanged in lattice, leading to

> str(lattice:::formattedTicksAndLabels.Date(range(foo + 1:10), at = foo + c(2, 4, 4, 6),  labels = letters[1:4]))
List of 4
 $ at           : num [1:3] 13516 13518 13520
 $ labels       : chr [1:4] "a" "b" "c" "d"
 $ check.overlap: logi FALSE
 $ num.limit    : num [1:2] 13515 13524

That is, 'at' is now of length 3 (and unlike axis.Date, this does not
subsequently cause an error, which probably would have been
preferable).

I will add a fix.

-Deepayan


> numeric scale in lattice plot
>
> xyplot(1:10 ~ 1:10, scales=list(x=list(relation="free", at=c
> (2,4,4,6), labels=list(letters[1:4]))))
>
> shows labels a, b, d
>
> date scale in lattice plot
>
> xyplot(1:10 ~ (as.Date("2007-01-01") + 1:10) , scales=list(x=list
> (relation="free", at=list(as.Date("2007-01-01")+c(2,4,4,6)),
> labels=list(letters[1:4]))))
>
> shows labels a, b, c
>
> Since this results in label c being placed at position 6, not 4, I
> feel this is a bug.
>
> -Alex Brown


From jholtman at gmail.com  Tue Jul 31 03:46:45 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 21:46:45 -0400
Subject: [R] the large dataset problem
In-Reply-To: <XFMail.070730182445.ted.harding@nessie.mcc.ac.uk>
References: <25934031.1185795647582.JavaMail.root@elwamui-mouette.atl.sa.earthlink.net>
	<XFMail.070730182445.ted.harding@nessie.mcc.ac.uk>
Message-ID: <644e1f320707301846k783a5556u4d0e76a06844fe65@mail.gmail.com>

FYI.  I used your script on a Windows machine with 1.5GHZ and using
the CYGWIN software that has the UNIX utilities.  The field as 1000
lines with 10,000 fields on each line.  Here is what it reported:

gawk 'BEGIN{FS=","}{print $(1) "," $(1000) "," $(1275) ","  $(5678)}'
< tempxx.txt > newdata.csv


real    0m0.806s
user    0m0.640s
sys     0m0.124s

So it took less than a second to process the file, so it still should
be pretty fast on windows.  BTW, the first run took 30 seconds of real
time due to the slow disk that I have.  The run above had the data
already cached in memory.


On 7/30/07, Ted Harding <ted.harding at nessie.mcc.ac.uk> wrote:
> On 30-Jul-07 11:40:47, Eric Doviak wrote:
> > [...]
>
> Sympathies for the constraints you are operating in!
>
> > The "Introduction to R" manual suggests modifying input files with
> > Perl. Any tips on how to get started? Would Perl Data Language (PDL) be
> > a good choice?  http://pdl.perl.org/index_en.html
>
> I've not used SIPP files, but itseems that they are available in
> "delimited" format, including CSV.
>
> For extracting a subset of fields (especially when large datasets may
> stretch RAM resources) I would use awk rather than perl, since it
> is a much lighter program, transparent to code for, efficient, and
> it will do that job.
>
> On a Linux/Unix system (see below), say I wanted to extract fields
> 1, 1000, 1275, .... , 5678 from a CSV file. Then the 'awk' line
> that would do it would look like
>
> awk '
>  BEGIN{FS=","}{print $(1) "," $(1000) "," $(1275) "," ... $(5678)
> ' < sippfile.csv > newdata.csv
>
> Awk reads one line at a tine, and does with it what you tell it to do.
> It will not be overcome by a file with an enormous number of lines.
> Perl would be similar. So long as one line fits comfortably into RAM,
> you would not be limited by file size (unless you're running out
> of disk space), and operation will be quick, even for very long
> lines (as an experiment, I just set up a file with 10,000 fields
> and 35 lines; awk output 6 selected fields from all 35 lines in
> about 1 second, on the 366MHz 128MB RAM machine I'm on at the
> moment. After transferring it to a 733MHz 512MB RAM machine, it was
> too quick to estimate; so I duplicated the lines to get a 363-line
> file, and now got those same fields out in a bit less than 1 second.
> So that's over 300 lines/second, 200,000 lines a minute, a million
> lines in 5 minutes; and all on rather puny hardware.).
>
> In practice, you might want to write a separate script which woould
> automatically create the necessary awk script (say if you supply
> the filed names, haing already coded the filed positions corresponding
> to filed names). You could exploit R's system() command to run the
> scripts from within R, and then load in the filtered data.
>
> > I wrote a script which loads large datasets a few lines at a time,
> > writes the dozen or so variables of interest to a CSV file, removes
> > the loaded data and then (via a "for" loop) loads the next few lines
> > .... I managed to get it to work with one of the SIPP core files,
> > but it's SLOOOOW.
>
> See above ...
>
> > Worse, if I discover later that I omitted a relevant variable,
> > then I'll have to run the whole script all over again.
>
> If the script worked quickly (as with awk), presumably you
> wouldn't mind so much?
>
> Regarding Linux/Unix versus Windows. It is general experience
> that Linux/Unix works faster, more cleanly and efficiently, and
> often more reliably, for similar tasks; and cam do so on low grade
> hardware. Also, these systems come with dozens of file-processing
> utilities (including perl and awk; also many others), each of which
> has been written to be efficient at precisely the repertoire of
> tasks it was designed for. A lot of Windows sotware carries a huge
> overhead of either cosmetic dross, or a pantechnicon of functionality
> of which you are only going to need 0.01% at any one time.
>
> The Unix utilities have been ported to Windows, long since, but
> I have no experience of using them in that environment. Others,
> who have, can advise! But I'd seriously suggest getting hold of them.
>
> Hoping this helps,
> Ted.
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <ted.harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 30-Jul-07                                       Time: 18:24:41
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From jholtman at gmail.com  Tue Jul 31 03:50:32 2007
From: jholtman at gmail.com (jim holtman)
Date: Mon, 30 Jul 2007 21:50:32 -0400
Subject: [R] Matrix Multiplication, Floating-Point, etc.
In-Reply-To: <BAY108-F372CBDB1B7D1A023C189ACAAEE0@phx.gbl>
References: <Pine.LNX.4.64.0707300927250.28546@tajo.ucsd.edu>
	<BAY108-F372CBDB1B7D1A023C189ACAAEE0@phx.gbl>
Message-ID: <644e1f320707301850t167a4e1ege0b0a07755e2b870@mail.gmail.com>

One thing to realize is that although it appears that the operations
are the same, the code that is being executed is different in the two
cases.  Due to the different sequence of instructions, there may be
round-off errors that are then introduced

On 7/30/07, Talbot Katz <topkatz at msn.com> wrote:
> Thank you for responding!
>
> I realize that floating point operations are often inexact, and indeed, the
> difference between the two answers is within the all.equal tolerance, as
> mentioned in FAQ 7.31 (cited by Charles):
>
> >(as.numeric(ev1%*%ev2))==(sum(ev1*ev2))
> [1] FALSE
> >all.equal((as.numeric(ev1%*%ev2)),(sum(ev1*ev2)))
> [1] TRUE
> >
>
> I suppose that's good enough for numerical computation.  But I was still
> surprised to see that matrix multiplication (ev1%*%ev2) doesn't give the
> exact right answer, whereas sum(ev1*ev2) does give the exact answer.  I
> would've expected them to perform the same two multiplications and one
> addition.  But I guess that's not the case.
>
> However, I did find that if I multiplied the two vectors by 10, making the
> entries integers (although the class was still "numeric" rather than
> "integer"), both computations gave equal answers of 0:
>
> >xf1<-10*ev1
> >xf2<-10*ev2
> >(as.numeric(xf1%*%xf2))==(sum(xf1*xf2))
> [1] TRUE
> >
>
> Perhaps the moral of the story is that one should exercise caution and keep
> track of significant digits.
>
> --  TMK  --
> 212-460-5430    home
> 917-656-5351    cell
>
>
>
> >From: "Charles C. Berry" <cberry at tajo.ucsd.edu>
> >To: Talbot Katz <topkatz at msn.com>
> >CC: r-help at stat.math.ethz.ch
> >Subject: Re: [R] Matrix Multiplication, Floating-Point, etc.
> >Date: Mon, 30 Jul 2007 09:27:42 -0700
> >
> >
> >
> >7.31 Why doesn't R think these numbers are equal?
> >
> >On Fri, 27 Jul 2007, Talbot Katz wrote:
> >
> >>Hi.
> >>
> >>I recently tried the following in R 2.5.1 on Windows XP:
> >>
> >>>ev2<-c(0.8,-0.6)
> >>>ev1<-c(0.6,0.8)
> >>>ev1%*%ev2
> >>              [,1]
> >>[1,] -2.664427e-17
> >>>sum(ev1*ev2)
> >>[1] 0
> >>>
> >>
> >>(I got the same result with R 2.4.1 on a different Windows XP machine.)
> >>
> >>I expect this issue is very familiar and probably has been discussed in
> >>this
> >>forum before.  Can someone please point me to some documentation or
> >>discussion about this?  Is there some standard way to get the "correct"
> >>answer from %*%?
> >>
> >>Thanks!
> >>
> >>--  TMK  --
> >>212-460-5430  home
> >>917-656-5351  cell
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >Charles C. Berry                            (858) 534-2098
> >                                             Dept of Family/Preventive
> >Medicine
> >E mailto:cberry at tajo.ucsd.edu              UC San Diego
> >http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From m_olshansky at yahoo.com  Tue Jul 31 04:06:22 2007
From: m_olshansky at yahoo.com (Moshe Olshansky)
Date: Mon, 30 Jul 2007 19:06:22 -0700 (PDT)
Subject: [R] Matrix Multiplication, Floating-Point, etc.
In-Reply-To: <BAY108-F372CBDB1B7D1A023C189ACAAEE0@phx.gbl>
Message-ID: <576316.21115.qm@web32207.mail.mud.yahoo.com>

After multiplication by 10 you get 6*8 = 48 - the
result is an exact machine number so there is no
roundoff, while 0.6*0.8 = 0.48, where neither of the 3
numbers (0.6, 0.8, 0.48) is an exact machine mumber.
However, (-0.6)*0.8 should be equal EXACTLY to
-(0.6*0.8), and in fact you get that sum(ev1*ev2) is
exactly 0.
What is strange is that you are not getting this
result from ev1 %*% ev2. This means that either %^%
uses some non-straightforward algorithm or it somehow
sets the rounding control to something different from
"round to nearest". In the later case (-0.6) does not
necessarily equal to -(0.6) and the rounding after
multiplication is not necessarily symetric.

Regards,

Moshe.

--- Talbot Katz <topkatz at msn.com> wrote:

> Thank you for responding!
> 
> I realize that floating point operations are often
> inexact, and indeed, the 
> difference between the two answers is within the
> all.equal tolerance, as 
> mentioned in FAQ 7.31 (cited by Charles):
> 
> >(as.numeric(ev1%*%ev2))==(sum(ev1*ev2))
> [1] FALSE
> >all.equal((as.numeric(ev1%*%ev2)),(sum(ev1*ev2)))
> [1] TRUE
> >
> 
> I suppose that's good enough for numerical
> computation.  But I was still 
> surprised to see that matrix multiplication
> (ev1%*%ev2) doesn't give the 
> exact right answer, whereas sum(ev1*ev2) does give
> the exact answer.  I 
> would've expected them to perform the same two
> multiplications and one 
> addition.  But I guess that's not the case.
> 
> However, I did find that if I multiplied the two
> vectors by 10, making the 
> entries integers (although the class was still
> "numeric" rather than 
> "integer"), both computations gave equal answers of
> 0:
> 
> >xf1<-10*ev1
> >xf2<-10*ev2
> >(as.numeric(xf1%*%xf2))==(sum(xf1*xf2))
> [1] TRUE
> >
> 
> Perhaps the moral of the story is that one should
> exercise caution and keep 
> track of significant digits.
> 
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
> 
> 
> 
> >From: "Charles C. Berry" <cberry at tajo.ucsd.edu>
> >To: Talbot Katz <topkatz at msn.com>
> >CC: r-help at stat.math.ethz.ch
> >Subject: Re: [R] Matrix Multiplication,
> Floating-Point, etc.
> >Date: Mon, 30 Jul 2007 09:27:42 -0700
> >
> >
> >
> >7.31 Why doesn't R think these numbers are equal?
> >
> >On Fri, 27 Jul 2007, Talbot Katz wrote:
> >
> >>Hi.
> >>
> >>I recently tried the following in R 2.5.1 on
> Windows XP:
> >>
> >>>ev2<-c(0.8,-0.6)
> >>>ev1<-c(0.6,0.8)
> >>>ev1%*%ev2
> >>              [,1]
> >>[1,] -2.664427e-17
> >>>sum(ev1*ev2)
> >>[1] 0
> >>>
> >>
> >>(I got the same result with R 2.4.1 on a different
> Windows XP machine.)
> >>
> >>I expect this issue is very familiar and probably
> has been discussed in 
> >>this
> >>forum before.  Can someone please point me to some
> documentation or
> >>discussion about this?  Is there some standard way
> to get the "correct"
> >>answer from %*%?
> >>
> >>Thanks!
> >>
> >>--  TMK  --
> >>212-460-5430	home
> >>917-656-5351	cell
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide 
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained,
> reproducible code.
> >>
> >
> >Charles C. Berry                            (858)
> 534-2098
> >                                             Dept
> of Family/Preventive 
> >Medicine
> >E mailto:cberry at tajo.ucsd.edu	            UC San
> Diego
> >http://famprevmed.ucsd.edu/faculty/cberry/  La
> Jolla, San Diego 92093-0901
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained,
> reproducible code.
>


From res90sx5 at verizon.net  Tue Jul 31 04:05:58 2007
From: res90sx5 at verizon.net (Daniel Nordlund)
Date: Mon, 30 Jul 2007 19:05:58 -0700
Subject: [R] Matrix Multiplication, Floating-Point, etc.
In-Reply-To: <BAY108-F372CBDB1B7D1A023C189ACAAEE0@phx.gbl>
Message-ID: <013501c7d317$563dfcc0$0201a8c0@Aragorn>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch]
> On Behalf Of Talbot Katz
> Sent: Monday, July 30, 2007 10:55 AM
> To: cberry at tajo.ucsd.edu
> Cc: HDoran at air.org; r-help at stat.math.ethz.ch
> Subject: Re: [R] Matrix Multiplication, Floating-Point, etc.
> 
> Thank you for responding!
> 
> I realize that floating point operations are often inexact, and indeed, the
> difference between the two answers is within the all.equal tolerance, as
> mentioned in FAQ 7.31 (cited by Charles):
> 
> >(as.numeric(ev1%*%ev2))==(sum(ev1*ev2))
> [1] FALSE
> >all.equal((as.numeric(ev1%*%ev2)),(sum(ev1*ev2)))
> [1] TRUE
> >
> 
> I suppose that's good enough for numerical computation.  But I was still
> surprised to see that matrix multiplication (ev1%*%ev2) doesn't give the
> exact right answer, whereas sum(ev1*ev2) does give the exact answer.  I
> would've expected them to perform the same two multiplications and one
> addition.  But I guess that's not the case.
> 
> However, I did find that if I multiplied the two vectors by 10, making the
> entries integers (although the class was still "numeric" rather than
> "integer"), both computations gave equal answers of 0:
> 
> >xf1<-10*ev1
> >xf2<-10*ev2
> >(as.numeric(xf1%*%xf2))==(sum(xf1*xf2))
> [1] TRUE
> >
> 
> Perhaps the moral of the story is that one should exercise caution and keep
> track of significant digits.
> 
> --  TMK  --
> 212-460-5430	home
> 917-656-5351	cell
> 
There may other issues involved here besides R version, floating point precision, and OS version.  On my WinXP system running R-2.5.1 binary from CRAN, I get what you expected:

> ev2<-c(0.8,-0.6)
> ev1<-c(0.6,0.8)
> ev1%*%ev2
     [,1]
[1,]    0
>

There could be differences in OS release, service packs installed, cpu, etc.  But the moral you draw is probably a reasonable one.  

Dan

Daniel Nordlund
Bothell, WA


From ggrothendieck at gmail.com  Tue Jul 31 04:28:41 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 30 Jul 2007 22:28:41 -0400
Subject: [R] zoo, ts and data.frames
In-Reply-To: <2d1ebb110707301208y399e3a14ga4f208ed853a55ba@mail.gmail.com>
References: <2d1ebb110707301208y399e3a14ga4f208ed853a55ba@mail.gmail.com>
Message-ID: <971536df0707301928n784a1betcc669565bdba8681@mail.gmail.com>

zoo already can handle multivariate data so you can just use
that directly rather than using data frames.

See the examples in ?plot.zoo and ?xyplot.zoo



On 7/30/07, Edna Bell <edna.bell01 at gmail.com> wrote:
> Hi R gurus:
>
> I have some zoo objects that I have put into a data.frame.
>
> However, when I try to plot the objects from the data frame, the x
> axis is now Index rather than time
>
> Is there a sort of zoo data frame or a multiple zoo object, please?
>
> Thanks for any help!
>
> Sincerely,
> Edna
> edna.bell01 at gmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From edd at debian.org  Tue Jul 31 04:31:20 2007
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 31 Jul 2007 02:31:20 +0000
Subject: [R] filenames
In-Reply-To: <989527a20707301258k43a492d9lb3bf9dd2723bb7c9@mail.gmail.com>
References: <989527a20707301258k43a492d9lb3bf9dd2723bb7c9@mail.gmail.com>
Message-ID: <20070731023120.GA6909@master.debian.org>

On Mon, Jul 30, 2007 at 09:58:27PM +0200, Dong GUO ?? wrote:
> I want to create filename from a loop, say, i=(1:10), and the final names
> will be file1.csv, file2.csv, file3.csv
> 
> in Python, it seems easer as -  "file"+str(i) +'.csv , but how can i do in R
> 
> > list = as.character(1:10)
> > list
>  [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10"

> myfiles <- paste("foo", 1:10, sep="")
> myfiles
 [1] "foo1"  "foo2"  "foo3"  "foo4"  "foo5"  "foo6"  "foo7"  "foo8"  "foo9" 
[10] "foo10"

Also see 

	help(file.path)

which helps you stick directories and files together in a portable and
non-OS-dependent fashion.

Dirk

-- 
Three out of two people have difficulties with fractions.


From ripley at stats.ox.ac.uk  Tue Jul 31 04:59:31 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 03:59:31 +0100 (BST)
Subject: [R] how to install rattle() in R-GUI2.5.1
In-Reply-To: <b4485c4c0707300939u1f1221b5ha58aac3404749b41@mail.gmail.com>
References: <b4485c4c0707300939u1f1221b5ha58aac3404749b41@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0707310354460.3851@gannet.stats.ox.ac.uk>

1) What is R-GUI?  And what is R-GUI2.4.4?

2) In R 2.5.1 under Windows this works if you choose a properly populated 
CRAN mirror.  I think http://cran.au.r-project.org has access problems.

On Tue, 31 Jul 2007, j.joshua thomas wrote:

> Dear Group,
>
> I have installed R-GUI 2.5.1, thereby i tried install
>
>> install.packages("RGtk2")
>> install.packages ("rattle")
>
> where i have problems such as below:
>
>> install.packages("RGtk2")
> Warning: unable to access index for repository
> http://cran.au.r-project.org/bin/windows/contrib/2.5
> Warning message:
> package 'RGtk2' is not available
>> install.packages("rattle")
> Warning: unable to access index for repository
> http://cran.au.r-project.org/bin/windows/contrib/2.5
> Warning message:
> package 'rattle' is not available
>
> which library has rattle() i have used the above method in R-GUI2.4.4
>
> need some help!
> Thanks in advance
> JJ
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jul 31 05:16:02 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 04:16:02 +0100 (BST)
Subject: [R] Prompt comes too late
In-Reply-To: <46ACCCC7020000EE00003F9B@valhall.syh.fi>
References: <46ACCCC7020000EE00003F9B@valhall.syh.fi>
Message-ID: <Pine.LNX.4.64.0707310413420.3851@gannet.stats.ox.ac.uk>

Using print() for a 'prompt' is rather unusual: people normally use 
message() or cat().

You haven't told us your OS, but if this is Windows, see rw-FAQ Q7.1.


On Sun, 29 Jul 2007, Ralf Finne wrote:

> Hi R fans
>
> I am trying to make a program to ask the user to choose data file:
>
> print("Choose data file please !")
>
> matr=read.table(file.choose(),dec=".",header=TRUE)
>
> The problem is that the prompt
> Choose data file please !
> comes after I have chosen the file.
>
> What  am doing wrong?
>
> Thanks in advance
> Ralf Finne
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jul 31 05:50:59 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 04:50:59 +0100 (BST)
Subject: [R] Creating an instance of R from MS Access?
In-Reply-To: <loom.20070730T221111-30@post.gmane.org>
References: <428638.87453.qm@web56609.mail.re3.yahoo.com>
	<loom.20070730T221111-30@post.gmane.org>
Message-ID: <Pine.LNX.4.64.0707310448001.11691@gannet.stats.ox.ac.uk>

On Mon, 30 Jul 2007, D L McArthur wrote:

> Felipe Carrillo <mazatlanmexico <at> yahoo.com> writes:
>> Hi all:
>> Does anyone know if it's at all possible to create a
>> connection to R from MS access? ...
>>
> See function "odbcConnectAccess" in package RODBC.

That is the other way, from R to an Access database.  I think Felipe 
Carrillo is looking for the DCOM interfaces: see

http://cran.r-project.org/contrib/extra/dcom/00ReadMe.html

(and also on www.omegahat.org, but I'd not recommend that one for 
beginners).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From gyadav at ccilindia.co.in  Tue Jul 31 07:18:19 2007
From: gyadav at ccilindia.co.in (gyadav at ccilindia.co.in)
Date: Tue, 31 Jul 2007 10:48:19 +0530
Subject: [R] help with ROC curve
In-Reply-To: <4BC833C82CD3564298A3A24BD299C0C8B8510B@pioneer.brickworkindia.
	local>
Message-ID: <OFB723D926.FE376305-ON65257329.001C4A69-65257329.001D2537@ccilindia.co.in>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/caf971e8/attachment.pl 

From PAlspach at hortresearch.co.nz  Tue Jul 31 07:17:29 2007
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Tue, 31 Jul 2007 17:17:29 +1200
Subject: [R] simple coding question
Message-ID: <EC0F8FF776F3F74E9C63CE16641C9628022BDB3B@AKLEXB02.hort.net.nz>


Kirsten

One way to do this:

kirsten <- c(123, 1234, 12345)
100*as.numeric(paste(substring(kirsten, 1, 3), substring(kirsten, 4, 5),
sep='.'))

HTH ........

Peter Alspach
  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kirsten Beyer
> Sent: Tuesday, 31 July 2007 9:31 a.m.
> To: r-help at stat.math.ethz.ch
> Subject: [R] simple coding question
> 
> I have a list of ICD9 (disease) codes with various formats - 3 digit,
> 4 digit, 5 digit.  The first three digits of these codes are 
> what I am most interested in.  I would like to either add 
> zeros to the 3 and 4 digit codes to make them 5 digit codes 
> or add decimal points to put them all in the format ###.##.  
> I did not see a function that allows me to do this in the 
> formatting command.  This seems simple - can someone help?
> 
> Thanks,
> K.Beyer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}


From wl2776 at gmail.com  Tue Jul 31 07:56:08 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 31 Jul 2007 09:56:08 +0400
Subject: [R] Call R program from C++ code
In-Reply-To: <002b01c7d2d6$fa965920$efc30b60$@com>
References: <000001c7d220$0f4735e0$2dd5a1a0$@edu>
	<11860280.post@talk.nabble.com> <002b01c7d2d6$fa965920$efc30b60$@com>
Message-ID: <721501985.20070731095608@gmail.com>

Hi Feng,

You cannot compile R scripts as a shared library.
You can compile the R engine itself (interpreter and some basic functions) as
.dll (or .so), dynamically load it during run time of your application
and call its low level functions.

All details are in the "Writing R extensions" manual, installed with
R (%R_HOME%/doc/R-exts.pdf).

Searching Google with "embedding R" gives the following (outdated) link
http://developer.r-project.org/embedded.html
This says basic things and references the "Writing R Extensions"
manual.

You can see the Rgnome sources for example.

Monday, July 30, 2007, 10:25:16 PM, you wrote:

K> Hi Vladimir:
K> 	Thank you very much! I'm very interested in this sentence: " R can
K> be compiled as a shared library object, that you can dynamically load from
K> your application and use its functions." That's would be great if R programs
K> can be "compiled" as shared library object(.lib? or .dll ? ) Can you please
K> give more details about that? 

K> Best,
K> Feng

K> -----Original Message-----
K> From: r-help-bounces at stat.math.ethz.ch
K> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vladimir Eremeev
K> Sent: 2007??7??30?? 5:16
K> To: r-help at stat.math.ethz.ch
K> Subject: Re: [R] Call R program from C++ code


K> "Writing R Extensions" manual contains chapters dedicated to parsing and
K> evaluating of the R extensions from C.
K> Also, I vaguely remember I've seen something like "Embedding R" somewhere in
K> manuals.
K> R can be compiled as a shared library object, that you can dynamically load
K> from your application and use its functions.
K> R doesn't have a compiler, it's the interpreted language. 
K> However, it can parse a character string representing an expresstion and
K> transform it into the internal form, ready for evaluation.


K> Feng Qiu wrote:
>> 
>> Hi All:
>> 
>>                I'm developing an application program using C++. From my
>> C++
>> code, I would call some R program I have written. I' wondering if R
>> provide
>> some compiler that can compile R program into executable program. I
>> searched
>> R-help, there are a lot of posts talking about writing C++ code in R
>> program, but few about calling R from C++. 
>> 
>>                I might be wrong that R doesn't have complier. What I'm
>> trying to do is to call R program from C++ code. Any help is highly
>> appreciated!
>> 




-- 
Best regards,
 Vladimir                            mailto:wl2776 at gmail.com


From sheck at ucar.edu  Tue Jul 31 01:05:37 2007
From: sheck at ucar.edu (Sherri Heck)
Date: Mon, 30 Jul 2007 17:05:37 -0600
Subject: [R] deseasonalizing package?
Message-ID: <46AE6EC1.7000507@ucar.edu>

Hi all-

I am looking for a way to calculate and subtract out a seasonal trend 
(i.e. carbon dioxide). Does anyone know if there is a package that does 
something similar?

thanks!

sherri


From david at watermarkgroup.com  Mon Jul 30 20:54:32 2007
From: david at watermarkgroup.com (David Rowntree)
Date: Mon, 30 Jul 2007 14:54:32 -0400
Subject: [R] Reading T and F as strings
Message-ID: <46AE33E8.6070900@watermarkgroup.com>

Hi,

I am trying to read a series of stock tickers into R, and I'm running 
into trouble with Ford (F) and the old AT&T (T).  Read.table seems to 
interpret these as boolean values instead of strings, even when I set 
colColumns to a vector of character(0)'s.  Is there a way to convince it 
to read them as strings?

Thanks in advance for the help,
David


From niederlein-rstat at yahoo.de  Tue Jul 31 09:06:26 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Tue, 31 Jul 2007 09:06:26 +0200
Subject: [R] how to combine data of several csv-files
In-Reply-To: <644e1f320707301214y437b31dfyaf241cafba9a32de@mail.gmail.com>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>	
	<46ADD4F1.8090901@yahoo.de>
	<644e1f320707301214y437b31dfyaf241cafba9a32de@mail.gmail.com>
Message-ID: <46AEDF72.8080402@yahoo.de>

Hi Jim,

that's exactly what I'm looking for. Thank you so much. I think, I should look 
for some further documentation on list handling.
Many thanks also to "ottorino-luca.pantani at unifi.it" ;) for spending time in 
finding a solution...

Have a nice day!
Antje


jim holtman schrieb:
> This should do it:
> 
>> v1 <- NA
>> v2 <- rnorm(6)
>> v3 <- rnorm(6)
>> v4 <- rnorm(6)
>> v5 <- rnorm(6)
>> v6 <- rnorm(6)
>> v7 <- rnorm(6)
>> v8 <- rnorm(6)
>> v8 <- NA
>>
>> list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
>> categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)
>>
>> # create partitioned list
>> list.cat <- split(list, categ)
>> # combine each partition into a matrix
>> list.mat <- lapply(list.cat, function(x) do.call('rbind', x))
>> # now take the means of each column
>> lapply(list.mat, colMeans)
> $cat1
> [1] -0.5699080  0.3855693  1.1051809  0.2379324  0.6684713  0.3240003
> 
> $cat2
> [1]  0.38160462 -0.10559496 -0.40963090 -0.09507354  0.95021406 -0.31491450
> 
> 
> 
> On 7/30/07, Antje <niederlein-rstat at yahoo.de> wrote:
>> okay, I played a bit around and now I have some kind of testcase for you:
>>
>> v1 <- NA
>> v2 <- rnorm(6)
>> v3 <- rnorm(6)
>> v4 <- rnorm(6)
>> v5 <- rnorm(6)
>> v6 <- rnorm(6)
>> v7 <- rnorm(6)
>> v8 <- rnorm(6)
>> v8 <- NA
>>


>> list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
>> categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)
>>
>>  > list
>> [[1]]
>> [1] NA
>>
>> [[2]]
>> [1] -0.6442149 -0.2047012 -1.1986041 -0.2097442 -0.7343465 -1.3888750
>>
>> [[3]]
>> [1]  0.02354036 -1.36186952 -0.42197792  1.50445971 -1.76763996  0.53722404
>>
>> [[4]]
>> [1] -1.40362589  0.13045724 -0.84651458  1.57005071  0.06961015  0.25269771
>>
>> [[5]]
>> [1] -1.1829260  2.1411553 -0.1327081 -0.1053442 -0.8179396 -1.2342698
>>
>> [[6]]
>> [1]  1.17099178  0.49248118 -0.18690065  1.50050976 -0.65552410 -0.01243247
>>
>> [[7]]
>> [1] -0.046778203 -0.233788840  0.443908897 -1.649740180  0.003991354 -0.228020092
>>
>> [[8]]
>> [1] NA
>>
>> now, I need the means (and sd) of element 1 of list[2],list[3],list[4] (because they belong to "cat1") and
>>
>> = mean(-0.6442149, 0.02354036, -1.40362589)
>>
>> the same for element 2 up to element 6 (--> I would the get a vector containing the means for "cat1")
>> the same for the vectors belonging to "cat2".
>>
>> does anybody now understand what I mean?
>>
>> Antje
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
>


From ggrothendieck at gmail.com  Tue Jul 31 09:11:13 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 Jul 2007 03:11:13 -0400
Subject: [R] Reading T and F as strings
In-Reply-To: <46AE33E8.6070900@watermarkgroup.com>
References: <46AE33E8.6070900@watermarkgroup.com>
Message-ID: <971536df0707310011g648e7203vb0795ac4a35ac14@mail.gmail.com>

Try this:

> Lines <- "Stock
+ T
+ F
+ "
> DF <- read.table(textConnection(Lines), header = TRUE,
+    colClasses = "character")
> class(DF$Stock)
[1] "character"


On 7/30/07, David Rowntree <david at watermarkgroup.com> wrote:
> Hi,
>
> I am trying to read a series of stock tickers into R, and I'm running
> into trouble with Ford (F) and the old AT&T (T).  Read.table seems to
> interpret these as boolean values instead of strings, even when I set
> colColumns to a vector of character(0)'s.  Is there a way to convince it
> to read them as strings?
>
> Thanks in advance for the help,
> David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wl2776 at gmail.com  Tue Jul 31 09:18:45 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 31 Jul 2007 00:18:45 -0700 (PDT)
Subject: [R] Prompt comes too late
In-Reply-To: <46ACCCC7020000EE00003F9B@valhall.syh.fi>
References: <46ACCCC7020000EE00003F9B@valhall.syh.fi>
Message-ID: <11919327.post@talk.nabble.com>


You can use 
choose.files(caption="Choose data file
please!",multi=FALSE,filters=Filters["All"])

This will show the prompt in the file selection dialog and also allows you
to restrict the list of possible choises with a filter.


Ralf Finne wrote:
> 
> I am trying to make a program to ask the user to choose data file:
> 
> print("Choose data file please !")
> 
> matr=read.table(file.choose(),dec=".",header=TRUE)
> 
> The problem is that the prompt 
> Choose data file please !   
> comes after I have chosen the file.
> 

-- 
View this message in context: http://www.nabble.com/Prompt-comes-too-late-tf4178913.html#a11919327
Sent from the R help mailing list archive at Nabble.com.


From raphi9 at gmx.net  Tue Jul 31 09:35:58 2007
From: raphi9 at gmx.net (Raphael Illi)
Date: Tue, 31 Jul 2007 09:35:58 +0200
Subject: [R] Kombination Regression mit AR/MA Prozess?
Message-ID: <46AEE65E.8030000@gmx.net>

Sehr geehrte Damen und Herren

Im Rahmen meiner Lizentiatsarbeit am Lehrstuhl Wirtschaftsgeschichte von 
Prof. Woitek arbeite ich mit der R Software um Einkommenselastizit?ten 
zu sch?tzen (log-log regressionen).
ich habe zuerst eine Elastizit?tensch?tzung gemacht "lm(lM~lPfPx+lY)".
Dank "dwtest(lM~lPfPx+lY)" wurde ich gewahr, dass die Residuen zu stark
autokorelliert sind.
Nach der Betrachtung der ACF und PACF Funktionen erscheint mir ein AR(1) 
am passendsten. deshalb hab ich "arima(x = lM, order =
c(1, 0, 0))" gesch?tzt. *Mit welchem Befehl* kann ich nun aber beides 
(log-log Regression und AR-oder MA-Prozess)
kombinieren und folgende Regressionsgleichung sch?tzen:
LogM~b0*Log(alpha)+b1*Log(Pf/Px)+b2*Log(Y)+b3*AR(1) ???

Besten Dank f?r Ihre prompte Hilfe.

Freundliche Gr?sse
Raphael Illi


From ripley at stats.ox.ac.uk  Tue Jul 31 09:46:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 08:46:36 +0100 (BST)
Subject: [R] deseasonalizing package?
In-Reply-To: <46AE6EC1.7000507@ucar.edu>
References: <46AE6EC1.7000507@ucar.edu>
Message-ID: <Pine.LNX.4.64.0707310838460.12979@gannet.stats.ox.ac.uk>

On Mon, 30 Jul 2007, Sherri Heck wrote:

> Hi all-
>
> I am looking for a way to calculate and subtract out a seasonal trend
> (i.e. carbon dioxide). Does anyone know if there is a package that does
> something similar?

Yes, package 'stats'. Try help.search("seasonal").

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jul 31 09:49:36 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 08:49:36 +0100 (BST)
Subject: [R] Reading T and F as strings
In-Reply-To: <46AE33E8.6070900@watermarkgroup.com>
References: <46AE33E8.6070900@watermarkgroup.com>
Message-ID: <Pine.LNX.4.64.0707310847060.12979@gannet.stats.ox.ac.uk>

On Mon, 30 Jul 2007, David Rowntree wrote:

> Hi,
>
> I am trying to read a series of stock tickers into R, and I'm running
> into trouble with Ford (F) and the old AT&T (T).  Read.table seems to
> interpret these as boolean values instead of strings, even when I set
> colColumns to a vector of character(0)'s.  Is there a way to convince it
> to read them as strings?

Set 'colClasses' to a vector of "character", just as the help file 
for read.table says.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Tue Jul 31 10:16:15 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 10:16:15 +0200
Subject: [R] R2WinBUGS more updates after model did not converge
In-Reply-To: <f8h2n1$4dt$1@sea.gmane.org>
References: <f8h2n1$4dt$1@sea.gmane.org>
Message-ID: <46AEEFCF.7050104@statistik.uni-dortmund.de>



toby909 at gmail.com wrote:
> After running a model for a while and seeing that it did not converge yet, how 
> can I continue to run, ie not starting anew, the model?
> 
> I know if I manually/interactively use winbugs, this is possible anytime, but 
> how can I do this in r2winbugs, so that my existing sim$sims.array and other 
> stuff in the object that bugs() returns gets extended?


This is not possible with R2WinBUGS. But you can use BRugs, which is 
much more flexible, has an improved interface and uses OpenBUGS.
It's CRAN version is based on OpenBUGS 2.x.y, but we (i.e. currently 
mainly Insightful people and myself) are on the way to release a new 
version based on OpenBUGS 3.x.y.

Uwe Ligges




> Thanks Toby
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Tue Jul 31 10:40:24 2007
From: petr.pikal at precheza.cz (Petr PIKAL)
Date: Tue, 31 Jul 2007 10:40:24 +0200
Subject: [R] Odp:  array loop
In-Reply-To: <989527a20707301511g71820657v7afbc59b5f01e22f@mail.gmail.com>
Message-ID: <OF8B2FB098.18254DD0-ONC1257329.002F45C8-C1257329.002F8319@precheza.cz>

Hi

as you say that the computing is part of a function than the best way to 
see what is hapenning is to use

debug(your.function)

see ?debug for options.

Regards

Petr
petr.pikal at precheza.cz

r-help-bounces at stat.math.ethz.ch napsal dne 31.07.2007 00:11:00:

> Dear all,
> 
> here are two arrays: region(26,31,8), nation(8)
> 
> I tried to get a new array, say, giGi(26,31,8)
> 
> giGi <- array(0,dim = c(region_dim))
> 
> for (i in (1:region_dim[3]))
> {
> giGi[,,i] = region[,,i]-nation[,i]
> 
> }
> 
> As the above is part of function, but results shows only giGi[,,1] has 
the
> right answers, all the others (giGi[,,2],..giGi[..8]) are zeros. I have
> checked array of region and nation, they are not zeros at all....
> 
> when I do manually, it is not the case, giGi has meanful numbers.
> 
> can some one tell me the trick in this process??
> 
> Many thanks in advance.
> Dong
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ottorino-luca.pantani at unifi.it  Tue Jul 31 10:40:01 2007
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Tue, 31 Jul 2007 10:40:01 +0200
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46AEDF72.8080402@yahoo.de>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
	<46ADD4F1.8090901@yahoo.de>
	<644e1f320707301214y437b31dfyaf241cafba9a32de@mail.gmail.com>
	<46AEDF72.8080402@yahoo.de>
Message-ID: <46AEF561.1000700@unifi.it>

> Hi Jim,
> that's exactly what I'm looking for. Thank you so much. I think, I 
> should look for some further documentation on list handling.
I think I will do the same...................
Thanks to Jim I learned "textConnection" and "rowMeans".

Jim, could you please go a step further and tell me how to use lapply to 
calculate
the sd instead of the mean of the same items?
I mean
sd(-0.6442149 0.02354036 -1.40362589)
sd(-1.1829260 1.17099178 -0.046778203)
sd(-0.2047012 -1.36186952 0.13045724)
etc

x <- read.table(textConnection("  v1 v2 v3  v4 v5  v6 v7 v8
NA -0.6442149  0.02354036 -1.40362589 -1.1829260  1.17099178 -0.046778203 NA
NA -0.2047012 -1.36186952  0.13045724  2.1411553  0.49248118 -0.233788840 NA
NA -1.1986041 -0.42197792 -0.84651458 -0.1327081 -0.18690065  0.443908897 NA
NA -0.2097442  1.50445971  1.57005071 -0.1053442  1.50050976 -1.649740180 NA
NA -0.7343465 -1.76763996  0.06961015 -0.8179396 -0.65552410  0.003991354 NA
NA -1.3888750  0.53722404  0.25269771 -1.2342698 -0.01243247 -0.228020092 NA"), header=TRUE)


From r.nieuwenhuis at student.ru.nl  Tue Jul 31 11:22:27 2007
From: r.nieuwenhuis at student.ru.nl (Rense Nieuwenhuis)
Date: Tue, 31 Jul 2007 11:22:27 +0200
Subject: [R] Extracting random parameters from summary lme and lmer
Message-ID: <E046EEDE-E65B-4BD6-9258-44B82F61D65B@student.ru.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/84f0a71e/attachment.pl 

From lterlemez at anadolu.edu.tr  Tue Jul 31 11:49:08 2007
From: lterlemez at anadolu.edu.tr (Levent TERLEMEZ)
Date: Tue, 31 Jul 2007 12:49:08 +0300
Subject: [R] Getting variable name used in function...
Message-ID: <57DAA4310C5E4722AE521930898845EA@anadolu.edu.tr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/87f4f20a/attachment.pl 

From ligges at statistik.uni-dortmund.de  Tue Jul 31 11:50:28 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 11:50:28 +0200
Subject: [R] A simple question about summary.glm
In-Reply-To: <11857514.post@talk.nabble.com>
References: <11857514.post@talk.nabble.com>
Message-ID: <46AF05E4.1060305@statistik.uni-dortmund.de>



michal33 wrote:
> Hello,
> 
> I am new to R and have tried to search similar questions but could not find
> exactly what I am looking for, but I apologize if the question was already
> asked.
> 
> I have 10 different treatments and want to know whether they affect the sex
> ratios of insect emergence. After running the glms I got this table:
> 
>       Df Deviance Resid. Df Resid. Dev      F   Pr(>F)   
> NULL                    133     9250.3                   
> sex    1    481.5       132     8768.9 7.7212 0.006314 **
> trt    9   1099.1       123     7669.7 1.9585 0.049780 * 
> 
> But now I would like to know WHICH of the treatments was significant. I
> tried to use Tukey test but for some reason it does not work. 
> My question is:
> I used the following function: 
>> summary(file.name, corr=F)
> and got the following table:
> Deviance Residuals: 
>     Min       1Q   Median       3Q      Max  
> -14.118   -4.808   -1.466    2.033   33.882  
> Coefficients:
>               Estimate Std. Error   t value Pr(>|t|)    
> (Intercept)  8.696e+00  1.893e+00     4.594 1.06e-05 ***
> sexm        -3.791e+00  1.364e+00    -2.779  0.00631 ** 
> trtccc      -1.050e+00  4.325e+00    -0.243  0.80859    
> trtcga3      2.450e+00  4.325e+00     0.566  0.57211    
> trtcga4     -2.300e+00  4.325e+00    -0.532  0.59584    
> trtg         1.550e+00  2.497e+00     0.621  0.53593    
> trtga4      -5.550e+00  4.325e+00    -1.283  0.20183    
> trtp         5.422e+00  2.566e+00     2.113  0.03658 *  
> trtpg       -1.850e+00  2.497e+00    -0.741  0.46019    
> trtw        -3.634e-17  2.497e+00 -1.46e-17  1.00000    
> trtwg       -3.750e+00  2.497e+00    -1.502  0.13573    
> 
> What do the stars  mean?


Well, you omitted the last lines from the output which include:

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


i.e. *** for values < 0.001; * for values in [0.01, 0.05) etc.

Uwe Ligges



  Is it the same as Tukey test that tells me which
> treatment is different from which? i.e. is trtp (with *) significantly
> different to the control (which, by the way do not appear in this list and I
> do not know why)? 
> 
> Thanks
> Michal
> 
>


From ligges at statistik.uni-dortmund.de  Tue Jul 31 11:52:10 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 11:52:10 +0200
Subject: [R] 'non-standard' folder names in R package
In-Reply-To: <BAY120-F1672E803D429C7BA726EC4C7EE0@phx.gbl>
References: <BAY120-F1672E803D429C7BA726EC4C7EE0@phx.gbl>
Message-ID: <46AF064A.1010607@statistik.uni-dortmund.de>



Tao Shi wrote:
> Hi list,
> 
> I use a .xml file for a function's demo in the R package I'm creating.  
> Since it doesn't belong to any of the 'standard' folders, i.e. those 
> mentioned in the 'Writing R Extension', I put it in a folder call 
> "myXML", much like the 'iris.xl' file in 'xls' folder from 'gdata' 
> package, for example.  After running R CMD build, I could see the .xml 
> file is in the ..tar.gz file.  However, after running R CMD INSTALL 
> -build, the file and the folder disappeared in both the .zip file and 
> the installed package.  (R CMD CHECK, of course, failed too before that, 
> as "myXML" can't be installed.)  Could anybody tell me what's the tricks 
> to keep those folders in my installation?  I'm using R-2.5.1 under WinXP.
> 
> Thank you very much!



If you want it to be in mypackage/myXML after installation, put it into 
mypackage/inst/myXML in the source package, as the manual you cited 
suggests.

Uwe Ligges




> ....Tao
> 
> _________________________________________________________________
> Need a brain boost? Recharge with a stimulating game. Play now! 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Tue Jul 31 11:55:04 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 11:55:04 +0200
Subject: [R] Getting variable name used in function...
In-Reply-To: <57DAA4310C5E4722AE521930898845EA@anadolu.edu.tr>
References: <57DAA4310C5E4722AE521930898845EA@anadolu.edu.tr>
Message-ID: <46AF06F8.4040908@statistik.uni-dortmund.de>



Levent TERLEMEZ wrote:
> Dear Users,
>     I am using functions for calculations in my study. I have two functions and one is calling the other two times one after another. But the called function deals with two different data object (matrix, data frame, etc.), so I could not make the second function output  data object-free (for example, ? would like to write csv files but could not give different file names) and I would like to get object name. Is it possible to get data object's name. And if it is possible, how can I assign this name as an new data object name with an extra addition, for example, data objects name+"2".
> 
> Thanks for your help.
> 
> Example:
> function1(var1,var2)
>     function2(var1)
>     function2(var2)
>     ...


Can you explain what you want in an example, please? I do not really 
undertsand what you are trying to do.

Uwe Ligges



> With my best regards,
> Levent TERLEMEZ.
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From p.dalgaard at biostat.ku.dk  Tue Jul 31 11:57:21 2007
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: Tue, 31 Jul 2007 11:57:21 +0200
Subject: [R] the large dataset problem
In-Reply-To: <XFMail.070730182445.ted.harding@nessie.mcc.ac.uk>
References: <XFMail.070730182445.ted.harding@nessie.mcc.ac.uk>
Message-ID: <46AF0781.60200@biostat.ku.dk>

(Ted Harding) wrote:
> On 30-Jul-07 11:40:47, Eric Doviak wrote:
>   
>> [...]
>>     
>
> Sympathies for the constraints you are operating in!
>
>   
>> The "Introduction to R" manual suggests modifying input files with
>> Perl. Any tips on how to get started? Would Perl Data Language (PDL) be
>> a good choice?  http://pdl.perl.org/index_en.html
>>     
>
> I've not used SIPP files, but itseems that they are available in
> "delimited" format, including CSV.
>
> For extracting a subset of fields (especially when large datasets may
> stretch RAM resources) I would use awk rather than perl, since it
> is a much lighter program, transparent to code for, efficient, and
> it will do that job.
>
> On a Linux/Unix system (see below), say I wanted to extract fields
> 1, 1000, 1275, .... , 5678 from a CSV file. Then the 'awk' line
> that would do it would look like
>
> awk '
>  BEGIN{FS=","}{print $(1) "," $(1000) "," $(1275) "," ... $(5678)
> ' < sippfile.csv > newdata.csv
>
> Awk reads one line at a tine, and does with it what you tell it to do.
>   
....

Yes, but notice that there are also options within R. If you use a 
carefully constructed colClasses= argument to 
read.table()/read.csv()/etc or what= argument to scan(), you don't get 
more columns than you ask for. The basic trick is to use "NULL" for each 
of the columns that you do NOT want, and preferably "numeric" or 
"character" or whatever for those that you want (NA lets read.table do 
it's usual trickery of guessing type from contents). However...
>   
>> I wrote a script which loads large datasets a few lines at a time,
>> writes the dozen or so variables of interest to a CSV file, removes
>> the loaded data and then (via a "for" loop) loads the next few lines
>> .... I managed to get it to work with one of the SIPP core files,
>> but it's SLOOOOW.
>>     
>
> See above ...
>
>   
Looking at the actual data files and data dictionaries (we're talking 
about http://www.bls.census.gov/sipp_ftp.html, right?), it looks like 
SIPP files are in a fixed-width format, which suggests that you might 
want  to employ read.fwf().  If you want to get really smart about it, 
extract the 'D' fields from the dictionary files

Try this

 dict <- readLines("ftp://www.sipp.census.gov/pub/sipp/2004/l04puw1d.txt")
 D.lines <- grep("^D ", dict)
 vdict <- read.table(con <- textConnection(dict[D.lines])); close(con)
 head(vdict)

a little bit of further fiddling and you have the list of field widths 
and variable names to feed to read.fwf(). Just subset the name list and 
set the field width negative for those variables that you wish to skip. 
Extracting value labels from the V fields looks like it could be done, 
but requires more thinking, especially where they straddle multiple 
lines (but hey, it's your job, not mine...)

    -Peter D.


From ligges at statistik.uni-dortmund.de  Tue Jul 31 11:58:32 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 11:58:32 +0200
Subject: [R] Problem with code
In-Reply-To: <7dacf7220707291354o1fd50389t9aac87d5669c1e95@mail.gmail.com>
References: <7dacf7220707291354o1fd50389t9aac87d5669c1e95@mail.gmail.com>
Message-ID: <46AF07C8.6070006@statistik.uni-dortmund.de>



Shawndelle Noble wrote:
> Hi I am having the following Warning message with this code:
> 
>> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file- reason 'No such file or directory' in: file(file, "r")
> 
> The files are present on a CD and USB key- I tried opening all the
> files-then running the script but, it makes no difference.

Then you are trying to open non-existing files or your mistyped the 
filenames. Simply check the filenames again.


> Basically....
> How do I ensure that R reads my files to perform functions- I have some
> files on CD and other on my a USB key.

By specifying the correct names.


> Also if I am using script that was already written, for new data - how do I
> know which info. in the existing script should be updated?

If the "script" (= function ?) is well written, you just have to give 
the filenames as arguments to the functions. If not, you have to look 
into the sources and generalize ...

Uwe Ligges



> Thanks
> faith1
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ligges at statistik.uni-dortmund.de  Tue Jul 31 12:04:45 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 12:04:45 +0200
Subject: [R] Fwd: Package manual examples - 'unexpected$undefined' errors
In-Reply-To: <acb54ab40707291347g35ad691ftabc52671ffb382ec@mail.gmail.com>
References: <acb54ab40707280303g4fc79bbyaf475eba094f8eb8@mail.gmail.com>
	<acb54ab40707291347g35ad691ftabc52671ffb382ec@mail.gmail.com>
Message-ID: <46AF093D.10702@statistik.uni-dortmund.de>

David Pain wrote:
> Bounced first time!
> 
> ---------- Forwarded message ----------
> From: David Pain <dsp0718 at googlemail.com>
> Date: 28-Jul-2007 11:03
> Subject: Package manual examples - 'unexpected$undefined' errors
> To: r-help at stat.math.ethz.ch
> 
> 
> Trying out an unfamiliar package, the natural thing is to use the
> examples given in the package's manual - hopefully, the writers of the
> package wouldn't include examples which didn't work!
> 
> Recently, though, I've been getting 'unexpected$undefined' error
> messages when doing this, despite having copy/pasted the text from the
> manual (taking out hard breaks on the way).

Well, we cannot know if "the package" does make problems (you haven't 
stated which one).


> Moreover, I've had error messages for commands which I've previously
> had work fine.
> 
> For instance, this from Zelig
> 
> z.out <? zelig(vote ~ race + educate, model = "logit", data = turnout)
> 
> has at different times worked fine and thrown up the error message.

So you tell us the some package influences Zelig's functionality, but 
how can we help if you do not tell us which one it is?

Uwe Ligges


> Any help gratefully received.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ripley at stats.ox.ac.uk  Tue Jul 31 12:10:14 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 11:10:14 +0100 (BST)
Subject: [R] Matrix Multiplication, Floating-Point, etc.
In-Reply-To: <576316.21115.qm@web32207.mail.mud.yahoo.com>
References: <576316.21115.qm@web32207.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.64.0707311008330.21293@auk.stats>

On Mon, 30 Jul 2007, Moshe Olshansky wrote:

> After multiplication by 10 you get 6*8 = 48 - the
> result is an exact machine number so there is no
> roundoff, while 0.6*0.8 = 0.48, where neither of the 3
> numbers (0.6, 0.8, 0.48) is an exact machine mumber.
> However, (-0.6)*0.8 should be equal EXACTLY to
> -(0.6*0.8), and in fact you get that sum(ev1*ev2) is
> exactly 0.
> What is strange is that you are not getting this
> result from ev1 %*% ev2. This means that either %^%
> uses some non-straightforward algorithm or it somehow
> sets the rounding control to something different from
> "round to nearest". In the later case (-0.6) does not
> necessarily equal to -(0.6) and the rounding after
> multiplication is not necessarily symetric.

Mr Olshansky seems unaware of the effects of extended-precision 
intermediate arithmetic on ix86 CPUs.

sum() does use a higher-precision accumulator (where available, including 
on Windows), but ev1*ev2 is done in R and so stored to basic precision. 
OTOH, %*% (sic) calls the BLAS routine dgemm and hence may accumulate in 
80-bit floating-point registers.  What result you get will depend on what 
compiler, compiler flags and BLAS is in use, but with the default 
reference BLAS it is very likely that some of the intermediate results are 
stored in FP registers to extended precision.

It is a simple experiment to confirm this: recompile the BLAS with 
-fforce-store and you do get 0 (at least on my Windows build system).

Let's see less speculation and more homework in future.


>
> Regards,
>
> Moshe.
>
> --- Talbot Katz <topkatz at msn.com> wrote:
>
>> Thank you for responding!
>>
>> I realize that floating point operations are often
>> inexact, and indeed, the
>> difference between the two answers is within the
>> all.equal tolerance, as
>> mentioned in FAQ 7.31 (cited by Charles):
>>
>>> (as.numeric(ev1%*%ev2))==(sum(ev1*ev2))
>> [1] FALSE
>>> all.equal((as.numeric(ev1%*%ev2)),(sum(ev1*ev2)))
>> [1] TRUE
>>>
>>
>> I suppose that's good enough for numerical
>> computation.  But I was still
>> surprised to see that matrix multiplication
>> (ev1%*%ev2) doesn't give the
>> exact right answer, whereas sum(ev1*ev2) does give
>> the exact answer.  I
>> would've expected them to perform the same two
>> multiplications and one
>> addition.  But I guess that's not the case.
>>
>> However, I did find that if I multiplied the two
>> vectors by 10, making the
>> entries integers (although the class was still
>> "numeric" rather than
>> "integer"), both computations gave equal answers of
>> 0:
>>
>>> xf1<-10*ev1
>>> xf2<-10*ev2
>>> (as.numeric(xf1%*%xf2))==(sum(xf1*xf2))
>> [1] TRUE
>>>
>>
>> Perhaps the moral of the story is that one should
>> exercise caution and keep
>> track of significant digits.
>>
>> --  TMK  --
>> 212-460-5430	home
>> 917-656-5351	cell
>>
>>
>>
>>> From: "Charles C. Berry" <cberry at tajo.ucsd.edu>
>>> To: Talbot Katz <topkatz at msn.com>
>>> CC: r-help at stat.math.ethz.ch
>>> Subject: Re: [R] Matrix Multiplication,
>> Floating-Point, etc.
>>> Date: Mon, 30 Jul 2007 09:27:42 -0700
>>>
>>>
>>>
>>> 7.31 Why doesn't R think these numbers are equal?
>>>
>>> On Fri, 27 Jul 2007, Talbot Katz wrote:
>>>
>>>> Hi.
>>>>
>>>> I recently tried the following in R 2.5.1 on
>> Windows XP:
>>>>
>>>>> ev2<-c(0.8,-0.6)
>>>>> ev1<-c(0.6,0.8)
>>>>> ev1%*%ev2
>>>>              [,1]
>>>> [1,] -2.664427e-17
>>>>> sum(ev1*ev2)
>>>> [1] 0
>>>>>
>>>>
>>>> (I got the same result with R 2.4.1 on a different
>> Windows XP machine.)
>>>>
>>>> I expect this issue is very familiar and probably
>> has been discussed in
>>>> this
>>>> forum before.  Can someone please point me to some
>> documentation or
>>>> discussion about this?  Is there some standard way
>> to get the "correct"
>>>> answer from %*%?
>>>>
>>>> Thanks!
>>>>
>>>> --  TMK  --
>>>> 212-460-5430	home
>>>> 917-656-5351	cell

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Rosalba.Miceli at istitutotumori.mi.it  Tue Jul 31 12:13:12 2007
From: Rosalba.Miceli at istitutotumori.mi.it (Miceli Rosalba)
Date: Tue, 31 Jul 2007 12:13:12 +0200
Subject: [R] Cox model with interaction
Message-ID: <F8CDC0E621784946AC1E52AA6EC07E373B821E@exchsrvr02.int.milano>

Dear R users,
I am trying to fit a Cox model such as:

Surv(time,event) ~ X1+X1:X2

or 
Surv(time,event) ~ X1*X2 -X2, 

This code is working with coxph but not with cph (nor with psm), and the error message is:

Error in if (!length(fname) || !any(fname == zname)) { : 
        missing value where TRUE/FALSE is required

Any idea about the cause of the problem and how to manage it within the Design library?

Thanks a lot for your help.

Rosalba

Dr. Rosalba Miceli
Unit? di Statistica Medica e Biometria
Fondazione IRCCS Istituto Nazionale Tumori
Via G. Venezian, 1
20133 Milan (Italy)
Tel:????????????? +39 2 23903198 / 23902456
Fax:???????????? +39 2 23902095
E-Mail:??????? rosalba.miceli at istitutotumori.mi.it
<www.istitutotumori.mi.it/biometria>


From ligges at statistik.uni-dortmund.de  Tue Jul 31 12:13:45 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 12:13:45 +0200
Subject: [R] deriv; loop
In-Reply-To: <11853456.post@talk.nabble.com>
References: <11853456.post@talk.nabble.com>
Message-ID: <46AF0B59.7090608@statistik.uni-dortmund.de>

francogrex wrote:
> Hi, 2 questions:
> 
> Question 1: example of what I currently do:
> 
> for(i in 1:6){sink("temp.txt",append=TRUE)
> dput(i+0)
> sink()}
> x=scan(file="temp.txt")
> print(prod(x))
> file.remove("C:/R-2.5.0/temp.txt")
> 
> But how to convert the output of the loop to a vector that I can manipulate
> (by prod or sum etc), without having to write and append to a file?


So, do you want the file at the end or not? If not:

x <- 1:6
prod(x)

and if this is really the solution you want, the please read the posting 
guide and the manuals before posting again.




> Question 2:
> 
>> deriv(~gamma(x),"x")
> 
> expression({
>     .expr1 <- gamma(x)
>     .value <- .expr1
>     .grad <- array(0, c(length(.value), 1), list(NULL, c("x")))
>     .grad[, "x"] <- .expr1 * psigamma(x)
>     attr(.value, "gradient") <- .grad
>     .value
> })
> 
> BUT
> 
>> deriv3(~gamma(x),"x")
> Error in deriv3.formula(~gamma(x), "x") : Function 'psigamma' is not in the
> derivatives table
> 
> What I want is the expression for the second derivative (which I believe is
> trigamma(x), or psigamma(x,1)), how can I obtain that?


By using some algebraic software (rather than a numeric one) or 
contributing complete derivatives tables for the next R release.

Uwe Ligges


From jholtman at gmail.com  Tue Jul 31 12:18:52 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 31 Jul 2007 06:18:52 -0400
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46AEF561.1000700@unifi.it>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
	<46ADD4F1.8090901@yahoo.de>
	<644e1f320707301214y437b31dfyaf241cafba9a32de@mail.gmail.com>
	<46AEDF72.8080402@yahoo.de> <46AEF561.1000700@unifi.it>
Message-ID: <644e1f320707310318w53abb02ey2e3c274a7a730f0d@mail.gmail.com>

Here is the modified script for computing the 'sd':

v1 <- NA
v2 <- rnorm(6)
v3 <- rnorm(6)
v4 <- rnorm(6)
v5 <- rnorm(6)
v6 <- rnorm(6)
v7 <- rnorm(6)
v8 <- rnorm(6)
v8 <- NA

list <- list(v1,v2,v3,v4,v5,v6,v7,v8)
categ <- c(NA,"cat1","cat1","cat1","cat2","cat2","cat2",NA)

# create partitioned list
list.cat <- split(list, categ)
# combine each partition into a matrix
list.mat <- lapply(list.cat, function(x) do.call('rbind', x))
# now take the means of each column
lapply(list.mat, colMeans)
# compute the 'sd' by using 'apply' on the columns
lapply(list.mat, apply, 2, sd)



On 7/31/07, 8rino-Luca Pantani <ottorino-luca.pantani at unifi.it> wrote:
> > Hi Jim,
> > that's exactly what I'm looking for. Thank you so much. I think, I
> > should look for some further documentation on list handling.
> I think I will do the same...................
> Thanks to Jim I learned "textConnection" and "rowMeans".
>
> Jim, could you please go a step further and tell me how to use lapply to
> calculate
> the sd instead of the mean of the same items?
> I mean
> sd(-0.6442149 0.02354036 -1.40362589)
> sd(-1.1829260 1.17099178 -0.046778203)
> sd(-0.2047012 -1.36186952 0.13045724)
> etc
>
> x <- read.table(textConnection("  v1 v2 v3  v4 v5  v6 v7 v8
> NA -0.6442149  0.02354036 -1.40362589 -1.1829260  1.17099178 -0.046778203 NA
> NA -0.2047012 -1.36186952  0.13045724  2.1411553  0.49248118 -0.233788840 NA
> NA -1.1986041 -0.42197792 -0.84651458 -0.1327081 -0.18690065  0.443908897 NA
> NA -0.2097442  1.50445971  1.57005071 -0.1053442  1.50050976 -1.649740180 NA
> NA -0.7343465 -1.76763996  0.06961015 -0.8179396 -0.65552410  0.003991354 NA
> NA -1.3888750  0.53722404  0.25269771 -1.2342698 -0.01243247 -0.228020092 NA"), header=TRUE)
>
>
>
>
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From ligges at statistik.uni-dortmund.de  Tue Jul 31 12:20:44 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 12:20:44 +0200
Subject: [R] line widths of plotting symbols in the lattice
In-Reply-To: <576227.63887.qm@web39701.mail.mud.yahoo.com>
References: <576227.63887.qm@web39701.mail.mud.yahoo.com>
Message-ID: <46AF0CFC.3050508@statistik.uni-dortmund.de>



Stephen Tucker wrote:
> Dear List,
> 
> Sorry, this is very simple but I can't seem to find any information regarding
> line widths of plotting symbols in the lattice package.
> 
> For instance, in traditional graphics:
> 
>> plot(1:10,lwd=3)
>> points(10:1,lwd=2,col=3)
> 
> 'lwd' allows control of plotting symbol line widths.


'lwd' is documented in ?gpar (the help page does not show up for me, 
I'll take a closer look why) and works for me:

xyplot(1:10 ~ 1:10, type = "l", lwd = 5)


Uwe Ligges



> I've tried looking through the documentation for xyplot, panel.points,
> trellis.par.set, and the R-help archives. Maybe it goes by another name?
> 
> Thanks in advance,
> 
> Stephen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From eiaaw at exchange.lancs.ac.uk  Tue Jul 31 12:21:57 2007
From: eiaaw at exchange.lancs.ac.uk (Wilson, Andrew)
Date: Tue, 31 Jul 2007 11:21:57 +0100
Subject: [R] Plotting a smooth curve from predict
Message-ID: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>

Probably a very simple query:

When I try to plot a curve from a fitted polynomial, it comes out rather
jagged, not smooth like fitted curves in other stats software.  Is there
a way of getting a smooth curve in R?

What I'm doing at the moment (for the sake of example) is:

> x <- c(1,2,3,4,5,6,7,8,9,10)

> y <- c(10,9,8,7,6,6.5,7,8,9,10)

> b <- data.frame(cbind(x,y))

> w <- gls(y ~ I(x)+I(x^2),correlation=corARMA(p=1),method="ML",data=b)

> plot(predict(w),type="l")

Many thanks,

Andrew Wilson


From jmb at mssl.ucl.ac.uk  Tue Jul 31 12:03:25 2007
From: jmb at mssl.ucl.ac.uk (Jenny Barnes)
Date: Tue, 31 Jul 2007 11:03:25 +0100 (BST)
Subject: [R] Overlaying a single contour from a new data array in
	levelplot
Message-ID: <200707311003.l6VA3PC8014257@msslhb.mssl.ucl.ac.uk>

Dear Deepayan,

Thank you once again. I needed to install the latest versions of R and lattice 
and now it all works fine and the border is in white, which is perfect. Thank 
you for all the support you offer users of lattice,

Best Wishes,

Jenny

>That should have been fixed by now. Is there anything that's not
>working as you expect? My code had:
>
>   lapply(add.cl, panel.polygon, border = 'red')
>
>which should have made the borders red. If it doesn't, you probably
>need to upgrade to a recent version of R/lattice. If it does, changing
>it to border='white' should suffice. If that doesn't work, please
>provide a reproducible example.
>
>-Deepayan
>

>
>On 7/30/07, Jenny Barnes <jmb at mssl.ucl.ac.uk> wrote:
>> Dear Deepayan
>>
>> Thank you for your response - it has proved very very helpful, I can't thank 
you
>> enough!
>>
>> I have another question for you if you have time to reply. I know you have 
been
>> asked about the colour of the polygon outline before (27 April 2007) and you
>> replied that is a bug and the border can only be black or transparent...
>>
>> I was wondering if you have found a way to change the colour of the outline
>> since this correspondence? If not please can you tell me how to get around 
this
>> myself? You mentioned writing a replacement to lpolygon - I do not know how 
to
>> do this - would it be possible for you to guide me further?
>
>> I would really benefit from having the border of the polygon in white as it 
goes
>> over the "sea" which is also white and would therefore only be seen over the
>> "land", much neater!
>>
>> Many thanks,
>>
>> Jenny
>>
>>
>>
>> On 7/24/07, Jenny Barnes <jmb_at_mssl.ucl.ac.uk> wrote:
>> > Dear R-Help community,
>> >
>> > I am trying to overlay a single contour line over a correlation plot using
>> > levelplot in the lattice package. These are the two arrays:
>> >
>> > 1) a correlation plot over Africa - so each grid square is a different 
colour
>> > dependent on correlation - this is in an array: result_cor with 
dim[465,465]
>> >
>> > 2) a single contour line from a ***different data source*** - this is from
>> data
>> > related to the p-values for the above correlation plot - I want to overlay
>> only
>> > the 95% confidence contour. The p-values are stored in an array:
>> result.p.values
>> > with same dimensions as above.
>> >
>> > I have read about using panel.levelplot and panel.contourplot in the R-help
>> > mailing list but I don't know the right way to call two different data 
arrays,
>> > can anybody help me please? I appreciate your time and help with this
>> question.
>>
>> I can think of a couple of different ways, but the simplest will probably be 
to
>> compute the single contour beforehand and add it after the standard levelplot
>> using a panel function. E.g., using the 'volcano' data for both matrices:
>>
>> ## you need the explicit x and y arguments because ## the default is 
different
>> from levelplot.
>>
>> vcl <- contourLines(x = seq_len(nrow(volcano)),
>>
>>                    y = seq_len(ncol(volcano)),
>>                    z = volcano,
>>                    levels = c(172, 182))
>>
>> levelplot(volcano, add.cl = vcl,
>>           panel = function(..., add.cl) {
>>               panel.levelplot(...)
>>               lapply(add.cl, panel.polygon, border = 'red')
>>           })
>>
>>
>> -Deepayan ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> Jennifer Barnes
>> PhD student: long range drought prediction
>> Climate Extremes Group
>> Department of Space and Climate Physics
>> University College London
>> Holmbury St Mary
>> Dorking, Surrey, RH5 6NT
>> Web: http://climate.mssl.ucl.ac.uk
>>
>>

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Jennifer Barnes
PhD student: long range drought prediction 
Climate Extremes Group
Department of Space and Climate Physics
University College London
Holmbury St Mary 
Dorking, Surrey, RH5 6NT
Tel: 01483 204149
Mob: 07916 139187
Web: http://climate.mssl.ucl.ac.uk


From niederlein-rstat at yahoo.de  Tue Jul 31 12:58:15 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Tue, 31 Jul 2007 12:58:15 +0200
Subject: [R] how to combine data of several csv-files
In-Reply-To: <46AEF561.1000700@unifi.it>
References: <46AD9A99.2010303@yahoo.de> <46ADB142.9090502@unifi.it>
	<46ADD4F1.8090901@yahoo.de>
	<644e1f320707301214y437b31dfyaf241cafba9a32de@mail.gmail.com>
	<46AEDF72.8080402@yahoo.de> <46AEF561.1000700@unifi.it>
Message-ID: <46AF15C7.4060203@yahoo.de>

Hey,

I had the same question concerning the sd calculation and I got the following 
solution:

list <- split(list, class_vec)
list <- lapply(list, function(x) do.call('rbind', x))

my.mean <- lapply(ret, FUN = function(x) {
	t <- as.matrix(x)
	rm <- as.matrix(apply( t, 1, FUN = function(w) { length(which(is.na(w) == 
TRUE)) } ))
	t <- matrix(t[rm == 0,], ncol=7)

	# I had to do the 2 lines above to remove rows belonging to a class but 
containing NA values... (cannot exclude)

	if(nrow(t) > 1) {
		apply(t, 2, mean)
	} else {
		if(nrow(t) == 1) { as.vector(t) }
		else { NA }
	}
})


Probably, there is a simpler solution to remove the NA rows, but it works ;); 
also with "sd".

Ciao,
Antje


8rino-Luca Pantani schrieb:
>> Hi Jim,
>> that's exactly what I'm looking for. Thank you so much. I think, I 
>> should look for some further documentation on list handling.
> I think I will do the same...................
> Thanks to Jim I learned "textConnection" and "rowMeans".
> 
> Jim, could you please go a step further and tell me how to use lapply to 
> calculate
> the sd instead of the mean of the same items?
> I mean
> sd(-0.6442149 0.02354036 -1.40362589)
> sd(-1.1829260 1.17099178 -0.046778203)
> sd(-0.2047012 -1.36186952 0.13045724)
> etc
> 
> x <- read.table(textConnection("  v1 v2 v3  v4 v5  v6 v7 v8
> NA -0.6442149  0.02354036 -1.40362589 -1.1829260  1.17099178 
> -0.046778203 NA
> NA -0.2047012 -1.36186952  0.13045724  2.1411553  0.49248118 
> -0.233788840 NA
> NA -1.1986041 -0.42197792 -0.84651458 -0.1327081 -0.18690065  
> 0.443908897 NA
> NA -0.2097442  1.50445971  1.57005071 -0.1053442  1.50050976 
> -1.649740180 NA
> NA -0.7343465 -1.76763996  0.06961015 -0.8179396 -0.65552410  
> 0.003991354 NA
> NA -1.3888750  0.53722404  0.25269771 -1.2342698 -0.01243247 
> -0.228020092 NA"), header=TRUE)
> 
> 
> 
> 
>


From ccleland at optonline.net  Tue Jul 31 12:58:21 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 31 Jul 2007 06:58:21 -0400
Subject: [R] Extracting random parameters from summary lme and lmer
In-Reply-To: <E046EEDE-E65B-4BD6-9258-44B82F61D65B@student.ru.nl>
References: <E046EEDE-E65B-4BD6-9258-44B82F61D65B@student.ru.nl>
Message-ID: <46AF15CD.5050401@optonline.net>

Rense Nieuwenhuis wrote:
> LS,
> 
> I'm estimating multilevel regression models, using the lme-function  
> from the nlme-package. Let's say that I estimated a model and stored  
> it inside the object named 'model'. The summary of that model is  
> shown below:
> 
> Using summary(model)$tTable , I receive the following output:
> 
>  > summary(model)$tTable
>                      Value  Std.Error   DF     t-value       p-value
> (Intercept)    0.23268607 0.09350662 3990   2.4884449  1.287080e-02
> sexM          -0.15338225 0.03169762 3990  -4.8389206  1.354802e-06
> standLRT       0.38593558 0.01677195 3990  23.0107762 4.005182e-110
> vrmid 50%      0.07606394 0.09389376   61   0.8101064  4.210281e-01
> vrtop 25%      0.24561327 0.10483374   61   2.3428838  2.241317e-02
> intakemid 50% -0.41469716 0.03177240 3990 -13.0521199  3.698344e-38
> intaketop 25% -0.75920783 0.05357980 3990 -14.1696648  1.666780e-44
> typeSngl       0.15680532 0.07173835   61   2.1857949  3.267903e-02
> 
> 
> All looks fine to me. The output above is simply  a section from the  
> full summary shown below. Now, I want to extract from the summary (or  
> the full model) the part stating the random parameters. More  
> specifically, I want to extract from the summary the following:
> 
> (Intercept) 0.2869401 (Intr)
> typeSngl    0.2791040 -0.617
> Residual    0.7302233
> 
> How could this be done, and how can the same be done using the lmer- 
> function from the lme4-package?

?VarCorr

> library(nlme)

> fm1 <- lme(distance ~ age, data = Orthodont) # random is ~ age

> summary(fm1)
Linear mixed-effects model fit by REML
 Data: Orthodont
       AIC      BIC    logLik
  454.6367 470.6173 -221.3183

Random effects:
 Formula: ~age | Subject
 Structure: General positive-definite
            StdDev    Corr
(Intercept) 2.3270339 (Intr)
age         0.2264276 -0.609
Residual    1.3100399

Fixed effects: distance ~ age
                Value Std.Error DF   t-value p-value
(Intercept) 16.761111 0.7752461 80 21.620375       0
age          0.660185 0.0712533 80  9.265334       0
 Correlation:
    (Intr)
age -0.848

Standardized Within-Group Residuals:
         Min           Q1          Med           Q3          Max
-3.223106017 -0.493760867  0.007316632  0.472151090  3.916032743

Number of Observations: 108
Number of Groups: 27

> VarCorr(fm1)
Subject = pdSymm(age)
            Variance   StdDev    Corr
(Intercept) 5.41508688 2.3270339 (Intr)
age         0.05126947 0.2264276 -0.609
Residual    1.71620459 1.3100399

> library(lme4)

> (fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy))
Linear mixed-effects model fit by REML
Formula: Reaction ~ Days + (Days | Subject)
   Data: sleepstudy
  AIC  BIC logLik MLdeviance REMLdeviance
 1754 1770 -871.8       1752         1744
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 610.835  24.7151
          Days         35.056   5.9208  0.067
 Residual             655.066  25.5943
number of obs: 180, groups: Subject, 18

Fixed effects:
            Estimate Std. Error t value
(Intercept)  251.405      6.820   36.86
Days          10.467      1.546    6.77

Correlation of Fixed Effects:
     (Intr)
Days -0.137

> VarCorr(fm1)
$Subject
2 x 2 Matrix of class "dpoMatrix"
            (Intercept)      Days
(Intercept)  610.834546  9.738707
Days           9.738707 35.056337

attr(,"sc")
   scale
25.59426

> Thanks for the effort,
> 
> Rense Nieuwenhuis
> 
> Linear mixed-effects model fit by REML
>   Data: Exam
>        AIC      BIC   logLik
>    9158.56 9234.241 -4567.28
> 
> Random effects:
>   Formula: ~type | school
>   Structure: General positive-definite, Log-Cholesky parametrization
>              StdDev    Corr
> (Intercept) 0.2869401 (Intr)
> typeSngl    0.2791040 -0.617
> Residual    0.7302233
> 
> Fixed effects: normexam ~ sex + standLRT + vr + intake + type
>                     Value  Std.Error   DF    t-value p-value
> (Intercept)    0.2326861 0.09350662 3990   2.488445  0.0129
> sexM          -0.1533822 0.03169762 3990  -4.838921  0.0000
> standLRT       0.3859356 0.01677195 3990  23.010776  0.0000
> vrmid 50%      0.0760639 0.09389376   61   0.810106  0.4210
> vrtop 25%      0.2456133 0.10483374   61   2.342884  0.0224
> intakemid 50% -0.4146972 0.03177240 3990 -13.052120  0.0000
> intaketop 25% -0.7592078 0.05357980 3990 -14.169665  0.0000
> typeSngl       0.1568053 0.07173835   61   2.185795  0.0327
>   Correlation:
>                (Intr) sexM   stnLRT vrm50% vrt25% int50% int25%
> sexM          -0.201
> standLRT      -0.125  0.028
> vrmid 50%     -0.742  0.028 -0.035
> vrtop 25%     -0.652  0.051 -0.065  0.649
> intakemid 50% -0.246 -0.011  0.541 -0.002  0.007
> intaketop 25% -0.218 -0.018  0.676  0.014  0.013  0.660
> typeSngl      -0.421  0.080  0.007  0.033 -0.027 -0.001  0.001
> 
> Standardized Within-Group Residuals:
>          Min          Q1         Med          Q3         Max
> -3.59074329 -0.63776965  0.03829878  0.67303837  3.33952680
> 
> Number of Observations: 4059
> Number of Groups: 65
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ccleland at optonline.net  Tue Jul 31 13:15:16 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 31 Jul 2007 07:15:16 -0400
Subject: [R] Plotting a smooth curve from predict
In-Reply-To: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
References: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
Message-ID: <46AF19C4.6030006@optonline.net>

Wilson, Andrew wrote:
> Probably a very simple query:
> 
> When I try to plot a curve from a fitted polynomial, it comes out rather
> jagged, not smooth like fitted curves in other stats software.  Is there
> a way of getting a smooth curve in R?
> 
> What I'm doing at the moment (for the sake of example) is:
> 
>> x <- c(1,2,3,4,5,6,7,8,9,10)
> 
>> y <- c(10,9,8,7,6,6.5,7,8,9,10)
> 
>> b <- data.frame(cbind(x,y))
> 
>> w <- gls(y ~ I(x)+I(x^2),correlation=corARMA(p=1),method="ML",data=b)
> 
>> plot(predict(w),type="l")

  Make predictions for more than 10 values of x:

x <- c(1,2,3,4,5,6,7,8,9,10)

y <- c(10,9,8,7,6,6.5,7,8,9,10)

b <- data.frame(cbind(x,y))

library(nlme)

w <- gls(y ~ I(x)+I(x^2), correlation=corARMA(p=1), method="ML", data=b)

plot(seq(1,10,len=100),
     predict(w, data.frame(x = seq(1,10, len=100))),
     xlab="x", ylab="Predicted y",
     type="l")

> Many thanks,
> 
> Andrew Wilson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code. 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From ripley at stats.ox.ac.uk  Tue Jul 31 13:16:13 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 12:16:13 +0100 (BST)
Subject: [R] line widths of plotting symbols in the lattice
In-Reply-To: <46AF0CFC.3050508@statistik.uni-dortmund.de>
References: <576227.63887.qm@web39701.mail.mud.yahoo.com>
	<46AF0CFC.3050508@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.64.0707311211230.32268@gannet.stats.ox.ac.uk>

On Tue, 31 Jul 2007, Uwe Ligges wrote:

>
>
> Stephen Tucker wrote:
>> Dear List,
>>
>> Sorry, this is very simple but I can't seem to find any information regarding
>> line widths of plotting symbols in the lattice package.
>>
>> For instance, in traditional graphics:
>>
>>> plot(1:10,lwd=3)
>>> points(10:1,lwd=2,col=3)
>>
>> 'lwd' allows control of plotting symbol line widths.
>
>
> 'lwd' is documented in ?gpar (the help page does not show up for me,
> I'll take a closer look why) and works for me:
>
> xyplot(1:10 ~ 1:10, type = "l", lwd = 5)

lattice imports grid, but you will need library(grid) to see the help 
pages.  The link to gpar on ?xyplot should work (and does for me).

>
>
> Uwe Ligges
>
>
>
>> I've tried looking through the documentation for xyplot, panel.points,
>> trellis.par.set, and the R-help archives. Maybe it goes by another name?
>>
>> Thanks in advance,
>>
>> Stephen
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ligges at statistik.uni-dortmund.de  Tue Jul 31 13:17:08 2007
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Jul 2007 13:17:08 +0200
Subject: [R] Plotting a smooth curve from predict
In-Reply-To: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
References: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
Message-ID: <46AF1A34.6000105@statistik.uni-dortmund.de>



Wilson, Andrew wrote:
> Probably a very simple query:
> 
> When I try to plot a curve from a fitted polynomial, it comes out rather
> jagged, not smooth like fitted curves in other stats software.  Is there
> a way of getting a smooth curve in R?
> 
> What I'm doing at the moment (for the sake of example) is:
> 
>> x <- c(1,2,3,4,5,6,7,8,9,10)
> 
>> y <- c(10,9,8,7,6,6.5,7,8,9,10)
> 
>> b <- data.frame(cbind(x,y))
> 
>> w <- gls(y ~ I(x)+I(x^2),correlation=corARMA(p=1),method="ML",data=b)
> 
>> plot(predict(w),type="l")


predict() predicts at all locations in x and you are drawing straight 
lines between these points.
Hence you need to predict in another resolution, e.g.:

  dat <- data.frame(x = seq(1, 10, by = 0.1))
  plot(predict(w, newdata = dat), type="l")

Uwe Ligges


> Many thanks,
> 
> Andrew Wilson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gavin.simpson at ucl.ac.uk  Tue Jul 31 13:21:36 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 31 Jul 2007 12:21:36 +0100
Subject: [R] Plotting a smooth curve from predict
In-Reply-To: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
References: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
Message-ID: <1185880896.31913.3.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-07-31 at 11:21 +0100, Wilson, Andrew wrote:
> Probably a very simple query:
> 
> When I try to plot a curve from a fitted polynomial, it comes out rather
> jagged, not smooth like fitted curves in other stats software.  Is there
> a way of getting a smooth curve in R?
> 
> What I'm doing at the moment (for the sake of example) is:
> 
> > x <- c(1,2,3,4,5,6,7,8,9,10)
> 
> > y <- c(10,9,8,7,6,6.5,7,8,9,10)
> 
> > b <- data.frame(cbind(x,y))
> 
> > w <- gls(y ~ I(x)+I(x^2),correlation=corARMA(p=1),method="ML",data=b)
> 
> > plot(predict(w),type="l")

replace the line above with the following:

pred.dat <- data.frame(x = seq(min(x), max(x), length.out = 100))
plot(predict(w, pred.dat), type = "l")

The general idea is to produce predictions over the range of x, so we
produce a new data frame with component x, that contains 100 values from
min(x) to max(x). We then get predicted values for each of these new
values of the predictor in pred.dat, and plot them

Increase/decrease length.out to get something suitably smooth without
sending your computer into meltdown.

HTH

G

> 
> Many thanks,
> 
> Andrew Wilson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From ripley at stats.ox.ac.uk  Tue Jul 31 13:21:45 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 12:21:45 +0100 (BST)
Subject: [R] Plotting a smooth curve from predict
In-Reply-To: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
References: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
Message-ID: <Pine.LNX.4.64.0707311216320.32268@gannet.stats.ox.ac.uk>

You are using fitted() implicitly here, so you are not plotting a smooth 
curve but a set of fitted values.

You need to really predict at a suitable range of data points, e.g.

xx <- seq(1, 10, len=500)
plot(xx, predict(w, list(x=xx)), type="l")


BTW, why are you not using poly()?


On Tue, 31 Jul 2007, Wilson, Andrew wrote:

> Probably a very simple query:
>
> When I try to plot a curve from a fitted polynomial, it comes out rather
> jagged, not smooth like fitted curves in other stats software.  Is there
> a way of getting a smooth curve in R?
>
> What I'm doing at the moment (for the sake of example) is:
>
>> x <- c(1,2,3,4,5,6,7,8,9,10)
>
>> y <- c(10,9,8,7,6,6.5,7,8,9,10)
>
>> b <- data.frame(cbind(x,y))
>
>> w <- gls(y ~ I(x)+I(x^2),correlation=corARMA(p=1),method="ML",data=b)
>
>> plot(predict(w),type="l")
>
> Many thanks,
>
> Andrew Wilson
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Thierry.ONKELINX at inbo.be  Tue Jul 31 13:37:49 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 31 Jul 2007 13:37:49 +0200
Subject: [R] Plotting a smooth curve from predict
In-Reply-To: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
References: <C85AE225842FA040BE74817BDFD0AEEF02682857@exchange-be3.lancs.local>
Message-ID: <2E9C414912813E4EB981326983E0A104036A311F@inboexch.inbo.be>

What you need is

b <- data.frame(x = 1:10, y = c(10,9,8,7,6,6.5,7,8,9,10))
w <- gls(y ~ I(x)+I(x^2),correlation=corARMA(p=1),method="ML",data=b)
Newdata <- data.frame(x = seq(1, 10, length = 41))
plot(predict(w, newdata = Newdata), type="l")

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx op inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: r-help-bounces op stat.math.ethz.ch 
> [mailto:r-help-bounces op stat.math.ethz.ch] Namens Wilson, Andrew
> Verzonden: dinsdag 31 juli 2007 12:22
> Aan: r-help op stat.math.ethz.ch
> Onderwerp: [R] Plotting a smooth curve from predict
> 
> Probably a very simple query:
> 
> When I try to plot a curve from a fitted polynomial, it comes 
> out rather jagged, not smooth like fitted curves in other 
> stats software.  Is there a way of getting a smooth curve in R?
> 
> What I'm doing at the moment (for the sake of example) is:
> 
> > x <- c(1,2,3,4,5,6,7,8,9,10)
> 
> > y <- c(10,9,8,7,6,6.5,7,8,9,10)
> 
> > b <- data.frame(cbind(x,y))
> 
> > w <- gls(y ~ 
> I(x)+I(x^2),correlation=corARMA(p=1),method="ML",data=b)
> 
> > plot(predict(w),type="l")
> 
> Many thanks,
> 
> Andrew Wilson
> 
> ______________________________________________
> R-help op stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From niederlein-rstat at yahoo.de  Tue Jul 31 14:04:37 2007
From: niederlein-rstat at yahoo.de (Antje)
Date: Tue, 31 Jul 2007 14:04:37 +0200
Subject: [R] remove NA rows and columns
Message-ID: <46AF2555.4020603@yahoo.de>

Hello,

I guess, it's a rather simple thing but I cannot find a short way to reduce a 
matrix, removing all rows and columns having just NA elements.

testmatrix <- matrix(nrow=6, ncol=4)
testmatrix[2:5,2:3] <- seq(2)

 > testmatrix
      [,1] [,2] [,3] [,4]
[1,]   NA   NA   NA   NA
[2,]   NA    1    1   NA
[3,]   NA    2    2   NA
[4,]   NA    1    1   NA
[5,]   NA    2    2   NA
[6,]   NA   NA   NA   NA

the new matrix should look like this (by the way, I don't "know" which rows and 
columns are the one to be deleted...

 > testmatrix
      [,1] [,2]
[1,]   1    1
[2,]   2    2
[3,]   1    1
[4,]   2    2

Ciao,
Antje


From wl2776 at gmail.com  Tue Jul 31 14:16:00 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 31 Jul 2007 05:16:00 -0700 (PDT)
Subject: [R] remove NA rows and columns
In-Reply-To: <46AF2555.4020603@yahoo.de>
References: <46AF2555.4020603@yahoo.de>
Message-ID: <11923002.post@talk.nabble.com>


> testmatrix
     [,1] [,2] [,3] [,4]
[1,]   NA   NA   NA   NA
[2,]   NA    1    1   NA
[3,]   NA    2    2   NA
[4,]   NA    1    1   NA
[5,]   NA    2    2   NA
[6,]   NA   NA   NA   NA

> tm1<-testmatrix[,-which(apply(testmatrix,2,function(x)all(is.na(x))))]
> tm1
     [,1] [,2]
[1,]   NA   NA
[2,]    1    1
[3,]    2    2
[4,]    1    1
[5,]    2    2
[6,]   NA   NA

> tm2<-tm1[-which(apply(testmatrix,1,function(x)all(is.na(x)))),]
> tm2
     [,1] [,2]
[1,]    1    1
[2,]    2    2
[3,]    1    1
[4,]    2    2
 

Antje wrote:
> 
> I guess, it's a rather simple thing but I cannot find a short way to
> reduce a 
> matrix, removing all rows and columns having just NA elements.
> 
> testmatrix <- matrix(nrow=6, ncol=4)
> testmatrix[2:5,2:3] <- seq(2)
> 
>  > testmatrix
>       [,1] [,2] [,3] [,4]
> [1,]   NA   NA   NA   NA
> [2,]   NA    1    1   NA
> [3,]   NA    2    2   NA
> [4,]   NA    1    1   NA
> [5,]   NA    2    2   NA
> [6,]   NA   NA   NA   NA
> 
> the new matrix should look like this (by the way, I don't "know" which
> rows and 
> columns are the one to be deleted...
> 
>  > testmatrix
>       [,1] [,2]
> [1,]   1    1
> [2,]   2    2
> [3,]   1    1
> [4,]   2    2
> 

-- 
View this message in context: http://www.nabble.com/remove-NA-rows-and-columns-tf4192605.html#a11923002
Sent from the R help mailing list archive at Nabble.com.


From ccleland at optonline.net  Tue Jul 31 14:15:35 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 31 Jul 2007 08:15:35 -0400
Subject: [R] remove NA rows and columns
In-Reply-To: <46AF2555.4020603@yahoo.de>
References: <46AF2555.4020603@yahoo.de>
Message-ID: <46AF27E7.6010109@optonline.net>

Antje wrote:
> Hello,
> 
> I guess, it's a rather simple thing but I cannot find a short way to reduce a 
> matrix, removing all rows and columns having just NA elements.
> 
> testmatrix <- matrix(nrow=6, ncol=4)
> testmatrix[2:5,2:3] <- seq(2)
> 
>  > testmatrix
>       [,1] [,2] [,3] [,4]
> [1,]   NA   NA   NA   NA
> [2,]   NA    1    1   NA
> [3,]   NA    2    2   NA
> [4,]   NA    1    1   NA
> [5,]   NA    2    2   NA
> [6,]   NA   NA   NA   NA
> 
> the new matrix should look like this (by the way, I don't "know" which rows and 
> columns are the one to be deleted...
> 
>  > testmatrix
>       [,1] [,2]
> [1,]   1    1
> [2,]   2    2
> [3,]   1    1
> [4,]   2    2

testmatrix[!apply(is.na(testmatrix), 1, all),
           !apply(is.na(testmatrix), 2, all)]

     [,1] [,2]
[1,]    1    1
[2,]    2    2
[3,]    1    1
[4,]    2    2

> Ciao,
> Antje
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 


-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From wl2776 at gmail.com  Tue Jul 31 14:21:23 2007
From: wl2776 at gmail.com (Vladimir Eremeev)
Date: Tue, 31 Jul 2007 05:21:23 -0700 (PDT)
Subject: [R] remove NA rows and columns
In-Reply-To: <11923002.post@talk.nabble.com>
References: <46AF2555.4020603@yahoo.de> <11923002.post@talk.nabble.com>
Message-ID: <11923066.post@talk.nabble.com>


Or, these operations can be called in one command:

> testmatrix[-which(apply(testmatrix,1,function(x)all(is.na(x)))),-which(apply(testmatrix,2,function(x)all(is.na(x))))]
     [,1] [,2]
[1,]    1    1
[2,]    2    2
[3,]    1    1
[4,]    2    2
> 



Vladimir Eremeev wrote:
> 
>> testmatrix
>      [,1] [,2] [,3] [,4]
> [1,]   NA   NA   NA   NA
> [2,]   NA    1    1   NA
> [3,]   NA    2    2   NA
> [4,]   NA    1    1   NA
> [5,]   NA    2    2   NA
> [6,]   NA   NA   NA   NA
> 
>> tm1<-testmatrix[,-which(apply(testmatrix,2,function(x)all(is.na(x))))]
>> tm1
>      [,1] [,2]
> [1,]   NA   NA
> [2,]    1    1
> [3,]    2    2
> [4,]    1    1
> [5,]    2    2
> [6,]   NA   NA
> 
>> tm2<-tm1[-which(apply(testmatrix,1,function(x)all(is.na(x)))),]
>> tm2
>      [,1] [,2]
> [1,]    1    1
> [2,]    2    2
> [3,]    1    1
> [4,]    2    2
>  
> 
> Antje wrote:
>> 
>> I guess, it's a rather simple thing but I cannot find a short way to
>> reduce a 
>> matrix, removing all rows and columns having just NA elements.
>> 
>> testmatrix <- matrix(nrow=6, ncol=4)
>> testmatrix[2:5,2:3] <- seq(2)
>> 
>>  > testmatrix
>>       [,1] [,2] [,3] [,4]
>> [1,]   NA   NA   NA   NA
>> [2,]   NA    1    1   NA
>> [3,]   NA    2    2   NA
>> [4,]   NA    1    1   NA
>> [5,]   NA    2    2   NA
>> [6,]   NA   NA   NA   NA
>> 
>> the new matrix should look like this (by the way, I don't "know" which
>> rows and 
>> columns are the one to be deleted...
>> 
>>  > testmatrix
>>       [,1] [,2]
>> [1,]   1    1
>> [2,]   2    2
>> [3,]   1    1
>> [4,]   2    2
>> 
> 
> 

-- 
View this message in context: http://www.nabble.com/remove-NA-rows-and-columns-tf4192605.html#a11923066
Sent from the R help mailing list archive at Nabble.com.


From raphi9 at gmx.net  Tue Jul 31 10:23:42 2007
From: raphi9 at gmx.net (Raphael Illi)
Date: Tue, 31 Jul 2007 10:23:42 +0200
Subject: [R] durbin-watson of a arima estimation?
Message-ID: <46AEF18E.2090907@gmx.net>

to whom it may concern

I estimated arima(x = lM, order = c(1, 0, 0),xreg=cbind(lPfPx,lY)).
how can i now run the durbin-watson test (dwtest())???

Thanx for your prompt response!

Raphael Illi
Graduate writing his thesis in economics on the growth performance in 
Subsaharan Africa


From edoviak at earthlink.net  Tue Jul 31 13:22:23 2007
From: edoviak at earthlink.net (Eric Doviak)
Date: Tue, 31 Jul 2007 07:22:23 -0400 (GMT-04:00)
Subject: [R] the large dataset problem
Message-ID: <4248793.1185880944134.JavaMail.root@elwamui-polski.atl.sa.earthlink.net>


Just a note of thanks for all the help I have received. I haven't gotten a chance to implement any of your suggestions because I'm still trying to catalog all of them! Thank you so much!

Just to recap (for my own benefit and to create a summary for others):

Bruce Bernzweig suggested using the  R.huge  package.

Ben Bolker pointed out that my original message wasn't clear and asked what I want to do with the data. At this point, just getting a dataset loaded would be wonderful, so I'm trying to trim variables (and if possible, I would also like to trim observations). He also provided an example of "vectorizing."

Ted Harding suggested that I use AWK to process the data and provided the necessary code. He also tested his code on older hardware running GNU-Linux (or Unix?) and showed that AWK can process the data even when the computer has very little memory and processing power. Jim Holtman had similar success when he used Cygwin's UNIX utilities on a machine running MS Windows. They both used the following code:

     gawk 'BEGIN{FS=","}{print $(1) "," $(1000) "," $(1275) ","  $(5678)}'
     < tempxx.txt > newdata.csv

Fortunately, there is a version of GAWK for MS Windows. ... Not that I like MS Windows. It's just that I'm forced to use that 19th century operating system on the job. (After using Debian at home and happily running RKWard for my dissertation, returning to Windows World is downright depressing). 

Roland Rau suggested that I use a database with RSQLite and pointed out that RODBC can work with MS Access. He also pointed me to a sub-chapter in Venables and Ripley's _S Programming_ and "The Whole-Object View" pages in John Chamber's _Programming with Data_. 

Greg Snow recommended  biglm  for regression analysis with data that is too large to fit into memory.

Last, but not least, Peter Dalgaard pointed out that there are options within R. He suggests using the colClasses= argument for when "reading" data and the what= argument for "scanning" data, so that you don't load more columns than necessary. He also provided the following script: 

     dict <- readLines("ftp://www.sipp.census.gov/pub/sipp/2004/l04puw1d.txt")
     D.lines <- grep("^D ", dict)
     vdict <- read.table(con <- textConnection(dict[D.lines])); close(con)
     head(vdict) 

I'll try these solutions and report back on my success.

Thanks again!
- Eric


From dusa.adrian at gmail.com  Tue Jul 31 14:24:11 2007
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Tue, 31 Jul 2007 15:24:11 +0300
Subject: [R] Looping through all possible combinations of cases
In-Reply-To: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>
References: <dae9a2a60707271135g2f15d71rea9429242f20f377@mail.gmail.com>
Message-ID: <200707311524.11745.dusa.adrian@gmail.com>

Here is another (simpler?) solution:

# your 1 column data is actually a vector
myvalues <- 1:10
names(myvalues) <- LETTERS[1:10]

# use the QCA package
library(QCA)
aa <- createMatrix(rep(2, length(myvalues)))

# set the number of combinations: 2, 3, 4 or whatever
combinations <- 2
sub.aa <- aa[rowSums(aa) == combinations, ]

result <- apply(sub.aa, 1, function(x)
                sum(myvalues[x == 1]))
names(result) <- apply(sub.aa, 1, function(x)
                       paste(names(myvalues)[x == 1], collapse=""))

HTH,
Adrian

On Friday 27 July 2007, Dimitri Liakhovitski wrote:
> Hello!
>
> I have a regular data frame (DATA) with 10 people and 1 column
> ('variable'). Its cases are people with names ('a', 'b', 'c', 'd',
> 'e', 'f', etc.). I would like to write a function that would sum up
> the values on 'variable' of all possible combinations of people, i.e.
>
> 1. I would like to write a loop - in such a way that it loops through
> each possible pair of cases (i.e., ab, ac, ad, etc.) and sums up their
> respective values on 'variable'
>
> 2. I would like to write a loop - in such a way that it loops through
> each possible trio of cases (i.e., abc, abd, abe, etc.) and sums up
> their respective values on 'variable'.
>
> 3.  I would like to write a loop - in such a way that it loops through
> each possible quartet of cases (i.e., abcd, abce, abcf, etc.) and sums
> up their respective values on 'variable'.
>
> etc.
>
> Then, at the end I want to capture all possible combinations that were
> considered (i.e., what elements were combined in it) and get the value
> of the sum for each combination.
>
> How should I do it?
> Thanks a lot!
> Dimitri



-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd
050025 Bucharest sector 5
Romania
Tel./Fax: +40 21 3126618 \
          +40 21 3120210 / int.101


From Luisr at frs.fo  Tue Jul 31 14:25:04 2007
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Tue, 31 Jul 2007 13:25:04 +0100
Subject: [R] reading and storing files in the workspace
Message-ID: <s6af3834.086@ffdata.setur.fo>

R-help,

I have a vector containing (test) some file names.
The files contents are matrixes.

> test

 [1] "aaOki.txt"    "aOki.txt"     "bOki.txt"     "c1Oki.txt"   
"c2Oki.txt"    "c3Oki.txt"    "cOki.txt"     "dOki.txt"     "dyp100.txt"
  "dyp200.txt"  
[11] "dyp300.txt"   "dyp400.txt"   "dyp500.txt"   "dyp600.txt"  
"dyp700.txt"   "dyp800.txt"   "eOki.txt"     "FBdyp100.txt"
"FBdyp150.txt" "FBdyp200.txt".............

What I want to do is to import to R using the same file name
and remove the ".txt" extension out of the object name.
Something like this:

for(i in test)
gsub("\\.", "", paste(i, sep = "")) <- read.table(file = paste(i, sep =
""), header = TRUE)

But I get the following message:

Error in gsub("\\.", "", paste(i, sep = "")) <- read.table(file =
paste(i,  : 
        target of assignment expands to non-language object


Thanks in advance.


> version
               _                           
platform       i386-pc-mingw32             
arch           i386                        
os             mingw32                     
system         i386, mingw32               
status                                     
major          2                           
minor          5.1                         
year           2007                        
month          06                          
day            27                          
svn rev        42083                       
language       R                           
version.string R version 2.5.1 (2007-06-27)


From ubk2101 at columbia.edu  Tue Jul 31 14:03:58 2007
From: ubk2101 at columbia.edu (Udaya B. Kogalur)
Date: Tue, 31 Jul 2007 08:03:58 -0400
Subject: [R] [R-pkgs] randomSurvivalForest 3.0.0 now available
In-Reply-To: <38c08c270707291256l41912585qde16fc1c2d8385b1@mail.gmail.com>
References: <38c08c270707291256l41912585qde16fc1c2d8385b1@mail.gmail.com>
Message-ID: <38c08c270707310503t5ecdd95cwe7c54423248dbbb0@mail.gmail.com>

Dear useRs:

Release 3.0.0 of the randomSurvivalForest, an ensemble tree method for
the analysis of right censored survival data,  package is now
available.

---------------------------------------------------------------------------------
CHANGES TO RELEASE 3.0.0

Release 3.0.0 represents a major upgrade in the functionality of the
2.x releases.  Key changes are as follows:

o Missing data can be imputed in both grow and predict mode.  This
  applies to variables as well as time and censoring outcome values.
  Values are imputed dynamically as the tree is grown using a new tree
  imputation methodology.  This produces an imputed forest which can be
  used for prediction purposes on test data sets with missing data.

o Importance values for variables are returned in predict mode when test
  data contains outcomes as well as variables.

o Fixed some bugs in plot.variable().  Thanks to Andy J. Minn for
pointing this out.

o Minor modification of PMML representation of RSF forest output to accomodate
  imputation.  The method of random seed chain recovery has been altered.
  Note that forests produced with prior releases will have to be
  regenerated using this release.  We apologize for the inconvenience.

---------------------------------------------------------------------------------

Thanks.

ubk

ubk2101 at columbia.edu

Udaya B. Kogalur, Ph.D.
Kogalur Shear Corporation
5425 Nestleway Drive, Suite L1
Clemmons, NC 27012

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages


From guo.dong99 at gmail.com  Tue Jul 31 15:27:35 2007
From: guo.dong99 at gmail.com (=?GB2312?B?RG9uZyBHVU8gufm2qw==?=)
Date: Tue, 31 Jul 2007 15:27:35 +0200
Subject: [R] array loop
In-Reply-To: <OF8B2FB098.18254DD0-ONC1257329.002F45C8-C1257329.002F8319@precheza.cz>
References: <989527a20707301511g71820657v7afbc59b5f01e22f@mail.gmail.com>
	<OF8B2FB098.18254DD0-ONC1257329.002F45C8-C1257329.002F8319@precheza.cz>
Message-ID: <989527a20707310627l5c2d91cenf4257fe58214459b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/334fb9bf/attachment.pl 

From gregor.gorjanc at bfro.uni-lj.si  Tue Jul 31 15:45:24 2007
From: gregor.gorjanc at bfro.uni-lj.si (Gregor Gorjanc)
Date: Tue, 31 Jul 2007 13:45:24 +0000 (UTC)
Subject: [R] error in using R2WinBUGS on Ubuntu 6.10 Linux
References: <97753.85074.qm@web43137.mail.sp1.yahoo.com>
	<46AE06C1.5030400@jmu.edu>
Message-ID: <loom.20070731T154420-26@post.gmane.org>

J. Patrick Meyer <meyerjp <at> jmu.edu> writes:
> 
> I'm using Wine 0.9.41, and the patched version of WinBUGS. Everything 
> seems to run correctly. I get the right output from WinBUGS. I'm not 
> sure to what the error message refers.

I also get this messages, but that is comming from wine and can be safely 
ignored.

Gregor


From gerald.jean at dgag.ca  Tue Jul 31 15:51:50 2007
From: gerald.jean at dgag.ca (gerald.jean at dgag.ca)
Date: Tue, 31 Jul 2007 09:51:50 -0400
Subject: [R] Data mining tools
Message-ID: <OF4674997A.E95A881F-ON85257329.004B64D9@spgdag.ca>

Hello there, apologies for cross-posting

my question is not an S/R question but there is so much knowledge
concentrated in those lists that I thought someone could point me in the
right direction.

A few months ago I read an article in a referenced journal comparing some
data mining programs, among which there was Insightful's I Miner, SAS'
Entreprise Miner, SPSS' Clementine (I think) and a few others.
Unfortunately I can't remember in which journal was the article published
or who was the author?  I have been Googling a lot to try to locate the
article but to no avail!  Would someone know who published the article and
in which journal?  By the way, any serious, published comparisons of data
mining programs would be welcomed as the company I work for is planning to
add a data mining program to our tool box soon.

Thanks for any leads,

G?rald Jean
Conseiller senior en statistiques, Actuariat
t?lephone            : (418) 835-4900 poste (7639)
t?lecopieur          : (418) 835-6657
courrier ?lectronique: gerald.jean at dgag.ca

"In God we trust, all others must bring data"  W. Edwards Deming

Le message ci-dessus, ainsi que les documents l'accompagnant, sont destin?s
uniquement aux personnes identifi?es et peuvent contenir des informations
privil?gi?es, confidentielles ou ne pouvant ?tre divulgu?es. Si vous avez
re?u ce message par erreur, veuillez le d?truire.

This communication (and/or the attachments) is intended for named
recipients only and may contain privileged or confidential information
which is not to be disclosed. If you received this communication by mistake
please destroy all copies.


From jmacdon at med.umich.edu  Tue Jul 31 15:53:43 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 31 Jul 2007 09:53:43 -0400
Subject: [R] problems saving and loading (PLMset) objects
In-Reply-To: <001701c7d2aa$45d5a0d0$4f3911ac@simugenqfwills>
References: <001701c7d2aa$45d5a0d0$4f3911ac@simugenqfwills>
Message-ID: <46AF3EE7.8000205@med.umich.edu>

Hi Quin,

First off, you should ask questions about Bioconductor packages on the 
BioC listserv rather than R-help.

Anyway, I don't think your PLMset objects are coming out all wrong - it 
doesn't appear that you are loading the affyPLM package first, which is 
required for R to know anything about the PLMset object (this object is 
defined in affyPLM, so without the package R has no idea what it is).

Best,

Jim



Quin Wills wrote:
> Hi
> 
>  
> 
> I'm running the latest R on a presumably up to date Linux server.
> 
>  
> 
> 'Doing something silly I'm sure, but can't see why my saved PLMset objects
> come out all wrong. To use an example:
> 
>  
> 
> Setting up an example PLMset (I have the same problem no matter what example
> I use)
> 
>> library(affyPLM)
> 
>> data(Dilution) # affybatch object
> 
>> Dilution = updateObject(Dilution)
> 
>> options(width=36)
> 
>> expr <- fitPLM(Dilution)
> 
>  
> 
>  
> 
> This works, and I'm able to get the probeset coefficients with coefs(expr).
> until I save and try reloading:
> 
>> save(expr, file="expr.RData")
> 
>> rm(expr) # just to be sure
> 
>> expr <- load(expr.RData)
> 
>  
> 
>  
> 
> Now, running coefs(expr) says:
> 
>> Error in function (classes, fdef, mtable) : unable to find an inherited
> method for function "coefs", for signature "character"
> 
>  
> 
>  
> 
> Trying str(exp) just gives the following:
> 
>> chr "exp"
> 
>  
> 
> expr.Rdata appears to save properly (in that there is an actual file with
> notable size in my working directory).
> 
>  
> 
> Thanks in advance,
> 
> Quin
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


From antje.niederlein at yahoo.de  Tue Jul 31 16:04:48 2007
From: antje.niederlein at yahoo.de (Antje)
Date: Tue, 31 Jul 2007 16:04:48 +0200
Subject: [R] remove NA rows and columns
In-Reply-To: <11923066.post@talk.nabble.com>
References: <46AF2555.4020603@yahoo.de> <11923002.post@talk.nabble.com>
	<11923066.post@talk.nabble.com>
Message-ID: <46AF4180.3020009@yahoo.de>

Hello,

thank you both very much!
It is as easy as expected... (I think I still have to learn a lot!)

Have a nice day!
Antje



Vladimir Eremeev schrieb:
> Or, these operations can be called in one command:
> 
>> testmatrix[-which(apply(testmatrix,1,function(x)all(is.na(x)))),-which(apply(testmatrix,2,function(x)all(is.na(x))))]
>      [,1] [,2]
> [1,]    1    1
> [2,]    2    2
> [3,]    1    1
> [4,]    2    2
> 
> 
> 
> Vladimir Eremeev wrote:
>>> testmatrix
>>      [,1] [,2] [,3] [,4]
>> [1,]   NA   NA   NA   NA
>> [2,]   NA    1    1   NA
>> [3,]   NA    2    2   NA
>> [4,]   NA    1    1   NA
>> [5,]   NA    2    2   NA
>> [6,]   NA   NA   NA   NA
>>
>>> tm1<-testmatrix[,-which(apply(testmatrix,2,function(x)all(is.na(x))))]
>>> tm1
>>      [,1] [,2]
>> [1,]   NA   NA
>> [2,]    1    1
>> [3,]    2    2
>> [4,]    1    1
>> [5,]    2    2
>> [6,]   NA   NA
>>
>>> tm2<-tm1[-which(apply(testmatrix,1,function(x)all(is.na(x)))),]
>>> tm2
>>      [,1] [,2]
>> [1,]    1    1
>> [2,]    2    2
>> [3,]    1    1
>> [4,]    2    2
>>  
>>
>> Antje wrote:
>>> I guess, it's a rather simple thing but I cannot find a short way to
>>> reduce a 
>>> matrix, removing all rows and columns having just NA elements.
>>>
>>> testmatrix <- matrix(nrow=6, ncol=4)
>>> testmatrix[2:5,2:3] <- seq(2)
>>>
>>>  > testmatrix
>>>       [,1] [,2] [,3] [,4]
>>> [1,]   NA   NA   NA   NA
>>> [2,]   NA    1    1   NA
>>> [3,]   NA    2    2   NA
>>> [4,]   NA    1    1   NA
>>> [5,]   NA    2    2   NA
>>> [6,]   NA   NA   NA   NA
>>>
>>> the new matrix should look like this (by the way, I don't "know" which
>>> rows and 
>>> columns are the one to be deleted...
>>>
>>>  > testmatrix
>>>       [,1] [,2]
>>> [1,]   1    1
>>> [2,]   2    2
>>> [3,]   1    1
>>> [4,]   2    2
>>>
>>
>


From wills at stats.ox.ac.uk  Tue Jul 31 16:35:09 2007
From: wills at stats.ox.ac.uk (Quin Wills)
Date: Tue, 31 Jul 2007 15:35:09 +0100
Subject: [R] problems saving and loading (PLMset) objects
In-Reply-To: <46AF3EE7.8000205@med.umich.edu>
Message-ID: <004101c7d380$053c48e0$0301a8c0@simugenqfwills>

Erm, Jim I am loading in the affyPLM package first (when needed) and this
was a question based on loading/saving R objects. PLMset was an example.

Many thanks,
Quin


-----Original Message-----
From: James W. MacDonald [mailto:jmacdon at med.umich.edu] 
Sent: 31 July 2007 14:54
To: Quin Wills
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] problems saving and loading (PLMset) objects

Hi Quin,

First off, you should ask questions about Bioconductor packages on the 
BioC listserv rather than R-help.

Anyway, I don't think your PLMset objects are coming out all wrong - it 
doesn't appear that you are loading the affyPLM package first, which is 
required for R to know anything about the PLMset object (this object is 
defined in affyPLM, so without the package R has no idea what it is).

Best,

Jim



Quin Wills wrote:
> Hi
> 
>  
> 
> I'm running the latest R on a presumably up to date Linux server.
> 
>  
> 
> 'Doing something silly I'm sure, but can't see why my saved PLMset objects
> come out all wrong. To use an example:
> 
>  
> 
> Setting up an example PLMset (I have the same problem no matter what
example
> I use)
> 
>> library(affyPLM)
> 
>> data(Dilution) # affybatch object
> 
>> Dilution = updateObject(Dilution)
> 
>> options(width=36)
> 
>> expr <- fitPLM(Dilution)
> 
>  
> 
>  
> 
> This works, and I'm able to get the probeset coefficients with
coefs(expr).
> until I save and try reloading:
> 
>> save(expr, file="expr.RData")
> 
>> rm(expr) # just to be sure
> 
>> expr <- load(expr.RData)
> 
>  
> 
>  
> 
> Now, running coefs(expr) says:
> 
>> Error in function (classes, fdef, mtable) : unable to find an inherited
> method for function "coefs", for signature "character"
> 
>  
> 
>  
> 
> Trying str(exp) just gives the following:
> 
>> chr "exp"
> 
>  
> 
> expr.Rdata appears to save properly (in that there is an actual file with
> notable size in my working directory).
> 
>  
> 
> Thanks in advance,
> 
> Quin
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


From vmuggeo at dssm.unipa.it  Tue Jul 31 17:05:00 2007
From: vmuggeo at dssm.unipa.it (vmuggeo at dssm.unipa.it)
Date: Tue, 31 Jul 2007 17:05:00 +0200 (CEST)
Subject: [R] Piecewise Regression with a known slope
In-Reply-To: <B609F230-7969-4C9B-84DF-6ABC7E3151CC@ucdavis.edu>
References: <B609F230-7969-4C9B-84DF-6ABC7E3151CC@ucdavis.edu>
Message-ID: <1259.151.29.18.92.1185894300.squirrel@dssm.unipa.it>

Dear Jarrett,

If I understand correctly your post, your constraints may be achieved
straightforwardly in segmented. See the code below.

> The error is also most likely gamma distributed..[SNIP]
The 'error' component can be specified in the 'initial' model by means of
the family argument in the glm() function

> I have attempted to use the segmented package, specifying something
> close to the visually estimated breakpoint as the value of psi, but
> it continues to return fits with a breakpoint that occurs somewhere
> in the middle of the linear portion of the line, ..[SNIP]

Mhmm..you can specify different starting values and to assess the
differences in the 'final' estimates. However if you says "..it continues
to return fits with a breakpoint that occurs somewhere in the middle.."
probably the ML estimate of the breakpoint is really in the middle.

Hope this helps you,

best,
vito



####simulate data
n<-50
x<-1:n/n
y<- 0-pmin(x-.5,0)+rnorm(50)*.03
plot(x,y) #This should be your scatterplot..
abline(0,0,lty=2)

o<-lm(y~x) #or glm(y~x,family=..)
o1<-segmented(o,seg.Z=~x,psi=list(x=.3))
slope(o1) #get the slope
points(x,fitted(o1),col=2)

#a parsimonious modelling: constrain right slope=0
o<-lm(y~1)
xx<- -x
o2<-segmented(o,seg.Z=~xx,psi=list(xx=-.3))
slope(o2)
points(x,fitted(o2),col=2)

#now constrain \hat{\mu}(x)=0 for x>psi

o<-lm(y~0)
o3<-segmented(o,seg.Z=~xx,psi=list(xx=-.3))
slope(o3)
points(x,fitted(o3),col=3)

> Hey, all.  I'm working on a data set with a broken stick linear
> regression where I know one of the two slopes.  It is a negative
> linear function until the line intersects with the x-axis, at which
> point it becomes 0.  It is not a nonlinear asymptotic function, and,
> indeed, using negative exponential or logistic types of fits as an
> approximation has tended to lead to an under or overestimation of
> values.  I am also very interested to know just what the breakpoint
> in the data is.
>
> Essentially
>
> if x<psi y = a + bx + error, where b is negative
> else y=0+error
>
> and I want to know the value of psi, as well as a and b.  The error
> is also most likely gamma distributed, as values<0 are not possible
> (nutrient concentrations).
>
> I have attempted to use the segmented package, specifying something
> close to the visually estimated breakpoint as the value of psi, but
> it continues to return fits with a breakpoint that occurs somewhere
> in the middle of the linear portion of the line, and the slope and
> intercept of the second half of the regression is not 0.
>
> Is there either a package that exists that will allow me to estimate
> such a model, or a function that I can use for optim or nlm (I admit,
> I am a rank novice at coding such functions).
>
> Thanks so much!
>
> -Jarrett
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>


From jmacdon at med.umich.edu  Tue Jul 31 17:11:04 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 31 Jul 2007 11:11:04 -0400
Subject: [R] problems saving and loading (PLMset) objects
In-Reply-To: <004101c7d380$053c48e0$0301a8c0@simugenqfwills>
References: <004101c7d380$053c48e0$0301a8c0@simugenqfwills>
Message-ID: <46AF5108.9000605@med.umich.edu>

 > library(affyPLM)
 > data(Dilution)
 > pset <- fitPLM(Dilution)
 > save(pset, file="tmp.Rdata")
 > q()
## restart R
 > library(affyPLM)
 > load("tmp.Rdata") ## use load() correctly
 > class(pset)
[1] "PLMset"
attr(,"package")
[1] "affyPLM"
 > erm <- load("tmp.Rdata") ## use load() incorrectly
 > class(erm)
[1] "character"

Best,

Jim




Quin Wills wrote:
> Erm, Jim I am loading in the affyPLM package first (when needed) and this
> was a question based on loading/saving R objects. PLMset was an example.
> 
> Many thanks,
> Quin
> 
> 
> -----Original Message-----
> From: James W. MacDonald [mailto:jmacdon at med.umich.edu] 
> Sent: 31 July 2007 14:54
> To: Quin Wills
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] problems saving and loading (PLMset) objects
> 
> Hi Quin,
> 
> First off, you should ask questions about Bioconductor packages on the 
> BioC listserv rather than R-help.
> 
> Anyway, I don't think your PLMset objects are coming out all wrong - it 
> doesn't appear that you are loading the affyPLM package first, which is 
> required for R to know anything about the PLMset object (this object is 
> defined in affyPLM, so without the package R has no idea what it is).
> 
> Best,
> 
> Jim
> 
> 
> 
> Quin Wills wrote:
>> Hi
>>
>>  
>>
>> I'm running the latest R on a presumably up to date Linux server.
>>
>>  
>>
>> 'Doing something silly I'm sure, but can't see why my saved PLMset objects
>> come out all wrong. To use an example:
>>
>>  
>>
>> Setting up an example PLMset (I have the same problem no matter what
> example
>> I use)
>>
>>> library(affyPLM)
>>> data(Dilution) # affybatch object
>>> Dilution = updateObject(Dilution)
>>> options(width=36)
>>> expr <- fitPLM(Dilution)
>>  
>>
>>  
>>
>> This works, and I'm able to get the probeset coefficients with
> coefs(expr).
>> until I save and try reloading:
>>
>>> save(expr, file="expr.RData")
>>> rm(expr) # just to be sure
>>> expr <- load(expr.RData)
>>  
>>
>>  
>>
>> Now, running coefs(expr) says:
>>
>>> Error in function (classes, fdef, mtable) : unable to find an inherited
>> method for function "coefs", for signature "character"
>>
>>  
>>
>>  
>>
>> Trying str(exp) just gives the following:
>>
>>> chr "exp"
>>  
>>
>> expr.Rdata appears to save properly (in that there is an actual file with
>> notable size in my working directory).
>>
>>  
>>
>> Thanks in advance,
>>
>> Quin
>>
>>  
>>
>>  
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


From fgaravito at guggenheimadvisors.com  Tue Jul 31 17:14:56 2007
From: fgaravito at guggenheimadvisors.com (Garavito,Fabian)
Date: Tue, 31 Jul 2007 16:14:56 +0100
Subject: [R] Error message when running lm() with na.action=NULL
Message-ID: <BA9B873BB7132F4B8853E1DE580C30194FB8AE@MGEXCLP1.investment.boi>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/919c5d00/attachment.pl 

From bolker at ufl.edu  Tue Jul 31 17:21:31 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 31 Jul 2007 15:21:31 +0000 (UTC)
Subject: [R]
	=?utf-8?q?stop_criteria_when_=22L-BFGS-B_needs_finite_values_?=
	=?utf-8?b?b2YgJ2ZuJyAiIGluCW9wdGlt?=
References: <cbca975a0707300959m65e5e54ife0d77cb148a5b9f@mail.gmail.com>
Message-ID: <loom.20070731T171858-254@post.gmane.org>

Dae-Jin Lee <dae-jin.lee <at> uc3m.es> writes:

> 
> 
> I would like to know how to include and "if" condition when this happen,
> could it be something like:
> 
> myfun <- optim(....)         #   run my function
> 
> ????
> if(myfun == ERROR) ..... #   when the error message is "L-BFGS-B needs
> finite values of 'fn'
> ????
> 

  See ?try :

e.g.

 myfun <- try(optim(...))
  if (class(myfun)=="try-error") { ... whatever ...
   } else {
     ... success ...
   }
  
  or tryCatch

  this is FAQ 7.32 ...

  cheers
    Ben Bolker


From england at cs.umn.edu  Tue Jul 31 18:14:40 2007
From: england at cs.umn.edu (Darin A. England)
Date: Tue, 31 Jul 2007 11:14:40 -0500
Subject: [R] simple coding question
In-Reply-To: <EC0F8FF776F3F74E9C63CE16641C9628022BDB3B@AKLEXB02.hort.net.nz>
References: <EC0F8FF776F3F74E9C63CE16641C9628022BDB3B@AKLEXB02.hort.net.nz>
Message-ID: <20070731161440.GA3092@cs.umn.edu>

I know that some ICD9 codes contain letters, so I suspect that they
are stored as "character". Here is a function that just pads zeros
on to the end to make the string five characters long.

format <- function(icd9) {
  len <- length(strsplit(icd9, "")[[1]])
  pad <- ""
  if (num <- 5-len) 
    pad <- paste(rep("0", times=num), collapse="")
  paste(icd9, pad , sep="", collapse="")
}

Then use sapply() on the vector that contains the codes. Probably
not too hard to stick a "." in there if you really want one.

Darin


On Tue, Jul 31, 2007 at 05:17:29PM +1200, Peter Alspach wrote:
> 
> Kirsten
> 
> One way to do this:
> 
> kirsten <- c(123, 1234, 12345)
> 100*as.numeric(paste(substring(kirsten, 1, 3), substring(kirsten, 4, 5),
> sep='.'))
> 
> HTH ........
> 
> Peter Alspach
>   
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kirsten Beyer
> > Sent: Tuesday, 31 July 2007 9:31 a.m.
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] simple coding question
> > 
> > I have a list of ICD9 (disease) codes with various formats - 3 digit,
> > 4 digit, 5 digit.  The first three digits of these codes are 
> > what I am most interested in.  I would like to either add 
> > zeros to the 3 and 4 digit codes to make them 5 digit codes 
> > or add decimal points to put them all in the format ###.##.  
> > I did not see a function that allows me to do this in the 
> > formatting command.  This seems simple - can someone help?
> > 
> > Thanks,
> > K.Beyer
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide 
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> 
> ______________________________________________________
> 
> The contents of this e-mail are privileged and/or confidenti...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From doc.evans at gmail.com  Tue Jul 31 18:22:40 2007
From: doc.evans at gmail.com (D. R. Evans)
Date: Tue, 31 Jul 2007 10:22:40 -0600
Subject: [R] Q: obtaining non-transparent background in png
Message-ID: <256f4e900707310922q68061669tdb714adf01ffeaa6@mail.gmail.com>

I am not understanding something about generating PNG plots.

I have tried several ways to obtain something other than a transparent
background, but nothing I've done seems to change the background.

For example:

dev.print(png, width=800, height=600, bg='red', filename='example.png')

which I thought would give a red background, simply gives the same
transparent background I always get.

And I also don't understand why the default background is transparent,
when the documentation seems to say that it's white:
  png(filename = "Rplot%03d.png", width = 480, height = 480,
         pointsize = 12, bg = "white",  res = NA,...)

(This is on a Kubuntu dapper 64-bit system.)

[I looked through the mail archives, and there seem to be a few very
old postings talking about the opposite problem, but nothing recent;
so I conclude that I'm doing something wrong.]


From jmacdon at med.umich.edu  Tue Jul 31 18:58:27 2007
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 31 Jul 2007 12:58:27 -0400
Subject: [R] Q: obtaining non-transparent background in png
In-Reply-To: <256f4e900707310922q68061669tdb714adf01ffeaa6@mail.gmail.com>
References: <256f4e900707310922q68061669tdb714adf01ffeaa6@mail.gmail.com>
Message-ID: <46AF6A33.3080603@med.umich.edu>

Both of these work for me:

par(bg="red")
plot(1:10)
dev.print(png, width=800, height=600, filename="tmp.png")

and

png("tmp.png", width=800, height=600, bg="red")
plot(1:10)
dev.off()

Best,

Jim



D. R. Evans wrote:
> I am not understanding something about generating PNG plots.
> 
> I have tried several ways to obtain something other than a transparent
> background, but nothing I've done seems to change the background.
> 
> For example:
> 
> dev.print(png, width=800, height=600, bg='red', filename='example.png')
> 
> which I thought would give a red background, simply gives the same
> transparent background I always get.
> 
> And I also don't understand why the default background is transparent,
> when the documentation seems to say that it's white:
>   png(filename = "Rplot%03d.png", width = 480, height = 480,
>          pointsize = 12, bg = "white",  res = NA,...)
> 
> (This is on a Kubuntu dapper 64-bit system.)
> 
> [I looked through the mail archives, and there seem to be a few very
> old postings talking about the opposite problem, but nothing recent;
> so I conclude that I'm doing something wrong.]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
James W. MacDonald, M.S.
Biostatistician
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623


From gavin.simpson at ucl.ac.uk  Tue Jul 31 19:00:17 2007
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 31 Jul 2007 18:00:17 +0100
Subject: [R] Q: obtaining non-transparent background in png
In-Reply-To: <256f4e900707310922q68061669tdb714adf01ffeaa6@mail.gmail.com>
References: <256f4e900707310922q68061669tdb714adf01ffeaa6@mail.gmail.com>
Message-ID: <1185901217.31913.45.camel@gsimpson.geog.ucl.ac.uk>

On Tue, 2007-07-31 at 10:22 -0600, D. R. Evans wrote:
> I am not understanding something about generating PNG plots.
> 
> I have tried several ways to obtain something other than a transparent
> background, but nothing I've done seems to change the background.
> 
> For example:
> 
> dev.print(png, width=800, height=600, bg='red', filename='example.png')
> 
> which I thought would give a red background, simply gives the same
> transparent background I always get.

?dev.print says:

     'dev.print' copies the graphics contents of the current device to
     a new device which has been created by the function specified by
     'device' and then shuts the new device.

Note "copies" - given that you've already drawn a figure with a white
background, should this then produce one that is red? However, you are
correct that it does produce a plot with a transparent background.

I find it easier to wrap my plotting commands in the relevant device,
e.g. this works with the desired background:

> png("mypng.png", height = 400, width = 400, bg = "red", 
      pointsize = 12)
> plot(1:10)
> dev.off()

Whereas these do not give "red" backgrounds as one might have expected,
but transparent ones:

> plot(1:10)
> dev.print(png, height = 400, width = 400, bg = "red", pointsize = 12, 
            filename = "mypng2.png")
X11
  2
> dev.copy(png, height = 400, width = 400, bg = "red", pointsize = 12, 
           filename = "mypng3.png")
PNG
  3
> dev.off()
X11
  2

Not sure whether this is as intentional or not, but it does not appear
to be passing the bg argument on to the 'device', or if it does, it is
not being used/respected - perhaps all that is need is clarification as
to what can be specified in '...' in ?dev.print

> version
               _
platform       i686-pc-linux-gnu
arch           i686
os             linux-gnu
system         i686, linux-gnu
status         Patched
major          2
minor          5.1
year           2007
month          07
day            05
svn rev        42131
language       R
version.string R version 2.5.1 Patched (2007-07-05 r42131)

G

> 
> And I also don't understand why the default background is transparent,
> when the documentation seems to say that it's white:
>   png(filename = "Rplot%03d.png", width = 480, height = 480,
>          pointsize = 12, bg = "white",  res = NA,...)
> 
> (This is on a Kubuntu dapper 64-bit system.)
> 
> [I looked through the mail archives, and there seem to be a few very
> old postings talking about the opposite problem, but nothing recent;
> so I conclude that I'm doing something wrong.]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
 Gavin Simpson                 [t] +44 (0)20 7679 0522
 ECRC, UCL Geography,          [f] +44 (0)20 7679 0565
 Pearson Building,             [e] gavin.simpsonATNOSPAMucl.ac.uk
 Gower Street, London          [w] http://www.ucl.ac.uk/~ucfagls/
 UK. WC1E 6BT.                 [w] http://www.freshwaters.org.uk
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%


From amnakhan493 at gmail.com  Tue Jul 31 19:21:01 2007
From: amnakhan493 at gmail.com (amna khan)
Date: Tue, 31 Jul 2007 10:21:01 -0700
Subject: [R] legend()
Message-ID: <3ffd3bb60707311021w562cb842j46993e388e91f7be@mail.gmail.com>

Hi Sir
How can I use legend() outside th e plot.
Please guid in this regard.
Thanks

-- 
AMINA SHAHZADI
Department of Statistics
GC University Lahore, Pakistan.
Email:
amnakhan493 at gmail.com
amna_989 at hotmail.com
amna_989 at yahoo.com


From HStevens at MUOhio.edu  Tue Jul 31 19:22:59 2007
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Tue, 31 Jul 2007 13:22:59 -0400
Subject: [R] italic greek symbols
Message-ID: <A060D7CB-2138-4BEF-B6BA-B1277304FB92@MUOhio.edu>

Hi Folks,
I am using R 2.5.1 on a Mac OS X 10.4.9, via ESS.

I would like to try to get an italic mu onto a plot axis label. I  
note that in a previous email,
(Thu, 4 May 2006 19:41:41 +0100 (BST)), Brian Ripley wrote,
"There is no italic symbol font available on most devices. So unless you
try to plot Greek (not math symbol Greek) you are out of luck. "

How does one plot Greek, or is there now another solution?

Thanks,
Hank



Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"


From doc.evans at gmail.com  Tue Jul 31 19:33:02 2007
From: doc.evans at gmail.com (D. R. Evans)
Date: Tue, 31 Jul 2007 11:33:02 -0600
Subject: [R] Q: obtaining non-transparent background in png
In-Reply-To: <1185901217.31913.45.camel@gsimpson.geog.ucl.ac.uk>
References: <256f4e900707310922q68061669tdb714adf01ffeaa6@mail.gmail.com>
	<1185901217.31913.45.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <256f4e900707311033q2547d0c7k2db740fe3184618d@mail.gmail.com>

On 31/07/07, Gavin Simpson <gavin.simpson at ucl.ac.uk> wrote:
> On Tue, 2007-07-31 at 10:22 -0600, D. R. Evans wrote:
> > I am not understanding something about generating PNG plots.
> >
> > I have tried several ways to obtain something other than a transparent
> > background, but nothing I've done seems to change the background.
> >
> > For example:
> >
> > dev.print(png, width=800, height=600, bg='red', filename='example.png')
> >
> > which I thought would give a red background, simply gives the same
> > transparent background I always get.
>
> ?dev.print says:
>
>      'dev.print' copies the graphics contents of the current device to
>      a new device which has been created by the function specified by
>      'device' and then shuts the new device.
>
> Note "copies" - given that you've already drawn a figure with a white
> background, should this then produce one that is red?

Well, I wondered about that, so the first thing I did was to test it
by changing a different parameter. I created an X11 plot with the
default size (480, I think?) and then printed it to a png with a width
of 800. That indeed created a PNG file of width 800, so I deduced from
that that it was OK to change the parameters of the plot in the
destination device.

> Not sure whether this is as intentional or not, but it does not appear
> to be passing the bg argument on to the 'device', or if it does, it is
> not being used/respected - perhaps all that is need is clarification as
> to what can be specified in '...' in ?dev.print

I think so. Either that or it seems to be a bug (I obviously don't
know enough about how things are supposed to work to make that
determination; but it does seem rather bug-like behaviour, especially
since one can certainly change some of the parameters associated with
the plot).

Anyway, it seems like I need an explicit "par(bg='red')" before
performing any graphical operations. That seems to do the trick.
Although it's still not clear how one would solve the general problem
in which one has an X11 plot with background colour A, but wants to
copy it to a  PNG with background colour B...

Weirdly (at least it seems weird to me) I just tried the following
with an unexpected result:

I created the following function:
"to.png" <-
function(FILENAME = 'Rplot%03d.png')
{ par(bg='blue')
  dev.print(png, width=800, height=600, filename=FILENAME)
}

par(bg='red')
plot(1:10)            # ok, I get a red plot

to.png('should-be-blue.png')             # png is red, not blue

plot(1:10)           # now it's blue (as I sort-of expected)

So it isn't obvious that there's anything one can put in the to.png()
function that will control the colour of the background for the PNG
output.


From emilio.gagliardi at gmail.com  Tue Jul 31 19:35:14 2007
From: emilio.gagliardi at gmail.com (Emilio Gagliardi)
Date: Tue, 31 Jul 2007 11:35:14 -0600
Subject: [R] how to sort dataframe levels
Message-ID: <3a9ebf340707311035u2914328fuad47170c8b3b0cb0@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/8548af94/attachment.pl 

From yuv_libran at yahoo.com  Tue Jul 31 19:35:07 2007
From: yuv_libran at yahoo.com (yuvika)
Date: Tue, 31 Jul 2007 10:35:07 -0700 (PDT)
Subject: [R] extract columns of a matrix/data frame
Message-ID: <481239.62518.qm@web31101.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/4776e28b/attachment.pl 

From ambertk at ohsu.edu  Tue Jul 31 19:50:09 2007
From: ambertk at ohsu.edu (Kyle.)
Date: Tue, 31 Jul 2007 10:50:09 -0700
Subject: [R] extract columns of a matrix/data frame
In-Reply-To: <481239.62518.qm@web31101.mail.mud.yahoo.com>
References: <481239.62518.qm@web31101.mail.mud.yahoo.com>
Message-ID: <8A1FA84B-BB6E-4990-A093-53DA9F509BFC@ohsu.edu>

You can use the grep function to obtain the column indices matching  
your specified criteria.  For example,

 > r1<-c(1,2,3,7,1,3,2)
 > r2<-c(4,5,7,8,1,4,3)
 > test<-matrix(c(r1,r2),byrow=TRUE)
 > colnames(test)<-c("a1","a2","b1","b2","b3","c1","c2")
 > test
      a1 a2 b1 b2 b3 c1 c2
[1,]  1  2  3  7  1  3  2
[2,]  4  5  7  8  1  4  3
 > grep("a",(colnames(test)))
[1] 1 2
test.a<-test[,grep("a",(colnames(test)))]
test.a
      a1 a2
[1,]  1  2
[2,]  4  5



On Jul 31, 2007, at 10:35 AM, yuvika wrote:

> Hello all,
>
>   I have a matrix whose column names look like
>
>   a1  a2  b1  b2  b3  c1 c2
>   1   2    3    7    1    3   2
>   4   6    7    8    1    4   3
>
>   Now, I can have any number of a's. not just two as shown above  
> and same goes for b's and c's.  I need to extract all the a's  
> columns and put them in another matrix, extract all b's columns and  
> put them in some matrix and same goes for "c". How can I identify  
> such pattern and get subsets of this matrix depending on columns  
> names?
>
>   I will appreciate a quick reply.
>   Thanks  a lot.
>
>
> ---------------------------------
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ambertk at ohsu.edu  Tue Jul 31 19:53:35 2007
From: ambertk at ohsu.edu (Kyle.)
Date: Tue, 31 Jul 2007 10:53:35 -0700
Subject: [R] extract columns of a matrix/data frame
In-Reply-To: <481239.62518.qm@web31101.mail.mud.yahoo.com>
References: <481239.62518.qm@web31101.mail.mud.yahoo.com>
Message-ID: <E9C125FB-8361-4C93-969F-B2B7D76DCB3F@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/5aa75d60/attachment.pl 

From ccleland at optonline.net  Tue Jul 31 19:53:08 2007
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 31 Jul 2007 13:53:08 -0400
Subject: [R] extract columns of a matrix/data frame
In-Reply-To: <481239.62518.qm@web31101.mail.mud.yahoo.com>
References: <481239.62518.qm@web31101.mail.mud.yahoo.com>
Message-ID: <46AF7704.6020901@optonline.net>

yuvika wrote:
> Hello all,
>    
>   I have a matrix whose column names look like
>    
>   a1  a2  b1  b2  b3  c1 c2
>   1   2    3    7    1    3   2
>   4   6    7    8    1    4   3
>    
>   Now, I can have any number of a's. not just two as shown above and same goes for b's and c's.  I need to extract all the a's columns and put them in another matrix, extract all b's columns and put them in some matrix and same goes for "c". How can I identify such pattern and get subsets of this matrix depending on columns names?
>    
>   I will appreciate a quick reply.
>   Thanks  a lot.

mymat <- matrix(runif(60), ncol=6)

colnames(mymat) <- c("a1","a2","b1","b2","c1","c2")

mymat
              a1         a2          b1         b2         c1         c2
 [1,] 0.73623481 0.25204019 0.332436396 0.36629507 0.39517285 0.62491949
 [2,] 0.48867382 0.20933245 0.511805497 0.03142542 0.82168732 0.20550784
 [3,] 0.89198874 0.24477456 0.629644977 0.23442137 0.17828551 0.29640615
 [4,] 0.99222414 0.49044514 0.571213786 0.91068115 0.09484414 0.78108139
 [5,] 0.66615787 0.13183354 0.004350679 0.32443025 0.38742483 0.76044740
 [6,] 0.06642704 0.96257552 0.189716240 0.83969989 0.53470898 0.28319039
 [7,] 0.31172264 0.20201281 0.577353264 0.62082694 0.31649255 0.40977000
 [8,] 0.52890283 0.46576510 0.107363256 0.72534897 0.12038182 0.06295499
 [9,] 0.55292555 0.76459699 0.212533012 0.73275529 0.98008863 0.85302931
[10,] 0.84320369 0.09958472 0.158443155 0.92321443 0.50935938 0.08514859

mymat[,grep("^a", colnames(mymat))]
              a1         a2
 [1,] 0.73623481 0.25204019
 [2,] 0.48867382 0.20933245
 [3,] 0.89198874 0.24477456
 [4,] 0.99222414 0.49044514
 [5,] 0.66615787 0.13183354
 [6,] 0.06642704 0.96257552
 [7,] 0.31172264 0.20201281
 [8,] 0.52890283 0.46576510
 [9,] 0.55292555 0.76459699
[10,] 0.84320369 0.09958472

?grep

> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 512-0171 (M, W, F)
fax: (917) 438-0894


From marc_schwartz at comcast.net  Tue Jul 31 20:15:24 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 31 Jul 2007 13:15:24 -0500
Subject: [R] extract columns of a matrix/data frame
In-Reply-To: <481239.62518.qm@web31101.mail.mud.yahoo.com>
References: <481239.62518.qm@web31101.mail.mud.yahoo.com>
Message-ID: <1185905724.3632.23.camel@Bellerophon.localdomain>

On Tue, 2007-07-31 at 10:35 -0700, yuvika wrote:
> Hello all,
>    
>   I have a matrix whose column names look like
>    
>   a1  a2  b1  b2  b3  c1 c2
>   1   2    3    7    1    3   2
>   4   6    7    8    1    4   3
>    
>   Now, I can have any number of a's. not just two as shown above and
> same goes for b's and c's.  I need to extract all the a's columns and
> put them in another matrix, extract all b's columns and put them in
> some matrix and same goes for "c". How can I identify such pattern and
> get subsets of this matrix depending on columns names?
>    
>   I will appreciate a quick reply.
>   Thanks  a lot.


If 'MAT' is your matrix:

> MAT
     a1 a2 b1 b2 b3 c1 c2
[1,]  1  2  3  7  1  3  2
[2,]  4  6  7  8  1  4  3


You can use:

> sapply(letters[1:3], function(x) MAT[, grep(x, colnames(MAT))])
$a
     a1 a2
[1,]  1  2
[2,]  4  6

$b
     b1 b2 b3
[1,]  3  7  1
[2,]  7  8  1

$c
     c1 c2
[1,]  3  2
[2,]  4  3


which returns a list containing the three matrices as a consequence of
subsetting 'MAT" based upon the colnames.

This uses sapply() to loop over letters[1:3], which is:

> letters[1:3]
[1] "a" "b" "c"

and then uses grep() to get the indices of the colnames matching the
individual letters, passed as 'x' in each iteration of the sapply()
loop. For example:

> grep("a", colnames(MAT))
[1] 1 2


You can then manipulate each sub-matrix in the list as you require.

See ?sapply, ?grep and ?letters

HTH,

Marc Schwartz


From james.milks at wright.edu  Tue Jul 31 20:13:02 2007
From: james.milks at wright.edu (James Milks)
Date: Tue, 31 Jul 2007 14:13:02 -0400
Subject: [R] choosing between Poisson regression models: no interactions vs.
 interactions
Message-ID: <ED3CAE36-581E-4802-9BB6-77A9DC792226@wright.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/f029566b/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jul 31 20:20:38 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 19:20:38 +0100 (BST)
Subject: [R] Q: obtaining non-transparent background in png
In-Reply-To: <1185901217.31913.45.camel@gsimpson.geog.ucl.ac.uk>
References: <256f4e900707310922q68061669tdb714adf01ffeaa6@mail.gmail.com>
	<1185901217.31913.45.camel@gsimpson.geog.ucl.ac.uk>
Message-ID: <Pine.LNX.4.64.0707311919430.10832@gannet.stats.ox.ac.uk>

You are *copying* the plot, and that means copying the background too (it
*is* part of the plot).  Almost certainly the plot you are copying had a
transparent background: that is the default for X11.

All the confusion seems to be over misreadings of this.


On Tue, 31 Jul 2007, Gavin Simpson wrote:

> On Tue, 2007-07-31 at 10:22 -0600, D. R. Evans wrote:
>> I am not understanding something about generating PNG plots.
>>
>> I have tried several ways to obtain something other than a transparent
>> background, but nothing I've done seems to change the background.
>>
>> For example:
>>
>> dev.print(png, width=800, height=600, bg='red', filename='example.png')
>>
>> which I thought would give a red background, simply gives the same
>> transparent background I always get.
>
> ?dev.print says:
>
>     'dev.print' copies the graphics contents of the current device to
>     a new device which has been created by the function specified by
>     'device' and then shuts the new device.
>
> Note "copies" - given that you've already drawn a figure with a white
> background, should this then produce one that is red? However, you are
> correct that it does produce a plot with a transparent background.
>
> I find it easier to wrap my plotting commands in the relevant device,
> e.g. this works with the desired background:
>
>> png("mypng.png", height = 400, width = 400, bg = "red",
>      pointsize = 12)
>> plot(1:10)
>> dev.off()
>
> Whereas these do not give "red" backgrounds as one might have expected,
> but transparent ones:
>
>> plot(1:10)
>> dev.print(png, height = 400, width = 400, bg = "red", pointsize = 12,
>            filename = "mypng2.png")
> X11
>  2
>> dev.copy(png, height = 400, width = 400, bg = "red", pointsize = 12,
>           filename = "mypng3.png")
> PNG
>  3
>> dev.off()
> X11
>  2
>
> Not sure whether this is as intentional or not, but it does not appear
> to be passing the bg argument on to the 'device', or if it does, it is
> not being used/respected - perhaps all that is need is clarification as
> to what can be specified in '...' in ?dev.print
>
>> version
>               _
> platform       i686-pc-linux-gnu
> arch           i686
> os             linux-gnu
> system         i686, linux-gnu
> status         Patched
> major          2
> minor          5.1
> year           2007
> month          07
> day            05
> svn rev        42131
> language       R
> version.string R version 2.5.1 Patched (2007-07-05 r42131)
>
> G
>
>>
>> And I also don't understand why the default background is transparent,
>> when the documentation seems to say that it's white:
>>   png(filename = "Rplot%03d.png", width = 480, height = 480,
>>          pointsize = 12, bg = "white",  res = NA,...)
>>
>> (This is on a Kubuntu dapper 64-bit system.)
>>
>> [I looked through the mail archives, and there seem to be a few very
>> old postings talking about the opposite problem, but nothing recent;
>> so I conclude that I'm doing something wrong.]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Tue Jul 31 20:23:35 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 31 Jul 2007 13:23:35 -0500
Subject: [R] italic greek symbols
In-Reply-To: <A060D7CB-2138-4BEF-B6BA-B1277304FB92@MUOhio.edu>
References: <A060D7CB-2138-4BEF-B6BA-B1277304FB92@MUOhio.edu>
Message-ID: <1185906215.3632.28.camel@Bellerophon.localdomain>

On Tue, 2007-07-31 at 13:22 -0400, Martin Henry H. Stevens wrote:
> Hi Folks,
> I am using R 2.5.1 on a Mac OS X 10.4.9, via ESS.
> 
> I would like to try to get an italic mu onto a plot axis label. I  
> note that in a previous email,
> (Thu, 4 May 2006 19:41:41 +0100 (BST)), Brian Ripley wrote,
> "There is no italic symbol font available on most devices. So unless you
> try to plot Greek (not math symbol Greek) you are out of luck. "
> 
> How does one plot Greek, or is there now another solution?
> 
> Thanks,
> Hank

You might want to look at the links in this post by Paul Murrell:

http://tolstoy.newcastle.edu.au/R/e2/help/06/12/7083.html

HTH,

Marc Schwartz


From ripley at stats.ox.ac.uk  Tue Jul 31 21:00:56 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 20:00:56 +0100 (BST)
Subject: [R] italic greek symbols
In-Reply-To: <A060D7CB-2138-4BEF-B6BA-B1277304FB92@MUOhio.edu>
References: <A060D7CB-2138-4BEF-B6BA-B1277304FB92@MUOhio.edu>
Message-ID: <Pine.LNX.4.64.0707311957220.11902@gannet.stats.ox.ac.uk>

On Tue, 31 Jul 2007, Martin Henry H. Stevens wrote:

> Hi Folks,
> I am using R 2.5.1 on a Mac OS X 10.4.9, via ESS.
>
> I would like to try to get an italic mu onto a plot axis label. I
> note that in a previous email,
> (Thu, 4 May 2006 19:41:41 +0100 (BST)), Brian Ripley wrote,
> "There is no italic symbol font available on most devices. So unless you
> try to plot Greek (not math symbol Greek) you are out of luck. "
>
> How does one plot Greek, or is there now another solution?

Use the appropriate Unicode characters.  Lower case mu is \u03bc.
Whether this works depends on the device and locale (as I said before)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Tue Jul 31 21:04:47 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 31 Jul 2007 14:04:47 -0500
Subject: [R] legend()
In-Reply-To: <3ffd3bb60707311021w562cb842j46993e388e91f7be@mail.gmail.com>
References: <3ffd3bb60707311021w562cb842j46993e388e91f7be@mail.gmail.com>
Message-ID: <1185908687.3632.39.camel@Bellerophon.localdomain>

On Tue, 2007-07-31 at 10:21 -0700, amna khan wrote:
> Hi Sir
> How can I use legend() outside th e plot.
> Please guid in this regard.
> Thanks


Create a plot, specifying outer margins to make space for the legend.
Then move the legend to the open region.

# Set 'xpd' to NA so that the legend is not clipped
# at the plot region, which it is by default
par(xpd = NA)

# Make some room at the right hand side
par(oma = c(0, 0, 0, 10))

# Do the plot
plot(1:5)

# Do the legend and use 'inset' to move the legend to
# the right hand outer margin
legend("topright", legend = 1:5, inset = c(-.4, .0))



You can adjust the outer margin settings and the 'inset' value as you
may require to make room for the legend on the side required.

See ?par and ?legend for more information.

Another option would be to use layout() to create more than one plot
region, perhaps adjusting the heights and/or widths of the plot regions,
such that the data plot goes into one region and the legend into the
other.  See ?layout for more information.

HTH,

Marc Schwartz


From bolker at ufl.edu  Tue Jul 31 21:13:16 2007
From: bolker at ufl.edu (Ben Bolker)
Date: Tue, 31 Jul 2007 19:13:16 +0000 (UTC)
Subject: [R] choosing between Poisson regression models: no interactions
	vs. interactions
References: <ED3CAE36-581E-4802-9BB6-77A9DC792226@wright.edu>
Message-ID: <loom.20070731T205121-60@post.gmane.org>

James Milks <james.milks <at> wright.edu> writes:

> 
> My problem is deciding which model to use.  I have created several,  
> one without interaction terms (Total.vines~Site+Species+DBH), one  
> with an interaction term between Site and Species  
> (Total.vines~Site*Species+DBH), and one with interactions between all  
> variables (Total.vines~Site*Species*DBH).  Here is my output from R  
> for the first two models (the last model has the same number (and  
> identity) of significant variables as the second model, even though  
> the last model had more interaction terms overall):
> 

  A few comments:

 - the narrow answer to your question is to use the
interaction model: this would be the answer in several
different statistical frameworks.  In information-criteria-land,
the AIC is 10 points lower which constitutes a much better
expected predictive accuracy.  In classical likelihood-ratio-testing
land (try anova(model1,model2,test="Chisq")), you can probably also reject
the null hypothesis that adding the interaction terms doesn't
improve the model (sorry about the convoluted language, but that's
what you get in LRT-land).  Also, the presence of *any* statistically
significant interaction suggests that you probably can't neglect interactions.
The number of significant terms in each model is largely
irrelevant.

  - You should probably consider whether there is overdispersion
in your data (e.g. try fitting a quasipoisson or negative binomial
model, although you can't use AIC or LRT (=anova()) with quasipoisson
models), since there usually is in ecological data.

  - your question suggests you might want to read up a bit
more on generalized linear models  -- Agresti's intro to categorical
data analysis or Crawley's intro to data analysis with S-PLUS
would work.  (If you're familiar with logistic regression, Poisson
regression should follow almost exactly the same rules and conventions.)

  good luck,
    Ben Bolker


From ambertk at ohsu.edu  Tue Jul 31 21:23:40 2007
From: ambertk at ohsu.edu (Kyle.)
Date: Tue, 31 Jul 2007 12:23:40 -0700
Subject: [R] choosing between Poisson regression models: no interactions
 vs. interactions
In-Reply-To: <ED3CAE36-581E-4802-9BB6-77A9DC792226@wright.edu>
References: <ED3CAE36-581E-4802-9BB6-77A9DC792226@wright.edu>
Message-ID: <C335DE59-5C11-4955-BAC1-89F535551FD6@ohsu.edu>

If you have access to it, chapter 7, section three of Venables and  
Ripley's "Modern Applied Statistics with S" is probably going to tell  
you everything you need to know.  In general, let's say you're  
wanting to look at the following:

	model 1:	Total.vines ~ Site + Species + DBH
	model 2:	Total.vines ~ Site * Species + DBH

Download the MASS package and load it into the library

 > library(MASS)
 > model1<-glm(Total.vines~Site+Specites+DBH,family=poisson)
 > model2<-addterm(model1, . ~ .*Species+DBH,test="Chisq")

This should return a table including the respective AICs, likelihood  
ratio tests, and the p-value associated with whether or not model2 is  
significantly different than model 1.  In general though, it's best  
to choose the model which minimizes the AIC.  Based on the output you  
included, it appears that the model including the interaction term is  
better, but the above might give you the information you seek.


Kyle H. Ambert
Graduate Student, Dept. Behavioral Neuroscience
Oregon Health & Science University
ambertk at ohsu.edu


On Jul 31, 2007, at 11:13 AM, James Milks wrote:

> R gurus,
>
> I'm working on data analysis for a small project.  My response
> variable is total vines per tree (median = 0, mean = 1.65, min = 0,
> max = 24).  My predictors are two categorical variables (four sites
> and four species) and one continuous (tree diameter at breast height
> (DBH)).  The main question I'm attempting to answer is whether or not
> the species identity of a tree has any effects on the number of vines
> clinging to the trunk.  Given that the response variable is count
> data, I decided to use Poisson regression, even though I'm not as
> familiar with it as linear or logit regression.
>
> My problem is deciding which model to use.  I have created several,
> one without interaction terms (Total.vines~Site+Species+DBH), one
> with an interaction term between Site and Species
> (Total.vines~Site*Species+DBH), and one with interactions between all
> variables (Total.vines~Site*Species*DBH).  Here is my output from R
> for the first two models (the last model has the same number (and
> identity) of significant variables as the second model, even though
> the last model had more interaction terms overall):
>
> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
> Call:
> glm(formula = Total.vines ~ Site + Species + DBH, family = poisson)
>
> Deviance Residuals:
>      Min       1Q   Median       3Q      Max
> -5.2067  -1.2915  -0.7095  -0.3525   6.3756
>
> Coefficients:
>                   Estimate Std. Error z value Pr(>|z|)
> (Intercept)     -2.987695   0.231428 -12.910  < 2e-16 ***
> SiteHuffman Dam  2.725193   0.249423  10.926  < 2e-16 ***
> SiteNarrows      1.902987   0.227599   8.361  < 2e-16 ***
> SiteSugar Creek  1.752754   0.242186   7.237 4.58e-13 ***
> SpeciesFRAM      0.955468   0.157423   6.069 1.28e-09 ***
> SpeciesPLOC      1.187903   0.141707   8.383  < 2e-16 ***
> SpeciesULAM      0.340792   0.184615   1.846   0.0649 .
> DBH              0.020708   0.001292  16.026  < 2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> (Dispersion parameter for poisson family taken to be 1)
>
>      Null deviance: 1972.3  on 544  degrees of freedom
> Residual deviance: 1290.0  on 537  degrees of freedom
> AIC: 1796.0
>
> Number of Fisher Scoring iterations: 6
>
> %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
> Call:
> glm(formula = Total.vines ~ Site * Species + DBH, family = poisson,
>      data = sycamores.1)
>
> Deviance Residuals:
>      Min       1Q   Median       3Q      Max
> -4.9815  -1.2370  -0.6339  -0.3403   6.5664
>
> Coefficients: (3 not defined because of singularities)
>                                Estimate Std. Error z value Pr(>|z|)
> (Intercept)                  -2.788243   0.303064  -9.200  < 2e-16 ***
> SiteHuffman Dam               1.838952   0.354127   5.193 2.07e-07 ***
> SiteNarrows                   2.252716   0.323184   6.970 3.16e-12 ***
> SiteSugar Creek             -12.961519 519.152077  -0.025 0.980082
> SpeciesFRAM                  13.938716 519.152230   0.027 0.978580
> SpeciesPLOC                   0.240223   0.540676   0.444 0.656824
> SpeciesULAM                   1.919586   0.540246   3.553 0.000381 ***
> DBH                           0.019984   0.001337  14.946  < 2e-16 ***
> SiteHuffman Dam:SpeciesFRAM -11.513823 519.152294  -0.022 0.982306
> SiteNarrows:SpeciesFRAM     -13.593127 519.152268  -0.026 0.979111
> SiteSugar Creek:SpeciesFRAM         NA         NA      NA       NA
> SiteHuffman Dam:SpeciesPLOC         NA         NA      NA       NA
> SiteNarrows:SpeciesPLOC       0.397503   0.555218   0.716 0.474028
> SiteSugar Creek:SpeciesPLOC  15.640450 519.152277   0.030 0.975966
> SiteHuffman Dam:SpeciesULAM  -0.102841   0.610027  -0.169 0.866124
> SiteNarrows:SpeciesULAM      -2.809092   0.606804  -4.629 3.67e-06 ***
> SiteSugar Creek:SpeciesULAM         NA         NA      NA       NA
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> (Dispersion parameter for poisson family taken to be 1)
>
>      Null deviance: 1972.3  on 544  degrees of freedom
> Residual deviance: 1178.7  on 531  degrees of freedom
> AIC: 1696.6
>
> Number of Fisher Scoring iterations: 13
> %%%%%%%%%%%%%%%%%%%%
>
> As you can see, the two models give very different output, especially
> in regards to whether or not the individual species are significant.
> In the no-interaction model, the only species that was not
> significant was ULAM.  In the one-way interaction model, ULAM was the
> only significant species.  My question is this: which model should I
> use when I present this analysis?  I know that the one-way
> interaction model has the lower AIC.  Should I base my choice solely
> on AIC?  The reasons I'm asking is that the second model has only one
> significant interaction term, fewer significant terms overall, and
> three undefined terms.
>
> Thanks for any guidance you can give to someone running his first
> Poisson regression.
>
> Jim Milks
>
> Graduate Student
> Environmental Sciences Ph.D. Program
> 136 Biological Sciences
> Wright State University
> 3640 Colonel Glenn Hwy
> Dayton, OH 45435
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From aserebre at win.tue.nl  Tue Jul 31 21:28:55 2007
From: aserebre at win.tue.nl (A Serebrenik)
Date: Tue, 31 Jul 2007 21:28:55 +0200 (CEST)
Subject: [R] contrasts error message in lm
Message-ID: <Pine.LNX.4.64.0707312124130.31125@pclin132>

Dear all,

I would like to find a linear regression model for a rather large dataset 
(27 independent variables). However, when I run lm the following error is 
reported:

> out <- lm(Result ~ AppealA + AppealsB + AppealC + AppealD + AppealE + 
Apply + ApplyAmount + Aprove + Closecase + Decidelocally + Healthassessment + 
HealthassessmentHealth + Postponedecision + Propertyassessment + PropertyassessmentPropertyvalue 
+ RejectA + RejectB + RejectC + RejectD + RejectE + Reportacceptance +
+ ReportrejectionA + ReportrejectionB + ReportrejectionC + ReportrejectionD +
+ ReportrejectionE + Timeout)

Error in `contrasts<-`(`*tmp*`, value = "contr.treatment") :
         contrasts can be applied only to factors with 2 or more levels


I've checked the documenation but somehow I fail to find an explanation 
what does it mean...

Thanks in advance,
Alexander Serebrenik


From jholtman at gmail.com  Tue Jul 31 21:30:27 2007
From: jholtman at gmail.com (jim holtman)
Date: Tue, 31 Jul 2007 15:30:27 -0400
Subject: [R] reading and storing files in the workspace
In-Reply-To: <s6af3834.086@ffdata.setur.fo>
References: <s6af3834.086@ffdata.setur.fo>
Message-ID: <644e1f320707311230j441c0567o8707f354403621e5@mail.gmail.com>

try:

for (i in test){
    assign(gsub(".txt$", "", i), read.table(i, header=TRUE))
}

On 7/31/07, Luis Ridao Cruz <Luisr at frs.fo> wrote:
> R-help,
>
> I have a vector containing (test) some file names.
> The files contents are matrixes.
>
> > test
>
>  [1] "aaOki.txt"    "aOki.txt"     "bOki.txt"     "c1Oki.txt"
> "c2Oki.txt"    "c3Oki.txt"    "cOki.txt"     "dOki.txt"     "dyp100.txt"
>  "dyp200.txt"
> [11] "dyp300.txt"   "dyp400.txt"   "dyp500.txt"   "dyp600.txt"
> "dyp700.txt"   "dyp800.txt"   "eOki.txt"     "FBdyp100.txt"
> "FBdyp150.txt" "FBdyp200.txt".............
>
> What I want to do is to import to R using the same file name
> and remove the ".txt" extension out of the object name.
> Something like this:
>
> for(i in test)
> gsub("\\.", "", paste(i, sep = "")) <- read.table(file = paste(i, sep =
> ""), header = TRUE)
>
> But I get the following message:
>
> Error in gsub("\\.", "", paste(i, sep = "")) <- read.table(file =
> paste(i,  :
>        target of assignment expands to non-language object
>
>
> Thanks in advance.
>
>
> > version
>               _
> platform       i386-pc-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status
> major          2
> minor          5.1
> year           2007
> month          06
> day            27
> svn rev        42083
> language       R
> version.string R version 2.5.1 (2007-06-27)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Jim Holtman
Cincinnati, OH
+1 513 646 9390

What is the problem you are trying to solve?


From info at aghmed.fsnet.co.uk  Tue Jul 31 19:21:22 2007
From: info at aghmed.fsnet.co.uk (Michael Dewey)
Date: Tue, 31 Jul 2007 18:21:22 +0100
Subject: [R] generating symmetric matrices
In-Reply-To: <155199.55586.qm@web63505.mail.re1.yahoo.com>
References: <40e66e0b0707300800w766f6eb7q36569596d1544254@mail.gmail.com>
	<155199.55586.qm@web63505.mail.re1.yahoo.com>
Message-ID: <Zen-1IFxW3-0001XP-VG@pythagoras.zen.co.uk>

At 16:29 30/07/2007, Gregory Gentlemen wrote:


>Douglas Bates <bates at stat.wisc.edu> wrote: On 7/27/07, Gregory 
>Gentlemen  wrote:
> > Greetings,
>
> > I have a seemingly simple task which I have not been able to 
> solve today. I want to construct a symmetric matrix of arbtriray 
> size w/o using loops. The following I thought would do it:
>
> > p <- 6
> > Rmat <- diag(p)
> > dat.cor <- rnorm(p*(p-1)/2)
> > Rmat[outer(1:p, 1:p, "<")] <- Rmat[outer(1:p, 1:p, ">")] <- dat.cor
>
> > However, the problem is that the matrix is filled by column and 
> so the resulting matrix is not symmetric.
>
>Could you provide more detail on the properties of the symmetric
>matrices that you would like to generate?  It seems that you are
>trying to generate correlation matrices.  Is that the case?  Do you
>wish the matrices to be a random sample from a specific distribution.
>If so, what distribution?
>
>Yes, my goal is to generate correlation matrices whose entries have 
>been sampled independently from a normal with a specified mean and variance.

Would it sufficient to use one of the results of
RSiteSearch("random multivariate normal", restrict = "functions")
or have I completely misunderstood what you want? (I appreciate this 
is not exactly what you say you want.)

>Thanks for the help.
>
>Greg
>
>
>---------------------------------
>
>         [[alternative HTML version deleted]]

Michael Dewey
http://www.aghmed.fsnet.co.uk


From HStevens at MUOhio.edu  Tue Jul 31 21:41:05 2007
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Tue, 31 Jul 2007 15:41:05 -0400
Subject: [R] italic greek symbols
In-Reply-To: <Pine.LNX.4.64.0707311957220.11902@gannet.stats.ox.ac.uk>
References: <A060D7CB-2138-4BEF-B6BA-B1277304FB92@MUOhio.edu>
	<Pine.LNX.4.64.0707311957220.11902@gannet.stats.ox.ac.uk>
Message-ID: <08F7F0C2-ACAF-4CFB-A5DE-8E486EAC1BE7@MUOhio.edu>

Thank you for this simple solution. It works well in R.app (R 2.5.1  
GUI 1.20 (4535) (4535)), but I can't figure out how to make it work  
in my Aquamacs+ESS.
When I attempt to set the locale, I get an error message. Any  
thoughts are appreciated.

Sys.setlocale("LC_CTYPE", "greek")
[1] ""
Warning message:
OS reports request to set locale to "greek" cannot be honored in:  
Sys.setlocale("LC_CTYPE", "greek")

Cheers,
Hank

On Jul 31, 2007, at 3:00 PM, Prof Brian Ripley wrote:

> On Tue, 31 Jul 2007, Martin Henry H. Stevens wrote:
>
>> Hi Folks,
>> I am using R 2.5.1 on a Mac OS X 10.4.9, via ESS.
>>
>> I would like to try to get an italic mu onto a plot axis label. I
>> note that in a previous email,
>> (Thu, 4 May 2006 19:41:41 +0100 (BST)), Brian Ripley wrote,
>> "There is no italic symbol font available on most devices. So  
>> unless you
>> try to plot Greek (not math symbol Greek) you are out of luck. "
>>
>> How does one plot Greek, or is there now another solution?
>
> Use the appropriate Unicode characters.  Lower case mu is \u03bc.
> Whether this works depends on the device and locale (as I said before)
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



Dr. Hank Stevens, Associate Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/~stevenmh/
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/

"E Pluribus Unum"


From marc_schwartz at comcast.net  Tue Jul 31 21:59:57 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 31 Jul 2007 14:59:57 -0500
Subject: [R] extract columns of a matrix/data frame
In-Reply-To: <905196.96786.qm@web31103.mail.mud.yahoo.com>
References: <905196.96786.qm@web31103.mail.mud.yahoo.com>
Message-ID: <1185911997.3632.77.camel@Bellerophon.localdomain>

On Tue, 2007-07-31 at 11:47 -0700, yuvika wrote:
> Hello,
>  
> Thanks for the immediate help. However, I have a question for you.
> let's say the matrix looks like this
>  
> name      a1   a2   b1   b2   c1   c2
> 0            4     2     7     8     1     2
> 0            3     6     9     2     2    9
> 1            2     7      9    2     4     2
> 1            3      2     2     6     7     8
> 2            2      7      8     3    4     2
> 3           4      6      8     9     0     2
> 3           6      8      9     3     6     7
>  
> Now, what i want to do is  still make submatrices but now make 3
> matrices(based on a,b,c  just like before) for name=0, 3 matrices for
> name=1 and so on..
> how can i do this?
>  
> looking forward for your help.
> thanks
> yuvika

Yuvika,

Please be sure to 'reply to all' so that the list thread stays intact
and can be of benefit to others in the archive.  Otherwise knowledge
transfer is lost...

In this case, we can split() the initial matrix based upon the 'name'
column and then still use the initial solution, with modifications. In
effect, we end up with 'nested' loops:


> MAT
     name a1 a2 b1 b2 c1 c2
[1,]    0  4  2  7  8  1  2
[2,]    0  3  6  9  2  2  9
[3,]    1  2  7  9  2  4  2
[4,]    1  3  2  2  6  7  8
[5,]    2  2  7  8  3  4  2
[6,]    3  4  6  8  9  0  2
[7,]    3  6  8  9  3  6  7


We first need to coerce the matrix to a data frame to use this approach:

DF <- as.data.frame(MAT)

> DF
  name a1 a2 b1 b2 c1 c2
1    0  4  2  7  8  1  2
2    0  3  6  9  2  2  9
3    1  2  7  9  2  4  2
4    1  3  2  2  6  7  8
5    2  2  7  8  3  4  2
6    3  4  6  8  9  0  2
7    3  6  8  9  3  6  7


# split() DF by the 'name' column
# strip the 'name' column while we are at it
DF.split <- split(DF[, -1], DF$name)


> DF.split
$`0`
  a1 a2 b1 b2 c1 c2
1  4  2  7  8  1  2
2  3  6  9  2  2  9

$`1`
  a1 a2 b1 b2 c1 c2
3  2  7  9  2  4  2
4  3  2  2  6  7  8

$`2`
  a1 a2 b1 b2 c1 c2
5  2  7  8  3  4  2

$`3`
  a1 a2 b1 b2 c1 c2
6  4  6  8  9  0  2
7  6  8  9  3  6  7


Now use lapply() to navigate the above list, then use the initial
solution with lapply() instead of sapply() on each data frame within the
list:

RES <- lapply(DF.split, 
              function(x) sapply(letters[1:3], 
                                 function(i) x[, grep(i, colnames(x))]))


> RES
$`0`
$`0`[[1]]
  a1 a2
1  4  2
2  3  6

$`0`[[2]]
  b1 b2
1  7  8
2  9  2

$`0`[[3]]
  c1 c2
1  1  2
2  2  9


$`1`
$`1`[[1]]
  a1 a2
3  2  7
4  3  2

$`1`[[2]]
  b1 b2
3  9  2
4  2  6

$`1`[[3]]
  c1 c2
3  4  2
4  7  8


$`2`
$`2`[[1]]
  a1 a2
5  2  7

$`2`[[2]]
  b1 b2
5  8  3

$`2`[[3]]
  c1 c2
5  4  2


$`3`
$`3`[[1]]
  a1 a2
6  4  6
7  6  8

$`3`[[2]]
  b1 b2
6  8  9
7  9  3

$`3`[[3]]
  c1 c2
6  0  2
7  6  7



HTH,

Marc


From gangchen at mail.nih.gov  Tue Jul 31 22:05:52 2007
From: gangchen at mail.nih.gov (Gang Chen)
Date: Tue, 31 Jul 2007 16:05:52 -0400
Subject: [R] Linear model without an intercept
In-Reply-To: <3D3D2D2C-2D9E-4FF5-A897-266A980EC0C1@mail.nih.gov>
References: <1DF7DB4AB44EFB41A60A889186D43359120A2F@srv-laminiere.arvalis-fr.com>
	<46ADD70B.6020502@kvl.dk>
	<3D3D2D2C-2D9E-4FF5-A897-266A980EC0C1@mail.nih.gov>
Message-ID: <00CDF3E0-289C-4F80-BE82-1CA3A5A0014D@mail.nih.gov>

I would like to run a regression analysis without a constant  
(intercept) or a special one-way within-subject (repeated-measures)  
ANOVA. I'm not sure if the following command lines are correct or not:

m1 <- lme(Resp ~ Cond - 1, random = ~  Cond - 1 | Subj, TestData)

or,

m2 <- lmer(Resp ~ Cond - 1 +( Cond - 1 | Subj), TestData)

Also I notice that both lme and lmer provide the correlation matrix  
of the fixed effects. So if there are more than 2 levels in the fixed  
effect of condition (Cond), how can I correct for sphericity  
violation in lme and lmer if I want to get an F test for condition  
effect?

Thanks,
Gang


From fgaravito at guggenheimadvisors.com  Tue Jul 31 20:03:51 2007
From: fgaravito at guggenheimadvisors.com (Garavito,Fabian)
Date: Tue, 31 Jul 2007 19:03:51 +0100
Subject: [R] Naming rows/columns in a 3 dimensional array/dataframe
Message-ID: <BA9B873BB7132F4B8853E1DE580C30194FB8B3@MGEXCLP1.investment.boi>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/aeff7fe7/attachment.pl 

From fgaravito at guggenheimadvisors.com  Tue Jul 31 22:06:45 2007
From: fgaravito at guggenheimadvisors.com (Garavito,Fabian)
Date: Tue, 31 Jul 2007 21:06:45 +0100
Subject: [R] Error message when running lm() with na.action=NULL
Message-ID: <BA9B873BB7132F4B8853E1DE580C30194FB8B5@MGEXCLP1.investment.boi>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/2034d40f/attachment.pl 

From marc_schwartz at comcast.net  Tue Jul 31 22:15:08 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 31 Jul 2007 15:15:08 -0500
Subject: [R] contrasts error message in lm
In-Reply-To: <Pine.LNX.4.64.0707312124130.31125@pclin132>
References: <Pine.LNX.4.64.0707312124130.31125@pclin132>
Message-ID: <1185912908.3632.91.camel@Bellerophon.localdomain>

On Tue, 2007-07-31 at 21:28 +0200, A Serebrenik wrote:
> Dear all,
> 
> I would like to find a linear regression model for a rather large dataset 
> (27 independent variables). However, when I run lm the following error is 
> reported:
> 
> > out <- lm(Result ~ AppealA + AppealsB + AppealC + AppealD + AppealE + 
> Apply + ApplyAmount + Aprove + Closecase + Decidelocally + Healthassessment + 
> HealthassessmentHealth + Postponedecision + Propertyassessment + PropertyassessmentPropertyvalue 
> + RejectA + RejectB + RejectC + RejectD + RejectE + Reportacceptance +
> + ReportrejectionA + ReportrejectionB + ReportrejectionC + ReportrejectionD +
> + ReportrejectionE + Timeout)
> 
> Error in `contrasts<-`(`*tmp*`, value = "contr.treatment") :
>          contrasts can be applied only to factors with 2 or more levels
> 
> 
> I've checked the documenation but somehow I fail to find an explanation 
> what does it mean...
> 
> Thanks in advance,
> Alexander Serebrenik


One or more of your IV's has a single factor level.  In other words for
example, instead of having "Yes" and "No" values, it only has one of the
two. Treatment contrasts are predicated on having a base or reference
level, against which the other levels for the factor are compared.

This can occur either natively in the source dataset, or as a
consequence of removing rows in your dataset that have missing values.
In the latter case, a factor with multiple levels may end up having a
single level as a consequence of the removal of all rows with the other
levels of that factor.

By default, R (and many other statistical applications) will remove
records containing missing values prior to applying regression
techniques on the dataset.

See ?lm and ?na.action (referenced in the former) for more information.

You can use:

  DF.tmp <- na.omit(DF)

where 'DF' is your source dataset, to remove rows with missing values.

Then use:

  summary(DF.tmp)

to review the residual dataset created and see what you have left.


In looking at your formula above, it may be that your variables are not
in a single data frame (I don't see a 'data' argument). In which case,
you can achieve a similar result by using:

  DF <- model.frame(Result ~ TheRestOfYourFormulaHere..)


and then using:

  summary(DF)


See ?model.frame

HTH,

Marc Schwartz


From thomas.pujol at yahoo.com  Tue Jul 31 22:18:32 2007
From: thomas.pujol at yahoo.com (Thomas Pujol)
Date: Tue, 31 Jul 2007 13:18:32 -0700 (PDT)
Subject: [R] aggregate.data.frame - prevent conversion to factors? show
	statistics for NA values of "by" variable?
Message-ID: <574927.90748.qm@web59315.mail.re1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20070731/ed355ea0/attachment.pl 

From ripley at stats.ox.ac.uk  Tue Jul 31 22:29:18 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 21:29:18 +0100 (BST)
Subject: [R] contrasts error message in lm
In-Reply-To: <Pine.LNX.4.64.0707312124130.31125@pclin132>
References: <Pine.LNX.4.64.0707312124130.31125@pclin132>
Message-ID: <Pine.LNX.4.64.0707312128180.3397@auk.stats>

On Tue, 31 Jul 2007, A Serebrenik wrote:

> Dear all,
>
> I would like to find a linear regression model for a rather large dataset
> (27 independent variables). However, when I run lm the following error is
> reported:
>
>> out <- lm(Result ~ AppealA + AppealsB + AppealC + AppealD + AppealE +
> Apply + ApplyAmount + Aprove + Closecase + Decidelocally + Healthassessment +
> HealthassessmentHealth + Postponedecision + Propertyassessment + PropertyassessmentPropertyvalue
> + RejectA + RejectB + RejectC + RejectD + RejectE + Reportacceptance +
> + ReportrejectionA + ReportrejectionB + ReportrejectionC + ReportrejectionD +
> + ReportrejectionE + Timeout)
>
> Error in `contrasts<-`(`*tmp*`, value = "contr.treatment") :
>         contrasts can be applied only to factors with 2 or more levels
>
>
> I've checked the documenation but somehow I fail to find an explanation
> what does it mean...

So one of your 'variables' isn't: it is a factor with only one level?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jul 31 22:31:47 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Jul 2007 21:31:47 +0100 (BST)
Subject: [R] italic greek symbols
In-Reply-To: <08F7F0C2-ACAF-4CFB-A5DE-8E486EAC1BE7@MUOhio.edu>
References: <A060D7CB-2138-4BEF-B6BA-B1277304FB92@MUOhio.edu>
	<Pine.LNX.4.64.0707311957220.11902@gannet.stats.ox.ac.uk>
	<08F7F0C2-ACAF-4CFB-A5DE-8E486EAC1BE7@MUOhio.edu>
Message-ID: <Pine.LNX.4.64.0707312129540.3397@auk.stats>

On Tue, 31 Jul 2007, Martin Henry H. Stevens wrote:

> Thank you for this simple solution. It works well in R.app (R 2.5.1
> GUI 1.20 (4535) (4535)), but I can't figure out how to make it work
> in my Aquamacs+ESS.
> When I attempt to set the locale, I get an error message. Any
> thoughts are appreciated.
>
> Sys.setlocale("LC_CTYPE", "greek")
> [1] ""
> Warning message:
> OS reports request to set locale to "greek" cannot be honored in:
> Sys.setlocale("LC_CTYPE", "greek")

You need to set a UTF-8 locale (and I believe that is all there are on 
MacOS, but of course it uses non-standard locale names: our MacOS box 
seems to be switched off so I cannot check easily).

>
> Cheers,
> Hank
>
> On Jul 31, 2007, at 3:00 PM, Prof Brian Ripley wrote:
>
>> On Tue, 31 Jul 2007, Martin Henry H. Stevens wrote:
>>
>>> Hi Folks,
>>> I am using R 2.5.1 on a Mac OS X 10.4.9, via ESS.
>>>
>>> I would like to try to get an italic mu onto a plot axis label. I
>>> note that in a previous email,
>>> (Thu, 4 May 2006 19:41:41 +0100 (BST)), Brian Ripley wrote,
>>> "There is no italic symbol font available on most devices. So
>>> unless you
>>> try to plot Greek (not math symbol Greek) you are out of luck. "
>>>
>>> How does one plot Greek, or is there now another solution?
>>
>> Use the appropriate Unicode characters.  Lower case mu is \u03bc.
>> Whether this works depends on the device and locale (as I said before)
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
>
> Dr. Hank Stevens, Associate Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/~stevenmh/
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
>
> "E Pluribus Unum"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at comcast.net  Tue Jul 31 22:35:23 2007
From: marc_schwartz at comcast.net (Marc Schwartz)
Date: Tue, 31 Jul 2007 15:35:23 -0500
Subject: [R] Naming rows/columns in a 3 dimensional array/dataframe
In-Reply-To: <BA9B873BB7132F4B8853E1DE580C30194FB8B3@MGEXCLP1.investment.boi>
References: <BA9B873BB7132F4B8853E1DE580C30194FB8B3@MGEXCLP1.investment.boi>
Message-ID: <1185914123.3632.100.camel@Bellerophon.localdomain>

On Tue, 2007-07-31 at 19:03 +0100, Garavito,Fabian wrote:
> Can I assign names to rows/columns in a i x k x j matrix / dataframe?

You can use an array:

ARR <- array(1:18, c(3, 3, 3), 
             dimnames = list(LETTERS[1:3], LETTERS[4:6], LETTERS[7:9]))

> ARR
, , G

  D E F
A 1 4 7
B 2 5 8
C 3 6 9

, , H

   D  E  F
A 10 13 16
B 11 14 17
C 12 15 18

, , I

  D E F
A 1 4 7
B 2 5 8
C 3 6 9


> ARR["B", "E", "H"]
[1] 14


See ?array

By definition, matrices and dataframes are two dimensional, though you
can have a list containing N of them.

HTH,

Marc Schwartz


From mark_difford at yahoo.co.uk  Tue Jul 31 23:55:58 2007
From: mark_difford at yahoo.co.uk (Mark Difford)
Date: Tue, 31 Jul 2007 14:55:58 -0700 (PDT)
Subject: [R] Data mining tools
In-Reply-To: <OF4674997A.E95A881F-ON85257329.004B64D9@spgdag.ca>
References: <OF4674997A.E95A881F-ON85257329.004B64D9@spgdag.ca>
Message-ID: <11935911.post@talk.nabble.com>


Hi G?rald,

I can't help you directly, but you haven't yet had a reply, so...

Googling, as you have found, will waste your time if you know more that you
Google for.  Clementine's quite unusual --- in the field of statistical
methods --- so target that.  Take the main Stats methods journals and search
them; or try something like CiteSeer:

http://citeseer.ist.psu.edu/cis?q=clementine&cs=1

Motto: Always write it down, even if that's all you...end up spending your
time doing.

Hope this helps,

Regards,
Mark.


gerald.jean wrote:
> 
> Hello there, apologies for cross-posting
> 
> my question is not an S/R question but there is so much knowledge
> concentrated in those lists that I thought someone could point me in the
> right direction.
> 
> A few months ago I read an article in a referenced journal comparing some
> data mining programs, among which there was Insightful's I Miner, SAS'
> Entreprise Miner, SPSS' Clementine (I think) and a few others.
> Unfortunately I can't remember in which journal was the article published
> or who was the author?  I have been Googling a lot to try to locate the
> article but to no avail!  Would someone know who published the article and
> in which journal?  By the way, any serious, published comparisons of data
> mining programs would be welcomed as the company I work for is planning to
> add a data mining program to our tool box soon.
> 
> Thanks for any leads,
> 
> G?rald Jean
> Conseiller senior en statistiques, Actuariat
> t?lephone            : (418) 835-4900 poste (7639)
> t?lecopieur          : (418) 835-6657
> courrier ?lectronique: gerald.jean at dgag.ca
> 
> "In God we trust, all others must bring data"  W. Edwards Deming
> 
> Le message ci-dessus, ainsi que les documents l'accompagnant, sont
> destin?s
> uniquement aux personnes identifi?es et peuvent contenir des informations
> privil?gi?es, confidentielles ou ne pouvant ?tre divulgu?es. Si vous avez
> re?u ce message par erreur, veuillez le d?truire.
> 
> This communication (and/or the attachments) is intended for named
> recipients only and may contain privileged or confidential information
> which is not to be disclosed. If you received this communication by
> mistake
> please destroy all copies.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/Data-mining-tools-tf4193959.html#a11935911
Sent from the R help mailing list archive at Nabble.com.


From michal_sa at hotmail.com  Tue Jul 31 23:48:22 2007
From: michal_sa at hotmail.com (michal33)
Date: Tue, 31 Jul 2007 14:48:22 -0700 (PDT)
Subject: [R] A simple question about summary.glm
In-Reply-To: <46AF05E4.1060305@statistik.uni-dortmund.de>
References: <11857514.post@talk.nabble.com>
	<46AF05E4.1060305@statistik.uni-dortmund.de>
Message-ID: <11935815.post@talk.nabble.com>


Thanks Uwe for your question,

Yesterday I found out what I wanted to know: 
In the summary table the values refer to the different of each treatment to
the first treatment alphabetical (in my case it was the control group and
therefore served as Tukey test, which is what I wanted in the first place).



Uwe Ligges wrote:
> 
> 
> 
> michal33 wrote:
>> Hello,
>> 
>> I am new to R and have tried to search similar questions but could not
>> find
>> exactly what I am looking for, but I apologize if the question was
>> already
>> asked.
>> 
>> I have 10 different treatments and want to know whether they affect the
>> sex
>> ratios of insect emergence. After running the glms I got this table:
>> 
>>       Df Deviance Resid. Df Resid. Dev      F   Pr(>F)   
>> NULL                    133     9250.3                   
>> sex    1    481.5       132     8768.9 7.7212 0.006314 **
>> trt    9   1099.1       123     7669.7 1.9585 0.049780 * 
>> 
>> But now I would like to know WHICH of the treatments was significant. I
>> tried to use Tukey test but for some reason it does not work. 
>> My question is:
>> I used the following function: 
>>> summary(file.name, corr=F)
>> and got the following table:
>> Deviance Residuals: 
>>     Min       1Q   Median       3Q      Max  
>> -14.118   -4.808   -1.466    2.033   33.882  
>> Coefficients:
>>               Estimate Std. Error   t value Pr(>|t|)    
>> (Intercept)  8.696e+00  1.893e+00     4.594 1.06e-05 ***
>> sexm        -3.791e+00  1.364e+00    -2.779  0.00631 ** 
>> trtccc      -1.050e+00  4.325e+00    -0.243  0.80859    
>> trtcga3      2.450e+00  4.325e+00     0.566  0.57211    
>> trtcga4     -2.300e+00  4.325e+00    -0.532  0.59584    
>> trtg         1.550e+00  2.497e+00     0.621  0.53593    
>> trtga4      -5.550e+00  4.325e+00    -1.283  0.20183    
>> trtp         5.422e+00  2.566e+00     2.113  0.03658 *  
>> trtpg       -1.850e+00  2.497e+00    -0.741  0.46019    
>> trtw        -3.634e-17  2.497e+00 -1.46e-17  1.00000    
>> trtwg       -3.750e+00  2.497e+00    -1.502  0.13573    
>> 
>> What do the stars  mean?
> 
> 
> Well, you omitted the last lines from the output which include:
> 
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> 
> i.e. *** for values < 0.001; * for values in [0.01, 0.05) etc.
> 
> Uwe Ligges
> 
> 
> 
>   Is it the same as Tukey test that tells me which
>> treatment is different from which? i.e. is trtp (with *) significantly
>> different to the control (which, by the way do not appear in this list
>> and I
>> do not know why)? 
>> 
>> Thanks
>> Michal
>> 
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 

-- 
View this message in context: http://www.nabble.com/A-simple-question-about-summary.glm-tf4167757.html#a11935815
Sent from the R help mailing list archive at Nabble.com.


