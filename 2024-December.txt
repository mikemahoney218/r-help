From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Sun Dec  1 03:27:29 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Sun, 1 Dec 2024 02:27:29 +0000
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
Message-ID: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>

Dear R help folks,

First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!

I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.

e.g. if my original data is 
 olddata
   ID date
    1     1
    1     1
    1     2
    1     2
    1     3
    1     3
    1     4
    1     4
    1     5
    1     5
    2     5
    2     5
    2     5
    2     6
    2     6
    2     6
    3   10
    3   10

the new data will be
newdata
   ID date  first
    1     1       1
    1     1       0
    1     2       0
    1     2       0
    1     3       0
    1     3       0
    1     4       0
    1     4       0
    1     5       0
    1     5       0
    2     5       1
    2     5       0
    2     5       0
    2     6       0
    2     6       0
    2     6       0
    3   10       1
    3   10       0

When I run the program below, I receive the following error:
Error in df[, "ID"] : incorrect number of dimensions

My code:
# Create data.frame
ID <- c(rep(1,10),rep(2,6),rep(3,2))
date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
          rep(5,3),rep(6,3),rep(10,2))
olddata <- data.frame(ID=ID,date=date)
class(olddata)
cat("This is the original data frame","\n")
print(olddata)
 
# This function is supposed to identify the first row 
# within each level of ID and, for the first row, set
# the variable first to 1, and for all rows other than
# the first row set first to 0.
mydoit <- function(df){
  value <- ifelse (first(df[,"ID"]),1,0)
  cat("value=",value,"\n")
  df[,"first"] <- value
}
newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)

Thank you,
John


John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?
PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




From bbo|ker @end|ng |rom gm@||@com  Sun Dec  1 03:35:38 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 30 Nov 2024 21:35:38 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <CABghstS905fhe3rLmbJtfBxDhzLcpqwmCD1jObsNhqVmTznkQw@mail.gmail.com>

I think as.numeric(! duplicated(group)) might do this for you ...

On Sat, Nov 30, 2024, 9:27 PM Sorkin, John <jsorkin at som.umaryland.edu>
wrote:

> Dear R help folks,
>
> First my apologizes for sending several related questions to the list
> server. I am trying to learn how to manipulate data in R . . . and am
> having difficulty getting my program to work. I greatly appreciate the help
> and support list member give!
>
> I am trying to write a program that will run through a data frame
> organized by ID and for the first line of each new group of data lines that
> has the same ID create a new variable first that will be 1 for the first
> line of the group and 0 for all other lines.
>
> e.g. if my original data is
>  olddata
>    ID date
>     1     1
>     1     1
>     1     2
>     1     2
>     1     3
>     1     3
>     1     4
>     1     4
>     1     5
>     1     5
>     2     5
>     2     5
>     2     5
>     2     6
>     2     6
>     2     6
>     3   10
>     3   10
>
> the new data will be
> newdata
>    ID date  first
>     1     1       1
>     1     1       0
>     1     2       0
>     1     2       0
>     1     3       0
>     1     3       0
>     1     4       0
>     1     4       0
>     1     5       0
>     1     5       0
>     2     5       1
>     2     5       0
>     2     5       0
>     2     6       0
>     2     6       0
>     2     6       0
>     3   10       1
>     3   10       0
>
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
>
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>           rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>   value <- ifelse (first(df[,"ID"]),1,0)
>   cat("value=",value,"\n")
>   df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
>
> Thank you,
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical
> Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of
> Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> <https://www.google.com/maps/search/10+North+Greene+Street?entry=gmail&source=g>
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From cry@n @end|ng |rom b|ngh@mton@edu  Sun Dec  1 03:46:57 2024
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Sat, 30 Nov 2024 21:46:57 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <600b676e-9dab-8f64-e492-39ad4f4ca281@binghamton.edu>

Personally, I'd do this in the tidyverse with dplyr and its row_number()
function.

olddata %>% group_by(ID) %>% mutate(first = as.integer(row_number() == 1))

--Chris Ryan

Sorkin, John wrote:
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>           rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)


From cry@n @end|ng |rom b|ngh@mton@edu  Sun Dec  1 03:51:48 2024
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Sat, 30 Nov 2024 21:51:48 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <600b676e-9dab-8f64-e492-39ad4f4ca281@binghamton.edu>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <600b676e-9dab-8f64-e492-39ad4f4ca281@binghamton.edu>
Message-ID: <f2250299-b8b5-a8a5-aacb-3779e51c397c@binghamton.edu>

Sorry, for completeness:

library(dplyr)
olddata %>% group_by(ID) %>% mutate(first = as.integer(row_number() == 1))

--Chris Ryan


Christopher W. Ryan wrote:
> Personally, I'd do this in the tidyverse with dplyr and its row_number()
> function.
> 
> olddata %>% group_by(ID) %>% mutate(first = as.integer(row_number() == 1))
> 
> --Chris Ryan
> 
> Sorkin, John wrote:
>> ID <- c(rep(1,10),rep(2,6),rep(3,2))
>> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>>           rep(5,3),rep(6,3),rep(10,2))
>> olddata <- data.frame(ID=ID,date=date)


From rmh @end|ng |rom temp|e@edu  Sun Dec  1 04:54:47 2024
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Sun, 1 Dec 2024 03:54:47 +0000
Subject: [R] 
 [External]  Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <ACBADD34-16BF-4F7F-99BC-3D1DE568CD48@temple.edu>

tmp.ID <- unique(olddata$ID)
Firsts <- match(tmp.ID, olddata$ID)
newdata <- cbind(olddata, First=0)
newdata$First[Firsts] <- 1
newdata

newdata$FirstDay <- 0
for (id in tmp.ID)
  newdata$FirstDay[newdata$ID == id] <- newdata$date[newdata$ID == id][1]
newdata


> On Nov 30, 2024, at 21:27, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Dear R help folks,
>
> First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
>
> I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
>
> e.g. if my original data is
> olddata
>   ID date
>    1     1
>    1     1
>    1     2
>    1     2
>    1     3
>    1     3
>    1     4
>    1     4
>    1     5
>    1     5
>    2     5
>    2     5
>    2     5
>    2     6
>    2     6
>    2     6
>    3   10
>    3   10
>
> the new data will be
> newdata
>   ID date  first
>    1     1       1
>    1     1       0
>    1     2       0
>    1     2       0
>    1     3       0
>    1     3       0
>    1     4       0
>    1     4       0
>    1     5       0
>    1     5       0
>    2     5       1
>    2     5       0
>    2     5       0
>    2     6       0
>    2     6       0
>    2     6       0
>    3   10       1
>    3   10       0
>
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
>
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>          rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>  value <- ifelse (first(df[,"ID"]),1,0)
>  cat("value=",value,"\n")
>  df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
>
> Thank you,
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sun Dec  1 05:33:42 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 30 Nov 2024 20:33:42 -0800
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>

May I ask *why* you want to do this?

It sounds to me like like you're using SAS-like strategies for your
data analysis rather than R-like.

-- Bert

-- Bert

On Sat, Nov 30, 2024 at 6:27?PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Dear R help folks,
>
> First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
>
> I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
>
> e.g. if my original data is
>  olddata
>    ID date
>     1     1
>     1     1
>     1     2
>     1     2
>     1     3
>     1     3
>     1     4
>     1     4
>     1     5
>     1     5
>     2     5
>     2     5
>     2     5
>     2     6
>     2     6
>     2     6
>     3   10
>     3   10
>
> the new data will be
> newdata
>    ID date  first
>     1     1       1
>     1     1       0
>     1     2       0
>     1     2       0
>     1     3       0
>     1     3       0
>     1     4       0
>     1     4       0
>     1     5       0
>     1     5       0
>     2     5       1
>     2     5       0
>     2     5       0
>     2     6       0
>     2     6       0
>     2     6       0
>     3   10       1
>     3   10       0
>
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
>
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>           rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>   value <- ifelse (first(df[,"ID"]),1,0)
>   cat("value=",value,"\n")
>   df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
>
> Thank you,
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Sun Dec  1 07:18:09 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Sun, 1 Dec 2024 01:18:09 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
Message-ID: <010901db43b8$cb3408b0$619c1a10$@gmail.com>

I was wondering along similar lines, Bert.

One way to get help is to ask how to do some single step of a larger strategy. That can lead to answers that may not be as applicable to the scenario.

Another way would be to include a synopsis of what they are trying to do.

But, as John says he is trying to learn and improve his abilities, perhaps he s getting what he wants.
After watching some of the exchanges in multiple questions, many seem to revolve around a wish to deal with sorted grouped data. He seems to have looked at some base R methods as well as packages like dplyr using tibbles as well as another package and format.

What interests me from a dplyr perspective is how many little embedded functions it makes available and some have been mentioned here. If you want to  add a column that contains the same value for each group, such as the minimum, mean, first and many other things, it is very easily doable.

The latest request seems to be a bit different as it wants a column with a 1 (presumably for TRUE) only for the first entry in  the group. Again, fairly easy using one of several hooks such as the rownumber being "1" versus not. There are many variations on the answer supplied depending on style and need, such as making a column that contains the row number, and in a later step, set those to zero that are not a one. 

But sometimes you want to ask what the overall algorithm is. Do you need extra columns to then use for some purpose, or could that purpose have been done another way such as doing some calculation only when rownumber is one.

As noted, R makes some operations fairly natural, in ways that differ from the "natural" way another program/environment does it. Sometimes a translation is not worth doing as compared to a reworked algorithm that makes good use of whichever package and related functionality you want to use. 

Assuming all these questions relate to the same project, I am not clear if and where the lookback at previous row/value fits.

Of course, John may not be free to share more in public.

Anyone want to suggest a book or two on data processing of this sort using R that might illustrate with examples galore on how various problems are solved and then perhaps some will be similar enough ...

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Saturday, November 30, 2024 11:34 PM
To: Sorkin, John <jsorkin at som.umaryland.edu>
Cc: r-help at r-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows

May I ask *why* you want to do this?

It sounds to me like like you're using SAS-like strategies for your
data analysis rather than R-like.

-- Bert

-- Bert

On Sat, Nov 30, 2024 at 6:27?PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Dear R help folks,
>
> First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
>
> I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
>
> e.g. if my original data is
>  olddata
>    ID date
>     1     1
>     1     1
>     1     2
>     1     2
>     1     3
>     1     3
>     1     4
>     1     4
>     1     5
>     1     5
>     2     5
>     2     5
>     2     5
>     2     6
>     2     6
>     2     6
>     3   10
>     3   10
>
> the new data will be
> newdata
>    ID date  first
>     1     1       1
>     1     1       0
>     1     2       0
>     1     2       0
>     1     3       0
>     1     3       0
>     1     4       0
>     1     4       0
>     1     5       0
>     1     5       0
>     2     5       1
>     2     5       0
>     2     5       0
>     2     6       0
>     2     6       0
>     2     6       0
>     3   10       1
>     3   10       0
>
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
>
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>           rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>   value <- ifelse (first(df[,"ID"]),1,0)
>   cat("value=",value,"\n")
>   df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
>
> Thank you,
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Dec  1 08:05:24 2024
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 1 Dec 2024 07:05:24 +0000
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>

?s 02:27 de 01/12/2024, Sorkin, John escreveu:
> Dear R help folks,
> 
> First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> 
> I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> 
> e.g. if my original data is
>   olddata
>     ID date
>      1     1
>      1     1
>      1     2
>      1     2
>      1     3
>      1     3
>      1     4
>      1     4
>      1     5
>      1     5
>      2     5
>      2     5
>      2     5
>      2     6
>      2     6
>      2     6
>      3   10
>      3   10
> 
> the new data will be
> newdata
>     ID date  first
>      1     1       1
>      1     1       0
>      1     2       0
>      1     2       0
>      1     3       0
>      1     3       0
>      1     4       0
>      1     4       0
>      1     5       0
>      1     5       0
>      2     5       1
>      2     5       0
>      2     5       0
>      2     6       0
>      2     6       0
>      2     6       0
>      3   10       1
>      3   10       0
> 
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
> 
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>            rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>   
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>    value <- ifelse (first(df[,"ID"]),1,0)
>    cat("value=",value,"\n")
>    df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> 
> Thank you,
> John
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
> 
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,

And here are two other solutions.


olddata$first <- with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == 
x[1L]))

olddata$first <- c(1L, diff(olddata$ID))


Of these two, diff is faster. But of all the solutions posted so far, 
Ben Bolker's is the fastest. And it can be made a little faster if 
as.integer substitutes for as.numeric.
And dplyr::mutate now has a .by argument, which avoids explicit the call 
to group_by, with a performance gain.


library(microbenchmark)

mb <- microbenchmark(
   ave = with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == x[1L])),
   dup_num = as.numeric(! duplicated(olddata$ID)),
   dup_int = as.integer(! duplicated(olddata$ID)),
   diff = diff = c(1L, diff(olddata$ID)),
   dplyr_grp = olddata %>% group_by(ID) %>% mutate(first = 
as.integer(row_number() == 1)),
   dplyr = olddata %>% mutate(first = as.integer(row_number() == 1), .by 
= ID)
)
print(mb, order = "median")



However, note that dplyr operates in entire data.frames and therefore is 
expected to be slower when tested against instructions that process one 
column only.


Hope this helps,

Rui Barradas


-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Dec  1 08:15:37 2024
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 1 Dec 2024 07:15:37 +0000
Subject: [R] dplyr summarize by groups
In-Reply-To: <005b01db3d6b$ed2288a0$c76799e0$@yahoo.com>
References: <005b01db3d6b$ed2288a0$c76799e0$.ref@yahoo.com>
 <005b01db3d6b$ed2288a0$c76799e0$@yahoo.com>
Message-ID: <d88925f6-9316-499a-b90b-46e38e696786@sapo.pt>

?s 05:52 de 23/11/2024, tgs77m--- via R-help escreveu:
> # Get mean, min, max sigma and skew by group
> 
>   options (digits = 3)
>   library (ISwR
> data(energy)
> 
> data %>%
>    group_by(stature) %>%
>    summarize(
>      Mean = mean(expend),
>      Min =  min(expend),
>      Max = max(expend),
>      Sigma = sd(expend),
>      Skew = skew(expend))
> 
> # Output
> 
>    stature  Mean   Min   Max Sigma  Skew
>    <fct>   <dbl> <dbl> <dbl> <dbl> <dbl>
> 1 lean     8.07  6.13  10.9  1.24 0.907
> 2 obese   10.3   8.79  12.8  1.40 0.587
> 
> Why does output stats vary in decimal places even when options (digits=3)
> were set?
> 
> All the best
> 
> Thomas S.
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,


"Why does output stats vary in decimal places even when options 
(digits=3) were set?"

Yes, they do but shouldn't they? I'm seeing all numbers with 3 digits. 
Display digits and rounding are not the same thing.

Also, you don't need to load the package to have access to one of its 
data sets,


data(energy, package = "ISwR")


will load it. In this case, it's probably even better, "energy" is not 
an uncommon source for data and there might be data sets with the same 
name in other packages.


Hope this helps,

Rui Barradas




-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sun Dec  1 13:46:06 2024
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sun, 1 Dec 2024 12:46:06 +0000
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <FC7E5425-9A46-4C5D-92D9-2584ABE02A46@utoronto.ca>


olddata$first <- as.numeric(! duplicated(olddata$ID))


:-)




> On Nov 30, 2024, at 22:27, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>          rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)


From bgunter@4567 @end|ng |rom gm@||@com  Sun Dec  1 17:30:03 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 1 Dec 2024 08:30:03 -0800
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>
Message-ID: <CAGxFJbTUEFgeeDXe0rpAe4EfGN8edJAC1oVSjMTKuat-iBkSww@mail.gmail.com>

Rui:
"f these two, diff is faster. But of all the solutions posted so far,
Ben Bolker's is the fastest."

But the explicit version of diff is still considerably faster:

> D <- c(rep(1,10),rep(2,6),rep(3,2))

> microbenchmark(c(1L,diff(D)), times = 1000L)
Unit: microseconds
           expr   min    lq    mean median    uq    max neval
 c(1L, diff(D)) 3.075 3.198 3.34396   3.28 3.362 29.684  1000

> microbenchmark( as.integer(!duplicated(D)), times =1000L)
Unit: microseconds
                       expr   min    lq     mean median   uq  max neval
 as.integer(!duplicated(D)) 1.476 1.558 1.644264  1.599 1.64 16.4  1000

> microbenchmark( D - c(0L, D[-length(D)]), times = 1000L)
Unit: nanoseconds  ## note that unit is nanoseconds not microseconds
                     expr min  lq    mean median  uq  max neval
 D - c(0L, D[-length(D)]) 369 410 489.335    492 533 9840  1000

Cheers,
Bert

On Sat, Nov 30, 2024 at 11:05?PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> ?s 02:27 de 01/12/2024, Sorkin, John escreveu:
> > Dear R help folks,
> >
> > First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> >
> > I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> >
> > e.g. if my original data is
> >   olddata
> >     ID date
> >      1     1
> >      1     1
> >      1     2
> >      1     2
> >      1     3
> >      1     3
> >      1     4
> >      1     4
> >      1     5
> >      1     5
> >      2     5
> >      2     5
> >      2     5
> >      2     6
> >      2     6
> >      2     6
> >      3   10
> >      3   10
> >
> > the new data will be
> > newdata
> >     ID date  first
> >      1     1       1
> >      1     1       0
> >      1     2       0
> >      1     2       0
> >      1     3       0
> >      1     3       0
> >      1     4       0
> >      1     4       0
> >      1     5       0
> >      1     5       0
> >      2     5       1
> >      2     5       0
> >      2     5       0
> >      2     6       0
> >      2     6       0
> >      2     6       0
> >      3   10       1
> >      3   10       0
> >
> > When I run the program below, I receive the following error:
> > Error in df[, "ID"] : incorrect number of dimensions
> >
> > My code:
> > # Create data.frame
> > ID <- c(rep(1,10),rep(2,6),rep(3,2))
> > date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
> >            rep(5,3),rep(6,3),rep(10,2))
> > olddata <- data.frame(ID=ID,date=date)
> > class(olddata)
> > cat("This is the original data frame","\n")
> > print(olddata)
> >
> > # This function is supposed to identify the first row
> > # within each level of ID and, for the first row, set
> > # the variable first to 1, and for all rows other than
> > # the first row set first to 0.
> > mydoit <- function(df){
> >    value <- ifelse (first(df[,"ID"]),1,0)
> >    cat("value=",value,"\n")
> >    df[,"first"] <- value
> > }
> > newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> >
> > Thank you,
> > John
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine, University of Maryland School of Medicine;
> > Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> > PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> > Senior Statistician University of Maryland Center for Vascular Research;
> >
> > Division of Gerontology and Paliative Care,
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > Cell phone 443-418-5382
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> Hello,
>
> And here are two other solutions.
>
>
> olddata$first <- with(olddata, ave(seq_along(ID), ID, FUN = \(x) x ==
> x[1L]))
>
> olddata$first <- c(1L, diff(olddata$ID))
>
>
> Of these two, diff is faster. But of all the solutions posted so far,
> Ben Bolker's is the fastest. And it can be made a little faster if
> as.integer substitutes for as.numeric.
> And dplyr::mutate now has a .by argument, which avoids explicit the call
> to group_by, with a performance gain.
>
>
> library(microbenchmark)
>
> mb <- microbenchmark(
>    ave = with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == x[1L])),
>    dup_num = as.numeric(! duplicated(olddata$ID)),
>    dup_int = as.integer(! duplicated(olddata$ID)),
>    diff = diff = c(1L, diff(olddata$ID)),
>    dplyr_grp = olddata %>% group_by(ID) %>% mutate(first =
> as.integer(row_number() == 1)),
>    dplyr = olddata %>% mutate(first = as.integer(row_number() == 1), .by
> = ID)
> )
> print(mb, order = "median")
>
>
>
> However, note that dplyr operates in entire data.frames and therefore is
> expected to be slower when tested against instructions that process one
> column only.
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> --
> Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
> www.avg.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Mon Dec  2 06:43:45 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 1 Dec 2024 21:43:45 -0800
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049F365BD14FF48597163E3E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
 <DM6PR03MB5049F365BD14FF48597163E3E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <CAGxFJbQ_YjZND6XFGgpA2fCOxWdw-zRwSTpr1OnZA-FsPxBW7A@mail.gmail.com>

OK.

(Note: I am ccing this to the list so that others can correct any
mistakes, misunderstandings, or misstatements that I may make; and
also give you more and better advice about how to proceed. For
example, there may be environmental packages ( see the environmetrics
task view, https://CRAN.R-project.org/view=Environmetrics) that
already do everything you want from your data source that someone else
could tell you about. Why reinvent the wheel if you don't have to? )

Unfortunately, you failed to show us the critical information
necessary to give you a definitive answer: the structure of a single
record. However, I will wing it based on your description and assume
that each record contains the following information something like
this: Latitude   Longitude  Date Time pm25 + other stuff maybe.  If
this is the case,

1) You do not need to sort anything. R has robust date-time
manipulation and computation capabilities to handle dates and times,
though in this case you only need dates. R knows all about how to
order dates and times.

2) You do not have to physically group your records according to
geographic location. R knows how to extract and manipulate groups
without this;

3) And as all you need is a daily average, you do not need to worry
about when days begin or end.

If this is a reasonably accurate interpretation of what needs to be
done, and your data are in a data frame called dat, then something
like the following will do it:
## First convert latitude and longitude into a single location factor, loc
dat$loc <- with(dat, paste0(Latitude, Longitude))

## you could also convert the latitude and longitude into actual
location names if you like; the "factor" data structure in R is one
simple way to do this, for example.

## Then a one liner in base R does what I think you want:
avgs <- aggregate(pm25 ~ loc + Date, FUN = mean, data = dat)

See ?aggregate for details. Note in particular that you can get
results simultaneously for several different pm's or whatevers. I
should also note that the so-called "Tidyverse" and "data.table"
groups of packages, and likely others, also can easily do these sorts
of things, though of course with different syntax, semantics, and
functionality, perhaps in ways that you might find simpler to master..
There are many good tutorials available for both base R and these
packages, but from my ignorant perspective, you need to first spend
some time to learn about R's basic data structures (factors, lists,
vectors, etc.) if you want to use R for serious data manipulation.
Finally, my best advice would be to forget about SAS if you wish to
use R. Trying to translate SAS paradigms into R is the devil's work.

Cheers,
Bert






On Sun, Dec 1, 2024 at 7:29?PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Bert, Avi:
>
> I stand accused of "using SAS-like strategies for your data analysis rather than R-like [analyses]." Although I am guilty, but I beg the court's mercy ;).  I have been a SAS programmer for more than 35 years. I have used R (and S-Plus) for about 20-years, but mostly for statistical analyses that required little or no data manipulation. I now need to use R for both statistical analyses and data manipulation and am trying to do in R what I can do easily is SAS.
>
> Here is a full description of my data manipulation problem:
> I have satellite data of airborne pollutants, obtained every 15-minutes over four-days from approximately 500 geographic areas=438 observations/4 days*500 geographic locations=approximately 250,000 individual observations. (I say approximately because I have slightly less than four-days data and I have slightly less than 500 geographic areas. The exact number of observations is of minor importance.)
>
> Each of the 500 geographic areas has a fixed longitude and latitude. Each of the 438 observations for each of the approximately 500 geographic areas has a date-time stamp. I need to compute average 24-hour pollutant (e.g. pm2.5) exposure for each day, across ALL 500 geographic areas. To accomplish this, I need to
>
> 1) Group data from each of the 500 geographic areas together
>
> 2) Within each geographic area order the observations by day and time
>         1) and 2) are easily accomplished using the R order function,
>       mydata<-mydata[order(mydata$lat_lon,mydata$Time),]
>
>  or SAS proc sort:
>         proc sort data=mydata;
>            by lat_lon daytime;  /* lat long is a string giving latitude and longitude */
>         run;
>
> 3) For each geographic area determine the records that mark the start and stop of each of the four days, let's say from 00:00 hrs to 23:59 hours, and create a variable, daynum that indicates the day number (valid values 1 to 4). I do not know how to accomplish this in R. This is the part of my analysis that I asked the R community to help me write. I know how this this can be easily done in SAS:
> * Arrange data date and time within each geographic region (i.e. lat_lon and daytime);
> proc sort data=mydata;
>   by lat_lon daytime;
> run;
>
> data mydata;
>   /* For each getgraphic area, each time a new record is run, keep the preceding value of daynum */
>   retain daynum;
>   set mydata;
>      by lat_lon daytime;
>   /* initialize daynum to 0 for first record from a given geographic location*/
>   if _n_ eq 1 then daynum=0;
>  /* Determine start of each day */
> mytime = timepart(daytime)  /* Extract time from date-time constant */
>   if mytime eq '00:00:00't then daynum=daynum+1; /* Increment daynum for each new day */
> run;
>
> 4) Get average value for a pollutant, pm25, by day across all 500 geographic areas. This is easily done in SAS using proc sort and proc means.
> proc sort data=mydata;
>   by daynum;
> run;
>
> * Get mean pm 2.5 by day accross all 500 geographic regions.;
> proc means data=mydata;
>   by daynum;
>   var pm25;
> run;
>
> If I can get step (3) above accomplished in R, I know how to accomplish step 4) in R using the by function:
> by(mydata[,"pm25"], mydata[,"daynum"],mean)
>
> I am trying to write the analysis described, and written in SAS, for 3) above in R. Please understand that I am fluent in SAS, and (except for straight forward analyses that require little or no data manipulation, where I am an intermediate programmer) i am an R tyro.
>
> Thank you for your help. My apologies for the long description of what I am trying to do. I sent this because you asked what I was trying to do and why I was doing it from the perspective of a SAS programmer rather than a matrix-based R programmer.
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
>
> ________________________________________
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Saturday, November 30, 2024 11:33 PM
> To: Sorkin, John
> Cc: r-help at r-project.org (r-help at r-project.org)
> Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows
>
> May I ask *why* you want to do this?
>
> It sounds to me like like you're using SAS-like strategies for your
> data analysis rather than R-like.
>
> -- Bert
>
> -- Bert
>
> On Sat, Nov 30, 2024 at 6:27?PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> >
> > Dear R help folks,
> >
> > First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> >
> > I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> >
> > e.g. if my original data is
> >  olddata
> >    ID date
> >     1     1
> >     1     1
> >     1     2
> >     1     2
> >     1     3
> >     1     3
> >     1     4
> >     1     4
> >     1     5
> >     1     5
> >     2     5
> >     2     5
> >     2     5
> >     2     6
> >     2     6
> >     2     6
> >     3   10
> >     3   10
> >
> > the new data will be
> > newdata
> >    ID date  first
> >     1     1       1
> >     1     1       0
> >     1     2       0
> >     1     2       0
> >     1     3       0
> >     1     3       0
> >     1     4       0
> >     1     4       0
> >     1     5       0
> >     1     5       0
> >     2     5       1
> >     2     5       0
> >     2     5       0
> >     2     6       0
> >     2     6       0
> >     2     6       0
> >     3   10       1
> >     3   10       0
> >
> > When I run the program below, I receive the following error:
> > Error in df[, "ID"] : incorrect number of dimensions
> >
> > My code:
> > # Create data.frame
> > ID <- c(rep(1,10),rep(2,6),rep(3,2))
> > date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
> >           rep(5,3),rep(6,3),rep(10,2))
> > olddata <- data.frame(ID=ID,date=date)
> > class(olddata)
> > cat("This is the original data frame","\n")
> > print(olddata)
> >
> > # This function is supposed to identify the first row
> > # within each level of ID and, for the first row, set
> > # the variable first to 1, and for all rows other than
> > # the first row set first to 0.
> > mydoit <- function(df){
> >   value <- ifelse (first(df[,"ID"]),1,0)
> >   cat("value=",value,"\n")
> >   df[,"first"] <- value
> > }
> > newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> >
> > Thank you,
> > John
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine, University of Maryland School of Medicine;
> > Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> > PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> > Senior Statistician University of Maryland Center for Vascular Research;
> >
> > Division of Gerontology and Paliative Care,
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > Cell phone 443-418-5382
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From ro||turner @end|ng |rom po@teo@net  Mon Dec  2 07:47:20 2024
From: ro||turner @end|ng |rom po@teo@net (Rolf Turner)
Date: Mon,  2 Dec 2024 06:47:20 +0000
Subject: [R] Fortune nomination.
In-Reply-To: <CAGxFJbQ_YjZND6XFGgpA2fCOxWdw-zRwSTpr1OnZA-FsPxBW7A@mail.gmail.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
 <DM6PR03MB5049F365BD14FF48597163E3E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQ_YjZND6XFGgpA2fCOxWdw-zRwSTpr1OnZA-FsPxBW7A@mail.gmail.com>
Message-ID: <20241202194720.3baf35af@new-hp>


On Sun, 1 Dec 2024 21:43:45 -0800
Bert Gunter <bgunter.4567 at gmail.com> wrote:


> Finally, my best advice would be to forget about SAS if you wish to
> use R. Trying to translate SAS paradigms into R is the devil's work.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Stats. Dep't. (secretaries) phone:
         +64-9-373-7599 ext. 89622
Home phone: +64-9-480-4619


From @vi@e@gross m@iii@g oii gm@ii@com  Mon Dec  2 06:39:28 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Mon, 2 Dec 2024 00:39:28 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB50499E32539371619AB9CE39E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>
 <CAGxFJbTUEFgeeDXe0rpAe4EfGN8edJAC1oVSjMTKuat-iBkSww@mail.gmail.com>
 <DM6PR03MB50499E32539371619AB9CE39E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <000701db447c$8df72760$a9e57620$@gmail.com>

John,

Thanks for enlightening us so we better understand.

I won't argue with your wish to learn to do things in base R first. I started that way, myself, and found lots of the commands not particularly easy to fit into a single worldview. Many functions I read about were promptly forgotten, especially those without great documentation and not enough examples of real world usage.

This is why some packages that came later are important as they generally try to come up with a somewhat consistent set of tools that often are also faster and more flexible. There is often a set of reasons various packages are created in the first place to meet real needs. And, I note that some may be subtle. Original R was often inconsistent in the order of command arguments while the dplyr and other tidyverse command try as much as possible to make the first argument be the one normally passed through a pipeline. R fairly recently added a native pipe operator that may be faster than the magrittr pipe but in some ways makes some functionality harder. The rest of R has not really been changed to make using commands in pipelines easy.

You seem to have also looked at data.table and given you may have large amounts of data, it may be designed in ways that might also be beneficial.

But as I do not want to relearn lots of R functions I never use, I will bow out from further discussion as what I would offer these days would probably not be what you want.

My personal opinion is that proper use of R can actually be far easier and more flexible than you had with the proprietary software that may largely consist of canned reports often used.

I do want to point out a few things to consider.

When you go grouping, you may want to consider grouping (as well as sorting) by multipole variables. You mention a variable with about 500 possibilities and then another variable with an ID number but did not say the ID number was unique across them all. 

And, I want to note you may want to also look into testing the sanity of your data. That is a wide area too. Things like duplicates, for example.

I do not know how many steps you can handle but there are sometimes designs that make an algorithm work differently.

Consider your request to find  the first row in each grouping and add a column with a 1, and 0 for all others. If that is what you need, fine.

But, what if instead you just added a row number. Some rows would have a 1, and some may have a 2, 3, or 4.

When you wanted  to so something to just the rows with a 1, you can filter out a subset of the data easily enough or apply a command only to those rows. But if you want to test if any entry has more than 4 rows, this could allow you to detect an error. Other ideas might be possible if that is how the data was saved.

And, if it really is a 0/1 choice, fine, but consider the advantages or disadvantages of what you save in the new column. Storing a numeric or an int can take up space when storing a Boolean or TRUE/FALSE is what you need. R gives you lots of flexibility which perhaps you did not have to think about before.

All I know is that so much of what you want to do is easily enough done with a pipeline or two in dplyr. But this is your task and you choose what makes sense. It specializes in group analysis and generates reports and so on. It may not be how you think. 


-----Original Message-----
From: Sorkin, John <jsorkin at som.umaryland.edu> 
Sent: Sunday, December 1, 2024 11:19 PM
To: Bert Gunter <bgunter.4567 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>; twoolman at ontargettek.com; tebert at ufl.edu; Bert Gunter <bgunter.4567 at gmail.com>; jdnewmil at dcn.davis.ca.us; avi.e.gross at gmail.com; therneau at mayo.edu; dwinsemius at comcast.net; tebert at ufl.edu; rmh at temple.edu; ken.knoblauch at inserm.fr; boris.steipe at utoronto.ca
Cc: r-help at r-project.org (r-help at r-project.org) <r-help at r-project.org>; kimmo.elo at uef.fi
Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows

Dear Colleagues,

I am grateful to all of you for helping me with my question, how to write R code that will identify the first row of each ID within a data frame, create a variable first=1 for the first row and first=0 for all repeats of the ID.

WOW!!!
I just saw Boris Steipe's answer to my question:
olddata$first <- as.numeric(! duplicated(olddata$ID))
The solution is elegant, short, easy to understand, and it uses base R! All important characteristics of a good solution, at least for me. While I want to learn solutions using packages that extend base R, I believe that a good programmer learns how to do something using the base language and once that is learned, explores way to solve a programing problem using advanced packages.

Each and every one of you (I hope I did not miss anyone in my list of email addresses) took the time to read my emails and respond to me. Your collective help is invaluable, and I am in your collect debt.

Many, many thanks,
John

John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




________________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Sunday, December 1, 2024 11:30 AM
To: Rui Barradas
Cc: Sorkin, John; r-help at r-project.org (r-help at r-project.org)
Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows

Rui:
"f these two, diff is faster. But of all the solutions posted so far,
Ben Bolker's is the fastest."

But the explicit version of diff is still considerably faster:

> D <- c(rep(1,10),rep(2,6),rep(3,2))

> microbenchmark(c(1L,diff(D)), times = 1000L)
Unit: microseconds
           expr   min    lq    mean median    uq    max neval
 c(1L, diff(D)) 3.075 3.198 3.34396   3.28 3.362 29.684  1000

> microbenchmark( as.integer(!duplicated(D)), times =1000L)
Unit: microseconds
                       expr   min    lq     mean median   uq  max neval
 as.integer(!duplicated(D)) 1.476 1.558 1.644264  1.599 1.64 16.4  1000

> microbenchmark( D - c(0L, D[-length(D)]), times = 1000L)
Unit: nanoseconds  ## note that unit is nanoseconds not microseconds
                     expr min  lq    mean median  uq  max neval
 D - c(0L, D[-length(D)]) 369 410 489.335    492 533 9840  1000

Cheers,
Bert

On Sat, Nov 30, 2024 at 11:05?PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> ?s 02:27 de 01/12/2024, Sorkin, John escreveu:
> > Dear R help folks,
> >
> > First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> >
> > I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> >
> > e.g. if my original data is
> >   olddata
> >     ID date
> >      1     1
> >      1     1
> >      1     2
> >      1     2
> >      1     3
> >      1     3
> >      1     4
> >      1     4
> >      1     5
> >      1     5
> >      2     5
> >      2     5
> >      2     5
> >      2     6
> >      2     6
> >      2     6
> >      3   10
> >      3   10
> >
> > the new data will be
> > newdata
> >     ID date  first
> >      1     1       1
> >      1     1       0
> >      1     2       0
> >      1     2       0
> >      1     3       0
> >      1     3       0
> >      1     4       0
> >      1     4       0
> >      1     5       0
> >      1     5       0
> >      2     5       1
> >      2     5       0
> >      2     5       0
> >      2     6       0
> >      2     6       0
> >      2     6       0
> >      3   10       1
> >      3   10       0
> >
> > When I run the program below, I receive the following error:
> > Error in df[, "ID"] : incorrect number of dimensions
> >
> > My code:
> > # Create data.frame
> > ID <- c(rep(1,10),rep(2,6),rep(3,2))
> > date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
> >            rep(5,3),rep(6,3),rep(10,2))
> > olddata <- data.frame(ID=ID,date=date)
> > class(olddata)
> > cat("This is the original data frame","\n")
> > print(olddata)
> >
> > # This function is supposed to identify the first row
> > # within each level of ID and, for the first row, set
> > # the variable first to 1, and for all rows other than
> > # the first row set first to 0.
> > mydoit <- function(df){
> >    value <- ifelse (first(df[,"ID"]),1,0)
> >    cat("value=",value,"\n")
> >    df[,"first"] <- value
> > }
> > newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> >
> > Thank you,
> > John
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine, University of Maryland School of Medicine;
> > Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> > PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> > Senior Statistician University of Maryland Center for Vascular Research;
> >
> > Division of Gerontology and Paliative Care,
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > Cell phone 443-418-5382
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> Hello,
>
> And here are two other solutions.
>
>
> olddata$first <- with(olddata, ave(seq_along(ID), ID, FUN = \(x) x ==
> x[1L]))
>
> olddata$first <- c(1L, diff(olddata$ID))
>
>
> Of these two, diff is faster. But of all the solutions posted so far,
> Ben Bolker's is the fastest. And it can be made a little faster if
> as.integer substitutes for as.numeric.
> And dplyr::mutate now has a .by argument, which avoids explicit the call
> to group_by, with a performance gain.
>
>
> library(microbenchmark)
>
> mb <- microbenchmark(
>    ave = with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == x[1L])),
>    dup_num = as.numeric(! duplicated(olddata$ID)),
>    dup_int = as.integer(! duplicated(olddata$ID)),
>    diff = diff = c(1L, diff(olddata$ID)),
>    dplyr_grp = olddata %>% group_by(ID) %>% mutate(first =
> as.integer(row_number() == 1)),
>    dplyr = olddata %>% mutate(first = as.integer(row_number() == 1), .by
> = ID)
> )
> print(mb, order = "median")
>
>
>
> However, note that dplyr operates in entire data.frames and therefore is
> expected to be slower when tested against instructions that process one
> column only.
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> --
> Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
> http://www.avg.com/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Mon Dec  2 05:18:40 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Mon, 2 Dec 2024 04:18:40 +0000
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <CAGxFJbTUEFgeeDXe0rpAe4EfGN8edJAC1oVSjMTKuat-iBkSww@mail.gmail.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>
 <CAGxFJbTUEFgeeDXe0rpAe4EfGN8edJAC1oVSjMTKuat-iBkSww@mail.gmail.com>
Message-ID: <DM6PR03MB50499E32539371619AB9CE39E2352@DM6PR03MB5049.namprd03.prod.outlook.com>

Dear Colleagues,

I am grateful to all of you for helping me with my question, how to write R code that will identify the first row of each ID within a data frame, create a variable first=1 for the first row and first=0 for all repeats of the ID.

WOW!!!
I just saw Boris Steipe's answer to my question:
olddata$first <- as.numeric(! duplicated(olddata$ID))
The solution is elegant, short, easy to understand, and it uses base R! All important characteristics of a good solution, at least for me. While I want to learn solutions using packages that extend base R, I believe that a good programmer learns how to do something using the base language and once that is learned, explores way to solve a programing problem using advanced packages.

Each and every one of you (I hope I did not miss anyone in my list of email addresses) took the time to read my emails and respond to me. Your collective help is invaluable, and I am in your collect debt.

Many, many thanks,
John

John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




________________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Sunday, December 1, 2024 11:30 AM
To: Rui Barradas
Cc: Sorkin, John; r-help at r-project.org (r-help at r-project.org)
Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows

Rui:
"f these two, diff is faster. But of all the solutions posted so far,
Ben Bolker's is the fastest."

But the explicit version of diff is still considerably faster:

> D <- c(rep(1,10),rep(2,6),rep(3,2))

> microbenchmark(c(1L,diff(D)), times = 1000L)
Unit: microseconds
           expr   min    lq    mean median    uq    max neval
 c(1L, diff(D)) 3.075 3.198 3.34396   3.28 3.362 29.684  1000

> microbenchmark( as.integer(!duplicated(D)), times =1000L)
Unit: microseconds
                       expr   min    lq     mean median   uq  max neval
 as.integer(!duplicated(D)) 1.476 1.558 1.644264  1.599 1.64 16.4  1000

> microbenchmark( D - c(0L, D[-length(D)]), times = 1000L)
Unit: nanoseconds  ## note that unit is nanoseconds not microseconds
                     expr min  lq    mean median  uq  max neval
 D - c(0L, D[-length(D)]) 369 410 489.335    492 533 9840  1000

Cheers,
Bert

On Sat, Nov 30, 2024 at 11:05?PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> ?s 02:27 de 01/12/2024, Sorkin, John escreveu:
> > Dear R help folks,
> >
> > First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> >
> > I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> >
> > e.g. if my original data is
> >   olddata
> >     ID date
> >      1     1
> >      1     1
> >      1     2
> >      1     2
> >      1     3
> >      1     3
> >      1     4
> >      1     4
> >      1     5
> >      1     5
> >      2     5
> >      2     5
> >      2     5
> >      2     6
> >      2     6
> >      2     6
> >      3   10
> >      3   10
> >
> > the new data will be
> > newdata
> >     ID date  first
> >      1     1       1
> >      1     1       0
> >      1     2       0
> >      1     2       0
> >      1     3       0
> >      1     3       0
> >      1     4       0
> >      1     4       0
> >      1     5       0
> >      1     5       0
> >      2     5       1
> >      2     5       0
> >      2     5       0
> >      2     6       0
> >      2     6       0
> >      2     6       0
> >      3   10       1
> >      3   10       0
> >
> > When I run the program below, I receive the following error:
> > Error in df[, "ID"] : incorrect number of dimensions
> >
> > My code:
> > # Create data.frame
> > ID <- c(rep(1,10),rep(2,6),rep(3,2))
> > date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
> >            rep(5,3),rep(6,3),rep(10,2))
> > olddata <- data.frame(ID=ID,date=date)
> > class(olddata)
> > cat("This is the original data frame","\n")
> > print(olddata)
> >
> > # This function is supposed to identify the first row
> > # within each level of ID and, for the first row, set
> > # the variable first to 1, and for all rows other than
> > # the first row set first to 0.
> > mydoit <- function(df){
> >    value <- ifelse (first(df[,"ID"]),1,0)
> >    cat("value=",value,"\n")
> >    df[,"first"] <- value
> > }
> > newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> >
> > Thank you,
> > John
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine, University of Maryland School of Medicine;
> > Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> > PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> > Senior Statistician University of Maryland Center for Vascular Research;
> >
> > Division of Gerontology and Paliative Care,
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > Cell phone 443-418-5382
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> Hello,
>
> And here are two other solutions.
>
>
> olddata$first <- with(olddata, ave(seq_along(ID), ID, FUN = \(x) x ==
> x[1L]))
>
> olddata$first <- c(1L, diff(olddata$ID))
>
>
> Of these two, diff is faster. But of all the solutions posted so far,
> Ben Bolker's is the fastest. And it can be made a little faster if
> as.integer substitutes for as.numeric.
> And dplyr::mutate now has a .by argument, which avoids explicit the call
> to group_by, with a performance gain.
>
>
> library(microbenchmark)
>
> mb <- microbenchmark(
>    ave = with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == x[1L])),
>    dup_num = as.numeric(! duplicated(olddata$ID)),
>    dup_int = as.integer(! duplicated(olddata$ID)),
>    diff = diff = c(1L, diff(olddata$ID)),
>    dplyr_grp = olddata %>% group_by(ID) %>% mutate(first =
> as.integer(row_number() == 1)),
>    dplyr = olddata %>% mutate(first = as.integer(row_number() == 1), .by
> = ID)
> )
> print(mb, order = "median")
>
>
>
> However, note that dplyr operates in entire data.frames and therefore is
> expected to be slower when tested against instructions that process one
> column only.
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> --
> Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
> http://www.avg.com/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Wed Dec  4 13:38:45 2024
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Wed, 4 Dec 2024 13:38:45 +0100
Subject: [R] Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
Message-ID: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>

Dear list,

is anyone aware of the following behavious of diag when used to replace 
diagonals (plural!) of a matrix?

Small example: The following is documented and clearly to be expected:

A <- matrix(0, nrow = 5, ncol = 5)
diag(A) <- 1; A


BUT, what about the following? When executing the code of `diag<-` line 
by line, it throws errors. So why does it work?

diag(A[-1, ]) <- 2; A

diag(A[-5, -1]) <- 3; A

diag(A[-5, -(1:2)]) <- 4; A


Any ideas?

  TIA and best regards  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 215
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
https://www.uni-giessen.de/math/eichner


From bgunter@4567 @end|ng |rom gm@||@com  Wed Dec  4 14:38:44 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 4 Dec 2024 05:38:44 -0800
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
Message-ID: <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>

matrices are vectors with a "dim" attribute.
So what I think is happening is:

> A <- matrix(1:25, nrow = 5, ncol = 5)
> diag(A[-1,]) <- 0
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16   21
[2,]    0    7   12   17   22
[3,]    3    0   13   18   23
[4,]    4    9    0   19   24
[5,]    5   10   15    0   25
>
> ## is equivqalent to:
>
> A <- matrix(1:25, nrow = 5, ncol = 5)
> wh <- c(diag(A[-1,]))   # A's vector indices of diag(A[-1,])
> A[wh] <- 0
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16   21
[2,]    0    7   12   17   22
[3,]    3    0   13   18   23
[4,]    4    9    0   19   24
[5,]    5   10   15    0   25

I didn't check, but I assume your other examples would work similarly.
Please repost if I am wrong.

Cheers,
Bert

On Wed, Dec 4, 2024 at 4:39?AM Gerrit Eichner
<gerrit.eichner at math.uni-giessen.de> wrote:
>
> Dear list,
>
> is anyone aware of the following behavious of diag when used to replace
> diagonals (plural!) of a matrix?
>
> Small example: The following is documented and clearly to be expected:
>
> A <- matrix(0, nrow = 5, ncol = 5)
> diag(A) <- 1; A
>
>
> BUT, what about the following? When executing the code of `diag<-` line
> by line, it throws errors. So why does it work?
>
> diag(A[-1, ]) <- 2; A
>
> diag(A[-5, -1]) <- 3; A
>
> diag(A[-5, -(1:2)]) <- 4; A
>
>
> Any ideas?
>
>   TIA and best regards  --  Gerrit
>
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner                   Mathematical Institute, Room 215
> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> https://www.uni-giessen.de/math/eichner
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Dec  4 15:31:30 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 4 Dec 2024 09:31:30 -0500
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
Message-ID: <f8d638cf-75ab-41c6-9127-b0a69c162330@gmail.com>

On 2024-12-04 7:38 a.m., Gerrit Eichner wrote:
> Dear list,
> 
> is anyone aware of the following behavious of diag when used to replace
> diagonals (plural!) of a matrix?
> 
> Small example: The following is documented and clearly to be expected:
> 
> A <- matrix(0, nrow = 5, ncol = 5)
> diag(A) <- 1; A
> 
> 
> BUT, what about the following? When executing the code of `diag<-` line
> by line, it throws errors. So why does it work?
> 
> diag(A[-1, ]) <- 2; A
> 
> diag(A[-5, -1]) <- 3; A
> 
> diag(A[-5, -(1:2)]) <- 4; A
> 

Could you show us the log of what you did that generated errors?  The 
statement `diag(A[-1, ]) <- 2` is pretty complex; it involves two 
assignment functions (both `diag<-` and `[<-`), so you might have tried 
to execute the wrong thing.

Duncan Murdoch


From dyk|m7411 @end|ng |rom gm@||@com  Thu Dec  5 05:44:25 2024
From: dyk|m7411 @end|ng |rom gm@||@com (D)
Date: Wed, 4 Dec 2024 23:44:25 -0500
Subject: [R] Save spatial data in a csv file
In-Reply-To: <CAP+bYWCkzSSiBJ_FPH7fyYYcUV=4yGRP_baBe4UvMbMg87p0OQ@mail.gmail.com>
References: <CADmwX-LXiw0yrVy2a0X+eERg=Vuv=pYGG8h16kdyuLLw3x2p+w@mail.gmail.com>
 <CAP+bYWCkzSSiBJ_FPH7fyYYcUV=4yGRP_baBe4UvMbMg87p0OQ@mail.gmail.com>
Message-ID: <CADmwX-+sph_hwWPrBNPaby6yyNYq0cWoX=pi3jeUmf6_A=iLDQ@mail.gmail.com>

Hi Hasan,

Thank you for the advice.  What does that mean by "respond with the output
of `dput(nyc_ct_geo)`"?  Please provide more details to address the
problem.  Thank you!

On Sat, Nov 30, 2024 at 12:47?PM Hasan Diwan <hasan.diwan at gmail.com> wrote:

> Kindly respond with the output of `dput(nyc_ct_geo)` -- thank you! -- H
>
>
> --
> OpenPGP: https://hasan.d8u.us/openpgp.asc
> If you wish to request my time, please do so using
> *bit.ly/hd1AppointmentRequest
> <http://bit.ly/hd1AppointmentRequest>*.
> Si vous voudriez faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
> <http://bit.ly/hd1AppointmentRequest>*.
>
> <https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
> >Sent
> from my mobile device
> Envoye de mon portable
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Thu Dec  5 12:04:13 2024
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Thu, 5 Dec 2024 12:04:13 +0100
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
 <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>
Message-ID: <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>

Hi, Bert,

thx, but I don't think so, because your example fails, if you use
different entries for matrix A than your vector 1:25. (Try A filled with 
only zeroes, e.g.)

I guess, Duncan Murdochs hint is crucial. I'll comment on his asap.

  Best regards  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 215
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
https://www.uni-giessen.de/math/eichner
---------------------------------------------------------------------

Am 04.12.2024 um 14:38 schrieb Bert Gunter:
> matrices are vectors with a "dim" attribute.
> So what I think is happening is:
> 
>> A <- matrix(1:25, nrow = 5, ncol = 5)
>> diag(A[-1,]) <- 0
>> A
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16   21
> [2,]    0    7   12   17   22
> [3,]    3    0   13   18   23
> [4,]    4    9    0   19   24
> [5,]    5   10   15    0   25
>>
>> ## is equivqalent to:
>>
>> A <- matrix(1:25, nrow = 5, ncol = 5)
>> wh <- c(diag(A[-1,]))   # A's vector indices of diag(A[-1,])
>> A[wh] <- 0
>> A
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16   21
> [2,]    0    7   12   17   22
> [3,]    3    0   13   18   23
> [4,]    4    9    0   19   24
> [5,]    5   10   15    0   25
> 
> I didn't check, but I assume your other examples would work similarly.
> Please repost if I am wrong.
> 
> Cheers,
> Bert
> 
> On Wed, Dec 4, 2024 at 4:39?AM Gerrit Eichner
> <gerrit.eichner at math.uni-giessen.de> wrote:
>>
>> Dear list,
>>
>> is anyone aware of the following behavious of diag when used to replace
>> diagonals (plural!) of a matrix?
>>
>> Small example: The following is documented and clearly to be expected:
>>
>> A <- matrix(0, nrow = 5, ncol = 5)
>> diag(A) <- 1; A
>>
>>
>> BUT, what about the following? When executing the code of `diag<-` line
>> by line, it throws errors. So why does it work?
>>
>> diag(A[-1, ]) <- 2; A
>>
>> diag(A[-5, -1]) <- 3; A
>>
>> diag(A[-5, -(1:2)]) <- 4; A
>>
>>
>> Any ideas?
>>
>>    TIA and best regards  --  Gerrit
>>
>> ---------------------------------------------------------------------
>> Dr. Gerrit Eichner                   Mathematical Institute, Room 215
>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>> https://www.uni-giessen.de/math/eichner
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Thu Dec  5 12:18:08 2024
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Thu, 5 Dec 2024 12:18:08 +0100
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <f8d638cf-75ab-41c6-9127-b0a69c162330@gmail.com>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
 <f8d638cf-75ab-41c6-9127-b0a69c162330@gmail.com>
Message-ID: <a9acfc1d-7b2b-4345-a6f6-f3d443378894@math.uni-giessen.de>

Hi, Duncan,

thx for your valuable hint re the two assigment functions! That might be 
the reason. Nevertheless, I couldn't figure out how they work together 
here. I simply executed the relevant lines of code which I found in 
`diag<-`body after assigning the matrix to x (which, of course, cannot 
yield the same as diag(A[-1,]) <- 7 in the light of your explanation):

A <- matrix(0, nrow = 5, ncol = 5)

x <- A[-1,]
value <- 7


# From `diag<-`'s body:

dx <- dim(x)
len.i <- min(dx)
len.v <- length(value)
if (len.i) {
   i <- seq_len(len.i)
   x[cbind(i, i)] <- value
   }
x



I fiddled a bit around with expressions, but to no avail.
Could you (or anybody else) maybe still shed some more light on this?
Thx a lot in advance!

  Best  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 215
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
https://www.uni-giessen.de/math/eichner
---------------------------------------------------------------------

Am 04.12.2024 um 15:31 schrieb Duncan Murdoch:
> On 2024-12-04 7:38 a.m., Gerrit Eichner wrote:
>> Dear list,
>>
>> is anyone aware of the following behavious of diag when used to replace
>> diagonals (plural!) of a matrix?
>>
>> Small example: The following is documented and clearly to be expected:
>>
>> A <- matrix(0, nrow = 5, ncol = 5)
>> diag(A) <- 1; A
>>
>>
>> BUT, what about the following? When executing the code of `diag<-` line
>> by line, it throws errors. So why does it work?
>>
>> diag(A[-1, ]) <- 2; A
>>
>> diag(A[-5, -1]) <- 3; A
>>
>> diag(A[-5, -(1:2)]) <- 4; A
>>
> 
> Could you show us the log of what you did that generated errors?? The 
> statement `diag(A[-1, ]) <- 2` is pretty complex; it involves two 
> assignment functions (both `diag<-` and `[<-`), so you might have tried 
> to execute the wrong thing.
> 
> Duncan Murdoch
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Dec  5 12:42:19 2024
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 5 Dec 2024 11:42:19 +0000
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
Message-ID: <fe70e869-53cc-47c6-8c8f-2a00c082e31a@sapo.pt>

?s 12:38 de 04/12/2024, Gerrit Eichner escreveu:
> Dear list,
> 
> is anyone aware of the following behavious of diag when used to replace 
> diagonals (plural!) of a matrix?
> 
> Small example: The following is documented and clearly to be expected:
> 
> A <- matrix(0, nrow = 5, ncol = 5)
> diag(A) <- 1; A
> 
> 
> BUT, what about the following? When executing the code of `diag<-` line 
> by line, it throws errors. So why does it work?
> 
> diag(A[-1, ]) <- 2; A
> 
> diag(A[-5, -1]) <- 3; A
> 
> diag(A[-5, -(1:2)]) <- 4; A
> 
> 
> Any ideas?
> 
>  ?TIA and best regards? --? Gerrit
> 
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner?????????????????? Mathematical Institute, Room 215
> gerrit.eichner at math.uni-giessen.de?? Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104????????? Arndtstr. 2, 35392 Giessen, Germany
> https://www.uni-giessen.de/math/eichner
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,

I cannot reproduce any error, everything works as expected.
sessionInfo at the end.



A <- matrix(0, nrow = 5, ncol = 5)
diag(A) <- 1; A
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    1    0    0    0    0
# [2,]    0    1    0    0    0
# [3,]    0    0    1    0    0
# [4,]    0    0    0    1    0
# [5,]    0    0    0    0    1

diag(A[-1, ]) <- 2; A
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    1    0    0    0    0
# [2,]    2    1    0    0    0
# [3,]    0    2    1    0    0
# [4,]    0    0    2    1    0
# [5,]    0    0    0    2    1

diag(A[-5, -1]) <- 3; A
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    1    3    0    0    0
# [2,]    2    1    3    0    0
# [3,]    0    2    1    3    0
# [4,]    0    0    2    1    3
# [5,]    0    0    0    2    1

diag(A[-5, -(1:2)]) <- 4; A
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    1    3    4    0    0
# [2,]    2    1    3    4    0
# [3,]    0    2    1    3    4
# [4,]    0    0    2    1    3
# [5,]    0    0    0    2    1

sessionInfo()
# R version 4.4.2 (2024-10-31 ucrt)
# Platform: x86_64-w64-mingw32/x64
# Running under: Windows 11 x64 (build 22631)
#
# Matrix products: default
#
#
# locale:
# [1] LC_COLLATE=Portuguese_Portugal.utf8 
LC_CTYPE=Portuguese_Portugal.utf8
# [3] LC_MONETARY=Portuguese_Portugal.utf8 LC_NUMERIC=C 

# [5] LC_TIME=Portuguese_Portugal.utf8
#
# time zone: Europe/Lisbon
# tzcode source: internal
#
# attached base packages:
# [1] stats     graphics  grDevices utils     datasets  methods   base
#
# loaded via a namespace (and not attached):
# [1] compiler_4.4.2
#



Hope this helps,

Rui Barradas


-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Dec  5 12:46:47 2024
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 5 Dec 2024 11:46:47 +0000
Subject: [R] Save spatial data in a csv file
Message-ID: <SV0P279MB04755E748B51A94694C104AEEE302@SV0P279MB0475.NORP279.PROD.OUTLOOK.COM>

As Hasan suggested, much more information is needed. It would be very helpful to know the class of nyc_ct_geo, and how it was created. It is possibly an sf object from the print output. 

>From this, assuming that it is an sf object, the geometries are polygons, not points, so need to be converted to text, best WKT (well-known text) to write to CSV. It is then best to use the appropriate function from sf with a CSV driver - minimal example:

library(sf)
nc <- st_read(system.file("gpkg/nc.gpkg", package="sf"))
tf <- tempfile(fileext=".csv")
st_write(nc_geom, tf, driver="CSV", layer_options=c("GEOMETRY=AS_WKT"))
file.show(tf)

With more than 2000 polygon objects in your case, this is hardly a sensible way to write geometries to file, and depends on the person receiving the file having software that can convert WKT back to geometries. For more details on the CSV driver for st_write, please see https://gdal.org/en/stable/drivers/vector/csv.html#vector-csv

Maybe following up on the R-sig-geo list would be helpful if clarification is needed.

Roger

--
Roger Bivand
Emeritus Professor
Norwegian School of Economics
Postboks 3490 Ytre Sandviken, 5045 Bergen, Norway
Roger.Bivand at nhh.no

From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Thu Dec  5 14:23:38 2024
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Thu, 5 Dec 2024 07:23:38 -0600
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
 <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>
 <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>
Message-ID: <9b75727b-a35b-4d56-9db4-5353b9e23d71@effectivedefense.org>

Hi, Gerrit:


On 12/5/24 05:04, Gerrit Eichner wrote:
> Hi, Bert,
> 
> thx, but I don't think so, because your example fails, if you use
> different entries for matrix A than your vector 1:25. (Try A filled with 
> only zeroes, e.g.)


YOU'VE MISUNDERSTOOD Bert's reply: A matrix is a vector, and Bert 
assigned the indices of the vector to the cells of the matrix.


 > A <- matrix(1:25, nrow = 5, ncol = 5)
 > A[1:25]
  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 
23 24 25
 > A[13]
[1] 13


	  "if you use different entries for matrix A", the entries are no 
longer the indices of A as a vector.


	  Does this fact make his example work for you?


	  It does for me.
	  Hope this helps.
	  Spencer Graves

> 
> I guess, Duncan Murdochs hint is crucial. I'll comment on his asap.
> 
>  ?Best regards? --? Gerrit
> 
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner?????????????????? Mathematical Institute, Room 215
> gerrit.eichner at math.uni-giessen.de?? Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104????????? Arndtstr. 2, 35392 Giessen, Germany
> https://www.uni-giessen.de/math/eichner
> ---------------------------------------------------------------------
> 
> Am 04.12.2024 um 14:38 schrieb Bert Gunter:
>> matrices are vectors with a "dim" attribute.
>> So what I think is happening is:
>>
>>> A <- matrix(1:25, nrow = 5, ncol = 5)
>>> diag(A[-1,]) <- 0
>>> A
>> ????? [,1] [,2] [,3] [,4] [,5]
>> [1,]??? 1??? 6?? 11?? 16?? 21
>> [2,]??? 0??? 7?? 12?? 17?? 22
>> [3,]??? 3??? 0?? 13?? 18?? 23
>> [4,]??? 4??? 9??? 0?? 19?? 24
>> [5,]??? 5?? 10?? 15??? 0?? 25
>>>
>>> ## is equivqalent to:
>>>
>>> A <- matrix(1:25, nrow = 5, ncol = 5)
>>> wh <- c(diag(A[-1,]))?? # A's vector indices of diag(A[-1,])
>>> A[wh] <- 0
>>> A
>> ????? [,1] [,2] [,3] [,4] [,5]
>> [1,]??? 1??? 6?? 11?? 16?? 21
>> [2,]??? 0??? 7?? 12?? 17?? 22
>> [3,]??? 3??? 0?? 13?? 18?? 23
>> [4,]??? 4??? 9??? 0?? 19?? 24
>> [5,]??? 5?? 10?? 15??? 0?? 25
>>
>> I didn't check, but I assume your other examples would work similarly.
>> Please repost if I am wrong.
>>
>> Cheers,
>> Bert
>>
>> On Wed, Dec 4, 2024 at 4:39?AM Gerrit Eichner
>> <gerrit.eichner at math.uni-giessen.de> wrote:
>>>
>>> Dear list,
>>>
>>> is anyone aware of the following behavious of diag when used to replace
>>> diagonals (plural!) of a matrix?
>>>
>>> Small example: The following is documented and clearly to be expected:
>>>
>>> A <- matrix(0, nrow = 5, ncol = 5)
>>> diag(A) <- 1; A
>>>
>>>
>>> BUT, what about the following? When executing the code of `diag<-` line
>>> by line, it throws errors. So why does it work?
>>>
>>> diag(A[-1, ]) <- 2; A
>>>
>>> diag(A[-5, -1]) <- 3; A
>>>
>>> diag(A[-5, -(1:2)]) <- 4; A
>>>
>>>
>>> Any ideas?
>>>
>>> ?? TIA and best regards? --? Gerrit
>>>
>>> ---------------------------------------------------------------------
>>> Dr. Gerrit Eichner?????????????????? Mathematical Institute, Room 215
>>> gerrit.eichner at math.uni-giessen.de?? Justus-Liebig-University Giessen
>>> Tel: +49-(0)641-99-32104????????? Arndtstr. 2, 35392 Giessen, Germany
>>> https://www.uni-giessen.de/math/eichner
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide https://www.R-project.org/posting- 
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Thu Dec  5 14:29:22 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 5 Dec 2024 05:29:22 -0800
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
 <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>
 <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>
Message-ID: <CAGxFJbTg6=awGNdmnBX6jxx_ncM4qPXFPPyv0X09QV+MJ=4QJA@mail.gmail.com>

Gerrit:
Oh, sorry. I was obviously unclear. What I meant by "equivalent" was
"this is how the subset of indices in A whose corresponding values are
changed is selected."  So the example was meant to show this, not as a
literal representation of the code.

This might be a clearer explanation, but feel free to ignore and
certainly no need to reply.

A <- matrix(0, nrow =5, ncol = 5)

Here is what  "diag(A[-1,] <- 1" does (semantically):
B <- A[-1,]
diag(B) <- 1
A[-1,] <- B  ## etc. for all your examples

This makes semantic sense to me, i.e. what you want "diag(A[-1,] <- 1"
to mean. This puzzled me, but maybe you understood this already and
just wanted to understand how the code does this. If so, my apologies
for the spam.

Cheers,
Bert

On Thu, Dec 5, 2024 at 3:03?AM Gerrit Eichner
<gerrit.eichner at math.uni-giessen.de> wrote:
>
> Hi, Bert,
>
> thx, but I don't think so, because your example fails, if you use
> different entries for matrix A than your vector 1:25. (Try A filled with
> only zeroes, e.g.)
>
> I guess, Duncan Murdochs hint is crucial. I'll comment on his asap.
>
>   Best regards  --  Gerrit
>
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner                   Mathematical Institute, Room 215
> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> https://www.uni-giessen.de/math/eichner
> ---------------------------------------------------------------------
>
> Am 04.12.2024 um 14:38 schrieb Bert Gunter:
> > matrices are vectors with a "dim" attribute.
> > So what I think is happening is:
> >
> >> A <- matrix(1:25, nrow = 5, ncol = 5)
> >> diag(A[-1,]) <- 0
> >> A
> >       [,1] [,2] [,3] [,4] [,5]
> > [1,]    1    6   11   16   21
> > [2,]    0    7   12   17   22
> > [3,]    3    0   13   18   23
> > [4,]    4    9    0   19   24
> > [5,]    5   10   15    0   25
> >>
> >> ## is equivqalent to:
> >>
> >> A <- matrix(1:25, nrow = 5, ncol = 5)
> >> wh <- c(diag(A[-1,]))   # A's vector indices of diag(A[-1,])
> >> A[wh] <- 0
> >> A
> >       [,1] [,2] [,3] [,4] [,5]
> > [1,]    1    6   11   16   21
> > [2,]    0    7   12   17   22
> > [3,]    3    0   13   18   23
> > [4,]    4    9    0   19   24
> > [5,]    5   10   15    0   25
> >
> > I didn't check, but I assume your other examples would work similarly.
> > Please repost if I am wrong.
> >
> > Cheers,
> > Bert
> >
> > On Wed, Dec 4, 2024 at 4:39?AM Gerrit Eichner
> > <gerrit.eichner at math.uni-giessen.de> wrote:
> >>
> >> Dear list,
> >>
> >> is anyone aware of the following behavious of diag when used to replace
> >> diagonals (plural!) of a matrix?
> >>
> >> Small example: The following is documented and clearly to be expected:
> >>
> >> A <- matrix(0, nrow = 5, ncol = 5)
> >> diag(A) <- 1; A
> >>
> >>
> >> BUT, what about the following? When executing the code of `diag<-` line
> >> by line, it throws errors. So why does it work?
> >>
> >> diag(A[-1, ]) <- 2; A
> >>
> >> diag(A[-5, -1]) <- 3; A
> >>
> >> diag(A[-5, -(1:2)]) <- 4; A
> >>
> >>
> >> Any ideas?
> >>
> >>    TIA and best regards  --  Gerrit
> >>
> >> ---------------------------------------------------------------------
> >> Dr. Gerrit Eichner                   Mathematical Institute, Room 215
> >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> >> https://www.uni-giessen.de/math/eichner
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.


From beto77051 @end|ng |rom gm@||@com  Thu Dec  5 16:21:26 2024
From: beto77051 @end|ng |rom gm@||@com (Carlos Alberto Baptista de Figueiredo)
Date: Thu, 5 Dec 2024 15:21:26 +0000
Subject: [R] Creating a script in Rstudio
Message-ID: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>

Hi there





I want to create a script in Rstudio and load in the reagent dataset
ensuring that the different data types in there (dates, text, etc) come
through correctly.



Best wishes

Carlos

From C@r|o@@F|gue|redo @end|ng |rom hc@he@|thc@re@co@uk  Thu Dec  5 16:16:07 2024
From: C@r|o@@F|gue|redo @end|ng |rom hc@he@|thc@re@co@uk (Figueiredo, Carlos)
Date: Thu, 5 Dec 2024 15:16:07 +0000
Subject: [R] Spreadsheets data
Message-ID: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>

Hi there


I want to create a script in Rstudio and load in the reagent dataset ensuring that the different data types in there (dates, text, etc) come through correctly.

Best wishes

Carlos


________________________________
Notice of Confidentiality: The contents of this e-mail and any attachments are confidential and copyright to HCA International and may also be privileged. They are intended only for the use of the addressee(s) shown above. Any unauthorised reading, use, distribution or copying of them is prohibited. If this e-mail has been sent to you in error, you may not take action based on it, nor may you rely on it for any purpose. Please let us know of the error by emailing HCA.DPO at hcahealthcare.co.uk and please return this e-mail. If you have a query relating to an upcoming appointment or your clinical treatment, please email Contact at hcahealthcare.co.uk.

Whilst all reasonable care has been taken to avoid the transmission of viruses, it is the responsibility of the recipient to ensure that onward transmission, opening or use of this message and any attachments will not adversely affect its systems or data. No responsibility is accepted by HCA International in this regard and the recipient should carry out such virus and other checks as it considers appropriate. HCA International is registered in the UK, Company Number 3020522, at 2 Cavendish Square, London, W1G 0PU.

________________________________

From tr@xp|@yer @end|ng |rom gm@||@com  Fri Dec  6 00:38:08 2024
From: tr@xp|@yer @end|ng |rom gm@||@com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Fri, 6 Dec 2024 00:38:08 +0100
Subject: [R] Spreadsheets data
In-Reply-To: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
References: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
Message-ID: <CAGAA5bdEtBqvNWnbYXieXdXBkXNL3cAyF3gtPhzqiKcH-4_cFQ@mail.gmail.com>

On Thu, 5 Dec 2024 at 23:16, Figueiredo, Carlos via R-help
<r-help at r-project.org> wrote:
>
> Hi there
>
>
> I want to create a script in Rstudio and load in the reagent dataset ensuring that the different data types in there (dates, text, etc) come through correctly.

What have you tried? How good do you know R?
R contains many functions to read input.

Regards
Martin


From bgunter@4567 @end|ng |rom gm@||@com  Fri Dec  6 01:22:09 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 5 Dec 2024 16:22:09 -0800
Subject: [R] Spreadsheets data
In-Reply-To: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
References: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
Message-ID: <CAGxFJbR2r7Sey=JLD5LUG7v2TTkk07S1YECiZ0swozk7aP+gKw@mail.gmail.com>

Please read **and follow** the posting guide linked below to learn how
to ask for help on this list.

Cheers,
Bert

On Thu, Dec 5, 2024 at 2:16?PM Figueiredo, Carlos via R-help
<r-help at r-project.org> wrote:
>
> Hi there
>
>
> I want to create a script in Rstudio and load in the reagent dataset ensuring that the different data types in there (dates, text, etc) come through correctly.
>
> Best wishes
>
> Carlos
>
>
> ________________________________
> Notice of Confidentiality: The contents of this e-mail and any attachments are confidential and copyright to HCA International and may also be privileged. They are intended only for the use of the addressee(s) shown above. Any unauthorised reading, use, distribution or copying of them is prohibited. If this e-mail has been sent to you in error, you may not take action based on it, nor may you rely on it for any purpose. Please let us know of the error by emailing HCA.DPO at hcahealthcare.co.uk and please return this e-mail. If you have a query relating to an upcoming appointment or your clinical treatment, please email Contact at hcahealthcare.co.uk.
>
> Whilst all reasonable care has been taken to avoid the transmission of viruses, it is the responsibility of the recipient to ensure that onward transmission, opening or use of this message and any attachments will not adversely affect its systems or data. No responsibility is accepted by HCA International in this regard and the recipient should carry out such virus and other checks as it considers appropriate. HCA International is registered in the UK, Company Number 3020522, at 2 Cavendish Square, London, W1G 0PU.
>
> ________________________________
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From po|c1410 @end|ng |rom gm@||@com  Fri Dec  6 02:18:06 2024
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Fri, 6 Dec 2024 01:18:06 +0000
Subject: [R] Creating a script in Rstudio
In-Reply-To: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>
References: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>
Message-ID: <CA+etgP=KPbsg-J02PQBbdMcfhhY-5h+7g0ueqNNpKdPqZMfRBQ@mail.gmail.com>

Carlos you are gonna have to provide at least a tiny smidge of information

Reagent dataset? What is this?

What is your problem with creating a script in R Studio. File New..

It kinda feels this is either "how do I use R studio" (in which case
YouTube feels your friend) or "how do I use some very specific package
which I am going to forget to tell you"

On Thu, 5 Dec 2024, 22:00 Carlos Alberto Baptista de Figueiredo, <
beto77051 at gmail.com> wrote:

> Hi there
>
>
>
>
>
> I want to create a script in Rstudio and load in the reagent dataset
> ensuring that the different data types in there (dates, text, etc) come
> through correctly.
>
>
>
> Best wishes
>
> Carlos
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From po|c1410 @end|ng |rom gm@||@com  Fri Dec  6 02:19:34 2024
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Fri, 6 Dec 2024 01:19:34 +0000
Subject: [R] Spreadsheets data
In-Reply-To: <CAGAA5bdEtBqvNWnbYXieXdXBkXNL3cAyF3gtPhzqiKcH-4_cFQ@mail.gmail.com>
References: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
 <CAGAA5bdEtBqvNWnbYXieXdXBkXNL3cAyF3gtPhzqiKcH-4_cFQ@mail.gmail.com>
Message-ID: <CA+etgPmS+vQpE=RdYbBt0rS8j9x4fvg=1bNjfASbcfpFzE7YHw@mail.gmail.com>

And R Studio has an import data function that will provide a import
command..

BUT

Spreadsheet is a broad scope

On Thu, 5 Dec 2024, 23:38 Martin M?ller Skarbiniks Pedersen, <
traxplayer at gmail.com> wrote:

> On Thu, 5 Dec 2024 at 23:16, Figueiredo, Carlos via R-help
> <r-help at r-project.org> wrote:
> >
> > Hi there
> >
> >
> > I want to create a script in Rstudio and load in the reagent dataset
> ensuring that the different data types in there (dates, text, etc) come
> through correctly.
>
> What have you tried? How good do you know R?
> R contains many functions to read input.
>
> Regards
> Martin
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Fri Dec  6 02:37:57 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Fri, 6 Dec 2024 01:37:57 +0000
Subject: [R] ggplot2: Plot multiple lines using stacked data.
Message-ID: <DM6PR03MB5049882B33D32BA92E474A27E2312@DM6PR03MB5049.namprd03.prod.outlook.com>

I am trying to use ggplot2 to create a figure with multiple lines, one line for each value of the variable Day. Each group of data for Day requires seven lines. The dataframe has data for 4 days and thus 4*7=28 lines.

I can create a plot, but the plot only contains dots. The dots for each day should be connected each day's data by a different line. There should be a total of four lines on the graph 

mydata <-structure(list(Day = c("25", "25", "25", "25", "25", "25", "25", 
                       "26", "26", "26", "26", "26", "26", "26", "27", "27", "27", "27", 
                       "27", "27", "27", "28", "28", "28", "28", "28", "28", "28"), 
               AQIGroup = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 
                        3L, 4L, 5L, 6L, 7L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 3L, 
                        4L, 5L, 6L, 7L), levels = c("Good", "Moderate", "UnForSome", 
                        "UH", "VUH", "Haz1", "Hax2"), class = "factor"), Freq = c(98.05, 
                         0.37, 0.27, 0.17, 0.26, 0.5, 0.38, 93.34, 4.34, 0.75, 0.42, 
                         0.44, 0.44, 0.27, 89.57, 7.8, 0.98, 0.38, 0.5, 0.52, 0.25, 
                        80.43, 13.33, 3.85, 0.76, 0.86, 0.28, 0.49)), class = "data.frame", row.names = c(NA, -28L))

# You can see that the data are stacked, one day on top of the next.
# Each day requires seven lines.
mydata

# Load ggplot2
if(!require(ggplot2)) {install.packages(ggplot2)}
library(ggplot2)
# Create a graph, with multiple lines, one line for each value of Day.
ggplot(mydata, aes(AQIGroup,Freq,color=Day)) +
  geom_point()+
  geom_line(aes(AQIGroup,Freq))

Thank you,
John


John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?
PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




From @vi@e@gross m@iii@g oii gm@ii@com  Fri Dec  6 03:18:36 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 5 Dec 2024 21:18:36 -0500
Subject: [R] ggplot2: Plot multiple lines using stacked data.
In-Reply-To: <DM6PR03MB5049882B33D32BA92E474A27E2312@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049882B33D32BA92E474A27E2312@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <001501db4785$287dff10$7979fd30$@gmail.com>

John,

I hate to break it to you that ggplot2 is not part of base R. "plot" and
maybe lattice qualify.

But I can work with you. Try this:

ggplot(data=mydata, 
       aes(x=AQIGroup,
           y=Freq,
           group=Day,
           color=Day)) +
  geom_point() +
  geom_line()

The main change besides naming arguments for clarity, is adding what it
should be grouped by. I believe you want it grouped by day, but am not
totally sure except that the grouping was not done by your last line that
sort of changed the x,y back to what it would be anyway.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Sorkin, John
Sent: Thursday, December 5, 2024 8:38 PM
To: r-help at r-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: [R] ggplot2: Plot multiple lines using stacked data.

I am trying to use ggplot2 to create a figure with multiple lines, one line
for each value of the variable Day. Each group of data for Day requires
seven lines. The dataframe has data for 4 days and thus 4*7=28 lines.

I can create a plot, but the plot only contains dots. The dots for each day
should be connected each day's data by a different line. There should be a
total of four lines on the graph 

mydata <-structure(list(Day = c("25", "25", "25", "25", "25", "25", "25", 
                       "26", "26", "26", "26", "26", "26", "26", "27", "27",
"27", "27", 
                       "27", "27", "27", "28", "28", "28", "28", "28", "28",
"28"), 
               AQIGroup = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 
                        3L, 4L, 5L, 6L, 7L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L,
2L, 3L, 
                        4L, 5L, 6L, 7L), levels = c("Good", "Moderate",
"UnForSome", 
                        "UH", "VUH", "Haz1", "Hax2"), class = "factor"),
Freq = c(98.05, 
                         0.37, 0.27, 0.17, 0.26, 0.5, 0.38, 93.34, 4.34,
0.75, 0.42, 
                         0.44, 0.44, 0.27, 89.57, 7.8, 0.98, 0.38, 0.5,
0.52, 0.25, 
                        80.43, 13.33, 3.85, 0.76, 0.86, 0.28, 0.49)), class
= "data.frame", row.names = c(NA, -28L))

# You can see that the data are stacked, one day on top of the next.
# Each day requires seven lines.
mydata

# Load ggplot2
if(!require(ggplot2)) {install.packages(ggplot2)}
library(ggplot2)
# Create a graph, with multiple lines, one line for each value of Day.
ggplot(mydata, aes(AQIGroup,Freq,color=Day)) +
  geom_point()+
  geom_line(aes(AQIGroup,Freq))

Thank you,
John


John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical
Center Geriatrics Research, Education, and Clinical Center;?
PI?Biostatistics and Informatics Core, University of Maryland School of
Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382



______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Dec  6 03:27:09 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 5 Dec 2024 21:27:09 -0500
Subject: [R] Spreadsheets data
In-Reply-To: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
References: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
Message-ID: <001901db4786$59edd470$0dc97d50$@gmail.com>

What exactly is stopping you, Carlos?

Do you know where the reagent dataset is? I don't. Is it being loaded from a
known format like a .CSV file, or part of an EXCEL spreadsheet and so on?

Does it come with a function in a package that you load and then invoke to
load it or make it visible? Or, should you already have copied it somewhere
on your disk? Depending on many scenarios, loading methods are available.

Moving on, if you read it in, often enough some columns are assigned a
different data type than you want. Some methods of reading in may allow you
to supply a list of hints as to what you want each column to be.

In any case, once you have read in the data, perhaps in the form of a
data.frame or tibble or other variants, you can generally execute commands
to replace a column with a transformation of your choice such as
as.integer() or convert it to a factor or even make new columns with other
visions of the original and so on.

Without some info on what you have tried and what failed, this is a very
basic request and chances are you won't get much help. 

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Figueiredo, Carlos
via R-help
Sent: Thursday, December 5, 2024 10:16 AM
To: r-help at r-project.org
Subject: [R] Spreadsheets data

Hi there


I want to create a script in Rstudio and load in the reagent dataset
ensuring that the different data types in there (dates, text, etc) come
through correctly.

Best wishes

Carlos


________________________________
Notice of Confidentiality: The contents of this e-mail and any attachments
are confidential and copyright to HCA International and may also be
privileged. They are intended only for the use of the addressee(s) shown
above. Any unauthorised reading, use, distribution or copying of them is
prohibited. If this e-mail has been sent to you in error, you may not take
action based on it, nor may you rely on it for any purpose. Please let us
know of the error by emailing HCA.DPO at hcahealthcare.co.uk and please return
this e-mail. If you have a query relating to an upcoming appointment or your
clinical treatment, please email Contact at hcahealthcare.co.uk.

Whilst all reasonable care has been taken to avoid the transmission of
viruses, it is the responsibility of the recipient to ensure that onward
transmission, opening or use of this message and any attachments will not
adversely affect its systems or data. No responsibility is accepted by HCA
International in this regard and the recipient should carry out such virus
and other checks as it considers appropriate. HCA International is
registered in the UK, Company Number 3020522, at 2 Cavendish Square, London,
W1G 0PU.

________________________________
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rb@er @end|ng |rom @t@u@edu  Fri Dec  6 15:50:45 2024
From: rb@er @end|ng |rom @t@u@edu (Robert Baer)
Date: Fri, 6 Dec 2024 08:50:45 -0600
Subject: [R] Creating a script in Rstudio
In-Reply-To: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>
References: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>
Message-ID: <6ba75355-89fa-4d8a-9246-e1bafa1dc5b8@atsu.edu>

This is off-topic for this list, but first use the file | import dataset 
menu.? Then go to the history tab, highlight the import statement that 
was generated and click to source.? You'll have to figure out where the 
reagent dataset is on your own.

On 12/5/2024 9:21 AM, Carlos Alberto Baptista de Figueiredo wrote:
> Hi there
>
>
>
>
>
> I want to create a script in Rstudio and load in the reagent dataset
> ensuring that the different data types in there (dates, text, etc) come
> through correctly.
>
>
>
> Best wishes
>
> Carlos
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
---
Robert W. Baer, Ph.D.
Professor of Physiology
Kirksville College of Osteopathic Medicine
A.T. Still Univerisity of Health Sciences
800 W. Jefferson St.
Kirksville, MO 63501


From dyk|m7411 @end|ng |rom gm@||@com  Fri Dec  6 21:31:15 2024
From: dyk|m7411 @end|ng |rom gm@||@com (D)
Date: Fri, 6 Dec 2024 15:31:15 -0500
Subject: [R] Sum by group
Message-ID: <CADmwX-+SgmyGXG3reyX3nH9fz3dPU=9DqdOJbdMHanqDhrjjTA@mail.gmail.com>

I have population data (?totpopE?) at the census tract level (?GEOID?),
which are nested within Precincts (?Precinct?). Please see below my data
structure.

I used the code to sum population data per precinct:

inters <- inters %>%

  group_by(Precinct) %>%

  mutate(TotalPop = sum(totpopE)

  )

However, said code produced too large sums because each census tract
(?GEOID?) has multiple observations/rows.  Each census tract has the same
value.  Is there any way I can use one value for each census tract to
estimate total populations at the precinct level?  It would be appreciated
if anyone can provide codes to sum population data per precinct.

> tail(df, n=20)Simple feature collection with 20 features and 3 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 989211 ymin: 193205 xmax: 997877 ymax: 222689
Projected CRS: NAD83 / New York Long Island (ftUS)# A tibble: 20 ? 4#
Groups:   GEOID [5]
   Precinct GEOID       totpopE                 geometry
   <fct>    <chr>         <dbl> <POINT [US_survey_foot]> 1 20
36061015700   10352          (989211 222689) 2 20       36061015700
10352          (989211 222689) 3 20       36061015700   10352
(989211 222689) 4 20       36061015700   10352          (989211
222689) 5 79       36047123700    9448          (996168 193934) 6 79
    36047123700    9448          (996598 193205) 7 79
36047123700    9448          (996598 193205) 8 79       36047123700
9448          (996598 193205) 9 19       36061013400   11387
(996703 219599)10 19       36061013400   11387          (996475
220183)11 19       36061013800   12826          (996871 222201)12 19
    36061013800   12826          (997576 221811)13 19
36061013800   12826          (997877 221608)14 19       36061013800
12826          (997877 221608)15 19       36061013800   12826
(996216 221621)16 13       36061004400   15945          (990562
205039)17 13       36061004400   15945          (989574 206434)18 13
    36061004400   15945          (989574 206434)19 13
36061004400   15945          (989574 206434)20 9        36061004400
15945          (989699 205397)

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Dec  7 08:48:46 2024
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 06 Dec 2024 23:48:46 -0800
Subject: [R] Sum by group
In-Reply-To: <CADmwX-+SgmyGXG3reyX3nH9fz3dPU=9DqdOJbdMHanqDhrjjTA@mail.gmail.com>
References: <CADmwX-+SgmyGXG3reyX3nH9fz3dPU=9DqdOJbdMHanqDhrjjTA@mail.gmail.com>
Message-ID: <6F67CF86-D6CC-44D7-8DE8-F81014B9CC17@dcn.davis.ca.us>

That is what the "summarise" function is designed to do (instead of mutate). All of the calculations in a summarise have to aggregate all group rows down to one value, but that is what you want in this case.

Please note that you are supposed to make your examples reproducible by including your library statements when posting on this mailing list. From your code sequence I can guess that you are using the dplyr package, but on this mailing list it is bad form to assume your readers know which user-contributed packages you are using. Perhaps use the "reprex" package to check that other people will have all the info they need to run your example in their computers so they can be clear what you are dealing with without guessing.

On December 6, 2024 12:31:15 PM PST, D <dykim7411 at gmail.com> wrote:
>I have population data (?totpopE?) at the census tract level (?GEOID?),
>which are nested within Precincts (?Precinct?). Please see below my data
>structure.
>
>I used the code to sum population data per precinct:
>
>inters <- inters %>%
>
>  group_by(Precinct) %>%
>
>  mutate(TotalPop = sum(totpopE)
>
>  )
>
>However, said code produced too large sums because each census tract
>(?GEOID?) has multiple observations/rows.  Each census tract has the same
>value.  Is there any way I can use one value for each census tract to
>estimate total populations at the precinct level?  It would be appreciated
>if anyone can provide codes to sum population data per precinct.
>
>> tail(df, n=20)Simple feature collection with 20 features and 3 fields
>Geometry type: POINT
>Dimension:     XY
>Bounding box:  xmin: 989211 ymin: 193205 xmax: 997877 ymax: 222689
>Projected CRS: NAD83 / New York Long Island (ftUS)# A tibble: 20 ? 4#
>Groups:   GEOID [5]
>   Precinct GEOID       totpopE                 geometry
>   <fct>    <chr>         <dbl> <POINT [US_survey_foot]> 1 20
>36061015700   10352          (989211 222689) 2 20       36061015700
>10352          (989211 222689) 3 20       36061015700   10352
>(989211 222689) 4 20       36061015700   10352          (989211
>222689) 5 79       36047123700    9448          (996168 193934) 6 79
>    36047123700    9448          (996598 193205) 7 79
>36047123700    9448          (996598 193205) 8 79       36047123700
>9448          (996598 193205) 9 19       36061013400   11387
>(996703 219599)10 19       36061013400   11387          (996475
>220183)11 19       36061013800   12826          (996871 222201)12 19
>    36061013800   12826          (997576 221811)13 19
>36061013800   12826          (997877 221608)14 19       36061013800
>12826          (997877 221608)15 19       36061013800   12826
>(996216 221621)16 13       36061004400   15945          (990562
>205039)17 13       36061004400   15945          (989574 206434)18 13
>    36061004400   15945          (989574 206434)19 13
>36061004400   15945          (989574 206434)20 9        36061004400
>15945          (989699 205397)
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From phii m@iii@g oii phiiipsmith@c@  Tue Dec 10 01:56:29 2024
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Mon, 09 Dec 2024 19:56:29 -0500
Subject: [R] Heat maps containing two types of variables
Message-ID: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>

I am working with a heat map, as in the REPREX below. The code works 
fine as long as "bigger numbers imply greener and smaller numbers imply 
redder". These are time series where bigger numbers are "better", like 
total employment for example. But I also have cases within the heat map 
where "bigger numbers imply redder and smaller numbers imply greener". 
These are time series where bigger numbers are "worse", like total 
unemployment for example. So suppose column B in dat is of the second 
type, i.e. "bigger numbers imply redder and smaller numbers imply 
greener". I would like the colour coding to be the reverse of what it is 
for columns A and C. How can I modify the code to accomplish this? I 
have tried different approaches with no success. Thanks for your help. 
Philip

# REPREX
library(ggplot2)
library(tidyr)
library(dplyr)
dat <- data.frame(
   date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
   A=c(1,3,3,4,2,6),
   B=c(3,5,6,4,8,9),
   C=c(10,8,17,19,26,22)
)
dat_long <- pivot_longer(dat,2:4,names_to="variable",values_to="value")
normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) }
dat_norm <- mutate(dat,across(2:4,normalize))
dat_long_norm <- 
pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
heatmap <- ggplot(dat_long, aes(x = date, y = variable,fill=norm_value)) 
+
   geom_tile() +
   geom_text(aes(label = as.character(value)),
     color = "black", size = 2.5) +
   labs(title="REPREX",x="",y="")+
   scale_fill_gradient2(low = "#E94A26", mid = "white", high = "#A1D385", 
midpoint = 0.5) +
   scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
     as.Date("2024-06-01"),by="month"),
     labels=function(x) format(x,"%b\n%Y"),position="top")+
   theme(legend.position="none")
heatmap
ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)


From tebert @end|ng |rom u||@edu  Tue Dec 10 05:33:53 2024
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 10 Dec 2024 04:33:53 +0000
Subject: [R] Heat maps containing two types of variables
In-Reply-To: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
References: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
Message-ID: <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>

What happens if you switch the colors in this line:
   scale_fill_gradient2(low = "#E94A26", mid = "white", high = "#A1D385", midpoint = 0.5) +
to be the following
   scale_fill_gradient2(low = "# A1D385", mid = "white", high = "# E94A26", midpoint = 0.5) +

That said, a red-green heat map may be unhelpful to color blind people.

So then you need two ggplot statements, one with each scale_fill_gradient2 and then specify which version to plot for each variable.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of phil at philipsmith.ca
Sent: Monday, December 9, 2024 7:56 PM
To: R-help at r-project.org
Subject: [R] Heat maps containing two types of variables

[External Email]

I am working with a heat map, as in the REPREX below. The code works fine as long as "bigger numbers imply greener and smaller numbers imply redder". These are time series where bigger numbers are "better", like total employment for example. But I also have cases within the heat map where "bigger numbers imply redder and smaller numbers imply greener".
These are time series where bigger numbers are "worse", like total unemployment for example. So suppose column B in dat is of the second type, i.e. "bigger numbers imply redder and smaller numbers imply greener". I would like the colour coding to be the reverse of what it is for columns A and C. How can I modify the code to accomplish this? I have tried different approaches with no success. Thanks for your help.
Philip

# REPREX
library(ggplot2)
library(tidyr)
library(dplyr)
dat <- data.frame(
   date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
   A=c(1,3,3,4,2,6),
   B=c(3,5,6,4,8,9),
   C=c(10,8,17,19,26,22)
)
dat_long <- pivot_longer(dat,2:4,names_to="variable",values_to="value")
normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) } dat_norm <- mutate(dat,across(2:4,normalize)) dat_long_norm <-
pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
heatmap <- ggplot(dat_long, aes(x = date, y = variable,fill=norm_value))
+
   geom_tile() +
   geom_text(aes(label = as.character(value)),
     color = "black", size = 2.5) +
   labs(title="REPREX",x="",y="")+
   scale_fill_gradient2(low = "#E94A26", mid = "white", high = "#A1D385", midpoint = 0.5) +
   scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
     as.Date("2024-06-01"),by="month"),
     labels=function(x) format(x,"%b\n%Y"),position="top")+
   theme(legend.position="none")
heatmap
ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From phii m@iii@g oii phiiipsmith@c@  Tue Dec 10 13:59:25 2024
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Tue, 10 Dec 2024 07:59:25 -0500
Subject: [R] Heat maps containing two types of variables
In-Reply-To: <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
 <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <5a3ca0d2997a114390cf70cc8336e9b6@philipsmith.ca>

Thank you for the suggestion. I tried it, but could not get it to work. 
When I added a second ggplot statement, I hit an error saying that one 
cannot add a ggplot to a ggplot object. So I added a second geom_tile 
statement instead. That worked, except that it warned that since a scale 
for fill was already present, the new fill would replace the old one. In 
other words, the colour scale was changed not just for the target, B, 
but also for the other two variables. So I am still searching for a 
solution.

Philip

On 2024-12-09 23:33, Ebert,Timothy Aaron wrote:
> What happens if you switch the colors in this line:
>    scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) +
> to be the following
>    scale_fill_gradient2(low = "# A1D385", mid = "white", high = "# 
> E94A26", midpoint = 0.5) +
> 
> That said, a red-green heat map may be unhelpful to color blind people.
> 
> So then you need two ggplot statements, one with each 
> scale_fill_gradient2 and then specify which version to plot for each 
> variable.
> 
> Tim
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of 
> phil at philipsmith.ca
> Sent: Monday, December 9, 2024 7:56 PM
> To: R-help at r-project.org
> Subject: [R] Heat maps containing two types of variables
> 
> [External Email]
> 
> I am working with a heat map, as in the REPREX below. The code works 
> fine as long as "bigger numbers imply greener and smaller numbers imply 
> redder". These are time series where bigger numbers are "better", like 
> total employment for example. But I also have cases within the heat map 
> where "bigger numbers imply redder and smaller numbers imply greener".
> These are time series where bigger numbers are "worse", like total 
> unemployment for example. So suppose column B in dat is of the second 
> type, i.e. "bigger numbers imply redder and smaller numbers imply 
> greener". I would like the colour coding to be the reverse of what it 
> is for columns A and C. How can I modify the code to accomplish this? I 
> have tried different approaches with no success. Thanks for your help.
> Philip
> 
> # REPREX
> library(ggplot2)
> library(tidyr)
> library(dplyr)
> dat <- data.frame(
>    
> date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
>    A=c(1,3,3,4,2,6),
>    B=c(3,5,6,4,8,9),
>    C=c(10,8,17,19,26,22)
> )
> dat_long <- pivot_longer(dat,2:4,names_to="variable",values_to="value")
> normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) } dat_norm 
> <- mutate(dat,across(2:4,normalize)) dat_long_norm <-
> pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
> dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
> heatmap <- ggplot(dat_long, aes(x = date, y = 
> variable,fill=norm_value))
> +
>    geom_tile() +
>    geom_text(aes(label = as.character(value)),
>      color = "black", size = 2.5) +
>    labs(title="REPREX",x="",y="")+
>    scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) +
>    scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
>      as.Date("2024-06-01"),by="month"),
>      labels=function(x) format(x,"%b\n%Y"),position="top")+
>    theme(legend.position="none")
> heatmap
> ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Tue Dec 10 14:35:26 2024
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 10 Dec 2024 13:35:26 +0000
Subject: [R] Heat maps containing two types of variables
In-Reply-To: <5a3ca0d2997a114390cf70cc8336e9b6@philipsmith.ca>
References: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
 <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <5a3ca0d2997a114390cf70cc8336e9b6@philipsmith.ca>
Message-ID: <CH3PR22MB4514AA72E844CC1572251AE2CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>

You will need to add code to tell R which variables to plot in reverse color order. In this code, I chose variable B to plot in reverse order. 


library(ggplot2)
library(tidyr)
library(dplyr)

dat <- data.frame(
  date = seq.Date(as.Date("2024-01-01"), as.Date("2024-06-01"), by = "month"),
  A = c(1, 3, 3, 4, 2, 6),
  B = c(3, 5, 6, 4, 8, 9),
  C = c(10, 8, 17, 19, 26, 22)
)
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
dat_long <- dat %>%
  mutate(across(2:4, normalize)) %>% 
  pivot_longer(cols = A:C, names_to = "variable", values_to = "norm_value") %>%
  left_join(
    dat %>%
      pivot_longer(cols = A:C, names_to = "variable", values_to = "value"),
    by = c("date", "variable")
  )

dat_long <- dat_long %>%
  mutate(adjusted_norm_value = ifelse(variable == "B", 1 - norm_value, norm_value))

heatmap <- ggplot(dat_long, aes(x = date, y = variable, fill = adjusted_norm_value)) +
  geom_tile() +
  geom_text(aes(label = as.character(value)), color = "black", size = 2.5) +
  labs(title = "REPREX", x = "", y = "") +
  scale_fill_gradient2(low = "#E94A26", mid = "white", high = "#A1D385", midpoint = 0.5) +
  scale_x_continuous(
    breaks = seq.Date(as.Date("2024-01-01"), as.Date("2024-06-01"), by = "month"),
    labels = function(x) format(x, "%b\n%Y"),
    position = "top"
  ) +
  theme(legend.position = "none")

heatmap

-----Original Message-----
From: phil at philipsmith.ca <phil at philipsmith.ca> 
Sent: Tuesday, December 10, 2024 7:59 AM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: R-help at r-project.org
Subject: Re: [R] Heat maps containing two types of variables

[External Email]

Thank you for the suggestion. I tried it, but could not get it to work.
When I added a second ggplot statement, I hit an error saying that one cannot add a ggplot to a ggplot object. So I added a second geom_tile statement instead. That worked, except that it warned that since a scale for fill was already present, the new fill would replace the old one. In other words, the colour scale was changed not just for the target, B, but also for the other two variables. So I am still searching for a solution.

Philip

On 2024-12-09 23:33, Ebert,Timothy Aaron wrote:
> What happens if you switch the colors in this line:
>    scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) + to be the following
>    scale_fill_gradient2(low = "# A1D385", mid = "white", high = "# 
> E94A26", midpoint = 0.5) +
>
> That said, a red-green heat map may be unhelpful to color blind people.
>
> So then you need two ggplot statements, one with each
> scale_fill_gradient2 and then specify which version to plot for each 
> variable.
>
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of 
> phil at philipsmith.ca
> Sent: Monday, December 9, 2024 7:56 PM
> To: R-help at r-project.org
> Subject: [R] Heat maps containing two types of variables
>
> [External Email]
>
> I am working with a heat map, as in the REPREX below. The code works 
> fine as long as "bigger numbers imply greener and smaller numbers 
> imply redder". These are time series where bigger numbers are 
> "better", like total employment for example. But I also have cases 
> within the heat map where "bigger numbers imply redder and smaller numbers imply greener".
> These are time series where bigger numbers are "worse", like total 
> unemployment for example. So suppose column B in dat is of the second 
> type, i.e. "bigger numbers imply redder and smaller numbers imply 
> greener". I would like the colour coding to be the reverse of what it 
> is for columns A and C. How can I modify the code to accomplish this? 
> I have tried different approaches with no success. Thanks for your help.
> Philip
>
> # REPREX
> library(ggplot2)
> library(tidyr)
> library(dplyr)
> dat <- data.frame(
>
> date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
>    A=c(1,3,3,4,2,6),
>    B=c(3,5,6,4,8,9),
>    C=c(10,8,17,19,26,22)
> )
> dat_long <- 
> pivot_longer(dat,2:4,names_to="variable",values_to="value")
> normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) } dat_norm
> <- mutate(dat,across(2:4,normalize)) dat_long_norm <-
> pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
> dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
> heatmap <- ggplot(dat_long, aes(x = date, y =
> variable,fill=norm_value))
> +
>    geom_tile() +
>    geom_text(aes(label = as.character(value)),
>      color = "black", size = 2.5) +
>    labs(title="REPREX",x="",y="")+
>    scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) +
>    scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
>      as.Date("2024-06-01"),by="month"),
>      labels=function(x) format(x,"%b\n%Y"),position="top")+
>    theme(legend.position="none")
> heatmap
> ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> %7C1a08885d2b61452a502308dd191a7c05%7C0d4da0f84a314d76ace60a62331e1b84
> %7C0%7C0%7C638694323735326272%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> %3D%3D%7C0%7C%7C%7C&sdata=AyPt4o4LzEivT0D6JfeB%2Bav7N2wnZM%2BxPkrLF9B3
> LvI%3D&reserved=0
> PLEASE do read the posting guide
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
> r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
> a08885d2b61452a502308dd191a7c05%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> %7C0%7C638694323735345552%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> 3D%7C0%7C%7C%7C&sdata=bH1aZLifelBywJO42%2BWGcyU1O6tmJbQi3DPC%2FjxWIKo%
> 3D&reserved=0 and provide commented, minimal, self-contained, 
> reproducible code.


From phii m@iii@g oii phiiipsmith@c@  Tue Dec 10 16:37:37 2024
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Tue, 10 Dec 2024 10:37:37 -0500
Subject: [R] Heat maps containing two types of variables
In-Reply-To: <CH3PR22MB4514AA72E844CC1572251AE2CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
 <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <5a3ca0d2997a114390cf70cc8336e9b6@philipsmith.ca>
 <CH3PR22MB4514AA72E844CC1572251AE2CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <e1c965c85d47ff39023d910be732ddaa@philipsmith.ca>

Thanks for sticking with me Tim. Your solution is clear and works like a 
charm.

Philip

On 2024-12-10 08:35, Ebert,Timothy Aaron wrote:
> You will need to add code to tell R which variables to plot in reverse 
> color order. In this code, I chose variable B to plot in reverse order.
> 
> 
> library(ggplot2)
> library(tidyr)
> library(dplyr)
> 
> dat <- data.frame(
>   date = seq.Date(as.Date("2024-01-01"), as.Date("2024-06-01"), by = 
> "month"),
>   A = c(1, 3, 3, 4, 2, 6),
>   B = c(3, 5, 6, 4, 8, 9),
>   C = c(10, 8, 17, 19, 26, 22)
> )
> normalize <- function(x) {
>   (x - min(x)) / (max(x) - min(x))
> }
> dat_long <- dat %>%
>   mutate(across(2:4, normalize)) %>%
>   pivot_longer(cols = A:C, names_to = "variable", values_to = 
> "norm_value") %>%
>   left_join(
>     dat %>%
>       pivot_longer(cols = A:C, names_to = "variable", values_to = 
> "value"),
>     by = c("date", "variable")
>   )
> 
> dat_long <- dat_long %>%
>   mutate(adjusted_norm_value = ifelse(variable == "B", 1 - norm_value, 
> norm_value))
> 
> heatmap <- ggplot(dat_long, aes(x = date, y = variable, fill = 
> adjusted_norm_value)) +
>   geom_tile() +
>   geom_text(aes(label = as.character(value)), color = "black", size = 
> 2.5) +
>   labs(title = "REPREX", x = "", y = "") +
>   scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) +
>   scale_x_continuous(
>     breaks = seq.Date(as.Date("2024-01-01"), as.Date("2024-06-01"), by 
> = "month"),
>     labels = function(x) format(x, "%b\n%Y"),
>     position = "top"
>   ) +
>   theme(legend.position = "none")
> 
> heatmap
> 
> -----Original Message-----
> From: phil at philipsmith.ca <phil at philipsmith.ca>
> Sent: Tuesday, December 10, 2024 7:59 AM
> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> Cc: R-help at r-project.org
> Subject: Re: [R] Heat maps containing two types of variables
> 
> [External Email]
> 
> Thank you for the suggestion. I tried it, but could not get it to work.
> When I added a second ggplot statement, I hit an error saying that one 
> cannot add a ggplot to a ggplot object. So I added a second geom_tile 
> statement instead. That worked, except that it warned that since a 
> scale for fill was already present, the new fill would replace the old 
> one. In other words, the colour scale was changed not just for the 
> target, B, but also for the other two variables. So I am still 
> searching for a solution.
> 
> Philip
> 
> On 2024-12-09 23:33, Ebert,Timothy Aaron wrote:
>> What happens if you switch the colors in this line:
>>    scale_fill_gradient2(low = "#E94A26", mid = "white", high =
>> "#A1D385", midpoint = 0.5) + to be the following
>>    scale_fill_gradient2(low = "# A1D385", mid = "white", high = "#
>> E94A26", midpoint = 0.5) +
>> 
>> That said, a red-green heat map may be unhelpful to color blind 
>> people.
>> 
>> So then you need two ggplot statements, one with each
>> scale_fill_gradient2 and then specify which version to plot for each
>> variable.
>> 
>> Tim
>> 
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of
>> phil at philipsmith.ca
>> Sent: Monday, December 9, 2024 7:56 PM
>> To: R-help at r-project.org
>> Subject: [R] Heat maps containing two types of variables
>> 
>> [External Email]
>> 
>> I am working with a heat map, as in the REPREX below. The code works
>> fine as long as "bigger numbers imply greener and smaller numbers
>> imply redder". These are time series where bigger numbers are
>> "better", like total employment for example. But I also have cases
>> within the heat map where "bigger numbers imply redder and smaller 
>> numbers imply greener".
>> These are time series where bigger numbers are "worse", like total
>> unemployment for example. So suppose column B in dat is of the second
>> type, i.e. "bigger numbers imply redder and smaller numbers imply
>> greener". I would like the colour coding to be the reverse of what it
>> is for columns A and C. How can I modify the code to accomplish this?
>> I have tried different approaches with no success. Thanks for your 
>> help.
>> Philip
>> 
>> # REPREX
>> library(ggplot2)
>> library(tidyr)
>> library(dplyr)
>> dat <- data.frame(
>> 
>> date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
>>    A=c(1,3,3,4,2,6),
>>    B=c(3,5,6,4,8,9),
>>    C=c(10,8,17,19,26,22)
>> )
>> dat_long <-
>> pivot_longer(dat,2:4,names_to="variable",values_to="value")
>> normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) } dat_norm
>> <- mutate(dat,across(2:4,normalize)) dat_long_norm <-
>> pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
>> dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
>> heatmap <- ggplot(dat_long, aes(x = date, y =
>> variable,fill=norm_value))
>> +
>>    geom_tile() +
>>    geom_text(aes(label = as.character(value)),
>>      color = "black", size = 2.5) +
>>    labs(title="REPREX",x="",y="")+
>>    scale_fill_gradient2(low = "#E94A26", mid = "white", high =
>> "#A1D385", midpoint = 0.5) +
>>    scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
>>      as.Date("2024-06-01"),by="month"),
>>      labels=function(x) format(x,"%b\n%Y"),position="top")+
>>    theme(legend.position="none")
>> heatmap
>> ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
>> %7C1a08885d2b61452a502308dd191a7c05%7C0d4da0f84a314d76ace60a62331e1b84
>> %7C0%7C0%7C638694323735326272%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
>> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
>> %3D%3D%7C0%7C%7C%7C&sdata=AyPt4o4LzEivT0D6JfeB%2Bav7N2wnZM%2BxPkrLF9B3
>> LvI%3D&reserved=0
>> PLEASE do read the posting guide
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
>> r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
>> a08885d2b61452a502308dd191a7c05%7C0d4da0f84a314d76ace60a62331e1b84%7C0
>> %7C0%7C638694323735345552%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
>> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
>> 3D%7C0%7C%7C%7C&sdata=bH1aZLifelBywJO42%2BWGcyU1O6tmJbQi3DPC%2FjxWIKo%
>> 3D&reserved=0 and provide commented, minimal, self-contained,
>> reproducible code.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Tue Dec 10 17:36:54 2024
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Tue, 10 Dec 2024 22:06:54 +0530
Subject: [R] outer() is not working with my simple function
Message-ID: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>

Hi,

I have below code

FN1 = function(x, y) 3

outer(1:9, 1:9, FN1)

With above I get error as below

Error in dim(robj) <- c(dX, dY) :

  dims [product 81] do not match the length of object [1]

Could you please help to understand why is it failing? I am just
expecting to get a matrix with all elements as 3


From |kw@|mmo @end|ng |rom gm@||@com  Tue Dec 10 17:43:05 2024
From: |kw@|mmo @end|ng |rom gm@||@com (Iris Simmons)
Date: Tue, 10 Dec 2024 11:43:05 -0500
Subject: [R] outer() is not working with my simple function
In-Reply-To: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>
References: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>
Message-ID: <CADNULg_p_y_91OrvKEpevQgE3+=N7dfd682Z+cq=bS=8c-mnpg@mail.gmail.com>

outer() expects a return value with the same length as that of x.

You could change the code to rep(3, length(x))

Or if you don't want to change the code of the function, you could make a
wrapper and use that instead:
function (x, y)
{
    v <- FN1(x, y)
    if (length(v) != 1) v else rep(v, length(x))
}


On Tue, Dec 10, 2024, 11:37 Christofer Bogaso <bogaso.christofer at gmail.com>
wrote:

> Hi,
>
> I have below code
>
> FN1 = function(x, y) 3
>
> outer(1:9, 1:9, FN1)
>
> With above I get error as below
>
> Error in dim(robj) <- c(dX, dY) :
>
>   dims [product 81] do not match the length of object [1]
>
> Could you please help to understand why is it failing? I am just
> expecting to get a matrix with all elements as 3
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Dec 10 17:44:12 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 10 Dec 2024 11:44:12 -0500
Subject: [R] outer() is not working with my simple function
In-Reply-To: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>
References: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>
Message-ID: <489a80da-58bf-4bae-a83e-5c8274be78c0@gmail.com>

On 2024-12-10 11:36 a.m., Christofer Bogaso wrote:
> Hi,
> 
> I have below code
> 
> FN1 = function(x, y) 3
> 
> outer(1:9, 1:9, FN1)
> 
> With above I get error as below
> 
> Error in dim(robj) <- c(dX, dY) :
> 
>    dims [product 81] do not match the length of object [1]
> 
> Could you please help to understand why is it failing? I am just
> expecting to get a matrix with all elements as 3

The help page says this about the function:  "It must be a vectorized 
function (or the name of one) expecting at least two arguments and 
returning a value with the same length as the first (and the second)." 
So your function could be

   FN1 <- function(x, y) rep_len(3, length(x))

and it would work.

Duncan Murdoch


From tder@mu@ @end|ng |rom mgb@org  Wed Dec 11 01:20:28 2024
From: tder@mu@ @end|ng |rom mgb@org (Deramus, Thomas Patrick)
Date: Wed, 11 Dec 2024 00:20:28 +0000
Subject: [R] Cores hang when calling mcapply
Message-ID: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>

Hi R users.

Apologies for the lack of concrete examples because the dataset is large, and it being so I believe is the issue.

I multiple, very large datasets for which I need to generate 0/1 absence/presence columns
Some include over 200M rows, with two columns that need presence/absence columns based on the strings contained within them, as an example, one set has ~29k unique values and the other with ~15k unique values (no overlap across the two).

Using a combination of custom functions:

crewjanitormakeclean <- function(df,columns) {
  df <- df |> mutate(across(columns, ~make_clean_names(.,  allow_dupes = TRUE)))
  return(df)
}

mass_pivot_wider <- function(df,column,prefix) {
  df <- df |> distinct() |> mutate(n = 1) |> pivot_wider(names_from = glue("{column}"), values_from = n, names_prefix = prefix, values_fill = list(n = 0))
  return(df)
}

sum_group_function <- function(df) {
  df <- df |> group_by(ID_Key) |> summarise(across(c(starts_with("column1_name_"),starts_with("column2_name_"),), ~ sum(.x, na.rm = TRUE))) |> ungroup()
  return(df)
}

and splitting up the data into a list of 110k individual dataframes based on Key_ID

temp <-
  open_dataset(
    sources = input_files,
    format = 'csv',
    unify_schema = TRUE,
    col_types = schema(
      "ID_Key" = string(),
      "column1" = string(),
      "column1" = string()
    )
  ) |> as_tibble()


  keeptabs <- split(temp, temp$ID_Key)


I used a multicore framework to distribute the `sum` functions across each Key_ID when a multicore argument is enabled.

??????
    if(isTRUE(multicore)){
      output <- mclapply(1:length(modtabs), function(i) crewjanitormakeclean(modtabs[[i]],c("string_columns_2","string_columns_1")), mc.cores = numcores)
      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_1","col_1_name_"), mc.cores = numcores)
      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_2","col_2_name_"), mc.cores = numcores)
    }else{
      output <- lapply(1:length(modtabs), function(i) crewjanitormakeclean(modtabs[[i]],c("string_columns_2","string_columns_1")))
      output <- lapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_1","col_1_name_"))
      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_2","col_2_name_"))
    }


Moving every Key_ID to a single row and then row-binding the data while creating new columns for the differences across `Key_ID`s from the pivot using the following solution (78 upvotes at time of this email):
https://stackoverflow.com/questions/3402371/combine-two-data-frames-by-rows-rbind-when-they-have-different-sets-of-columns

  allNms <- unique(unlist(lapply(keeptabs, names)))

  output <- do.call(rbind,
                     c(lapply(keeptabs,
                              function(x) data.frame(c((x), sapply(setdiff(allNms, names(x)),
                                                                 function(y) NA))) |> as_tibble()),
                       make.row.names=FALSE)) |> mutate(across(c(starts_with("column1_name_"), starts_with("column2_name_")), coalesce, 0))

However, I have noticed that the jobs seem to "hang" after a while, with the initial 30 or so (numcores == 30 in the workflow, equal to the number of cores I reserved) at 100% of the requested CPU, and then several "zombie" processes occur and the cores just stop at 0% and never proceed, usually dying with a timeout or not all jobs running to completion failure to join of some kind.

This happens in both base R and RStudio, and I haven't been able to figure out if it's something wrong with the code, the size of the data, or our architecture, but I would appreciate any suggestions as to what I might be able to do about this.

Before they are suggested, I have also tried this same approach with foreach, snow, future, and furr packages, and base parallel with mc.apply seems to be the only thing that works for at least one dataset.

In the event it has something to do with our architecture, here is what we are running on and our loaded packages:

OS Information:
NAME="Red Hat Enterprise Linux"
VERSION="9.3 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
Operating System: Red Hat Enterprise Linux 9.3 (Plow)
     CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
          Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
    Architecture: x86-64
 Hardware Vendor: Dell Inc.
  Hardware Model: PowerEdge R840
Firmware Version: 2.15.1

R Version:
R.Version()
$platform
[1] "x86_64-pc-linux-gnu"
$arch
[1] "x86_64"
$os
[1] "linux-gnu"
$system
[1] "x86_64, linux-gnu"
$status
[1] ""
$major
[1] "4"
$minor
[1] "3.2"
$year
[1] "2023"
$month
[1] "10"
$day
[1] "31"
$`svn rev`
[1] "85441"
$language
[1] "R"
$version.string
[1] "R version 4.3.2 (2023-10-31)"
$nickname
[1] "Eye Holes"

RStudio Server Version:
RStudio 2023.09.1+494 "Desert Sunflower" Release (cd7011dce393115d3a7c3db799dda4b1c7e88711, 2023-10-16) for RHEL 9
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36

PPM Repo:
https://packagemanager.posit.co/cran/__linux__/rhel9/latest

attached base packages:
[1] parallel  stats     graphics  grDevices datasets  utils     methods   base

other attached packages:
 [1] listenv_0.9.1        microbenchmark_1.5.0 dbplyr_2.4.0         duckplyr_0.4.1       readxl_1.4.3         fastDummies_1.7.3
 [7] glue_1.8.0           arrow_14.0.2.1       data.table_1.15.2    toolbox_0.1.1        janitor_2.2.0        lubridate_1.9.3
[13] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4          purrr_1.0.2          readr_2.1.5          tidyr_1.3.1
[19] tibble_3.2.1         ggplot2_3.5.0        tidyverse_2.0.0      duckdb_1.1.2         DBI_1.2.3            fs_1.6.3


Happy to provide additional information if it would be helpful.

Thank you in advance!
The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline <https://www.massgeneralbrigham.org/complianceline> .
Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail. 

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Dec 11 01:41:25 2024
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 10 Dec 2024 16:41:25 -0800
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
Message-ID: <D15380AC-2594-40CF-9175-DE0D09FECB32@dcn.davis.ca.us>

Maybe ask on the HPC list? [1]

A general tip... you may be running out of memory. If at all possible you need to make sure you extract the data subsets in the parent process, and limit the amount of environment data passed into the child processes. That is, instead of using an integer counter to index into the modtabs list, iterate over the list itself so that each process doesnt need a copy of the whole data set.

I know that fork lets you share memory blocks temporarily between the parent and child processes, but it is fragile... if you modify the data at all then the sharing stops for that memory block and you get an explosion of memory use.

[1] <https://stat.ethz.ch/mailman/listinfo/r-sig-hpc>

On December 10, 2024 4:20:28 PM PST, "Deramus, Thomas Patrick" <tderamus at mgb.org> wrote:
>Hi R users.
>
>Apologies for the lack of concrete examples because the dataset is large, and it being so I believe is the issue.
>
>I multiple, very large datasets for which I need to generate 0/1 absence/presence columns
>Some include over 200M rows, with two columns that need presence/absence columns based on the strings contained within them, as an example, one set has ~29k unique values and the other with ~15k unique values (no overlap across the two).
>
>Using a combination of custom functions:
>
>crewjanitormakeclean <- function(df,columns) {
>  df <- df |> mutate(across(columns, ~make_clean_names(.,  allow_dupes = TRUE)))
>  return(df)
>}
>
>mass_pivot_wider <- function(df,column,prefix) {
>  df <- df |> distinct() |> mutate(n = 1) |> pivot_wider(names_from = glue("{column}"), values_from = n, names_prefix = prefix, values_fill = list(n = 0))
>  return(df)
>}
>
>sum_group_function <- function(df) {
>  df <- df |> group_by(ID_Key) |> summarise(across(c(starts_with("column1_name_"),starts_with("column2_name_"),), ~ sum(.x, na.rm = TRUE))) |> ungroup()
>  return(df)
>}
>
>and splitting up the data into a list of 110k individual dataframes based on Key_ID
>
>temp <-
>  open_dataset(
>    sources = input_files,
>    format = 'csv',
>    unify_schema = TRUE,
>    col_types = schema(
>      "ID_Key" = string(),
>      "column1" = string(),
>      "column1" = string()
>    )
>  ) |> as_tibble()
>
>
>  keeptabs <- split(temp, temp$ID_Key)
>
>
>I used a multicore framework to distribute the `sum` functions across each Key_ID when a multicore argument is enabled.
>
>??????
>    if(isTRUE(multicore)){
>      output <- mclapply(1:length(modtabs), function(i) crewjanitormakeclean(modtabs[[i]],c("string_columns_2","string_columns_1")), mc.cores = numcores)
>      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_1","col_1_name_"), mc.cores = numcores)
>      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_2","col_2_name_"), mc.cores = numcores)
>    }else{
>      output <- lapply(1:length(modtabs), function(i) crewjanitormakeclean(modtabs[[i]],c("string_columns_2","string_columns_1")))
>      output <- lapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_1","col_1_name_"))
>      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_2","col_2_name_"))
>    }
>
>
>Moving every Key_ID to a single row and then row-binding the data while creating new columns for the differences across `Key_ID`s from the pivot using the following solution (78 upvotes at time of this email):
>https://stackoverflow.com/questions/3402371/combine-two-data-frames-by-rows-rbind-when-they-have-different-sets-of-columns
>
>  allNms <- unique(unlist(lapply(keeptabs, names)))
>
>  output <- do.call(rbind,
>                     c(lapply(keeptabs,
>                              function(x) data.frame(c((x), sapply(setdiff(allNms, names(x)),
>                                                                 function(y) NA))) |> as_tibble()),
>                       make.row.names=FALSE)) |> mutate(across(c(starts_with("column1_name_"), starts_with("column2_name_")), coalesce, 0))
>
>However, I have noticed that the jobs seem to "hang" after a while, with the initial 30 or so (numcores == 30 in the workflow, equal to the number of cores I reserved) at 100% of the requested CPU, and then several "zombie" processes occur and the cores just stop at 0% and never proceed, usually dying with a timeout or not all jobs running to completion failure to join of some kind.
>
>This happens in both base R and RStudio, and I haven't been able to figure out if it's something wrong with the code, the size of the data, or our architecture, but I would appreciate any suggestions as to what I might be able to do about this.
>
>Before they are suggested, I have also tried this same approach with foreach, snow, future, and furr packages, and base parallel with mc.apply seems to be the only thing that works for at least one dataset.
>
>In the event it has something to do with our architecture, here is what we are running on and our loaded packages:
>
>OS Information:
>NAME="Red Hat Enterprise Linux"
>VERSION="9.3 (Plow)"
>ID="rhel"
>ID_LIKE="fedora"
>VERSION_ID="9.3"
>PLATFORM_ID="platform:el9"
>PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
>ANSI_COLOR="0;31"
>LOGO="fedora-logo-icon"
>CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
>HOME_URL="https://www.redhat.com/"
>DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
>BUG_REPORT_URL="https://bugzilla.redhat.com/"
>REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
>REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
>REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
>REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
>Operating System: Red Hat Enterprise Linux 9.3 (Plow)
>     CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
>          Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
>    Architecture: x86-64
> Hardware Vendor: Dell Inc.
>  Hardware Model: PowerEdge R840
>Firmware Version: 2.15.1
>
>R Version:
>R.Version()
>$platform
>[1] "x86_64-pc-linux-gnu"
>$arch
>[1] "x86_64"
>$os
>[1] "linux-gnu"
>$system
>[1] "x86_64, linux-gnu"
>$status
>[1] ""
>$major
>[1] "4"
>$minor
>[1] "3.2"
>$year
>[1] "2023"
>$month
>[1] "10"
>$day
>[1] "31"
>$`svn rev`
>[1] "85441"
>$language
>[1] "R"
>$version.string
>[1] "R version 4.3.2 (2023-10-31)"
>$nickname
>[1] "Eye Holes"
>
>RStudio Server Version:
>RStudio 2023.09.1+494 "Desert Sunflower" Release (cd7011dce393115d3a7c3db799dda4b1c7e88711, 2023-10-16) for RHEL 9
>Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36
>
>PPM Repo:
>https://packagemanager.posit.co/cran/__linux__/rhel9/latest
>
>attached base packages:
>[1] parallel  stats     graphics  grDevices datasets  utils     methods   base
>
>other attached packages:
> [1] listenv_0.9.1        microbenchmark_1.5.0 dbplyr_2.4.0         duckplyr_0.4.1       readxl_1.4.3         fastDummies_1.7.3
> [7] glue_1.8.0           arrow_14.0.2.1       data.table_1.15.2    toolbox_0.1.1        janitor_2.2.0        lubridate_1.9.3
>[13] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4          purrr_1.0.2          readr_2.1.5          tidyr_1.3.1
>[19] tibble_3.2.1         ggplot2_3.5.0        tidyverse_2.0.0      duckdb_1.1.2         DBI_1.2.3            fs_1.6.3
>
>
>Happy to provide additional information if it would be helpful.
>
>Thank you in advance!
>The information in this e-mail is intended only for the...{{dropped:23}}


From g@@@powe|| @end|ng |rom protonm@||@com  Wed Dec 11 01:52:27 2024
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Wed, 11 Dec 2024 00:52:27 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
Message-ID: <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>

Hello Thomas,

Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.

Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead  a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.

It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R. 


An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.

Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -  and writes intermediate results to disk, then combines/mergesresults at the end.

Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.

If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.

Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.

> library(data.table)
> library(arrow)
>
> # Step A: Load data efficiently as data.table
> dt <- as.data.table(
>   open_dataset(
>    sources = input_files,
>    format = 'csv',
>    unify_schema = TRUE,
>    col_types = schema(
>      "ID_Key" = string(),
>      "column1" = string(),
>      "column2" = string()
>    )
>  ) |> 

>    collect()
> )
>
> # Step B: Clean names once
> # Assume `crewjanitormakeclean` essentially standardizes column names
> dt[, column1 := janitor::make_clean_names(column1, allow_dupes =  

> TRUE)]
> dt[, column2 := janitor::make_clean_names(column2, allow_dupes = 

>  TRUE)]
>
> # Step C: Create presence/absence indicators using data.table
> # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> # For large unique values, consider chunking if needed.
> out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate = 

> length, value.var = "column1")
> out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate = 

> length, value.var = "column2")
>
> # Step D: Merge the two wide tables by ID_Key
> # Fill missing columns with 0 using data.table on-the-fly operations
> all_cols <- unique(c(names(out1), names(out2)))
> out1_missing <- setdiff(all_cols, names(out1))
> out2_missing <- setdiff(all_cols, names(out2))
>
> # Add missing columns with 0
> for (col in out1_missing) out1[, (col) := 0]
> for (col in out2_missing) out2[, (col) := 0]
>
> # Ensure column order alignment if needed
> setcolorder(out1, all_cols)
> setcolorder(out2, all_cols)
>
> # Combine by ID_Key (since they share same columns now)
> final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
>
> # Step E: If needed, summarize across ID_Key to sum presence 

> indicators
> final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by = 

> ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
>
> # note that final_result should now contain summed presence/absence 

> (0/1) indicators.




Hope this helps!
gregg
somewhereinArizona
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 509 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20241211/2385b049/attachment.sig>

From @k@h@y_e4 @end|ng |rom hotm@||@com  Wed Dec 11 14:16:59 2024
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Wed, 11 Dec 2024 13:16:59 +0000
Subject: [R] SQL and R
Message-ID: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>

dear Members,
                            I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me to some online resources that shed some light on this? Finally, how does SQL complement R? Are both dependent?

THanking you,
Yours sincerely,
AKSHAY M KULKARNI

[https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>

	[[alternative HTML version deleted]]


From tder@mu@ @end|ng |rom mgb@org  Wed Dec 11 14:51:36 2024
From: tder@mu@ @end|ng |rom mgb@org (Deramus, Thomas Patrick)
Date: Wed, 11 Dec 2024 13:51:36 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
 <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>
Message-ID: <PH0PR04MB74932EAF56BA4F38915F2004CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>

About to try this implementation.

As a follow-up, this is the exact error:

Lost warning messages
Error: no more error handlers available (recursive errors?); invoking 'abort' restart
Execution halted
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)

________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Tuesday, December 10, 2024 7:52 PM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

Hello Thomas,

Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.

Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead  a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.

It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R.


An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.

Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -  and writes intermediate results to disk, then combines/mergesresults at the end.

Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.

If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.

Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.

> library(data.table)
> library(arrow)
>
> # Step A: Load data efficiently as data.table
> dt <- as.data.table(
>   open_dataset(
>    sources = input_files,
>    format = 'csv',
>    unify_schema = TRUE,
>    col_types = schema(
>      "ID_Key" = string(),
>      "column1" = string(),
>      "column2" = string()
>    )
>  ) |>

>    collect()
> )
>
> # Step B: Clean names once
> # Assume `crewjanitormakeclean` essentially standardizes column names
> dt[, column1 := janitor::make_clean_names(column1, allow_dupes =

> TRUE)]
> dt[, column2 := janitor::make_clean_names(column2, allow_dupes =

>  TRUE)]
>
> # Step C: Create presence/absence indicators using data.table
> # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> # For large unique values, consider chunking if needed.
> out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate =

> length, value.var = "column1")
> out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate =

> length, value.var = "column2")
>
> # Step D: Merge the two wide tables by ID_Key
> # Fill missing columns with 0 using data.table on-the-fly operations
> all_cols <- unique(c(names(out1), names(out2)))
> out1_missing <- setdiff(all_cols, names(out1))
> out2_missing <- setdiff(all_cols, names(out2))
>
> # Add missing columns with 0
> for (col in out1_missing) out1[, (col) := 0]
> for (col in out2_missing) out2[, (col) := 0]
>
> # Ensure column order alignment if needed
> setcolorder(out1, all_cols)
> setcolorder(out2, all_cols)
>
> # Combine by ID_Key (since they share same columns now)
> final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
>
> # Step E: If needed, summarize across ID_Key to sum presence

> indicators
> final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by =

> ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
>
> # note that final_result should now contain summed presence/absence

> (0/1) indicators.




Hope this helps!
gregg
somewhereinArizona
The information in this e-mail is intended only for the ...{{dropped:14}}


From bbo|ker @end|ng |rom gm@||@com  Wed Dec 11 16:33:43 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Wed, 11 Dec 2024 10:33:43 -0500
Subject: [R] SQL and R
In-Reply-To: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
Message-ID: <4d3fd4aa-4f2b-41df-ba31-afacbb4501ba@gmail.com>

   Others may know more than I do, but roughly:

   (1) SQL provides access to relational database management systems 
that are much more robust and handle large-scale data;
   (2) methods based on SQL will often handle data that are too large to 
fit in memory

   R complements SQL by providing a much  larger set of statistical 
tools. The typical workflow would be that you would use SQL queries to 
do the extraction, subsetting, and simple manipulation of data to reduce 
the data to a manageable size for analysis on a single machine, then use 
R to analyze it.

   See also various packages (arrow, dbplyr) that provide alternative 
big-data workflows.

   cheers
    Ben Bolker


On 12/11/24 08:16, akshay kulkarni wrote:
> dear Members,
>                              I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me to some online resources that shed some light on this? Finally, how does SQL complement R? Are both dependent?
> 
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
> 
> [https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
* E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From bgunter@4567 @end|ng |rom gm@||@com  Wed Dec 11 16:55:01 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 11 Dec 2024 07:55:01 -0800
Subject: [R] SQL and R
In-Reply-To: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAGxFJbREO0Rk=i-_XO9iYkYXS9z+ZU594nZ7w+ix84KXXS+=zA@mail.gmail.com>

Just a slight technical note -- Ben gave you a good answer already, imo.

The note is: R is Turing complete, which mean that *anything* any
language can do, R could be programmed to do also. The point is what
can be done well in R and what can be done (often much) better with
other tools, as Ben explained.

Cheers,
Bert

On Wed, Dec 11, 2024 at 5:17?AM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> dear Members,
>                             I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me to some online resources that shed some light on this? Finally, how does SQL complement R? Are both dependent?
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
> [https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Wed Dec 11 17:11:27 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 11 Dec 2024 11:11:27 -0500
Subject: [R] SQL and R
In-Reply-To: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
Message-ID: <008d01db4be7$5593f4d0$00bbde70$@gmail.com>

Akshay,

Your question has way too many answers.

SQL has a long history and early versions came long before R arrived on the
scene. There is a huge embedded base of hardware and software dedicated to
managing databases. It has some features that most R programs do not even
dream of doing. Besides easily handling massive amounts of data or sometimes
tweaking queries to possibly run more efficiently, there are all kinds of
issue of how to manage multiple people accessing and changing the data at
about the same time, or rolling the data back to an earlier checkpoint.

R came along later and, as Ben pointed out, adds all kinds of things SQL
does not have and likely does not need, or alternate ways to do things.

For many people now, the workload is to use a programming language, and R is
not the only one used, which has enhanced with packages or modules that
allow access in a fairly general way to one or many databases running
various versions of SQL. The programmer uses this API in many ways.

In some ways, it is just a way to tell the database what to do without much
other processing. You can ask to open a connection to the server, do a query
that gets translated to SQL (or you can provide the actual SQL)  and let the
remote (or local) machine do much of the work. For example, imagine a
database with terabytes of data and all you want is a few rows/columns that
meet your query. In R, you might have to open a collection of huge CSV files
and fill more memory than you have and do the query somehow. If the data is
remote, we are talking about a huge receiving of data. Using SQL divides the
work so you do parts here and parts there.

Why use a local MYSQL? Part of the answer is that you have a fairly
optimized and debugged system that does it well and lets the programmer
focus on the parts they need to add within R like complex analyses. Part is
portability, as you can later move the data outside your machine and with
minor changes, your program should still work. And, there are many other
scenarios such as wanting to gather data from different sources such as
connecting to multiple remote databases and getting filtered data and doing
an analysis across that data and perhaps updating them.

R used in ways like this provides lots of flexibility. But part of the
question is like asking why there are a hundred programming languages still
in use out there. Why do we need so many? In short, we don't necessarily
need all or even most of them but they are there because various people
developed them and used them and it is not trivial to get people to switch
and maybe abandon all the older software or try to rewrite it.

Having said that, I think a large fraction of R users have never had any
particular reason to learn SQL. Many have never used it directly or even
indirectly. I know someone who I have programmed for who calls some expert
to do a SQL query and save the results in CSV files and then works directly
in R on those files. I have pointed out to them that their life could be
even easier if they got a more focused dump of the SQL data with some of the
added processing done in SQL and then a smaller amount of data coming into
the R side.

I also note that languages like R and python can have parts that run fairly
slowly. Arguably, most versions of SQL have been tuned over decades ...


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of akshay kulkarni
Sent: Wednesday, December 11, 2024 8:17 AM
To: R help Mailing list <r-help at r-project.org>
Subject: [R] SQL and R

dear Members,
                            I have recently started studying SQL and MySQL.
My question is, what exactly is SQL used for? That is, whatever can be done
by SQL, like subsetting and filtering of data sets, can also be done by R.
What's, then, the advantage of SQL?  It is OK if you tag this question as
offtopic, but I could'nt find any info on the web. Can you please refer me
to some online resources that shed some light on this? Finally, how does SQL
complement R? Are both dependent?

THanking you,
Yours sincerely,
AKSHAY M KULKARNI

[https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-oran
ge-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=ema
il&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&ut
m_source=link&utm_campaign=sig-email&utm_content=webmail>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From g@@@powe|| @end|ng |rom protonm@||@com  Wed Dec 11 17:41:36 2024
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Wed, 11 Dec 2024 16:41:36 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <PH0PR04MB74932EAF56BA4F38915F2004CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
 <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>
 <PH0PR04MB74932EAF56BA4F38915F2004CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
Message-ID: <K-Jmkrf3aApJi7lie-0cVW3OvZ2F5szMiIsSq9HHQBSQZCZbjVuRuN3woxIEh5eCLx9JYk4B70PRvp_d_5zOGvZcWjG4Cm2M_DsMg38JdFk=@protonmail.com>

Thomas,
I'm curious - what OS are you running this on, and how much memory does the computer have??

Let me know if that code worked out as I hoped.

regards,
gregg


On Wednesday, December 11th, 2024 at 6:51 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:

> About to try this implementation.
> 

> As a follow-up, this is the exact error:
> 

> Lost warning messages
> Error: no more error handlers available (recursive errors?); invoking 'abort' restart
> Execution halted
> Error: cons memory exhausted (limit reached?)
> Error: cons memory exhausted (limit reached?)
> Error: cons memory exhausted (limit reached?)
> Error: cons memory exhausted (limit reached?)
> 

> 

> 

> From: Gregg Powell <g.a.powell at protonmail.com>
> Sent: Tuesday, December 10, 2024 7:52 PM
> To: Deramus, Thomas Patrick <tderamus at mgb.org>
> Cc: r-help at r-project.org <r-help at r-project.org>
> Subject: Re: [R] Cores hang when calling mcapply
> 

> Hello Thomas,
> 

> Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.
> 

> Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead? a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.
> 

> It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R.
> 

> 

> An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.
> 

> Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -? and writes intermediate results to disk, then combines/mergesresults at the end.
> 

> Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.
> 

> If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.
> 

> Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.
> 

> > library(data.table)
> > library(arrow)
> >
> > # Step A: Load data efficiently as data.table
> > dt <- as.data.table(
> >?? open_dataset(
> >??? sources = input_files,
> >??? format = 'csv',
> >??? unify_schema = TRUE,
> >??? col_types = schema(
> >????? "ID_Key" = string(),
> >????? "column1" = string(),
> >????? "column2" = string()
> >??? )
> >? ) |>
> 

> >??? collect()
> > )
> >
> > # Step B: Clean names once
> > # Assume `crewjanitormakeclean` essentially standardizes column names
> > dt[, column1 := janitor::make_clean_names(column1, allow_dupes =?
> 

> > TRUE)]
> > dt[, column2 := janitor::make_clean_names(column2, allow_dupes =
> 

> >? TRUE)]
> >
> > # Step C: Create presence/absence indicators using data.table
> > # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> > # For large unique values, consider chunking if needed.
> > out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate =
> 

> > length, value.var = "column1")
> > out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate =
> 

> > length, value.var = "column2")
> >
> > # Step D: Merge the two wide tables by ID_Key
> > # Fill missing columns with 0 using data.table on-the-fly operations
> > all_cols <- unique(c(names(out1), names(out2)))
> > out1_missing <- setdiff(all_cols, names(out1))
> > out2_missing <- setdiff(all_cols, names(out2))
> >
> > # Add missing columns with 0
> > for (col in out1_missing) out1[, (col) := 0]
> > for (col in out2_missing) out2[, (col) := 0]
> >
> > # Ensure column order alignment if needed
> > setcolorder(out1, all_cols)
> > setcolorder(out2, all_cols)
> >
> > # Combine by ID_Key (since they share same columns now)
> > final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
> >
> > # Step E: If needed, summarize across ID_Key to sum presence
> 

> > indicators
> > final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by =
> 

> > ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
> >
> > # note that final_result should now contain summed presence/absence
> 

> > (0/1) indicators.
> 

> 

> 

> 

> Hope this helps!
> gregg
> somewhereinArizona
> 

> The information in this e-mail is intended only for the person to whom it is addressed.? If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline .
> 

> 

> 

> Please note that this e-mail is not secure (encrypted).? If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.? Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 603 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20241211/6beda415/attachment.sig>

From tder@mu@ @end|ng |rom mgb@org  Wed Dec 11 17:49:37 2024
From: tder@mu@ @end|ng |rom mgb@org (Deramus, Thomas Patrick)
Date: Wed, 11 Dec 2024 16:49:37 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <K-Jmkrf3aApJi7lie-0cVW3OvZ2F5szMiIsSq9HHQBSQZCZbjVuRuN3woxIEh5eCLx9JYk4B70PRvp_d_5zOGvZcWjG4Cm2M_DsMg38JdFk=@protonmail.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
 <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>
 <PH0PR04MB74932EAF56BA4F38915F2004CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <K-Jmkrf3aApJi7lie-0cVW3OvZ2F5szMiIsSq9HHQBSQZCZbjVuRuN3woxIEh5eCLx9JYk4B70PRvp_d_5zOGvZcWjG4Cm2M_DsMg38JdFk=@protonmail.com>
Message-ID: <PH0PR04MB7493EF3DA4FE6201CB9C72D1CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>

It's Redhat Enterprise Linux 9

Specifically:
OS Information:
NAME="Red Hat Enterprise Linux"
VERSION="9.3 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
Operating System: Red Hat Enterprise Linux 9.3 (Plow)
     CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
          Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
    Architecture: x86-64
 Hardware Vendor: Dell Inc.
  Hardware Model: PowerEdge R840
Firmware Version: 2.15.1


Regarding RAM restrictions, here are the specs:
               total        used        free      shared  buff/cache   available
Mem:           753Gi        70Gi       600Gi       2.9Gi        89Gi       683Gi
Swap:          4.0Gi       2.5Gi       1.5Gi

It's a multi-user server so naturally things fluctuate.

Regarding possible CPU restrictions, here are the specs of our server:

    Thread(s) per core:  2
    Core(s) per socket:  20
    Socket(s):           4
    Stepping:            4
    CPU(s) scaling MHz:  50%
    CPU max MHz:         3700.0000
    CPU min MHz:         1000.0000



________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Wednesday, December 11, 2024 11:41 AM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

Thomas,
I'm curious - what OS are you running this on, and how much memory does the computer have?

Let me know if that code worked out as I hoped.

regards,
gregg

On Wednesday, December 11th, 2024 at 6:51 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
About to try this implementation.

As a follow-up, this is the exact error:

Lost warning messages
Error: no more error handlers available (recursive errors?); invoking 'abort' restart
Execution halted
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)

________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Tuesday, December 10, 2024 7:52 PM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

Hello Thomas,

Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.

Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead  a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.

It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R.


An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.

Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -  and writes intermediate results to disk, then combines/mergesresults at the end.

Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.

If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.

Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.

> library(data.table)
> library(arrow)
>
> # Step A: Load data efficiently as data.table
> dt <- as.data.table(
>   open_dataset(
>    sources = input_files,
>    format = 'csv',
>    unify_schema = TRUE,
>    col_types = schema(
>      "ID_Key" = string(),
>      "column1" = string(),
>      "column2" = string()
>    )
>  ) |>

>    collect()
> )
>
> # Step B: Clean names once
> # Assume `crewjanitormakeclean` essentially standardizes column names
> dt[, column1 := janitor::make_clean_names(column1, allow_dupes =

> TRUE)]
> dt[, column2 := janitor::make_clean_names(column2, allow_dupes =

>  TRUE)]
>
> # Step C: Create presence/absence indicators using data.table
> # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> # For large unique values, consider chunking if needed.
> out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate =

> length, value.var = "column1")
> out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate =

> length, value.var = "column2")
>
> # Step D: Merge the two wide tables by ID_Key
> # Fill missing columns with 0 using data.table on-the-fly operations
> all_cols <- unique(c(names(out1), names(out2)))
> out1_missing <- setdiff(all_cols, names(out1))
> out2_missing <- setdiff(all_cols, names(out2))
>
> # Add missing columns with 0
> for (col in out1_missing) out1[, (col) := 0]
> for (col in out2_missing) out2[, (col) := 0]
>
> # Ensure column order alignment if needed
> setcolorder(out1, all_cols)
> setcolorder(out2, all_cols)
>
> # Combine by ID_Key (since they share same columns now)
> final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
>
> # Step E: If needed, summarize across ID_Key to sum presence

> indicators
> final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by =

> ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
>
> # note that final_result should now contain summed presence/absence

> (0/1) indicators.




Hope this helps!
gregg
somewhereinArizona

The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline<https://secure-web.cisco.com/100Hi_g3yJTBAxxYPpHz7NQcwx9A06rN2U4Dh4wHDMTBLGJ5yiq8PszsdEMlRsD9C7ESGkM_88I-b3jy2QGu-x35cEfVZ6QUW9Uf8aQihVfQOceLc1DZr3AXcvDUKCpPFXgrBqSOYdHyh31yQD3--3ltk0pjgK_te1I0M6i_pIUEbdE334rYMuIxAhmwI48EHf8_k9wuSXxBrgiQc9D6Lsakb1w2RckPEmz4DmrbjWfLR4hx3ylUbDTf9UR_eWYxzvivxt6NpRfEN3T9WpUjWHHTNIdUphSKwmR8Sk8i4-KqcpJdPjHKC1185wd1Sr78X/https%3A%2F%2Fwww.massgeneralbrigham.org%2Fcomplianceline> .


Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.

The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline <https://www.massgeneralbrigham.org/complianceline> .
Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail. 

	[[alternative HTML version deleted]]


From pro|jcn@@h @end|ng |rom gm@||@com  Wed Dec 11 17:57:59 2024
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 11 Dec 2024 11:57:59 -0500
Subject: [R] SQL and R - tangential
In-Reply-To: <008d01db4be7$5593f4d0$00bbde70$@gmail.com>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <008d01db4be7$5593f4d0$00bbde70$@gmail.com>
Message-ID: <85bf9905-4180-446f-864b-5c7d488611fc@gmail.com>

My late friend Morven Gentleman, not long after he stepped down from being chair
of Computer Science at Waterloo, said that it seemed computer scientists had to create
a new computer language for every new problem they encountered.

If we could use least squares to measure this approximation, we'd likely be suspicious
of a terribly small error measure or overly high R^2.

JN

On 2024-12-11 11:11, avi.e.gross at gmail.com wrote:
> Akshay,
> 
> Your question has way too many answers.
> 
> SQL has a long history and early versions came long before R arrived on the
> scene. There is a huge embedded base of hardware and software dedicated to
> managing databases. It has some features that most R programs do not even
> dream of doing. Besides easily handling massive amounts of data or sometimes
> tweaking queries to possibly run more efficiently, there are all kinds of
> issue of how to manage multiple people accessing and changing the data at
> about the same time, or rolling the data back to an earlier checkpoint.
> 
> R came along later and, as Ben pointed out, adds all kinds of things SQL
> does not have and likely does not need, or alternate ways to do things.
> 
> For many people now, the workload is to use a programming language, and R is
> not the only one used, which has enhanced with packages or modules that
> allow access in a fairly general way to one or many databases running
> various versions of SQL. The programmer uses this API in many ways.
> 
> In some ways, it is just a way to tell the database what to do without much
> other processing. You can ask to open a connection to the server, do a query
> that gets translated to SQL (or you can provide the actual SQL)  and let the
> remote (or local) machine do much of the work. For example, imagine a
> database with terabytes of data and all you want is a few rows/columns that
> meet your query. In R, you might have to open a collection of huge CSV files
> and fill more memory than you have and do the query somehow. If the data is
> remote, we are talking about a huge receiving of data. Using SQL divides the
> work so you do parts here and parts there.
> 
> Why use a local MYSQL? Part of the answer is that you have a fairly
> optimized and debugged system that does it well and lets the programmer
> focus on the parts they need to add within R like complex analyses. Part is
> portability, as you can later move the data outside your machine and with
> minor changes, your program should still work. And, there are many other
> scenarios such as wanting to gather data from different sources such as
> connecting to multiple remote databases and getting filtered data and doing
> an analysis across that data and perhaps updating them.
> 
> R used in ways like this provides lots of flexibility. But part of the
> question is like asking why there are a hundred programming languages still
> in use out there. Why do we need so many? In short, we don't necessarily
> need all or even most of them but they are there because various people
> developed them and used them and it is not trivial to get people to switch
> and maybe abandon all the older software or try to rewrite it.
> 
> Having said that, I think a large fraction of R users have never had any
> particular reason to learn SQL. Many have never used it directly or even
> indirectly. I know someone who I have programmed for who calls some expert
> to do a SQL query and save the results in CSV files and then works directly
> in R on those files. I have pointed out to them that their life could be
> even easier if they got a more focused dump of the SQL data with some of the
> added processing done in SQL and then a smaller amount of data coming into
> the R side.
> 
> I also note that languages like R and python can have parts that run fairly
> slowly. Arguably, most versions of SQL have been tuned over decades ...
> 
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of akshay kulkarni
> Sent: Wednesday, December 11, 2024 8:17 AM
> To: R help Mailing list <r-help at r-project.org>
> Subject: [R] SQL and R
> 
> dear Members,
>                              I have recently started studying SQL and MySQL.
> My question is, what exactly is SQL used for? That is, whatever can be done
> by SQL, like subsetting and filtering of data sets, can also be done by R.
> What's, then, the advantage of SQL?  It is OK if you tag this question as
> offtopic, but I could'nt find any info on the web. Can you please refer me
> to some online resources that shed some light on this? Finally, how does SQL
> complement R? Are both dependent?
> 
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
> 
> [https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-oran
> ge-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=ema
> il&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&ut
> m_source=link&utm_campaign=sig-email&utm_content=webmail>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From g@@@powe|| @end|ng |rom protonm@||@com  Wed Dec 11 20:11:56 2024
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Wed, 11 Dec 2024 19:11:56 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <PH0PR04MB7493EF3DA4FE6201CB9C72D1CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
 <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>
 <PH0PR04MB74932EAF56BA4F38915F2004CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <K-Jmkrf3aApJi7lie-0cVW3OvZ2F5szMiIsSq9HHQBSQZCZbjVuRuN3woxIEh5eCLx9JYk4B70PRvp_d_5zOGvZcWjG4Cm2M_DsMg38JdFk=@protonmail.com>
 <PH0PR04MB7493EF3DA4FE6201CB9C72D1CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
Message-ID: <cAYww-6WFIIKmwBU2q6Wc4pgQskAHuHN1kL1GzYAwGd1exbdHijmhuRrwNMK45riNN8KKNTcjRF686SShKo5zmWXs-gvMR0jh5znWtyHoms=@protonmail.com>

How is the server configured to handle memory distribution for individual users. I see it has over 700GB of total system memory, but how much can be assigned it each individual user?

AAgain - just curious, and wondering how much memory was assigned to your instance when you were running R.

regards,
Gregg



On Wednesday, December 11th, 2024 at 9:49 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:

> It's Redhat Enterprise Linux 9
> 

> Specifically:
> OS Information:
> NAME="Red Hat Enterprise Linux"
> VERSION="9.3 (Plow)"
> ID="rhel"
> ID_LIKE="fedora"
> VERSION_ID="9.3"
> PLATFORM_ID="platform:el9"
> PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
> ANSI_COLOR="0;31"
> LOGO="fedora-logo-icon"
> CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
> HOME_URL="https://www.redhat.com/"
> DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
> BUG_REPORT_URL="https://bugzilla.redhat.com/"
> REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
> REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
> REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
> REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
> Operating System: Red Hat Enterprise Linux 9.3 (Plow)
> ? ? ?CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
> ? ? ? ? ? Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
> ? ? Architecture: x86-64
> ?Hardware Vendor: Dell Inc.
> ? Hardware Model: PowerEdge R840
> Firmware Version: 2.15.1
> 

> 

> Regarding RAM restrictions, here are the specs:
> ? ? ? ? ? ? ? ?total ? ? ? ?used ? ? ? ?free ? ? ?shared ?buff/cache ? available
> Mem: ? ? ? ? ? 753Gi?? ? ? ?70Gi ? ? ? 600Gi ? ? ? 2.9Gi ? ? ? ?89Gi ? ? ? 683Gi
> Swap: ? ? ? ? ?4.0Gi ? ? ? 2.5Gi ? ? ? 1.5Gi
> 

> It's a multi-user server so naturally things fluctuate.
> 

> Regarding possible CPU restrictions, here are the specs of our server:
> ? ? Thread(s) per core: ?2
> ? ? Core(s) per socket: ?20
> ? ? Socket(s): ? ? ? ? ? 4
> ? ? Stepping: ? ? ? ? ? ?4
> ? ? CPU(s) scaling MHz: ?50%
> ? ? CPU max MHz: ? ? ? ? 3700.0000
> ? ? CPU min MHz: ? ? ? ? 1000.0000
> 

> 

> 

> 

> 

> From: Gregg Powell <g.a.powell at protonmail.com>
> Sent: Wednesday, December 11, 2024 11:41 AM
> To: Deramus, Thomas Patrick <tderamus at mgb.org>
> Cc: r-help at r-project.org <r-help at r-project.org>
> Subject: Re: [R] Cores hang when calling mcapply
> 

> Thomas,
> I'm curious - what OS are you running this on, and how much memory does the computer have??
> 

> Let me know if that code worked out as I hoped.
> 

> regards,
> gregg
> 

> 

> On Wednesday, December 11th, 2024 at 6:51 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
> 

> > About to try this implementation.
> > 

> > As a follow-up, this is the exact error:
> > 

> > Lost warning messages
> > Error: no more error handlers available (recursive errors?); invoking 'abort' restart
> > Execution halted
> > Error: cons memory exhausted (limit reached?)
> > Error: cons memory exhausted (limit reached?)
> > Error: cons memory exhausted (limit reached?)
> > Error: cons memory exhausted (limit reached?)
> > 

> > 

> > 

> > From: Gregg Powell <g.a.powell at protonmail.com>
> > Sent: Tuesday, December 10, 2024 7:52 PM
> > To: Deramus, Thomas Patrick <tderamus at mgb.org>
> > Cc: r-help at r-project.org <r-help at r-project.org>
> > Subject: Re: [R] Cores hang when calling mcapply
> > 

> > Hello Thomas,
> > 

> > Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.
> > 

> > Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead? a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.
> > 

> > It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R.
> > 

> > 

> > An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.
> > 

> > Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -? and writes intermediate results to disk, then combines/mergesresults at the end.
> > 

> > Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.
> > 

> > If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.
> > 

> > Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.
> > 

> > > library(data.table)
> > > library(arrow)
> > >
> > > # Step A: Load data efficiently as data.table
> > > dt <- as.data.table(
> > >?? open_dataset(
> > >??? sources = input_files,
> > >??? format = 'csv',
> > >??? unify_schema = TRUE,
> > >??? col_types = schema(
> > >????? "ID_Key" = string(),
> > >????? "column1" = string(),
> > >????? "column2" = string()
> > >??? )
> > >? ) |>
> > 

> > >??? collect()
> > > )
> > >
> > > # Step B: Clean names once
> > > # Assume `crewjanitormakeclean` essentially standardizes column names
> > > dt[, column1 := janitor::make_clean_names(column1, allow_dupes =?
> > 

> > > TRUE)]
> > > dt[, column2 := janitor::make_clean_names(column2, allow_dupes =
> > 

> > >? TRUE)]
> > >
> > > # Step C: Create presence/absence indicators using data.table
> > > # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> > > # For large unique values, consider chunking if needed.
> > > out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate =
> > 

> > > length, value.var = "column1")
> > > out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate =
> > 

> > > length, value.var = "column2")
> > >
> > > # Step D: Merge the two wide tables by ID_Key
> > > # Fill missing columns with 0 using data.table on-the-fly operations
> > > all_cols <- unique(c(names(out1), names(out2)))
> > > out1_missing <- setdiff(all_cols, names(out1))
> > > out2_missing <- setdiff(all_cols, names(out2))
> > >
> > > # Add missing columns with 0
> > > for (col in out1_missing) out1[, (col) := 0]
> > > for (col in out2_missing) out2[, (col) := 0]
> > >
> > > # Ensure column order alignment if needed
> > > setcolorder(out1, all_cols)
> > > setcolorder(out2, all_cols)
> > >
> > > # Combine by ID_Key (since they share same columns now)
> > > final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
> > >
> > > # Step E: If needed, summarize across ID_Key to sum presence
> > 

> > > indicators
> > > final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by =
> > 

> > > ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
> > >
> > > # note that final_result should now contain summed presence/absence
> > 

> > > (0/1) indicators.
> > 

> > 

> > 

> > 

> > Hope this helps!
> > gregg
> > somewhereinArizona
> > 

> > The information in this e-mail is intended only for the person to whom it is addressed.? If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline .
> > 

> > 

> > 

> > Please note that this e-mail is not secure (encrypted).? If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.? Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.?
> 

> The information in this e-mail is intended only for the person to whom it is addressed.? If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline .
> 

> 

> 

> Please note that this e-mail is not secure (encrypted).? If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.? Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 603 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20241211/98c681de/attachment.sig>

From @vi@e@gross m@iii@g oii gm@ii@com  Wed Dec 11 21:03:05 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Wed, 11 Dec 2024 15:03:05 -0500
Subject: [R] SQL and R - tangential
In-Reply-To: <85bf9905-4180-446f-864b-5c7d488611fc@gmail.com>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <008d01db4be7$5593f4d0$00bbde70$@gmail.com>
 <85bf9905-4180-446f-864b-5c7d488611fc@gmail.com>
Message-ID: <00d901db4c07$b0f7bf30$12e73d90$@gmail.com>

Actually, tangentially, JC, I have a deep suspicion that many computer
languages are not written to solve problems. They are sometimes an effort by
someone to implement a new paradigm different than what others have tried
before or to protect the programmer from themselves or to demand extensive
rigor so the result almost proves itself and so on.

Only after the language is in place, might they look for a problem that
might be solved using it.

Kidding aside, some aspects of R may not have been as useful early on in
that the use of multi-dimensional arrays is not as common a need in many
areas of endeavor. But it has taken on more meaning in areas like AI. Other
languages that did not support some concepts like that well, have often had
to add it in.

The problem today in looking at computer languages is how ALIKE they are
becoming as features keep being grafted on to make each language have
features already found in another.

If you look at the question about SQL and R, you might note that although
base R had lots of functionality, some aspects you see in extensions like
dplyr are attempts to in some way mimic SQL or do things different or even
better. Consider the many clauses allowed in a SQL "select" statement and
map that onto a dplyr query in a pipeline with what seems like lots of
clauses. Base R before the pipe did not lend itself so easily to that.


-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of J C Nash
Sent: Wednesday, December 11, 2024 11:58 AM
To: r-help at r-project.org
Subject: Re: [R] SQL and R - tangential

My late friend Morven Gentleman, not long after he stepped down from being
chair
of Computer Science at Waterloo, said that it seemed computer scientists had
to create
a new computer language for every new problem they encountered.

If we could use least squares to measure this approximation, we'd likely be
suspicious
of a terribly small error measure or overly high R^2.

JN

On 2024-12-11 11:11, avi.e.gross at gmail.com wrote:
> Akshay,
> 
> Your question has way too many answers.
> 
> SQL has a long history and early versions came long before R arrived on
the
> scene. There is a huge embedded base of hardware and software dedicated to
> managing databases. It has some features that most R programs do not even
> dream of doing. Besides easily handling massive amounts of data or
sometimes
> tweaking queries to possibly run more efficiently, there are all kinds of
> issue of how to manage multiple people accessing and changing the data at
> about the same time, or rolling the data back to an earlier checkpoint.
> 
> R came along later and, as Ben pointed out, adds all kinds of things SQL
> does not have and likely does not need, or alternate ways to do things.
> 
> For many people now, the workload is to use a programming language, and R
is
> not the only one used, which has enhanced with packages or modules that
> allow access in a fairly general way to one or many databases running
> various versions of SQL. The programmer uses this API in many ways.
> 
> In some ways, it is just a way to tell the database what to do without
much
> other processing. You can ask to open a connection to the server, do a
query
> that gets translated to SQL (or you can provide the actual SQL)  and let
the
> remote (or local) machine do much of the work. For example, imagine a
> database with terabytes of data and all you want is a few rows/columns
that
> meet your query. In R, you might have to open a collection of huge CSV
files
> and fill more memory than you have and do the query somehow. If the data
is
> remote, we are talking about a huge receiving of data. Using SQL divides
the
> work so you do parts here and parts there.
> 
> Why use a local MYSQL? Part of the answer is that you have a fairly
> optimized and debugged system that does it well and lets the programmer
> focus on the parts they need to add within R like complex analyses. Part
is
> portability, as you can later move the data outside your machine and with
> minor changes, your program should still work. And, there are many other
> scenarios such as wanting to gather data from different sources such as
> connecting to multiple remote databases and getting filtered data and
doing
> an analysis across that data and perhaps updating them.
> 
> R used in ways like this provides lots of flexibility. But part of the
> question is like asking why there are a hundred programming languages
still
> in use out there. Why do we need so many? In short, we don't necessarily
> need all or even most of them but they are there because various people
> developed them and used them and it is not trivial to get people to switch
> and maybe abandon all the older software or try to rewrite it.
> 
> Having said that, I think a large fraction of R users have never had any
> particular reason to learn SQL. Many have never used it directly or even
> indirectly. I know someone who I have programmed for who calls some expert
> to do a SQL query and save the results in CSV files and then works
directly
> in R on those files. I have pointed out to them that their life could be
> even easier if they got a more focused dump of the SQL data with some of
the
> added processing done in SQL and then a smaller amount of data coming into
> the R side.
> 
> I also note that languages like R and python can have parts that run
fairly
> slowly. Arguably, most versions of SQL have been tuned over decades ...
> 
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of akshay kulkarni
> Sent: Wednesday, December 11, 2024 8:17 AM
> To: R help Mailing list <r-help at r-project.org>
> Subject: [R] SQL and R
> 
> dear Members,
>                              I have recently started studying SQL and
MySQL.
> My question is, what exactly is SQL used for? That is, whatever can be
done
> by SQL, like subsetting and filtering of data sets, can also be done by R.
> What's, then, the advantage of SQL?  It is OK if you tag this question as
> offtopic, but I could'nt find any info on the web. Can you please refer me
> to some online resources that shed some light on this? Finally, how does
SQL
> complement R? Are both dependent?
> 
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
> 
>
[https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-oran
>
ge-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=ema
> il&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>
Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&ut
> m_source=link&utm_campaign=sig-email&utm_content=webmail>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Wed Dec 11 21:31:10 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Wed, 11 Dec 2024 20:31:10 +0000
Subject: [R] aggregate produces  results in unexpected format
Message-ID: <DM6PR03MB5049620104DE1F51AD0B353DE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>

I am trying to use the aggregate function to run a function, catsbydat2, that produces the mean, minimum, maximum, and number of observations of the values in a dataframe, inJan2Test, by levels of the dataframe variable MyDay. The output should be in the form of a dataframe.

#my code:
# This function should process a data frame and return a data frame
# containing the mean, minimum, maximum, and number of observations
# in the data frame for each level of MyDay.
catsbyday2 <- function(df){
  # Create a matrix to hold the calculated values.
  xx <- matrix(nrow=1,ncol=4)
  # Give names to the columns.
  colnames(xx) <- c("Mean","min","max","Nobs")
  cat("This is the matrix that will hold the results\n",xx,"\n")

  # For each level of the indexing variable, MyDay, compute the
  # mean, minimum, maximum, and number of observations in the
  # dataframe passed to the function.
  xx[,1] <- mean(df)
  xx[,2] <- min(df)
  xx[,3] <- max(df)
  xx[,4] <- length(df)
  cat("These are the dimensions of the matrix in the function",dim(xx),"\n")
  print(xx)
  return(xx)
}

# Create data frame
inJan2Test <- data.frame(MyDay=rep(c(1,2,3),4),AveragePM2_5=c(10,20,30,
                                                              11,21,31,
                                                              12,22,32,
                                                              15,25,35))
str(inJan2Test)
cat("This is the data frame","\n")
inJan2Test

xx <- aggregate(inJan2Test[,"AveragePM2_5"],list(inJan2Test[,"MyDay"]),catsbyday2,simplify=FALSE)
xx
class(xx)
str(xx)
names(xx)

# Create a data frame in the format that I expect aggregate would return
examplar <- data.frame(mean=c(12,22,32),min=c(10,20,30),max=c(15,25,35),length=c(4,4,4))
examplar
str(examplar)


While the output is correct (the mean, mean etc. are correctly calculated), the format of the output is not what I want.  

(1) Although the returned object appears to be a data frame, it does appear to be a "normal" data frame. (see the output of  
(2) The column names I define in the function are not part of the data frame that is created.
(3) The returned values on each row are separated by commas. I would expect them to be separated by spaces.
(4) When I run str() on the output it appears that the output dataframe contains a list. 
> str(xx)
'data.frame':	3 obs. of  2 variables:
 $ Group.1: num  1 2 3
 $ x      :List of 3
  ..$ : num [1, 1:4] 12 10 15 4
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"
  ..$ : num [1, 1:4] 22 20 25 4
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"
  ..$ : num [1, 1:4] 32 30 35 4
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"

I want it to simply be a numeric dataframe:

mean  min max length
   12      10    15     4
   22      20    25     4
   32      30     35    4

which should return the following str

examplar <- data.frame(mean=c(12,22,32),min=c(10,20,30),max=c(15,25,35),length=c(4,4,4))
examplar
str(examplar)

'data.frame':	3 obs. of  4 variables:
 $ mean  : num  12 22 32
 $ min   : num  10 20 30
 $ max   : num  15 25 35
 $ length: num  4 4 4

John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?
PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




From e@ @end|ng |rom enr|co@chum@nn@net  Wed Dec 11 21:54:04 2024
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Wed, 11 Dec 2024 21:54:04 +0100
Subject: [R] aggregate produces  results in unexpected format
In-Reply-To: <DM6PR03MB5049620104DE1F51AD0B353DE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
 (John Sorkin's message of "Wed, 11 Dec 2024 20:31:10 +0000")
References: <DM6PR03MB5049620104DE1F51AD0B353DE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <87v7vq3pqr.fsf@enricoschumann.net>

On Wed, 11 Dec 2024, Sorkin, John writes:

> I am trying to use the aggregate function to run a function, catsbydat2, that produces the mean, minimum, maximum, and number of observations of the values in a dataframe, inJan2Test, by levels of the dataframe variable MyDay. The output should be in the form of a dataframe.
>
> #my code:
> # This function should process a data frame and return a data frame
> # containing the mean, minimum, maximum, and number of observations
> # in the data frame for each level of MyDay.
> catsbyday2 <- function(df){
>   # Create a matrix to hold the calculated values.
>   xx <- matrix(nrow=1,ncol=4)
>   # Give names to the columns.
>   colnames(xx) <- c("Mean","min","max","Nobs")
>   cat("This is the matrix that will hold the results\n",xx,"\n")
>
>   # For each level of the indexing variable, MyDay, compute the
>   # mean, minimum, maximum, and number of observations in the
>   # dataframe passed to the function.
>   xx[,1] <- mean(df)
>   xx[,2] <- min(df)
>   xx[,3] <- max(df)
>   xx[,4] <- length(df)
>   cat("These are the dimensions of the matrix in the function",dim(xx),"\n")
>   print(xx)
>   return(xx)
> }
>
> # Create data frame
> inJan2Test <- data.frame(MyDay=rep(c(1,2,3),4),AveragePM2_5=c(10,20,30,
>                                                               11,21,31,
>                                                               12,22,32,
>                                                               15,25,35))
> str(inJan2Test)
> cat("This is the data frame","\n")
> inJan2Test
>
> xx <- aggregate(inJan2Test[,"AveragePM2_5"],list(inJan2Test[,"MyDay"]),catsbyday2,simplify=FALSE)
> xx
> class(xx)
> str(xx)
> names(xx)
>
> # Create a data frame in the format that I expect aggregate would return
> examplar <- data.frame(mean=c(12,22,32),min=c(10,20,30),max=c(15,25,35),length=c(4,4,4))
> examplar
> str(examplar)
>
>
> While the output is correct (the mean, mean etc. are correctly calculated), the format of the output is not what I want.  
>
> (1) Although the returned object appears to be a data frame, it does appear to be a "normal" data frame. (see the output of  
> (2) The column names I define in the function are not part of the data frame that is created.
> (3) The returned values on each row are separated by commas. I would expect them to be separated by spaces.
> (4) When I run str() on the output it appears that the output dataframe contains a list. 
>> str(xx)
> 'data.frame':	3 obs. of  2 variables:
>  $ Group.1: num  1 2 3
>  $ x      :List of 3
>   ..$ : num [1, 1:4] 12 10 15 4
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : NULL
>   .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"
>   ..$ : num [1, 1:4] 22 20 25 4
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : NULL
>   .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"
>   ..$ : num [1, 1:4] 32 30 35 4
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : NULL
>   .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"
>
> I want it to simply be a numeric dataframe:
>
> mean  min max length
>    12      10    15     4
>    22      20    25     4
>    32      30     35    4
>
> which should return the following str
>
> examplar <- data.frame(mean=c(12,22,32),min=c(10,20,30),max=c(15,25,35),length=c(4,4,4))
> examplar
> str(examplar)
>
> 'data.frame':	3 obs. of  4 variables:
>  $ mean  : num  12 22 32
>  $ min   : num  10 20 30
>  $ max   : num  15 25 35
>  $ length: num  4 4 4

You'll no doubt get answers that use 'aggregate', but
for such calculations I find 'tapply' much easier/clearer:

    res <- tapply(inJan2Test$AveragePM2_5,  ## what to compute on
                  inJan2Test$MyDay,         ## what to group by
                  function(x) c(mean = mean(x),  ## what to do for each group
                                min = min(x),
                                max = max(x),                            
                                length = length(x)))

The result will be a list of vectors, which you can
bind together:

    do.call(rbind, res)
    ##   min max mean length
    ## 1  10  15   12      4
    ## 2  20  25   22      4
    ## 3  30  35   32      4


(Though the result is a numeric matrix. But that is
 only one 'as.data.frame' away from a data.frame, if it
 has to be one.)

kind regards
    Enrico




> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?
> PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382

-- 
Enrico Schumann
Lucerne, Switzerland
https://enricoschumann.net


From no@p@m @end|ng |rom ||@@e@NA  Wed Dec 11 22:23:12 2024
From: no@p@m @end|ng |rom ||@@e@NA (Eberhard W Lisse)
Date: Wed, 11 Dec 2024 23:23:12 +0200
Subject: [R] SQL and R
In-Reply-To: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
Message-ID: <vjcvs1$9tc$1@ciao.gmane.io>

Looks like an assignment question.

If so, do your homework yourself. Google is your friend

el


On 2024-12-11 15:16, akshay kulkarni wrote:
> dear Members, I have recently started studying SQL and MySQL. My
> question is, what exactly is SQL used for? That is, whatever can be
> done by SQL, like subsetting and filtering of data sets, can also be
> done by R. What's, then, the advantage of SQL?  It is OK if you tag
> this question as offtopic, but I could'nt find any info on the web.
> Can you please refer me to some online resources that shed some
> light on this? Finally, how does SQL complement R? Are both
> dependent?
> 
> THanking you, Yours sincerely, AKSHAY M KULKARNI


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Wed Dec 11 22:39:30 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Wed, 11 Dec 2024 21:39:30 +0000
Subject: [R] SQL and R
In-Reply-To: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
Message-ID: <DM6PR03MB5049662D073ECE775995168AE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>

Dear Askay,

I believe my grey hair allows me to help answer your question. SQL, and its progenitor SEQUEL, were developed specifically to manipulate relational databases. It was developed in the early 1970s (equivalent to the historical bronze age) when the concept of a relational database (see https://en.wikipedia.org/wiki/Relational_database) and Codd's 12-rules were being developed (see https://en.wikipedia.org/wiki/Codd%27s_12_rules)

At the time, the concept of a relation database and a programming language dedicated to manipulating them was revolutionary. The concept was clearly needed, important, and well used; a commercial version of SQL, Oracle, made Larry Ellison more than a quarter billionaire.

S, one of the progenitors of R, was developed later. In 1975 by John Chambers, Rick Becker, Trevor Hastie, and William Cleveland (all of whom, I believe worked at Bell Labs) developed S as a general programming language. It was NOT developed specifically for the manipulation of relational databases. S had modest success in academia. S-Plus, a commercial version of R was developed fairly recently in 1988 by a company Statistical Sciences. The founder of Statistical Sciences was R. Douglas Marin who was a professor of statistics at the University of Washington, Seattle.

S was also the progenitor of R. R was developed by Ross Ihaka and Robert Gentlemen in 1993, faculty members of the University of Auckland. Given the ubiquity of R in academia, it is clear that S, much like SQL has been extraordinarily successful.

John



John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




________________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
Sent: Wednesday, December 11, 2024 8:16 AM
To: R help Mailing  list
Subject: [R] SQL and R

dear Members,
                            I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me to some online resources that shed some light on this? Finally, how does SQL complement R? Are both dependent?

THanking you,
Yours sincerely,
AKSHAY M KULKARNI

[https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.http://www.avast.com/<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Dec 11 23:17:10 2024
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 11 Dec 2024 22:17:10 +0000
Subject: [R] aggregate produces results in unexpected format
In-Reply-To: <DM6PR03MB5049620104DE1F51AD0B353DE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049620104DE1F51AD0B353DE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <1c5ce988-078c-4fac-a386-3a27d1f2234b@sapo.pt>

?s 20:31 de 11/12/2024, Sorkin, John escreveu:
> I am trying to use the aggregate function to run a function, catsbydat2, that produces the mean, minimum, maximum, and number of observations of the values in a dataframe, inJan2Test, by levels of the dataframe variable MyDay. The output should be in the form of a dataframe.
> 
> #my code:
> # This function should process a data frame and return a data frame
> # containing the mean, minimum, maximum, and number of observations
> # in the data frame for each level of MyDay.
> catsbyday2 <- function(df){
>    # Create a matrix to hold the calculated values.
>    xx <- matrix(nrow=1,ncol=4)
>    # Give names to the columns.
>    colnames(xx) <- c("Mean","min","max","Nobs")
>    cat("This is the matrix that will hold the results\n",xx,"\n")
> 
>    # For each level of the indexing variable, MyDay, compute the
>    # mean, minimum, maximum, and number of observations in the
>    # dataframe passed to the function.
>    xx[,1] <- mean(df)
>    xx[,2] <- min(df)
>    xx[,3] <- max(df)
>    xx[,4] <- length(df)
>    cat("These are the dimensions of the matrix in the function",dim(xx),"\n")
>    print(xx)
>    return(xx)
> }
> 
> # Create data frame
> inJan2Test <- data.frame(MyDay=rep(c(1,2,3),4),AveragePM2_5=c(10,20,30,
>                                                                11,21,31,
>                                                                12,22,32,
>                                                                15,25,35))
> str(inJan2Test)
> cat("This is the data frame","\n")
> inJan2Test
> 
> xx <- aggregate(inJan2Test[,"AveragePM2_5"],list(inJan2Test[,"MyDay"]),catsbyday2,simplify=FALSE)
> xx
> class(xx)
> str(xx)
> names(xx)
> 
> # Create a data frame in the format that I expect aggregate would return
> examplar <- data.frame(mean=c(12,22,32),min=c(10,20,30),max=c(15,25,35),length=c(4,4,4))
> examplar
> str(examplar)
> 
> 
> While the output is correct (the mean, mean etc. are correctly calculated), the format of the output is not what I want.
> 
> (1) Although the returned object appears to be a data frame, it does appear to be a "normal" data frame. (see the output of
> (2) The column names I define in the function are not part of the data frame that is created.
> (3) The returned values on each row are separated by commas. I would expect them to be separated by spaces.
> (4) When I run str() on the output it appears that the output dataframe contains a list.
>> str(xx)
> 'data.frame':	3 obs. of  2 variables:
>   $ Group.1: num  1 2 3
>   $ x      :List of 3
>    ..$ : num [1, 1:4] 12 10 15 4
>    .. ..- attr(*, "dimnames")=List of 2
>    .. .. ..$ : NULL
>    .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"
>    ..$ : num [1, 1:4] 22 20 25 4
>    .. ..- attr(*, "dimnames")=List of 2
>    .. .. ..$ : NULL
>    .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"
>    ..$ : num [1, 1:4] 32 30 35 4
>    .. ..- attr(*, "dimnames")=List of 2
>    .. .. ..$ : NULL
>    .. .. ..$ : chr [1:4] "Mean" "min" "max" "Nobs"
> 
> I want it to simply be a numeric dataframe:
> 
> mean  min max length
>     12      10    15     4
>     22      20    25     4
>     32      30     35    4
> 
> which should return the following str
> 
> examplar <- data.frame(mean=c(12,22,32),min=c(10,20,30),max=c(15,25,35),length=c(4,4,4))
> examplar
> str(examplar)
> 
> 'data.frame':	3 obs. of  4 variables:
>   $ mean  : num  12 22 32
>   $ min   : num  10 20 30
>   $ max   : num  15 25 35
>   $ length: num  4 4 4
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
> 
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,

The code can be made much simpler. The summary statistics function is a 
one-liner, it just computes and returns a named vector.
But the statistics are now in a matrix, the last column is a matrix 
column and if you print the result, agg, you will see the name 
AveragePM2_5 with suffixes "mean", "min", "max" and "nobs" appended.

You can solve this by removing that column from the result and cbind it 
with the rest of the agg data.frame.




catsbyday2 <- function(x) {
   c(mean = mean(x), min = min(x), max = max(x), nobs = length(x))
}

agg <- aggregate(AveragePM2_5 ~ MyDay, inJan2Test, FUN = catsbyday2)

# The 2nd column is a matrix 3x4
str(agg)
#> 'data.frame':    3 obs. of  2 variables:
#>  $ MyDay       : num  1 2 3
#>  $ AveragePM2_5: num [1:3, 1:4] 12 22 32 10 20 30 15 25 35 4 ...
#>   ..- attr(*, "dimnames")=List of 2
#>   .. ..$ : NULL
#>   .. ..$ : chr [1:4] "mean" "min" "max" "nobs"

# this solves it, the method cbind.data.frame is
# called since the 1st argument is a df
cbind(agg[-ncol(agg)], agg[[ncol(agg)]])
#>   MyDay mean min max nobs
#> 1     1   12  10  15    4
#> 2     2   22  20  25    4
#> 3     3   32  30  35    4


# a data.frame
agg[-ncol(agg)]
#>   MyDay
#> 1     1
#> 2     2
#> 3     3

# the matrix column
agg[[ncol(agg)]]
#>      mean min max nobs
#> [1,]   12  10  15    4
#> [2,]   22  20  25    4
#> [3,]   32  30  35    4



Hope this helps,

Rui Barradas


-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From po|c1410 @end|ng |rom gm@||@com  Wed Dec 11 23:52:10 2024
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Wed, 11 Dec 2024 22:52:10 +0000
Subject: [R] SQL and R
In-Reply-To: <DM6PR03MB5049662D073ECE775995168AE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <DM6PR03MB5049662D073ECE775995168AE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <CA+etgP=f45fr73hqmD=SjMSd=dzPM_GO8cao4G1RgHFT7iJssQ@mail.gmail.com>

And to answer the dependency question.

Neither is dependent on the other. But both can be complimentary.

If you consider that SQL*may* be a route to accessing your data (if it's in
a database).

And R *may* be a route to analysis of the data.

If the data is in a CSV file, Excel file, API etc.  you don't need SQL. IF
it is in a database, you might extract it to CSV etc. or you might directly
access it from R

If ALL you want is some simple number counts you can do that in SQL. NO
NEED FOR R. So for instance you could have a database for a warehouse.
There could be three tables in that database (you'll have far more).

Orders

Customers

Products

Products lists all your products - with a product code.

Customers lists all the customers with a customer code

And orders lists the customer code, date and product ordered.

SQL will let you create a "view" of that data that shows the customer
address, and product name.

SQL can also tell you how many of a product were ordered by postal area in
a month even though the data is not organised like that.

R can do that analysis too. But it could do far more statistical analysis
and say run a stats test to see if male customers are more likely to buy
beer than female.  I don't think that's possible in SQL.

Sometimes you could just pull the whole database into R and analyse it. But
if you imagine that database was actually every sale made in Walmart in the
last 10 years, the database is huge.  It would be better to extract the
beer sales and males and females only which is hopefully smaller to
analyse...

On Wed, 11 Dec 2024, 21:39 Sorkin, John, <jsorkin at som.umaryland.edu> wrote:

> Dear Askay,
>
> I believe my grey hair allows me to help answer your question. SQL, and
> its progenitor SEQUEL, were developed specifically to manipulate relational
> databases. It was developed in the early 1970s (equivalent to the
> historical bronze age) when the concept of a relational database (see
> https://en.wikipedia.org/wiki/Relational_database) and Codd's 12-rules
> were being developed (see https://en.wikipedia.org/wiki/Codd%27s_12_rules)
>
> At the time, the concept of a relation database and a programming language
> dedicated to manipulating them was revolutionary. The concept was clearly
> needed, important, and well used; a commercial version of SQL, Oracle, made
> Larry Ellison more than a quarter billionaire.
>
> S, one of the progenitors of R, was developed later. In 1975 by John
> Chambers, Rick Becker, Trevor Hastie, and William Cleveland (all of whom, I
> believe worked at Bell Labs) developed S as a general programming language.
> It was NOT developed specifically for the manipulation of relational
> databases. S had modest success in academia. S-Plus, a commercial version
> of R was developed fairly recently in 1988 by a company Statistical
> Sciences. The founder of Statistical Sciences was R. Douglas Marin who was
> a professor of statistics at the University of Washington, Seattle.
>
> S was also the progenitor of R. R was developed by Ross Ihaka and Robert
> Gentlemen in 1993, faculty members of the University of Auckland. Given the
> ubiquity of R in academia, it is clear that S, much like SQL has been
> extraordinarily successful.
>
> John
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical
> Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of
> Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> <https://www.google.com/maps/search/10+North+Greene+Street?entry=gmail&source=g>
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
>
> ________________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <
> akshay_e4 at hotmail.com>
> Sent: Wednesday, December 11, 2024 8:16 AM
> To: R help Mailing  list
> Subject: [R] SQL and R
>
> dear Members,
>                             I have recently started studying SQL and
> MySQL. My question is, what exactly is SQL used for? That is, whatever can
> be done by SQL, like subsetting and filtering of data sets, can also be
> done by R. What's, then, the advantage of SQL?  It is OK if you tag this
> question as offtopic, but I could'nt find any info on the web. Can you
> please refer me to some online resources that shed some light on this?
> Finally, how does SQL complement R? Are both dependent?
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
> [
> https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif
> ]<
> https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> Virus-free.http://www.avast.com/<
> https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Dec 11 16:39:57 2024
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 11 Dec 2024 07:39:57 -0800
Subject: [R] SQL and R
In-Reply-To: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
Message-ID: <68723FFC-0E72-4424-AFE3-E484A7174CB8@dcn.davis.ca.us>

Some people prefer SQL syntax. Also, SQL implementations are generally intrinsically linked with persistent disk storage, so it works straightforwardly with data sets larger than RAM. Finally, most implementations support shared access to the data from multiple clients.

A long time ago in a computer with little RAM I used to use SQL a lot. But I have not really used it for many years now. Your needs may vary.

On December 11, 2024 5:16:59 AM PST, akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>dear Members,
>                            I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me to some online resources that shed some light on this? Finally, how does SQL complement R? Are both dependent?
>
>THanking you,
>Yours sincerely,
>AKSHAY M KULKARNI
>
>[https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Dec 12 10:11:39 2024
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 12 Dec 2024 09:11:39 +0000
Subject: [R] SQL and R
In-Reply-To: <CAGxFJbREO0Rk=i-_XO9iYkYXS9z+ZU594nZ7w+ix84KXXS+=zA@mail.gmail.com>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAGxFJbREO0Rk=i-_XO9iYkYXS9z+ZU594nZ7w+ix84KXXS+=zA@mail.gmail.com>
Message-ID: <PU4P216MB15680489D63F4F07A7A9393EC83F2@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Bert,
                   THanks a lot
________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Wednesday, December 11, 2024 9:25 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] SQL and R

Just a slight technical note -- Ben gave you a good answer already, imo.

The note is: R is Turing complete, which mean that *anything* any
language can do, R could be programmed to do also. The point is what
can be done well in R and what can be done (often much) better with
other tools, as Ben explained.

Cheers,
Bert

On Wed, Dec 11, 2024 at 5:17?AM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> dear Members,
>                             I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me to some online resources that shed some light on this? Finally, how does SQL complement R? Are both dependent?
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
> [https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Dec 12 10:17:15 2024
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 12 Dec 2024 09:17:15 +0000
Subject: [R] SQL and R - tangential
In-Reply-To: <85bf9905-4180-446f-864b-5c7d488611fc@gmail.com>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <008d01db4be7$5593f4d0$00bbde70$@gmail.com>
 <85bf9905-4180-446f-864b-5c7d488611fc@gmail.com>
Message-ID: <PU4P216MB1568E6FB0E53FB1B51601284C83F2@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear JC,
                 THanks .....

THanking you,
Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of J C Nash <profjcnash at gmail.com>
Sent: Wednesday, December 11, 2024 10:27 PM
To: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] SQL and R - tangential

My late friend Morven Gentleman, not long after he stepped down from being chair
of Computer Science at Waterloo, said that it seemed computer scientists had to create
a new computer language for every new problem they encountered.

If we could use least squares to measure this approximation, we'd likely be suspicious
of a terribly small error measure or overly high R^2.

JN

On 2024-12-11 11:11, avi.e.gross at gmail.com wrote:
> Akshay,
>
> Your question has way too many answers.
>
> SQL has a long history and early versions came long before R arrived on the
> scene. There is a huge embedded base of hardware and software dedicated to
> managing databases. It has some features that most R programs do not even
> dream of doing. Besides easily handling massive amounts of data or sometimes
> tweaking queries to possibly run more efficiently, there are all kinds of
> issue of how to manage multiple people accessing and changing the data at
> about the same time, or rolling the data back to an earlier checkpoint.
>
> R came along later and, as Ben pointed out, adds all kinds of things SQL
> does not have and likely does not need, or alternate ways to do things.
>
> For many people now, the workload is to use a programming language, and R is
> not the only one used, which has enhanced with packages or modules that
> allow access in a fairly general way to one or many databases running
> various versions of SQL. The programmer uses this API in many ways.
>
> In some ways, it is just a way to tell the database what to do without much
> other processing. You can ask to open a connection to the server, do a query
> that gets translated to SQL (or you can provide the actual SQL)  and let the
> remote (or local) machine do much of the work. For example, imagine a
> database with terabytes of data and all you want is a few rows/columns that
> meet your query. In R, you might have to open a collection of huge CSV files
> and fill more memory than you have and do the query somehow. If the data is
> remote, we are talking about a huge receiving of data. Using SQL divides the
> work so you do parts here and parts there.
>
> Why use a local MYSQL? Part of the answer is that you have a fairly
> optimized and debugged system that does it well and lets the programmer
> focus on the parts they need to add within R like complex analyses. Part is
> portability, as you can later move the data outside your machine and with
> minor changes, your program should still work. And, there are many other
> scenarios such as wanting to gather data from different sources such as
> connecting to multiple remote databases and getting filtered data and doing
> an analysis across that data and perhaps updating them.
>
> R used in ways like this provides lots of flexibility. But part of the
> question is like asking why there are a hundred programming languages still
> in use out there. Why do we need so many? In short, we don't necessarily
> need all or even most of them but they are there because various people
> developed them and used them and it is not trivial to get people to switch
> and maybe abandon all the older software or try to rewrite it.
>
> Having said that, I think a large fraction of R users have never had any
> particular reason to learn SQL. Many have never used it directly or even
> indirectly. I know someone who I have programmed for who calls some expert
> to do a SQL query and save the results in CSV files and then works directly
> in R on those files. I have pointed out to them that their life could be
> even easier if they got a more focused dump of the SQL data with some of the
> added processing done in SQL and then a smaller amount of data coming into
> the R side.
>
> I also note that languages like R and python can have parts that run fairly
> slowly. Arguably, most versions of SQL have been tuned over decades ...
>
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of akshay kulkarni
> Sent: Wednesday, December 11, 2024 8:17 AM
> To: R help Mailing list <r-help at r-project.org>
> Subject: [R] SQL and R
>
> dear Members,
>                              I have recently started studying SQL and MySQL.
> My question is, what exactly is SQL used for? That is, whatever can be done
> by SQL, like subsetting and filtering of data sets, can also be done by R.
> What's, then, the advantage of SQL?  It is OK if you tag this question as
> offtopic, but I could'nt find any info on the web. Can you please refer me

SQL
> complement R? Are both dependent?
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
> [https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-oran
> ge-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=ema
> il&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&ut
> m_source=link&utm_campaign=sig-email&utm_content=webmail>
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Dec 12 10:20:19 2024
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 12 Dec 2024 09:20:19 +0000
Subject: [R] SQL and R
In-Reply-To: <DM6PR03MB5049662D073ECE775995168AE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <DM6PR03MB5049662D073ECE775995168AE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <PU4P216MB156823812BEDA6B85C0C08CBC83F2@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear John,
                    THanks a lot.

THanking you,
Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: Sorkin, John <jsorkin at som.umaryland.edu>
Sent: Thursday, December 12, 2024 3:09 AM
To: akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>; avi.e.gross at gmail.com <avi.e.gross at gmail.com>; Ben Bolker <bbolker at gmail.com>
Subject: Re: SQL and R

Dear Askay,

I believe my grey hair allows me to help answer your question. SQL, and its progenitor SEQUEL, were developed specifically to manipulate relational databases. It was developed in the early 1970s (equivalent to the historical bronze age) when the concept of a relational database (see https://en.wikipedia.org/wiki/Relational_database) and Codd's 12-rules were being developed (see https://en.wikipedia.org/wiki/Codd%27s_12_rules)

At the time, the concept of a relation database and a programming language dedicated to manipulating them was revolutionary. The concept was clearly needed, important, and well used; a commercial version of SQL, Oracle, made Larry Ellison more than a quarter billionaire.

S, one of the progenitors of R, was developed later. In 1975 by John Chambers, Rick Becker, Trevor Hastie, and William Cleveland (all of whom, I believe worked at Bell Labs) developed S as a general programming language. It was NOT developed specifically for the manipulation of relational databases. S had modest success in academia. S-Plus, a commercial version of R was developed fairly recently in 1988 by a company Statistical Sciences. The founder of Statistical Sciences was R. Douglas Marin who was a professor of statistics at the University of Washington, Seattle.

S was also the progenitor of R. R was developed by Ross Ihaka and Robert Gentlemen in 1993, faculty members of the University of Auckland. Given the ubiquity of R in academia, it is clear that S, much like SQL has been extraordinarily successful.

John



John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




________________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of akshay kulkarni <akshay_e4 at hotmail.com>
Sent: Wednesday, December 11, 2024 8:16 AM
To: R help Mailing  list
Subject: [R] SQL and R

dear Members,
                            I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me
QL complement R? Are both dependent?

THanking you,
Yours sincerely,
AKSHAY M KULKARNI

[https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.http://www.avast.com/<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Dec 12 10:24:17 2024
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 12 Dec 2024 09:24:17 +0000
Subject: [R] SQL and R
In-Reply-To: <CA+etgP=f45fr73hqmD=SjMSd=dzPM_GO8cao4G1RgHFT7iJssQ@mail.gmail.com>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <DM6PR03MB5049662D073ECE775995168AE23E2@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CA+etgP=f45fr73hqmD=SjMSd=dzPM_GO8cao4G1RgHFT7iJssQ@mail.gmail.com>
Message-ID: <PU4P216MB15686566716F770356FDC2C0C83F2@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Calum,
                      THanks for the simple and practical answer.

Thanking you,
Yours sincerely
AKSHAY M KULKARNI
________________________________
From: CALUM POLWART <polc1410 at gmail.com>
Sent: Thursday, December 12, 2024 4:22 AM
To: Sorkin, John <jsorkin at som.umaryland.edu>
Cc: akshay kulkarni <akshay_e4 at hotmail.com>; R help Mailing list <r-help at r-project.org>; avi.e.gross at gmail.com <avi.e.gross at gmail.com>; Ben Bolker <bbolker at gmail.com>
Subject: Re: [R] SQL and R


And to answer the dependency question.

Neither is dependent on the other. But both can be complimentary.

If you consider that SQL*may* be a route to accessing your data (if it's in a database).

And R *may* be a route to analysis of the data.

If the data is in a CSV file, Excel file, API etc.  you don't need SQL. IF it is in a database, you might extract it to CSV etc. or you might directly access it from R

If ALL you want is some simple number counts you can do that in SQL. NO NEED FOR R. So for instance you could have a database for a warehouse. There could be three tables in that database (you'll have far more).

Orders

Customers

Products

Products lists all your products - with a product code.

Customers lists all the customers with a customer code

And orders lists the customer code, date and product ordered.

SQL will let you create a "view" of that data that shows the customer address, and product name.

SQL can also tell you how many of a product were ordered by postal area in a month even though the data is not organised like that.

R can do that analysis too. But it could do far more statistical analysis and say run a stats test to see if male customers are more likely to buy beer than female.  I don't think that's possible in SQL.

Sometimes you could just pull the whole database into R and analyse it. But if you imagine that database was actually every sale made in Walmart in the last 10 years, the database is huge.  It would be better to extract the beer sales and males and females only which is hopefully smaller to analyse...

On Wed, 11 Dec 2024, 21:39 Sorkin, John, <jsorkin at som.umaryland.edu<mailto:jsorkin at som.umaryland.edu>> wrote:
Dear Askay,

I believe my grey hair allows me to help answer your question. SQL, and its progenitor SEQUEL, were developed specifically to manipulate relational databases. It was developed in the early 1970s (equivalent to the historical bronze age) when the concept of a relational database (see https://en.wikipedia.org/wiki/Relational_database) and Codd's 12-rules were being developed (see https://en.wikipedia.org/wiki/Codd%27s_12_rules)

At the time, the concept of a relation database and a programming language dedicated to manipulating them was revolutionary. The concept was clearly needed, important, and well used; a commercial version of SQL, Oracle, made Larry Ellison more than a quarter billionaire.

S, one of the progenitors of R, was developed later. In 1975 by John Chambers, Rick Becker, Trevor Hastie, and William Cleveland (all of whom, I believe worked at Bell Labs) developed S as a general programming language. It was NOT developed specifically for the manipulation of relational databases. S had modest success in academia. S-Plus, a commercial version of R was developed fairly recently in 1988 by a company Statistical Sciences. The founder of Statistical Sciences was R. Douglas Marin who was a professor of statistics at the University of Washington, Seattle.

S was also the progenitor of R. R was developed by Ross Ihaka and Robert Gentlemen in 1993, faculty members of the University of Auckland. Given the ubiquity of R in academia, it is clear that S, much like SQL has been extraordinarily successful.

John



John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street<https://www.google.com/maps/search/10+North+Greene+Street?entry=gmail&source=g>
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




________________________________________
From: R-help <r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>> on behalf of akshay kulkarni <akshay_e4 at hotmail.com<mailto:akshay_e4 at hotmail.com>>
Sent: Wednesday, December 11, 2024 8:16 AM
To: R help Mailing  list
Subject: [R] SQL and R

dear Members,
                            I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me
QL complement R? Are both dependent?

THanking you,
Yours sincerely,
AKSHAY M KULKARNI

[https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.http://www.avast.com/<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Dec 12 10:27:16 2024
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 12 Dec 2024 09:27:16 +0000
Subject: [R] SQL and R
In-Reply-To: <vjcvs1$9tc$1@ciao.gmane.io>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <vjcvs1$9tc$1@ciao.gmane.io>
Message-ID: <PU4P216MB15688DBD44B2D201D00F291DC83F2@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Eberhard,
                            Yes I did much research in google afetr the posting in this list. THe answers here and my research has made me understand SQL vis- a-vis  R pretty well....thanks agian,

THanking you,
Yours sincerely,
AKSHAY M KULKARNI

________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of Eberhard W Lisse <nospam at lisse.NA>
Sent: Thursday, December 12, 2024 2:53 AM
To: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] SQL and R

Looks like an assignment question.

If so, do your homework yourself. Google is your friend

el


On 2024-12-11 15:16, akshay kulkarni wrote:
> dear Members, I have recently started studying SQL and MySQL. My
> question is, what exactly is SQL used for? That is, whatever can be
> done by SQL, like subsetting and filtering of data sets, can also be
> done by R. What's, then, the advantage of SQL?  It is OK if you tag
> this question as offtopic, but I could'nt find any info on the web.

> light on this? Finally, how does SQL complement R? Are both
> dependent?
>
> THanking you, Yours sincerely, AKSHAY M KULKARNI

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ggrothend|eck @end|ng |rom gm@||@com  Thu Dec 12 15:09:14 2024
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Thu, 12 Dec 2024 09:09:14 -0500
Subject: [R] SQL and R
In-Reply-To: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
Message-ID: <CAP01uRnh8uCr+o3DeqedEfDW-=EPKOivpBNZf9dUReAJs-TeTQ@mail.gmail.com>

The advantages of SQL are that
- it can be used from many languages so if you know SQL you can easily
move that part of your code to python, say,
and visa versa
- it is widely used
- it can handle data stored outside of R and possibly otherwise too large for R
- some SQL databases support multiple concurrent users
- depending on the database it may be used to communicate the data to others
- one big difference is that SQL is declarative rather than
imperative.  You tell it what you want rather
than how to get it and the optimizer will figure out how to create a
low level query to get the result.
For example, in the simple case below it will simply scan the BOD
table but in more complex cases
there may be many approaches and it will try to find a good one:

library(sqldf)
sqldf("explain query plan select * from BOD")
##   id parent notused   detail
## 1  2      0       0 SCAN BOD

sqldf("explain select * from BOD")
##    addr       opcode p1 p2 p3   p4 p5 comment
## 1     0         Init  0 10  0 <NA>  0      NA
## 2     1     OpenRead  0  2  0    2  0      NA
## 3     2       Rewind  0  9  0 <NA>  0      NA
## 4     3       Column  0  0  1 <NA>  0      NA
## 5     4 RealAffinity  1  0  0 <NA>  0      NA
## 6     5       Column  0  1  2 <NA>  0      NA
## 7     6 RealAffinity  2  0  0 <NA>  0      NA
## 8     7    ResultRow  1  2  0 <NA>  0      NA
## 9     8         Next  0  3  0 <NA>  1      NA
## 10    9         Halt  0  0  0 <NA>  0      NA
## 11   10  Transaction  0  0  1    0  1      NA
## 12   11         Goto  0  1  0 <NA>  0      NA

On Wed, Dec 11, 2024 at 8:17?AM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> dear Members,
>                             I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me to some online resources that shed some light on this? Finally, how does SQL complement R? Are both dependent?
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
> [https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From @k@h@y_e4 @end|ng |rom hotm@||@com  Thu Dec 12 15:40:29 2024
From: @k@h@y_e4 @end|ng |rom hotm@||@com (akshay kulkarni)
Date: Thu, 12 Dec 2024 14:40:29 +0000
Subject: [R] SQL and R
In-Reply-To: <CAP01uRnh8uCr+o3DeqedEfDW-=EPKOivpBNZf9dUReAJs-TeTQ@mail.gmail.com>
References: <SL2P216MB1561751390461734F553EB2CC83E2@SL2P216MB1561.KORP216.PROD.OUTLOOK.COM>
 <CAP01uRnh8uCr+o3DeqedEfDW-=EPKOivpBNZf9dUReAJs-TeTQ@mail.gmail.com>
Message-ID: <PU4P216MB1568FD258672FB2B7EE9485AC83F2@PU4P216MB1568.KORP216.PROD.OUTLOOK.COM>

Dear Gabor,
                      THanks a lot....

THanking you,
Yours sincerely,
AKSHAY M KULKARNI
________________________________
From: Gabor Grothendieck <ggrothendieck at gmail.com>
Sent: Thursday, December 12, 2024 7:39 PM
To: akshay kulkarni <akshay_e4 at hotmail.com>
Cc: R help Mailing list <r-help at r-project.org>
Subject: Re: [R] SQL and R

The advantages of SQL are that
- it can be used from many languages so if you know SQL you can easily
move that part of your code to python, say,
and visa versa
- it is widely used
- it can handle data stored outside of R and possibly otherwise too large for R
- some SQL databases support multiple concurrent users
- depending on the database it may be used to communicate the data to others
- one big difference is that SQL is declarative rather than
imperative.  You tell it what you want rather
than how to get it and the optimizer will figure out how to create a
low level query to get the result.
For example, in the simple case below it will simply scan the BOD
table but in more complex cases
there may be many approaches and it will try to find a good one:

library(sqldf)
sqldf("explain query plan select * from BOD")
##   id parent notused   detail
## 1  2      0       0 SCAN BOD

sqldf("explain select * from BOD")
##    addr       opcode p1 p2 p3   p4 p5 comment
## 1     0         Init  0 10  0 <NA>  0      NA
## 2     1     OpenRead  0  2  0    2  0      NA
## 3     2       Rewind  0  9  0 <NA>  0      NA
## 4     3       Column  0  0  1 <NA>  0      NA
## 5     4 RealAffinity  1  0  0 <NA>  0      NA
## 6     5       Column  0  1  2 <NA>  0      NA
## 7     6 RealAffinity  2  0  0 <NA>  0      NA
## 8     7    ResultRow  1  2  0 <NA>  0      NA
## 9     8         Next  0  3  0 <NA>  1      NA
## 10    9         Halt  0  0  0 <NA>  0      NA
## 11   10  Transaction  0  0  1    0  1      NA
## 12   11         Goto  0  1  0 <NA>  0      NA

On Wed, Dec 11, 2024 at 8:17?AM akshay kulkarni <akshay_e4 at hotmail.com> wrote:
>
> dear Members,
>                             I have recently started studying SQL and MySQL. My question is, what exactly is SQL used for? That is, whatever can be done by SQL, like subsetting and filtering of data sets, can also be done by R. What's, then, the advantage of SQL?  It is OK if you tag this question as offtopic, but I could'nt find any info on the web. Can you please refer me to some online resources that shed some light on this? Finally, how does SQL complement R? Are both dependent?
>
> THanking you,
> Yours sincerely,
> AKSHAY M KULKARNI
>
> [https://s-install.avcdn.net/ipm/preview/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>  Virus-free.www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



--
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From tder@mu@ @end|ng |rom mgb@org  Thu Dec 12 16:54:06 2024
From: tder@mu@ @end|ng |rom mgb@org (Deramus, Thomas Patrick)
Date: Thu, 12 Dec 2024 15:54:06 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <cAYww-6WFIIKmwBU2q6Wc4pgQskAHuHN1kL1GzYAwGd1exbdHijmhuRrwNMK45riNN8KKNTcjRF686SShKo5zmWXs-gvMR0jh5znWtyHoms=@protonmail.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
 <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>
 <PH0PR04MB74932EAF56BA4F38915F2004CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <K-Jmkrf3aApJi7lie-0cVW3OvZ2F5szMiIsSq9HHQBSQZCZbjVuRuN3woxIEh5eCLx9JYk4B70PRvp_d_5zOGvZcWjG4Cm2M_DsMg38JdFk=@protonmail.com>
 <PH0PR04MB7493EF3DA4FE6201CB9C72D1CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <cAYww-6WFIIKmwBU2q6Wc4pgQskAHuHN1kL1GzYAwGd1exbdHijmhuRrwNMK45riNN8KKNTcjRF686SShKo5zmWXs-gvMR0jh5znWtyHoms=@protonmail.com>
Message-ID: <PH0PR04MB749370B3AB16FDB914F2407BCB3F2@PH0PR04MB7493.namprd04.prod.outlook.com>

Hi Gregg.

Just wanted to follow up on the solution you proposed.

I had to make some adjustments to get exactly what I wanted, but it works, and takes about 15 minutes on our server configuration:

    temp <-
??????open_dataset(
????????????sources = input_files,
????????????format = 'csv',
????????????unify_schema = TRUE,
????????????col_types = schema(
????????????"ID_Key" = string(),
????????????"column1" = string(),
????????????"column2" = string()
????????????)
??????) |> as_tibble()
??????
??keeptabs <- split(temp, temp$ID_Key)

    if(isTRUE(multicore)){
      keeptabs <- mclapply(1:length(keeptabs), function(i) crewjanitormakeclean(keeptabs[[i]],c("column1","column2")), mc.cores = numcores)
    }else{
      keeptabs <- lapply(1:length(keeptabs), function(i) crewjanitormakeclean(keeptabs[[i]],c("column1","column2")))
    }

    keeptabs <- bind_rows(keeptabs)

    out1 <- dcast(keeptabs, ID_Key ~ column1, fun.aggregate = length, value.var = "column1")
    out2 <- dcast(keeptabs, ID_Key ~ column2, fun.aggregate = length, value.var = "column2")
    out1 <- setDT(out1 |> rename_with(~ paste0("column1_name_", .x, recycle0 = TRUE), -ID_Key))
    out2 <- setDT(out1 |> rename_with(~ paste0("column2_name_", .x, recycle0 = TRUE), -ID_Key))
    all_cols <- unique(c(names(out1), names(out2)))
    out1_missing <- setdiff(all_cols, names(out1))
    out2_missing <- setdiff(all_cols, names(out2))
    for (col in out1_missing) out1[, (col) := 0]
    for (col in out2_missing) out2[, (col) := 0]
    setcolorder(out1, all_cols)
    setcolorder(out2, all_cols)
    final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
    final_result <- as_tibble(final_dt[, lapply(.SD, sum, na.rm = TRUE), by = ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")])

Worth noting however:


  *
I unfortunately had to keep the multicore parameters for the janitor package to use make_clean_names() because it just took to long  to run it on the full dataframe, but deploying data.table CONSIDERABLY reduces the time and memory overhead to the point where it only takes about 15 minutes to run one of my smaller dataframes.
  *
I keep getting the following warning message:
     *
The dcast generic in data.table has been passed a tbl_df and will attempt to redirect to the relevant reshape2 method; please note that reshape2 is superseded and is no longer actively developed, and this redirection is now deprecated. Please do this redirection yourself like reshape2::dcast(keeptabs). In the next version, this warning will become an error.
     *
So a new call may be needed to keep this approach from failing if we update our version of data.table in the future
  *
The initial approach was replacing all the KeyID variables with what was basically row numbers and that would have made merging back to the main key document an issue so I changed the rbind funciton to keep this from happening.

Thank you for all your help on this!
-Thomas DeRamus


________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Wednesday, December 11, 2024 2:11 PM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

How is the server configured to handle memory distribution for individual users. I see it has over 700GB of total system memory, but how much can be assigned it each individual user?

AAgain - just curious, and wondering how much memory was assigned to your instance when you were running R.

regards,
Gregg


On Wednesday, December 11th, 2024 at 9:49 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
It's Redhat Enterprise Linux 9

Specifically:
OS Information:
NAME="Red Hat Enterprise Linux"
VERSION="9.3 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
Operating System: Red Hat Enterprise Linux 9.3 (Plow)
     CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
          Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
    Architecture: x86-64
 Hardware Vendor: Dell Inc.
  Hardware Model: PowerEdge R840
Firmware Version: 2.15.1


Regarding RAM restrictions, here are the specs:
               total        used        free      shared  buff/cache   available
Mem:           753Gi        70Gi       600Gi       2.9Gi        89Gi       683Gi
Swap:          4.0Gi       2.5Gi       1.5Gi

It's a multi-user server so naturally things fluctuate.

Regarding possible CPU restrictions, here are the specs of our server:

    Thread(s) per core:  2
    Core(s) per socket:  20
    Socket(s):           4
    Stepping:            4
    CPU(s) scaling MHz:  50%
    CPU max MHz:         3700.0000
    CPU min MHz:         1000.0000



________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Wednesday, December 11, 2024 11:41 AM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

Thomas,
I'm curious - what OS are you running this on, and how much memory does the computer have?

Let me know if that code worked out as I hoped.

regards,
gregg

On Wednesday, December 11th, 2024 at 6:51 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
About to try this implementation.

As a follow-up, this is the exact error:

Lost warning messages
Error: no more error handlers available (recursive errors?); invoking 'abort' restart
Execution halted
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)

________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Tuesday, December 10, 2024 7:52 PM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

Hello Thomas,

Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.

Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead  a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.

It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R.


An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.

Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -  and writes intermediate results to disk, then combines/mergesresults at the end.

Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.

If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.

Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.

> library(data.table)
> library(arrow)
>
> # Step A: Load data efficiently as data.table
> dt <- as.data.table(
>   open_dataset(
>    sources = input_files,
>    format = 'csv',
>    unify_schema = TRUE,
>    col_types = schema(
>      "ID_Key" = string(),
>      "column1" = string(),
>      "column2" = string()
>    )
>  ) |>

>    collect()
> )
>
> # Step B: Clean names once
> # Assume `crewjanitormakeclean` essentially standardizes column names
> dt[, column1 := janitor::make_clean_names(column1, allow_dupes =

> TRUE)]
> dt[, column2 := janitor::make_clean_names(column2, allow_dupes =

>  TRUE)]
>
> # Step C: Create presence/absence indicators using data.table
> # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> # For large unique values, consider chunking if needed.
> out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate =

> length, value.var = "column1")
> out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate =

> length, value.var = "column2")
>
> # Step D: Merge the two wide tables by ID_Key
> # Fill missing columns with 0 using data.table on-the-fly operations
> all_cols <- unique(c(names(out1), names(out2)))
> out1_missing <- setdiff(all_cols, names(out1))
> out2_missing <- setdiff(all_cols, names(out2))
>
> # Add missing columns with 0
> for (col in out1_missing) out1[, (col) := 0]
> for (col in out2_missing) out2[, (col) := 0]
>
> # Ensure column order alignment if needed
> setcolorder(out1, all_cols)
> setcolorder(out2, all_cols)
>
> # Combine by ID_Key (since they share same columns now)
> final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
>
> # Step E: If needed, summarize across ID_Key to sum presence

> indicators
> final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by =

> ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
>
> # note that final_result should now contain summed presence/absence

> (0/1) indicators.




Hope this helps!
gregg
somewhereinArizona

The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline<https://secure-web.cisco.com/100Hi_g3yJTBAxxYPpHz7NQcwx9A06rN2U4Dh4wHDMTBLGJ5yiq8PszsdEMlRsD9C7ESGkM_88I-b3jy2QGu-x35cEfVZ6QUW9Uf8aQihVfQOceLc1DZr3AXcvDUKCpPFXgrBqSOYdHyh31yQD3--3ltk0pjgK_te1I0M6i_pIUEbdE334rYMuIxAhmwI48EHf8_k9wuSXxBrgiQc9D6Lsakb1w2RckPEmz4DmrbjWfLR4hx3ylUbDTf9UR_eWYxzvivxt6NpRfEN3T9WpUjWHHTNIdUphSKwmR8Sk8i4-KqcpJdPjHKC1185wd1Sr78X/https%3A%2F%2Fwww.massgeneralbrigham.org%2Fcomplianceline> .


Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.


The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline<https://secure-web.cisco.com/1zMX7N7YTozoGwIAk3PWA50iqx1YK6HX1taKVKQz0HqImamktAnbE_Lhw9DITz4CNk4YtcTHiaVjRHBbsNJA6nMZFdvqvYOAa3K6EjvpWhTCeZuriog-AChP7n3TbQ2Bhfkgoqdr0SXJuHbtoWHgBArTDyMzA5e1muIeHZy1FHSNCrKM-Le9MboKJd65PrLi2bLIy_jGP8xQRMsS85zV5Pq6FGo9AR5qNHn0s8h9XiHbj0sIORoRALetWpNBqvavE37HV7v08WLPtq_BGRRIwYK15jIaMIiNtwPM06KI0VQh5nqvY8VNGV7Awx4Ou4Ns-/https%3A%2F%2Fwww.massgeneralbrigham.org%2Fcomplianceline> .


Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.

The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline <https://www.massgeneralbrigham.org/complianceline> .
Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail. 

	[[alternative HTML version deleted]]


From w||||@mwdun|@p @end|ng |rom gm@||@com  Thu Dec 12 17:42:14 2024
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Thu, 12 Dec 2024 08:42:14 -0800
Subject: [R] [off-topic] crossword
Message-ID: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>

The New York Times crossword this morning had the clue (51 down, 5 letters)
"Writes in C or R, say".

-Bill

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Thu Dec 12 17:55:52 2024
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Thu, 12 Dec 2024 09:55:52 -0700
Subject: [R] [off-topic] crossword
In-Reply-To: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
Message-ID: <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>

RULES!


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap <williamwdunlap at gmail.com>
wrote:

> The New York Times crossword this morning had the clue (51 down, 5 letters)
> "Writes in C or R, say".
>
> -Bill
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From g@@@powe|| @end|ng |rom protonm@||@com  Thu Dec 12 18:30:55 2024
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Thu, 12 Dec 2024 17:30:55 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <PH0PR04MB749370B3AB16FDB914F2407BCB3F2@PH0PR04MB7493.namprd04.prod.outlook.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
 <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>
 <PH0PR04MB74932EAF56BA4F38915F2004CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <K-Jmkrf3aApJi7lie-0cVW3OvZ2F5szMiIsSq9HHQBSQZCZbjVuRuN3woxIEh5eCLx9JYk4B70PRvp_d_5zOGvZcWjG4Cm2M_DsMg38JdFk=@protonmail.com>
 <PH0PR04MB7493EF3DA4FE6201CB9C72D1CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <cAYww-6WFIIKmwBU2q6Wc4pgQskAHuHN1kL1GzYAwGd1exbdHijmhuRrwNMK45riNN8KKNTcjRF686SShKo5zmWXs-gvMR0jh5znWtyHoms=@protonmail.com>
 <PH0PR04MB749370B3AB16FDB914F2407BCB3F2@PH0PR04MB7493.namprd04.prod.outlook.com>
Message-ID: <dWDmtt1iT8wBvk9zOzrgJI23hwIy5Y3eYKU2dK0Wj-mAYCylf63rs7Cp8yUPXD03p5X7D_jRXz-9JSHk5h_nq2Ut1yCdcwhV4b_vJcffc_g=@protonmail.com>

Hi Thomas,

Glad to hear the suggestion helped, and that switching to a `data.table` approach reduced the processing time and memory overhead?15 minutes for one of the smaller datasets is certainly better! Sounds like the adjustments you devised, especially keeping the multicore approach for `make_clean_names()` and ensuring that `ID_Key` values remain intact, were the missing components you needed to fit it into your workflow.

I believe the warning message regarding `dcast()` occurs because `keeptabs` is a `tbl_df` from the tidyverse rather than a base `data.frame` or `data.table`. The `data.table` implementation of `dcast()` expects a `data.table` or `data.frame`. When it detects a `tbl_df`, it tries to redirect to `reshape2::dcast()`, but since that appears to be deprecated, it will fail in future versions at some point.
To avoid this, consider converting?`keeptabs` into a `data.table` directly before calling `dcast()`. For example:

    >?setDT(keeptabs)
    > out1 <- dcast(keeptabs, ID_Key ~ column1, fun.aggregate = length, value.var = "column1")
    > out2 <- dcast(keeptabs, ID_Key ~ column2, fun.aggregate = length, value.var = "column2")
    


If?`keeptabs` is a `data.table` at the time of calling `dcast()`, this ensures the `data.table` method of `dcast()` is used and should eliminate the warning message maintaining compatibility with? future updates of the `data.table` package.

Also,?If you must retain?`keeptabs` as a tibble for other parts of the workflow, you might consider converting it right before the `dcast()` call... do the wide conversionand then proceed. This should keep the workflow stable and ensure that you won?t run into issues down the road - when the redirect to `reshape2::dcast()` becomes unsupported.

Regarding the note on multicore usage for `make_clean_names()`, your alternative approach -splitting the data and applying `make_clean_names()` in parallel before merging - was a good idea. I think relying on `data.table` for the main pivot operations clearly made the process more memory and time-efficient. Remaining parallelization overhead is likely a necessary step given the size of the datasets you're working with and the complexity of the initial cleaning.

Anyway, happy the suggestion moved you closer to the results you were looking for. I don't often respond on r-help often, mostly just a lurker.

All the best,

gregg
Sierra Vista, Arizona


On Thursday, December 12th, 2024 at 8:54 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:

> Hi Gregg.
> 

> Just wanted to follow up on the solution you proposed.
> 

> I had to make some adjustments to get exactly what I wanted, but it works, and takes about 15 minutes on our server configuration:
> 

> ? ? temp <-
> ??????open_dataset(
> ????????????sources = input_files,
> ????????????format = 'csv',
> ????????????unify_schema = TRUE,
> ????????????col_types = schema(
> ????????????"ID_Key" = string(),
> ????????????"column1" = string(),
> ????????????"column2" = string()
> ????????????)
> ??????) |> as_tibble()
> 

> ??keeptabs <- split(temp, temp$ID_Key)
> 

> ? ? if(isTRUE(multicore)){
> ? ? ? keeptabs <- mclapply(1:length(keeptabs), function(i) crewjanitormakeclean(keeptabs[[i]],c("column1","column2")), mc.cores = numcores)
> ? ? }else{
> ? ? ? keeptabs <- lapply(1:length(keeptabs), function(i) crewjanitormakeclean(keeptabs[[i]],c("column1","column2")))
> ? ? }
> 

> ? ? keeptabs <- bind_rows(keeptabs)
> 

> ? ? out1 <- dcast(keeptabs, ID_Key ~ column1, fun.aggregate = length, value.var = "column1")
> ? ? out2 <- dcast(keeptabs, ID_Key ~ column2, fun.aggregate = length, value.var = "column2")
> ? ? out1 <- setDT(out1 |> rename_with(~ paste0("column1_name_", .x, recycle0 = TRUE), -ID_Key))
> ? ? out2 <- setDT(out1 |> rename_with(~ paste0("column2_name_", .x, recycle0 = TRUE), -ID_Key))
> ? ? all_cols <- unique(c(names(out1), names(out2)))
> ? ? out1_missing <- setdiff(all_cols, names(out1))
> ? ? out2_missing <- setdiff(all_cols, names(out2))
> ? ? for (col in out1_missing) out1[, (col) := 0]
> ? ? for (col in out2_missing) out2[, (col) := 0]
> ? ? setcolorder(out1, all_cols)
> ? ? setcolorder(out2, all_cols)
> ? ? final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
> ? ? final_result <- as_tibble(final_dt[, lapply(.SD, sum, na.rm = TRUE), by = ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")])
> 

> 

> Worth noting however:
> 

> 

> -   I unfortunately had to keep the `multicore`?parameters for the `janitor`?package to use `make_clean_names()`?`because it just took to long? to run it on the full dataframe, but` deploying data.table CONSIDERABLY?reduces the time and memory overhead to the point where it only takes about 15 minutes to run one of my smaller dataframes.
>     

> -   I keep getting the following warning message:
>     

> 

> -   The dcast generic in data.table has been passed a tbl_df and will attempt to redirect to the relevant reshape2 method; please note that reshape2 is superseded and is no longer actively developed, and this redirection is now deprecated. Please do this redirection yourself like reshape2::dcast(keeptabs). In the next version, this warning will become an error.
>     

> -   So a new call may be needed to keep this approach from failing if we update our version of `data.table`?in the future
>     

> 

> -   The initial approach was replacing all the `KeyID`?variables with what was basically row numbers and that would have made merging back to the main key document an issue so I changed the rbind funciton to keep this from happening.
>     

> 

> 

> Thank you for all your help on this!
> -Thomas DeRamus
> 

> 

> 

> 

> From:?Gregg Powell <g.a.powell at protonmail.com>
> Sent:?Wednesday, December 11, 2024 2:11 PM
> To:?Deramus, Thomas Patrick <tderamus at mgb.org>
> Cc:?r-help at r-project.org <r-help at r-project.org>
> Subject:?Re: [R] Cores hang when calling mcapply
> 

> How is the server configured to handle memory distribution for individual users. I see it has over 700GB of total system memory, but how much can be assigned it each individual user?
> 

> AAgain - just curious, and wondering how much memory was assigned to your instance when you were running R.
> 

> regards,
> Gregg
> 

> 

> On Wednesday, December 11th, 2024 at 9:49 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
> 

> > It's Redhat Enterprise Linux 9
> > 

> > Specifically:
> > OS Information:
> > NAME="Red Hat Enterprise Linux"
> > VERSION="9.3 (Plow)"
> > ID="rhel"
> > ID_LIKE="fedora"
> > VERSION_ID="9.3"
> > PLATFORM_ID="platform:el9"
> > PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
> > ANSI_COLOR="0;31"
> > LOGO="fedora-logo-icon"
> > CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
> > HOME_URL="https://www.redhat.com/"
> > DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
> > BUG_REPORT_URL="https://bugzilla.redhat.com/"
> > REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
> > REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
> > REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
> > REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
> > Operating System: Red Hat Enterprise Linux 9.3 (Plow)
> > ? ? ?CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
> > ? ? ? ? ? Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
> > ? ? Architecture: x86-64
> > ?Hardware Vendor: Dell Inc.
> > ? Hardware Model: PowerEdge R840
> > Firmware Version: 2.15.1
> > 

> > 

> > Regarding RAM restrictions, here are the specs:
> > ? ? ? ? ? ? ? ?total ? ? ? ?used ? ? ? ?free ? ? ?shared ?buff/cache ? available
> > Mem: ? ? ? ? ? 753Gi?? ? ? ?70Gi ? ? ? 600Gi ? ? ? 2.9Gi ? ? ? ?89Gi ? ? ? 683Gi
> > Swap: ? ? ? ? ?4.0Gi ? ? ? 2.5Gi ? ? ? 1.5Gi
> > 

> > It's a multi-user server so naturally things fluctuate.
> > 

> > Regarding possible CPU restrictions, here are the specs of our server:
> > ? ? Thread(s) per core: ?2
> > ? ? Core(s) per socket: ?20
> > ? ? Socket(s): ? ? ? ? ? 4
> > ? ? Stepping: ? ? ? ? ? ?4
> > ? ? CPU(s) scaling MHz: ?50%
> > ? ? CPU max MHz: ? ? ? ? 3700.0000
> > ? ? CPU min MHz: ? ? ? ? 1000.0000
> > 

> > 

> > 

> > 

> > 

> > From:?Gregg Powell <g.a.powell at protonmail.com>
> > Sent:?Wednesday, December 11, 2024 11:41 AM
> > To:?Deramus, Thomas Patrick <tderamus at mgb.org>
> > Cc:?r-help at r-project.org <r-help at r-project.org>
> > Subject:?Re: [R] Cores hang when calling mcapply
> > 

> > Thomas,
> > I'm curious - what OS are you running this on, and how much memory does the computer have??
> > 

> > Let me know if that code worked out as I hoped.
> > 

> > regards,
> > gregg
> > 

> > On Wednesday, December 11th, 2024 at 6:51 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
> > 

> > > About to try this implementation.
> > > 

> > > As a follow-up, this is the exact error:
> > > 

> > > Lost warning messages
> > > Error: no more error handlers available (recursive errors?); invoking 'abort' restart
> > > Execution halted
> > > Error: cons memory exhausted (limit reached?)
> > > Error: cons memory exhausted (limit reached?)
> > > Error: cons memory exhausted (limit reached?)
> > > Error: cons memory exhausted (limit reached?)
> > > 

> > > 

> > > 

> > > From:?Gregg Powell <g.a.powell at protonmail.com>
> > > Sent:?Tuesday, December 10, 2024 7:52 PM
> > > To:?Deramus, Thomas Patrick <tderamus at mgb.org>
> > > Cc:?r-help at r-project.org <r-help at r-project.org>
> > > Subject:?Re: [R] Cores hang when calling mcapply
> > > 

> > > Hello Thomas,
> > > 

> > > Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.
> > > 

> > > Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead? a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.
> > > 

> > > It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R.
> > > 

> > > 

> > > An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.
> > > 

> > > Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -? and writes intermediate results to disk, then combines/mergesresults at the end.
> > > 

> > > Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.
> > > 

> > > If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.
> > > 

> > > Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.
> > > 

> > > > library(data.table)
> > > > library(arrow)
> > > >
> > > > # Step A: Load data efficiently as data.table
> > > > dt <- as.data.table(
> > > >?? open_dataset(
> > > >??? sources = input_files,
> > > >??? format = 'csv',
> > > >??? unify_schema = TRUE,
> > > >??? col_types = schema(
> > > >????? "ID_Key" = string(),
> > > >????? "column1" = string(),
> > > >????? "column2" = string()
> > > >??? )
> > > >? ) |>
> > > 

> > > >??? collect()
> > > > )
> > > >
> > > > # Step B: Clean names once
> > > > # Assume `crewjanitormakeclean` essentially standardizes column names
> > > > dt[, column1 := janitor::make_clean_names(column1, allow_dupes =?
> > > 

> > > > TRUE)]
> > > > dt[, column2 := janitor::make_clean_names(column2, allow_dupes =
> > > 

> > > >? TRUE)]
> > > >
> > > > # Step C: Create presence/absence indicators using data.table
> > > > # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> > > > # For large unique values, consider chunking if needed.
> > > > out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate =
> > > 

> > > > length, value.var = "column1")
> > > > out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate =
> > > 

> > > > length, value.var = "column2")
> > > >
> > > > # Step D: Merge the two wide tables by ID_Key
> > > > # Fill missing columns with 0 using data.table on-the-fly operations
> > > > all_cols <- unique(c(names(out1), names(out2)))
> > > > out1_missing <- setdiff(all_cols, names(out1))
> > > > out2_missing <- setdiff(all_cols, names(out2))
> > > >
> > > > # Add missing columns with 0
> > > > for (col in out1_missing) out1[, (col) := 0]
> > > > for (col in out2_missing) out2[, (col) := 0]
> > > >
> > > > # Ensure column order alignment if needed
> > > > setcolorder(out1, all_cols)
> > > > setcolorder(out2, all_cols)
> > > >
> > > > # Combine by ID_Key (since they share same columns now)
> > > > final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
> > > >
> > > > # Step E: If needed, summarize across ID_Key to sum presence
> > > 

> > > > indicators
> > > > final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by =
> > > 

> > > > ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
> > > >
> > > > # note that final_result should now contain summed presence/absence
> > > 

> > > > (0/1) indicators.
> > > 

> > > 

> > > 

> > > 

> > > Hope this helps!
> > > gregg
> > > somewhereinArizona
> > > 

> > > The information in this e-mail is intended only for the person to whom it is addressed.? If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline?.
> > > 

> > > 

> > > 

> > > Please note that this e-mail is not secure (encrypted).? If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.? Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.?
> > 

> > 

> > 

> > The information in this e-mail is intended only for the person to whom it is addressed.? If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline?.
> > 

> > 

> > 

> > Please note that this e-mail is not secure (encrypted).? If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.? Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.?
> 

> 

> 

> The information in this e-mail is intended only for the person to whom it is addressed.? If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline .
> 

> 

> 

> Please note that this e-mail is not secure (encrypted).? If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.? Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 603 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20241212/819b4174/attachment.sig>

From tder@mu@ @end|ng |rom mgb@org  Thu Dec 12 18:58:44 2024
From: tder@mu@ @end|ng |rom mgb@org (Deramus, Thomas Patrick)
Date: Thu, 12 Dec 2024 17:58:44 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <dWDmtt1iT8wBvk9zOzrgJI23hwIy5Y3eYKU2dK0Wj-mAYCylf63rs7Cp8yUPXD03p5X7D_jRXz-9JSHk5h_nq2Ut1yCdcwhV4b_vJcffc_g=@protonmail.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
 <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>
 <PH0PR04MB74932EAF56BA4F38915F2004CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <K-Jmkrf3aApJi7lie-0cVW3OvZ2F5szMiIsSq9HHQBSQZCZbjVuRuN3woxIEh5eCLx9JYk4B70PRvp_d_5zOGvZcWjG4Cm2M_DsMg38JdFk=@protonmail.com>
 <PH0PR04MB7493EF3DA4FE6201CB9C72D1CB3E2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <cAYww-6WFIIKmwBU2q6Wc4pgQskAHuHN1kL1GzYAwGd1exbdHijmhuRrwNMK45riNN8KKNTcjRF686SShKo5zmWXs-gvMR0jh5znWtyHoms=@protonmail.com>
 <PH0PR04MB749370B3AB16FDB914F2407BCB3F2@PH0PR04MB7493.namprd04.prod.outlook.com>
 <dWDmtt1iT8wBvk9zOzrgJI23hwIy5Y3eYKU2dK0Wj-mAYCylf63rs7Cp8yUPXD03p5X7D_jRXz-9JSHk5h_nq2Ut1yCdcwhV4b_vJcffc_g=@protonmail.com>
Message-ID: <PH0PR04MB74935CE43A8DC11DB3ADB7B3CB3F2@PH0PR04MB7493.namprd04.prod.outlook.com>

Will do!

Thanks for all your help!

-Thomas

________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Thursday, December 12, 2024 12:30 PM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply


Hi Thomas,

Glad to hear the suggestion helped, and that switching to a data.table approach reduced the processing time and memory overhead?15 minutes for one of the smaller datasets is certainly better! Sounds like the adjustments you devised, especially keeping the multicore approach for make_clean_names() and ensuring that ID_Key values remain intact, were the missing components you needed to fit it into your workflow.

I believe the warning message regarding dcast() occurs because keeptabs is a tbl_df from the tidyverse rather than a base data.frame or data.table. The data.table implementation of dcast() expects a data.table or data.frame. When it detects a tbl_df, it tries to redirect to reshape2::dcast(), but since that appears to be deprecated, it will fail in future versions at some point.
To avoid this, consider converting keeptabs into a data.table directly before calling dcast(). For example:

> setDT(keeptabs)
> out1 <- dcast(keeptabs, ID_Key ~ column1, fun.aggregate = length, value.var = "column1")
> out2 <- dcast(keeptabs, ID_Key ~ column2, fun.aggregate = length, value.var = "column2")


If keeptabs is a data.table at the time of calling dcast(), this ensures the data.table method of dcast() is used and should eliminate the warning message maintaining compatibility with  future updates of the data.table package.

Also, If you must retain keeptabs as a tibble for other parts of the workflow, you might consider converting it right before the dcast() call... do the wide conversionand then proceed. This should keep the workflow stable and ensure that you won?t run into issues down the road - when the redirect to reshape2::dcast() becomes unsupported.

Regarding the note on multicore usage for make_clean_names(), your alternative approach -splitting the data and applying make_clean_names() in parallel before merging - was a good idea. I think relying on data.table for the main pivot operations clearly made the process more memory and time-efficient. Remaining parallelization overhead is likely a necessary step given the size of the datasets you're working with and the complexity of the initial cleaning.

Anyway, happy the suggestion moved you closer to the results you were looking for. I don't often respond on r-help often, mostly just a lurker.

All the best,

gregg
Sierra Vista, Arizona


On Thursday, December 12th, 2024 at 8:54 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
Hi Gregg.

Just wanted to follow up on the solution you proposed.

I had to make some adjustments to get exactly what I wanted, but it works, and takes about 15 minutes on our server configuration:

    temp <-
??????open_dataset(
????????????sources = input_files,
????????????format = 'csv',
????????????unify_schema = TRUE,
????????????col_types = schema(
????????????"ID_Key" = string(),
????????????"column1" = string(),
????????????"column2" = string()
????????????)
??????) |> as_tibble()
??????
??keeptabs <- split(temp, temp$ID_Key)

    if(isTRUE(multicore)){
      keeptabs <- mclapply(1:length(keeptabs), function(i) crewjanitormakeclean(keeptabs[[i]],c("column1","column2")), mc.cores = numcores)
    }else{
      keeptabs <- lapply(1:length(keeptabs), function(i) crewjanitormakeclean(keeptabs[[i]],c("column1","column2")))
    }

    keeptabs <- bind_rows(keeptabs)

    out1 <- dcast(keeptabs, ID_Key ~ column1, fun.aggregate = length, value.var = "column1")
    out2 <- dcast(keeptabs, ID_Key ~ column2, fun.aggregate = length, value.var = "column2")
    out1 <- setDT(out1 |> rename_with(~ paste0("column1_name_", .x, recycle0 = TRUE), -ID_Key))
    out2 <- setDT(out1 |> rename_with(~ paste0("column2_name_", .x, recycle0 = TRUE), -ID_Key))
    all_cols <- unique(c(names(out1), names(out2)))
    out1_missing <- setdiff(all_cols, names(out1))
    out2_missing <- setdiff(all_cols, names(out2))
    for (col in out1_missing) out1[, (col) := 0]
    for (col in out2_missing) out2[, (col) := 0]
    setcolorder(out1, all_cols)
    setcolorder(out2, all_cols)
    final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
    final_result <- as_tibble(final_dt[, lapply(.SD, sum, na.rm = TRUE), by = ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")])

Worth noting however:


  *
I unfortunately had to keep the multicore parameters for the janitor package to use make_clean_names() because it just took to long  to run it on the full dataframe, but deploying data.table CONSIDERABLY reduces the time and memory overhead to the point where it only takes about 15 minutes to run one of my smaller dataframes.
  *
I keep getting the following warning message:
     *
The dcast generic in data.table has been passed a tbl_df and will attempt to redirect to the relevant reshape2 method; please note that reshape2 is superseded and is no longer actively developed, and this redirection is now deprecated. Please do this redirection yourself like reshape2::dcast(keeptabs). In the next version, this warning will become an error.
     *
So a new call may be needed to keep this approach from failing if we update our version of data.table in the future
  *
The initial approach was replacing all the KeyID variables with what was basically row numbers and that would have made merging back to the main key document an issue so I changed the rbind funciton to keep this from happening.

Thank you for all your help on this!
-Thomas DeRamus


________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Wednesday, December 11, 2024 2:11 PM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

How is the server configured to handle memory distribution for individual users. I see it has over 700GB of total system memory, but how much can be assigned it each individual user?

AAgain - just curious, and wondering how much memory was assigned to your instance when you were running R.

regards,
Gregg


On Wednesday, December 11th, 2024 at 9:49 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
It's Redhat Enterprise Linux 9

Specifically:
OS Information:
NAME="Red Hat Enterprise Linux"
VERSION="9.3 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
Operating System: Red Hat Enterprise Linux 9.3 (Plow)
     CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
          Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
    Architecture: x86-64
 Hardware Vendor: Dell Inc.
  Hardware Model: PowerEdge R840
Firmware Version: 2.15.1


Regarding RAM restrictions, here are the specs:
               total        used        free      shared  buff/cache   available
Mem:           753Gi        70Gi       600Gi       2.9Gi        89Gi       683Gi
Swap:          4.0Gi       2.5Gi       1.5Gi

It's a multi-user server so naturally things fluctuate.

Regarding possible CPU restrictions, here are the specs of our server:

    Thread(s) per core:  2
    Core(s) per socket:  20
    Socket(s):           4
    Stepping:            4
    CPU(s) scaling MHz:  50%
    CPU max MHz:         3700.0000
    CPU min MHz:         1000.0000



________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Wednesday, December 11, 2024 11:41 AM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

Thomas,
I'm curious - what OS are you running this on, and how much memory does the computer have?

Let me know if that code worked out as I hoped.

regards,
gregg

On Wednesday, December 11th, 2024 at 6:51 AM, Deramus, Thomas Patrick <tderamus at mgb.org> wrote:
About to try this implementation.

As a follow-up, this is the exact error:

Lost warning messages
Error: no more error handlers available (recursive errors?); invoking 'abort' restart
Execution halted
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)
Error: cons memory exhausted (limit reached?)

________________________________
From: Gregg Powell <g.a.powell at protonmail.com>
Sent: Tuesday, December 10, 2024 7:52 PM
To: Deramus, Thomas Patrick <tderamus at mgb.org>
Cc: r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Cores hang when calling mcapply

Hello Thomas,

Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.

Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead  a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.

It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R.


An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.

Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -  and writes intermediate results to disk, then combines/mergesresults at the end.

Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.

If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.

Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.

> library(data.table)
> library(arrow)
>
> # Step A: Load data efficiently as data.table
> dt <- as.data.table(
>   open_dataset(
>    sources = input_files,
>    format = 'csv',
>    unify_schema = TRUE,
>    col_types = schema(
>      "ID_Key" = string(),
>      "column1" = string(),
>      "column2" = string()
>    )
>  ) |>

>    collect()
> )
>
> # Step B: Clean names once
> # Assume `crewjanitormakeclean` essentially standardizes column names
> dt[, column1 := janitor::make_clean_names(column1, allow_dupes =

> TRUE)]
> dt[, column2 := janitor::make_clean_names(column2, allow_dupes =

>  TRUE)]
>
> # Step C: Create presence/absence indicators using data.table
> # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> # For large unique values, consider chunking if needed.
> out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate =

> length, value.var = "column1")
> out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate =

> length, value.var = "column2")
>
> # Step D: Merge the two wide tables by ID_Key
> # Fill missing columns with 0 using data.table on-the-fly operations
> all_cols <- unique(c(names(out1), names(out2)))
> out1_missing <- setdiff(all_cols, names(out1))
> out2_missing <- setdiff(all_cols, names(out2))
>
> # Add missing columns with 0
> for (col in out1_missing) out1[, (col) := 0]
> for (col in out2_missing) out2[, (col) := 0]
>
> # Ensure column order alignment if needed
> setcolorder(out1, all_cols)
> setcolorder(out2, all_cols)
>
> # Combine by ID_Key (since they share same columns now)
> final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
>
> # Step E: If needed, summarize across ID_Key to sum presence

> indicators
> final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by =

> ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
>
> # note that final_result should now contain summed presence/absence

> (0/1) indicators.




Hope this helps!
gregg
somewhereinArizona

The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline<https://secure-web.cisco.com/100Hi_g3yJTBAxxYPpHz7NQcwx9A06rN2U4Dh4wHDMTBLGJ5yiq8PszsdEMlRsD9C7ESGkM_88I-b3jy2QGu-x35cEfVZ6QUW9Uf8aQihVfQOceLc1DZr3AXcvDUKCpPFXgrBqSOYdHyh31yQD3--3ltk0pjgK_te1I0M6i_pIUEbdE334rYMuIxAhmwI48EHf8_k9wuSXxBrgiQc9D6Lsakb1w2RckPEmz4DmrbjWfLR4hx3ylUbDTf9UR_eWYxzvivxt6NpRfEN3T9WpUjWHHTNIdUphSKwmR8Sk8i4-KqcpJdPjHKC1185wd1Sr78X/https%3A%2F%2Fwww.massgeneralbrigham.org%2Fcomplianceline> .


Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.


The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline<https://secure-web.cisco.com/1zMX7N7YTozoGwIAk3PWA50iqx1YK6HX1taKVKQz0HqImamktAnbE_Lhw9DITz4CNk4YtcTHiaVjRHBbsNJA6nMZFdvqvYOAa3K6EjvpWhTCeZuriog-AChP7n3TbQ2Bhfkgoqdr0SXJuHbtoWHgBArTDyMzA5e1muIeHZy1FHSNCrKM-Le9MboKJd65PrLi2bLIy_jGP8xQRMsS85zV5Pq6FGo9AR5qNHn0s8h9XiHbj0sIORoRALetWpNBqvavE37HV7v08WLPtq_BGRRIwYK15jIaMIiNtwPM06KI0VQh5nqvY8VNGV7Awx4Ou4Ns-/https%3A%2F%2Fwww.massgeneralbrigham.org%2Fcomplianceline> .


Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.


The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline<https://secure-web.cisco.com/1dy-rJYlYO741hKQKugITSH7HzO6bvQ1Y9DlweEMm_fA7hbkhk-BePUt2OGFn8uSOOahmGDPqqL24q1sH48-g75C5HassbPXi8cU-TVTeleSHgO7zNbTg0hUdTHcHhWSM-eUcKrcpEhhXz_G9TS2pFdoUZSGvwHhH9QYHJUaJLdoDB7Ik_shhRZpx5IqVE0NrJyLINeL8zoc8gWbD3yjYk4JS7FiO5kdbqMy_5ZHS8UEfW4jBrglm6k0pxlxyJrxXuqsCNYYkzFTWCV4oUthAwQxQQaZwza3Z0UWPumIv2rS9ICStzy-nJkujKJCJZNpH/https%3A%2F%2Fwww.massgeneralbrigham.org%2Fcomplianceline> .


Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail.

The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline <https://www.massgeneralbrigham.org/complianceline> .
Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail. 

	[[alternative HTML version deleted]]


From jrkr|de@u @end|ng |rom gm@||@com  Thu Dec 12 20:24:13 2024
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Thu, 12 Dec 2024 14:24:13 -0500
Subject: [R] Fortune nomination.
In-Reply-To: <20241202194720.3baf35af@new-hp>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
 <DM6PR03MB5049F365BD14FF48597163E3E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQ_YjZND6XFGgpA2fCOxWdw-zRwSTpr1OnZA-FsPxBW7A@mail.gmail.com>
 <20241202194720.3baf35af@new-hp>
Message-ID: <CAKZQJMAT7tRJnTuUUS0Ph6y_DGu6QCz-aNKNt+hTEeZV2WJCtw@mail.gmail.com>

Concur

On Mon, 2 Dec 2024 at 01:47, Rolf Turner <rolfturner at posteo.net> wrote:

>
> On Sun, 1 Dec 2024 21:43:45 -0800
> Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
>
> > Finally, my best advice would be to forget about SAS if you wish to
> > use R. Trying to translate SAS paradigms into R is the devil's work.
>
> cheers,
>
> Rolf Turner
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Stats. Dep't. (secretaries) phone:
>          +64-9-373-7599 ext. 89622
> Home phone: +64-9-480-4619
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From tebert @end|ng |rom u||@edu  Fri Dec 13 05:25:28 2024
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Fri, 13 Dec 2024 04:25:28 +0000
Subject: [R] [off-topic] crossword
In-Reply-To: <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
Message-ID: <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>

I do not understand the question and I do not understand the answer. Possibly one confounds the other.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
Sent: Thursday, December 12, 2024 11:56 AM
To: Bill Dunlap <williamwdunlap at gmail.com>
Cc: r-help at R-project.org
Subject: Re: [R] [off-topic] crossword

[External Email]

RULES!


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap <williamwdunlap at gmail.com>
wrote:

> The New York Times crossword this morning had the clue (51 down, 5
> letters) "Writes in C or R, say".
>
> -Bill
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat/
> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
> %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
> c%3D&reserved=0
> PLEASE do read the posting guide
> https://www/.
> r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C9
> 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
> eserved=0 and provide commented, minimal, self-contained, reproducible
> code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From po|c1410 @end|ng |rom gm@||@com  Fri Dec 13 09:02:32 2024
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Fri, 13 Dec 2024 08:02:32 +0000
Subject: [R] [off-topic] crossword
In-Reply-To: <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>

Well to complicate things, I don't think RULES is the answer.

This is a cryptic crossword clue. They usually contain the answer twice
(well... Cryptically!!)

Writes in C or R, say.

I think the answer is CODER

If you look up the definition of say in the dictionary one option is:



   1. give instructions to or direct somebody to do something with
   authority (verb)


That's the simple part of the clue. (Notice the comma cryptic clues have
two parts giving the "same" answer)

The more complex part I think is  that and 'ode' (a poem that is written
like it is said or something) is written in between C and R  giving C ODE
R,

...


Very happy to be corrected...


(Oh and as a third part a coder writes in C or R... I hope the JavaScript
kids are listening ;-) )

On Fri, 13 Dec 2024, 04:26 Ebert,Timothy Aaron, <tebert at ufl.edu> wrote:

> I do not understand the question and I do not understand the answer.
> Possibly one confounds the other.
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
> Sent: Thursday, December 12, 2024 11:56 AM
> To: Bill Dunlap <williamwdunlap at gmail.com>
> Cc: r-help at R-project.org
> Subject: Re: [R] [off-topic] crossword
>
> [External Email]
>
> RULES!
>
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>
> On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap <williamwdunlap at gmail.com>
> wrote:
>
> > The New York Times crossword this morning had the clue (51 down, 5
> > letters) "Writes in C or R, say".
> >
> > -Bill
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat/
> > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> > %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
> > %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> > %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
> > c%3D&reserved=0
> > PLEASE do read the posting guide
> > https://www/.
> > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C9
> > 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> > %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> > 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
> > eserved=0 and provide commented, minimal, self-contained, reproducible
> > code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |vo@we|ch @end|ng |rom gm@||@com  Fri Dec 13 01:23:01 2024
From: |vo@we|ch @end|ng |rom gm@||@com (Ivo Welch)
Date: Thu, 12 Dec 2024 16:23:01 -0800
Subject: [R] Weird Behavior of mean
Message-ID: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>

Is the following a strange behavior for `mean` vs. `sd` ?

```
$ R --vanilla. ## 4.4.2
> x=c(NA,1,2,3)
> c( mean(x,na.rm=T), sd(x,na.rm=T) )
[1] 2 1
> T=20   ## bad idea for a parameter.  T is also used for TRUE
> c( mean(x,na.rm=T), sd(x,na.rm=T) )
[1] NA  1
>
```

This one was a baffler for me to track down for a few hours...


From po|c1410 @end|ng |rom gm@||@com  Fri Dec 13 09:56:15 2024
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Fri, 13 Dec 2024 08:56:15 +0000
Subject: [R] Weird Behavior of mean
In-Reply-To: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
Message-ID: <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>

I've not checked the code, but I think that result would happen if mean
uses something like

if (na.rm == TRUE) {
  # do something to remove the NA's
}

And as uses something like

If (na.rm != FALSE) {
  # do something to remove the NA's
}

Or perhaps ever na.rm == T


If you ever see posts from Bert on here with T and F, he is hard core
thorough and uses full words for exactly this reason, someone can reassign
F as True if they want and your code will melt!

On Fri, 13 Dec 2024, 08:31 Ivo Welch, <ivo.welch at gmail.com> wrote:

> Is the following a strange behavior for `mean` vs. `sd` ?
>
> ```
> $ R --vanilla. ## 4.4.2
> > x=c(NA,1,2,3)
> > c( mean(x,na.rm=T), sd(x,na.rm=T) )
> [1] 2 1
> > T=20   ## bad idea for a parameter.  T is also used for TRUE
> > c( mean(x,na.rm=T), sd(x,na.rm=T) )
> [1] NA  1
> >
> ```
>
> This one was a baffler for me to track down for a few hours...
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Dec 13 10:21:16 2024
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 13 Dec 2024 10:21:16 +0100
Subject: [R] Weird Behavior of mean
In-Reply-To: <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
 <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
Message-ID: <26459.64652.135529.538036@stat.math.ethz.ch>

>>>>> CALUM POLWART 
>>>>>     on Fri, 13 Dec 2024 08:56:15 +0000 writes:

    > I've not checked the code, but I think that result would
    > happen if mean uses something like

    > if (na.rm == TRUE) { # do something to remove the NA's }

    > And as uses something like

    > If (na.rm != FALSE) { # do something to remove the NA's }

    > Or perhaps ever na.rm == T

Hey, "be careful you are insulting" (;-) R core code would *never* contain stupid silly code 
such as   <logical> == TRUE 


> fortunes :: fortune("TRUE.*TRUE", fixed=FALSE)

Ted Harding: But you can also do these with 'any' and 'all', e.g. any(v==TRUE).
Thomas Lumley: or any( (v==TRUE)==TRUE), or any( ((v==TRUE)==TRUE)==TRUE)... Or, perhaps, any(v). Lewis Carroll wrote a nice piece
on this theme.
   -- Ted Harding and Thomas Lumley (about implementing an 'or' of a logical vector)
      R-help (August 2004)

It uses  if(isTRUE(na.rm))
and yes,
people using  T  for TRUE  are "selber tschuld" (Swiss German for "your fault!")

Martin


    > If you ever see posts from Bert on here with T and F, he
    > is hard core thorough and uses full words for exactly this
    > reason, someone can reassign F as True if they want and
    > your code will melt!

    > On Fri, 13 Dec 2024, 08:31 Ivo Welch,
    > <ivo.welch at gmail.com> wrote:

    >> Is the following a strange behavior for `mean` vs. `sd` ?
    >> 
    >> ``` $ R --vanilla. ## 4.4.2 > x=c(NA,1,2,3) > c(
    >> mean(x,na.rm=T), sd(x,na.rm=T) ) [1] 2 1 > T=20 ## bad
    >> idea for a parameter.  T is also used for TRUE > c(
    >> mean(x,na.rm=T), sd(x,na.rm=T) ) [1] NA 1
    >> >
    >> ```
    >> 
    >> This one was a baffler for me to track down for a few
    >> hours...
    >> 
    >> ______________________________________________
    >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide
    >> https://www.R-project.org/posting-guide.html and provide
    >> commented, minimal, self-contained, reproducible code.
    >> 

    > 	[[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > https://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Fri Dec 13 15:31:40 2024
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Fri, 13 Dec 2024 14:31:40 +0000
Subject: [R] [off-topic] crossword
In-Reply-To: <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
Message-ID: <CH3PR22MB4514AA26F0B0D3C828A471CACF382@CH3PR22MB4514.namprd22.prod.outlook.com>

Ok, that answer I understand and see it fitting. In that context there are several other possibilities that could also work:
Programmer, hacker, geek, cracker. There are other choices, but the other options like ?skiddie? are ones that I do not recognize and have omitted.

Tim

From: CALUM POLWART <polc1410 at gmail.com>
Sent: Friday, December 13, 2024 3:03 AM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: Erin Hodgess <erinm.hodgess at gmail.com>; Bill Dunlap <williamwdunlap at gmail.com>; r-help at R-project.org
Subject: Re: [R] [off-topic] crossword

[External Email]

Well to complicate things, I don't think RULES is the answer.

This is a cryptic crossword clue. They usually contain the answer twice (well... Cryptically!!)

Writes in C or R, say.

I think the answer is CODER

If you look up the definition of say in the dictionary one option is:



  1.  give instructions to or direct somebody to do something with authority (verb)

That's the simple part of the clue. (Notice the comma cryptic clues have two parts giving the "same" answer)

The more complex part I think is  that and 'ode' (a poem that is written like it is said or something) is written in between C and R  giving C ODE R,

...



Very happy to be corrected...



(Oh and as a third part a coder writes in C or R... I hope the JavaScript kids are listening ;-) )

On Fri, 13 Dec 2024, 04:26 Ebert,Timothy Aaron, <tebert at ufl.edu<mailto:tebert at ufl.edu>> wrote:
I do not understand the question and I do not understand the answer. Possibly one confounds the other.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>> On Behalf Of Erin Hodgess
Sent: Thursday, December 12, 2024 11:56 AM
To: Bill Dunlap <williamwdunlap at gmail.com<mailto:williamwdunlap at gmail.com>>
Cc: r-help at R-project.org<mailto:r-help at R-project.org>
Subject: Re: [R] [off-topic] crossword

[External Email]

RULES!


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com<mailto:erinm.hodgess at gmail.com>


On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap <williamwdunlap at gmail.com<mailto:williamwdunlap at gmail.com>>
wrote:

> The New York Times crossword this morning had the clue (51 down, 5
> letters) "Writes in C or R, say".
>
> -Bill
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat/
> .ethz.ch<http://ethz.ch/>%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu<http://40ufl.edu/>
> %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
> %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
> c%3D&reserved=0
> PLEASE do read the posting guide
> https://www/.
> r-project.org<http://r-project.org/>%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu<http://40ufl.edu/>%7C9
> 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
> eserved=0 and provide commented, minimal, self-contained, reproducible
> code.
>

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html<https://www.r-project.org/posting-guide.html>
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From w||||@mwdun|@p @end|ng |rom gm@||@com  Fri Dec 13 15:38:34 2024
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Fri, 13 Dec 2024 06:38:34 -0800
Subject: [R] [off-topic] crossword
In-Reply-To: <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <CAHqSRuQ9RdT1eNPWmVfjVkQUV+aNxuVxuoM5LETygFNyF7UkFA@mail.gmail.com>

The answer for "Writes in C and R, say" was "codes".  I was impressed that
R has passed into popular culture.

On Thu, Dec 12, 2024 at 8:25?PM Ebert,Timothy Aaron <tebert at ufl.edu> wrote:

> I do not understand the question and I do not understand the answer.
> Possibly one confounds the other.
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
> Sent: Thursday, December 12, 2024 11:56 AM
> To: Bill Dunlap <williamwdunlap at gmail.com>
> Cc: r-help at R-project.org
> Subject: Re: [R] [off-topic] crossword
>
> [External Email]
>
> RULES!
>
>
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>
> On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap <williamwdunlap at gmail.com>
> wrote:
>
> > The New York Times crossword this morning had the clue (51 down, 5
> > letters) "Writes in C or R, say".
> >
> > -Bill
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat/
> > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> > %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
> > %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> > %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
> > c%3D&reserved=0
> > PLEASE do read the posting guide
> > https://www/.
> > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C9
> > 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> > %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> > 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
> > eserved=0 and provide commented, minimal, self-contained, reproducible
> > code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From o||v|er@crouzet @end|ng |rom un|v-n@nte@@|r  Fri Dec 13 15:51:26 2024
From: o||v|er@crouzet @end|ng |rom un|v-n@nte@@|r (Olivier Crouzet)
Date: Fri, 13 Dec 2024 15:51:26 +0100
Subject: [R] [off-topic] crossword
In-Reply-To: <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
Message-ID: <20241213155126.20a2efbd7633df2d08fb4925@univ-nantes.fr>

Thank you all for the helpful and enlightening comments. One question
though, isn't "say" a synonym in oral forms of american english for
"for example"? Which would translate to:

> Writes in C or R, [for example]. 

which would involve that C and R are possible examples of the usage
contexts considered here in which someone would "write"?

This would then make perfect sense to me for the proposed answer:
"codes".

Yours.
Olivier.


On Fri, 13 Dec 2024
08:02:32 +0000 CALUM POLWART <polc1410 at gmail.com> wrote:

> Well to complicate things, I don't think RULES is the answer.
> 
> This is a cryptic crossword clue. They usually contain the answer
> twice (well... Cryptically!!)
> 
> Writes in C or R, say.
> 
> I think the answer is CODER
> 
> If you look up the definition of say in the dictionary one option is:
> 
> 
> 
>    1. give instructions to or direct somebody to do something with
>    authority (verb)
> 
> 
> That's the simple part of the clue. (Notice the comma cryptic clues
> have two parts giving the "same" answer)
> 
> The more complex part I think is  that and 'ode' (a poem that is
> written like it is said or something) is written in between C and R
> giving C ODE R,
> 
> ...
> 
> 
> Very happy to be corrected...
> 
> 
> (Oh and as a third part a coder writes in C or R... I hope the
> JavaScript kids are listening ;-) )
> 
> On Fri, 13 Dec 2024, 04:26 Ebert,Timothy Aaron, <tebert at ufl.edu>
> wrote:
> 
> > I do not understand the question and I do not understand the answer.
> > Possibly one confounds the other.
> >
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin
> > Hodgess Sent: Thursday, December 12, 2024 11:56 AM
> > To: Bill Dunlap <williamwdunlap at gmail.com>
> > Cc: r-help at R-project.org
> > Subject: Re: [R] [off-topic] crossword
> >
> > [External Email]
> >
> > RULES!
> >
> >
> > Erin Hodgess, PhD
> > mailto: erinm.hodgess at gmail.com
> >
> >
> > On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap
> > <williamwdunlap at gmail.com> wrote:
> >
> > > The New York Times crossword this morning had the clue (51 down, 5
> > > letters) "Writes in C or R, say".
> > >
> > > -Bill
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat/
> > > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> > > %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
> > > %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> > > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> > > %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
> > > c%3D&reserved=0
> > > PLEASE do read the posting guide
> > > https://www/.
> > > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C9
> > > 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> > > %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> > > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> > > 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
> > > eserved=0 and provide commented, minimal, self-contained,
> > > reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


-- 
  Olivier Crouzet, PhD
  http://olivier.ghostinthemachine.space
  /Ma?tre de Conf?rences/
  @LLING - Laboratoire de Linguistique de Nantes
    UMR6310 CNRS / Universit? de Nantes


From er|nm@hodge@@ @end|ng |rom gm@||@com  Fri Dec 13 16:05:52 2024
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Fri, 13 Dec 2024 08:05:52 -0700
Subject: [R] [off-topic] crossword
In-Reply-To: <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
Message-ID: <CACxE24=A-fJhy8Y3fEfmJu5K2Ass_PMNO9Wy3qP0xGuyEgKCKw@mail.gmail.com>

RULES means that anyone who uses C or R RULES the Universe.  They just do.

Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Fri, Dec 13, 2024 at 1:02?AM CALUM POLWART <polc1410 at gmail.com> wrote:

> Well to complicate things, I don't think RULES is the answer.
>
> This is a cryptic crossword clue. They usually contain the answer twice
> (well... Cryptically!!)
>
> Writes in C or R, say.
>
> I think the answer is CODER
>
> If you look up the definition of say in the dictionary one option is:
>
>
>
>    1. give instructions to or direct somebody to do something with
>    authority (verb)
>
>
> That's the simple part of the clue. (Notice the comma cryptic clues have
> two parts giving the "same" answer)
>
> The more complex part I think is  that and 'ode' (a poem that is written
> like it is said or something) is written in between C and R  giving C ODE
> R,
>
> ...
>
>
> Very happy to be corrected...
>
>
> (Oh and as a third part a coder writes in C or R... I hope the JavaScript
> kids are listening ;-) )
>
> On Fri, 13 Dec 2024, 04:26 Ebert,Timothy Aaron, <tebert at ufl.edu> wrote:
>
>> I do not understand the question and I do not understand the answer.
>> Possibly one confounds the other.
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
>> Sent: Thursday, December 12, 2024 11:56 AM
>> To: Bill Dunlap <williamwdunlap at gmail.com>
>> Cc: r-help at R-project.org
>> Subject: Re: [R] [off-topic] crossword
>>
>> [External Email]
>>
>> RULES!
>>
>>
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>
>> On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap <williamwdunlap at gmail.com>
>> wrote:
>>
>> > The New York Times crossword this morning had the clue (51 down, 5
>> > letters) "Writes in C or R, say".
>> >
>> > -Bill
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat/
>> > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
>> > %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
>> > %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
>> > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
>> > %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
>> > c%3D&reserved=0
>> > PLEASE do read the posting guide
>> > https://www/.
>> > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C9
>> > 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
>> > %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
>> > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
>> > 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
>> > eserved=0 and provide commented, minimal, self-contained, reproducible
>> > code.
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.r-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From ewoo @end|ng |rom je||er|e@@com  Fri Dec 13 15:24:18 2024
From: ewoo @end|ng |rom je||er|e@@com (Edward Woo)
Date: Fri, 13 Dec 2024 14:24:18 +0000
Subject: [R] Confirmation of no Electronic Communication functionality
Message-ID: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>

Hi,

I am trying to download R version 3.6.1 onto my company computer and need an email confirming that r does not include any electronic communication functionality within the program.

Thank you very much!

Best,
Edward Woo


Edward Woo
Municipals Securities Group
Jefferies LLC
520 Madison Avenue, 3rd Floor
New York, NY 10022
Phone: 212-708-2958
ewoo at jefferies.com<mailto:ewoo at jefferies.com>


Jefferies archives and monitors outgoing and incoming e-mail. The contents of this email, including any attachments, are confidential to the ordinary user of the email address to which it was addressed. If you are not the addressee of this email you may not copy, forward, disclose or otherwise use it or any part of it in any form whatsoever. This email may be produced at the request of regulators or in connection with civil litigation. Jefferies accepts no liability for any errors or omissions arising as a result of transmission. Use by other than intended recipients is prohibited.

	[[alternative HTML version deleted]]


From |kry|ov @end|ng |rom d|@root@org  Fri Dec 13 16:34:30 2024
From: |kry|ov @end|ng |rom d|@root@org (Ivan Krylov)
Date: Fri, 13 Dec 2024 18:34:30 +0300
Subject: [R] Confirmation of no Electronic Communication functionality
In-Reply-To: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
References: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
Message-ID: <20241213183430.2d04ba13@arachnoid>

Dear Edward Woo,

Welcome to R-help!

? Fri, 13 Dec 2024 14:24:18 +0000
Edward Woo via R-help <r-help at r-project.org> ?????:

> need an email confirming that r does not include any electronic
> communication functionality within the program

Could you please provide a definition of electronic communication
functionality? (For example, does download.file() or install.packages()
count as one?)

There may be some documents useful for you at
<https://www.r-project.org/certification.html>.

-- 
Best regards,
Ivan


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Dec 13 16:45:29 2024
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 13 Dec 2024 07:45:29 -0800
Subject: [R] Confirmation of no Electronic Communication functionality
In-Reply-To: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
References: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
Message-ID: <E05927BE-5E15-4199-8737-56F14D77281D@dcn.davis.ca.us>

R is a programming language. If you program it to communicate, it will. Also, it is designed to work with user-contributed packages of code... the process of downloading the packages is communication, and the downloaded packages may communicate if you invoke them (that is often why they are created).

You may need a more nuanced view of software restrictions if you have this discussion about using R with your IT people than "has no electronic communication functionality". For example, the code is open-source... if there is untrusted behavior then it can be seen in the source code.

On December 13, 2024 6:24:18 AM PST, Edward Woo via R-help <r-help at r-project.org> wrote:
>Hi,
>
>I am trying to download R version 3.6.1 onto my company computer and need an email confirming that r does not include any electronic communication functionality within the program.
>
>Thank you very much!
>
>Best,
>Edward Woo
>
>
>Edward Woo
>Municipals Securities Group
>Jefferies LLC
>520 Madison Avenue, 3rd Floor
>New York, NY 10022
>Phone: 212-708-2958
>ewoo at jefferies.com<mailto:ewoo at jefferies.com>
>
>
>Jefferies archives and monitors outgoing and incoming e-mail. The contents of this email, including any attachments, are confidential to the ordinary user of the email address to which it was addressed. If you are not the addressee of this email you may not copy, forward, disclose or otherwise use it or any part of it in any form whatsoever. This email may be produced at the request of regulators or in connection with civil litigation. Jefferies accepts no liability for any errors or omissions arising as a result of transmission. Use by other than intended recipients is prohibited.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Dec 13 16:50:07 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 13 Dec 2024 07:50:07 -0800
Subject: [R] Confirmation of no Electronic Communication functionality
In-Reply-To: <20241213183430.2d04ba13@arachnoid>
References: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
 <20241213183430.2d04ba13@arachnoid>
Message-ID: <CAGxFJbQ+Bn1-arkkWkRv7tt65jahu58ORy=WbWFp9YvmxwvNbg@mail.gmail.com>

Thanks, Ivan. Exactly my reaction.

And as a Turing complete language, R allows one to do anything in R --
including writing an email package that does email directly from R:
see package emayili .

Indeed, I think one would find it difficult to find *any* software
that does not interact with the internet in some way these days.

Cheers,
Bert

On Fri, Dec 13, 2024 at 7:35?AM Ivan Krylov via R-help
<r-help at r-project.org> wrote:
>
> Dear Edward Woo,
>
> Welcome to R-help!
>
> ? Fri, 13 Dec 2024 14:24:18 +0000
> Edward Woo via R-help <r-help at r-project.org> ?????:
>
> > need an email confirming that r does not include any electronic
> > communication functionality within the program
>
> Could you please provide a definition of electronic communication
> functionality? (For example, does download.file() or install.packages()
> count as one?)
>
> There may be some documents useful for you at
> <https://www.r-project.org/certification.html>.
>
> --
> Best regards,
> Ivan
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@rc_@chw@rtz @end|ng |rom me@com  Fri Dec 13 16:58:59 2024
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Fri, 13 Dec 2024 10:58:59 -0500
Subject: [R] Confirmation of no Electronic Communication functionality
In-Reply-To: <E05927BE-5E15-4199-8737-56F14D77281D@dcn.davis.ca.us>
References: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
 <E05927BE-5E15-4199-8737-56F14D77281D@dcn.davis.ca.us>
Message-ID: <3B9F3BB6-5DDF-42CD-8755-E58CCC406F13@me.com>

Hi,

In addition to the replies that you have already received, it is reasonable to question why you wish to download a version of R that is over 5 years old. Version 3.6.1 was released in July of 2019.

If you are concerned about security, which is going to be a scenario specific discussion, as Jeff rightly notes below, you might also want to be aware that R version 4.4.0 was patched earlier this year for a reported code execution vulnerability that is relevant to versions of R back to 1.4.0.

  https://blog.r-project.org/2024/05/10/statement-on-cve-2024-27322/

Thus, using a currently released and supported version of R would be in your best interest.

Regards,

Marc Schwarzt

> On Dec 13, 2024, at 10:45?AM, Jeff Newmiller via R-help <r-help at r-project.org> wrote:
> 
> R is a programming language. If you program it to communicate, it will. Also, it is designed to work with user-contributed packages of code... the process of downloading the packages is communication, and the downloaded packages may communicate if you invoke them (that is often why they are created).
> 
> You may need a more nuanced view of software restrictions if you have this discussion about using R with your IT people than "has no electronic communication functionality". For example, the code is open-source... if there is untrusted behavior then it can be seen in the source code.
> 
> On December 13, 2024 6:24:18 AM PST, Edward Woo via R-help <r-help at r-project.org> wrote:
>> Hi,
>> 
>> I am trying to download R version 3.6.1 onto my company computer and need an email confirming that r does not include any electronic communication functionality within the program.
>> 
>> Thank you very much!
>> 
>> Best,
>> Edward Woo
>> 
>> 
>> Edward Woo
>> Municipals Securities Group
>> Jefferies LLC
>> 520 Madison Avenue, 3rd Floor
>> New York, NY 10022
>> Phone: 212-708-2958
>> ewoo at jefferies.com<mailto:ewoo at jefferies.com>
>> 
>> 


From @ggp@@erge| @end|ng |rom gm@||@com  Fri Dec 13 17:04:56 2024
From: @ggp@@erge| @end|ng |rom gm@||@com (Sergei Ko)
Date: Fri, 13 Dec 2024 16:04:56 +0000
Subject: [R] Confirmation of no Electronic Communication functionality
In-Reply-To: <CAGxFJbQ+Bn1-arkkWkRv7tt65jahu58ORy=WbWFp9YvmxwvNbg@mail.gmail.com>
References: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
 <20241213183430.2d04ba13@arachnoid>
 <CAGxFJbQ+Bn1-arkkWkRv7tt65jahu58ORy=WbWFp9YvmxwvNbg@mail.gmail.com>
Message-ID: <CAK2fHGen8z31OBx=vRO9xfcC2gNNX1jpqqMkvMwZbD9W0gP2YA@mail.gmail.com>

I suspect it might be relevant for RStudio, but definitely not for pure R.
On top of that you can just copy R to a flash drive or any accessible
folder. It will work.
Regards,
Sergiy

On Fri, 13 Dec 2024, 15:56 Bert Gunter, <bgunter.4567 at gmail.com> wrote:

> Thanks, Ivan. Exactly my reaction.
>
> And as a Turing complete language, R allows one to do anything in R --
> including writing an email package that does email directly from R:
> see package emayili .
>
> Indeed, I think one would find it difficult to find *any* software
> that does not interact with the internet in some way these days.
>
> Cheers,
> Bert
>
> On Fri, Dec 13, 2024 at 7:35?AM Ivan Krylov via R-help
> <r-help at r-project.org> wrote:
> >
> > Dear Edward Woo,
> >
> > Welcome to R-help!
> >
> > ? Fri, 13 Dec 2024 14:24:18 +0000
> > Edward Woo via R-help <r-help at r-project.org> ?????:
> >
> > > need an email confirming that r does not include any electronic
> > > communication functionality within the program
> >
> > Could you please provide a definition of electronic communication
> > functionality? (For example, does download.file() or install.packages()
> > count as one?)
> >
> > There may be some documents useful for you at
> > <https://www.r-project.org/certification.html>.
> >
> > --
> > Best regards,
> > Ivan
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Fri Dec 13 18:35:31 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 13 Dec 2024 12:35:31 -0500
Subject: [R] Confirmation of no Electronic Communication functionality
In-Reply-To: <CAK2fHGen8z31OBx=vRO9xfcC2gNNX1jpqqMkvMwZbD9W0gP2YA@mail.gmail.com>
References: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
 <20241213183430.2d04ba13@arachnoid>
 <CAGxFJbQ+Bn1-arkkWkRv7tt65jahu58ORy=WbWFp9YvmxwvNbg@mail.gmail.com>
 <CAK2fHGen8z31OBx=vRO9xfcC2gNNX1jpqqMkvMwZbD9W0gP2YA@mail.gmail.com>
Message-ID: <4c419ff6-d9c5-4597-8325-751dc5d3c17f@gmail.com>

   This reminds me a little bit of working in a secure data center (no 
external network connections from any of the workstations) where the 
security policy did not allow shell access, but did allow R (disallowing 
it would have crippled half of the projects using the data center).  I 
didn't go out of my way to tell them about system() ...

   cheers
     Ben Bolker

On 12/13/24 11:04, Sergei Ko wrote:
> I suspect it might be relevant for RStudio, but definitely not for pure R.
> On top of that you can just copy R to a flash drive or any accessible
> folder. It will work.
> Regards,
> Sergiy
> 
> On Fri, 13 Dec 2024, 15:56 Bert Gunter, <bgunter.4567 at gmail.com> wrote:
> 
>> Thanks, Ivan. Exactly my reaction.
>>
>> And as a Turing complete language, R allows one to do anything in R --
>> including writing an email package that does email directly from R:
>> see package emayili .
>>
>> Indeed, I think one would find it difficult to find *any* software
>> that does not interact with the internet in some way these days.
>>
>> Cheers,
>> Bert
>>
>> On Fri, Dec 13, 2024 at 7:35?AM Ivan Krylov via R-help
>> <r-help at r-project.org> wrote:
>>>
>>> Dear Edward Woo,
>>>
>>> Welcome to R-help!
>>>
>>> ? Fri, 13 Dec 2024 14:24:18 +0000
>>> Edward Woo via R-help <r-help at r-project.org> ?????:
>>>
>>>> need an email confirming that r does not include any electronic
>>>> communication functionality within the program
>>>
>>> Could you please provide a definition of electronic communication
>>> functionality? (For example, does download.file() or install.packages()
>>> count as one?)
>>>
>>> There may be some documents useful for you at
>>> <https://www.r-project.org/certification.html>.
>>>
>>> --
>>> Best regards,
>>> Ivan
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> https://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
* E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From d@n|e|obo9976 @end|ng |rom gm@||@com  Fri Dec 13 18:52:15 2024
From: d@n|e|obo9976 @end|ng |rom gm@||@com (Daniel Lobo)
Date: Fri, 13 Dec 2024 23:22:15 +0530
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
Message-ID: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>

Hi,

I have below non-linear constraint optimization problem

#Original artificial data

library(nloptr)

set.seed(1)
A <- 1.34
B <- 0.5673
C <- 6.356
D <- -1.234
x <- seq(0.5, 20, length.out = 500)
y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)

#Objective function

X <- cbind(1, x, x^2, log(x))
f <- function(theta) {
sum(abs(X %*% theta - y))
}

#Constraint

eps <- 1e-4

hin <- function(theta) {
  abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
}

Hx <- function(theta) {
  X[100, , drop = FALSE] %*% theta - (120 - eps)
}

#Optimization with nloptr

Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
# -0.2186159 -0.5032066  6.4458823 -0.4125948

However this does not appear to be optimal value. For example, if I
use below set,
0.222, 6.999, 6.17, -19.371, value of my objective function is lower
that that using nloptr

I just wonder in the package nloptr is good for non-linear optimization?


From pro|jcn@@h @end|ng |rom gm@||@com  Fri Dec 13 19:03:19 2024
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Fri, 13 Dec 2024 13:03:19 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
Message-ID: <b4bbb669-0e11-4018-a4fb-9f117ff4151a@gmail.com>

COBYLA stands for Contrained Optimization by Linear Approximation.

You seem to have some squares in your functions. Maybe BOBYQA would
be a better choice, though it only does bounds, so you'd have to introduce
a penalty, but then more of the optimx solvers would be available. With
only 4 parameters, possibly one of the Nelder-Mead variants (anms?) would
be suitable at least for tryout.

Optimizers are like other tools. Some are chainsaws, others are scalpels.
Don't do neurosurgery with a chainsaw unless you want a mess.

Have you checked that the objective and contraint are computed correctly?
 > 50% of "your software doesn't work" in optimization are due to such errors.

John Nash


On 2024-12-13 12:52, Daniel Lobo wrote:
> Hi,
> 
> I have below non-linear constraint optimization problem
> 
> #Original artificial data
> 
> library(nloptr)
> 
> set.seed(1)
> A <- 1.34
> B <- 0.5673
> C <- 6.356
> D <- -1.234
> x <- seq(0.5, 20, length.out = 500)
> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
> 
> #Objective function
> 
> X <- cbind(1, x, x^2, log(x))
> f <- function(theta) {
> sum(abs(X %*% theta - y))
> }
> 
> #Constraint
> 
> eps <- 1e-4
> 
> hin <- function(theta) {
>    abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
> }
> 
> Hx <- function(theta) {
>    X[100, , drop = FALSE] %*% theta - (120 - eps)
> }
> 
> #Optimization with nloptr
> 
> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
> # -0.2186159 -0.5032066  6.4458823 -0.4125948
> 
> However this does not appear to be optimal value. For example, if I
> use below set,
> 0.222, 6.999, 6.17, -19.371, value of my objective function is lower
> that that using nloptr
> 
> I just wonder in the package nloptr is good for non-linear optimization?
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From d@n|e|obo9976 @end|ng |rom gm@||@com  Fri Dec 13 19:03:57 2024
From: d@n|e|obo9976 @end|ng |rom gm@||@com (Daniel Lobo)
Date: Fri, 13 Dec 2024 23:33:57 +0530
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
Message-ID: <CADZb7hrUnNGZ6LT6g3rQr8QR-veRmwiRzeCD4XpfJC29jihqWQ@mail.gmail.com>

A small correction, the below combination

2.02, 6.764, 6.186, -20.095

Gives better result.

On Fri, 13 Dec 2024 at 23:22, Daniel Lobo <danielobo9976 at gmail.com> wrote:
>
> Hi,
>
> I have below non-linear constraint optimization problem
>
> #Original artificial data
>
> library(nloptr)
>
> set.seed(1)
> A <- 1.34
> B <- 0.5673
> C <- 6.356
> D <- -1.234
> x <- seq(0.5, 20, length.out = 500)
> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
>
> #Objective function
>
> X <- cbind(1, x, x^2, log(x))
> f <- function(theta) {
> sum(abs(X %*% theta - y))
> }
>
> #Constraint
>
> eps <- 1e-4
>
> hin <- function(theta) {
>   abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
> }
>
> Hx <- function(theta) {
>   X[100, , drop = FALSE] %*% theta - (120 - eps)
> }
>
> #Optimization with nloptr
>
> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
> # -0.2186159 -0.5032066  6.4458823 -0.4125948
>
> However this does not appear to be optimal value. For example, if I
> use below set,
> 0.222, 6.999, 6.17, -19.371, value of my objective function is lower
> that that using nloptr
>
> I just wonder in the package nloptr is good for non-linear optimization?


From bbo|ker @end|ng |rom gm@||@com  Fri Dec 13 19:06:17 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 13 Dec 2024 13:06:17 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <b4bbb669-0e11-4018-a4fb-9f117ff4151a@gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <b4bbb669-0e11-4018-a4fb-9f117ff4151a@gmail.com>
Message-ID: <4353c435-99d2-463b-8059-50a1939ab1f4@gmail.com>


   Fortune candidate?  (Prof Zeileis, are you still collecting these?)

> Optimizers are like other tools. Some are chainsaws, others are scalpels.
> Don't do neurosurgery with a chainsaw unless you want a mess.


From d@n|e|obo9976 @end|ng |rom gm@||@com  Fri Dec 13 19:05:53 2024
From: d@n|e|obo9976 @end|ng |rom gm@||@com (Daniel Lobo)
Date: Fri, 13 Dec 2024 23:35:53 +0530
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <b4bbb669-0e11-4018-a4fb-9f117ff4151a@gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <b4bbb669-0e11-4018-a4fb-9f117ff4151a@gmail.com>
Message-ID: <CADZb7hp5JN3rrSmfDGvVnQqWxmWfV-fQ+CX6QgqFObv6=iZwEQ@mail.gmail.com>

Thanks for your reply.

I have checked the optimized value and applicable constraints. Both
set of the values of parameters satisfy the constraints.

What other solver would you suggest for this problem?

On Fri, 13 Dec 2024 at 23:33, J C Nash <profjcnash at gmail.com> wrote:
>
> COBYLA stands for Contrained Optimization by Linear Approximation.
>
> You seem to have some squares in your functions. Maybe BOBYQA would
> be a better choice, though it only does bounds, so you'd have to introduce
> a penalty, but then more of the optimx solvers would be available. With
> only 4 parameters, possibly one of the Nelder-Mead variants (anms?) would
> be suitable at least for tryout.
>
> Optimizers are like other tools. Some are chainsaws, others are scalpels.
> Don't do neurosurgery with a chainsaw unless you want a mess.
>
> Have you checked that the objective and contraint are computed correctly?
>  > 50% of "your software doesn't work" in optimization are due to such errors.
>
> John Nash
>
>
> On 2024-12-13 12:52, Daniel Lobo wrote:
> > Hi,
> >
> > I have below non-linear constraint optimization problem
> >
> > #Original artificial data
> >
> > library(nloptr)
> >
> > set.seed(1)
> > A <- 1.34
> > B <- 0.5673
> > C <- 6.356
> > D <- -1.234
> > x <- seq(0.5, 20, length.out = 500)
> > y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
> >
> > #Objective function
> >
> > X <- cbind(1, x, x^2, log(x))
> > f <- function(theta) {
> > sum(abs(X %*% theta - y))
> > }
> >
> > #Constraint
> >
> > eps <- 1e-4
> >
> > hin <- function(theta) {
> >    abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
> > }
> >
> > Hx <- function(theta) {
> >    X[100, , drop = FALSE] %*% theta - (120 - eps)
> > }
> >
> > #Optimization with nloptr
> >
> > Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> > list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
> > # -0.2186159 -0.5032066  6.4458823 -0.4125948
> >
> > However this does not appear to be optimal value. For example, if I
> > use below set,
> > 0.222, 6.999, 6.17, -19.371, value of my objective function is lower
> > that that using nloptr
> >
> > I just wonder in the package nloptr is good for non-linear optimization?
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


From d@n|e|obo9976 @end|ng |rom gm@||@com  Fri Dec 13 19:10:22 2024
From: d@n|e|obo9976 @end|ng |rom gm@||@com (Daniel Lobo)
Date: Fri, 13 Dec 2024 23:40:22 +0530
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <CADZb7hqCVAY9XyJWYjOrzVcFOnp1M5H6B9DMQCtA5+8RvBOr9g@mail.gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <b4bbb669-0e11-4018-a4fb-9f117ff4151a@gmail.com>
 <CADZb7hqCVAY9XyJWYjOrzVcFOnp1M5H6B9DMQCtA5+8RvBOr9g@mail.gmail.com>
Message-ID: <CADZb7homrUNxwbWNeTtmmv55n7xdMG4jnvf4xRLzfQpBooWPMw@mail.gmail.com>

Adding R help

On Fri, 13 Dec 2024 at 23:39, Daniel Lobo <danielobo9976 at gmail.com> wrote:
>
> If I use "algorithm" = "BOBYQA", the nloptr() fails with below message
>
> Error in is.nloptr(ret) :
>
>   Incorrect algorithm supplied. Use one of the following:
>
> NLOPT_GN_DIRECT
>
> NLOPT_GN_DIRECT_L
>
> NLOPT_GN_DIRECT_L_RAND
>
> NLOPT_GN_DIRECT_NOSCAL
>
> NLOPT_GN_DIRECT_L_NOSCAL
>
> NLOPT_GN_DIRECT_L_RAND_NOSCAL
>
> NLOPT_GN_ORIG_DIRECT
>
> NLOPT_GN_ORIG_DIRECT_L
>
> NLOPT_GD_STOGO
>
> NLOPT_GD_STOGO_RAND
>
> NLOPT_LD_SLSQP
>
> NLOPT_LD_LBFGS_NOCEDAL
>
> NLOPT_LD_LBFGS
>
> NLOPT_LN_PRAXIS
>
> NLOPT_LD_VAR1
>
> NLOPT_LD_VAR2
>
> NLOPT_LD_TNEWTON
>
> NLOPT_LD_TNEWTON_RESTART
>
> NLOPT_LD_TNEWTON_PRECOND
>
> NLOPT_LD_TNEWTON_PRECOND_RESTART
>
> NLOPT_GN_CRS2_LM
>
> NLOPT_GN_MLSL
>
> NLOPT_GD_MLSL
>
> NLOPT_GN_MLSL_LDS
>
> NLOPT_GD_MLSL_LDS
>
> NLOPT_LD_MMA
>
> NLOPT_LD_CCSAQ
>
> NLOPT_LN_COBYLA
>
> NLOPT_LN_NEWUOA
>
> NLOPT_LN_NEWUOA_BOUND
>
> NLOPT_LN_NELDERMEAD
>
> NLOPT_LN_SBPLX
>
> NLOPT_LN_AUGLAG
>
> NLOPT_LD_AUGLAG
>
> NLOPT_LN_AUGLAG_EQ
>
> NLOPT_LD_AUGLAG_EQ
>
> NLOPT_LN_BOBYQA
>
> NLOPT_GN_ISRES
>
> NLOPT_GN_ESCH
>
>
> On Fri, 13 Dec 2024 at 23:33, J C Nash <profjcnash at gmail.com> wrote:
> >
> > COBYLA stands for Contrained Optimization by Linear Approximation.
> >
> > You seem to have some squares in your functions. Maybe BOBYQA would
> > be a better choice, though it only does bounds, so you'd have to introduce
> > a penalty, but then more of the optimx solvers would be available. With
> > only 4 parameters, possibly one of the Nelder-Mead variants (anms?) would
> > be suitable at least for tryout.
> >
> > Optimizers are like other tools. Some are chainsaws, others are scalpels.
> > Don't do neurosurgery with a chainsaw unless you want a mess.
> >
> > Have you checked that the objective and contraint are computed correctly?
> >  > 50% of "your software doesn't work" in optimization are due to such errors.
> >
> > John Nash
> >
> >
> > On 2024-12-13 12:52, Daniel Lobo wrote:
> > > Hi,
> > >
> > > I have below non-linear constraint optimization problem
> > >
> > > #Original artificial data
> > >
> > > library(nloptr)
> > >
> > > set.seed(1)
> > > A <- 1.34
> > > B <- 0.5673
> > > C <- 6.356
> > > D <- -1.234
> > > x <- seq(0.5, 20, length.out = 500)
> > > y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
> > >
> > > #Objective function
> > >
> > > X <- cbind(1, x, x^2, log(x))
> > > f <- function(theta) {
> > > sum(abs(X %*% theta - y))
> > > }
> > >
> > > #Constraint
> > >
> > > eps <- 1e-4
> > >
> > > hin <- function(theta) {
> > >    abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
> > > }
> > >
> > > Hx <- function(theta) {
> > >    X[100, , drop = FALSE] %*% theta - (120 - eps)
> > > }
> > >
> > > #Optimization with nloptr
> > >
> > > Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> > > list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
> > > # -0.2186159 -0.5032066  6.4458823 -0.4125948
> > >
> > > However this does not appear to be optimal value. For example, if I
> > > use below set,
> > > 0.222, 6.999, 6.17, -19.371, value of my objective function is lower
> > > that that using nloptr
> > >
> > > I just wonder in the package nloptr is good for non-linear optimization?
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >

-------------- next part --------------
A non-text attachment was scrubbed...
Name: dummy.png
Type: image/png
Size: 163 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20241213/f6a8a348/attachment.png>

From murdoch@dunc@n @end|ng |rom gm@||@com  Fri Dec 13 19:45:47 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 13 Dec 2024 13:45:47 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
Message-ID: <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>

You posted a version of this question on StackOverflow, and were given 
advice there that you ignored.

nloptr() clearly indicates that it is quitting without reaching an 
optimum, but you are hiding that message.  Don't do that.

Duncan Murdoch

On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
> library(nloptr)
> 
> set.seed(1)
> A <- 1.34
> B <- 0.5673
> C <- 6.356
> D <- -1.234
> x <- seq(0.5, 20, length.out = 500)
> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
> 
> #Objective function
> 
> X <- cbind(1, x, x^2, log(x))
> f <- function(theta) {
> sum(abs(X %*% theta - y))
> }
> 
> #Constraint
> 
> eps <- 1e-4
> 
> hin <- function(theta) {
>    abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
> }
> 
> Hx <- function(theta) {
>    X[100, , drop = FALSE] %*% theta - (120 - eps)
> }
> 
> #Optimization with nloptr
> 
> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
> # -0.2186159 -0.5032066  6.4458823 -0.4125948


From pro|jcn@@h @end|ng |rom gm@||@com  Fri Dec 13 20:30:03 2024
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Fri, 13 Dec 2024 14:30:03 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
Message-ID: <0c195643-1416-4069-9c46-5edfe895c432@gmail.com>

The following may or may not be relevant, but definitely getting somewhat different results.
As this was a quick and dirty try while having a snack, it may have bugs.

# Lobo2412.R  -- from R Help 20241213

#Original artificial data

library(optimx)
library(nloptr)
library(alabama)

set.seed(1)
A <- 1.34
B <- 0.5673
C <- 6.356
D <- -1.234
x <- seq(0.5, 20, length.out = 500)
y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)

#Objective function

X <- cbind(1, x, x^2, log(x))
flobo <- function(theta) {
sum(abs(X %*% theta - y))
}

#Constraint

eps <- 1e-4

hinlobo <- function(theta) {
   abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps # ?? weird! (1e-4 - 1e-3)
}

Hxlobo <- function(theta) {
   X[100, , drop = FALSE] %*% theta - (120 - eps) # ditto -- also constant
}

conobj<-function(tt){
    ob <- flobo(tt)
    ci <- hinlobo(tt)
    if (ci > 0) {ci <- 0}
    ce <- Hxlobo(tt)
    si<-1; se<-1
    val<-ob+si*ci^2+se*ce^2
    cat("f, ci, ce,ob,val:"," ",ci," ",ce," ",ob," ",val," at "); print(tt)
    val
}

t0<-rep(0,4)
conobj(t0)
t1 <- c(2.02, 6.764, 6.186, -20.095)
conobj(t1)
t2 <- c( -0.2186159, -0.5032066,  6.4458823, -0.4125948)
conobj(t2)


solo<-optimr(t0, conobj, gr="grcentral", method="anms", control=list(trace=1))
solo
conobj(solo$par)
#Optimization with nloptr

# Sol = nloptr::auglag(t0, flobo, eval_g_ineq = hinlobo, eval_g_eq = Hxlobo, opts =
# list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8, print_level=1))
# -0.2186159 -0.5032066  6.4458823 -0.4125948

sol <- auglag(par=t0, fn=flobo, hin=hinlobo, heq=Hxlobo, control.outer=list(trace=TRUE))
sol

#==================================

J Nash

On 2024-12-13 13:45, Duncan Murdoch wrote:
> You posted a version of this question on StackOverflow, and were given advice there that you ignored.
> 
> nloptr() clearly indicates that it is quitting without reaching an optimum, but you are hiding that message.? Don't do 
> that.
> 
> Duncan Murdoch
> 
> On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
>> library(nloptr)
>>
>> set.seed(1)
>> A <- 1.34
>> B <- 0.5673
>> C <- 6.356
>> D <- -1.234
>> x <- seq(0.5, 20, length.out = 500)
>> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
>>
>> #Objective function
>>
>> X <- cbind(1, x, x^2, log(x))
>> f <- function(theta) {
>> sum(abs(X %*% theta - y))
>> }
>>
>> #Constraint
>>
>> eps <- 1e-4
>>
>> hin <- function(theta) {
>> ?? abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
>> }
>>
>> Hx <- function(theta) {
>> ?? X[100, , drop = FALSE] %*% theta - (120 - eps)
>> }
>>
>> #Optimization with nloptr
>>
>> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
>> # -0.2186159 -0.5032066? 6.4458823 -0.4125948
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @teven@|onzoe|||@ @end|ng |rom gm@||@com  Fri Dec 13 16:35:45 2024
From: @teven@|onzoe|||@ @end|ng |rom gm@||@com (Steven Ellis)
Date: Fri, 13 Dec 2024 10:35:45 -0500
Subject: [R] Confirmation of no Electronic Communication functionality
In-Reply-To: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
References: <SA1PR06MB794417F60CA2413E7CEC63A5C0382@SA1PR06MB7944.namprd06.prod.outlook.com>
Message-ID: <CAHOiV1pjmfw6iikMTFVKQFy4ts9kfScb1Vo91J52ch_TUMyhNQ@mail.gmail.com>

Hi Edward,

R can indeed initiate electronic communication, e.g. by downloading new
libraries (`install.packages()`). You may be best off using a container.

Steven

On Fri, Dec 13, 2024, 10:25?AM Edward Woo via R-help <r-help at r-project.org>
wrote:

> Hi,
>
> I am trying to download R version 3.6.1 onto my company computer and need
> an email confirming that r does not include any electronic communication
> functionality within the program.
>
> Thank you very much!
>
> Best,
> Edward Woo
>
>
> Edward Woo
> Municipals Securities Group
> Jefferies LLC
> 520 Madison Avenue, 3rd Floor
> New York, NY 10022
> Phone: 212-708-2958
> ewoo at jefferies.com<mailto:ewoo at jefferies.com>
>
>
> Jefferies archives and monitors outgoing and incoming e-mail. The contents
> of this email, including any attachments, are confidential to the ordinary
> user of the email address to which it was addressed. If you are not the
> addressee of this email you may not copy, forward, disclose or otherwise
> use it or any part of it in any form whatsoever. This email may be produced
> at the request of regulators or in connection with civil litigation.
> Jefferies accepts no liability for any errors or omissions arising as a
> result of transmission. Use by other than intended recipients is prohibited.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |vo@we|ch @end|ng |rom uc|@@edu  Fri Dec 13 20:21:13 2024
From: |vo@we|ch @end|ng |rom uc|@@edu (ivo welch)
Date: Fri, 13 Dec 2024 11:21:13 -0800
Subject: [R] Weird Behavior of mean
In-Reply-To: <26459.64652.135529.538036@stat.math.ethz.ch>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
 <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
 <26459.64652.135529.538036@stat.math.ethz.ch>
Message-ID: <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>

isn't this still a little R buglet?  I have overwritten T (even if my
schuld [franconian], it is not that uncommon an error, because T is also a
common abbreviation for the end of a time series; namespace pollution in R
can be quite annoying, even though I understand that it is convenient in
interactive mode).  Nevertheless, I am passing into mean() a positive
number for na.rm, and by definition, a positive number still means TRUE.
 besides, sd() and mean() should probably treat this similarly, anyway.  I
do see the argument that functions cannot be proof against redefinitions of
all sorts of objects that they can use.    more philosophically, some
variables should not be overwritable, or at least trigger a warning.

As Dante wrote, Abandon all hope ye who enter R.

--
Ivo Welch (ivo.welch at ucla.edu)

	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Dec 13 20:38:58 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Fri, 13 Dec 2024 14:38:58 -0500
Subject: [R] [off-topic] crossword
In-Reply-To: <20241213155126.20a2efbd7633df2d08fb4925@univ-nantes.fr>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
 <20241213155126.20a2efbd7633df2d08fb4925@univ-nantes.fr>
Message-ID: <004f01db4d96$a7b51fe0$f71f5fa0$@gmail.com>

Since this is a discussion about a specific crossword puzzle the right answer has to fit with any words coming in from the other direction or it gets cross.

I thought the clue hinted it started with C and ended with R and that the languages were chosen for no reason other than that they helped make a clue. It would otherwise be equally valid to choose COBOL and RUST. This has nothing specific about R, or C, for that matter. Anyone who writes code for computers in any language can be called a CODER.

But since CODES and CODER and many other words like PRINT may make sense, it still can be necessary to have it fit the crossword puzzle. Since it mentioned R and not it's cousin S, I think CODER is more likely the answer than CODES.

Not that it changes our lives in the slightest way. I suspect people who are dedicated cruciverbalists need not know anything about the C and R languages or even programming in general. They are supposed to figure out it is an ODE between C and R.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Olivier Crouzet
Sent: Friday, December 13, 2024 9:51 AM
To: r-help at r-project.org
Subject: Re: [R] [off-topic] crossword

Thank you all for the helpful and enlightening comments. One question
though, isn't "say" a synonym in oral forms of american english for
"for example"? Which would translate to:

> Writes in C or R, [for example]. 

which would involve that C and R are possible examples of the usage
contexts considered here in which someone would "write"?

This would then make perfect sense to me for the proposed answer:
"codes".

Yours.
Olivier.


On Fri, 13 Dec 2024
08:02:32 +0000 CALUM POLWART <polc1410 at gmail.com> wrote:

> Well to complicate things, I don't think RULES is the answer.
> 
> This is a cryptic crossword clue. They usually contain the answer
> twice (well... Cryptically!!)
> 
> Writes in C or R, say.
> 
> I think the answer is CODER
> 
> If you look up the definition of say in the dictionary one option is:
> 
> 
> 
>    1. give instructions to or direct somebody to do something with
>    authority (verb)
> 
> 
> That's the simple part of the clue. (Notice the comma cryptic clues
> have two parts giving the "same" answer)
> 
> The more complex part I think is  that and 'ode' (a poem that is
> written like it is said or something) is written in between C and R
> giving C ODE R,
> 
> ...
> 
> 
> Very happy to be corrected...
> 
> 
> (Oh and as a third part a coder writes in C or R... I hope the
> JavaScript kids are listening ;-) )
> 
> On Fri, 13 Dec 2024, 04:26 Ebert,Timothy Aaron, <tebert at ufl.edu>
> wrote:
> 
> > I do not understand the question and I do not understand the answer.
> > Possibly one confounds the other.
> >
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin
> > Hodgess Sent: Thursday, December 12, 2024 11:56 AM
> > To: Bill Dunlap <williamwdunlap at gmail.com>
> > Cc: r-help at R-project.org
> > Subject: Re: [R] [off-topic] crossword
> >
> > [External Email]
> >
> > RULES!
> >
> >
> > Erin Hodgess, PhD
> > mailto: erinm.hodgess at gmail.com
> >
> >
> > On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap
> > <williamwdunlap at gmail.com> wrote:
> >
> > > The New York Times crossword this morning had the clue (51 down, 5
> > > letters) "Writes in C or R, say".
> > >
> > > -Bill
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat/
> > > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> > > %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
> > > %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> > > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> > > %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
> > > c%3D&reserved=0
> > > PLEASE do read the posting guide
> > > https://www/.
> > > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C9
> > > 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> > > %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> > > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> > > 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
> > > eserved=0 and provide commented, minimal, self-contained,
> > > reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


-- 
  Olivier Crouzet, PhD
  http://olivier.ghostinthemachine.space
  /Ma?tre de Conf?rences/
  @LLING - Laboratoire de Linguistique de Nantes
    UMR6310 CNRS / Universit? de Nantes

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Fri Dec 13 20:42:51 2024
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Fri, 13 Dec 2024 14:42:51 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <0c195643-1416-4069-9c46-5edfe895c432@gmail.com>
References: <0c195643-1416-4069-9c46-5edfe895c432@gmail.com>
Message-ID: <2224905d-24a0-4520-80a0-959148ce5c84@gmail.com>

Setting penalty scales si, se at 1e+4 gets results somewhat near the alabama results.

The problem seems quite sensitive to the constraint.

JN


-------- Forwarded Message --------
Subject: Re: [R] Non linear optimization with nloptr package fail to produce true optimal result
Date: Fri, 13 Dec 2024 14:30:03 -0500
From: J C Nash <profjcnash at gmail.com>
To: r-help at r-project.org

The following may or may not be relevant, but definitely getting somewhat different results.
As this was a quick and dirty try while having a snack, it may have bugs.

# Lobo2412.R  -- from R Help 20241213

#Original artificial data

library(optimx)
library(nloptr)
library(alabama)

set.seed(1)
A <- 1.34
B <- 0.5673
C <- 6.356
D <- -1.234
x <- seq(0.5, 20, length.out = 500)
y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)

#Objective function

X <- cbind(1, x, x^2, log(x))
flobo <- function(theta) {
sum(abs(X %*% theta - y))
}

#Constraint

eps <- 1e-4

hinlobo <- function(theta) {
   abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps # ?? weird! (1e-4 - 1e-3)
}

Hxlobo <- function(theta) {
   X[100, , drop = FALSE] %*% theta - (120 - eps) # ditto -- also constant
}

conobj<-function(tt){
    ob <- flobo(tt)
    ci <- hinlobo(tt)
    if (ci > 0) {ci <- 0}
    ce <- Hxlobo(tt)
    si<-1; se<-1
    val<-ob+si*ci^2+se*ce^2
    cat("f, ci, ce,ob,val:"," ",ci," ",ce," ",ob," ",val," at "); print(tt)
    val
}

t0<-rep(0,4)
conobj(t0)
t1 <- c(2.02, 6.764, 6.186, -20.095)
conobj(t1)
t2 <- c( -0.2186159, -0.5032066,  6.4458823, -0.4125948)
conobj(t2)


solo<-optimr(t0, conobj, gr="grcentral", method="anms", control=list(trace=1))
solo
conobj(solo$par)
#Optimization with nloptr

# Sol = nloptr::auglag(t0, flobo, eval_g_ineq = hinlobo, eval_g_eq = Hxlobo, opts =
# list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8, print_level=1))
# -0.2186159 -0.5032066  6.4458823 -0.4125948

sol <- auglag(par=t0, fn=flobo, hin=hinlobo, heq=Hxlobo, control.outer=list(trace=TRUE))
sol

#==================================

J Nash

On 2024-12-13 13:45, Duncan Murdoch wrote:
> You posted a version of this question on StackOverflow, and were given advice there that you ignored.
> 
> nloptr() clearly indicates that it is quitting without reaching an optimum, but you are hiding that message.? Don't do 
> that.
> 
> Duncan Murdoch
> 
> On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
>> library(nloptr)
>>
>> set.seed(1)
>> A <- 1.34
>> B <- 0.5673
>> C <- 6.356
>> D <- -1.234
>> x <- seq(0.5, 20, length.out = 500)
>> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
>>
>> #Objective function
>>
>> X <- cbind(1, x, x^2, log(x))
>> f <- function(theta) {
>> sum(abs(X %*% theta - y))
>> }
>>
>> #Constraint
>>
>> eps <- 1e-4
>>
>> hin <- function(theta) {
>> ?? abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
>> }
>>
>> Hx <- function(theta) {
>> ?? X[100, , drop = FALSE] %*% theta - (120 - eps)
>> }
>>
>> #Optimization with nloptr
>>
>> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
>> # -0.2186159 -0.5032066? 6.4458823 -0.4125948
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j|ox @end|ng |rom mcm@@ter@c@  Fri Dec 13 20:44:10 2024
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 13 Dec 2024 14:44:10 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
Message-ID: <40fa65f7-e98e-436a-99f3-f9d07b08eb8c@mcmaster.ca>

Dear Daniel et al.,

Following on Duncan's remark and examining the message produced by 
nloptr(), I simply tried increasing the maximum number of function 
evaluations:
------ snip -------

 > nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
+          list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8,
+               maxeval = 1e5)
+ )

Call:

nloptr(x0 = rep(0, 4), eval_f = f, eval_g_ineq = hin, eval_g_eq = Hx,
     opts = list(algorithm = "NLOPT_LN_COBYLA", xtol_rel = 1e-08,
         maxeval = 1e+05))


Minimization using NLopt version 2.7.1

NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped
because xtol_rel or xtol_abs (above) was reached. )

Number of Iterations....: 46317
Termination conditions:  xtol_rel: 1e-08	maxeval: 1e+05
Number of inequality constraints:  1
Number of equality constraints:    1
Optimal value of objective function:  1287.71725107671
Optimal value of controls: 1.576708 6.456606 6.195305 -19.008

---------- snip ----------

That produces a solution closer to, and better than, the one that you 
suggested (which you obtained how?):

 > f(c(0.222, 6.999, 6.17, -19.371))
[1] 1325.076

I hope this helps,
  John
-- 
John Fox, Professor Emeritus
McMaster University
Hamilton, Ontario, Canada
web: https://www.john-fox.ca/
--
On 2024-12-13 1:45 p.m., Duncan Murdoch wrote:
> Caution: External email.
> 
> 
> You posted a version of this question on StackOverflow, and were given
> advice there that you ignored.
> 
> nloptr() clearly indicates that it is quitting without reaching an
> optimum, but you are hiding that message.? Don't do that.
> 
> Duncan Murdoch
> 
> On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
>> library(nloptr)
>>
>> set.seed(1)
>> A <- 1.34
>> B <- 0.5673
>> C <- 6.356
>> D <- -1.234
>> x <- seq(0.5, 20, length.out = 500)
>> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
>>
>> #Objective function
>>
>> X <- cbind(1, x, x^2, log(x))
>> f <- function(theta) {
>> sum(abs(X %*% theta - y))
>> }
>>
>> #Constraint
>>
>> eps <- 1e-4
>>
>> hin <- function(theta) {
>> ?? abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
>> }
>>
>> Hx <- function(theta) {
>> ?? X[100, , drop = FALSE] %*% theta - (120 - eps)
>> }
>>
>> #Optimization with nloptr
>>
>> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
>> # -0.2186159 -0.5032066? 6.4458823 -0.4125948
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Fri Dec 13 20:51:01 2024
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Fri, 13 Dec 2024 14:51:01 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <40fa65f7-e98e-436a-99f3-f9d07b08eb8c@mcmaster.ca>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
 <40fa65f7-e98e-436a-99f3-f9d07b08eb8c@mcmaster.ca>
Message-ID: <50219f52-0a01-400f-b7e3-7275ff2f1278@gmail.com>

Interesting that alabama and nloptr both use auglag but alabama gets a lower objective fn.
I think there could be lots of exploration of controls and settings to play with to find out
what is going on.


alabama::auglag
f, ci, ce,ob,val:   0   -4.71486e-08   1029.77   1029.77  at [1]  -0.610594   4.307408   6.254267 -11.919881
 > solfox<-c( 1.576708, 6.456606, 6.195305, -19.008 )
 > conobj(solfox)
f, ci, ce,ob,val:   0   -1.031085e-05   1287.707   1287.707  at [1]   1.576708   6.456606   6.195305 -19.008000
          [,1]
[1,] 1287.707

ci is the inequality constraint fn, ce the equality one, ob is the raw objective, val the penalized one, then the 4 
parameters.

JN


On 2024-12-13 14:44, John Fox wrote:
> Dear Daniel et al.,
> 
> Following on Duncan's remark and examining the message produced by nloptr(), I simply tried increasing the maximum 
> number of function evaluations:
> ------ snip -------
> 
>  > nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> +????????? list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8,
> +?????????????? maxeval = 1e5)
> + )
> 
> Call:
> 
> nloptr(x0 = rep(0, 4), eval_f = f, eval_g_ineq = hin, eval_g_eq = Hx,
>  ??? opts = list(algorithm = "NLOPT_LN_COBYLA", xtol_rel = 1e-08,
>  ??????? maxeval = 1e+05))
> 
> 
> Minimization using NLopt version 2.7.1
> 
> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped
> because xtol_rel or xtol_abs (above) was reached. )
> 
> Number of Iterations....: 46317
> Termination conditions:? xtol_rel: 1e-08??? maxeval: 1e+05
> Number of inequality constraints:? 1
> Number of equality constraints:??? 1
> Optimal value of objective function:? 1287.71725107671
> Optimal value of controls: 1.576708 6.456606 6.195305 -19.008
> 
> ---------- snip ----------
> 
> That produces a solution closer to, and better than, the one that you suggested (which you obtained how?):
> 
>  > f(c(0.222, 6.999, 6.17, -19.371))
> [1] 1325.076
> 
> I hope this helps,
>  ?John


From d@n|e|obo9976 @end|ng |rom gm@||@com  Fri Dec 13 20:51:47 2024
From: d@n|e|obo9976 @end|ng |rom gm@||@com (Daniel Lobo)
Date: Sat, 14 Dec 2024 01:21:47 +0530
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <40fa65f7-e98e-436a-99f3-f9d07b08eb8c@mcmaster.ca>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
 <40fa65f7-e98e-436a-99f3-f9d07b08eb8c@mcmaster.ca>
Message-ID: <CADZb7ho=4_gF4cGMNWp-f8SDi9uMzC3CKhOStXZ3WbDOzxgBqA@mail.gmail.com>

Looks like the solution 1.576708   6.456606   6.195305 -19.007996 is
the best solution that nloptr can produce by increasing the iteration
numbers.

The better set of solution is obtained using pracma package.

On Sat, 14 Dec 2024 at 01:14, John Fox <jfox at mcmaster.ca> wrote:
>
> Dear Daniel et al.,
>
> Following on Duncan's remark and examining the message produced by
> nloptr(), I simply tried increasing the maximum number of function
> evaluations:
> ------ snip -------
>
>  > nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> +          list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8,
> +               maxeval = 1e5)
> + )
>
> Call:
>
> nloptr(x0 = rep(0, 4), eval_f = f, eval_g_ineq = hin, eval_g_eq = Hx,
>      opts = list(algorithm = "NLOPT_LN_COBYLA", xtol_rel = 1e-08,
>          maxeval = 1e+05))
>
>
> Minimization using NLopt version 2.7.1
>
> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped
> because xtol_rel or xtol_abs (above) was reached. )
>
> Number of Iterations....: 46317
> Termination conditions:  xtol_rel: 1e-08        maxeval: 1e+05
> Number of inequality constraints:  1
> Number of equality constraints:    1
> Optimal value of objective function:  1287.71725107671
> Optimal value of controls: 1.576708 6.456606 6.195305 -19.008
>
> ---------- snip ----------
>
> That produces a solution closer to, and better than, the one that you
> suggested (which you obtained how?):
>
>  > f(c(0.222, 6.999, 6.17, -19.371))
> [1] 1325.076
>
> I hope this helps,
>   John
> --
> John Fox, Professor Emeritus
> McMaster University
> Hamilton, Ontario, Canada
> web: https://www.john-fox.ca/
> --
> On 2024-12-13 1:45 p.m., Duncan Murdoch wrote:
> > Caution: External email.
> >
> >
> > You posted a version of this question on StackOverflow, and were given
> > advice there that you ignored.
> >
> > nloptr() clearly indicates that it is quitting without reaching an
> > optimum, but you are hiding that message.  Don't do that.
> >
> > Duncan Murdoch
> >
> > On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
> >> library(nloptr)
> >>
> >> set.seed(1)
> >> A <- 1.34
> >> B <- 0.5673
> >> C <- 6.356
> >> D <- -1.234
> >> x <- seq(0.5, 20, length.out = 500)
> >> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
> >>
> >> #Objective function
> >>
> >> X <- cbind(1, x, x^2, log(x))
> >> f <- function(theta) {
> >> sum(abs(X %*% theta - y))
> >> }
> >>
> >> #Constraint
> >>
> >> eps <- 1e-4
> >>
> >> hin <- function(theta) {
> >>    abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
> >> }
> >>
> >> Hx <- function(theta) {
> >>    X[100, , drop = FALSE] %*% theta - (120 - eps)
> >> }
> >>
> >> #Optimization with nloptr
> >>
> >> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> >> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
> >> # -0.2186159 -0.5032066  6.4458823 -0.4125948
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Dec 13 21:13:19 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Fri, 13 Dec 2024 15:13:19 -0500
Subject: [R] [off-topic] crossword
In-Reply-To: <CACxE24=A-fJhy8Y3fEfmJu5K2Ass_PMNO9Wy3qP0xGuyEgKCKw@mail.gmail.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
 <CACxE24=A-fJhy8Y3fEfmJu5K2Ass_PMNO9Wy3qP0xGuyEgKCKw@mail.gmail.com>
Message-ID: <007401db4d9b$73e049b0$5ba0dd10$@gmail.com>

I hesitate to say this and stray back on topic.

R is basically an extension of C as it was written in C and many added functions end up being rewritten in a variant of C, albeit other languages may at times intrude. Something similar could be said of C-Python. 

But I truly doubt the ones making this puzzle know or care. If they eventually want to make some new puzzle, they can search for a list of all programming languages invented and find stuff like here:

https://en.wikipedia.org/wiki/List_of_programming_languages

I have studied lots of languages but this list is daunting. It does have quite a few single (or close) character language names the cruciverbalist can use.

There was an A+ it seems and a B, and besides C, quite a few variants like C++, C--, C*. C#, and there was a D, and an E, and not only an F but an F# and F*.

There was no G but there is a GO, and there is not only a J but a sharp and ++ version, and a K but no L nor even a Noel.

There is an M, of sorts and variants and a P and variants and there is a Q.

We then get to R and there is also an R++, whatever that is. Given R does not support ++, I wonder.

And, of course, an assortment of S, and a T and even a V and oddly an X++ and Z++.

So, someone who knows nothing about programming has a number of games they can play just using the list of languages they can search for. R was not chosen as something they had heard of, just as a letter of the alphabet that would help them make a puzzle. Neither, necessarily, was C.

All this leads to a question.

S was supposed to mean something like Statistics and was not public domain. When R came along as a public language, I thing someone considered it as S-- meaning S sripped of something and obviously that became the letter before S, meaning R.

What if we wanted a spin off of R, perhaps a more lightweight version, or one where the Tidyverse was part of the base? The obvious (to maybe just me) idea would be that R-- -> Q, right?

But as Q is already taken, it may never happen.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
Sent: Friday, December 13, 2024 10:06 AM
To: CALUM POLWART <polc1410 at gmail.com>
Cc: r-help at r-project.org
Subject: Re: [R] [off-topic] crossword

RULES means that anyone who uses C or R RULES the Universe.  They just do.

Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Fri, Dec 13, 2024 at 1:02?AM CALUM POLWART <polc1410 at gmail.com> wrote:

> Well to complicate things, I don't think RULES is the answer.
>
> This is a cryptic crossword clue. They usually contain the answer twice
> (well... Cryptically!!)
>
> Writes in C or R, say.
>
> I think the answer is CODER
>
> If you look up the definition of say in the dictionary one option is:
>
>
>
>    1. give instructions to or direct somebody to do something with
>    authority (verb)
>
>
> That's the simple part of the clue. (Notice the comma cryptic clues have
> two parts giving the "same" answer)
>
> The more complex part I think is  that and 'ode' (a poem that is written
> like it is said or something) is written in between C and R  giving C ODE
> R,
>
> ...
>
>
> Very happy to be corrected...
>
>
> (Oh and as a third part a coder writes in C or R... I hope the JavaScript
> kids are listening ;-) )
>
> On Fri, 13 Dec 2024, 04:26 Ebert,Timothy Aaron, <tebert at ufl.edu> wrote:
>
>> I do not understand the question and I do not understand the answer.
>> Possibly one confounds the other.
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin Hodgess
>> Sent: Thursday, December 12, 2024 11:56 AM
>> To: Bill Dunlap <williamwdunlap at gmail.com>
>> Cc: r-help at R-project.org
>> Subject: Re: [R] [off-topic] crossword
>>
>> [External Email]
>>
>> RULES!
>>
>>
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>
>> On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap <williamwdunlap at gmail.com>
>> wrote:
>>
>> > The New York Times crossword this morning had the clue (51 down, 5
>> > letters) "Writes in C or R, say".
>> >
>> > -Bill
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat/
>> > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
>> > %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
>> > %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
>> > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
>> > %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
>> > c%3D&reserved=0
>> > PLEASE do read the posting guide
>> > https://www/.
>> > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C9
>> > 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
>> > %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
>> > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
>> > 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
>> > eserved=0 and provide commented, minimal, self-contained, reproducible
>> > code.
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.r-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Dec 13 21:22:12 2024
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 13 Dec 2024 12:22:12 -0800
Subject: [R] Weird Behavior of mean
In-Reply-To: <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
 <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
 <26459.64652.135529.538036@stat.math.ethz.ch>
 <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>
Message-ID: <229162EB-88D8-4445-B486-E0EE6DA675BC@dcn.davis.ca.us>

This was documented in [1] forever ago. I would not miss it if a future version of R chose to remove those variables.

[1] The R Inferno, 8.1.32

On December 13, 2024 11:21:13 AM PST, ivo welch <ivo.welch at ucla.edu> wrote:
>isn't this still a little R buglet?  I have overwritten T (even if my
>schuld [franconian], it is not that uncommon an error, because T is also a
>common abbreviation for the end of a time series; namespace pollution in R
>can be quite annoying, even though I understand that it is convenient in
>interactive mode).  Nevertheless, I am passing into mean() a positive
>number for na.rm, and by definition, a positive number still means TRUE.
> besides, sd() and mean() should probably treat this similarly, anyway.  I
>do see the argument that functions cannot be proof against redefinitions of
>all sorts of objects that they can use.    more philosophically, some
>variables should not be overwritable, or at least trigger a warning.
>
>As Dante wrote, Abandon all hope ye who enter R.
>
>--
>Ivo Welch (ivo.welch at ucla.edu)
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From d@n|e|obo9976 @end|ng |rom gm@||@com  Fri Dec 13 19:55:43 2024
From: d@n|e|obo9976 @end|ng |rom gm@||@com (Daniel Lobo)
Date: Sat, 14 Dec 2024 00:25:43 +0530
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
Message-ID: <CADZb7hpi55JqjOD9Fr_v1J=HOTTmmmT4VqkdKaOPR+qhO8RYXg@mail.gmail.com>

Hi Duncan,

I take your advice.

I posted here in search for a better answer to my problem as I could
not get that there.

My question is:
1. Why nloptr() is failing where other programs can continue with the
same set of data, numbers, and constraints?
2. Is this enough ground to say that nloptr is inferior and user
should not use this in complex problems?

I wish to get a thoughtful answer to above as my working environment
only has the nloptr package installed, and it is an isolated system
due to security issues and installation of a new package requires lot
lot of approvals and time consuming.

BTW, if someone interested here is my original post
https://stackoverflow.com/a/79271318/15910619

On Sat, 14 Dec 2024 at 00:15, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> You posted a version of this question on StackOverflow, and were given
> advice there that you ignored.
>
> nloptr() clearly indicates that it is quitting without reaching an
> optimum, but you are hiding that message.  Don't do that.
>
> Duncan Murdoch
>
> On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
> > library(nloptr)
> >
> > set.seed(1)
> > A <- 1.34
> > B <- 0.5673
> > C <- 6.356
> > D <- -1.234
> > x <- seq(0.5, 20, length.out = 500)
> > y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
> >
> > #Objective function
> >
> > X <- cbind(1, x, x^2, log(x))
> > f <- function(theta) {
> > sum(abs(X %*% theta - y))
> > }
> >
> > #Constraint
> >
> > eps <- 1e-4
> >
> > hin <- function(theta) {
> >    abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
> > }
> >
> > Hx <- function(theta) {
> >    X[100, , drop = FALSE] %*% theta - (120 - eps)
> > }
> >
> > #Optimization with nloptr
> >
> > Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> > list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
> > # -0.2186159 -0.5032066  6.4458823 -0.4125948
>


From w||||@mwdun|@p @end|ng |rom gm@||@com  Fri Dec 13 21:47:03 2024
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Fri, 13 Dec 2024 12:47:03 -0800
Subject: [R] [off-topic] crossword
In-Reply-To: <004f01db4d96$a7b51fe0$f71f5fa0$@gmail.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
 <20241213155126.20a2efbd7633df2d08fb4925@univ-nantes.fr>
 <004f01db4d96$a7b51fe0$f71f5fa0$@gmail.com>
Message-ID: <CAHqSRuQEXxi_nTZ3XqgM3MuGMSwfNShJEp42WvEpphkNKcmLKQ@mail.gmail.com>

Crossword answers have to be drop-in replacements for the clue in a
sentence.  Hence replacing
   "She writes in C and R, say."
with
   "She codes"
would work, but "She coder" would not.

(If one interpreted C and R as the names of third party candidates for
office, then "She votes" would work,
but the across words made "codes" a more reasonable answer.)

I was impressed that they expected the typical NYT reader to know that R
was a programming language.

-Bill

On Fri, Dec 13, 2024 at 11:40?AM <avi.e.gross at gmail.com> wrote:

> Since this is a discussion about a specific crossword puzzle the right
> answer has to fit with any words coming in from the other direction or it
> gets cross.
>
> I thought the clue hinted it started with C and ended with R and that the
> languages were chosen for no reason other than that they helped make a
> clue. It would otherwise be equally valid to choose COBOL and RUST. This
> has nothing specific about R, or C, for that matter. Anyone who writes code
> for computers in any language can be called a CODER.
>
> But since CODES and CODER and many other words like PRINT may make sense,
> it still can be necessary to have it fit the crossword puzzle. Since it
> mentioned R and not it's cousin S, I think CODER is more likely the answer
> than CODES.
>
> Not that it changes our lives in the slightest way. I suspect people who
> are dedicated cruciverbalists need not know anything about the C and R
> languages or even programming in general. They are supposed to figure out
> it is an ODE between C and R.
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Olivier Crouzet
> Sent: Friday, December 13, 2024 9:51 AM
> To: r-help at r-project.org
> Subject: Re: [R] [off-topic] crossword
>
> Thank you all for the helpful and enlightening comments. One question
> though, isn't "say" a synonym in oral forms of american english for
> "for example"? Which would translate to:
>
> > Writes in C or R, [for example].
>
> which would involve that C and R are possible examples of the usage
> contexts considered here in which someone would "write"?
>
> This would then make perfect sense to me for the proposed answer:
> "codes".
>
> Yours.
> Olivier.
>
>
> On Fri, 13 Dec 2024
> 08:02:32 +0000 CALUM POLWART <polc1410 at gmail.com> wrote:
>
> > Well to complicate things, I don't think RULES is the answer.
> >
> > This is a cryptic crossword clue. They usually contain the answer
> > twice (well... Cryptically!!)
> >
> > Writes in C or R, say.
> >
> > I think the answer is CODER
> >
> > If you look up the definition of say in the dictionary one option is:
> >
> >
> >
> >    1. give instructions to or direct somebody to do something with
> >    authority (verb)
> >
> >
> > That's the simple part of the clue. (Notice the comma cryptic clues
> > have two parts giving the "same" answer)
> >
> > The more complex part I think is  that and 'ode' (a poem that is
> > written like it is said or something) is written in between C and R
> > giving C ODE R,
> >
> > ...
> >
> >
> > Very happy to be corrected...
> >
> >
> > (Oh and as a third part a coder writes in C or R... I hope the
> > JavaScript kids are listening ;-) )
> >
> > On Fri, 13 Dec 2024, 04:26 Ebert,Timothy Aaron, <tebert at ufl.edu>
> > wrote:
> >
> > > I do not understand the question and I do not understand the answer.
> > > Possibly one confounds the other.
> > >
> > > -----Original Message-----
> > > From: R-help <r-help-bounces at r-project.org> On Behalf Of Erin
> > > Hodgess Sent: Thursday, December 12, 2024 11:56 AM
> > > To: Bill Dunlap <williamwdunlap at gmail.com>
> > > Cc: r-help at R-project.org
> > > Subject: Re: [R] [off-topic] crossword
> > >
> > > [External Email]
> > >
> > > RULES!
> > >
> > >
> > > Erin Hodgess, PhD
> > > mailto: erinm.hodgess at gmail.com
> > >
> > >
> > > On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap
> > > <williamwdunlap at gmail.com> wrote:
> > >
> > > > The New York Times crossword this morning had the clue (51 down, 5
> > > > letters) "Writes in C or R, say".
> > > >
> > > > -Bill
> > > >
> > > >         [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat/
> > > > .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%
> 40ufl.edu
> > > >
> %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
> > > >
> %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> > > >
> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> > > >
> %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
> > > > c%3D&reserved=0
> > > > PLEASE do read the posting guide
> > > > https://www/.
> > > > r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu
> %7C9
> > > >
> 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> > > >
> %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> > > >
> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> > > >
> 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
> > > > eserved=0 and provide commented, minimal, self-contained,
> > > > reproducible code.
> > > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > https://www.r-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > https://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > https://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
>
> --
>   Olivier Crouzet, PhD
>   http://olivier.ghostinthemachine.space
>   /Ma?tre de Conf?rences/
>   @LLING - Laboratoire de Linguistique de Nantes
>     UMR6310 CNRS / Universit? de Nantes
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Fri Dec 13 21:48:27 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 13 Dec 2024 15:48:27 -0500
Subject: [R] Weird Behavior of mean
In-Reply-To: <229162EB-88D8-4445-B486-E0EE6DA675BC@dcn.davis.ca.us>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
 <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
 <26459.64652.135529.538036@stat.math.ethz.ch>
 <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>
 <229162EB-88D8-4445-B486-E0EE6DA675BC@dcn.davis.ca.us>
Message-ID: <CABghstTqtFp-EMX0udqpomTeeeaDnsvdO3YrXXXfRT9t+UaF-Q@mail.gmail.com>

  This example is a little more subtle than that; the OP knows about
name masking, but was expecting T to be coerced to logical, which
would ordinarily be a reasonable expectation (IMO) ...

On Fri, Dec 13, 2024 at 3:40?PM Jeff Newmiller via R-help
<r-help at r-project.org> wrote:
>
> This was documented in [1] forever ago. I would not miss it if a future version of R chose to remove those variables.
>
> [1] The R Inferno, 8.1.32
>
> On December 13, 2024 11:21:13 AM PST, ivo welch <ivo.welch at ucla.edu> wrote:
> >isn't this still a little R buglet?  I have overwritten T (even if my
> >schuld [franconian], it is not that uncommon an error, because T is also a
> >common abbreviation for the end of a time series; namespace pollution in R
> >can be quite annoying, even though I understand that it is convenient in
> >interactive mode).  Nevertheless, I am passing into mean() a positive
> >number for na.rm, and by definition, a positive number still means TRUE.
> > besides, sd() and mean() should probably treat this similarly, anyway.  I
> >do see the argument that functions cannot be proof against redefinitions of
> >all sorts of objects that they can use.    more philosophically, some
> >variables should not be overwritable, or at least trigger a warning.
> >
> >As Dante wrote, Abandon all hope ye who enter R.
> >
> >--
> >Ivo Welch (ivo.welch at ucla.edu)
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bbo|ker @end|ng |rom gm@||@com  Fri Dec 13 21:52:52 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 13 Dec 2024 15:52:52 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <CADZb7hpi55JqjOD9Fr_v1J=HOTTmmmT4VqkdKaOPR+qhO8RYXg@mail.gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
 <CADZb7hpi55JqjOD9Fr_v1J=HOTTmmmT4VqkdKaOPR+qhO8RYXg@mail.gmail.com>
Message-ID: <CABghstSOBRHt8Ufw78+-xUhFjo04QOTMeF4x2f6ZD1SWz0Lvmw@mail.gmail.com>

  It's a long way from "X works better than Y on this particular
problem" to "X is superior to Y".  It's a somewhat loose analogy, but
the 'no free lunch theorem'
<https://en.wikipedia.org/wiki/No_free_lunch_theorem> asserts that if
we consider a broad enough class of optimization problems, *no*
optimization algorithm is uniformly best ...

   That answers question #2 ("no"). Answering question #1 requires a
lot of work digging into the details of the problem, which I don't
have the time & energy to do right now ... someone who knows a lot
more about optimization *might* be able to answer based on previous
experience and background knowledge, but even they might have to spend
a lot of time digging ...


On Fri, Dec 13, 2024 at 3:46?PM Daniel Lobo <danielobo9976 at gmail.com> wrote:
>
> Hi Duncan,
>
> I take your advice.
>
> I posted here in search for a better answer to my problem as I could
> not get that there.
>
> My question is:
> 1. Why nloptr() is failing where other programs can continue with the
> same set of data, numbers, and constraints?
> 2. Is this enough ground to say that nloptr is inferior and user
> should not use this in complex problems?
>
> I wish to get a thoughtful answer to above as my working environment
> only has the nloptr package installed, and it is an isolated system
> due to security issues and installation of a new package requires lot
> lot of approvals and time consuming.
>
> BTW, if someone interested here is my original post
> https://stackoverflow.com/a/79271318/15910619
>
> On Sat, 14 Dec 2024 at 00:15, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >
> > You posted a version of this question on StackOverflow, and were given
> > advice there that you ignored.
> >
> > nloptr() clearly indicates that it is quitting without reaching an
> > optimum, but you are hiding that message.  Don't do that.
> >
> > Duncan Murdoch
> >
> > On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
> > > library(nloptr)
> > >
> > > set.seed(1)
> > > A <- 1.34
> > > B <- 0.5673
> > > C <- 6.356
> > > D <- -1.234
> > > x <- seq(0.5, 20, length.out = 500)
> > > y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
> > >
> > > #Objective function
> > >
> > > X <- cbind(1, x, x^2, log(x))
> > > f <- function(theta) {
> > > sum(abs(X %*% theta - y))
> > > }
> > >
> > > #Constraint
> > >
> > > eps <- 1e-4
> > >
> > > hin <- function(theta) {
> > >    abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
> > > }
> > >
> > > Hx <- function(theta) {
> > >    X[100, , drop = FALSE] %*% theta - (120 - eps)
> > > }
> > >
> > > #Optimization with nloptr
> > >
> > > Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
> > > list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
> > > # -0.2186159 -0.5032066  6.4458823 -0.4125948
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Fri Dec 13 22:01:34 2024
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Fri, 13 Dec 2024 16:01:34 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <CADZb7hpi55JqjOD9Fr_v1J=HOTTmmmT4VqkdKaOPR+qhO8RYXg@mail.gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
 <CADZb7hpi55JqjOD9Fr_v1J=HOTTmmmT4VqkdKaOPR+qhO8RYXg@mail.gmail.com>
Message-ID: <59da027a-e354-4849-a487-2d83937a685a@gmail.com>



On 2024-12-13 13:55, Daniel Lobo wrote:
> 1. Why nloptr() is failing where other programs can continue with the
> same set of data, numbers, and constraints?
> 2. Is this enough ground to say that nloptr is inferior and user
> should not use this in complex problems?

As I indicated in a recent response, nloptr has a lot of settings and possibilities.
It is VERY easy to choose poor values. I think Ravi Varadhan has probably done a lot
of work in alabama to set things up so the user has an easier time. I certainly did.
In fact I'm now getting complaints in your modified script when I try nloptr, but that's
likely some typo or other.

I substituted optim's Nelder-Mead for anms, and got same results essentially, so you
could run with just old-standard optim() which is built into R. (I even wrote the original
BASIC code in 1975.)

JN


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Dec 13 22:12:14 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Fri, 13 Dec 2024 16:12:14 -0500
Subject: [R] [off-topic] crossword
In-Reply-To: <CAHqSRuQEXxi_nTZ3XqgM3MuGMSwfNShJEp42WvEpphkNKcmLKQ@mail.gmail.com>
References: <CAHqSRuQ81PH1oZXj9ULy7jPkaBFTsN0NPx0xWu5NQDfZ893U6g@mail.gmail.com>
 <CACxE24mbyh5cOJHoO2ihC5fLjTLmfHq6j0cS3BZ03heCKSQvkQ@mail.gmail.com>
 <CH3PR22MB451457C1AEF97F7351FE7939CF382@CH3PR22MB4514.namprd22.prod.outlook.com>
 <CA+etgPmbxREJho8ZAN3ZUCVSwUJ+DVM=yoJs9axKA+GN2fipqA@mail.gmail.com>
 <20241213155126.20a2efbd7633df2d08fb4925@univ-nantes.fr>
 <004f01db4d96$a7b51fe0$f71f5fa0$@gmail.com>
 <CAHqSRuQEXxi_nTZ3XqgM3MuGMSwfNShJEp42WvEpphkNKcmLKQ@mail.gmail.com>
Message-ID: <009f01db4da3$aebe32b0$0c3a9810$@gmail.com>

Not to beat a dead horse, but if they wanted an answer of CODES, then why not use S instead of R in the ?

 

From: Bill Dunlap <williamwdunlap at gmail.com> 
Sent: Friday, December 13, 2024 3:47 PM
To: avi.e.gross at gmail.com
Cc: Olivier Crouzet <olivier.crouzet at univ-nantes.fr>; r-help at r-project.org
Subject: Re: [R] [off-topic] crossword

 

Crossword answers have to be drop-in replacements for the clue in a sentence.  Hence replacing

   "She writes in C and R, say."

with

   "She codes"

would work, but "She coder" would not.

 

(If one interpreted C and R as the names of third party candidates for office, then "She votes" would work,

but the across words made "codes" a more reasonable answer.)

 

I was impressed that they expected the typical NYT reader to know that R was a programming language.

 

-Bill

 

On Fri, Dec 13, 2024 at 11:40?AM <avi.e.gross at gmail.com <mailto:avi.e.gross at gmail.com> > wrote:

Since this is a discussion about a specific crossword puzzle the right answer has to fit with any words coming in from the other direction or it gets cross.

I thought the clue hinted it started with C and ended with R and that the languages were chosen for no reason other than that they helped make a clue. It would otherwise be equally valid to choose COBOL and RUST. This has nothing specific about R, or C, for that matter. Anyone who writes code for computers in any language can be called a CODER.

But since CODES and CODER and many other words like PRINT may make sense, it still can be necessary to have it fit the crossword puzzle. Since it mentioned R and not it's cousin S, I think CODER is more likely the answer than CODES.

Not that it changes our lives in the slightest way. I suspect people who are dedicated cruciverbalists need not know anything about the C and R languages or even programming in general. They are supposed to figure out it is an ODE between C and R.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org <mailto:r-help-bounces at r-project.org> > On Behalf Of Olivier Crouzet
Sent: Friday, December 13, 2024 9:51 AM
To: r-help at r-project.org <mailto:r-help at r-project.org> 
Subject: Re: [R] [off-topic] crossword

Thank you all for the helpful and enlightening comments. One question
though, isn't "say" a synonym in oral forms of american english for
"for example"? Which would translate to:

> Writes in C or R, [for example]. 

which would involve that C and R are possible examples of the usage
contexts considered here in which someone would "write"?

This would then make perfect sense to me for the proposed answer:
"codes".

Yours.
Olivier.


On Fri, 13 Dec 2024
08:02:32 +0000 CALUM POLWART <polc1410 at gmail.com <mailto:polc1410 at gmail.com> > wrote:

> Well to complicate things, I don't think RULES is the answer.
> 
> This is a cryptic crossword clue. They usually contain the answer
> twice (well... Cryptically!!)
> 
> Writes in C or R, say.
> 
> I think the answer is CODER
> 
> If you look up the definition of say in the dictionary one option is:
> 
> 
> 
>    1. give instructions to or direct somebody to do something with
>    authority (verb)
> 
> 
> That's the simple part of the clue. (Notice the comma cryptic clues
> have two parts giving the "same" answer)
> 
> The more complex part I think is  that and 'ode' (a poem that is
> written like it is said or something) is written in between C and R
> giving C ODE R,
> 
> ...
> 
> 
> Very happy to be corrected...
> 
> 
> (Oh and as a third part a coder writes in C or R... I hope the
> JavaScript kids are listening ;-) )
> 
> On Fri, 13 Dec 2024, 04:26 Ebert,Timothy Aaron, <tebert at ufl.edu <mailto:tebert at ufl.edu> >
> wrote:
> 
> > I do not understand the question and I do not understand the answer.
> > Possibly one confounds the other.
> >
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org <mailto:r-help-bounces at r-project.org> > On Behalf Of Erin
> > Hodgess Sent: Thursday, December 12, 2024 11:56 AM
> > To: Bill Dunlap <williamwdunlap at gmail.com <mailto:williamwdunlap at gmail.com> >
> > Cc: r-help at R-project.org <mailto:r-help at R-project.org> 
> > Subject: Re: [R] [off-topic] crossword
> >
> > [External Email]
> >
> > RULES!
> >
> >
> > Erin Hodgess, PhD
> > mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com> 
> >
> >
> > On Thu, Dec 12, 2024 at 9:42?AM Bill Dunlap
> > <williamwdunlap at gmail.com <mailto:williamwdunlap at gmail.com> > wrote:
> >
> > > The New York Times crossword this morning had the clue (51 down, 5
> > > letters) "Writes in C or R, say".
> > >
> > > -Bill
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat/
> > > .ethz.ch <http://ethz.ch> %2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu <http://40ufl.edu> 
> > > %7C9366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84
> > > %7C0%7C0%7C638696193817496836%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> > > OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> > > %3D%3D%7C0%7C%7C%7C&sdata=hhGilTFwNpgxXeDLO0HS7l4ofoCk%2FXuGhYx3QkuzFj
> > > c%3D&reserved=0
> > > PLEASE do read the posting guide
> > > https://www/.
> > > r-project.org <http://r-project.org> %2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu <http://40ufl.edu> %7C9
> > > 366be18e5a944b55dba08dd1acde487%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> > > %7C0%7C638696193817516516%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> > > dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> > > 3D%7C0%7C%7C%7C&sdata=Z7oKvEwSTzOCNXiMVlYtWtGTg30KADP8F09tQyv3fkA%3D&r
> > > eserved=0 and provide commented, minimal, self-contained,
> > > reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > ______________________________________________
> > R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
>       [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


-- 
  Olivier Crouzet, PhD
  http://olivier.ghostinthemachine.space
  /Ma?tre de Conf?rences/
  @LLING - Laboratoire de Linguistique de Nantes
    UMR6310 CNRS / Universit? de Nantes

______________________________________________
R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Dec 13 22:23:39 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Fri, 13 Dec 2024 16:23:39 -0500
Subject: [R] Weird Behavior of mean
In-Reply-To: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
Message-ID: <00af01db4da5$476c8b00$d645a100$@gmail.com>

Ivo,

To be a bit clearer, your problem was not with mean(0 but with any function
which is expecting a Boolean setting of TRUE/FALSE and instead got a
nonsense value of 20.

This gets even weirder in languages which automagcally transform many other
values to be considered true. Languages like C consider many non-zero values
to be true, including 20 as an integer. A language like python accepts a
very wide variety of things as truthy and the few that qualify as falsy are
empty strings, and zero as well as other empties like an empty list, empty
dictionary, empty set, empty range, or even a complex number with both a
zero real and imaginary part. 

Some see such designs as saving lots of labor and others suggest it would be
better and clearer programming practice to only accept explicit code that is
guaranteed to produce a Boolean such as is_empty(list) or whatever.

In this case, it is simply fairly poor design to alias T/F to TRUE/FALSE for
convenience and then have people inadvertently reuse the variables and
produce odd results. In languages which support constants that can be set
and protected from change, fine. But in R, I never use T/F as the longer
versions are clearer and less error prone.

I don't blame you for being teed off at why things where not working when T
was redefined.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Ivo Welch
Sent: Thursday, December 12, 2024 7:23 PM
To: r-help at r-project.org
Subject: [R] Weird Behavior of mean

Is the following a strange behavior for `mean` vs. `sd` ?

```
$ R --vanilla. ## 4.4.2
> x=c(NA,1,2,3)
> c( mean(x,na.rm=T), sd(x,na.rm=T) )
[1] 2 1
> T=20   ## bad idea for a parameter.  T is also used for TRUE
> c( mean(x,na.rm=T), sd(x,na.rm=T) )
[1] NA  1
>
```

This one was a baffler for me to track down for a few hours...

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From j|ox @end|ng |rom mcm@@ter@c@  Fri Dec 13 23:11:34 2024
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 13 Dec 2024 17:11:34 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <CADZb7ho=4_gF4cGMNWp-f8SDi9uMzC3CKhOStXZ3WbDOzxgBqA@mail.gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
 <40fa65f7-e98e-436a-99f3-f9d07b08eb8c@mcmaster.ca>
 <CADZb7ho=4_gF4cGMNWp-f8SDi9uMzC3CKhOStXZ3WbDOzxgBqA@mail.gmail.com>
Message-ID: <d3a84d15-be60-4128-b18e-7167e343aa14@mcmaster.ca>

Dear Daniel,

On 2024-12-13 2:51 p.m., Daniel Lobo wrote:
> Caution: External email.
> 
> 
> Looks like the solution 1.576708   6.456606   6.195305 -19.007996 is
> the best solution that nloptr can produce by increasing the iteration
> numbers.
> 
> The better set of solution is obtained using pracma package.

Not if I read the output correctly. As I showed, the result from 
pracma:fmincon() produces a larger value of the objective function than 
the result I obtained from nloptr().

John Nash (who is an expert on optimization -- I'm not) obtained an even 
lower value of the objective function from alabama::auglag().

As others have pointed out, one can't really draw general conclusions 
from a particular example, and like others, I don't have the time or 
inclination to figure out why your problem appears to be ill-conditioned 
(though note that the columns of X, excluding the constant, are highly 
correlated).

Best,
  John

> 
> On Sat, 14 Dec 2024 at 01:14, John Fox <jfox at mcmaster.ca> wrote:
>>
>> Dear Daniel et al.,
>>
>> Following on Duncan's remark and examining the message produced by
>> nloptr(), I simply tried increasing the maximum number of function
>> evaluations:
>> ------ snip -------
>>
>>   > nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>> +          list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8,
>> +               maxeval = 1e5)
>> + )
>>
>> Call:
>>
>> nloptr(x0 = rep(0, 4), eval_f = f, eval_g_ineq = hin, eval_g_eq = Hx,
>>       opts = list(algorithm = "NLOPT_LN_COBYLA", xtol_rel = 1e-08,
>>           maxeval = 1e+05))
>>
>>
>> Minimization using NLopt version 2.7.1
>>
>> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped
>> because xtol_rel or xtol_abs (above) was reached. )
>>
>> Number of Iterations....: 46317
>> Termination conditions:  xtol_rel: 1e-08        maxeval: 1e+05
>> Number of inequality constraints:  1
>> Number of equality constraints:    1
>> Optimal value of objective function:  1287.71725107671
>> Optimal value of controls: 1.576708 6.456606 6.195305 -19.008
>>
>> ---------- snip ----------
>>
>> That produces a solution closer to, and better than, the one that you
>> suggested (which you obtained how?):
>>
>>   > f(c(0.222, 6.999, 6.17, -19.371))
>> [1] 1325.076
>>
>> I hope this helps,
>>    John
>> --
>> John Fox, Professor Emeritus
>> McMaster University
>> Hamilton, Ontario, Canada
>> web: https://www.john-fox.ca/
>> --
>> On 2024-12-13 1:45 p.m., Duncan Murdoch wrote:
>>> Caution: External email.
>>>
>>>
>>> You posted a version of this question on StackOverflow, and were given
>>> advice there that you ignored.
>>>
>>> nloptr() clearly indicates that it is quitting without reaching an
>>> optimum, but you are hiding that message.  Don't do that.
>>>
>>> Duncan Murdoch
>>>
>>> On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
>>>> library(nloptr)
>>>>
>>>> set.seed(1)
>>>> A <- 1.34
>>>> B <- 0.5673
>>>> C <- 6.356
>>>> D <- -1.234
>>>> x <- seq(0.5, 20, length.out = 500)
>>>> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
>>>>
>>>> #Objective function
>>>>
>>>> X <- cbind(1, x, x^2, log(x))
>>>> f <- function(theta) {
>>>> sum(abs(X %*% theta - y))
>>>> }
>>>>
>>>> #Constraint
>>>>
>>>> eps <- 1e-4
>>>>
>>>> hin <- function(theta) {
>>>>     abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
>>>> }
>>>>
>>>> Hx <- function(theta) {
>>>>     X[100, , drop = FALSE] %*% theta - (120 - eps)
>>>> }
>>>>
>>>> #Optimization with nloptr
>>>>
>>>> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>>>> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
>>>> # -0.2186159 -0.5032066  6.4458823 -0.4125948
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide https://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri Dec 13 23:30:36 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 13 Dec 2024 17:30:36 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <d3a84d15-be60-4128-b18e-7167e343aa14@mcmaster.ca>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
 <40fa65f7-e98e-436a-99f3-f9d07b08eb8c@mcmaster.ca>
 <CADZb7ho=4_gF4cGMNWp-f8SDi9uMzC3CKhOStXZ3WbDOzxgBqA@mail.gmail.com>
 <d3a84d15-be60-4128-b18e-7167e343aa14@mcmaster.ca>
Message-ID: <327f0531-8be1-4074-b8a2-eede3d2aa619@gmail.com>

On 2024-12-13 5:11 p.m., John Fox wrote:
> Dear Daniel,
> 
> On 2024-12-13 2:51 p.m., Daniel Lobo wrote:
>> Caution: External email.
>>
>>
>> Looks like the solution 1.576708   6.456606   6.195305 -19.007996 is
>> the best solution that nloptr can produce by increasing the iteration
>> numbers.
>>
>> The better set of solution is obtained using pracma package.
> 
> Not if I read the output correctly. As I showed, the result from
> pracma:fmincon() produces a larger value of the objective function than
> the result I obtained from nloptr().
> 
> John Nash (who is an expert on optimization -- I'm not) obtained an even
> lower value of the objective function from alabama::auglag().
> 
> As others have pointed out, one can't really draw general conclusions
> from a particular example, and like others, I don't have the time or
> inclination to figure out why your problem appears to be ill-conditioned
> (though note that the columns of X, excluding the constant, are highly
> correlated).

I would guess that the main difficulty with the example is the "sum of 
absolute errors" objective function.  That objective function has a 
discontinuous derivative which will cause problems for many optimizers. 
It may also have many local optima, though I don't know in this case.

Duncan Murdoch

> 
> Best,
>    John
> 
>>
>> On Sat, 14 Dec 2024 at 01:14, John Fox <jfox at mcmaster.ca> wrote:
>>>
>>> Dear Daniel et al.,
>>>
>>> Following on Duncan's remark and examining the message produced by
>>> nloptr(), I simply tried increasing the maximum number of function
>>> evaluations:
>>> ------ snip -------
>>>
>>>    > nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>>> +          list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8,
>>> +               maxeval = 1e5)
>>> + )
>>>
>>> Call:
>>>
>>> nloptr(x0 = rep(0, 4), eval_f = f, eval_g_ineq = hin, eval_g_eq = Hx,
>>>        opts = list(algorithm = "NLOPT_LN_COBYLA", xtol_rel = 1e-08,
>>>            maxeval = 1e+05))
>>>
>>>
>>> Minimization using NLopt version 2.7.1
>>>
>>> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped
>>> because xtol_rel or xtol_abs (above) was reached. )
>>>
>>> Number of Iterations....: 46317
>>> Termination conditions:  xtol_rel: 1e-08        maxeval: 1e+05
>>> Number of inequality constraints:  1
>>> Number of equality constraints:    1
>>> Optimal value of objective function:  1287.71725107671
>>> Optimal value of controls: 1.576708 6.456606 6.195305 -19.008
>>>
>>> ---------- snip ----------
>>>
>>> That produces a solution closer to, and better than, the one that you
>>> suggested (which you obtained how?):
>>>
>>>    > f(c(0.222, 6.999, 6.17, -19.371))
>>> [1] 1325.076
>>>
>>> I hope this helps,
>>>     John
>>> --
>>> John Fox, Professor Emeritus
>>> McMaster University
>>> Hamilton, Ontario, Canada
>>> web: https://www.john-fox.ca/
>>> --
>>> On 2024-12-13 1:45 p.m., Duncan Murdoch wrote:
>>>> Caution: External email.
>>>>
>>>>
>>>> You posted a version of this question on StackOverflow, and were given
>>>> advice there that you ignored.
>>>>
>>>> nloptr() clearly indicates that it is quitting without reaching an
>>>> optimum, but you are hiding that message.  Don't do that.
>>>>
>>>> Duncan Murdoch
>>>>
>>>> On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
>>>>> library(nloptr)
>>>>>
>>>>> set.seed(1)
>>>>> A <- 1.34
>>>>> B <- 0.5673
>>>>> C <- 6.356
>>>>> D <- -1.234
>>>>> x <- seq(0.5, 20, length.out = 500)
>>>>> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
>>>>>
>>>>> #Objective function
>>>>>
>>>>> X <- cbind(1, x, x^2, log(x))
>>>>> f <- function(theta) {
>>>>> sum(abs(X %*% theta - y))
>>>>> }
>>>>>
>>>>> #Constraint
>>>>>
>>>>> eps <- 1e-4
>>>>>
>>>>> hin <- function(theta) {
>>>>>      abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
>>>>> }
>>>>>
>>>>> Hx <- function(theta) {
>>>>>      X[100, , drop = FALSE] %*% theta - (120 - eps)
>>>>> }
>>>>>
>>>>> #Optimization with nloptr
>>>>>
>>>>> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>>>>> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
>>>>> # -0.2186159 -0.5032066  6.4458823 -0.4125948
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide https://www.R-project.org/posting-
>>>> guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
> 
>


From bgunter@4567 @end|ng |rom gm@||@com  Fri Dec 13 23:43:49 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 13 Dec 2024 14:43:49 -0800
Subject: [R] Weird Behavior of mean
In-Reply-To: <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
 <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
 <26459.64652.135529.538036@stat.math.ethz.ch>
 <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>
Message-ID: <CAGxFJbQu35UTMrBGSqqdJFS4FYz7bT3VM=D92YsPBTzYuHKqnQ@mail.gmail.com>

Ivo, et al.:
--IMHO only ... and with apologies for verbosity

Defining, let alone enforcing, "consistent behavior" can be a
philosophical conundrum: what one person deems "consistent" behavior
for a function across different data structures and circumstances may
not be the same as another's. While you may consider the issue clear
here, a glance at the source code shows that may not necessarily be
the case: mean() is an S3 generic, but sd() is derived from var()
which is in turn based on cov(), for which NA handling is more
complex.

Anyway, for me, the only defensible standard should be is that the
*documented* behavior for overloaded function names is that they
should be accurately documented for each use case, whether or not the
semantics conform to any particular paradigm of consistency. By this
standard, I think mean() is behaving correctly, as its Help page says:

na.rm
a *logical* evaluating to TRUE or FALSE indicating whether NA values
should be stripped before the computation proceeds. [emphasis added]
Note: *not* a value that can be *coerced* to logical, but an actual
logical expression.

But sd() is not, as its Help page says:
na.rm
logical. Should missing values be removed?
Note: So seemingly same as above, but as you noted, will work for
values that can be coerced to logical and not just actual logical
expressions.

Cheers,
Bert



On Fri, Dec 13, 2024 at 11:43?AM ivo welch <ivo.welch at ucla.edu> wrote:
>
> isn't this still a little R buglet?  I have overwritten T (even if my
> schuld [franconian], it is not that uncommon an error, because T is also a
> common abbreviation for the end of a time series; namespace pollution in R
> can be quite annoying, even though I understand that it is convenient in
> interactive mode).  Nevertheless, I am passing into mean() a positive
> number for na.rm, and by definition, a positive number still means TRUE.
>  besides, sd() and mean() should probably treat this similarly, anyway.  I
> do see the argument that functions cannot be proof against redefinitions of
> all sorts of objects that they can use.    more philosophically, some
> variables should not be overwritable, or at least trigger a warning.
>
> As Dante wrote, Abandon all hope ye who enter R.
>
> --
> Ivo Welch (ivo.welch at ucla.edu)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bbo|ker @end|ng |rom gm@||@com  Sat Dec 14 00:17:54 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Fri, 13 Dec 2024 18:17:54 -0500
Subject: [R] Weird Behavior of mean
In-Reply-To: <CAGxFJbQu35UTMrBGSqqdJFS4FYz7bT3VM=D92YsPBTzYuHKqnQ@mail.gmail.com>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
 <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
 <26459.64652.135529.538036@stat.math.ethz.ch>
 <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>
 <CAGxFJbQu35UTMrBGSqqdJFS4FYz7bT3VM=D92YsPBTzYuHKqnQ@mail.gmail.com>
Message-ID: <7baccbf1-1a88-4c4b-894e-1a9c66abed6d@gmail.com>

   Thanks, I had missed/forgotten the fact that there is also an 
inconsistency between mean.default() and sd().

   sd() calls var(), which evaluates if(na.rm) [i.e., it will try to 
coerce `na.rm` to logical rather than testing isTRUE]

  IM(H?)O, it would be best for both mean.default() and sd() to use 
if(isTRUE(as.logical(na.rm))) -- this converts NULL, numeric(0), zero 
numeric values, etc. to FALSE, non-zero numeric values (including 
complex numbers not equal to 0+0i) to TRUE ... fails on un-coerceable 
stuff like functions, environments ...


?as.logical? attempts to coerce its argument to be of logical
      type.  In numeric and complex vectors, zeros are ?FALSE? and
      non-zero values are ?TRUE?.  For ?factor?s, this uses the ?levels?
      (labels).  Like ?as.vector? it strips attributes including names.
      Character strings ?c("T", "TRUE", "True", "true")? are regarded as
      true, ?c("F", "FALSE", "False", "false")? as false, and all others
      as ?NA?.


On 2024-12-13 5:43 p.m., Bert Gunter wrote:
> Ivo, et al.:
> --IMHO only ... and with apologies for verbosity
> 
> Defining, let alone enforcing, "consistent behavior" can be a
> philosophical conundrum: what one person deems "consistent" behavior
> for a function across different data structures and circumstances may
> not be the same as another's. While you may consider the issue clear
> here, a glance at the source code shows that may not necessarily be
> the case: mean() is an S3 generic, but sd() is derived from var()
> which is in turn based on cov(), for which NA handling is more
> complex.
> 
> Anyway, for me, the only defensible standard should be is that the
> *documented* behavior for overloaded function names is that they
> should be accurately documented for each use case, whether or not the
> semantics conform to any particular paradigm of consistency. By this
> standard, I think mean() is behaving correctly, as its Help page says:
> 
> na.rm
> a *logical* evaluating to TRUE or FALSE indicating whether NA values
> should be stripped before the computation proceeds. [emphasis added]
> Note: *not* a value that can be *coerced* to logical, but an actual
> logical expression.
> 
> But sd() is not, as its Help page says:
> na.rm
> logical. Should missing values be removed?
> Note: So seemingly same as above, but as you noted, will work for
> values that can be coerced to logical and not just actual logical
> expressions.
> 
> Cheers,
> Bert
> 
> 
> 
> On Fri, Dec 13, 2024 at 11:43?AM ivo welch <ivo.welch at ucla.edu> wrote:
>>
>> isn't this still a little R buglet?  I have overwritten T (even if my
>> schuld [franconian], it is not that uncommon an error, because T is also a
>> common abbreviation for the end of a time series; namespace pollution in R
>> can be quite annoying, even though I understand that it is convenient in
>> interactive mode).  Nevertheless, I am passing into mean() a positive
>> number for na.rm, and by definition, a positive number still means TRUE.
>>   besides, sd() and mean() should probably treat this similarly, anyway.  I
>> do see the argument that functions cannot be proof against redefinitions of
>> all sorts of objects that they can use.    more philosophically, some
>> variables should not be overwritable, or at least trigger a warning.
>>
>> As Dante wrote, Abandon all hope ye who enter R.
>>
>> --
>> Ivo Welch (ivo.welch at ucla.edu)
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
 > E-mail is sent at my convenience; I don't expect replies outside of 
working hours.


From r@oknz @end|ng |rom gm@||@com  Sat Dec 14 00:28:19 2024
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Sat, 14 Dec 2024 12:28:19 +1300
Subject: [R] Weird Behavior of mean
In-Reply-To: <7baccbf1-1a88-4c4b-894e-1a9c66abed6d@gmail.com>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
 <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
 <26459.64652.135529.538036@stat.math.ethz.ch>
 <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>
 <CAGxFJbQu35UTMrBGSqqdJFS4FYz7bT3VM=D92YsPBTzYuHKqnQ@mail.gmail.com>
 <7baccbf1-1a88-4c4b-894e-1a9c66abed6d@gmail.com>
Message-ID: <CABcYAd+FWDFH_jgzB4d3quAfSMOVi_0urMYCsd1a+dZKbMbTug@mail.gmail.com>

My preference would  be for anything that is defined as taking a
"logical" parameter to report an error if given anything else.

On Sat, 14 Dec 2024 at 12:21, Ben Bolker <bbolker at gmail.com> wrote:
>
>    Thanks, I had missed/forgotten the fact that there is also an
> inconsistency between mean.default() and sd().
>
>    sd() calls var(), which evaluates if(na.rm) [i.e., it will try to
> coerce `na.rm` to logical rather than testing isTRUE]
>
>   IM(H?)O, it would be best for both mean.default() and sd() to use
> if(isTRUE(as.logical(na.rm))) -- this converts NULL, numeric(0), zero
> numeric values, etc. to FALSE, non-zero numeric values (including
> complex numbers not equal to 0+0i) to TRUE ... fails on un-coerceable
> stuff like functions, environments ...
>
>
> ?as.logical? attempts to coerce its argument to be of logical
>       type.  In numeric and complex vectors, zeros are ?FALSE? and
>       non-zero values are ?TRUE?.  For ?factor?s, this uses the ?levels?
>       (labels).  Like ?as.vector? it strips attributes including names.
>       Character strings ?c("T", "TRUE", "True", "true")? are regarded as
>       true, ?c("F", "FALSE", "False", "false")? as false, and all others
>       as ?NA?.
>
>
> On 2024-12-13 5:43 p.m., Bert Gunter wrote:
> > Ivo, et al.:
> > --IMHO only ... and with apologies for verbosity
> >
> > Defining, let alone enforcing, "consistent behavior" can be a
> > philosophical conundrum: what one person deems "consistent" behavior
> > for a function across different data structures and circumstances may
> > not be the same as another's. While you may consider the issue clear
> > here, a glance at the source code shows that may not necessarily be
> > the case: mean() is an S3 generic, but sd() is derived from var()
> > which is in turn based on cov(), for which NA handling is more
> > complex.
> >
> > Anyway, for me, the only defensible standard should be is that the
> > *documented* behavior for overloaded function names is that they
> > should be accurately documented for each use case, whether or not the
> > semantics conform to any particular paradigm of consistency. By this
> > standard, I think mean() is behaving correctly, as its Help page says:
> >
> > na.rm
> > a *logical* evaluating to TRUE or FALSE indicating whether NA values
> > should be stripped before the computation proceeds. [emphasis added]
> > Note: *not* a value that can be *coerced* to logical, but an actual
> > logical expression.
> >
> > But sd() is not, as its Help page says:
> > na.rm
> > logical. Should missing values be removed?
> > Note: So seemingly same as above, but as you noted, will work for
> > values that can be coerced to logical and not just actual logical
> > expressions.
> >
> > Cheers,
> > Bert
> >
> >
> >
> > On Fri, Dec 13, 2024 at 11:43?AM ivo welch <ivo.welch at ucla.edu> wrote:
> >>
> >> isn't this still a little R buglet?  I have overwritten T (even if my
> >> schuld [franconian], it is not that uncommon an error, because T is also a
> >> common abbreviation for the end of a time series; namespace pollution in R
> >> can be quite annoying, even though I understand that it is convenient in
> >> interactive mode).  Nevertheless, I am passing into mean() a positive
> >> number for na.rm, and by definition, a positive number still means TRUE.
> >>   besides, sd() and mean() should probably treat this similarly, anyway.  I
> >> do see the argument that functions cannot be proof against redefinitions of
> >> all sorts of objects that they can use.    more philosophically, some
> >> variables should not be overwritable, or at least trigger a warning.
> >>
> >> As Dante wrote, Abandon all hope ye who enter R.
> >>
> >> --
> >> Ivo Welch (ivo.welch at ucla.edu)
> >>
> >>          [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Dr. Benjamin Bolker
> Professor, Mathematics & Statistics and Biology, McMaster University
> Director, School of Computational Science and Engineering
>  > E-mail is sent at my convenience; I don't expect replies outside of
> working hours.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sat Dec 14 00:31:43 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 13 Dec 2024 15:31:43 -0800
Subject: [R] Weird Behavior of mean
In-Reply-To: <7baccbf1-1a88-4c4b-894e-1a9c66abed6d@gmail.com>
References: <CACi4-Jn_sTEFbnTRYY0H0tToen1uJPA2+w0dZCO9eTrtRZ9FYA@mail.gmail.com>
 <CA+etgPkpJWeXpZbx4JEvkT6ku6sWk7-z1PV8Mv8CwnRHKhqc8A@mail.gmail.com>
 <26459.64652.135529.538036@stat.math.ethz.ch>
 <CAJrNScQm15iBBE625wVR8rwpNzxN0+4ukernKW8-YXck119zYA@mail.gmail.com>
 <CAGxFJbQu35UTMrBGSqqdJFS4FYz7bT3VM=D92YsPBTzYuHKqnQ@mail.gmail.com>
 <7baccbf1-1a88-4c4b-894e-1a9c66abed6d@gmail.com>
Message-ID: <CAGxFJbRaPHwGc+J5dgqLxU22_wVTQ5WUDrE36h6h5BLfX2BvhA@mail.gmail.com>

Sounds reasonable, but I leave it to wiser heads than me to decide. My
only point is that whatever is done be accurately documented. At
present, that does not appear to be the case. ... and yes, "accurate"
documentation is not easy either.

-- Bert

On Fri, Dec 13, 2024 at 3:20?PM Ben Bolker <bbolker at gmail.com> wrote:
>
>    Thanks, I had missed/forgotten the fact that there is also an
> inconsistency between mean.default() and sd().
>
>    sd() calls var(), which evaluates if(na.rm) [i.e., it will try to
> coerce `na.rm` to logical rather than testing isTRUE]
>
>   IM(H?)O, it would be best for both mean.default() and sd() to use
> if(isTRUE(as.logical(na.rm))) -- this converts NULL, numeric(0), zero
> numeric values, etc. to FALSE, non-zero numeric values (including
> complex numbers not equal to 0+0i) to TRUE ... fails on un-coerceable
> stuff like functions, environments ...
>
>
> ?as.logical? attempts to coerce its argument to be of logical
>       type.  In numeric and complex vectors, zeros are ?FALSE? and
>       non-zero values are ?TRUE?.  For ?factor?s, this uses the ?levels?
>       (labels).  Like ?as.vector? it strips attributes including names.
>       Character strings ?c("T", "TRUE", "True", "true")? are regarded as
>       true, ?c("F", "FALSE", "False", "false")? as false, and all others
>       as ?NA?.
>
>
> On 2024-12-13 5:43 p.m., Bert Gunter wrote:
> > Ivo, et al.:
> > --IMHO only ... and with apologies for verbosity
> >
> > Defining, let alone enforcing, "consistent behavior" can be a
> > philosophical conundrum: what one person deems "consistent" behavior
> > for a function across different data structures and circumstances may
> > not be the same as another's. While you may consider the issue clear
> > here, a glance at the source code shows that may not necessarily be
> > the case: mean() is an S3 generic, but sd() is derived from var()
> > which is in turn based on cov(), for which NA handling is more
> > complex.
> >
> > Anyway, for me, the only defensible standard should be is that the
> > *documented* behavior for overloaded function names is that they
> > should be accurately documented for each use case, whether or not the
> > semantics conform to any particular paradigm of consistency. By this
> > standard, I think mean() is behaving correctly, as its Help page says:
> >
> > na.rm
> > a *logical* evaluating to TRUE or FALSE indicating whether NA values
> > should be stripped before the computation proceeds. [emphasis added]
> > Note: *not* a value that can be *coerced* to logical, but an actual
> > logical expression.
> >
> > But sd() is not, as its Help page says:
> > na.rm
> > logical. Should missing values be removed?
> > Note: So seemingly same as above, but as you noted, will work for
> > values that can be coerced to logical and not just actual logical
> > expressions.
> >
> > Cheers,
> > Bert
> >
> >
> >
> > On Fri, Dec 13, 2024 at 11:43?AM ivo welch <ivo.welch at ucla.edu> wrote:
> >>
> >> isn't this still a little R buglet?  I have overwritten T (even if my
> >> schuld [franconian], it is not that uncommon an error, because T is also a
> >> common abbreviation for the end of a time series; namespace pollution in R
> >> can be quite annoying, even though I understand that it is convenient in
> >> interactive mode).  Nevertheless, I am passing into mean() a positive
> >> number for na.rm, and by definition, a positive number still means TRUE.
> >>   besides, sd() and mean() should probably treat this similarly, anyway.  I
> >> do see the argument that functions cannot be proof against redefinitions of
> >> all sorts of objects that they can use.    more philosophically, some
> >> variables should not be overwritable, or at least trigger a warning.
> >>
> >> As Dante wrote, Abandon all hope ye who enter R.
> >>
> >> --
> >> Ivo Welch (ivo.welch at ucla.edu)
> >>
> >>          [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Dr. Benjamin Bolker
> Professor, Mathematics & Statistics and Biology, McMaster University
> Director, School of Computational Science and Engineering
>  > E-mail is sent at my convenience; I don't expect replies outside of
> working hours.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j|ox @end|ng |rom mcm@@ter@c@  Sat Dec 14 01:48:01 2024
From: j|ox @end|ng |rom mcm@@ter@c@ (John Fox)
Date: Fri, 13 Dec 2024 19:48:01 -0500
Subject: [R] Non linear optimization with nloptr package fail to produce
 true optimal result
In-Reply-To: <327f0531-8be1-4074-b8a2-eede3d2aa619@gmail.com>
References: <CADZb7hqta0RT0ib7_s1=8DK_+ss3rPKWY_-tdx1EO9v+r6DAeg@mail.gmail.com>
 <ceb3b186-3f4d-4688-9f25-628db863a587@gmail.com>
 <40fa65f7-e98e-436a-99f3-f9d07b08eb8c@mcmaster.ca>
 <CADZb7ho=4_gF4cGMNWp-f8SDi9uMzC3CKhOStXZ3WbDOzxgBqA@mail.gmail.com>
 <d3a84d15-be60-4128-b18e-7167e343aa14@mcmaster.ca>
 <327f0531-8be1-4074-b8a2-eede3d2aa619@gmail.com>
Message-ID: <2e081f31-daa1-4413-8363-d234c68be0c7@mcmaster.ca>

Hello Duncan,

On 2024-12-13 5:30 p.m., Duncan Murdoch wrote:
> Caution: External email.
> 
> 
> On 2024-12-13 5:11 p.m., John Fox wrote:
>> Dear Daniel,
>>
>> On 2024-12-13 2:51 p.m., Daniel Lobo wrote:
>>> Caution: External email.
>>>
>>>
>>> Looks like the solution 1.576708?? 6.456606?? 6.195305 -19.007996 is
>>> the best solution that nloptr can produce by increasing the iteration
>>> numbers.
>>>
>>> The better set of solution is obtained using pracma package.
>>
>> Not if I read the output correctly. As I showed, the result from
>> pracma:fmincon() produces a larger value of the objective function than
>> the result I obtained from nloptr().
>>
>> John Nash (who is an expert on optimization -- I'm not) obtained an even
>> lower value of the objective function from alabama::auglag().
>>
>> As others have pointed out, one can't really draw general conclusions
>> from a particular example, and like others, I don't have the time or
>> inclination to figure out why your problem appears to be ill-conditioned
>> (though note that the columns of X, excluding the constant, are highly
>> correlated).
> 
> I would guess that the main difficulty with the example is the "sum of
> absolute errors" objective function.? That objective function has a

That was one of my first thoughts, but when I tried substituting the 
least-squares objective function, I observed similar behaviour.

> discontinuous derivative which will cause problems for many optimizers.
> It may also have many local optima, though I don't know in this case.

The model being fit is pretty simple -- a constrained linear model -- 
but as I said the columns of X are fairly highly correlated (though not 
so much as to create problems for unconstrained least-squares):

cor(X[, -1])
           x
x 1.0000000 0.9710284 0.9254732
   0.9710284 1.0000000 0.8201191
   0.9254732 0.8201191 1.0000000

Columns 2 and 3 are x^2 and log(x).

Also, the various solutions aren't all that different, and the 
regression residuals are relatively very small:

 > fits <- cbind(X %*% res$solution,
+               X %*% c(0.222, 6.999, 6.17, -19.371),
+               X %*% c(-0.610594, 4.307408, 6.254267, -11.919881),
+               y)
 > colnames(fits) = c("nloptr", "fmincon", "auglag", "y")
 > cor(fits)
            nloptr   fmincon    auglag         y
nloptr  1.0000000 0.9999997 0.9999990 0.9999930
fmincon 0.9999997 1.0000000 0.9999988 0.9999923
auglag  0.9999990 0.9999988 1.0000000 0.9999970
y       0.9999930 0.9999923 0.9999970 1.0000000
 > cor(log(fits))
            nloptr   fmincon    auglag         y
nloptr  1.0000000 0.9999796 0.9989490 0.9957987
fmincon 0.9999796 1.0000000 0.9991778 0.9961371
auglag  0.9989490 0.9991778 1.0000000 0.9986251
y       0.9957987 0.9961371 0.9986251 1.0000000

And take a look at (graph not shown):

plot(x, log(y), col="lightgray")
lines(x, log(fits[, 1]), col="magenta")
lines(x, log(fits[, 2]), col="blue")
lines(x, log(fits[, 3]), col="orange")
legend("bottomright", inset=0.025, col=c("magenta", "blue", "orange"),
        lty=1, legend=c("nloptr", "fmincon", "auglag"))

There's something peculiar going on at the lower-left.

Best,
  John

> 
> Duncan Murdoch
> 
>>
>> Best,
>> ?? John
>>
>>>
>>> On Sat, 14 Dec 2024 at 01:14, John Fox <jfox at mcmaster.ca> wrote:
>>>>
>>>> Dear Daniel et al.,
>>>>
>>>> Following on Duncan's remark and examining the message produced by
>>>> nloptr(), I simply tried increasing the maximum number of function
>>>> evaluations:
>>>> ------ snip -------
>>>>
>>>> ?? > nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>>>> +????????? list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8,
>>>> +?????????????? maxeval = 1e5)
>>>> + )
>>>>
>>>> Call:
>>>>
>>>> nloptr(x0 = rep(0, 4), eval_f = f, eval_g_ineq = hin, eval_g_eq = Hx,
>>>> ?????? opts = list(algorithm = "NLOPT_LN_COBYLA", xtol_rel = 1e-08,
>>>> ?????????? maxeval = 1e+05))
>>>>
>>>>
>>>> Minimization using NLopt version 2.7.1
>>>>
>>>> NLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped
>>>> because xtol_rel or xtol_abs (above) was reached. )
>>>>
>>>> Number of Iterations....: 46317
>>>> Termination conditions:? xtol_rel: 1e-08??????? maxeval: 1e+05
>>>> Number of inequality constraints:? 1
>>>> Number of equality constraints:??? 1
>>>> Optimal value of objective function:? 1287.71725107671
>>>> Optimal value of controls: 1.576708 6.456606 6.195305 -19.008
>>>>
>>>> ---------- snip ----------
>>>>
>>>> That produces a solution closer to, and better than, the one that you
>>>> suggested (which you obtained how?):
>>>>
>>>> ?? > f(c(0.222, 6.999, 6.17, -19.371))
>>>> [1] 1325.076
>>>>
>>>> I hope this helps,
>>>> ??? John
>>>> -- 
>>>> John Fox, Professor Emeritus
>>>> McMaster University
>>>> Hamilton, Ontario, Canada
>>>> web: https://www.john-fox.ca/
>>>> -- 
>>>> On 2024-12-13 1:45 p.m., Duncan Murdoch wrote:
>>>>> Caution: External email.
>>>>>
>>>>>
>>>>> You posted a version of this question on StackOverflow, and were given
>>>>> advice there that you ignored.
>>>>>
>>>>> nloptr() clearly indicates that it is quitting without reaching an
>>>>> optimum, but you are hiding that message.? Don't do that.
>>>>>
>>>>> Duncan Murdoch
>>>>>
>>>>> On 2024-12-13 12:52 p.m., Daniel Lobo wrote:
>>>>>> library(nloptr)
>>>>>>
>>>>>> set.seed(1)
>>>>>> A <- 1.34
>>>>>> B <- 0.5673
>>>>>> C <- 6.356
>>>>>> D <- -1.234
>>>>>> x <- seq(0.5, 20, length.out = 500)
>>>>>> y <- A + B * x + C * x^2 + D * log(x) + runif(500, 0, 3)
>>>>>>
>>>>>> #Objective function
>>>>>>
>>>>>> X <- cbind(1, x, x^2, log(x))
>>>>>> f <- function(theta) {
>>>>>> sum(abs(X %*% theta - y))
>>>>>> }
>>>>>>
>>>>>> #Constraint
>>>>>>
>>>>>> eps <- 1e-4
>>>>>>
>>>>>> hin <- function(theta) {
>>>>>> ???? abs(sum(X %*% theta) - sum(y)) - 1e-3 + eps
>>>>>> }
>>>>>>
>>>>>> Hx <- function(theta) {
>>>>>> ???? X[100, , drop = FALSE] %*% theta - (120 - eps)
>>>>>> }
>>>>>>
>>>>>> #Optimization with nloptr
>>>>>>
>>>>>> Sol = nloptr(rep(0, 4), f, eval_g_ineq = hin, eval_g_eq = Hx, opts =
>>>>>> list("algorithm" = "NLOPT_LN_COBYLA", "xtol_rel" = 1.0e-8))$solution
>>>>>> # -0.2186159 -0.5032066? 6.4458823 -0.4125948
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide https://www.R-project.org/posting-
>>>>> guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>
>>
>


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Sat Dec 14 02:27:36 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Sat, 14 Dec 2024 01:27:36 +0000
Subject: [R] Fortune Nomination
Message-ID: <DM6PR03MB5049D0A16F186DDB2E122E25E2392@DM6PR03MB5049.namprd03.prod.outlook.com>

As Dante wrote, Abandon all hope ye who enter R.

Written by ivo welch 12/13 2:43 PM

John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?
PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




