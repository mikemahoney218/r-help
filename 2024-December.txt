From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Sun Dec  1 03:27:29 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Sun, 1 Dec 2024 02:27:29 +0000
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
Message-ID: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>

Dear R help folks,

First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!

I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.

e.g. if my original data is 
 olddata
   ID date
    1     1
    1     1
    1     2
    1     2
    1     3
    1     3
    1     4
    1     4
    1     5
    1     5
    2     5
    2     5
    2     5
    2     6
    2     6
    2     6
    3   10
    3   10

the new data will be
newdata
   ID date  first
    1     1       1
    1     1       0
    1     2       0
    1     2       0
    1     3       0
    1     3       0
    1     4       0
    1     4       0
    1     5       0
    1     5       0
    2     5       1
    2     5       0
    2     5       0
    2     6       0
    2     6       0
    2     6       0
    3   10       1
    3   10       0

When I run the program below, I receive the following error:
Error in df[, "ID"] : incorrect number of dimensions

My code:
# Create data.frame
ID <- c(rep(1,10),rep(2,6),rep(3,2))
date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
          rep(5,3),rep(6,3),rep(10,2))
olddata <- data.frame(ID=ID,date=date)
class(olddata)
cat("This is the original data frame","\n")
print(olddata)
 
# This function is supposed to identify the first row 
# within each level of ID and, for the first row, set
# the variable first to 1, and for all rows other than
# the first row set first to 0.
mydoit <- function(df){
  value <- ifelse (first(df[,"ID"]),1,0)
  cat("value=",value,"\n")
  df[,"first"] <- value
}
newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)

Thank you,
John


John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?
PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




From bbo|ker @end|ng |rom gm@||@com  Sun Dec  1 03:35:38 2024
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 30 Nov 2024 21:35:38 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <CABghstS905fhe3rLmbJtfBxDhzLcpqwmCD1jObsNhqVmTznkQw@mail.gmail.com>

I think as.numeric(! duplicated(group)) might do this for you ...

On Sat, Nov 30, 2024, 9:27 PM Sorkin, John <jsorkin at som.umaryland.edu>
wrote:

> Dear R help folks,
>
> First my apologizes for sending several related questions to the list
> server. I am trying to learn how to manipulate data in R . . . and am
> having difficulty getting my program to work. I greatly appreciate the help
> and support list member give!
>
> I am trying to write a program that will run through a data frame
> organized by ID and for the first line of each new group of data lines that
> has the same ID create a new variable first that will be 1 for the first
> line of the group and 0 for all other lines.
>
> e.g. if my original data is
>  olddata
>    ID date
>     1     1
>     1     1
>     1     2
>     1     2
>     1     3
>     1     3
>     1     4
>     1     4
>     1     5
>     1     5
>     2     5
>     2     5
>     2     5
>     2     6
>     2     6
>     2     6
>     3   10
>     3   10
>
> the new data will be
> newdata
>    ID date  first
>     1     1       1
>     1     1       0
>     1     2       0
>     1     2       0
>     1     3       0
>     1     3       0
>     1     4       0
>     1     4       0
>     1     5       0
>     1     5       0
>     2     5       1
>     2     5       0
>     2     5       0
>     2     6       0
>     2     6       0
>     2     6       0
>     3   10       1
>     3   10       0
>
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
>
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>           rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>   value <- ifelse (first(df[,"ID"]),1,0)
>   cat("value=",value,"\n")
>   df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
>
> Thank you,
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical
> Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of
> Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> <https://www.google.com/maps/search/10+North+Greene+Street?entry=gmail&source=g>
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From cry@n @end|ng |rom b|ngh@mton@edu  Sun Dec  1 03:46:57 2024
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Sat, 30 Nov 2024 21:46:57 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <600b676e-9dab-8f64-e492-39ad4f4ca281@binghamton.edu>

Personally, I'd do this in the tidyverse with dplyr and its row_number()
function.

olddata %>% group_by(ID) %>% mutate(first = as.integer(row_number() == 1))

--Chris Ryan

Sorkin, John wrote:
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>           rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)


From cry@n @end|ng |rom b|ngh@mton@edu  Sun Dec  1 03:51:48 2024
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Sat, 30 Nov 2024 21:51:48 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <600b676e-9dab-8f64-e492-39ad4f4ca281@binghamton.edu>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <600b676e-9dab-8f64-e492-39ad4f4ca281@binghamton.edu>
Message-ID: <f2250299-b8b5-a8a5-aacb-3779e51c397c@binghamton.edu>

Sorry, for completeness:

library(dplyr)
olddata %>% group_by(ID) %>% mutate(first = as.integer(row_number() == 1))

--Chris Ryan


Christopher W. Ryan wrote:
> Personally, I'd do this in the tidyverse with dplyr and its row_number()
> function.
> 
> olddata %>% group_by(ID) %>% mutate(first = as.integer(row_number() == 1))
> 
> --Chris Ryan
> 
> Sorkin, John wrote:
>> ID <- c(rep(1,10),rep(2,6),rep(3,2))
>> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>>           rep(5,3),rep(6,3),rep(10,2))
>> olddata <- data.frame(ID=ID,date=date)


From rmh @end|ng |rom temp|e@edu  Sun Dec  1 04:54:47 2024
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Sun, 1 Dec 2024 03:54:47 +0000
Subject: [R] 
 [External]  Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <ACBADD34-16BF-4F7F-99BC-3D1DE568CD48@temple.edu>

tmp.ID <- unique(olddata$ID)
Firsts <- match(tmp.ID, olddata$ID)
newdata <- cbind(olddata, First=0)
newdata$First[Firsts] <- 1
newdata

newdata$FirstDay <- 0
for (id in tmp.ID)
  newdata$FirstDay[newdata$ID == id] <- newdata$date[newdata$ID == id][1]
newdata


> On Nov 30, 2024, at 21:27, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Dear R help folks,
>
> First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
>
> I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
>
> e.g. if my original data is
> olddata
>   ID date
>    1     1
>    1     1
>    1     2
>    1     2
>    1     3
>    1     3
>    1     4
>    1     4
>    1     5
>    1     5
>    2     5
>    2     5
>    2     5
>    2     6
>    2     6
>    2     6
>    3   10
>    3   10
>
> the new data will be
> newdata
>   ID date  first
>    1     1       1
>    1     1       0
>    1     2       0
>    1     2       0
>    1     3       0
>    1     3       0
>    1     4       0
>    1     4       0
>    1     5       0
>    1     5       0
>    2     5       1
>    2     5       0
>    2     5       0
>    2     6       0
>    2     6       0
>    2     6       0
>    3   10       1
>    3   10       0
>
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
>
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>          rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>  value <- ifelse (first(df[,"ID"]),1,0)
>  cat("value=",value,"\n")
>  df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
>
> Thank you,
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sun Dec  1 05:33:42 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 30 Nov 2024 20:33:42 -0800
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>

May I ask *why* you want to do this?

It sounds to me like like you're using SAS-like strategies for your
data analysis rather than R-like.

-- Bert

-- Bert

On Sat, Nov 30, 2024 at 6:27?PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Dear R help folks,
>
> First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
>
> I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
>
> e.g. if my original data is
>  olddata
>    ID date
>     1     1
>     1     1
>     1     2
>     1     2
>     1     3
>     1     3
>     1     4
>     1     4
>     1     5
>     1     5
>     2     5
>     2     5
>     2     5
>     2     6
>     2     6
>     2     6
>     3   10
>     3   10
>
> the new data will be
> newdata
>    ID date  first
>     1     1       1
>     1     1       0
>     1     2       0
>     1     2       0
>     1     3       0
>     1     3       0
>     1     4       0
>     1     4       0
>     1     5       0
>     1     5       0
>     2     5       1
>     2     5       0
>     2     5       0
>     2     6       0
>     2     6       0
>     2     6       0
>     3   10       1
>     3   10       0
>
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
>
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>           rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>   value <- ifelse (first(df[,"ID"]),1,0)
>   cat("value=",value,"\n")
>   df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
>
> Thank you,
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Sun Dec  1 07:18:09 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Sun, 1 Dec 2024 01:18:09 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
Message-ID: <010901db43b8$cb3408b0$619c1a10$@gmail.com>

I was wondering along similar lines, Bert.

One way to get help is to ask how to do some single step of a larger strategy. That can lead to answers that may not be as applicable to the scenario.

Another way would be to include a synopsis of what they are trying to do.

But, as John says he is trying to learn and improve his abilities, perhaps he s getting what he wants.
After watching some of the exchanges in multiple questions, many seem to revolve around a wish to deal with sorted grouped data. He seems to have looked at some base R methods as well as packages like dplyr using tibbles as well as another package and format.

What interests me from a dplyr perspective is how many little embedded functions it makes available and some have been mentioned here. If you want to  add a column that contains the same value for each group, such as the minimum, mean, first and many other things, it is very easily doable.

The latest request seems to be a bit different as it wants a column with a 1 (presumably for TRUE) only for the first entry in  the group. Again, fairly easy using one of several hooks such as the rownumber being "1" versus not. There are many variations on the answer supplied depending on style and need, such as making a column that contains the row number, and in a later step, set those to zero that are not a one. 

But sometimes you want to ask what the overall algorithm is. Do you need extra columns to then use for some purpose, or could that purpose have been done another way such as doing some calculation only when rownumber is one.

As noted, R makes some operations fairly natural, in ways that differ from the "natural" way another program/environment does it. Sometimes a translation is not worth doing as compared to a reworked algorithm that makes good use of whichever package and related functionality you want to use. 

Assuming all these questions relate to the same project, I am not clear if and where the lookback at previous row/value fits.

Of course, John may not be free to share more in public.

Anyone want to suggest a book or two on data processing of this sort using R that might illustrate with examples galore on how various problems are solved and then perhaps some will be similar enough ...

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Bert Gunter
Sent: Saturday, November 30, 2024 11:34 PM
To: Sorkin, John <jsorkin at som.umaryland.edu>
Cc: r-help at r-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows

May I ask *why* you want to do this?

It sounds to me like like you're using SAS-like strategies for your
data analysis rather than R-like.

-- Bert

-- Bert

On Sat, Nov 30, 2024 at 6:27?PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Dear R help folks,
>
> First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
>
> I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
>
> e.g. if my original data is
>  olddata
>    ID date
>     1     1
>     1     1
>     1     2
>     1     2
>     1     3
>     1     3
>     1     4
>     1     4
>     1     5
>     1     5
>     2     5
>     2     5
>     2     5
>     2     6
>     2     6
>     2     6
>     3   10
>     3   10
>
> the new data will be
> newdata
>    ID date  first
>     1     1       1
>     1     1       0
>     1     2       0
>     1     2       0
>     1     3       0
>     1     3       0
>     1     4       0
>     1     4       0
>     1     5       0
>     1     5       0
>     2     5       1
>     2     5       0
>     2     5       0
>     2     6       0
>     2     6       0
>     2     6       0
>     3   10       1
>     3   10       0
>
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
>
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>           rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>   value <- ifelse (first(df[,"ID"]),1,0)
>   cat("value=",value,"\n")
>   df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
>
> Thank you,
> John
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Dec  1 08:05:24 2024
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 1 Dec 2024 07:05:24 +0000
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>

?s 02:27 de 01/12/2024, Sorkin, John escreveu:
> Dear R help folks,
> 
> First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> 
> I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> 
> e.g. if my original data is
>   olddata
>     ID date
>      1     1
>      1     1
>      1     2
>      1     2
>      1     3
>      1     3
>      1     4
>      1     4
>      1     5
>      1     5
>      2     5
>      2     5
>      2     5
>      2     6
>      2     6
>      2     6
>      3   10
>      3   10
> 
> the new data will be
> newdata
>     ID date  first
>      1     1       1
>      1     1       0
>      1     2       0
>      1     2       0
>      1     3       0
>      1     3       0
>      1     4       0
>      1     4       0
>      1     5       0
>      1     5       0
>      2     5       1
>      2     5       0
>      2     5       0
>      2     6       0
>      2     6       0
>      2     6       0
>      3   10       1
>      3   10       0
> 
> When I run the program below, I receive the following error:
> Error in df[, "ID"] : incorrect number of dimensions
> 
> My code:
> # Create data.frame
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>            rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)
> cat("This is the original data frame","\n")
> print(olddata)
>   
> # This function is supposed to identify the first row
> # within each level of ID and, for the first row, set
> # the variable first to 1, and for all rows other than
> # the first row set first to 0.
> mydoit <- function(df){
>    value <- ifelse (first(df[,"ID"]),1,0)
>    cat("value=",value,"\n")
>    df[,"first"] <- value
> }
> newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> 
> Thank you,
> John
> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
> 
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,

And here are two other solutions.


olddata$first <- with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == 
x[1L]))

olddata$first <- c(1L, diff(olddata$ID))


Of these two, diff is faster. But of all the solutions posted so far, 
Ben Bolker's is the fastest. And it can be made a little faster if 
as.integer substitutes for as.numeric.
And dplyr::mutate now has a .by argument, which avoids explicit the call 
to group_by, with a performance gain.


library(microbenchmark)

mb <- microbenchmark(
   ave = with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == x[1L])),
   dup_num = as.numeric(! duplicated(olddata$ID)),
   dup_int = as.integer(! duplicated(olddata$ID)),
   diff = diff = c(1L, diff(olddata$ID)),
   dplyr_grp = olddata %>% group_by(ID) %>% mutate(first = 
as.integer(row_number() == 1)),
   dplyr = olddata %>% mutate(first = as.integer(row_number() == 1), .by 
= ID)
)
print(mb, order = "median")



However, note that dplyr operates in entire data.frames and therefore is 
expected to be slower when tested against instructions that process one 
column only.


Hope this helps,

Rui Barradas


-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Dec  1 08:15:37 2024
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 1 Dec 2024 07:15:37 +0000
Subject: [R] dplyr summarize by groups
In-Reply-To: <005b01db3d6b$ed2288a0$c76799e0$@yahoo.com>
References: <005b01db3d6b$ed2288a0$c76799e0$.ref@yahoo.com>
 <005b01db3d6b$ed2288a0$c76799e0$@yahoo.com>
Message-ID: <d88925f6-9316-499a-b90b-46e38e696786@sapo.pt>

?s 05:52 de 23/11/2024, tgs77m--- via R-help escreveu:
> # Get mean, min, max sigma and skew by group
> 
>   options (digits = 3)
>   library (ISwR
> data(energy)
> 
> data %>%
>    group_by(stature) %>%
>    summarize(
>      Mean = mean(expend),
>      Min =  min(expend),
>      Max = max(expend),
>      Sigma = sd(expend),
>      Skew = skew(expend))
> 
> # Output
> 
>    stature  Mean   Min   Max Sigma  Skew
>    <fct>   <dbl> <dbl> <dbl> <dbl> <dbl>
> 1 lean     8.07  6.13  10.9  1.24 0.907
> 2 obese   10.3   8.79  12.8  1.40 0.587
> 
> Why does output stats vary in decimal places even when options (digits=3)
> were set?
> 
> All the best
> 
> Thomas S.
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,


"Why does output stats vary in decimal places even when options 
(digits=3) were set?"

Yes, they do but shouldn't they? I'm seeing all numbers with 3 digits. 
Display digits and rounding are not the same thing.

Also, you don't need to load the package to have access to one of its 
data sets,


data(energy, package = "ISwR")


will load it. In this case, it's probably even better, "energy" is not 
an uncommon source for data and there might be data sets with the same 
name in other packages.


Hope this helps,

Rui Barradas




-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From bor|@@@te|pe @end|ng |rom utoronto@c@  Sun Dec  1 13:46:06 2024
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Sun, 1 Dec 2024 12:46:06 +0000
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <FC7E5425-9A46-4C5D-92D9-2584ABE02A46@utoronto.ca>


olddata$first <- as.numeric(! duplicated(olddata$ID))


:-)




> On Nov 30, 2024, at 22:27, Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> 
> ID <- c(rep(1,10),rep(2,6),rep(3,2))
> date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
>          rep(5,3),rep(6,3),rep(10,2))
> olddata <- data.frame(ID=ID,date=date)
> class(olddata)


From bgunter@4567 @end|ng |rom gm@||@com  Sun Dec  1 17:30:03 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 1 Dec 2024 08:30:03 -0800
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>
Message-ID: <CAGxFJbTUEFgeeDXe0rpAe4EfGN8edJAC1oVSjMTKuat-iBkSww@mail.gmail.com>

Rui:
"f these two, diff is faster. But of all the solutions posted so far,
Ben Bolker's is the fastest."

But the explicit version of diff is still considerably faster:

> D <- c(rep(1,10),rep(2,6),rep(3,2))

> microbenchmark(c(1L,diff(D)), times = 1000L)
Unit: microseconds
           expr   min    lq    mean median    uq    max neval
 c(1L, diff(D)) 3.075 3.198 3.34396   3.28 3.362 29.684  1000

> microbenchmark( as.integer(!duplicated(D)), times =1000L)
Unit: microseconds
                       expr   min    lq     mean median   uq  max neval
 as.integer(!duplicated(D)) 1.476 1.558 1.644264  1.599 1.64 16.4  1000

> microbenchmark( D - c(0L, D[-length(D)]), times = 1000L)
Unit: nanoseconds  ## note that unit is nanoseconds not microseconds
                     expr min  lq    mean median  uq  max neval
 D - c(0L, D[-length(D)]) 369 410 489.335    492 533 9840  1000

Cheers,
Bert

On Sat, Nov 30, 2024 at 11:05?PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> ?s 02:27 de 01/12/2024, Sorkin, John escreveu:
> > Dear R help folks,
> >
> > First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> >
> > I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> >
> > e.g. if my original data is
> >   olddata
> >     ID date
> >      1     1
> >      1     1
> >      1     2
> >      1     2
> >      1     3
> >      1     3
> >      1     4
> >      1     4
> >      1     5
> >      1     5
> >      2     5
> >      2     5
> >      2     5
> >      2     6
> >      2     6
> >      2     6
> >      3   10
> >      3   10
> >
> > the new data will be
> > newdata
> >     ID date  first
> >      1     1       1
> >      1     1       0
> >      1     2       0
> >      1     2       0
> >      1     3       0
> >      1     3       0
> >      1     4       0
> >      1     4       0
> >      1     5       0
> >      1     5       0
> >      2     5       1
> >      2     5       0
> >      2     5       0
> >      2     6       0
> >      2     6       0
> >      2     6       0
> >      3   10       1
> >      3   10       0
> >
> > When I run the program below, I receive the following error:
> > Error in df[, "ID"] : incorrect number of dimensions
> >
> > My code:
> > # Create data.frame
> > ID <- c(rep(1,10),rep(2,6),rep(3,2))
> > date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
> >            rep(5,3),rep(6,3),rep(10,2))
> > olddata <- data.frame(ID=ID,date=date)
> > class(olddata)
> > cat("This is the original data frame","\n")
> > print(olddata)
> >
> > # This function is supposed to identify the first row
> > # within each level of ID and, for the first row, set
> > # the variable first to 1, and for all rows other than
> > # the first row set first to 0.
> > mydoit <- function(df){
> >    value <- ifelse (first(df[,"ID"]),1,0)
> >    cat("value=",value,"\n")
> >    df[,"first"] <- value
> > }
> > newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> >
> > Thank you,
> > John
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine, University of Maryland School of Medicine;
> > Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> > PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> > Senior Statistician University of Maryland Center for Vascular Research;
> >
> > Division of Gerontology and Paliative Care,
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > Cell phone 443-418-5382
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> Hello,
>
> And here are two other solutions.
>
>
> olddata$first <- with(olddata, ave(seq_along(ID), ID, FUN = \(x) x ==
> x[1L]))
>
> olddata$first <- c(1L, diff(olddata$ID))
>
>
> Of these two, diff is faster. But of all the solutions posted so far,
> Ben Bolker's is the fastest. And it can be made a little faster if
> as.integer substitutes for as.numeric.
> And dplyr::mutate now has a .by argument, which avoids explicit the call
> to group_by, with a performance gain.
>
>
> library(microbenchmark)
>
> mb <- microbenchmark(
>    ave = with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == x[1L])),
>    dup_num = as.numeric(! duplicated(olddata$ID)),
>    dup_int = as.integer(! duplicated(olddata$ID)),
>    diff = diff = c(1L, diff(olddata$ID)),
>    dplyr_grp = olddata %>% group_by(ID) %>% mutate(first =
> as.integer(row_number() == 1)),
>    dplyr = olddata %>% mutate(first = as.integer(row_number() == 1), .by
> = ID)
> )
> print(mb, order = "median")
>
>
>
> However, note that dplyr operates in entire data.frames and therefore is
> expected to be slower when tested against instructions that process one
> column only.
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> --
> Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
> www.avg.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Mon Dec  2 06:43:45 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 1 Dec 2024 21:43:45 -0800
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB5049F365BD14FF48597163E3E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
 <DM6PR03MB5049F365BD14FF48597163E3E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <CAGxFJbQ_YjZND6XFGgpA2fCOxWdw-zRwSTpr1OnZA-FsPxBW7A@mail.gmail.com>

OK.

(Note: I am ccing this to the list so that others can correct any
mistakes, misunderstandings, or misstatements that I may make; and
also give you more and better advice about how to proceed. For
example, there may be environmental packages ( see the environmetrics
task view, https://CRAN.R-project.org/view=Environmetrics) that
already do everything you want from your data source that someone else
could tell you about. Why reinvent the wheel if you don't have to? )

Unfortunately, you failed to show us the critical information
necessary to give you a definitive answer: the structure of a single
record. However, I will wing it based on your description and assume
that each record contains the following information something like
this: Latitude   Longitude  Date Time pm25 + other stuff maybe.  If
this is the case,

1) You do not need to sort anything. R has robust date-time
manipulation and computation capabilities to handle dates and times,
though in this case you only need dates. R knows all about how to
order dates and times.

2) You do not have to physically group your records according to
geographic location. R knows how to extract and manipulate groups
without this;

3) And as all you need is a daily average, you do not need to worry
about when days begin or end.

If this is a reasonably accurate interpretation of what needs to be
done, and your data are in a data frame called dat, then something
like the following will do it:
## First convert latitude and longitude into a single location factor, loc
dat$loc <- with(dat, paste0(Latitude, Longitude))

## you could also convert the latitude and longitude into actual
location names if you like; the "factor" data structure in R is one
simple way to do this, for example.

## Then a one liner in base R does what I think you want:
avgs <- aggregate(pm25 ~ loc + Date, FUN = mean, data = dat)

See ?aggregate for details. Note in particular that you can get
results simultaneously for several different pm's or whatevers. I
should also note that the so-called "Tidyverse" and "data.table"
groups of packages, and likely others, also can easily do these sorts
of things, though of course with different syntax, semantics, and
functionality, perhaps in ways that you might find simpler to master..
There are many good tutorials available for both base R and these
packages, but from my ignorant perspective, you need to first spend
some time to learn about R's basic data structures (factors, lists,
vectors, etc.) if you want to use R for serious data manipulation.
Finally, my best advice would be to forget about SAS if you wish to
use R. Trying to translate SAS paradigms into R is the devil's work.

Cheers,
Bert






On Sun, Dec 1, 2024 at 7:29?PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Bert, Avi:
>
> I stand accused of "using SAS-like strategies for your data analysis rather than R-like [analyses]." Although I am guilty, but I beg the court's mercy ;).  I have been a SAS programmer for more than 35 years. I have used R (and S-Plus) for about 20-years, but mostly for statistical analyses that required little or no data manipulation. I now need to use R for both statistical analyses and data manipulation and am trying to do in R what I can do easily is SAS.
>
> Here is a full description of my data manipulation problem:
> I have satellite data of airborne pollutants, obtained every 15-minutes over four-days from approximately 500 geographic areas=438 observations/4 days*500 geographic locations=approximately 250,000 individual observations. (I say approximately because I have slightly less than four-days data and I have slightly less than 500 geographic areas. The exact number of observations is of minor importance.)
>
> Each of the 500 geographic areas has a fixed longitude and latitude. Each of the 438 observations for each of the approximately 500 geographic areas has a date-time stamp. I need to compute average 24-hour pollutant (e.g. pm2.5) exposure for each day, across ALL 500 geographic areas. To accomplish this, I need to
>
> 1) Group data from each of the 500 geographic areas together
>
> 2) Within each geographic area order the observations by day and time
>         1) and 2) are easily accomplished using the R order function,
>       mydata<-mydata[order(mydata$lat_lon,mydata$Time),]
>
>  or SAS proc sort:
>         proc sort data=mydata;
>            by lat_lon daytime;  /* lat long is a string giving latitude and longitude */
>         run;
>
> 3) For each geographic area determine the records that mark the start and stop of each of the four days, let's say from 00:00 hrs to 23:59 hours, and create a variable, daynum that indicates the day number (valid values 1 to 4). I do not know how to accomplish this in R. This is the part of my analysis that I asked the R community to help me write. I know how this this can be easily done in SAS:
> * Arrange data date and time within each geographic region (i.e. lat_lon and daytime);
> proc sort data=mydata;
>   by lat_lon daytime;
> run;
>
> data mydata;
>   /* For each getgraphic area, each time a new record is run, keep the preceding value of daynum */
>   retain daynum;
>   set mydata;
>      by lat_lon daytime;
>   /* initialize daynum to 0 for first record from a given geographic location*/
>   if _n_ eq 1 then daynum=0;
>  /* Determine start of each day */
> mytime = timepart(daytime)  /* Extract time from date-time constant */
>   if mytime eq '00:00:00't then daynum=daynum+1; /* Increment daynum for each new day */
> run;
>
> 4) Get average value for a pollutant, pm25, by day across all 500 geographic areas. This is easily done in SAS using proc sort and proc means.
> proc sort data=mydata;
>   by daynum;
> run;
>
> * Get mean pm 2.5 by day accross all 500 geographic regions.;
> proc means data=mydata;
>   by daynum;
>   var pm25;
> run;
>
> If I can get step (3) above accomplished in R, I know how to accomplish step 4) in R using the by function:
> by(mydata[,"pm25"], mydata[,"daynum"],mean)
>
> I am trying to write the analysis described, and written in SAS, for 3) above in R. Please understand that I am fluent in SAS, and (except for straight forward analyses that require little or no data manipulation, where I am an intermediate programmer) i am an R tyro.
>
> Thank you for your help. My apologies for the long description of what I am trying to do. I sent this because you asked what I was trying to do and why I was doing it from the perspective of a SAS programmer rather than a matrix-based R programmer.
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine, University of Maryland School of Medicine;
> Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> Senior Statistician University of Maryland Center for Vascular Research;
>
> Division of Gerontology and Paliative Care,
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> Cell phone 443-418-5382
>
>
>
>
> ________________________________________
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Saturday, November 30, 2024 11:33 PM
> To: Sorkin, John
> Cc: r-help at r-project.org (r-help at r-project.org)
> Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows
>
> May I ask *why* you want to do this?
>
> It sounds to me like like you're using SAS-like strategies for your
> data analysis rather than R-like.
>
> -- Bert
>
> -- Bert
>
> On Sat, Nov 30, 2024 at 6:27?PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
> >
> > Dear R help folks,
> >
> > First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> >
> > I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> >
> > e.g. if my original data is
> >  olddata
> >    ID date
> >     1     1
> >     1     1
> >     1     2
> >     1     2
> >     1     3
> >     1     3
> >     1     4
> >     1     4
> >     1     5
> >     1     5
> >     2     5
> >     2     5
> >     2     5
> >     2     6
> >     2     6
> >     2     6
> >     3   10
> >     3   10
> >
> > the new data will be
> > newdata
> >    ID date  first
> >     1     1       1
> >     1     1       0
> >     1     2       0
> >     1     2       0
> >     1     3       0
> >     1     3       0
> >     1     4       0
> >     1     4       0
> >     1     5       0
> >     1     5       0
> >     2     5       1
> >     2     5       0
> >     2     5       0
> >     2     6       0
> >     2     6       0
> >     2     6       0
> >     3   10       1
> >     3   10       0
> >
> > When I run the program below, I receive the following error:
> > Error in df[, "ID"] : incorrect number of dimensions
> >
> > My code:
> > # Create data.frame
> > ID <- c(rep(1,10),rep(2,6),rep(3,2))
> > date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
> >           rep(5,3),rep(6,3),rep(10,2))
> > olddata <- data.frame(ID=ID,date=date)
> > class(olddata)
> > cat("This is the original data frame","\n")
> > print(olddata)
> >
> > # This function is supposed to identify the first row
> > # within each level of ID and, for the first row, set
> > # the variable first to 1, and for all rows other than
> > # the first row set first to 0.
> > mydoit <- function(df){
> >   value <- ifelse (first(df[,"ID"]),1,0)
> >   cat("value=",value,"\n")
> >   df[,"first"] <- value
> > }
> > newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> >
> > Thank you,
> > John
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine, University of Maryland School of Medicine;
> > Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> > PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> > Senior Statistician University of Maryland Center for Vascular Research;
> >
> > Division of Gerontology and Paliative Care,
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > Cell phone 443-418-5382
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From ro||turner @end|ng |rom po@teo@net  Mon Dec  2 07:47:20 2024
From: ro||turner @end|ng |rom po@teo@net (Rolf Turner)
Date: Mon,  2 Dec 2024 06:47:20 +0000
Subject: [R] Fortune nomination.
In-Reply-To: <CAGxFJbQ_YjZND6XFGgpA2fCOxWdw-zRwSTpr1OnZA-FsPxBW7A@mail.gmail.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQzXRAQzgnyS7QTiN3ewCkLiMvpJb6AiEhTp4EvstesHw@mail.gmail.com>
 <DM6PR03MB5049F365BD14FF48597163E3E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
 <CAGxFJbQ_YjZND6XFGgpA2fCOxWdw-zRwSTpr1OnZA-FsPxBW7A@mail.gmail.com>
Message-ID: <20241202194720.3baf35af@new-hp>


On Sun, 1 Dec 2024 21:43:45 -0800
Bert Gunter <bgunter.4567 at gmail.com> wrote:


> Finally, my best advice would be to forget about SAS if you wish to
> use R. Trying to translate SAS paradigms into R is the devil's work.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Stats. Dep't. (secretaries) phone:
         +64-9-373-7599 ext. 89622
Home phone: +64-9-480-4619


From @vi@e@gross m@iii@g oii gm@ii@com  Mon Dec  2 06:39:28 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Mon, 2 Dec 2024 00:39:28 -0500
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <DM6PR03MB50499E32539371619AB9CE39E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>
 <CAGxFJbTUEFgeeDXe0rpAe4EfGN8edJAC1oVSjMTKuat-iBkSww@mail.gmail.com>
 <DM6PR03MB50499E32539371619AB9CE39E2352@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <000701db447c$8df72760$a9e57620$@gmail.com>

John,

Thanks for enlightening us so we better understand.

I won't argue with your wish to learn to do things in base R first. I started that way, myself, and found lots of the commands not particularly easy to fit into a single worldview. Many functions I read about were promptly forgotten, especially those without great documentation and not enough examples of real world usage.

This is why some packages that came later are important as they generally try to come up with a somewhat consistent set of tools that often are also faster and more flexible. There is often a set of reasons various packages are created in the first place to meet real needs. And, I note that some may be subtle. Original R was often inconsistent in the order of command arguments while the dplyr and other tidyverse command try as much as possible to make the first argument be the one normally passed through a pipeline. R fairly recently added a native pipe operator that may be faster than the magrittr pipe but in some ways makes some functionality harder. The rest of R has not really been changed to make using commands in pipelines easy.

You seem to have also looked at data.table and given you may have large amounts of data, it may be designed in ways that might also be beneficial.

But as I do not want to relearn lots of R functions I never use, I will bow out from further discussion as what I would offer these days would probably not be what you want.

My personal opinion is that proper use of R can actually be far easier and more flexible than you had with the proprietary software that may largely consist of canned reports often used.

I do want to point out a few things to consider.

When you go grouping, you may want to consider grouping (as well as sorting) by multipole variables. You mention a variable with about 500 possibilities and then another variable with an ID number but did not say the ID number was unique across them all. 

And, I want to note you may want to also look into testing the sanity of your data. That is a wide area too. Things like duplicates, for example.

I do not know how many steps you can handle but there are sometimes designs that make an algorithm work differently.

Consider your request to find  the first row in each grouping and add a column with a 1, and 0 for all others. If that is what you need, fine.

But, what if instead you just added a row number. Some rows would have a 1, and some may have a 2, 3, or 4.

When you wanted  to so something to just the rows with a 1, you can filter out a subset of the data easily enough or apply a command only to those rows. But if you want to test if any entry has more than 4 rows, this could allow you to detect an error. Other ideas might be possible if that is how the data was saved.

And, if it really is a 0/1 choice, fine, but consider the advantages or disadvantages of what you save in the new column. Storing a numeric or an int can take up space when storing a Boolean or TRUE/FALSE is what you need. R gives you lots of flexibility which perhaps you did not have to think about before.

All I know is that so much of what you want to do is easily enough done with a pipeline or two in dplyr. But this is your task and you choose what makes sense. It specializes in group analysis and generates reports and so on. It may not be how you think. 


-----Original Message-----
From: Sorkin, John <jsorkin at som.umaryland.edu> 
Sent: Sunday, December 1, 2024 11:19 PM
To: Bert Gunter <bgunter.4567 at gmail.com>; Rui Barradas <ruipbarradas at sapo.pt>; twoolman at ontargettek.com; tebert at ufl.edu; Bert Gunter <bgunter.4567 at gmail.com>; jdnewmil at dcn.davis.ca.us; avi.e.gross at gmail.com; therneau at mayo.edu; dwinsemius at comcast.net; tebert at ufl.edu; rmh at temple.edu; ken.knoblauch at inserm.fr; boris.steipe at utoronto.ca
Cc: r-help at r-project.org (r-help at r-project.org) <r-help at r-project.org>; kimmo.elo at uef.fi
Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows

Dear Colleagues,

I am grateful to all of you for helping me with my question, how to write R code that will identify the first row of each ID within a data frame, create a variable first=1 for the first row and first=0 for all repeats of the ID.

WOW!!!
I just saw Boris Steipe's answer to my question:
olddata$first <- as.numeric(! duplicated(olddata$ID))
The solution is elegant, short, easy to understand, and it uses base R! All important characteristics of a good solution, at least for me. While I want to learn solutions using packages that extend base R, I believe that a good programmer learns how to do something using the base language and once that is learned, explores way to solve a programing problem using advanced packages.

Each and every one of you (I hope I did not miss anyone in my list of email addresses) took the time to read my emails and respond to me. Your collective help is invaluable, and I am in your collect debt.

Many, many thanks,
John

John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




________________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Sunday, December 1, 2024 11:30 AM
To: Rui Barradas
Cc: Sorkin, John; r-help at r-project.org (r-help at r-project.org)
Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows

Rui:
"f these two, diff is faster. But of all the solutions posted so far,
Ben Bolker's is the fastest."

But the explicit version of diff is still considerably faster:

> D <- c(rep(1,10),rep(2,6),rep(3,2))

> microbenchmark(c(1L,diff(D)), times = 1000L)
Unit: microseconds
           expr   min    lq    mean median    uq    max neval
 c(1L, diff(D)) 3.075 3.198 3.34396   3.28 3.362 29.684  1000

> microbenchmark( as.integer(!duplicated(D)), times =1000L)
Unit: microseconds
                       expr   min    lq     mean median   uq  max neval
 as.integer(!duplicated(D)) 1.476 1.558 1.644264  1.599 1.64 16.4  1000

> microbenchmark( D - c(0L, D[-length(D)]), times = 1000L)
Unit: nanoseconds  ## note that unit is nanoseconds not microseconds
                     expr min  lq    mean median  uq  max neval
 D - c(0L, D[-length(D)]) 369 410 489.335    492 533 9840  1000

Cheers,
Bert

On Sat, Nov 30, 2024 at 11:05?PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> ?s 02:27 de 01/12/2024, Sorkin, John escreveu:
> > Dear R help folks,
> >
> > First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> >
> > I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> >
> > e.g. if my original data is
> >   olddata
> >     ID date
> >      1     1
> >      1     1
> >      1     2
> >      1     2
> >      1     3
> >      1     3
> >      1     4
> >      1     4
> >      1     5
> >      1     5
> >      2     5
> >      2     5
> >      2     5
> >      2     6
> >      2     6
> >      2     6
> >      3   10
> >      3   10
> >
> > the new data will be
> > newdata
> >     ID date  first
> >      1     1       1
> >      1     1       0
> >      1     2       0
> >      1     2       0
> >      1     3       0
> >      1     3       0
> >      1     4       0
> >      1     4       0
> >      1     5       0
> >      1     5       0
> >      2     5       1
> >      2     5       0
> >      2     5       0
> >      2     6       0
> >      2     6       0
> >      2     6       0
> >      3   10       1
> >      3   10       0
> >
> > When I run the program below, I receive the following error:
> > Error in df[, "ID"] : incorrect number of dimensions
> >
> > My code:
> > # Create data.frame
> > ID <- c(rep(1,10),rep(2,6),rep(3,2))
> > date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
> >            rep(5,3),rep(6,3),rep(10,2))
> > olddata <- data.frame(ID=ID,date=date)
> > class(olddata)
> > cat("This is the original data frame","\n")
> > print(olddata)
> >
> > # This function is supposed to identify the first row
> > # within each level of ID and, for the first row, set
> > # the variable first to 1, and for all rows other than
> > # the first row set first to 0.
> > mydoit <- function(df){
> >    value <- ifelse (first(df[,"ID"]),1,0)
> >    cat("value=",value,"\n")
> >    df[,"first"] <- value
> > }
> > newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> >
> > Thank you,
> > John
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine, University of Maryland School of Medicine;
> > Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> > PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> > Senior Statistician University of Maryland Center for Vascular Research;
> >
> > Division of Gerontology and Paliative Care,
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > Cell phone 443-418-5382
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> Hello,
>
> And here are two other solutions.
>
>
> olddata$first <- with(olddata, ave(seq_along(ID), ID, FUN = \(x) x ==
> x[1L]))
>
> olddata$first <- c(1L, diff(olddata$ID))
>
>
> Of these two, diff is faster. But of all the solutions posted so far,
> Ben Bolker's is the fastest. And it can be made a little faster if
> as.integer substitutes for as.numeric.
> And dplyr::mutate now has a .by argument, which avoids explicit the call
> to group_by, with a performance gain.
>
>
> library(microbenchmark)
>
> mb <- microbenchmark(
>    ave = with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == x[1L])),
>    dup_num = as.numeric(! duplicated(olddata$ID)),
>    dup_int = as.integer(! duplicated(olddata$ID)),
>    diff = diff = c(1L, diff(olddata$ID)),
>    dplyr_grp = olddata %>% group_by(ID) %>% mutate(first =
> as.integer(row_number() == 1)),
>    dplyr = olddata %>% mutate(first = as.integer(row_number() == 1), .by
> = ID)
> )
> print(mb, order = "median")
>
>
>
> However, note that dplyr operates in entire data.frames and therefore is
> expected to be slower when tested against instructions that process one
> column only.
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> --
> Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
> http://www.avg.com/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Mon Dec  2 05:18:40 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Mon, 2 Dec 2024 04:18:40 +0000
Subject: [R] Identify first row of each ID within a data frame,
 create a variable first =1 for the first row and first=0 of all
 other rows
In-Reply-To: <CAGxFJbTUEFgeeDXe0rpAe4EfGN8edJAC1oVSjMTKuat-iBkSww@mail.gmail.com>
References: <DM6PR03MB5049ED4EE95641C693B3D7EAE2342@DM6PR03MB5049.namprd03.prod.outlook.com>
 <b888c330-de36-4a00-aeff-b38385baa085@sapo.pt>
 <CAGxFJbTUEFgeeDXe0rpAe4EfGN8edJAC1oVSjMTKuat-iBkSww@mail.gmail.com>
Message-ID: <DM6PR03MB50499E32539371619AB9CE39E2352@DM6PR03MB5049.namprd03.prod.outlook.com>

Dear Colleagues,

I am grateful to all of you for helping me with my question, how to write R code that will identify the first row of each ID within a data frame, create a variable first=1 for the first row and first=0 for all repeats of the ID.

WOW!!!
I just saw Boris Steipe's answer to my question:
olddata$first <- as.numeric(! duplicated(olddata$ID))
The solution is elegant, short, easy to understand, and it uses base R! All important characteristics of a good solution, at least for me. While I want to learn solutions using packages that extend base R, I believe that a good programmer learns how to do something using the base language and once that is learned, explores way to solve a programing problem using advanced packages.

Each and every one of you (I hope I did not miss anyone in my list of email addresses) took the time to read my emails and respond to me. Your collective help is invaluable, and I am in your collect debt.

Many, many thanks,
John

John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




________________________________________
From: Bert Gunter <bgunter.4567 at gmail.com>
Sent: Sunday, December 1, 2024 11:30 AM
To: Rui Barradas
Cc: Sorkin, John; r-help at r-project.org (r-help at r-project.org)
Subject: Re: [R] Identify first row of each ID within a data frame, create a variable first =1 for the first row and first=0 of all other rows

Rui:
"f these two, diff is faster. But of all the solutions posted so far,
Ben Bolker's is the fastest."

But the explicit version of diff is still considerably faster:

> D <- c(rep(1,10),rep(2,6),rep(3,2))

> microbenchmark(c(1L,diff(D)), times = 1000L)
Unit: microseconds
           expr   min    lq    mean median    uq    max neval
 c(1L, diff(D)) 3.075 3.198 3.34396   3.28 3.362 29.684  1000

> microbenchmark( as.integer(!duplicated(D)), times =1000L)
Unit: microseconds
                       expr   min    lq     mean median   uq  max neval
 as.integer(!duplicated(D)) 1.476 1.558 1.644264  1.599 1.64 16.4  1000

> microbenchmark( D - c(0L, D[-length(D)]), times = 1000L)
Unit: nanoseconds  ## note that unit is nanoseconds not microseconds
                     expr min  lq    mean median  uq  max neval
 D - c(0L, D[-length(D)]) 369 410 489.335    492 533 9840  1000

Cheers,
Bert

On Sat, Nov 30, 2024 at 11:05?PM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> ?s 02:27 de 01/12/2024, Sorkin, John escreveu:
> > Dear R help folks,
> >
> > First my apologizes for sending several related questions to the list server. I am trying to learn how to manipulate data in R . . . and am having difficulty getting my program to work. I greatly appreciate the help and support list member give!
> >
> > I am trying to write a program that will run through a data frame organized by ID and for the first line of each new group of data lines that has the same ID create a new variable first that will be 1 for the first line of the group and 0 for all other lines.
> >
> > e.g. if my original data is
> >   olddata
> >     ID date
> >      1     1
> >      1     1
> >      1     2
> >      1     2
> >      1     3
> >      1     3
> >      1     4
> >      1     4
> >      1     5
> >      1     5
> >      2     5
> >      2     5
> >      2     5
> >      2     6
> >      2     6
> >      2     6
> >      3   10
> >      3   10
> >
> > the new data will be
> > newdata
> >     ID date  first
> >      1     1       1
> >      1     1       0
> >      1     2       0
> >      1     2       0
> >      1     3       0
> >      1     3       0
> >      1     4       0
> >      1     4       0
> >      1     5       0
> >      1     5       0
> >      2     5       1
> >      2     5       0
> >      2     5       0
> >      2     6       0
> >      2     6       0
> >      2     6       0
> >      3   10       1
> >      3   10       0
> >
> > When I run the program below, I receive the following error:
> > Error in df[, "ID"] : incorrect number of dimensions
> >
> > My code:
> > # Create data.frame
> > ID <- c(rep(1,10),rep(2,6),rep(3,2))
> > date <- c(rep(1,2),rep(2,2),rep(3,2),rep(4,2),rep(5,2),
> >            rep(5,3),rep(6,3),rep(10,2))
> > olddata <- data.frame(ID=ID,date=date)
> > class(olddata)
> > cat("This is the original data frame","\n")
> > print(olddata)
> >
> > # This function is supposed to identify the first row
> > # within each level of ID and, for the first row, set
> > # the variable first to 1, and for all rows other than
> > # the first row set first to 0.
> > mydoit <- function(df){
> >    value <- ifelse (first(df[,"ID"]),1,0)
> >    cat("value=",value,"\n")
> >    df[,"first"] <- value
> > }
> > newdata <- aggregate(olddata,list(olddata[,"ID"]),mydoit)
> >
> > Thank you,
> > John
> >
> >
> > John David Sorkin M.D., Ph.D.
> > Professor of Medicine, University of Maryland School of Medicine;
> > Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;
> > PI Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
> > Senior Statistician University of Maryland Center for Vascular Research;
> >
> > Division of Gerontology and Paliative Care,
> > 10 North Greene Street
> > GRECC (BT/18/GR)
> > Baltimore, MD 21201-1524
> > Cell phone 443-418-5382
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> Hello,
>
> And here are two other solutions.
>
>
> olddata$first <- with(olddata, ave(seq_along(ID), ID, FUN = \(x) x ==
> x[1L]))
>
> olddata$first <- c(1L, diff(olddata$ID))
>
>
> Of these two, diff is faster. But of all the solutions posted so far,
> Ben Bolker's is the fastest. And it can be made a little faster if
> as.integer substitutes for as.numeric.
> And dplyr::mutate now has a .by argument, which avoids explicit the call
> to group_by, with a performance gain.
>
>
> library(microbenchmark)
>
> mb <- microbenchmark(
>    ave = with(olddata, ave(seq_along(ID), ID, FUN = \(x) x == x[1L])),
>    dup_num = as.numeric(! duplicated(olddata$ID)),
>    dup_int = as.integer(! duplicated(olddata$ID)),
>    diff = diff = c(1L, diff(olddata$ID)),
>    dplyr_grp = olddata %>% group_by(ID) %>% mutate(first =
> as.integer(row_number() == 1)),
>    dplyr = olddata %>% mutate(first = as.integer(row_number() == 1), .by
> = ID)
> )
> print(mb, order = "median")
>
>
>
> However, note that dplyr operates in entire data.frames and therefore is
> expected to be slower when tested against instructions that process one
> column only.
>
>
> Hope this helps,
>
> Rui Barradas
>
>
> --
> Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
> http://www.avg.com/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Wed Dec  4 13:38:45 2024
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Wed, 4 Dec 2024 13:38:45 +0100
Subject: [R] Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
Message-ID: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>

Dear list,

is anyone aware of the following behavious of diag when used to replace 
diagonals (plural!) of a matrix?

Small example: The following is documented and clearly to be expected:

A <- matrix(0, nrow = 5, ncol = 5)
diag(A) <- 1; A


BUT, what about the following? When executing the code of `diag<-` line 
by line, it throws errors. So why does it work?

diag(A[-1, ]) <- 2; A

diag(A[-5, -1]) <- 3; A

diag(A[-5, -(1:2)]) <- 4; A


Any ideas?

  TIA and best regards  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 215
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
https://www.uni-giessen.de/math/eichner


From bgunter@4567 @end|ng |rom gm@||@com  Wed Dec  4 14:38:44 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 4 Dec 2024 05:38:44 -0800
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
Message-ID: <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>

matrices are vectors with a "dim" attribute.
So what I think is happening is:

> A <- matrix(1:25, nrow = 5, ncol = 5)
> diag(A[-1,]) <- 0
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16   21
[2,]    0    7   12   17   22
[3,]    3    0   13   18   23
[4,]    4    9    0   19   24
[5,]    5   10   15    0   25
>
> ## is equivqalent to:
>
> A <- matrix(1:25, nrow = 5, ncol = 5)
> wh <- c(diag(A[-1,]))   # A's vector indices of diag(A[-1,])
> A[wh] <- 0
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    6   11   16   21
[2,]    0    7   12   17   22
[3,]    3    0   13   18   23
[4,]    4    9    0   19   24
[5,]    5   10   15    0   25

I didn't check, but I assume your other examples would work similarly.
Please repost if I am wrong.

Cheers,
Bert

On Wed, Dec 4, 2024 at 4:39?AM Gerrit Eichner
<gerrit.eichner at math.uni-giessen.de> wrote:
>
> Dear list,
>
> is anyone aware of the following behavious of diag when used to replace
> diagonals (plural!) of a matrix?
>
> Small example: The following is documented and clearly to be expected:
>
> A <- matrix(0, nrow = 5, ncol = 5)
> diag(A) <- 1; A
>
>
> BUT, what about the following? When executing the code of `diag<-` line
> by line, it throws errors. So why does it work?
>
> diag(A[-1, ]) <- 2; A
>
> diag(A[-5, -1]) <- 3; A
>
> diag(A[-5, -(1:2)]) <- 4; A
>
>
> Any ideas?
>
>   TIA and best regards  --  Gerrit
>
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner                   Mathematical Institute, Room 215
> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> https://www.uni-giessen.de/math/eichner
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Dec  4 15:31:30 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 4 Dec 2024 09:31:30 -0500
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
Message-ID: <f8d638cf-75ab-41c6-9127-b0a69c162330@gmail.com>

On 2024-12-04 7:38 a.m., Gerrit Eichner wrote:
> Dear list,
> 
> is anyone aware of the following behavious of diag when used to replace
> diagonals (plural!) of a matrix?
> 
> Small example: The following is documented and clearly to be expected:
> 
> A <- matrix(0, nrow = 5, ncol = 5)
> diag(A) <- 1; A
> 
> 
> BUT, what about the following? When executing the code of `diag<-` line
> by line, it throws errors. So why does it work?
> 
> diag(A[-1, ]) <- 2; A
> 
> diag(A[-5, -1]) <- 3; A
> 
> diag(A[-5, -(1:2)]) <- 4; A
> 

Could you show us the log of what you did that generated errors?  The 
statement `diag(A[-1, ]) <- 2` is pretty complex; it involves two 
assignment functions (both `diag<-` and `[<-`), so you might have tried 
to execute the wrong thing.

Duncan Murdoch


From dyk|m7411 @end|ng |rom gm@||@com  Thu Dec  5 05:44:25 2024
From: dyk|m7411 @end|ng |rom gm@||@com (D)
Date: Wed, 4 Dec 2024 23:44:25 -0500
Subject: [R] Save spatial data in a csv file
In-Reply-To: <CAP+bYWCkzSSiBJ_FPH7fyYYcUV=4yGRP_baBe4UvMbMg87p0OQ@mail.gmail.com>
References: <CADmwX-LXiw0yrVy2a0X+eERg=Vuv=pYGG8h16kdyuLLw3x2p+w@mail.gmail.com>
 <CAP+bYWCkzSSiBJ_FPH7fyYYcUV=4yGRP_baBe4UvMbMg87p0OQ@mail.gmail.com>
Message-ID: <CADmwX-+sph_hwWPrBNPaby6yyNYq0cWoX=pi3jeUmf6_A=iLDQ@mail.gmail.com>

Hi Hasan,

Thank you for the advice.  What does that mean by "respond with the output
of `dput(nyc_ct_geo)`"?  Please provide more details to address the
problem.  Thank you!

On Sat, Nov 30, 2024 at 12:47?PM Hasan Diwan <hasan.diwan at gmail.com> wrote:

> Kindly respond with the output of `dput(nyc_ct_geo)` -- thank you! -- H
>
>
> --
> OpenPGP: https://hasan.d8u.us/openpgp.asc
> If you wish to request my time, please do so using
> *bit.ly/hd1AppointmentRequest
> <http://bit.ly/hd1AppointmentRequest>*.
> Si vous voudriez faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
> <http://bit.ly/hd1AppointmentRequest>*.
>
> <https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
> >Sent
> from my mobile device
> Envoye de mon portable
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Thu Dec  5 12:04:13 2024
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Thu, 5 Dec 2024 12:04:13 +0100
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
 <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>
Message-ID: <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>

Hi, Bert,

thx, but I don't think so, because your example fails, if you use
different entries for matrix A than your vector 1:25. (Try A filled with 
only zeroes, e.g.)

I guess, Duncan Murdochs hint is crucial. I'll comment on his asap.

  Best regards  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 215
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
https://www.uni-giessen.de/math/eichner
---------------------------------------------------------------------

Am 04.12.2024 um 14:38 schrieb Bert Gunter:
> matrices are vectors with a "dim" attribute.
> So what I think is happening is:
> 
>> A <- matrix(1:25, nrow = 5, ncol = 5)
>> diag(A[-1,]) <- 0
>> A
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16   21
> [2,]    0    7   12   17   22
> [3,]    3    0   13   18   23
> [4,]    4    9    0   19   24
> [5,]    5   10   15    0   25
>>
>> ## is equivqalent to:
>>
>> A <- matrix(1:25, nrow = 5, ncol = 5)
>> wh <- c(diag(A[-1,]))   # A's vector indices of diag(A[-1,])
>> A[wh] <- 0
>> A
>       [,1] [,2] [,3] [,4] [,5]
> [1,]    1    6   11   16   21
> [2,]    0    7   12   17   22
> [3,]    3    0   13   18   23
> [4,]    4    9    0   19   24
> [5,]    5   10   15    0   25
> 
> I didn't check, but I assume your other examples would work similarly.
> Please repost if I am wrong.
> 
> Cheers,
> Bert
> 
> On Wed, Dec 4, 2024 at 4:39?AM Gerrit Eichner
> <gerrit.eichner at math.uni-giessen.de> wrote:
>>
>> Dear list,
>>
>> is anyone aware of the following behavious of diag when used to replace
>> diagonals (plural!) of a matrix?
>>
>> Small example: The following is documented and clearly to be expected:
>>
>> A <- matrix(0, nrow = 5, ncol = 5)
>> diag(A) <- 1; A
>>
>>
>> BUT, what about the following? When executing the code of `diag<-` line
>> by line, it throws errors. So why does it work?
>>
>> diag(A[-1, ]) <- 2; A
>>
>> diag(A[-5, -1]) <- 3; A
>>
>> diag(A[-5, -(1:2)]) <- 4; A
>>
>>
>> Any ideas?
>>
>>    TIA and best regards  --  Gerrit
>>
>> ---------------------------------------------------------------------
>> Dr. Gerrit Eichner                   Mathematical Institute, Room 215
>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>> https://www.uni-giessen.de/math/eichner
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Thu Dec  5 12:18:08 2024
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Thu, 5 Dec 2024 12:18:08 +0100
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <f8d638cf-75ab-41c6-9127-b0a69c162330@gmail.com>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
 <f8d638cf-75ab-41c6-9127-b0a69c162330@gmail.com>
Message-ID: <a9acfc1d-7b2b-4345-a6f6-f3d443378894@math.uni-giessen.de>

Hi, Duncan,

thx for your valuable hint re the two assigment functions! That might be 
the reason. Nevertheless, I couldn't figure out how they work together 
here. I simply executed the relevant lines of code which I found in 
`diag<-`body after assigning the matrix to x (which, of course, cannot 
yield the same as diag(A[-1,]) <- 7 in the light of your explanation):

A <- matrix(0, nrow = 5, ncol = 5)

x <- A[-1,]
value <- 7


# From `diag<-`'s body:

dx <- dim(x)
len.i <- min(dx)
len.v <- length(value)
if (len.i) {
   i <- seq_len(len.i)
   x[cbind(i, i)] <- value
   }
x



I fiddled a bit around with expressions, but to no avail.
Could you (or anybody else) maybe still shed some more light on this?
Thx a lot in advance!

  Best  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 215
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
https://www.uni-giessen.de/math/eichner
---------------------------------------------------------------------

Am 04.12.2024 um 15:31 schrieb Duncan Murdoch:
> On 2024-12-04 7:38 a.m., Gerrit Eichner wrote:
>> Dear list,
>>
>> is anyone aware of the following behavious of diag when used to replace
>> diagonals (plural!) of a matrix?
>>
>> Small example: The following is documented and clearly to be expected:
>>
>> A <- matrix(0, nrow = 5, ncol = 5)
>> diag(A) <- 1; A
>>
>>
>> BUT, what about the following? When executing the code of `diag<-` line
>> by line, it throws errors. So why does it work?
>>
>> diag(A[-1, ]) <- 2; A
>>
>> diag(A[-5, -1]) <- 3; A
>>
>> diag(A[-5, -(1:2)]) <- 4; A
>>
> 
> Could you show us the log of what you did that generated errors?? The 
> statement `diag(A[-1, ]) <- 2` is pretty complex; it involves two 
> assignment functions (both `diag<-` and `[<-`), so you might have tried 
> to execute the wrong thing.
> 
> Duncan Murdoch
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu Dec  5 12:42:19 2024
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 5 Dec 2024 11:42:19 +0000
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
Message-ID: <fe70e869-53cc-47c6-8c8f-2a00c082e31a@sapo.pt>

?s 12:38 de 04/12/2024, Gerrit Eichner escreveu:
> Dear list,
> 
> is anyone aware of the following behavious of diag when used to replace 
> diagonals (plural!) of a matrix?
> 
> Small example: The following is documented and clearly to be expected:
> 
> A <- matrix(0, nrow = 5, ncol = 5)
> diag(A) <- 1; A
> 
> 
> BUT, what about the following? When executing the code of `diag<-` line 
> by line, it throws errors. So why does it work?
> 
> diag(A[-1, ]) <- 2; A
> 
> diag(A[-5, -1]) <- 3; A
> 
> diag(A[-5, -(1:2)]) <- 4; A
> 
> 
> Any ideas?
> 
>  ?TIA and best regards? --? Gerrit
> 
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner?????????????????? Mathematical Institute, Room 215
> gerrit.eichner at math.uni-giessen.de?? Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104????????? Arndtstr. 2, 35392 Giessen, Germany
> https://www.uni-giessen.de/math/eichner
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
Hello,

I cannot reproduce any error, everything works as expected.
sessionInfo at the end.



A <- matrix(0, nrow = 5, ncol = 5)
diag(A) <- 1; A
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    1    0    0    0    0
# [2,]    0    1    0    0    0
# [3,]    0    0    1    0    0
# [4,]    0    0    0    1    0
# [5,]    0    0    0    0    1

diag(A[-1, ]) <- 2; A
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    1    0    0    0    0
# [2,]    2    1    0    0    0
# [3,]    0    2    1    0    0
# [4,]    0    0    2    1    0
# [5,]    0    0    0    2    1

diag(A[-5, -1]) <- 3; A
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    1    3    0    0    0
# [2,]    2    1    3    0    0
# [3,]    0    2    1    3    0
# [4,]    0    0    2    1    3
# [5,]    0    0    0    2    1

diag(A[-5, -(1:2)]) <- 4; A
#      [,1] [,2] [,3] [,4] [,5]
# [1,]    1    3    4    0    0
# [2,]    2    1    3    4    0
# [3,]    0    2    1    3    4
# [4,]    0    0    2    1    3
# [5,]    0    0    0    2    1

sessionInfo()
# R version 4.4.2 (2024-10-31 ucrt)
# Platform: x86_64-w64-mingw32/x64
# Running under: Windows 11 x64 (build 22631)
#
# Matrix products: default
#
#
# locale:
# [1] LC_COLLATE=Portuguese_Portugal.utf8 
LC_CTYPE=Portuguese_Portugal.utf8
# [3] LC_MONETARY=Portuguese_Portugal.utf8 LC_NUMERIC=C 

# [5] LC_TIME=Portuguese_Portugal.utf8
#
# time zone: Europe/Lisbon
# tzcode source: internal
#
# attached base packages:
# [1] stats     graphics  grDevices utils     datasets  methods   base
#
# loaded via a namespace (and not attached):
# [1] compiler_4.4.2
#



Hope this helps,

Rui Barradas


-- 
Este e-mail foi analisado pelo software antiv?rus AVG para verificar a presen?a de v?rus.
www.avg.com


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Dec  5 12:46:47 2024
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 5 Dec 2024 11:46:47 +0000
Subject: [R] Save spatial data in a csv file
Message-ID: <SV0P279MB04755E748B51A94694C104AEEE302@SV0P279MB0475.NORP279.PROD.OUTLOOK.COM>

As Hasan suggested, much more information is needed. It would be very helpful to know the class of nyc_ct_geo, and how it was created. It is possibly an sf object from the print output. 

>From this, assuming that it is an sf object, the geometries are polygons, not points, so need to be converted to text, best WKT (well-known text) to write to CSV. It is then best to use the appropriate function from sf with a CSV driver - minimal example:

library(sf)
nc <- st_read(system.file("gpkg/nc.gpkg", package="sf"))
tf <- tempfile(fileext=".csv")
st_write(nc_geom, tf, driver="CSV", layer_options=c("GEOMETRY=AS_WKT"))
file.show(tf)

With more than 2000 polygon objects in your case, this is hardly a sensible way to write geometries to file, and depends on the person receiving the file having software that can convert WKT back to geometries. For more details on the CSV driver for st_write, please see https://gdal.org/en/stable/drivers/vector/csv.html#vector-csv

Maybe following up on the R-sig-geo list would be helpful if clarification is needed.

Roger

--
Roger Bivand
Emeritus Professor
Norwegian School of Economics
Postboks 3490 Ytre Sandviken, 5045 Bergen, Norway
Roger.Bivand at nhh.no

From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Thu Dec  5 14:23:38 2024
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Thu, 5 Dec 2024 07:23:38 -0600
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
 <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>
 <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>
Message-ID: <9b75727b-a35b-4d56-9db4-5353b9e23d71@effectivedefense.org>

Hi, Gerrit:


On 12/5/24 05:04, Gerrit Eichner wrote:
> Hi, Bert,
> 
> thx, but I don't think so, because your example fails, if you use
> different entries for matrix A than your vector 1:25. (Try A filled with 
> only zeroes, e.g.)


YOU'VE MISUNDERSTOOD Bert's reply: A matrix is a vector, and Bert 
assigned the indices of the vector to the cells of the matrix.


 > A <- matrix(1:25, nrow = 5, ncol = 5)
 > A[1:25]
  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 
23 24 25
 > A[13]
[1] 13


	  "if you use different entries for matrix A", the entries are no 
longer the indices of A as a vector.


	  Does this fact make his example work for you?


	  It does for me.
	  Hope this helps.
	  Spencer Graves

> 
> I guess, Duncan Murdochs hint is crucial. I'll comment on his asap.
> 
>  ?Best regards? --? Gerrit
> 
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner?????????????????? Mathematical Institute, Room 215
> gerrit.eichner at math.uni-giessen.de?? Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104????????? Arndtstr. 2, 35392 Giessen, Germany
> https://www.uni-giessen.de/math/eichner
> ---------------------------------------------------------------------
> 
> Am 04.12.2024 um 14:38 schrieb Bert Gunter:
>> matrices are vectors with a "dim" attribute.
>> So what I think is happening is:
>>
>>> A <- matrix(1:25, nrow = 5, ncol = 5)
>>> diag(A[-1,]) <- 0
>>> A
>> ????? [,1] [,2] [,3] [,4] [,5]
>> [1,]??? 1??? 6?? 11?? 16?? 21
>> [2,]??? 0??? 7?? 12?? 17?? 22
>> [3,]??? 3??? 0?? 13?? 18?? 23
>> [4,]??? 4??? 9??? 0?? 19?? 24
>> [5,]??? 5?? 10?? 15??? 0?? 25
>>>
>>> ## is equivqalent to:
>>>
>>> A <- matrix(1:25, nrow = 5, ncol = 5)
>>> wh <- c(diag(A[-1,]))?? # A's vector indices of diag(A[-1,])
>>> A[wh] <- 0
>>> A
>> ????? [,1] [,2] [,3] [,4] [,5]
>> [1,]??? 1??? 6?? 11?? 16?? 21
>> [2,]??? 0??? 7?? 12?? 17?? 22
>> [3,]??? 3??? 0?? 13?? 18?? 23
>> [4,]??? 4??? 9??? 0?? 19?? 24
>> [5,]??? 5?? 10?? 15??? 0?? 25
>>
>> I didn't check, but I assume your other examples would work similarly.
>> Please repost if I am wrong.
>>
>> Cheers,
>> Bert
>>
>> On Wed, Dec 4, 2024 at 4:39?AM Gerrit Eichner
>> <gerrit.eichner at math.uni-giessen.de> wrote:
>>>
>>> Dear list,
>>>
>>> is anyone aware of the following behavious of diag when used to replace
>>> diagonals (plural!) of a matrix?
>>>
>>> Small example: The following is documented and clearly to be expected:
>>>
>>> A <- matrix(0, nrow = 5, ncol = 5)
>>> diag(A) <- 1; A
>>>
>>>
>>> BUT, what about the following? When executing the code of `diag<-` line
>>> by line, it throws errors. So why does it work?
>>>
>>> diag(A[-1, ]) <- 2; A
>>>
>>> diag(A[-5, -1]) <- 3; A
>>>
>>> diag(A[-5, -(1:2)]) <- 4; A
>>>
>>>
>>> Any ideas?
>>>
>>> ?? TIA and best regards? --? Gerrit
>>>
>>> ---------------------------------------------------------------------
>>> Dr. Gerrit Eichner?????????????????? Mathematical Institute, Room 215
>>> gerrit.eichner at math.uni-giessen.de?? Justus-Liebig-University Giessen
>>> Tel: +49-(0)641-99-32104????????? Arndtstr. 2, 35392 Giessen, Germany
>>> https://www.uni-giessen.de/math/eichner
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide https://www.R-project.org/posting- 
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting- 
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Thu Dec  5 14:29:22 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 5 Dec 2024 05:29:22 -0800
Subject: [R] 
 Undocumented behaviour of diag when replacing the diagonal of a
 matrix?
In-Reply-To: <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>
References: <982a9d37-0f89-42bc-9394-69f8080bf227@math.uni-giessen.de>
 <CAGxFJbQAb21XyihMJ7Fiu+ctLdPdkFodjWJggQgvxKjeZ_QHpQ@mail.gmail.com>
 <305073a6-0b86-4b57-970a-da8df733163d@math.uni-giessen.de>
Message-ID: <CAGxFJbTg6=awGNdmnBX6jxx_ncM4qPXFPPyv0X09QV+MJ=4QJA@mail.gmail.com>

Gerrit:
Oh, sorry. I was obviously unclear. What I meant by "equivalent" was
"this is how the subset of indices in A whose corresponding values are
changed is selected."  So the example was meant to show this, not as a
literal representation of the code.

This might be a clearer explanation, but feel free to ignore and
certainly no need to reply.

A <- matrix(0, nrow =5, ncol = 5)

Here is what  "diag(A[-1,] <- 1" does (semantically):
B <- A[-1,]
diag(B) <- 1
A[-1,] <- B  ## etc. for all your examples

This makes semantic sense to me, i.e. what you want "diag(A[-1,] <- 1"
to mean. This puzzled me, but maybe you understood this already and
just wanted to understand how the code does this. If so, my apologies
for the spam.

Cheers,
Bert

On Thu, Dec 5, 2024 at 3:03?AM Gerrit Eichner
<gerrit.eichner at math.uni-giessen.de> wrote:
>
> Hi, Bert,
>
> thx, but I don't think so, because your example fails, if you use
> different entries for matrix A than your vector 1:25. (Try A filled with
> only zeroes, e.g.)
>
> I guess, Duncan Murdochs hint is crucial. I'll comment on his asap.
>
>   Best regards  --  Gerrit
>
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner                   Mathematical Institute, Room 215
> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> https://www.uni-giessen.de/math/eichner
> ---------------------------------------------------------------------
>
> Am 04.12.2024 um 14:38 schrieb Bert Gunter:
> > matrices are vectors with a "dim" attribute.
> > So what I think is happening is:
> >
> >> A <- matrix(1:25, nrow = 5, ncol = 5)
> >> diag(A[-1,]) <- 0
> >> A
> >       [,1] [,2] [,3] [,4] [,5]
> > [1,]    1    6   11   16   21
> > [2,]    0    7   12   17   22
> > [3,]    3    0   13   18   23
> > [4,]    4    9    0   19   24
> > [5,]    5   10   15    0   25
> >>
> >> ## is equivqalent to:
> >>
> >> A <- matrix(1:25, nrow = 5, ncol = 5)
> >> wh <- c(diag(A[-1,]))   # A's vector indices of diag(A[-1,])
> >> A[wh] <- 0
> >> A
> >       [,1] [,2] [,3] [,4] [,5]
> > [1,]    1    6   11   16   21
> > [2,]    0    7   12   17   22
> > [3,]    3    0   13   18   23
> > [4,]    4    9    0   19   24
> > [5,]    5   10   15    0   25
> >
> > I didn't check, but I assume your other examples would work similarly.
> > Please repost if I am wrong.
> >
> > Cheers,
> > Bert
> >
> > On Wed, Dec 4, 2024 at 4:39?AM Gerrit Eichner
> > <gerrit.eichner at math.uni-giessen.de> wrote:
> >>
> >> Dear list,
> >>
> >> is anyone aware of the following behavious of diag when used to replace
> >> diagonals (plural!) of a matrix?
> >>
> >> Small example: The following is documented and clearly to be expected:
> >>
> >> A <- matrix(0, nrow = 5, ncol = 5)
> >> diag(A) <- 1; A
> >>
> >>
> >> BUT, what about the following? When executing the code of `diag<-` line
> >> by line, it throws errors. So why does it work?
> >>
> >> diag(A[-1, ]) <- 2; A
> >>
> >> diag(A[-5, -1]) <- 3; A
> >>
> >> diag(A[-5, -(1:2)]) <- 4; A
> >>
> >>
> >> Any ideas?
> >>
> >>    TIA and best regards  --  Gerrit
> >>
> >> ---------------------------------------------------------------------
> >> Dr. Gerrit Eichner                   Mathematical Institute, Room 215
> >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> >> https://www.uni-giessen.de/math/eichner
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.


From beto77051 @end|ng |rom gm@||@com  Thu Dec  5 16:21:26 2024
From: beto77051 @end|ng |rom gm@||@com (Carlos Alberto Baptista de Figueiredo)
Date: Thu, 5 Dec 2024 15:21:26 +0000
Subject: [R] Creating a script in Rstudio
Message-ID: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>

Hi there





I want to create a script in Rstudio and load in the reagent dataset
ensuring that the different data types in there (dates, text, etc) come
through correctly.



Best wishes

Carlos

From C@r|o@@F|gue|redo @end|ng |rom hc@he@|thc@re@co@uk  Thu Dec  5 16:16:07 2024
From: C@r|o@@F|gue|redo @end|ng |rom hc@he@|thc@re@co@uk (Figueiredo, Carlos)
Date: Thu, 5 Dec 2024 15:16:07 +0000
Subject: [R] Spreadsheets data
Message-ID: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>

Hi there


I want to create a script in Rstudio and load in the reagent dataset ensuring that the different data types in there (dates, text, etc) come through correctly.

Best wishes

Carlos


________________________________
Notice of Confidentiality: The contents of this e-mail and any attachments are confidential and copyright to HCA International and may also be privileged. They are intended only for the use of the addressee(s) shown above. Any unauthorised reading, use, distribution or copying of them is prohibited. If this e-mail has been sent to you in error, you may not take action based on it, nor may you rely on it for any purpose. Please let us know of the error by emailing HCA.DPO at hcahealthcare.co.uk and please return this e-mail. If you have a query relating to an upcoming appointment or your clinical treatment, please email Contact at hcahealthcare.co.uk.

Whilst all reasonable care has been taken to avoid the transmission of viruses, it is the responsibility of the recipient to ensure that onward transmission, opening or use of this message and any attachments will not adversely affect its systems or data. No responsibility is accepted by HCA International in this regard and the recipient should carry out such virus and other checks as it considers appropriate. HCA International is registered in the UK, Company Number 3020522, at 2 Cavendish Square, London, W1G 0PU.

________________________________

From tr@xp|@yer @end|ng |rom gm@||@com  Fri Dec  6 00:38:08 2024
From: tr@xp|@yer @end|ng |rom gm@||@com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Fri, 6 Dec 2024 00:38:08 +0100
Subject: [R] Spreadsheets data
In-Reply-To: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
References: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
Message-ID: <CAGAA5bdEtBqvNWnbYXieXdXBkXNL3cAyF3gtPhzqiKcH-4_cFQ@mail.gmail.com>

On Thu, 5 Dec 2024 at 23:16, Figueiredo, Carlos via R-help
<r-help at r-project.org> wrote:
>
> Hi there
>
>
> I want to create a script in Rstudio and load in the reagent dataset ensuring that the different data types in there (dates, text, etc) come through correctly.

What have you tried? How good do you know R?
R contains many functions to read input.

Regards
Martin


From bgunter@4567 @end|ng |rom gm@||@com  Fri Dec  6 01:22:09 2024
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 5 Dec 2024 16:22:09 -0800
Subject: [R] Spreadsheets data
In-Reply-To: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
References: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
Message-ID: <CAGxFJbR2r7Sey=JLD5LUG7v2TTkk07S1YECiZ0swozk7aP+gKw@mail.gmail.com>

Please read **and follow** the posting guide linked below to learn how
to ask for help on this list.

Cheers,
Bert

On Thu, Dec 5, 2024 at 2:16?PM Figueiredo, Carlos via R-help
<r-help at r-project.org> wrote:
>
> Hi there
>
>
> I want to create a script in Rstudio and load in the reagent dataset ensuring that the different data types in there (dates, text, etc) come through correctly.
>
> Best wishes
>
> Carlos
>
>
> ________________________________
> Notice of Confidentiality: The contents of this e-mail and any attachments are confidential and copyright to HCA International and may also be privileged. They are intended only for the use of the addressee(s) shown above. Any unauthorised reading, use, distribution or copying of them is prohibited. If this e-mail has been sent to you in error, you may not take action based on it, nor may you rely on it for any purpose. Please let us know of the error by emailing HCA.DPO at hcahealthcare.co.uk and please return this e-mail. If you have a query relating to an upcoming appointment or your clinical treatment, please email Contact at hcahealthcare.co.uk.
>
> Whilst all reasonable care has been taken to avoid the transmission of viruses, it is the responsibility of the recipient to ensure that onward transmission, opening or use of this message and any attachments will not adversely affect its systems or data. No responsibility is accepted by HCA International in this regard and the recipient should carry out such virus and other checks as it considers appropriate. HCA International is registered in the UK, Company Number 3020522, at 2 Cavendish Square, London, W1G 0PU.
>
> ________________________________
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From po|c1410 @end|ng |rom gm@||@com  Fri Dec  6 02:18:06 2024
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Fri, 6 Dec 2024 01:18:06 +0000
Subject: [R] Creating a script in Rstudio
In-Reply-To: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>
References: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>
Message-ID: <CA+etgP=KPbsg-J02PQBbdMcfhhY-5h+7g0ueqNNpKdPqZMfRBQ@mail.gmail.com>

Carlos you are gonna have to provide at least a tiny smidge of information

Reagent dataset? What is this?

What is your problem with creating a script in R Studio. File New..

It kinda feels this is either "how do I use R studio" (in which case
YouTube feels your friend) or "how do I use some very specific package
which I am going to forget to tell you"

On Thu, 5 Dec 2024, 22:00 Carlos Alberto Baptista de Figueiredo, <
beto77051 at gmail.com> wrote:

> Hi there
>
>
>
>
>
> I want to create a script in Rstudio and load in the reagent dataset
> ensuring that the different data types in there (dates, text, etc) come
> through correctly.
>
>
>
> Best wishes
>
> Carlos
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From po|c1410 @end|ng |rom gm@||@com  Fri Dec  6 02:19:34 2024
From: po|c1410 @end|ng |rom gm@||@com (CALUM POLWART)
Date: Fri, 6 Dec 2024 01:19:34 +0000
Subject: [R] Spreadsheets data
In-Reply-To: <CAGAA5bdEtBqvNWnbYXieXdXBkXNL3cAyF3gtPhzqiKcH-4_cFQ@mail.gmail.com>
References: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
 <CAGAA5bdEtBqvNWnbYXieXdXBkXNL3cAyF3gtPhzqiKcH-4_cFQ@mail.gmail.com>
Message-ID: <CA+etgPmS+vQpE=RdYbBt0rS8j9x4fvg=1bNjfASbcfpFzE7YHw@mail.gmail.com>

And R Studio has an import data function that will provide a import
command..

BUT

Spreadsheet is a broad scope

On Thu, 5 Dec 2024, 23:38 Martin M?ller Skarbiniks Pedersen, <
traxplayer at gmail.com> wrote:

> On Thu, 5 Dec 2024 at 23:16, Figueiredo, Carlos via R-help
> <r-help at r-project.org> wrote:
> >
> > Hi there
> >
> >
> > I want to create a script in Rstudio and load in the reagent dataset
> ensuring that the different data types in there (dates, text, etc) come
> through correctly.
>
> What have you tried? How good do you know R?
> R contains many functions to read input.
>
> Regards
> Martin
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Fri Dec  6 02:37:57 2024
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Fri, 6 Dec 2024 01:37:57 +0000
Subject: [R] ggplot2: Plot multiple lines using stacked data.
Message-ID: <DM6PR03MB5049882B33D32BA92E474A27E2312@DM6PR03MB5049.namprd03.prod.outlook.com>

I am trying to use ggplot2 to create a figure with multiple lines, one line for each value of the variable Day. Each group of data for Day requires seven lines. The dataframe has data for 4 days and thus 4*7=28 lines.

I can create a plot, but the plot only contains dots. The dots for each day should be connected each day's data by a different line. There should be a total of four lines on the graph 

mydata <-structure(list(Day = c("25", "25", "25", "25", "25", "25", "25", 
                       "26", "26", "26", "26", "26", "26", "26", "27", "27", "27", "27", 
                       "27", "27", "27", "28", "28", "28", "28", "28", "28", "28"), 
               AQIGroup = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 
                        3L, 4L, 5L, 6L, 7L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 3L, 
                        4L, 5L, 6L, 7L), levels = c("Good", "Moderate", "UnForSome", 
                        "UH", "VUH", "Haz1", "Hax2"), class = "factor"), Freq = c(98.05, 
                         0.37, 0.27, 0.17, 0.26, 0.5, 0.38, 93.34, 4.34, 0.75, 0.42, 
                         0.44, 0.44, 0.27, 89.57, 7.8, 0.98, 0.38, 0.5, 0.52, 0.25, 
                        80.43, 13.33, 3.85, 0.76, 0.86, 0.28, 0.49)), class = "data.frame", row.names = c(NA, -28L))

# You can see that the data are stacked, one day on top of the next.
# Each day requires seven lines.
mydata

# Load ggplot2
if(!require(ggplot2)) {install.packages(ggplot2)}
library(ggplot2)
# Create a graph, with multiple lines, one line for each value of Day.
ggplot(mydata, aes(AQIGroup,Freq,color=Day)) +
  geom_point()+
  geom_line(aes(AQIGroup,Freq))

Thank you,
John


John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical Center Geriatrics Research, Education, and Clinical Center;?
PI?Biostatistics and Informatics Core, University of Maryland School of Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382




From @vi@e@gross m@iii@g oii gm@ii@com  Fri Dec  6 03:18:36 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 5 Dec 2024 21:18:36 -0500
Subject: [R] ggplot2: Plot multiple lines using stacked data.
In-Reply-To: <DM6PR03MB5049882B33D32BA92E474A27E2312@DM6PR03MB5049.namprd03.prod.outlook.com>
References: <DM6PR03MB5049882B33D32BA92E474A27E2312@DM6PR03MB5049.namprd03.prod.outlook.com>
Message-ID: <001501db4785$287dff10$7979fd30$@gmail.com>

John,

I hate to break it to you that ggplot2 is not part of base R. "plot" and
maybe lattice qualify.

But I can work with you. Try this:

ggplot(data=mydata, 
       aes(x=AQIGroup,
           y=Freq,
           group=Day,
           color=Day)) +
  geom_point() +
  geom_line()

The main change besides naming arguments for clarity, is adding what it
should be grouped by. I believe you want it grouped by day, but am not
totally sure except that the grouping was not done by your last line that
sort of changed the x,y back to what it would be anyway.

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Sorkin, John
Sent: Thursday, December 5, 2024 8:38 PM
To: r-help at r-project.org (r-help at r-project.org) <r-help at r-project.org>
Subject: [R] ggplot2: Plot multiple lines using stacked data.

I am trying to use ggplot2 to create a figure with multiple lines, one line
for each value of the variable Day. Each group of data for Day requires
seven lines. The dataframe has data for 4 days and thus 4*7=28 lines.

I can create a plot, but the plot only contains dots. The dots for each day
should be connected each day's data by a different line. There should be a
total of four lines on the graph 

mydata <-structure(list(Day = c("25", "25", "25", "25", "25", "25", "25", 
                       "26", "26", "26", "26", "26", "26", "26", "27", "27",
"27", "27", 
                       "27", "27", "27", "28", "28", "28", "28", "28", "28",
"28"), 
               AQIGroup = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 2L, 
                        3L, 4L, 5L, 6L, 7L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L,
2L, 3L, 
                        4L, 5L, 6L, 7L), levels = c("Good", "Moderate",
"UnForSome", 
                        "UH", "VUH", "Haz1", "Hax2"), class = "factor"),
Freq = c(98.05, 
                         0.37, 0.27, 0.17, 0.26, 0.5, 0.38, 93.34, 4.34,
0.75, 0.42, 
                         0.44, 0.44, 0.27, 89.57, 7.8, 0.98, 0.38, 0.5,
0.52, 0.25, 
                        80.43, 13.33, 3.85, 0.76, 0.86, 0.28, 0.49)), class
= "data.frame", row.names = c(NA, -28L))

# You can see that the data are stacked, one day on top of the next.
# Each day requires seven lines.
mydata

# Load ggplot2
if(!require(ggplot2)) {install.packages(ggplot2)}
library(ggplot2)
# Create a graph, with multiple lines, one line for each value of Day.
ggplot(mydata, aes(AQIGroup,Freq,color=Day)) +
  geom_point()+
  geom_line(aes(AQIGroup,Freq))

Thank you,
John


John David Sorkin M.D., Ph.D.
Professor of Medicine, University of Maryland School of Medicine;
Associate Director for Biostatistics and Informatics, Baltimore VA Medical
Center Geriatrics Research, Education, and Clinical Center;?
PI?Biostatistics and Informatics Core, University of Maryland School of
Medicine Claude D. Pepper Older Americans Independence Center;
Senior Statistician University of Maryland Center for Vascular Research;

Division of Gerontology and Paliative Care,
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
Cell phone 443-418-5382



______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From @vi@e@gross m@iii@g oii gm@ii@com  Fri Dec  6 03:27:09 2024
From: @vi@e@gross m@iii@g oii gm@ii@com (@vi@e@gross m@iii@g oii gm@ii@com)
Date: Thu, 5 Dec 2024 21:27:09 -0500
Subject: [R] Spreadsheets data
In-Reply-To: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
References: <LO0P123MB742395A956A62EBB4DE1E7F3A8302@LO0P123MB7423.GBRP123.PROD.OUTLOOK.COM>
Message-ID: <001901db4786$59edd470$0dc97d50$@gmail.com>

What exactly is stopping you, Carlos?

Do you know where the reagent dataset is? I don't. Is it being loaded from a
known format like a .CSV file, or part of an EXCEL spreadsheet and so on?

Does it come with a function in a package that you load and then invoke to
load it or make it visible? Or, should you already have copied it somewhere
on your disk? Depending on many scenarios, loading methods are available.

Moving on, if you read it in, often enough some columns are assigned a
different data type than you want. Some methods of reading in may allow you
to supply a list of hints as to what you want each column to be.

In any case, once you have read in the data, perhaps in the form of a
data.frame or tibble or other variants, you can generally execute commands
to replace a column with a transformation of your choice such as
as.integer() or convert it to a factor or even make new columns with other
visions of the original and so on.

Without some info on what you have tried and what failed, this is a very
basic request and chances are you won't get much help. 

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Figueiredo, Carlos
via R-help
Sent: Thursday, December 5, 2024 10:16 AM
To: r-help at r-project.org
Subject: [R] Spreadsheets data

Hi there


I want to create a script in Rstudio and load in the reagent dataset
ensuring that the different data types in there (dates, text, etc) come
through correctly.

Best wishes

Carlos


________________________________
Notice of Confidentiality: The contents of this e-mail and any attachments
are confidential and copyright to HCA International and may also be
privileged. They are intended only for the use of the addressee(s) shown
above. Any unauthorised reading, use, distribution or copying of them is
prohibited. If this e-mail has been sent to you in error, you may not take
action based on it, nor may you rely on it for any purpose. Please let us
know of the error by emailing HCA.DPO at hcahealthcare.co.uk and please return
this e-mail. If you have a query relating to an upcoming appointment or your
clinical treatment, please email Contact at hcahealthcare.co.uk.

Whilst all reasonable care has been taken to avoid the transmission of
viruses, it is the responsibility of the recipient to ensure that onward
transmission, opening or use of this message and any attachments will not
adversely affect its systems or data. No responsibility is accepted by HCA
International in this regard and the recipient should carry out such virus
and other checks as it considers appropriate. HCA International is
registered in the UK, Company Number 3020522, at 2 Cavendish Square, London,
W1G 0PU.

________________________________
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
https://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From rb@er @end|ng |rom @t@u@edu  Fri Dec  6 15:50:45 2024
From: rb@er @end|ng |rom @t@u@edu (Robert Baer)
Date: Fri, 6 Dec 2024 08:50:45 -0600
Subject: [R] Creating a script in Rstudio
In-Reply-To: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>
References: <CAFB91SjNYCgytMLw4MO9ComMAbVHn1tNzf=J9XN=eECrxccK7Q@mail.gmail.com>
Message-ID: <6ba75355-89fa-4d8a-9246-e1bafa1dc5b8@atsu.edu>

This is off-topic for this list, but first use the file | import dataset 
menu.? Then go to the history tab, highlight the import statement that 
was generated and click to source.? You'll have to figure out where the 
reagent dataset is on your own.

On 12/5/2024 9:21 AM, Carlos Alberto Baptista de Figueiredo wrote:
> Hi there
>
>
>
>
>
> I want to create a script in Rstudio and load in the reagent dataset
> ensuring that the different data types in there (dates, text, etc) come
> through correctly.
>
>
>
> Best wishes
>
> Carlos
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-- 
---
Robert W. Baer, Ph.D.
Professor of Physiology
Kirksville College of Osteopathic Medicine
A.T. Still Univerisity of Health Sciences
800 W. Jefferson St.
Kirksville, MO 63501


From dyk|m7411 @end|ng |rom gm@||@com  Fri Dec  6 21:31:15 2024
From: dyk|m7411 @end|ng |rom gm@||@com (D)
Date: Fri, 6 Dec 2024 15:31:15 -0500
Subject: [R] Sum by group
Message-ID: <CADmwX-+SgmyGXG3reyX3nH9fz3dPU=9DqdOJbdMHanqDhrjjTA@mail.gmail.com>

I have population data (?totpopE?) at the census tract level (?GEOID?),
which are nested within Precincts (?Precinct?). Please see below my data
structure.

I used the code to sum population data per precinct:

inters <- inters %>%

  group_by(Precinct) %>%

  mutate(TotalPop = sum(totpopE)

  )

However, said code produced too large sums because each census tract
(?GEOID?) has multiple observations/rows.  Each census tract has the same
value.  Is there any way I can use one value for each census tract to
estimate total populations at the precinct level?  It would be appreciated
if anyone can provide codes to sum population data per precinct.

> tail(df, n=20)Simple feature collection with 20 features and 3 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 989211 ymin: 193205 xmax: 997877 ymax: 222689
Projected CRS: NAD83 / New York Long Island (ftUS)# A tibble: 20 ? 4#
Groups:   GEOID [5]
   Precinct GEOID       totpopE                 geometry
   <fct>    <chr>         <dbl> <POINT [US_survey_foot]> 1 20
36061015700   10352          (989211 222689) 2 20       36061015700
10352          (989211 222689) 3 20       36061015700   10352
(989211 222689) 4 20       36061015700   10352          (989211
222689) 5 79       36047123700    9448          (996168 193934) 6 79
    36047123700    9448          (996598 193205) 7 79
36047123700    9448          (996598 193205) 8 79       36047123700
9448          (996598 193205) 9 19       36061013400   11387
(996703 219599)10 19       36061013400   11387          (996475
220183)11 19       36061013800   12826          (996871 222201)12 19
    36061013800   12826          (997576 221811)13 19
36061013800   12826          (997877 221608)14 19       36061013800
12826          (997877 221608)15 19       36061013800   12826
(996216 221621)16 13       36061004400   15945          (990562
205039)17 13       36061004400   15945          (989574 206434)18 13
    36061004400   15945          (989574 206434)19 13
36061004400   15945          (989574 206434)20 9        36061004400
15945          (989699 205397)

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Dec  7 08:48:46 2024
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 06 Dec 2024 23:48:46 -0800
Subject: [R] Sum by group
In-Reply-To: <CADmwX-+SgmyGXG3reyX3nH9fz3dPU=9DqdOJbdMHanqDhrjjTA@mail.gmail.com>
References: <CADmwX-+SgmyGXG3reyX3nH9fz3dPU=9DqdOJbdMHanqDhrjjTA@mail.gmail.com>
Message-ID: <6F67CF86-D6CC-44D7-8DE8-F81014B9CC17@dcn.davis.ca.us>

That is what the "summarise" function is designed to do (instead of mutate). All of the calculations in a summarise have to aggregate all group rows down to one value, but that is what you want in this case.

Please note that you are supposed to make your examples reproducible by including your library statements when posting on this mailing list. From your code sequence I can guess that you are using the dplyr package, but on this mailing list it is bad form to assume your readers know which user-contributed packages you are using. Perhaps use the "reprex" package to check that other people will have all the info they need to run your example in their computers so they can be clear what you are dealing with without guessing.

On December 6, 2024 12:31:15 PM PST, D <dykim7411 at gmail.com> wrote:
>I have population data (?totpopE?) at the census tract level (?GEOID?),
>which are nested within Precincts (?Precinct?). Please see below my data
>structure.
>
>I used the code to sum population data per precinct:
>
>inters <- inters %>%
>
>  group_by(Precinct) %>%
>
>  mutate(TotalPop = sum(totpopE)
>
>  )
>
>However, said code produced too large sums because each census tract
>(?GEOID?) has multiple observations/rows.  Each census tract has the same
>value.  Is there any way I can use one value for each census tract to
>estimate total populations at the precinct level?  It would be appreciated
>if anyone can provide codes to sum population data per precinct.
>
>> tail(df, n=20)Simple feature collection with 20 features and 3 fields
>Geometry type: POINT
>Dimension:     XY
>Bounding box:  xmin: 989211 ymin: 193205 xmax: 997877 ymax: 222689
>Projected CRS: NAD83 / New York Long Island (ftUS)# A tibble: 20 ? 4#
>Groups:   GEOID [5]
>   Precinct GEOID       totpopE                 geometry
>   <fct>    <chr>         <dbl> <POINT [US_survey_foot]> 1 20
>36061015700   10352          (989211 222689) 2 20       36061015700
>10352          (989211 222689) 3 20       36061015700   10352
>(989211 222689) 4 20       36061015700   10352          (989211
>222689) 5 79       36047123700    9448          (996168 193934) 6 79
>    36047123700    9448          (996598 193205) 7 79
>36047123700    9448          (996598 193205) 8 79       36047123700
>9448          (996598 193205) 9 19       36061013400   11387
>(996703 219599)10 19       36061013400   11387          (996475
>220183)11 19       36061013800   12826          (996871 222201)12 19
>    36061013800   12826          (997576 221811)13 19
>36061013800   12826          (997877 221608)14 19       36061013800
>12826          (997877 221608)15 19       36061013800   12826
>(996216 221621)16 13       36061004400   15945          (990562
>205039)17 13       36061004400   15945          (989574 206434)18 13
>    36061004400   15945          (989574 206434)19 13
>36061004400   15945          (989574 206434)20 9        36061004400
>15945          (989699 205397)
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide https://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From phii m@iii@g oii phiiipsmith@c@  Tue Dec 10 01:56:29 2024
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Mon, 09 Dec 2024 19:56:29 -0500
Subject: [R] Heat maps containing two types of variables
Message-ID: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>

I am working with a heat map, as in the REPREX below. The code works 
fine as long as "bigger numbers imply greener and smaller numbers imply 
redder". These are time series where bigger numbers are "better", like 
total employment for example. But I also have cases within the heat map 
where "bigger numbers imply redder and smaller numbers imply greener". 
These are time series where bigger numbers are "worse", like total 
unemployment for example. So suppose column B in dat is of the second 
type, i.e. "bigger numbers imply redder and smaller numbers imply 
greener". I would like the colour coding to be the reverse of what it is 
for columns A and C. How can I modify the code to accomplish this? I 
have tried different approaches with no success. Thanks for your help. 
Philip

# REPREX
library(ggplot2)
library(tidyr)
library(dplyr)
dat <- data.frame(
   date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
   A=c(1,3,3,4,2,6),
   B=c(3,5,6,4,8,9),
   C=c(10,8,17,19,26,22)
)
dat_long <- pivot_longer(dat,2:4,names_to="variable",values_to="value")
normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) }
dat_norm <- mutate(dat,across(2:4,normalize))
dat_long_norm <- 
pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
heatmap <- ggplot(dat_long, aes(x = date, y = variable,fill=norm_value)) 
+
   geom_tile() +
   geom_text(aes(label = as.character(value)),
     color = "black", size = 2.5) +
   labs(title="REPREX",x="",y="")+
   scale_fill_gradient2(low = "#E94A26", mid = "white", high = "#A1D385", 
midpoint = 0.5) +
   scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
     as.Date("2024-06-01"),by="month"),
     labels=function(x) format(x,"%b\n%Y"),position="top")+
   theme(legend.position="none")
heatmap
ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)


From tebert @end|ng |rom u||@edu  Tue Dec 10 05:33:53 2024
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 10 Dec 2024 04:33:53 +0000
Subject: [R] Heat maps containing two types of variables
In-Reply-To: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
References: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
Message-ID: <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>

What happens if you switch the colors in this line:
   scale_fill_gradient2(low = "#E94A26", mid = "white", high = "#A1D385", midpoint = 0.5) +
to be the following
   scale_fill_gradient2(low = "# A1D385", mid = "white", high = "# E94A26", midpoint = 0.5) +

That said, a red-green heat map may be unhelpful to color blind people.

So then you need two ggplot statements, one with each scale_fill_gradient2 and then specify which version to plot for each variable.

Tim

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of phil at philipsmith.ca
Sent: Monday, December 9, 2024 7:56 PM
To: R-help at r-project.org
Subject: [R] Heat maps containing two types of variables

[External Email]

I am working with a heat map, as in the REPREX below. The code works fine as long as "bigger numbers imply greener and smaller numbers imply redder". These are time series where bigger numbers are "better", like total employment for example. But I also have cases within the heat map where "bigger numbers imply redder and smaller numbers imply greener".
These are time series where bigger numbers are "worse", like total unemployment for example. So suppose column B in dat is of the second type, i.e. "bigger numbers imply redder and smaller numbers imply greener". I would like the colour coding to be the reverse of what it is for columns A and C. How can I modify the code to accomplish this? I have tried different approaches with no success. Thanks for your help.
Philip

# REPREX
library(ggplot2)
library(tidyr)
library(dplyr)
dat <- data.frame(
   date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
   A=c(1,3,3,4,2,6),
   B=c(3,5,6,4,8,9),
   C=c(10,8,17,19,26,22)
)
dat_long <- pivot_longer(dat,2:4,names_to="variable",values_to="value")
normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) } dat_norm <- mutate(dat,across(2:4,normalize)) dat_long_norm <-
pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
heatmap <- ggplot(dat_long, aes(x = date, y = variable,fill=norm_value))
+
   geom_tile() +
   geom_text(aes(label = as.character(value)),
     color = "black", size = 2.5) +
   labs(title="REPREX",x="",y="")+
   scale_fill_gradient2(low = "#E94A26", mid = "white", high = "#A1D385", midpoint = 0.5) +
   scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
     as.Date("2024-06-01"),by="month"),
     labels=function(x) format(x,"%b\n%Y"),position="top")+
   theme(legend.position="none")
heatmap
ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide https://www.r-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From phii m@iii@g oii phiiipsmith@c@  Tue Dec 10 13:59:25 2024
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Tue, 10 Dec 2024 07:59:25 -0500
Subject: [R] Heat maps containing two types of variables
In-Reply-To: <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
 <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <5a3ca0d2997a114390cf70cc8336e9b6@philipsmith.ca>

Thank you for the suggestion. I tried it, but could not get it to work. 
When I added a second ggplot statement, I hit an error saying that one 
cannot add a ggplot to a ggplot object. So I added a second geom_tile 
statement instead. That worked, except that it warned that since a scale 
for fill was already present, the new fill would replace the old one. In 
other words, the colour scale was changed not just for the target, B, 
but also for the other two variables. So I am still searching for a 
solution.

Philip

On 2024-12-09 23:33, Ebert,Timothy Aaron wrote:
> What happens if you switch the colors in this line:
>    scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) +
> to be the following
>    scale_fill_gradient2(low = "# A1D385", mid = "white", high = "# 
> E94A26", midpoint = 0.5) +
> 
> That said, a red-green heat map may be unhelpful to color blind people.
> 
> So then you need two ggplot statements, one with each 
> scale_fill_gradient2 and then specify which version to plot for each 
> variable.
> 
> Tim
> 
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of 
> phil at philipsmith.ca
> Sent: Monday, December 9, 2024 7:56 PM
> To: R-help at r-project.org
> Subject: [R] Heat maps containing two types of variables
> 
> [External Email]
> 
> I am working with a heat map, as in the REPREX below. The code works 
> fine as long as "bigger numbers imply greener and smaller numbers imply 
> redder". These are time series where bigger numbers are "better", like 
> total employment for example. But I also have cases within the heat map 
> where "bigger numbers imply redder and smaller numbers imply greener".
> These are time series where bigger numbers are "worse", like total 
> unemployment for example. So suppose column B in dat is of the second 
> type, i.e. "bigger numbers imply redder and smaller numbers imply 
> greener". I would like the colour coding to be the reverse of what it 
> is for columns A and C. How can I modify the code to accomplish this? I 
> have tried different approaches with no success. Thanks for your help.
> Philip
> 
> # REPREX
> library(ggplot2)
> library(tidyr)
> library(dplyr)
> dat <- data.frame(
>    
> date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
>    A=c(1,3,3,4,2,6),
>    B=c(3,5,6,4,8,9),
>    C=c(10,8,17,19,26,22)
> )
> dat_long <- pivot_longer(dat,2:4,names_to="variable",values_to="value")
> normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) } dat_norm 
> <- mutate(dat,across(2:4,normalize)) dat_long_norm <-
> pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
> dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
> heatmap <- ggplot(dat_long, aes(x = date, y = 
> variable,fill=norm_value))
> +
>    geom_tile() +
>    geom_text(aes(label = as.character(value)),
>      color = "black", size = 2.5) +
>    labs(title="REPREX",x="",y="")+
>    scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) +
>    scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
>      as.Date("2024-06-01"),by="month"),
>      labels=function(x) format(x,"%b\n%Y"),position="top")+
>    theme(legend.position="none")
> heatmap
> ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> https://www.r-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tebert @end|ng |rom u||@edu  Tue Dec 10 14:35:26 2024
From: tebert @end|ng |rom u||@edu (Ebert,Timothy Aaron)
Date: Tue, 10 Dec 2024 13:35:26 +0000
Subject: [R] Heat maps containing two types of variables
In-Reply-To: <5a3ca0d2997a114390cf70cc8336e9b6@philipsmith.ca>
References: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
 <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <5a3ca0d2997a114390cf70cc8336e9b6@philipsmith.ca>
Message-ID: <CH3PR22MB4514AA72E844CC1572251AE2CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>

You will need to add code to tell R which variables to plot in reverse color order. In this code, I chose variable B to plot in reverse order. 


library(ggplot2)
library(tidyr)
library(dplyr)

dat <- data.frame(
  date = seq.Date(as.Date("2024-01-01"), as.Date("2024-06-01"), by = "month"),
  A = c(1, 3, 3, 4, 2, 6),
  B = c(3, 5, 6, 4, 8, 9),
  C = c(10, 8, 17, 19, 26, 22)
)
normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
dat_long <- dat %>%
  mutate(across(2:4, normalize)) %>% 
  pivot_longer(cols = A:C, names_to = "variable", values_to = "norm_value") %>%
  left_join(
    dat %>%
      pivot_longer(cols = A:C, names_to = "variable", values_to = "value"),
    by = c("date", "variable")
  )

dat_long <- dat_long %>%
  mutate(adjusted_norm_value = ifelse(variable == "B", 1 - norm_value, norm_value))

heatmap <- ggplot(dat_long, aes(x = date, y = variable, fill = adjusted_norm_value)) +
  geom_tile() +
  geom_text(aes(label = as.character(value)), color = "black", size = 2.5) +
  labs(title = "REPREX", x = "", y = "") +
  scale_fill_gradient2(low = "#E94A26", mid = "white", high = "#A1D385", midpoint = 0.5) +
  scale_x_continuous(
    breaks = seq.Date(as.Date("2024-01-01"), as.Date("2024-06-01"), by = "month"),
    labels = function(x) format(x, "%b\n%Y"),
    position = "top"
  ) +
  theme(legend.position = "none")

heatmap

-----Original Message-----
From: phil at philipsmith.ca <phil at philipsmith.ca> 
Sent: Tuesday, December 10, 2024 7:59 AM
To: Ebert,Timothy Aaron <tebert at ufl.edu>
Cc: R-help at r-project.org
Subject: Re: [R] Heat maps containing two types of variables

[External Email]

Thank you for the suggestion. I tried it, but could not get it to work.
When I added a second ggplot statement, I hit an error saying that one cannot add a ggplot to a ggplot object. So I added a second geom_tile statement instead. That worked, except that it warned that since a scale for fill was already present, the new fill would replace the old one. In other words, the colour scale was changed not just for the target, B, but also for the other two variables. So I am still searching for a solution.

Philip

On 2024-12-09 23:33, Ebert,Timothy Aaron wrote:
> What happens if you switch the colors in this line:
>    scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) + to be the following
>    scale_fill_gradient2(low = "# A1D385", mid = "white", high = "# 
> E94A26", midpoint = 0.5) +
>
> That said, a red-green heat map may be unhelpful to color blind people.
>
> So then you need two ggplot statements, one with each
> scale_fill_gradient2 and then specify which version to plot for each 
> variable.
>
> Tim
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of 
> phil at philipsmith.ca
> Sent: Monday, December 9, 2024 7:56 PM
> To: R-help at r-project.org
> Subject: [R] Heat maps containing two types of variables
>
> [External Email]
>
> I am working with a heat map, as in the REPREX below. The code works 
> fine as long as "bigger numbers imply greener and smaller numbers 
> imply redder". These are time series where bigger numbers are 
> "better", like total employment for example. But I also have cases 
> within the heat map where "bigger numbers imply redder and smaller numbers imply greener".
> These are time series where bigger numbers are "worse", like total 
> unemployment for example. So suppose column B in dat is of the second 
> type, i.e. "bigger numbers imply redder and smaller numbers imply 
> greener". I would like the colour coding to be the reverse of what it 
> is for columns A and C. How can I modify the code to accomplish this? 
> I have tried different approaches with no success. Thanks for your help.
> Philip
>
> # REPREX
> library(ggplot2)
> library(tidyr)
> library(dplyr)
> dat <- data.frame(
>
> date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
>    A=c(1,3,3,4,2,6),
>    B=c(3,5,6,4,8,9),
>    C=c(10,8,17,19,26,22)
> )
> dat_long <- 
> pivot_longer(dat,2:4,names_to="variable",values_to="value")
> normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) } dat_norm
> <- mutate(dat,across(2:4,normalize)) dat_long_norm <-
> pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
> dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
> heatmap <- ggplot(dat_long, aes(x = date, y =
> variable,fill=norm_value))
> +
>    geom_tile() +
>    geom_text(aes(label = as.character(value)),
>      color = "black", size = 2.5) +
>    labs(title="REPREX",x="",y="")+
>    scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) +
>    scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
>      as.Date("2024-06-01"),by="month"),
>      labels=function(x) format(x,"%b\n%Y"),position="top")+
>    theme(legend.position="none")
> heatmap
> ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
> %7C1a08885d2b61452a502308dd191a7c05%7C0d4da0f84a314d76ace60a62331e1b84
> %7C0%7C0%7C638694323735326272%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
> %3D%3D%7C0%7C%7C%7C&sdata=AyPt4o4LzEivT0D6JfeB%2Bav7N2wnZM%2BxPkrLF9B3
> LvI%3D&reserved=0
> PLEASE do read the posting guide
> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
> r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
> a08885d2b61452a502308dd191a7c05%7C0d4da0f84a314d76ace60a62331e1b84%7C0
> %7C0%7C638694323735345552%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
> 3D%7C0%7C%7C%7C&sdata=bH1aZLifelBywJO42%2BWGcyU1O6tmJbQi3DPC%2FjxWIKo%
> 3D&reserved=0 and provide commented, minimal, self-contained, 
> reproducible code.


From phii m@iii@g oii phiiipsmith@c@  Tue Dec 10 16:37:37 2024
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Tue, 10 Dec 2024 10:37:37 -0500
Subject: [R] Heat maps containing two types of variables
In-Reply-To: <CH3PR22MB4514AA72E844CC1572251AE2CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
References: <306707f054a523ecc38c58c238225d7b@philipsmith.ca>
 <CH3PR22MB4514A30D6D8E43040C194A22CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
 <5a3ca0d2997a114390cf70cc8336e9b6@philipsmith.ca>
 <CH3PR22MB4514AA72E844CC1572251AE2CF3D2@CH3PR22MB4514.namprd22.prod.outlook.com>
Message-ID: <e1c965c85d47ff39023d910be732ddaa@philipsmith.ca>

Thanks for sticking with me Tim. Your solution is clear and works like a 
charm.

Philip

On 2024-12-10 08:35, Ebert,Timothy Aaron wrote:
> You will need to add code to tell R which variables to plot in reverse 
> color order. In this code, I chose variable B to plot in reverse order.
> 
> 
> library(ggplot2)
> library(tidyr)
> library(dplyr)
> 
> dat <- data.frame(
>   date = seq.Date(as.Date("2024-01-01"), as.Date("2024-06-01"), by = 
> "month"),
>   A = c(1, 3, 3, 4, 2, 6),
>   B = c(3, 5, 6, 4, 8, 9),
>   C = c(10, 8, 17, 19, 26, 22)
> )
> normalize <- function(x) {
>   (x - min(x)) / (max(x) - min(x))
> }
> dat_long <- dat %>%
>   mutate(across(2:4, normalize)) %>%
>   pivot_longer(cols = A:C, names_to = "variable", values_to = 
> "norm_value") %>%
>   left_join(
>     dat %>%
>       pivot_longer(cols = A:C, names_to = "variable", values_to = 
> "value"),
>     by = c("date", "variable")
>   )
> 
> dat_long <- dat_long %>%
>   mutate(adjusted_norm_value = ifelse(variable == "B", 1 - norm_value, 
> norm_value))
> 
> heatmap <- ggplot(dat_long, aes(x = date, y = variable, fill = 
> adjusted_norm_value)) +
>   geom_tile() +
>   geom_text(aes(label = as.character(value)), color = "black", size = 
> 2.5) +
>   labs(title = "REPREX", x = "", y = "") +
>   scale_fill_gradient2(low = "#E94A26", mid = "white", high = 
> "#A1D385", midpoint = 0.5) +
>   scale_x_continuous(
>     breaks = seq.Date(as.Date("2024-01-01"), as.Date("2024-06-01"), by 
> = "month"),
>     labels = function(x) format(x, "%b\n%Y"),
>     position = "top"
>   ) +
>   theme(legend.position = "none")
> 
> heatmap
> 
> -----Original Message-----
> From: phil at philipsmith.ca <phil at philipsmith.ca>
> Sent: Tuesday, December 10, 2024 7:59 AM
> To: Ebert,Timothy Aaron <tebert at ufl.edu>
> Cc: R-help at r-project.org
> Subject: Re: [R] Heat maps containing two types of variables
> 
> [External Email]
> 
> Thank you for the suggestion. I tried it, but could not get it to work.
> When I added a second ggplot statement, I hit an error saying that one 
> cannot add a ggplot to a ggplot object. So I added a second geom_tile 
> statement instead. That worked, except that it warned that since a 
> scale for fill was already present, the new fill would replace the old 
> one. In other words, the colour scale was changed not just for the 
> target, B, but also for the other two variables. So I am still 
> searching for a solution.
> 
> Philip
> 
> On 2024-12-09 23:33, Ebert,Timothy Aaron wrote:
>> What happens if you switch the colors in this line:
>>    scale_fill_gradient2(low = "#E94A26", mid = "white", high =
>> "#A1D385", midpoint = 0.5) + to be the following
>>    scale_fill_gradient2(low = "# A1D385", mid = "white", high = "#
>> E94A26", midpoint = 0.5) +
>> 
>> That said, a red-green heat map may be unhelpful to color blind 
>> people.
>> 
>> So then you need two ggplot statements, one with each
>> scale_fill_gradient2 and then specify which version to plot for each
>> variable.
>> 
>> Tim
>> 
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of
>> phil at philipsmith.ca
>> Sent: Monday, December 9, 2024 7:56 PM
>> To: R-help at r-project.org
>> Subject: [R] Heat maps containing two types of variables
>> 
>> [External Email]
>> 
>> I am working with a heat map, as in the REPREX below. The code works
>> fine as long as "bigger numbers imply greener and smaller numbers
>> imply redder". These are time series where bigger numbers are
>> "better", like total employment for example. But I also have cases
>> within the heat map where "bigger numbers imply redder and smaller 
>> numbers imply greener".
>> These are time series where bigger numbers are "worse", like total
>> unemployment for example. So suppose column B in dat is of the second
>> type, i.e. "bigger numbers imply redder and smaller numbers imply
>> greener". I would like the colour coding to be the reverse of what it
>> is for columns A and C. How can I modify the code to accomplish this?
>> I have tried different approaches with no success. Thanks for your 
>> help.
>> Philip
>> 
>> # REPREX
>> library(ggplot2)
>> library(tidyr)
>> library(dplyr)
>> dat <- data.frame(
>> 
>> date=seq.Date(as.Date("2024-01-01"),as.Date("2024-06-01"),by="month"),
>>    A=c(1,3,3,4,2,6),
>>    B=c(3,5,6,4,8,9),
>>    C=c(10,8,17,19,26,22)
>> )
>> dat_long <-
>> pivot_longer(dat,2:4,names_to="variable",values_to="value")
>> normalize <- function(x) { y <- (x-min(x))/(max(x)-min(x)) } dat_norm
>> <- mutate(dat,across(2:4,normalize)) dat_long_norm <-
>> pivot_longer(dat_norm,2:4,names_to="variable",values_to="norm_value")
>> dat_long <- inner_join(dat_long,dat_long_norm,by=c("date","variable"))
>> heatmap <- ggplot(dat_long, aes(x = date, y =
>> variable,fill=norm_value))
>> +
>>    geom_tile() +
>>    geom_text(aes(label = as.character(value)),
>>      color = "black", size = 2.5) +
>>    labs(title="REPREX",x="",y="")+
>>    scale_fill_gradient2(low = "#E94A26", mid = "white", high =
>> "#A1D385", midpoint = 0.5) +
>>    scale_x_continuous(breaks=seq.Date(as.Date("2024-01-01"),
>>      as.Date("2024-06-01"),by="month"),
>>      labels=function(x) format(x,"%b\n%Y"),position="top")+
>>    theme(legend.position="none")
>> heatmap
>> ggsave("REPREXHeatmap.png",heatmap,height=3.5,width=4.9,dpi=200)
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat
>> .ethz.ch%2Fmailman%2Flistinfo%2Fr-help&data=05%7C02%7Ctebert%40ufl.edu
>> %7C1a08885d2b61452a502308dd191a7c05%7C0d4da0f84a314d76ace60a62331e1b84
>> %7C0%7C0%7C638694323735326272%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGki
>> OnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ
>> %3D%3D%7C0%7C%7C%7C&sdata=AyPt4o4LzEivT0D6JfeB%2Bav7N2wnZM%2BxPkrLF9B3
>> LvI%3D&reserved=0
>> PLEASE do read the posting guide
>> https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.
>> r-project.org%2Fposting-guide.html&data=05%7C02%7Ctebert%40ufl.edu%7C1
>> a08885d2b61452a502308dd191a7c05%7C0d4da0f84a314d76ace60a62331e1b84%7C0
>> %7C0%7C638694323735345552%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRy
>> dWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%
>> 3D%7C0%7C%7C%7C&sdata=bH1aZLifelBywJO42%2BWGcyU1O6tmJbQi3DPC%2FjxWIKo%
>> 3D&reserved=0 and provide commented, minimal, self-contained,
>> reproducible code.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Tue Dec 10 17:36:54 2024
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Tue, 10 Dec 2024 22:06:54 +0530
Subject: [R] outer() is not working with my simple function
Message-ID: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>

Hi,

I have below code

FN1 = function(x, y) 3

outer(1:9, 1:9, FN1)

With above I get error as below

Error in dim(robj) <- c(dX, dY) :

  dims [product 81] do not match the length of object [1]

Could you please help to understand why is it failing? I am just
expecting to get a matrix with all elements as 3


From |kw@|mmo @end|ng |rom gm@||@com  Tue Dec 10 17:43:05 2024
From: |kw@|mmo @end|ng |rom gm@||@com (Iris Simmons)
Date: Tue, 10 Dec 2024 11:43:05 -0500
Subject: [R] outer() is not working with my simple function
In-Reply-To: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>
References: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>
Message-ID: <CADNULg_p_y_91OrvKEpevQgE3+=N7dfd682Z+cq=bS=8c-mnpg@mail.gmail.com>

outer() expects a return value with the same length as that of x.

You could change the code to rep(3, length(x))

Or if you don't want to change the code of the function, you could make a
wrapper and use that instead:
function (x, y)
{
    v <- FN1(x, y)
    if (length(v) != 1) v else rep(v, length(x))
}


On Tue, Dec 10, 2024, 11:37 Christofer Bogaso <bogaso.christofer at gmail.com>
wrote:

> Hi,
>
> I have below code
>
> FN1 = function(x, y) 3
>
> outer(1:9, 1:9, FN1)
>
> With above I get error as below
>
> Error in dim(robj) <- c(dX, dY) :
>
>   dims [product 81] do not match the length of object [1]
>
> Could you please help to understand why is it failing? I am just
> expecting to get a matrix with all elements as 3
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> https://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Dec 10 17:44:12 2024
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 10 Dec 2024 11:44:12 -0500
Subject: [R] outer() is not working with my simple function
In-Reply-To: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>
References: <CA+dpOJmGPUf6p5WpGj+zbo3qyznzgLswnq0S-Bisud1x1fPkeA@mail.gmail.com>
Message-ID: <489a80da-58bf-4bae-a83e-5c8274be78c0@gmail.com>

On 2024-12-10 11:36 a.m., Christofer Bogaso wrote:
> Hi,
> 
> I have below code
> 
> FN1 = function(x, y) 3
> 
> outer(1:9, 1:9, FN1)
> 
> With above I get error as below
> 
> Error in dim(robj) <- c(dX, dY) :
> 
>    dims [product 81] do not match the length of object [1]
> 
> Could you please help to understand why is it failing? I am just
> expecting to get a matrix with all elements as 3

The help page says this about the function:  "It must be a vectorized 
function (or the name of one) expecting at least two arguments and 
returning a value with the same length as the first (and the second)." 
So your function could be

   FN1 <- function(x, y) rep_len(3, length(x))

and it would work.

Duncan Murdoch


From tder@mu@ @end|ng |rom mgb@org  Wed Dec 11 01:20:28 2024
From: tder@mu@ @end|ng |rom mgb@org (Deramus, Thomas Patrick)
Date: Wed, 11 Dec 2024 00:20:28 +0000
Subject: [R] Cores hang when calling mcapply
Message-ID: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>

Hi R users.

Apologies for the lack of concrete examples because the dataset is large, and it being so I believe is the issue.

I multiple, very large datasets for which I need to generate 0/1 absence/presence columns
Some include over 200M rows, with two columns that need presence/absence columns based on the strings contained within them, as an example, one set has ~29k unique values and the other with ~15k unique values (no overlap across the two).

Using a combination of custom functions:

crewjanitormakeclean <- function(df,columns) {
  df <- df |> mutate(across(columns, ~make_clean_names(.,  allow_dupes = TRUE)))
  return(df)
}

mass_pivot_wider <- function(df,column,prefix) {
  df <- df |> distinct() |> mutate(n = 1) |> pivot_wider(names_from = glue("{column}"), values_from = n, names_prefix = prefix, values_fill = list(n = 0))
  return(df)
}

sum_group_function <- function(df) {
  df <- df |> group_by(ID_Key) |> summarise(across(c(starts_with("column1_name_"),starts_with("column2_name_"),), ~ sum(.x, na.rm = TRUE))) |> ungroup()
  return(df)
}

and splitting up the data into a list of 110k individual dataframes based on Key_ID

temp <-
  open_dataset(
    sources = input_files,
    format = 'csv',
    unify_schema = TRUE,
    col_types = schema(
      "ID_Key" = string(),
      "column1" = string(),
      "column1" = string()
    )
  ) |> as_tibble()


  keeptabs <- split(temp, temp$ID_Key)


I used a multicore framework to distribute the `sum` functions across each Key_ID when a multicore argument is enabled.

??????
    if(isTRUE(multicore)){
      output <- mclapply(1:length(modtabs), function(i) crewjanitormakeclean(modtabs[[i]],c("string_columns_2","string_columns_1")), mc.cores = numcores)
      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_1","col_1_name_"), mc.cores = numcores)
      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_2","col_2_name_"), mc.cores = numcores)
    }else{
      output <- lapply(1:length(modtabs), function(i) crewjanitormakeclean(modtabs[[i]],c("string_columns_2","string_columns_1")))
      output <- lapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_1","col_1_name_"))
      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_2","col_2_name_"))
    }


Moving every Key_ID to a single row and then row-binding the data while creating new columns for the differences across `Key_ID`s from the pivot using the following solution (78 upvotes at time of this email):
https://stackoverflow.com/questions/3402371/combine-two-data-frames-by-rows-rbind-when-they-have-different-sets-of-columns

  allNms <- unique(unlist(lapply(keeptabs, names)))

  output <- do.call(rbind,
                     c(lapply(keeptabs,
                              function(x) data.frame(c((x), sapply(setdiff(allNms, names(x)),
                                                                 function(y) NA))) |> as_tibble()),
                       make.row.names=FALSE)) |> mutate(across(c(starts_with("column1_name_"), starts_with("column2_name_")), coalesce, 0))

However, I have noticed that the jobs seem to "hang" after a while, with the initial 30 or so (numcores == 30 in the workflow, equal to the number of cores I reserved) at 100% of the requested CPU, and then several "zombie" processes occur and the cores just stop at 0% and never proceed, usually dying with a timeout or not all jobs running to completion failure to join of some kind.

This happens in both base R and RStudio, and I haven't been able to figure out if it's something wrong with the code, the size of the data, or our architecture, but I would appreciate any suggestions as to what I might be able to do about this.

Before they are suggested, I have also tried this same approach with foreach, snow, future, and furr packages, and base parallel with mc.apply seems to be the only thing that works for at least one dataset.

In the event it has something to do with our architecture, here is what we are running on and our loaded packages:

OS Information:
NAME="Red Hat Enterprise Linux"
VERSION="9.3 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
Operating System: Red Hat Enterprise Linux 9.3 (Plow)
     CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
          Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
    Architecture: x86-64
 Hardware Vendor: Dell Inc.
  Hardware Model: PowerEdge R840
Firmware Version: 2.15.1

R Version:
R.Version()
$platform
[1] "x86_64-pc-linux-gnu"
$arch
[1] "x86_64"
$os
[1] "linux-gnu"
$system
[1] "x86_64, linux-gnu"
$status
[1] ""
$major
[1] "4"
$minor
[1] "3.2"
$year
[1] "2023"
$month
[1] "10"
$day
[1] "31"
$`svn rev`
[1] "85441"
$language
[1] "R"
$version.string
[1] "R version 4.3.2 (2023-10-31)"
$nickname
[1] "Eye Holes"

RStudio Server Version:
RStudio 2023.09.1+494 "Desert Sunflower" Release (cd7011dce393115d3a7c3db799dda4b1c7e88711, 2023-10-16) for RHEL 9
Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36

PPM Repo:
https://packagemanager.posit.co/cran/__linux__/rhel9/latest

attached base packages:
[1] parallel  stats     graphics  grDevices datasets  utils     methods   base

other attached packages:
 [1] listenv_0.9.1        microbenchmark_1.5.0 dbplyr_2.4.0         duckplyr_0.4.1       readxl_1.4.3         fastDummies_1.7.3
 [7] glue_1.8.0           arrow_14.0.2.1       data.table_1.15.2    toolbox_0.1.1        janitor_2.2.0        lubridate_1.9.3
[13] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4          purrr_1.0.2          readr_2.1.5          tidyr_1.3.1
[19] tibble_3.2.1         ggplot2_3.5.0        tidyverse_2.0.0      duckdb_1.1.2         DBI_1.2.3            fs_1.6.3


Happy to provide additional information if it would be helpful.

Thank you in advance!
The information in this e-mail is intended only for the person to whom it is addressed.  If you believe this e-mail was sent to you in error and the e-mail contains patient information, please contact the Mass General Brigham Compliance HelpLine at https://www.massgeneralbrigham.org/complianceline <https://www.massgeneralbrigham.org/complianceline> .
Please note that this e-mail is not secure (encrypted).  If you do not wish to continue communication over unencrypted e-mail, please notify the sender of this message immediately.  Continuing to send or respond to e-mail after receiving this message means you understand and accept this risk and wish to continue to communicate over unencrypted e-mail. 

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Dec 11 01:41:25 2024
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 10 Dec 2024 16:41:25 -0800
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
Message-ID: <D15380AC-2594-40CF-9175-DE0D09FECB32@dcn.davis.ca.us>

Maybe ask on the HPC list? [1]

A general tip... you may be running out of memory. If at all possible you need to make sure you extract the data subsets in the parent process, and limit the amount of environment data passed into the child processes. That is, instead of using an integer counter to index into the modtabs list, iterate over the list itself so that each process doesnt need a copy of the whole data set.

I know that fork lets you share memory blocks temporarily between the parent and child processes, but it is fragile... if you modify the data at all then the sharing stops for that memory block and you get an explosion of memory use.

[1] <https://stat.ethz.ch/mailman/listinfo/r-sig-hpc>

On December 10, 2024 4:20:28 PM PST, "Deramus, Thomas Patrick" <tderamus at mgb.org> wrote:
>Hi R users.
>
>Apologies for the lack of concrete examples because the dataset is large, and it being so I believe is the issue.
>
>I multiple, very large datasets for which I need to generate 0/1 absence/presence columns
>Some include over 200M rows, with two columns that need presence/absence columns based on the strings contained within them, as an example, one set has ~29k unique values and the other with ~15k unique values (no overlap across the two).
>
>Using a combination of custom functions:
>
>crewjanitormakeclean <- function(df,columns) {
>  df <- df |> mutate(across(columns, ~make_clean_names(.,  allow_dupes = TRUE)))
>  return(df)
>}
>
>mass_pivot_wider <- function(df,column,prefix) {
>  df <- df |> distinct() |> mutate(n = 1) |> pivot_wider(names_from = glue("{column}"), values_from = n, names_prefix = prefix, values_fill = list(n = 0))
>  return(df)
>}
>
>sum_group_function <- function(df) {
>  df <- df |> group_by(ID_Key) |> summarise(across(c(starts_with("column1_name_"),starts_with("column2_name_"),), ~ sum(.x, na.rm = TRUE))) |> ungroup()
>  return(df)
>}
>
>and splitting up the data into a list of 110k individual dataframes based on Key_ID
>
>temp <-
>  open_dataset(
>    sources = input_files,
>    format = 'csv',
>    unify_schema = TRUE,
>    col_types = schema(
>      "ID_Key" = string(),
>      "column1" = string(),
>      "column1" = string()
>    )
>  ) |> as_tibble()
>
>
>  keeptabs <- split(temp, temp$ID_Key)
>
>
>I used a multicore framework to distribute the `sum` functions across each Key_ID when a multicore argument is enabled.
>
>??????
>    if(isTRUE(multicore)){
>      output <- mclapply(1:length(modtabs), function(i) crewjanitormakeclean(modtabs[[i]],c("string_columns_2","string_columns_1")), mc.cores = numcores)
>      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_1","col_1_name_"), mc.cores = numcores)
>      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_2","col_2_name_"), mc.cores = numcores)
>    }else{
>      output <- lapply(1:length(modtabs), function(i) crewjanitormakeclean(modtabs[[i]],c("string_columns_2","string_columns_1")))
>      output <- lapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_1","col_1_name_"))
>      output <- mclapply(1:length(modtabs), function(i) mass_pivot_wider(modtabs[[i]],"string_columns_2","col_2_name_"))
>    }
>
>
>Moving every Key_ID to a single row and then row-binding the data while creating new columns for the differences across `Key_ID`s from the pivot using the following solution (78 upvotes at time of this email):
>https://stackoverflow.com/questions/3402371/combine-two-data-frames-by-rows-rbind-when-they-have-different-sets-of-columns
>
>  allNms <- unique(unlist(lapply(keeptabs, names)))
>
>  output <- do.call(rbind,
>                     c(lapply(keeptabs,
>                              function(x) data.frame(c((x), sapply(setdiff(allNms, names(x)),
>                                                                 function(y) NA))) |> as_tibble()),
>                       make.row.names=FALSE)) |> mutate(across(c(starts_with("column1_name_"), starts_with("column2_name_")), coalesce, 0))
>
>However, I have noticed that the jobs seem to "hang" after a while, with the initial 30 or so (numcores == 30 in the workflow, equal to the number of cores I reserved) at 100% of the requested CPU, and then several "zombie" processes occur and the cores just stop at 0% and never proceed, usually dying with a timeout or not all jobs running to completion failure to join of some kind.
>
>This happens in both base R and RStudio, and I haven't been able to figure out if it's something wrong with the code, the size of the data, or our architecture, but I would appreciate any suggestions as to what I might be able to do about this.
>
>Before they are suggested, I have also tried this same approach with foreach, snow, future, and furr packages, and base parallel with mc.apply seems to be the only thing that works for at least one dataset.
>
>In the event it has something to do with our architecture, here is what we are running on and our loaded packages:
>
>OS Information:
>NAME="Red Hat Enterprise Linux"
>VERSION="9.3 (Plow)"
>ID="rhel"
>ID_LIKE="fedora"
>VERSION_ID="9.3"
>PLATFORM_ID="platform:el9"
>PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
>ANSI_COLOR="0;31"
>LOGO="fedora-logo-icon"
>CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
>HOME_URL="https://www.redhat.com/"
>DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
>BUG_REPORT_URL="https://bugzilla.redhat.com/"
>REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
>REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
>REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
>REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
>Operating System: Red Hat Enterprise Linux 9.3 (Plow)
>     CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
>          Kernel: Linux 5.14.0-362.13.1.el9_3.x86_64
>    Architecture: x86-64
> Hardware Vendor: Dell Inc.
>  Hardware Model: PowerEdge R840
>Firmware Version: 2.15.1
>
>R Version:
>R.Version()
>$platform
>[1] "x86_64-pc-linux-gnu"
>$arch
>[1] "x86_64"
>$os
>[1] "linux-gnu"
>$system
>[1] "x86_64, linux-gnu"
>$status
>[1] ""
>$major
>[1] "4"
>$minor
>[1] "3.2"
>$year
>[1] "2023"
>$month
>[1] "10"
>$day
>[1] "31"
>$`svn rev`
>[1] "85441"
>$language
>[1] "R"
>$version.string
>[1] "R version 4.3.2 (2023-10-31)"
>$nickname
>[1] "Eye Holes"
>
>RStudio Server Version:
>RStudio 2023.09.1+494 "Desert Sunflower" Release (cd7011dce393115d3a7c3db799dda4b1c7e88711, 2023-10-16) for RHEL 9
>Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36
>
>PPM Repo:
>https://packagemanager.posit.co/cran/__linux__/rhel9/latest
>
>attached base packages:
>[1] parallel  stats     graphics  grDevices datasets  utils     methods   base
>
>other attached packages:
> [1] listenv_0.9.1        microbenchmark_1.5.0 dbplyr_2.4.0         duckplyr_0.4.1       readxl_1.4.3         fastDummies_1.7.3
> [7] glue_1.8.0           arrow_14.0.2.1       data.table_1.15.2    toolbox_0.1.1        janitor_2.2.0        lubridate_1.9.3
>[13] forcats_1.0.0        stringr_1.5.1        dplyr_1.1.4          purrr_1.0.2          readr_2.1.5          tidyr_1.3.1
>[19] tibble_3.2.1         ggplot2_3.5.0        tidyverse_2.0.0      duckdb_1.1.2         DBI_1.2.3            fs_1.6.3
>
>
>Happy to provide additional information if it would be helpful.
>
>Thank you in advance!
>The information in this e-mail is intended only for the...{{dropped:23}}


From g@@@powe|| @end|ng |rom protonm@||@com  Wed Dec 11 01:52:27 2024
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg Powell)
Date: Wed, 11 Dec 2024 00:52:27 +0000
Subject: [R] Cores hang when calling mcapply
In-Reply-To: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
References: <SA2PR04MB75005AE94F0FE0B9F22E94EDCB3E2@SA2PR04MB7500.namprd04.prod.outlook.com>
Message-ID: <4E0RX-KjQotMJQqoZSTmkP26Nat7LaPPYPmThroNPdSsqyCVyZVxJcwib8VZ1wUxk4h_FaU-JgHsn-uV3MqfDsuG-Y2Kq9xOjXCZThR1rxc=@protonmail.com>

Hello Thomas,

Consider that the primary bottleneck may be tied to memory usage and the complexity of pivoting extremely large datasets into wide formats with tens of thousands of unique values per column. Extremely large expansions of columns inherently stress both memory and CPU, and splitting into 110k separate data frames before pivoting and combining them again is likely causing resource overhead and system instability.

Perhaps, evaluate if the presence/absence transformation can be done in a more memory-efficient manner without pivoting all at once. Since you are dealing with extremely large data, a more incremental or streaming approach may be necessary. Instead of splitting into thousands of individual data frames and trying to pivot each in parallel, consider instead  a method that processes segments of data to incrementally build a large sparse matrix or a compressed representation, then combine results at the end.

It's probbaly better to move away from `pivot_wider()` on a massive scale and attempt a data.table-based approach, which is often more memory-efficient and faster for large-scale operations in R. 


An alternate way would be data.table?s `dcast()` can handle large data more efficiently, and data.table?s in-memory operations often reduce overhead compared to tidyverse pivoting functions.

Also - consider using data.table?s `fread()` or `arrow::open_dataset()` directly with `as.data.table()` to keep everything in a data.table format. For example, you can do a large `dcast()` operation to create presence/absence columns by group. If your categories are extremely large, consider an approach that processes categories in segments as I mentioned earlier -  and writes intermediate results to disk, then combines/mergesresults at the end.

Limit parallelization when dealing with massive reshapes. Instead of trying to parallelize the entire pivot across thousands of subsets, run a single parallelized chunking approach that processes manageable subsets and writes out intermediate results (for example... using `fwrite()` for each subset). After processing, load and combine these intermediate results. This manual segmenting approach can circumvent the "zombie" processes you mentioned - that I think arise from overly complex parallel nesting and excessivememory utilization.

If the presence/absence indicators are ultimately sparse (many zeros and few ones), consider storing the result in a sparse matrix format (for exapmple- `Matrix` package in R). Instead of creating thousands of columns as dense integers, using a sparse matrix representation should dramatically reduce memory. After processing the data into a sparse format, you can then save it in a suitable file format and only convert to a dense format if absolutely necessary.

Below is a reworked code segment using data.table for a more scalable approach. Note that this is a conceptual template. In practice, adapt the chunk sizes and filtering operations to your workflow. The idea is to avoid creating 110k separate data frames and to handle the pivot in a data.table manner that?s more robust and less memory intensve. Here, presence/absence encoding is done by grouping and casting directly rather than repeatedly splitting and row-binding.

> library(data.table)
> library(arrow)
>
> # Step A: Load data efficiently as data.table
> dt <- as.data.table(
>   open_dataset(
>    sources = input_files,
>    format = 'csv',
>    unify_schema = TRUE,
>    col_types = schema(
>      "ID_Key" = string(),
>      "column1" = string(),
>      "column2" = string()
>    )
>  ) |> 

>    collect()
> )
>
> # Step B: Clean names once
> # Assume `crewjanitormakeclean` essentially standardizes column names
> dt[, column1 := janitor::make_clean_names(column1, allow_dupes =  

> TRUE)]
> dt[, column2 := janitor::make_clean_names(column2, allow_dupes = 

>  TRUE)]
>
> # Step C: Create presence/absence indicators using data.table
> # Use dcast to pivot wide. Set n=1 for presence, 0 for absence.
> # For large unique values, consider chunking if needed.
> out1 <- dcast(dt[!is.na(column1)], ID_Key ~ column1, fun.aggregate = 

> length, value.var = "column1")
> out2 <- dcast(dt[!is.na(column2)], ID_Key ~ column2, fun.aggregate = 

> length, value.var = "column2")
>
> # Step D: Merge the two wide tables by ID_Key
> # Fill missing columns with 0 using data.table on-the-fly operations
> all_cols <- unique(c(names(out1), names(out2)))
> out1_missing <- setdiff(all_cols, names(out1))
> out2_missing <- setdiff(all_cols, names(out2))
>
> # Add missing columns with 0
> for (col in out1_missing) out1[, (col) := 0]
> for (col in out2_missing) out2[, (col) := 0]
>
> # Ensure column order alignment if needed
> setcolorder(out1, all_cols)
> setcolorder(out2, all_cols)
>
> # Combine by ID_Key (since they share same columns now)
> final_dt <- rbindlist(list(out1, out2), use.names = TRUE, fill = TRUE)
>
> # Step E: If needed, summarize across ID_Key to sum presence 

> indicators
> final_result <- final_dt[, lapply(.SD, sum, na.rm = TRUE), by = 

> ID_Key, .SDcols = setdiff(names(final_dt), "ID_Key")]
>
> # note that final_result should now contain summed presence/absence 

> (0/1) indicators.




Hope this helps!
gregg
somewhereinArizona
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 509 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20241211/2385b049/attachment.sig>

